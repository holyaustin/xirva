[{"id": "1104.0126", "submitter": "David Vallet David Vallet", "authors": "Fabian Abel, Ilknur Celik, Claudia Hauff, Laura Hollink, Geert-Jan\n  Houben", "title": "U-Sem: Semantic Enrichment, User Modeling and Mining of Usage Data on\n  the Social Web", "comments": "1st International Workshop on Usage Analysis and the Web of Data\n  (USEWOD2011) in the 20th International World Wide Web Conference (WWW2011),\n  Hyderabad, India, March 28th, 2011", "journal-ref": null, "doi": null, "report-no": "WWW2011USEWOD/2011/abecelhauholhou", "categories": "cs.IR cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growing popularity of Social Web applications, more and more user\ndata is published on the Web everyday. Our research focuses on investigating\nways of mining data from such platforms that can be used for modeling users and\nfor semantically augmenting user profiles. This process can enhance adaptation\nand personalization in various adaptive Web-based systems. In this paper, we\npresent the U-Sem people modeling service, a framework for the semantic\nenrichment and mining of people's profiles from usage data on the Social Web.\nWe explain the architecture of our people modeling service and describe its\napplication in an adult e-learning context as an example. Versions: Mar 21,\n10:10, Mar 25, 09:37\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2011 09:59:10 GMT"}], "update_date": "2011-04-04", "authors_parsed": [["Abel", "Fabian", ""], ["Celik", "Ilknur", ""], ["Hauff", "Claudia", ""], ["Hollink", "Laura", ""], ["Houben", "Geert-Jan", ""]]}, {"id": "1104.0128", "submitter": "David Vallet David Vallet", "authors": "Vera Hollink, Arjen de Vries", "title": "Towards an automated query modification assistant", "comments": "1st International Workshop on Usage Analysis and the Web of Data\n  (USEWOD2011) in the 20th International World Wide Web Conference (WWW2011),\n  Hyderabad, India, March 28th, 2011", "journal-ref": null, "doi": null, "report-no": "WWW2011USEWOD/2011/holvri", "categories": "cs.IR cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Users who need several queries before finding what they need can benefit from\nan automatic search assistant that provides feedback on their query\nmodification strategies. We present a method to learn from a search log which\ntypes of query modifications have and have not been effective in the past. The\nmethod analyses query modifications along two dimensions: a traditional\nterm-based dimension and a semantic dimension, for which queries are enriches\nwith linked data entities. Applying the method to the search logs of two search\nengines, we identify six opportunities for a query modification assistant to\nimprove search: modification strategies that are commonly used, but that often\ndo not lead to satisfactory results.\n", "versions": [{"version": "v1", "created": "Fri, 1 Apr 2011 10:03:45 GMT"}], "update_date": "2011-04-04", "authors_parsed": [["Hollink", "Vera", ""], ["de Vries", "Arjen", ""]]}, {"id": "1104.0843", "submitter": "Minghao Yin", "authors": "Jian Gao, Minghao Yin, and Ke Xu", "title": "Phase Transitions in Knowledge Compilation: an Experimental Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phase transitions in many complex combinational problems have been widely\nstudied in the past decade. In this paper, we investigate phase transitions in\nthe knowledge compilation empirically, where DFA, OBDD and d-DNNF are chosen as\nthe target languages to compile random k-SAT instances. We perform intensive\nexperiments to analyze the sizes of compilation results and draw the following\nconclusions: there exists an easy-hard-easy pattern in compilations; the peak\npoint of sizes in the pattern is only related to the ratio of the number of\nclauses to that of variables when k is fixed, regardless of target languages;\nmost sizes of compilation results increase exponentially with the number of\nvariables growing, but there also exists a phase transition that separates a\npolynomial-increment region from the exponential-increment region; Moreover, we\nexplain why the phase transition in compilations occurs by analyzing\nmicrostructures of DFAs, and conclude that a kind of solution\ninterchangeability with more than 2 variables has a sharp transition near the\npeak point of the easy-hard-easy pattern, and thus it has a great impact on\nsizes of DFAs.\n", "versions": [{"version": "v1", "created": "Tue, 5 Apr 2011 13:25:43 GMT"}, {"version": "v2", "created": "Sun, 17 Apr 2011 12:41:23 GMT"}, {"version": "v3", "created": "Fri, 3 Jun 2011 07:05:11 GMT"}], "update_date": "2011-06-06", "authors_parsed": [["Gao", "Jian", ""], ["Yin", "Minghao", ""], ["Xu", "Ke", ""]]}, {"id": "1104.1045", "submitter": "Martin Hils", "authors": "Manuel Bodirsky, Martin Hils and Alex Krimkevich", "title": "Tractable Set Constraints", "comments": "An extended abstract of this paper appears in Proceedings of\n  IJCAI-11. The third author left the author team for the preparation of the\n  journal version. Several mistakes in the proofs have been removed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many fundamental problems in artificial intelligence, knowledge\nrepresentation, and verification involve reasoning about sets and relations\nbetween sets and can be modeled as set constraint satisfaction problems (set\nCSPs). Such problems are frequently intractable, but there are several\nimportant set CSPs that are known to be polynomial-time tractable. We introduce\na large class of set CSPs that can be solved in quadratic time. Our class,\nwhich we call EI, contains all previously known tractable set CSPs, but also\nsome new ones that are of crucial importance for example in description logics.\nThe class of EI set constraints has an elegant universal-algebraic\ncharacterization, which we use to show that every set constraint language that\nproperly contains all EI set constraints already has a finite sublanguage with\nan NP-hard constraint satisfaction problem.\n", "versions": [{"version": "v1", "created": "Wed, 6 Apr 2011 09:35:13 GMT"}, {"version": "v2", "created": "Wed, 18 Jul 2012 12:03:20 GMT"}], "update_date": "2012-07-19", "authors_parsed": [["Bodirsky", "Manuel", ""], ["Hils", "Martin", ""], ["Krimkevich", "Alex", ""]]}, {"id": "1104.1477", "submitter": "Arijit  Laha Ph.D.", "authors": "Arijit Laha", "title": "An Agent-based Architecture for a Knowledge-work Support System", "comments": "8 pages, ACM Compute 2011, Bangalore, India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enhancement of technology-based system support for knowledge workers is an\nissue of great importance. The \"Knowledge work Support System (KwSS)\" framework\nanalyzes this issue from a holistic perspective. KwSS proposes a set of design\nprinciples for building a comprehensive IT-based support system, which enhances\nthe capability of a human agent for performing a set of complex and\ninterrelated knowledge-works relevant to one or more target task-types within a\ndomain of professional activities. In this paper, we propose a high-level,\nsoftware-agent based architecture for realizing a KwSS system that incorporates\nthese design principles. Here we focus on developing a number of crucial\nenabling components of the architecture, including (1) an Activity Theory-based\nnovel modeling technique for knowledgeintensive activities; (2) a graph\ntheoretic formalism for representing these models in a knowledge base in\nconjunction with relevant entity taxonomies/ontologies; and (3) an algorithm\nfor reasoning, using the knowledge base, about various aspects of possible\nsupports for activities at performance-time.\n", "versions": [{"version": "v1", "created": "Fri, 8 Apr 2011 03:52:04 GMT"}], "update_date": "2011-04-11", "authors_parsed": [["Laha", "Arijit", ""]]}, {"id": "1104.1677", "submitter": "Muhammad Zaheer Aslam", "authors": "Bashir Ahmad, Shakeel Ahmad, Shahid Hussain, Muhammad Zaheer Aslam and\n  Zafar Abbas", "title": "Automatic Vehicle Checking Agent (VCA)", "comments": "5 pages, 2 figures", "journal-ref": "Control Theory and Informatics,ISSN 2224-5774 (print) ISSN\n  2225-0492 (online),Vol 1, No.2, 2011", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  A definition of intelligence is given in terms of performance that can be\nquantitatively measured. In this study, we have presented a conceptual model of\nIntelligent Agent System for Automatic Vehicle Checking Agent (VCA). To achieve\nthis goal, we have introduced several kinds of agents that exhibit intelligent\nfeatures. These are the Management agent, internal agent, External Agent,\nWatcher agent and Report agent. Metrics and measurements are suggested for\nevaluating the performance of Automatic Vehicle Checking Agent (VCA). Calibrate\ndata and test facilities are suggested to facilitate the development of\nintelligent systems.\n", "versions": [{"version": "v1", "created": "Sat, 9 Apr 2011 06:31:24 GMT"}, {"version": "v2", "created": "Sat, 3 Dec 2011 17:22:50 GMT"}], "update_date": "2011-12-06", "authors_parsed": [["Ahmad", "Bashir", ""], ["Ahmad", "Shakeel", ""], ["Hussain", "Shahid", ""], ["Aslam", "Muhammad Zaheer", ""], ["Abbas", "Zafar", ""]]}, {"id": "1104.1678", "submitter": "Muhammad Zaheer Aslam", "authors": "Muhammad Zaheer Aslam, Nasimullah, Abdur Rashid Khan", "title": "A Proposed Decision Support System/Expert System for Guiding Fresh\n  Students in Selecting a Faculty in Gomal University, Pakistan", "comments": "I have withdrawn for some changes", "journal-ref": "Industrial Engineering Letters www.iiste.org ISSN 2224-6096\n  (Print) ISSN 2225-0581(Online) Vol 1, No.4, 2011", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the design and development of a proposed rule based\nDecision Support System that will help students in selecting the best suitable\nfaculty/major decision while taking admission in Gomal University, Dera Ismail\nKhan, Pakistan. The basic idea of our approach is to design a model for testing\nand measuring the student capabilities like intelligence, understanding,\ncomprehension, mathematical concepts plus his/her past academic record plus\nhis/her intelligence level, and applying the module results to a rule-based\ndecision support system to determine the compatibility of those capabilities\nwith the available faculties/majors in Gomal University. The result is shown as\na list of suggested faculties/majors with the student capabilities and\nabilities.\n", "versions": [{"version": "v1", "created": "Sat, 9 Apr 2011 06:32:13 GMT"}, {"version": "v2", "created": "Fri, 20 Jan 2012 06:55:33 GMT"}, {"version": "v3", "created": "Thu, 8 Mar 2012 04:18:26 GMT"}], "update_date": "2012-03-09", "authors_parsed": [["Aslam", "Muhammad Zaheer", ""], ["Nasimullah", "", ""], ["Khan", "Abdur Rashid", ""]]}, {"id": "1104.1924", "submitter": "David Tolpin", "authors": "David Tolpin, Solomon Eyal Shimony", "title": "Rational Deployment of CSP Heuristics", "comments": "7 pages, 2 figures, to appear in IJCAI-2011, http://www.ijcai.org/", "journal-ref": "IJCAI-2011", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heuristics are crucial tools in decreasing search effort in varied fields of\nAI. In order to be effective, a heuristic must be efficient to compute, as well\nas provide useful information to the search algorithm. However, some well-known\nheuristics which do well in reducing backtracking are so heavy that the gain of\ndeploying them in a search algorithm might be outweighed by their overhead.\n  We propose a rational metareasoning approach to decide when to deploy\nheuristics, using CSP backtracking search as a case study. In particular, a\nvalue of information approach is taken to adaptive deployment of solution-count\nestimation heuristics for value ordering. Empirical results show that indeed\nthe proposed mechanism successfully balances the tradeoff between decreasing\nbacktracking and heuristic computational overhead, resulting in a significant\noverall search time reduction.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2011 12:12:14 GMT"}], "update_date": "2011-04-12", "authors_parsed": [["Tolpin", "David", ""], ["Shimony", "Solomon Eyal", ""]]}, {"id": "1104.2018", "submitter": "Varun Kanade", "authors": "Sham Kakade and Adam Tauman Kalai and Varun Kanade and Ohad Shamir", "title": "Efficient Learning of Generalized Linear and Single Index Models with\n  Isotonic Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized Linear Models (GLMs) and Single Index Models (SIMs) provide\npowerful generalizations of linear regression, where the target variable is\nassumed to be a (possibly unknown) 1-dimensional function of a linear\npredictor. In general, these problems entail non-convex estimation procedures,\nand, in practice, iterative local search heuristics are often used. Kalai and\nSastry (2009) recently provided the first provably efficient method for\nlearning SIMs and GLMs, under the assumptions that the data are in fact\ngenerated under a GLM and under certain monotonicity and Lipschitz constraints.\nHowever, to obtain provable performance, the method requires a fresh sample\nevery iteration. In this paper, we provide algorithms for learning GLMs and\nSIMs, which are both computationally and statistically efficient. We also\nprovide an empirical study, demonstrating their feasibility in practice.\n", "versions": [{"version": "v1", "created": "Mon, 11 Apr 2011 18:24:01 GMT"}], "update_date": "2011-04-14", "authors_parsed": [["Kakade", "Sham", ""], ["Kalai", "Adam Tauman", ""], ["Kanade", "Varun", ""], ["Shamir", "Ohad", ""]]}, {"id": "1104.2444", "submitter": "Claus-Peter Wirth", "authors": "Claus-Peter Wirth", "title": "A Simplified and Improved Free-Variable Framework for Hilbert's epsilon\n  as an Operator of Indefinite Committed Choice", "comments": "ii + 92 pages. arXiv admin note: text overlap with arXiv:0902.3749", "journal-ref": "IfCoLog Journal of Logics and their Applications, Vol. 4, number\n  2, March 2017, pp. 435-526", "doi": null, "report-no": "SEKI Report SR-2011-01", "categories": "cs.AI math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Free variables occur frequently in mathematics and computer science with ad\nhoc and altering semantics. We present the most recent version of our\nfree-variable framework for two-valued logics with properly improved\nfunctionality, but only two kinds of free variables left (instead of three):\nimplicitly universally and implicitly existentially quantified ones, now simply\ncalled \"free atoms\" and \"free variables\", respectively. The quantificational\nexpressiveness and the problem-solving facilities of our framework exceed\nstandard first-order and even higher-order modal logics, and directly support\nFermat's descente infinie. With the improved version of our framework, we can\nnow model also Henkin quantification, neither using quantifiers (binders) nor\nraising (Skolemization). We propose a new semantics for Hilbert's epsilon as a\nchoice operator with the following features: We avoid overspecification (such\nas right-uniqueness), but admit indefinite choice, committed choice, and\nclassical logics. Moreover, our semantics for the epsilon supports reductive\nproof search optimally.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2011 10:49:30 GMT"}, {"version": "v2", "created": "Sun, 5 Jun 2011 16:55:37 GMT"}, {"version": "v3", "created": "Sun, 17 Jul 2011 16:57:27 GMT"}, {"version": "v4", "created": "Tue, 17 Jan 2012 21:46:59 GMT"}, {"version": "v5", "created": "Thu, 24 Jan 2013 20:33:15 GMT"}, {"version": "v6", "created": "Thu, 9 May 2013 16:06:40 GMT"}, {"version": "v7", "created": "Tue, 19 May 2015 19:24:23 GMT"}, {"version": "v8", "created": "Fri, 19 Jun 2015 07:08:21 GMT"}, {"version": "v9", "created": "Wed, 1 Feb 2017 22:15:43 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Wirth", "Claus-Peter", ""]]}, {"id": "1104.2541", "submitter": "Serge Gaspers", "authors": "Serge Gaspers and Stefan Szeider", "title": "Kernels for Global Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bessiere et al. (AAAI'08) showed that several intractable global constraints\ncan be efficiently propagated when certain natural problem parameters are\nsmall. In particular, the complete propagation of a global constraint is\nfixed-parameter tractable in k - the number of holes in domains - whenever\nbound consistency can be enforced in polynomial time; this applies to the\nglobal constraints AtMost-NValue and Extended Global Cardinality (EGC).\n  In this paper we extend this line of research and introduce the concept of\nreduction to a problem kernel, a key concept of parameterized complexity, to\nthe field of global constraints. In particular, we show that the consistency\nproblem for AtMost-NValue constraints admits a linear time reduction to an\nequivalent instance on O(k^2) variables and domain values. This small kernel\ncan be used to speed up the complete propagation of NValue constraints. We\ncontrast this result by showing that the consistency problem for EGC\nconstraints does not admit a reduction to a polynomial problem kernel unless\nthe polynomial hierarchy collapses.\n", "versions": [{"version": "v1", "created": "Wed, 13 Apr 2011 16:27:20 GMT"}], "update_date": "2011-04-14", "authors_parsed": [["Gaspers", "Serge", ""], ["Szeider", "Stefan", ""]]}, {"id": "1104.2788", "submitter": "Johannes Klaus Fichte", "authors": "Johannes Klaus Fichte and Stefan Szeider", "title": "Backdoors to Tractable Answer-Set Programming", "comments": "This paper extends and updates papers that appeared in the\n  proceedings of IJCAI'11 (arXiv:1104.2788) and ESSLLI'11 (arXiv:1205.3663). We\n  provide a higher detail level, full proofs and more examples; present new\n  results on preprocessing, a general method to lift parameters from normal\n  programs to disjunctive programs, and a theoretical comparison of\n  ASP-parameters; and provide some empirical data", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answer Set Programming (ASP) is an increasingly popular framework for\ndeclarative programming that admits the description of problems by means of\nrules and constraints that form a disjunctive logic program. In particular,\nmany AI problems such as reasoning in a nonmonotonic setting can be directly\nformulated in ASP. Although the main problems of ASP are of high computational\ncomplexity, located at the second level of the Polynomial Hierarchy, several\nrestrictions of ASP have been identified in the literature, under which ASP\nproblems become tractable.\n  In this paper we use the concept of backdoors to identify new restrictions\nthat make ASP problems tractable. Small backdoors are sets of atoms that\nrepresent \"clever reasoning shortcuts\" through the search space and represent a\nhidden structure in the problem input. The concept of backdoors is widely used\nin the areas of propositional satisfiability and constraint satisfaction. We\nshow that it can be fruitfully adapted to ASP. We demonstrate how backdoors can\nserve as a unifying framework that accommodates several tractable restrictions\nof ASP known from the literature. Furthermore, we show how backdoors allow us\nto deploy recent algorithmic results from parameterized complexity theory to\nthe domain of answer set programming.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2011 14:59:45 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2012 19:55:55 GMT"}, {"version": "v3", "created": "Thu, 3 May 2012 17:23:05 GMT"}, {"version": "v4", "created": "Thu, 6 Mar 2014 17:39:40 GMT"}], "update_date": "2014-03-07", "authors_parsed": [["Fichte", "Johannes Klaus", ""], ["Szeider", "Stefan", ""]]}, {"id": "1104.2825", "submitter": "Carsten Lutz", "authors": "Carsten Lutz and Frank Wolter", "title": "Foundations for Uniform Interpolation and Forgetting in Expressive\n  Description Logics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study uniform interpolation and forgetting in the description logic ALC.\nOur main results are model-theoretic characterizations of uniform inter-\npolants and their existence in terms of bisimula- tions, tight complexity\nbounds for deciding the existence of uniform interpolants, an approach to\ncomputing interpolants when they exist, and tight bounds on their size. We use\na mix of model- theoretic and automata-theoretic methods that, as a by-product,\nalso provides characterizations of and decision procedures for conservative\nextensions.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2011 16:37:48 GMT"}], "update_date": "2011-04-15", "authors_parsed": [["Lutz", "Carsten", ""], ["Wolter", "Frank", ""]]}, {"id": "1104.2829", "submitter": "Carlos Gershenson", "authors": "Carlos Gershenson and David A. Rosenblueth", "title": "Self-organizing traffic lights at multiple-street intersections", "comments": "32 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": "C3 Report, 2011.02", "categories": "nlin.AO cs.AI nlin.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Summary: Traffic light coordination is a complex problem. In this paper, we\nextend previous work on an abstract model of city traffic to allow for multiple\nstreet intersections. We test a self-organizing method in our model, showing\nthat it is close to theoretical optima and superior to a traditional method of\ntraffic light coordination.\n  Abstract: The elementary cellular automaton following rule 184 can mimic\nparticles flowing in one direction at a constant speed. This automaton can\ntherefore model highway traffic. In a recent paper, we have incorporated\nintersections regulated by traffic lights to this model using exclusively\nelementary cellular automata. In such a paper, however, we only explored a\nrectangular grid. We now extend our model to more complex scenarios employing\nan hexagonal grid. This extension shows first that our model can readily\nincorporate multiple-way intersections and hence simulate complex scenarios. In\naddition, the current extension allows us to study and evaluate the behavior of\ntwo different kinds of traffic light controller for a grid of six-way streets\nallowing for either two or three street intersections: a traffic light that\ntries to adapt to the amount of traffic (which results in self-organizing\ntraffic lights) and a system of synchronized traffic lights with coordinated\nrigid periods (sometimes called the \"green wave\" method). We observe a tradeoff\nbetween system capacity and topological complexity. The green wave method is\nunable to cope with the complexity of a higher-capacity scenario, while the\nself-organizing method is scalable, adapting to the complexity of a scenario\nand exploiting its maximum capacity. Additionally, in this paper we propose a\nbenchmark, independent of methods and models, to measure the performance of a\ntraffic light controller comparing it against a theoretical optimum.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2011 16:51:37 GMT"}], "update_date": "2011-04-15", "authors_parsed": [["Gershenson", "Carlos", ""], ["Rosenblueth", "David A.", ""]]}, {"id": "1104.2842", "submitter": "Sebastian Ordyniak", "authors": "Sebastian Ordyniak and Stefan Szeider", "title": "Augmenting Tractable Fragments of Abstract Argumentation", "comments": "accepted for ijcai 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new and compelling approach to the efficient solution of\nimportant computational problems that arise in the context of abstract\nargumentation. Our approach makes known algorithms defined for restricted\nfragments generally applicable, at a computational cost that scales with the\ndistance from the fragment. Thus, in a certain sense, we gradually augment\ntractable fragments. Surprisingly, it turns out that some tractable fragments\nadmit such an augmentation and that others do not.\n  More specifically, we show that the problems of credulous and skeptical\nacceptance are fixed-parameter tractable when parameterized by the distance\nfrom the fragment of acyclic argumentation frameworks. Other tractable\nfragments such as the fragments of symmetrical and bipartite frameworks seem to\nprohibit an augmentation: the acceptance problems are already intractable for\nframeworks at distance 1 from the fragments.\n  For our study we use a broad setting and consider several different\nsemantics. For the algorithmic results we utilize recent advances in\nfixed-parameter tractability.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2011 17:31:07 GMT"}, {"version": "v2", "created": "Fri, 15 Apr 2011 12:19:54 GMT"}], "update_date": "2011-04-18", "authors_parsed": [["Ordyniak", "Sebastian", ""], ["Szeider", "Stefan", ""]]}, {"id": "1104.3152", "submitter": "Christopher Marriott", "authors": "Chris Marriott and Carlos Gershenson", "title": "Polyethism in a colony of artificial ants", "comments": "8 pages, 4 figures, submitted to ECAL 11", "journal-ref": "Advances in Artificial Life, ECAL 2011: Proceedings of the\n  Eleventh European Conference on the Synthesis and Simulation of Living\n  Systems, pp. 498-505, 2011", "doi": null, "report-no": "C3 Report 2011.03", "categories": "cs.AI nlin.AO q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore self-organizing strategies for role assignment in a foraging task\ncarried out by a colony of artificial agents. Our strategies are inspired by\nvarious mechanisms of division of labor (polyethism) observed in eusocial\ninsects like ants, termites, or bees. Specifically we instantiate models of\ncaste polyethism and age or temporal polyethism to evaluated the benefits to\nforaging in a dynamic environment. Our experiment is directly related to the\nexploration/exploitation trade of in machine learning.\n", "versions": [{"version": "v1", "created": "Fri, 15 Apr 2011 20:24:54 GMT"}], "update_date": "2011-09-06", "authors_parsed": [["Marriott", "Chris", ""], ["Gershenson", "Carlos", ""]]}, {"id": "1104.3250", "submitter": "Salah Rifai", "authors": "Salah Rifai, Xavier Glorot, Yoshua Bengio, Pascal Vincent", "title": "Adding noise to the input of a model trained with a regularized\n  objective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regularization is a well studied problem in the context of neural networks.\nIt is usually used to improve the generalization performance when the number of\ninput samples is relatively small or heavily contaminated with noise. The\nregularization of a parametric model can be achieved in different manners some\nof which are early stopping (Morgan and Bourlard, 1990), weight decay, output\nsmoothing that are used to avoid overfitting during the training of the\nconsidered model. From a Bayesian point of view, many regularization techniques\ncorrespond to imposing certain prior distributions on model parameters (Krogh\nand Hertz, 1991). Using Bishop's approximation (Bishop, 1995) of the objective\nfunction when a restricted type of noise is added to the input of a parametric\nfunction, we derive the higher order terms of the Taylor expansion and analyze\nthe coefficients of the regularization terms induced by the noisy input. In\nparticular we study the effect of penalizing the Hessian of the mapping\nfunction with respect to the input in terms of generalization performance. We\nalso show how we can control independently this coefficient by explicitly\npenalizing the Jacobian of the mapping function on corrupted inputs.\n", "versions": [{"version": "v1", "created": "Sat, 16 Apr 2011 18:09:13 GMT"}], "update_date": "2011-04-19", "authors_parsed": [["Rifai", "Salah", ""], ["Glorot", "Xavier", ""], ["Bengio", "Yoshua", ""], ["Vincent", "Pascal", ""]]}, {"id": "1104.3344", "submitter": "Diederik Aerts", "authors": "Diederik Aerts, Liane Gabora, Sandro Sozzo and Tomas Veloz", "title": "Quantum Structure in Cognition: Fundamentals and Applications", "comments": "9 pages", "journal-ref": "In V. Privman and V. Ovchinnikov (Eds.), IARIA, Proceedings of the\n  Fifth International Conference on Quantum, Nano and Micro Technologies, pp.\n  57-62, 2011", "doi": null, "report-no": null, "categories": "cs.AI cs.IR quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experiments in cognitive science and decision theory show that the ways in\nwhich people combine concepts and make decisions cannot be described by\nclassical logic and probability theory. This has serious implications for\napplied disciplines such as information retrieval, artificial intelligence and\nrobotics. Inspired by a mathematical formalism that generalizes quantum\nmechanics the authors have constructed a contextual framework for both concept\nrepresentation and decision making, together with quantum models that are in\nstrong alignment with experimental data. The results can be interpreted by\nassuming the existence in human thought of a double-layered structure, a\n'classical logical thought' and a 'quantum conceptual thought', the latter\nbeing responsible of the above paradoxes and nonclassical effects. The presence\nof a quantum structure in cognition is relevant, for it shows that quantum\nmechanics provides not only a useful modeling tool for experimental data but\nalso supplies a structural model for human and artificial thought processes.\nThis approach has strong connections with theories formalizing meaning, such as\nsemantic analysis, and has also a deep impact on computer science, information\nretrieval and artificial intelligence. More specifically, the links with\ninformation retrieval are discussed in this paper.\n", "versions": [{"version": "v1", "created": "Sun, 17 Apr 2011 20:28:49 GMT"}], "update_date": "2013-01-08", "authors_parsed": [["Aerts", "Diederik", ""], ["Gabora", "Liane", ""], ["Sozzo", "Sandro", ""], ["Veloz", "Tomas", ""]]}, {"id": "1104.3345", "submitter": "Diederik Aerts", "authors": "Diederik Aerts, Marek Czachor and Sandro Sozzo", "title": "Quantum Interaction Approach in Cognition, Artificial Intelligence and\n  Robotics", "comments": "10 pages", "journal-ref": "In V. Privman and V. Ovchinnikov (Eds.), IARIA, Proceedings of the\n  Fifth International Conference on Quantum, Nano and Micro Technologies, pp.\n  35-40, 2011", "doi": null, "report-no": null, "categories": "cs.AI cs.RO quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The mathematical formalism of quantum mechanics has been successfully\nemployed in the last years to model situations in which the use of classical\nstructures gives rise to problematical situations, and where typically quantum\neffects, such as 'contextuality' and 'entanglement', have been recognized. This\n'Quantum Interaction Approach' is briefly reviewed in this paper focusing, in\nparticular, on the quantum models that have been elaborated to describe how\nconcepts combine in cognitive science, and on the ensuing identification of a\nquantum structure in human thought. We point out that these results provide\ninteresting insights toward the development of a unified theory for meaning and\nknowledge formalization and representation. Then, we analyze the technological\naspects and implications of our approach, and a particular attention is devoted\nto the connections with symbolic artificial intelligence, quantum computation\nand robotics.\n", "versions": [{"version": "v1", "created": "Sun, 17 Apr 2011 20:30:18 GMT"}], "update_date": "2013-01-08", "authors_parsed": [["Aerts", "Diederik", ""], ["Czachor", "Marek", ""], ["Sozzo", "Sandro", ""]]}, {"id": "1104.3904", "submitter": "Lovro \\v{S}ubelj", "authors": "Lovro \\v{S}ubelj, \\v{S}tefan Furlan and Marko Bajec", "title": "An expert system for detecting automobile insurance fraud using social\n  network analysis", "comments": null, "journal-ref": "Expert Syst. Appl. 38(1), 1039-1052 (2011)", "doi": "10.1016/j.eswa.2010.07.143", "report-no": null, "categories": "cs.AI cs.SI physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article proposes an expert system for detection, and subsequent\ninvestigation, of groups of collaborating automobile insurance fraudsters. The\nsystem is described and examined in great detail, several technical\ndifficulties in detecting fraud are also considered, for it to be applicable in\npractice. Opposed to many other approaches, the system uses networks for\nrepresentation of data. Networks are the most natural representation of such a\nrelational domain, allowing formulation and analysis of complex relations\nbetween entities. Fraudulent entities are found by employing a novel assessment\nalgorithm, \\textit{Iterative Assessment Algorithm} (\\textit{IAA}), also\npresented in the article. Besides intrinsic attributes of entities, the\nalgorithm explores also the relations between entities. The prototype was\nevaluated and rigorously analyzed on real world data. Results show that\nautomobile insurance fraud can be efficiently detected with the proposed system\nand that appropriate data representation is vital.\n", "versions": [{"version": "v1", "created": "Tue, 19 Apr 2011 23:30:17 GMT"}], "update_date": "2011-04-21", "authors_parsed": [["\u0160ubelj", "Lovro", ""], ["Furlan", "\u0160tefan", ""], ["Bajec", "Marko", ""]]}, {"id": "1104.3927", "submitter": "Christian Drescher", "authors": "Christian Drescher and Toby Walsh", "title": "Translation-based Constraint Answer Set Solving", "comments": "Self-archived version for IJCAI'11 Best Paper Track submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We solve constraint satisfaction problems through translation to answer set\nprogramming (ASP). Our reformulations have the property that unit-propagation\nin the ASP solver achieves well defined local consistency properties like arc,\nbound and range consistency. Experiments demonstrate the computational value of\nthis approach.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2011 02:31:07 GMT"}], "update_date": "2011-04-21", "authors_parsed": [["Drescher", "Christian", ""], ["Walsh", "Toby", ""]]}, {"id": "1104.3929", "submitter": "Libin Shen", "authors": "Libin Shen", "title": "Understanding Exhaustive Pattern Learning", "comments": "15 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pattern learning in an important problem in Natural Language Processing\n(NLP). Some exhaustive pattern learning (EPL) methods (Bod, 1992) were proved\nto be flawed (Johnson, 2002), while similar algorithms (Och and Ney, 2004)\nshowed great advantages on other tasks, such as machine translation. In this\narticle, we first formalize EPL, and then show that the probability given by an\nEPL model is constant-factor approximation of the probability given by an\nensemble method that integrates exponential number of models obtained with\nvarious segmentations of the training data. This work for the first time\nprovides theoretical justification for the widely used EPL algorithm in NLP,\nwhich was previously viewed as a flawed heuristic method. Better understanding\nof EPL may lead to improved pattern learning algorithms in future.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2011 02:49:59 GMT"}], "update_date": "2011-04-21", "authors_parsed": [["Shen", "Libin", ""]]}, {"id": "1104.4024", "submitter": "Marco Pretti", "authors": "Alessandro Pelizzola, Marco Pretti, and Jort van Mourik", "title": "Palette-colouring: a belief-propagation approach", "comments": "22 pages, 7 figures", "journal-ref": null, "doi": "10.1088/1742-5468/2011/05/P05010", "report-no": null, "categories": "cond-mat.stat-mech cs.AI cs.DS math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a variation of the prototype combinatorial-optimisation problem\nknown as graph-colouring. Our optimisation goal is to colour the vertices of a\ngraph with a fixed number of colours, in a way to maximise the number of\ndifferent colours present in the set of nearest neighbours of each given\nvertex. This problem, which we pictorially call \"palette-colouring\", has been\nrecently addressed as a basic example of problem arising in the context of\ndistributed data storage. Even though it has not been proved to be NP complete,\nrandom search algorithms find the problem hard to solve. Heuristics based on a\nnaive belief propagation algorithm are observed to work quite well in certain\nconditions. In this paper, we build upon the mentioned result, working out the\ncorrect belief propagation algorithm, which needs to take into account the\nmany-body nature of the constraints present in this problem. This method\nimproves the naive belief propagation approach, at the cost of increased\ncomputational effort. We also investigate the emergence of a satisfiable to\nunsatisfiable \"phase transition\" as a function of the vertex mean degree, for\ndifferent ensembles of sparse random graphs in the large size (\"thermodynamic\")\nlimit.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2011 13:48:32 GMT"}], "update_date": "2015-05-27", "authors_parsed": [["Pelizzola", "Alessandro", ""], ["Pretti", "Marco", ""], ["van Mourik", "Jort", ""]]}, {"id": "1104.4053", "submitter": "Maurizio Lenzerini", "authors": "Maurizio Lenzerini, Domenico Fabio Savo", "title": "On the evolution of the instance level of DL-lite knowledge bases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent papers address the issue of updating the instance level of knowledge\nbases expressed in Description Logic following a model-based approach. One of\nthe outcomes of these papers is that the result of updating a knowledge base K\nis generally not expressible in the Description Logic used to express K. In\nthis paper we introduce a formula-based approach to this problem, by revisiting\nsome research work on formula-based updates developed in the '80s, in\nparticular the WIDTIO (When In Doubt, Throw It Out) approach. We show that our\noperator enjoys desirable properties, including that both insertions and\ndeletions according to such operator can be expressed in the DL used for the\noriginal KB. Also, we present polynomial time algorithms for the evolution of\nthe instance level knowledge bases expressed in the most expressive Description\nLogics of the DL-lite family.\n", "versions": [{"version": "v1", "created": "Wed, 20 Apr 2011 15:19:14 GMT"}], "update_date": "2011-04-21", "authors_parsed": [["Lenzerini", "Maurizio", ""], ["Savo", "Domenico Fabio", ""]]}, {"id": "1104.4153", "submitter": "Salah Rifai", "authors": "Salah Rifai, Xavier Muller, Xavier Glorot, Gregoire Mesnil, Yoshua\n  Bengio and Pascal Vincent", "title": "Learning invariant features through local space contraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present in this paper a novel approach for training deterministic\nauto-encoders. We show that by adding a well chosen penalty term to the\nclassical reconstruction cost function, we can achieve results that equal or\nsurpass those attained by other regularized auto-encoders as well as denoising\nauto-encoders on a range of datasets. This penalty term corresponds to the\nFrobenius norm of the Jacobian matrix of the encoder activations with respect\nto the input. We show that this penalty term results in a localized space\ncontraction which in turn yields robust features on the activation layer.\nFurthermore, we show how this penalty term is related to both regularized\nauto-encoders and denoising encoders and how it can be seen as a link between\ndeterministic and non-deterministic auto-encoders. We find empirically that\nthis penalty helps to carve a representation that better captures the local\ndirections of variation dictated by the data, corresponding to a\nlower-dimensional non-linear manifold, while being more invariant to the vast\nmajority of directions orthogonal to the manifold. Finally, we show that by\nusing the learned features to initialize a MLP, we achieve state of the art\nclassification error on a range of datasets, surpassing other methods of\npre-training.\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2011 01:39:25 GMT"}], "update_date": "2011-04-22", "authors_parsed": [["Rifai", "Salah", ""], ["Muller", "Xavier", ""], ["Glorot", "Xavier", ""], ["Mesnil", "Gregoire", ""], ["Bengio", "Yoshua", ""], ["Vincent", "Pascal", ""]]}, {"id": "1104.4290", "submitter": "Sebastian Ordyniak", "authors": "Eun Jung Kim, Sebastian Ordyniak, Stefan Szeider", "title": "Algorithms and Complexity Results for Persuasive Argumentation", "comments": null, "journal-ref": "Artificial Intelligence 175 (2011) pp. 1722-1736", "doi": "10.1016/j.artint.2011.03.001", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of arguments as abstract entities and their interaction as\nintroduced by Dung (Artificial Intelligence 177, 1995) has become one of the\nmost active research branches within Artificial Intelligence and Reasoning. A\nmain issue for abstract argumentation systems is the selection of acceptable\nsets of arguments. Value-based argumentation, as introduced by Bench-Capon (J.\nLogic Comput. 13, 2003), extends Dung's framework. It takes into account the\nrelative strength of arguments with respect to some ranking representing an\naudience: an argument is subjectively accepted if it is accepted with respect\nto some audience, it is objectively accepted if it is accepted with respect to\nall audiences. Deciding whether an argument is subjectively or objectively\naccepted, respectively, are computationally intractable problems. In fact, the\nproblems remain intractable under structural restrictions that render the main\ncomputational problems for non-value-based argumentation systems tractable. In\nthis paper we identify nontrivial classes of value-based argumentation systems\nfor which the acceptance problems are polynomial-time tractable. The classes\nare defined by means of structural restrictions in terms of the underlying\ngraphical structure of the value-based system. Furthermore we show that the\nacceptance problems are intractable for two classes of value-based systems that\nwhere conjectured to be tractable by Dunne (Artificial Intelligence 171, 2007).\n", "versions": [{"version": "v1", "created": "Thu, 21 Apr 2011 15:22:36 GMT"}], "update_date": "2011-05-16", "authors_parsed": [["Kim", "Eun Jung", ""], ["Ordyniak", "Sebastian", ""], ["Szeider", "Stefan", ""]]}, {"id": "1104.4617", "submitter": "Michael Codish", "authors": "Amit Metodi and Michael Codish and Vitaly Lagoon and Peter J. Stuckey", "title": "Boolean Equi-propagation for Optimized SAT Encoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to propagation based solving, Boolean\nequi-propagation, where constraints are modelled as propagators of information\nabout equalities between Boolean literals. Propagation based solving applies\nthis information as a form of partial evaluation resulting in optimized SAT\nencodings. We demonstrate for a variety of benchmarks that our approach results\nin smaller CNF encodings and leads to speed-ups in solving times.\n", "versions": [{"version": "v1", "created": "Sun, 24 Apr 2011 10:39:07 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Metodi", "Amit", ""], ["Codish", "Michael", ""], ["Lagoon", "Vitaly", ""], ["Stuckey", "Peter J.", ""]]}, {"id": "1104.4910", "submitter": "Minghao Yin", "authors": "Jian Gao, Minghao Yin, Junping Zhou", "title": "Hybrid Tractable Classes of Binary Quantified Constraint Satisfaction\n  Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the hybrid tractability of binary Quantified\nConstraint Satisfaction Problems (QCSPs). First, a basic tractable class of\nbinary QCSPs is identified by using the broken-triangle property. In this\nclass, the variable ordering for the broken-triangle property must be same as\nthat in the prefix of the QCSP. Second, we break this restriction to allow that\nexistentially quantified variables can be shifted within or out of their\nblocks, and thus identify some novel tractable classes by introducing the\nbroken-angle property. Finally, we identify a more generalized tractable class,\ni.e., the min-of-max extendable class for QCSPs.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2011 13:08:48 GMT"}], "update_date": "2011-04-27", "authors_parsed": [["Gao", "Jian", ""], ["Yin", "Minghao", ""], ["Zhou", "Junping", ""]]}, {"id": "1104.4950", "submitter": "Hamed Hassanzadeh", "authors": "Hamed Hassanzadeh and MohammadReza Keyvanpour", "title": "A Machine Learning Based Analytical Framework for Semantic Annotation\n  Requirements", "comments": null, "journal-ref": "International Journal of Web & Semantic Technology (IJWesT), Vol.\n  2, No. 2, pp. 27-38, Aprill 2011", "doi": "10.5121/ijwest.2011.2203", "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The Semantic Web is an extension of the current web in which information is\ngiven well-defined meaning. The perspective of Semantic Web is to promote the\nquality and intelligence of the current web by changing its contents into\nmachine understandable form. Therefore, semantic level information is one of\nthe cornerstones of the Semantic Web. The process of adding semantic metadata\nto web resources is called Semantic Annotation. There are many obstacles\nagainst the Semantic Annotation, such as multilinguality, scalability, and\nissues which are related to diversity and inconsistency in content of different\nweb pages. Due to the wide range of domains and the dynamic environments that\nthe Semantic Annotation systems must be performed on, the problem of automating\nannotation process is one of the significant challenges in this domain. To\novercome this problem, different machine learning approaches such as supervised\nlearning, unsupervised learning and more recent ones like, semi-supervised\nlearning and active learning have been utilized. In this paper we present an\ninclusive layered classification of Semantic Annotation challenges and discuss\nthe most important issues in this field. Also, we review and analyze machine\nlearning applications for solving semantic annotation problems. For this goal,\nthe article tries to closely study and categorize related researches for better\nunderstanding and to reach a framework that can map machine learning techniques\ninto the Semantic Annotation challenges and requirements.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2011 15:36:47 GMT"}], "update_date": "2012-08-06", "authors_parsed": [["Hassanzadeh", "Hamed", ""], ["Keyvanpour", "MohammadReza", ""]]}, {"id": "1104.4966", "submitter": "Jean Vincent Fonou Dombeu", "authors": "Jean Vincent Fonou Dombeu and Magda Huisman", "title": "Combining Ontology Development Methodologies and Semantic Web Platforms\n  for E-government Domain Ontology Development", "comments": "14 pages", "journal-ref": "International Journal of Web & Semantic Technology (IJWesT), Vol.\n  2, No. 2, April 2011", "doi": "10.5121/ijwest.2011.2202", "report-no": null, "categories": "cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key challenges in electronic government (e-government) is the\ndevelopment of systems that can be easily integrated and interoperated to\nprovide seamless services delivery to citizens. In recent years, Semantic Web\ntechnologies based on ontology have emerged as promising solutions to the above\nengineering problems. However, current research practicing semantic development\nin e-government does not focus on the application of available methodologies\nand platforms for developing government domain ontologies. Furthermore, only a\nfew of these researches provide detailed guidelines for developing semantic\nontology models from a government service domain. This research presents a case\nstudy combining an ontology building methodology and two state-of-the-art\nSemantic Web platforms namely Protege and Java Jena ontology API for semantic\nontology development in e-government. Firstly, a framework adopted from the\nUschold and King ontology building methodology is employed to build a domain\nontology describing the semantic content of a government service domain.\nThereafter, UML is used to semi-formally represent the domain ontology.\nFinally, Protege and Jena API are employed to create the Web Ontology Language\n(OWL) and Resource Description Framework (RDF) representations of the domain\nontology respectively to enable its computer processing. The study aims at: (1)\nproviding e-government developers, particularly those from the developing world\nwith detailed guidelines for practicing semantic content development in their\ne-government projects and (2), strengthening the adoption of semantic\ntechnologies in e-government. The study would also be of interest to novice\nSemantic Web developers who might used it as a starting point for further\ninvestigations.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2011 16:55:42 GMT"}], "update_date": "2011-04-27", "authors_parsed": [["Dombeu", "Jean Vincent Fonou", ""], ["Huisman", "Magda", ""]]}, {"id": "1104.4993", "submitter": "Berit Gru{\\ss}ien", "authors": "Hubie Chen, Victor Dalmau, Berit Gru{\\ss}ien", "title": "Arc Consistency and Friends", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A natural and established way to restrict the constraint satisfaction problem\nis to fix the relations that can be used to pose constraints; such a family of\nrelations is called a constraint language. In this article, we study arc\nconsistency, a heavily investigated inference method, and three extensions\nthereof from the perspective of constraint languages. We conduct a comparison\nof the studied methods on the basis of which constraint languages they solve,\nand we present new polynomial-time tractability results for singleton arc\nconsistency, the most powerful method studied.\n", "versions": [{"version": "v1", "created": "Tue, 26 Apr 2011 18:52:57 GMT"}], "update_date": "2011-04-27", "authors_parsed": [["Chen", "Hubie", ""], ["Dalmau", "Victor", ""], ["Gru\u00dfien", "Berit", ""]]}, {"id": "1104.5069", "submitter": "Tuan Nguyen", "authors": "Tuan Nguyen and Subbarao Kambhampati and Minh Do", "title": "Synthesizing Robust Plans under Incomplete Domain Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most current planners assume complete domain models and focus on generating\ncorrect plans. Unfortunately, domain modeling is a laborious and error-prone\ntask. While domain experts cannot guarantee completeness, often they are able\nto circumscribe the incompleteness of the model by providing annotations as to\nwhich parts of the domain model may be incomplete. In such cases, the goal\nshould be to generate plans that are robust with respect to any known\nincompleteness of the domain. In this paper, we first introduce annotations\nexpressing the knowledge of the domain incompleteness, and formalize the notion\nof plan robustness with respect to an incomplete domain model. We then propose\nan approach to compiling the problem of finding robust plans to the conformant\nprobabilistic planning problem. We present experimental results with\nProbabilistic-FF, a state-of-the-art planner, showing the promise of our\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2011 04:05:19 GMT"}], "update_date": "2011-04-28", "authors_parsed": [["Nguyen", "Tuan", ""], ["Kambhampati", "Subbarao", ""], ["Do", "Minh", ""]]}, {"id": "1104.5256", "submitter": "Shilin Ding", "authors": "Shilin Ding", "title": "Learning Undirected Graphical Models with Structure Penalty", "comments": "20 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In undirected graphical models, learning the graph structure and learning the\nfunctions that relate the predictive variables (features) to the responses\ngiven the structure are two topics that have been widely investigated in\nmachine learning and statistics. Learning graphical models in two stages will\nhave problems because graph structure may change after considering the\nfeatures. The main contribution of this paper is the proposed method that\nlearns the graph structure and functions on the graph at the same time. General\ngraphical models with binary outcomes conditioned on predictive variables are\nproved to be equivalent to multivariate Bernoulli model. The reparameterization\nof the potential functions in graphical model by conditional log odds ratios in\nmultivariate Bernoulli model offers advantage in the representation of the\nconditional independence structure in the model. Additionally, we impose a\nstructure penalty on groups of conditional log odds ratios to learn the graph\nstructure. These groups of functions are designed with overlaps to enforce\nhierarchical function selection. In this way, we are able to shrink higher\norder interactions to obtain a sparse graph structure. Simulation studies show\nthat the method is able to recover the graph structure. The analysis of county\ndata from Census Bureau gives interesting relations between unemployment rate,\ncrime and others discovered by the model.\n", "versions": [{"version": "v1", "created": "Wed, 27 Apr 2011 21:40:09 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Ding", "Shilin", ""]]}, {"id": "1104.5566", "submitter": "Stefan Szeider", "authors": "Stefan Szeider", "title": "Limits of Preprocessing", "comments": "This is a slightly longer version of a paper that appeared in the\n  proceedings of AAAI 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a first theoretical analysis of the power of polynomial-time\npreprocessing for important combinatorial problems from various areas in AI. We\nconsider problems from Constraint Satisfaction, Global Constraints,\nSatisfiability, Nonmonotonic and Bayesian Reasoning. We show that, subject to a\ncomplexity theoretic assumption, none of the considered problems can be reduced\nby polynomial-time preprocessing to a problem kernel whose size is polynomial\nin a structural problem parameter of the input, such as induced width or\nbackdoor size. Our results provide a firm theoretical boundary for the\nperformance of polynomial-time preprocessing algorithms for the considered\nproblems.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2011 08:31:41 GMT"}, {"version": "v2", "created": "Thu, 11 Aug 2011 15:53:23 GMT"}], "update_date": "2011-08-12", "authors_parsed": [["Szeider", "Stefan", ""]]}, {"id": "1104.5601", "submitter": "Shie Mannor", "authors": "Shie Mannor and John Tsitsiklis", "title": "Mean-Variance Optimization in Markov Decision Processes", "comments": "A full version of an ICML 2011 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider finite horizon Markov decision processes under performance\nmeasures that involve both the mean and the variance of the cumulative reward.\nWe show that either randomized or history-based policies can improve\nperformance. We prove that the complexity of computing a policy that maximizes\nthe mean reward under a variance constraint is NP-hard for some cases, and\nstrongly NP-hard for others. We finally offer pseudopolynomial exact and\napproximation algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 29 Apr 2011 11:39:40 GMT"}], "update_date": "2011-05-02", "authors_parsed": [["Mannor", "Shie", ""], ["Tsitsiklis", "John", ""]]}]