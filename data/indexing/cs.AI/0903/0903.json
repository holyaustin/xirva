[{"id": "0903.0041", "submitter": "Vit Niennattrakul", "authors": "Vit Niennattrakul and Chotirat Ann Ratanamahatana", "title": "Learning DTW Global Constraint for Time Series Classification", "comments": "The first runner up of Workshop and Challenge on Time Series\n  Classification held in conjunction with SIGKDD 2007. 8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  1-Nearest Neighbor with the Dynamic Time Warping (DTW) distance is one of the\nmost effective classifiers on time series domain. Since the global constraint\nhas been introduced in speech community, many global constraint models have\nbeen proposed including Sakoe-Chiba (S-C) band, Itakura Parallelogram, and\nRatanamahatana-Keogh (R-K) band. The R-K band is a general global constraint\nmodel that can represent any global constraints with arbitrary shape and size\neffectively. However, we need a good learning algorithm to discover the most\nsuitable set of R-K bands, and the current R-K band learning algorithm still\nsuffers from an 'overfitting' phenomenon. In this paper, we propose two new\nlearning algorithms, i.e., band boundary extraction algorithm and iterative\nlearning algorithm. The band boundary extraction is calculated from the bound\nof all possible warping paths in each class, and the iterative learning is\nadjusted from the original R-K band learning. We also use a Silhouette index, a\nwell-known clustering validation technique, as a heuristic function, and the\nlower bound function, LB_Keogh, to enhance the prediction speed. Twenty\ndatasets, from the Workshop and Challenge on Time Series Classification, held\nin conjunction of the SIGKDD 2007, are used to evaluate our approach.\n", "versions": [{"version": "v1", "created": "Sat, 28 Feb 2009 05:46:31 GMT"}], "update_date": "2009-03-03", "authors_parsed": [["Niennattrakul", "Vit", ""], ["Ratanamahatana", "Chotirat Ann", ""]]}, {"id": "0903.0174", "submitter": "Zhe Chen", "authors": "Zhe Chen, Dunwei Wen", "title": "Accelerating and Evaluation of Syntactic Parsing in Natural Language\n  Question Answering Systems", "comments": "7 pages, International Conference on Artificial Intelligence\n  (ICAI'07)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of Natural Language Processing (NLP), more and more\nsystems want to adopt NLP in User Interface Module to process user input, in\norder to communicate with user in a natural way. However, this raises a speed\nproblem. That is, if NLP module can not process sentences in durable time\ndelay, users will never use the system. As a result, systems which are strict\nwith processing time, such as dialogue systems, web search systems, automatic\ncustomer service systems, especially real-time systems, have to abandon NLP\nmodule in order to get a faster system response. This paper aims to solve the\nspeed problem. In this paper, at first, the construction of a syntactic parser\nwhich is based on corpus machine learning and statistics model is introduced,\nand then a speed problem analysis is performed on the parser and its\nalgorithms. Based on the analysis, two accelerating methods, Compressed POS Set\nand Syntactic Patterns Pruning, are proposed, which can effectively improve the\ntime efficiency of parsing in NLP module. To evaluate different parameters in\nthe accelerating algorithms, two new factors, PT and RT, are introduced and\nexplained in detail. Experiments are also completed to prove and test these\nmethods, which will surely contribute to the application of NLP.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2009 20:39:52 GMT"}, {"version": "v2", "created": "Tue, 10 Mar 2009 11:37:36 GMT"}], "update_date": "2009-09-30", "authors_parsed": [["Chen", "Zhe", ""], ["Wen", "Dunwei", ""]]}, {"id": "0903.0194", "submitter": "Marko A. Rodriguez", "authors": "Marko A. Rodriguez", "title": "A Graph Analysis of the Linked Data Cloud", "comments": null, "journal-ref": null, "doi": null, "report-no": "KRS-2009-01", "categories": "cs.CY cs.AI cs.SC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The Linked Data community is focused on integrating Resource Description\nFramework (RDF) data sets into a single unified representation known as the Web\nof Data. The Web of Data can be traversed by both man and machine and shows\npromise as the \\textit{de facto} standard for integrating data world wide much\nlike the World Wide Web is the \\textit{de facto} standard for integrating\ndocuments. On February 27$^\\text{th}$ of 2009, an updated Linked Data cloud\nvisualization was made publicly available. This visualization represents the\nvarious RDF data sets currently in the Linked Data cloud and their interlinking\nrelationships. For the purposes of this article, this visual representation was\nmanually transformed into a directed graph and analyzed.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2009 04:47:28 GMT"}], "update_date": "2009-03-03", "authors_parsed": [["Rodriguez", "Marko A.", ""]]}, {"id": "0903.0200", "submitter": "Marko A. Rodriguez", "authors": "Marko A. Rodriguez and Alberto Pepe", "title": "Faith in the Algorithm, Part 1: Beyond the Turing Test", "comments": null, "journal-ref": "Proceedings of the AISB Symposium on Computing and Philosophy, The\n  Society for the Study of Artificial Intelligence and Simulation of Behaviour,\n  Edinburgh, Scotland, April 2009.", "doi": null, "report-no": "LA-UR-09-00052", "categories": "cs.CY cs.AI", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Since the Turing test was first proposed by Alan Turing in 1950, the primary\ngoal of artificial intelligence has been predicated on the ability for\ncomputers to imitate human behavior. However, the majority of uses for the\ncomputer can be said to fall outside the domain of human abilities and it is\nexactly outside of this domain where computers have demonstrated their greatest\ncontribution to intelligence. Another goal for artificial intelligence is one\nthat is not predicated on human mimicry, but instead, on human amplification.\nThis article surveys various systems that contribute to the advancement of\nhuman and social intelligence.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2009 02:01:40 GMT"}], "update_date": "2010-02-16", "authors_parsed": [["Rodriguez", "Marko A.", ""], ["Pepe", "Alberto", ""]]}, {"id": "0903.0211", "submitter": "Nina Narodytska", "authors": "Christian Bessiere, Emmanuel Hebrard, Brahim Hnich, Zeynep Kiziltan,\n  Toby Walsh", "title": "Range and Roots: Two Common Patterns for Specifying and Propagating\n  Counting and Occurrence Constraints", "comments": "41 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Range and Roots which are two common patterns useful for\nspecifying a wide range of counting and occurrence constraints. We design\nspecialised propagation algorithms for these two patterns. Counting and\noccurrence constraints specified using these patterns thus directly inherit a\npropagation algorithm. To illustrate the capabilities of the Range and Roots\nconstraints, we specify a number of global constraints taken from the\nliterature. Preliminary experiments demonstrate that propagating counting and\noccurrence constraints using these two patterns leads to a small loss in\nperformance when compared to specialised global constraints and is competitive\nwith alternative decompositions using elementary constraints.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2009 05:58:11 GMT"}], "update_date": "2009-03-03", "authors_parsed": [["Bessiere", "Christian", ""], ["Hebrard", "Emmanuel", ""], ["Hnich", "Brahim", ""], ["Kiziltan", "Zeynep", ""], ["Walsh", "Toby", ""]]}, {"id": "0903.0276", "submitter": "Maziar Nekovee", "authors": "Maziar Nekovee", "title": "Impact of Cognitive Radio on Future Management of Spectrum", "comments": "Invited Paper, presented at the International Conference on Cognitive\n  Radio Oriented Wireless Communications and Networks (CrownCom), May 2008,\n  Singapore", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cognitive radio is a breakthrough technology which is expected to have a\nprofound impact on the way radio spectrum will be accessed, managed and shared\nin the future. In this paper I examine some of the implications of cognitive\nradio for future management of spectrum. Both a near-term view involving the\nopportunistic spectrum access model and a longer-term view involving a\nself-regulating dynamic spectrum access model within a society of cognitive\nradios are discussed.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2009 12:27:12 GMT"}], "update_date": "2009-03-03", "authors_parsed": [["Nekovee", "Maziar", ""]]}, {"id": "0903.0279", "submitter": "Jean Dezert", "authors": "Jean Dezert (ONERA), Florentin Smarandache", "title": "An introduction to DSmT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The management and combination of uncertain, imprecise, fuzzy and even\nparadoxical or high conflicting sources of information has always been, and\nstill remains today, of primal importance for the development of reliable\nmodern information systems involving artificial reasoning. In this\nintroduction, we present a survey of our recent theory of plausible and\nparadoxical reasoning, known as Dezert-Smarandache Theory (DSmT), developed for\ndealing with imprecise, uncertain and conflicting sources of information. We\nfocus our presentation on the foundations of DSmT and on its most important\nrules of combination, rather than on browsing specific applications of DSmT\navailable in literature. Several simple examples are given throughout this\npresentation to show the efficiency and the generality of this new approach.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2009 12:31:00 GMT"}], "update_date": "2009-03-03", "authors_parsed": [["Dezert", "Jean", "", "ONERA"], ["Smarandache", "Florentin", ""]]}, {"id": "0903.0314", "submitter": "Marvin Schiller", "authors": "Marvin Schiller and Christoph Benzmueller", "title": "Granularity-Adaptive Proof Presentation", "comments": "Extended Version. This SEKI Working-Paper refines and extends the\n  following publication: Granularity-Adaptive Proof Presentation. Proceedings\n  of the 14th International Conference on Artificial Intelligence in Education;\n  Brighton, UK, 2009. Submitted", "journal-ref": null, "doi": null, "report-no": "SEKI Working-Paper SWP-2009-01", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When mathematicians present proofs they usually adapt their explanations to\ntheir didactic goals and to the (assumed) knowledge of their addressees. Modern\nautomated theorem provers, in contrast, present proofs usually at a fixed level\nof detail (also called granularity). Often these presentations are neither\nintended nor suitable for human use. A challenge therefore is to develop user-\nand goal-adaptive proof presentation techniques that obey common mathematical\npractice. We present a flexible and adaptive approach to proof presentation\nthat exploits machine learning techniques to extract a model of the specific\ngranularity of proof examples and employs this model for the automated\ngeneration of further proofs at an adapted level of granularity.\n", "versions": [{"version": "v1", "created": "Mon, 2 Mar 2009 15:52:15 GMT"}, {"version": "v2", "created": "Tue, 3 Mar 2009 13:55:04 GMT"}, {"version": "v3", "created": "Tue, 12 May 2009 19:43:59 GMT"}, {"version": "v4", "created": "Mon, 25 May 2009 21:27:30 GMT"}], "update_date": "2009-05-26", "authors_parsed": [["Schiller", "Marvin", ""], ["Benzmueller", "Christoph", ""]]}, {"id": "0903.0422", "submitter": "Hirotaka Ono", "authors": "Kazuhisa Makino and Hirotaka Ono", "title": "Deductive Inference for the Interiors and Exteriors of Horn Theories", "comments": "20 pages, 1 figure, An extended abstract of this article was\n  presented in Proceedings of Algorithms and Computation, 19th International\n  Symposium (ISAAC 2008), Lecture Notes in Computer Science, Vol. 5369, pp.\n  390-401, Springer-Verlag Berlin Heidelberg, 2008", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC cs.DS cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the deductive inference for the interiors and\nexteriors of Horn knowledge bases, where the interiors and exteriors were\nintroduced by Makino and Ibaraki to study stability properties of knowledge\nbases. We present a linear time algorithm for the deduction for the interiors\nand show that it is co-NP-complete for the deduction for the exteriors. Under\nmodel-based representation, we show that the deduction problem for interiors is\nNP-complete while the one for exteriors is co-NP-complete. As for Horn\nenvelopes of the exteriors, we show that it is linearly solvable under\nmodel-based representation, while it is co-NP-complete under formula-based\nrepresentation. We also discuss the polynomially solvable cases for all the\nintractable problems.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2009 01:58:52 GMT"}], "update_date": "2009-03-04", "authors_parsed": [["Makino", "Kazuhisa", ""], ["Ono", "Hirotaka", ""]]}, {"id": "0903.0460", "submitter": "Toby Walsh", "authors": "Alan Frisch, Brahim Hnich, Zeynep Kiziltan, Ian Miguel, Toby Walsh", "title": "Filtering Algorithms for the Multiset Ordering Constraint", "comments": null, "journal-ref": "Artificial Intelligence, 173 (2), 299-328, 2009", "doi": "10.1016/j.artint.2008.11.001", "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constraint programming (CP) has been used with great success to tackle a wide\nvariety of constraint satisfaction problems which are computationally\nintractable in general. Global constraints are one of the important factors\nbehind the success of CP. In this paper, we study a new global constraint, the\nmultiset ordering constraint, which is shown to be useful in symmetry breaking\nand searching for leximin optimal solutions in CP. We propose efficient and\neffective filtering algorithms for propagating this global constraint. We show\nthat the algorithms are sound and complete and we discuss possible extensions.\nWe also consider alternative propagation methods based on existing constraints\nin CP toolkits. Our experimental results on a number of benchmark problems\ndemonstrate that propagating the multiset ordering constraint via a dedicated\nalgorithm can be very beneficial.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2009 08:04:42 GMT"}], "update_date": "2009-03-04", "authors_parsed": [["Frisch", "Alan", ""], ["Hnich", "Brahim", ""], ["Kiziltan", "Zeynep", ""], ["Miguel", "Ian", ""], ["Walsh", "Toby", ""]]}, {"id": "0903.0465", "submitter": "Toby Walsh", "authors": "Toby Walsh", "title": "Breaking Value Symmetry", "comments": "Proceedings of the Twenty-Third AAAI Conference on Artificial\n  Intelligence", "journal-ref": "AAAI 2008: 1585-1588", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symmetry is an important factor in solving many constraint satisfaction\nproblems. One common type of symmetry is when we have symmetric values. In a\nrecent series of papers, we have studied methods to break value symmetries. Our\nresults identify computational limits on eliminating value symmetry. For\ninstance, we prove that pruning all symmetric values is NP-hard in general.\nNevertheless, experiments show that much value symmetry can be broken in\npractice. These results may be useful to researchers in planning, scheduling\nand other areas as value symmetry occurs in many different domains.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2009 08:36:47 GMT"}], "update_date": "2009-03-04", "authors_parsed": [["Walsh", "Toby", ""]]}, {"id": "0903.0467", "submitter": "Toby Walsh", "authors": "Christian Bessiere and Emmanuel Hebrard and Brahim Hnich and Zeynep\n  Kiziltan and Toby Walsh", "title": "The Parameterized Complexity of Global Constraints", "comments": "Proceedings of the Twenty-Third AAAI Conference on Artificial\n  Intelligence", "journal-ref": "AAAI-2008, 235-240, 2008", "doi": null, "report-no": null, "categories": "cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We argue that parameterized complexity is a useful tool with which to study\nglobal constraints. In particular, we show that many global constraints which\nare intractable to propagate completely have natural parameters which make them\nfixed-parameter tractable and which are easy to compute. This tractability\ntends either to be the result of a simple dynamic program or of a decomposition\nwhich has a strong backdoor of bounded size. This strong backdoor is often a\ncycle cutset. We also show that parameterized complexity can be used to study\nother aspects of constraint programming like symmetry breaking. For instance,\nwe prove that value symmetry is fixed-parameter tractable to break in the\nnumber of symmetries. Finally, we argue that parameterized complexity can be\nused to derive results about the approximability of constraint propagation.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2009 08:44:47 GMT"}], "update_date": "2009-03-04", "authors_parsed": [["Bessiere", "Christian", ""], ["Hebrard", "Emmanuel", ""], ["Hnich", "Brahim", ""], ["Kiziltan", "Zeynep", ""], ["Walsh", "Toby", ""]]}, {"id": "0903.0470", "submitter": "Toby Walsh", "authors": "Claude-Guy Quimper and Toby Walsh", "title": "Decompositions of Grammar Constraints", "comments": "Proceedings of the Twenty-Third AAAI Conference on Artificial\n  Intelligence", "journal-ref": "AAAI 2008: 1567-1570", "doi": null, "report-no": null, "categories": "cs.AI cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wide range of constraints can be compactly specified using automata or\nformal languages. In a sequence of recent papers, we have shown that an\neffective means to reason with such specifications is to decompose them into\nprimitive constraints. We can then, for instance, use state of the art SAT\nsolvers and profit from their advanced features like fast unit propagation,\nclause learning, and conflict-based search heuristics. This approach holds\npromise for solving combinatorial problems in scheduling, rostering, and\nconfiguration, as well as problems in more diverse areas like bioinformatics,\nsoftware testing and natural language processing. In addition, decomposition\nmay be an effective method to propagate other global constraints.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2009 08:53:03 GMT"}], "update_date": "2009-03-04", "authors_parsed": [["Quimper", "Claude-Guy", ""], ["Walsh", "Toby", ""]]}, {"id": "0903.0471", "submitter": "Toby Walsh", "authors": "Christian Bessiere, Emmanuel Hebrard, Brahim Hnich, Zeynep Kiziltan,\n  Toby Walsh", "title": "SLIDE: A Useful Special Case of the CARDPATH Constraint", "comments": "18th European Conference on Artificial Intelligence", "journal-ref": "ECAI 2008: 475-479", "doi": "10.3233/978-1-58603-891-5-475", "report-no": null, "categories": "cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the CardPath constraint. This ensures a given constraint holds a\nnumber of times down a sequence of variables. We show that SLIDE, a special\ncase of CardPath where the slid constraint must hold always, can be used to\nencode a wide range of sliding sequence constraints including CardPath itself.\nWe consider how to propagate SLIDE and provide a complete propagator for\nCardPath. Since propagation is NP-hard in general, we identify special cases\nwhere propagation takes polynomial time. Our experiments demonstrate that using\nSLIDE to encode global constraints can be as efficient and effective as\nspecialised propagators.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2009 09:06:22 GMT"}], "update_date": "2009-03-04", "authors_parsed": [["Bessiere", "Christian", ""], ["Hebrard", "Emmanuel", ""], ["Hnich", "Brahim", ""], ["Kiziltan", "Zeynep", ""], ["Walsh", "Toby", ""]]}, {"id": "0903.0475", "submitter": "Nina Narodytska", "authors": "George Katsirelos, Nina Narodytska, Toby Walsh", "title": "Reformulating Global Grammar Constraints", "comments": "15 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An attractive mechanism to specify global constraints in rostering and other\ndomains is via formal languages. For instance, the Regular and Grammar\nconstraints specify constraints in terms of the languages accepted by an\nautomaton and a context-free grammar respectively. Taking advantage of the\nfixed length of the constraint, we give an algorithm to transform a\ncontext-free grammar into an automaton. We then study the use of minimization\ntechniques to reduce the size of such automata and speed up propagation. We\nshow that minimizing such automata after they have been unfolded and domains\ninitially reduced can give automata that are more compact than minimizing\nbefore unfolding and reducing. Experimental results show that such\ntransformations can improve the size of rostering problems that we can 'model\nand run'.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2009 09:31:41 GMT"}], "update_date": "2009-03-04", "authors_parsed": [["Katsirelos", "George", ""], ["Narodytska", "Nina", ""], ["Walsh", "Toby", ""]]}, {"id": "0903.0479", "submitter": "Nina Narodytska", "authors": "George Katsirelos, Nina Narodytska, Toby Walsh", "title": "Combining Symmetry Breaking and Global Constraints", "comments": "15 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new family of constraints which combine together lexicographical\nordering constraints for symmetry breaking with other common global\nconstraints. We give a general purpose propagator for this family of\nconstraints, and show how to improve its complexity by exploiting properties of\nthe included global constraints.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2009 09:52:01 GMT"}], "update_date": "2009-03-04", "authors_parsed": [["Katsirelos", "George", ""], ["Narodytska", "Nina", ""], ["Walsh", "Toby", ""]]}, {"id": "0903.0695", "submitter": "Shai Haim", "authors": "Shai Haim and Toby Walsh", "title": "Online Estimation of SAT Solving Runtime", "comments": "6 pages, 3 figures. Proc. of the 11th International Conf. on Theory\n  and Applications of Satisfiability Testing, Guangzhou, China, May 2008", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an online method for estimating the cost of solving SAT problems.\nModern SAT solvers present several challenges to estimate search cost including\nnon-chronological backtracking, learning and restarts. Our method uses a linear\nmodel trained on data gathered at the start of search. We show the\neffectiveness of this method using random and structured problems. We\ndemonstrate that predictions made in early restarts can be used to improve\nlater predictions. We also show that we can use such cost estimations to select\na solver from a portfolio.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2009 04:56:07 GMT"}], "update_date": "2009-03-05", "authors_parsed": [["Haim", "Shai", ""], ["Walsh", "Toby", ""]]}, {"id": "0903.0735", "submitter": "Joost Broekens", "authors": "Joost Broekens", "title": "Modeling the Experience of Emotion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Affective computing has proven to be a viable field of research comprised of\na large number of multidisciplinary researchers resulting in work that is\nwidely published. The majority of this work consists of computational models of\nemotion recognition, computational modeling of causal factors of emotion and\nemotion expression through rendered and robotic faces. A smaller part is\nconcerned with modeling the effects of emotion, formal modeling of cognitive\nappraisal theory and models of emergent emotions. Part of the motivation for\naffective computing as a field is to better understand emotional processes\nthrough computational modeling. One of the four major topics in affective\ncomputing is computers that have emotions (the others are recognizing,\nexpressing and understanding emotions). A critical and neglected aspect of\nhaving emotions is the experience of emotion (Barrett, Mesquita, Ochsner, and\nGross, 2007): what does the content of an emotional episode look like, how does\nthis content change over time and when do we call the episode emotional. Few\nmodeling efforts have these topics as primary focus. The launch of a journal on\nsynthetic emotions should motivate research initiatives in this direction, and\nthis research should have a measurable impact on emotion research in\npsychology. I show that a good way to do so is to investigate the psychological\ncore of what an emotion is: an experience. I present ideas on how the\nexperience of emotion could be modeled and provide evidence that several\ncomputational models of emotion are already addressing the issue.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2009 11:26:24 GMT"}], "update_date": "2009-03-05", "authors_parsed": [["Broekens", "Joost", ""]]}, {"id": "0903.0786", "submitter": "Carlos Loria-Saenz A", "authors": "Carlos Loria-Saenz", "title": "On Requirements for Programming Exercises from an E-learning Perspective", "comments": "ii + 31 pages", "journal-ref": null, "doi": null, "report-no": "SEKI Working-Paper SWP-2008-01", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we deal with the question of modeling programming exercises for\nnovices pointing to an e-learning scenario. Our purpose is to identify basic\nrequirements, raise some key questions and propose potential answers from a\nconceptual perspective. Presented as a general picture, we hypothetically\nsituate our work in a general context where e-learning instructional material\nneeds to be adapted to form part of an introductory Computer Science (CS)\ne-learning course at the CS1-level. Meant is a potential course which aims at\nimproving novices skills and knowledge on the essentials of programming by\nusing e-learning based approaches in connection (at least conceptually) with a\ngeneral host framework like Activemath (www.activemath.org). Our elaboration\ncovers contextual and, particularly, cognitive elements preparing the terrain\nfor eventual research stages in a derived project, as indicated. We concentrate\nour main efforts on reasoning mechanisms about exercise complexity that can\neventually offer tool support for the task of exercise authoring. We base our\nrequirements analysis on our own perception of the exercise subsystem provided\nby Activemath especially within the domain reasoner area. We enrich the\nanalysis by bringing to the discussion several relevant contextual elements\nfrom the CS1 courses, its definition and implementation. Concerning cognitive\nmodels and exercises, we build upon the principles of Bloom's Taxonomy as a\nrelatively standardized basis and use them as a framework for study and\nanalysis of complexity in basic programming exercises. Our analysis includes\nrequirements for the domain reasoner which are necessary for the exercise\nanalysis. We propose for such a purpose a three-layered conceptual model\nconsidering exercise evaluation, programming and metaprogramming.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2009 15:29:42 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2009 18:50:57 GMT"}], "update_date": "2009-03-13", "authors_parsed": [["Loria-Saenz", "Carlos", ""]]}, {"id": "0903.0829", "submitter": "Marko Horvat", "authors": "Marko Horvat, Sinisa Popovic, Nikola Bogunovic and Kresimir Cosic", "title": "Tagging multimedia stimuli with ontologies", "comments": "7 pages, 7 figures, 1 table, submitted for publication (MIPRO 2009)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Successful management of emotional stimuli is a pivotal issue concerning\nAffective Computing (AC) and the related research. As a subfield of Artificial\nIntelligence, AC is concerned not only with the design of computer systems and\nthe accompanying hardware that can recognize, interpret, and process human\nemotions, but also with the development of systems that can trigger human\nemotional response in an ordered and controlled manner. This requires the\nmaximum attainable precision and efficiency in the extraction of data from\nemotionally annotated databases While these databases do use keywords or tags\nfor description of the semantic content, they do not provide either the\nnecessary flexibility or leverage needed to efficiently extract the pertinent\nemotional content. Therefore, to this extent we propose an introduction of\nontologies as a new paradigm for description of emotionally annotated data. The\nability to select and sequence data based on their semantic attributes is vital\nfor any study involving metadata, semantics and ontological sorting like the\nSemantic Web or the Social Semantic Desktop, and the approach described in the\npaper facilitates reuse in these areas as well.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2009 18:13:41 GMT"}], "update_date": "2009-03-05", "authors_parsed": [["Horvat", "Marko", ""], ["Popovic", "Sinisa", ""], ["Bogunovic", "Nikola", ""], ["Cosic", "Kresimir", ""]]}, {"id": "0903.0843", "submitter": "Jordi Planes", "authors": "Vasco Manquinho and Joao Marques-Silva and Jordi Planes", "title": "Algorithms for Weighted Boolean Optimization", "comments": "14 pages, 2 algorithms, 3 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The Pseudo-Boolean Optimization (PBO) and Maximum Satisfiability (MaxSAT)\nproblems are natural optimization extensions of Boolean Satisfiability (SAT).\n  In the recent past, different algorithms have been proposed for PBO and for\nMaxSAT, despite the existence of straightforward mappings from PBO to MaxSAT\nand vice-versa. This papers proposes Weighted Boolean Optimization (WBO), a new\nunified framework that aggregates and extends PBO and MaxSAT. In addition, the\npaper proposes a new unsatisfiability-based algorithm for WBO, based on recent\nunsatisfiability-based algorithms for MaxSAT. Besides standard MaxSAT, the new\nalgorithm can also be used to solve weighted MaxSAT and PBO, handling\npseudo-Boolean constraints either natively or by translation to clausal form.\nExperimental results illustrate that unsatisfiability-based algorithms for\nMaxSAT can be orders of magnitude more efficient than existing dedicated\nalgorithms. Finally, the paper illustrates how other algorithms for either PBO\nor MaxSAT can be extended to WBO.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2009 20:21:56 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2009 09:18:32 GMT"}], "update_date": "2009-03-06", "authors_parsed": [["Manquinho", "Vasco", ""], ["Marques-Silva", "Joao", ""], ["Planes", "Jordi", ""]]}, {"id": "0903.1095", "submitter": "Jakub Mare\\v{c}ek", "authors": "Edmund K. Burke, Jakub Marecek, Andrew J. Parkes, Hana Rudova", "title": "Decomposition, Reformulation, and Diving in University Course\n  Timetabling", "comments": "45 pages, 7 figures. Improved typesetting of figures and tables", "journal-ref": "Computers and Operations Research (2010) 37(3), 582-597", "doi": "10.1016/j.cor.2009.02.023", "report-no": "NOTTCS-TR-2008-02", "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-life optimisation problems, there are multiple interacting\ncomponents in a solution. For example, different components might specify\nassignments to different kinds of resource. Often, each component is associated\nwith different sets of soft constraints, and so with different measures of soft\nconstraint violation. The goal is then to minimise a linear combination of such\nmeasures. This paper studies an approach to such problems, which can be thought\nof as multiphase exploitation of multiple objective-/value-restricted\nsubmodels. In this approach, only one computationally difficult component of a\nproblem and the associated subset of objectives is considered at first. This\nproduces partial solutions, which define interesting neighbourhoods in the\nsearch space of the complete problem. Often, it is possible to pick the initial\ncomponent so that variable aggregation can be performed at the first stage, and\nthe neighbourhoods to be explored next are guaranteed to contain feasible\nsolutions. Using integer programming, it is then easy to implement heuristics\nproducing solutions with bounds on their quality.\n  Our study is performed on a university course timetabling problem used in the\n2007 International Timetabling Competition, also known as the Udine Course\nTimetabling Problem. In the proposed heuristic, an objective-restricted\nneighbourhood generator produces assignments of periods to events, with\ndecreasing numbers of violations of two period-related soft constraints. Those\nare relaxed into assignments of events to days, which define neighbourhoods\nthat are easier to search with respect to all four soft constraints. Integer\nprogramming formulations for all subproblems are given and evaluated using ILOG\nCPLEX 11. The wider applicability of this approach is analysed and discussed.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2009 20:40:32 GMT"}, {"version": "v2", "created": "Fri, 20 Mar 2009 17:16:13 GMT"}], "update_date": "2014-04-10", "authors_parsed": [["Burke", "Edmund K.", ""], ["Marecek", "Jakub", ""], ["Parkes", "Andrew J.", ""], ["Rudova", "Hana", ""]]}, {"id": "0903.1136", "submitter": "Toby Walsh", "authors": "Toby Walsh", "title": "Symmetry Breaking Using Value Precedence", "comments": "17th European Conference on Artificial Intelligence", "journal-ref": "ECAI 2006, 168-172", "doi": "10.1088/1126-6708/2009/06/075", "report-no": null, "categories": "cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a comprehensive study of the use of value precedence constraints\nto break value symmetry. We first give a simple encoding of value precedence\ninto ternary constraints that is both efficient and effective at breaking\nsymmetry. We then extend value precedence to deal with a number of\ngeneralizations like wreath value and partial interchangeability. We also show\nthat value precedence is closely related to lexicographical ordering. Finally,\nwe consider the interaction between value precedence and symmetry breaking\nconstraints for variable symmetries.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2009 01:04:50 GMT"}], "update_date": "2015-05-13", "authors_parsed": [["Walsh", "Toby", ""]]}, {"id": "0903.1137", "submitter": "Toby Walsh", "authors": "Toby Walsh", "title": "Complexity of Terminating Preference Elicitation", "comments": "7th International Joint Conference on Autonomous Agents and\n  Multiagent Systems (AAMAS 2008)", "journal-ref": "AAMAS 2008: 967-974", "doi": null, "report-no": null, "categories": "cs.AI cs.CC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complexity theory is a useful tool to study computational issues surrounding\nthe elicitation of preferences, as well as the strategic manipulation of\nelections aggregating together preferences of multiple agents. We study here\nthe complexity of determining when we can terminate eliciting preferences, and\nprove that the complexity depends on the elicitation strategy. We show, for\ninstance, that it may be better from a computational perspective to elicit all\npreferences from one agent at a time than to elicit individual preferences from\nmultiple agents. We also study the connection between the strategic\nmanipulation of an election and preference elicitation. We show that what we\ncan manipulate affects the computational complexity of manipulation. In\nparticular, we prove that there are voting rules which are easy to manipulate\nif we can change all of an agent's vote, but computationally intractable if we\ncan change only some of their preferences. This suggests that, as with\npreference elicitation, a fine-grained view of manipulation may be informative.\nFinally, we study the connection between predicting the winner of an election\nand preference elicitation. Based on this connection, we identify a voting rule\nwhere it is computationally difficult to decide the probability of a candidate\nwinning given a probability distribution over the votes.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2009 01:14:44 GMT"}], "update_date": "2012-04-18", "authors_parsed": [["Walsh", "Toby", ""]]}, {"id": "0903.1139", "submitter": "Toby Walsh", "authors": "Christian Bessiere and Emmanuel Hebrard and Brahim Hnich and Toby\n  Walsh", "title": "The Complexity of Reasoning with Global Constraints", "comments": null, "journal-ref": "Constraints 12(2): 239-259 (2007)", "doi": null, "report-no": null, "categories": "cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constraint propagation is one of the techniques central to the success of\nconstraint programming. To reduce search, fast algorithms associated with each\nconstraint prune the domains of variables. With global (or non-binary)\nconstraints, the cost of such propagation may be much greater than the\nquadratic cost for binary constraints. We therefore study the computational\ncomplexity of reasoning with global constraints. We first characterise a number\nof important questions related to constraint propagation. We show that such\nquestions are intractable in general, and identify dependencies between the\ntractability and intractability of the different questions. We then demonstrate\nhow the tools of computational complexity can be used in the design and\nanalysis of specific global constraints. In particular, we illustrate how\ncomputational complexity can be used to determine when a lesser level of local\nconsistency should be enforced, when constraints can be safely generalized,\nwhen decomposing constraints will reduce the amount of pruning, and when\ncombining constraints is tractable.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2009 01:32:14 GMT"}], "update_date": "2009-03-09", "authors_parsed": [["Bessiere", "Christian", ""], ["Hebrard", "Emmanuel", ""], ["Hnich", "Brahim", ""], ["Walsh", "Toby", ""]]}, {"id": "0903.1146", "submitter": "Toby Walsh", "authors": "Toby Walsh", "title": "Breaking Value Symmetry", "comments": "Principles and Practice of Constraint Programming - CP 2007, 13th\n  International Conference, CP 2007, Providence, RI, USA, September 23-27,\n  2007, Proceedings. Lecture Notes in Computer Science 4741 Springer 2007, ISBN\n  978-3-540-74969-", "journal-ref": null, "doi": null, "report-no": "COMIC-2007-008", "categories": "cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One common type of symmetry is when values are symmetric. For example, if we\nare assigning colours (values) to nodes (variables) in a graph colouring\nproblem then we can uniformly interchange the colours throughout a colouring.\nFor a problem with value symmetries, all symmetric solutions can be eliminated\nin polynomial time. However, as we show here, both static and dynamic methods\nto deal with symmetry have computational limitations. With static methods,\npruning all symmetric values is NP-hard in general. With dynamic methods, we\ncan take exponential time on problems which static methods solve without\nsearch.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2009 03:50:17 GMT"}], "update_date": "2009-03-09", "authors_parsed": [["Walsh", "Toby", ""]]}, {"id": "0903.1147", "submitter": "Toby Walsh", "authors": "Yasuhiko Takenaga and Toby Walsh", "title": "Tetravex is NP-complete", "comments": null, "journal-ref": "Inf. Process. Lett. 99(5): 171-174 (2006)", "doi": null, "report-no": null, "categories": "cs.CC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tetravex is a widely played one person computer game in which you are given\n$n^2$ unit tiles, each edge of which is labelled with a number. The objective\nis to place each tile within a $n$ by $n$ square such that all neighbouring\nedges are labelled with an identical number. Unfortunately, playing Tetravex is\ncomputationally hard. More precisely, we prove that deciding if there is a\ntiling of the Tetravex board is NP-complete. Deciding where to place the tiles\nis therefore NP-hard. This may help to explain why Tetravex is a good puzzle.\nThis result compliments a number of similar results for one person games\ninvolving tiling. For example, NP-completeness results have been shown for: the\noffline version of Tetris, KPlumber (which involves rotating tiles containing\ndrawings of pipes to make a connected network), and shortest sliding puzzle\nproblems. It raises a number of open questions. For example, is the infinite\nversion Turing-complete? How do we generate Tetravex problems which are truly\npuzzling as random NP-complete problems are often surprising easy to solve? Can\nwe observe phase transition behaviour? What about the complexity of the problem\nwhen it is guaranteed to have an unique solution? How do we generate puzzles\nwith unique solutions?\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2009 04:00:47 GMT"}], "update_date": "2012-04-18", "authors_parsed": [["Takenaga", "Yasuhiko", ""], ["Walsh", "Toby", ""]]}, {"id": "0903.1150", "submitter": "Toby Walsh", "authors": "S. Armagan Tarim and Suresh Manandhar and Toby Walsh", "title": "Stochastic Constraint Programming: A Scenario-Based Approach", "comments": null, "journal-ref": "Constraints 11(1): 53-80 (2006)", "doi": "10.1007/s10601-006-6849-7", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To model combinatorial decision problems involving uncertainty and\nprobability, we introduce scenario based stochastic constraint programming.\nStochastic constraint programs contain both decision variables, which we can\nset, and stochastic variables, which follow a discrete probability\ndistribution. We provide a semantics for stochastic constraint programs based\non scenario trees. Using this semantics, we can compile stochastic constraint\nprograms down into conventional (non-stochastic) constraint programs. This\nallows us to exploit the full power of existing constraint solvers. We have\nimplemented this framework for decision making under uncertainty in stochastic\nOPL, a language which is based on the OPL constraint modelling language\n[Hentenryck et al., 1999]. To illustrate the potential of this framework, we\nmodel a wide range of problems in areas as diverse as portfolio\ndiversification, agricultural planning and production/inventory management.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2009 04:12:20 GMT"}], "update_date": "2009-03-09", "authors_parsed": [["Tarim", "S. Armagan", ""], ["Manandhar", "Suresh", ""], ["Walsh", "Toby", ""]]}, {"id": "0903.1152", "submitter": "Toby Walsh", "authors": "Toby Walsh", "title": "Stochastic Constraint Programming", "comments": "Proceedings of the 15th Eureopean Conference on Artificial\n  Intelligence", "journal-ref": "ECAI 2002: 111-115", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To model combinatorial decision problems involving uncertainty and\nprobability, we introduce stochastic constraint programming. Stochastic\nconstraint programs contain both decision variables (which we can set) and\nstochastic variables (which follow a probability distribution). They combine\ntogether the best features of traditional constraint satisfaction, stochastic\ninteger programming, and stochastic satisfiability. We give a semantics for\nstochastic constraint programs, and propose a number of complete algorithms and\napproximation procedures. Finally, we discuss a number of extensions of\nstochastic constraint programming to relax various assumptions like the\nindependence between stochastic variables, and compare with other approaches\nfor decision making under uncertainty.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2009 04:20:41 GMT"}], "update_date": "2009-03-09", "authors_parsed": [["Walsh", "Toby", ""]]}, {"id": "0903.1451", "submitter": "Frederic Dambreville", "authors": "Frederic Dambreville (DGA/Cta/DT/Gip)", "title": "Definition of evidence fusion rules on the basis of Referee Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI math.PR stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter defines a new concept and framework for constructing fusion\nrules for evidences. This framework is based on a referee function, which does\na decisional arbitrament conditionally to basic decisions provided by the\nseveral sources of information. A simple sampling method is derived from this\nframework. The purpose of this sampling approach is to avoid the combinatorics\nwhich are inherent to the definition of fusion rules of evidences. This\ndefinition of the fusion rule by the means of a sampling process makes possible\nthe construction of several rules on the basis of an algorithmic implementation\nof the referee function, instead of a mathematical formulation. Incidentally,\nit is a versatile and intuitive way for defining rules. The framework is\nimplemented for various well known evidence rules. On the basis of this\nframework, new rules for combining evidences are proposed, which takes into\naccount a consensual evaluation of the sources of information.\n", "versions": [{"version": "v1", "created": "Sun, 8 Mar 2009 19:59:39 GMT"}, {"version": "v2", "created": "Mon, 30 Mar 2009 11:49:20 GMT"}], "update_date": "2009-03-30", "authors_parsed": [["Dambreville", "Frederic", "", "DGA/Cta/DT/Gip"]]}, {"id": "0903.1659", "submitter": "Zhe Chen", "authors": "Zhe Chen", "title": "Heuristic Reasoning on Graph and Game Complexity of Sudoku", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Sudoku puzzle has achieved worldwide popularity recently, and attracted\ngreat attention of the computational intelligence community. Sudoku is always\nconsidered as Satisfiability Problem or Constraint Satisfaction Problem. In\nthis paper, we propose to focus on the essential graph structure underlying the\nSudoku puzzle. First, we formalize Sudoku as a graph. Then a solving algorithm\nbased on heuristic reasoning on the graph is proposed. The related r-Reduction\ntheorem, inference theorem and their properties are proved, providing the\nformal basis for developments of Sudoku solving systems. In order to evaluate\nthe difficulty levels of puzzles, a quantitative measurement of the complexity\nlevel of Sudoku puzzles based on the graph structure and information theory is\nproposed. Experimental results show that all the puzzles can be solved fast\nusing the proposed heuristic reasoning, and that the proposed game complexity\nmetrics can discriminate difficulty levels of puzzles perfectly.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2009 22:35:14 GMT"}], "update_date": "2009-03-11", "authors_parsed": [["Chen", "Zhe", ""]]}, {"id": "0903.1878", "submitter": "Denis Mindolin", "authors": "Denis Mindolin, Jan Chomicki", "title": "Contracting preference relations for database applications", "comments": "44 pages, 15 figures, submitted to the Special Issue of AIJ on\n  Preferences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The binary relation framework has been shown to be applicable to many\nreal-life preference handling scenarios. Here we study preference contraction:\nthe problem of discarding selected preferences. We argue that the property of\nminimality and the preservation of strict partial orders are crucial for\ncontractions. Contractions can be further constrained by specifying which\npreferences should be protected. We consider two classes of preference\nrelations: finite and finitely representable. We present algorithms for\ncomputing minimal and preference-protecting minimal contractions for finite as\nwell as finitely representable preference relations. We study relationships\nbetween preference change in the binary relation framework and belief change in\nthe belief revision theory. We also introduce some preference query\noptimization techniques which can be used in the presence of contraction. We\nevaluate the proposed algorithms experimentally and present the results.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2009 22:34:46 GMT"}], "update_date": "2009-03-12", "authors_parsed": [["Mindolin", "Denis", ""], ["Chomicki", "Jan", ""]]}, {"id": "0903.2528", "submitter": "Chendong Li", "authors": "Chendong Li", "title": "Airport Gate Assignment A Hybrid Model and Implementation", "comments": "5 pages, 2 figures, submitted to IC-AI 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.OH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of airlines, airports today become much busier and\nmore complicated than previous days. During airlines daily operations,\nassigning the available gates to the arriving aircrafts based on the fixed\nschedule is a very important issue, which motivates researchers to study and\nsolve Airport Gate Assignment Problems (AGAP) with all kinds of\nstate-of-the-art combinatorial optimization techniques. In this paper, we study\nthe AGAP and propose a novel hybrid mathematical model based on the method of\nconstraint programming and 0 - 1 mixed-integer programming. With the objective\nto minimize the number of gate conflicts of any two adjacent aircrafts assigned\nto the same gate, we build a mathematical model with logical constraints and\nthe binary constraints. For practical considerations, the potential objective\nof the model is also to minimize the number of gates that airlines must lease\nor purchase in order to run their business smoothly. We implement the model in\nthe Optimization Programming Language (OPL) and carry out empirical studies\nwith the data obtained from online timetable of Continental Airlines, Houston\nGorge Bush Intercontinental Airport IAH, which demonstrate that our model can\nprovide an efficient evaluation criteria for the airline companies to estimate\nthe efficiency of their current gate assignments.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2009 05:23:11 GMT"}], "update_date": "2009-03-17", "authors_parsed": [["Li", "Chendong", ""]]}, {"id": "0903.2851", "submitter": "Kamalika Chaudhuri", "authors": "Kamalika Chaudhuri, Yoav Freund, Daniel Hsu", "title": "A parameter-free hedging algorithm", "comments": "Updated Version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of decision-theoretic online learning (DTOL). Motivated\nby practical applications, we focus on DTOL when the number of actions is very\nlarge. Previous algorithms for learning in this framework have a tunable\nlearning rate parameter, and a barrier to using online-learning in practical\napplications is that it is not understood how to set this parameter optimally,\nparticularly when the number of actions is large.\n  In this paper, we offer a clean solution by proposing a novel and completely\nparameter-free algorithm for DTOL. We introduce a new notion of regret, which\nis more natural for applications with a large number of actions. We show that\nour algorithm achieves good performance with respect to this new notion of\nregret; in addition, it also achieves performance close to that of the best\nbounds achieved by previous algorithms with optimally-tuned parameters,\naccording to previous notions of regret.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2009 20:48:33 GMT"}, {"version": "v2", "created": "Mon, 18 Jan 2010 23:58:51 GMT"}], "update_date": "2010-01-19", "authors_parsed": [["Chaudhuri", "Kamalika", ""], ["Freund", "Yoav", ""], ["Hsu", "Daniel", ""]]}, {"id": "0903.2862", "submitter": "Kamalika Chaudhuri", "authors": "Kamalika Chaudhuri, Yoav Freund, Daniel Hsu", "title": "Tracking using explanation-based modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the tracking problem, namely, estimating the hidden state of an\nobject over time, from unreliable and noisy measurements. The standard\nframework for the tracking problem is the generative framework, which is the\nbasis of solutions such as the Bayesian algorithm and its approximation, the\nparticle filters. However, the problem with these solutions is that they are\nvery sensitive to model mismatches. In this paper, motivated by online\nlearning, we introduce a new framework -- an {\\em explanatory} framework -- for\ntracking. We provide an efficient tracking algorithm for this framework. We\nprovide experimental results comparing our algorithm to the Bayesian algorithm\non simulated data. Our experiments show that when there are slight model\nmismatches, our algorithm vastly outperforms the Bayesian algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2009 21:26:55 GMT"}, {"version": "v2", "created": "Tue, 19 Jan 2010 00:15:59 GMT"}], "update_date": "2010-01-19", "authors_parsed": [["Chaudhuri", "Kamalika", ""], ["Freund", "Yoav", ""], ["Hsu", "Daniel", ""]]}, {"id": "0903.2972", "submitter": "Ivo Danihelka", "authors": "Ivo Danihelka", "title": "Optimistic Simulated Exploration as an Incentive for Real Exploration", "comments": "accepted, noted that the initial path was 217 steps long", "journal-ref": "POSTER 2009", "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many reinforcement learning exploration techniques are overly optimistic and\ntry to explore every state. Such exploration is impossible in environments with\nthe unlimited number of states. I propose to use simulated exploration with an\noptimistic model to discover promising paths for real exploration. This reduces\nthe needs for the real exploration.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2009 14:24:13 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2009 13:31:55 GMT"}, {"version": "v3", "created": "Wed, 20 May 2009 18:44:07 GMT"}], "update_date": "2009-05-20", "authors_parsed": [["Danihelka", "Ivo", ""]]}, {"id": "0903.3103", "submitter": "Chunhua Shen", "authors": "Chunhua Shen, Sakrapee Paisitkriangkrai, and Jian Zhang", "title": "Efficiently Learning a Detection Cascade with Sparse Eigenvectors", "comments": "12 pages, conference version published in CVPR2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we first show that feature selection methods other than\nboosting can also be used for training an efficient object detector. In\nparticular, we introduce Greedy Sparse Linear Discriminant Analysis (GSLDA)\n\\cite{Moghaddam2007Fast} for its conceptual simplicity and computational\nefficiency; and slightly better detection performance is achieved compared with\n\\cite{Viola2004Robust}. Moreover, we propose a new technique, termed Boosted\nGreedy Sparse Linear Discriminant Analysis (BGSLDA), to efficiently train a\ndetection cascade. BGSLDA exploits the sample re-weighting property of boosting\nand the class-separability criterion of GSLDA.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2009 08:17:05 GMT"}], "update_date": "2009-03-19", "authors_parsed": [["Shen", "Chunhua", ""], ["Paisitkriangkrai", "Sakrapee", ""], ["Zhang", "Jian", ""]]}, {"id": "0903.3127", "submitter": "Tamir Hazan", "authors": "Tamir Hazan and Amnon Shashua", "title": "Norm-Product Belief Propagation: Primal-Dual Message-Passing for\n  Approximate Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we treat both forms of probabilistic inference, estimating\nmarginal probabilities of the joint distribution and finding the most probable\nassignment, through a unified message-passing algorithm architecture. We\ngeneralize the Belief Propagation (BP) algorithms of sum-product and\nmax-product and tree-rewaighted (TRW) sum and max product algorithms (TRBP) and\nintroduce a new set of convergent algorithms based on \"convex-free-energy\" and\nLinear-Programming (LP) relaxation as a zero-temprature of a\nconvex-free-energy. The main idea of this work arises from taking a general\nperspective on the existing BP and TRBP algorithms while observing that they\nall are reductions from the basic optimization formula of $f + \\sum_i h_i$\nwhere the function $f$ is an extended-valued, strictly convex but non-smooth\nand the functions $h_i$ are extended-valued functions (not necessarily convex).\nWe use tools from convex duality to present the \"primal-dual ascent\" algorithm\nwhich is an extension of the Bregman successive projection scheme and is\ndesigned to handle optimization of the general type $f + \\sum_i h_i$. Mapping\nthe fractional-free-energy variational principle to this framework introduces\nthe \"norm-product\" message-passing. Special cases include sum-product and\nmax-product (BP algorithms) and the TRBP algorithms. When the\nfractional-free-energy is set to be convex (convex-free-energy) the\nnorm-product is globally convergent for estimating of marginal probabilities\nand for approximating the LP-relaxation. We also introduce another branch of\nthe norm-product, the \"convex-max-product\". The convex-max-product is\nconvergent (unlike max-product) and aims at solving the LP-relaxation.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2009 10:33:31 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2009 10:07:09 GMT"}, {"version": "v3", "created": "Tue, 16 Jun 2009 12:07:38 GMT"}, {"version": "v4", "created": "Mon, 28 Jun 2010 15:37:57 GMT"}], "update_date": "2010-06-29", "authors_parsed": [["Hazan", "Tamir", ""], ["Shashua", "Amnon", ""]]}, {"id": "0903.3669", "submitter": "Xiu-Li Wang", "authors": "Xiuli Wang", "title": "Comment on \"Language Trees and Zipping\" arXiv:cond-mat/0108530", "comments": "2 pages comment on arXiv:cond-mat/0108530, arXiv:cond-mat/0203275", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IT math.IT", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Every encoding has priori information if the encoding represents any semantic\ninformation of the unverse or object. Encoding means mapping from the unverse\nto the string or strings of digits. The semantic here is used in the\nmodel-theoretic sense or denotation of the object. If encoding or strings of\nsymbols is the adequate and true mapping of model or object, and the mapping is\nrecursive or computable, the distance between two strings (text) is mapping the\ndistance between models. We then are able to measure the distance by computing\nthe distance between the two strings. Otherwise, we may take a misleading\ncourse. \"Language tree\" may not be a family tree in the sense of historical\nlinguistics. Rather it just means the similarity.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2009 14:29:11 GMT"}], "update_date": "2009-03-24", "authors_parsed": [["Wang", "Xiuli", ""]]}, {"id": "0903.3926", "submitter": "Andreas Meier", "authors": "Martin Homik, Andreas Meier", "title": "Designing a GUI for Proofs - Evaluation of an HCI Experiment", "comments": null, "journal-ref": null, "doi": null, "report-no": "SEKI Working-Paper SWP-2005-01", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Often user interfaces of theorem proving systems focus on assisting\nparticularly trained and skilled users, i.e., proof experts. As a result, the\nsystems are difficult to use for non-expert users. This paper describes a paper\nand pencil HCI experiment, in which (non-expert) students were asked to make\nsuggestions for a GUI for an interactive system for mathematical proofs. They\nhad to explain the usage of the GUI by applying it to construct a proof sketch\nfor a given theorem. The evaluation of the experiment provides insights for the\ninteraction design for non-expert users and the needs and wants of this user\ngroup.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2009 18:38:01 GMT"}], "update_date": "2009-03-24", "authors_parsed": [["Homik", "Martin", ""], ["Meier", "Andreas", ""]]}, {"id": "0903.4132", "submitter": "Bernat Corominas-Murtra BCM", "authors": "Joaqu\\'in Go\\~ni, I\\~nigo Martincorena, Bernat Corominas-Murtra,\n  Gonzalo Arrondo, Sergio Ardanza-Trevijano, Pablo Villoslada", "title": "Switcher-random-walks: a cognitive-inspired mechanism for network\n  exploration", "comments": "9 pages, 3 figures. Accepted in \"International Journal of\n  Bifurcations and Chaos\": Special issue on \"Modelling and Computation on\n  Complex Networks\"", "journal-ref": "International Journal of Bifurcation and Chaos 20, 913-922 (2010)", "doi": "10.1142/S0218127410026204", "report-no": null, "categories": "cs.AI cond-mat.dis-nn physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic memory is the subsystem of human memory that stores knowledge of\nconcepts or meanings, as opposed to life specific experiences. The organization\nof concepts within semantic memory can be understood as a semantic network,\nwhere the concepts (nodes) are associated (linked) to others depending on\nperceptions, similarities, etc. Lexical access is the complementary part of\nthis system and allows the retrieval of such organized knowledge. While\nconceptual information is stored under certain underlying organization (and\nthus gives rise to a specific topology), it is crucial to have an accurate\naccess to any of the information units, e.g. the concepts, for efficiently\nretrieving semantic information for real-time needings. An example of an\ninformation retrieval process occurs in verbal fluency tasks, and it is known\nto involve two different mechanisms: -clustering-, or generating words within a\nsubcategory, and, when a subcategory is exhausted, -switching- to a new\nsubcategory. We extended this approach to random-walking on a network\n(clustering) in combination to jumping (switching) to any node with certain\nprobability and derived its analytical expression based on Markov chains.\nResults show that this dual mechanism contributes to optimize the exploration\nof different network models in terms of the mean first passage time.\nAdditionally, this cognitive inspired dual mechanism opens a new framework to\nbetter understand and evaluate exploration, propagation and transport phenomena\nin other complex systems where switching-like phenomena are feasible.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2009 16:52:18 GMT"}], "update_date": "2010-06-18", "authors_parsed": [["Go\u00f1i", "Joaqu\u00edn", ""], ["Martincorena", "I\u00f1igo", ""], ["Corominas-Murtra", "Bernat", ""], ["Arrondo", "Gonzalo", ""], ["Ardanza-Trevijano", "Sergio", ""], ["Villoslada", "Pablo", ""]]}, {"id": "0903.4217", "submitter": "John Langford", "authors": "Alina Beygelzimer, John Langford, Yuri Lifshits, Gregory Sorkin, and\n  Alex Strehl", "title": "Conditional Probability Tree Estimation Analysis and Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of estimating the conditional probability of a label\nin time $O(\\log n)$, where $n$ is the number of possible labels. We analyze a\nnatural reduction of this problem to a set of binary regression problems\norganized in a tree structure, proving a regret bound that scales with the\ndepth of the tree. Motivated by this analysis, we propose the first online\nalgorithm which provably constructs a logarithmic depth tree on the set of\nlabels to solve this problem. We test the algorithm empirically, showing that\nit works succesfully on a dataset with roughly $10^6$ labels.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2009 00:28:44 GMT"}, {"version": "v2", "created": "Wed, 3 Jun 2009 21:19:34 GMT"}], "update_date": "2009-06-04", "authors_parsed": [["Beygelzimer", "Alina", ""], ["Langford", "John", ""], ["Lifshits", "Yuri", ""], ["Sorkin", "Gregory", ""], ["Strehl", "Alex", ""]]}, {"id": "0903.4513", "submitter": "Elena Wishnevskaya S.", "authors": "Elena S. Vishnevskaya", "title": "Building the information kernel and the problem of recognition", "comments": "This paper has been withdrawn by the author", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At this point in time there is a need for a new representation of different\ninformation, to identify and organize descending its characteristics. Today,\nscience is a powerful tool for the description of reality - the numbers. Why\nthe most important property of numbers. Suppose we have a number 0.2351734, it\nis clear that the figures are there in order of importance. If necessary, we\ncan round the number up to some value, eg 0.235. Arguably, the 0,235 - the most\nimportant information of 0.2351734. Thus, we can reduce the size of numbers is\nnot losing much with the accuracy. Clearly, if learning to provide a graphical\nor audio information kernel, we can provide the most relevant information,\ndiscarding the rest. Introduction of various kinds of information in an\ninformation kernel, is an important task, to solve many problems in artificial\nintelligence and information theory.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2009 10:05:29 GMT"}, {"version": "v2", "created": "Fri, 27 Mar 2009 09:48:51 GMT"}, {"version": "v3", "created": "Tue, 14 Apr 2009 13:26:01 GMT"}, {"version": "v4", "created": "Wed, 15 Apr 2009 06:37:09 GMT"}, {"version": "v5", "created": "Mon, 16 May 2011 08:12:10 GMT"}, {"version": "v6", "created": "Wed, 12 Oct 2011 03:02:51 GMT"}, {"version": "v7", "created": "Thu, 13 Oct 2011 08:13:25 GMT"}], "update_date": "2011-10-14", "authors_parsed": [["Vishnevskaya", "Elena S.", ""]]}, {"id": "0903.4856", "submitter": "Martin Jaggi", "authors": "Bernd G\\\"artner, Joachim Giesen, Martin Jaggi and Torsten Welsch", "title": "A Combinatorial Algorithm to Compute Regularization Paths", "comments": "7 Pages, 1 Figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a wide variety of regularization methods, algorithms computing the entire\nsolution path have been developed recently. Solution path algorithms do not\nonly compute the solution for one particular value of the regularization\nparameter but the entire path of solutions, making the selection of an optimal\nparameter much easier. Most of the currently used algorithms are not robust in\nthe sense that they cannot deal with general or degenerate input. Here we\npresent a new robust, generic method for parametric quadratic programming. Our\nalgorithm directly applies to nearly all machine learning applications, where\nso far every application required its own different algorithm.\n  We illustrate the usefulness of our method by applying it to a very low rank\nproblem which could not be solved by existing path tracking methods, namely to\ncompute part-worth values in choice based conjoint analysis, a popular\ntechnique from market research to estimate consumers preferences on a class of\nparameterized options.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2009 18:16:04 GMT"}], "update_date": "2009-03-30", "authors_parsed": [["G\u00e4rtner", "Bernd", ""], ["Giesen", "Joachim", ""], ["Jaggi", "Martin", ""], ["Welsch", "Torsten", ""]]}, {"id": "0903.4930", "submitter": "Petar Kormushev", "authors": "Petar Kormushev, Kohei Nomoto, Fangyan Dong, Kaoru Hirota", "title": "Time manipulation technique for speeding up reinforcement learning in\n  simulations", "comments": "12 pages", "journal-ref": "International Journal of Cybernetics and Information Technologies,\n  vol. 8, no. 1, pp. 12-24, 2008", "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A technique for speeding up reinforcement learning algorithms by using time\nmanipulation is proposed. It is applicable to failure-avoidance control\nproblems running in a computer simulation. Turning the time of the simulation\nbackwards on failure events is shown to speed up the learning by 260% and\nimprove the state space exploration by 12% on the cart-pole balancing task,\ncompared to the conventional Q-learning and Actor-Critic algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2009 01:09:00 GMT"}], "update_date": "2009-03-31", "authors_parsed": [["Kormushev", "Petar", ""], ["Nomoto", "Kohei", ""], ["Dong", "Fangyan", ""], ["Hirota", "Kaoru", ""]]}, {"id": "0903.5054", "submitter": "Knud Thomsen", "authors": "Knud Thomsen", "title": "Flow of Activity in the Ouroboros Model", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Ouroboros Model is a new conceptual proposal for an algorithmic structure\nfor efficient data processing in living beings as well as for artificial\nagents. Its central feature is a general repetitive loop where one iteration\ncycle sets the stage for the next. Sensory input activates data structures\n(schemata) with similar constituents encountered before, thus expectations are\nkindled. This corresponds to the highlighting of empty slots in the selected\nschema, and these expectations are compared with the actually encountered\ninput. Depending on the outcome of this consumption analysis different next\nsteps like search for further data or a reset, i.e. a new attempt employing\nanother schema, are triggered. Monitoring of the whole process, and in\nparticular of the flow of activation directed by the consumption analysis,\nyields valuable feedback for the optimum allocation of attention and resources\nincluding the selective establishment of useful new memory entries.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2009 15:29:17 GMT"}], "update_date": "2009-03-31", "authors_parsed": [["Thomsen", "Knud", ""]]}, {"id": "0903.5188", "submitter": "Vyacheslav Yukalov", "authors": "V.I. Yukalov and D. Sornette", "title": "Quantum decision theory as quantum theory of measurement", "comments": "Latex file, 11 pages", "journal-ref": "Phys. Lett. A 372 (2008) 6867-6871", "doi": "10.1016/j.physleta.2008.09.053", "report-no": null, "categories": "quant-ph cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general theory of quantum information processing devices, that\ncan be applied to human decision makers, to atomic multimode registers, or to\nmolecular high-spin registers. Our quantum decision theory is a generalization\nof the quantum theory of measurement, endowed with an action ring, a prospect\nlattice and a probability operator measure. The algebra of probability\noperators plays the role of the algebra of local observables. Because of the\ncomposite nature of prospects and of the entangling properties of the\nprobability operators, quantum interference terms appear, which make actions\nnoncommutative and the prospect probabilities non-additive. The theory provides\nthe basis for explaining a variety of paradoxes typical of the application of\nclassical utility theory to real human decision making. The principal advantage\nof our approach is that it is formulated as a self-consistent mathematical\ntheory, which allows us to explain not just one effect but actually all known\nparadoxes in human decision making. Being general, the approach can serve as a\ntool for characterizing quantum information processing by means of atomic,\nmolecular, and condensed-matter systems.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2009 11:29:30 GMT"}], "update_date": "2009-11-13", "authors_parsed": [["Yukalov", "V. I.", ""], ["Sornette", "D.", ""]]}, {"id": "0903.5289", "submitter": "Vincent Rialle", "authors": "Vincent Rialle (TIMC, DMIS), Annick Vila, Yves Besnard (TIMC)", "title": "Heterogeneous knowledge representation using a finite automaton and\n  first order logic: a case study in electromyography", "comments": null, "journal-ref": "Artificial Intelligence in Medicine 3, 2 (1991) 65-74", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a certain number of situations, human cognitive functioning is difficult\nto represent with classical artificial intelligence structures. Such a\ndifficulty arises in the polyneuropathy diagnosis which is based on the spatial\ndistribution, along the nerve fibres, of lesions, together with the synthesis\nof several partial diagnoses. Faced with this problem while building up an\nexpert system (NEUROP), we developed a heterogeneous knowledge representation\nassociating a finite automaton with first order logic. A number of knowledge\nrepresentation problems raised by the electromyography test features are\nexamined in this study and the expert system architecture allowing such a\nknowledge modeling are laid out.\n", "versions": [{"version": "v1", "created": "Mon, 30 Mar 2009 19:08:10 GMT"}], "update_date": "2009-03-31", "authors_parsed": [["Rialle", "Vincent", "", "TIMC, DMIS"], ["Vila", "Annick", "", "TIMC"], ["Besnard", "Yves", "", "TIMC"]]}]