[{"id": "1107.0018", "submitter": "A. Al-Ani", "authors": "A. Al-Ani, M. Deriche", "title": "A New Technique for Combining Multiple Classifiers using The\n  Dempster-Shafer Theory of Evidence", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 17, pages\n  333-361, 2002", "doi": "10.1613/jair.1026", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new classifier combination technique based on the\nDempster-Shafer theory of evidence. The Dempster-Shafer theory of evidence is a\npowerful method for combining measures of evidence from different classifiers.\nHowever, since each of the available methods that estimates the evidence of\nclassifiers has its own limitations, we propose here a new implementation which\nadapts to training data so that the overall mean square error is minimized. The\nproposed technique is shown to outperform most available classifier combination\nmethods when tested on three different classification problems.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2011 20:31:52 GMT"}], "update_date": "2011-07-04", "authors_parsed": [["Al-Ani", "A.", ""], ["Deriche", "M.", ""]]}, {"id": "1107.0019", "submitter": "S. Acid", "authors": "S. Acid, L. M. de Campos", "title": "Searching for Bayesian Network Structures in the Space of Restricted\n  Acyclic Partially Directed Graphs", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 18, pages\n  445-490, 2003", "doi": "10.1613/jair.1061", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although many algorithms have been designed to construct Bayesian network\nstructures using different approaches and principles, they all employ only two\nmethods: those based on independence criteria, and those based on a scoring\nfunction and a search procedure (although some methods combine the two). Within\nthe score+search paradigm, the dominant approach uses local search methods in\nthe space of directed acyclic graphs (DAGs), where the usual choices for\ndefining the elementary modifications (local changes) that can be applied are\narc addition, arc deletion, and arc reversal. In this paper, we propose a new\nlocal search method that uses a different search space, and which takes account\nof the concept of equivalence between network structures: restricted acyclic\npartially directed graphs (RPDAGs). In this way, the number of different\nconfigurations of the search space is reduced, thus improving efficiency.\nMoreover, although the final result must necessarily be a local optimum given\nthe nature of the search method, the topology of the new search space, which\navoids making early decisions about the directions of the arcs, may help to\nfind better local optima than those obtained by searching in the DAG space.\nDetailed results of the evaluation of the proposed search method on several\ntest problems, including the well-known Alarm Monitoring System, are also\npresented.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2011 20:32:05 GMT"}], "update_date": "2011-07-04", "authors_parsed": [["Acid", "S.", ""], ["de Campos", "L. M.", ""]]}, {"id": "1107.0020", "submitter": "O. Grumberg", "authors": "O. Grumberg, S. Livne, S. Markovitch", "title": "Learning to Order BDD Variables in Verification", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 18, pages\n  83-116, 2003", "doi": "10.1613/jair.1096", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The size and complexity of software and hardware systems have significantly\nincreased in the past years. As a result, it is harder to guarantee their\ncorrect behavior. One of the most successful methods for automated verification\nof finite-state systems is model checking. Most of the current model-checking\nsystems use binary decision diagrams (BDDs) for the representation of the\ntested model and in the verification process of its properties. Generally, BDDs\nallow a canonical compact representation of a boolean function (given an order\nof its variables). The more compact the BDD is, the better performance one gets\nfrom the verifier. However, finding an optimal order for a BDD is an\nNP-complete problem. Therefore, several heuristic methods based on expert\nknowledge have been developed for variable ordering. We propose an alternative\napproach in which the variable ordering algorithm gains 'ordering experience'\nfrom training models and uses the learned knowledge for finding good orders.\nOur methodology is based on offline learning of pair precedence classifiers\nfrom training models, that is, learning which variable pair permutation is more\nlikely to lead to a good order. For each training model, a number of training\nsequences are evaluated. Every training model variable pair permutation is then\ntagged based on its performance on the evaluated orders. The tagged\npermutations are then passed through a feature extractor and are given as\nexamples to a classifier creation algorithm. Given a model for which an order\nis requested, the ordering algorithm consults each precedence classifier and\nconstructs a pair precedence table which is used to create the order. Our\nalgorithm was integrated with SMV, which is one of the most widely used\nverification systems. Preliminary empirical evaluation of our methodology,\nusing real benchmark models, shows performance that is better than random\nordering and is competitive with existing algorithms that use expert knowledge.\nWe believe that in sub-domains of models (alu, caches, etc.) our system will\nprove even more valuable. This is because it features the ability to learn\nsub-domain knowledge, something that no other ordering algorithm does.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2011 20:32:16 GMT"}], "update_date": "2011-07-04", "authors_parsed": [["Grumberg", "O.", ""], ["Livne", "S.", ""], ["Markovitch", "S.", ""]]}, {"id": "1107.0021", "submitter": "W. E. Walsh", "authors": "W. E. Walsh, M. P. Wellman", "title": "Decentralized Supply Chain Formation: A Market Protocol and Competitive\n  Equilibrium Analysis", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 19, pages\n  513-567, 2003", "doi": "10.1613/jair.1213", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Supply chain formation is the process of determining the structure and terms\nof exchange relationships to enable a multilevel, multiagent production\nactivity. We present a simple model of supply chains, highlighting two\ncharacteristic features: hierarchical subtask decomposition, and resource\ncontention. To decentralize the formation process, we introduce a market price\nsystem over the resources produced along the chain. In a competitive\nequilibrium for this system, agents choose locally optimal allocations with\nrespect to prices, and outcomes are optimal overall. To determine prices, we\ndefine a market protocol based on distributed, progressive auctions, and\nmyopic, non-strategic agent bidding policies. In the presence of resource\ncontention, this protocol produces better solutions than the greedy protocols\ncommon in the artificial intelligence and multiagent systems literature. The\nprotocol often converges to high-value supply chains, and when competitive\nequilibria exist, typically to approximate competitive equilibria. However,\ncomplementarities in agent production technologies can cause the protocol to\nwastefully allocate inputs to agents that do not produce their outputs. A\nsubsequent decommitment phase recovers a significant fraction of the lost\nsurplus.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2011 20:32:28 GMT"}], "update_date": "2011-07-04", "authors_parsed": [["Walsh", "W. E.", ""], ["Wellman", "M. P.", ""]]}, {"id": "1107.0022", "submitter": "D. Monderer", "authors": "D. Monderer, M. Tennenholtz", "title": "K-Implementation", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 21, pages\n  37-62, 2004", "doi": "10.1613/jair.1231", "report-no": null, "categories": "cs.GT cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses an interested party who wishes to influence the behavior\nof agents in a game (multi-agent interaction), which is not under his control.\nThe interested party cannot design a new game, cannot enforce agents' behavior,\ncannot enforce payments by the agents, and cannot prohibit strategies available\nto the agents. However, he can influence the outcome of the game by committing\nto non-negative monetary transfers for the different strategy profiles that may\nbe selected by the agents. The interested party assumes that agents are\nrational in the commonly agreed sense that they do not use dominated\nstrategies. Hence, a certain subset of outcomes is implemented in a given game\nif by adding non-negative payments, rational players will necessarily produce\nan outcome in this subset. Obviously, by making sufficiently big payments one\ncan implement any desirable outcome. The question is what is the cost of\nimplementation? In this paper we introduce the notion of k-implementation of a\ndesired set of strategy profiles, where k stands for the amount of payment that\nneed to be actually made in order to implement desirable outcomes. A major\npoint in k-implementation is that monetary offers need not necessarily\nmaterialize when following desired behaviors. We define and study\nk-implementation in the contexts of games with complete and incomplete\ninformation. In the latter case we mainly focus on the VCG games. Our setting\nis later extended to deal with mixed strategies using correlation devices.\nTogether, the paper introduces and studies the implementation of desirable\noutcomes by a reliable party who cannot modify game rules (i.e. provide\nprotocols), complementing previous work in mechanism design, while making it\nmore applicable to many realistic CS settings.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2011 20:32:38 GMT"}], "update_date": "2011-07-04", "authors_parsed": [["Monderer", "D.", ""], ["Tennenholtz", "M.", ""]]}, {"id": "1107.0023", "submitter": "C. Boutilier", "authors": "C. Boutilier, R. I. Brafman, C. Domshlak, H. H. Hoos, D. Poole", "title": "CP-nets: A Tool for Representing and Reasoning withConditional Ceteris\n  Paribus Preference Statements", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 21, pages\n  135-191, 2004", "doi": "10.1613/jair.1234", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information about user preferences plays a key role in automated decision\nmaking. In many domains it is desirable to assess such preferences in a\nqualitative rather than quantitative way. In this paper, we propose a\nqualitative graphical representation of preferences that reflects conditional\ndependence and independence of preference statements under a ceteris paribus\n(all else being equal) interpretation. Such a representation is often compact\nand arguably quite natural in many circumstances. We provide a formal semantics\nfor this model, and describe how the structure of the network can be exploited\nin several inference tasks, such as determining whether one outcome dominates\n(is preferred to) another, ordering a set outcomes according to the preference\nrelation, and constructing the best outcome subject to available evidence.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2011 20:32:52 GMT"}], "update_date": "2011-07-04", "authors_parsed": [["Boutilier", "C.", ""], ["Brafman", "R. I.", ""], ["Domshlak", "C.", ""], ["Hoos", "H. H.", ""], ["Poole", "D.", ""]]}, {"id": "1107.0024", "submitter": "A. Darwiche", "authors": "A. Darwiche, J. D. Park", "title": "Complexity Results and Approximation Strategies for MAP Explanations", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 21, pages\n  101-133, 2006", "doi": "10.1613/jair.1236", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MAP is the problem of finding a most probable instantiation of a set of\nvariables given evidence. MAP has always been perceived to be significantly\nharder than the related problems of computing the probability of a variable\ninstantiation Pr, or the problem of computing the most probable explanation\n(MPE). This paper investigates the complexity of MAP in Bayesian networks.\nSpecifically, we show that MAP is complete for NP^PP and provide further\nnegative complexity results for algorithms based on variable elimination. We\nalso show that MAP remains hard even when MPE and Pr become easy. For example,\nwe show that MAP is NP-complete when the networks are restricted to polytrees,\nand even then can not be effectively approximated. Given the difficulty of\ncomputing MAP exactly, and the difficulty of approximating MAP while providing\nuseful guarantees on the resulting approximation, we investigate best effort\napproximations. We introduce a generic MAP approximation framework. We provide\ntwo instantiations of the framework; one for networks which are amenable to\nexact inference Pr, and one for networks for which even exact inference is too\nhard. This allows MAP approximation on networks that are too complex to even\nexactly solve the easier problems, Pr and MPE. Experimental results indicate\nthat using these approximation algorithms provides much better solutions than\nstandard techniques, and provide accurate MAP estimates in many cases.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2011 20:33:03 GMT"}], "update_date": "2011-07-04", "authors_parsed": [["Darwiche", "A.", ""], ["Park", "J. D.", ""]]}, {"id": "1107.0025", "submitter": "S. Edelkamp", "authors": "S. Edelkamp", "title": "Taming Numbers and Durations in the Model Checking Integrated Planning\n  System", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 20, pages\n  195-238, 2003", "doi": "10.1613/jair.1302", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Model Checking Integrated Planning System (MIPS) is a temporal least\ncommitment heuristic search planner based on a flexible object-oriented\nworkbench architecture. Its design clearly separates explicit and symbolic\ndirected exploration algorithms from the set of on-line and off-line computed\nestimates and associated data structures. MIPS has shown distinguished\nperformance in the last two international planning competitions. In the last\nevent the description language was extended from pure propositional planning to\ninclude numerical state variables, action durations, and plan quality objective\nfunctions. Plans were no longer sequences of actions but time-stamped\nschedules. As a participant of the fully automated track of the competition,\nMIPS has proven to be a general system; in each track and every benchmark\ndomain it efficiently computed plans of remarkable quality. This article\nintroduces and analyzes the most important algorithmic novelties that were\nnecessary to tackle the new layers of expressiveness in the benchmark problems\nand to achieve a high level of performance. The extensions include critical\npath analysis of sequentially generated plans to generate corresponding optimal\nparallel plans. The linear time algorithm to compute the parallel plan bypasses\nknown NP hardness results for partial ordering by scheduling plans with respect\nto the set of actions and the imposed precedence relations. The efficiency of\nthis algorithm also allows us to improve the exploration guidance: for each\nencountered planning state the corresponding approximate sequential plan is\nscheduled. One major strength of MIPS is its static analysis phase that grounds\nand simplifies parameterized predicates, functions and operators, that infers\nknowledge to minimize the state description length, and that detects domain\nobject symmetries. The latter aspect is analyzed in detail. MIPS has been\ndeveloped to serve as a complete and optimal state space planner, with\nadmissible estimates, exploration engines and branching cuts. In the\ncompetition version, however, certain performance compromises had to be made,\nincluding floating point arithmetic, weighted heuristic search exploration\naccording to an inadmissible estimate and parameterized optimization.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2011 20:33:38 GMT"}], "update_date": "2011-07-04", "authors_parsed": [["Edelkamp", "S.", ""]]}, {"id": "1107.0026", "submitter": "M. J. Nederhof", "authors": "M. J. Nederhof, G. Satta", "title": "IDL-Expressions: A Formalism for Representing and Parsing Finite\n  Languages in Natural Language Processing", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 21, pages\n  287-317, 2004", "doi": "10.1613/jair.1309", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a formalism for representation of finite languages, referred to as\nthe class of IDL-expressions, which combines concepts that were only considered\nin isolation in existing formalisms. The suggested applications are in natural\nlanguage processing, more specifically in surface natural language generation\nand in machine translation, where a sentence is obtained by first generating a\nlarge set of candidate sentences, represented in a compact way, and then by\nfiltering such a set through a parser. We study several formal properties of\nIDL-expressions and compare this new formalism with more standard ones. We also\npresent a novel parsing algorithm for IDL-expressions and prove a non-trivial\nupper bound on its time complexity.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2011 20:33:50 GMT"}], "update_date": "2011-07-04", "authors_parsed": [["Nederhof", "M. J.", ""], ["Satta", "G.", ""]]}, {"id": "1107.0027", "submitter": "T. Kocka", "authors": "T. Kocka, N. L. Zhang", "title": "Effective Dimensions of Hierarchical Latent Class Models", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 21, pages\n  1-17, 2004", "doi": "10.1613/jair.1311", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical latent class (HLC) models are tree-structured Bayesian networks\nwhere leaf nodes are observed while internal nodes are latent. There are no\ntheoretically well justified model selection criteria for HLC models in\nparticular and Bayesian networks with latent nodes in general. Nonetheless,\nempirical studies suggest that the BIC score is a reasonable criterion to use\nin practice for learning HLC models. Empirical studies also suggest that\nsometimes model selection can be improved if standard model dimension is\nreplaced with effective model dimension in the penalty term of the BIC score.\nEffective dimensions are difficult to compute. In this paper, we prove a\ntheorem that relates the effective dimension of an HLC model to the effective\ndimensions of a number of latent class models. The theorem makes it\ncomputationally feasible to compute the effective dimensions of large HLC\nmodels. The theorem can also be used to compute the effective dimensions of\ngeneral tree models.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2011 20:34:11 GMT"}], "update_date": "2011-07-04", "authors_parsed": [["Kocka", "T.", ""], ["Zhang", "N. L.", ""]]}, {"id": "1107.0029", "submitter": "M. H. Goker", "authors": "M. H. Goker, P. Langley, C. A. Thompson", "title": "A Personalized System for Conversational Recommendations", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 21, pages\n  393-428, 2004", "doi": "10.1613/jair.1318", "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Searching for and making decisions about information is becoming increasingly\ndifficult as the amount of information and number of choices increases.\nRecommendation systems help users find items of interest of a particular type,\nsuch as movies or restaurants, but are still somewhat awkward to use. Our\nsolution is to take advantage of the complementary strengths of personalized\nrecommendation systems and dialogue systems, creating personalized aides. We\npresent a system -- the Adaptive Place Advisor -- that treats item selection as\nan interactive, conversational process, with the program inquiring about item\nattributes and the user responding. Individual, long-term user preferences are\nunobtrusively obtained in the course of normal recommendation dialogues and\nused to direct future conversations with the same user. We present a novel user\nmodel that influences both item search and the questions asked during a\nconversation. We demonstrate the effectiveness of our system in significantly\nreducing the time and number of interactions required to find a satisfactory\nitem, as compared to a control group of users interacting with a non-adaptive\nversion of the system.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2011 20:34:41 GMT"}], "update_date": "2011-07-04", "authors_parsed": [["Goker", "M. H.", ""], ["Langley", "P.", ""], ["Thompson", "C. A.", ""]]}, {"id": "1107.0030", "submitter": "O. Arieli", "authors": "O. Arieli, M. Bruynooghe, M. Denecker, B. Van Nuffelen", "title": "Coherent Integration of Databases by Abductive Logic Programming", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 21, pages\n  245-286, 2004", "doi": "10.1613/jair.1322", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an abductive method for a coherent integration of independent\ndata-sources. The idea is to compute a list of data-facts that should be\ninserted to the amalgamated database or retracted from it in order to restore\nits consistency. This method is implemented by an abductive solver, called\nAsystem, that applies SLDNFA-resolution on a meta-theory that relates\ndifferent, possibly contradicting, input databases. We also give a pure\nmodel-theoretic analysis of the possible ways to `recover' consistent data from\nan inconsistent database in terms of those models of the database that exhibit\nas minimal inconsistent information as reasonably possible. This allows us to\ncharacterize the `recovered databases' in terms of the `preferred' (i.e., most\nconsistent) models of the theory. The outcome is an abductive-based application\nthat is sound and complete with respect to a corresponding model-based,\npreferential semantics, and -- to the best of our knowledge -- is more\nexpressive (thus more general) than any other implementation of coherent\nintegration of databases.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2011 20:34:53 GMT"}], "update_date": "2011-07-04", "authors_parsed": [["Arieli", "O.", ""], ["Bruynooghe", "M.", ""], ["Denecker", "M.", ""], ["Van Nuffelen", "B.", ""]]}, {"id": "1107.0031", "submitter": "P. Gorniak", "authors": "P. Gorniak, D. Roy", "title": "Grounded Semantic Composition for Visual Scenes", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 21, pages\n  429-470, 2004", "doi": "10.1613/jair.1327", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a visually-grounded language understanding model based on a study\nof how people verbally describe objects in scenes. The emphasis of the model is\non the combination of individual word meanings to produce meanings for complex\nreferring expressions. The model has been implemented, and it is able to\nunderstand a broad range of spatial referring expressions. We describe our\nimplementation of word level visually-grounded semantics and their embedding in\na compositional parsing framework. The implemented system selects the correct\nreferents in response to natural language expressions for a large percentage of\ntest cases. In an analysis of the system's successes and failures we reveal how\nvisual context influences the semantics of utterances and propose future\nextensions to the model that take such context into account.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2011 20:35:04 GMT"}], "update_date": "2011-07-04", "authors_parsed": [["Gorniak", "P.", ""], ["Roy", "D.", ""]]}, {"id": "1107.0034", "submitter": "K. M. Lochner", "authors": "K. M. Lochner, D. M. Reeves, Y. Vorobeychik, M. P. Wellman", "title": "Price Prediction in a Trading Agent Competition", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 21, pages\n  19-36, 2004", "doi": "10.1613/jair.1333", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 2002 Trading Agent Competition (TAC) presented a challenging market game\nin the domain of travel shopping. One of the pivotal issues in this domain is\nuncertainty about hotel prices, which have a significant influence on the\nrelative cost of alternative trip schedules. Thus, virtually all participants\nemploy some method for predicting hotel prices. We survey approaches employed\nin the tournament, finding that agents apply an interesting diversity of\ntechniques, taking into account differing sources of evidence bearing on\nprices. Based on data provided by entrants on their agents' actual predictions\nin the TAC-02 finals and semifinals, we analyze the relative efficacy of these\napproaches. The results show that taking into account game-specific information\nabout flight prices is a major distinguishing factor. Machine learning methods\neffectively induce the relationship between flight and hotel prices from game\ndata, and a purely analytical approach based on competitive equilibrium\nanalysis achieves equal accuracy with no historical data. Employing a new\nmeasure of prediction quality, we relate absolute accuracy to bottom-line\nperformance in the game.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2011 20:35:25 GMT"}], "update_date": "2011-07-04", "authors_parsed": [["Lochner", "K. M.", ""], ["Reeves", "D. M.", ""], ["Vorobeychik", "Y.", ""], ["Wellman", "M. P.", ""]]}, {"id": "1107.0035", "submitter": "J. Keppens", "authors": "J. Keppens, Q. Shen", "title": "Compositional Model Repositories via Dynamic Constraint Satisfaction\n  with Order-of-Magnitude Preferences", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 21, pages\n  499-550, 2004", "doi": "10.1613/jair.1335", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The predominant knowledge-based approach to automated model construction,\ncompositional modelling, employs a set of models of particular functional\ncomponents. Its inference mechanism takes a scenario describing the constituent\ninteracting components of a system and translates it into a useful mathematical\nmodel. This paper presents a novel compositional modelling approach aimed at\nbuilding model repositories. It furthers the field in two respects. Firstly, it\nexpands the application domain of compositional modelling to systems that can\nnot be easily described in terms of interacting functional components, such as\necological systems. Secondly, it enables the incorporation of user preferences\ninto the model selection process. These features are achieved by casting the\ncompositional modelling problem as an activity-based dynamic preference\nconstraint satisfaction problem, where the dynamic constraints describe the\nrestrictions imposed over the composition of partial models and the preferences\ncorrespond to those of the user of the automated modeller. In addition, the\npreference levels are represented through the use of symbolic values that\ndiffer in orders of magnitude.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2011 20:35:37 GMT"}], "update_date": "2011-07-04", "authors_parsed": [["Keppens", "J.", ""], ["Shen", "Q.", ""]]}, {"id": "1107.0036", "submitter": "A. Borodin", "authors": "A. Borodin, R. El-Yaniv, V. Gogan", "title": "Can We Learn to Beat the Best Stock", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 21, pages\n  579-594, 2004", "doi": "10.1613/jair.1336", "report-no": null, "categories": "cs.AI q-fin.TR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel algorithm for actively trading stocks is presented. While traditional\nexpert advice and \"universal\" algorithms (as well as standard technical trading\nheuristics) attempt to predict winners or trends, our approach relies on\npredictable statistical relations between all pairs of stocks in the market.\nOur empirical results on historical markets provide strong evidence that this\ntype of technical trading can \"beat the market\" and moreover, can beat the best\nstock in the market. In doing so we utilize a new idea for smoothing critical\nparameters in the context of expert learning.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2011 20:36:41 GMT"}], "update_date": "2011-07-04", "authors_parsed": [["Borodin", "A.", ""], ["El-Yaniv", "R.", ""], ["Gogan", "V.", ""]]}, {"id": "1107.0037", "submitter": "R. Miikkulainen", "authors": "R. Miikkulainen, K. O. Stanley", "title": "Competitive Coevolution through Evolutionary Complexification", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 21, pages\n  63-100, 2004", "doi": "10.1613/jair.1338", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two major goals in machine learning are the discovery and improvement of\nsolutions to complex problems. In this paper, we argue that complexification,\ni.e. the incremental elaboration of solutions through adding new structure,\nachieves both these goals. We demonstrate the power of complexification through\nthe NeuroEvolution of Augmenting Topologies (NEAT) method, which evolves\nincreasingly complex neural network architectures. NEAT is applied to an\nopen-ended coevolutionary robot duel domain where robot controllers compete\nhead to head. Because the robot duel domain supports a wide range of\nstrategies, and because coevolution benefits from an escalating arms race, it\nserves as a suitable testbed for studying complexification. When compared to\nthe evolution of networks with fixed structure, complexifying evolution\ndiscovers significantly more sophisticated strategies. The results suggest that\nin order to discover and improve complex solutions, evolution, and search in\ngeneral, should be allowed to complexify as well as optimize.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2011 20:36:55 GMT"}], "update_date": "2011-07-04", "authors_parsed": [["Miikkulainen", "R.", ""], ["Stanley", "K. O.", ""]]}, {"id": "1107.0038", "submitter": "B. Hnich", "authors": "B. Hnich, B. M. Smith, T. Walsh", "title": "Dual Modelling of Permutation and Injection Problems", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 21, pages\n  357-391, 2004", "doi": "10.1613/jair.1351", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When writing a constraint program, we have to choose which variables should\nbe the decision variables, and how to represent the constraints on these\nvariables. In many cases, there is considerable choice for the decision\nvariables. Consider, for example, permutation problems in which we have as many\nvalues as variables, and each variable takes an unique value. In such problems,\nwe can choose between a primal and a dual viewpoint. In the dual viewpoint,\neach dual variable represents one of the primal values, whilst each dual value\nrepresents one of the primal variables. Alternatively, by means of channelling\nconstraints to link the primal and dual variables, we can have a combined model\nwith both sets of variables. In this paper, we perform an extensive theoretical\nand empirical study of such primal, dual and combined models for two classes of\nproblems: permutation problems and injection problems. Our results show that it\noften be advantageous to use multiple viewpoints, and to have constraints which\nchannel between them to maintain consistency. They also illustrate a general\nmethodology for comparing different constraint models.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2011 20:37:09 GMT"}], "update_date": "2011-07-04", "authors_parsed": [["Hnich", "B.", ""], ["Smith", "B. M.", ""], ["Walsh", "T.", ""]]}, {"id": "1107.0040", "submitter": "H. E. Dixon", "authors": "H. E. Dixon, M. L. Ginsberg, A. J. Parkes", "title": "Generalizing Boolean Satisfiability I: Background and Survey of Existing\n  Work", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 21, pages\n  193-243, 2004", "doi": "10.1613/jair.1353", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is the first of three planned papers describing ZAP, a satisfiability\nengine that substantially generalizes existing tools while retaining the\nperformance characteristics of modern high-performance solvers. The fundamental\nidea underlying ZAP is that many problems passed to such engines contain rich\ninternal structure that is obscured by the Boolean representation used; our\ngoal is to define a representation in which this structure is apparent and can\neasily be exploited to improve computational performance. This paper is a\nsurvey of the work underlying ZAP, and discusses previous attempts to improve\nthe performance of the Davis-Putnam-Logemann-Loveland algorithm by exploiting\nthe structure of the problem being solved. We examine existing ideas including\nextensions of the Boolean language to allow cardinality constraints,\npseudo-Boolean representations, symmetry, and a limited form of quantification.\nWhile this paper is intended as a survey, our research results are contained in\nthe two subsequent articles, with the theoretical structure of ZAP described in\nthe second paper in this series, and ZAP's implementation described in the\nthird.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2011 20:38:04 GMT"}], "update_date": "2011-07-04", "authors_parsed": [["Dixon", "H. E.", ""], ["Ginsberg", "M. L.", ""], ["Parkes", "A. J.", ""]]}, {"id": "1107.0041", "submitter": "A. Ben-Yair", "authors": "A. Ben-Yair, A. Felner, S. Kraus, N. Netanyahu, R. Stern", "title": "PHA*: Finding the Shortest Path with A* in An Unknown Physical\n  Environment", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 21, pages\n  631-670, 2004", "doi": "10.1613/jair.1373", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of finding the shortest path between two points in an\nunknown real physical environment, where a traveling agent must move around in\nthe environment to explore unknown territory. We introduce the Physical-A*\nalgorithm (PHA*) for solving this problem. PHA* expands all the mandatory nodes\nthat A* would expand and returns the shortest path between the two points.\nHowever, due to the physical nature of the problem, the complexity of the\nalgorithm is measured by the traveling effort of the moving agent and not by\nthe number of generated nodes, as in standard A*. PHA* is presented as a\ntwo-level algorithm, such that its high level, A*, chooses the next node to be\nexpanded and its low level directs the agent to that node in order to explore\nit. We present a number of variations for both the high-level and low-level\nprocedures and evaluate their performance theoretically and experimentally. We\nshow that the travel cost of our best variation is fairly close to the optimal\ntravel cost, assuming that the mandatory nodes of A* are known in advance. We\nthen generalize our algorithm to the multi-agent case, where a number of\ncooperative agents are designed to solve the problem. Specifically, we provide\nan experimental implementation for such a system. It should be noted that the\nproblem addressed here is not a navigation problem, but rather a problem of\nfinding the shortest path between two points for future usage.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2011 20:38:33 GMT"}], "update_date": "2011-07-04", "authors_parsed": [["Ben-Yair", "A.", ""], ["Felner", "A.", ""], ["Kraus", "S.", ""], ["Netanyahu", "N.", ""], ["Stern", "R.", ""]]}, {"id": "1107.0042", "submitter": "N. L. Zhang", "authors": "N. L. Zhang, W. Zhang", "title": "Restricted Value Iteration: Theory and Algorithms", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 23, pages\n  123-165, 2005", "doi": "10.1613/jair.1379", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Value iteration is a popular algorithm for finding near optimal policies for\nPOMDPs. It is inefficient due to the need to account for the entire belief\nspace, which necessitates the solution of large numbers of linear programs. In\nthis paper, we study value iteration restricted to belief subsets. We show\nthat, together with properly chosen belief subsets, restricted value iteration\nyields near-optimal policies and we give a condition for determining whether a\ngiven belief subset would bring about savings in space and time. We also apply\nrestricted value iteration to two interesting classes of POMDPs, namely\ninformative POMDPs and near-discernible POMDPs.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2011 20:38:52 GMT"}], "update_date": "2011-07-04", "authors_parsed": [["Zhang", "N. L.", ""], ["Zhang", "W.", ""]]}, {"id": "1107.0043", "submitter": "D. Cohen", "authors": "D. Cohen, M. Cooper, P. Jeavons, A. Krokhin", "title": "A Maximal Tractable Class of Soft Constraints", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 22, pages\n  1-22, 2004", "doi": "10.1613/jair.1400", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many researchers in artificial intelligence are beginning to explore the use\nof soft constraints to express a set of (possibly conflicting) problem\nrequirements. A soft constraint is a function defined on a collection of\nvariables which associates some measure of desirability with each possible\ncombination of values for those variables. However, the crucial question of the\ncomputational complexity of finding the optimal solution to a collection of\nsoft constraints has so far received very little attention. In this paper we\nidentify a class of soft binary constraints for which the problem of finding\nthe optimal solution is tractable. In other words, we show that for any given\nset of such constraints, there exists a polynomial time algorithm to determine\nthe assignment having the best overall combined measure of desirability. This\ntractable class includes many commonly-occurring soft constraints, such as 'as\nnear as possible' or 'as soon as possible after', as well as crisp constraints\nsuch as 'greater than'. Finally, we show that this tractable class is maximal,\nin the sense that adding any other form of soft binary constraint which is not\nin the class gives rise to a class of problems which is NP-hard.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2011 20:39:17 GMT"}], "update_date": "2011-07-04", "authors_parsed": [["Cohen", "D.", ""], ["Cooper", "M.", ""], ["Jeavons", "P.", ""], ["Krokhin", "A.", ""]]}, {"id": "1107.0044", "submitter": "P. Beame", "authors": "P. Beame, H. Kautz, A. Sabharwal", "title": "Towards Understanding and Harnessing the Potential of Clause Learning", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 22, pages\n  319-351, 2004", "doi": "10.1613/jair.1410", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient implementations of DPLL with the addition of clause learning are\nthe fastest complete Boolean satisfiability solvers and can handle many\nsignificant real-world problems, such as verification, planning and design.\nDespite its importance, little is known of the ultimate strengths and\nlimitations of the technique. This paper presents the first precise\ncharacterization of clause learning as a proof system (CL), and begins the task\nof understanding its power by relating it to the well-studied resolution proof\nsystem. In particular, we show that with a new learning scheme, CL can provide\nexponentially shorter proofs than many proper refinements of general resolution\n(RES) satisfying a natural property. These include regular and Davis-Putnam\nresolution, which are already known to be much stronger than ordinary DPLL. We\nalso show that a slight variant of CL with unlimited restarts is as powerful as\nRES itself. Translating these analytical results to practice, however, presents\na challenge because of the nondeterministic nature of clause learning\nalgorithms. We propose a novel way of exploiting the underlying problem\nstructure, in the form of a high level problem description such as a graph or\nPDDL specification, to guide clause learning algorithms toward faster\nsolutions. We show that this leads to exponential speed-ups on grid and\nrandomized pebbling problems, as well as substantial improvements on certain\nordering formulas.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2011 20:39:28 GMT"}], "update_date": "2011-07-04", "authors_parsed": [["Beame", "P.", ""], ["Kautz", "H.", ""], ["Sabharwal", "A.", ""]]}, {"id": "1107.0045", "submitter": "C. Cayrol", "authors": "C. Cayrol, M. C. Lagasquie-Schiex", "title": "Graduality in Argumentation", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 23, pages\n  245-297, 2005", "doi": "10.1613/jair.1411", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Argumentation is based on the exchange and valuation of interacting\narguments, followed by the selection of the most acceptable of them (for\nexample, in order to take a decision, to make a choice). Starting from the\nframework proposed by Dung in 1995, our purpose is to introduce 'graduality' in\nthe selection of the best arguments, i.e., to be able to partition the set of\nthe arguments in more than the two usual subsets of 'selected' and\n'non-selected' arguments in order to represent different levels of selection.\nOur basic idea is that an argument is all the more acceptable if it can be\npreferred to its attackers. First, we discuss general principles underlying a\n'gradual' valuation of arguments based on their interactions. Following these\nprinciples, we define several valuation models for an abstract argumentation\nsystem. Then, we introduce 'graduality' in the concept of acceptability of\narguments. We propose new acceptability classes and a refinement of existing\nclasses taking advantage of an available 'gradual' valuation.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2011 20:39:39 GMT"}], "update_date": "2011-07-04", "authors_parsed": [["Cayrol", "C.", ""], ["Lagasquie-Schiex", "M. C.", ""]]}, {"id": "1107.0046", "submitter": "P. Derbeko", "authors": "P. Derbeko, R. El-Yaniv, R. Meir", "title": "Explicit Learning Curves for Transduction and Application to Clustering\n  and Compression Algorithms", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 22, pages\n  117-142, 2004", "doi": "10.1613/jair.1417", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inductive learning is based on inferring a general rule from a finite data\nset and using it to label new data. In transduction one attempts to solve the\nproblem of using a labeled training set to label a set of unlabeled points,\nwhich are given to the learner prior to learning. Although transduction seems\nat the outset to be an easier task than induction, there have not been many\nprovably useful algorithms for transduction. Moreover, the precise relation\nbetween induction and transduction has not yet been determined. The main\ntheoretical developments related to transduction were presented by Vapnik more\nthan twenty years ago. One of Vapnik's basic results is a rather tight error\nbound for transductive classification based on an exact computation of the\nhypergeometric tail. While tight, this bound is given implicitly via a\ncomputational routine. Our first contribution is a somewhat looser but explicit\ncharacterization of a slightly extended PAC-Bayesian version of Vapnik's\ntransductive bound. This characterization is obtained using concentration\ninequalities for the tail of sums of random variables obtained by sampling\nwithout replacement. We then derive error bounds for compression schemes such\nas (transductive) support vector machines and for transduction algorithms based\non clustering. The main observation used for deriving these new error bounds\nand algorithms is that the unlabeled test points, which in the transductive\nsetting are known in advance, can be used in order to construct useful data\ndependent prior distributions over the hypothesis space.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2011 20:39:52 GMT"}], "update_date": "2011-07-04", "authors_parsed": [["Derbeko", "P.", ""], ["El-Yaniv", "R.", ""], ["Meir", "R.", ""]]}, {"id": "1107.0047", "submitter": "C. V. Goldman", "authors": "C. V. Goldman, S. Zilberstein", "title": "Decentralized Control of Cooperative Systems: Categorization and\n  Complexity Analysis", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 22, pages\n  143-174, 2004", "doi": "10.1613/jair.1427", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decentralized control of cooperative systems captures the operation of a\ngroup of decision makers that share a single global objective. The difficulty\nin solving optimally such problems arises when the agents lack full\nobservability of the global state of the system when they operate. The general\nproblem has been shown to be NEXP-complete. In this paper, we identify classes\nof decentralized control problems whose complexity ranges between NEXP and P.\nIn particular, we study problems characterized by independent transitions,\nindependent observations, and goal-oriented objective functions. Two algorithms\nare shown to solve optimally useful classes of goal-oriented decentralized\nprocesses in polynomial time. This paper also studies information sharing among\nthe decision-makers, which can improve their performance. We distinguish\nbetween three ways in which agents can exchange information: indirect\ncommunication, direct communication and sharing state features that are not\ncontrolled by the agents. Our analysis shows that for every class of problems\nwe consider, introducing direct or indirect communication does not change the\nworst-case complexity. The results provide a better understanding of the\ncomplexity of decentralized control problems that arise in practice and\nfacilitate the development of planning algorithms for these problems.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2011 20:40:04 GMT"}], "update_date": "2011-07-04", "authors_parsed": [["Goldman", "C. V.", ""], ["Zilberstein", "S.", ""]]}, {"id": "1107.0048", "submitter": "E. Celaya", "authors": "E. Celaya, J. M. Porta", "title": "Reinforcement Learning for Agents with Many Sensors and Actuators Acting\n  in Categorizable Environments", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 23, pages\n  79-122, 2005", "doi": "10.1613/jair.1437", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we confront the problem of applying reinforcement learning to\nagents that perceive the environment through many sensors and that can perform\nparallel actions using many actuators as is the case in complex autonomous\nrobots. We argue that reinforcement learning can only be successfully applied\nto this case if strong assumptions are made on the characteristics of the\nenvironment in which the learning is performed, so that the relevant sensor\nreadings and motor commands can be readily identified. The introduction of such\nassumptions leads to strongly-biased learning systems that can eventually lose\nthe generality of traditional reinforcement-learning algorithms. In this line,\nwe observe that, in realistic situations, the reward received by the robot\ndepends only on a reduced subset of all the executed actions and that only a\nreduced subset of the sensor inputs (possibly different in each situation and\nfor each action) are relevant to predict the reward. We formalize this property\nin the so called 'categorizability assumption' and we present an algorithm that\ntakes advantage of the categorizability of the environment, allowing a decrease\nin the learning time with respect to existing reinforcement-learning\nalgorithms. Results of the application of the algorithm to a couple of\nsimulated realistic-robotic problems (landmark-based navigation and the\nsix-legged robot gait generation) are reported to validate our approach and to\ncompare it to existing flat and generalization-based reinforcement-learning\napproaches.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2011 20:40:15 GMT"}], "update_date": "2011-07-04", "authors_parsed": [["Celaya", "E.", ""], ["Porta", "J. M.", ""]]}, {"id": "1107.0050", "submitter": "A. Felner", "authors": "A. Felner, S. Hanan, R. E. Korf", "title": "Additive Pattern Database Heuristics", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 22, pages\n  279-318, 2004", "doi": "10.1613/jair.1480", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore a method for computing admissible heuristic evaluation functions\nfor search problems. It utilizes pattern databases, which are precomputed\ntables of the exact cost of solving various subproblems of an existing problem.\nUnlike standard pattern database heuristics, however, we partition our problems\ninto disjoint subproblems, so that the costs of solving the different\nsubproblems can be added together without overestimating the cost of solving\nthe original problem. Previously, we showed how to statically partition the\nsliding-tile puzzles into disjoint groups of tiles to compute an admissible\nheuristic, using the same partition for each state and problem instance. Here\nwe extend the method and show that it applies to other domains as well. We also\npresent another method for additive heuristics which we call dynamically\npartitioned pattern databases. Here we partition the problem into disjoint\nsubproblems for each state of the search dynamically. We discuss the pros and\ncons of each of these methods and apply both methods to three different problem\ndomains: the sliding-tile puzzles, the 4-peg Towers of Hanoi problem, and\nfinding an optimal vertex cover of a graph. We find that in some problem\ndomains, static partitioning is most effective, while in others dynamic\npartitioning is a better choice. In each of these problem domains, either\nstatically partitioned or dynamically partitioned pattern database heuristics\nare the best known heuristics for the problem.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2011 20:41:12 GMT"}], "update_date": "2011-07-04", "authors_parsed": [["Felner", "A.", ""], ["Hanan", "S.", ""], ["Korf", "R. E.", ""]]}, {"id": "1107.0051", "submitter": "R. Begleiter", "authors": "R. Begleiter, R. El-Yaniv, G. Yona", "title": "On Prediction Using Variable Order Markov Models", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 22, pages\n  385-421, 2004", "doi": "10.1613/jair.1491", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with algorithms for prediction of discrete sequences\nover a finite alphabet, using variable order Markov models. The class of such\nalgorithms is large and in principle includes any lossless compression\nalgorithm. We focus on six prominent prediction algorithms, including Context\nTree Weighting (CTW), Prediction by Partial Match (PPM) and Probabilistic\nSuffix Trees (PSTs). We discuss the properties of these algorithms and compare\ntheir performance using real life sequences from three domains: proteins,\nEnglish text and music pieces. The comparison is made with respect to\nprediction quality as measured by the average log-loss. We also compare\nclassification algorithms based on these predictors with respect to a number of\nlarge protein classification tasks. Our results indicate that a \"decomposed\"\nCTW (a variant of the CTW algorithm) and PPM outperform all other algorithms in\nsequence prediction tasks. Somewhat surprisingly, a different algorithm, which\nis a modification of the Lempel-Ziv compression algorithm, significantly\noutperforms all algorithms on the protein classification problems.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2011 20:43:01 GMT"}], "update_date": "2011-07-04", "authors_parsed": [["Begleiter", "R.", ""], ["El-Yaniv", "R.", ""], ["Yona", "G.", ""]]}, {"id": "1107.0052", "submitter": "J. Hoffmann", "authors": "J. Hoffmann, J. Porteous, L. Sebastia", "title": "Ordered Landmarks in Planning", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 22, pages\n  215-278, 2004", "doi": "10.1613/jair.1492", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many known planning tasks have inherent constraints concerning the best order\nin which to achieve the goals. A number of research efforts have been made to\ndetect such constraints and to use them for guiding search, in the hope of\nspeeding up the planning process. We go beyond the previous approaches by\nconsidering ordering constraints not only over the (top-level) goals, but also\nover the sub-goals that will necessarily arise during planning. Landmarks are\nfacts that must be true at some point in every valid solution plan. We extend\nKoehler and Hoffmann's definition of reasonable orders between top level goals\nto the more general case of landmarks. We show how landmarks can be found, how\ntheir reasonable orders can be approximated, and how this information can be\nused to decompose a given planning task into several smaller sub-tasks. Our\nmethodology is completely domain- and planner-independent. The implementation\ndemonstrates that the approach can yield significant runtime performance\nimprovements when used as a control loop around state-of-the-art sub-optimal\nplanning systems, as exemplified by FF and LPG.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2011 20:43:14 GMT"}], "update_date": "2011-07-04", "authors_parsed": [["Hoffmann", "J.", ""], ["Porteous", "J.", ""], ["Sebastia", "L.", ""]]}, {"id": "1107.0053", "submitter": "Daniel Bryce", "authors": "N. Roy, G. Gordon, S. Thrun", "title": "Finding Approximate POMDP solutions Through Belief Compression", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 23, pages\n  1-40, 2005", "doi": "10.1613/jair.1496", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard value function approaches to finding policies for Partially\nObservable Markov Decision Processes (POMDPs) are generally considered to be\nintractable for large models. The intractability of these algorithms is to a\nlarge extent a consequence of computing an exact, optimal policy over the\nentire belief space. However, in real-world POMDP problems, computing the\noptimal policy for the full belief space is often unnecessary for good control\neven for problems with complicated policy classes. The beliefs experienced by\nthe controller often lie near a structured, low-dimensional subspace embedded\nin the high-dimensional belief space. Finding a good approximation to the\noptimal value function for only this subspace can be much easier than computing\nthe full value function. We introduce a new method for solving large-scale\nPOMDPs by reducing the dimensionality of the belief space. We use Exponential\nfamily Principal Components Analysis (Collins, Dasgupta and Schapire, 2002) to\nrepresent sparse, high-dimensional belief spaces using small sets of learned\nfeatures of the belief state. We then plan only in terms of the low-dimensional\nbelief features. By planning in this low-dimensional space, we can find\npolicies for POMDP models that are orders of magnitude larger than models that\ncan be handled by conventional techniques. We demonstrate the use of this\nalgorithm on a synthetic problem and on mobile robot navigation tasks.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2011 20:44:33 GMT"}, {"version": "v2", "created": "Tue, 4 Oct 2011 15:16:13 GMT"}], "update_date": "2011-10-05", "authors_parsed": [["Roy", "N.", ""], ["Gordon", "G.", ""], ["Thrun", "S.", ""]]}, {"id": "1107.0054", "submitter": "W. P. Birmingham", "authors": "W. P. Birmingham, C. J. Meek", "title": "A Comprehensive Trainable Error Model for Sung Music Queries", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 22, pages\n  57-91, 2004", "doi": "10.1613/jair.1334", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a model for errors in sung queries, a variant of the hidden Markov\nmodel (HMM). This is a solution to the problem of identifying the degree of\nsimilarity between a (typically error-laden) sung query and a potential target\nin a database of musical works, an important problem in the field of music\ninformation retrieval. Similarity metrics are a critical component of\nquery-by-humming (QBH) applications which search audio and multimedia databases\nfor strong matches to oral queries. Our model comprehensively expresses the\ntypes of error or variation between target and query: cumulative and\nnon-cumulative local errors, transposition, tempo and tempo changes,\ninsertions, deletions and modulation. The model is not only expressive, but\nautomatically trainable, or able to learn and generalize from query examples.\nWe present results of simulations, designed to assess the discriminatory\npotential of the model, and tests with real sung queries, to demonstrate\nrelevance to real-world applications.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2011 20:44:46 GMT"}], "update_date": "2011-07-04", "authors_parsed": [["Birmingham", "W. P.", ""], ["Meek", "C. J.", ""]]}, {"id": "1107.0055", "submitter": "W. Zhang", "authors": "W. Zhang", "title": "Phase Transitions and Backbones of the Asymmetric Traveling Salesman\n  Problem", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 21, pages\n  471-497, 2004", "doi": "10.1613/jair.1389", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there has been much interest in phase transitions of\ncombinatorial problems. Phase transitions have been successfully used to\nanalyze combinatorial optimization problems, characterize their typical-case\nfeatures and locate the hardest problem instances. In this paper, we study\nphase transitions of the asymmetric Traveling Salesman Problem (ATSP), an\nNP-hard combinatorial optimization problem that has many real-world\napplications. Using random instances of up to 1,500 cities in which intercity\ndistances are uniformly distributed, we empirically show that many properties\nof the problem, including the optimal tour cost and backbone size, experience\nsharp transitions as the precision of intercity distances increases across a\ncritical value. Our experimental results on the costs of the ATSP tours and\nassignment problem agree with the theoretical result that the asymptotic cost\nof assignment problem is pi ^2 /6 the number of cities goes to infinity. In\naddition, we show that the average computational cost of the well-known\nbranch-and-bound subtour elimination algorithm for the problem also exhibits a\nthrashing behavior, transitioning from easy to difficult as the distance\nprecision increases. These results answer positively an open question regarding\nthe existence of phase transitions in the ATSP, and provide guidance on how\ndifficult ATSP problem instances should be generated.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2011 20:45:03 GMT"}], "update_date": "2011-07-04", "authors_parsed": [["Zhang", "W.", ""]]}, {"id": "1107.0082", "submitter": "Andrzej Brodzik", "authors": "Andrzej K. Brodzik and Robert H. Enders", "title": "A case of combination of evidence in the Dempster-Shafer theory\n  inconsistent with evaluation of probabilities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Dempster-Shafer theory of evidence accumulation is one of the main tools\nfor combining data obtained from multiple sources. In this paper a special case\nof combination of two bodies of evidence with non-zero conflict coefficient is\nconsidered. It is shown that application of the Dempster-Shafer rule of\ncombination in this case leads to an evaluation of masses of the combined\nbodies that is different from the evaluation of the corresponding probabilities\nobtained by application of the law of total probability. This finding supports\nthe view that probabilistic interpretation of results of the Dempster-Shafer\nanalysis in the general case is not appropriate.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2011 22:51:01 GMT"}], "update_date": "2011-07-04", "authors_parsed": [["Brodzik", "Andrzej K.", ""], ["Enders", "Robert H.", ""]]}, {"id": "1107.0089", "submitter": "Jun-Yi Chai", "authors": "Junyi Chai, James N.K. Liu", "title": "Towards a Reliable Framework of Uncertainty-Based Group Decision Support\n  System", "comments": "Accepted paper in IEEE-ICDM2010; Print ISBN: 978-1-4244-9244-2", "journal-ref": null, "doi": "10.1109/ICDMW.2010.80", "report-no": null, "categories": "cs.SY cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study proposes a framework of Uncertainty-based Group Decision Support\nSystem (UGDSS). It provides a platform for multiple criteria decision analysis\nin six aspects including (1) decision environment, (2) decision problem, (3)\ndecision group, (4) decision conflict, (5) decision schemes and (6) group\nnegotiation. Based on multiple artificial intelligent technologies, this\nframework provides reliable support for the comprehensive manipulation of\napplications and advanced decision approaches through the design of an\nintegrated multi-agents architecture.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2011 00:40:34 GMT"}], "update_date": "2011-07-04", "authors_parsed": [["Chai", "Junyi", ""], ["Liu", "James N. K.", ""]]}, {"id": "1107.0098", "submitter": "Alexander Davydov", "authors": "Alexander Y. Davydov", "title": "A Probabilistic Attack on NP-complete Problems", "comments": "16 pages, 8 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.AI cs.DM cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using the probability theory-based approach, this paper reveals the\nequivalence of an arbitrary NP-complete problem to a problem of checking\nwhether a level set of a specifically constructed harmonic cost function (with\nall diagonal entries of its Hessian matrix equal to zero) intersects with a\nunit hypercube in many-dimensional Euclidean space. This connection suggests\nthe possibility that methods of continuous mathematics can provide crucial\ninsights into the most intriguing open questions in modern complexity theory.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2011 03:43:19 GMT"}, {"version": "v2", "created": "Thu, 21 Jul 2011 01:57:22 GMT"}, {"version": "v3", "created": "Thu, 2 Aug 2012 21:14:17 GMT"}], "update_date": "2012-08-06", "authors_parsed": [["Davydov", "Alexander Y.", ""]]}, {"id": "1107.0134", "submitter": "Vladimir Kurbalija", "authors": "Vladimir Kurbalija, Milo\\v{s} Radovanovi\\'c, Zoltan Geler, Mirjana\n  Ivanovi\\'c", "title": "The Influence of Global Constraints on Similarity Measures for\n  Time-Series Databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A time series consists of a series of values or events obtained over repeated\nmeasurements in time. Analysis of time series represents and important tool in\nmany application areas, such as stock market analysis, process and quality\ncontrol, observation of natural phenomena, medical treatments, etc. A vital\ncomponent in many types of time-series analysis is the choice of an appropriate\ndistance/similarity measure. Numerous measures have been proposed to date, with\nthe most successful ones based on dynamic programming. Being of quadratic time\ncomplexity, however, global constraints are often employed to limit the search\nspace in the matrix during the dynamic programming procedure, in order to speed\nup computation. Furthermore, it has been reported that such constrained\nmeasures can also achieve better accuracy. In this paper, we investigate two\nrepresentative time-series distance/similarity measures based on dynamic\nprogramming, Dynamic Time Warping (DTW) and Longest Common Subsequence (LCS),\nand the effects of global constraints on them. Through extensive experiments on\na large number of time-series data sets, we demonstrate how global constrains\ncan significantly reduce the computation time of DTW and LCS. We also show\nthat, if the constraint parameter is tight enough (less than 10-15% of\ntime-series length), the constrained measure becomes significantly different\nfrom its unconstrained counterpart, in the sense of producing qualitatively\ndifferent 1-nearest neighbor graphs. This observation explains the potential\nfor accuracy gains when using constrained measures, highlighting the need for\ncareful tuning of constraint parameters in order to achieve a good trade-off\nbetween speed and accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2011 08:05:40 GMT"}, {"version": "v2", "created": "Wed, 25 Dec 2013 11:47:39 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Kurbalija", "Vladimir", ""], ["Radovanovi\u0107", "Milo\u0161", ""], ["Geler", "Zoltan", ""], ["Ivanovi\u0107", "Mirjana", ""]]}, {"id": "1107.0194", "submitter": "Jitesh Dundas", "authors": "Jitesh Dundas", "title": "Law of Connectivity in Machine Learning", "comments": "Keywords- Machine Learning; unknown entities; independence;\n  interaction; coverage, silent connections; ISSN 1473-804x online, 1473-8031\n  print", "journal-ref": "I. J. of SIMULATION Vol. 11 No 5 1-10 Dec 2010", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present in this paper our law that there is always a connection present\nbetween two entities, with a selfconnection being present at least in each\nnode. An entity is an object, physical or imaginary, that is connected by a\npath (or connection) and which is important for achieving the desired result of\nthe scenario. In machine learning, we state that for any scenario, a subject\nentity is always, directly or indirectly, connected and affected by single or\nmultiple independent / dependent entities, and their impact on the subject\nentity is dependent on various factors falling into the categories such as the\nexistenc\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2011 11:08:32 GMT"}], "update_date": "2011-07-04", "authors_parsed": [["Dundas", "Jitesh", ""]]}, {"id": "1107.0268", "submitter": "Mladen Nikolic", "authors": "Mladen Nikolic, Filip Maric, Predrag Janicic", "title": "Simple Algorithm Portfolio for SAT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The importance of algorithm portfolio techniques for SAT has long been noted,\nand a number of very successful systems have been devised, including the most\nsuccessful one --- SATzilla. However, all these systems are quite complex (to\nunderstand, reimplement, or modify). In this paper we propose a new algorithm\nportfolio for SAT that is extremely simple, but in the same time so efficient\nthat it outperforms SATzilla. For a new SAT instance to be solved, our\nportfolio finds its k-nearest neighbors from the training set and invokes a\nsolver that performs the best at those instances. The main distinguishing\nfeature of our algorithm portfolio is the locality of the selection procedure\n--- the selection of a SAT solver is based only on few instances similar to the\ninput one.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2011 16:20:44 GMT"}, {"version": "v2", "created": "Tue, 13 Dec 2011 14:38:07 GMT"}], "update_date": "2011-12-14", "authors_parsed": [["Nikolic", "Mladen", ""], ["Maric", "Filip", ""], ["Janicic", "Predrag", ""]]}, {"id": "1107.0399", "submitter": "Oleg Kupervasser", "authors": "Oleg Kupervasser, Vladimir Voronov", "title": "Vision-Based Navigation I: A navigation filter for fusing\n  DTM/correspondence updates", "comments": "26 pages, 3 figures, in English and in Russian. arXiv admin note:\n  substantial text overlap with arXiv:1106.6341, arXiv:1107.1470", "journal-ref": "Proceedings of the IEEE International Conference on Robotics and\n  Biomimetics (ROBIO), 2011 , Page(s): 1591 - 1596", "doi": "10.1109/ROBIO.2011.6181516", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An algorithm for pose and motion estimation using corresponding features in\nimages and a digital terrain map is proposed. Using a Digital Terrain (or\nDigital Elevation) Map (DTM/DEM) as a global reference enables recovering the\nabsolute position and orientation of the camera. In order to do this, the DTM\nis used to formulate a constraint between corresponding features in two\nconsecutive frames. The utilization of data is shown to improve the robustness\nand accuracy of the inertial navigation algorithm. Extended Kalman filter was\nused to combine results of inertial navigation algorithm and proposed\nvision-based navigation algorithm. The feasibility of this algorithms is\nestablished through numerical simulations.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jul 2011 17:34:30 GMT"}, {"version": "v2", "created": "Thu, 11 Aug 2011 20:02:53 GMT"}, {"version": "v3", "created": "Mon, 23 Apr 2012 21:37:28 GMT"}, {"version": "v4", "created": "Tue, 10 Jul 2012 18:40:53 GMT"}], "update_date": "2012-11-11", "authors_parsed": [["Kupervasser", "Oleg", ""], ["Voronov", "Vladimir", ""]]}, {"id": "1107.0434", "submitter": "Adrian Silvescu", "authors": "Adrian Silvescu and Vasant Honavar", "title": "Abstraction Super-structuring Normal Forms: Towards a Theory of\n  Structural Induction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.FL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Induction is the process by which we obtain predictive laws or theories or\nmodels of the world. We consider the structural aspect of induction. We answer\nthe question as to whether we can find a finite and minmalistic set of\noperations on structural elements in terms of which any theory can be\nexpressed. We identify abstraction (grouping similar entities) and\nsuper-structuring (combining topologically e.g., spatio-temporally close\nentities) as the essential structural operations in the induction process. We\nshow that only two more structural operations, namely, reverse abstraction and\nreverse super-structuring (the duals of abstraction and super-structuring\nrespectively) suffice in order to exploit the full power of Turing-equivalent\ngenerative grammars in induction. We explore the implications of this theorem\nwith respect to the nature of hidden variables, radical positivism and the\n2-century old claim of David Hume about the principles of connexion among\nideas.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jul 2011 07:33:51 GMT"}], "update_date": "2011-07-05", "authors_parsed": [["Silvescu", "Adrian", ""], ["Honavar", "Vasant", ""]]}, {"id": "1107.0998", "submitter": "Samuel Epstein", "authors": "Samuel Epstein and Margrit Betke", "title": "An Information Theoretic Representation of Agent Dynamics as Set\n  Intersections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We represent agents as sets of strings. Each string encodes a potential\ninteraction with another agent or environment. We represent the total set of\ndynamics between two agents as the intersection of their respective strings, we\nprove complexity properties of player interactions using Algorithmic\nInformation Theory. We show how the proposed construction is compatible with\nUniversal Artificial Intelligence, in that the AIXI model can be seen as\nuniversal with respect to interaction.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2011 22:23:42 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Epstein", "Samuel", ""], ["Betke", "Margrit", ""]]}, {"id": "1107.1020", "submitter": "Jun-Yi Chai", "authors": "Junyi Chai, James N.K. Liu", "title": "A Novel Multicriteria Group Decision Making Approach With Intuitionistic\n  Fuzzy SIR Method", "comments": "Paper presented at the 2010 World Automation Congress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The superiority and inferiority ranking (SIR) method is a generation of the\nwell-known PROMETHEE method, which can be more efficient to deal with\nmulti-criterion decision making (MCDM) problem. Intuitionistic fuzzy sets\n(IFSs), as an important extension of fuzzy sets (IFs), include both membership\nfunctions and non-membership functions and can be used to, more precisely\ndescribe uncertain information. In real world, decision situations are usually\nunder uncertain environment and involve multiple individuals who have their own\npoints of view on handing of decision problems. In order to solve uncertainty\ngroup MCDM problem, we propose a novel intuitionistic fuzzy SIR method in this\npaper. This approach uses intuitionistic fuzzy aggregation operators and SIR\nranking methods to handle uncertain information; integrate individual opinions\ninto group opinions; make decisions on multiple-criterion; and finally\nstructure a specific decision map. The proposed approach is illustrated in a\nsimulation of group decision making problem related to supply chain management.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2011 03:32:21 GMT"}], "update_date": "2011-07-07", "authors_parsed": [["Chai", "Junyi", ""], ["Liu", "James N. K.", ""]]}, {"id": "1107.1322", "submitter": "Gabriel Dulac-Arnold", "authors": "Gabriel Dulac-Arnold, Ludovic Denoyer, Patrick Gallinari", "title": "Text Classification: A Sequential Reading Approach", "comments": "ECIR2011", "journal-ref": "Lecture Notes in Computer Science, 2011, Volume 6611/2011, 411-423", "doi": "10.1007/978-3-642-20161-5_41", "report-no": null, "categories": "cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to model the text classification process as a sequential decision\nprocess. In this process, an agent learns to classify documents into topics\nwhile reading the document sentences sequentially and learns to stop as soon as\nenough information was read for deciding. The proposed algorithm is based on a\nmodelisation of Text Classification as a Markov Decision Process and learns by\nusing Reinforcement Learning. Experiments on four different classical\nmono-label corpora show that the proposed approach performs comparably to\nclassical SVM approaches for large training sets, and better for small training\nsets. In addition, the model automatically adapts its reading process to the\nquantity of training information provided.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2011 09:09:19 GMT"}, {"version": "v2", "created": "Fri, 8 Jul 2011 07:39:52 GMT"}, {"version": "v3", "created": "Mon, 29 Aug 2011 17:45:53 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Dulac-Arnold", "Gabriel", ""], ["Denoyer", "Ludovic", ""], ["Gallinari", "Patrick", ""]]}, {"id": "1107.1470", "submitter": "Oleg Kupervasser", "authors": "Oleg Kupervasser, Ronen Lerner, Ehud Rivlin, Hector Rotstein", "title": "Vision-Based Navigation II: Error Analysis for a Navigation Algorithm\n  based on Optical-Flow and a Digital Terrain Map", "comments": "10 pages,12 figures, 2 tables", "journal-ref": "Proceedings of the 2008 IEEE/ION Position, Location and Navigation\n  Symposium, P.1203-1212", "doi": "10.1109/PLANS.2008.4570040", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper deals with the error analysis of a navigation algorithm that uses\nas input a sequence of images acquired by a moving camera and a Digital Terrain\nMap (DTM) of the region been imaged by the camera during the motion. The main\nsources of error are more or less straightforward to identify: camera\nresolution, structure of the observed terrain and DTM accuracy, field of view\nand camera trajectory. After characterizing and modeling these error sources in\nthe framework of the CDTM algorithm, a closed form expression for their effect\non the pose and motion errors of the camera can be found. The analytic\nexpression provides a priori measurements for the accuracy in terms of the\nparameters mentioned above.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2011 18:09:55 GMT"}, {"version": "v2", "created": "Thu, 11 Aug 2011 23:14:05 GMT"}], "update_date": "2011-08-15", "authors_parsed": [["Kupervasser", "Oleg", ""], ["Lerner", "Ronen", ""], ["Rivlin", "Ehud", ""], ["Rotstein", "Hector", ""]]}, {"id": "1107.1686", "submitter": "Carlos Damasio", "authors": "Carlos Viegas Dam\\'asio, Alun Preece, Umberto Straccia", "title": "Proceedings of the Doctoral Consortium and Poster Session of the 5th\n  International Symposium on Rules (RuleML 2011@IJCAI)", "comments": "HTML file with clickable links to papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the papers presented at the first edition of the\nDoctoral Consortium of the 5th International Symposium on Rules (RuleML\n2011@IJCAI) held on July 19th, 2011 in Barcelona, as well as the poster session\npapers of the RuleML 2011@IJCAI main conference.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2011 18:00:49 GMT"}], "update_date": "2011-07-11", "authors_parsed": [["Dam\u00e1sio", "Carlos Viegas", ""], ["Preece", "Alun", ""], ["Straccia", "Umberto", ""]]}, {"id": "1107.1805", "submitter": "Maksims Volkovs", "authors": "Maksims N. Volkovs, Hugo Larochelle, Richard S. Zemel", "title": "Loss-sensitive Training of Probabilistic Conditional Random Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of training probabilistic conditional random fields\n(CRFs) in the context of a task where performance is measured using a specific\nloss function. While maximum likelihood is the most common approach to training\nCRFs, it ignores the inherent structure of the task's loss function. We\ndescribe alternatives to maximum likelihood which take that loss into account.\nThese include a novel adaptation of a loss upper bound from the structured SVMs\nliterature to the CRF context, as well as a new loss-inspired KL divergence\nobjective which relies on the probabilistic nature of CRFs. These\nloss-sensitive objectives are compared to maximum likelihood using ranking as a\nbenchmark task. This comparison confirms the importance of incorporating loss\ninformation in the probabilistic training of CRFs, with the loss-inspired KL\noutperforming all other objectives.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jul 2011 17:58:46 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Volkovs", "Maksims N.", ""], ["Larochelle", "Hugo", ""], ["Zemel", "Richard S.", ""]]}, {"id": "1107.1851", "submitter": "Dohan Kim", "authors": "Dohan Kim", "title": "Task swapping networks in distributed systems", "comments": "This is a preprint of a paper whose final and definite form is\n  published in: Int. J. Comput. Math. 90 (2013), 2221-2243 (DOI:\n  10.1080/00207160.2013.772985)", "journal-ref": "Int. J. Comput. Math. 90 (2013), 2221-2243", "doi": "10.1080/00207160.2013.772985", "report-no": null, "categories": "cs.DC cs.AI cs.NI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this paper we propose task swapping networks for task reassignments by\nusing task swappings in distributed systems. Some classes of task reassignments\nare achieved by using iterative local task swappings between software agents in\ndistributed systems. We use group-theoretic methods to find a minimum-length\nsequence of adjacent task swappings needed from a source task assignment to a\ntarget task assignment in a task swapping network of several well-known\ntopologies.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jul 2011 11:35:16 GMT"}, {"version": "v2", "created": "Sun, 25 Sep 2011 22:48:59 GMT"}, {"version": "v3", "created": "Thu, 5 Dec 2013 15:22:29 GMT"}], "update_date": "2013-12-06", "authors_parsed": [["Kim", "Dohan", ""]]}, {"id": "1107.1932", "submitter": "Sleiman Rabah", "authors": "Sleiman Rabah and Dan Ni and Payam Jahanshahi and Luis Felipe Guzman", "title": "Current State and Challenges of Automatic Planning in Web Service\n  Composition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper gives a survey on the current state of Web Service Compositions\nand the difficulties and solutions to automated Web Service Compositions. This\nfirst gives a definition of Web Service Composition and the motivation and goal\nof it. It then explores into why we need automated Web Service Compositions and\nformally defines the domains. Techniques and solutions are proposed by the\npapers we surveyed to solve the current difficulty of automated Web Service\nComposition. Verification and future work is discussed at the end to further\nextend the topic.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2011 04:24:48 GMT"}], "update_date": "2015-03-12", "authors_parsed": [["Rabah", "Sleiman", ""], ["Ni", "Dan", ""], ["Jahanshahi", "Payam", ""], ["Guzman", "Luis Felipe", ""]]}, {"id": "1107.1950", "submitter": "Gopalakrishnan Tr Nair", "authors": "Dr T.R. Gopalakrishnan Nair, Meenakshi Malhotra", "title": "Knowledge Embedding and Retrieval Strategies in an Informledge System", "comments": "5 pages, 7 pages, International Conferenceon Information and\n  Knowledge Management (ICIKM-IEEE), Haikou, China, 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Informledge System (ILS) is a knowledge network with autonomous nodes and\nintelligent links that integrate and structure the pieces of knowledge. In this\npaper, we put forward the strategies for knowledge embedding and retrieval in\nan ILS. ILS is a powerful knowledge network system dealing with logical storage\nand connectivity of information units to form knowledge using autonomous nodes\nand multi-lateral links. In ILS, the autonomous nodes known as Knowledge\nNetwork Nodes (KNN)s play vital roles which are not only used in storage,\nparsing and in forming the multi-lateral linkages between knowledge points but\nalso in helping the realization of intelligent retrieval of linked information\nunits in the form of knowledge. Knowledge built in to the ILS forms the shape\nof sphere. The intelligence incorporated into the links of a KNN helps in\nretrieving various knowledge threads from a specific set of KNNs. A developed\nentity of information realized through KNN forms in to the shape of a knowledge\ncone\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2011 07:13:43 GMT"}], "update_date": "2011-07-12", "authors_parsed": [["Nair", "Dr T. R. Gopalakrishnan", ""], ["Malhotra", "Meenakshi", ""]]}, {"id": "1107.1956", "submitter": "Gopalakrishnan Tr Nair", "authors": "Dr T.R. Gopalakrishnan Nair, Meenakshi Malhotra", "title": "Informledge System: A Modified Knowledge Network with Autonomous Nodes\n  using Multi-lateral Links", "comments": "4 pages, 5 figures, International Conference on Knowledge Engineering\n  and Ontology Development, KEOD 2010, Proceeding of KEOD-2010, pp 351-354,\n  Valencia-Spain, October 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in the field of Artificial Intelligence is continually progressing\nto simulate the human knowledge into automated intelligent knowledge base,\nwhich can encode and retrieve knowledge efficiently along with the capability\nof being is consistent and scalable at all times. However, there is no system\nat hand that can match the diversified abilities of human knowledge base. In\nthis position paper, we put forward a theoretical model of a different system\nthat intends to integrate pieces of knowledge, Informledge System (ILS). ILS\nwould encode the knowledge, by virtue of knowledge units linked across\ndiversified domains. The proposed ILS comprises of autonomous knowledge units\ntermed as Knowledge Network Node (KNN), which would help in efficient\ncross-linking of knowledge units to encode fresh knowledge. These links are\nreasoned and inferred by the Parser and Link Manager, which are part of KNN.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2011 07:54:51 GMT"}], "update_date": "2011-07-12", "authors_parsed": [["Nair", "Dr T. R. Gopalakrishnan", ""], ["Malhotra", "Meenakshi", ""]]}, {"id": "1107.2086", "submitter": "Carlos Viegas Dam\\'asio", "authors": "Elisa Marengo, Matteo Baldoni, and Cristina Baroglio", "title": "Extend Commitment Protocols with Temporal Regulations: Why and How", "comments": "Proceedings of the Doctoral Consortium and Poster Session of the 5th\n  International Symposium on Rules (RuleML 2011@IJCAI), pages 1-8\n  (arXiv:1107.1686)", "journal-ref": null, "doi": null, "report-no": "RuleML-DC/2011/01", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proposal of Elisa Marengo's thesis is to extend commitment protocols to\nexplicitly account for temporal regulations. This extension will satisfy two\nneeds: (1) it will allow representing, in a flexible and modular way, temporal\nregulations with a normative force, posed on the interaction, so as to\nrepresent conventions, laws and suchlike; (2) it will allow committing to\ncomplex conditions, which describe not only what will be achieved but to some\nextent also how. These two aspects will be deeply investigated in the proposal\nof a unified framework, which is part of the ongoing work and will be included\nin the thesis.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2011 18:48:59 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Marengo", "Elisa", ""], ["Baldoni", "Matteo", ""], ["Baroglio", "Cristina", ""]]}, {"id": "1107.2087", "submitter": "Carlos Viegas Dam\\'asio", "authors": "Przemyslaw Woznowski, Alun Preece", "title": "Rule-Based Semantic Sensing", "comments": "Proceedings of the Doctoral Consortium and Poster Session of the 5th\n  International Symposium on Rules (RuleML 2011@IJCAI), pages 9-16\n  (arXiv:1107.1686)", "journal-ref": null, "doi": null, "report-no": "RuleML-DC/2011/02", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rule-Based Systems have been in use for decades to solve a variety of\nproblems but not in the sensor informatics domain. Rules aid the aggregation of\nlow-level sensor readings to form a more complete picture of the real world and\nhelp to address 10 identified challenges for sensor network middleware. This\npaper presents the reader with an overview of a system architecture and a pilot\napplication to demonstrate the usefulness of a system integrating rules with\nsensor middleware.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2011 18:50:19 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Woznowski", "Przemyslaw", ""], ["Preece", "Alun", ""]]}, {"id": "1107.2088", "submitter": "Carlos Viegas Dam\\'asio", "authors": "Antonius Weinzierl", "title": "Advancing Multi-Context Systems by Inconsistency Management", "comments": "Proceedings of the Doctoral Consortium and Poster Session of the 5th\n  International Symposium on Rules (RuleML 2011@IJCAI), pages 17-24\n  (arXiv:1107.1686)", "journal-ref": null, "doi": null, "report-no": "RuleML-DC/2011/03", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-Context Systems are an expressive formalism to model (possibly)\nnon-monotonic information exchange between heterogeneous knowledge bases. Such\ninformation exchange, however, often comes with unforseen side-effects leading\nto violation of constraints, making the system inconsistent, and thus unusable.\nAlthough there are many approaches to assess and repair a single inconsistent\nknowledge base, the heterogeneous nature of Multi-Context Systems poses\nproblems which have not yet been addressed in a satisfying way: How to identify\nand explain a inconsistency that spreads over multiple knowledge bases with\ndifferent logical formalisms (e.g., logic programs and ontologies)? What are\nthe causes of inconsistency if inference/information exchange is non-monotonic\n(e.g., absent information as cause)? How to deal with inconsistency if access\nto knowledge bases is restricted (e.g., companies exchange information, but do\nnot allow arbitrary modifications to their knowledge bases)? Many traditional\napproaches solely aim for a consistent system, but automatic removal of\ninconsistency is not always desireable. Therefore a human operator has to be\nsupported in finding the erroneous parts contributing to the inconsistency. In\nmy thesis those issues will be adressed mainly from a foundational perspective,\nwhile our research project also provides algorithms and prototype\nimplementations.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2011 18:52:29 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Weinzierl", "Antonius", ""]]}, {"id": "1107.2089", "submitter": "Carlos Viegas Dam\\'asio", "authors": "Jaroslaw Bak", "title": "Rule-based query answering method for a knowledge base of economic\n  crimes", "comments": "Proceedings of the Doctoral Consortium and Poster Session of the 5th\n  International Symposium on Rules (RuleML 2011@IJCAI), pages 25-32\n  (arXiv:1107.1686)", "journal-ref": null, "doi": null, "report-no": "RuleML-DC/2011/04", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a description of the PhD thesis which aims to propose a rule-based\nquery answering method for relational data. In this approach we use an\nadditional knowledge which is represented as a set of rules and describes the\nsource data at concept (ontological) level. Queries are posed in the terms of\nabstract level. We present two methods. The first one uses hybrid reasoning and\nthe second one exploits only forward chaining. These two methods are\ndemonstrated by the prototypical implementation of the system coupled with the\nJess engine. Tests are performed on the knowledge base of the selected economic\ncrimes: fraudulent disbursement and money laundering.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2011 18:53:32 GMT"}], "update_date": "2011-07-12", "authors_parsed": [["Bak", "Jaroslaw", ""]]}, {"id": "1107.2090", "submitter": "Carlos Viegas Dam\\'asio", "authors": "Alexander Sellner, Christopher Schwarz, Erwin Zinser", "title": "Semantic-ontological combination of Business Rules and Business\n  Processes in IT Service Management", "comments": "Proceedings of the Doctoral Consortium and Poster Session of the 5th\n  International Symposium on Rules (RuleML 2011@IJCAI), pages 33-40\n  (arXiv:1107.1686)", "journal-ref": null, "doi": null, "report-no": "RuleML-DC/2011/05", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  IT Service Management deals with managing a broad range of items related to\ncomplex system environments. As there is both, a close connection to business\ninterests and IT infrastructure, the application of semantic expressions which\nare seamlessly integrated within applications for managing ITSM environments,\ncan help to improve transparency and profitability. This paper focuses on the\nchallenges regarding the integration of semantics and ontologies within ITSM\nenvironments. It will describe the paradigm of relationships and inheritance\nwithin complex service trees and will present an approach of ontologically\nexpressing them. Furthermore, the application of SBVR-based rules as executable\nSQL triggers will be discussed. Finally, the broad range of topics for further\nresearch, derived from the findings, will be presented.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2011 18:54:36 GMT"}], "update_date": "2011-07-12", "authors_parsed": [["Sellner", "Alexander", ""], ["Schwarz", "Christopher", ""], ["Zinser", "Erwin", ""]]}, {"id": "1107.2126", "submitter": "Sahin Emrah Amrahov", "authors": "\\c{S}ahin Emrah Amrahov and Iman N. Askerzade", "title": "Strong Solutions of the Fuzzy Linear Systems", "comments": "11 pages", "journal-ref": "CMES: Computer Modeling in Engineering & Sciences, Vol. 76, No. 4,\n  pp. 207-216, 2011", "doi": "10.3970/cmes.2011.076.207", "report-no": null, "categories": "cs.NA cs.AI cs.IT math.IT math.LO math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a fuzzy linear system with crisp coefficient matrix and with an\narbitrary fuzzy number in parametric form on the right-hand side. It is known\nthat the well-known existence and uniqueness theorem of a strong fuzzy solution\nis equivalent to the following: The coefficient matrix is the product of a\npermutation matrix and a diagonal matrix. This means that this theorem can be\napplicable only for a special form of linear systems, namely, only when the\nsystem consists of equations, each of which has exactly one variable. We prove\nan existence and uniqueness theorem, which can be use on more general systems.\nThe necessary and sufficient conditions of the theorem are dependent on both\nthe coefficient matrix and the right-hand side. This theorem is a\ngeneralization of the well-known existence and uniqueness theorem for the\nstrong solution.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2011 20:04:01 GMT"}, {"version": "v2", "created": "Sun, 21 Aug 2011 06:12:18 GMT"}], "update_date": "2011-11-01", "authors_parsed": [["Amrahov", "\u015eahin Emrah", ""], ["Askerzade", "Iman N.", ""]]}, {"id": "1107.2699", "submitter": "Mauricio A. \\'Alvarez", "authors": "Mauricio A. \\'Alvarez and David Luengo and Neil D. Lawrence", "title": "Linear Latent Force Models using Gaussian Processes", "comments": "20 pages, 2 figures. Extended technical report of the Conference\n  Paper \"Latent force models\" in D. van Dyk and M. Welling (eds) Proceedings of\n  the Twelfth International Workshop on Artificial Intelligence and Statistics,\n  JMLR W&CP 5, Clearwater Beach, FL, pp 9--16", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purely data driven approaches for machine learning present difficulties when\ndata is scarce relative to the complexity of the model or when the model is\nforced to extrapolate. On the other hand, purely mechanistic approaches need to\nidentify and specify all the interactions in the problem at hand (which may not\nbe feasible) and still leave the issue of how to parameterize the system. In\nthis paper, we present a hybrid approach using Gaussian processes and\ndifferential equations to combine data driven modelling with a physical model\nof the system. We show how different, physically-inspired, kernel functions can\nbe developed through sensible, simple, mechanistic assumptions about the\nunderlying system. The versatility of our approach is illustrated with three\ncase studies from motion capture, computational biology and geostatistics.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2011 23:26:42 GMT"}, {"version": "v2", "created": "Fri, 13 Mar 2020 09:47:06 GMT"}], "update_date": "2020-03-16", "authors_parsed": [["\u00c1lvarez", "Mauricio A.", ""], ["Luengo", "David", ""], ["Lawrence", "Neil D.", ""]]}, {"id": "1107.2788", "submitter": "Eray Ozkural", "authors": "Eray \\\"Ozkural", "title": "Diverse Consequences of Algorithmic Probability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AI cs.CY math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We reminisce and discuss applications of algorithmic probability to a wide\nrange of problems in artificial intelligence, philosophy and technological\nsociety. We propose that Solomonoff has effectively axiomatized the field of\nartificial intelligence, therefore establishing it as a rigorous scientific\ndiscipline. We also relate to our own work in incremental machine learning and\nphilosophy of complexity.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2011 11:23:03 GMT"}, {"version": "v2", "created": "Mon, 7 Nov 2011 20:01:37 GMT"}], "update_date": "2014-01-17", "authors_parsed": [["\u00d6zkural", "Eray", ""]]}, {"id": "1107.2822", "submitter": "Baris Sertkaya", "authors": "Baris Sertkaya", "title": "A Survey on how Description Logic Ontologies Benefit from Formal Concept\n  Analysis", "comments": "Invited paper that appeared in the Proceedings of the 7th\n  International Conference on Concept Lattices and Their Applications, (CLA\n  2010)", "journal-ref": "Proceedings of the 7th International Conference on Concept\n  Lattices and Their Applications, (CLA 2010), volume 672 of CEUR Workshop\n  Proceedings, pages 2-21. 2010", "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the notion of a concept as a collection of objects sharing certain\nproperties, and the notion of a conceptual hierarchy are fundamental to both\nFormal Concept Analysis and Description Logics, the ways concepts are described\nand obtained differ significantly between these two research areas. Despite\nthese differences, there have been several attempts to bridge the gap between\nthese two formalisms, and attempts to apply methods from one field in the\nother. The present work aims to give an overview on the research done in\ncombining Description Logics and Formal Concept Analysis.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2011 13:48:45 GMT"}], "update_date": "2011-07-15", "authors_parsed": [["Sertkaya", "Baris", ""]]}, {"id": "1107.2997", "submitter": "Jun-Yi Chai", "authors": "Junyi Chai, James N.K. Liu", "title": "An Ontology-driven Framework for Supporting Complex Decision Process", "comments": "Paper presented at the 2010 World Automation Congress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study proposes a framework of ONTOlogy-based Group Decision Support\nSystem (ONTOGDSS) for decision process which exhibits the complex structure of\ndecision-problem and decision-group. It is capable of reducing the complexity\nof problem structure and group relations. The system allows decision makers to\nparticipate in group decision-making through the web environment, via the\nontology relation. It facilitates the management of decision process as a\nwhole, from criteria generation, alternative evaluation, and opinion\ninteraction to decision aggregation. The embedded ontology structure in\nONTOGDSS provides the important formal description features to facilitate\ndecision analysis and verification. It examines the software architecture, the\nselection methods, the decision path, etc. Finally, the ontology application of\nthis system is illustrated with specific real case to demonstrate its\npotentials towards decision-making development.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2011 07:17:08 GMT"}], "update_date": "2011-07-18", "authors_parsed": [["Chai", "Junyi", ""], ["Liu", "James N. K.", ""]]}, {"id": "1107.3298", "submitter": "Pierre De Loor", "authors": "Pierre De Loor (LISYC, CERV), Favier Pierre-Alexandre (LISYC)", "title": "From decision to action : intentionality, a guide for the specification\n  of intelligent agents' behaviour", "comments": null, "journal-ref": "International Journal of Image and Graphics 6, 1 (2006) 87-99", "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces a reflexion about behavioural specification for\ninteractive and participative agent-based simulation in virtual reality. Within\nthis context, it is neces sary to reach a high level of expressivness in order\nto enforce interactions between the designer and the behavioural model during\nthe in-line prototyping. This requires to consider the need of semantic very\nearly in the design process. The Intentional agent model is here exposed as a\npossible answer. It relies on a mixed imperative and declarative approach which\nfocuses on the link between decision and action. The design of a tool able to\nsimulate virtual environment implying agents based on this model is discuss\n", "versions": [{"version": "v1", "created": "Sun, 17 Jul 2011 13:02:15 GMT"}], "update_date": "2011-07-19", "authors_parsed": [["De Loor", "Pierre", "", "LISYC, CERV"], ["Pierre-Alexandre", "Favier", "", "LISYC"]]}, {"id": "1107.3302", "submitter": "Mahdaoui Rafik", "authors": "Rafik Mahdaoui, Leila Hayet Mouss, Mohamed Djamel Mouss, Ouahiba\n  Chouhal", "title": "A Temporal Neuro-Fuzzy Monitoring System to Manufacturing Systems", "comments": "10 pages, 11 figures, IJCSI International Journal of Computer Science\n  Issues, Vol. 8, Issue 3, No. 1, May 2011 ISSN (Online): 1694-0814\n  www.IJCSI.org", "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 8,\n  Issue 3, No. 1, May 2011 ISSN (Online): 1694-0814 www.IJCSI.org", "doi": null, "report-no": "IJCSI-8-3-1-237-246.pdf", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fault diagnosis and failure prognosis are essential techniques in improving\nthe safety of many manufacturing systems. Therefore, on-line fault detection\nand isolation is one of the most important tasks in safety-critical and\nintelligent control systems. Computational intelligence techniques are being\ninvestigated as extension of the traditional fault diagnosis methods. This\npaper discusses the Temporal Neuro-Fuzzy Systems (TNFS) fault diagnosis within\nan application study of a manufacturing system. The key issues of finding a\nsuitable structure for detecting and isolating ten realistic actuator faults\nare described. Within this framework, data-processing interactive software of\nsimulation baptized NEFDIAG (NEuro Fuzzy DIAGnosis) version 1.0 is developed.\n  This software devoted primarily to creation, training and test of a\nclassification Neuro-Fuzzy system of industrial process failures. NEFDIAG can\nbe represented like a special type of fuzzy perceptron, with three layers used\nto classify patterns and failures. The system selected is the workshop of\nSCIMAT clinker, cement factory in Algeria.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jul 2011 14:13:34 GMT"}], "update_date": "2011-07-19", "authors_parsed": [["Mahdaoui", "Rafik", ""], ["Mouss", "Leila Hayet", ""], ["Mouss", "Mohamed Djamel", ""], ["Chouhal", "Ouahiba", ""]]}, {"id": "1107.3326", "submitter": "Pierre De Loor", "authors": "Pierre De Loor (LISYC, CERV), Romain B\\'enard (LISYC), Chevaillier\n  Pierre (LISYC, CERV)", "title": "Real-time retrieval for case-based reasoning in interactive\n  multiagent-based simulations", "comments": null, "journal-ref": "Expert Systems with Applications 38, 5 (2011) 5145-5153", "doi": "10.1016/j.eswa.2010.10.048", "report-no": null, "categories": "cs.AI cs.IR cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to present the principles and results about\ncase-based reasoning adapted to real- time interactive simulations, more\nprecisely concerning retrieval mechanisms. The article begins by introducing\nthe constraints involved in interactive multiagent-based simulations. The\nsecond section pre- sents a framework stemming from case-based reasoning by\nautonomous agents. Each agent uses a case base of local situations and, from\nthis base, it can choose an action in order to interact with other auton- omous\nagents or users' avatars. We illustrate this framework with an example\ndedicated to the study of dynamic situations in football. We then go on to\naddress the difficulties of conducting such simulations in real-time and\npropose a model for case and for case base. Using generic agents and adequate\ncase base structure associated with a dedicated recall algorithm, we improve\nretrieval performance under time pressure compared to classic CBR techniques.\nWe present some results relating to the performance of this solution. The\narticle concludes by outlining future development of our project.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jul 2011 19:21:12 GMT"}], "update_date": "2011-07-19", "authors_parsed": [["De Loor", "Pierre", "", "LISYC, CERV"], ["B\u00e9nard", "Romain", "", "LISYC"], ["Pierre", "Chevaillier", "", "LISYC, CERV"]]}, {"id": "1107.3342", "submitter": "Sam Ganzfried", "authors": "Sam Ganzfried", "title": "Computing Strong Game-Theoretic Strategies in Jotto", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a new approach that computes approximate equilibrium strategies in\nJotto, a popular word game. Jotto is an extremely large two-player game of\nimperfect information; its game tree has many orders of magnitude more states\nthan games previously studied, including no-limit Texas hold 'em. To address\nthe fact that the game is so large, we propose a novel strategy representation\ncalled oracular form, in which we do not explicitly represent a strategy, but\nrather appeal to an oracle that quickly outputs a sample move from the\nstrategy's distribution. Our overall approach is based on an extension of the\nfictitious play algorithm to this oracular setting. We demonstrate the\nsuperiority of our computed strategies over the strategies computed by a\nbenchmark algorithm, both in terms of head-to-head and worst-case performance.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jul 2011 23:54:09 GMT"}, {"version": "v2", "created": "Wed, 20 Jul 2011 18:48:54 GMT"}, {"version": "v3", "created": "Fri, 11 Mar 2016 18:28:34 GMT"}], "update_date": "2016-03-15", "authors_parsed": [["Ganzfried", "Sam", ""]]}, {"id": "1107.3383", "submitter": "Martin Lukac", "authors": "Maarti nLukac, Marek Perkowski, Michitaka Kameyama", "title": "Evolutionary Quantum Logic Synthesis of Boolean Reversible Logic\n  Circuits Embedded in Ternary Quantum Space using Heuristics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been experimentally proven that realizing universal quantum gates\nusing higher-radices logic is practically and technologically possible. We\ndeveloped a Parallel Genetic Algorithm that synthesizes Boolean reversible\ncircuits realized with a variety of quantum gates on qudits with various\nradices. In order to allow synthesizing circuits of medium sizes in the higher\nradix quantum space we performed the experiments using a GPU accelerated\nGenetic Algorithm. Using the accelerated GA we compare heuristic improvements\nto the mutation process based on cost minimization, on the adaptive cost of the\nprimitives and improvements due to Baldwinian vs. Lamarckian GA. We also\ndescribe various fitness function formulations that allowed for various\nrealizations of well known universal Boolean reversible or\nquantum-probabilistic circuits.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2011 08:50:49 GMT"}], "update_date": "2011-07-19", "authors_parsed": [["nLukac", "Maarti", ""], ["Perkowski", "Marek", ""], ["Kameyama", "Michitaka", ""]]}, {"id": "1107.3663", "submitter": "Antoine Bordes", "authors": "Antoine Bordes, Xavier Glorot, Jason Weston, Yoshua Bengio", "title": "Towards Open-Text Semantic Parsing via Multi-Task Learning of Structured\n  Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open-text (or open-domain) semantic parsers are designed to interpret any\nstatement in natural language by inferring a corresponding meaning\nrepresentation (MR). Unfortunately, large scale systems cannot be easily\nmachine-learned due to lack of directly supervised data. We propose here a\nmethod that learns to assign MRs to a wide range of text (using a dictionary of\nmore than 70,000 words, which are mapped to more than 40,000 entities) thanks\nto a training scheme that combines learning from WordNet and ConceptNet with\nlearning from raw text. The model learns structured embeddings of words,\nentities and MRs via a multi-task training process operating on these diverse\nsources of data that integrates all the learnt knowledge into a single system.\nThis work ends up combining methods for knowledge acquisition, semantic\nparsing, and word-sense disambiguation. Experiments on various tasks indicate\nthat our approach is indeed successful and can form a basis for future more\nsophisticated systems.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2011 09:44:09 GMT"}], "update_date": "2011-07-20", "authors_parsed": [["Bordes", "Antoine", ""], ["Glorot", "Xavier", ""], ["Weston", "Jason", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1107.3765", "submitter": "Jordan Boyd-Graber", "authors": "Ke Zhai, Jordan Boyd-Graber, and Nima Asadi", "title": "Using Variational Inference and MapReduce to Scale Topic Modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent Dirichlet Allocation (LDA) is a popular topic modeling technique for\nexploring document collections. Because of the increasing prevalence of large\ndatasets, there is a need to improve the scalability of inference of LDA. In\nthis paper, we propose a technique called ~\\emph{MapReduce LDA} (Mr. LDA) to\naccommodate very large corpus collections in the MapReduce framework. In\ncontrast to other techniques to scale inference for LDA, which use Gibbs\nsampling, we use variational inference. Our solution efficiently distributes\ncomputation and is relatively simple to implement. More importantly, this\nvariational implementation, unlike highly tuned and specialized\nimplementations, is easily extensible. We demonstrate two extensions of the\nmodel possible with this scalable framework: informed priors to guide topic\ndiscovery and modeling topics from a multilingual corpus.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2011 16:32:22 GMT"}], "update_date": "2011-07-20", "authors_parsed": [["Zhai", "Ke", ""], ["Boyd-Graber", "Jordan", ""], ["Asadi", "Nima", ""]]}, {"id": "1107.3792", "submitter": "C. Seshadhri", "authors": "C. Seshadhri and Yevgeniy Vorobeychik and Jackson R. Mayo and Robert\n  C. Armstrong and Joseph R. Ruthruff", "title": "Influence and Dynamic Behavior in Random Boolean Networks", "comments": "To appear as a Letter in Physical Review Letters 8 pages, 4 figures", "journal-ref": "Phys. Rev. Lett. 107, 108701 (2011)", "doi": "10.1103/PhysRevLett.107.108701", "report-no": null, "categories": "cond-mat.dis-nn cs.AI cs.DM nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a rigorous mathematical framework for analyzing dynamics of a\nbroad class of Boolean network models. We use this framework to provide the\nfirst formal proof of many of the standard critical transition results in\nBoolean network analysis, and offer analogous characterizations for novel\nclasses of random Boolean networks. We precisely connect the short-run dynamic\nbehavior of a Boolean network to the average influence of the transfer\nfunctions. We show that some of the assumptions traditionally made in the more\ncommon mean-field analysis of Boolean networks do not hold in general.\n  For example, we offer some evidence that imbalance, or expected internal\ninhomogeneity, of transfer functions is a crucial feature that tends to drive\nquiescent behavior far more strongly than previously observed.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2011 17:45:40 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Seshadhri", "C.", ""], ["Vorobeychik", "Yevgeniy", ""], ["Mayo", "Jackson R.", ""], ["Armstrong", "Robert C.", ""], ["Ruthruff", "Joseph R.", ""]]}, {"id": "1107.3894", "submitter": "Lu Dang Khoa Nguyen", "authors": "Nguyen Lu Dang Khoa and Sanjay Chawla", "title": "Online Anomaly Detection Systems Using Incremental Commute Time", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commute Time Distance (CTD) is a random walk based metric on graphs. CTD has\nfound widespread applications in many domains including personalized search,\ncollaborative filtering and making search engines robust against manipulation.\nOur interest is inspired by the use of CTD as a metric for anomaly detection.\nIt has been shown that CTD can be used to simultaneously identify both global\nand local anomalies. Here we propose an accurate and efficient approximation\nfor computing the CTD in an incremental fashion in order to facilitate\nreal-time applications. An online anomaly detection algorithm is designed where\nthe CTD of each new arriving data point to any point in the current graph can\nbe estimated in constant time ensuring a real-time response. Moreover, the\nproposed approach can also be applied in many other applications that utilize\ncommute time distance.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2011 05:35:40 GMT"}, {"version": "v2", "created": "Wed, 27 Jul 2011 06:37:01 GMT"}], "update_date": "2011-07-28", "authors_parsed": [["Khoa", "Nguyen Lu Dang", ""], ["Chawla", "Sanjay", ""]]}, {"id": "1107.4035", "submitter": "David Poole", "authors": "David Poole, Fahiem Bacchus, Jacek Kisynski", "title": "Towards Completely Lifted Search-based Probabilistic Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The promise of lifted probabilistic inference is to carry out probabilistic\ninference in a relational probabilistic model without needing to reason about\neach individual separately (grounding out the representation) by treating the\nundistinguished individuals as a block. Current exact methods still need to\nground out in some cases, typically because the representation of the\nintermediate results is not closed under the lifted operations. We set out to\nanswer the question as to whether there is some fundamental reason why lifted\nalgorithms would need to ground out undifferentiated individuals. We have two\nmain results: (1) We completely characterize the cases where grounding is\npolynomial in a population size, and show how we can do lifted inference in\ntime polynomial in the logarithm of the population size for these cases. (2)\nFor the case of no-argument and single-argument parametrized random variables\nwhere the grounding is not polynomial in a population size, we present lifted\ninference which is polynomial in the population size whereas grounding is\nexponential. Neither of these cases requires reasoning separately about the\nindividuals that are not explicitly mentioned.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2011 17:04:12 GMT"}, {"version": "v2", "created": "Thu, 21 Jul 2011 15:01:14 GMT"}], "update_date": "2011-07-22", "authors_parsed": [["Poole", "David", ""], ["Bacchus", "Fahiem", ""], ["Kisynski", "Jacek", ""]]}, {"id": "1107.4057", "submitter": "Nick Mehrdad Loghmani", "authors": "Nick Mehrdad Loghmani", "title": "The Harmonic Theory; A mathematical framework to build intelligent\n  contextual and adaptive computing, cognition and sensory system", "comments": "16 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Harmonic theory provides a mathematical framework to describe the structure,\nbehavior, evolution and emergence of harmonic systems. A harmonic system is\ncontext aware, contains elements that manifest characteristics either\ncollaboratively or independently according to system's expression and can\ninteract with its environment. This theory provides a fresh way to analyze\nemergence and collaboration of \"ad-hoc\" and complex systems.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2011 18:25:55 GMT"}], "update_date": "2011-10-27", "authors_parsed": [["Loghmani", "Nick Mehrdad", ""]]}, {"id": "1107.4161", "submitter": "Sebastien Verel", "authors": "Fabio Daolio (ISI), S\\'ebastien Verel (INRIA Lille - Nord Europe),\n  Gabriela Ochoa, Marco Tomassini (ISI)", "title": "Local Optima Networks of the Quadratic Assignment Problem", "comments": null, "journal-ref": "IEEE world conference on computational intelligence (WCCI - CEC),\n  Barcelona : Spain (2010)", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using a recently proposed model for combinatorial landscapes, Local Optima\nNetworks (LON), we conduct a thorough analysis of two types of instances of the\nQuadratic Assignment Problem (QAP). This network model is a reduction of the\nlandscape in which the nodes correspond to the local optima, and the edges\naccount for the notion of adjacency between their basins of attraction. The\nmodel was inspired by the notion of 'inherent network' of potential energy\nsurfaces proposed in physical-chemistry. The local optima networks extracted\nfrom the so called uniform and real-like QAP instances, show features clearly\ndistinguishing these two types of instances. Apart from a clear confirmation\nthat the search difficulty increases with the problem dimension, the analysis\nprovides new confirming evidence explaining why the real-like instances are\neasier to solve exactly using heuristic search, while the uniform instances are\neasier to solve approximately. Although the local optima network model is still\nunder development, we argue that it provides a novel view of combinatorial\nlandscapes, opening up the possibilities for new analytical tools and\nunderstanding of problem difficulty in combinatorial optimization.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2011 05:07:25 GMT"}], "update_date": "2011-07-22", "authors_parsed": [["Daolio", "Fabio", "", "ISI"], ["Verel", "S\u00e9bastien", "", "INRIA Lille - Nord Europe"], ["Ochoa", "Gabriela", "", "ISI"], ["Tomassini", "Marco", "", "ISI"]]}, {"id": "1107.4162", "submitter": "Sebastien Verel", "authors": "S\\'ebastien Verel (INRIA Lille - Nord Europe), Gabriela Ochoa, Marco\n  Tomassini (ISI)", "title": "Local Optima Networks of NK Landscapes with Neutrality", "comments": "IEEE Transactions on Evolutionary Computation volume 14, 6 (2010) to\n  appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In previous work we have introduced a network-based model that abstracts many\ndetails of the underlying landscape and compresses the landscape information\ninto a weighted, oriented graph which we call the local optima network. The\nvertices of this graph are the local optima of the given fitness landscape,\nwhile the arcs are transition probabilities between local optima basins. Here\nwe extend this formalism to neutral fitness landscapes, which are common in\ndifficult combinatorial search spaces. By using two known neutral variants of\nthe NK family (i.e. NKp and NKq) in which the amount of neutrality can be tuned\nby a parameter, we show that our new definitions of the optima networks and the\nassociated basins are consistent with the previous definitions for the\nnon-neutral case. Moreover, our empirical study and statistical analysis show\nthat the features of neutral landscapes interpolate smoothly between landscapes\nwith maximum neutrality and non-neutral ones. We found some unknown structural\ndifferences between the two studied families of neutral landscapes. But\noverall, the network features studied confirmed that neutrality, in landscapes\nwith percolating neutral networks, may enhance heuristic search. Our current\nmethodology requires the exhaustive enumeration of the underlying search space.\nTherefore, sampling techniques should be developed before this analysis can\nhave practical implications. We argue, however, that the proposed model offers\na new perspective into the problem difficulty of combinatorial optimization\nproblems and may inspire the design of more effective search heuristics.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2011 05:08:03 GMT"}], "update_date": "2011-07-22", "authors_parsed": [["Verel", "S\u00e9bastien", "", "INRIA Lille - Nord Europe"], ["Ochoa", "Gabriela", "", "ISI"], ["Tomassini", "Marco", "", "ISI"]]}, {"id": "1107.4163", "submitter": "Sebastien Verel", "authors": "David Simoncini, S\\'ebastien Verel, Philippe Collard, Manuel Clergue", "title": "Centric selection: a way to tune the exploration/exploitation trade-off", "comments": null, "journal-ref": "GECCO'09, Montreal : Canada (2009)", "doi": "10.1145/1569901.1570023", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the exploration / exploitation trade-off in cellular\ngenetic algorithms. We define a new selection scheme, the centric selection,\nwhich is tunable and allows controlling the selective pressure with a single\nparameter. The equilibrium model is used to study the influence of the centric\nselection on the selective pressure and a new model which takes into account\nproblem dependent statistics and selective pressure in order to deal with the\nexploration / exploitation trade-off is proposed: the punctuated equilibria\nmodel. Performances on the quadratic assignment problem and NK-Landscapes put\nin evidence an optimal exploration / exploitation trade-off on both of the\nclasses of problems. The punctuated equilibria model is used to explain these\nresults.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2011 05:08:30 GMT"}], "update_date": "2011-07-22", "authors_parsed": [["Simoncini", "David", ""], ["Verel", "S\u00e9bastien", ""], ["Collard", "Philippe", ""], ["Clergue", "Manuel", ""]]}, {"id": "1107.4164", "submitter": "Sebastien Verel", "authors": "Leonardo Vanneschi (DISCo), S\\'ebastien Verel, Philippe Collard, Marco\n  Tomassini (ISI)", "title": "NK landscapes difficulty and Negative Slope Coefficient: How Sampling\n  Influences the Results", "comments": null, "journal-ref": "evoNum workshop of evostar conference, Tubingen : Germany (2009)", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Negative Slope Coefficient is an indicator of problem hardness that has been\nintroduced in 2004 and that has returned promising results on a large set of\nproblems. It is based on the concept of fitness cloud and works by partitioning\nthe cloud into a number of bins representing as many different regions of the\nfitness landscape. The measure is calculated by joining the bins centroids by\nsegments and summing all their negative slopes. In this paper, for the first\ntime, we point out a potential problem of the Negative Slope Coefficient: we\nstudy its value for different instances of the well known NK-landscapes and we\nshow how this indicator is dramatically influenced by the minimum number of\npoints contained into a bin. Successively, we formally justify this behavior of\nthe Negative Slope Coefficient and we discuss pros and cons of this measure.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2011 05:08:50 GMT"}], "update_date": "2011-07-22", "authors_parsed": [["Vanneschi", "Leonardo", "", "DISCo"], ["Verel", "S\u00e9bastien", "", "ISI"], ["Collard", "Philippe", "", "ISI"], ["Tomassini", "Marco", "", "ISI"]]}, {"id": "1107.4212", "submitter": "Umberto Straccia", "authors": "Marco Cerami and Umberto Straccia", "title": "On the Undecidability of Fuzzy Description Logics with GCIs with\n  Lukasiewicz t-norm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently there have been some unexpected results concerning Fuzzy Description\nLogics (FDLs) with General Concept Inclusions (GCIs). They show that, unlike\nthe classical case, the DL ALC with GCIs does not have the finite model\nproperty under Lukasiewicz Logic or Product Logic and, specifically, knowledge\nbase satisfiability is an undecidable problem for Product Logic. We complete\nhere the analysis by showing that knowledge base satisfiability is also an\nundecidable problem for Lukasiewicz Logic.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2011 09:38:11 GMT"}, {"version": "v2", "created": "Mon, 25 Jul 2011 13:35:28 GMT"}, {"version": "v3", "created": "Wed, 27 Jul 2011 15:48:55 GMT"}], "update_date": "2011-07-28", "authors_parsed": [["Cerami", "Marco", ""], ["Straccia", "Umberto", ""]]}, {"id": "1107.4303", "submitter": "Kostyantyn Shchekotykhin", "authors": "Kostyantyn Shchekotykhin, Gerhard Friedrich, Philipp Fleiss, Patrick\n  Rodler", "title": "Interactive ontology debugging: two query strategies for efficient fault\n  localization", "comments": "Published in Web Semantics: Science, Services and Agents on the World\n  Wide Web. arXiv admin note: substantial text overlap with arXiv:1004.5339", "journal-ref": "Journal of Web Semantics 12 (2012) 88-103", "doi": "10.1016/j.websem.2011.12.006", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective debugging of ontologies is an important prerequisite for their\nbroad application, especially in areas that rely on everyday users to create\nand maintain knowledge bases, such as the Semantic Web. In such systems\nontologies capture formalized vocabularies of terms shared by its users.\nHowever in many cases users have different local views of the domain, i.e. of\nthe context in which a given term is used. Inappropriate usage of terms\ntogether with natural complications when formulating and understanding logical\ndescriptions may result in faulty ontologies. Recent ontology debugging\napproaches use diagnosis methods to identify causes of the faults. In most\ndebugging scenarios these methods return many alternative diagnoses, thus\nplacing the burden of fault localization on the user. This paper demonstrates\nhow the target diagnosis can be identified by performing a sequence of\nobservations, that is, by querying an oracle about entailments of the target\nontology. To identify the best query we propose two query selection strategies:\na simple \"split-in-half\" strategy and an entropy-based strategy. The latter\nallows knowledge about typical user errors to be exploited to minimize the\nnumber of queries. Our evaluation showed that the entropy-based method\nsignificantly reduces the number of required queries compared to the\n\"split-in-half\" approach. We experimented with different probability\ndistributions of user errors and different qualities of the a-priori\nprobabilities. Our measurements demonstrated the superiority of entropy-based\nquery selection even in cases where all fault probabilities are equal, i.e.\nwhere no information about typical user errors is available.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2011 10:02:07 GMT"}, {"version": "v2", "created": "Sun, 27 Apr 2014 10:20:14 GMT"}], "update_date": "2014-04-29", "authors_parsed": [["Shchekotykhin", "Kostyantyn", ""], ["Friedrich", "Gerhard", ""], ["Fleiss", "Philipp", ""], ["Rodler", "Patrick", ""]]}, {"id": "1107.4502", "submitter": "Jerome Euzenat", "authors": "Fran\\c{c}ois Scharffe (LIRMM), J\\'er\\^ome Euzenat (INRIA Grenoble\n  Rh\\^one-Alpes / LIG Laboratoire d'Informatique de Grenoble)", "title": "MeLinDa: an interlinking framework for the web of data", "comments": "N&deg; RR-7691 (2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The web of data consists of data published on the web in such a way that they\ncan be interpreted and connected together. It is thus critical to establish\nlinks between these data, both for the web of data and for the semantic web\nthat it contributes to feed. We consider here the various techniques developed\nfor that purpose and analyze their commonalities and differences. We propose a\ngeneral framework and show how the diverse techniques fit in the framework.\nFrom this framework we consider the relation between data interlinking and\nontology matching. Although, they can be considered similar at a certain level\n(they both relate formal entities), they serve different purposes, but would\nfind a mutual benefit at collaborating. We thus present a scheme under which it\nis possible for data linking tools to take advantage of ontology alignments.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2011 12:48:32 GMT"}], "update_date": "2011-07-25", "authors_parsed": [["Scharffe", "Fran\u00e7ois", "", "LIRMM"], ["Euzenat", "J\u00e9r\u00f4me", "", "INRIA Grenoble\n  Rh\u00f4ne-Alpes / LIG Laboratoire d'Informatique de Grenoble"]]}, {"id": "1107.4553", "submitter": "Thierry Boy de la Tour", "authors": "Thierry Boy de la Tour, Mnacho Echenim", "title": "Solving Linear Constraints in Elementary Abelian p-Groups of Symmetries", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symmetries occur naturally in CSP or SAT problems and are not very difficult\nto discover, but using them to prune the search space tends to be very\nchallenging. Indeed, this usually requires finding specific elements in a group\nof symmetries that can be huge, and the problem of their very existence is\nNP-hard. We formulate such an existence problem as a constraint problem on one\nvariable (the symmetry to be used) ranging over a group, and try to find\nrestrictions that may be solved in polynomial time. By considering a simple\nform of constraints (restricted by a cardinality k) and the class of groups\nthat have the structure of Fp-vector spaces, we propose a partial algorithm\nbased on linear algebra. This polynomial algorithm always applies when k=p=2,\nbut may fail otherwise as we prove the problem to be NP-hard for all other\nvalues of k and p. Experiments show that this approach though restricted should\nallow for an efficient use of at least some groups of symmetries. We conclude\nwith a few directions to be explored to efficiently solve this problem on the\ngeneral case.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2011 15:52:26 GMT"}], "update_date": "2011-07-25", "authors_parsed": [["de la Tour", "Thierry Boy", ""], ["Echenim", "Mnacho", ""]]}, {"id": "1107.4570", "submitter": "Marco Manna", "authors": "Marco Manna, Francesco Ricca and Giorgio Terracina", "title": "Consistent Query Answering via ASP from Different Perspectives: Theory\n  and Practice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A data integration system provides transparent access to different data\nsources by suitably combining their data, and providing the user with a unified\nview of them, called global schema. However, source data are generally not\nunder the control of the data integration process, thus integrated data may\nviolate global integrity constraints even in presence of locally-consistent\ndata sources. In this scenario, it may be anyway interesting to retrieve as\nmuch consistent information as possible. The process of answering user queries\nunder global constraint violations is called consistent query answering (CQA).\nSeveral notions of CQA have been proposed, e.g., depending on whether\nintegrated information is assumed to be sound, complete, exact or a variant of\nthem. This paper provides a contribution in this setting: it uniforms solutions\ncoming from different perspectives under a common ASP-based core, and provides\nquery-driven optimizations designed for isolating and eliminating\ninefficiencies of the general approach for computing consistent answers.\nMoreover, the paper introduces some new theoretical results enriching existing\nknowledge on decidability and complexity of the considered problems. The\neffectiveness of the approach is evidenced by experimental results.\n  To appear in Theory and Practice of Logic Programming (TPLP).\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2011 16:45:21 GMT"}, {"version": "v2", "created": "Fri, 7 Oct 2011 14:09:50 GMT"}], "update_date": "2011-10-10", "authors_parsed": [["Manna", "Marco", ""], ["Ricca", "Francesco", ""], ["Terracina", "Giorgio", ""]]}, {"id": "1107.4573", "submitter": "Peter Turney", "authors": "Peter D. Turney (National Research Council of Canada)", "title": "Analogy perception applied to seven tests of word comprehension", "comments": "related work available at http://purl.org/peter.turney/", "journal-ref": "Journal of Experimental & Theoretical Artificial Intelligence\n  (JETAI), 2011, Volume 23, Issue 3, pages 343-362", "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been argued that analogy is the core of cognition. In AI research,\nalgorithms for analogy are often limited by the need for hand-coded high-level\nrepresentations as input. An alternative approach is to use high-level\nperception, in which high-level representations are automatically generated\nfrom raw data. Analogy perception is the process of recognizing analogies using\nhigh-level perception. We present PairClass, an algorithm for analogy\nperception that recognizes lexical proportional analogies using representations\nthat are automatically generated from a large corpus of raw textual data. A\nproportional analogy is an analogy of the form A:B::C:D, meaning \"A is to B as\nC is to D\". A lexical proportional analogy is a proportional analogy with\nwords, such as carpenter:wood::mason:stone. PairClass represents the semantic\nrelations between two words using a high-dimensional feature vector, in which\nthe elements are based on frequencies of patterns in the corpus. PairClass\nrecognizes analogies by applying standard supervised machine learning\ntechniques to the feature vectors. We show how seven different tests of word\ncomprehension can be framed as problems of analogy perception and we then apply\nPairClass to the seven resulting sets of analogy perception problems. We\nachieve competitive results on all seven tests. This is the first time a\nuniform approach has handled such a range of tests of word comprehension.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2011 16:54:11 GMT"}], "update_date": "2011-07-25", "authors_parsed": [["Turney", "Peter D.", "", "National Research Council of Canada"]]}, {"id": "1107.4651", "submitter": "Nittaya Kerdprasop Prof.", "authors": "Nittaya Kerdprasop and Kittisak Kerdprasop", "title": "Higher Order Programming to Mine Knowledge for a Modern Medical Expert\n  System", "comments": "9 pages", "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 8,\n  Issue 3, May 2011 ISSN (Online): 1694-0814 www.IJCSI.org", "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge mining is the process of deriving new and useful knowledge from\nvast volumes of data and background knowledge. Modern healthcare organizations\nregularly generate huge amount of electronic data stored in the databases.\nThese data are a valuable resource for mining useful knowledge to help medical\npractitioners making appropriate and accurate decision on the diagnosis and\ntreatment of diseases. In this paper, we propose the design of a novel medical\nexpert system based on a logic-programming framework. The proposed system\nincludes a knowledge-mining component as a repertoire of tools for discovering\nuseful knowledge. The implementation of classification and association mining\ntools based on the higher order and meta-level programming schemes using Prolog\nhas been presented to express the power of logic-based language. Such language\nalso provides a pattern matching facility, which is an essential function for\nthe development of knowledge-intensive tasks. Besides the major goal of medical\ndecision support, the knowledge discovered by our logic-based knowledge-mining\ncomponent can also be deployed as background knowledge to pre-treatment data\nfrom other sources as well as to guard the data repositories against constraint\nviolation. A framework for knowledge deployment is also presented.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jul 2011 03:04:22 GMT"}], "update_date": "2011-07-26", "authors_parsed": [["Kerdprasop", "Nittaya", ""], ["Kerdprasop", "Kittisak", ""]]}, {"id": "1107.4747", "submitter": "Fabrizio Riguzzi PhD", "authors": "Fabrizio Riguzzi and Terrance Swift", "title": "The PITA System: Tabling and Answer Subsumption for Reasoning under\n  Uncertainty", "comments": null, "journal-ref": "Theory and Practice of Logic Programming, 27th International\n  Conference on Logic Programming (ICLP'11) Special Issue, 11(4-5), 433-449,\n  2011", "doi": "10.1017/S147106841100010X", "report-no": null, "categories": "cs.AI cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real world domains require the representation of a measure of\nuncertainty. The most common such representation is probability, and the\ncombination of probability with logic programs has given rise to the field of\nProbabilistic Logic Programming (PLP), leading to languages such as the\nIndependent Choice Logic, Logic Programs with Annotated Disjunctions (LPADs),\nProblog, PRISM and others. These languages share a similar distribution\nsemantics, and methods have been devised to translate programs between these\nlanguages. The complexity of computing the probability of queries to these\ngeneral PLP programs is very high due to the need to combine the probabilities\nof explanations that may not be exclusive. As one alternative, the PRISM system\nreduces the complexity of query answering by restricting the form of programs\nit can evaluate. As an entirely different alternative, Possibilistic Logic\nPrograms adopt a simpler metric of uncertainty than probability. Each of these\napproaches -- general PLP, restricted PLP, and Possibilistic Logic Programming\n-- can be useful in different domains depending on the form of uncertainty to\nbe represented, on the form of programs needed to model problems, and on the\nscale of the problems to be solved. In this paper, we show how the PITA system,\nwhich originally supported the general PLP language of LPADs, can also\nefficiently support restricted PLP and Possibilistic Logic Programs. PITA\nrelies on tabling with answer subsumption and consists of a transformation\nalong with an API for library functions that interface with answer subsumption.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jul 2011 11:22:41 GMT"}], "update_date": "2011-07-26", "authors_parsed": [["Riguzzi", "Fabrizio", ""], ["Swift", "Terrance", ""]]}, {"id": "1107.4865", "submitter": "Joost Vennekens", "authors": "Joost Vennekens", "title": "Actual Causation in CP-logic", "comments": null, "journal-ref": "Theory and Practice of Logic Programming, 27th Int'l. Conference\n  on Logic Programming (ICLP'11) Special Issue, volume 11, issue 4-5,\n  p.647-662, 2011", "doi": "10.1017/S1471068411000226", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a causal model of some domain and a particular story that has taken\nplace in this domain, the problem of actual causation is deciding which of the\npossible causes for some effect actually caused it. One of the most influential\napproaches to this problem has been developed by Halpern and Pearl in the\ncontext of structural models. In this paper, I argue that this is actually not\nthe best setting for studying this problem. As an alternative, I offer the\nprobabilistic logic programming language of CP-logic. Unlike structural models,\nCP-logic incorporates the deviant/default distinction that is generally\nconsidered an important aspect of actual causation, and it has an explicitly\ndynamic semantics, which helps to formalize the stories that serve as input to\nan actual causation problem.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2011 08:24:50 GMT"}], "update_date": "2011-07-26", "authors_parsed": [["Vennekens", "Joost", ""]]}, {"id": "1107.4937", "submitter": "Nicolas Peltier", "authors": "Mnacho Echenim and Nicolas Peltier", "title": "Instantiation Schemes for Nested Theories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates under which conditions instantiation-based proof\nprocedures can be combined in a nested way, in order to mechanically construct\nnew instantiation procedures for richer theories. Interesting applications in\nthe field of verification are emphasized, particularly for handling extensions\nof the theory of arrays.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2011 13:14:54 GMT"}], "update_date": "2011-07-26", "authors_parsed": [["Echenim", "Mnacho", ""], ["Peltier", "Nicolas", ""]]}, {"id": "1107.4966", "submitter": "Lilyana Mihalkova", "authors": "Lilyana Mihalkova and Lise Getoor", "title": "Lifted Graphical Models: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a survey of work on lifted graphical models. We review\na general form for a lifted graphical model, a par-factor graph, and show how a\nnumber of existing statistical relational representations map to this\nformalism. We discuss inference algorithms, including lifted inference\nalgorithms, that efficiently compute the answers to probabilistic queries. We\nalso review work in learning lifted graphical models from data. It is our\nbelief that the need for statistical relational models (whether it goes by that\nname or another) will grow in the coming decades, as we are inundated with data\nwhich is a mix of structured and unstructured, with entities and relations\nextracted in a noisy manner from text, and with the need to reason effectively\nwith this data. We hope that this synthesis of ideas from many different\nresearch groups will provide an accessible starting point for new researchers\nin this expanding field.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2011 14:56:18 GMT"}, {"version": "v2", "created": "Fri, 26 Aug 2011 17:27:04 GMT"}], "update_date": "2011-08-29", "authors_parsed": [["Mihalkova", "Lilyana", ""], ["Getoor", "Lise", ""]]}, {"id": "1107.4967", "submitter": "Domenico Corapi Domenico Corapi", "authors": "Domenico Corapi, Alessandra Russo, Marina De Vos, Julian Padget, Ken\n  Satoh", "title": "Normative design using inductive learning", "comments": "Theory and Practice of Logic Programming, 27th Int'l. Conference on\n  Logic Programming (ICLP'11) Special Issue, volume 11, issue 4-5, 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a use-case-driven iterative design methodology for\nnormative frameworks, also called virtual institutions, which are used to\ngovern open systems. Our computational model represents the normative framework\nas a logic program under answer set semantics (ASP). By means of an inductive\nlogic programming approach, implemented using ASP, it is possible to synthesise\nnew rules and revise the existing ones. The learning mechanism is guided by the\ndesigner who describes the desired properties of the framework through use\ncases, comprising (i) event traces that capture possible scenarios, and (ii) a\nstate that describes the desired outcome. The learning process then proposes\nadditional rules, or changes to current rules, to satisfy the constraints\nexpressed in the use cases. Thus, the contribution of this paper is a process\nfor the elaboration and revision of a normative framework by means of a\nsemi-automatic and iterative process driven from specifications of\n(un)desirable behaviour. The process integrates a novel and general methodology\nfor theory revision based on ASP.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2011 15:01:50 GMT"}], "update_date": "2011-07-26", "authors_parsed": [["Corapi", "Domenico", ""], ["Russo", "Alessandra", ""], ["De Vos", "Marina", ""], ["Padget", "Julian", ""], ["Satoh", "Ken", ""]]}, {"id": "1107.4969", "submitter": "Yizhao Ni", "authors": "Yizhao Ni, Matt Mcvicar, Raul Santos-Rodriguez and Tijl De Bie", "title": "An end-to-end machine learning system for harmonic analysis of music", "comments": "MIREX report and preparation of Journal submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new system for simultaneous estimation of keys, chords, and bass\nnotes from music audio. It makes use of a novel chromagram representation of\naudio that takes perception of loudness into account. Furthermore, it is fully\nbased on machine learning (instead of expert knowledge), such that it is\npotentially applicable to a wider range of genres as long as training data is\navailable. As compared to other models, the proposed system is fast and memory\nefficient, while achieving state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2011 15:09:37 GMT"}], "update_date": "2011-07-26", "authors_parsed": [["Ni", "Yizhao", ""], ["Mcvicar", "Matt", ""], ["Santos-Rodriguez", "Raul", ""], ["De Bie", "Tijl", ""]]}, {"id": "1107.4985", "submitter": "Andreas Damianou Mr", "authors": "Andreas C. Damianou, Michalis K. Titsias, Neil D. Lawrence", "title": "Variational Gaussian Process Dynamical Systems", "comments": "16 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High dimensional time series are endemic in applications of machine learning\nsuch as robotics (sensor data), computational biology (gene expression data),\nvision (video sequences) and graphics (motion capture data). Practical\nnonlinear probabilistic approaches to this data are required. In this paper we\nintroduce the variational Gaussian process dynamical system. Our work builds on\nrecent variational approximations for Gaussian process latent variable models\nto allow for nonlinear dimensionality reduction simultaneously with learning a\ndynamical prior in the latent space. The approach also allows for the\nappropriate dimensionality of the latent space to be automatically determined.\nWe demonstrate the model on a human motion capture data set and a series of\nhigh resolution video sequences.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2011 15:54:05 GMT"}], "update_date": "2011-07-26", "authors_parsed": [["Damianou", "Andreas C.", ""], ["Titsias", "Michalis K.", ""], ["Lawrence", "Neil D.", ""]]}, {"id": "1107.5000", "submitter": "Fabricio Martins Lopes", "authors": "Fabr\\'icio Martins Lopes and David C. Martins-Jr and Junior Barrera\n  and Roberto M. Cesar-Jr", "title": "An iterative feature selection method for GRNs inference by exploring\n  topological properties", "comments": "10 pages, 5 figures, SFFS search method based on scale-free network\n  topology", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IT math.IT q-bio.MN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important problem in bioinformatics is the inference of gene regulatory\nnetworks (GRN) from temporal expression profiles. In general, the main\nlimitations faced by GRN inference methods is the small number of samples with\nhuge dimensionalities and the noisy nature of the expression measurements. In\nface of these limitations, alternatives are needed to get better accuracy on\nthe GRNs inference problem. This work addresses this problem by presenting an\nalternative feature selection method that applies prior knowledge on its search\nstrategy, called SFFS-BA. The proposed search strategy is based on the\nSequential Floating Forward Selection (SFFS) algorithm, with the inclusion of a\nscale-free (Barab\\'asi-Albert) topology information in order to guide the\nsearch process to improve inference. The proposed algorithm explores the\nscale-free property by pruning the search space and using a power law as a\nweight for reducing it. In this way, the search space traversed by the SFFS-BA\nmethod combines a breadth-first search when the number of combinations is small\n(<k> <= 2) with a depth-first search when the number of combinations becomes\nexplosive (<k> >= 3), being guided by the scale-free prior information.\nExperimental results show that the SFFS-BA provides a better inference\nsimilarities than SFS and SFFS, keeping the robustness of the SFS and SFFS\nmethods, thus presenting very good results.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2011 17:04:33 GMT"}], "update_date": "2011-07-26", "authors_parsed": [["Lopes", "Fabr\u00edcio Martins", ""], ["Martins-Jr", "David C.", ""], ["Barrera", "Junior", ""], ["Cesar-Jr", "Roberto M.", ""]]}, {"id": "1107.5236", "submitter": "Wael Emara", "authors": "Wael Emara and Mehmed Kantardzic", "title": "Submodular Optimization for Efficient Semi-supervised Support Vector\n  Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present a quadratic programming approximation of the\nSemi-Supervised Support Vector Machine (S3VM) problem, namely approximate\nQP-S3VM, that can be efficiently solved using off the shelf optimization\npackages. We prove that this approximate formulation establishes a relation\nbetween the low density separation and the graph-based models of\nsemi-supervised learning (SSL) which is important to develop a unifying\nframework for semi-supervised learning methods. Furthermore, we propose the\nnovel idea of representing SSL problems as submodular set functions and use\nefficient submodular optimization algorithms to solve them. Using this new idea\nwe develop a representation of the approximate QP-S3VM as a maximization of a\nsubmodular set function which makes it possible to optimize using efficient\ngreedy algorithms. We demonstrate that the proposed methods are accurate and\nprovide significant improvement in time complexity over the state of the art in\nthe literature.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2011 15:11:10 GMT"}, {"version": "v2", "created": "Tue, 23 Aug 2011 17:42:35 GMT"}], "update_date": "2011-08-24", "authors_parsed": [["Emara", "Wael", ""], ["Kantardzic", "Mehmed", ""]]}, {"id": "1107.5242", "submitter": "Conrad Drescher", "authors": "Conrad Drescher and Michael Thielscher", "title": "ALPprolog --- A New Logic Programming Method for Dynamic Domains", "comments": "16 pages", "journal-ref": "Theory and Practice of Logic Programming, 11(4-5), 451-468, 2011", "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logic programming is a powerful paradigm for programming autonomous agents in\ndynamic domains, as witnessed by languages such as Golog and Flux. In this work\nwe present ALPprolog, an expressive, yet efficient, logic programming language\nfor the online control of agents that have to reason about incomplete\ninformation and sensing actions.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2011 15:28:13 GMT"}], "update_date": "2011-07-27", "authors_parsed": [["Drescher", "Conrad", ""], ["Thielscher", "Michael", ""]]}, {"id": "1107.5387", "submitter": "Tauseef Gulrez", "authors": "Tauseef Gulrez, Alessandro Tognetti, Alon Fishbach, Santiago Acosta,\n  Christopher Scharver, Danilo De Rossi and Ferdinando A. Mussa-Ivaldi", "title": "Controlling wheelchairs by body motions: A learning framework for the\n  adaptive remapping of space", "comments": "This paper was published in the proceedings of Cognitive Systems\n  2008, Karlsruhe, Germany", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to operate a vehicle is generally accomplished by forming a new\ncognitive map between the body motions and extrapersonal space. Here, we\nconsider the challenge of remapping movement-to-space representations in\nsurvivors of spinal cord injury, for the control of powered wheelchairs. Our\ngoal is to facilitate this remapping by developing interfaces between residual\nbody motions and navigational commands that exploit the degrees of freedom that\ndisabled individuals are most capable to coordinate. We present a new framework\nfor allowing spinal cord injured persons to control powered wheelchairs through\nsignals derived from their residual mobility. The main novelty of this approach\nlies in substituting the more common joystick controllers of powered\nwheelchairs with a sensor shirt. This allows the whole upper body of the user\nto operate as an adaptive joystick. Considerations about learning and risks\nhave lead us to develop a safe testing environment in 3D Virtual Reality. A\nPersonal Augmented Reality Immersive System (PARIS) allows us to analyse\nlearning skills and provide users with an adequate training to control a\nsimulated wheelchair through the signals generated by body motions in a safe\nenvironment. We provide a description of the basic theory, of the development\nphases and of the operation of the complete system. We also present preliminary\nresults illustrating the processing of the data and supporting of the\nfeasibility of this approach.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2011 05:30:40 GMT"}], "update_date": "2011-07-28", "authors_parsed": [["Gulrez", "Tauseef", ""], ["Tognetti", "Alessandro", ""], ["Fishbach", "Alon", ""], ["Acosta", "Santiago", ""], ["Scharver", "Christopher", ""], ["De Rossi", "Danilo", ""], ["Mussa-Ivaldi", "Ferdinando A.", ""]]}, {"id": "1107.5462", "submitter": "Gabriela Ochoa", "authors": "Edmund Burke, Tim Curtois, Matthew Hyde, Gabriela Ochoa, Jose A.\n  Vazquez-Rodriguez", "title": "HyFlex: A Benchmark Framework for Cross-domain Heuristic Search", "comments": "28 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automating the design of heuristic search methods is an active research field\nwithin computer science, artificial intelligence and operational research. In\norder to make these methods more generally applicable, it is important to\neliminate or reduce the role of the human expert in the process of designing an\neffective methodology to solve a given computational search problem.\nResearchers developing such methodologies are often constrained on the number\nof problem domains on which to test their adaptive, self-configuring\nalgorithms; which can be explained by the inherent difficulty of implementing\ntheir corresponding domain specific software components.\n  This paper presents HyFlex, a software framework for the development of\ncross-domain search methodologies. The framework features a common software\ninterface for dealing with different combinatorial optimisation problems, and\nprovides the algorithm components that are problem specific. In this way, the\nalgorithm designer does not require a detailed knowledge the problem domains,\nand thus can concentrate his/her efforts in designing adaptive general-purpose\nheuristic search algorithms. Four hard combinatorial problems are fully\nimplemented (maximum satisfiability, one dimensional bin packing, permutation\nflow shop and personnel scheduling), each containing a varied set of instance\ndata (including real-world industrial applications) and an extensive set of\nproblem specific heuristics and search operators. The framework forms the basis\nfor the first International Cross-domain Heuristic Search Challenge (CHeSC),\nand it is currently in use by the international research community. In summary,\nHyFlex represents a valuable new benchmark of heuristic search generality, with\nwhich adaptive cross-domain algorithms are being easily developed, and reliably\ncompared.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2011 13:07:39 GMT"}], "update_date": "2011-07-28", "authors_parsed": [["Burke", "Edmund", ""], ["Curtois", "Tim", ""], ["Hyde", "Matthew", ""], ["Ochoa", "Gabriela", ""], ["Vazquez-Rodriguez", "Jose A.", ""]]}, {"id": "1107.5474", "submitter": "Gonzalo A. Aranda-Corral", "authors": "Gonzalo A. Aranda-Corral, Joaqu\\'in Borrego-D\\'iaz and Juan\n  Gal\\'an-P\\'aez", "title": "Selecting Attributes for Sport Forecasting using Formal Concept Analysis", "comments": "Paper 3 for the Complex Systems in Sports Workshop 2011 (CS-Sports\n  2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In order to address complex systems, apply pattern recongnition on their\nevolution could play an key role to understand their dynamics. Global patterns\nare required to detect emergent concepts and trends, some of them with\nqualitative nature. Formal Concept Analysis (FCA) is a theory whose goal is to\ndiscover and to extract Knowledge from qualitative data. It provides tools for\nreasoning with implication basis (and association rules). Implications and\nassociation rules are usefull to reasoning on previously selected attributes,\nproviding a formal foundation for logical reasoning. In this paper we analyse\nhow to apply FCA reasoning to increase confidence in sports betting, by means\nof detecting temporal regularities from data. It is applied to build a\nKnowledge-Based system for confidence reasoning.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2011 13:52:20 GMT"}, {"version": "v2", "created": "Thu, 4 Aug 2011 11:46:30 GMT"}], "update_date": "2011-08-05", "authors_parsed": [["Aranda-Corral", "Gonzalo A.", ""], ["Borrego-D\u00edaz", "Joaqu\u00edn", ""], ["Gal\u00e1n-P\u00e1ez", "Juan", ""]]}, {"id": "1107.5528", "submitter": "Marcus Hutter", "authors": "Tor Lattimore and Marcus Hutter", "title": "Time Consistent Discounting", "comments": "17 LaTeX pages, 5 figures", "journal-ref": "Proc. 22nd International Conf. on Algorithmic Learning Theory\n  (ALT-2011) pages 383-397", "doi": null, "report-no": null, "categories": "cs.AI cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A possibly immortal agent tries to maximise its summed discounted rewards\nover time, where discounting is used to avoid infinite utilities and encourage\nthe agent to value current rewards more than future ones. Some commonly used\ndiscount functions lead to time-inconsistent behavior where the agent changes\nits plan over time. These inconsistencies can lead to very poor behavior. We\ngeneralise the usual discounted utility model to one where the discount\nfunction changes with the age of the agent. We then give a simple\ncharacterisation of time-(in)consistent discount functions and show the\nexistence of a rational policy for an agent that knows its discount function is\ntime-inconsistent.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2011 16:37:11 GMT"}], "update_date": "2014-07-15", "authors_parsed": [["Lattimore", "Tor", ""], ["Hutter", "Marcus", ""]]}, {"id": "1107.5537", "submitter": "Marcus Hutter", "authors": "Tor Lattimore and Marcus Hutter", "title": "Asymptotically Optimal Agents", "comments": "21 LaTeX pages", "journal-ref": "Proc. 22nd International Conf. on Algorithmic Learning Theory\n  (ALT-2011) pages 368-382", "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial general intelligence aims to create agents capable of learning to\nsolve arbitrary interesting problems. We define two versions of asymptotic\noptimality and prove that no agent can satisfy the strong version while in some\ncases, depending on discounting, there does exist a non-computable weak\nasymptotically optimal agent.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2011 16:51:48 GMT"}], "update_date": "2012-02-10", "authors_parsed": [["Lattimore", "Tor", ""], ["Hutter", "Marcus", ""]]}, {"id": "1107.5742", "submitter": "Martin Gebser", "authors": "Martin Gebser and Roland Kaminski and Torsten Schaub", "title": "Complex Optimization in Answer Set Programming", "comments": "18 pages, 5 figures", "journal-ref": "Theory and Practice of Logic Programming, 11(4-5), 821-839, 2011", "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preference handling and optimization are indispensable means for addressing\nnon-trivial applications in Answer Set Programming (ASP). However, their\nimplementation becomes difficult whenever they bring about a significant\nincrease in computational complexity. As a consequence, existing ASP systems do\nnot offer complex optimization capacities, supporting, for instance,\ninclusion-based minimization or Pareto efficiency. Rather, such complex\ncriteria are typically addressed by resorting to dedicated modeling techniques,\nlike saturation. Unlike the ease of common ASP modeling, however, these\ntechniques are rather involved and hardly usable by ASP laymen. We address this\nproblem by developing a general implementation technique by means of\nmeta-programming, thus reusing existing ASP systems to capture various forms of\nqualitative preferences among answer sets. In this way, complex preferences and\noptimization capacities become readily available for ASP applications.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2011 15:36:51 GMT"}], "update_date": "2011-07-29", "authors_parsed": [["Gebser", "Martin", ""], ["Kaminski", "Roland", ""], ["Schaub", "Torsten", ""]]}, {"id": "1107.5766", "submitter": "Pedro Alejandro Ortega", "authors": "Pedro A. Ortega and Daniel A. Braun", "title": "Information, Utility & Bounded Rationality", "comments": "10 pages. The original publication is available at\n  www.springerlink.com", "journal-ref": "The Fourth Conference on General Artificial Intelligence (AGI-11),\n  2011", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Perfectly rational decision-makers maximize expected utility, but crucially\nignore the resource costs incurred when determining optimal actions. Here we\nemploy an axiomatic framework for bounded rational decision-making based on a\nthermodynamic interpretation of resource costs as information costs. This leads\nto a variational \"free utility\" principle akin to thermodynamical free energy\nthat trades off utility and information costs. We show that bounded optimal\ncontrol solutions can be derived from this variational principle, which leads\nin general to stochastic policies. Furthermore, we show that risk-sensitive and\nrobust (minimax) control schemes fall out naturally from this framework if the\nenvironment is considered as a bounded rational and perfectly rational\nopponent, respectively. When resource costs are ignored, the maximum expected\nutility principle is recovered.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2011 16:53:15 GMT"}], "update_date": "2011-07-29", "authors_parsed": [["Ortega", "Pedro A.", ""], ["Braun", "Daniel A.", ""]]}, {"id": "1107.5930", "submitter": "C\\`esar Ferri", "authors": "Jos\\'e Hern\\'andez-Orallo, Peter Flach, C\\`esar Ferri", "title": "Technical Note: Towards ROC Curves in Cost Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ROC curves and cost curves are two popular ways of visualising classifier\nperformance, finding appropriate thresholds according to the operating\ncondition, and deriving useful aggregated measures such as the area under the\nROC curve (AUC) or the area under the optimal cost curve. In this note we\npresent some new findings and connections between ROC space and cost space, by\nusing the expected loss over a range of operating conditions. In particular, we\nshow that ROC curves can be transferred to cost space by means of a very\nnatural way of understanding how thresholds should be chosen, by selecting the\nthreshold such that the proportion of positive predictions equals the operating\ncondition (either in the form of cost proportion or skew). We call these new\ncurves {ROC Cost Curves}, and we demonstrate that the expected loss as measured\nby the area under these curves is linearly related to AUC. This opens up a\nseries of new possibilities and clarifies the notion of cost curve and its\nrelation to ROC analysis. In addition, we show that for a classifier that\nassigns the scores in an evenly-spaced way, these curves are equal to the Brier\nCurves. As a result, this establishes the first clear connection between AUC\nand the Brier score.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2011 11:03:38 GMT"}], "update_date": "2011-08-01", "authors_parsed": [["Hern\u00e1ndez-Orallo", "Jos\u00e9", ""], ["Flach", "Peter", ""], ["Ferri", "C\u00e8sar", ""]]}]