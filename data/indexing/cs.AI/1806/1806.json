[{"id": "1806.00047", "submitter": "Valts Blukis", "authors": "Valts Blukis and Nataly Brukhim and Andrew Bennett and Ross A. Knepper\n  and Yoav Artzi", "title": "Following High-level Navigation Instructions on a Simulated Quadcopter\n  with Imitation Learning", "comments": "To appear in Robotics: Science and Systems (RSS), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method for following high-level navigation instructions by\nmapping directly from images, instructions and pose estimates to continuous\nlow-level velocity commands for real-time control. The Grounded Semantic\nMapping Network (GSMN) is a fully-differentiable neural network architecture\nthat builds an explicit semantic map in the world reference frame by\nincorporating a pinhole camera projection model within the network. The\ninformation stored in the map is learned from experience, while the\nlocal-to-world transformation is computed explicitly. We train the model using\nDAggerFM, a modified variant of DAgger that trades tabular convergence\nguarantees for improved training speed and memory use. We test GSMN in virtual\nenvironments on a realistic quadcopter simulator and show that incorporating an\nexplicit mapping and grounding modules allows GSMN to outperform strong neural\nbaselines and almost reach an expert policy performance. Finally, we analyze\nthe learned map representations and show that using an explicit map leads to an\ninterpretable instruction-following model.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 18:42:26 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Blukis", "Valts", ""], ["Brukhim", "Nataly", ""], ["Bennett", "Andrew", ""], ["Knepper", "Ross A.", ""], ["Artzi", "Yoav", ""]]}, {"id": "1806.00050", "submitter": "Heinrich Jiang", "authors": "Andrew Cotter, Maya Gupta, Heinrich Jiang, James Muller, Taman\n  Narayan, Serena Wang, Tao Zhu", "title": "Interpretable Set Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose learning flexible but interpretable functions that aggregate a\nvariable-length set of permutation-invariant feature vectors to predict a\nlabel. We use a deep lattice network model so we can architect the model\nstructure to enhance interpretability, and add monotonicity constraints between\ninputs-and-outputs. We then use the proposed set function to automate the\nengineering of dense, interpretable features from sparse categorical features,\nwhich we call semantic feature engine. Experiments on real-world data show the\nachieved accuracy is similar to deep sets or deep neural networks, and is\neasier to debug and understand.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 18:53:15 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Cotter", "Andrew", ""], ["Gupta", "Maya", ""], ["Jiang", "Heinrich", ""], ["Muller", "James", ""], ["Narayan", "Taman", ""], ["Wang", "Serena", ""], ["Zhu", "Tao", ""]]}, {"id": "1806.00054", "submitter": "Taesung Lee", "authors": "Taesung Lee, Benjamin Edwards, Ian Molloy, Dong Su", "title": "Defending Against Machine Learning Model Stealing Attacks Using\n  Deceptive Perturbations", "comments": "Under review for a peer review conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models are vulnerable to simple model stealing attacks if\nthe adversary can obtain output labels for chosen inputs. To protect against\nthese attacks, it has been proposed to limit the information provided to the\nadversary by omitting probability scores, significantly impacting the utility\nof the provided service. In this work, we illustrate how a service provider can\nstill provide useful, albeit misleading, class probability information, while\nsignificantly limiting the success of the attack. Our defense forces the\nadversary to discard the class probabilities, requiring significantly more\nqueries before they can train a model with comparable performance. We evaluate\nseveral attack strategies, model architectures, and hyperparameters under\nvarying adversarial models, and evaluate the efficacy of our defense against\nthe strongest adversary. Finally, we quantify the amount of noise injected into\nthe class probabilities to mesure the loss in utility, e.g., adding 1.26 nats\nper query on CIFAR-10 and 3.27 on MNIST. Our evaluation shows our defense can\ndegrade the accuracy of the stolen model at least 20%, or require up to 64\ntimes more queries while keeping the accuracy of the protected model almost\nintact.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 19:09:15 GMT"}, {"version": "v2", "created": "Wed, 19 Sep 2018 18:09:42 GMT"}, {"version": "v3", "created": "Wed, 12 Dec 2018 17:13:47 GMT"}, {"version": "v4", "created": "Thu, 13 Dec 2018 16:41:41 GMT"}], "update_date": "2018-12-14", "authors_parsed": [["Lee", "Taesung", ""], ["Edwards", "Benjamin", ""], ["Molloy", "Ian", ""], ["Su", "Dong", ""]]}, {"id": "1806.00064", "submitter": "Zhun Liu", "authors": "Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshminarasimhan, Paul Pu\n  Liang, Amir Zadeh, Louis-Philippe Morency", "title": "Efficient Low-rank Multimodal Fusion with Modality-Specific Factors", "comments": "* Equal contribution. 10 pages. Accepted by ACL 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multimodal research is an emerging field of artificial intelligence, and one\nof the main research problems in this field is multimodal fusion. The fusion of\nmultimodal data is the process of integrating multiple unimodal representations\ninto one compact multimodal representation. Previous research in this field has\nexploited the expressiveness of tensors for multimodal representation. However,\nthese methods often suffer from exponential increase in dimensions and in\ncomputational complexity introduced by transformation of input into tensor. In\nthis paper, we propose the Low-rank Multimodal Fusion method, which performs\nmultimodal fusion using low-rank tensors to improve efficiency. We evaluate our\nmodel on three different tasks: multimodal sentiment analysis, speaker trait\nanalysis, and emotion recognition. Our model achieves competitive results on\nall these tasks while drastically reducing computational complexity. Additional\nexperiments also show that our model can perform robustly for a wide range of\nlow-rank settings, and is indeed much more efficient in both training and\ninference compared to other methods that utilize tensor representations.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 19:28:23 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Liu", "Zhun", ""], ["Shen", "Ying", ""], ["Lakshminarasimhan", "Varun Bharadhwaj", ""], ["Liang", "Paul Pu", ""], ["Zadeh", "Amir", ""], ["Morency", "Louis-Philippe", ""]]}, {"id": "1806.00069", "submitter": "Leilani Gilpin", "authors": "Leilani H. Gilpin, David Bau, Ben Z. Yuan, Ayesha Bajwa, Michael\n  Specter, Lalana Kagal", "title": "Explaining Explanations: An Overview of Interpretability of Machine\n  Learning", "comments": "The 5th IEEE International Conference on Data Science and Advanced\n  Analytics (DSAA 2018). [Research Track]", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has recently been a surge of work in explanatory artificial\nintelligence (XAI). This research area tackles the important problem that\ncomplex machines and algorithms often cannot provide insights into their\nbehavior and thought processes. XAI allows users and parts of the internal\nsystem to be more transparent, providing explanations of their decisions in\nsome level of detail. These explanations are important to ensure algorithmic\nfairness, identify potential bias/problems in the training data, and to ensure\nthat the algorithms perform as expected. However, explanations produced by\nthese systems is neither standardized nor systematically assessed. In an effort\nto create best practices and identify open challenges, we provide our\ndefinition of explainability and show how it can be used to classify existing\nliterature. We discuss why current approaches to explanatory methods especially\nfor deep neural networks are insufficient. Finally, based on our survey, we\nconclude with suggested future research directions for explanatory artificial\nintelligence.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 19:48:00 GMT"}, {"version": "v2", "created": "Mon, 4 Jun 2018 16:03:52 GMT"}, {"version": "v3", "created": "Sun, 3 Feb 2019 21:06:50 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Gilpin", "Leilani H.", ""], ["Bau", "David", ""], ["Yuan", "Ben Z.", ""], ["Bajwa", "Ayesha", ""], ["Specter", "Michael", ""], ["Kagal", "Lalana", ""]]}, {"id": "1806.00119", "submitter": "Christoph Redl", "authors": "Christoph Redl", "title": "Technical Report: Inconsistency in Answer Set Programs and Extensions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answer Set Programming (ASP) is a well-known problem solving approach based\non nonmonotonic logic programs. HEX-programs extend ASP with external atoms for\naccessing arbitrary external information, which can introduce values that do\nnot appear in the input program. In this work we consider inconsistent ASP- and\nHEX-programs, i.e., programs without answer sets. We study characterizations of\ninconsistency, introduce a novel notion for explaining inconsistencies in terms\nof input facts, analyze the complexity of reasoning tasks in context of\ninconsistency analysis, and present techniques for computing inconsistency\nreasons. This theoretical work is motivated by two concrete applications, which\nwe also present. The first one is the new modeling technique of query answering\nover subprograms as a convenient alternative to the well-known saturation\ntechnique. The second application is a new evaluation algorithm for\nHEX-programs based on conflict-driven learning for programs with multiple\ncomponents: while for certain program classes previous techniques suffer an\nevaluation bottleneck, the new approach shows significant, potentially\nexponential speedup in our experiments. Since well-known ASP extensions such as\nconstraint ASP and DL-programs correspond to special cases of HEX, all\npresented results are interesting beyond the specific formalism.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 22:11:21 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Redl", "Christoph", ""]]}, {"id": "1806.00143", "submitter": "Priyam Parashar", "authors": "Priyam Parashar, Akansel Cosgun, Alireza Nakhaei and Kikuo Fujimura", "title": "Modeling Preemptive Behaviors for Uncommon Hazardous Situations From\n  Demonstrations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a learning from demonstration approach to programming\nsafe, autonomous behaviors for uncommon driving scenarios. Simulation is used\nto re-create a targeted driving situation, one containing a road-side hazard\ncreating a significant occlusion in an urban neighborhood, and collect optimal\ndriving behaviors from 24 users. Paper employs a key-frame based approach\ncombined with an algorithm to linearly combine models in order to extend the\nbehavior to novel variations of the target situation. This approach is\ntheoretically agnostic to the kind of LfD framework used for modeling data and\nour results suggest it generalizes well to variations containing an additional\nnumber of hazards occurring in sequence. The linear combination algorithm is\ninformed by analysis of driving data, which also suggests that decision-making\nalgorithms need to consider a trade-off between road-rules and immediate\nrewards to tackle some complex cases.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 00:05:56 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Parashar", "Priyam", ""], ["Cosgun", "Akansel", ""], ["Nakhaei", "Alireza", ""], ["Fujimura", "Kikuo", ""]]}, {"id": "1806.00175", "submitter": "Ramtin Keramati", "authors": "Ramtin Keramati, Jay Whang, Patrick Cho, Emma Brunskill", "title": "Fast Exploration with Simplified Models and Approximately Optimistic\n  Planning in Model Based Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans learn to play video games significantly faster than the\nstate-of-the-art reinforcement learning (RL) algorithms. People seem to build\nsimple models that are easy to learn to support planning and strategic\nexploration. Inspired by this, we investigate two issues in leveraging\nmodel-based RL for sample efficiency. First we investigate how to perform\nstrategic exploration when exact planning is not feasible and empirically show\nthat optimistic Monte Carlo Tree Search outperforms posterior sampling methods.\nSecond we show how to learn simple deterministic models to support fast\nlearning using object representation. We illustrate the benefit of these ideas\nby introducing a novel algorithm, Strategic Object Oriented Reinforcement\nLearning (SOORL), that outperforms state-of-the-art algorithms in the game of\nPitfall! in less than 50 episodes.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 02:54:06 GMT"}, {"version": "v2", "created": "Mon, 26 Nov 2018 04:56:07 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Keramati", "Ramtin", ""], ["Whang", "Jay", ""], ["Cho", "Patrick", ""], ["Brunskill", "Emma", ""]]}, {"id": "1806.00201", "submitter": "Nicholas Guttenberg", "authors": "Nicholas Guttenberg, Martin Biehl, Nathaniel Virgo, Ryota Kanai", "title": "Being curious about the answers to questions: novelty search with\n  learned attention", "comments": "8 pages, 7 figures, ALife 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the use of attentional neural network layers in order to learn\na `behavior characterization' which can be used to drive novelty search and\ncuriosity-based policies. The space is structured towards answering a\nparticular distribution of questions, which are used in a supervised way to\ntrain the attentional neural network. We find that in a 2d exploration task,\nthe structure of the space successfully encodes local sensory-motor\ncontingencies such that even a greedy local `do the most novel action' policy\nwith no reinforcement learning or evolution can explore the space quickly. We\nalso apply this to a high/low number guessing game task, and find that guessing\naccording to the learned attention profile performs active inference and can\ndiscover the correct number more quickly than an exact but passive approach.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 05:32:47 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Guttenberg", "Nicholas", ""], ["Biehl", "Martin", ""], ["Virgo", "Nathaniel", ""], ["Kanai", "Ryota", ""]]}, {"id": "1806.00258", "submitter": "Chenhui Chu", "authors": "Chenhui Chu and Rui Wang", "title": "A Survey of Domain Adaptation for Neural Machine Translation", "comments": "COLING 2018, 16 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural machine translation (NMT) is a deep learning based approach for\nmachine translation, which yields the state-of-the-art translation performance\nin scenarios where large-scale parallel corpora are available. Although the\nhigh-quality and domain-specific translation is crucial in the real world,\ndomain-specific corpora are usually scarce or nonexistent, and thus vanilla NMT\nperforms poorly in such scenarios. Domain adaptation that leverages both\nout-of-domain parallel corpora as well as monolingual corpora for in-domain\ntranslation, is very important for domain-specific translation. In this paper,\nwe give a comprehensive survey of the state-of-the-art domain adaptation\ntechniques for NMT.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 09:54:32 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Chu", "Chenhui", ""], ["Wang", "Rui", ""]]}, {"id": "1806.00340", "submitter": "Luke Oakden-Rayner", "authors": "William Gale, Luke Oakden-Rayner, Gustavo Carneiro, Andrew P Bradley,\n  Lyle J Palmer", "title": "Producing radiologist-quality reports for interpretable artificial\n  intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current approaches to explaining the decisions of deep learning systems for\nmedical tasks have focused on visualising the elements that have contributed to\neach decision. We argue that such approaches are not enough to \"open the black\nbox\" of medical decision making systems because they are missing a key\ncomponent that has been used as a standard communication tool between doctors\nfor centuries: language. We propose a model-agnostic interpretability method\nthat involves training a simple recurrent neural network model to produce\ndescriptive sentences to clarify the decision of deep learning classifiers.\n  We test our method on the task of detecting hip fractures from frontal pelvic\nx-rays. This process requires minimal additional labelling despite producing\ntext containing elements that the original deep learning classification model\nwas not specifically trained to detect.\n  The experimental results show that: 1) the sentences produced by our method\nconsistently contain the desired information, 2) the generated sentences are\npreferred by doctors compared to current tools that create saliency maps, and\n3) the combination of visualisations and generated text is better than either\nalone.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 13:47:12 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Gale", "William", ""], ["Oakden-Rayner", "Luke", ""], ["Carneiro", "Gustavo", ""], ["Bradley", "Andrew P", ""], ["Palmer", "Lyle J", ""]]}, {"id": "1806.00352", "submitter": "Mieczys{\\l}aw K{\\l}opotek", "authors": "Mieczys{\\l}aw A. K{\\l}opotek", "title": "Too Fast Causal Inference under Causal Insufficiency", "comments": "40 pages. arXiv admin note: text overlap with arXiv:1705.10308", "journal-ref": null, "doi": null, "report-no": "ICS-PAS Reports 761/94", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causally insufficient structures (models with latent or hidden variables, or\nwith confounding etc.) of joint probability distributions have been subject of\nintense study not only in statistics, but also in various AI systems. In AI,\nbelief networks, being representations of joint probability distribution with\nan underlying directed acyclic graph structure, are paid special attention due\nto the fact that efficient reasoning (uncertainty propagation) methods have\nbeen developed for belief network structures. Algorithms have been therefore\ndeveloped to acquire the belief network structure from data. As artifacts due\nto variable hiding negatively influence the performance of derived belief\nnetworks, models with latent variables have been studied and several algorithms\nfor learning belief network structure under causal insufficiency have also been\ndeveloped.\n  Regrettably, some of them are known already to be erroneous (e.g. IC\nalgorithm of [Pearl:Verma:91]. This paper is devoted to another algorithm, the\nFast Causal Inference (FCI) Algorithm of [Spirtes:93]. It is proven by a\nspecially constructed example that this algorithm, as it stands in\n[Spirtes:93], is also erroneous. Fundamental reason for failure of this\nalgorithm is the temporary introduction of non-real links between nodes of the\nnetwork with the intention of later removal. While for trivial dependency\nstructures these non-real links may be actually removed, this may not be the\ncase for complex ones, e.g. for the case described in this paper. A remedy of\nthis failure is proposed.\n", "versions": [{"version": "v1", "created": "Wed, 30 May 2018 19:32:39 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["K\u0142opotek", "Mieczys\u0142aw A.", ""]]}, {"id": "1806.00354", "submitter": "Shane Steinert-Threlkeld", "authors": "Sandro Pezzelle, Shane Steinert-Threlkeld, Raffaela Bernardi, Jakub\n  Szymanik", "title": "Some of Them Can be Guessed! Exploring the Effect of Linguistic Context\n  in Predicting Quantifiers", "comments": "ACL 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the role of linguistic context in predicting quantifiers (`few',\n`all'). We collect crowdsourced data from human participants and test various\nmodels in a local (single-sentence) and a global context (multi-sentence)\ncondition. Models significantly out-perform humans in the former setting and\nare only slightly better in the latter. While human performance improves with\nmore linguistic context (especially on proportional quantifiers), model\nperformance suffers. Models are very effective in exploiting lexical and\nmorpho-syntactic patterns; humans are better at genuinely understanding the\nmeaning of the (global) context.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 14:02:39 GMT"}], "update_date": "2018-06-04", "authors_parsed": [["Pezzelle", "Sandro", ""], ["Steinert-Threlkeld", "Shane", ""], ["Bernardi", "Raffaela", ""], ["Szymanik", "Jakub", ""]]}, {"id": "1806.00358", "submitter": "Rajarshi Das", "authors": "Michael Boratko, Harshit Padigela, Divyendra Mikkilineni, Pritish\n  Yuvraj, Rajarshi Das, Andrew McCallum, Maria Chang, Achille Fokoue-Nkoutche,\n  Pavan Kapanipathi, Nicholas Mattei, Ryan Musa, Kartik Talamadupula, Michael\n  Witbrock", "title": "A Systematic Classification of Knowledge, Reasoning, and Context within\n  the ARC Dataset", "comments": "Presented at the Machine Reading for Question Answering (MRQA 2018)\n  Workshop at the 55th Annual Meeting of the Association for Computational\n  Linguistics (ACL 2018). 11 pages, 5 tables, 4 figures. Added missing\n  citations in the latest draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent work of Clark et al. introduces the AI2 Reasoning Challenge (ARC)\nand the associated ARC dataset that partitions open domain, complex science\nquestions into an Easy Set and a Challenge Set. That paper includes an analysis\nof 100 questions with respect to the types of knowledge and reasoning required\nto answer them; however, it does not include clear definitions of these types,\nnor does it offer information about the quality of the labels. We propose a\ncomprehensive set of definitions of knowledge and reasoning types necessary for\nanswering the questions in the ARC dataset. Using ten annotators and a\nsophisticated annotation interface, we analyze the distribution of labels\nacross the Challenge Set and statistics related to them. Additionally, we\ndemonstrate that although naive information retrieval methods return sentences\nthat are irrelevant to answering the query, sufficient supporting text is often\npresent in the (ARC) corpus. Evaluating with human-selected relevant sentences\nimproves the performance of a neural machine comprehension model by 42 points.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 14:06:45 GMT"}, {"version": "v2", "created": "Mon, 4 Feb 2019 20:59:32 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Boratko", "Michael", ""], ["Padigela", "Harshit", ""], ["Mikkilineni", "Divyendra", ""], ["Yuvraj", "Pritish", ""], ["Das", "Rajarshi", ""], ["McCallum", "Andrew", ""], ["Chang", "Maria", ""], ["Fokoue-Nkoutche", "Achille", ""], ["Kapanipathi", "Pavan", ""], ["Mattei", "Nicholas", ""], ["Musa", "Ryan", ""], ["Talamadupula", "Kartik", ""], ["Witbrock", "Michael", ""]]}, {"id": "1806.00499", "submitter": "Aditya Ramesh", "authors": "Aditya Ramesh and Yann LeCun", "title": "Backpropagation for Implicit Spectral Densities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Most successful machine intelligence systems rely on gradient-based learning,\nwhich is made possible by backpropagation. Some systems are designed to aid us\nin interpreting data when explicit goals cannot be provided. These unsupervised\nsystems are commonly trained by backpropagating through a likelihood function.\nWe introduce a tool that allows us to do this even when the likelihood is not\nexplicitly set, by instead using the implicit likelihood of the model.\nExplicitly defining the likelihood often entails making heavy-handed\nassumptions that impede our ability to solve challenging tasks. On the other\nhand, the implicit likelihood of the model is accessible without the need for\nsuch assumptions. Our tool, which we call spectral backpropagation, allows us\nto optimize it in much greater generality than what has been attempted before.\nGANs can also be viewed as a technique for optimizing implicit likelihoods. We\nstudy them using spectral backpropagation in order to demonstrate robustness\nfor high-dimensional problems, and identify two novel properties of the\ngenerator G: (1) there exist aberrant, nonsensical outputs to which G assigns\nvery high likelihood, and (2) the eigenvectors of the metric induced by G over\nlatent space correspond to quasi-disentangled explanatory factors.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 18:28:20 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Ramesh", "Aditya", ""], ["LeCun", "Yann", ""]]}, {"id": "1806.00540", "submitter": "Kenneth Young", "authors": "Kenny J. Young, Richard S. Sutton, Shuo Yang", "title": "Integrating Episodic Memory into a Reinforcement Learning Agent using\n  Reservoir Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Episodic memory is a psychology term which refers to the ability to recall\nspecific events from the past. We suggest one advantage of this particular type\nof memory is the ability to easily assign credit to a specific state when\nremembered information is found to be useful. Inspired by this idea, and the\nincreasing popularity of external memory mechanisms to handle long-term\ndependencies in deep learning systems, we propose a novel algorithm which uses\na reservoir sampling procedure to maintain an external memory consisting of a\nfixed number of past states. The algorithm allows a deep reinforcement learning\nagent to learn online to preferentially remember those states which are found\nto be useful to recall later on. Critically this method allows for efficient\nonline computation of gradient estimates with respect to the write process of\nthe external memory. Thus unlike most prior mechanisms for external memory it\nis feasible to use in an online reinforcement learning setting.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 20:52:31 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Young", "Kenny J.", ""], ["Sutton", "Richard S.", ""], ["Yang", "Shuo", ""]]}, {"id": "1806.00553", "submitter": "Christopher Stanton", "authors": "Christopher Stanton and Jeff Clune", "title": "Deep Curiosity Search: Intra-Life Exploration Can Improve Performance on\n  Challenging Deep Reinforcement Learning Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional exploration methods in RL require agents to perform random\nactions to find rewards. But these approaches struggle on sparse-reward domains\nlike Montezuma's Revenge where the probability that any random action sequence\nleads to reward is extremely low. Recent algorithms have performed well on such\ntasks by encouraging agents to visit new states or perform new actions in\nrelation to all prior training episodes (which we call across-training\nnovelty). But such algorithms do not consider whether an agent exhibits\nintra-life novelty: doing something new within the current episode, regardless\nof whether those behaviors have been performed in previous episodes. We\nhypothesize that across-training novelty might discourage agents from\nrevisiting initially non-rewarding states that could become important stepping\nstones later in training. We introduce Deep Curiosity Search (DeepCS), which\nencourages intra-life exploration by rewarding agents for visiting as many\ndifferent states as possible within each episode, and show that DeepCS matches\nthe performance of current state-of-the-art methods on Montezuma's Revenge. We\nfurther show that DeepCS improves exploration on Amidar, Freeway, Gravitar, and\nTutankham (many of which are hard exploration games). Surprisingly, DeepCS\ndoubles A2C performance on Seaquest, a game we would not have expected to\nbenefit from intra-life exploration because the arena is small and already\neasily navigated by naive exploration techniques. In one run, DeepCS achieves a\nmaximum training score of 80,000 points on Seaquest, higher than any methods\nother than Ape-X. The strong performance of DeepCS on these sparse- and\ndense-reward tasks suggests that encouraging intra-life novelty is an\ninteresting, new approach for improving performance in Deep RL and motivates\nfurther research into hybridizing across-training and intra-life exploration\nmethods.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 22:09:51 GMT"}, {"version": "v2", "created": "Mon, 22 Oct 2018 17:23:01 GMT"}, {"version": "v3", "created": "Sat, 24 Nov 2018 00:29:31 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Stanton", "Christopher", ""], ["Clune", "Jeff", ""]]}, {"id": "1806.00588", "submitter": "Xing Shi", "authors": "Xing Shi, Shizhen Xu, Kevin Knight", "title": "Fast Locality Sensitive Hashing for Beam Search on GPU", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DC cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a GPU-based Locality Sensitive Hashing (LSH) algorithm to speed up\nbeam search for sequence models. We utilize the winner-take-all (WTA) hash,\nwhich is based on relative ranking order of hidden dimensions and thus\nresilient to perturbations in numerical values. Our algorithm is designed by\nfully considering the underling architecture of CUDA-enabled GPUs\n(Algorithm/Architecture Co-design): 1) A parallel Cuckoo hash table is applied\nfor LSH code lookup (guaranteed O(1) lookup time); 2) Candidate lists are\nshared across beams to maximize the parallelism; 3) Top frequent words are\nmerged into candidate lists to improve performance. Experiments on 4\nlarge-scale neural machine translation models demonstrate that our algorithm\ncan achieve up to 4x speedup on softmax module, and 2x overall speedup without\nhurting BLEU on GPU.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2018 06:18:15 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Shi", "Xing", ""], ["Xu", "Shizhen", ""], ["Knight", "Kevin", ""]]}, {"id": "1806.00589", "submitter": "Yiming Zhang", "authors": "Yiming Zhang, Quan Ho Vuong, Kenny Song, Xiao-Yue Gong, Keith W. Ross", "title": "Efficient Entropy for Policy Gradient with Multidimensional Action Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep reinforcement learning has been shown to be adept at\nsolving sequential decision processes with high-dimensional state spaces such\nas in the Atari games. Many reinforcement learning problems, however, involve\nhigh-dimensional discrete action spaces as well as high-dimensional state\nspaces. This paper considers entropy bonus, which is used to encourage\nexploration in policy gradient. In the case of high-dimensional action spaces,\ncalculating the entropy and its gradient requires enumerating all the actions\nin the action space and running forward and backpropagation for each action,\nwhich may be computationally infeasible. We develop several novel unbiased\nestimators for the entropy bonus and its gradient. We apply these estimators to\nseveral models for the parameterized policies, including Independent Sampling,\nCommNet, Autoregressive with Modified MDP, and Autoregressive with LSTM.\nFinally, we test our algorithms on two environments: a multi-hunter\nmulti-rabbit grid game and a multi-agent multi-arm bandit problem. The results\nshow that our entropy estimators substantially improve performance with\nmarginal additional computational cost.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2018 06:25:19 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Zhang", "Yiming", ""], ["Vuong", "Quan Ho", ""], ["Song", "Kenny", ""], ["Gong", "Xiao-Yue", ""], ["Ross", "Keith W.", ""]]}, {"id": "1806.00608", "submitter": "Daniel Huang", "authors": "Daniel Huang, Prafulla Dhariwal, Dawn Song, Ilya Sutskever", "title": "GamePad: A Learning Environment for Theorem Proving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.LO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a system called GamePad that can be used to\nexplore the application of machine learning methods to theorem proving in the\nCoq proof assistant. Interactive theorem provers such as Coq enable users to\nconstruct machine-checkable proofs in a step-by-step manner. Hence, they\nprovide an opportunity to explore theorem proving with human supervision. We\nuse GamePad to synthesize proofs for a simple algebraic rewrite problem and\ntrain baseline models for a formalization of the Feit-Thompson theorem. We\naddress position evaluation (i.e., predict the number of proof steps left) and\ntactic prediction (i.e., predict the next proof step) tasks, which arise\nnaturally in tactic-based theorem proving.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2018 09:19:08 GMT"}, {"version": "v2", "created": "Fri, 21 Dec 2018 18:37:30 GMT"}], "update_date": "2018-12-24", "authors_parsed": [["Huang", "Daniel", ""], ["Dhariwal", "Prafulla", ""], ["Song", "Dawn", ""], ["Sutskever", "Ilya", ""]]}, {"id": "1806.00610", "submitter": "Fernando Mart\\'inez Plumed", "authors": "Fernando Mart\\'inez-Plumed, Shahar Avin, Miles Brundage, Allan Dafoe,\n  Sean \\'O h\\'Eigeartaigh, Jos\\'e Hern\\'andez-Orallo", "title": "Accounting for the Neglected Dimensions of AI Progress", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze and reframe AI progress. In addition to the prevailing metrics of\nperformance, we highlight the usually neglected costs paid in the development\nand deployment of a system, including: data, expert knowledge, human oversight,\nsoftware resources, computing cycles, hardware and network facilities,\ndevelopment time, etc. These costs are paid throughout the life cycle of an AI\nsystem, fall differentially on different individuals, and vary in magnitude\ndepending on the replicability and generality of the AI solution. The\nmultidimensional performance and cost space can be collapsed to a single\nutility metric for a user with transitive and complete preferences. Even absent\na single utility function, AI advances can be generically assessed by whether\nthey expand the Pareto (optimal) surface. We explore a subset of these\nneglected dimensions using the two case studies of Alpha* and ALE. This\nbroadened conception of progress in AI should lead to novel ways of measuring\nsuccess in AI, and can help set milestones for future progress.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2018 09:21:12 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Mart\u00ednez-Plumed", "Fernando", ""], ["Avin", "Shahar", ""], ["Brundage", "Miles", ""], ["Dafoe", "Allan", ""], ["h\u00c9igeartaigh", "Sean \u00d3", ""], ["Hern\u00e1ndez-Orallo", "Jos\u00e9", ""]]}, {"id": "1806.00683", "submitter": "Vijaya Sai Krishna Gottipati", "authors": "Sai Krishna G.V., Kyle Goyette, Ahmad Chamseddine, Breandan Considine", "title": "Deep Pepper: Expert Iteration based Chess agent in the Reinforcement\n  Learning Setting", "comments": "Tabula Rasa, Chess engine, Learning Fast and Slow, Reinforcement\n  Learning, Alpha Zero", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An almost-perfect chess playing agent has been a long standing challenge in\nthe field of Artificial Intelligence. Some of the recent advances demonstrate\nwe are approaching that goal. In this project, we provide methods for faster\ntraining of self-play style algorithms, mathematical details of the algorithm\nused, various potential future directions, and discuss most of the relevant\nwork in the area of computer chess. Deep Pepper uses embedded knowledge to\naccelerate the training of the chess engine over a \"tabula rasa\" system such as\nAlpha Zero. We also release our code to promote further research.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2018 18:35:37 GMT"}, {"version": "v2", "created": "Wed, 17 Oct 2018 21:01:36 GMT"}], "update_date": "2018-10-19", "authors_parsed": [["V.", "Sai Krishna G.", ""], ["Goyette", "Kyle", ""], ["Chamseddine", "Ahmad", ""], ["Considine", "Breandan", ""]]}, {"id": "1806.00712", "submitter": "Shiwen Shen", "authors": "Shiwen Shen, Simon X. Han, Denise R. Aberle, Alex A.T. Bui, Willliam\n  Hsu", "title": "An Interpretable Deep Hierarchical Semantic Convolutional Neural Network\n  for Lung Nodule Malignancy Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep learning methods are increasingly being applied to tasks such as\ncomputer-aided diagnosis, these models are difficult to interpret, do not\nincorporate prior domain knowledge, and are often considered as a \"black-box.\"\nThe lack of model interpretability hinders them from being fully understood by\ntarget users such as radiologists. In this paper, we present a novel\ninterpretable deep hierarchical semantic convolutional neural network (HSCNN)\nto predict whether a given pulmonary nodule observed on a computed tomography\n(CT) scan is malignant. Our network provides two levels of output: 1) low-level\nradiologist semantic features, and 2) a high-level malignancy prediction score.\nThe low-level semantic outputs quantify the diagnostic features used by\nradiologists and serve to explain how the model interprets the images in an\nexpert-driven manner. The information from these low-level tasks, along with\nthe representations learned by the convolutional layers, are then combined and\nused to infer the high-level task of predicting nodule malignancy. This unified\narchitecture is trained by optimizing a global loss function including both\nlow- and high-level tasks, thereby learning all the parameters within a joint\nframework. Our experimental results using the Lung Image Database Consortium\n(LIDC) show that the proposed method not only produces interpretable lung\ncancer predictions but also achieves significantly better results compared to\ncommon 3D CNN approaches.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2018 22:41:28 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Shen", "Shiwen", ""], ["Han", "Simon X.", ""], ["Aberle", "Denise R.", ""], ["Bui", "Alex A. T.", ""], ["Hsu", "Willliam", ""]]}, {"id": "1806.00727", "submitter": "Nisar Ahmed", "authors": "Luke Burks, Ian Loefgren, Luke Barbier, Jeremy Muesing, Jamison\n  McGinley, Sousheel Vunnam, and Nisar Ahmed", "title": "Closed-loop Bayesian Semantic Data Fusion for Collaborative\n  Human-Autonomy Target Search", "comments": "Final version accepted and submitted to 2018 FUSION Conference\n  (Cambridge, UK, July 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In search applications, autonomous unmanned vehicles must be able to\nefficiently reacquire and localize mobile targets that can remain out of view\nfor long periods of time in large spaces. As such, all available information\nsources must be actively leveraged -- including imprecise but readily available\nsemantic observations provided by humans. To achieve this, this work develops\nand validates a novel collaborative human-machine sensing solution for dynamic\ntarget search. Our approach uses continuous partially observable Markov\ndecision process (CPOMDP) planning to generate vehicle trajectories that\noptimally exploit imperfect detection data from onboard sensors, as well as\nsemantic natural language observations that can be specifically requested from\nhuman sensors. The key innovation is a scalable hierarchical Gaussian mixture\nmodel formulation for efficiently solving CPOMDPs with semantic observations in\ncontinuous dynamic state spaces. The approach is demonstrated and validated\nwith a real human-robot team engaged in dynamic indoor target search and\ncapture scenarios on a custom testbed.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 02:03:11 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Burks", "Luke", ""], ["Loefgren", "Ian", ""], ["Barbier", "Luke", ""], ["Muesing", "Jeremy", ""], ["McGinley", "Jamison", ""], ["Vunnam", "Sousheel", ""], ["Ahmed", "Nisar", ""]]}, {"id": "1806.00738", "submitter": "Diana Gonz\\'alez-Rico", "authors": "Diana Gonzalez-Rico, Gibran Fuentes-Pineda", "title": "Contextualize, Show and Tell: A Neural Visual Storyteller", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a neural model for generating short stories from image sequences,\nwhich extends the image description model by Vinyals et al. (Vinyals et al.,\n2015). This extension relies on an encoder LSTM to compute a context vector of\neach story from the image sequence. This context vector is used as the first\nstate of multiple independent decoder LSTMs, each of which generates the\nportion of the story corresponding to each image in the sequence by taking the\nimage embedding as the first input. Our model showed competitive results with\nthe METEOR metric and human ratings in the internal track of the Visual\nStorytelling Challenge 2018.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 05:09:54 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Gonzalez-Rico", "Diana", ""], ["Fuentes-Pineda", "Gibran", ""]]}, {"id": "1806.00754", "submitter": "Hwiyeol Jo", "authors": "Hwiyeol Jo and Jeong Ryu", "title": "Psychological State in Text: A Limitation of Sentiment Analysis", "comments": "In Proceedings of IJCAI-ECAI Workshop on AI and Computational\n  Psychology: Theories, Algorithms and Applications (CompPsy)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Starting with the idea that sentiment analysis models should be able to\npredict not only positive or negative but also other psychological states of a\nperson, we implement a sentiment analysis model to investigate the relationship\nbetween the model and emotional state. We first examine psychological\nmeasurements of 64 participants and ask them to write a book report about a\nstory. After that, we train our sentiment analysis model using crawled movie\nreview data. We finally evaluate participants' writings, using the pretrained\nmodel as a concept of transfer learning. The result shows that sentiment\nanalysis model performs good at predicting a score, but the score does not have\nany correlation with human's self-checked sentiment.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 08:52:23 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Jo", "Hwiyeol", ""], ["Ryu", "Jeong", ""]]}, {"id": "1806.00770", "submitter": "Aleksandar Bojchevski", "authors": "Federico Monti, Oleksandr Shchur, Aleksandar Bojchevski, Or Litany,\n  Stephan G\\\"unnemann, Michael M. Bronstein", "title": "Dual-Primal Graph Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there has been a surge of interest in developing deep\nlearning methods for non-Euclidean structured data such as graphs. In this\npaper, we propose Dual-Primal Graph CNN, a graph convolutional architecture\nthat alternates convolution-like operations on the graph and its dual. Our\napproach allows to learn both vertex- and edge features and generalizes the\nprevious graph attention (GAT) model. We provide extensive experimental\nvalidation showing state-of-the-art results on a variety of tasks tested on\nestablished graph benchmarks, including CORA and Citeseer citation networks as\nwell as MovieLens, Flixter, Douban and Yahoo Music graph-guided recommender\nsystems.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 11:15:15 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Monti", "Federico", ""], ["Shchur", "Oleksandr", ""], ["Bojchevski", "Aleksandar", ""], ["Litany", "Or", ""], ["G\u00fcnnemann", "Stephan", ""], ["Bronstein", "Michael M.", ""]]}, {"id": "1806.00778", "submitter": "Yi Tay", "authors": "Yi Tay, Luu Anh Tuan, Siu Cheung Hui", "title": "Multi-Cast Attention Networks for Retrieval-based Question Answering and\n  Response Prediction", "comments": "Accepted to KDD 2018 (Paper titled only \"Multi-Cast Attention\n  Networks\" in KDD version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attention is typically used to select informative sub-phrases that are used\nfor prediction. This paper investigates the novel use of attention as a form of\nfeature augmentation, i.e, casted attention. We propose Multi-Cast Attention\nNetworks (MCAN), a new attention mechanism and general model architecture for a\npotpourri of ranking tasks in the conversational modeling and question\nanswering domains. Our approach performs a series of soft attention operations,\neach time casting a scalar feature upon the inner word embeddings. The key idea\nis to provide a real-valued hint (feature) to a subsequent encoder layer and is\ntargeted at improving the representation learning process. There are several\nadvantages to this design, e.g., it allows an arbitrary number of attention\nmechanisms to be casted, allowing for multiple attention types (e.g.,\nco-attention, intra-attention) and attention variants (e.g., alignment-pooling,\nmax-pooling, mean-pooling) to be executed simultaneously. This not only\neliminates the costly need to tune the nature of the co-attention layer, but\nalso provides greater extents of explainability to practitioners. Via extensive\nexperiments on four well-known benchmark datasets, we show that MCAN achieves\nstate-of-the-art performance. On the Ubuntu Dialogue Corpus, MCAN outperforms\nexisting state-of-the-art models by $9\\%$. MCAN also achieves the best\nperforming score to date on the well-studied TrecQA dataset.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 12:22:28 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Tay", "Yi", ""], ["Tuan", "Luu Anh", ""], ["Hui", "Siu Cheung", ""]]}, {"id": "1806.00797", "submitter": "Juan-Pablo Ortega", "authors": "Lyudmila Grigoryeva and Juan-Pablo Ortega", "title": "Echo state networks are universal", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows that echo state networks are universal uniform approximants\nin the context of discrete-time fading memory filters with uniformly bounded\ninputs defined on negative infinite times. This result guarantees that any\nfading memory input/output system in discrete time can be realized as a simple\nfinite-dimensional neural network-type state-space model with a static linear\nreadout map. This approximation is valid for infinite time intervals. The proof\nof this statement is based on fundamental results, also presented in this work,\nabout the topological nature of the fading memory property and about reservoir\ncomputing systems generated by continuous reservoir maps.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 13:38:41 GMT"}, {"version": "v2", "created": "Sun, 26 Aug 2018 11:17:29 GMT"}], "update_date": "2018-08-28", "authors_parsed": [["Grigoryeva", "Lyudmila", ""], ["Ortega", "Juan-Pablo", ""]]}, {"id": "1806.00805", "submitter": "William Vega-Brown", "authors": "William Vega-Brown and Nicholas Roy", "title": "Admissible Abstractions for Near-optimal Task and Motion Planning", "comments": "This document is an extended version of a paper to appear in the\n  \"27th International Joint Conference on Artificial Intelligence\" in July 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define an admissibility condition for abstractions expressed using angelic\nsemantics and show that these conditions allow us to accelerate planning while\npreserving the ability to find the optimal motion plan. We then derive\nadmissible abstractions for two motion planning domains with continuous state.\nWe extract upper and lower bounds on the cost of concrete motion plans using\nlocal metric and topological properties of the problem domain. These bounds\nguide the search for a plan while maintaining performance guarantees. We show\nthat abstraction can dramatically reduce the complexity of search relative to a\ndirect motion planner. Using our abstractions, we find near-optimal motion\nplans in planning problems involving $10^{13}$ states without using a separate\ntask planner.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 14:56:42 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Vega-Brown", "William", ""], ["Roy", "Nicholas", ""]]}, {"id": "1806.00806", "submitter": "Jong Chul Ye", "authors": "Eunju Cha, Eung Yeop Kim, and Jong Chul Ye", "title": "k-Space Deep Learning for Parallel MRI: Application to Time-Resolved MR\n  Angiography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time-resolved angiography with interleaved stochastic trajectories (TWIST)\nhas been widely used for dynamic contrast enhanced MRI (DCE-MRI). To achieve\nhighly accelerated acquisitions, TWIST combines the periphery of the k-space\ndata from several adjacent frames to reconstruct one temporal frame. However,\nthis view-sharing scheme limits the true temporal resolution of TWIST.\nMoreover, the k-space sampling patterns have been specially designed for a\nspecific generalized autocalibrating partial parallel acquisition (GRAPPA)\nfactor so that it is not possible to reduce the number of view-sharing once the\nk-data is acquired. To address these issues, this paper proposes a novel\nk-space deep learning approach for parallel MRI. In particular, we have\ndesigned our neural network so that accurate k-space interpolations are\nperformed simultaneously for multiple coils by exploiting the redundancies\nalong the coils and images. Reconstruction results using in vivo TWIST data set\nconfirm that the proposed method can immediately generate high-quality\nreconstruction results with various choices of view- sharing, allowing us to\nexploit the trade-off between spatial and temporal resolution in time-resolved\nMR angiography.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 14:56:46 GMT"}, {"version": "v2", "created": "Sun, 10 Jun 2018 06:51:12 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Cha", "Eunju", ""], ["Kim", "Eung Yeop", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1806.00807", "submitter": "Badri Narayana Patro", "authors": "Badri N. Patro, Vinod K. Kurmi, Sandeep Kumar, Vinay P. Namboodiri", "title": "Learning Semantic Sentence Embeddings using Sequential Pair-wise\n  Discriminator", "comments": "COLING 2018 (accepted)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we propose a method for obtaining sentence-level embeddings.\nWhile the problem of securing word-level embeddings is very well studied, we\npropose a novel method for obtaining sentence-level embeddings. This is\nobtained by a simple method in the context of solving the paraphrase generation\ntask. If we use a sequential encoder-decoder model for generating paraphrase,\nwe would like the generated paraphrase to be semantically close to the original\nsentence. One way to ensure this is by adding constraints for true paraphrase\nembeddings to be close and unrelated paraphrase candidate sentence embeddings\nto be far. This is ensured by using a sequential pair-wise discriminator that\nshares weights with the encoder that is trained with a suitable loss function.\nOur loss function penalizes paraphrase sentence embedding distances from being\ntoo large. This loss is used in combination with a sequential encoder-decoder\nnetwork. We also validated our method by evaluating the obtained embeddings for\na sentiment analysis task. The proposed method results in semantic embeddings\nand outperforms the state-of-the-art on the paraphrase generation and sentiment\nanalysis task on standard datasets. These results are also shown to be\nstatistically significant.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 15:00:05 GMT"}, {"version": "v2", "created": "Mon, 11 Jun 2018 14:07:37 GMT"}, {"version": "v3", "created": "Fri, 15 Jun 2018 12:26:48 GMT"}, {"version": "v4", "created": "Mon, 2 Jul 2018 05:26:02 GMT"}, {"version": "v5", "created": "Thu, 14 Mar 2019 19:14:10 GMT"}], "update_date": "2019-03-18", "authors_parsed": [["Patro", "Badri N.", ""], ["Kurmi", "Vinod K.", ""], ["Kumar", "Sandeep", ""], ["Namboodiri", "Vinay P.", ""]]}, {"id": "1806.00852", "submitter": "Xiang Jiang", "authors": "Xiang Jiang, Mohammad Havaei, Gabriel Chartrand, Hassan Chouaib,\n  Thomas Vincent, Andrew Jesson, Nicolas Chapados, Stan Matwin", "title": "On the Importance of Attention in Meta-Learning for Few-Shot Text\n  Classification", "comments": "13 pages, 4 figures, submitted to NIPS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current deep learning based text classification methods are limited by their\nability to achieve fast learning and generalization when the data is scarce. We\naddress this problem by integrating a meta-learning procedure that uses the\nknowledge learned across many tasks as an inductive bias towards better natural\nlanguage understanding. Based on the Model-Agnostic Meta-Learning framework\n(MAML), we introduce the Attentive Task-Agnostic Meta-Learning (ATAML)\nalgorithm for text classification. The essential difference between MAML and\nATAML is in the separation of task-agnostic representation learning and\ntask-specific attentive adaptation. The proposed ATAML is designed to encourage\ntask-agnostic representation learning by way of task-agnostic parameterization\nand facilitate task-specific adaptation via attention mechanisms. We provide\nevidence to show that the attention mechanism in ATAML has a synergistic effect\non learning performance. In comparisons with models trained from random\ninitialization, pretrained models and meta trained MAML, our proposed ATAML\nmethod generalizes better on single-label and multi-label classification tasks\nin miniRCV1 and miniReuters-21578 datasets.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 19:16:50 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Jiang", "Xiang", ""], ["Havaei", "Mohammad", ""], ["Chartrand", "Gabriel", ""], ["Chouaib", "Hassan", ""], ["Vincent", "Thomas", ""], ["Jesson", "Andrew", ""], ["Chapados", "Nicolas", ""], ["Matwin", "Stan", ""]]}, {"id": "1806.00882", "submitter": "Mohammad-Ali Javidian", "authors": "Mohammad Ali Javidian and Marco Valtorta", "title": "Structural Learning of Multivariate Regression Chain Graphs via\n  Decomposition", "comments": "19 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the decomposition approach for learning Bayesian networks (BNs)\nproposed by (Xie et. al.) to learning multivariate regression chain graphs (MVR\nCGs), which include BNs as a special case. The same advantages of this\ndecomposition approach hold in the more general setting: reduced complexity and\nincreased power of computational independence tests. Moreover, latent (hidden)\nvariables can be represented in MVR CGs by using bidirected edges, and our\nalgorithm correctly recovers any independence structure that is faithful to an\nMVR CG, thus greatly extending the range of applications of decomposition-based\nmodel selection techniques. Simulations under a variety of settings demonstrate\nthe competitive performance of our method in comparison with the PC-like\nalgorithm (Sonntag and Pena). In fact, the decomposition-based algorithm\nusually outperforms the PC-like algorithm except in running time. The\nperformance of both algorithms is much better when the underlying graph is\nsparse.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jun 2018 21:26:36 GMT"}, {"version": "v2", "created": "Mon, 24 Feb 2020 19:08:21 GMT"}], "update_date": "2020-02-26", "authors_parsed": [["Javidian", "Mohammad Ali", ""], ["Valtorta", "Marco", ""]]}, {"id": "1806.00931", "submitter": "Tariq Daouda", "authors": "Tariq Daouda (1 and 2 and 3), Jeremie Zumer (1 and 4 and 3), Claude\n  Perreault (1 and 5 and 3) and S\\'ebastien Lemieux (1 and 4 and 3) ((1)\n  Institute for Research in Immunology and Cancer, (2) Department of\n  biochemistry, (3) Universit\\'e de Montr\\'eal, (4) Department of Computer\n  Science and Operations Research, (5) Department of Medicine)", "title": "Holographic Neural Architectures", "comments": "10 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG q-bio.GN q-bio.TO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation learning is at the heart of what makes deep learning\neffective. In this work, we introduce a new framework for representation\nlearning that we call \"Holographic Neural Architectures\" (HNAs). In the same\nway that an observer can experience the 3D structure of a holographed object by\nlooking at its hologram from several angles, HNAs derive Holographic\nRepresentations from the training set. These representations can then be\nexplored by moving along a continuous bounded single dimension. We show that\nHNAs can be used to make generative networks, state-of-the-art regression\nmodels and that they are inherently highly resistant to noise. Finally, we\nargue that because of their denoising abilities and their capacity to\ngeneralize well from very few examples, models based upon HNAs are particularly\nwell suited for biological applications where training examples are rare or\nnoisy.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 02:41:20 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Daouda", "Tariq", "", "1 and 2 and 3"], ["Zumer", "Jeremie", "", "1 and 4 and 3"], ["Perreault", "Claude", "", "1 and 5 and 3"], ["Lemieux", "S\u00e9bastien", "", "1 and 4 and 3"]]}, {"id": "1806.00938", "submitter": "Ara Vartanian", "authors": "Evan Hernandez, Ara Vartanian, and Xiaojin Zhu", "title": "Program Synthesis from Visual Specification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Program synthesis is the process of automatically translating a specification\ninto computer code. Traditional synthesis settings require a formal, precise\nspecification. Motivated by computer education applications where a student\nlearns to code simple turtle-style drawing programs, we study a novel synthesis\nsetting where only a noisy user-intention drawing is specified. This allows\nstudents to sketch their intended output, optionally together with their own\nincomplete program, to automatically produce a completed program. We formulate\nthis synthesis problem as search in the space of programs, with the score of a\nstate being the Hausdorff distance between the program output and the user\ndrawing. We compare several search algorithms on a corpus consisting of real\nuser drawings and the corresponding programs, and demonstrate that our\nalgorithms can synthesize programs optimally satisfying the specification.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 03:24:34 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Hernandez", "Evan", ""], ["Vartanian", "Ara", ""], ["Zhu", "Xiaojin", ""]]}, {"id": "1806.00949", "submitter": "Shay Moran", "authors": "Noga Alon and Roi Livni and Maryanthe Malliaris and Shay Moran", "title": "Private PAC learning implies finite Littlestone dimension", "comments": "STOC camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR math.LO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that every approximately differentially private learning algorithm\n(possibly improper) for a class $H$ with Littlestone dimension~$d$ requires\n$\\Omega\\bigl(\\log^*(d)\\bigr)$ examples. As a corollary it follows that the\nclass of thresholds over $\\mathbb{N}$ can not be learned in a private manner;\nthis resolves open question due to [Bun et al., 2015, Feldman and Xiao, 2015].\nWe leave as an open question whether every class with a finite Littlestone\ndimension can be learned by an approximately differentially private algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 04:35:29 GMT"}, {"version": "v2", "created": "Wed, 13 Feb 2019 04:53:31 GMT"}, {"version": "v3", "created": "Fri, 8 Mar 2019 16:12:06 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Alon", "Noga", ""], ["Livni", "Roi", ""], ["Malliaris", "Maryanthe", ""], ["Moran", "Shay", ""]]}, {"id": "1806.00952", "submitter": "Navid Azizan Ruhi", "authors": "Navid Azizan and Babak Hassibi", "title": "Stochastic Gradient/Mirror Descent: Minimax Optimality and Implicit\n  Regularization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic descent methods (of the gradient and mirror varieties) have become\nincreasingly popular in optimization. In fact, it is now widely recognized that\nthe success of deep learning is not only due to the special deep architecture\nof the models, but also due to the behavior of the stochastic descent methods\nused, which play a key role in reaching \"good\" solutions that generalize well\nto unseen data. In an attempt to shed some light on why this is the case, we\nrevisit some minimax properties of stochastic gradient descent (SGD) for the\nsquare loss of linear models---originally developed in the 1990's---and extend\nthem to general stochastic mirror descent (SMD) algorithms for general loss\nfunctions and nonlinear models. In particular, we show that there is a\nfundamental identity which holds for SMD (and SGD) under very general\nconditions, and which implies the minimax optimality of SMD (and SGD) for\nsufficiently small step size, and for a general class of loss functions and\ngeneral nonlinear models. We further show that this identity can be used to\nnaturally establish other properties of SMD (and SGD), namely convergence and\nimplicit regularization for over-parameterized linear models (in what is now\nbeing called the \"interpolating regime\"), some of which have been shown in\ncertain cases in prior literature. We also argue how this identity can be used\nin the so-called \"highly over-parameterized\" nonlinear setting (where the\nnumber of parameters far exceeds the number of data points) to provide insights\ninto why SMD (and SGD) may have similar convergence and implicit regularization\nproperties for deep learning.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 04:53:00 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 04:59:28 GMT"}, {"version": "v3", "created": "Sun, 30 Sep 2018 08:05:37 GMT"}, {"version": "v4", "created": "Thu, 17 Jan 2019 21:53:08 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Azizan", "Navid", ""], ["Hassibi", "Babak", ""]]}, {"id": "1806.00960", "submitter": "Barton Lee", "authors": "Haris Aziz, Hau Chan, Barton E. Lee, and David C. Parkes", "title": "The Capacity Constrained Facility Location problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We initiate the study of the capacity constrained facility location problem\nfrom a mechanism design perspective. The capacity constrained setting leads to\na new strategic environment where a facility serves a subset of the population,\nwhich is endogenously determined by the ex-post Nash equilibrium of an induced\nsubgame and is not directly controlled by the mechanism designer. Our focus is\non mechanisms that are ex-post dominant-strategy incentive compatible (DIC) at\nthe reporting stage. We provide a complete characterization of DIC mechanisms\nvia the family of Generalized Median Mechanisms (GMMs). In general, the social\nwelfare optimal mechanism is not DIC. Adopting the worst-case approximation\nmeasure, we attain tight lower bounds on the approximation ratio of any DIC\nmechanism. The well-known median mechanism is shown to be optimal among the\nfamily of DIC mechanisms for certain capacity ranges. Surprisingly, the\nframework we introduce provides a new characterization for the family of GMMs,\nand is responsive to gaps in the current social choice literature highlighted\nby Border and Jordan (1983) and Barbar{\\`a}, Mass{\\'o} and Serizawa (1998).\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 05:28:33 GMT"}, {"version": "v2", "created": "Fri, 15 Feb 2019 19:51:03 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Aziz", "Haris", ""], ["Chan", "Hau", ""], ["Lee", "Barton E.", ""], ["Parkes", "David C.", ""]]}, {"id": "1806.00979", "submitter": "Patricio Cerda", "authors": "Patricio Cerda (PARIETAL), Ga\\\"el Varoquaux (PARIETAL), Bal\\'azs\n  K\\'egl (LAL, CNRS)", "title": "Similarity encoding for learning with dirty categorical variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For statistical learning, categorical variables in a table are usually\nconsidered as discrete entities and encoded separately to feature vectors,\ne.g., with one-hot encoding. \"Dirty\" non-curated data gives rise to categorical\nvariables with a very high cardinality but redundancy: several categories\nreflect the same entity. In databases, this issue is typically solved with a\ndeduplication step. We show that a simple approach that exposes the redundancy\nto the learning algorithm brings significant gains. We study a generalization\nof one-hot encoding, similarity encoding, that builds feature vectors from\nsimilarities across categories. We perform a thorough empirical validation on\nnon-curated tables, a problem seldom studied in machine learning. Results on\nseven real-world datasets show that similarity encoding brings significant\ngains in prediction in comparison with known encoding methods for categories or\nstrings, notably one-hot encoding and bag of character n-grams. We draw\npractical recommendations for encoding dirty categories: 3-gram similarity\nappears to be a good choice to capture morphological resemblance. For very\nhigh-cardinality, dimensionality reduction significantly reduces the\ncomputational cost with little loss in performance: random projections or\nchoosing a subset of prototype categories still outperforms classic encoding\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 06:46:22 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Cerda", "Patricio", "", "PARIETAL"], ["Varoquaux", "Ga\u00ebl", "", "PARIETAL"], ["K\u00e9gl", "Bal\u00e1zs", "", "LAL, CNRS"]]}, {"id": "1806.00984", "submitter": "Md. Shah Fahad", "authors": "Md. Shah Fahad, Jainath Yadav, Gyadhar Pradhan, Akshay Deepak", "title": "DNN-HMM based Speaker Adaptive Emotion Recognition using Proposed Epoch\n  and MFCC Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech is produced when time varying vocal tract system is excited with time\nvarying excitation source. Therefore, the information present in a speech such\nas message, emotion, language, speaker is due to the combined effect of both\nexcitation source and vocal tract system. However, there is very less\nutilization of excitation source features to recognize emotion. In our earlier\nwork, we have proposed a novel method to extract glottal closure instants\n(GCIs) known as epochs. In this paper, we have explored epoch features namely\ninstantaneous pitch, phase and strength of epochs for discriminating emotions.\nWe have combined the excitation source features and the well known\nMale-frequency cepstral coefficient (MFCC) features to develop an emotion\nrecognition system with improved performance. DNN-HMM speaker adaptive models\nhave been developed using MFCC, epoch and combined features. IEMOCAP emotional\ndatabase has been used to evaluate the models. The average accuracy for emotion\nrecognition system when using MFCC and epoch features separately is 59.25% and\n54.52% respectively. The recognition performance improves to 64.2% when MFCC\nand epoch features are combined.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 06:58:45 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Fahad", "Md. Shah", ""], ["Yadav", "Jainath", ""], ["Pradhan", "Gyadhar", ""], ["Deepak", "Akshay", ""]]}, {"id": "1806.01044", "submitter": "Jasper De Bock", "authors": "Jasper De Bock and Gert de Cooman", "title": "A Desirability-Based Axiomatisation for Coherent Choice Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Choice functions constitute a simple, direct and very general mathematical\nframework for modelling choice under uncertainty. In particular, they are able\nto represent the set-valued choices that typically arise from applying decision\nrules to imprecise-probabilistic uncertainty models. We provide them with a\nclear interpretation in terms of attitudes towards gambling, borrowing ideas\nfrom the theory of sets of desirable gambles, and we use this interpretation to\nderive a set of basic axioms. We show that these axioms lead to a full-fledged\ntheory of coherent choice functions, which includes a representation in terms\nof sets of desirable gambles, and a conservative inference method.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 11:01:50 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["De Bock", "Jasper", ""], ["de Cooman", "Gert", ""]]}, {"id": "1806.01130", "submitter": "Ludmila Kuncheva", "authors": "Julian Zubek and Ludmila Kuncheva", "title": "Learning from Exemplars and Prototypes in Machine Learning and\n  Psychology", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper draws a parallel between similarity-based categorisation models\ndeveloped in cognitive psychology and the nearest neighbour classifier (1-NN)\nin machine learning. Conceived as a result of the historical rivalry between\nprototype theories (abstraction) and exemplar theories (memorisation), recent\nmodels of human categorisation seek a compromise in-between. Regarding the\nstimuli (entities to be categorised) as points in a metric space, machine\nlearning offers a large collection of methods to select a small, representative\nand discriminative point set. These methods are known under various names:\ninstance selection, data editing, prototype selection, prototype generation or\nprototype replacement. The nearest neighbour classifier is used with the\nselected reference set. Such a set can be interpreted as a data-driven\ncategorisation model. We juxtapose the models from the two fields to enable\ncross-referencing. We believe that both machine learning and cognitive\npsychology can draw inspiration from the comparison and enrich their repertoire\nof similarity-based models.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 14:05:07 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Zubek", "Julian", ""], ["Kuncheva", "Ludmila", ""]]}, {"id": "1806.01151", "submitter": "Ivan Bravi", "authors": "Ivan Bravi, Jialin Liu, Diego Perez-Liebana, Simon Lucas", "title": "Shallow decision-making analysis in General Video Game Playing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The General Video Game AI competitions have been the testing ground for\nseveral techniques for game playing, such as evolutionary computation\ntechniques, tree search algorithms, hyper heuristic based or knowledge based\nalgorithms. So far the metrics used to evaluate the performance of agents have\nbeen win ratio, game score and length of games. In this paper we provide a\nwider set of metrics and a comparison method for evaluating and comparing\nagents. The metrics and the comparison method give shallow introspection into\nthe agent's decision making process and they can be applied to any agent\nregardless of its algorithmic nature. In this work, the metrics and the\ncomparison method are used to measure the impact of the terms that compose a\ntree policy of an MCTS based agent, comparing with several baseline agents. The\nresults clearly show how promising such general approach is and how it can be\nuseful to understand the behaviour of an AI agent, in particular, how the\ncomparison with baseline agents can help understanding the shape of the agent\ndecision landscape. The presented metrics and comparison method represent a\nstep toward to more descriptive ways of logging and analysing agent's\nbehaviours.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 14:47:46 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Bravi", "Ivan", ""], ["Liu", "Jialin", ""], ["Perez-Liebana", "Diego", ""], ["Lucas", "Simon", ""]]}, {"id": "1806.01159", "submitter": "Edward Pyzer-Knapp", "authors": "Matthew Groves and Edward O. Pyzer-Knapp", "title": "Efficient and Scalable Batch Bayesian Optimization Using K-Means", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present K-Means Batch Bayesian Optimization (KMBBO), a novel batch\nsampling algorithm for Bayesian Optimization (BO). KMBBO uses unsupervised\nlearning to efficiently estimate peaks of the model acquisition function. We\nshow in empirical experiments that our method outperforms the current\nstate-of-the-art batch allocation algorithms on a variety of test problems\nincluding tuning of algorithm hyper-parameters and a challenging drug discovery\nproblem. In order to accommodate the real-world problem of high dimensional\ndata, we propose a modification to KMBBO by combining it with compressed\nsensing to project the optimization into a lower dimensional subspace. We\ndemonstrate empirically that this 2-step method outperforms algorithms where no\ndimensionality reduction has taken place.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 15:24:51 GMT"}, {"version": "v2", "created": "Wed, 19 Sep 2018 08:25:25 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Groves", "Matthew", ""], ["Pyzer-Knapp", "Edward O.", ""]]}, {"id": "1806.01175", "submitter": "Artemij Amiranashvili", "authors": "Artemij Amiranashvili, Alexey Dosovitskiy, Vladlen Koltun, Thomas Brox", "title": "TD or not TD: Analyzing the Role of Temporal Differencing in Deep\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our understanding of reinforcement learning (RL) has been shaped by\ntheoretical and empirical results that were obtained decades ago using tabular\nrepresentations and linear function approximators. These results suggest that\nRL methods that use temporal differencing (TD) are superior to direct Monte\nCarlo estimation (MC). How do these results hold up in deep RL, which deals\nwith perceptually complex environments and deep nonlinear models? In this\npaper, we re-examine the role of TD in modern deep RL, using specially designed\nenvironments that control for specific factors that affect performance, such as\nreward sparsity, reward delay, and the perceptual complexity of the task. When\ncomparing TD with infinite-horizon MC, we are able to reproduce classic results\nin modern settings. Yet we also find that finite-horizon MC is not inferior to\nTD, even when rewards are sparse or delayed. This makes MC a viable alternative\nto TD in deep RL.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 16:16:51 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Amiranashvili", "Artemij", ""], ["Dosovitskiy", "Alexey", ""], ["Koltun", "Vladlen", ""], ["Brox", "Thomas", ""]]}, {"id": "1806.01186", "submitter": "Victoria Krakovna", "authors": "Victoria Krakovna, Laurent Orseau, Ramana Kumar, Miljan Martic, Shane\n  Legg", "title": "Penalizing side effects using stepwise relative reachability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we design safe reinforcement learning agents that avoid unnecessary\ndisruptions to their environment? We show that current approaches to penalizing\nside effects can introduce bad incentives, e.g. to prevent any irreversible\nchanges in the environment, including the actions of other agents. To isolate\nthe source of such undesirable incentives, we break down side effects penalties\ninto two components: a baseline state and a measure of deviation from this\nbaseline state. We argue that some of these incentives arise from the choice of\nbaseline, and others arise from the choice of deviation measure. We introduce a\nnew variant of the stepwise inaction baseline and a new deviation measure based\non relative reachability of states. The combination of these design choices\navoids the given undesirable incentives, while simpler baselines and the\nunreachability measure fail. We demonstrate this empirically by comparing\ndifferent combinations of baseline and deviation measure choices on a set of\ngridworld experiments designed to illustrate possible bad incentives.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 16:30:17 GMT"}, {"version": "v2", "created": "Fri, 8 Mar 2019 09:17:21 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Krakovna", "Victoria", ""], ["Orseau", "Laurent", ""], ["Kumar", "Ramana", ""], ["Martic", "Miljan", ""], ["Legg", "Shane", ""]]}, {"id": "1806.01203", "submitter": "Jessica Hamrick", "authors": "Jessica B. Hamrick, Kelsey R. Allen, Victor Bapst, Tina Zhu, Kevin R.\n  McKee, Joshua B. Tenenbaum, Peter W. Battaglia", "title": "Relational inductive bias for physical construction in humans and\n  machines", "comments": "In Proceedings of the Annual Meeting of the Cognitive Science Society\n  (CogSci 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While current deep learning systems excel at tasks such as object\nclassification, language processing, and gameplay, few can construct or modify\na complex system such as a tower of blocks. We hypothesize that what these\nsystems lack is a \"relational inductive bias\": a capacity for reasoning about\ninter-object relations and making choices over a structured description of a\nscene. To test this hypothesis, we focus on a task that involves gluing pairs\nof blocks together to stabilize a tower, and quantify how well humans perform.\nWe then introduce a deep reinforcement learning agent which uses object- and\nrelation-centric scene and policy representations and apply it to the task. Our\nresults show that these structured representations allow the agent to\noutperform both humans and more naive approaches, suggesting that relational\ninductive bias is an important component in solving structured reasoning\nproblems and for building more intelligent, flexible machines.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 16:45:19 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Hamrick", "Jessica B.", ""], ["Allen", "Kelsey R.", ""], ["Bapst", "Victor", ""], ["Zhu", "Tina", ""], ["McKee", "Kevin R.", ""], ["Tenenbaum", "Joshua B.", ""], ["Battaglia", "Peter W.", ""]]}, {"id": "1806.01235", "submitter": "Emmanouil Antonios Platanios", "authors": "Emmanouil Antonios Platanios and Alex Smola", "title": "Deep Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an algorithm for deep learning on networks and graphs. It relies\non the notion that many graph algorithms, such as PageRank, Weisfeiler-Lehman,\nor Message Passing can be expressed as iterative vertex updates. Unlike\nprevious methods which rely on the ingenuity of the designer, Deep Graphs are\nadaptive to the estimation problem. Training and deployment are both efficient,\nsince the cost is $O(|E| + |V|)$, where $E$ and $V$ are the sets of edges and\nvertices respectively. In short, we learn the recurrent update functions rather\nthan positing their specific functional form. This yields an algorithm that\nachieves excellent accuracy on both graph labeling and regression tasks.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 17:24:18 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Platanios", "Emmanouil Antonios", ""], ["Smola", "Alex", ""]]}, {"id": "1806.01242", "submitter": "Alvaro Sanchez-Gonzalez", "authors": "Alvaro Sanchez-Gonzalez, Nicolas Heess, Jost Tobias Springenberg, Josh\n  Merel, Martin Riedmiller, Raia Hadsell and Peter Battaglia", "title": "Graph networks as learnable physics engines for inference and control", "comments": "ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding and interacting with everyday physical scenes requires rich\nknowledge about the structure of the world, represented either implicitly in a\nvalue or policy function, or explicitly in a transition model. Here we\nintroduce a new class of learnable models--based on graph networks--which\nimplement an inductive bias for object- and relation-centric representations of\ncomplex, dynamical systems. Our results show that as a forward model, our\napproach supports accurate predictions from real and simulated data, and\nsurprisingly strong and efficient generalization, across eight distinct\nphysical systems which we varied parametrically and structurally. We also found\nthat our inference model can perform system identification. Our models are also\ndifferentiable, and support online planning via gradient-based trajectory\noptimization, as well as offline policy optimization. Our framework offers new\nopportunities for harnessing and exploiting rich knowledge about the world, and\ntakes a key step toward building machines with more human-like representations\nof the world.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 17:29:40 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Sanchez-Gonzalez", "Alvaro", ""], ["Heess", "Nicolas", ""], ["Springenberg", "Jost Tobias", ""], ["Merel", "Josh", ""], ["Riedmiller", "Martin", ""], ["Hadsell", "Raia", ""], ["Battaglia", "Peter", ""]]}, {"id": "1806.01246", "submitter": "Yang Zhang", "authors": "Ahmed Salem, Yang Zhang, Mathias Humbert, Pascal Berrang, Mario Fritz,\n  Michael Backes", "title": "ML-Leaks: Model and Data Independent Membership Inference Attacks and\n  Defenses on Machine Learning Models", "comments": "NDSS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) has become a core component of many real-world\napplications and training data is a key factor that drives current progress.\nThis huge success has led Internet companies to deploy machine learning as a\nservice (MLaaS). Recently, the first membership inference attack has shown that\nextraction of information on the training set is possible in such MLaaS\nsettings, which has severe security and privacy implications.\n  However, the early demonstrations of the feasibility of such attacks have\nmany assumptions on the adversary, such as using multiple so-called shadow\nmodels, knowledge of the target model structure, and having a dataset from the\nsame distribution as the target model's training data. We relax all these key\nassumptions, thereby showing that such attacks are very broadly applicable at\nlow cost and thereby pose a more severe risk than previously thought. We\npresent the most comprehensive study so far on this emerging and developing\nthreat using eight diverse datasets which show the viability of the proposed\nattacks across domains.\n  In addition, we propose the first effective defense mechanisms against such\nbroader class of membership inference attacks that maintain a high level of\nutility of the ML model.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 17:38:42 GMT"}, {"version": "v2", "created": "Fri, 14 Dec 2018 19:39:43 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Salem", "Ahmed", ""], ["Zhang", "Yang", ""], ["Humbert", "Mathias", ""], ["Berrang", "Pascal", ""], ["Fritz", "Mario", ""], ["Backes", "Michael", ""]]}, {"id": "1806.01258", "submitter": "Emmanouil Antonios Platanios", "authors": "Emmanouil Antonios Platanios", "title": "Agreement-based Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model selection is a problem that has occupied machine learning researchers\nfor a long time. Recently, its importance has become evident through\napplications in deep learning. We propose an agreement-based learning framework\nthat prevents many of the pitfalls associated with model selection. It relies\non coupling the training of multiple models by encouraging them to agree on\ntheir predictions while training. In contrast with other model selection and\ncombination approaches used in machine learning, the proposed framework is\ninspired by human learning. We also propose a learning algorithm defined within\nthis framework which manages to significantly outperform alternatives in\npractice, and whose performance improves further with the availability of\nunlabeled data. Finally, we describe a number of potential directions for\ndeveloping more flexible agreement-based learning algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 17:55:12 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Platanios", "Emmanouil Antonios", ""]]}, {"id": "1806.01261", "submitter": "Peter Battaglia", "authors": "Peter W. Battaglia, Jessica B. Hamrick, Victor Bapst, Alvaro\n  Sanchez-Gonzalez, Vinicius Zambaldi, Mateusz Malinowski, Andrea Tacchetti,\n  David Raposo, Adam Santoro, Ryan Faulkner, Caglar Gulcehre, Francis Song,\n  Andrew Ballard, Justin Gilmer, George Dahl, Ashish Vaswani, Kelsey Allen,\n  Charles Nash, Victoria Langston, Chris Dyer, Nicolas Heess, Daan Wierstra,\n  Pushmeet Kohli, Matt Botvinick, Oriol Vinyals, Yujia Li, Razvan Pascanu", "title": "Relational inductive biases, deep learning, and graph networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial intelligence (AI) has undergone a renaissance recently, making\nmajor progress in key domains such as vision, language, control, and\ndecision-making. This has been due, in part, to cheap data and cheap compute\nresources, which have fit the natural strengths of deep learning. However, many\ndefining characteristics of human intelligence, which developed under much\ndifferent pressures, remain out of reach for current approaches. In particular,\ngeneralizing beyond one's experiences--a hallmark of human intelligence from\ninfancy--remains a formidable challenge for modern AI.\n  The following is part position paper, part review, and part unification. We\nargue that combinatorial generalization must be a top priority for AI to\nachieve human-like abilities, and that structured representations and\ncomputations are key to realizing this objective. Just as biology uses nature\nand nurture cooperatively, we reject the false choice between\n\"hand-engineering\" and \"end-to-end\" learning, and instead advocate for an\napproach which benefits from their complementary strengths. We explore how\nusing relational inductive biases within deep learning architectures can\nfacilitate learning about entities, relations, and rules for composing them. We\npresent a new building block for the AI toolkit with a strong relational\ninductive bias--the graph network--which generalizes and extends various\napproaches for neural networks that operate on graphs, and provides a\nstraightforward interface for manipulating structured knowledge and producing\nstructured behaviors. We discuss how graph networks can support relational\nreasoning and combinatorial generalization, laying the foundation for more\nsophisticated, interpretable, and flexible patterns of reasoning. As a\ncompanion to this paper, we have released an open-source software library for\nbuilding graph networks, with demonstrations of how to use them in practice.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 17:58:18 GMT"}, {"version": "v2", "created": "Mon, 11 Jun 2018 13:33:54 GMT"}, {"version": "v3", "created": "Wed, 17 Oct 2018 17:51:36 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Battaglia", "Peter W.", ""], ["Hamrick", "Jessica B.", ""], ["Bapst", "Victor", ""], ["Sanchez-Gonzalez", "Alvaro", ""], ["Zambaldi", "Vinicius", ""], ["Malinowski", "Mateusz", ""], ["Tacchetti", "Andrea", ""], ["Raposo", "David", ""], ["Santoro", "Adam", ""], ["Faulkner", "Ryan", ""], ["Gulcehre", "Caglar", ""], ["Song", "Francis", ""], ["Ballard", "Andrew", ""], ["Gilmer", "Justin", ""], ["Dahl", "George", ""], ["Vaswani", "Ashish", ""], ["Allen", "Kelsey", ""], ["Nash", "Charles", ""], ["Langston", "Victoria", ""], ["Dyer", "Chris", ""], ["Heess", "Nicolas", ""], ["Wierstra", "Daan", ""], ["Kohli", "Pushmeet", ""], ["Botvinick", "Matt", ""], ["Vinyals", "Oriol", ""], ["Li", "Yujia", ""], ["Pascanu", "Razvan", ""]]}, {"id": "1806.01264", "submitter": "Subhabrata Mukherjee", "authors": "Guineng Zheng, Subhabrata Mukherjee, Xin Luna Dong, Feifei Li", "title": "OpenTag: Open Attribute Value Extraction from Product Profiles [Deep\n  Learning, Active Learning, Named Entity Recognition]", "comments": "Proceedings of the 24th ACM SIGKDD International Conference on\n  Knowledge Discovery and Data Mining, London, UK, August 19-23, 2018", "journal-ref": null, "doi": "10.1145/3219819.3219839", "report-no": null, "categories": "cs.CL cs.AI cs.IR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extraction of missing attribute values is to find values describing an\nattribute of interest from a free text input. Most past related work on\nextraction of missing attribute values work with a closed world assumption with\nthe possible set of values known beforehand, or use dictionaries of values and\nhand-crafted features. How can we discover new attribute values that we have\nnever seen before? Can we do this with limited human annotation or supervision?\nWe study this problem in the context of product catalogs that often have\nmissing values for many attributes of interest.\n  In this work, we leverage product profile information such as titles and\ndescriptions to discover missing values of product attributes. We develop a\nnovel deep tagging model OpenTag for this extraction problem with the following\ncontributions: (1) we formalize the problem as a sequence tagging task, and\npropose a joint model exploiting recurrent neural networks (specifically,\nbidirectional LSTM) to capture context and semantics, and Conditional Random\nFields (CRF) to enforce tagging consistency, (2) we develop a novel attention\nmechanism to provide interpretable explanation for our model's decisions, (3)\nwe propose a novel sampling strategy exploring active learning to reduce the\nburden of human annotation. OpenTag does not use any dictionary or hand-crafted\nfeatures as in prior works. Extensive experiments in real-life datasets in\ndifferent domains show that OpenTag with our active learning strategy discovers\nnew attribute values from as few as 150 annotated samples (reduction in 3.3x\namount of annotation effort) with a high F-score of 83%, outperforming\nstate-of-the-art models.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 19:41:07 GMT"}, {"version": "v2", "created": "Sat, 6 Oct 2018 17:29:28 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Zheng", "Guineng", ""], ["Mukherjee", "Subhabrata", ""], ["Dong", "Xin Luna", ""], ["Li", "Feifei", ""]]}, {"id": "1806.01265", "submitter": "Kavosh Asadi", "authors": "Kavosh Asadi, Evan Cater, Dipendra Misra, Michael L. Littman", "title": "Equivalence Between Wasserstein and Value-Aware Loss for Model-based\n  Reinforcement Learning", "comments": "Accepted at the FAIM workshop \"Prediction and Generative Modeling in\n  Reinforcement Learning\", Stockholm, Sweden, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a generative model is a key component of model-based reinforcement\nlearning. Though learning a good model in the tabular setting is a simple task,\nlearning a useful model in the approximate setting is challenging. In this\ncontext, an important question is the loss function used for model learning as\nvarying the loss function can have a remarkable impact on effectiveness of\nplanning. Recently Farahmand et al. (2017) proposed a value-aware model\nlearning (VAML) objective that captures the structure of value function during\nmodel learning. Using tools from Asadi et al. (2018), we show that minimizing\nthe VAML objective is in fact equivalent to minimizing the Wasserstein metric.\nThis equivalence improves our understanding of value-aware models, and also\ncreates a theoretical foundation for applications of Wasserstein in model-based\nreinforcement~learning.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 21:54:18 GMT"}, {"version": "v2", "created": "Sun, 8 Jul 2018 12:53:13 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Asadi", "Kavosh", ""], ["Cater", "Evan", ""], ["Misra", "Dipendra", ""], ["Littman", "Michael L.", ""]]}, {"id": "1806.01267", "submitter": "Daiki Kimura", "authors": "Daiki Kimura, Subhajit Chaudhury, Ryuki Tachibana, Sakyasingha\n  Dasgupta", "title": "Internal Model from Observations for Reward Shaping", "comments": "7 pages, 6 figures, ICML workshop (ALA 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning methods require careful design involving a reward\nfunction to obtain the desired action policy for a given task. In the absence\nof hand-crafted reward functions, prior work on the topic has proposed several\nmethods for reward estimation by using expert state trajectories and action\npairs. However, there are cases where complete or good action information\ncannot be obtained from expert demonstrations. We propose a novel reinforcement\nlearning method in which the agent learns an internal model of observation on\nthe basis of expert-demonstrated state trajectories to estimate rewards without\ncompletely learning the dynamics of the external environment from state-action\npairs. The internal model is obtained in the form of a predictive model for the\ngiven expert state distribution. During reinforcement learning, the agent\npredicts the reward as a function of the difference between the actual state\nand the state predicted by the internal model. We conducted multiple\nexperiments in environments of varying complexity, including the Super Mario\nBros and Flappy Bird games. We show our method successfully trains good\npolicies directly from expert game-play videos.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2018 08:15:38 GMT"}, {"version": "v2", "created": "Thu, 28 Jun 2018 03:56:36 GMT"}, {"version": "v3", "created": "Tue, 3 Jul 2018 22:43:21 GMT"}, {"version": "v4", "created": "Sun, 14 Oct 2018 13:15:09 GMT"}], "update_date": "2018-10-16", "authors_parsed": [["Kimura", "Daiki", ""], ["Chaudhury", "Subhajit", ""], ["Tachibana", "Ryuki", ""], ["Dasgupta", "Sakyasingha", ""]]}, {"id": "1806.01322", "submitter": "Tim Taylor", "authors": "Tim Taylor and Alan Dorin", "title": "Past Visions of Artificial Futures: One Hundred and Fifty Years under\n  the Spectre of Evolving Machines", "comments": "To appear in Proceedings of the Artificial Life Conference 2018\n  (ALIFE 2018), MIT Press", "journal-ref": "Proceedings of the Artificial Life Conference 2018, T. Ikegami et\n  al. (eds.), MIT Press (pp.91-98)", "doi": "10.1162/isal_a_00022", "report-no": null, "categories": "cs.AI cs.CY cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The influence of Artificial Intelligence (AI) and Artificial Life (ALife)\ntechnologies upon society, and their potential to fundamentally shape the\nfuture evolution of humankind, are topics very much at the forefront of current\nscientific, governmental and public debate. While these might seem like very\nmodern concerns, they have a long history that is often disregarded in\ncontemporary discourse. Insofar as current debates do acknowledge the history\nof these ideas, they rarely look back further than the origin of the modern\ndigital computer age in the 1940s-50s. In this paper we explore the earlier\nhistory of these concepts. We focus in particular on the idea of\nself-reproducing and evolving machines, and potential implications for our own\nspecies. We show that discussion of these topics arose in the 1860s, within a\ndecade of the publication of Darwin's The Origin of Species, and attracted\nincreasing interest from scientists, novelists and the general public in the\nearly 1900s. After introducing the relevant work from this period, we\ncategorise the various visions presented by these authors of the future\nimplications of evolving machines for humanity. We suggest that current debates\non the co-evolution of society and technology can be enriched by a proper\nappreciation of the long history of the ideas involved.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 18:52:40 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Taylor", "Tim", ""], ["Dorin", "Alan", ""]]}, {"id": "1806.01347", "submitter": "Josiah Hanna", "authors": "Josiah P. Hanna, Scott Niekum, Peter Stone", "title": "Importance Sampling Policy Evaluation with an Estimated Behavior Policy", "comments": "Accepted to ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of off-policy evaluation in Markov decision\nprocesses. Off-policy evaluation is the task of evaluating the expected return\nof one policy with data generated by a different, behavior policy. Importance\nsampling is a technique for off-policy evaluation that re-weights off-policy\nreturns to account for differences in the likelihood of the returns between the\ntwo policies. In this paper, we study importance sampling with an estimated\nbehavior policy where the behavior policy estimate comes from the same set of\ndata used to compute the importance sampling estimate. We find that this\nestimator often lowers the mean squared error of off-policy evaluation compared\nto importance sampling with the true behavior policy or using a behavior policy\nthat is estimated from a separate data set. Intuitively, estimating the\nbehavior policy in this way corrects for error due to sampling in the\naction-space. Our empirical results also extend to other popular variants of\nimportance sampling and show that estimating a non-Markovian behavior policy\ncan further lower large-sample mean squared error even when the true behavior\npolicy is Markovian.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 19:47:24 GMT"}, {"version": "v2", "created": "Mon, 24 Sep 2018 16:58:16 GMT"}, {"version": "v3", "created": "Thu, 9 May 2019 20:47:55 GMT"}], "update_date": "2019-05-13", "authors_parsed": [["Hanna", "Josiah P.", ""], ["Niekum", "Scott", ""], ["Stone", "Peter", ""]]}, {"id": "1806.01363", "submitter": "Giuseppe Cuccu", "authors": "Giuseppe Cuccu, Julian Togelius, Philippe Cudre-Mauroux", "title": "Playing Atari with Six Neurons", "comments": "Accepted at AAMAS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning, applied to vision-based problems like Atari\ngames, maps pixels directly to actions; internally, the deep neural network\nbears the responsibility of both extracting useful information and making\ndecisions based on it. By separating the image processing from decision-making,\none could better understand the complexity of each task, as well as potentially\nfind smaller policy representations that are easier for humans to understand\nand may generalize better. To this end, we propose a new method for learning\npolicies and compact state representations separately but simultaneously for\npolicy approximation in reinforcement learning. State representations are\ngenerated by an encoder based on two novel algorithms: Increasing Dictionary\nVector Quantization makes the encoder capable of growing its dictionary size\nover time, to address new observations as they appear in an open-ended\nonline-learning context; Direct Residuals Sparse Coding encodes observations by\ndisregarding reconstruction error minimization, and aiming instead for highest\ninformation inclusion. The encoder autonomously selects observations online to\ntrain on, in order to maximize code sparsity. As the dictionary size increases,\nthe encoder produces increasingly larger inputs for the neural network: this is\naddressed by a variation of the Exponential Natural Evolution Strategies\nalgorithm which adapts its probability distribution dimensionality along the\nrun. We test our system on a selection of Atari games using tiny neural\nnetworks of only 6 to 18 neurons (depending on the game's controls). These are\nstill capable of achieving results comparable---and occasionally superior---to\nstate-of-the-art techniques which use two orders of magnitude more neurons.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 20:09:43 GMT"}, {"version": "v2", "created": "Sun, 3 Mar 2019 11:42:42 GMT"}], "update_date": "2019-03-05", "authors_parsed": [["Cuccu", "Giuseppe", ""], ["Togelius", "Julian", ""], ["Cudre-Mauroux", "Philippe", ""]]}, {"id": "1806.01368", "submitter": "Vahid Behzadan", "authors": "Vahid Behzadan and Arslan Munir", "title": "Adversarial Reinforcement Learning Framework for Benchmarking Collision\n  Avoidance Mechanisms in Autonomous Vehicles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapidly growing interest in autonomous navigation, the body of\nresearch on motion planning and collision avoidance techniques has enjoyed an\naccelerating rate of novel proposals and developments. However, the complexity\nof new techniques and their safety requirements render the bulk of current\nbenchmarking frameworks inadequate, thus leaving the need for efficient\ncomparison techniques unanswered. This work proposes a novel framework based on\ndeep reinforcement learning for benchmarking the behavior of collision\navoidance mechanisms under the worst-case scenario of dealing with an optimal\nadversarial agent, trained to drive the system into unsafe states. We describe\nthe architecture and flow of this framework as a benchmarking solution, and\ndemonstrate its efficacy via a practical case study of comparing the\nreliability of two collision avoidance mechanisms in response to intentional\ncollision attempts.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 20:17:40 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Behzadan", "Vahid", ""], ["Munir", "Arslan", ""]]}, {"id": "1806.01387", "submitter": "Christian Guckelsberger", "authors": "Christian Guckelsberger, Christoph Salge, Julian Togelius", "title": "New And Surprising Ways to Be Mean. Adversarial NPCs with Coupled\n  Empowerment Minimisation", "comments": "IEEE Computational Intelligence and Games (CIG) conference, 2018,\n  Maastricht. 8 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Creating Non-Player Characters (NPCs) that can react robustly to unforeseen\nplayer behaviour or novel game content is difficult and time-consuming. This\nhinders the design of believable characters, and the inclusion of NPCs in games\nthat rely heavily on procedural content generation. We have previously\naddressed this challenge by means of empowerment, a model of intrinsic\nmotivation, and demonstrated how a coupled empowerment maximisation (CEM)\npolicy can yield generic, companion-like behaviour. In this paper, we extend\nthe CEM framework with a minimisation policy to give rise to adversarial\nbehaviour. We conduct a qualitative, exploratory study in a dungeon-crawler\ngame, demonstrating that CEM can exploit the affordances of different content\nfacets in adaptive adversarial behaviour without modifications to the policy.\nChanges to the level design, underlying mechanics and our character's actions\ndo not threaten our NPC's robustness, but yield new and surprising ways to be\nmean.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 21:02:49 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Guckelsberger", "Christian", ""], ["Salge", "Christoph", ""], ["Togelius", "Julian", ""]]}, {"id": "1806.01488", "submitter": "Cheng Soon Ong", "authors": "Finnian Lattimore and Cheng Soon Ong", "title": "A Primer on Causal Analysis", "comments": "Parts of this document are copied verbatim from Finnian Lattimore's\n  PhD thesis, ANU 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We provide a conceptual map to navigate causal analysis problems. Focusing on\nthe case of discrete random variables, we consider the case of causal effect\nestimation from observational data. The presented approaches apply also to\ncontinuous variables, but the issue of estimation becomes more complex. We then\nintroduce the four schools of thought for causal analysis\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 04:19:22 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Lattimore", "Finnian", ""], ["Ong", "Cheng Soon", ""]]}, {"id": "1806.01502", "submitter": "Yen Yu", "authors": "Yen Yu, Acer Y.C. Chang, Ryota Kanai", "title": "Boredom-driven curious learning by Homeo-Heterostatic Value Gradients", "comments": "21 pages, 4 figures", "journal-ref": null, "doi": "10.3389/fnbot.2018.00088", "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the Homeo-Heterostatic Value Gradients (HHVG) algorithm\nas a formal account on the constructive interplay between boredom and curiosity\nwhich gives rise to effective exploration and superior forward model learning.\nWe envisaged actions as instrumental in agent's own epistemic disclosure. This\nmotivated two central algorithmic ingredients: devaluation and devaluation\nprogress, both underpin agent's cognition concerning intrinsically generated\nrewards. The two serve as an instantiation of homeostatic and heterostatic\nintrinsic motivation. A key insight from our algorithm is that the two\nseemingly opposite motivations can be reconciled---without which exploration\nand information-gathering cannot be effectively carried out. We supported this\nclaim with empirical evidence, showing that boredom-enabled agents consistently\noutperformed other curious or explorative agent variants in model building\nbenchmarks based on self-assisted experience accumulation.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 05:34:46 GMT"}], "update_date": "2019-02-07", "authors_parsed": [["Yu", "Yen", ""], ["Chang", "Acer Y. C.", ""], ["Kanai", "Ryota", ""]]}, {"id": "1806.01526", "submitter": "Piek Vossen", "authors": "Piek Vossen, Selene Baez, Lenka Baj\\v{c}eti\\'c, and Bram Kraaijeveld", "title": "Leolani: a reference machine with a theory of mind for social\n  communication", "comments": "Invited keynote at 21st International Conference on Text, Speech and\n  Dialogue, https://www.tsdconference.org/tsd2018/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our state of mind is based on experiences and what other people tell us. This\nmay result in conflicting information, uncertainty, and alternative facts. We\npresent a robot that models relativity of knowledge and perception within\nsocial interaction following principles of the theory of mind. We utilized\nvision and speech capabilities on a Pepper robot to build an interaction model\nthat stores the interpretations of perceptions and conversations in combination\nwith provenance on its sources. The robot learns directly from what people tell\nit, possibly in relation to its perception. We demonstrate how the robot's\ncommunication is driven by hunger to acquire more knowledge from and on people\nand objects, to resolve uncertainties and conflicts, and to share awareness of\nthe per- ceived environment. Likewise, the robot can make reference to the\nworld and its knowledge about the world and the encounters with people that\nyielded this knowledge.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 07:36:36 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Vossen", "Piek", ""], ["Baez", "Selene", ""], ["Baj\u010deti\u0107", "Lenka", ""], ["Kraaijeveld", "Bram", ""]]}, {"id": "1806.01563", "submitter": "Fuyuan Xiao", "authors": "Fuyuan Xiao", "title": "Multi-sensor data fusion based on a generalised belief divergence\n  measure", "comments": "35 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-sensor data fusion technology plays an important role in real\napplications. Because of the flexibility and effectiveness in modelling and\nprocessing the uncertain information regardless of prior probabilities,\nDempster-Shafer evidence theory is widely applied in a variety of fields of\ninformation fusion. However, counter-intuitive results may come out when fusing\nthe highly conflicting evidences. In order to deal with this problem, a novel\nmethod for multi-sensor data fusion based on a new generalised belief\ndivergence measure of evidences is proposed. Firstly, the reliability weights\nof evidences are determined by considering the sufficiency and importance of\nthe evidences. After that, on account of the reliability weights of evidences,\na new Generalised Belief Jensen-Shannon divergence (GBJS) is designed to\nmeasure the discrepancy and conflict degree among multiple evidences, which can\nbe utilised to measure the support degrees of evidences. Afterwards, the\nsupport degrees of evidences are used to adjust the bodies of the evidences\nbefore using the Dempster's combination rule. Finally, an application in fault\ndiagnosis demonstrates the validity of the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 08:59:45 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Xiao", "Fuyuan", ""]]}, {"id": "1806.01650", "submitter": "Md. Noor-E-Alam", "authors": "Dizuo Jiang, Md Mahmudul Hassan, Tasnim Ibn Faiz and Md. Noor-E-Alam", "title": "A Possibility Distribution Based Multi-Criteria Decision Algorithm for\n  Resilient Supplier Selection Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thus far, limited research has been performed on resilient supplier selection\n- a problem that requires simultaneous consideration of a set of numerical and\nlinguistic evaluation criteria, which are substantially different from\ntraditional supplier selection problem. Essentially, resilient supplier\nselection entails key sourcing decision for an organization to gain competitive\nadvantage. In the presence of multiple conflicting evaluation criteria,\ncontradicting decision makers, and imprecise decision relevant information\n(DRI), this problem becomes even more difficult to solve with the classical\noptimization approaches. However, prior research focusing on MCDA based\nsupplier selection problem has been lacking in the ability to provide a\nseamless integration of numerical and linguistic evaluation criteria along with\nthe consideration of multiple decision makers. To address these challenges, we\npresent a comprehensive decision-making framework for ranking a set of\nsuppliers from resiliency perspective. The proposed algorithm is capable of\nleveraging imprecise and aggregated DRI obtained from crisp numerical\nassessments and reliability adjusted linguistic appraisals from a group of\ndecision makers. We adapt two popular tools - Single Valued Neutrosophic Sets\n(SVNS) and Interval-valued fuzzy sets (IVFS), and for the first time extend\nthem to incorporate both crisp and linguistic evaluations in a group decision\nmaking platform to obtain aggregated SVNS and IVFS decision matrix. This\ninformation is then used to rank the resilient suppliers by using TOPSIS\nmethod. We present a case study to illustrate the mechanism of the proposed\nalgorithm.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 03:21:00 GMT"}, {"version": "v2", "created": "Sat, 6 Apr 2019 20:17:17 GMT"}], "update_date": "2019-04-09", "authors_parsed": [["Jiang", "Dizuo", ""], ["Hassan", "Md Mahmudul", ""], ["Faiz", "Tasnim Ibn", ""], ["Noor-E-Alam", "Md.", ""]]}, {"id": "1806.01655", "submitter": "P.K. Srijith", "authors": "Vinayak Kumar, Vaibhav Singh, P. K. Srijith, Andreas Damianou", "title": "Deep Gaussian Processes with Convolutional Kernels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Gaussian processes (DGPs) provide a Bayesian non-parametric alternative\nto standard parametric deep learning models. A DGP is formed by stacking\nmultiple GPs resulting in a well-regularized composition of functions. The\nBayesian framework that equips the model with attractive properties, such as\nimplicit capacity control and predictive uncertainty, makes it at the same time\nchallenging to combine with a convolutional structure. This has hindered the\napplication of DGPs in computer vision tasks, an area where deep parametric\nmodels (i.e. CNNs) have made breakthroughs. Standard kernels used in DGPs such\nas radial basis functions (RBFs) are insufficient for handling pixel\nvariability in raw images. In this paper, we build on the recent convolutional\nGP to develop Convolutional DGP (CDGP) models which effectively capture image\nlevel features through the use of convolution kernels, therefore opening up the\nway for applying DGPs to computer vision tasks. Our model learns local spatial\ninfluence and outperforms strong GP based baselines on multi-class image\nclassification. We also consider various constructions of convolution kernel\nover the image patches, analyze the computational trade-offs and provide an\nefficient framework for convolutional DGP models. The experimental results on\nimage data such as MNIST, rectangles-image, CIFAR10 and Caltech101 demonstrate\nthe effectiveness of the proposed approaches.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 12:41:14 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Kumar", "Vinayak", ""], ["Singh", "Vaibhav", ""], ["Srijith", "P. K.", ""], ["Damianou", "Andreas", ""]]}, {"id": "1806.01661", "submitter": "Avi Loeb", "authors": "Abraham Loeb (Harvard)", "title": "Experimental Tests of Spirituality", "comments": "2 pages, accepted for publication in Scientific American", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.pop-ph astro-ph.IM cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We currently harness technologies that could shed new light on old\nphilosophical questions, such as whether our mind entails anything beyond our\nbody or whether our moral values reflect universal truth.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 16:28:21 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Loeb", "Abraham", "", "Harvard"]]}, {"id": "1806.01664", "submitter": "Thomas Bonald", "authors": "Thomas Bonald, Bertrand Charpentier, Alexis Galland, Alexandre\n  Hollocou", "title": "Hierarchical Graph Clustering using Node Pair Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel hierarchical graph clustering algorithm inspired by\nmodularity-based clustering techniques. The algorithm is agglomerative and\nbased on a simple distance between clusters induced by the probability of\nsampling node pairs. We prove that this distance is reducible, which enables\nthe use of the nearest-neighbor chain to speed up the agglomeration. The output\nof the algorithm is a regular dendrogram, which reveals the multi-scale\nstructure of the graph. The results are illustrated on both synthetic and real\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 12:54:07 GMT"}, {"version": "v2", "created": "Fri, 22 Jun 2018 17:22:19 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Bonald", "Thomas", ""], ["Charpentier", "Bertrand", ""], ["Galland", "Alexis", ""], ["Hollocou", "Alexandre", ""]]}, {"id": "1806.01709", "submitter": "Leonidas Doumas", "authors": "Leonidas A. A. Doumas, Guillermo Puebla, Andrea E. Martin", "title": "Human-like generalization in a machine through predicate learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans readily generalize, applying prior knowledge to novel situations and\nstimuli. Advances in machine learning and artificial intelligence have begun to\napproximate and even surpass human performance, but machine systems reliably\nstruggle to generalize information to untrained situations. We describe a\nneural network model that is trained to play one video game (Breakout) and\ndemonstrates one-shot generalization to a new game (Pong). The model\ngeneralizes by learning representations that are functionally and formally\nsymbolic from training data, without feedback, and without requiring that\nstructured representations be specified a priori. The model uses unsupervised\ncomparison to discover which characteristics of the input are invariant, and to\nlearn relational predicates; it then applies these predicates to arguments in a\nsymbolic fashion, using oscillatory regularities in network firing to\ndynamically bind predicates to arguments. We argue that models of human\ncognition must account for far-reaching and flexible generalization, and that\nin order to do so, models must be able to discover symbolic representations\nfrom unstructured data, a process we call predicate learning. Only then can\nmodels begin to adequately explain where human-like representations come from,\nwhy human cognition is the way it is, and why it continues to differ from\nmachine intelligence in crucial ways.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 14:21:20 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2018 20:05:11 GMT"}, {"version": "v3", "created": "Thu, 7 Mar 2019 10:41:38 GMT"}], "update_date": "2019-03-08", "authors_parsed": [["Doumas", "Leonidas A. A.", ""], ["Puebla", "Guillermo", ""], ["Martin", "Andrea E.", ""]]}, {"id": "1806.01756", "submitter": "Daniel T Chang", "authors": "Daniel T Chang", "title": "Concept-Oriented Deep Learning", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concepts are the foundation of human deep learning, understanding, and\nknowledge integration and transfer. We propose concept-oriented deep learning\n(CODL) which extends (machine) deep learning with concept representations and\nconceptual understanding capability. CODL addresses some of the major\nlimitations of deep learning: interpretability, transferability, contextual\nadaptation, and requirement for lots of labeled training data. We discuss the\nmajor aspects of CODL including concept graph, concept representations, concept\nexemplars, and concept representation learning systems supporting incremental\nand continual learning.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 15:50:30 GMT"}], "update_date": "2018-06-06", "authors_parsed": [["Chang", "Daniel T", ""]]}, {"id": "1806.01825", "submitter": "G. Zacharias Holland", "authors": "G. Zacharias Holland, Erin J. Talvitie, and Michael Bowling", "title": "The Effect of Planning Shape on Dyna-style Planning in High-dimensional\n  State Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dyna is a fundamental approach to model-based reinforcement learning (MBRL)\nthat interleaves planning, acting, and learning in an online setting. In the\nmost typical application of Dyna, the dynamics model is used to generate\none-step transitions from selected start states from the agent's history, which\nare used to update the agent's value function or policy as if they were real\nexperiences. In this work, one-step Dyna was applied to several games from the\nArcade Learning Environment (ALE). We found that the model-based updates\noffered surprisingly little benefit over simply performing more updates with\nthe agent's existing experience, even when using a perfect model. We\nhypothesize that to get the most from planning, the model must be used to\ngenerate unfamiliar experience. To test this, we experimented with the \"shape\"\nof planning in multiple different concrete instantiations of Dyna, performing\nfewer, longer rollouts, rather than many short rollouts. We found that planning\nshape has a profound impact on the efficacy of Dyna for both perfect and\nlearned models. In addition to these findings regarding Dyna in general, our\nresults represent, to our knowledge, the first time that a learned dynamics\nmodel has been successfully used for planning in the ALE, suggesting that Dyna\nmay be a viable approach to MBRL in the ALE and other high-dimensional\nproblems.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 17:31:02 GMT"}, {"version": "v2", "created": "Fri, 8 Jun 2018 20:46:56 GMT"}, {"version": "v3", "created": "Fri, 29 Mar 2019 03:00:57 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Holland", "G. Zacharias", ""], ["Talvitie", "Erin J.", ""], ["Bowling", "Michael", ""]]}, {"id": "1806.01883", "submitter": "Tim Taylor", "authors": "Tim Taylor", "title": "Evolutionary Innovations and Where to Find Them: Routes to Open-Ended\n  Evolution in Natural and Artificial Systems", "comments": "This version (v4) to appear in the journal Artificial Life 25(2),\n  2019. Previous version (v3) was presented at the Third Workshop on Open-Ended\n  Evolution (OEE3), Tokyo, Japan, July 2018 (hosted by the 2018 Conference on\n  Artificial Life)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a high-level conceptual framework to help orient the\ndiscussion and implementation of open-endedness in evolutionary systems.\nDrawing upon earlier work by Banzhaf et al., three different kinds of\nopen-endedness are identified: exploratory, expansive, and transformational.\nThese are characterised in terms of their relationship to the search space of\nphenotypic behaviours. A formalism is introduced to describe three key\nprocesses required for an evolutionary process: the generation of a phenotype\nfrom a genetic description, the evaluation of that phenotype, and the\nreproduction with variation of individuals according to their evaluation. The\ndistinction is made between intrinsic and extrinsic implementations of these\nprocesses. A discussion then investigates how various interactions between\nthese processes, and their modes of implementation, can lead to open-endedness.\nHowever, an important contribution of the paper is the demonstration that these\nconsiderations relate to exploratory open-endedness only. Conditions for the\nimplementation of the more interesting kinds of open-endedness - expansive and\ntransformational - are also discussed, emphasizing factors such as multiple\ndomains of behaviour, transdomain bridges, and non-additive compositional\nsystems. These factors relate not to the generic evolutionary properties of\nindividuals and populations, but rather to the nature of the building blocks\nout of which individual organisms are constructed, and the laws and properties\nof the environment in which they exist. The paper ends with suggestions of how\nthe framework can be used to categorise and compare the open-ended evolutionary\npotential of different systems, how it might guide the design of systems with\ngreater capacity for open-ended evolution, and how it might be further\nimproved.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 18:30:22 GMT"}, {"version": "v2", "created": "Mon, 2 Jul 2018 17:18:41 GMT"}, {"version": "v3", "created": "Tue, 17 Jul 2018 18:30:53 GMT"}, {"version": "v4", "created": "Thu, 21 Feb 2019 09:18:52 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Taylor", "Tim", ""]]}, {"id": "1806.01910", "submitter": "Robert Peharz", "authors": "Robert Peharz, Antonio Vergari, Karl Stelzner, Alejandro Molina,\n  Martin Trapp, Kristian Kersting, and Zoubin Ghahramani", "title": "Probabilistic Deep Learning using Random Sum-Product Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The need for consistent treatment of uncertainty has recently triggered\nincreased interest in probabilistic deep learning methods. However, most\ncurrent approaches have severe limitations when it comes to inference, since\nmany of these models do not even permit to evaluate exact data likelihoods.\nSum-product networks (SPNs), on the other hand, are an excellent architecture\nin that regard, as they allow to efficiently evaluate likelihoods, as well as\narbitrary marginalization and conditioning tasks. Nevertheless, SPNs have not\nbeen fully explored as serious deep learning models, likely due to their\nspecial structural requirements, which complicate learning. In this paper, we\nmake a drastic simplification and use random SPN structures which are trained\nin a \"classical deep learning manner\", i.e. employing automatic\ndifferentiation, SGD, and GPU support. The resulting models, called RAT-SPNs,\nyield prediction results comparable to deep neural networks, while still being\ninterpretable as generative model and maintaining well-calibrated\nuncertainties. This property makes them highly robust under missing input\nfeatures and enables them to naturally detect outliers and peculiar samples.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 19:44:44 GMT"}, {"version": "v2", "created": "Fri, 22 Jun 2018 14:46:45 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Peharz", "Robert", ""], ["Vergari", "Antonio", ""], ["Stelzner", "Karl", ""], ["Molina", "Alejandro", ""], ["Trapp", "Martin", ""], ["Kersting", "Kristian", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1806.01911", "submitter": "Rakshith Shetty", "authors": "Rakshith Shetty, Mario Fritz, Bernt Schiele", "title": "Adversarial Scene Editing: Automatic Object Removal from Weak\n  Supervision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While great progress has been made recently in automatic image manipulation,\nit has been limited to object centric images like faces or structured scene\ndatasets. In this work, we take a step towards general scene-level image\nediting by developing an automatic interaction-free object removal model. Our\nmodel learns to find and remove objects from general scene images using\nimage-level labels and unpaired data in a generative adversarial network (GAN)\nframework. We achieve this with two key contributions: a two-stage editor\narchitecture consisting of a mask generator and image in-painter that\nco-operate to remove objects, and a novel GAN based prior for the mask\ngenerator that allows us to flexibly incorporate knowledge about object shapes.\nWe experimentally show on two datasets that our method effectively removes a\nwide variety of objects using weak supervision only\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 19:45:20 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Shetty", "Rakshith", ""], ["Fritz", "Mario", ""], ["Schiele", "Bernt", ""]]}, {"id": "1806.01946", "submitter": "Dzmitry Bahdanau", "authors": "Dzmitry Bahdanau, Felix Hill, Jan Leike, Edward Hughes, Arian\n  Hosseini, Pushmeet Kohli, Edward Grefenstette", "title": "Learning to Understand Goal Specifications by Modelling Reward", "comments": "19 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown that deep reinforcement-learning agents can learn to\nfollow language-like instructions from infrequent environment rewards. However,\nthis places on environment designers the onus of designing language-conditional\nreward functions which may not be easily or tractably implemented as the\ncomplexity of the environment and the language scales. To overcome this\nlimitation, we present a framework within which instruction-conditional RL\nagents are trained using rewards obtained not from the environment, but from\nreward models which are jointly trained from expert examples. As reward models\nimprove, they learn to accurately reward agents for completing tasks for\nenvironment configurations---and for instructions---not present amongst the\nexpert data. This framework effectively separates the representation of what\ninstructions require from how they can be executed. In a simple grid world, it\nenables an agent to learn a range of commands requiring interaction with blocks\nand understanding of spatial relations and underspecified abstract\narrangements. We further show the method allows our agent to adapt to changes\nin the environment without requiring new expert examples.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jun 2018 22:01:51 GMT"}, {"version": "v2", "created": "Tue, 2 Oct 2018 13:49:58 GMT"}, {"version": "v3", "created": "Fri, 15 Feb 2019 17:54:56 GMT"}, {"version": "v4", "created": "Mon, 23 Dec 2019 16:41:02 GMT"}], "update_date": "2019-12-24", "authors_parsed": [["Bahdanau", "Dzmitry", ""], ["Hill", "Felix", ""], ["Leike", "Jan", ""], ["Hughes", "Edward", ""], ["Hosseini", "Arian", ""], ["Kohli", "Pushmeet", ""], ["Grefenstette", "Edward", ""]]}, {"id": "1806.01984", "submitter": "Tristan Sylvain", "authors": "Margaux Luck, Tristan Sylvain, Joseph Paul Cohen, Heloise Cardinal,\n  Andrea Lodi, Yoshua Bengio", "title": "Learning to rank for censored survival data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Survival analysis is a type of semi-supervised ranking task where the target\noutput (the survival time) is often right-censored. Utilizing this information\nis a challenge because it is not obvious how to correctly incorporate these\ncensored examples into a model. We study how three categories of loss\nfunctions, namely partial likelihood methods, rank methods, and our\nclassification method based on a Wasserstein metric (WM) and the non-parametric\nKaplan Meier estimate of the probability density to impute the labels of\ncensored examples, can take advantage of this information. The proposed method\nallows us to have a model that predict the probability distribution of an\nevent. If a clinician had access to the detailed probability of an event over\ntime this would help in treatment planning. For example, determining if the\nrisk of kidney graft rejection is constant or peaked after some time. Also, we\ndemonstrate that this approach directly optimizes the expected C-index which is\nthe most common evaluation metric for ranking survival models.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 02:30:00 GMT"}, {"version": "v2", "created": "Fri, 8 Jun 2018 18:55:36 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Luck", "Margaux", ""], ["Sylvain", "Tristan", ""], ["Cohen", "Joseph Paul", ""], ["Cardinal", "Heloise", ""], ["Lodi", "Andrea", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1806.02027", "submitter": "Yi Wu", "authors": "Yi Wu, Siddharth Srivastava, Nicholas Hay, Simon Du, Stuart Russell", "title": "Discrete-Continuous Mixtures in Probabilistic Programming: Generalized\n  Semantics and Inference Algorithms", "comments": "Accepted by ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the recent successes of probabilistic programming languages (PPLs) in\nAI applications, PPLs offer only limited support for random variables whose\ndistributions combine discrete and continuous elements. We develop the notion\nof measure-theoretic Bayesian networks (MTBNs) and use it to provide more\ngeneral semantics for PPLs with arbitrarily many random variables defined over\narbitrary measure spaces. We develop two new general sampling algorithms that\nare provably correct under the MTBN framework: the lexicographic likelihood\nweighting (LLW) for general MTBNs and the lexicographic particle filter (LPF),\na specialized algorithm for state-space models. We further integrate MTBNs into\na widely used PPL system, BLOG, and verify the effectiveness of the new\ninference algorithms through representative examples.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 06:37:46 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 10:32:36 GMT"}, {"version": "v3", "created": "Fri, 8 Jun 2018 05:33:34 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Wu", "Yi", ""], ["Srivastava", "Siddharth", ""], ["Hay", "Nicholas", ""], ["Du", "Simon", ""], ["Russell", "Stuart", ""]]}, {"id": "1806.02056", "submitter": "Farhan Khawar", "authors": "Farhan Khawar, Nevin L. Zhang", "title": "Learning Hierarchical Item Categories from Implicit Feedback Data for\n  Efficient Recommendations and Browsing", "comments": "Published in SIGIR 2019 Workshop on ExplainAble Recommendation and\n  Search (EARS'19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Searching, browsing, and recommendations are common ways in which the \"choice\noverload\" faced by users in the online marketplace can be mitigated. In this\npaper we propose the use of hierarchical item categories, obtained from\nimplicit feedback data, to enable efficient browsing and recommendations. We\npresent a method of creating hierarchical item categories from implicit\nfeedback data only i.e., without any other information on the items like name,\ngenre etc. Categories created in this fashion are based on users'\nco-consumption of items. Thus, they can be more useful for users in finding\ninteresting and relevant items while they are browsing through the hierarchy.\nWe also show that this item hierarchy can be useful in making category based\nrecommendations, which makes the recommendations more explainable and increases\nthe diversity of the recommendations without compromising much on the accuracy.\nItem hierarchy can also be useful in the creation of an automatic item taxonomy\nskeleton by bypassing manual labeling and annotation. This can especially be\nuseful for small vendors. Our data-driven hierarchical categories are based on\nhierarchical latent tree analysis (HLTA) which has been previously used for\ntext analysis. We present a scaled up learning algorithm \\emph{HLTA-Forest} so\nthat HLTA can be applied to implicit feedback data.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 08:25:58 GMT"}, {"version": "v2", "created": "Fri, 21 Jun 2019 07:20:20 GMT"}], "update_date": "2019-06-24", "authors_parsed": [["Khawar", "Farhan", ""], ["Zhang", "Nevin L.", ""]]}, {"id": "1806.02091", "submitter": "Andreas Hein M.", "authors": "Andreas Makoto Hein, H\\'el\\`ene Condat", "title": "Can Machines Design? An Artificial General Intelligence Approach", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.24564.45448", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can machines design? Can they come up with creative solutions to problems and\nbuild tools and artifacts across a wide range of domains? Recent advances in\nthe field of computational creativity and formal Artificial General\nIntelligence (AGI) provide frameworks for machines with the general ability to\ndesign. In this paper we propose to integrate a formal computational creativity\nframework into the G\\\"odel machine framework. We call the resulting framework\ndesign G\\\"odel machine. Such a machine could solve a variety of design problems\nby generating novel concepts. In addition, it could change the way these\nconcepts are generated by modifying itself. The design G\\\"odel machine is able\nto improve its initial design program, once it has proven that a modification\nwould increase its return on the utility function. Finally, we sketch out a\nspecific version of the design G\\\"odel machine which specifically addresses the\ndesign of complex software and hardware systems. Future work aims at the\ndevelopment of a more formal version of the design G\\\"odel machine and a proof\nof concept implementation.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 09:41:58 GMT"}, {"version": "v2", "created": "Thu, 7 Jun 2018 15:24:42 GMT"}, {"version": "v3", "created": "Fri, 22 Jun 2018 22:42:46 GMT"}, {"version": "v4", "created": "Tue, 26 Jun 2018 08:23:40 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Hein", "Andreas Makoto", ""], ["Condat", "H\u00e9l\u00e8ne", ""]]}, {"id": "1806.02127", "submitter": "Lavindra de Silva", "authors": "Lavindra de Silva", "title": "Addendum to \"HTN Acting: A Formalism and an Algorithm\"", "comments": "This paper is a more detailed version of the following publication:\n  Lavindra de Silva, \"HTN Acting: A Formalism and an Algorithm\", in Proceedings\n  of AAMAS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical Task Network (HTN) planning is a practical and efficient\napproach to planning when the 'standard operating procedures' for a domain are\navailable. Like Belief-Desire-Intention (BDI) agent reasoning, HTN planning\nperforms hierarchical and context-based refinement of goals into subgoals and\nbasic actions. However, while HTN planners 'lookahead' over the consequences of\nchoosing one refinement over another, BDI agents interleave refinement with\nacting. There has been renewed interest in making HTN planners behave more like\nBDI agent systems, e.g. to have a unified representation for acting and\nplanning. However, past work on the subject has remained informal or\nimplementation-focused. This paper is a formal account of 'HTN acting', which\nsupports interleaved deliberation, acting, and failure recovery. We use the\nsyntax of the most general HTN planning formalism and build on its core\nsemantics, and we provide an algorithm which combines our new formalism with\nthe processing of exogenous events. We also study the properties of HTN acting\nand its relation to HTN planning.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 11:33:26 GMT"}, {"version": "v2", "created": "Sun, 4 Jul 2021 17:49:20 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["de Silva", "Lavindra", ""]]}, {"id": "1806.02137", "submitter": "Abel Torres-Montoya", "authors": "Abel Torres Montoya", "title": "A New Framework for Machine Intelligence: Concepts and Prototype", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) and artificial intelligence (AI) have become hot topics\nin many information processing areas, from chatbots to scientific data\nanalysis. At the same time, there is uncertainty about the possibility of\nextending predominant ML technologies to become general solutions with\ncontinuous learning capabilities. Here, a simple, yet comprehensive,\ntheoretical framework for intelligent systems is presented. A combination of\nMirror Compositional Representations (MCR) and a Solution-Critic Loop (SCL) is\nproposed as a generic approach for different types of problems. A prototype\nimplementation is presented for document comparison using English Wikipedia\ncorpus.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 12:06:33 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Montoya", "Abel Torres", ""]]}, {"id": "1806.02180", "submitter": "Chun Kit Yeung", "authors": "Chun-Kit Yeung and Dit-Yan Yeung", "title": "Addressing Two Problems in Deep Knowledge Tracing via\n  Prediction-Consistent Regularization", "comments": "10 pages, L@S 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge tracing is one of the key research areas for empowering\npersonalized education. It is a task to model students' mastery level of a\nknowledge component (KC) based on their historical learning trajectories. In\nrecent years, a recurrent neural network model called deep knowledge tracing\n(DKT) has been proposed to handle the knowledge tracing task and literature has\nshown that DKT generally outperforms traditional methods. However, through our\nextensive experimentation, we have noticed two major problems in the DKT model.\nThe first problem is that the model fails to reconstruct the observed input. As\na result, even when a student performs well on a KC, the prediction of that\nKC's mastery level decreases instead, and vice versa. Second, the predicted\nperformance for KCs across time-steps is not consistent. This is undesirable\nand unreasonable because student's performance is expected to transit gradually\nover time. To address these problems, we introduce regularization terms that\ncorrespond to reconstruction and waviness to the loss function of the original\nDKT model to enhance the consistency in prediction. Experiments show that the\nregularized loss function effectively alleviates the two problems without\ndegrading the original task of DKT.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 13:41:48 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Yeung", "Chun-Kit", ""], ["Yeung", "Dit-Yan", ""]]}, {"id": "1806.02215", "submitter": "David Pfau", "authors": "David Pfau, Stig Petersen, Ashish Agarwal, David G. T. Barrett,\n  Kimberly L. Stachenfeld", "title": "Spectral Inference Networks: Unifying Deep and Spectral Learning", "comments": "Fixed typo in math in section 4", "journal-ref": "Seventh International Conference on Learning Representations (ICLR\n  2019)", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Spectral Inference Networks, a framework for learning\neigenfunctions of linear operators by stochastic optimization. Spectral\nInference Networks generalize Slow Feature Analysis to generic symmetric\noperators, and are closely related to Variational Monte Carlo methods from\ncomputational physics. As such, they can be a powerful tool for unsupervised\nrepresentation learning from video or graph-structured data. We cast training\nSpectral Inference Networks as a bilevel optimization problem, which allows for\nonline learning of multiple eigenfunctions. We show results of training\nSpectral Inference Networks on problems in quantum mechanics and feature\nlearning for videos on synthetic datasets. Our results demonstrate that\nSpectral Inference Networks accurately recover eigenfunctions of linear\noperators and can discover interpretable representations from video in a fully\nunsupervised manner.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 14:28:03 GMT"}, {"version": "v2", "created": "Wed, 1 May 2019 18:11:43 GMT"}, {"version": "v3", "created": "Thu, 16 Jan 2020 17:02:46 GMT"}], "update_date": "2020-01-17", "authors_parsed": [["Pfau", "David", ""], ["Petersen", "Stig", ""], ["Agarwal", "Ashish", ""], ["Barrett", "David G. T.", ""], ["Stachenfeld", "Kimberly L.", ""]]}, {"id": "1806.02239", "submitter": "Kuldeep S. Meel", "authors": "Kuldeep S. Meel", "title": "Constrained Counting and Sampling: Bridging the Gap between Theory and\n  Practice", "comments": null, "journal-ref": "PhD Thesis, Rice University, 2018", "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constrained counting and sampling are two fundamental problems in Computer\nScience with numerous applications, including network reliability, privacy,\nprobabilistic reasoning, and constrained-random verification. In constrained\ncounting, the task is to compute the total weight, subject to a given weighting\nfunction, of the set of solutions of the given constraints. In constrained\nsampling, the task is to sample randomly, subject to a given weighting\nfunction, from the set of solutions to a set of given constraints.\nConsequently, constrained counting and sampling have been subject to intense\ntheoretical and empirical investigations over the years. Prior work, however,\noffered either heuristic techniques with poor guarantees of accuracy or\napproaches with proven guarantees but poor performance in practice.\n  In this thesis, we introduce a novel hashing-based algorithmic framework for\nconstrained sampling and counting that combines the classical algorithmic\ntechnique of universal hashing with the dramatic progress made in combinatorial\nreasoning tools, in particular, SAT and SMT, over the past two decades. The\nresulting frameworks for counting (ApproxMC2) and sampling (UniGen) can handle\nformulas with up to million variables representing a significant boost up from\nthe prior state of the art tools' capability to handle few hundreds of\nvariables. If the initial set of constraints is expressed as Disjunctive Normal\nForm (DNF), ApproxMC is the only known Fully Polynomial Randomized\nApproximation Scheme (FPRAS) that does not involve Monte Carlo steps. By\nexploiting the connection between definability of formulas and variance of the\ndistribution of solutions in a cell defined by 3-universal hash functions, we\nintroduced an algorithmic technique, MIS, that reduced the size of XOR\nconstraints employed in the underlying universal hash functions by as much as\ntwo orders of magnitude.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 15:16:32 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Meel", "Kuldeep S.", ""]]}, {"id": "1806.02242", "submitter": "Anne-Francoise Cutting-Decelle", "authors": "A.F. Cutting-Decelle, A. Digeon, R.I. Young, J.L. Barraud, P. Lamboley", "title": "Extraction Of Technical Information From Normative Documents Using\n  Automated Methods Based On Ontologies : Application To The Iso 15531 Mandate\n  Standard - Methodology And First Results", "comments": "28 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Problems faced by international standardization bodies become more and more\ncrucial as the number and the size of the standards they produce increase.\nSometimes, also, the lack of coordination among the committees in charge of the\ndevelopment of standards may lead to overlaps, mistakes or incompatibilities in\nthe documents. The aim of this study is to present a methodology enabling an\nautomatic extraction of the technical concepts (terms) found in normative\ndocuments, through the use of semantic tools coming from the field of language\nprocessing. The first part of the paper provides a description of the\nstandardization world, its structure, its way of working and the problems\nfaced; we then introduce the concepts of semantic annotation, information\nextraction and the software tools available in this domain. The next section\nexplains the concept of ontology and its potential use in the field of\nstandardization. We propose here a methodology enabling the extraction of\ntechnical information from a given normative corpus, based on a semantic\nannotation process done according to reference ontologies. The application to\nthe ISO 15531 MANDATE corpus provides a first use case of the methodology\ndescribed in this paper. The paper ends with the description of the first\nexperimental results produced by this approach, and with some issues and\nperspectives, notably its application to other standards and, or Technical\nCommittees and the possibility offered to create pre-defined technical\ndictionaries of terms.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 15:22:02 GMT"}, {"version": "v2", "created": "Sat, 16 Jun 2018 16:59:27 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Cutting-Decelle", "A. F.", ""], ["Digeon", "A.", ""], ["Young", "R. I.", ""], ["Barraud", "J. L.", ""], ["Lamboley", "P.", ""]]}, {"id": "1806.02256", "submitter": "Liang Tong", "authors": "Liang Tong, Sixie Yu, Scott Alfeld, Yevgeniy Vorobeychik", "title": "Adversarial Regression with Multiple Learners", "comments": "Accepted by ICML'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the considerable success enjoyed by machine learning techniques in\npractice, numerous studies demonstrated that many approaches are vulnerable to\nattacks. An important class of such attacks involves adversaries changing\nfeatures at test time to cause incorrect predictions. Previous investigations\nof this problem pit a single learner against an adversary. However, in many\nsituations an adversary's decision is aimed at a collection of learners, rather\nthan specifically targeted at each independently. We study the problem of\nadversarial linear regression with multiple learners. We approximate the\nresulting game by exhibiting an upper bound on learner loss functions, and show\nthat the resulting game has a unique symmetric equilibrium. We present an\nalgorithm for computing this equilibrium, and show through extensive\nexperiments that equilibrium models are significantly more robust than\nconventional regularized linear regression.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 15:44:53 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Tong", "Liang", ""], ["Yu", "Sixie", ""], ["Alfeld", "Scott", ""], ["Vorobeychik", "Yevgeniy", ""]]}, {"id": "1806.02281", "submitter": "Rohan Ramanath", "authors": "Rohan Ramanath, Gungor Polatkan, Liqin Xu, Harold Lee, Bo Hu, Shan\n  Zhou", "title": "Deploying Deep Ranking Models for Search Verticals", "comments": "Published at the SysML Conference - 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an architecture executing a complex machine\nlearning model such as a neural network capturing semantic similarity between a\nquery and a document; and deploy to a real-world production system serving\n500M+users. We present the challenges that arise in a real-world system and how\nwe solve them. We demonstrate that our architecture provides competitive\nmodeling capability without any significant performance impact to the system in\nterms of latency. Our modular solution and insights can be used by other\nreal-world search systems to realize and productionize recent gains in neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 16:20:24 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Ramanath", "Rohan", ""], ["Polatkan", "Gungor", ""], ["Xu", "Liqin", ""], ["Lee", "Harold", ""], ["Hu", "Bo", ""], ["Zhou", "Shan", ""]]}, {"id": "1806.02308", "submitter": "Hector Geffner", "authors": "Hector Geffner", "title": "Model-free, Model-based, and General Intelligence", "comments": null, "journal-ref": "Invited talk. IJCAI 2018", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  During the 60s and 70s, AI researchers explored intuitions about intelligence\nby writing programs that displayed intelligent behavior. Many good ideas came\nout from this work but programs written by hand were not robust or general.\nAfter the 80s, research increasingly shifted to the development of learners\ncapable of inferring behavior and functions from experience and data, and\nsolvers capable of tackling well-defined but intractable models like SAT,\nclassical planning, Bayesian networks, and POMDPs. The learning approach has\nachieved considerable success but results in black boxes that do not have the\nflexibility, transparency, and generality of their model-based counterparts.\nModel-based approaches, on the other hand, require models and scalable\nalgorithms. Model-free learners and model-based solvers have close parallels\nwith Systems 1 and 2 in current theories of the human mind: the first, a fast,\nopaque, and inflexible intuitive mind; the second, a slow, transparent, and\nflexible analytical mind. In this paper, I review developments in AI and draw\non these theories to discuss the gap between model-free learners and\nmodel-based solvers, a gap that needs to be bridged in order to have\nintelligent systems that are robust and general.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 17:15:27 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Geffner", "Hector", ""]]}, {"id": "1806.02311", "submitter": "Youssef Alami Mejjati", "authors": "Youssef A. Mejjati and Christian Richardt and James Tompkin and Darren\n  Cosker and Kwang In Kim", "title": "Unsupervised Attention-guided Image to Image Translation", "comments": null, "journal-ref": "NIPS 2018", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current unsupervised image-to-image translation techniques struggle to focus\ntheir attention on individual objects without altering the background or the\nway multiple objects interact within a scene. Motivated by the important role\nof attention in human perception, we tackle this limitation by introducing\nunsupervised attention mechanisms that are jointly adversarialy trained with\nthe generators and discriminators. We demonstrate qualitatively and\nquantitatively that our approach is able to attend to relevant regions in the\nimage without requiring supervision, and that by doing so it achieves more\nrealistic mappings compared to recent approaches.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 17:21:38 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 21:13:13 GMT"}, {"version": "v3", "created": "Thu, 8 Nov 2018 14:43:37 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Mejjati", "Youssef A.", ""], ["Richardt", "Christian", ""], ["Tompkin", "James", ""], ["Cosker", "Darren", ""], ["Kim", "Kwang In", ""]]}, {"id": "1806.02322", "submitter": "Hadi Ghauch", "authors": "Hadi Ghauch, Mikael Skoglund, Hossein Shokri-Ghadikolaei, Carlo\n  Fischione, Ali H. Sayed", "title": "Learning Kolmogorov Models for Binary Random Variables", "comments": "9 pages, accecpted to ICML 2018: Workshop on Nonconvex Optimization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We summarize our recent findings, where we proposed a framework for learning\na Kolmogorov model, for a collection of binary random variables. More\nspecifically, we derive conditions that link outcomes of specific random\nvariables, and extract valuable relations from the data. We also propose an\nalgorithm for computing the model and show its first-order optimality, despite\nthe combinatorial nature of the learning problem. We apply the proposed\nalgorithm to recommendation systems, although it is applicable to other\nscenarios. We believe that the work is a significant step toward interpretable\nmachine learning.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 17:39:00 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Ghauch", "Hadi", ""], ["Skoglund", "Mikael", ""], ["Shokri-Ghadikolaei", "Hossein", ""], ["Fischione", "Carlo", ""], ["Sayed", "Ali H.", ""]]}, {"id": "1806.02336", "submitter": "Naoyuki Ichimura Dr.", "authors": "Naoyuki Ichimura", "title": "Spatial Frequency Loss for Learning Convolutional Autoencoders", "comments": "9 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a learning method for convolutional autoencoders (CAEs)\nfor extracting features from images. CAEs can be obtained by utilizing\nconvolutional neural networks to learn an approximation to the identity\nfunction in an unsupervised manner. The loss function based on the pixel loss\n(PL) that is the mean squared error between the pixel values of original and\nreconstructed images is the common choice for learning. However, using the loss\nfunction leads to blurred reconstructed images. A method for learning CAEs\nusing a loss function computed from features reflecting spatial frequencies is\nproposed to mitigate the problem. The blurs in reconstructed images show lack\nof high spatial frequency components mainly constituting edges and detailed\ntextures that are important features for tasks such as object detection and\nspatial matching. In order to evaluate the lack of components, a convolutional\nlayer with a Laplacian filter bank as weights is added to CAEs and the mean\nsquared error of features in a subband, called the spatial frequency loss\n(SFL), is computed from the outputs of each filter. The learning is performed\nusing a loss function based on the SFL. Empirical evaluation demonstrates that\nusing the SFL reduces the blurs in reconstructed images.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 08:34:12 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Ichimura", "Naoyuki", ""]]}, {"id": "1806.02373", "submitter": "Mieczys{\\l}aw K{\\l}opotek", "authors": "Mieczys{\\l}aw A. K{\\l}opotek", "title": "Dempsterian-Shaferian Belief Network From Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shenoy and Shafer {Shenoy:90} demonstrated that both for Dempster-Shafer\nTheory and probability theory there exists a possibility to calculate\nefficiently marginals of joint belief distributions (by so-called local\ncomputations) provided that the joint distribution can be decomposed\n(factorized) into a belief network. A number of algorithms exists for\ndecomposition of probabilistic joint belief distribution into a bayesian\n(belief) network from data. For example\n  Spirtes, Glymour and Schein{Spirtes:90b} formulated a Conjecture that a\ndirect dependence test and a head-to-head meeting test would suffice to\nconstrue bayesian network from data in such a way that Pearl's concept of\nd-separation {Geiger:90} applies.\n  This paper is intended to transfer Spirtes, Glymour and Scheines\n{Spirtes:90b} approach onto the ground of the Dempster-Shafer Theory (DST). For\nthis purpose, a frequentionistic interpretation of the DST developed in\n{Klopotek:93b} is exploited. A special notion of conditionality for DST is\nintroduced and demonstrated to behave with respect to Pearl's d-separation\n{Geiger:90} much the same way as conditional probability (though some\ndifferences like non-uniqueness are evident). Based on this, an algorithm\nanalogous to that from {Spirtes:90b} is developed.\n  The notion of a partially oriented graph (pog) is introduced and within this\ngraph the notion of p-d-separation is defined. If direct dependence test and\nhead-to-head meeting test are used to orient the pog then its p-d-separation is\nshown to be equivalent to the Pearl's d-separation for any compatible dag.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 18:27:39 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["K\u0142opotek", "Mieczys\u0142aw A.", ""]]}, {"id": "1806.02375", "submitter": "Johan Bjorck", "authors": "Johan Bjorck, Carla Gomes, Bart Selman, Kilian Q. Weinberger", "title": "Understanding Batch Normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Batch normalization (BN) is a technique to normalize activations in\nintermediate layers of deep neural networks. Its tendency to improve accuracy\nand speed up training have established BN as a favorite technique in deep\nlearning. Yet, despite its enormous success, there remains little consensus on\nthe exact reason and mechanism behind these improvements. In this paper we take\na step towards a better understanding of BN, following an empirical approach.\nWe conduct several experiments, and show that BN primarily enables training\nwith larger learning rates, which is the cause for faster convergence and\nbetter generalization. For networks without BN we demonstrate how large\ngradient updates can result in diverging loss and activations growing\nuncontrollably with network depth, which limits possible learning rates. BN\navoids this problem by constantly correcting activations to be zero-mean and of\nunit standard deviation, which enables larger gradient steps, yields faster\nconvergence and may help bypass sharp local minima. We further show various\nways in which gradients and activations of deep unnormalized networks are\nill-behaved. We contrast our results against recent findings in random matrix\ntheory, shedding new light on classical initialization schemes and their\nconsequences.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 03:57:56 GMT"}, {"version": "v2", "created": "Mon, 6 Aug 2018 15:12:45 GMT"}, {"version": "v3", "created": "Sun, 16 Sep 2018 23:22:43 GMT"}, {"version": "v4", "created": "Fri, 30 Nov 2018 05:56:20 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Bjorck", "Johan", ""], ["Gomes", "Carla", ""], ["Selman", "Bart", ""], ["Weinberger", "Kilian Q.", ""]]}, {"id": "1806.02380", "submitter": "Matthew Kusner", "authors": "Matt J. Kusner, Chris Russell, Joshua R. Loftus, Ricardo Silva", "title": "Causal Interventions for Fairness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most approaches in algorithmic fairness constrain machine learning methods so\nthe resulting predictions satisfy one of several intuitive notions of fairness.\nWhile this may help private companies comply with non-discrimination laws or\navoid negative publicity, we believe it is often too little, too late. By the\ntime the training data is collected, individuals in disadvantaged groups have\nalready suffered from discrimination and lost opportunities due to factors out\nof their control. In the present work we focus instead on interventions such as\na new public policy, and in particular, how to maximize their positive effects\nwhile improving the fairness of the overall system. We use causal methods to\nmodel the effects of interventions, allowing for potential interference--each\nindividual's outcome may depend on who else receives the intervention. We\ndemonstrate this with an example of allocating a budget of teaching resources\nusing a dataset of schools in New York City.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 18:46:11 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Kusner", "Matt J.", ""], ["Russell", "Chris", ""], ["Loftus", "Joshua R.", ""], ["Silva", "Ricardo", ""]]}, {"id": "1806.02415", "submitter": "Cheol Young Park", "authors": "Cheol Young Park, Kathryn Blackmond Laskey, Paulo C. G. Costa, Shou\n  Matsumoto", "title": "Gaussian Mixture Reduction for Time-Constrained Approximate Inference in\n  Hybrid Bayesian Networks", "comments": null, "journal-ref": "Appl. Sci. 2019, 9, 2055", "doi": "10.3390/app9102055", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hybrid Bayesian Networks (HBNs), which contain both discrete and continuous\nvariables, arise naturally in many application areas (e.g., image\nunderstanding, data fusion, medical diagnosis, fraud detection). This paper\nconcerns inference in an important subclass of HBNs, the conditional Gaussian\n(CG) networks, in which all continuous random variables have Gaussian\ndistributions and all children of continuous random variables must be\ncontinuous. Inference in CG networks can be NP-hard even for special-case\nstructures, such as poly-trees, where inference in discrete Bayesian networks\ncan be performed in polynomial time. Therefore, approximate inference is\nrequired. In approximate inference, it is often necessary to trade off accuracy\nagainst solution time. This paper presents an extension to the Hybrid Message\nPassing inference algorithm for general CG networks and an algorithm for\noptimizing its accuracy given a bound on computation time. The extended\nalgorithm uses Gaussian mixture reduction to prevent an exponential increase in\nthe number of Gaussian mixture components. The trade-off algorithm performs\npre-processing to find optimal run-time settings for the extended algorithm.\nExperimental results for four CG networks compare performance of the extended\nalgorithm with existing algorithms and show the optimal settings for these CG\nnetworks.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 20:38:27 GMT"}], "update_date": "2019-05-21", "authors_parsed": [["Park", "Cheol Young", ""], ["Laskey", "Kathryn Blackmond", ""], ["Costa", "Paulo C. G.", ""], ["Matsumoto", "Shou", ""]]}, {"id": "1806.02421", "submitter": "Cheol Young Park", "authors": "Cheol Young Park, Kathryn Blackmond Laskey", "title": "Human-aided Multi-Entity Bayesian Networks Learning from Relational Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An Artificial Intelligence (AI) system is an autonomous system which emulates\nhuman mental and physical activities such as Observe, Orient, Decide, and Act,\ncalled the OODA process. An AI system performing the OODA process requires a\nsemantically rich representation to handle a complex real world situation and\nability to reason under uncertainty about the situation. Multi-Entity Bayesian\nNetworks (MEBNs) combines First-Order Logic with Bayesian Networks for\nrepresenting and reasoning about uncertainty in complex, knowledge-rich\ndomains. MEBN goes beyond standard Bayesian networks to enable reasoning about\nan unknown number of entities interacting with each other in various types of\nrelationships, a key requirement for the OODA process of an AI system. MEBN\nmodels have heretofore been constructed manually by a domain expert. However,\nmanual MEBN modeling is labor-intensive and insufficiently agile. To address\nthese problems, an efficient method is needed for MEBN modeling. One of the\nmethods is to use machine learning to learn a MEBN model in whole or in part\nfrom data. In the era of Big Data, data-rich environments, characterized by\nuncertainty and complexity, have become ubiquitous. The larger the data sample\nis, the more accurate the results of the machine learning approach can be.\nTherefore, machine learning has potential to improve the quality of MEBN models\nas well as the effectiveness for MEBN modeling. In this research, we study a\nMEBN learning framework to develop a MEBN model from a combination of domain\nexpert's knowledge and data. To evaluate the MEBN learning framework, we\nconduct an experiment to compare the MEBN learning framework and the existing\nmanual MEBN modeling in terms of development efficiency.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 20:51:06 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Park", "Cheol Young", ""], ["Laskey", "Kathryn Blackmond", ""]]}, {"id": "1806.02448", "submitter": "Philip Bontrager", "authors": "Ruben Rodriguez Torrado, Philip Bontrager, Julian Togelius, Jialin\n  Liu, Diego Perez-Liebana", "title": "Deep Reinforcement Learning for General Video Game AI", "comments": "8 pages, 4 figures, Accepted at the conference on Computational\n  Intelligence and Games 2018 IEEE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The General Video Game AI (GVGAI) competition and its associated software\nframework provides a way of benchmarking AI algorithms on a large number of\ngames written in a domain-specific description language. While the competition\nhas seen plenty of interest, it has so far focused on online planning,\nproviding a forward model that allows the use of algorithms such as Monte Carlo\nTree Search.\n  In this paper, we describe how we interface GVGAI to the OpenAI Gym\nenvironment, a widely used way of connecting agents to reinforcement learning\nproblems. Using this interface, we characterize how widely used implementations\nof several deep reinforcement learning algorithms fare on a number of GVGAI\ngames. We further analyze the results to provide a first indication of the\nrelative difficulty of these games relative to each other, and relative to\nthose in the Arcade Learning Environment under similar conditions.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 22:39:26 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Torrado", "Ruben Rodriguez", ""], ["Bontrager", "Philip", ""], ["Togelius", "Julian", ""], ["Liu", "Jialin", ""], ["Perez-Liebana", "Diego", ""]]}, {"id": "1806.02457", "submitter": "Cheol Young Park", "authors": "Cheol Young Park and Kathryn Blackmond Laskey", "title": "Reference Model of Multi-Entity Bayesian Networks for Predictive\n  Situation Awareness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the past quarter-century, situation awareness (SAW) has become a\ncritical research theme, because of its importance. Since the concept of SAW\nwas first introduced during World War I, various versions of SAW have been\nresearched and introduced. Predictive Situation Awareness (PSAW) focuses on the\nability to predict aspects of a temporally evolving situation over time. PSAW\nrequires a formal representation and a reasoning method using such a\nrepresentation. A Multi-Entity Bayesian Network (MEBN) is a knowledge\nrepresentation formalism combining Bayesian Networks (BN) with First-Order\nLogic (FOL). MEBN can be used to represent uncertain situations (supported by\nBN) as well as complex situations (supported by FOL). Also, efficient reasoning\nalgorithms for MEBN have been developed. MEBN can be a formal representation to\nsupport PSAW and has been used for several PSAW systems. Although several MEBN\napplications for PSAW exist, very little work can be found in the literature\nthat attempts to generalize a MEBN model to support PSAW. In this research, we\ndefine a reference model for MEBN in PSAW, called a PSAW-MEBN reference model.\nThe PSAW-MEBN reference model enables us to easily develop a MEBN model for\nPSAW by supporting the design of a MEBN model for PSAW. In this research, we\nintroduce two example use cases using the PSAW-MEBN reference model to develop\nMEBN models to support PSAW: a Smart Manufacturing System and a Maritime Domain\nAwareness System.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 23:17:12 GMT"}, {"version": "v2", "created": "Fri, 8 Jun 2018 00:37:20 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Park", "Cheol Young", ""], ["Laskey", "Kathryn Blackmond", ""]]}, {"id": "1806.02473", "submitter": "Jiaxuan You", "authors": "Jiaxuan You, Bowen Liu, Rex Ying, Vijay Pande, Jure Leskovec", "title": "Graph Convolutional Policy Network for Goal-Directed Molecular Graph\n  Generation", "comments": "NeurIPS 2018, spotlight presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating novel graph structures that optimize given objectives while\nobeying some given underlying rules is fundamental for chemistry, biology and\nsocial science research. This is especially important in the task of molecular\ngraph generation, whose goal is to discover novel molecules with desired\nproperties such as drug-likeness and synthetic accessibility, while obeying\nphysical laws such as chemical valency. However, designing models to find\nmolecules that optimize desired properties while incorporating highly complex\nand non-differentiable rules remains to be a challenging task. Here we propose\nGraph Convolutional Policy Network (GCPN), a general graph convolutional\nnetwork based model for goal-directed graph generation through reinforcement\nlearning. The model is trained to optimize domain-specific rewards and\nadversarial loss through policy gradient, and acts in an environment that\nincorporates domain-specific rules. Experimental results show that GCPN can\nachieve 61% improvement on chemical property optimization over state-of-the-art\nbaselines while resembling known molecules, and achieve 184% improvement on the\nconstrained property optimization task.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 00:47:09 GMT"}, {"version": "v2", "created": "Sun, 18 Nov 2018 01:17:56 GMT"}, {"version": "v3", "created": "Mon, 25 Feb 2019 03:19:38 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["You", "Jiaxuan", ""], ["Liu", "Bowen", ""], ["Ying", "Rex", ""], ["Pande", "Vijay", ""], ["Leskovec", "Jure", ""]]}, {"id": "1806.02501", "submitter": "Ellis Ratner", "authors": "Ellis Ratner, Dylan Hadfield-Menell, Anca D. Dragan", "title": "Simplifying Reward Design through Divide-and-Conquer", "comments": "Robotics: Science and Systems (RSS) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing a good reward function is essential to robot planning and\nreinforcement learning, but it can also be challenging and frustrating. The\nreward needs to work across multiple different environments, and that often\nrequires many iterations of tuning. We introduce a novel divide-and-conquer\napproach that enables the designer to specify a reward separately for each\nenvironment. By treating these separate reward functions as observations about\nthe underlying true reward, we derive an approach to infer a common reward\nacross all environments. We conduct user studies in an abstract grid world\ndomain and in a motion planning domain for a 7-DOF manipulator that measure\nuser effort and solution quality. We show that our method is faster, easier to\nuse, and produces a higher quality solution than the typical method of\ndesigning a reward jointly across all environments. We additionally conduct a\nseries of experiments that measure the sensitivity of these results to\ndifferent properties of the reward design task, such as the number of\nenvironments, the number of feasible solutions per environment, and the\nfraction of the total features that vary within each environment. We find that\nindependent reward design outperforms the standard, joint, reward design\nprocess but works best when the design problem can be divided into simpler\nsubproblems.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 03:49:05 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Ratner", "Ellis", ""], ["Hadfield-Menell", "Dylan", ""], ["Dragan", "Anca D.", ""]]}, {"id": "1806.02508", "submitter": "Chen Chen", "authors": "Chen Chen, Qizhen Weng, Wei Wang, Baochun Li, Bo Li", "title": "Semi-Dynamic Load Balancing: Efficient Distributed Learning in\n  Non-Dedicated Environments", "comments": null, "journal-ref": null, "doi": "10.1145/3419111.3421299", "report-no": null, "categories": "cs.DC cs.AI cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) models are increasingly trained in clusters with\nnon-dedicated workers possessing heterogeneous resources. In such scenarios,\nmodel training efficiency can be negatively affected by stragglers -- workers\nthat run much slower than others. Efficient model training requires eliminating\nsuch stragglers, yet for modern ML workloads, existing load balancing\nstrategies are inefficient and even infeasible. In this paper, we propose a\nnovel strategy called semi-dynamic load balancing to eliminate stragglers of\ndistributed ML workloads. The key insight is that ML workers shall be\nload-balanced at iteration boundaries, being non-intrusive to intra-iteration\nexecution. We develop LB-BSP based on such an insight, which is an integrated\nworker coordination mechanism that adapts workers' load to their instantaneous\nprocessing capabilities by right-sizing the sample batches at the\nsynchronization barriers. We have custom-designed the batch sizing algorithm\nrespectively for CPU and GPU clusters based on their own characteristics.\nLB-BSP has been implemented as a Python module for ML frameworks like\nTensorFlow and PyTorch. Our EC2 deployment confirms that LB-BSP is practical,\neffective and light-weight, and is able to accelerating distributed training by\nup to $54\\%$.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 04:15:58 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2020 14:37:47 GMT"}], "update_date": "2020-12-09", "authors_parsed": [["Chen", "Chen", ""], ["Weng", "Qizhen", ""], ["Wang", "Wei", ""], ["Li", "Baochun", ""], ["Li", "Bo", ""]]}, {"id": "1806.02510", "submitter": "Alexandre Maurer", "authors": "El Mahdi El Mhamdi, Rachid Guerraoui, L\\^e Nguy\\^en Hoang, Alexandre\n  Maurer", "title": "Removing Algorithmic Discrimination (With Minimal Individual Error)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of correcting group discriminations within a score\nfunction, while minimizing the individual error. Each group is described by a\nprobability density function on the set of profiles. We first solve the problem\nanalytically in the case of two populations, with a uniform bonus-malus on the\nzones where each population is a majority. We then address the general case of\nn populations, where the entanglement of populations does not allow a similar\nanalytical solution. We show that an approximate solution with an arbitrarily\nhigh level of precision can be computed with linear programming. Finally, we\naddress the inverse problem where the error should not go beyond a certain\nvalue and we seek to minimize the discrimination.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 04:49:46 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Mhamdi", "El Mahdi El", ""], ["Guerraoui", "Rachid", ""], ["Hoang", "L\u00ea Nguy\u00ean", ""], ["Maurer", "Alexandre", ""]]}, {"id": "1806.02566", "submitter": "Jiaqi Li", "authors": "Jiaqi Li and Zhifeng Zhao and Rongpeng Li and Honggang Zhang", "title": "AI-based Two-Stage Intrusion Detection for Software Defined IoT Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software Defined Internet of Things (SD-IoT) Networks profits from\ncentralized management and interactive resource sharing which enhances the\nefficiency and scalability of IoT applications. But with the rapid growth in\nservices and applications, it is vulnerable to possible attacks and faces\nsevere security challenges. Intrusion detection has been widely used to ensure\nnetwork security, but classical detection means are usually signature-based or\nexplicit-behavior-based and fail to detect unknown attacks intelligently, which\nare hard to satisfy the requirements of SD-IoT Networks. In this paper, we\npropose an AI-based two-stage intrusion detection empowered by software defined\ntechnology. It flexibly captures network flows with a globle view and detects\nattacks intelligently through applying AI algorithms. We firstly leverage Bat\nalgorithm with swarm division and Differential Mutation to select typical\nfeatures. Then, we exploit Random forest through adaptively altering the\nweights of samples using weighted voting mechanism to classify flows.\nEvaluation results prove that the modified intelligent algorithms select more\nimportant features and achieve superior performance in flow classification. It\nis also verified that intelligent intrusion detection shows better accuracy\nwith lower overhead comparied with existing solutions.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 08:57:44 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Li", "Jiaqi", ""], ["Zhao", "Zhifeng", ""], ["Li", "Rongpeng", ""], ["Zhang", "Honggang", ""]]}, {"id": "1806.02577", "submitter": "Roberta Calegari", "authors": "Roberta Calegari, Enrico Denti, Stefano Mariani, Andrea Omicini", "title": "Logic Programming as a Service", "comments": null, "journal-ref": "CALEGARI, R., DENTI, E., MARIANI, S., & OMICINI, A. (2018). Logic\n  programming as a service. Theory and Practice of Logic Programming, 18(5-6),\n  846-873", "doi": "10.1017/S1471068418000364", "report-no": null, "categories": "cs.AI cs.PL cs.SE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  New generations of distributed systems are opening novel perspectives for\nlogic programming (LP): on the one hand, service-oriented architectures\nrepresent nowadays the standard approach for distributed systems engineering;\non the other hand, pervasive systems mandate for situated intelligence. In this\npaper we introduce the notion of Logic Programming as a Service (LPaaS) as a\nmeans to address the needs of pervasive intelligent systems through logic\nengines exploited as a distributed service. First we define the abstract\narchitectural model by re-interpreting classical LP notions in the new context;\nthen we elaborate on the nature of LP interpreted as a service by describing\nthe basic LPaaS interface. Finally, we show how LPaaS works in practice by\ndiscussing its implementation in terms of distributed tuProlog engines,\naccounting for basic issues such as interoperability and configurability.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 09:24:01 GMT"}, {"version": "v2", "created": "Tue, 25 Sep 2018 12:27:36 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Calegari", "Roberta", ""], ["Denti", "Enrico", ""], ["Mariani", "Stefano", ""], ["Omicini", "Andrea", ""]]}, {"id": "1806.02623", "submitter": "Jie Zhang", "authors": "Jie Zhang and Yan Wang and Jie Tang and Ming Ding", "title": "Spectral Network Embedding: A Fast and Scalable Method via Sparsity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network embedding aims to learn low-dimensional representations of nodes in a\nnetwork, while the network structure and inherent properties are preserved. It\nhas attracted tremendous attention recently due to significant progress in\ndownstream network learning tasks, such as node classification, link\nprediction, and visualization. However, most existing network embedding methods\nsuffer from the expensive computations due to the large volume of networks. In\nthis paper, we propose a $10\\times \\sim 100\\times$ faster network embedding\nmethod, called Progle, by elegantly utilizing the sparsity property of online\nnetworks and spectral analysis. In Progle, we first construct a \\textit{sparse}\nproximity matrix and train the network embedding efficiently via sparse matrix\ndecomposition. Then we introduce a network propagation pattern via spectral\nanalysis to incorporate local and global structure information into the\nembedding. Besides, this model can be generalized to integrate network\ninformation into other insufficiently trained embeddings at speed. Benefiting\nfrom sparse spectral network embedding, our experiment on four different\ndatasets shows that Progle outperforms or is comparable to state-of-the-art\nunsupervised comparison approaches---DeepWalk, LINE, node2vec, GraRep, and\nHOPE, regarding accuracy, while is $10\\times$ faster than the fastest\nword2vec-based method. Finally, we validate the scalability of Progle both in\nreal large-scale networks and multiple scales of synthetic networks.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 11:38:34 GMT"}, {"version": "v2", "created": "Wed, 13 Jun 2018 15:00:39 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Zhang", "Jie", ""], ["Wang", "Yan", ""], ["Tang", "Jie", ""], ["Ding", "Ming", ""]]}, {"id": "1806.02639", "submitter": "Han Cai", "authors": "Han Cai, Jiacheng Yang, Weinan Zhang, Song Han, Yong Yu", "title": "Path-Level Network Transformation for Efficient Architecture Search", "comments": "ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a new function-preserving transformation for efficient neural\narchitecture search. This network transformation allows reusing previously\ntrained networks and existing successful architectures that improves sample\nefficiency. We aim to address the limitation of current network transformation\noperations that can only perform layer-level architecture modifications, such\nas adding (pruning) filters or inserting (removing) a layer, which fails to\nchange the topology of connection paths. Our proposed path-level transformation\noperations enable the meta-controller to modify the path topology of the given\nnetwork while keeping the merits of reusing weights, and thus allow efficiently\ndesigning effective structures with complex path topologies like Inception\nmodels. We further propose a bidirectional tree-structured reinforcement\nlearning meta-controller to explore a simple yet highly expressive\ntree-structured architecture space that can be viewed as a generalization of\nmulti-branch architectures. We experimented on the image classification\ndatasets with limited computational resources (about 200 GPU-hours), where we\nobserved improved parameter efficiency and better test results (97.70% test\naccuracy on CIFAR-10 with 14.3M parameters and 74.6% top-1 accuracy on ImageNet\nin the mobile setting), demonstrating the effectiveness and transferability of\nour designed architectures.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 12:25:05 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Cai", "Han", ""], ["Yang", "Jiacheng", ""], ["Zhang", "Weinan", ""], ["Han", "Song", ""], ["Yu", "Yong", ""]]}, {"id": "1806.02664", "submitter": "Yuval Atzmon", "authors": "Yuval Atzmon and Gal Chechik", "title": "Probabilistic AND-OR Attribute Grouping for Zero-Shot Learning", "comments": "Accepted to the Conference on Uncertainty in Artificial Intelligence\n  (UAI), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In zero-shot learning (ZSL), a classifier is trained to recognize visual\nclasses without any image samples. Instead, it is given semantic information\nabout the class, like a textual description or a set of attributes. Learning\nfrom attributes could benefit from explicitly modeling structure of the\nattribute space. Unfortunately, learning of general structure from empirical\nsamples is hard with typical dataset sizes.\n  Here we describe LAGO, a probabilistic model designed to capture natural soft\nand-or relations across groups of attributes. We show how this model can be\nlearned end-to-end with a deep attribute-detection model. The soft group\nstructure can be learned from data jointly as part of the model, and can also\nreadily incorporate prior knowledge about groups if available. The soft and-or\nstructure succeeds to capture meaningful and predictive structures, improving\nthe accuracy of zero-shot learning on two of three benchmarks.\n  Finally, LAGO reveals a unified formulation over two ZSL approaches: DAP\n(Lampert et al., 2009) and ESZSL (Romera-Paredes & Torr, 2015). Interestingly,\ntaking only one singleton group for each attribute, introduces a new\nsoft-relaxation of DAP, that outperforms DAP by ~40.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 13:25:01 GMT"}, {"version": "v2", "created": "Sat, 7 Jul 2018 12:32:48 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Atzmon", "Yuval", ""], ["Chechik", "Gal", ""]]}, {"id": "1806.02711", "submitter": "Bogdan Kulynych", "authors": "Bogdan Kulynych, Rebekah Overdorf, Carmela Troncoso, Seda G\\\"urses", "title": "POTs: Protective Optimization Technologies", "comments": "Appears in Conference on Fairness, Accountability, and Transparency\n  (FAT* 2020). Bogdan Kulynych and Rebekah Overdorf contributed equally to this\n  work. Version v1/v2 by Seda G\\\"urses, Rebekah Overdorf, and Ero Balsa was\n  presented at HotPETS 2018 and at PiMLAI 2018", "journal-ref": null, "doi": "10.1145/3351095.3372853", "report-no": null, "categories": "cs.CY cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithmic fairness aims to address the economic, moral, social, and\npolitical impact that digital systems have on populations through solutions\nthat can be applied by service providers. Fairness frameworks do so, in part,\nby mapping these problems to a narrow definition and assuming the service\nproviders can be trusted to deploy countermeasures. Not surprisingly, these\ndecisions limit fairness frameworks' ability to capture a variety of harms\ncaused by systems.\n  We characterize fairness limitations using concepts from requirements\nengineering and from social sciences. We show that the focus on algorithms'\ninputs and outputs misses harms that arise from systems interacting with the\nworld; that the focus on bias and discrimination omits broader harms on\npopulations and their environments; and that relying on service providers\nexcludes scenarios where they are not cooperative or intentionally adversarial.\n  We propose Protective Optimization Technologies (POTs). POTs provide means\nfor affected parties to address the negative impacts of systems in the\nenvironment, expanding avenues for political contestation. POTs intervene from\noutside the system, do not require service providers to cooperate, and can\nserve to correct, shift, or expose harms that systems impose on populations and\ntheir environments. We illustrate the potential and limitations of POTs in two\ncase studies: countering road congestion caused by traffic-beating\napplications, and recalibrating credit scoring for loan applicants.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 14:41:32 GMT"}, {"version": "v2", "created": "Mon, 18 Jun 2018 14:46:42 GMT"}, {"version": "v3", "created": "Thu, 30 Aug 2018 12:24:14 GMT"}, {"version": "v4", "created": "Wed, 18 Dec 2019 17:07:17 GMT"}, {"version": "v5", "created": "Thu, 19 Dec 2019 09:40:31 GMT"}, {"version": "v6", "created": "Sun, 26 Jan 2020 10:37:31 GMT"}], "update_date": "2020-01-28", "authors_parsed": [["Kulynych", "Bogdan", ""], ["Overdorf", "Rebekah", ""], ["Troncoso", "Carmela", ""], ["G\u00fcrses", "Seda", ""]]}, {"id": "1806.02739", "submitter": "Alban Laflaqui\\`ere Dr", "authors": "Alban Laflaqui\\`ere, J.Kevin O'Regan, Bruno Gas, Alexander Terekhov", "title": "Discovering space - Grounding spatial topology and metric regularity in\n  a naive agent's sensorimotor experience", "comments": "59 pages, 16 figures, submitted to Neural Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In line with the sensorimotor contingency theory, we investigate the problem\nof the perception of space from a fundamental sensorimotor perspective. Despite\nits pervasive nature in our perception of the world, the origin of the concept\nof space remains largely mysterious. For example in the context of artificial\nperception, this issue is usually circumvented by having engineers pre-define\nthe spatial structure of the problem the agent has to face. We here show that\nthe structure of space can be autonomously discovered by a naive agent in the\nform of sensorimotor regularities, that correspond to so called compensable\nsensory experiences: these are experiences that can be generated either by the\nagent or its environment. By detecting such compensable experiences the agent\ncan infer the topological and metric structure of the external space in which\nits body is moving. We propose a theoretical description of the nature of these\nregularities and illustrate the approach on a simulated robotic arm equipped\nwith an eye-like sensor, and which interacts with an object. Finally we show\nhow these regularities can be used to build an internal representation of the\nsensor's external spatial configuration.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 15:51:21 GMT"}, {"version": "v2", "created": "Wed, 3 Oct 2018 11:47:51 GMT"}], "update_date": "2018-10-04", "authors_parsed": [["Laflaqui\u00e8re", "Alban", ""], ["O'Regan", "J. Kevin", ""], ["Gas", "Bruno", ""], ["Terekhov", "Alexander", ""]]}, {"id": "1806.02794", "submitter": "Joseph Pfeiffer", "authors": "Elon Portugaly, Joseph J. Pfeiffer III", "title": "Unbiased Estimation of the Value of an Optimized Policy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized trials, also known as A/B tests, are used to select between two\npolicies: a control and a treatment. Given a corresponding set of features, we\ncan ideally learn an optimized policy P that maps the A/B test data features to\naction space and optimizes reward. However, although A/B testing provides an\nunbiased estimator for the value of deploying B (i.e., switching from policy A\nto B), direct application of those samples to learn the the optimized policy P\ngenerally does not provide an unbiased estimator of the value of P as the\nsamples were observed when constructing P. In situations where the cost and\nrisks associated of deploying a policy are high, such an unbiased estimator is\nhighly desirable.\n  We present a procedure for learning optimized policies and getting unbiased\nestimates for the value of deploying them. We wrap any policy learning\nprocedure with a bagging process and obtain out-of-bag policy inclusion\ndecisions for each sample. We then prove that inverse-propensity-weighting\neffect estimator is unbiased when applied to the optimized subset. Likewise, we\napply the same idea to obtain out-of-bag unbiased per-sample value estimate of\nthe measurement that is independent of the randomized treatment, and use these\nestimates to build an unbiased doubly-robust effect estimator. Lastly, we\nempirically shown that even when the average treatment effect is negative we\ncan find a positive optimized policy.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 17:12:50 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Portugaly", "Elon", ""], ["Pfeiffer", "Joseph J.", "III"]]}, {"id": "1806.02813", "submitter": "John Co-Reyes", "authors": "John D. Co-Reyes, YuXuan Liu, Abhishek Gupta, Benjamin Eysenbach,\n  Pieter Abbeel, Sergey Levine", "title": "Self-Consistent Trajectory Autoencoder: Hierarchical Reinforcement\n  Learning with Trajectory Embeddings", "comments": "Accepted at ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we take a representation learning perspective on hierarchical\nreinforcement learning, where the problem of learning lower layers in a\nhierarchy is transformed into the problem of learning trajectory-level\ngenerative models. We show that we can learn continuous latent representations\nof trajectories, which are effective in solving temporally extended and\nmulti-stage problems. Our proposed model, SeCTAR, draws inspiration from\nvariational autoencoders, and learns latent representations of trajectories. A\nkey component of this method is to learn both a latent-conditioned policy and a\nlatent-conditioned model which are consistent with each other. Given the same\nlatent, the policy generates a trajectory which should match the trajectory\npredicted by the model. This model provides a built-in prediction mechanism, by\npredicting the outcome of closed loop policy behavior. We propose a novel\nalgorithm for performing hierarchical RL with this model, combining model-based\nplanning in the learned latent space with an unsupervised exploration\nobjective. We show that our model is effective at reasoning over long horizons\nwith sparse rewards for several simulated tasks, outperforming standard\nreinforcement learning methods and prior methods for hierarchical reasoning,\nmodel-based planning, and exploration.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 17:49:08 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Co-Reyes", "John D.", ""], ["Liu", "YuXuan", ""], ["Gupta", "Abhishek", ""], ["Eysenbach", "Benjamin", ""], ["Abbeel", "Pieter", ""], ["Levine", "Sergey", ""]]}, {"id": "1806.02814", "submitter": "Denis Newman-Griffis", "authors": "Denis Newman-Griffis and Ayah Zirikly", "title": "Embedding Transfer for Low-Resource Medical Named Entity Recognition: A\n  Case Study on Patient Mobility", "comments": "Accepted to BioNLP 2018. 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functioning is gaining recognition as an important indicator of global\nhealth, but remains under-studied in medical natural language processing\nresearch. We present the first analysis of automatically extracting\ndescriptions of patient mobility, using a recently-developed dataset of free\ntext electronic health records. We frame the task as a named entity recognition\n(NER) problem, and investigate the applicability of NER techniques to mobility\nextraction. As text corpora focused on patient functioning are scarce, we\nexplore domain adaptation of word embeddings for use in a recurrent neural\nnetwork NER system. We find that embeddings trained on a small in-domain corpus\nperform nearly as well as those learned from large out-of-domain corpora, and\nthat domain adaptation techniques yield additional improvements in both\nprecision and recall. Our analysis identifies several significant challenges in\nextracting descriptions of patient mobility, including the length and\ncomplexity of annotated entities and high linguistic variability in mobility\ndescriptions.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 17:49:54 GMT"}], "update_date": "2018-06-08", "authors_parsed": [["Newman-Griffis", "Denis", ""], ["Zirikly", "Ayah", ""]]}, {"id": "1806.02817", "submitter": "Chelsea Finn", "authors": "Chelsea Finn, Kelvin Xu, Sergey Levine", "title": "Probabilistic Model-Agnostic Meta-Learning", "comments": "NeurIPS 2018. First two authors contributed equally. Supplementary\n  results available at https://sites.google.com/view/probabilistic-maml/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-learning for few-shot learning entails acquiring a prior over previous\ntasks and experiences, such that new tasks be learned from small amounts of\ndata. However, a critical challenge in few-shot learning is task ambiguity:\neven when a powerful prior can be meta-learned from a large number of prior\ntasks, a small dataset for a new task can simply be too ambiguous to acquire a\nsingle model (e.g., a classifier) for that task that is accurate. In this\npaper, we propose a probabilistic meta-learning algorithm that can sample\nmodels for a new task from a model distribution. Our approach extends\nmodel-agnostic meta-learning, which adapts to new tasks via gradient descent,\nto incorporate a parameter distribution that is trained via a variational lower\nbound. At meta-test time, our algorithm adapts via a simple procedure that\ninjects noise into gradient descent, and at meta-training time, the model is\ntrained such that this stochastic adaptation procedure produces samples from\nthe approximate model posterior. Our experimental results show that our method\ncan sample plausible classifiers and regressors in ambiguous few-shot learning\nproblems. We also show how reasoning about ambiguity can also be used for\ndownstream active learning problems.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 17:53:20 GMT"}, {"version": "v2", "created": "Thu, 17 Oct 2019 00:40:46 GMT"}], "update_date": "2019-10-18", "authors_parsed": [["Finn", "Chelsea", ""], ["Xu", "Kelvin", ""], ["Levine", "Sergey", ""]]}, {"id": "1806.02847", "submitter": "Trieu Trinh", "authors": "Trieu H. Trinh and Quoc V. Le", "title": "A Simple Method for Commonsense Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Commonsense reasoning is a long-standing challenge for deep learning. For\nexample, it is difficult to use neural networks to tackle the Winograd Schema\ndataset (Levesque et al., 2011). In this paper, we present a simple method for\ncommonsense reasoning with neural networks, using unsupervised learning. Key to\nour method is the use of language models, trained on a massive amount of\nunlabled data, to score multiple choice questions posed by commonsense\nreasoning tests. On both Pronoun Disambiguation and Winograd Schema challenges,\nour models outperform previous state-of-the-art methods by a large margin,\nwithout using expensive annotated knowledge bases or hand-engineered features.\nWe train an array of large RNN language models that operate at word or\ncharacter level on LM-1-Billion, CommonCrawl, SQuAD, Gutenberg Books, and a\ncustomized corpus for this task and show that diversity of training data plays\nan important role in test performance. Further analysis also shows that our\nsystem successfully discovers important features of the context that decide the\ncorrect answer, indicating a good grasp of commonsense knowledge.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 18:13:08 GMT"}, {"version": "v2", "created": "Thu, 26 Sep 2019 22:33:06 GMT"}], "update_date": "2019-09-30", "authors_parsed": [["Trinh", "Trieu H.", ""], ["Le", "Quoc V.", ""]]}, {"id": "1806.02855", "submitter": "Henri Palacci", "authors": "Henri Palacci, Henry Hess", "title": "Scalable Natural Gradient Langevin Dynamics in Practice", "comments": "ICML 2018 Workshop on Non-Convex Optimization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Gradient Langevin Dynamics (SGLD) is a sampling scheme for\nBayesian modeling adapted to large datasets and models. SGLD relies on the\ninjection of Gaussian Noise at each step of a Stochastic Gradient Descent (SGD)\nupdate. In this scheme, every component in the noise vector is independent and\nhas the same scale, whereas the parameters we seek to estimate exhibit strong\nvariations in scale and significant correlation structures, leading to poor\nconvergence and mixing times. We compare different preconditioning approaches\nto the normalization of the noise vector and benchmark these approaches on the\nfollowing criteria: 1) mixing times of the multivariate parameter vector, 2)\nregularizing effect on small dataset where it is easy to overfit, 3) covariate\nshift detection and 4) resistance to adversarial examples.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 18:38:15 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Palacci", "Henri", ""], ["Hess", "Henry", ""]]}, {"id": "1806.02873", "submitter": "Jinyang Gao", "authors": "Xiangrui Cai, Jinyang Gao, Kee Yuan Ngiam, Beng Chin Ooi, Ying Zhang,\n  Xiaojie Yuan", "title": "Medical Concept Embedding with Time-Aware Attention", "comments": "7 pages. IJCAI-ECAI-2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embeddings of medical concepts such as medication, procedure and diagnosis\ncodes in Electronic Medical Records (EMRs) are central to healthcare analytics.\nPrevious work on medical concept embedding takes medical concepts and EMRs as\nwords and documents respectively. Nevertheless, such models miss out the\ntemporal nature of EMR data. On the one hand, two consecutive medical concepts\ndo not indicate they are temporally close, but the correlations between them\ncan be revealed by the time gap. On the other hand, the temporal scopes of\nmedical concepts often vary greatly (e.g., \\textit{common cold} and\n\\textit{diabetes}). In this paper, we propose to incorporate the temporal\ninformation to embed medical codes. Based on the Continuous Bag-of-Words model,\nwe employ the attention mechanism to learn a \"soft\" time-aware context window\nfor each medical concept. Experiments on public and proprietary datasets\nthrough clustering and nearest neighbour search tasks demonstrate the\neffectiveness of our model, showing that it outperforms five state-of-the-art\nbaselines.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 07:45:06 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Cai", "Xiangrui", ""], ["Gao", "Jinyang", ""], ["Ngiam", "Kee Yuan", ""], ["Ooi", "Beng Chin", ""], ["Zhang", "Ying", ""], ["Yuan", "Xiaojie", ""]]}, {"id": "1806.02891", "submitter": "Bolei Zhou", "authors": "Bolei Zhou, Yiyou Sun, David Bau, Antonio Torralba", "title": "Revisiting the Importance of Individual Units in CNNs via Ablation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We revisit the importance of the individual units in Convolutional Neural\nNetworks (CNNs) for visual recognition. By conducting unit ablation experiments\non CNNs trained on large scale image datasets, we demonstrate that, though\nablating any individual unit does not hurt overall classification accuracy, it\ndoes lead to significant damage on the accuracy of specific classes. This\nresult shows that an individual unit is specialized to encode information\nrelevant to a subset of classes. We compute the correlation between the\naccuracy drop under unit ablation and various attributes of an individual unit\nsuch as class selectivity and weight L1 norm. We confirm that unit attributes\nsuch as class selectivity are a poor predictor for impact on overall accuracy\nas found previously in recent work \\cite{morcos2018importance}. However, our\nresults show that class selectivity along with other attributes are good\npredictors of the importance of one unit to individual classes. We evaluate the\nimpact of random rotation, batch normalization, and dropout to the importance\nof units to specific classes. Our results show that units with high selectivity\nplay an important role in network classification power at the individual class\nlevel. Understanding and interpreting the behavior of these units is necessary\nand meaningful.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 20:40:56 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Zhou", "Bolei", ""], ["Sun", "Yiyou", ""], ["Bau", "David", ""], ["Torralba", "Antonio", ""]]}, {"id": "1806.02901", "submitter": "Ben Athiwaratkun", "authors": "Ben Athiwaratkun, Andrew Gordon Wilson, Anima Anandkumar", "title": "Probabilistic FastText for Multi-Sense Word Embeddings", "comments": "Published at ACL 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Probabilistic FastText, a new model for word embeddings that can\ncapture multiple word senses, sub-word structure, and uncertainty information.\nIn particular, we represent each word with a Gaussian mixture density, where\nthe mean of a mixture component is given by the sum of n-grams. This\nrepresentation allows the model to share statistical strength across sub-word\nstructures (e.g. Latin roots), producing accurate representations of rare,\nmisspelt, or even unseen words. Moreover, each component of the mixture can\ncapture a different word sense. Probabilistic FastText outperforms both\nFastText, which has no probabilistic model, and dictionary-level probabilistic\nembeddings, which do not incorporate subword structures, on several\nword-similarity benchmarks, including English RareWord and foreign language\ndatasets. We also achieve state-of-art performance on benchmarks that measure\nability to discern different meanings. Thus, the proposed model is the first to\nachieve multi-sense representations while having enriched semantics on rare\nwords.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 20:57:22 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Athiwaratkun", "Ben", ""], ["Wilson", "Andrew Gordon", ""], ["Anandkumar", "Anima", ""]]}, {"id": "1806.02908", "submitter": "Fahim Mohammad PhD", "authors": "Fahim Mohammad", "title": "Is preprocessing of text really worth your time for online comment\n  classification?", "comments": "11 pages,including Appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large proportion of online comments present on public domains are\nconstructive, however a significant proportion are toxic in nature. The\ncomments contain lot of typos which increases the number of features manifold,\nmaking the ML model difficult to train. Considering the fact that the data\nscientists spend approximately 80% of their time in collecting, cleaning and\norganizing their data [1], we explored how much effort should we invest in the\npreprocessing (transformation) of raw comments before feeding it to the\nstate-of-the-art classification models. With the help of four models on Jigsaw\ntoxic comment classification data, we demonstrated that the training of model\nwithout any transformation produce relatively decent model. Applying even basic\ntransformations, in some cases, lead to worse performance and should be applied\nwith caution.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 21:29:39 GMT"}, {"version": "v2", "created": "Wed, 29 Aug 2018 22:45:27 GMT"}], "update_date": "2018-08-31", "authors_parsed": [["Mohammad", "Fahim", ""]]}, {"id": "1806.02918", "submitter": "Maria Shugrina", "authors": "Maria Shugrina, Amlan Kar, Karan Singh, Sanja Fidler", "title": "Color Sails: Discrete-Continuous Palettes for Deep Color Exploration", "comments": "14 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present color sails, a discrete-continuous color gamut representation that\nextends the color gradient analogy to three dimensions and allows interactive\ncontrol of the color blending behavior. Our representation models a wide\nvariety of color distributions in a compact manner, and lends itself to\napplications such as color exploration for graphic design, illustration and\nsimilar fields. We propose a Neural Network that can fit a color sail to any\nimage. Then, the user can adjust color sail parameters to change the base\ncolors, their blending behavior and the number of colors, exploring a wide\nrange of options for the original design. In addition, we propose a Deep\nLearning model that learns to automatically segment an image into\ncolor-compatible alpha masks, each equipped with its own color sail. This\nallows targeted color exploration by either editing their corresponding color\nsails or using standard software packages. Our model is trained on a custom\ndiverse dataset of art and design. We provide both quantitative evaluations,\nand a user study, demonstrating the effectiveness of color sail interaction.\nInteractive demos are available at www.colorsails.com.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 22:42:00 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Shugrina", "Maria", ""], ["Kar", "Amlan", ""], ["Singh", "Karan", ""], ["Fidler", "Sanja", ""]]}, {"id": "1806.02932", "submitter": "Riley Simmons-Edler", "authors": "Riley Simmons-Edler, Anders Miltner, Sebastian Seung", "title": "Program Synthesis Through Reinforcement Learning Guided Tree Search", "comments": "9 pages, 5 figures, Submitted to NIPS 2018 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Program Synthesis is the task of generating a program from a provided\nspecification. Traditionally, this has been treated as a search problem by the\nprogramming languages (PL) community and more recently as a supervised learning\nproblem by the machine learning community. Here, we propose a third approach,\nrepresenting the task of synthesizing a given program as a Markov decision\nprocess solvable via reinforcement learning(RL). From observations about the\nstates of partial programs, we attempt to find a program that is optimal over a\nprovided reward metric on pairs of programs and states. We instantiate this\napproach on a subset of the RISC-V assembly language operating on floating\npoint numbers, and as an optimization inspired by search-based techniques from\nthe PL community, we combine RL with a priority search tree. We evaluate this\ninstantiation and demonstrate the effectiveness of our combined method compared\nto a variety of baselines, including a pure RL ablation and a state of the art\nMarkov chain Monte Carlo search method on this task.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 00:53:43 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Simmons-Edler", "Riley", ""], ["Miltner", "Anders", ""], ["Seung", "Sebastian", ""]]}, {"id": "1806.02942", "submitter": "Yu Li", "authors": "Yu Li, Zhongxiao Li, Lizhong Ding, Yijie Pan, Chao Huang, Yuhui Hu,\n  Wei Chen, Xin Gao", "title": "SupportNet: solving catastrophic forgetting in class incremental\n  learning with support data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A plain well-trained deep learning model often does not have the ability to\nlearn new knowledge without forgetting the previously learned knowledge, which\nis known as catastrophic forgetting. Here we propose a novel method,\nSupportNet, to efficiently and effectively solve the catastrophic forgetting\nproblem in the class incremental learning scenario. SupportNet combines the\nstrength of deep learning and support vector machine (SVM), where SVM is used\nto identify the support data from the old data, which are fed to the deep\nlearning model together with the new data for further training so that the\nmodel can review the essential information of the old data when learning the\nnew information. Two powerful consolidation regularizers are applied to\nstabilize the learned representation and ensure the robustness of the learned\nmodel. We validate our method with comprehensive experiments on various tasks,\nwhich show that SupportNet drastically outperforms the state-of-the-art\nincremental learning methods and even reaches similar performance as the deep\nlearning model trained from scratch on both old and new data. Our program is\naccessible at: https://github.com/lykaust15/SupportNet\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 01:58:51 GMT"}, {"version": "v2", "created": "Sat, 1 Sep 2018 12:37:58 GMT"}, {"version": "v3", "created": "Fri, 28 Dec 2018 08:51:17 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Li", "Yu", ""], ["Li", "Zhongxiao", ""], ["Ding", "Lizhong", ""], ["Pan", "Yijie", ""], ["Huang", "Chao", ""], ["Hu", "Yuhui", ""], ["Chen", "Wei", ""], ["Gao", "Xin", ""]]}, {"id": "1806.02967", "submitter": "Xinye Cai", "authors": "Xinye Cai, Haoran Sun, Chunyang Zhu, Zhenyu Li, Qingfu Zhang", "title": "Locating the boundaries of Pareto fronts: A Many-Objective Evolutionary\n  Algorithm Based on Corner Solution Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an evolutionary many-objective optimization algorithm based on\ncorner solution search (MaOEA-CS) was proposed. MaOEA-CS implicitly contains\ntwo phases: the exploitative search for the most important boundary optimal\nsolutions - corner solutions, at the first phase, and the use of angle-based\nselection [1] with the explorative search for the extension of PF approximation\nat the second phase. Due to its high efficiency and robustness to the shapes of\nPFs, it has won the CEC'2017 Competition on Evolutionary Many-Objective\nOptimization. In addition, MaOEA-CS has also been applied on two real-world\nengineering optimization problems with very irregular PFs. The experimental\nresults show that MaOEA-CS outperforms other six state-of-the-art compared\nalgorithms, which indicates it has the ability to handle real-world complex\noptimization problems with irregular PFs.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 04:34:15 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Cai", "Xinye", ""], ["Sun", "Haoran", ""], ["Zhu", "Chunyang", ""], ["Li", "Zhenyu", ""], ["Zhang", "Qingfu", ""]]}, {"id": "1806.02985", "submitter": "Motoya Ohnishi", "authors": "Motoya Ohnishi, Masahiro Yukawa, Mikael Johansson, Masashi Sugiyama", "title": "Continuous-time Value Function Approximation in Reproducing Kernel\n  Hilbert Spaces", "comments": "NeurIPS 2018 - Advances in Neural Information Processing Systems\n  (with the supplementary document)", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the success of reinforcement learning (RL) for discrete-time\ntasks such as AlphaGo and Atari games, there has been a recent surge of\ninterest in using RL for continuous-time control of physical systems (cf. many\nchallenging tasks in OpenAI Gym and DeepMind Control Suite). Since\ndiscretization of time is susceptible to error, it is methodologically more\ndesirable to handle the system dynamics directly in continuous time. However,\nvery few techniques exist for continuous-time RL and they lack flexibility in\nvalue function approximation. In this paper, we propose a novel framework for\nmodel-based continuous-time value function approximation in reproducing kernel\nHilbert spaces. The resulting framework is so flexible that it can accommodate\nany kind of kernel-based approach, such as Gaussian processes and kernel\nadaptive filters, and it allows us to handle uncertainties and nonstationarity\nwithout prior knowledge about the environment or what basis functions to\nemploy. We demonstrate the validity of the presented framework through\nexperiments.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 06:50:14 GMT"}, {"version": "v2", "created": "Fri, 26 Oct 2018 00:08:42 GMT"}, {"version": "v3", "created": "Fri, 30 Nov 2018 10:10:05 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Ohnishi", "Motoya", ""], ["Yukawa", "Masahiro", ""], ["Johansson", "Mikael", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1806.02997", "submitter": "Aleksei Vasilev", "authors": "Aleksei Vasilev, Vladimir Golkov, Marc Meissner, Ilona Lipp, Eleonora\n  Sgarlata, Valentina Tomassini, Derek K. Jones, Daniel Cremers", "title": "q-Space Novelty Detection with Variational Autoencoders", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In machine learning, novelty detection is the task of identifying novel\nunseen data. During training, only samples from the normal class are available.\nTest samples are classified as normal or abnormal by assignment of a novelty\nscore. Here we propose novelty detection methods based on training variational\nautoencoders (VAEs) on normal data. Since abnormal samples are not used during\ntraining, we define novelty metrics based on the (partially complementary)\nassumptions that the VAE is less capable of reconstructing abnormal samples\nwell; that abnormal samples more strongly violate the VAE regularizer; and that\nabnormal samples differ from normal samples not only in input-feature space,\nbut also in the VAE latent space and VAE output. These approaches, combined\nwith various possibilities of using (e.g. sampling) the probabilistic VAE to\nobtain scalar novelty scores, yield a large family of methods. We apply these\nmethods to magnetic resonance imaging, namely to the detection of\ndiffusion-space (q-space) abnormalities in diffusion MRI scans of multiple\nsclerosis patients, i.e. to detect multiple sclerosis lesions without using any\nlesion labels for training. Many of our methods outperform previously proposed\nq-space novelty detection methods. We also evaluate the proposed methods on the\nMNIST handwritten digits dataset and show that many of them are able to\noutperform the state of the art.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 07:28:36 GMT"}, {"version": "v2", "created": "Thu, 25 Oct 2018 17:34:51 GMT"}], "update_date": "2018-10-26", "authors_parsed": [["Vasilev", "Aleksei", ""], ["Golkov", "Vladimir", ""], ["Meissner", "Marc", ""], ["Lipp", "Ilona", ""], ["Sgarlata", "Eleonora", ""], ["Tomassini", "Valentina", ""], ["Jones", "Derek K.", ""], ["Cremers", "Daniel", ""]]}, {"id": "1806.03155", "submitter": "Joberto Martins", "authors": "Eliseu Oliveira (UNIFACS), Rafael Freitas, Joberto Martins (UNIFACS)", "title": "Evaluating CBR Similarity Functions for BAM Switching in Networks with\n  Dynamic Traffic Profile", "comments": "https://lrsm.ibisc.univ-evry.fr/Advance2017/", "journal-ref": null, "doi": "10.5281/zenodo.1291127", "report-no": null, "categories": "cs.NI cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an increasingly complex scenario for network management, a solution that\nallows configuration in more autonomous way with less intervention of the\nnetwork manager is expected. This paper presents an evaluation of similarity\nfunctions that are necessary in the context of using a learning strategy for\nfinding solutions. The learning approach considered is based on Case-Based\nReasoning (CBR) and is applied to a network scenario where different Bandwidth\nAllocation Models (BAMs) behaviors are used and must be eventually switched\nlooking for the best possible network operation. In this context, it is\nrequired to identify and configure an adequate similarity function that will be\nused in the learning process to recover similar solutions previously\nconsidered. This paper introduces the similarity functions, explains the\nrelevant aspects of the learning process in which the similarity function plays\na role and, finally, presents a proof of concept for a specific similarity\nfunction adopted. Results show that the similarity function was capable to get\nsimilar results from the existing use case database. As such, the use of\nsimilarity functions with CBR technique has proved to be potentially\nsatisfactory for supporting BAM switching decisions mostly driven by the\ndynamics of input traffic profile.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 13:48:52 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Oliveira", "Eliseu", "", "UNIFACS"], ["Freitas", "Rafael", "", "UNIFACS"], ["Martins", "Joberto", "", "UNIFACS"]]}, {"id": "1806.03192", "submitter": "Emilia Gomez", "authors": "Emilia G\\'omez, Carlos Castillo, Vicky Charisi, Ver\\'onica Dahl,\n  Gustavo Deco, Blagoj Delipetrev, Nicole Dewandre, Miguel \\'Angel\n  Gonz\\'alez-Ballester, Fabien Gouyon, Jos\\'e Hern\\'andez-Orallo, Perfecto\n  Herrera, Anders Jonsson, Ansgar Koene, Martha Larson, Ram\\'on L\\'opez de\n  M\\'antaras, Bertin Martens, Marius Miron, Rub\\'en Moreno-Bote, Nuria Oliver,\n  Antonio Puertas Gallardo, Heike Schweitzer, Nuria Sebastian, Xavier Serra,\n  Joan Serr\\`a, Song\\\"ul Tolan, Karina Vold", "title": "Assessing the impact of machine intelligence on human behaviour: an\n  interdisciplinary endeavour", "comments": "Proceedings of 1st HUMAINT (Human Behaviour and Machine Intelligence)\n  workshop, Barcelona, Spain, March 5-6, 2018, edited by European Commission,\n  Seville, 2018, JRC111773\n  https://ec.europa.eu/jrc/communities/community/humaint/document/assessing-impact-machine-intelligence-human-behaviour-interdisciplinary.\n  arXiv admin note: text overlap with arXiv:1409.3097 by other authors", "journal-ref": null, "doi": null, "report-no": "JRC111773", "categories": "cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This document contains the outcome of the first Human behaviour and machine\nintelligence (HUMAINT) workshop that took place 5-6 March 2018 in Barcelona,\nSpain. The workshop was organized in the context of a new research programme at\nthe Centre for Advanced Studies, Joint Research Centre of the European\nCommission, which focuses on studying the potential impact of artificial\nintelligence on human behaviour. The workshop gathered an interdisciplinary\ngroup of experts to establish the state of the art research in the field and a\nlist of future research challenges to be addressed on the topic of human and\nmachine intelligence, algorithm's potential impact on human cognitive\ncapabilities and decision making, and evaluation and regulation needs. The\ndocument is made of short position statements and identification of challenges\nprovided by each expert, and incorporates the result of the discussions carried\nout during the workshop. In the conclusion section, we provide a list of\nemerging research topics and strategies to be addressed in the near future.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jun 2018 15:20:09 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["G\u00f3mez", "Emilia", ""], ["Castillo", "Carlos", ""], ["Charisi", "Vicky", ""], ["Dahl", "Ver\u00f3nica", ""], ["Deco", "Gustavo", ""], ["Delipetrev", "Blagoj", ""], ["Dewandre", "Nicole", ""], ["Gonz\u00e1lez-Ballester", "Miguel \u00c1ngel", ""], ["Gouyon", "Fabien", ""], ["Hern\u00e1ndez-Orallo", "Jos\u00e9", ""], ["Herrera", "Perfecto", ""], ["Jonsson", "Anders", ""], ["Koene", "Ansgar", ""], ["Larson", "Martha", ""], ["de M\u00e1ntaras", "Ram\u00f3n L\u00f3pez", ""], ["Martens", "Bertin", ""], ["Miron", "Marius", ""], ["Moreno-Bote", "Rub\u00e9n", ""], ["Oliver", "Nuria", ""], ["Gallardo", "Antonio Puertas", ""], ["Schweitzer", "Heike", ""], ["Sebastian", "Nuria", ""], ["Serra", "Xavier", ""], ["Serr\u00e0", "Joan", ""], ["Tolan", "Song\u00fcl", ""], ["Vold", "Karina", ""]]}, {"id": "1806.03240", "submitter": "Radek Pel\\'anek", "authors": "Radek Pel\\'anek and Tom\\'a\\v{s} Effenberger and Mat\\v{e}j Van\\v{e}k\n  and Vojt\\v{e}ch Sassmann and Dominik Gmiterko", "title": "Measuring Item Similarity in Introductory Programming: Python and Robot\n  Programming Case Studies", "comments": "Full version of the L@S'18 paper \"Measuring Item Similarity in\n  Introductory Programming\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A personalized learning system needs a large pool of items for learners to\nsolve. When working with a large pool of items, it is useful to measure the\nsimilarity of items. We outline a general approach to measuring the similarity\nof items and discuss specific measures for items used in introductory\nprogramming. Evaluation of quality of similarity measures is difficult. To this\nend, we propose an evaluation approach utilizing three levels of abstraction.\nWe illustrate our approach to measuring similarity and provide evaluation using\nitems from three diverse programming environments.\n", "versions": [{"version": "v1", "created": "Thu, 24 May 2018 11:12:25 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Pel\u00e1nek", "Radek", ""], ["Effenberger", "Tom\u00e1\u0161", ""], ["Van\u011bk", "Mat\u011bj", ""], ["Sassmann", "Vojt\u011bch", ""], ["Gmiterko", "Dominik", ""]]}, {"id": "1806.03256", "submitter": "Chun Kit Yeung", "authors": "Chun-kit Yeung, Zizheng Lin, Kai Yang, Dit-yan Yeung", "title": "Incorporating Features Learned by an Enhanced Deep Knowledge Tracing\n  Model for STEM/Non-STEM Job Prediction", "comments": "10 pages; Career prediction; Knowledge tracing; Machine learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 2017 ASSISTments Data Mining competition aims to use data from a\nlongitudinal study for predicting a brand-new outcome of students which had\nnever been studied before by the educational data mining research community.\nSpecifically, it facilitates research in developing predictive models that\npredict whether the first job of a student out of college belongs to a STEM\n(the acronym for science, technology, engineering, and mathematics) field. This\nis based on the student's learning history on the ASSISTments blended learning\nplatform in the form of extensive clickstream data gathered during the middle\nschool years. To tackle this challenge, we first estimate the expected\nknowledge state of students with respect to different mathematical skills using\na deep knowledge tracing (DKT) model and an enhanced DKT (DKT+) model. We then\ncombine the features corresponding to the DKT/DKT+ expected knowledge state\nwith other features extracted directly from the student profile in the dataset\nto train several machine learning models for the STEM/non-STEM job prediction.\nOur experiments show that models trained with the combined features generally\nperform better than the models trained with the student profile alone. Detailed\nanalysis of the student's knowledge state reveals that, when compared with\nnon-STEM students, STEM students generally show a higher mastery level and a\nhigher learning gain in mathematics.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 13:57:47 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Yeung", "Chun-kit", ""], ["Lin", "Zizheng", ""], ["Yang", "Kai", ""], ["Yeung", "Dit-yan", ""]]}, {"id": "1806.03267", "submitter": "Aboul Ella Hassanien Abo", "authors": "Mohamed Yorky and Aboul Ella Hassanien", "title": "Orbital Petri Nets: A Novel Petri Net Approach", "comments": "10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Petri Nets is very interesting tool for studying and simulating different\nbehaviors of information systems. It can be used in different applications\nbased on the appropriate class of Petri Nets whereas it is classical, colored\nor timed Petri Nets. In this paper we introduce a new approach of Petri Nets\ncalled orbital Petri Nets (OPN) for studying the orbital rotating systems\nwithin a specific domain. The study investigated and analyzed OPN with\nhighlighting the problem of space debris collision problem as a case study. The\nmathematical investigation results of two OPN models proved that space debris\ncollision problem can be prevented based on the new method of firing sequence\nin OPN. By this study, new smart algorithms can be implemented and simulated by\norbital Petri Nets for mitigating the space debris collision problem as a next\nwork.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 16:31:46 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Yorky", "Mohamed", ""], ["Hassanien", "Aboul Ella", ""]]}, {"id": "1806.03335", "submitter": "Ian Osband", "authors": "Ian Osband, John Aslanides, Albin Cassirer", "title": "Randomized Prior Functions for Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dealing with uncertainty is essential for efficient reinforcement learning.\nThere is a growing literature on uncertainty estimation for deep learning from\nfixed datasets, but many of the most popular approaches are poorly-suited to\nsequential decision problems. Other methods, such as bootstrap sampling, have\nno mechanism for uncertainty that does not come from the observed data. We\nhighlight why this can be a crucial shortcoming and propose a simple remedy\nthrough addition of a randomized untrainable `prior' network to each ensemble\nmember. We prove that this approach is efficient with linear representations,\nprovide simple illustrations of its efficacy with nonlinear representations and\nshow that this approach scales to large-scale problems far better than previous\nattempts.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 19:47:54 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2018 17:53:47 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Osband", "Ian", ""], ["Aslanides", "John", ""], ["Cassirer", "Albin", ""]]}, {"id": "1806.03361", "submitter": "Jessica Souza Sena", "authors": "Jessica Sena and Artur Jordao and William Robson Schwartz", "title": "A Content-Based Late Fusion Approach Applied to Pedestrian Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The variety of pedestrians detectors proposed in recent years has encouraged\nsome works to fuse pedestrian detectors to achieve a more accurate detection.\nThe intuition behind is to combine the detectors based on its spatial\nconsensus. We propose a novel method called Content-Based Spatial Consensus\n(CSBC), which, in addition to relying on spatial consensus, considers the\ncontent of the detection windows to learn a weighted-fusion of pedestrian\ndetectors. The result is a reduction in false alarms and an enhancement in the\ndetection. In this work, we also demonstrate that there is small influence of\nthe feature used to learn the contents of the windows of each detector, which\nenables our method to be efficient even employing simple features. The CSBC\novercomes state-of-the-art fusion methods in the ETH dataset and in the Caltech\ndataset. Particularly, our method is more efficient since fewer detectors are\nnecessary to achieve expressive results.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 21:35:09 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Sena", "Jessica", ""], ["Jordao", "Artur", ""], ["Schwartz", "William Robson", ""]]}, {"id": "1806.03379", "submitter": "Suren Jayasuriya", "authors": "Li-Chi Huang, Kuldeep Kulkarni, Anik Jha, Suhas Lohit, Suren\n  Jayasuriya, Pavan Turaga", "title": "CS-VQA: Visual Question Answering with Compressively Sensed Images", "comments": "5 pages, 2 figures, accepted to ICIP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Question Answering (VQA) is a complex semantic task requiring both\nnatural language processing and visual recognition. In this paper, we explore\nwhether VQA is solvable when images are captured in a sub-Nyquist compressive\nparadigm. We develop a series of deep-network architectures that exploit\navailable compressive data to increasing degrees of accuracy, and show that VQA\nis indeed solvable in the compressed domain. Our results show that there is\nnominal degradation in VQA performance when using compressive measurements, but\nthat accuracy can be recovered when VQA pipelines are used in conjunction with\nstate-of-the-art deep neural networks for CS reconstruction. The results\npresented yield important implications for resource-constrained VQA\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 23:26:22 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Huang", "Li-Chi", ""], ["Kulkarni", "Kuldeep", ""], ["Jha", "Anik", ""], ["Lohit", "Suhas", ""], ["Jayasuriya", "Suren", ""], ["Turaga", "Pavan", ""]]}, {"id": "1806.03417", "submitter": "Maximilian Nickel", "authors": "Maximilian Nickel, Douwe Kiela", "title": "Learning Continuous Hierarchies in the Lorentz Model of Hyperbolic\n  Geometry", "comments": "Accepted at ICML'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are concerned with the discovery of hierarchical relationships from\nlarge-scale unstructured similarity scores. For this purpose, we study\ndifferent models of hyperbolic space and find that learning embeddings in the\nLorentz model is substantially more efficient than in the Poincar\\'e-ball\nmodel. We show that the proposed approach allows us to learn high-quality\nembeddings of large taxonomies which yield improvements over Poincar\\'e\nembeddings, especially in low dimensions. Lastly, we apply our model to\ndiscover hierarchies in two real-world datasets: we show that an embedding in\nhyperbolic space can reveal important aspects of a company's organizational\nstructure as well as reveal historical relationships between language families.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2018 05:56:50 GMT"}, {"version": "v2", "created": "Sun, 8 Jul 2018 13:06:31 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Nickel", "Maximilian", ""], ["Kiela", "Douwe", ""]]}, {"id": "1806.03455", "submitter": "Bradley Alexander", "authors": "Brad Alexander", "title": "A Preliminary Exploration of Floating Point Grammatical Evolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current GP frameworks are highly effective on a range of real and simulated\nbenchmarks. However, due to the high dimensionality of the genotypes for GP,\nthe task of visualising the fitness landscape for GP search can be difficult.\nThis paper describes a new framework: Floating Point Grammatical Evolution\n(FP-GE) which uses a single floating point genotype to encode an individual\nprogram. This encoding permits easier visualisation of the fitness landscape\narbitrary problems by providing a way to map fitness against a single\ndimension. The new framework also makes it trivially easy to apply continuous\nsearch algorithms, such as Differential Evolution, to the search problem. In\nthis work, the FP-GE framework is tested against several regression problems,\nvisualising the search landscape for these and comparing different search\nmeta-heuristics.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2018 10:51:39 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Alexander", "Brad", ""]]}, {"id": "1806.03492", "submitter": "Joshua Bertram", "authors": "Josh Bertram and Peng Wei", "title": "Explainable Deterministic MDPs", "comments": "Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for a certain class of Markov Decision Processes (MDPs)\nthat can relate the optimal policy back to one or more reward sources in the\nenvironment. For a given initial state, without fully computing the value\nfunction, q-value function, or the optimal policy the algorithm can determine\nwhich rewards will and will not be collected, whether a given reward will be\ncollected only once or continuously, and which local maximum within the value\nfunction the initial state will ultimately lead to. We demonstrate that the\nmethod can be used to map the state space to identify regions that are\ndominated by one reward source and can fully analyze the state space to explain\nall actions. We provide a mathematical framework to show how all of this is\npossible without first computing the optimal policy or value function.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2018 15:44:54 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Bertram", "Josh", ""], ["Wei", "Peng", ""]]}, {"id": "1806.03497", "submitter": "Siyuan Qi", "authors": "Siyuan Qi, Baoxiong Jia, Song-Chun Zhu", "title": "Generalized Earley Parser: Bridging Symbolic Grammars and Sequence Data\n  for Future Prediction", "comments": "ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Future predictions on sequence data (e.g., videos or audios) require the\nalgorithms to capture non-Markovian and compositional properties of high-level\nsemantics. Context-free grammars are natural choices to capture such\nproperties, but traditional grammar parsers (e.g., Earley parser) only take\nsymbolic sentences as inputs. In this paper, we generalize the Earley parser to\nparse sequence data which is neither segmented nor labeled. This generalized\nEarley parser integrates a grammar parser with a classifier to find the optimal\nsegmentation and labels, and makes top-down future predictions. Experiments\nshow that our method significantly outperforms other approaches for future\nhuman activity prediction.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2018 16:07:02 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Qi", "Siyuan", ""], ["Jia", "Baoxiong", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "1806.03517", "submitter": "Hanan Hindy", "authors": "Hanan Hindy and David Brosset and Ethan Bayne and Amar Seeam and\n  Christos Tachtatzis and Robert Atkinson and Xavier Bellekens", "title": "A Taxonomy of Network Threats and the Effect of Current Datasets on\n  Intrusion Detection Systems", "comments": "28 Pages, 6 Figures", "journal-ref": "IEEE Access, 2020", "doi": "10.1109/ACCESS.2020.3000179", "report-no": null, "categories": "cs.CR cs.AI cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the world moves towards being increasingly dependent on computers and\nautomation, building secure applications, systems and networks are some of the\nmain challenges faced in the current decade. The number of threats that\nindividuals and businesses face is rising exponentially due to the increasing\ncomplexity of networks and services of modern networks. To alleviate the impact\nof these threats, researchers have proposed numerous solutions for anomaly\ndetection; however, current tools often fail to adapt to ever-changing\narchitectures, associated threats and zero-day attacks. This manuscript aims to\npinpoint research gaps and shortcomings of current datasets, their impact on\nbuilding Network Intrusion Detection Systems (NIDS) and the growing number of\nsophisticated threats. To this end, this manuscript provides researchers with\ntwo key pieces of information; a survey of prominent datasets, analyzing their\nuse and impact on the development of the past decade's Intrusion Detection\nSystems (IDS) and a taxonomy of network threats and associated tools to carry\nout these attacks. The manuscript highlights that current IDS research covers\nonly 33.3% of our threat taxonomy. Current datasets demonstrate a clear lack of\nreal-network threats, attack representation and include a large number of\ndeprecated threats, which together limit the detection accuracy of current\nmachine learning IDS approaches. The unique combination of the taxonomy and the\nanalysis of the datasets provided in this manuscript aims to improve the\ncreation of datasets and the collection of real-world data. As a result, this\nwill improve the efficiency of the next generation IDS and reflect network\nthreats more accurately within new datasets.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2018 17:47:55 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2020 14:27:56 GMT"}], "update_date": "2020-06-08", "authors_parsed": [["Hindy", "Hanan", ""], ["Brosset", "David", ""], ["Bayne", "Ethan", ""], ["Seeam", "Amar", ""], ["Tachtatzis", "Christos", ""], ["Atkinson", "Robert", ""], ["Bellekens", "Xavier", ""]]}, {"id": "1806.03536", "submitter": "Keyulu Xu", "authors": "Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi\n  Kawarabayashi, Stefanie Jegelka", "title": "Representation Learning on Graphs with Jumping Knowledge Networks", "comments": "ICML 2018, accepted as a long oral presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent deep learning approaches for representation learning on graphs follow\na neighborhood aggregation procedure. We analyze some important properties of\nthese models, and propose a strategy to overcome those. In particular, the\nrange of \"neighboring\" nodes that a node's representation draws from strongly\ndepends on the graph structure, analogous to the spread of a random walk. To\nadapt to local neighborhood properties and tasks, we explore an architecture --\njumping knowledge (JK) networks -- that flexibly leverages, for each node,\ndifferent neighborhood ranges to enable better structure-aware representation.\nIn a number of experiments on social, bioinformatics and citation networks, we\ndemonstrate that our model achieves state-of-the-art performance. Furthermore,\ncombining the JK framework with models like Graph Convolutional Networks,\nGraphSAGE and Graph Attention Networks consistently improves those models'\nperformance.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2018 19:49:57 GMT"}, {"version": "v2", "created": "Mon, 25 Jun 2018 19:52:28 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Xu", "Keyulu", ""], ["Li", "Chengtao", ""], ["Tian", "Yonglong", ""], ["Sonobe", "Tomohiro", ""], ["Kawarabayashi", "Ken-ichi", ""], ["Jegelka", "Stefanie", ""]]}, {"id": "1806.03561", "submitter": "Peter Clark", "authors": "Peter Clark", "title": "What Knowledge is Needed to Solve the RTE5 Textual Entailment Challenge?", "comments": "Reprint of an unpublished 2010 Working Note", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document gives a knowledge-oriented analysis of about 20 interesting\nRecognizing Textual Entailment (RTE) examples, drawn from the 2005 RTE5\ncompetition test set. The analysis ignores shallow statistical matching\ntechniques between T and H, and rather asks: What would it take to reasonably\ninfer that T implies H? What world knowledge would be needed for this task?\nAlthough such knowledge-intensive techniques have not had much success in RTE\nevaluations, ultimately an intelligent system should be expected to know and\ndeploy this kind of world knowledge required to perform this kind of reasoning.\n  The selected examples are typically ones which our RTE system (called BLUE)\ngot wrong and ones which require world knowledge to answer. In particular, the\nanalysis covers cases where there was near-perfect lexical overlap between T\nand H, yet the entailment was NO, i.e., examples that most likely all current\nRTE systems will have got wrong. A nice example is #341 (page 26), that\nrequires inferring from \"a river floods\" that \"a river overflows its banks\".\nSeems it should be easy, right? Enjoy!\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 00:33:47 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Clark", "Peter", ""]]}, {"id": "1806.03563", "submitter": "Hao Zhou", "authors": "Hao Henry Zhou, Yunyang Xiong, Vikas Singh", "title": "Building Bayesian Neural Networks with Blocks: On Structure,\n  Interpretability and Uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide simple schemes to build Bayesian Neural Networks (BNNs), block by\nblock, inspired by a recent idea of computation skeletons. We show how by\nadjusting the types of blocks that are used within the computation skeleton, we\ncan identify interesting relationships with Deep Gaussian Processes (DGPs),\ndeep kernel learning (DKL), random features type approximation and other\ntopics. We give strategies to approximate the posterior via doubly stochastic\nvariational inference for such models which yield uncertainty estimates. We\ngive a detailed theoretical analysis and point out extensions that may be of\nindependent interest. As a special case, we instantiate our procedure to define\na Bayesian {\\em additive} Neural network -- a promising strategy to identify\nstatistical interactions and has direct benefits for obtaining interpretable\nmodels.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 00:58:58 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Zhou", "Hao Henry", ""], ["Xiong", "Yunyang", ""], ["Singh", "Vikas", ""]]}, {"id": "1806.03568", "submitter": "Nan Wang", "authors": "Nan Wang, Hongning Wang, Yiling Jia, Yue Yin", "title": "Explainable Recommendation via Multi-Task Learning in Opinionated Text\n  Data", "comments": "10 pages, SIGIR 2018", "journal-ref": null, "doi": "10.1145/3209978.3210010", "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explaining automatically generated recommendations allows users to make more\ninformed and accurate decisions about which results to utilize, and therefore\nimproves their satisfaction. In this work, we develop a multi-task learning\nsolution for explainable recommendation. Two companion learning tasks of user\npreference modeling for recommendation} and \\textit{opinionated content\nmodeling for explanation are integrated via a joint tensor factorization. As a\nresult, the algorithm predicts not only a user's preference over a list of\nitems, i.e., recommendation, but also how the user would appreciate a\nparticular item at the feature level, i.e., opinionated textual explanation.\nExtensive experiments on two large collections of Amazon and Yelp reviews\nconfirmed the effectiveness of our solution in both recommendation and\nexplanation tasks, compared with several existing recommendation algorithms.\nAnd our extensive user study clearly demonstrates the practical value of the\nexplainable recommendations generated by our algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 01:43:40 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Wang", "Nan", ""], ["Wang", "Hongning", ""], ["Jia", "Yiling", ""], ["Yin", "Yue", ""]]}, {"id": "1806.03582", "submitter": "Punit Rathore", "authors": "Punit Rathore, Dheeraj Kumar, Sutharshan Rajasegarar, Marimuthu\n  Palaniswami, James C. Bezdek", "title": "A Scalable Framework for Trajectory Prediction", "comments": "Accepted in IEEE Transactions on Intelligent Transportation System.\n  Info: 15 Pages, 9 Figures, 5 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Trajectory prediction (TP) is of great importance for a wide range of\nlocation-based applications in intelligent transport systems such as\nlocation-based advertising, route planning, traffic management, and early\nwarning systems. In the last few years, the widespread use of GPS navigation\nsystems and wireless communication technology enabled vehicles has resulted in\nhuge volumes of trajectory data. The task of utilizing this data employing\nspatio-temporal techniques for trajectory prediction in an efficient and\naccurate manner is an ongoing research problem. Existing TP approaches are\nlimited to short-term predictions. Moreover, they cannot handle a large volume\nof trajectory data for long-term prediction. To address these limitations, we\npropose a scalable clustering and Markov chain based hybrid framework, called\nTraj-clusiVAT-based TP, for both short-term and long-term trajectory\nprediction, which can handle a large number of overlapping trajectories in a\ndense road network. Traj-clusiVAT can also determine the number of clusters,\nwhich represent different movement behaviours in input trajectory data. In our\nexperiments, we compare our proposed approach with a mixed Markov model\n(MMM)-based scheme, and a trajectory clustering, NETSCAN-based TP method for\nboth short- and long-term trajectory predictions. We performed our experiments\non two real, vehicle trajectory datasets, including a large-scale trajectory\ndataset consisting of 3.28 million trajectories obtained from 15,061 taxis in\nSingapore over a period of one month. Experimental results on two real\ntrajectory datasets show that our proposed approach outperforms the existing\napproaches in terms of both short- and long-term prediction performances, based\non prediction accuracy and distance error (in km).\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 04:05:18 GMT"}, {"version": "v2", "created": "Wed, 20 Jun 2018 09:18:51 GMT"}, {"version": "v3", "created": "Wed, 27 Feb 2019 17:56:13 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Rathore", "Punit", ""], ["Kumar", "Dheeraj", ""], ["Rajasegarar", "Sutharshan", ""], ["Palaniswami", "Marimuthu", ""], ["Bezdek", "James C.", ""]]}, {"id": "1806.03600", "submitter": "Levi Lucio", "authors": "Moussa Amrani (1) and Levi L\\'ucio (2) and Adrien Bibal (1)\n  (University of Namur, Faculty of Computer Science, PReCiSE / NaDI, Namur,\n  Belgium (2) fortiss GmbH, M\\\"unchen, Germany)", "title": "ML + FV = $\\heartsuit$? A Survey on the Application of Machine Learning\n  to Formal Verification", "comments": "13 pages, no figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Formal Verification (FV) and Machine Learning (ML) can seem incompatible due\nto their opposite mathematical foundations and their use in real-life problems:\nFV mostly relies on discrete mathematics and aims at ensuring correctness; ML\noften relies on probabilistic models and consists of learning patterns from\ntraining data. In this paper, we postulate that they are complementary in\npractice, and explore how ML helps FV in its classical approaches: static\nanalysis, model-checking, theorem-proving, and SAT solving. We draw a landscape\nof the current practice and catalog some of the most prominent uses of ML\ninside FV tools, thus offering a new perspective on FV techniques that can help\nresearchers and practitioners to better locate the possible synergies. We\ndiscuss lessons learned from our work, point to possible improvements and offer\nvisions for the future of the domain in the light of the science of software\nand systems modeling.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 07:29:41 GMT"}, {"version": "v2", "created": "Tue, 12 Jun 2018 11:16:34 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Amrani", "Moussa", ""], ["L\u00facio", "Levi", ""], ["Bibal", "Adrien", ""]]}, {"id": "1806.03671", "submitter": "Aaron M. Roth", "authors": "Aaron M. Roth, Umang Bhatt, Tamara Amin, Afsaneh Doryab, Fei Fang,\n  Manuela Veloso", "title": "The Impact of Humanoid Affect Expression on Human Behavior in a\n  Game-Theoretic Setting", "comments": "presented at 1st Workshop on Humanizing AI (HAI) at IJCAI'18 in\n  Stockholm, Sweden", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of robot and other intelligent and autonomous\nagents, how a human could be influenced by a robot's expressed mood when making\ndecisions becomes a crucial question in human-robot interaction. In this pilot\nstudy, we investigate (1) in what way a robot can express a certain mood to\ninfluence a human's decision making behavioral model; (2) how and to what\nextent the human will be influenced in a game theoretic setting. More\nspecifically, we create an NLP model to generate sentences that adhere to a\nspecific affective expression profile. We use these sentences for a humanoid\nrobot as it plays a Stackelberg security game against a human. We investigate\nthe behavioral model of the human player.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 15:20:20 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Roth", "Aaron M.", ""], ["Bhatt", "Umang", ""], ["Amin", "Tamara", ""], ["Doryab", "Afsaneh", ""], ["Fang", "Fei", ""], ["Veloso", "Manuela", ""]]}, {"id": "1806.03692", "submitter": "Junyang Lin", "authors": "Junyang Lin, Xu Sun, Xuancheng Ren, Shuming Ma, Jinsong Su, Qi Su", "title": "Deconvolution-Based Global Decoding for Neural Machine Translation", "comments": "Accepted by COLING 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A great proportion of sequence-to-sequence (Seq2Seq) models for Neural\nMachine Translation (NMT) adopt Recurrent Neural Network (RNN) to generate\ntranslation word by word following a sequential order. As the studies of\nlinguistics have proved that language is not linear word sequence but sequence\nof complex structure, translation at each step should be conditioned on the\nwhole target-side context. To tackle the problem, we propose a new NMT model\nthat decodes the sequence with the guidance of its structural prediction of the\ncontext of the target sequence. Our model generates translation based on the\nstructural prediction of the target-side context so that the translation can be\nfreed from the bind of sequential order. Experimental results demonstrate that\nour model is more competitive compared with the state-of-the-art methods, and\nthe analysis reflects that our model is also robust to translating sentences of\ndifferent lengths and it also reduces repetition with the instruction from the\ntarget-side context for decoding.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 17:05:31 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Lin", "Junyang", ""], ["Sun", "Xu", ""], ["Ren", "Xuancheng", ""], ["Ma", "Shuming", ""], ["Su", "Jinsong", ""], ["Su", "Qi", ""]]}, {"id": "1806.03751", "submitter": "Michael Hauser", "authors": "Michael Hauser, Sean Gunn, Samer Saab Jr, Asok Ray", "title": "State Space Representations of Deep Neural Networks", "comments": null, "journal-ref": "Neural Computation, Volume 31, Issue 3, March 2019, p.538-554", "doi": "10.1162/neco_a_01165", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper deals with neural networks as dynamical systems governed by\ndifferential or difference equations. It shows that the introduction of skip\nconnections into network architectures, such as residual networks and dense\nnetworks, turns a system of static equations into a system of dynamical\nequations with varying levels of smoothness on the layer-wise transformations.\nClosed form solutions for the state space representations of general dense\nnetworks, as well as $k^{th}$ order smooth networks, are found in general\nsettings. Furthermore, it is shown that imposing $k^{th}$ order smoothness on a\nnetwork architecture with $d$-many nodes per layer increases the state space\ndimension by a multiple of $k$, and so the effective embedding dimension of the\ndata manifold is $k \\cdot d$-many dimensions. It follows that network\narchitectures of these types reduce the number of parameters needed to maintain\nthe same embedding dimension by a factor of $k^2$ when compared to an\nequivalent first-order, residual network, significantly motivating the\ndevelopment of network architectures of these types. Numerical simulations were\nrun to validate parts of the developed theory.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 00:26:13 GMT"}, {"version": "v2", "created": "Wed, 13 Jun 2018 02:57:47 GMT"}, {"version": "v3", "created": "Thu, 21 Feb 2019 21:11:08 GMT"}], "update_date": "2019-02-25", "authors_parsed": [["Hauser", "Michael", ""], ["Gunn", "Sean", ""], ["Saab", "Samer", "Jr"], ["Ray", "Asok", ""]]}, {"id": "1806.03793", "submitter": "Siyuan Li", "authors": "Siyuan Li, Fangda Gu, Guangxiang Zhu, Chongjie Zhang", "title": "Context-Aware Policy Reuse", "comments": "Camera-ready version for AAMAS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning can greatly speed up reinforcement learning for a new task\nby leveraging policies of relevant tasks.\n  Existing works of policy reuse either focus on only selecting a single best\nsource policy for transfer without considering contexts, or cannot guarantee to\nlearn an optimal policy for a target task.\n  To improve transfer efficiency and guarantee optimality, we develop a novel\npolicy reuse method, called Context-Aware Policy reuSe (CAPS), that enables\nmulti-policy transfer. Our method learns when and which source policy is best\nfor reuse, as well as when to terminate its reuse. CAPS provides theoretical\nguarantees in convergence and optimality for both source policy selection and\ntarget task learning. Empirical results on a grid-based navigation domain and\nthe Pygame Learning Environment demonstrate that CAPS significantly outperforms\nother state-of-the-art policy reuse methods.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 03:37:43 GMT"}, {"version": "v2", "created": "Thu, 14 Jun 2018 02:53:52 GMT"}, {"version": "v3", "created": "Thu, 28 Jun 2018 11:01:33 GMT"}, {"version": "v4", "created": "Fri, 8 Mar 2019 14:13:36 GMT"}], "update_date": "2019-03-11", "authors_parsed": [["Li", "Siyuan", ""], ["Gu", "Fangda", ""], ["Zhu", "Guangxiang", ""], ["Zhang", "Chongjie", ""]]}, {"id": "1806.03806", "submitter": "Ketan Patil", "authors": "Ketan Patil, Aditya Kanade", "title": "Greybox fuzzing as a contextual bandits problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Greybox fuzzing is one of the most useful and effective techniques for the\nbug detection in large scale application programs. It uses minimal amount of\ninstrumentation. American Fuzzy Lop (AFL) is a popular coverage based\nevolutionary greybox fuzzing tool. AFL performs extremely well in fuzz testing\nlarge applications and finding critical vulnerabilities, but AFL involves a lot\nof heuristics while deciding the favored test case(s), skipping test cases\nduring fuzzing, assigning fuzzing iterations to test case(s). In this work, we\naim at replacing the heuristics the AFL uses while assigning the fuzzing\niterations to a test case during the random fuzzing. We formalize this problem\nas a `contextual bandit problem' and we propose an algorithm to solve this\nproblem. We have implemented our approach on top of the AFL. We modify the\nAFL's heuristics with our learned model through the policy gradient method. Our\nlearning algorithm selects the multiplier of the number of fuzzing iterations\nto be assigned to a test case during random fuzzing, given a fixed length\nsubstring of the test case to be fuzzed. We fuzz the substring with this new\nenergy value and continuously updates the policy based upon the interesting\ntest cases it produces on fuzzing.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 04:49:00 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Patil", "Ketan", ""], ["Kanade", "Aditya", ""]]}, {"id": "1806.03820", "submitter": "Malayandi Palaniappan", "authors": "Dhruv Malik, Malayandi Palaniappan, Jaime F. Fisac, Dylan\n  Hadfield-Menell, Stuart Russell, Anca D. Dragan", "title": "An Efficient, Generalized Bellman Update For Cooperative Inverse\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal is for AI systems to correctly identify and act according to their\nhuman user's objectives. Cooperative Inverse Reinforcement Learning (CIRL)\nformalizes this value alignment problem as a two-player game between a human\nand robot, in which only the human knows the parameters of the reward function:\nthe robot needs to learn them as the interaction unfolds. Previous work showed\nthat CIRL can be solved as a POMDP, but with an action space size exponential\nin the size of the reward parameter space. In this work, we exploit a specific\nproperty of CIRL---the human is a full information agent---to derive an\noptimality-preserving modification to the standard Bellman update; this reduces\nthe complexity of the problem by an exponential factor and allows us to relax\nCIRL's assumption of human rationality. We apply this update to a variety of\nPOMDP solvers and find that it enables us to scale CIRL to non-trivial\nproblems, with larger reward parameter spaces, and larger action spaces for\nboth robot and human. In solutions to these larger problems, the human exhibits\npedagogic (teaching) behavior, while the robot interprets it as such and\nattains higher value for the human.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 06:06:43 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Malik", "Dhruv", ""], ["Palaniappan", "Malayandi", ""], ["Fisac", "Jaime F.", ""], ["Hadfield-Menell", "Dylan", ""], ["Russell", "Stuart", ""], ["Dragan", "Anca D.", ""]]}, {"id": "1806.03934", "submitter": "Ella Gale", "authors": "Ella M. Gale, Nicolas Martin, Jeffrey S. Bowers", "title": "When and where do feed-forward neural networks learn localist\n  representations?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.ET cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  According to parallel distributed processing (PDP) theory in psychology,\nneural networks (NN) learn distributed rather than interpretable localist\nrepresentations. This view has been held so strongly that few researchers have\nanalysed single units to determine if this assumption is correct. However,\nrecent results from psychology, neuroscience and computer science have shown\nthe occasional existence of local codes emerging in artificial and biological\nneural networks. In this paper, we undertake the first systematic survey of\nwhen local codes emerge in a feed-forward neural network, using generated input\nand output data with known qualities. We find that the number of local codes\nthat emerge from a NN follows a well-defined distribution across the number of\nhidden layer neurons, with a peak determined by the size of input data, number\nof examples presented and the sparsity of input data. Using a 1-hot output code\ndrastically decreases the number of local codes on the hidden layer. The number\nof emergent local codes increases with the percentage of dropout applied to the\nhidden layer, suggesting that the localist encoding may offer a resilience to\nnoisy networks. This data suggests that localist coding can emerge from\nfeed-forward PDP networks and suggests some of the conditions that may lead to\ninterpretable localist representations in the cortex. The findings highlight\nhow local codes should not be dismissed out of hand.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 12:21:55 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Gale", "Ella M.", ""], ["Martin", "Nicolas", ""], ["Bowers", "Jeffrey S.", ""]]}, {"id": "1806.03960", "submitter": "Ruohan ZHang", "authors": "Ruohan Zhang, Zhuode Liu, Luxin Zhang, Jake A. Whritner, Karl S.\n  Muller, Mary M. Hayhoe, Dana H. Ballard", "title": "AGIL: Learning Attention from Human for Visuomotor Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When intelligent agents learn visuomotor behaviors from human demonstrations,\nthey may benefit from knowing where the human is allocating visual attention,\nwhich can be inferred from their gaze. A wealth of information regarding\nintelligent decision making is conveyed by human gaze allocation; hence,\nexploiting such information has the potential to improve the agents'\nperformance. With this motivation, we propose the AGIL (Attention Guided\nImitation Learning) framework. We collect high-quality human action and gaze\ndata while playing Atari games in a carefully controlled experimental setting.\nUsing these data, we first train a deep neural network that can predict human\ngaze positions and visual attention with high accuracy (the gaze network) and\nthen train another network to predict human actions (the policy network).\nIncorporating the learned attention model from the gaze network into the policy\nnetwork significantly improves the action prediction accuracy and task\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jun 2018 18:36:36 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Zhang", "Ruohan", ""], ["Liu", "Zhuode", ""], ["Zhang", "Luxin", ""], ["Whritner", "Jake A.", ""], ["Muller", "Karl S.", ""], ["Hayhoe", "Mary M.", ""], ["Ballard", "Dana H.", ""]]}, {"id": "1806.04017", "submitter": "Aboul Ella Hassanien Abo", "authors": "Aya Salama Abdelhady, Aboul Ella Hassanenin, and Aly Fahmy", "title": "Sheep identity recognition, age and weight estimation datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Increased interest of scientists, producers and consumers in sheep\nidentification has been stimulated by the dramatic increase in population and\nthe urge to increase productivity. The world population is expected to exceed\n9.6 million in 2050. For this reason, awareness is raised towards the necessity\nof effective livestock production. Sheep is considered as one of the main of\nfood resources. Most of the research now is directed towards developing real\ntime applications that facilitate sheep identification for breed management and\ngathering related information like weight and age. Weight and age are key\nmatrices in assessing the effectiveness of production. For this reason, visual\nanalysis proved recently its significant success over other approaches. Visual\nanalysis techniques need enough images for testing and study completion. For\nthis reason, collecting sheep images database is a vital step to fulfill such\nobjective. We provide here datasets for testing and comparing such algorithms\nwhich are under development. Our collected dataset consists of 416 color images\nfor different features of sheep in different postures. Images were collected\nfifty two sheep at a range of year from three months to six years. For each\nsheep, two images were captured for both sides of the body, two images for both\nsides of the face, one image from the top view, one image for the hip and one\nimage for the teeth. The collected images cover different illumination, quality\nlevels and angle of rotation. The allocated data set can be used to test sheep\nidentification, weigh estimation, and age detection algorithms. Such algorithms\nare crucial for disease management, animal assessment and ownership.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jun 2018 16:31:19 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Abdelhady", "Aya Salama", ""], ["Hassanenin", "Aboul Ella", ""], ["Fahmy", "Aly", ""]]}, {"id": "1806.04067", "submitter": "Tobias Baumann", "authors": "Tobias Baumann, Thore Graepel, John Shawe-Taylor", "title": "Adaptive Mechanism Design: Learning to Promote Cooperation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the future, artificial learning agents are likely to become increasingly\nwidespread in our society. They will interact with both other learning agents\nand humans in a variety of complex settings including social dilemmas. We\nconsider the problem of how an external agent can promote cooperation between\nartificial learners by distributing additional rewards and punishments based on\nobserving the learners' actions. We propose a rule for automatically learning\nhow to create right incentives by considering the players' anticipated\nparameter updates. Using this learning rule leads to cooperation with high\nsocial welfare in matrix games in which the agents would otherwise learn to\ndefect with high probability. We show that the resulting cooperative outcome is\nstable in certain games even if the planning agent is turned off after a given\nnumber of episodes, while other games require ongoing intervention to maintain\nmutual cooperation. However, even in the latter case, the amount of necessary\nadditional incentives decreases over time.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 15:48:37 GMT"}, {"version": "v2", "created": "Wed, 20 Nov 2019 11:14:13 GMT"}], "update_date": "2019-11-21", "authors_parsed": [["Baumann", "Tobias", ""], ["Graepel", "Thore", ""], ["Shawe-Taylor", "John", ""]]}, {"id": "1806.04168", "submitter": "Yikang Shen", "authors": "Yikang Shen, Zhouhan Lin, Athul Paul Jacob, Alessandro Sordoni, Aaron\n  Courville, Yoshua Bengio", "title": "Straight to the Tree: Constituency Parsing with Neural Syntactic\n  Distance", "comments": "Published at ACL2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel constituency parsing scheme. The model\npredicts a vector of real-valued scalars, named syntactic distances, for each\nsplit position in the input sentence. The syntactic distances specify the order\nin which the split points will be selected, recursively partitioning the input,\nin a top-down fashion. Compared to traditional shift-reduce parsing schemes,\nour approach is free from the potential problem of compounding errors, while\nbeing faster and easier to parallelize. Our model achieves competitive\nperformance amongst single model, discriminative parsers in the PTB dataset and\noutperforms previous models in the CTB dataset.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 18:18:00 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Shen", "Yikang", ""], ["Lin", "Zhouhan", ""], ["Jacob", "Athul Paul", ""], ["Sordoni", "Alessandro", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1806.04169", "submitter": "Ian Goodfellow", "authors": "Ian Goodfellow", "title": "Defense Against the Dark Arts: An overview of adversarial example\n  security research and future research directions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a summary of a keynote lecture at the Deep Learning\nSecurity workshop at IEEE Security and Privacy 2018. This lecture summarizes\nthe state of the art in defenses against adversarial examples and provides\nrecommendations for future research directions on this topic.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 18:22:45 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Goodfellow", "Ian", ""]]}, {"id": "1806.04189", "submitter": "Minjia Zhang", "authors": "Minjia Zhang, Xiaodong Liu, Wenhan Wang, Jianfeng Gao, Yuxiong He", "title": "Navigating with Graph Representations for Fast and Scalable Decoding of\n  Neural Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural language models (NLMs) have recently gained a renewed interest by\nachieving state-of-the-art performance across many natural language processing\n(NLP) tasks. However, NLMs are very computationally demanding largely due to\nthe computational cost of the softmax layer over a large vocabulary. We observe\nthat, in decoding of many NLP tasks, only the probabilities of the top-K\nhypotheses need to be calculated preciously and K is often much smaller than\nthe vocabulary size. This paper proposes a novel softmax layer approximation\nalgorithm, called Fast Graph Decoder (FGD), which quickly identifies, for a\ngiven context, a set of K words that are most likely to occur according to a\nNLM. We demonstrate that FGD reduces the decoding time by an order of magnitude\nwhile attaining close to the full softmax baseline accuracy on neural machine\ntranslation and language modeling tasks. We also prove the theoretical\nguarantee on the softmax approximation quality.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 18:57:49 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Zhang", "Minjia", ""], ["Liu", "Xiaodong", ""], ["Wang", "Wenhan", ""], ["Gao", "Jianfeng", ""], ["He", "Yuxiong", ""]]}, {"id": "1806.04212", "submitter": "Lasya Venneti", "authors": "Lasya Venneti and Aniket Alam", "title": "How Curiosity can be modeled for a Clickbait Detector", "comments": "This work was presented at 1st Workshop on Humanizing AI (HAI) at\n  IJCAI'18 in Stockholm, Sweden", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The impact of continually evolving digital technologies and the proliferation\nof communications and content has now been widely acknowledged to be central to\nunderstanding our world. What is less acknowledged is that this is based on the\nsuccessful arousing of curiosity both at the collective and individual levels.\nAdvertisers, communication professionals and news editors are in constant\ncompetition to capture attention of the digital population perennially shifty\nand distracted. This paper, tries to understand how curiosity works in the\ndigital world by attempting the first ever work done on quantifying human\ncuriosity, basing itself on various theories drawn from humanities and social\nsciences. Curious communication pushes people to spot, read and click the\nmessage from their social feed or any other form of online presentation. Our\napproach focuses on measuring the strength of the stimulus to generate reader\ncuriosity by using unsupervised and supervised machine learning algorithms, but\nis also informed by philosophical, psychological, neural and cognitive studies\non this topic. Manually annotated news headlines - clickbaits - have been\nselected for the study, which are known to have drawn huge reader response. A\nbinary classifier was developed based on human curiosity (unlike the work done\nso far using words and other linguistic features). Our classifier shows an\naccuracy of 97% . This work is part of the research in computational humanities\non digital politics quantifying the emotions of curiosity and outrage on\ndigital media.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 19:39:42 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Venneti", "Lasya", ""], ["Alam", "Aniket", ""]]}, {"id": "1806.04216", "submitter": "Hang Ma", "authors": "Hang Ma, Glenn Wagner, Ariel Felner, Jiaoyang Li, T. K. Satish Kumar,\n  Sven Koenig", "title": "Multi-Agent Path Finding with Deadlines", "comments": "IJCAI 2018, to appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formalize Multi-Agent Path Finding with Deadlines (MAPF-DL). The objective\nis to maximize the number of agents that can reach their given goal vertices\nfrom their given start vertices within the deadline, without colliding with\neach other. We first show that MAPF-DL is NP-hard to solve optimally. We then\npresent two classes of optimal algorithms, one based on a reduction of MAPF-DL\nto a flow problem and a subsequent compact integer linear programming\nformulation of the resulting reduced abstracted multi-commodity flow network\nand the other one based on novel combinatorial search algorithms. Our empirical\nresults demonstrate that these MAPF-DL solvers scale well and each one\ndominates the other ones in different scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 19:54:38 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Ma", "Hang", ""], ["Wagner", "Glenn", ""], ["Felner", "Ariel", ""], ["Li", "Jiaoyang", ""], ["Kumar", "T. K. Satish", ""], ["Koenig", "Sven", ""]]}, {"id": "1806.04225", "submitter": "Anirudha Majumdar", "authors": "Anirudha Majumdar, Alec Farid, Anoopkumar Sonar", "title": "PAC-Bayes Control: Learning Policies that Provably Generalize to Novel\n  Environments", "comments": "Extended version of paper presented at the 2018 Conference on Robot\n  Learning (CoRL)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal is to learn control policies for robots that provably generalize\nwell to novel environments given a dataset of example environments. The key\ntechnical idea behind our approach is to leverage tools from generalization\ntheory in machine learning by exploiting a precise analogy (which we present in\nthe form of a reduction) between generalization of control policies to novel\nenvironments and generalization of hypotheses in the supervised learning\nsetting. In particular, we utilize the Probably Approximately Correct\n(PAC)-Bayes framework, which allows us to obtain upper bounds that hold with\nhigh probability on the expected cost of (stochastic) control policies across\nnovel environments. We propose policy learning algorithms that explicitly seek\nto minimize this upper bound. The corresponding optimization problem can be\nsolved using convex optimization (Relative Entropy Programming in particular)\nin the setting where we are optimizing over a finite policy space. In the more\ngeneral setting of continuously parameterized policies (e.g., neural network\npolicies), we minimize this upper bound using stochastic gradient descent. We\npresent simulated results of our approach applied to learning (1) reactive\nobstacle avoidance policies and (2) neural network-based grasping policies. We\nalso present hardware results for the Parrot Swing drone navigating through\ndifferent obstacle environments. Our examples demonstrate the potential of our\napproach to provide strong generalization guarantees for robotic systems with\ncontinuous state and action spaces, complicated (e.g., nonlinear) dynamics,\nrich sensory inputs (e.g., depth images), and neural network-based policies.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 20:23:25 GMT"}, {"version": "v2", "created": "Thu, 14 Jun 2018 22:42:12 GMT"}, {"version": "v3", "created": "Wed, 17 Jul 2019 21:43:11 GMT"}, {"version": "v4", "created": "Sat, 22 Feb 2020 22:22:25 GMT"}, {"version": "v5", "created": "Tue, 25 Aug 2020 21:54:26 GMT"}], "update_date": "2020-08-27", "authors_parsed": [["Majumdar", "Anirudha", ""], ["Farid", "Alec", ""], ["Sonar", "Anoopkumar", ""]]}, {"id": "1806.04234", "submitter": "Ulle Endriss", "authors": "Ulle Endriss", "title": "Lecture Notes on Fair Division", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fair division is the problem of dividing one or several goods amongst two or\nmore agents in a way that satisfies a suitable fairness criterion. These Notes\nprovide a succinct introduction to the field. We cover three main topics.\nFirst, we need to define what is to be understood by a \"fair\" allocation of\ngoods to individuals. We present an overview of the most important fairness\ncriteria (as well as the closely related criteria for economic efficiency)\ndeveloped in the literature, together with a short discussion of their\naxiomatic foundations. Second, we give an introduction to cake-cutting\nprocedures as an example of methods for fairly dividing a single divisible\nresource amongst a group of individuals. Third, we discuss the combinatorial\noptimisation problem of fairly allocating a set of indivisible goods to a group\nof agents, covering both centralised algorithms (similar to auctions) and a\ndistributed approach based on negotiation.\n  While the classical literature on fair division has largely developed within\nEconomics, these Notes are specifically written for readers with a background\nin Computer Science or similar, and who may be (or may wish to be) engaged in\nresearch in Artificial Intelligence, Multiagent Systems, or Computational\nSocial Choice. References for further reading, as well as a small number of\nexercises, are included.\n  Notes prepared for a tutorial at the 11th European Agent Systems Summer\nSchool (EASSS-2009), Torino, Italy, 31 August and 1 September 2009. Updated for\na tutorial at the COST-ADT Doctoral School on Computational Social Choice,\nEstoril, Portugal, 9--14 April 2010.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 20:41:23 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Endriss", "Ulle", ""]]}, {"id": "1806.04242", "submitter": "Thomas Moerland", "authors": "Thomas M. Moerland, Joost Broekens and Catholijn M. Jonker", "title": "The Potential of the Return Distribution for Exploration in RL", "comments": "Published at the Exploration in Reinforcement Learning Workshop at\n  the 35th International Conference on Machine Learning, Stockholm, Sweden", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies the potential of the return distribution for exploration\nin deterministic reinforcement learning (RL) environments. We study network\nlosses and propagation mechanisms for Gaussian, Categorical and Gaussian\nmixture distributions. Combined with exploration policies that leverage this\nreturn distribution, we solve, for example, a randomized Chain task of length\n100, which has not been reported before when learning with neural networks.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 21:02:50 GMT"}, {"version": "v2", "created": "Mon, 2 Jul 2018 18:49:35 GMT"}], "update_date": "2018-07-04", "authors_parsed": [["Moerland", "Thomas M.", ""], ["Broekens", "Joost", ""], ["Jonker", "Catholijn M.", ""]]}, {"id": "1806.04265", "submitter": "Wojciech Samek", "authors": "Clemens Seibold, Wojciech Samek, Anna Hilsmann, Peter Eisert", "title": "Accurate and Robust Neural Networks for Security Related Applications\n  Exampled by Face Morphing Attacks", "comments": "16 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks tend to learn only what they need for a task. A\nmanipulation of the training data can counter this phenomenon. In this paper,\nwe study the effect of different alterations of the training data, which limit\nthe amount and position of information that is available for the decision\nmaking. We analyze the accuracy and robustness against semantic and black box\nattacks on the networks that were trained on different training data\nmodifications for the particular example of morphing attacks. A morphing attack\nis an attack on a biometric facial recognition system where the system is\nfooled to match two different individuals with the same synthetic face image.\nSuch a synthetic image can be created by aligning and blending images of the\ntwo individuals that should be matched with this image.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 23:24:11 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Seibold", "Clemens", ""], ["Samek", "Wojciech", ""], ["Hilsmann", "Anna", ""], ["Eisert", "Peter", ""]]}, {"id": "1806.04284", "submitter": "Chenhui Chu", "authors": "Chenhui Chu, Mayu Otani and Yuta Nakashima", "title": "iParaphrasing: Extracting Visually Grounded Paraphrases via an Image", "comments": "COLING 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A paraphrase is a restatement of the meaning of a text in other words.\nParaphrases have been studied to enhance the performance of many natural\nlanguage processing tasks. In this paper, we propose a novel task iParaphrasing\nto extract visually grounded paraphrases (VGPs), which are different phrasal\nexpressions describing the same visual concept in an image. These extracted\nVGPs have the potential to improve language and image multimodal tasks such as\nvisual question answering and image captioning. How to model the similarity\nbetween VGPs is the key of iParaphrasing. We apply various existing methods as\nwell as propose a novel neural network-based method with image attention, and\nreport the results of the first attempt toward iParaphrasing.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 00:58:59 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Chu", "Chenhui", ""], ["Otani", "Mayu", ""], ["Nakashima", "Yuta", ""]]}, {"id": "1806.04325", "submitter": "Jasper C.H. Lee", "authors": "Jasper C.H. Lee, Jimmy H.M. Lee, Allen Z. Zhong", "title": "Augmenting Stream Constraint Programming with Eventuality Conditions", "comments": "Added proofs and an appendix containing a constraint model that was\n  not included in the previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stream constraint programming is a recent addition to the family of\nconstraint programming frameworks, where variable domains are sets of infinite\nstreams over finite alphabets. Previous works showed promising results for its\napplicability to real-world planning and control problems. In this paper,\nmotivated by the modelling of planning applications, we improve the\nexpressiveness of the framework by introducing 1) the \"until\" constraint, a new\nconstruct that is adapted from Linear Temporal Logic and 2) the @ operator on\nstreams, a syntactic sugar for which we provide a more efficient solving\nalgorithm over simple desugaring. For both constructs, we propose corresponding\nnovel solving algorithms and prove their correctness. We present competitive\nexperimental results on the Missionaries and Cannibals logic puzzle and a\nstandard path planning application on the grid, by comparing with Apt and\nBrand's method for verifying eventuality conditions using a CP approach.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 04:20:02 GMT"}, {"version": "v2", "created": "Mon, 6 Aug 2018 20:17:43 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Lee", "Jasper C. H.", ""], ["Lee", "Jimmy H. M.", ""], ["Zhong", "Allen Z.", ""]]}, {"id": "1806.04387", "submitter": "Bhargav Chippada", "authors": "Bhargav Chippada and Shubajit Saha", "title": "Knowledge Amalgam: Generating Jokes and Quotes Together", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating humor and quotes are very challenging problems in the field of\ncomputational linguistics and are often tackled separately. In this paper, we\npresent a controlled Long Short-Term Memory (LSTM) architecture which is\ntrained with categorical data like jokes and quotes together by passing\ncategory as an input along with the sequence of words. The idea is that a\nsingle neural net will learn the structure of both jokes and quotes to generate\nthem on demand according to input category. Importantly, we believe the neural\nnet has more knowledge as it's trained on different datasets and hence will\nenable it to generate more creative jokes or quotes from the mixture of\ninformation. May the network generate a funny inspirational joke!\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 08:22:21 GMT"}, {"version": "v2", "created": "Wed, 13 Jun 2018 15:24:29 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Chippada", "Bhargav", ""], ["Saha", "Shubajit", ""]]}, {"id": "1806.04471", "submitter": "Frank Glavin", "authors": "Anthony M. Colwell and Frank G. Glavin", "title": "Colwell's Castle Defence: A Custom Game Using Dynamic Difficulty\n  Adjustment to Increase Player Enjoyment", "comments": "25th Irish Conference on Artificial Intelligence and Cognitive\n  Science At: Dublin Institute of Technology (http://ceur-ws.org/Vol-2086/)", "journal-ref": "In proceedings of AICS, Dublin Institute of Technology, pp.\n  275-282 (2017)", "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dynamic Difficulty Adjustment (DDA) is a mechanism used in video games that\nautomatically tailors the individual gaming experience to match an appropriate\ndifficulty setting. This is generally achieved by removing pre-defined\ndifficulty tiers such as Easy, Medium and Hard; and instead concentrates on\nbalancing the gameplay to match the challenge to the individual's abilities.\nThe work presented in this paper examines the implementation of DDA in a custom\nsurvival game developed by the author, namely Colwell's Castle Defence. The\npremise of this arcade-style game is to defend a castle from hordes of oncoming\nenemies. The AI system that we developed adjusts the enemy spawn rate based on\nthe current performance of the player. Specifically, we read the Player Health\nand Gate Health at the end of each level and then assign the player with an\nappropriate difficulty tier for the proceeding level. We tested the impact of\nour technique on thirty human players and concluded, based on questionnaire\nfeedback, that enabling the technique led to more enjoyable gameplay.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 12:46:18 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Colwell", "Anthony M.", ""], ["Glavin", "Frank G.", ""]]}, {"id": "1806.04497", "submitter": "Frank Glavin", "authors": "David L. Smyth, James Fennell, Sai Abinesh, Nazli B. Karimi, Frank G.\n  Glavin, Ihsan Ullah, Brett Drury, Michael G. Madden", "title": "A Virtual Environment with Multi-Robot Navigation, Analytics, and\n  Decision Support for Critical Incident Investigation", "comments": "27th International Joint Conference on Artificial Intelligence\n  (IJCAI), Stockholm, Sweden", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accidents and attacks that involve chemical, biological, radiological/nuclear\nor explosive (CBRNE) substances are rare, but can be of high consequence. Since\nthe investigation of such events is not anybody's routine work, a range of AI\ntechniques can reduce investigators' cognitive load and support\ndecision-making, including: planning the assessment of the scene; ongoing\nevaluation and updating of risks; control of autonomous vehicles for collecting\nimages and sensor data; reviewing images/videos for items of interest;\nidentification of anomalies; and retrieval of relevant documentation. Because\nof the rare and high-risk nature of these events, realistic simulations can\nsupport the development and evaluation of AI-based tools. We have developed\nrealistic models of CBRNE scenarios and implemented an initial set of tools.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 13:26:56 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Smyth", "David L.", ""], ["Fennell", "James", ""], ["Abinesh", "Sai", ""], ["Karimi", "Nazli B.", ""], ["Glavin", "Frank G.", ""], ["Ullah", "Ihsan", ""], ["Drury", "Brett", ""], ["Madden", "Michael G.", ""]]}, {"id": "1806.04535", "submitter": "Srishti Aggarwal", "authors": "Srishti Aggarwal, Kritik Mathur, Radhika Mamidi", "title": "Automatic Target Recovery for Hindi-English Code Mixed Puns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order for our computer systems to be more human-like, with a higher\nemotional quotient, they need to be able to process and understand intrinsic\nhuman language phenomena like humour. In this paper, we consider a subtype of\nhumour - puns, which are a common type of wordplay-based jokes. In particular,\nwe consider code-mixed puns which have become increasingly mainstream on social\nmedia, in informal conversations and advertisements and aim to build a system\nwhich can automatically identify the pun location and recover the target of\nsuch puns. We first study and classify code-mixed puns into two categories\nnamely intra-sentential and intra-word, and then propose a four-step algorithm\nto recover the pun targets for puns belonging to the intra-sentential category.\nOur algorithm uses language models, and phonetic similarity-based features to\nget the desired results. We test our approach on a small set of code-mixed\npunning advertisements, and observe that our system is successfully able to\nrecover the targets for 67% of the puns.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 09:45:34 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Aggarwal", "Srishti", ""], ["Mathur", "Kritik", ""], ["Mamidi", "Radhika", ""]]}, {"id": "1806.04552", "submitter": "Sreecharan Sankaranarayanan", "authors": "Sreecharan Sankaranarayanan, Raghuram Mandyam Annasamy, Katia Sycara,\n  Carolyn Penstein Ros\\'e", "title": "Combining Model-Free Q-Ensembles and Model-Based Approaches for Informed\n  Exploration", "comments": "Submitted to the Thirty-Second Annual Conference on Neural\n  Information Processing Systems (NIPS 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Q-Ensembles are a model-free approach where input images are fed into\ndifferent Q-networks and exploration is driven by the assumption that\nuncertainty is proportional to the variance of the output Q-values obtained.\nThey have been shown to perform relatively well compared to other exploration\nstrategies. Further, model-based approaches, such as encoder-decoder models\nhave been used successfully for next frame prediction given previous frames.\nThis paper proposes to integrate the model-free Q-ensembles and model-based\napproaches with the hope of compounding the benefits of both and achieving\nsuperior exploration as a result. Results show that a model-based trajectory\nmemory approach when combined with Q-ensembles produces superior performance\nwhen compared to only using Q-ensembles.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 14:24:02 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Sankaranarayanan", "Sreecharan", ""], ["Annasamy", "Raghuram Mandyam", ""], ["Sycara", "Katia", ""], ["Ros\u00e9", "Carolyn Penstein", ""]]}, {"id": "1806.04562", "submitter": "Thanh Thi Nguyen", "authors": "Thanh Nguyen, Ngoc Duy Nguyen, Saeid Nahavandi", "title": "Multi-Agent Deep Reinforcement Learning with Human Strategies", "comments": "2019 IEEE International Conference on Industrial Technology (ICIT),\n  Melbourne, Australia", "journal-ref": "2019 IEEE International Conference on Industrial Technology (ICIT)", "doi": "10.1109/ICIT.2019.8755032", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has enabled traditional reinforcement learning methods to deal\nwith high-dimensional problems. However, one of the disadvantages of deep\nreinforcement learning methods is the limited exploration capacity of learning\nagents. In this paper, we introduce an approach that integrates human\nstrategies to increase the exploration capacity of multiple deep reinforcement\nlearning agents. We also report the development of our own multi-agent\nenvironment called Multiple Tank Defence to simulate the proposed approach. The\nresults show the significant performance improvement of multiple agents that\nhave learned cooperatively with human strategies. This implies that there is a\ncritical need for human intellect teamed with machines to solve complex\nproblems. In addition, the success of this simulation indicates that our\nmulti-agent environment can be used as a testbed platform to develop and\nvalidate other multi-agent control algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 14:40:24 GMT"}, {"version": "v2", "created": "Thu, 30 May 2019 05:58:28 GMT"}], "update_date": "2019-07-30", "authors_parsed": [["Nguyen", "Thanh", ""], ["Nguyen", "Ngoc Duy", ""], ["Nahavandi", "Saeid", ""]]}, {"id": "1806.04611", "submitter": "Mejdi Ben Dkhil", "authors": "Mejdi Ben Dkhil, Ali Wali, and Adel M. Alimi", "title": "A Hierarchical Fuzzy System for an Advanced Driving Assistance System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we present a hierarchical fuzzy system by evaluating the risk\nstate for a Driver Assistance System in order to contribute in reducing the\nroad accident's number. A key component of this system is its ability to\ncontinually detect and test the inside and outside risks in real time: The\noutside car risks by detecting various road moving objects; this proposed\nsystem stands on computer vision approaches. The inside risks by presenting an\nautomatic system for drowsy driving identification or detection by evaluating\nEEG signals of the driver; this developed system is based on computer vision\ntechniques and biometrics factors (electroencephalogram EEG). This proposed\nsystem is then composed of three main modules. The first module is responsible\nfor identifying the driver drowsiness state through his eye movements (physical\ndrowsiness). The second one is responsible for detecting and analysing his\nphysiological signals to also identify his drowsiness state (moral drowsiness).\nThe third module is responsible to evaluate the road driving risks by detecting\nof the road different moving objects in a real time. The final decision will be\nobtained by merging of the three detection systems through the use of fuzzy\ndecision rules. Finally, the proposed approach has been improved on ten samples\nfrom a proposed dataset.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jun 2018 18:33:42 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Dkhil", "Mejdi Ben", ""], ["Wali", "Ali", ""], ["Alimi", "Adel M.", ""]]}, {"id": "1806.04624", "submitter": "Yangchen Pan", "authors": "Yangchen Pan, Muhammad Zaheer, Adam White, Andrew Patterson, Martha\n  White", "title": "Organizing Experience: A Deeper Look at Replay Mechanisms for\n  Sample-based Planning in Continuous State Domains", "comments": "IJCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based strategies for control are critical to obtain sample efficient\nlearning. Dyna is a planning paradigm that naturally interleaves learning and\nplanning, by simulating one-step experience to update the action-value\nfunction. This elegant planning strategy has been mostly explored in the\ntabular setting. The aim of this paper is to revisit sample-based planning, in\nstochastic and continuous domains with learned models. We first highlight the\nflexibility afforded by a model over Experience Replay (ER). Replay-based\nmethods can be seen as stochastic planning methods that repeatedly sample from\na buffer of recent agent-environment interactions and perform updates to\nimprove data efficiency. We show that a model, as opposed to a replay buffer,\nis particularly useful for specifying which states to sample from during\nplanning, such as predecessor states that propagate information in reverse from\na state more quickly. We introduce a semi-parametric model learning approach,\ncalled Reweighted Experience Models (REMs), that makes it simple to sample next\nstates or predecessors. We demonstrate that REM-Dyna exhibits similar\nadvantages over replay-based methods in learning in continuous state problems,\nand that the performance gap grows when moving to stochastic domains, of\nincreasing size.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 16:07:31 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Pan", "Yangchen", ""], ["Zaheer", "Muhammad", ""], ["White", "Adam", ""], ["Patterson", "Andrew", ""], ["White", "Martha", ""]]}, {"id": "1806.04640", "submitter": "Abhishek Gupta", "authors": "Abhishek Gupta, Benjamin Eysenbach, Chelsea Finn, Sergey Levine", "title": "Unsupervised Meta-Learning for Reinforcement Learning", "comments": "First two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-learning algorithms use past experience to learn to quickly solve new\ntasks. In the context of reinforcement learning, meta-learning algorithms\nacquire reinforcement learning procedures to solve new problems more\nefficiently by utilizing experience from prior tasks. The performance of\nmeta-learning algorithms depends on the tasks available for meta-training: in\nthe same way that supervised learning generalizes best to test points drawn\nfrom the same distribution as the training points, meta-learning methods\ngeneralize best to tasks from the same distribution as the meta-training tasks.\nIn effect, meta-reinforcement learning offloads the design burden from\nalgorithm design to task design. If we can automate the process of task design\nas well, we can devise a meta-learning algorithm that is truly automated. In\nthis work, we take a step in this direction, proposing a family of unsupervised\nmeta-learning algorithms for reinforcement learning. We motivate and describe a\ngeneral recipe for unsupervised meta-reinforcement learning, and present an\ninstantiation of this approach. Our conceptual and theoretical contributions\nconsist of formulating the unsupervised meta-reinforcement learning problem and\ndescribing how task proposals based on mutual information can be used to train\noptimal meta-learners. Our experimental results indicate that unsupervised\nmeta-reinforcement learning effectively acquires accelerated reinforcement\nlearning procedures without the need for manual task design and these\nprocedures exceed the performance of learning from scratch.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 16:48:52 GMT"}, {"version": "v2", "created": "Fri, 13 Dec 2019 22:01:41 GMT"}, {"version": "v3", "created": "Thu, 30 Apr 2020 16:55:56 GMT"}], "update_date": "2020-05-01", "authors_parsed": [["Gupta", "Abhishek", ""], ["Eysenbach", "Benjamin", ""], ["Finn", "Chelsea", ""], ["Levine", "Sergey", ""]]}, {"id": "1806.04718", "submitter": "Ahmed Khalifa", "authors": "Ahmed Khalifa, Scott Lee, Andy Nealen, Julian Togelius", "title": "Talakat: Bullet Hell Generation through Constrained Map-Elites", "comments": "The paper will be published in GECCO 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe a search-based approach to generating new levels for bullet hell\ngames, which are action games characterized by and requiring avoidance of a\nvery large amount of projectiles. Levels are represented using a\ndomain-specific description language, and search in the space defined by this\nlanguage is performed by a novel variant of the Map-Elites algorithm which\nincorporates a feasible- infeasible approach to constraint satisfaction.\nSimulation-based evaluation is used to gauge the fitness of levels, using an\nagent based on best-first search. The performance of the agent can be tuned\naccording to the two dimensions of strategy and dexterity, making it possible\nto search for level configurations that require a specific combination of both.\nAs far as we know, this paper describes the first generator for this game\ngenre, and includes several algorithmic innovations.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 19:02:19 GMT"}, {"version": "v2", "created": "Thu, 14 Jun 2018 01:38:52 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Khalifa", "Ahmed", ""], ["Lee", "Scott", ""], ["Nealen", "Andy", ""], ["Togelius", "Julian", ""]]}, {"id": "1806.04795", "submitter": "David Hallac", "authors": "David Hallac, Suvrat Bhooshan, Michael Chen, Kacem Abida, Rok Sosic,\n  Jure Leskovec", "title": "Drive2Vec: Multiscale State-Space Embedding of Vehicular Sensor Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With automobiles becoming increasingly reliant on sensors to perform various\ndriving tasks, it is important to encode the relevant CAN bus sensor data in a\nway that captures the general state of the vehicle in a compact form. In this\npaper, we develop a deep learning-based method, called Drive2Vec, for embedding\nsuch sensor data in a low-dimensional yet actionable form. Our method is based\non stacked gated recurrent units (GRUs). It accepts a short interval of\nautomobile sensor data as input and computes a low-dimensional representation\nof that data, which can then be used to accurately solve a range of tasks. With\nthis representation, we (1) predict the exact values of the sensors in the\nshort term (up to three seconds in the future), (2) forecast the long-term\naverage values of these same sensors, (3) infer additional contextual\ninformation that is not encoded in the data, including the identity of the\ndriver behind the wheel, and (4) build a knowledge base that can be used to\nauto-label data and identify risky states. We evaluate our approach on a\ndataset collected by Audi, which equipped a fleet of test vehicles with data\nloggers to store all sensor readings on 2,098 hours of driving on real roads.\nWe show in several experiments that our method outperforms other baselines by\nup to 90%, and we further demonstrate how these embeddings of sensor data can\nbe used to solve a variety of real-world automotive applications.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 23:19:54 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Hallac", "David", ""], ["Bhooshan", "Suvrat", ""], ["Chen", "Michael", ""], ["Abida", "Kacem", ""], ["Sosic", "Rok", ""], ["Leskovec", "Jure", ""]]}, {"id": "1806.04808", "submitter": "Guansong Pang", "authors": "Guansong Pang, Longbing Cao, Ling Chen, Huan Liu", "title": "Learning Representations of Ultrahigh-dimensional Data for Random\n  Distance-based Outlier Detection", "comments": "10 pages, 4 figures, 3 tables. To appear in the proceedings of KDD18,\n  Long presentation (oral)", "journal-ref": null, "doi": "10.1145/3219819.3220042", "report-no": null, "categories": "cs.LG cs.AI cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning expressive low-dimensional representations of ultrahigh-dimensional\ndata, e.g., data with thousands/millions of features, has been a major way to\nenable learning methods to address the curse of dimensionality. However,\nexisting unsupervised representation learning methods mainly focus on\npreserving the data regularity information and learning the representations\nindependently of subsequent outlier detection methods, which can result in\nsuboptimal and unstable performance of detecting irregularities (i.e.,\noutliers).\n  This paper introduces a ranking model-based framework, called RAMODO, to\naddress this issue. RAMODO unifies representation learning and outlier\ndetection to learn low-dimensional representations that are tailored for a\nstate-of-the-art outlier detection approach - the random distance-based\napproach. This customized learning yields more optimal and stable\nrepresentations for the targeted outlier detectors. Additionally, RAMODO can\nleverage little labeled data as prior knowledge to learn more expressive and\napplication-relevant representations. We instantiate RAMODO to an efficient\nmethod called REPEN to demonstrate the performance of RAMODO.\n  Extensive empirical results on eight real-world ultrahigh dimensional data\nsets show that REPEN (i) enables a random distance-based detector to obtain\nsignificantly better AUC performance and two orders of magnitude speedup; (ii)\nperforms substantially better and more stably than four state-of-the-art\nrepresentation learning methods; and (iii) leverages less than 1% labeled data\nto achieve up to 32% AUC improvement.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 00:53:56 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Pang", "Guansong", ""], ["Cao", "Longbing", ""], ["Chen", "Ling", ""], ["Liu", "Huan", ""]]}, {"id": "1806.04854", "submitter": "Mohammad Emtiyaz Khan", "authors": "Mohammad Emtiyaz Khan, Didrik Nielsen, Voot Tangkaratt, Wu Lin, Yarin\n  Gal, Akash Srivastava", "title": "Fast and Scalable Bayesian Deep Learning by Weight-Perturbation in Adam", "comments": "Camera ready version", "journal-ref": "Thirty-fifth International Conference on Machine Learning, 2018", "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertainty computation in deep learning is essential to design robust and\nreliable systems. Variational inference (VI) is a promising approach for such\ncomputation, but requires more effort to implement and execute compared to\nmaximum-likelihood methods. In this paper, we propose new natural-gradient\nalgorithms to reduce such efforts for Gaussian mean-field VI. Our algorithms\ncan be implemented within the Adam optimizer by perturbing the network weights\nduring gradient evaluations, and uncertainty estimates can be cheaply obtained\nby using the vector that adapts the learning rate. This requires lower memory,\ncomputation, and implementation effort than existing VI methods, while\nobtaining uncertainty estimates of comparable quality. Our empirical results\nconfirm this and further suggest that the weight-perturbation in our algorithm\ncould be useful for exploration in reinforcement learning and stochastic\noptimization.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 05:45:22 GMT"}, {"version": "v2", "created": "Sat, 7 Jul 2018 12:19:00 GMT"}, {"version": "v3", "created": "Thu, 2 Aug 2018 08:21:25 GMT"}], "update_date": "2018-08-03", "authors_parsed": [["Khan", "Mohammad Emtiyaz", ""], ["Nielsen", "Didrik", ""], ["Tangkaratt", "Voot", ""], ["Lin", "Wu", ""], ["Gal", "Yarin", ""], ["Srivastava", "Akash", ""]]}, {"id": "1806.04899", "submitter": "Yijun Bian", "authors": "Yijun Bian and Yijun Wang and Yaqiang Yao and Huanhuan Chen", "title": "Ensemble Pruning based on Objection Maximization with a General\n  Distributed Framework", "comments": "Accepted by TNNLS", "journal-ref": null, "doi": "10.1109/TNNLS.2019.2945116", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensemble pruning, selecting a subset of individual learners from an original\nensemble, alleviates the deficiencies of ensemble learning on the cost of time\nand space. Accuracy and diversity serve as two crucial factors while they\nusually conflict with each other. To balance both of them, we formalize the\nensemble pruning problem as an objection maximization problem based on\ninformation entropy. Then we propose an ensemble pruning method including a\ncentralized version and a distributed version, in which the latter is to speed\nup the former. At last, we extract a general distributed framework for ensemble\npruning, which can be widely suitable for most of the existing ensemble pruning\nmethods and achieve less time consuming without much accuracy degradation.\nExperimental results validate the efficiency of our framework and methods,\nparticularly concerning a remarkable improvement of the execution speed,\naccompanied by gratifying accuracy performance.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 08:58:49 GMT"}, {"version": "v2", "created": "Sat, 16 Jun 2018 03:40:07 GMT"}, {"version": "v3", "created": "Mon, 30 Sep 2019 12:22:24 GMT"}], "update_date": "2021-01-20", "authors_parsed": [["Bian", "Yijun", ""], ["Wang", "Yijun", ""], ["Yao", "Yaqiang", ""], ["Chen", "Huanhuan", ""]]}, {"id": "1806.04915", "submitter": "Dimiter Dobrev", "authors": "Dimiter Dobrev", "title": "The IQ of Artificial Intelligence", "comments": null, "journal-ref": "Serdica Journal of Computing, Vol. 13, Number 1-2, 2019, pp.41-70", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  All it takes to identify the computer programs which are Artificial\nIntelligence is to give them a test and award AI to those that pass the test.\nLet us say that the scores they earn at the test will be called IQ. We cannot\npinpoint a minimum IQ threshold that a program has to cover in order to be AI,\nhowever, we will choose a certain value. Thus, our definition for AI will be\nany program the IQ of which is above the chosen value. While this idea has\nalready been implemented in [3], here we will revisit this construct in order\nto introduce certain improvements.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 09:29:42 GMT"}], "update_date": "2020-10-12", "authors_parsed": [["Dobrev", "Dimiter", ""]]}, {"id": "1806.04952", "submitter": "Markus Schr\\\"oder", "authors": "Markus Schr\\\"oder and Christian Jilek and J\\\"orn Hees and Andreas\n  Dengel", "title": "Towards Semantically Enhanced Data Understanding", "comments": "4 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of machine learning, data understanding is the practice of\ngetting initial insights in unknown datasets. Such knowledge-intensive tasks\nrequire a lot of documentation, which is necessary for data scientists to grasp\nthe meaning of the data. Usually, documentation is separate from the data in\nvarious external documents, diagrams, spreadsheets and tools which causes\nconsiderable look up overhead. Moreover, other supporting applications are not\nable to consume and utilize such unstructured data. That is why we propose a\nmethodology that uses a single semantic model that interlinks data with its\ndocumentation. Hence, data scientists are able to directly look up the\nconnected information about the data by simply following links. Equally, they\ncan browse the documentation which always refers to the data. Furthermore, the\nmodel can be used by other approaches providing additional support, like\nsearching, comparing, integrating or visualizing data. To showcase our approach\nwe also demonstrate an early prototype.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 11:19:33 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Schr\u00f6der", "Markus", ""], ["Jilek", "Christian", ""], ["Hees", "J\u00f6rn", ""], ["Dengel", "Andreas", ""]]}, {"id": "1806.04959", "submitter": "Hoda Heidari", "authors": "Hoda Heidari, Claudio Ferrari, Krishna P. Gummadi, and Andreas Krause", "title": "Fairness Behind a Veil of Ignorance: A Welfare Analysis for Automated\n  Decision Making", "comments": "Conference: Thirty-second Conference on Neural Information Processing\n  Systems (NIPS 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We draw attention to an important, yet largely overlooked aspect of\nevaluating fairness for automated decision making systems---namely risk and\nwelfare considerations. Our proposed family of measures corresponds to the\nlong-established formulations of cardinal social welfare in economics, and is\njustified by the Rawlsian conception of fairness behind a veil of ignorance.\nThe convex formulation of our welfare-based measures of fairness allows us to\nintegrate them as a constraint into any convex loss minimization pipeline. Our\nempirical analysis reveals interesting trade-offs between our proposal and (a)\nprediction accuracy, (b) group discrimination, and (c) Dwork et al.'s notion of\nindividual fairness. Furthermore and perhaps most importantly, our work\nprovides both heuristic justification and empirical evidence suggesting that a\nlower-bound on our measures often leads to bounded inequality in algorithmic\noutcomes; hence presenting the first computationally feasible mechanism for\nbounding individual-level inequality.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 11:36:05 GMT"}, {"version": "v2", "created": "Mon, 29 Oct 2018 09:39:14 GMT"}, {"version": "v3", "created": "Wed, 28 Nov 2018 13:29:41 GMT"}, {"version": "v4", "created": "Fri, 11 Jan 2019 11:36:36 GMT"}], "update_date": "2019-01-14", "authors_parsed": [["Heidari", "Hoda", ""], ["Ferrari", "Claudio", ""], ["Gummadi", "Krishna P.", ""], ["Krause", "Andreas", ""]]}, {"id": "1806.04968", "submitter": "Chengliang Chai", "authors": "Chengliang Chai, Ju Fan, Guoliang Li, Jiannan Wang, Yudian Zheng", "title": "Crowd-Powered Data Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many data mining tasks cannot be completely addressed by auto- mated\nprocesses, such as sentiment analysis and image classification. Crowdsourcing\nis an effective way to harness the human cognitive ability to process these\nmachine-hard tasks. Thanks to public crowdsourcing platforms, e.g., Amazon\nMechanical Turk and Crowd- Flower, we can easily involve hundreds of thousands\nof ordinary workers (i.e., the crowd) to address these machine-hard tasks. In\nthis tutorial, we will survey and synthesize a wide spectrum of existing\nstudies on crowd-powered data mining. We first give an overview of\ncrowdsourcing, and then summarize the fundamental techniques, including quality\ncontrol, cost control, and latency control, which must be considered in\ncrowdsourced data mining. Next we review crowd-powered data mining operations,\nincluding classification, clustering, pattern mining, machine learning using\nthe crowd (including deep learning, transfer learning and semi-supervised\nlearning) and knowledge discovery. Finally, we provide the emerging challenges\nin crowdsourced data mining.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 12:04:20 GMT"}, {"version": "v2", "created": "Fri, 19 Oct 2018 02:31:26 GMT"}], "update_date": "2018-10-22", "authors_parsed": [["Chai", "Chengliang", ""], ["Fan", "Ju", ""], ["Li", "Guoliang", ""], ["Wang", "Jiannan", ""], ["Zheng", "Yudian", ""]]}, {"id": "1806.05049", "submitter": "Paul Swoboda", "authors": "Paul Swoboda and Vladimir Kolmogorov", "title": "MAP inference via Block-Coordinate Frank-Wolfe Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new proximal bundle method for Maximum-A-Posteriori (MAP)\ninference in structured energy minimization problems. The method optimizes a\nLagrangean relaxation of the original energy minimization problem using a multi\nplane block-coordinate Frank-Wolfe method that takes advantage of the specific\nstructure of the Lagrangean decomposition. We show empirically that our method\noutperforms state-of-the-art Lagrangean decomposition based algorithms on some\nchallenging Markov Random Field, multi-label discrete tomography and graph\nmatching problems.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 14:03:38 GMT"}, {"version": "v2", "created": "Fri, 5 Apr 2019 14:27:02 GMT"}], "update_date": "2019-04-08", "authors_parsed": [["Swoboda", "Paul", ""], ["Kolmogorov", "Vladimir", ""]]}, {"id": "1806.05085", "submitter": "Jingyan Wang", "authors": "Jingyan Wang and Nihar B. Shah", "title": "Your 2 is My 1, Your 3 is My 9: Handling Arbitrary Miscalibrations in\n  Ratings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardinal scores (numeric ratings) collected from people are well known to\nsuffer from miscalibrations. A popular approach to address this issue is to\nassume simplistic models of miscalibration (such as linear biases) to de-bias\nthe scores. This approach, however, often fares poorly because people's\nmiscalibrations are typically far more complex and not well understood. In the\nabsence of simplifying assumptions on the miscalibration, it is widely believed\nby the crowdsourcing community that the only useful information in the cardinal\nscores is the induced ranking. In this paper, inspired by the framework of\nStein's shrinkage, empirical Bayes, and the classic two-envelope problem, we\ncontest this widespread belief. Specifically, we consider cardinal scores with\narbitrary (or even adversarially chosen) miscalibrations which are only\nrequired to be consistent with the induced ranking. We design estimators which\ndespite making no assumptions on the miscalibration, strictly and uniformly\noutperform all possible estimators that rely on only the ranking. Our\nestimators are flexible in that they can be used as a plug-in for a variety of\napplications, and we provide a proof-of-concept for A/B testing and ranking.\nOur results thus provide novel insights in the eternal debate between cardinal\nand ordinal data.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 14:28:41 GMT"}, {"version": "v2", "created": "Thu, 13 Sep 2018 03:03:58 GMT"}], "update_date": "2018-09-14", "authors_parsed": [["Wang", "Jingyan", ""], ["Shah", "Nihar B.", ""]]}, {"id": "1806.05106", "submitter": "Frank Glavin", "authors": "Frank G. Glavin and Michael G. Madden", "title": "DRE-Bot: A Hierarchical First Person Shooter Bot Using Multiple\n  Sarsa({\\lambda}) Reinforcement Learners", "comments": "17th International Conference on Computer Games (CGAMES) 2012", "journal-ref": "In Computer Games (CGAMES), 2012 17th International Conference on,\n  pp. 148-152. IEEE, 2012", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes an architecture for controlling non-player characters\n(NPC) in the First Person Shooter (FPS) game Unreal Tournament 2004.\nSpecifically, the DRE-Bot architecture is made up of three reinforcement\nlearners, Danger, Replenish and Explore, which use the tabular Sarsa({\\lambda})\nalgorithm. This algorithm enables the NPC to learn through trial and error\nbuilding up experience over time in an approach inspired by human learning.\nExperimentation is carried to measure the performance of DRE-Bot when competing\nagainst fixed strategy bots that ship with the game. The discount parameter,\n{\\gamma}, and the trace parameter, {\\lambda}, are also varied to see if their\nvalues have an effect on the performance.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 15:19:34 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Glavin", "Frank G.", ""], ["Madden", "Michael G.", ""]]}, {"id": "1806.05108", "submitter": "Theophanes Raptis Mr", "authors": "Theophanes E. Raptis", "title": "Holographic Automata for Ambient Immersive A. I. via Reservoir Computing", "comments": "14 p., 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove the existence of a semilinear representation of Cellular Automata\n(CA) with the introduction of multiple convolution kernels. Examples of the\ntechnique are presented for rules akin to the \"edge-of-chaos\" including the\nTuring universal rule 110 for further utilization in the area of reservoir\ncomputing. We also examine the significance of their dual representation on a\nfrequency or wavelength domain as a superposition of plane waves for\ndistributed computing applications including a new proposal for a \"Hologrid\"\nthat could be realized with present Wi-Fi,Li-Fi technologies.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2018 13:01:39 GMT"}, {"version": "v2", "created": "Wed, 20 Jun 2018 13:25:20 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Raptis", "Theophanes E.", ""]]}, {"id": "1806.05112", "submitter": "Junpei Komiyama", "authors": "Junpei Komiyama and Hajime Shimao", "title": "Comparing Fairness Criteria Based on Social Outcome", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fairness in algorithmic decision-making processes is attracting increasing\nconcern. When an algorithm is applied to human-related decision-making an\nestimator solely optimizing its predictive power can learn biases on the\nexisting data, which motivates us the notion of fairness in machine learning.\nwhile several different notions are studied in the literature, little studies\nare done on how these notions affect the individuals. We demonstrate such a\ncomparison between several policies induced by well-known fairness criteria,\nincluding the color-blind (CB), the demographic parity (DP), and the equalized\nodds (EO). We show that the EO is the only criterion among them that removes\ngroup-level disparity. Empirical studies on the social welfare and disparity of\nthese policies are conducted.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 15:34:13 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Komiyama", "Junpei", ""], ["Shimao", "Hajime", ""]]}, {"id": "1806.05117", "submitter": "Frank Glavin", "authors": "Frank G. Glavin and Michael G. Madden", "title": "Learning to Shoot in First Person Shooter Games by Stabilizing Actions\n  and Clustering Rewards for Reinforcement Learning", "comments": "IEEE Conference on Computational Intelligence and Games (CIG), 2015", "journal-ref": "In Conference on Computational Intelligence and Games, pp.\n  344-351. 2015", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While reinforcement learning (RL) has been applied to turn-based board games\nfor many years, more complex games involving decision-making in real-time are\nbeginning to receive more attention. A challenge in such environments is that\nthe time that elapses between deciding to take an action and receiving a reward\nbased on its outcome can be longer than the interval between successive\ndecisions. We explore this in the context of a non-player character (NPC) in a\nmodern first-person shooter game. Such games take place in 3D environments\nwhere players, both human and computer-controlled, compete by engaging in\ncombat and completing task objectives. We investigate the use of RL to enable\nNPCs to gather experience from game-play and improve their shooting skill over\ntime from a reward signal based on the damage caused to opponents. We propose a\nnew method for RL updates and reward calculations, in which the updates are\ncarried out periodically, after each shooting encounter has ended, and a new\nweighted-reward mechanism is used which increases the reward applied to actions\nthat lead to damaging the opponent in successive hits in what we term \"hit\nclusters\".\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 15:41:34 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Glavin", "Frank G.", ""], ["Madden", "Michael G.", ""]]}, {"id": "1806.05141", "submitter": "Baibhab Chatterjee", "authors": "Baibhab Chatterjee, Priyadarshini Panda, Shovan Maity, Ayan Biswas,\n  Kaushik Roy, and Shreyas Sen", "title": "Exploiting Inherent Error-Resiliency of Neuromorphic Computing to\n  achieve Extreme Energy-Efficiency through Mixed-Signal Neurons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuromorphic computing, inspired by the brain, promises extreme efficiency\nfor certain classes of learning tasks, such as classification and pattern\nrecognition. The performance and power consumption of neuromorphic computing\ndepends heavily on the choice of the neuron architecture. Digital neurons\n(Dig-N) are conventionally known to be accurate and efficient at high speed,\nwhile suffering from high leakage currents from a large number of transistors\nin a large design. On the other hand, analog/mixed-signal neurons are prone to\nnoise, variability and mismatch, but can lead to extremely low-power designs.\nIn this work, we will analyze, compare and contrast existing neuron\narchitectures with a proposed mixed-signal neuron (MS-N) in terms of\nperformance, power and noise, thereby demonstrating the applicability of the\nproposed mixed-signal neuron for achieving extreme energy-efficiency in\nneuromorphic computing. The proposed MS-N is implemented in 65 nm CMOS\ntechnology and exhibits > 100X better energy-efficiency across all frequencies\nover two traditional digital neurons synthesized in the same technology node.\nWe also demonstrate that the inherent error-resiliency of a fully connected or\neven convolutional neural network (CNN) can handle the noise as well as the\nmanufacturing non-idealities of the MS-N up to certain degrees. Notably, a\nsystem-level implementation on MNIST datasets exhibits a worst-case increase in\nclassification error by 2.1% when the integrated noise power in the bandwidth\nis ~ 0.1 uV2, along with +-3{\\sigma} amount of variation and mismatch\nintroduced in the transistor parameters for the proposed neuron with 8-bit\nprecision.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 16:43:05 GMT"}], "update_date": "2018-06-14", "authors_parsed": [["Chatterjee", "Baibhab", ""], ["Panda", "Priyadarshini", ""], ["Maity", "Shovan", ""], ["Biswas", "Ayan", ""], ["Roy", "Kaushik", ""], ["Sen", "Shreyas", ""]]}, {"id": "1806.05180", "submitter": "Andreas Hanselowski Dr.", "authors": "Andreas Hanselowski, Avinesh PVS, Benjamin Schiller, Felix Caspelherr,\n  Debanjan Chaudhuri, Christian M. Meyer and Iryna Gurevych", "title": "A Retrospective Analysis of the Fake News Challenge Stance Detection\n  Task", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 2017 Fake News Challenge Stage 1 (FNC-1) shared task addressed a stance\nclassification task as a crucial first step towards detecting fake news. To\ndate, there is no in-depth analysis paper to critically discuss FNC-1's\nexperimental setup, reproduce the results, and draw conclusions for\nnext-generation stance classification methods. In this paper, we provide such\nan in-depth analysis for the three top-performing systems. We first find that\nFNC-1's proposed evaluation metric favors the majority class, which can be\neasily classified, and thus overestimates the true discriminative power of the\nmethods. Therefore, we propose a new F1-based metric yielding a changed system\nranking. Next, we compare the features and architectures used, which leads to a\nnovel feature-rich stacked LSTM model that performs on par with the best\nsystems, but is superior in predicting minority classes. To understand the\nmethods' ability to generalize, we derive a new dataset and perform both\nin-domain and cross-domain experiments. Our qualitative and quantitative study\nhelps interpreting the original FNC-1 scores and understand which features help\nimproving performance and why. Our new dataset and all source code used during\nthe reproduction study are publicly available for future research.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 15:38:09 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Hanselowski", "Andreas", ""], ["PVS", "Avinesh", ""], ["Schiller", "Benjamin", ""], ["Caspelherr", "Felix", ""], ["Chaudhuri", "Debanjan", ""], ["Meyer", "Christian M.", ""], ["Gurevych", "Iryna", ""]]}, {"id": "1806.05234", "submitter": "Daniele Funaro", "authors": "Daniele Funaro", "title": "Understanding the Meaning of Understanding", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can we train a machine to detect if another machine has understood a concept?\nIn principle, this is possible by conducting tests on the subject of that\nconcept. However we want this procedure to be done by avoiding direct\nquestions. In other words, we would like to isolate the absolute meaning of an\nabstract idea by putting it into a class of equivalence, hence without adopting\nstraight definitions or showing how this idea \"works\" in practice. We discuss\nthe metaphysical implications hidden in the above question, with the aim of\nproviding a plausible reference framework.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 19:26:55 GMT"}, {"version": "v2", "created": "Tue, 5 Feb 2019 13:31:59 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Funaro", "Daniele", ""]]}, {"id": "1806.05236", "submitter": "Vikas Verma", "authors": "Vikas Verma, Alex Lamb, Christopher Beckham, Amir Najafi, Ioannis\n  Mitliagkas, Aaron Courville, David Lopez-Paz, Yoshua Bengio", "title": "Manifold Mixup: Better Representations by Interpolating Hidden States", "comments": "To appear in ICML 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks excel at learning the training data, but often provide\nincorrect and confident predictions when evaluated on slightly different test\nexamples. This includes distribution shifts, outliers, and adversarial\nexamples. To address these issues, we propose Manifold Mixup, a simple\nregularizer that encourages neural networks to predict less confidently on\ninterpolations of hidden representations. Manifold Mixup leverages semantic\ninterpolations as additional training signal, obtaining neural networks with\nsmoother decision boundaries at multiple levels of representation. As a result,\nneural networks trained with Manifold Mixup learn class-representations with\nfewer directions of variance. We prove theory on why this flattening happens\nunder ideal conditions, validate it on practical situations, and connect it to\nprevious works on information theory and generalization. In spite of incurring\nno significant computation and being implemented in a few lines of code,\nManifold Mixup improves strong baselines in supervised learning, robustness to\nsingle-step adversarial attacks, and test log-likelihood.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 19:32:59 GMT"}, {"version": "v2", "created": "Mon, 9 Jul 2018 18:10:16 GMT"}, {"version": "v3", "created": "Thu, 4 Oct 2018 20:49:11 GMT"}, {"version": "v4", "created": "Tue, 5 Feb 2019 18:18:18 GMT"}, {"version": "v5", "created": "Sat, 9 Mar 2019 18:27:06 GMT"}, {"version": "v6", "created": "Tue, 12 Mar 2019 02:14:07 GMT"}, {"version": "v7", "created": "Sat, 11 May 2019 16:50:55 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Verma", "Vikas", ""], ["Lamb", "Alex", ""], ["Beckham", "Christopher", ""], ["Najafi", "Amir", ""], ["Mitliagkas", "Ioannis", ""], ["Courville", "Aaron", ""], ["Lopez-Paz", "David", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1806.05250", "submitter": "Edward Raff", "authors": "Jared Sylvester, Edward Raff", "title": "What About Applied Fairness?", "comments": "Accepted at Machine Learning: The Debates (ML-D), at ICML Stockholm,\n  Sweden, 2018. 5 pages", "journal-ref": "Machine Learning: The Debates (ML-D), at ICML Stockholm, Sweden,\n  2018", "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning practitioners are often ambivalent about the ethical aspects\nof their products. We believe anything that gets us from that current state to\none in which our systems are achieving some degree of fairness is an\nimprovement that should be welcomed. This is true even when that progress does\nnot get us 100% of the way to the goal of \"complete\" fairness or perfectly\nalign with our personal belief on which measure of fairness is used. Some\nmeasure of fairness being built would still put us in a better position than\nthe status quo. Impediments to getting fairness and ethical concerns applied in\nreal applications, whether they are abstruse philosophical debates or technical\noverhead such as the introduction of ever more hyper-parameters, should be\navoided. In this paper we further elaborate on our argument for this viewpoint\nand its importance.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 20:15:28 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Sylvester", "Jared", ""], ["Raff", "Edward", ""]]}, {"id": "1806.05292", "submitter": "Aleksandr Panov", "authors": "Aleksandr I. Panov, Aleksey Skrynnik", "title": "Automatic formation of the structure of abstract machines in\n  hierarchical reinforcement learning with state clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new approach to hierarchy formation and task decomposition in\nhierarchical reinforcement learning. Our method is based on the Hierarchy Of\nAbstract Machines (HAM) framework because HAM approach is able to design\nefficient controllers that will realize specific behaviors in real robots. The\nkey to our algorithm is the introduction of the internal or \"mental\"\nenvironment in which the state represents the structure of the HAM hierarchy.\nThe internal action in this environment leads to changes the hierarchy of HAMs.\nWe propose the classical Q-learning procedure in the internal environment which\nallows the agent to obtain an optimal hierarchy. We extends the HAM framework\nby adding on-model approach to select the appropriate sub-machine to execute\naction sequences for certain class of external environment states. Preliminary\nexperiments demonstrated the prospects of the method.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 22:40:49 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Panov", "Aleksandr I.", ""], ["Skrynnik", "Aleksey", ""]]}, {"id": "1806.05298", "submitter": "Juan C. Cuevas-Tello", "authors": "J.C. Cuevas-Tello", "title": "Apuntes de Redes Neuronales Artificiales", "comments": "20 pages, in Spanish", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  These handouts are designed for people who is just starting involved with the\ntopic artificial neural networks. We show how it works a single artificial\nneuron (McCulloch & Pitt model), mathematically and graphically. We do explain\nthe delta rule, a learning algorithm to find the neuron weights. We also\npresent some examples in MATLAB/Octave. There are examples for classification\ntask for lineal and non-lineal problems. At the end, we present an artificial\nneural network, a feed-forward neural network along its learning algorithm\nbackpropagation.\n  -----\n  Estos apuntes est\\'an dise\\~nados para personas que por primera vez se\nintroducen en el tema de las redes neuronales artificiales. Se muestra el\nfuncionamiento b\\'asico de una neurona, matem\\'aticamente y gr\\'aficamente. Se\nexplica la Regla Delta, algoritmo deaprendizaje para encontrar los pesos de una\nneurona. Tambi\\'en se muestran ejemplos en MATLAB/Octave. Hay ejemplos para\nproblemas de clasificaci\\'on, para problemas lineales y no-lineales. En la\nparte final se muestra la arquitectura de red neuronal artificial conocida como\nbackpropagation.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 23:23:22 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Cuevas-Tello", "J. C.", ""]]}, {"id": "1806.05299", "submitter": "Takayuki Yamada", "authors": "Takayuki Yamada", "title": "Geometric Shape Features Extraction Using a Steady State Partial\n  Differential Equation System", "comments": "31 pages, 10 figures", "journal-ref": "Journal of Computational Design and Engineering, 2019", "doi": "10.1016/j.jcde.2019.03.006", "report-no": null, "categories": "cs.CV cs.AI cs.GR math.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A unified method for extracting geometric shape features from binary image\ndata using a steady state partial differential equation (PDE) system as a\nboundary value problem is presented in this paper. The PDE and functions are\nformulated to extract the thickness, orientation, and skeleton simultaneously.\nThe main advantages of the proposed method is that the orientation is defined\nwithout derivatives and thickness computation is not imposed a topological\nconstraint on the target shape. A one-dimensional analytical solution is\nprovided to validate the proposed method. In addition, two-dimensional\nnumerical examples are presented to confirm the usefulness of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 23:33:08 GMT"}, {"version": "v2", "created": "Fri, 25 Jan 2019 02:57:20 GMT"}, {"version": "v3", "created": "Sat, 13 Apr 2019 22:57:30 GMT"}], "update_date": "2019-04-17", "authors_parsed": [["Yamada", "Takayuki", ""]]}, {"id": "1806.05337", "submitter": "Chandan Singh", "authors": "Chandan Singh, W. James Murdoch, Bin Yu", "title": "Hierarchical interpretations for neural network predictions", "comments": "Published in ICLR 2019", "journal-ref": "ICLR 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) have achieved impressive predictive performance\ndue to their ability to learn complex, non-linear relationships between\nvariables. However, the inability to effectively visualize these relationships\nhas led to DNNs being characterized as black boxes and consequently limited\ntheir applications. To ameliorate this problem, we introduce the use of\nhierarchical interpretations to explain DNN predictions through our proposed\nmethod, agglomerative contextual decomposition (ACD). Given a prediction from a\ntrained DNN, ACD produces a hierarchical clustering of the input features,\nalong with the contribution of each cluster to the final prediction. This\nhierarchy is optimized to identify clusters of features that the DNN learned\nare predictive. Using examples from Stanford Sentiment Treebank and ImageNet,\nwe show that ACD is effective at diagnosing incorrect predictions and\nidentifying dataset bias. Through human experiments, we demonstrate that ACD\nenables users both to identify the more accurate of two DNNs and to better\ntrust a DNN's outputs. We also find that ACD's hierarchy is largely robust to\nadversarial perturbations, implying that it captures fundamental aspects of the\ninput and ignores spurious noise.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 02:41:03 GMT"}, {"version": "v2", "created": "Wed, 16 Jan 2019 07:15:40 GMT"}], "update_date": "2019-01-17", "authors_parsed": [["Singh", "Chandan", ""], ["Murdoch", "W. James", ""], ["Yu", "Bin", ""]]}, {"id": "1806.05415", "submitter": "Alberto Maria Metelli", "authors": "Alberto Maria Metelli, Mirco Mutti and Marcello Restelli", "title": "Configurable Markov Decision Processes", "comments": null, "journal-ref": "Proceedings of the 35 th International Conference on Machine\n  Learning, Stockholm, Sweden, PMLR 80, 2018", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world problems, there is the possibility to configure, to a\nlimited extent, some environmental parameters to improve the performance of a\nlearning agent. In this paper, we propose a novel framework, Configurable\nMarkov Decision Processes (Conf-MDPs), to model this new type of interaction\nwith the environment. Furthermore, we provide a new learning algorithm, Safe\nPolicy-Model Iteration (SPMI), to jointly and adaptively optimize the policy\nand the environment configuration. After having introduced our approach and\nderived some theoretical results, we present the experimental evaluation in two\nexplicative problems to show the benefits of the environment configurability on\nthe performance of the learned policy.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 08:54:38 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Metelli", "Alberto Maria", ""], ["Mutti", "Mirco", ""], ["Restelli", "Marcello", ""]]}, {"id": "1806.05421", "submitter": "Rahaf Aljundi", "authors": "Rahaf Aljundi, Marcus Rohrbach and Tinne Tuytelaars", "title": "Selfless Sequential Learning", "comments": "Published as a conference paper at ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential learning, also called lifelong learning, studies the problem of\nlearning tasks in a sequence with access restricted to only the data of the\ncurrent task. In this paper we look at a scenario with fixed model capacity,\nand postulate that the learning process should not be selfish, i.e. it should\naccount for future tasks to be added and thus leave enough capacity for them.\nTo achieve Selfless Sequential Learning we study different regularization\nstrategies and activation functions. We find that imposing sparsity at the\nlevel of the representation (i.e.~neuron activations) is more beneficial for\nsequential learning than encouraging parameter sparsity. In particular, we\npropose a novel regularizer, that encourages representation sparsity by means\nof neural inhibition. It results in few active neurons which in turn leaves\nmore free neurons to be utilized by upcoming tasks. As neural inhibition over\nan entire layer can be too drastic, especially for complex tasks requiring\nstrong representations, our regularizer only inhibits other neurons in a local\nneighbourhood, inspired by lateral inhibition processes in the brain. We\ncombine our novel regularizer, with state-of-the-art lifelong learning methods\nthat penalize changes to important previously learned parts of the network. We\nshow that our new regularizer leads to increased sparsity which translates in\nconsistent performance improvement %over alternative regularizers we studied on\ndiverse datasets.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 09:06:10 GMT"}, {"version": "v2", "created": "Fri, 15 Jun 2018 07:23:59 GMT"}, {"version": "v3", "created": "Mon, 8 Oct 2018 13:58:26 GMT"}, {"version": "v4", "created": "Sun, 16 Dec 2018 02:13:23 GMT"}, {"version": "v5", "created": "Fri, 12 Apr 2019 14:47:02 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Aljundi", "Rahaf", ""], ["Rohrbach", "Marcus", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1806.05484", "submitter": "Lina Rojas-Barahona", "authors": "Lina M.Rojas-Barahona, Stefan Ultes, Pawel Budzianowski, I\\~nigo\n  Casanueva, Milica Gasic, Bo-Hsiang Tseng and Steve Young", "title": "Nearly Zero-Shot Learning for Semantic Decoding in Spoken Dialogue\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents two ways of dealing with scarce data in semantic decoding\nusing N-Best speech recognition hypotheses. First, we learn features by using a\ndeep learning architecture in which the weights for the unknown and known\ncategories are jointly optimised. Second, an unsupervised method is used for\nfurther tuning the weights. Sharing weights injects prior knowledge to unknown\ncategories. The unsupervised tuning (i.e. the risk minimisation) improves the\nF-Measure when recognising nearly zero-shot data on the DSTC3 corpus. This\nunsupervised method can be applied subject to two assumptions: the rank of the\nclass marginal is assumed to be known and the class-conditional scores of the\nclassifier are assumed to follow a Gaussian distribution.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 11:47:14 GMT"}, {"version": "v2", "created": "Thu, 21 Jun 2018 11:55:21 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Rojas-Barahona", "Lina M.", ""], ["Ultes", "Stefan", ""], ["Budzianowski", "Pawel", ""], ["Casanueva", "I\u00f1igo", ""], ["Gasic", "Milica", ""], ["Tseng", "Bo-Hsiang", ""], ["Young", "Steve", ""]]}, {"id": "1806.05502", "submitter": "Fabian B. Fuchs Mr", "authors": "Fabian B. Fuchs, Oliver Groth, Adam R. Kosiorek, Alex Bewley, Markus\n  Wulfmeier, Andrea Vedaldi, Ingmar Posner", "title": "Scrutinizing and De-Biasing Intuitive Physics with Neural Stethoscopes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visually predicting the stability of block towers is a popular task in the\ndomain of intuitive physics. While previous work focusses on prediction\naccuracy, a one-dimensional performance measure, we provide a broader analysis\nof the learned physical understanding of the final model and how the learning\nprocess can be guided. To this end, we introduce neural stethoscopes as a\ngeneral purpose framework for quantifying the degree of importance of specific\nfactors of influence in deep neural networks as well as for actively promoting\nand suppressing information as appropriate. In doing so, we unify concepts from\nmultitask learning as well as training with auxiliary and adversarial losses.\nWe apply neural stethoscopes to analyse the state-of-the-art neural network for\nstability prediction. We show that the baseline model is susceptible to being\nmisled by incorrect visual cues. This leads to a performance breakdown to the\nlevel of random guessing when training on scenarios where visual cues are\ninversely correlated with stability. Using stethoscopes to promote meaningful\nfeature extraction increases performance from 51% to 90% prediction accuracy.\nConversely, training on an easy dataset where visual cues are positively\ncorrelated with stability, the baseline model learns a bias leading to poor\nperformance on a harder dataset. Using an adversarial stethoscope, the network\nis successfully de-biased, leading to a performance increase from 66% to 88%.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 12:35:50 GMT"}, {"version": "v2", "created": "Fri, 15 Jun 2018 09:51:18 GMT"}, {"version": "v3", "created": "Thu, 2 May 2019 19:31:50 GMT"}, {"version": "v4", "created": "Wed, 4 Sep 2019 16:32:21 GMT"}, {"version": "v5", "created": "Fri, 6 Sep 2019 13:49:37 GMT"}], "update_date": "2019-09-09", "authors_parsed": [["Fuchs", "Fabian B.", ""], ["Groth", "Oliver", ""], ["Kosiorek", "Adam R.", ""], ["Bewley", "Alex", ""], ["Wulfmeier", "Markus", ""], ["Vedaldi", "Andrea", ""], ["Posner", "Ingmar", ""]]}, {"id": "1806.05522", "submitter": "Won-Yong Shin", "authors": "Minh D. Nguyen, Won-Yong Shin", "title": "Improved Density-Based Spatio--Textual Clustering on Social Media", "comments": "14 pages, 10 figures, 6 tables, Submitted for publication to the IEEE\n  Transactions on Knowledge and Data Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DBSCAN may not be sufficient when the input data type is heterogeneous in\nterms of textual description. When we aim to discover clusters of geo-tagged\nrecords relevant to a particular point-of-interest (POI) on social media,\nexamining only one type of input data (e.g., the tweets relevant to a POI) may\ndraw an incomplete picture of clusters due to noisy regions. To overcome this\nproblem, we introduce DBSTexC, a newly defined density-based clustering\nalgorithm using spatio--textual information. We first characterize POI-relevant\nand POI-irrelevant tweets as the texts that include and do not include a POI\nname or its semantically coherent variations, respectively. By leveraging the\nproportion of POI-relevant and POI-irrelevant tweets, the proposed algorithm\ndemonstrates much higher clustering performance than the DBSCAN case in terms\nof $\\mathcal{F}_1$ score and its variants. While DBSTexC performs exactly as\nDBSCAN with the textually homogeneous inputs, it far outperforms DBSCAN with\nthe textually heterogeneous inputs. Furthermore, to further improve the\nclustering quality by fully capturing the geographic distribution of tweets, we\npresent fuzzy DBSTexC (F-DBSTexC), an extension of DBSTexC, which incorporates\nthe notion of fuzzy clustering into the DBSTexC. We then demonstrate the\nrobustness of F-DBSTexC via intensive experiments. The computational complexity\nof our algorithms is also analytically and numerically shown.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 13:11:52 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Nguyen", "Minh D.", ""], ["Shin", "Won-Yong", ""]]}, {"id": "1806.05554", "submitter": "Frank Glavin", "authors": "Frank G. Glavin and Michael G. Madden", "title": "Adaptive Shooting for Bots in First Person Shooter Games Using\n  Reinforcement Learning", "comments": "IEEE Transactions on Computational Intelligence and AI in Games\n  (2015)", "journal-ref": "Glavin, Frank G., and Michael G. Madden. \"Adaptive shooting for\n  bots in first person shooter games using reinforcement learning.\" IEEE\n  Transactions on Computational Intelligence and AI in Games 7, no. 2: 180-192.\n  (2015)", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In current state-of-the-art commercial first person shooter games, computer\ncontrolled bots, also known as non player characters, can often be easily\ndistinguishable from those controlled by humans. Tell-tale signs such as failed\nnavigation, \"sixth sense\" knowledge of human players' whereabouts and\ndeterministic, scripted behaviors are some of the causes of this. We propose,\nhowever, that one of the biggest indicators of non humanlike behavior in these\ngames can be found in the weapon shooting capability of the bot. Consistently\nperfect accuracy and \"locking on\" to opponents in their visual field from any\ndistance are indicative capabilities of bots that are not found in human\nplayers. Traditionally, the bot is handicapped in some way with either a timed\nreaction delay or a random perturbation to its aim, which doesn't adapt or\nimprove its technique over time. We hypothesize that enabling the bot to learn\nthe skill of shooting through trial and error, in the same way a human player\nlearns, will lead to greater variation in game-play and produce less\npredictable non player characters. This paper describes a reinforcement\nlearning shooting mechanism for adapting shooting over time based on a dynamic\nreward signal from the amount of damage caused to opponents.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 14:00:14 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Glavin", "Frank G.", ""], ["Madden", "Michael G.", ""]]}, {"id": "1806.05594", "submitter": "Ben Athiwaratkun", "authors": "Ben Athiwaratkun, Marc Finzi, Pavel Izmailov, Andrew Gordon Wilson", "title": "There Are Many Consistent Explanations of Unlabeled Data: Why You Should\n  Average", "comments": "Appears at ICLR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Presently the most successful approaches to semi-supervised learning are\nbased on consistency regularization, whereby a model is trained to be robust to\nsmall perturbations of its inputs and parameters. To understand consistency\nregularization, we conceptually explore how loss geometry interacts with\ntraining procedures. The consistency loss dramatically improves generalization\nperformance over supervised-only training; however, we show that SGD struggles\nto converge on the consistency loss and continues to make large steps that lead\nto changes in predictions on the test data. Motivated by these observations, we\npropose to train consistency-based methods with Stochastic Weight Averaging\n(SWA), a recent approach which averages weights along the trajectory of SGD\nwith a modified learning rate schedule. We also propose fast-SWA, which further\naccelerates convergence by averaging multiple points within each cycle of a\ncyclical learning rate schedule. With weight averaging, we achieve the best\nknown semi-supervised results on CIFAR-10 and CIFAR-100, over many different\nquantities of labeled training data. For example, we achieve 5.0% error on\nCIFAR-10 with only 4000 labels, compared to the previous best result in the\nliterature of 6.3%.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 14:58:36 GMT"}, {"version": "v2", "created": "Tue, 19 Jun 2018 16:21:21 GMT"}, {"version": "v3", "created": "Thu, 21 Feb 2019 15:26:31 GMT"}], "update_date": "2019-02-22", "authors_parsed": [["Athiwaratkun", "Ben", ""], ["Finzi", "Marc", ""], ["Izmailov", "Pavel", ""], ["Wilson", "Andrew Gordon", ""]]}, {"id": "1806.05631", "submitter": "Sammie Katt", "authors": "Sammie Katt, Frans A. Oliehoek, Christopher Amato", "title": "Learning in POMDPs with Monte Carlo Tree Search", "comments": null, "journal-ref": "Proceedings of the 34th International Conference on Machine\n  Learning, PMLR 70:1819-1827, 2017", "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The POMDP is a powerful framework for reasoning under outcome and information\nuncertainty, but constructing an accurate POMDP model is difficult.\nBayes-Adaptive Partially Observable Markov Decision Processes (BA-POMDPs)\nextend POMDPs to allow the model to be learned during execution. BA-POMDPs are\na Bayesian RL approach that, in principle, allows for an optimal trade-off\nbetween exploitation and exploration. Unfortunately, BA-POMDPs are currently\nimpractical to solve for any non-trivial domain. In this paper, we extend the\nMonte-Carlo Tree Search method POMCP to BA-POMDPs and show that the resulting\nmethod, which we call BA-POMCP, is able to tackle problems that previous\nsolution methods have been unable to solve. Additionally, we introduce several\ntechniques that exploit the BA-POMDP structure to improve the efficiency of\nBA-POMCP along with proof of their convergence.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 16:17:44 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Katt", "Sammie", ""], ["Oliehoek", "Frans A.", ""], ["Amato", "Christopher", ""]]}, {"id": "1806.05635", "submitter": "Junhyuk Oh", "authors": "Junhyuk Oh, Yijie Guo, Satinder Singh, Honglak Lee", "title": "Self-Imitation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes Self-Imitation Learning (SIL), a simple off-policy\nactor-critic algorithm that learns to reproduce the agent's past good\ndecisions. This algorithm is designed to verify our hypothesis that exploiting\npast good experiences can indirectly drive deep exploration. Our empirical\nresults show that SIL significantly improves advantage actor-critic (A2C) on\nseveral hard exploration Atari games and is competitive to the state-of-the-art\ncount-based exploration methods. We also show that SIL improves proximal policy\noptimization (PPO) on MuJoCo tasks.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 16:25:55 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Oh", "Junhyuk", ""], ["Guo", "Yijie", ""], ["Singh", "Satinder", ""], ["Lee", "Honglak", ""]]}, {"id": "1806.05660", "submitter": "\\'Angel Alexander Cabrera", "authors": "\\'Angel Alexander Cabrera, Fred Hohman, Jason Lin, Duen Horng Chau", "title": "Interactive Classification for Deep Learning Interpretation", "comments": "Presented as a demo at CVPR'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an interactive system enabling users to manipulate images to\nexplore the robustness and sensitivity of deep learning image classifiers.\nUsing modern web technologies to run in-browser inference, users can remove\nimage features using inpainting algorithms and obtain new classifications in\nreal time, which allows them to ask a variety of \"what if\" questions by\nexperimentally modifying images and seeing how the model reacts. Our system\nallows users to compare and contrast what image regions humans and machine\nlearning models use for classification, revealing a wide range of surprising\nresults ranging from spectacular failures (e.g., a \"water bottle\" image becomes\na \"concert\" when removing a person) to impressive resilience (e.g., a \"baseball\nplayer\" image remains correctly classified even without a glove or base). We\ndemonstrate our system at The 2018 Conference on Computer Vision and Pattern\nRecognition (CVPR) for the audience to try it live. Our system is open-sourced\nat https://github.com/poloclub/interactive-classification. A video demo is\navailable at https://youtu.be/llub5GcOF6w.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 17:36:02 GMT"}, {"version": "v2", "created": "Wed, 10 Apr 2019 20:57:55 GMT"}], "update_date": "2019-04-12", "authors_parsed": [["Cabrera", "\u00c1ngel Alexander", ""], ["Hohman", "Fred", ""], ["Lin", "Jason", ""], ["Chau", "Duen Horng", ""]]}, {"id": "1806.05695", "submitter": "Dennis George Wilson", "authors": "Dennis G Wilson, Sylvain Cussat-Blanc, Herv\\'e Luga, Julian F Miller", "title": "Evolving simple programs for playing Atari games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cartesian Genetic Programming (CGP) has previously shown capabilities in\nimage processing tasks by evolving programs with a function set specialized for\ncomputer vision. A similar approach can be applied to Atari playing. Programs\nare evolved using mixed type CGP with a function set suited for matrix\noperations, including image processing, but allowing for controller behavior to\nemerge. While the programs are relatively small, many controllers are\ncompetitive with state of the art methods for the Atari benchmark set and\nrequire less training time. By evaluating the programs of the best evolved\nindividuals, simple but effective strategies can be found.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 18:10:46 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Wilson", "Dennis G", ""], ["Cussat-Blanc", "Sylvain", ""], ["Luga", "Herv\u00e9", ""], ["Miller", "Julian F", ""]]}, {"id": "1806.05740", "submitter": "Rediet Abebe", "authors": "Rediet Abebe, Shawndra Hill, Jennifer Wortman Vaughan, Peter M. Small,\n  H. Andrew Schwartz", "title": "Using Search Queries to Understand Health Information Needs in Africa", "comments": "Extended version of an ICWSM 2019 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lack of comprehensive, high-quality health data in developing nations\ncreates a roadblock for combating the impacts of disease. One key challenge is\nunderstanding the health information needs of people in these nations. Without\nunderstanding people's everyday needs, concerns, and misconceptions, health\norganizations and policymakers lack the ability to effectively target education\nand programming efforts. In this paper, we propose a bottom-up approach that\nuses search data from individuals to uncover and gain insight into health\ninformation needs in Africa. We analyze Bing searches related to HIV/AIDS,\nmalaria, and tuberculosis from all 54 African nations. For each disease, we\nautomatically derive a set of common search themes or topics, revealing a\nwide-spread interest in various types of information, including disease\nsymptoms, drugs, concerns about breastfeeding, as well as stigma, beliefs in\nnatural cures, and other topics that may be hard to uncover through traditional\nsurveys. We expose the different patterns that emerge in health information\nneeds by demographic groups (age and sex) and country. We also uncover\ndiscrepancies in the quality of content returned by search engines to users by\ntopic. Combined, our results suggest that search data can help illuminate\nhealth information needs in Africa and inform discussions on health policy and\ntargeted education efforts both on- and offline.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 20:48:41 GMT"}, {"version": "v2", "created": "Wed, 17 Apr 2019 17:49:06 GMT"}], "update_date": "2019-04-18", "authors_parsed": [["Abebe", "Rediet", ""], ["Hill", "Shawndra", ""], ["Vaughan", "Jennifer Wortman", ""], ["Small", "Peter M.", ""], ["Schwartz", "H. Andrew", ""]]}, {"id": "1806.05759", "submitter": "Ari Morcos", "authors": "Ari S. Morcos, Maithra Raghu, and Samy Bengio", "title": "Insights on representational similarity in neural networks with\n  canonical correlation", "comments": "NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Comparing different neural network representations and determining how\nrepresentations evolve over time remain challenging open questions in our\nunderstanding of the function of neural networks. Comparing representations in\nneural networks is fundamentally difficult as the structure of representations\nvaries greatly, even across groups of networks trained on identical tasks, and\nover the course of training. Here, we develop projection weighted CCA\n(Canonical Correlation Analysis) as a tool for understanding neural networks,\nbuilding off of SVCCA, a recently proposed method (Raghu et al., 2017). We\nfirst improve the core method, showing how to differentiate between signal and\nnoise, and then apply this technique to compare across a group of CNNs,\ndemonstrating that networks which generalize converge to more similar\nrepresentations than networks which memorize, that wider networks converge to\nmore similar solutions than narrow networks, and that trained networks with\nidentical topology but different learning rates converge to distinct clusters\nwith diverse representations. We also investigate the representational dynamics\nof RNNs, across both training and sequential timesteps, finding that RNNs\nconverge in a bottom-up pattern over the course of training and that the hidden\nstate is highly variable over the course of a sequence, even when accounting\nfor linear transforms. Together, these results provide new insights into the\nfunction of CNNs and RNNs, and demonstrate the utility of using CCA to\nunderstand representations.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 22:34:11 GMT"}, {"version": "v2", "created": "Thu, 21 Jun 2018 23:09:23 GMT"}, {"version": "v3", "created": "Tue, 23 Oct 2018 18:59:02 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Morcos", "Ari S.", ""], ["Raghu", "Maithra", ""], ["Bengio", "Samy", ""]]}, {"id": "1806.05767", "submitter": "Ahmed Qureshi", "authors": "Ahmed H. Qureshi, Anthony Simeonov, Mayur J. Bency and Michael C. Yip", "title": "Motion Planning Networks", "comments": "Paper published in ICRA'19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fast and efficient motion planning algorithms are crucial for many\nstate-of-the-art robotics applications such as self-driving cars. Existing\nmotion planning methods become ineffective as their computational complexity\nincreases exponentially with the dimensionality of the motion planning problem.\nTo address this issue, we present Motion Planning Networks (MPNet), a neural\nnetwork-based novel planning algorithm. The proposed method encodes the given\nworkspaces directly from a point cloud measurement and generates the end-to-end\ncollision-free paths for the given start and goal configurations. We evaluate\nMPNet on various 2D and 3D environments including the planning of a 7 DOF\nBaxter robot manipulator. The results show that MPNet is not only consistently\ncomputationally efficient in all environments but also generalizes to\ncompletely unseen environments. The results also show that the computation time\nof MPNet consistently remains less than 1 second in all presented experiments,\nwhich is significantly lower than existing state-of-the-art motion planning\nalgorithms.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 23:48:08 GMT"}, {"version": "v2", "created": "Sun, 24 Feb 2019 07:05:44 GMT"}], "update_date": "2019-02-26", "authors_parsed": [["Qureshi", "Ahmed H.", ""], ["Simeonov", "Anthony", ""], ["Bency", "Mayur J.", ""], ["Yip", "Michael C.", ""]]}, {"id": "1806.05780", "submitter": "Kamyar Azizzadenesheli Ph.D.", "authors": "Kamyar Azizzadenesheli, Brandon Yang, Weitang Liu, Zachary C Lipton,\n  Animashree Anandkumar", "title": "Surprising Negative Results for Generative Adversarial Tree Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While many recent advances in deep reinforcement learning (RL) rely on\nmodel-free methods, model-based approaches remain an alluring prospect for\ntheir potential to exploit unsupervised data to learn environment model. In\nthis work, we provide an extensive study on the design of deep generative\nmodels for RL environments and propose a sample efficient and robust method to\nlearn the model of Atari environments. We deploy this model and propose\ngenerative adversarial tree search (GATS) a deep RL algorithm that learns the\nenvironment model and implements Monte Carlo tree search (MCTS) on the learned\nmodel for planning. While MCTS on the learned model is computationally\nexpensive, similar to AlphaGo, GATS follows depth limited MCTS. GATS employs\ndeep Q network (DQN) and learns a Q-function to assign values to the leaves of\nthe tree in MCTS. We theoretical analyze GATS vis-a-vis the bias-variance\ntrade-off and show GATS is able to mitigate the worst-case error in the\nQ-estimate. While we were expecting GATS to enjoy a better sample complexity\nand faster converges to better policies, surprisingly, GATS fails to outperform\nDQN. We provide a study on which we show why depth limited MCTS fails to\nperform desirably.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 01:35:03 GMT"}, {"version": "v2", "created": "Wed, 28 Nov 2018 07:09:45 GMT"}, {"version": "v3", "created": "Thu, 24 Jan 2019 04:38:55 GMT"}, {"version": "v4", "created": "Thu, 5 Sep 2019 02:31:13 GMT"}], "update_date": "2019-09-06", "authors_parsed": [["Azizzadenesheli", "Kamyar", ""], ["Yang", "Brandon", ""], ["Liu", "Weitang", ""], ["Lipton", "Zachary C", ""], ["Anandkumar", "Animashree", ""]]}, {"id": "1806.05794", "submitter": "Mohsen Imani", "authors": "Mohsen Imani, Mohammad Samragh, Yeseong Kim, Saransh Gupta, Farinaz\n  Koushanfar, Tajana Rosing", "title": "RAPIDNN: In-Memory Deep Neural Network Acceleration Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNN) have demonstrated effectiveness for various\napplications such as image processing, video segmentation, and speech\nrecognition. Running state-of-the-art DNNs on current systems mostly relies on\neither generalpurpose processors, ASIC designs, or FPGA accelerators, all of\nwhich suffer from data movements due to the limited onchip memory and data\ntransfer bandwidth. In this work, we propose a novel framework, called RAPIDNN,\nwhich processes all DNN operations within the memory to minimize the cost of\ndata movement. To enable in-memory processing, RAPIDNN reinterprets a DNN model\nand maps it into a specialized accelerator, which is designed using\nnon-volatile memory blocks that model four fundamental DNN operations, i.e.,\nmultiplication, addition, activation functions, and pooling. The framework\nextracts representative operands of a DNN model, e.g., weights and input\nvalues, using clustering methods to optimize the model for in-memory\nprocessing. Then, it maps the extracted operands and their precomputed results\ninto the accelerator memory blocks. At runtime, the accelerator identifies\ncomputation results based on efficient in-memory search capability which also\nprovides tunability of approximation to further improve computation efficiency.\nOur evaluation shows that RAPIDNN achieves 68.4x, 49.5x energy efficiency\nimprovement and 48.1x, 10.9x speedup as compared to ISAAC and PipeLayer, the\nstate-of-the-art DNN accelerators, while ensuring less than 0.3% of quality\nloss.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 03:07:55 GMT"}, {"version": "v2", "created": "Wed, 8 Aug 2018 05:05:01 GMT"}, {"version": "v3", "created": "Thu, 9 Aug 2018 03:09:32 GMT"}, {"version": "v4", "created": "Thu, 11 Apr 2019 18:36:50 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Imani", "Mohsen", ""], ["Samragh", "Mohammad", ""], ["Kim", "Yeseong", ""], ["Gupta", "Saransh", ""], ["Koushanfar", "Farinaz", ""], ["Rosing", "Tajana", ""]]}, {"id": "1806.05898", "submitter": "Miquel Junyent", "authors": "Miquel Junyent, Anders Jonsson, Vicen\\c{c} G\\'omez", "title": "Improving width-based planning with compact policies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal action selection in decision problems characterized by sparse,\ndelayed rewards is still an open challenge. For these problems, current deep\nreinforcement learning methods require enormous amounts of data to learn\ncontrollers that reach human-level performance. In this work, we propose a\nmethod that interleaves planning and learning to address this issue. The\nplanning step hinges on the Iterated-Width (IW) planner, a state of the art\nplanner that makes explicit use of the state representation to perform\nstructured exploration. IW is able to scale up to problems independently of the\nsize of the state space. From the state-actions visited by IW, the learning\nstep estimates a compact policy, which in turn is used to guide the planning\nstep. The type of exploration used by our method is radically different than\nthe standard random exploration used in RL. We evaluate our method in simple\nproblems where we show it to have superior performance than the\nstate-of-the-art reinforcement learning algorithms A2C and Alpha Zero. Finally,\nwe present preliminary results in a subset of the Atari games suite.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 10:41:23 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Junyent", "Miquel", ""], ["Jonsson", "Anders", ""], ["G\u00f3mez", "Vicen\u00e7", ""]]}, {"id": "1806.06003", "submitter": "Markus Wulfmeier", "authors": "Markus Wulfmeier", "title": "On Machine Learning and Structure for Mobile Robots", "comments": "Informal Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to recent advances - compute, data, models - the role of learning in\nautonomous systems has expanded significantly, rendering new applications\npossible for the first time. While some of the most significant benefits are\nobtained in the perception modules of the software stack, other aspects\ncontinue to rely on known manual procedures based on prior knowledge on\ngeometry, dynamics, kinematics etc. Nonetheless, learning gains relevance in\nthese modules when data collection and curation become easier than manual rule\ndesign. Building on this coarse and broad survey of current research, the final\nsections aim to provide insights into future potentials and challenges as well\nas the necessity of structure in current practical applications.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 14:49:35 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Wulfmeier", "Markus", ""]]}, {"id": "1806.06010", "submitter": "Thommen George Karimpanal", "authors": "Thommen George Karimpanal", "title": "A Self-Replication Basis for Designing Complex Agents", "comments": "2 pages, 1 figure", "journal-ref": "Proceedings of the Genetic and Evolutionary Computation Conference\n  Companion, Pages 45-46, Kyoto, Japan, July 15 - 19, 2018", "doi": "10.1145/3205651.3208762", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we describe a self-replication-based mechanism for designing\nagents of increasing complexity. We demonstrate the validity of this approach\nby solving simple, standard evolutionary computation problems in simulation. In\nthe context of these simulation results, we describe the fundamental\ndifferences of this approach when compared to traditional approaches. Further,\nwe highlight the possible advantages of applying this approach to the problem\nof designing complex artificial agents, along with the potential drawbacks and\nissues to be addressed in the future.\n", "versions": [{"version": "v1", "created": "Fri, 18 May 2018 03:57:41 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Karimpanal", "Thommen George", ""]]}, {"id": "1806.06055", "submitter": "Huang Lingxiao", "authors": "L. Elisa Celis, Lingxiao Huang, Vijay Keswani, Nisheeth K. Vishnoi", "title": "Classification with Fairness Constraints: A Meta-Algorithm with Provable\n  Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing classification algorithms that are fair with respect to sensitive\nattributes of the data has become an important problem due to the growing\ndeployment of classification algorithms in various social contexts. Several\nrecent works have focused on fairness with respect to a specific metric,\nmodeled the corresponding fair classification problem as a constrained\noptimization problem, and developed tailored algorithms to solve them. Despite\nthis, there still remain important metrics for which we do not have fair\nclassifiers and many of the aforementioned algorithms do not come with\ntheoretical guarantees; perhaps because the resulting optimization problem is\nnon-convex. The main contribution of this paper is a new meta-algorithm for\nclassification that takes as input a large class of fairness constraints, with\nrespect to multiple non-disjoint sensitive attributes, and which comes with\nprovable guarantees. This is achieved by first developing a meta-algorithm for\na large family of classification problems with convex constraints, and then\nshowing that classification problems with general types of fairness constraints\ncan be reduced to those in this family. We present empirical results that show\nthat our algorithm can achieve near-perfect fairness with respect to various\nfairness metrics, and that the loss in accuracy due to the imposed fairness\nconstraints is often small. Overall, this work unifies several prior works on\nfair classification, presents a practical algorithm with theoretical\nguarantees, and can handle fairness metrics that were previously not possible.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 17:45:58 GMT"}, {"version": "v2", "created": "Thu, 2 Aug 2018 17:53:43 GMT"}, {"version": "v3", "created": "Wed, 15 Apr 2020 06:08:21 GMT"}], "update_date": "2020-04-16", "authors_parsed": [["Celis", "L. Elisa", ""], ["Huang", "Lingxiao", ""], ["Keswani", "Vijay", ""], ["Vishnoi", "Nisheeth K.", ""]]}, {"id": "1806.06108", "submitter": "Edward Raff", "authors": "William Fleshman, Edward Raff, Jared Sylvester, Steven Forsyth, Mark\n  McLean", "title": "Non-Negative Networks Against Adversarial Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": "AICS/2019/08", "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial attacks against neural networks are a problem of considerable\nimportance, for which effective defenses are not yet readily available. We make\nprogress toward this problem by showing that non-negative weight constraints\ncan be used to improve resistance in specific scenarios. In particular, we show\nthat they can provide an effective defense for binary classification problems\nwith asymmetric cost, such as malware or spam detection. We also show the\npotential for non-negativity to be helpful to non-binary problems by applying\nit to image classification.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 20:02:57 GMT"}, {"version": "v2", "created": "Thu, 3 Jan 2019 20:23:59 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Fleshman", "William", ""], ["Raff", "Edward", ""], ["Sylvester", "Jared", ""], ["Forsyth", "Steven", ""], ["McLean", "Mark", ""]]}, {"id": "1806.06192", "submitter": "Zachary Kaden", "authors": "Hima Varsha Dureddy, Zachary Kaden", "title": "Handling Cold-Start Collaborative Filtering with Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge in recommender systems is handling new users, whom are also\ncalled $\\textit{cold-start}$ users. In this paper, we propose a novel approach\nfor learning an optimal series of questions with which to interview cold-start\nusers for movie recommender systems. We propose learning interview questions\nusing Deep Q Networks to create user profiles to make better recommendations to\ncold-start users. While our proposed system is trained using a movie\nrecommender system, our Deep Q Network model should generalize across various\ntypes of recommender systems.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 05:58:00 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Dureddy", "Hima Varsha", ""], ["Kaden", "Zachary", ""]]}, {"id": "1806.06205", "submitter": "Xiaowang Zhang", "authors": "Lijing Zhang and Xiaowang Zhang and Zhiyong Feng", "title": "TrQuery: An Embedding-based Framework for Recommanding SPARQL Queries", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an embedding-based framework (TrQuery) for\nrecommending solutions of a SPARQL query, including approximate solutions when\nexact querying solutions are not available due to incompleteness or\ninconsistencies of real-world RDF data. Within this framework, embedding is\napplied to score solutions together with edit distance so that we could obtain\nmore fine-grained recommendations than those recommendations via edit distance.\nFor instance, graphs of two querying solutions with a similar structure can be\ndistinguished in our proposed framework while the edit distance depending on\nstructural difference becomes unable. To this end, we propose a novel score\nmodel built on vector space generated in embedding system to compute the\nsimilarity between an approximate subgraph matching and a whole graph matching.\nFinally, we evaluate our approach on large RDF datasets DBpedia and YAGO, and\nexperimental results show that TrQuery exhibits an excellent behavior in terms\nof both effectiveness and efficiency.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 08:17:08 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Zhang", "Lijing", ""], ["Zhang", "Xiaowang", ""], ["Feng", "Zhiyong", ""]]}, {"id": "1806.06207", "submitter": "Wlodzislaw Duch", "authors": "W{\\l}odzis{\\l}aw Duch and Karol Grudzi\\'nsk", "title": "Meta-learning: searching in the model space", "comments": "6 pages. To our best knowledge this is the first paper on\n  meta-learning as search in the model spaces; for later developments search\n  \"Duch meta-learning\"", "journal-ref": "Proceedings of the International Conference on Neural Information\n  Processing, Shanghai, 2001, Vol. I, pp. 235-240", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is no free lunch, no single learning algorithm that will outperform\nother algorithms on all data. In practice different approaches are tried and\nthe best algorithm selected. An alternative solution is to build new algorithms\non demand by creating a framework that accommodates many algorithms. The best\ncombination of parameters and procedures is searched here in the space of all\npossible models belonging to the framework of Similarity-Based Methods (SBMs).\nSuch meta-learning approach gives a chance to find the best method in all\ncases. Issues related to the meta-learning and first tests of this approach are\npresented.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 08:24:35 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Duch", "W\u0142odzis\u0142aw", ""], ["Grudzi\u0144sk", "Karol", ""]]}, {"id": "1806.06208", "submitter": "Sauradip Nag", "authors": "Sauradip Nag, Pallab Kumar Ganguly, Sumit Roy, Sourab Jha, Krishna\n  Bose, Abhishek Jha, Kousik Dasgupta", "title": "Offline Extraction of Indic Regional Language from Natural Scene Image\n  using Text Segmentation and Deep Convolutional Sequence", "comments": "Accepted in Second International Conference on Computational\n  Intelligence, Communications, and Business Analytics (CICBA-2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Regional language extraction from a natural scene image is always a\nchallenging proposition due to its dependence on the text information extracted\nfrom Image. Text Extraction on the other hand varies on different lighting\ncondition, arbitrary orientation, inadequate text information, heavy background\ninfluence over text and change of text appearance. This paper presents a novel\nunified method for tackling the above challenges. The proposed work uses an\nimage correction and segmentation technique on the existing Text Detection\nPipeline an Efficient and Accurate Scene Text Detector (EAST). EAST uses\nstandard PVAnet architecture to select features and non maximal suppression to\ndetect text from image. Text recognition is done using combined architecture of\nMaxOut convolution neural network (CNN) and Bidirectional long short term\nmemory (LSTM) network. After recognizing text using the Deep Learning based\napproach, the native Languages are translated to English and tokenized using\nstandard Text Tokenizers. The tokens that very likely represent a location is\nused to find the Global Positioning System (GPS) coordinates of the location\nand subsequently the regional languages spoken in that location is extracted.\nThe proposed method is tested on a self generated dataset collected from\nGovernment of India dataset and experimented on Standard Dataset to evaluate\nthe performance of the proposed technique. Comparative study with a few\nstate-of-the-art methods on text detection, recognition and extraction of\nregional language from images shows that the proposed method outperforms the\nexisting methods.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 08:31:06 GMT"}, {"version": "v2", "created": "Fri, 6 Jul 2018 20:10:03 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Nag", "Sauradip", ""], ["Ganguly", "Pallab Kumar", ""], ["Roy", "Sumit", ""], ["Jha", "Sourab", ""], ["Bose", "Krishna", ""], ["Jha", "Abhishek", ""], ["Dasgupta", "Kousik", ""]]}, {"id": "1806.06232", "submitter": "Alexandre Quemy", "authors": "Alexandre Quemy", "title": "Binary Classification in Unstructured Space With Hypergraph Case-Based\n  Reasoning", "comments": "Accepted for publication by Information Systems. Arxiv version\n  contains the additional material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Binary classification is one of the most common problem in machine learning.\nIt consists in predicting whether a given element belongs to a particular\nclass. In this paper, a new algorithm for binary classification is proposed\nusing a hypergraph representation. The method is agnostic to data\nrepresentation, can work with multiple data sources or in non-metric spaces,\nand accommodates with missing values. As a result, it drastically reduces the\nneed for data preprocessing or feature engineering. Each element to be\nclassified is partitioned according to its interactions with the training set.\nFor each class, a seminorm over the training set partition is learnt to\nrepresent the distribution of evidence supporting this class.\n  Empirical validation demonstrates its high potential on a wide range of\nwell-known datasets and the results are compared to the state-of-the-art. The\ntime complexity is given and empirically validated. Its robustness with regard\nto hyperparameter sensitivity is studied and compared to standard\nclassification methods. Finally, the limitation of the model space is\ndiscussed, and some potential solutions proposed.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 12:27:14 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 15:00:54 GMT"}, {"version": "v3", "created": "Sat, 9 Mar 2019 14:32:09 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Quemy", "Alexandre", ""]]}, {"id": "1806.06266", "submitter": "Han Zhao", "authors": "Yichong Xu, Han Zhao, Xiaofei Shi, Jeremy Zhang, and Nihar B. Shah", "title": "On Strategyproof Conference Peer Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider peer review in a conference setting where there is typically an\noverlap between the set of reviewers and the set of authors. This overlap can\nincentivize strategic reviews to influence the final ranking of one's own\npapers. In this work, we address this problem through the lens of social\nchoice, and present a theoretical framework for strategyproof and efficient\npeer review. We first present and analyze an algorithm for reviewer-assignment\nand aggregation that guarantees strategyproofness and a natural efficiency\nproperty called unanimity, when the authorship graph satisfies a simple\nproperty. Our algorithm is based on the so-called partitioning method, and can\nbe thought as a generalization of this method to conference peer review\nsettings. We then empirically show that the requisite property on the\nauthorship graph is indeed satisfied in the submission data from the ICLR\nconference, and further demonstrate a simple trick to make the partitioning\nmethod more practically appealing for conference peer review. Finally, we\ncomplement our positive results with negative theoretical results where we\nprove that under various ways of strengthening the requirements, it is\nimpossible for any algorithm to be strategyproof and efficient.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 16:40:39 GMT"}, {"version": "v2", "created": "Wed, 14 Nov 2018 15:52:02 GMT"}, {"version": "v3", "created": "Sat, 1 Feb 2020 03:01:04 GMT"}], "update_date": "2020-02-04", "authors_parsed": [["Xu", "Yichong", ""], ["Zhao", "Han", ""], ["Shi", "Xiaofei", ""], ["Zhang", "Jeremy", ""], ["Shah", "Nihar B.", ""]]}, {"id": "1806.06298", "submitter": "Xianglei Xing", "authors": "Xianglei Xing, Ruiqi Gao, Tian Han, Song-Chun Zhu, Ying Nian Wu", "title": "Deformable Generator Network: Unsupervised Disentanglement of Appearance\n  and Geometry", "comments": "version 3", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deformable generator model to disentangle the appearance and\ngeometric information for both image and video data in a purely unsupervised\nmanner. The appearance generator network models the information related to\nappearance, including color, illumination, identity or category, while the\ngeometric generator performs geometric warping, such as rotation and\nstretching, through generating deformation field which is used to warp the\ngenerated appearance to obtain the final image or video sequences. Two\ngenerators take independent latent vectors as input to disentangle the\nappearance and geometric information from image or video sequences. For video\ndata, a nonlinear transition model is introduced to both the appearance and\ngeometric generators to capture the dynamics over time. The proposed scheme is\ngeneral and can be easily integrated into different generative models. An\nextensive set of qualitative and quantitative experiments shows that the\nappearance and geometric information can be well disentangled, and the learned\ngeometric generator can be conveniently transferred to other image datasets to\nfacilitate knowledge transfer tasks.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 21:17:02 GMT"}, {"version": "v2", "created": "Tue, 11 Dec 2018 05:20:31 GMT"}, {"version": "v3", "created": "Tue, 14 Jan 2020 01:26:23 GMT"}], "update_date": "2020-01-31", "authors_parsed": [["Xing", "Xianglei", ""], ["Gao", "Ruiqi", ""], ["Han", "Tian", ""], ["Zhu", "Song-Chun", ""], ["Wu", "Ying Nian", ""]]}, {"id": "1806.06301", "submitter": "Thomas Lansdall-Welfare", "authors": "Adam Sutton, Thomas Lansdall-Welfare, Nello Cristianini", "title": "Biased Embeddings from Wild Data: Measuring, Understanding and Removing", "comments": "Author's original version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many modern Artificial Intelligence (AI) systems make use of data embeddings,\nparticularly in the domain of Natural Language Processing (NLP). These\nembeddings are learnt from data that has been gathered \"from the wild\" and have\nbeen found to contain unwanted biases. In this paper we make three\ncontributions towards measuring, understanding and removing this problem. We\npresent a rigorous way to measure some of these biases, based on the use of\nword lists created for social psychology applications; we observe how gender\nbias in occupations reflects actual gender bias in the same occupations in the\nreal world; and finally we demonstrate how a simple projection can\nsignificantly reduce the effects of embedding bias. All this is part of an\nongoing effort to understand how trust can be built into AI systems.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 21:46:59 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Sutton", "Adam", ""], ["Lansdall-Welfare", "Thomas", ""], ["Cristianini", "Nello", ""]]}, {"id": "1806.06349", "submitter": "Hao Zhu", "authors": "Huiming Jin, Hao Zhu, Zhiyuan Liu, Ruobing Xie, Maosong Sun, Fen Lin,\n  Leyu Lin", "title": "Incorporating Chinese Characters of Words for Lexical Sememe Prediction", "comments": "Accepted as an ACL 2018 long paper. The first two authors contribute\n  equally. Code is available at\n  https://github.com/thunlp/Character-enhanced-Sememe-Prediction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sememes are minimum semantic units of concepts in human languages, such that\neach word sense is composed of one or multiple sememes. Words are usually\nmanually annotated with their sememes by linguists, and form linguistic\ncommon-sense knowledge bases widely used in various NLP tasks. Recently, the\nlexical sememe prediction task has been introduced. It consists of\nautomatically recommending sememes for words, which is expected to improve\nannotation efficiency and consistency. However, existing methods of lexical\nsememe prediction typically rely on the external context of words to represent\nthe meaning, which usually fails to deal with low-frequency and\nout-of-vocabulary words. To address this issue for Chinese, we propose a novel\nframework to take advantage of both internal character information and external\ncontext information of words. We experiment on HowNet, a Chinese sememe\nknowledge base, and demonstrate that our framework outperforms state-of-the-art\nbaselines by a large margin, and maintains a robust performance even for\nlow-frequency words.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2018 08:44:55 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Jin", "Huiming", ""], ["Zhu", "Hao", ""], ["Liu", "Zhiyuan", ""], ["Xie", "Ruobing", ""], ["Sun", "Maosong", ""], ["Lin", "Fen", ""], ["Lin", "Leyu", ""]]}, {"id": "1806.06371", "submitter": "Lisa Beinborn", "authors": "Lisa Beinborn, Teresa Botschen and Iryna Gurevych", "title": "Multimodal Grounding for Language Processing", "comments": "The paper has been published in the Proceedings of the 27 Conference\n  of Computational Linguistics. Please refer to this version for citations:\n  https://www.aclweb.org/anthology/papers/C/C18/C18-1197/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This survey discusses how recent developments in multimodal processing\nfacilitate conceptual grounding of language. We categorize the information flow\nin multimodal processing with respect to cognitive models of human information\nprocessing and analyze different methods for combining multimodal\nrepresentations. Based on this methodological inventory, we discuss the benefit\nof multimodal grounding for a variety of language processing tasks and the\nchallenges that arise. We particularly focus on multimodal grounding of verbs\nwhich play a crucial role for the compositional power of language.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2018 12:13:37 GMT"}, {"version": "v2", "created": "Wed, 3 Jul 2019 09:28:26 GMT"}], "update_date": "2019-07-04", "authors_parsed": [["Beinborn", "Lisa", ""], ["Botschen", "Teresa", ""], ["Gurevych", "Iryna", ""]]}, {"id": "1806.06408", "submitter": "Lisa Lee", "authors": "Lisa Lee, Emilio Parisotto, Devendra Singh Chaplot, Eric Xing, Ruslan\n  Salakhutdinov", "title": "Gated Path Planning Networks", "comments": "ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Value Iteration Networks (VINs) are effective differentiable path planning\nmodules that can be used by agents to perform navigation while still\nmaintaining end-to-end differentiability of the entire architecture. Despite\ntheir effectiveness, they suffer from several disadvantages including training\ninstability, random seed sensitivity, and other optimization problems. In this\nwork, we reframe VINs as recurrent-convolutional networks which demonstrates\nthat VINs couple recurrent convolutions with an unconventional max-pooling\nactivation. From this perspective, we argue that standard gated recurrent\nupdate equations could potentially alleviate the optimization issues plaguing\nVIN. The resulting architecture, which we call the Gated Path Planning Network,\nis shown to empirically outperform VIN on a variety of metrics such as learning\nspeed, hyperparameter sensitivity, iteration count, and even generalization.\nFurthermore, we show that this performance gap is consistent across different\nmaze transition types, maze sizes and even show success on a challenging 3D\nenvironment, where the planner is only provided with first-person RGB images.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2018 16:32:52 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Lee", "Lisa", ""], ["Parisotto", "Emilio", ""], ["Chaplot", "Devendra Singh", ""], ["Xing", "Eric", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1806.06411", "submitter": "Svitlana Vakulenko", "authors": "Svitlana Vakulenko, Maarten de Rijke, Michael Cochez, Vadim Savenkov,\n  Axel Polleres", "title": "Measuring Semantic Coherence of a Conversation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational systems have become increasingly popular as a way for humans\nto interact with computers. To be able to provide intelligent responses,\nconversational systems must correctly model the structure and semantics of a\nconversation. We introduce the task of measuring semantic (in)coherence in a\nconversation with respect to background knowledge, which relies on the\nidentification of semantic relations between concepts introduced during a\nconversation. We propose and evaluate graph-based and machine learning-based\napproaches for measuring semantic coherence using knowledge graphs, their\nvector space embeddings and word embedding models, as sources of background\nknowledge. We demonstrate how these approaches are able to uncover different\ncoherence patterns in conversations on the Ubuntu Dialogue Corpus.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2018 16:38:48 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Vakulenko", "Svitlana", ""], ["de Rijke", "Maarten", ""], ["Cochez", "Michael", ""], ["Savenkov", "Vadim", ""], ["Polleres", "Axel", ""]]}, {"id": "1806.06446", "submitter": "Yi Tay", "authors": "Yi Tay, Shuai Zhang, Luu Anh Tuan, Siu Cheung Hui", "title": "Self-Attentive Neural Collaborative Filtering", "comments": "We discovered a bug in our tensorflow implementation that involved\n  accidental mixing of vectors across batches, rendering the main claim of the\n  paper incorrect. We are withdrawing this paper until we find out why", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper has been withdrawn as we discovered a bug in our tensorflow\nimplementation that involved accidental mixing of vectors across batches. This\nlead to different inference results given different batch sizes which is\ncompletely strange. The performance scores still remain the same but we\nconcluded that it was not the self-attention that contributed to the\nperformance. We are withdrawing the paper because this renders the main claim\nof the paper false. Thanks to Guan Xinyu from NUS for discovering this issue in\nour previously open source code.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2018 20:58:12 GMT"}, {"version": "v2", "created": "Thu, 19 Jul 2018 12:04:56 GMT"}], "update_date": "2018-07-20", "authors_parsed": [["Tay", "Yi", ""], ["Zhang", "Shuai", ""], ["Tuan", "Luu Anh", ""], ["Hui", "Siu Cheung", ""]]}, {"id": "1806.06464", "submitter": "Aditya Grover", "authors": "Aditya Grover, Maruan Al-Shedivat, Jayesh K. Gupta, Yura Burda,\n  Harrison Edwards", "title": "Learning Policy Representations in Multiagent Systems", "comments": "ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling agent behavior is central to understanding the emergence of complex\nphenomena in multiagent systems. Prior work in agent modeling has largely been\ntask-specific and driven by hand-engineering domain-specific prior knowledge.\nWe propose a general learning framework for modeling agent behavior in any\nmultiagent system using only a handful of interaction data. Our framework casts\nagent modeling as a representation learning problem. Consequently, we construct\na novel objective inspired by imitation learning and agent identification and\ndesign an algorithm for unsupervised learning of representations of agent\npolicies. We demonstrate empirically the utility of the proposed framework in\n(i) a challenging high-dimensional competitive environment for continuous\ncontrol and (ii) a cooperative environment for communication, on supervised\npredictive tasks, unsupervised clustering, and policy optimization using deep\nreinforcement learning.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2018 23:29:19 GMT"}, {"version": "v2", "created": "Tue, 31 Jul 2018 21:36:26 GMT"}], "update_date": "2018-08-02", "authors_parsed": [["Grover", "Aditya", ""], ["Al-Shedivat", "Maruan", ""], ["Gupta", "Jayesh K.", ""], ["Burda", "Yura", ""], ["Edwards", "Harrison", ""]]}, {"id": "1806.06478", "submitter": "Muhao Chen", "authors": "Muhao Chen, Yingtao Tian, Kai-Wei Chang, Steven Skiena, Carlo Zaniolo", "title": "Co-training Embeddings of Knowledge Graphs and Entity Descriptions for\n  Cross-lingual Entity Alignment", "comments": "To appear in IJCAI-18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilingual knowledge graph (KG) embeddings provide latent semantic\nrepresentations of entities and structured knowledge with cross-lingual\ninferences, which benefit various knowledge-driven cross-lingual NLP tasks.\nHowever, precisely learning such cross-lingual inferences is usually hindered\nby the low coverage of entity alignment in many KGs. Since many multilingual\nKGs also provide literal descriptions of entities, in this paper, we introduce\nan embedding-based approach which leverages a weakly aligned multilingual KG\nfor semi-supervised cross-lingual learning using entity descriptions. Our\napproach performs co-training of two embedding models, i.e. a multilingual KG\nembedding model and a multilingual literal description embedding model. The\nmodels are trained on a large Wikipedia-based trilingual dataset where most\nentity alignment is unknown to training. Experimental results show that the\nperformance of the proposed approach on the entity alignment task improves at\neach iteration of co-training, and eventually reaches a stage at which it\nsignificantly surpasses previous approaches. We also show that our approach has\npromising abilities for zero-shot entity alignment, and cross-lingual KG\ncompletion.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 02:06:46 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Chen", "Muhao", ""], ["Tian", "Yingtao", ""], ["Chang", "Kai-Wei", ""], ["Skiena", "Steven", ""], ["Zaniolo", "Carlo", ""]]}, {"id": "1806.06496", "submitter": "Zecheng He", "authors": "Zecheng He, Aswin Raghavan, Guangyuan Hu, Sek Chai and Ruby Lee", "title": "Power-Grid Controller Anomaly Detection with Enhanced Temporal Deep\n  Learning", "comments": "Accepted in the 18th IEEE International Conference on Trust, Security\n  and Privacy in Computing and Communications (TrustCom'19)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Controllers of security-critical cyber-physical systems, like the power grid,\nare a very important class of computer systems. Attacks against the control\ncode of a power-grid system, especially zero-day attacks, can be catastrophic.\nEarlier detection of the anomalies can prevent further damage. However,\ndetecting zero-day attacks is extremely challenging because they have no known\ncode and have unknown behavior. Furthermore, if data collected from the\ncontroller is transferred to a server through networks for analysis and\ndetection of anomalous behavior, this creates a very large attack surface and\nalso delays detection.\n  In order to address this problem, we propose Reconstruction Error\nDistribution (RED) of Hardware Performance Counters (HPCs), and a data-driven\ndefense system based on it. Specifically, we first train a temporal deep\nlearning model, using only normal HPC readings from legitimate processes that\nrun daily in these power-grid systems, to model the normal behavior of the\npower-grid controller. Then, we run this model using real-time data from\ncommonly available HPCs. We use the proposed RED to enhance the temporal deep\nlearning detection of anomalous behavior, by estimating distribution deviations\nfrom the normal behavior with an effective statistical test. Experimental\nresults on a real power-grid controller show that we can detect anomalous\nbehavior with high accuracy (>99.9%), nearly zero false positives and short\n(<360ms) latency.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 04:28:18 GMT"}, {"version": "v2", "created": "Fri, 21 Sep 2018 03:20:39 GMT"}, {"version": "v3", "created": "Sun, 23 Jun 2019 00:23:12 GMT"}], "update_date": "2019-06-25", "authors_parsed": [["He", "Zecheng", ""], ["Raghavan", "Aswin", ""], ["Hu", "Guangyuan", ""], ["Chai", "Sek", ""], ["Lee", "Ruby", ""]]}, {"id": "1806.06505", "submitter": "Ildefons Magrans de Abril", "authors": "Ildefons Magrans de Abril, Ryota Kanai", "title": "A unified strategy for implementing curiosity and empowerment driven\n  reinforcement learning", "comments": "13 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although there are many approaches to implement intrinsically motivated\nartificial agents, the combined usage of multiple intrinsic drives remains\nstill a relatively unexplored research area. Specifically, we hypothesize that\na mechanism capable of quantifying and controlling the evolution of the\ninformation flow between the agent and the environment could be the fundamental\ncomponent for implementing a higher degree of autonomy into artificial\nintelligent agents. This paper propose a unified strategy for implementing two\nsemantically orthogonal intrinsic motivations: curiosity and empowerment.\nCuriosity reward informs the agent about the relevance of a recent agent\naction, whereas empowerment is implemented as the opposite information flow\nfrom the agent to the environment that quantifies the agent's potential of\ncontrolling its own future. We show that an additional homeostatic drive is\nderived from the curiosity reward, which generalizes and enhances the\ninformation gain of a classical curious/heterostatic reinforcement learning\nagent. We show how a shared internal model by curiosity and empowerment\nfacilitates a more efficient training of the empowerment function. Finally, we\ndiscuss future directions for further leveraging the interplay between these\ntwo intrinsic rewards.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 05:58:04 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["de Abril", "Ildefons Magrans", ""], ["Kanai", "Ryota", ""]]}, {"id": "1806.06506", "submitter": "Ahmed Imtiaz Humayun", "authors": "Ahmed Imtiaz Humayun, Md. Tauhiduzzaman Khan, Shabnam Ghaffarzadegan,\n  Zhe Feng and Taufiq Hasan", "title": "An Ensemble of Transfer, Semi-supervised and Supervised Learning Methods\n  for Pathological Heart Sound Classification", "comments": "5 pages, 5 figures, Interspeech 2018 accepted manuscript", "journal-ref": null, "doi": "10.21437/Interspeech.2018-2413", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this work, we propose an ensemble of classifiers to distinguish between\nvarious degrees of abnormalities of the heart using Phonocardiogram (PCG)\nsignals acquired using digital stethoscopes in a clinical setting, for the\nINTERSPEECH 2018 Computational Paralinguistics (ComParE) Heart Beats\nSubChallenge. Our primary classification framework constitutes a convolutional\nneural network with 1D-CNN time-convolution (tConv) layers, which uses features\ntransferred from a model trained on the 2016 Physionet Heart Sound Database. We\nalso employ a Representation Learning (RL) approach to generate features in an\nunsupervised manner using Deep Recurrent Autoencoders and use Support Vector\nMachine (SVM) and Linear Discriminant Analysis (LDA) classifiers. Finally, we\nutilize an SVM classifier on a high-dimensional segment-level feature extracted\nusing various functionals on short-term acoustic features, i.e., Low-Level\nDescriptors (LLD). An ensemble of the three different approaches provides a\nrelative improvement of 11.13% compared to our best single sub-system in terms\nof the Unweighted Average Recall (UAR) performance metric on the evaluation\ndataset.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 06:04:12 GMT"}, {"version": "v2", "created": "Sun, 7 Oct 2018 05:53:06 GMT"}], "update_date": "2019-04-30", "authors_parsed": [["Humayun", "Ahmed Imtiaz", ""], ["Khan", "Md. Tauhiduzzaman", ""], ["Ghaffarzadegan", "Shabnam", ""], ["Feng", "Zhe", ""], ["Hasan", "Taufiq", ""]]}, {"id": "1806.06514", "submitter": "Shengjia Zhao", "authors": "Shengjia Zhao, Jiaming Song, Stefano Ermon", "title": "The Information Autoencoding Family: A Lagrangian Perspective on Latent\n  Variable Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large number of objectives have been proposed to train latent variable\ngenerative models. We show that many of them are Lagrangian dual functions of\nthe same primal optimization problem. The primal problem optimizes the mutual\ninformation between latent and visible variables, subject to the constraints of\naccurately modeling the data distribution and performing correct amortized\ninference. By choosing to maximize or minimize mutual information, and choosing\ndifferent Lagrange multipliers, we obtain different objectives including\nInfoGAN, ALI/BiGAN, ALICE, CycleGAN, beta-VAE, adversarial autoencoders, AVB,\nAS-VAE and InfoVAE. Based on this observation, we provide an exhaustive\ncharacterization of the statistical and computational trade-offs made by all\nthe training objectives in this class of Lagrangian duals. Next, we propose a\ndual optimization method where we optimize model parameters as well as the\nLagrange multipliers. This method achieves Pareto optimal solutions in terms of\noptimizing information and satisfying the constraints.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 06:51:28 GMT"}, {"version": "v2", "created": "Sun, 8 Jul 2018 00:21:02 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Zhao", "Shengjia", ""], ["Song", "Jiaming", ""], ["Ermon", "Stefano", ""]]}, {"id": "1806.06519", "submitter": "Adrien Deli\\`ege Mr", "authors": "Adrien Deli\\`ege, Anthony Cioppa, Marc Van Droogenbroeck", "title": "HitNet: a neural network with capsules embedded in a Hit-or-Miss layer,\n  extended with hybrid data augmentation and ghost capsules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks designed for the task of classification have become a\ncommodity in recent years. Many works target the development of better\nnetworks, which results in a complexification of their architectures with more\nlayers, multiple sub-networks, or even the combination of multiple classifiers.\nIn this paper, we show how to redesign a simple network to reach excellent\nperformances, which are better than the results reproduced with CapsNet on\nseveral datasets, by replacing a layer with a Hit-or-Miss layer. This layer\ncontains activated vectors, called capsules, that we train to hit or miss a\ncentral capsule by tailoring a specific centripetal loss function. We also show\nhow our network, named HitNet, is capable of synthesizing a representative\nsample of the images of a given class by including a reconstruction network.\nThis possibility allows to develop a data augmentation step combining\ninformation from the data space and the feature space, resulting in a hybrid\ndata augmentation process. In addition, we introduce the possibility for\nHitNet, to adopt an alternative to the true target when needed by using the new\nconcept of ghost capsules, which is used here to detect potentially mislabeled\nimages in the training data.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 07:08:11 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Deli\u00e8ge", "Adrien", ""], ["Cioppa", "Anthony", ""], ["Van Droogenbroeck", "Marc", ""]]}, {"id": "1806.06569", "submitter": "Steve Heim", "authors": "Steve Heim, Alexander Spr\\\"owitz", "title": "Learning from Outside the Viability Kernel: Why we Should Build Robots\n  that can Fall with Grace", "comments": "Proceedings of the 2018 IEEE International Conference on SImulation,\n  Modeling and Programming for Autonomous Robots (SIMPAR), Brisbane, Australia,\n  16-19 2018", "journal-ref": null, "doi": "10.1109/SIMPAR.2018.8376271", "report-no": null, "categories": "cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite impressive results using reinforcement learning to solve complex\nproblems from scratch, in robotics this has still been largely limited to\nmodel-based learning with very informative reward functions. One of the major\nchallenges is that the reward landscape often has large patches with no\ngradient, making it difficult to sample gradients effectively. We show here\nthat the robot state-initialization can have a more important effect on the\nreward landscape than is generally expected. In particular, we show the\ncounter-intuitive benefit of including initializations that are unviable, in\nother words initializing in states that are doomed to fail.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 09:28:52 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Heim", "Steve", ""], ["Spr\u00f6witz", "Alexander", ""]]}, {"id": "1806.06685", "submitter": "San Pham Ms", "authors": "Matthieu De Laere, San Tu Pham and Patrick De Causmaecker", "title": "Solving the Steiner Tree Problem in graphs with Variable Neighborhood\n  Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Steiner Tree Problem (STP) in graphs is an important problem with various\napplications in many areas such as design of integrated circuits, evolution\ntheory, networking, etc. In this paper, we propose an algorithm to solve the\nSTP. The algorithm includes a reducer and a solver using Variable Neighborhood\nDescent (VND), interacting with each other during the search. New constructive\nheuristics and a vertex score system for intensification purpose are proposed.\nThe algorithm is tested on a set of benchmarks which shows encouraging results.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 21:39:20 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["De Laere", "Matthieu", ""], ["Pham", "San Tu", ""], ["De Causmaecker", "Patrick", ""]]}, {"id": "1806.06734", "submitter": "Laurent Besacier", "authors": "Pierre Godard, Marcely Zanon-Boito, Lucas Ondel, Alexandre Berard,\n  Fran\\c{c}ois Yvon, Aline Villavicencio, and Laurent Besacier", "title": "Unsupervised Word Segmentation from Speech with Attention", "comments": "Interspeech 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a first attempt to perform attentional word segmentation directly\nfrom the speech signal, with the final goal to automatically identify lexical\nunits in a low-resource, unwritten language (UL). Our methodology assumes a\npairing between recordings in the UL with translations in a well-resourced\nlanguage. It uses Acoustic Unit Discovery (AUD) to convert speech into a\nsequence of pseudo-phones that is segmented using neural soft-alignments\nproduced by a neural machine translation model. Evaluation uses an actual Bantu\nUL, Mboshi; comparisons to monolingual and bilingual baselines illustrate the\npotential of attentional word segmentation for language documentation.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 14:35:14 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Godard", "Pierre", ""], ["Zanon-Boito", "Marcely", ""], ["Ondel", "Lucas", ""], ["Berard", "Alexandre", ""], ["Yvon", "Fran\u00e7ois", ""], ["Villavicencio", "Aline", ""], ["Besacier", "Laurent", ""]]}, {"id": "1806.06790", "submitter": "Roel Dobbe", "authors": "Roel Dobbe, Oscar Sondermeijer, David Fridovich-Keil, Daniel Arnold,\n  Duncan Callaway and Claire Tomlin", "title": "Towards Distributed Energy Services: Decentralizing Optimal Power Flow\n  with Machine Learning", "comments": "Accepted for publication. To appear in the IEEE Transactions on Smart\n  Grid", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT cs.SY math.IT math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The implementation of optimal power flow (OPF) methods to perform voltage and\npower flow regulation in electric networks is generally believed to require\nextensive communication. We consider distribution systems with multiple\ncontrollable Distributed Energy Resources (DERs) and present a data-driven\napproach to learn control policies for each DER to reconstruct and mimic the\nsolution to a centralized OPF problem from solely locally available\ninformation. Collectively, all local controllers closely match the centralized\nOPF solution, providing near optimal performance and satisfaction of system\nconstraints. A rate distortion framework enables the analysis of how well the\nresulting fully decentralized control policies are able to reconstruct the OPF\nsolution. The methodology provides a natural extension to decide what nodes a\nDER should communicate with to improve the reconstruction of its individual\npolicy. The method is applied on both single- and three-phase test feeder\nnetworks using data from real loads and distributed generators, focusing on\nDERs that do not exhibit inter-temporal dependencies. It provides a framework\nfor Distribution System Operators to efficiently plan and operate the\ncontributions of DERs to achieve Distributed Energy Services in distribution\nnetworks.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 19:46:37 GMT"}, {"version": "v2", "created": "Fri, 19 Apr 2019 21:30:07 GMT"}, {"version": "v3", "created": "Tue, 13 Aug 2019 23:30:09 GMT"}], "update_date": "2019-08-15", "authors_parsed": [["Dobbe", "Roel", ""], ["Sondermeijer", "Oscar", ""], ["Fridovich-Keil", "David", ""], ["Arnold", "Daniel", ""], ["Callaway", "Duncan", ""], ["Tomlin", "Claire", ""]]}, {"id": "1806.06798", "submitter": "Yunhao Tang", "authors": "Yunhao Tang and Shipra Agrawal", "title": "Implicit Policy for Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Implicit Policy, a general class of expressive policies that can\nflexibly represent complex action distributions in reinforcement learning, with\nefficient algorithms to compute entropy regularized policy gradients. We\nempirically show that, despite its simplicity in implementation, entropy\nregularization combined with a rich policy class can attain desirable\nproperties displayed under maximum entropy reinforcement learning framework,\nsuch as robustness and multi-modality.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 08:24:36 GMT"}, {"version": "v2", "created": "Sun, 3 Feb 2019 16:26:40 GMT"}], "update_date": "2019-02-05", "authors_parsed": [["Tang", "Yunhao", ""], ["Agrawal", "Shipra", ""]]}, {"id": "1806.06914", "submitter": "Shangda Li", "authors": "Shangda Li, Selina Bing, Steven Yang", "title": "Distributional Advantage Actor-Critic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In traditional reinforcement learning, an agent maximizes the reward\ncollected during its interaction with the environment by approximating the\noptimal policy through the estimation of value functions. Typically, given a\nstate s and action a, the corresponding value is the expected discounted sum of\nrewards. The optimal action is then chosen to be the action a with the largest\nvalue estimated by value function. However, recent developments have shown both\ntheoretical and experimental evidence of superior performance when value\nfunction is replaced with value distribution in context of deep Q learning [1].\nIn this paper, we develop a new algorithm that combines advantage actor-critic\nwith value distribution estimated by quantile regression. We evaluated this new\nalgorithm, termed Distributional Advantage Actor-Critic (DA2C or QR-A2C) on a\nvariety of tasks, and observed it to achieve at least as good as baseline\nalgorithms, and outperforming baseline in some tasks with smaller variance and\nincreased stability.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jun 2018 06:17:53 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Li", "Shangda", ""], ["Bing", "Selina", ""], ["Yang", "Steven", ""]]}, {"id": "1806.06920", "submitter": "Abbas Abdolmaleki", "authors": "Abbas Abdolmaleki, Jost Tobias Springenberg, Yuval Tassa, Remi Munos,\n  Nicolas Heess, Martin Riedmiller", "title": "Maximum a Posteriori Policy Optimisation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT cs.RO math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new algorithm for reinforcement learning called Maximum\naposteriori Policy Optimisation (MPO) based on coordinate ascent on a relative\nentropy objective. We show that several existing methods can directly be\nrelated to our derivation. We develop two off-policy algorithms and demonstrate\nthat they are competitive with the state-of-the-art in deep reinforcement\nlearning. In particular, for continuous control, our method outperforms\nexisting methods with respect to sample efficiency, premature convergence and\nrobustness to hyperparameter settings while achieving similar or better final\nperformance.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 12:46:23 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Abdolmaleki", "Abbas", ""], ["Springenberg", "Jost Tobias", ""], ["Tassa", "Yuval", ""], ["Munos", "Remi", ""], ["Heess", "Nicolas", ""], ["Riedmiller", "Martin", ""]]}, {"id": "1806.06923", "submitter": "Will Dabney", "authors": "Will Dabney, Georg Ostrovski, David Silver, R\\'emi Munos", "title": "Implicit Quantile Networks for Distributional Reinforcement Learning", "comments": "ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we build on recent advances in distributional reinforcement\nlearning to give a generally applicable, flexible, and state-of-the-art\ndistributional variant of DQN. We achieve this by using quantile regression to\napproximate the full quantile function for the state-action return\ndistribution. By reparameterizing a distribution over the sample space, this\nyields an implicitly defined return distribution and gives rise to a large\nclass of risk-sensitive policies. We demonstrate improved performance on the 57\nAtari 2600 games in the ALE, and use our algorithm's implicitly defined\ndistributions to study the effects of risk-sensitive policies in Atari games.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 14:28:37 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Dabney", "Will", ""], ["Ostrovski", "Georg", ""], ["Silver", "David", ""], ["Munos", "R\u00e9mi", ""]]}, {"id": "1806.06927", "submitter": "Jaehong Kim", "authors": "Jaehong Kim, Sangyeul Lee, Sungwan Kim, Moonsu Cha, Jung Kwon Lee,\n  Youngduck Choi, Yongseok Choi, Dong-Yeon Cho, Jiwon Kim", "title": "Auto-Meta: Automated Gradient Based Meta Learner Search", "comments": "Presented at NIPS 2018 Workshop on Meta-Learning (MetaLearn 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully automating machine learning pipelines is one of the key challenges of\ncurrent artificial intelligence research, since practical machine learning\noften requires costly and time-consuming human-powered processes such as model\ndesign, algorithm development, and hyperparameter tuning. In this paper, we\nverify that automated architecture search synergizes with the effect of\ngradient-based meta learning. We adopt the progressive neural architecture\nsearch \\cite{liu:pnas_google:DBLP:journals/corr/abs-1712-00559} to find optimal\narchitectures for meta-learners. The gradient based meta-learner whose\narchitecture was automatically found achieved state-of-the-art results on the\n5-shot 5-way Mini-ImageNet classification problem with $74.65\\%$ accuracy,\nwhich is $11.54\\%$ improvement over the result obtained by the first\ngradient-based meta-learner called MAML\n\\cite{finn:maml:DBLP:conf/icml/FinnAL17}. To our best knowledge, this work is\nthe first successful neural architecture search implementation in the context\nof meta learning.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 04:28:02 GMT"}, {"version": "v2", "created": "Mon, 10 Dec 2018 19:02:53 GMT"}], "update_date": "2018-12-12", "authors_parsed": [["Kim", "Jaehong", ""], ["Lee", "Sangyeul", ""], ["Kim", "Sungwan", ""], ["Cha", "Moonsu", ""], ["Lee", "Jung Kwon", ""], ["Choi", "Youngduck", ""], ["Choi", "Yongseok", ""], ["Cho", "Dong-Yeon", ""], ["Kim", "Jiwon", ""]]}, {"id": "1806.06928", "submitter": "Risto Vuorio", "authors": "Risto Vuorio, Dong-Yeon Cho, Daejoong Kim, and Jiwon Kim", "title": "Meta Continual Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using neural networks in practical settings would benefit from the ability of\nthe networks to learn new tasks throughout their lifetimes without forgetting\nthe previous tasks. This ability is limited in the current deep neural networks\nby a problem called catastrophic forgetting, where training on new tasks tends\nto severely degrade performance on previous tasks. One way to lessen the impact\nof the forgetting problem is to constrain parameters that are important to\nprevious tasks to stay close to the optimal parameters. Recently, multiple\ncompetitive approaches for computing the importance of the parameters with\nrespect to the previous tasks have been presented. In this paper, we propose a\nlearning to optimize algorithm for mitigating catastrophic forgetting. Instead\nof trying to formulate a new constraint function ourselves, we propose to train\nanother neural network to predict parameter update steps that respect the\nimportance of parameters to the previous tasks. In the proposed meta-training\nscheme, the update predictor is trained to minimize loss on a combination of\ncurrent and past tasks. We show experimentally that the proposed approach works\nin the continual learning setting.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 06:49:54 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Vuorio", "Risto", ""], ["Cho", "Dong-Yeon", ""], ["Kim", "Daejoong", ""], ["Kim", "Jiwon", ""]]}, {"id": "1806.06931", "submitter": "Yangchen Pan", "authors": "Yangchen Pan, Amir-massoud Farahmand, Martha White, Saleh Nabi, Piyush\n  Grover, Daniel Nikovski", "title": "Reinforcement Learning with Function-Valued Action Spaces for Partial\n  Differential Equation Control", "comments": "ICML2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown that reinforcement learning (RL) is a promising\napproach to control dynamical systems described by partial differential\nequations (PDE). This paper shows how to use RL to tackle more general PDE\ncontrol problems that have continuous high-dimensional action spaces with\nspatial relationship among action dimensions. In particular, we propose the\nconcept of action descriptors, which encode regularities among\nspatially-extended action dimensions and enable the agent to control\nhigh-dimensional action PDEs. We provide theoretical evidence suggesting that\nthis approach can be more sample efficient compared to a conventional approach\nthat treats each action dimension separately and does not explicitly exploit\nthe spatial regularity of the action space. The action descriptor approach is\nthen used within the deep deterministic policy gradient algorithm. Experiments\non two PDE control problems, with up to 256-dimensional continuous actions,\nshow the advantage of the proposed approach over the conventional one.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 03:47:12 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Pan", "Yangchen", ""], ["Farahmand", "Amir-massoud", ""], ["White", "Martha", ""], ["Nabi", "Saleh", ""], ["Grover", "Piyush", ""], ["Nikovski", "Daniel", ""]]}, {"id": "1806.06946", "submitter": "Alexey Potapov", "authors": "Alexey Potapov, Innokentii Zhdanov, Oleg Scherbakov, Nikolai\n  Skorobogatko, Hugo Latapie, Enzo Fenoglio", "title": "Semantic Image Retrieval by Uniting Deep Neural Networks and Cognitive\n  Architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image and video retrieval by their semantic content has been an important and\nchallenging task for years, because it ultimately requires bridging the\nsymbolic/subsymbolic gap. Recent successes in deep learning enabled detection\nof objects belonging to many classes greatly outperforming traditional computer\nvision techniques. However, deep learning solutions capable of executing\nretrieval queries are still not available. We propose a hybrid solution\nconsisting of a deep neural network for object detection and a cognitive\narchitecture for query execution. Specifically, we use YOLOv2 and OpenCog.\nQueries allowing the retrieval of video frames containing objects of specified\nclasses and specified spatial arrangement are implemented.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 14:53:53 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Potapov", "Alexey", ""], ["Zhdanov", "Innokentii", ""], ["Scherbakov", "Oleg", ""], ["Skorobogatko", "Nikolai", ""], ["Latapie", "Hugo", ""], ["Fenoglio", "Enzo", ""]]}, {"id": "1806.06950", "submitter": "Patrick Chen", "authors": "Patrick H. Chen, Si Si, Yang Li, Ciprian Chelba, Cho-jui Hsieh", "title": "GroupReduce: Block-Wise Low-Rank Approximation for Neural Language Model\n  Shrinking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model compression is essential for serving large deep neural nets on devices\nwith limited resources or applications that require real-time responses. As a\ncase study, a state-of-the-art neural language model usually consists of one or\nmore recurrent layers sandwiched between an embedding layer used for\nrepresenting input tokens and a softmax layer for generating output tokens. For\nproblems with a very large vocabulary size, the embedding and the softmax\nmatrices can account for more than half of the model size. For instance, the\nbigLSTM model achieves state-of- the-art performance on the One-Billion-Word\n(OBW) dataset with around 800k vocabulary, and its word embedding and softmax\nmatrices use more than 6GBytes space, and are responsible for over 90% of the\nmodel parameters. In this paper, we propose GroupReduce, a novel compression\nmethod for neural language models, based on vocabulary-partition (block) based\nlow-rank matrix approximation and the inherent frequency distribution of tokens\n(the power-law distribution of words). The experimental results show our method\ncan significantly outperform traditional compression methods such as low-rank\napproximation and pruning. On the OBW dataset, our method achieved 6.6 times\ncompression rate for the embedding and softmax matrices, and when combined with\nquantization, our method can achieve 26 times compression rate, which\ntranslates to a factor of 12.8 times compression for the entire model with very\nlittle degradation in perplexity.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 23:08:15 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Chen", "Patrick H.", ""], ["Si", "Si", ""], ["Li", "Yang", ""], ["Chelba", "Ciprian", ""], ["Hsieh", "Cho-jui", ""]]}, {"id": "1806.06953", "submitter": "Wenjia Meng", "authors": "Wenjia Meng, Qian Zheng, Long Yang, Pengfei Li, Gang Pan", "title": "Qualitative Measurements of Policy Discrepancy for Return-Based Deep\n  Q-Network", "comments": null, "journal-ref": null, "doi": "10.1109/TNNLS.2019.2948892", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deep Q-network (DQN) and return-based reinforcement learning are two\npromising algorithms proposed in recent years. DQN brings advances to complex\nsequential decision problems, while return-based algorithms have advantages in\nmaking use of sample trajectories. In this paper, we propose a general\nframework to combine DQN and most of the return-based reinforcement learning\nalgorithms, named R-DQN. We show the performance of traditional DQN can be\nimproved effectively by introducing return-based reinforcement learning. In\norder to further improve the R-DQN, we design a strategy with two measurements\nwhich can qualitatively measure the policy discrepancy. Moreover, we give the\ntwo measurements' bounds in the proposed R-DQN framework. We show that\nalgorithms with our strategy can accurately express the trace coefficient and\nachieve a better approximation to return. The experiments, conducted on several\nrepresentative tasks from the OpenAI Gym library, validate the effectiveness of\nthe proposed measurements. The results also show that the algorithms with our\nstrategy outperform the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jun 2018 12:12:18 GMT"}, {"version": "v2", "created": "Sun, 8 Jul 2018 10:26:32 GMT"}, {"version": "v3", "created": "Thu, 28 Nov 2019 13:31:08 GMT"}], "update_date": "2019-12-02", "authors_parsed": [["Meng", "Wenjia", ""], ["Zheng", "Qian", ""], ["Yang", "Long", ""], ["Li", "Pengfei", ""], ["Pan", "Gang", ""]]}, {"id": "1806.06972", "submitter": "Soumya Wadhwa", "authors": "Soumya Wadhwa and Khyathi Raghavi Chandu and Eric Nyberg", "title": "Comparative Analysis of Neural QA models on SQuAD", "comments": "Accepted at Workshop on Machine Reading for Question Answering\n  (MRQA), ACL 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of Question Answering has gained prominence in the past few decades\nfor testing the ability of machines to understand natural language. Large\ndatasets for Machine Reading have led to the development of neural models that\ncater to deeper language understanding compared to information retrieval tasks.\nDifferent components in these neural architectures are intended to tackle\ndifferent challenges. As a first step towards achieving generalization across\nmultiple domains, we attempt to understand and compare the peculiarities of\nexisting end-to-end neural models on the Stanford Question Answering Dataset\n(SQuAD) by performing quantitative as well as qualitative analysis of the\nresults attained by each of them. We observed that prediction errors reflect\ncertain model-specific biases, which we further discuss in this paper.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 22:29:51 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Wadhwa", "Soumya", ""], ["Chandu", "Khyathi Raghavi", ""], ["Nyberg", "Eric", ""]]}, {"id": "1806.07011", "submitter": "Xavier Puig", "authors": "Xavier Puig, Kevin Ra, Marko Boben, Jiaman Li, Tingwu Wang, Sanja\n  Fidler, Antonio Torralba", "title": "VirtualHome: Simulating Household Activities via Programs", "comments": "CVPR 2018 (Oral)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we are interested in modeling complex activities that occur in\na typical household. We propose to use programs, i.e., sequences of atomic\nactions and interactions, as a high level representation of complex tasks.\nPrograms are interesting because they provide a non-ambiguous representation of\na task, and allow agents to execute them. However, nowadays, there is no\ndatabase providing this type of information. Towards this goal, we first\ncrowd-source programs for a variety of activities that happen in people's\nhomes, via a game-like interface used for teaching kids how to code. Using the\ncollected dataset, we show how we can learn to extract programs directly from\nnatural language descriptions or from videos. We then implement the most common\natomic (inter)actions in the Unity3D game engine, and use our programs to\n\"drive\" an artificial agent to execute tasks in a simulated household\nenvironment. Our VirtualHome simulator allows us to create a large activity\nvideo dataset with rich ground-truth, enabling training and testing of video\nunderstanding models. We further showcase examples of our agent performing\ntasks in our VirtualHome based on language descriptions.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 02:16:44 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Puig", "Xavier", ""], ["Ra", "Kevin", ""], ["Boben", "Marko", ""], ["Li", "Jiaman", ""], ["Wang", "Tingwu", ""], ["Fidler", "Sanja", ""], ["Torralba", "Antonio", ""]]}, {"id": "1806.07037", "submitter": "Shota Motoura", "authors": "Shota Motoura, Kazeto Yamamoto, Shumpei Kubosawa, Takashi Onishi", "title": "Translating MFM into FOL: towards plant operation planning", "comments": null, "journal-ref": "Proceedings of the Third International Workshop on Functional\n  Modelling for Design and Operation of Engineering Systems, 24 - 25 May, 2018,\n  Kurashiki, Japan", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a method to translate multilevel flow modeling (MFM) into\na first-order language (FOL), which enables the utilisation of logical\ntechniques, such as inference engines and abductive reasoners. An example of\nthis is a planning task for a toy plant that can be solved in FOL using\nabduction. In addition, owing to the expressivity of FOL, the language is\ncapable of describing actions and their preconditions. This allows the\nderivation of procedures consisting of multiple actions.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 04:24:16 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Motoura", "Shota", ""], ["Yamamoto", "Kazeto", ""], ["Kubosawa", "Shumpei", ""], ["Onishi", "Takashi", ""]]}, {"id": "1806.07072", "submitter": "Sauradip Nag", "authors": "Sauradip Nag, Palaiahnakote Shivakumara, Wu Yirui, Umapada Pal, and\n  Tong Lu", "title": "A New COLD Feature based Handwriting Analysis for Ethnicity/Nationality\n  Identification", "comments": "Accepted in ICFHR18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CG cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying crime for forensic investigating teams when crimes involve people\nof different nationals is challenging. This paper proposes a new method for\nethnicity (nationality) identification based on Cloud of Line Distribution\n(COLD) features of handwriting components. The proposed method, at first,\nexplores tangent angle for the contour pixels in each row and the mean of\nintensity values of each row in an image for segmenting text lines. For\nsegmented text lines, we use tangent angle and direction of base lines to\nremove rule lines in the image. We use polygonal approximation for finding\ndominant points for contours of edge components. Then the proposed method\nconnects the nearest dominant points of every dominant point, which results in\nline segments of dominant point pairs. For each line segment, the proposed\nmethod estimates angle and length, which gives a point in polar domain. For all\nthe line segments, the proposed method generates dense points in polar domain,\nwhich results in COLD distribution. As character component shapes change,\naccording to nationals, the shape of the distribution changes. This observation\nis extracted based on distance from pixels of distribution to Principal Axis of\nthe distribution. Then the features are subjected to an SVM classifier for\nidentifying nationals. Experiments are conducted on a complex dataset, which\nshow the proposed method is effective and outperforms the existing method\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 07:14:54 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Nag", "Sauradip", ""], ["Shivakumara", "Palaiahnakote", ""], ["Yirui", "Wu", ""], ["Pal", "Umapada", ""], ["Lu", "Tong", ""]]}, {"id": "1806.07082", "submitter": "Santtu Tikka", "authors": "Santtu Tikka and Juha Karvanen", "title": "Simplifying Probabilistic Expressions in Causal Inference", "comments": "This is the version published in JMLR", "journal-ref": "Journal of Machine Learning Research (JMLR), 18(36):1-30, 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Obtaining a non-parametric expression for an interventional distribution is\none of the most fundamental tasks in causal inference. Such an expression can\nbe obtained for an identifiable causal effect by an algorithm or by manual\napplication of do-calculus. Often we are left with a complicated expression\nwhich can lead to biased or inefficient estimates when missing data or\nmeasurement errors are involved. We present an automatic simplification\nalgorithm that seeks to eliminate symbolically unnecessary variables from these\nexpressions by taking advantage of the structure of the underlying graphical\nmodel. Our method is applicable to all causal effect formulas and is readily\navailable in the R package causaleffect.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 07:45:14 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Tikka", "Santtu", ""], ["Karvanen", "Juha", ""]]}, {"id": "1806.07111", "submitter": "Giuseppe De Nittis", "authors": "Giuseppe De Nittis and Nicola Gatti", "title": "Facing Multiple Attacks in Adversarial Patrolling Games with Alarmed\n  Targets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on adversarial patrolling games on arbitrary graphs, where the\nDefender can control a mobile resource, the targets are alarmed by an alarm\nsystem, and the Attacker can observe the actions of the mobile resource of the\nDefender and perform different attacks exploiting multiple resources. This\nscenario can be modeled as a zero-sum extensive-form game in which each player\ncan play multiple times. The game tree is exponentially large both in the size\nof the graph and in the number of attacking resources. We show that when the\nnumber of the Attacker's resources is free, the problem of computing the\nequilibrium path is NP-hard, while when the number of resources is fixed, the\nequilibrium path can be computed in poly-time. We provide a dynamic-programming\nalgorithm that, given the number of the Attacker's resources, computes the\nequilibrium path requiring poly-time in the size of the graph and exponential\ntime in the number of the resources. Furthermore, since in real-world scenarios\nit is implausible that the Defender knows the number of attacking resources, we\nstudy the robustness of the Defender's strategy when she makes a wrong guess\nabout that number. We show that even the error of just a single resource can\nlead to an arbitrary inefficiency, when the inefficiency is defined as the\nratio of the Defender's utilities obtained with a wrong guess and a correct\nguess. However, a more suitable definition of inefficiency is given by the\ndifference of the Defender's utilities: this way, we observe that the higher\nthe error in the estimation, the higher the loss for the Defender. Then, we\ninvestigate the performance of online algorithms when no information about the\nAttacker's resources is available. Finally, we resort to randomized online\nalgorithms showing that we can obtain a competitive factor that is twice better\nthan the one that can be achieved by any deterministic online algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 08:57:03 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["De Nittis", "Giuseppe", ""], ["Gatti", "Nicola", ""]]}, {"id": "1806.07129", "submitter": "Dennis Collaris", "authors": "Dennis Collaris, Leo M. Vink, Jarke J. van Wijk", "title": "Instance-Level Explanations for Fraud Detection: A Case Study", "comments": "presented at 2018 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2018), Stockholm, Sweden", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fraud detection is a difficult problem that can benefit from predictive\nmodeling. However, the verification of a prediction is challenging; for a\nsingle insurance policy, the model only provides a prediction score. We present\na case study where we reflect on different instance-level model explanation\ntechniques to aid a fraud detection team in their work. To this end, we\ndesigned two novel dashboards combining various state-of-the-art explanation\ntechniques. These enable the domain expert to analyze and understand\npredictions, dramatically speeding up the process of filtering potential fraud\ncases. Finally, we discuss the lessons learned and outline open research\nissues.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 09:42:36 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Collaris", "Dennis", ""], ["Vink", "Leo M.", ""], ["van Wijk", "Jarke J.", ""]]}, {"id": "1806.07135", "submitter": "Luca Pulina", "authors": "Arthur Bit-Monnot, Francesco Leofante, Luca Pulina, Erika Abraham, and\n  Armando Tacchella", "title": "SMarTplan: a Task Planner for Smart Factories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart factories are on the verge of becoming the new industrial paradigm,\nwherein optimization permeates all aspects of production, from concept\ngeneration to sales. To fully pursue this paradigm, flexibility in the\nproduction means as well as in their timely organization is of paramount\nimportance. AI is planning a major role in this transition, but the scenarios\nencountered in practice might be challenging for current tools. Task planning\nis one example where AI enables more efficient and flexible operation through\nan online automated adaptation and rescheduling of the activities to cope with\nnew operational constraints and demands.\n  In this paper we present SMarTplan, a task planner specifically conceived to\ndeal with real-world scenarios in the emerging smart factory paradigm.\nIncluding both special-purpose and general-purpose algorithms, SMarTplan is\nbased on current automated reasoning technology and it is designed to tackle\ncomplex application domains. In particular, we show its effectiveness on a\nlogistic scenario, by comparing its specialized version with the general\npurpose one, and extending the comparison to other state-of-the-art task\nplanners.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 10:01:48 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Bit-Monnot", "Arthur", ""], ["Leofante", "Francesco", ""], ["Pulina", "Luca", ""], ["Abraham", "Erika", ""], ["Tacchella", "Armando", ""]]}, {"id": "1806.07164", "submitter": "Saurabh Joshi", "authors": "Saurabh Joshi, Prateek Kumar, Ruben Martins and Sukrut Rao", "title": "Approximation Strategies for Incomplete MaxSAT", "comments": "10 pages, 3 algorithms, 1 figure, International Conference on\n  Principles and Practice of Constraint Programming (CP) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incomplete MaxSAT solving aims to quickly find a solution that attempts to\nminimize the sum of the weights of the unsatisfied soft clauses without\nproviding any optimality guarantees.\n  In this paper, we propose two approximation strategies for improving\nincomplete MaxSAT solving. In one of the strategies, we cluster the weights and\napproximate them with a representative weight. In another strategy, we break up\nthe problem of minimizing the sum of weights of unsatisfiable clauses into\nmultiple minimization subproblems. Experimental results show that approximation\nstrategies can be used to find better solutions than the best incomplete\nsolvers in the MaxSAT Evaluation 2017.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 11:37:50 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Joshi", "Saurabh", ""], ["Kumar", "Prateek", ""], ["Martins", "Ruben", ""], ["Rao", "Sukrut", ""]]}, {"id": "1806.07172", "submitter": "Santtu Tikka", "authors": "Santtu Tikka and Juha Karvanen", "title": "Surrogate Outcomes and Transportability", "comments": "This is the version published in the International Journal of\n  Approximate Reasoning", "journal-ref": "International Journal of Approximate Reasoning, 2019; 108: 21-37", "doi": "10.1016/j.ijar.2019.02.007", "report-no": null, "categories": "cs.AI stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Identification of causal effects is one of the most fundamental tasks of\ncausal inference. We consider an identifiability problem where some\nexperimental and observational data are available but neither data alone is\nsufficient for the identification of the causal effect of interest. Instead of\nthe outcome of interest, surrogate outcomes are measured in the experiments.\nThis problem is a generalization of identifiability using surrogate experiments\nand we label it as surrogate outcome identifiability. We show that the concept\nof transportability provides a sufficient criteria for determining surrogate\noutcome identifiability for a large class of queries.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 12:01:29 GMT"}, {"version": "v2", "created": "Thu, 21 Jun 2018 11:24:39 GMT"}, {"version": "v3", "created": "Thu, 29 Nov 2018 07:27:49 GMT"}, {"version": "v4", "created": "Tue, 12 Mar 2019 12:49:08 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Tikka", "Santtu", ""], ["Karvanen", "Juha", ""]]}, {"id": "1806.07199", "submitter": "Umberto Grandi", "authors": "Umberto Grandi", "title": "Agent-Mediated Social Choice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Direct democracy is often proposed as a possible solution to the 21st-century\nproblems of democracy. However, this suggestion clashes with the size and\ncomplexity of 21st-century societies, entailing an excessive cognitive burden\non voters, who would have to submit informed opinions on an excessive number of\nissues. In this paper I argue for the development of voting avatars, autonomous\nagents debating and voting on behalf of each citizen. Theoretical research from\nartificial intelligence, and in particular multiagent systems and computational\nsocial choice, proposes 21st-century techniques for this purpose, from the\ncompact representation of a voter's preferences and values, to the development\nof voting procedures for autonomous agents use only.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 13:03:11 GMT"}, {"version": "v2", "created": "Tue, 10 Jul 2018 10:11:49 GMT"}], "update_date": "2018-07-11", "authors_parsed": [["Grandi", "Umberto", ""]]}, {"id": "1806.07239", "submitter": "Yutaka Nagashima", "authors": "Yutaka Nagashima, Yilun He", "title": "PaMpeR: Proof Method Recommendation System for Isabelle/HOL", "comments": "An anonymized version of this paper has been submitted to a Computer\n  Science conference in April 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deciding which sub-tool to use for a given proof state requires expertise\nspecific to each ITP. To mitigate this problem, we present PaMpeR, a Proof\nMethod Recommendation system for Isabelle/HOL. Given a proof state, PaMpeR\nrecommends proof methods to discharge the proof goal and provides qualitative\nexplanations as to why it suggests these methods. PaMpeR generates these\nrecommendations based on existing hand-written proof corpora, thus transferring\nexperienced users' expertise to new users. Our evaluation shows that PaMpeR\ncorrectly predicts experienced users' proof methods invocation especially when\nit comes to special purpose proof methods.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 13:58:14 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Nagashima", "Yutaka", ""], ["He", "Yilun", ""]]}, {"id": "1806.07247", "submitter": "Canyi Lu", "authors": "Canyi Lu", "title": "Tensor-Tensor Product Toolbox", "comments": "arXiv admin note: substantial text overlap with arXiv:1804.03728.\n  Carnegie Mellon University", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The tensor-tensor product (t-product) [M. E. Kilmer and C. D. Martin, 2011]\nis a natural generalization of matrix multiplication. Based on t-product, many\noperations on matrix can be extended to tensor cases, including tensor SVD,\ntensor spectral norm, tensor nuclear norm [C. Lu, et al., 2018] and many\nothers. The linear algebraic structure of tensors are similar to the matrix\ncases. We develop a Matlab toolbox to implement several basic operations on\ntensors based on t-product. The toolbox is available at\nhttps://github.com/canyilu/tproduct.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jun 2018 08:14:42 GMT"}, {"version": "v2", "created": "Wed, 20 Jun 2018 03:23:18 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Lu", "Canyi", ""]]}, {"id": "1806.07297", "submitter": "TImothee Lacroix", "authors": "Timoth\\'ee Lacroix, Nicolas Usunier, Guillaume Obozinski", "title": "Canonical Tensor Decomposition for Knowledge Base Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of Knowledge Base Completion can be framed as a 3rd-order binary\ntensor completion problem. In this light, the Canonical Tensor Decomposition\n(CP) (Hitchcock, 1927) seems like a natural solution; however, current\nimplementations of CP on standard Knowledge Base Completion benchmarks are\nlagging behind their competitors. In this work, we attempt to understand the\nlimits of CP for knowledge base completion. First, we motivate and test a novel\nregularizer, based on tensor nuclear $p$-norms. Then, we present a\nreformulation of the problem that makes it invariant to arbitrary choices in\nthe inclusion of predicates or their reciprocals in the dataset. These two\nmethods combined allow us to beat the current state of the art on several\ndatasets with a CP decomposition, and obtain even better results using the more\nadvanced ComplEx model.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 14:57:18 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Lacroix", "Timoth\u00e9e", ""], ["Usunier", "Nicolas", ""], ["Obozinski", "Guillaume", ""]]}, {"id": "1806.07304", "submitter": "Han Guo", "authors": "Han Guo, Ramakanth Pasunuru, Mohit Bansal", "title": "Dynamic Multi-Level Multi-Task Learning for Sentence Simplification", "comments": "COLING 2018 (15 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sentence simplification aims to improve readability and understandability,\nbased on several operations such as splitting, deletion, and paraphrasing.\nHowever, a valid simplified sentence should also be logically entailed by its\ninput sentence. In this work, we first present a strong pointer-copy mechanism\nbased sequence-to-sequence sentence simplification model, and then improve its\nentailment and paraphrasing capabilities via multi-task learning with related\nauxiliary tasks of entailment and paraphrase generation. Moreover, we propose a\nnovel 'multi-level' layered soft sharing approach where each auxiliary task\nshares different (higher versus lower) level layers of the sentence\nsimplification model, depending on the task's semantic versus lexico-syntactic\nnature. We also introduce a novel multi-armed bandit based training approach\nthat dynamically learns how to effectively switch across tasks during\nmulti-task learning. Experiments on multiple popular datasets demonstrate that\nour model outperforms competitive simplification systems in SARI and FKGL\nautomatic metrics, and human evaluation. Further, we present several ablation\nanalyses on alternative layer sharing methods, soft versus hard sharing,\ndynamic multi-armed bandit sampling approaches, and our model's learned\nentailment and paraphrasing skills.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 15:21:37 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Guo", "Han", ""], ["Pasunuru", "Ramakanth", ""], ["Bansal", "Mohit", ""]]}, {"id": "1806.07342", "submitter": "Anton Kolonin", "authors": "Anton Kolonin, Ben Goertzel, Deborah Duong, Matt Ikle", "title": "A Reputation System for Artificial Societies", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One approach to achieving artificial general intelligence (AGI) is through\nthe emergence of complex structures and dynamic properties arising from\ndecentralized networks of interacting artificial intelligence (AI) agents.\nUnderstanding the principles of consensus in societies and finding ways to make\nconsensus more reliable becomes critically important as connectivity and\ninteraction speed increase in modern distributed systems of hybrid collective\nintelligences, which include both humans and computer systems. We propose a new\nform of reputation-based consensus with greater resistance to reputation gaming\nthan current systems have. We discuss options for its implementation, and\nprovide initial practical results.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 16:50:46 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Kolonin", "Anton", ""], ["Goertzel", "Ben", ""], ["Duong", "Deborah", ""], ["Ikle", "Matt", ""]]}, {"id": "1806.07346", "submitter": "Imon Banerjee", "authors": "Imon Banerjee, Hailey H. Choi, Terry Desser, Daniel L. Rubin", "title": "A Scalable Machine Learning Approach for Inferring Probabilistic\n  US-LI-RADS Categorization", "comments": "AMIA Annual Symposium 2018 (accepted)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a scalable computerized approach for large-scale inference of\nLiver Imaging Reporting and Data System (LI-RADS) final assessment categories\nin narrative ultrasound (US) reports. Although our model was trained on reports\ncreated using a LI-RADS template, it was also able to infer LI-RADS scoring for\nunstructured reports that were created before the LI-RADS guidelines were\nestablished. No human-labelled data was required in any step of this study; for\ntraining, LI-RADS scores were automatically extracted from those reports that\ncontained structured LI-RADS scores, and it translated the derived knowledge to\nreasoning on unstructured radiology reports. By providing automated LI-RADS\ncategorization, our approach may enable standardizing screening recommendations\nand treatment planning of patients at risk for hepatocellular carcinoma, and it\nmay facilitate AI-based healthcare research with US images by offering large\nscale text mining and data gathering opportunities from standard hospital\nclinical data repositories.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 20:11:22 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Banerjee", "Imon", ""], ["Choi", "Hailey H.", ""], ["Desser", "Terry", ""], ["Rubin", "Daniel L.", ""]]}, {"id": "1806.07366", "submitter": "David Duvenaud", "authors": "Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, David Duvenaud", "title": "Neural Ordinary Differential Equations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new family of deep neural network models. Instead of\nspecifying a discrete sequence of hidden layers, we parameterize the derivative\nof the hidden state using a neural network. The output of the network is\ncomputed using a black-box differential equation solver. These continuous-depth\nmodels have constant memory cost, adapt their evaluation strategy to each\ninput, and can explicitly trade numerical precision for speed. We demonstrate\nthese properties in continuous-depth residual networks and continuous-time\nlatent variable models. We also construct continuous normalizing flows, a\ngenerative model that can train by maximum likelihood, without partitioning or\nordering the data dimensions. For training, we show how to scalably\nbackpropagate through any ODE solver, without access to its internal\noperations. This allows end-to-end training of ODEs within larger models.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 17:50:12 GMT"}, {"version": "v2", "created": "Wed, 3 Oct 2018 00:13:07 GMT"}, {"version": "v3", "created": "Mon, 22 Oct 2018 22:06:50 GMT"}, {"version": "v4", "created": "Tue, 15 Jan 2019 01:56:48 GMT"}, {"version": "v5", "created": "Sat, 14 Dec 2019 02:01:18 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["Chen", "Ricky T. Q.", ""], ["Rubanova", "Yulia", ""], ["Bettencourt", "Jesse", ""], ["Duvenaud", "David", ""]]}, {"id": "1806.07371", "submitter": "Guangxiang Zhu", "authors": "Guangxiang Zhu, Zhiao Huang and Chongjie Zhang", "title": "Object-Oriented Dynamics Predictor", "comments": "Accepted to NIPS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalization has been one of the major challenges for learning dynamics\nmodels in model-based reinforcement learning. However, previous work on\naction-conditioned dynamics prediction focuses on learning the pixel-level\nmotion and thus does not generalize well to novel environments with different\nobject layouts. In this paper, we present a novel object-oriented framework,\ncalled object-oriented dynamics predictor (OODP), which decomposes the\nenvironment into objects and predicts the dynamics of objects conditioned on\nboth actions and object-to-object relations. It is an end-to-end neural network\nand can be trained in an unsupervised manner. To enable the generalization\nability of dynamics learning, we design a novel CNN-based relation mechanism\nthat is class-specific (rather than object-specific) and exploits the locality\nprinciple. Empirical results show that OODP significantly outperforms previous\nmethods in terms of generalization over novel environments with various object\nlayouts. OODP is able to learn from very few environments and accurately\npredict dynamics in a large number of unseen environments. In addition, OODP\nlearns semantically and visually interpretable dynamics models.\n", "versions": [{"version": "v1", "created": "Fri, 25 May 2018 13:54:36 GMT"}, {"version": "v2", "created": "Sat, 27 Oct 2018 12:01:37 GMT"}, {"version": "v3", "created": "Tue, 30 Oct 2018 05:39:08 GMT"}], "update_date": "2018-10-31", "authors_parsed": [["Zhu", "Guangxiang", ""], ["Huang", "Zhiao", ""], ["Zhang", "Chongjie", ""]]}, {"id": "1806.07376", "submitter": "Mehul Bhatt", "authors": "Jakob Suchan, Mehul Bhatt, Srikrishna Vardarajan, Seyed Ali Amirshahi,\n  Stella Yu", "title": "Semantic Analysis of (Reflectional) Visual Symmetry: A Human-Centred\n  Computational Model for Declarative Explainability", "comments": "Preprint of accepted article / Journal: Advances in Cognitive\n  Systems. ( http://www.cogsys.org/journal )", "journal-ref": "Advances in Cognitive Systems. (http://www.cogsys.org/journal),\n  2018", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a computational model for the semantic interpretation of symmetry\nin naturalistic scenes. Key features include a human-centred representation,\nand a declarative, explainable interpretation model supporting deep semantic\nquestion-answering founded on an integration of methods in knowledge\nrepresentation and deep learning based computer vision. In the backdrop of the\nvisual arts, we showcase the framework's capability to generate human-centred,\nqueryable, relational structures, also evaluating the framework with an\nempirical study on the human perception of visual symmetry. Our framework\nrepresents and is driven by the application of foundational, integrated Vision\nand Knowledge Representation and Reasoning methods for applications in the\narts, and the psychological and social sciences.\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 11:47:46 GMT"}, {"version": "v2", "created": "Fri, 14 Sep 2018 17:59:34 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Suchan", "Jakob", ""], ["Bhatt", "Mehul", ""], ["Vardarajan", "Srikrishna", ""], ["Amirshahi", "Seyed Ali", ""], ["Yu", "Stella", ""]]}, {"id": "1806.07377", "submitter": "Shani Gamrian", "authors": "Shani Gamrian, Yoav Goldberg", "title": "Transfer Learning for Related Reinforcement Learning Tasks via\n  Image-to-Image Translation", "comments": "Proceedings of the 36th International Conference on Machine Learning\n  (ICML 2019)", "journal-ref": "Proceedings of the 36th International Conference on Machine\n  Learning, PMLR 97:2063-2072, 2019", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the remarkable success of Deep RL in learning control policies from\nraw pixels, the resulting models do not generalize. We demonstrate that a\ntrained agent fails completely when facing small visual changes, and that\nfine-tuning---the common transfer learning paradigm---fails to adapt to these\nchanges, to the extent that it is faster to re-train the model from scratch. We\nshow that by separating the visual transfer task from the control policy we\nachieve substantially better sample efficiency and transfer behavior, allowing\nan agent trained on the source task to transfer well to the target tasks. The\nvisual mapping from the target to the source domain is performed using\nunaligned GANs, resulting in a control policy that can be further improved\nusing imitation learning from imperfect demonstrations. We demonstrate the\napproach on synthetic visual variants of the Breakout game, as well as on\ntransfer between subsequent levels of Road Fighter, a Nintendo car-driving\ngame. A visualization of our approach can be seen in\nhttps://youtu.be/4mnkzYyXMn4 and https://youtu.be/KCGTrQi6Ogo .\n", "versions": [{"version": "v1", "created": "Thu, 31 May 2018 09:25:29 GMT"}, {"version": "v2", "created": "Mon, 25 Jun 2018 15:24:18 GMT"}, {"version": "v3", "created": "Tue, 26 Jun 2018 14:03:11 GMT"}, {"version": "v4", "created": "Wed, 12 Dec 2018 19:43:31 GMT"}, {"version": "v5", "created": "Sat, 2 Feb 2019 16:09:02 GMT"}, {"version": "v6", "created": "Thu, 4 Jul 2019 06:51:34 GMT"}], "update_date": "2019-07-05", "authors_parsed": [["Gamrian", "Shani", ""], ["Goldberg", "Yoav", ""]]}, {"id": "1806.07380", "submitter": "Jingqing Zhang", "authors": "Binbing Liao, Jingqing Zhang, Chao Wu, Douglas McIlwraith, Tong Chen,\n  Shengwen Yang, Yike Guo, Fei Wu", "title": "Deep Sequence Learning with Auxiliary Information for Traffic Prediction", "comments": "KDD 2018. The first two authors share equal contributions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting traffic conditions from online route queries is a challenging task\nas there are many complicated interactions over the roads and crowds involved.\nIn this paper, we intend to improve traffic prediction by appropriate\nintegration of three kinds of implicit but essential factors encoded in\nauxiliary information. We do this within an encoder-decoder sequence learning\nframework that integrates the following data: 1) offline geographical and\nsocial attributes. For example, the geographical structure of roads or public\nsocial events such as national celebrations; 2) road intersection information.\nIn general, traffic congestion occurs at major junctions; 3) online crowd\nqueries. For example, when many online queries issued for the same destination\ndue to a public performance, the traffic around the destination will\npotentially become heavier at this location after a while. Qualitative and\nquantitative experiments on a real-world dataset from Baidu have demonstrated\nthe effectiveness of our framework.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jun 2018 13:38:22 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Liao", "Binbing", ""], ["Zhang", "Jingqing", ""], ["Wu", "Chao", ""], ["McIlwraith", "Douglas", ""], ["Chen", "Tong", ""], ["Yang", "Shengwen", ""], ["Guo", "Yike", ""], ["Wu", "Fei", ""]]}, {"id": "1806.07439", "submitter": "Yun Long", "authors": "Yun Long, Xueyuan She, Saibal Mukhopadhyay", "title": "HybridNet: Integrating Model-based and Data-driven Learning to Predict\n  Evolution of Dynamical Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The robotic systems continuously interact with complex dynamical systems in\nthe physical world. Reliable predictions of spatiotemporal evolution of these\ndynamical systems, with limited knowledge of system dynamics, are crucial for\nautonomous operation. In this paper, we present HybridNet, a framework that\nintegrates data-driven deep learning and model-driven computation to reliably\npredict spatiotemporal evolution of a dynamical systems even with in-exact\nknowledge of their parameters. A data-driven deep neural network (DNN) with\nConvolutional LSTM (ConvLSTM) as the backbone is employed to predict the\ntime-varying evolution of the external forces/perturbations. On the other hand,\nthe model-driven computation is performed using Cellular Neural Network (CeNN),\na neuro-inspired algorithm to model dynamical systems defined by coupled\npartial differential equations (PDEs). CeNN converts the intricate numerical\ncomputation into a series of convolution operations, enabling a trainable PDE\nsolver. With a feedback control loop, HybridNet can learn the physical\nparameters governing the system's dynamics in real-time, and accordingly adapt\nthe computation models to enhance prediction accuracy for time-evolving\ndynamical systems. The experimental results on two dynamical systems, namely,\nheat convection-diffusion system, and fluid dynamical system, demonstrate that\nthe HybridNet produces higher accuracy than the state-of-the-art deep learning\nbased approach.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 19:32:42 GMT"}, {"version": "v2", "created": "Sat, 5 Jan 2019 07:12:14 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Long", "Yun", ""], ["She", "Xueyuan", ""], ["Mukhopadhyay", "Saibal", ""]]}, {"id": "1806.07461", "submitter": "Khanh Hung Hoang", "authors": "Khanh-Hung Hoang and Tu-Bao Ho", "title": "Learning Treatment Regimens from Electronic Medical Records", "comments": "This is a post-peer-review, pre-copyedit version of an article\n  published in PAKDD 2018's proceeding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Appropriate treatment regimens play a vital role in improving patient health\nstatus. Although some achievements have been made, few of the recent studies of\nlearning treatment regimens have exploited different kinds of patient\ninformation due to the difficulty in adopting heterogeneous data to many data\nmining methods. Moreover, current studies seem too rigid with fixed intervals\nof treatment periods corresponding to the varying lengths of hospital stay. To\nthis end, this work proposes a generic data-driven framework which can derive\ngroup-treatment regimens from electronic medical records by utilizing a\nmixed-variate restricted Boltzmann machine and incorporating medical domain\nknowledge. We conducted experiments on coronary artery disease as a case study.\nThe obtained results show that the framework is promising and capable of\nassisting physicians in making clinical decisions.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jun 2018 22:59:29 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Hoang", "Khanh-Hung", ""], ["Ho", "Tu-Bao", ""]]}, {"id": "1806.07470", "submitter": "Marcel Robeer", "authors": "Jasper van der Waa, Marcel Robeer, Jurriaan van Diggelen, Matthieu\n  Brinkhuis, Mark Neerincx", "title": "Contrastive Explanations with Local Foil Trees", "comments": "presented at 2018 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2018), Stockholm, Sweden", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in interpretable Machine Learning (iML) and eXplainable AI\n(XAI) construct explanations based on the importance of features in\nclassification tasks. However, in a high-dimensional feature space this\napproach may become unfeasible without restraining the set of important\nfeatures. We propose to utilize the human tendency to ask questions like \"Why\nthis output (the fact) instead of that output (the foil)?\" to reduce the number\nof features to those that play a main role in the asked contrast. Our proposed\nmethod utilizes locally trained one-versus-all decision trees to identify the\ndisjoint set of rules that causes the tree to classify data points as the foil\nand not as the fact. In this study we illustrate this approach on three\nbenchmark classification tasks.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 21:12:37 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["van der Waa", "Jasper", ""], ["Robeer", "Marcel", ""], ["van Diggelen", "Jurriaan", ""], ["Brinkhuis", "Matthieu", ""], ["Neerincx", "Mark", ""]]}, {"id": "1806.07495", "submitter": "Hamed Shahbazi", "authors": "Hamed Shahbazi, Xiaoli Z. Fern, Reza Ghaeini, Chao Ma, Rasha Obeidat,\n  Prasad Tadepalli", "title": "Joint Neural Entity Disambiguation with Output Space Search", "comments": "Accepted as a long paper at COLING 2018, 11 pages", "journal-ref": "Proceedings of COLING 2018", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a novel model for entity disambiguation that\ncombines both local contextual information and global evidences through Limited\nDiscrepancy Search (LDS). Given an input document, we start from a complete\nsolution constructed by a local model and conduct a search in the space of\npossible corrections to improve the local solution from a global view point.\nOur search utilizes a heuristic function to focus more on the least confident\nlocal decisions and a pruning function to score the global solutions based on\ntheir local fitness and the global coherences among the predicted entities.\nExperimental results on CoNLL 2003 and TAC 2010 benchmarks verify the\neffectiveness of our model.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 23:05:18 GMT"}], "update_date": "2019-08-23", "authors_parsed": [["Shahbazi", "Hamed", ""], ["Fern", "Xiaoli Z.", ""], ["Ghaeini", "Reza", ""], ["Ma", "Chao", ""], ["Obeidat", "Rasha", ""], ["Tadepalli", "Prasad", ""]]}, {"id": "1806.07498", "submitter": "Thibault Laugel", "authors": "Thibault Laugel, Xavier Renard, Marie-Jeanne Lesot, Christophe\n  Marsala, Marcin Detyniecki", "title": "Defining Locality for Surrogates in Post-hoc Interpretablity", "comments": "presented at 2018 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2018), Stockholm, Sweden", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local surrogate models, to approximate the local decision boundary of a\nblack-box classifier, constitute one approach to generate explanations for the\nrationale behind an individual prediction made by the back-box. This paper\nhighlights the importance of defining the right locality, the neighborhood on\nwhich a local surrogate is trained, in order to approximate accurately the\nlocal black-box decision boundary. Unfortunately, as shown in this paper, this\nissue is not only a parameter or sampling distribution challenge and has a\nmajor impact on the relevance and quality of the approximation of the local\nblack-box decision boundary and thus on the meaning and accuracy of the\ngenerated explanation. To overcome the identified problems, quantified with an\nadapted measure and procedure, we propose to generate surrogate-based\nexplanations for individual predictions based on a sampling centered on\nparticular place of the decision boundary, relevant for the prediction to be\nexplained, rather than on the prediction itself as it is classically done. We\nevaluate the novel approach compared to state-of-the-art methods and a\nstraightforward improvement thereof on four UCI datasets.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 23:18:48 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Laugel", "Thibault", ""], ["Renard", "Xavier", ""], ["Lesot", "Marie-Jeanne", ""], ["Marsala", "Christophe", ""], ["Detyniecki", "Marcin", ""]]}, {"id": "1806.07550", "submitter": "Shilin Zhu", "authors": "Shilin Zhu, Xin Dong, Hao Su", "title": "Binary Ensemble Neural Network: More Bits per Network or More Networks\n  per Bit?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary neural networks (BNN) have been studied extensively since they run\ndramatically faster at lower memory and power consumption than floating-point\nnetworks, thanks to the efficiency of bit operations. However, contemporary\nBNNs whose weights and activations are both single bits suffer from severe\naccuracy degradation. To understand why, we investigate the representation\nability, speed and bias/variance of BNNs through extensive experiments. We\nconclude that the error of BNNs is predominantly caused by the intrinsic\ninstability (training time) and non-robustness (train & test time). Inspired by\nthis investigation, we propose the Binary Ensemble Neural Network (BENN) which\nleverages ensemble methods to improve the performance of BNNs with limited\nefficiency cost. While ensemble techniques have been broadly believed to be\nonly marginally helpful for strong classifiers such as deep neural networks,\nour analyses and experiments show that they are naturally a perfect fit to\nboost BNNs. We find that our BENN, which is faster and much more robust than\nstate-of-the-art binary networks, can even surpass the accuracy of the\nfull-precision floating number network with the same architecture.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 04:48:18 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 07:08:37 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Zhu", "Shilin", ""], ["Dong", "Xin", ""], ["Su", "Hao", ""]]}, {"id": "1806.07552", "submitter": "Richard Tomsett", "authors": "Richard Tomsett, Dave Braines, Dan Harborne, Alun Preece, Supriyo\n  Chakraborty", "title": "Interpretable to Whom? A Role-based Model for Analyzing Interpretable\n  Machine Learning Systems", "comments": "presented at 2018 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2018), Stockholm, Sweden", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several researchers have argued that a machine learning system's\ninterpretability should be defined in relation to a specific agent or task: we\nshould not ask if the system is interpretable, but to whom is it interpretable.\nWe describe a model intended to help answer this question, by identifying\ndifferent roles that agents can fulfill in relation to the machine learning\nsystem. We illustrate the use of our model in a variety of scenarios, exploring\nhow an agent's role influences its goals, and the implications for defining\ninterpretability. Finally, we make suggestions for how our model could be\nuseful to interpretability researchers, system developers, and regulatory\nbodies auditing machine learning systems.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 04:52:33 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Tomsett", "Richard", ""], ["Braines", "Dave", ""], ["Harborne", "Dan", ""], ["Preece", "Alun", ""], ["Chakraborty", "Supriyo", ""]]}, {"id": "1806.07635", "submitter": "Ren\\'e Schuster", "authors": "Patrik Feth, Mohammed Naveed Akram, Ren\\'e Schuster and Oliver\n  Wasenm\\\"uller", "title": "Dynamic Risk Assessment for Vehicles of Higher Automation Levels by Deep\n  Learning", "comments": null, "journal-ref": "International Workshop on Artificial Intelligence Safety\n  Engineering (WAISE) 2018", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicles of higher automation levels require the creation of situation\nawareness. One important aspect of this situation awareness is an understanding\nof the current risk of a driving situation. In this work, we present a novel\napproach for the dynamic risk assessment of driving situations based on images\nof a front stereo camera using deep learning. To this end, we trained a deep\nneural network with recorded monocular images, disparity maps and a risk metric\nfor diverse traffic scenes. Our approach can be used to create the\naforementioned situation awareness of vehicles of higher automation levels and\ncan serve as a heterogeneous channel to systems based on radar or lidar sensors\nthat are used traditionally for the calculation of risk metrics.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 09:41:14 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Feth", "Patrik", ""], ["Akram", "Mohammed Naveed", ""], ["Schuster", "Ren\u00e9", ""], ["Wasenm\u00fcller", "Oliver", ""]]}, {"id": "1806.07637", "submitter": "Frank Glavin", "authors": "Frank G. Glavin and Michael G. Madden", "title": "Skilled Experience Catalogue: A Skill-Balancing Mechanism for Non-Player\n  Characters using Reinforcement Learning", "comments": "IEEE Conference on Computational Intelligence and Games (CIG). August\n  2018", "journal-ref": "IEEE Conference on Computational Intelligence and Games (CIG18),\n  Maastricht, The Netherlands, (2018)", "doi": "10.1109/CIG.2018.8490405", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a skill-balancing mechanism for adversarial\nnon-player characters (NPCs), called Skilled Experience Catalogue (SEC). The\nobjective of this mechanism is to approximately match the skill level of an NPC\nto an opponent in real-time. We test the technique in the context of a\nFirst-Person Shooter (FPS) game. Specifically, the technique adjusts a\nreinforcement learning NPC's proficiency with a weapon based on its current\nperformance against an opponent. Firstly, a catalogue of experience, in the\nform of stored learning policies, is built up by playing a series of training\ngames. Once the NPC has been sufficiently trained, the catalogue acts as a\ntimeline of experience with incremental knowledge milestones in the form of\nstored learning policies. If the NPC is performing poorly, it can jump to a\nlater stage in the learning timeline to be equipped with more informed\ndecision-making. Likewise, if it is performing significantly better than the\nopponent, it will jump to an earlier stage. The NPC continues to learn in\nreal-time using reinforcement learning but its policy is adjusted, as required,\nby loading the most suitable milestones for the current circumstances.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 09:41:54 GMT"}], "update_date": "2018-11-12", "authors_parsed": [["Glavin", "Frank G.", ""], ["Madden", "Michael G.", ""]]}, {"id": "1806.07685", "submitter": "Ivo D\\\"untsch", "authors": "Ivo D\\\"untsch, G\\\"unther Gediga, Hui Wang", "title": "Approximation by filter functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this exploratory article, we draw attention to the common formal ground\namong various estimators such as the belief functions of evidence theory and\ntheir relatives, approximation quality of rough set theory, and contextual\nprobability. The unifying concept will be a general filter function composed of\na basic probability and a weighting which varies according to the problem at\nhand. To compare the various filter functions we conclude with a simulation\nstudy with an example from the area of item response theory.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 12:09:52 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["D\u00fcntsch", "Ivo", ""], ["Gediga", "G\u00fcnther", ""], ["Wang", "Hui", ""]]}, {"id": "1806.07697", "submitter": "Zhao Kang", "authors": "Zhao Kang, Xiao Lu, Jinfeng Yi, Zenglin Xu", "title": "Self-weighted Multiple Kernel Learning for Graph-based Clustering and\n  Semi-supervised Classification", "comments": "Accepted by IJCAI 2018, Code is available", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple kernel learning (MKL) method is generally believed to perform better\nthan single kernel method. However, some empirical studies show that this is\nnot always true: the combination of multiple kernels may even yield an even\nworse performance than using a single kernel. There are two possible reasons\nfor the failure: (i) most existing MKL methods assume that the optimal kernel\nis a linear combination of base kernels, which may not hold true; and (ii) some\nkernel weights are inappropriately assigned due to noises and carelessly\ndesigned algorithms. In this paper, we propose a novel MKL framework by\nfollowing two intuitive assumptions: (i) each kernel is a perturbation of the\nconsensus kernel; and (ii) the kernel that is close to the consensus kernel\nshould be assigned a large weight. Impressively, the proposed method can\nautomatically assign an appropriate weight to each kernel without introducing\nadditional parameters, as existing methods do. The proposed framework is\nintegrated into a unified framework for graph-based clustering and\nsemi-supervised classification. We have conducted experiments on multiple\nbenchmark datasets and our empirical results verify the superiority of the\nproposed framework.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 12:46:43 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Kang", "Zhao", ""], ["Lu", "Xiao", ""], ["Yi", "Jinfeng", ""], ["Xu", "Zenglin", ""]]}, {"id": "1806.07709", "submitter": "Anthony Young", "authors": "Anthony Peter Young", "title": "Notes on Abstract Argumentation Theory", "comments": "93 pages, 38 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note reviews Section 2 of Dung's seminal 1995 paper on abstract\nargumentation theory. In particular, we clarify and make explicit all of the\nproofs mentioned therein, and provide more examples to illustrate the\ndefinitions, with the aim to help readers approaching abstract argumentation\ntheory for the first time. However, we provide minimal commentary and will\nrefer the reader to Dung's paper for the intuitions behind various concepts.\nThe appropriate mathematical prerequisites are provided in the appendices.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jun 2018 22:30:19 GMT"}, {"version": "v2", "created": "Tue, 1 Jan 2019 22:24:00 GMT"}, {"version": "v3", "created": "Sat, 22 Feb 2020 21:19:49 GMT"}], "update_date": "2020-02-25", "authors_parsed": [["Young", "Anthony Peter", ""]]}, {"id": "1806.07717", "submitter": "Joerg Puehrer", "authors": "Gerhard Brewka, J\\\"org P\\\"uhrer, Hannes Strass, Johannes P. Wallner,\n  Stefan Woltran", "title": "Weighted Abstract Dialectical Frameworks: Extended and Revised Report", "comments": "This is an extended and corrected version of the paper Weighted\n  Abstract Dialectical Frameworks published in the Proceedings of the 32nd AAAI\n  Conference on Artificial Intelligence (AAAI 2018)", "journal-ref": null, "doi": null, "report-no": "DBAI-TR-2018-110", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstract Dialectical Frameworks (ADFs) generalize Dung's argumentation\nframeworks allowing various relationships among arguments to be expressed in a\nsystematic way. We further generalize ADFs so as to accommodate arbitrary\nacceptance degrees for the arguments. This makes ADFs applicable in domains\nwhere both the initial status of arguments and their relationship are only\ninsufficiently specified by Boolean functions. We define all standard ADF\nsemantics for the weighted case, including grounded, preferred and stable\nsemantics. We illustrate our approach using acceptance degrees from the unit\ninterval and show how other valuation structures can be integrated. In each\ncase it is sufficient to specify how the generalized acceptance conditions are\nrepresented by formulas, and to specify the information ordering underlying the\ncharacteristic ADF operator. We also present complexity results for problems\nrelated to weighted ADFs.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 13:26:03 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2018 08:01:55 GMT"}], "update_date": "2018-09-10", "authors_parsed": [["Brewka", "Gerhard", ""], ["P\u00fchrer", "J\u00f6rg", ""], ["Strass", "Hannes", ""], ["Wallner", "Johannes P.", ""], ["Woltran", "Stefan", ""]]}, {"id": "1806.07722", "submitter": "Paul Kinsler", "authors": "Paul Kinsler", "title": "Stylized innovation: interrogating incrementally available randomised\n  dictionaries", "comments": "12 pages. Note: some pdf viewers have trouble rendering some of the\n  figures perfectly", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by recent work of Fink, Reeves, Palma and Farr (2017) on innovation\nin language, gastronomy, and technology, I study how new symbol discovery\nmanifests itself in terms of additional \"word\" vocabulary being available from\ndictionaries generated from a finite number of symbols. Several distinct\ndictionary generation models are investigated using numerical simulation, with\nemphasis on the scaling of knowledge as dictionary generators and parameters\nare varied, and the role of which order the symbols are discovered in.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jun 2018 09:29:05 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Kinsler", "Paul", ""]]}, {"id": "1806.07723", "submitter": "Lei Ma", "authors": "Lei Ma, Fuyuan Zhang, Minhui Xue, Bo Li, Yang Liu, Jianjun Zhao,\n  Yadong Wang", "title": "Combinatorial Testing for Deep Learning Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (DL) has achieved remarkable progress over the past decade and\nbeen widely applied to many safety-critical applications. However, the\nrobustness of DL systems recently receives great concerns, such as adversarial\nexamples against computer vision systems, which could potentially result in\nsevere consequences. Adopting testing techniques could help to evaluate the\nrobustness of a DL system and therefore detect vulnerabilities at an early\nstage. The main challenge of testing such systems is that its runtime state\nspace is too large: if we view each neuron as a runtime state for DL, then a DL\nsystem often contains massive states, rendering testing each state almost\nimpossible. For traditional software, combinatorial testing (CT) is an\neffective testing technique to reduce the testing space while obtaining\nrelatively high defect detection abilities. In this paper, we perform an\nexploratory study of CT on DL systems. We adapt the concept in CT and propose a\nset of coverage criteria for DL systems, as well as a CT coverage guided test\ngeneration technique. Our evaluation demonstrates that CT provides a promising\navenue for testing DL systems. We further pose several open questions and\ninteresting directions for combinatorial testing of DL systems.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 13:42:37 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Ma", "Lei", ""], ["Zhang", "Fuyuan", ""], ["Xue", "Minhui", ""], ["Li", "Bo", ""], ["Liu", "Yang", ""], ["Zhao", "Jianjun", ""], ["Wang", "Yadong", ""]]}, {"id": "1806.07757", "submitter": "Giuseppe De Nittis", "authors": "Giuseppe De Nittis and Nicola Gatti", "title": "How to Maximize the Spread of Social Influence: A Survey", "comments": "arXiv admin note: text overlap with arXiv:1601.06551,\n  arXiv:1602.05240, arXiv:1408.6282, arXiv:1212.0884, arXiv:1503.00024,\n  arXiv:1602.00165, arXiv:1111.4795 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This survey presents the main results achieved for the influence maximization\nproblem in social networks. This problem is well studied in the literature and,\nthanks to its recent applications, some of which currently deployed on the\nfield, it is receiving more and more attention in the scientific community. The\nproblem can be formulated as follows: given a graph, with each node having a\ncertain probability of influencing its neighbors, select a subset of vertices\nso that the number of nodes in the network that are influenced is maximized.\nStarting from this model, we introduce the main theoretical developments and\ncomputational results that have been achieved, taking into account different\ndiffusion models describing how the information spreads throughout the network,\nvarious ways in which the sources of information could be placed, and how to\ntackle the problem in the presence of uncertainties affecting the network.\nFinally, we present one of the main application that has been developed and\ndeployed exploiting tools and techniques previously discussed.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jun 2018 14:57:22 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["De Nittis", "Giuseppe", ""], ["Gatti", "Nicola", ""]]}, {"id": "1806.07822", "submitter": "Tanmay Shankar", "authors": "Tanmay Shankar, Nicholas Rhinehart, Katharina Muelling, Kris M. Kitani", "title": "Learning Neural Parsers with Deterministic Differentiable Imitation\n  Learning", "comments": "Accepted to Conference on Robot Learning, CoRL 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the problem of learning to decompose spatial tasks into segments,\nas exemplified by the problem of a painting robot covering a large object.\nInspired by the ability of classical decision tree algorithms to construct\nstructured partitions of their input spaces, we formulate the problem of\ndecomposing objects into segments as a parsing approach. We make the insight\nthat the derivation of a parse-tree that decomposes the object into segments\nclosely resembles a decision tree constructed by ID3, which can be done when\nthe ground-truth available. We learn to imitate an expert parsing oracle, such\nthat our neural parser can generalize to parse natural images without ground\ntruth. We introduce a novel deterministic policy gradient update, DRAG (i.e.,\nDeteRministically AGgrevate) in the form of a deterministic actor-critic\nvariant of AggreVaTeD, to train our neural parser. From another perspective,\nour approach is a variant of the Deterministic Policy Gradient suitable for the\nimitation learning setting. The deterministic policy representation offered by\ntraining our neural parser with DRAG allows it to outperform state of the art\nimitation and reinforcement learning approaches.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 16:15:54 GMT"}, {"version": "v2", "created": "Wed, 19 Sep 2018 14:58:04 GMT"}], "update_date": "2018-09-20", "authors_parsed": [["Shankar", "Tanmay", ""], ["Rhinehart", "Nicholas", ""], ["Muelling", "Katharina", ""], ["Kitani", "Kris M.", ""]]}, {"id": "1806.07840", "submitter": "Xu Chen", "authors": "En Li and Zhi Zhou and Xu Chen", "title": "Edge Intelligence: On-Demand Deep Learning Model Co-Inference with\n  Device-Edge Synergy", "comments": "ACM SIGCOMM Workshop on Mobile Edge Communications, Budapest,\n  Hungary, August 21-23, 2018. https://dl.acm.org/authorize?N665473", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.CV cs.MM cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the backbone technology of machine learning, deep neural networks (DNNs)\nhave have quickly ascended to the spotlight. Running DNNs on\nresource-constrained mobile devices is, however, by no means trivial, since it\nincurs high performance and energy overhead. While offloading DNNs to the cloud\nfor execution suffers unpredictable performance, due to the uncontrolled long\nwide-area network latency. To address these challenges, in this paper, we\npropose Edgent, a collaborative and on-demand DNN co-inference framework with\ndevice-edge synergy. Edgent pursues two design knobs: (1) DNN partitioning that\nadaptively partitions DNN computation between device and edge, in order to\nleverage hybrid computation resources in proximity for real-time DNN inference.\n(2) DNN right-sizing that accelerates DNN inference through early-exit at a\nproper intermediate DNN layer to further reduce the computation latency. The\nprototype implementation and extensive evaluations based on Raspberry Pi\ndemonstrate Edgent's effectiveness in enabling on-demand low-latency edge\nintelligence.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 16:56:54 GMT"}, {"version": "v2", "created": "Thu, 21 Jun 2018 03:36:27 GMT"}, {"version": "v3", "created": "Fri, 14 Sep 2018 02:37:07 GMT"}, {"version": "v4", "created": "Thu, 27 Dec 2018 11:49:55 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Li", "En", ""], ["Zhou", "Zhi", ""], ["Chen", "Xu", ""]]}, {"id": "1806.07851", "submitter": "Stephen James", "authors": "Jan Matas, Stephen James, Andrew J. Davison", "title": "Sim-to-Real Reinforcement Learning for Deformable Object Manipulation", "comments": "Published at the Conference on Robot Learning (CoRL) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have seen much recent progress in rigid object manipulation, but\ninteraction with deformable objects has notably lagged behind. Due to the large\nconfiguration space of deformable objects, solutions using traditional\nmodelling approaches require significant engineering work. Perhaps then,\nbypassing the need for explicit modelling and instead learning the control in\nan end-to-end manner serves as a better approach? Despite the growing interest\nin the use of end-to-end robot learning approaches, only a small amount of work\nhas focused on their applicability to deformable object manipulation. Moreover,\ndue to the large amount of data needed to learn these end-to-end solutions, an\nemerging trend is to learn control policies in simulation and then transfer\nthem over to the real world. To-date, no work has explored whether it is\npossible to learn and transfer deformable object policies. We believe that if\nsim-to-real methods are to be employed further, then it should be possible to\nlearn to interact with a wide variety of objects, and not only rigid objects.\nIn this work, we use a combination of state-of-the-art deep reinforcement\nlearning algorithms to solve the problem of manipulating deformable objects\n(specifically cloth). We evaluate our approach on three tasks --- folding a\ntowel up to a mark, folding a face towel diagonally, and draping a piece of\ncloth over a hanger. Our agents are fully trained in simulation with domain\nrandomisation, and then successfully deployed in the real world without having\nseen any real deformable objects.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 17:22:12 GMT"}, {"version": "v2", "created": "Mon, 8 Oct 2018 01:32:02 GMT"}], "update_date": "2018-10-09", "authors_parsed": [["Matas", "Jan", ""], ["James", "Stephen", ""], ["Davison", "Andrew J.", ""]]}, {"id": "1806.07857", "submitter": "Jose A. Arjona-Medina", "authors": "Jose A. Arjona-Medina, Michael Gillhofer, Michael Widrich, Thomas\n  Unterthiner, Johannes Brandstetter, Sepp Hochreiter", "title": "RUDDER: Return Decomposition for Delayed Rewards", "comments": "9 Pages plus appendix. For videos https://goo.gl/EQerZV", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose RUDDER, a novel reinforcement learning approach for delayed\nrewards in finite Markov decision processes (MDPs). In MDPs the Q-values are\nequal to the expected immediate reward plus the expected future rewards. The\nlatter are related to bias problems in temporal difference (TD) learning and to\nhigh variance problems in Monte Carlo (MC) learning. Both problems are even\nmore severe when rewards are delayed. RUDDER aims at making the expected future\nrewards zero, which simplifies Q-value estimation to computing the mean of the\nimmediate reward. We propose the following two new concepts to push the\nexpected future rewards toward zero. (i) Reward redistribution that leads to\nreturn-equivalent decision processes with the same optimal policies and, when\noptimal, zero expected future rewards. (ii) Return decomposition via\ncontribution analysis which transforms the reinforcement learning task into a\nregression task at which deep learning excels. On artificial tasks with delayed\nrewards, RUDDER is significantly faster than MC and exponentially faster than\nMonte Carlo Tree Search (MCTS), TD({\\lambda}), and reward shaping approaches.\nAt Atari games, RUDDER on top of a Proximal Policy Optimization (PPO) baseline\nimproves the scores, which is most prominent at games with delayed rewards.\nSource code is available at \\url{https://github.com/ml-jku/rudder} and\ndemonstration videos at \\url{https://goo.gl/EQerZV}.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 17:34:07 GMT"}, {"version": "v2", "created": "Fri, 25 Jan 2019 13:45:22 GMT"}, {"version": "v3", "created": "Tue, 10 Sep 2019 16:27:52 GMT"}], "update_date": "2019-09-11", "authors_parsed": [["Arjona-Medina", "Jose A.", ""], ["Gillhofer", "Michael", ""], ["Widrich", "Michael", ""], ["Unterthiner", "Thomas", ""], ["Brandstetter", "Johannes", ""], ["Hochreiter", "Sepp", ""]]}, {"id": "1806.07912", "submitter": "Yanqi Zhou", "authors": "Yanqi Zhou, Siavash Ebrahimi, Sercan \\\"O. Ar{\\i}k, Haonan Yu, Hairong\n  Liu, Greg Diamos", "title": "Resource-Efficient Neural Architect", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Architecture Search (NAS) is a laborious process. Prior work on\nautomated NAS targets mainly on improving accuracy, but lacks consideration of\ncomputational resource use. We propose the Resource-Efficient Neural Architect\n(RENA), an efficient resource-constrained NAS using reinforcement learning with\nnetwork embedding. RENA uses a policy network to process the network embeddings\nto generate new configurations. We demonstrate RENA on image recognition and\nkeyword spotting (KWS) problems. RENA can find novel architectures that achieve\nhigh performance even with tight resource constraints. For CIFAR10, it achieves\n2.95% test error when compute intensity is greater than 100 FLOPs/byte, and\n3.87% test error when model size is less than 3M parameters. For Google Speech\nCommands Dataset, RENA achieves the state-of-the-art accuracy without resource\nconstraints, and it outperforms the optimized architectures with tight resource\nconstraints.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jun 2018 20:41:32 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Zhou", "Yanqi", ""], ["Ebrahimi", "Siavash", ""], ["Ar\u0131k", "Sercan \u00d6.", ""], ["Yu", "Haonan", ""], ["Liu", "Hairong", ""], ["Diamos", "Greg", ""]]}, {"id": "1806.07917", "submitter": "Jakub Sygnowski", "authors": "Chrisantha Thomas Fernando, Jakub Sygnowski, Simon Osindero, Jane\n  Wang, Tom Schaul, Denis Teplyashin, Pablo Sprechmann, Alexander Pritzel,\n  Andrei A. Rusu", "title": "Meta-Learning by the Baldwin Effect", "comments": null, "journal-ref": null, "doi": "10.1145/3205651.3205763", "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scope of the Baldwin effect was recently called into question by two\npapers that closely examined the seminal work of Hinton and Nowlan. To this\ndate there has been no demonstration of its necessity in empirically\nchallenging tasks. Here we show that the Baldwin effect is capable of evolving\nfew-shot supervised and reinforcement learning mechanisms, by shaping the\nhyperparameters and the initial parameters of deep learning algorithms.\nFurthermore it can genetically accommodate strong learning biases on the same\nset of problems as a recent machine learning algorithm called MAML \"Model\nAgnostic Meta-Learning\" which uses second-order gradients instead of evolution\nto learn a set of reference parameters (initial weights) that can allow rapid\nadaptation to tasks sampled from a distribution. Whilst in simple cases MAML is\nmore data efficient than the Baldwin effect, the Baldwin effect is more general\nin that it does not require gradients to be backpropagated to the reference\nparameters or hyperparameters, and permits effectively any number of gradient\nupdates in the inner loop. The Baldwin effect learns strong learning dependent\nbiases, rather than purely genetically accommodating fixed behaviours in a\nlearning independent manner.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jun 2018 08:39:03 GMT"}, {"version": "v2", "created": "Fri, 22 Jun 2018 09:55:17 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Fernando", "Chrisantha Thomas", ""], ["Sygnowski", "Jakub", ""], ["Osindero", "Simon", ""], ["Wang", "Jane", ""], ["Schaul", "Tom", ""], ["Teplyashin", "Denis", ""], ["Sprechmann", "Pablo", ""], ["Pritzel", "Alexander", ""], ["Rusu", "Andrei A.", ""]]}, {"id": "1806.07937", "submitter": "Amy Zhang", "authors": "Amy Zhang, Nicolas Ballas, Joelle Pineau", "title": "A Dissection of Overfitting and Generalization in Continuous\n  Reinforcement Learning", "comments": "20 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The risks and perils of overfitting in machine learning are well known.\nHowever most of the treatment of this, including diagnostic tools and remedies,\nwas developed for the supervised learning case. In this work, we aim to offer\nnew perspectives on the characterization and prevention of overfitting in deep\nReinforcement Learning (RL) methods, with a particular focus on continuous\ndomains. We examine several aspects, such as how to define and diagnose\noverfitting in MDPs, and how to reduce risks by injecting sufficient training\ndiversity. This work complements recent findings on the brittleness of deep RL\nmethods and offers practical observations for RL researchers and practitioners.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 19:27:59 GMT"}, {"version": "v2", "created": "Mon, 25 Jun 2018 17:09:04 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Zhang", "Amy", ""], ["Ballas", "Nicolas", ""], ["Pineau", "Joelle", ""]]}, {"id": "1806.07955", "submitter": "Xinyi Wang", "authors": "Xinyi Wang, Salvador Aguinaga, Tim Weninger, David Chiang", "title": "Growing Better Graphs With Latent-Variable Probabilistic Graph Grammars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work in graph models has found that probabilistic hyperedge\nreplacement grammars (HRGs) can be extracted from graphs and used to generate\nnew random graphs with graph properties and substructures close to the\noriginal. In this paper, we show how to add latent variables to the model,\ntrained using Expectation-Maximization, to generate still better graphs, that\nis, ones that generalize better to the test data. We evaluate the new method by\nseparating training and test graphs, building the model on the former and\nmeasuring the likelihood of the latter, as a more stringent test of how well\nthe model can generalize to new graphs. On this metric, we find that our\nlatent-variable HRGs consistently outperform several existing graph models and\nprovide interesting insights into the building blocks of real world networks.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 11:36:43 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Wang", "Xinyi", ""], ["Aguinaga", "Salvador", ""], ["Weninger", "Tim", ""], ["Chiang", "David", ""]]}, {"id": "1806.08047", "submitter": "Damian Mrowca", "authors": "Damian Mrowca, Chengxu Zhuang, Elias Wang, Nick Haber, Li Fei-Fei,\n  Joshua B. Tenenbaum, Daniel L. K. Yamins", "title": "Flexible Neural Representation for Physics Prediction", "comments": "23 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans have a remarkable capacity to understand the physical dynamics of\nobjects in their environment, flexibly capturing complex structures and\ninteractions at multiple levels of detail. Inspired by this ability, we propose\na hierarchical particle-based object representation that covers a wide variety\nof types of three-dimensional objects, including both arbitrary rigid\ngeometrical shapes and deformable materials. We then describe the Hierarchical\nRelation Network (HRN), an end-to-end differentiable neural network based on\nhierarchical graph convolution, that learns to predict physical dynamics in\nthis representation. Compared to other neural network baselines, the HRN\naccurately handles complex collisions and nonrigid deformations, generating\nplausible dynamics predictions at long time scales in novel settings, and\nscaling to large scene configurations. These results demonstrate an\narchitecture with the potential to form the basis of next-generation physics\npredictors for use in computer vision, robotics, and quantitative cognitive\nscience.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 02:19:50 GMT"}, {"version": "v2", "created": "Sat, 27 Oct 2018 05:28:48 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Mrowca", "Damian", ""], ["Zhuang", "Chengxu", ""], ["Wang", "Elias", ""], ["Haber", "Nick", ""], ["Fei-Fei", "Li", ""], ["Tenenbaum", "Joshua B.", ""], ["Yamins", "Daniel L. K.", ""]]}, {"id": "1806.08055", "submitter": "Prashan Mathugama Babun Appuhamilage", "authors": "Prashan Madumal, Tim Miller, Frank Vetere, Liz Sonenberg", "title": "Towards a Grounded Dialog Model for Explainable Artificial Intelligence", "comments": "15 pages, First international workshop on socio-cognitive systems at\n  Federated AI Meeting (FAIM) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To generate trust with their users, Explainable Artificial Intelligence (XAI)\nsystems need to include an explanation model that can communicate the internal\ndecisions, behaviours and actions to the interacting humans. Successful\nexplanation involves both cognitive and social processes. In this paper we\nfocus on the challenge of meaningful interaction between an explainer and an\nexplainee and investigate the structural aspects of an explanation in order to\npropose a human explanation dialog model. We follow a bottom-up approach to\nderive the model by analysing transcripts of 398 different explanation dialog\ntypes. We use grounded theory to code and identify key components of which an\nexplanation dialog consists. We carry out further analysis to identify the\nrelationships between components and sequences and cycles that occur in a\ndialog. We present a generalized state model obtained by the analysis and\ncompare it with an existing conceptual dialog model of explanation.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 03:22:54 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Madumal", "Prashan", ""], ["Miller", "Tim", ""], ["Vetere", "Frank", ""], ["Sonenberg", "Liz", ""]]}, {"id": "1806.08065", "submitter": "Devendra Singh Chaplot", "authors": "Devendra Singh Chaplot, Christopher MacLellan, Ruslan Salakhutdinov,\n  Kenneth Koedinger", "title": "Learning Cognitive Models using Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A cognitive model of human learning provides information about skills a\nlearner must acquire to perform accurately in a task domain. Cognitive models\nof learning are not only of scientific interest, but are also valuable in\nadaptive online tutoring systems. A more accurate model yields more effective\ntutoring through better instructional decisions. Prior methods of automated\ncognitive model discovery have typically focused on well-structured domains,\nrelied on student performance data or involved substantial human knowledge\nengineering. In this paper, we propose Cognitive Representation Learner\n(CogRL), a novel framework to learn accurate cognitive models in ill-structured\ndomains with no data and little to no human knowledge engineering. Our\ncontribution is two-fold: firstly, we show that representations learnt using\nCogRL can be used for accurate automatic cognitive model discovery without\nusing any student performance data in several ill-structured domains: Rumble\nBlocks, Chinese Character, and Article Selection. This is especially effective\nand useful in domains where an accurate human-authored cognitive model is\nunavailable or authoring a cognitive model is difficult. Secondly, for domains\nwhere a cognitive model is available, we show that representations learned\nthrough CogRL can be used to get accurate estimates of skill difficulty and\nlearning rate parameters without using any student performance data. These\nestimates are shown to highly correlate with estimates using student\nperformance data on an Article Selection dataset.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 04:43:35 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Chaplot", "Devendra Singh", ""], ["MacLellan", "Christopher", ""], ["Salakhutdinov", "Ruslan", ""], ["Koedinger", "Kenneth", ""]]}, {"id": "1806.08083", "submitter": "Martin  Biehl", "authors": "Martin Biehl (1), Christian Guckelsberger (2), Christoph Salge (3 and\n  4), Sim\\'on C. Smith (4 and 5), Daniel Polani (4) ((1) Araya Inc., Tokyo,\n  Japan, (2) Computational Creativity Group, Department of Computing,\n  Goldsmiths, University of London, London, UK, (3) Game Innovation Lab,\n  Department of Computer Science and Engineering, New York University, New York\n  City, NY, USA, (4) Sepia Lab, Adaptive Systems Research Group, Department of\n  Computer Science, University of Hertfordshire, Hatfield, UK, (5) Institute of\n  Perception, Action and Behaviour, School of Informatics, The University of\n  Edinburgh, UK)", "title": "Expanding the Active Inference Landscape: More Intrinsic Motivations in\n  the Perception-Action Loop", "comments": "53 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active inference is an ambitious theory that treats perception, inference and\naction selection of autonomous agents under the heading of a single principle.\nIt suggests biologically plausible explanations for many cognitive phenomena,\nincluding consciousness. In active inference, action selection is driven by an\nobjective function that evaluates possible future actions with respect to\ncurrent, inferred beliefs about the world. Active inference at its core is\nindependent from extrinsic rewards, resulting in a high level of robustness\nacross e.g.\\ different environments or agent morphologies. In the literature,\nparadigms that share this independence have been summarised under the notion of\nintrinsic motivations. In general and in contrast to active inference, these\nmodels of motivation come without a commitment to particular inference and\naction selection mechanisms. In this article, we study if the inference and\naction selection machinery of active inference can also be used by alternatives\nto the originally included intrinsic motivation. The perception-action loop\nexplicitly relates inference and action selection to the environment and agent\nmemory, and is consequently used as foundation for our analysis. We reconstruct\nthe active inference approach, locate the original formulation within, and show\nhow alternative intrinsic motivations can be used while keeping many of the\noriginal features intact. Furthermore, we illustrate the connection to\nuniversal reinforcement learning by means of our formalism. Active inference\nresearch may profit from comparisons of the dynamics induced by alternative\nintrinsic motivations. Research on intrinsic motivations may profit from an\nadditional way to implement intrinsically motivated agents that also share the\nbiological plausibility of active inference.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 06:53:45 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Biehl", "Martin", "", "3 and\n  4"], ["Guckelsberger", "Christian", "", "3 and\n  4"], ["Salge", "Christoph", "", "3 and\n  4"], ["Smith", "Sim\u00f3n C.", "", "4 and 5"], ["Polani", "Daniel", ""]]}, {"id": "1806.08122", "submitter": "Wenxia Guo", "authors": "Yufei Ye, Xiaoqin Ren, Jin Wang, Lingxiao Xu, Wenxia Guo, Wenqiang\n  Huang and Wenhong Tian", "title": "A New Approach for Resource Scheduling with Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid development of deep learning, deep reinforcement learning\n(DRL) began to appear in the field of resource scheduling in recent years.\nBased on the previous research on DRL in the literature, we introduce online\nresource scheduling algorithm DeepRM2 and the offline resource scheduling\nalgorithm DeepRM_Off. Compared with the state-of-the-art DRL algorithm DeepRM\nand heuristic algorithms, our proposed algorithms have faster convergence speed\nand better scheduling efficiency with regarding to average slowdown time, job\ncompletion time and rewards.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 09:04:26 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Ye", "Yufei", ""], ["Ren", "Xiaoqin", ""], ["Wang", "Jin", ""], ["Xu", "Lingxiao", ""], ["Guo", "Wenxia", ""], ["Huang", "Wenqiang", ""], ["Tian", "Wenhong", ""]]}, {"id": "1806.08156", "submitter": "Jose M. Pe\\~na", "authors": "Jose M. Pe\\~na", "title": "Identifiability of Gaussian Structural Equation Models with Dependent\n  Errors Having Equal Variances", "comments": "7th Causal Inference Workshop at UAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we prove that some Gaussian structural equation models with\ndependent errors having equal variances are identifiable from their\ncorresponding Gaussian distributions. Specifically, we prove identifiability\nfor the Gaussian structural equation models that can be represented as\nAndersson-Madigan-Perlman chain graphs (Andersson et al., 2001). These chain\ngraphs were originally developed to represent independence models. However,\nthey are also suitable for representing causal models with additive noise\n(Pe\\~na, 2016. Our result implies then that these causal models can be\nidentified from observational data alone. Our result generalizes the result by\nPeters and B\\\"uhlmann (2014), who considered independent errors having equal\nvariances. The suitability of the equal error variances assumption should be\nassessed on a per domain basis.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 10:23:23 GMT"}, {"version": "v2", "created": "Mon, 25 Jun 2018 07:43:09 GMT"}, {"version": "v3", "created": "Sat, 4 Aug 2018 07:35:23 GMT"}, {"version": "v4", "created": "Tue, 28 Aug 2018 20:33:50 GMT"}], "update_date": "2018-08-30", "authors_parsed": [["Pe\u00f1a", "Jose M.", ""]]}, {"id": "1806.08202", "submitter": "Hussein AL-Natsheh", "authors": "Hussein T. Al-Natsheh, Lucie Martinet, Fabrice Muhlenbach, Fabien\n  Rico, Djamel A. Zighed", "title": "Metadata Enrichment of Multi-Disciplinary Digital Library: A\n  Semantic-based Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the scientific digital libraries, some papers from different research\ncommunities can be described by community-dependent keywords even if they share\na semantically similar topic. Articles that are not tagged with enough keyword\nvariations are poorly indexed in any information retrieval system which limits\npotentially fruitful exchanges between scientific disciplines. In this paper,\nwe introduce a novel experimentally designed pipeline for multi-label\nsemantic-based tagging developed for open-access metadata digital libraries.\nThe approach starts by learning from a standard scientific categorization and a\nsample of topic tagged articles to find semantically relevant articles and\nenrich its metadata accordingly. Our proposed pipeline aims to enable\nresearchers reaching articles from various disciplines that tend to use\ndifferent terminologies. It allows retrieving semantically relevant articles\ngiven a limited known variation of search terms. In addition to achieving an\naccuracy that is higher than an expanded query based method using a topic\nsynonym set extracted from a semantic network, our experiments also show a\nhigher computational scalability versus other comparable techniques. We created\na new benchmark extracted from the open-access metadata of a scientific digital\nlibrary and published it along with the experiment code to allow further\nresearch in the topic.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 12:35:23 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Al-Natsheh", "Hussein T.", ""], ["Martinet", "Lucie", ""], ["Muhlenbach", "Fabrice", ""], ["Rico", "Fabien", ""], ["Zighed", "Djamel A.", ""]]}, {"id": "1806.08245", "submitter": "Ching Tarn", "authors": "Ching Tarn, Yinan Zhang, Ye Feng", "title": "Reductive Clustering: An Efficient Linear-time Graph-based Divisive\n  Cluster Analysis Approach", "comments": "http://res.ctarn.io/reductive-clustering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.DB cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient linear-time graph-based divisive cluster analysis\napproach called Reductive Clustering. The approach tries to reveal the\nhierarchical structural information through reducing the graph into a more\nconcise one repeatedly. With the reductions, the original graph can be divided\ninto subgraphs recursively, and a lite informative dendrogram is constructed\nbased on the divisions. The reduction consists of three steps: selection,\nconnection, and partition. First a subset of vertices of the graph are selected\nas representatives to build a concise graph. The representatives are\nre-connected to maintain a consistent structure with the previous graph. If\npossible, the concise graph is divided into subgraphs, and each subgraph is\nfurther reduced recursively until the termination condition is met. We discuss\nthe approach, along with several selection and connection methods, in detail\nboth theoretically and experimentally in this paper. Our implementations run in\nlinear time and achieve outstanding performance on various types of datasets.\nExperimental results show that they outperform state-of-the-art clustering\nalgorithms with significantly less computing resource requirements.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 13:44:17 GMT"}, {"version": "v2", "created": "Thu, 16 May 2019 06:40:56 GMT"}, {"version": "v3", "created": "Fri, 25 Sep 2020 12:20:22 GMT"}], "update_date": "2020-09-28", "authors_parsed": [["Tarn", "Ching", ""], ["Zhang", "Yinan", ""], ["Feng", "Ye", ""]]}, {"id": "1806.08247", "submitter": "Eric Verbeek", "authors": "H.M.W. Verbeek and R. Medeiros de Carvalho", "title": "Log Skeletons: A Classification Approach to Process Discovery", "comments": "16 pages with 9 figures, followed by an appendix of 14 pages with 17\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  To test the effectiveness of process discovery algorithms, a Process\nDiscovery Contest (PDC) has been set up. This PDC uses a classification\napproach to measure this effectiveness: The better the discovered model can\nclassify whether or not a new trace conforms to the event log, the better the\ndiscovery algorithm is supposed to be. Unfortunately, even the state-of-the-art\nfully-automated discovery algorithms score poorly on this classification. Even\nthe best of these algorithms, the Inductive Miner, scored only 147 correct\nclassified traces out of 200 traces on the PDC of 2017. This paper introduces\nthe rule-based log skeleton model, which is closely related to the Declare\nconstraint model, together with a way to classify traces using this model. This\nclassification using log skeletons is shown to score better on the PDC of 2017\nthan state-of-the-art discovery algorithms: 194 out of 200. As a result, one\ncan argue that the fully-automated algorithm to construct (or: discover) a log\nskeleton from an event log outperforms existing state-of-the-art\nfully-automated discovery algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 13:51:56 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Verbeek", "H. M. W.", ""], ["de Carvalho", "R. Medeiros", ""]]}, {"id": "1806.08340", "submitter": "Kiri Wagstaff", "authors": "Kiri L. Wagstaff and Jake Lee", "title": "Interpretable Discovery in Large Image Data Sets", "comments": "Presented at the 2018 ICML Workshop on Human Interpretability in\n  Machine Learning (WHI 2018), Stockholm, Sweden", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated detection of new, interesting, unusual, or anomalous images within\nlarge data sets has great value for applications from surveillance (e.g.,\nairport security) to science (observations that don't fit a given theory can\nlead to new discoveries). Many image data analysis systems are turning to\nconvolutional neural networks (CNNs) to represent image content due to their\nsuccess in achieving high classification accuracy rates. However, CNN\nrepresentations are notoriously difficult for humans to interpret. We describe\na new strategy that combines novelty detection with CNN image features to\nachieve rapid discovery with interpretable explanations of novel image content.\nWe applied this technique to familiar images from ImageNet as well as to a\nscientific image collection from planetary science.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 17:30:26 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Wagstaff", "Kiri L.", ""], ["Lee", "Jake", ""]]}, {"id": "1806.08354", "submitter": "Deepak Pathak", "authors": "Deepak Pathak, Yide Shentu, Dian Chen, Pulkit Agrawal, Trevor Darrell,\n  Sergey Levine, Jitendra Malik", "title": "Learning Instance Segmentation by Interaction", "comments": "Website at https://pathak22.github.io/seg-by-interaction/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for building an active agent that learns to segment\nits visual observations into individual objects by interacting with its\nenvironment in a completely self-supervised manner. The agent uses its current\nsegmentation model to infer pixels that constitute objects and refines the\nsegmentation model by interacting with these pixels. The model learned from\nover 50K interactions generalizes to novel objects and backgrounds. To deal\nwith noisy training signal for segmenting objects obtained by self-supervised\ninteractions, we propose robust set loss. A dataset of robot's interactions\nalong-with a few human labeled examples is provided as a benchmark for future\nresearch. We test the utility of the learned segmentation model by providing\nresults on a downstream vision-based control task of rearranging multiple\nobjects into target configurations from visual inputs alone. Videos, code, and\nrobotic interaction dataset are available at\nhttps://pathak22.github.io/seg-by-interaction/\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 17:59:09 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Pathak", "Deepak", ""], ["Shentu", "Yide", ""], ["Chen", "Dian", ""], ["Agrawal", "Pulkit", ""], ["Darrell", "Trevor", ""], ["Levine", "Sergey", ""], ["Malik", "Jitendra", ""]]}, {"id": "1806.08463", "submitter": "Alexander Wong", "authors": "Rene Bidart and Alexander Wong", "title": "TriResNet: A Deep Triple-stream Residual Network for Histopathology\n  Grading", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While microscopic analysis of histopathological slides is generally\nconsidered as the gold standard method for performing cancer diagnosis and\ngrading, the current method for analysis is extremely time consuming and labour\nintensive as it requires pathologists to visually inspect tissue samples in a\ndetailed fashion for the presence of cancer. As such, there has been\nsignificant recent interest in computer aided diagnosis systems for analysing\nhistopathological slides for cancer grading to aid pathologists to perform\ncancer diagnosis and grading in a more efficient, accurate, and consistent\nmanner. In this work, we investigate and explore a deep triple-stream residual\nnetwork (TriResNet) architecture for the purpose of tile-level histopathology\ngrading, which is the critical first step to computer-aided whole-slide\nhistopathology grading. In particular, the design mentality behind the proposed\nTriResNet network architecture is to facilitate for the learning of a more\ndiverse set of quantitative features to better characterize the complex tissue\ncharacteristics found in histopathology samples. Experimental results on two\nwidely-used computer-aided histopathology benchmark datasets (CAMELYON16\ndataset and Invasive Ductal Carcinoma (IDC) dataset) demonstrated that the\nproposed TriResNet network architecture was able to achieve noticeably improved\naccuracies when compared with two other state-of-the-art deep convolutional\nneural network architectures. Based on these promising results, the hope is\nthat the proposed TriResNet network architecture could become a useful tool to\naiding pathologists increase the consistency, speed, and accuracy of the\nhistopathology grading process.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 01:18:14 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Bidart", "Rene", ""], ["Wong", "Alexander", ""]]}, {"id": "1806.08465", "submitter": "Kunhong Liu Dr", "authors": "Kaijie Feng, Kunhong Liu, Beizhan Wang", "title": "A Novel ECOC Algorithm with Centroid Distance Based Soft Coding Scheme", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In ECOC framework, the ternary coding strategy is widely deployed in coding\nprocess. It relabels classes with {\"-1,0,1\" }, where -1/1 means to assign the\ncorresponding classes to the negative/positive group, and label 0 leads to\nignore the corresponding classes in the training process. However, the\napplication of hard labels may lose some information about the tendency of\nclass distributions. Instead, we propose a Centroid distance-based Soft coding\nscheme to indicate such tendency, named as CSECOC. In our algorithm, Sequential\nForward Floating Selection (SFFS) is applied to search an optimal class\nassignment by minimizing the ratio of intra-group and inter-group distance. In\nthis way, a hard coding matrix is generated initially. Then we propose a\nmeasure, named as coverage, to describe the probability of a sample in a class\nfalling to a correct group. The coverage of a class a group replace the\ncorresponding hard element, so as to form a soft coding matrix. Compared with\nthe hard ones, such soft elements can reflect the tendency of a class belonging\nto positive or negative group. Instead of classifiers, regressors are used as\nbase learners in this algorithm. To the best of our knowledge, it is the first\ntime that soft coding scheme has been proposed. The results on five UCI\ndatasets show that compared with some state-of-art ECOC algorithms, our\nalgorithm can produce comparable or better classification accuracy with small\nscale ensembles.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 01:35:39 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Feng", "Kaijie", ""], ["Liu", "Kunhong", ""], ["Wang", "Beizhan", ""]]}, {"id": "1806.08479", "submitter": "Xinlei Pan", "authors": "Xinlei Pan, Eshed Ohn-Bar, Nicholas Rhinehart, Yan Xu, Yilin Shen,\n  Kris M. Kitani", "title": "Human-Interactive Subgoal Supervision for Efficient Inverse\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans are able to understand and perform complex tasks by strategically\nstructuring the tasks into incremental steps or subgoals. For a robot\nattempting to learn to perform a sequential task with critical subgoal states,\nsuch states can provide a natural opportunity for interaction with a human\nexpert. This paper analyzes the benefit of incorporating a notion of subgoals\ninto Inverse Reinforcement Learning (IRL) with a Human-In-The-Loop (HITL)\nframework. The learning process is interactive, with a human expert first\nproviding input in the form of full demonstrations along with some subgoal\nstates. These subgoal states define a set of subtasks for the learning agent to\ncomplete in order to achieve the final goal. The learning agent queries for\npartial demonstrations corresponding to each subtask as needed when the agent\nstruggles with the subtask. The proposed Human Interactive IRL (HI-IRL)\nframework is evaluated on several discrete path-planning tasks. We demonstrate\nthat subgoal-based interactive structuring of the learning task results in\nsignificantly more efficient learning, requiring only a fraction of the\ndemonstration data needed for learning the underlying reward function with the\nbaseline IRL model.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 03:24:00 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Pan", "Xinlei", ""], ["Ohn-Bar", "Eshed", ""], ["Rhinehart", "Nicholas", ""], ["Xu", "Yan", ""], ["Shen", "Yilin", ""], ["Kitani", "Kris M.", ""]]}, {"id": "1806.08544", "submitter": "Simon Lucas", "authors": "Simon M. Lucas", "title": "Game AI Research with Fast Planet Wars Variants", "comments": "To appear in Proceedings of IEEE Conference on Computational and\n  Games, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a new implementation of Planet Wars, designed from the\noutset for Game AI research. The skill-depth of the game makes it a challenge\nfor game-playing agents, and the speed of more than 1 million game ticks per\nsecond enables rapid experimentation and prototyping. The parameterised nature\nof the game together with an interchangeable actuator model make it well suited\nto automated game tuning. The game is designed to be fun to play for humans,\nand is directly playable by General Video Game AI agents.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 08:18:53 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Lucas", "Simon M.", ""]]}, {"id": "1806.08554", "submitter": "Bei Chen", "authors": "Yihong Chen, Bei Chen, Xuguang Duan, Jian-Guang Lou, Yue Wang, Wenwu\n  Zhu, Yong Cao", "title": "Learning-to-Ask: Knowledge Acquisition via 20 Questions", "comments": "Accepted by KDD 2018", "journal-ref": null, "doi": "10.1145/3219819.3220047", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Almost all the knowledge empowered applications rely upon accurate knowledge,\nwhich has to be either collected manually with high cost, or extracted\nautomatically with unignorable errors. In this paper, we study 20 Questions, an\nonline interactive game where each question-response pair corresponds to a fact\nof the target entity, to acquire highly accurate knowledge effectively with\nnearly zero labor cost. Knowledge acquisition via 20 Questions predominantly\npresents two challenges to the intelligent agent playing games with human\nplayers. The first one is to seek enough information and identify the target\nentity with as few questions as possible, while the second one is to leverage\nthe remaining questioning opportunities to acquire valuable knowledge\neffectively, both of which count on good questioning strategies. To address\nthese challenges, we propose the Learning-to-Ask (LA) framework, within which\nthe agent learns smart questioning strategies for information seeking and\nknowledge acquisition by means of deep reinforcement learning and generalized\nmatrix factorization respectively. In addition, a Bayesian approach to\nrepresent knowledge is adopted to ensure robustness to noisy user responses.\nSimulating experiments on real data show that LA is able to equip the agent\nwith effective questioning strategies, which result in high winning rates and\nrapid knowledge acquisition. Moreover, the questioning strategies for\ninformation seeking and knowledge acquisition boost the performance of each\nother, allowing the agent to start with a relatively small knowledge set and\nquickly improve its knowledge base in the absence of constant human\nsupervision.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 08:48:49 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Chen", "Yihong", ""], ["Chen", "Bei", ""], ["Duan", "Xuguang", ""], ["Lou", "Jian-Guang", ""], ["Wang", "Yue", ""], ["Zhu", "Wenwu", ""], ["Cao", "Yong", ""]]}, {"id": "1806.08561", "submitter": "Giacomo Spigler", "authors": "Giacomo Spigler", "title": "The Temporal Singularity: time-accelerated simulated civilizations and\n  their implications", "comments": "To appear in the conference proceedings of the AGI-18 conference\n  (published in the Springer's Lecture Notes in AI series)", "journal-ref": null, "doi": "10.1007/978-3-319-97676-1_20", "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Provided significant future progress in artificial intelligence and\ncomputing, it may ultimately be possible to create multiple Artificial General\nIntelligences (AGIs), and possibly entire societies living within simulated\nenvironments. In that case, it should be possible to improve the problem\nsolving capabilities of the system by increasing the speed of the simulation.\nIf a minimal simulation with sufficient capabilities is created, it might\nmanage to increase its own speed by accelerating progress in science and\ntechnology, in a way similar to the Technological Singularity. This may\nultimately lead to large simulated civilizations unfolding at extreme temporal\nspeedups, achieving what from the outside would look like a Temporal\nSingularity. Here we discuss the feasibility of the minimal simulation and the\npotential advantages, dangers, and connection to the Fermi paradox of the\nTemporal Singularity. The medium-term importance of the topic derives from the\namount of computational power required to start the process, which could be\navailable within the next decades, making the Temporal Singularity\ntheoretically possible before the end of the century.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 09:02:29 GMT"}], "update_date": "2020-02-11", "authors_parsed": [["Spigler", "Giacomo", ""]]}, {"id": "1806.08568", "submitter": "Vincenzo Lomonaco", "authors": "Davide Maltoni and Vincenzo Lomonaco", "title": "Continuous Learning in Single-Incremental-Task Scenarios", "comments": "26 pages, 13 figures; v3: major revision (e.g. added Sec. 4.4),\n  several typos and minor mistakes corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It was recently shown that architectural, regularization and rehearsal\nstrategies can be used to train deep models sequentially on a number of\ndisjoint tasks without forgetting previously acquired knowledge. However, these\nstrategies are still unsatisfactory if the tasks are not disjoint but\nconstitute a single incremental task (e.g., class-incremental learning). In\nthis paper we point out the differences between multi-task and\nsingle-incremental-task scenarios and show that well-known approaches such as\nLWF, EWC and SI are not ideal for incremental task scenarios. A new approach,\ndenoted as AR1, combining architectural and regularization strategies is then\nspecifically proposed. AR1 overhead (in term of memory and computation) is very\nsmall thus making it suitable for online learning. When tested on CORe50 and\niCIFAR-100, AR1 outperformed existing regularization strategies by a good\nmargin.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 09:22:42 GMT"}, {"version": "v2", "created": "Tue, 28 Aug 2018 11:13:40 GMT"}, {"version": "v3", "created": "Tue, 22 Jan 2019 21:49:25 GMT"}], "update_date": "2019-01-24", "authors_parsed": [["Maltoni", "Davide", ""], ["Lomonaco", "Vincenzo", ""]]}, {"id": "1806.08616", "submitter": "Alexandros Kouris", "authors": "Stylianos I. Venieris, Alexandros Kouris and Christos-Savvas Bouganis", "title": "Deploying Deep Neural Networks in the Embedded Space", "comments": "Accepted at MobiSys18: 2nd International Workshop on Embedded and\n  Mobile Deep Learning (EMDL) 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Deep Neural Networks (DNNs) have emerged as the dominant model\nacross various AI applications. In the era of IoT and mobile systems, the\nefficient deployment of DNNs on embedded platforms is vital to enable the\ndevelopment of intelligent applications. This paper summarises our recent work\non the optimised mapping of DNNs on embedded settings. By covering such diverse\ntopics as DNN-to-accelerator toolflows, high-throughput cascaded classifiers\nand domain-specific model design, the presented set of works aim to enable the\ndeployment of sophisticated deep learning models on cutting-edge mobile and\nembedded systems.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 12:01:11 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Venieris", "Stylianos I.", ""], ["Kouris", "Alexandros", ""], ["Bouganis", "Christos-Savvas", ""]]}, {"id": "1806.08663", "submitter": "Jinfeng Zhang", "authors": "Albert Steppi, Jinchan Qu, Minjing Tao, Tingting Zhao, Xiaodong Pang,\n  Jinfeng Zhang", "title": "Simulation Study on a New Peer Review Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing volume of scientific publications and grant proposals has\ngenerated an unprecedentedly high workload to scientific communities.\nConsequently, review quality has been decreasing and review outcomes have\nbecome less correlated with the real merits of the papers and proposals. A\nnovel distributed peer review (DPR) approach has recently been proposed to\naddress these issues. The new approach assigns principal investigators (PIs)\nwho submitted proposals (or papers) to the same program as reviewers. Each PI\nreviews and ranks a small number (such as seven) of other PIs' proposals. The\nindividual rankings are then used to estimate a global ranking of all proposals\nusing the Modified Borda Count (MBC). In this study, we perform simulation\nstudies to investigate several parameters important for the decision making\nwhen adopting this new approach. We also propose a new method called\nConcordance Index-based Global Ranking (CIGR) to estimate global ranking from\nindividual rankings. An efficient simulated annealing algorithm is designed to\nsearch the optimal Concordance Index (CI). Moreover, we design a new balanced\nreview assignment procedure, which can result in significantly better\nperformance for both MBC and CIGR methods. We found that CIGR performs better\nthan MBC when the review quality is relatively high. As review quality and\nreview difficulty are tightly correlated, we constructed a boundary in the\nspace of review quality vs review difficulty that separates the CIGR-superior\nand MBC-superior regions. Finally, we propose a multi-stage DPR strategy based\non CIGR, which has the potential to substantially improve the overall review\nperformance while reducing the review workload.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jun 2018 23:34:59 GMT"}, {"version": "v2", "created": "Tue, 11 Dec 2018 18:06:53 GMT"}, {"version": "v3", "created": "Mon, 7 Oct 2019 18:06:39 GMT"}], "update_date": "2019-10-09", "authors_parsed": [["Steppi", "Albert", ""], ["Qu", "Jinchan", ""], ["Tao", "Minjing", ""], ["Zhao", "Tingting", ""], ["Pang", "Xiaodong", ""], ["Zhang", "Jinfeng", ""]]}, {"id": "1806.08686", "submitter": "Stefan Lattner", "authors": "Stefan Lattner, Maarten Grachten, Gerhard Widmer", "title": "A Predictive Model for Music Based on Learned Interval Representations", "comments": "Paper accepted at the 19th International Society for Music\n  Information Retrieval Conference, ISMIR 2018, Paris, France, September 23-27;\n  8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Connectionist sequence models (e.g., RNNs) applied to musical sequences\nsuffer from two known problems: First, they have strictly \"absolute pitch\nperception\". Therefore, they fail to generalize over musical concepts which are\ncommonly perceived in terms of relative distances between pitches (e.g.,\nmelodies, scale types, modes, cadences, or chord types). Second, they fall\nshort of capturing the concepts of repetition and musical form. In this paper\nwe introduce the recurrent gated autoencoder (RGAE), a recurrent neural network\nwhich learns and operates on interval representations of musical sequences. The\nrelative pitch modeling increases generalization and reduces sparsity in the\ninput data. Furthermore, it can learn sequences of copy-and-shift operations\n(i.e. chromatically transposed copies of musical fragments)---a promising\ncapability for learning musical repetition structure. We show that the RGAE\nimproves the state of the art for general connectionist sequence models in\nlearning to predict monophonic melodies, and that ensembles of relative and\nabsolute music processing models improve the results appreciably. Furthermore,\nwe show that the relative pitch processing of the RGAE naturally facilitates\nthe learning and the generation of sequences of copy-and-shift operations,\nwherefore the RGAE greatly outperforms a common absolute pitch recurrent neural\nnetwork on this task.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 14:17:04 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Lattner", "Stefan", ""], ["Grachten", "Maarten", ""], ["Widmer", "Gerhard", ""]]}, {"id": "1806.08694", "submitter": "Mostafa Dehghani", "authors": "Mostafa Dehghani and Jaap Kamps", "title": "Learning to Rank from Samples of Variable Quality", "comments": "Presented at The First International SIGIR2016 Workshop on Learning\n  From Limited Or Noisy Data For Information Retrieval. arXiv admin note:\n  substantial text overlap with arXiv:1711.02799", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep neural networks requires many training samples, but in\npractice, training labels are expensive to obtain and may be of varying\nquality, as some may be from trusted expert labelers while others might be from\nheuristics or other sources of weak supervision such as crowd-sourcing. This\ncreates a fundamental quality-versus quantity trade-off in the learning\nprocess. Do we learn from the small amount of high-quality data or the\npotentially large amount of weakly-labeled data? We argue that if the learner\ncould somehow know and take the label-quality into account when learning the\ndata representation, we could get the best of both worlds. To this end, we\nintroduce \"fidelity-weighted learning\" (FWL), a semi-supervised student-teacher\napproach for training deep neural networks using weakly-labeled data. FWL\nmodulates the parameter updates to a student network (trained on the task we\ncare about) on a per-sample basis according to the posterior confidence of its\nlabel-quality estimated by a teacher (who has access to the high-quality\nlabels). Both student and teacher are learned from the data. We evaluate FWL on\ndocument ranking where we outperform state-of-the-art alternative\nsemi-supervised methods.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 09:40:20 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["Dehghani", "Mostafa", ""], ["Kamps", "Jaap", ""]]}, {"id": "1806.08730", "submitter": "Nitish Shirish Keskar", "authors": "Bryan McCann and Nitish Shirish Keskar and Caiming Xiong and Richard\n  Socher", "title": "The Natural Language Decathlon: Multitask Learning as Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has improved performance on many natural language processing\n(NLP) tasks individually. However, general NLP models cannot emerge within a\nparadigm that focuses on the particularities of a single metric, dataset, and\ntask. We introduce the Natural Language Decathlon (decaNLP), a challenge that\nspans ten tasks: question answering, machine translation, summarization,\nnatural language inference, sentiment analysis, semantic role labeling,\nzero-shot relation extraction, goal-oriented dialogue, semantic parsing, and\ncommonsense pronoun resolution. We cast all tasks as question answering over a\ncontext. Furthermore, we present a new Multitask Question Answering Network\n(MQAN) jointly learns all tasks in decaNLP without any task-specific modules or\nparameters in the multitask setting. MQAN shows improvements in transfer\nlearning for machine translation and named entity recognition, domain\nadaptation for sentiment analysis and natural language inference, and zero-shot\ncapabilities for text classification. We demonstrate that the MQAN's\nmulti-pointer-generator decoder is key to this success and performance further\nimproves with an anti-curriculum training strategy. Though designed for\ndecaNLP, MQAN also achieves state of the art results on the WikiSQL semantic\nparsing task in the single-task setting. We also release code for procuring and\nprocessing data, training and evaluating models, and reproducing all\nexperiments for decaNLP.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 16:39:26 GMT"}], "update_date": "2018-06-25", "authors_parsed": [["McCann", "Bryan", ""], ["Keskar", "Nitish Shirish", ""], ["Xiong", "Caiming", ""], ["Socher", "Richard", ""]]}, {"id": "1806.08781", "submitter": "Johannes Bausch", "authors": "Johannes Bausch and Felix Leditzky", "title": "Quantum Codes from Neural Networks", "comments": "58 pages, 19 figures; source code in ancillary files. Significantly\n  extended results and findings", "journal-ref": "New Journal of Physics 22, 023005 (2020)", "doi": "10.1088/1367-2630/ab6cdd", "report-no": null, "categories": "quant-ph cond-mat.dis-nn cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the usefulness of applying neural networks as a variational state\nansatz for many-body quantum systems in the context of quantum\ninformation-processing tasks. In the neural network state ansatz, the complex\namplitude function of a quantum state is computed by a neural network. The\nresulting multipartite entanglement structure captured by this ansatz has\nproven rich enough to describe the ground states and unitary dynamics of\nvarious physical systems of interest. In the present paper, we initiate the\nstudy of neural network states in quantum information-processing tasks. We\ndemonstrate that neural network states are capable of efficiently representing\nquantum codes for quantum information transmission and quantum error\ncorrection, supplying further evidence for the usefulness of neural network\nstates to describe multipartite entanglement. In particular, we show the\nfollowing main results: a) Neural network states yield quantum codes with a\nhigh coherent information for two important quantum channels, the generalized\namplitude damping channel and the dephrasure channel. These codes outperform\nall other known codes for these channels, and cannot be found using a direct\nparametrization of the quantum state. b) For the depolarizing channel, the\nneural network state ansatz reliably finds the best known codes given by\nrepetition codes. c) Neural network states can be used to represent absolutely\nmaximally entangled states, a special type of quantum error-correcting codes.\nIn all three cases, the neural network state ansatz provides an efficient and\nversatile means as a variational parametrization of these highly entangled\nstates.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 17:55:24 GMT"}, {"version": "v2", "created": "Thu, 24 Oct 2019 17:41:30 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Bausch", "Johannes", ""], ["Leditzky", "Felix", ""]]}, {"id": "1806.08874", "submitter": "Eray Ozkural", "authors": "Eray \\\"Ozkural", "title": "The Foundations of Deep Learning with a Path Towards General\n  Intelligence", "comments": "Submitted to AGI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Like any field of empirical science, AI may be approached axiomatically. We\nformulate requirements for a general-purpose, human-level AI system in terms of\npostulates. We review the methodology of deep learning, examining the explicit\nand tacit assumptions in deep learning research. Deep Learning methodology\nseeks to overcome limitations in traditional machine learning research as it\ncombines facets of model richness, generality, and practical applicability. The\nmethodology so far has produced outstanding results due to a productive synergy\nof function approximation, under plausible assumptions of irreducibility and\nthe efficiency of back-propagation family of algorithms. We examine these\nwinning traits of deep learning, and also observe the various known failure\nmodes of deep learning. We conclude by giving recommendations on how to extend\ndeep learning methodology to cover the postulates of general-purpose AI\nincluding modularity, and cognitive architecture. We also relate deep learning\nto advances in theoretical neuroscience research.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 22:52:12 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["\u00d6zkural", "Eray", ""]]}, {"id": "1806.08894", "submitter": "Sajad Mousavi", "authors": "Seyed Sajad Mousavi, Michael Schukat, Enda Howley", "title": "Deep Reinforcement Learning: An Overview", "comments": "Proceedings of SAI Intelligent Systems Conference (IntelliSys) 2016", "journal-ref": null, "doi": "10.1007/978-3-319-56991-8_32", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, a specific machine learning method called deep learning has\ngained huge attraction, as it has obtained astonishing results in broad\napplications such as pattern recognition, speech recognition, computer vision,\nand natural language processing. Recent research has also been shown that deep\nlearning techniques can be combined with reinforcement learning methods to\nlearn useful representations for the problems with high dimensional raw data\ninput. This chapter reviews the recent advances in deep reinforcement learning\nwith a focus on the most used deep architectures such as autoencoders,\nconvolutional neural networks and recurrent neural networks which have\nsuccessfully been come together with the reinforcement learning framework.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jun 2018 02:18:26 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Mousavi", "Seyed Sajad", ""], ["Schukat", "Michael", ""], ["Howley", "Enda", ""]]}, {"id": "1806.08908", "submitter": "Eray Ozkural", "authors": "Eray \\\"Ozkural", "title": "Zeta Distribution and Transfer Learning Problem", "comments": "Submitted to AGI 2018, pre-print", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the relations between the zeta distribution and algorithmic\ninformation theory via a new model of the transfer learning problem. The\nprogram distribution is approximated by a zeta distribution with parameter near\n$1$. We model the training sequence as a stochastic process. We analyze the\nupper temporal bound for learning a training sequence and its entropy rates,\nassuming an oracle for the transfer learning problem. We argue from empirical\nevidence that power-law models are suitable for natural processes. Four\nsequence models are proposed. Random typing model is like no-free lunch where\ntransfer learning does not work. Zeta process independently samples programs\nfrom the zeta distribution. A model of common sub-programs inspired by genetics\nuses a database of sub-programs. An evolutionary zeta process samples mutations\nfrom Zeta distribution. The analysis of stochastic processes inspired by\nevolution suggest that AI may be feasible in nature, countering no-free lunch\nsort of arguments.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jun 2018 04:47:37 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["\u00d6zkural", "Eray", ""]]}, {"id": "1806.08915", "submitter": "Przemyslaw Biecek", "authors": "Przemyslaw Biecek", "title": "DALEX: explainers for complex predictive models", "comments": "12 pages", "journal-ref": "Journal of Machine Learning Research 19 (2018) 1-5", "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive modeling is invaded by elastic, yet complex methods such as neural\nnetworks or ensembles (model stacking, boosting or bagging). Such methods are\nusually described by a large number of parameters or hyper parameters - a price\nthat one needs to pay for elasticity. The very number of parameters makes\nmodels hard to understand. This paper describes a consistent collection of\nexplainers for predictive models, a.k.a. black boxes. Each explainer is a\ntechnique for exploration of a black box model. Presented approaches are\nmodel-agnostic, what means that they extract useful information from any\npredictive method despite its internal structure. Each explainer is linked with\na specific aspect of a model. Some are useful in decomposing predictions, some\nserve better in understanding performance, while others are useful in\nunderstanding importance and conditional responses of a particular variable.\nEvery explainer presented in this paper works for a single model or for a\ncollection of models. In the latter case, models can be compared against each\nother. Such comparison helps to find strengths and weaknesses of different\napproaches and gives additional possibilities for model validation. Presented\nexplainers are implemented in the DALEX package for R. They are based on a\nuniform standardized grammar of model exploration which may be easily extended.\nThe current implementation supports the most popular frameworks for\nclassification and regression.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jun 2018 06:28:38 GMT"}, {"version": "v2", "created": "Thu, 5 Jul 2018 10:15:54 GMT"}], "update_date": "2019-03-01", "authors_parsed": [["Biecek", "Przemyslaw", ""]]}, {"id": "1806.08925", "submitter": "Janardan Misra", "authors": "Janardan Misra", "title": "An Inductive Formalization of Self Reproduction in Dynamical Hierarchies", "comments": "Preprint of the paper appearing in proceedings of the 10th\n  International Conference on the Simulation and Synthesis of Living Systems\n  (ALife X), pp. 553-558, MIT Press, 2006", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Formalizing self reproduction in dynamical hierarchies is one of the\nimportant problems in Artificial Life (AL) studies. We study, in this paper, an\ninductively defined algebraic framework for self reproduction on macroscopic\norganizational levels under dynamical system setting for simulated AL models\nand explore some existential results. Starting with defining self reproduction\nfor atomic entities we define self reproduction with possible mutations on\nhigher organizational levels in terms of hierarchical sets and the\ncorresponding inductively defined `meta' - reactions. We introduce constraints\nto distinguish a collection of entities from genuine cases of emergent\norganizational structures.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jun 2018 07:47:57 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Misra", "Janardan", ""]]}, {"id": "1806.08941", "submitter": "Janardan Misra", "authors": "Janardan Misra", "title": "A Recursive PLS (Partial Least Squares) based Approach for Enterprise\n  Threat Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the existing solutions to enterprise threat management are preventive\napproaches prescribing means to prevent policy violations with varying degrees\nof success. In this paper we consider the complementary scenario where a number\nof security violations have already occurred, or security threats, or\nvulnerabilities have been reported and a security administrator needs to\ngenerate optimal response to these security events. We present a principled\napproach to study and model the human expertise in responding to the emergent\nthreats owing to these security events. A recursive Partial Least Squares based\nadaptive learning model is defined using a factorial analysis of the security\nevents together with a method for estimating the effect of global context\ndependent semantic information used by the security administrators. Presented\nmodel is theoretically optimal and operationally recursive in nature to deal\nwith the set of security events being generated continuously. We discuss the\nunderlying challenges and ways in which the model could be operationalized in\ncentralized versus decentralized, and real-time versus batch processing modes.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jun 2018 10:12:38 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Misra", "Janardan", ""]]}, {"id": "1806.08984", "submitter": "Markus Wagner", "authors": "Thomas Weise, Zijun Wu, Markus Wagner", "title": "An Improved Generic Bet-and-Run Strategy for Speeding Up Stochastic\n  Local Search", "comments": "results publicly available: https://zenodo.org/record/1253770", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A commonly used strategy for improving optimization algorithms is to restart\nthe algorithm when it is believed to be trapped in an inferior part of the\nsearch space. Building on the recent success of Bet-and-Run approaches for\nrestarted local search solvers, we introduce an improved generic Bet-and-Run\nstrategy. The goal is to obtain the best possible results within a given time\nbudget t using a given black-box optimization algorithm. If no prior knowledge\nabout problem features and algorithm behavior is available, the question about\nhow to use the time budget most efficiently arises. We propose to first start\nk>=1 independent runs of the algorithm during an initialization budget t1<t,\npausing these runs, then apply a decision maker D to choose 1<=m<=k runs from\nthem (consuming t2>=0 time units in doing so), and then continuing these runs\nfor the remaining t3=t-t1-t2 time units. In previous Bet-and-Run strategies,\nthe decision maker D=currentBest would simply select the run with the best-\nso-far results at negligible time. We propose using more advanced methods to\ndiscriminate between \"good\" and \"bad\" sample runs, with the goal of increasing\nthe correlation of the chosen run with the a-posteriori best one. We test\nseveral different approaches, including neural networks trained or polynomials\nfitted on the current trace of the algorithm to predict which run may yield the\nbest results if granted the remaining budget. We show with extensive\nexperiments that this approach can yield better results than the previous\nmethods, but also find that the currentBest method is a very reliable and\nrobust baseline approach.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jun 2018 15:36:04 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Weise", "Thomas", ""], ["Wu", "Zijun", ""], ["Wagner", "Markus", ""]]}, {"id": "1806.09029", "submitter": "Jonathan K Kummerfeld", "authors": "Catherine Finegan-Dollak, Jonathan K. Kummerfeld, Li Zhang, Karthik\n  Ramanathan, Sesh Sadasivam, Rui Zhang, Dragomir Radev", "title": "Improving Text-to-SQL Evaluation Methodology", "comments": "To appear at ACL 2018", "journal-ref": "ACL (2018) 351-360", "doi": "10.18653/v1/P18-1033", "report-no": null, "categories": "cs.CL cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To be informative, an evaluation must measure how well systems generalize to\nrealistic unseen data. We identify limitations of and propose improvements to\ncurrent evaluations of text-to-SQL systems. First, we compare human-generated\nand automatically generated questions, characterizing properties of queries\nnecessary for real-world applications. To facilitate evaluation on multiple\ndatasets, we release standardized and improved versions of seven existing\ndatasets and one new text-to-SQL dataset. Second, we show that the current\ndivision of data into training and test sets measures robustness to variations\nin the way questions are asked, but only partially tests how well systems\ngeneralize to new queries; therefore, we propose a complementary dataset split\nfor evaluation of future work. Finally, we demonstrate how the common practice\nof anonymizing variables during evaluation removes an important challenge of\nthe task. Our observations highlight key difficulties, and our methodology\nenables effective measurement of future development.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jun 2018 20:02:55 GMT"}], "update_date": "2020-06-05", "authors_parsed": [["Finegan-Dollak", "Catherine", ""], ["Kummerfeld", "Jonathan K.", ""], ["Zhang", "Li", ""], ["Ramanathan", "Karthik", ""], ["Sadasivam", "Sesh", ""], ["Zhang", "Rui", ""], ["Radev", "Dragomir", ""]]}, {"id": "1806.09030", "submitter": "Javid Ebrahimi", "authors": "Javid Ebrahimi, Daniel Lowd, Dejing Dou", "title": "On Adversarial Examples for Character-Level Neural Machine Translation", "comments": null, "journal-ref": "COLING 2018", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Evaluating on adversarial examples has become a standard procedure to measure\nrobustness of deep learning models. Due to the difficulty of creating white-box\nadversarial examples for discrete text input, most analyses of the robustness\nof NLP models have been done through black-box adversarial examples. We\ninvestigate adversarial examples for character-level neural machine translation\n(NMT), and contrast black-box adversaries with a novel white-box adversary,\nwhich employs differentiable string-edit operations to rank adversarial\nchanges. We propose two novel types of attacks which aim to remove or change a\nword in a translation, rather than simply break the NMT. We demonstrate that\nwhite-box adversarial examples are significantly stronger than their black-box\ncounterparts in different attack scenarios, which show more serious\nvulnerabilities than previously known. In addition, after performing\nadversarial training, which takes only 3 times longer than regular training, we\ncan improve the model's robustness significantly.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jun 2018 20:08:56 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Ebrahimi", "Javid", ""], ["Lowd", "Daniel", ""], ["Dou", "Dejing", ""]]}, {"id": "1806.09141", "submitter": "Raanan Rohekar", "authors": "Raanan Y. Rohekar, Shami Nisimov, Yaniv Gurwicz, Guy Koren, Gal Novik", "title": "Constructing Deep Neural Networks by Bayesian Network Structure Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We introduce a principled approach for unsupervised structure learning of\ndeep neural networks. We propose a new interpretation for depth and inter-layer\nconnectivity where conditional independencies in the input distribution are\nencoded hierarchically in the network structure. Thus, the depth of the network\nis determined inherently. The proposed method casts the problem of neural\nnetwork structure learning as a problem of Bayesian network structure learning.\nThen, instead of directly learning the discriminative structure, it learns a\ngenerative graph, constructs its stochastic inverse, and then constructs a\ndiscriminative graph. We prove that conditional-dependency relations among the\nlatent variables in the generative graph are preserved in the class-conditional\ndiscriminative graph. We demonstrate on image classification benchmarks that\nthe deepest layers (convolutional and dense) of common networks can be replaced\nby significantly smaller learned structures, while maintaining classification\naccuracy---state-of-the-art on tested benchmarks. Our structure learning\nalgorithm requires a small computational cost and runs efficiently on a\nstandard desktop CPU.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jun 2018 13:05:06 GMT"}, {"version": "v2", "created": "Tue, 16 Oct 2018 11:50:14 GMT"}, {"version": "v3", "created": "Wed, 17 Oct 2018 12:11:20 GMT"}], "update_date": "2018-10-18", "authors_parsed": [["Rohekar", "Raanan Y.", ""], ["Nisimov", "Shami", ""], ["Gurwicz", "Yaniv", ""], ["Koren", "Guy", ""], ["Novik", "Gal", ""]]}, {"id": "1806.09192", "submitter": "Aristide Charles Yedia Tossou", "authors": "Aristide C. Y. Tossou and Christos Dimitrakakis", "title": "On The Differential Privacy of Thompson Sampling With Gaussian Prior", "comments": "Accepted in Privacy in Machine Learning and Artificial Intelligence\n  Workshop 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that Thompson Sampling with Gaussian Prior as detailed by Algorithm 2\nin (Agrawal & Goyal, 2013) is already differentially private. Theorem 1 show\nthat it enjoys a very competitive privacy loss of only $\\mathcal{O}(\\ln^2 T)$\nafter T rounds. Finally, Theorem 2 show that one can control the privacy loss\nto any desirable $\\epsilon$ level by appropriately increasing the variance of\nthe samples from the Gaussian posterior. And this increases the regret only by\na term of $\\mathcal{O}(\\frac{\\ln^2 T}{\\epsilon})$. This compares favorably to\nthe previous result for Thompson Sampling in the literature ((Mishra &\nThakurta, 2015)) which adds a term of $\\mathcal{O}(\\frac{K \\ln^3\nT}{\\epsilon^2})$ to the regret in order to achieve the same privacy level.\nFurthermore, our result use the basic Thompson Sampling with few modifications\nwhereas the result of (Mishra & Thakurta, 2015) required sophisticated\nconstructions.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jun 2018 18:37:09 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Tossou", "Aristide C. Y.", ""], ["Dimitrakakis", "Christos", ""]]}, {"id": "1806.09256", "submitter": "Cagatay Demiralp", "authors": "Marco Cavallo and \\c{C}a\\u{g}atay Demiralp", "title": "Track Xplorer: A System for Visual Analysis of Sensor-based Motor\n  Activity Predictions", "comments": "EuroVis'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CV cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the rapid commoditization of wearable sensors, detecting human movements\nfrom sensor datasets has become increasingly common over a wide range of\napplications. To detect activities, data scientists iteratively experiment with\ndifferent classifiers before deciding which model to deploy. Effective\nreasoning about and comparison of alternative classifiers are crucial in\nsuccessful model development. This is, however, inherently difficult in\ndeveloping classifiers for sensor data, where the intricacy of long temporal\nsequences, high prediction frequency, and imprecise labeling make standard\nevaluation methods relatively ineffective and even misleading. We introduce\nTrack Xplorer, an interactive visualization system to query, analyze, and\ncompare the predictions of sensor-data classifiers. Track Xplorer enables users\nto interactively explore and compare the results of different classifiers, and\nassess their accuracy with respect to the ground-truth labels and video.\nThrough integration with a version control system, Track Xplorer supports\ntracking of models and their parameters without additional workload on model\ndevelopers. Track Xplorer also contributes an extensible algebra over track\nrepresentations to filter, compose, and compare classification outputs,\nenabling users to reason effectively about classifier performance. We apply\nTrack Xplorer in a collaborative project to develop classifiers to detect\nmovements from multisensor data gathered from Parkinson's disease patients. We\ndemonstrate how Track Xplorer helps identify early on possible systemic data\nerrors, effectively track and compare the results of different classifiers, and\nreason about and pinpoint the causes of misclassifications.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 02:19:24 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Cavallo", "Marco", ""], ["Demiralp", "\u00c7a\u011fatay", ""]]}, {"id": "1806.09328", "submitter": "Conrad Sanderson", "authors": "Majid Namazi, Conrad Sanderson, M.A. Hakim Newton, M.M.A. Polash,\n  Abdul Sattar", "title": "Diversified Late Acceptance Search", "comments": null, "journal-ref": "Lecture Notes in Computer Science, Vol. 11320, pp. 299-311, 2018", "doi": "10.1007/978-3-030-03991-2_29", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The well-known Late Acceptance Hill Climbing (LAHC) search aims to overcome\nthe main downside of traditional Hill Climbing (HC) search, which is often\nquickly trapped in a local optimum due to strictly accepting only non-worsening\nmoves within each iteration. In contrast, LAHC also accepts worsening moves, by\nkeeping a circular array of fitness values of previously visited solutions and\ncomparing the fitness values of candidate solutions against the least recent\nelement in the array. While this straightforward strategy has proven effective,\nthere are nevertheless situations where LAHC can unfortunately behave in a\nsimilar manner to HC. For example, when a new local optimum is found, often the\nsame fitness value is stored many times in the array. To address this\nshortcoming, we propose new acceptance and replacement strategies to take into\naccount worsening, improving, and sideways movement scenarios with the aim to\nimprove the diversity of values in the array. Compared to LAHC, the proposed\nDiversified Late Acceptance Search approach is shown to lead to better quality\nsolutions that are obtained with a lower number of iterations on benchmark\nTravelling Salesman Problems and Quadratic Assignment Problems.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 08:47:08 GMT"}, {"version": "v2", "created": "Mon, 10 Sep 2018 05:04:55 GMT"}, {"version": "v3", "created": "Mon, 10 Dec 2018 04:17:27 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Namazi", "Majid", ""], ["Sanderson", "Conrad", ""], ["Newton", "M. A. Hakim", ""], ["Polash", "M. M. A.", ""], ["Sattar", "Abdul", ""]]}, {"id": "1806.09351", "submitter": "Rituraj Kaushik", "authors": "Rituraj Kaushik, Konstantinos Chatzilygeroudis, Jean-Baptiste Mouret", "title": "Multi-objective Model-based Policy Search for Data-efficient Learning\n  with Sparse Rewards", "comments": "Conference on Robot Learning (CoRL)- 2018; Code at\n  https://github.com/resibots/kaushik_2018_multi-dex ; Video at\n  https://youtu.be/9ZLwUxAAq6M", "journal-ref": "Proceedings of the Conference on Robot Learning, PMLR 87:839-855,\n  2018", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The most data-efficient algorithms for reinforcement learning in robotics are\nmodel-based policy search algorithms, which alternate between learning a\ndynamical model of the robot and optimizing a policy to maximize the expected\nreturn given the model and its uncertainties. However, the current algorithms\nlack an effective exploration strategy to deal with sparse or misleading reward\nscenarios: if they do not experience any state with a positive reward during\nthe initial random exploration, it is very unlikely to solve the problem. Here,\nwe propose a novel model-based policy search algorithm, Multi-DEX, that\nleverages a learned dynamical model to efficiently explore the task space and\nsolve tasks with sparse rewards in a few episodes. To achieve this, we frame\nthe policy search problem as a multi-objective, model-based policy optimization\nproblem with three objectives: (1) generate maximally novel state trajectories,\n(2) maximize the expected return and (3) keep the system in state-space regions\nfor which the model is as accurate as possible. We then optimize these\nobjectives using a Pareto-based multi-objective optimization algorithm. The\nexperiments show that Multi-DEX is able to solve sparse reward scenarios (with\na simulated robotic arm) in much lower interaction time than VIME, TRPO,\nGEP-PG, CMA-ES and Black-DROPS.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 09:46:47 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2018 10:20:33 GMT"}, {"version": "v3", "created": "Tue, 3 Mar 2020 22:57:46 GMT"}], "update_date": "2020-03-05", "authors_parsed": [["Kaushik", "Rituraj", ""], ["Chatzilygeroudis", "Konstantinos", ""], ["Mouret", "Jean-Baptiste", ""]]}, {"id": "1806.09444", "submitter": "Nikita Jaipuria", "authors": "Nikita Jaipuria, Golnaz Habibi, Jonathan P. How", "title": "A Transferable Pedestrian Motion Prediction Model for Intersections with\n  Different Geometries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel framework for accurate pedestrian intent\nprediction at intersections. Given some prior knowledge of the curbside\ngeometry, the presented framework can accurately predict pedestrian\ntrajectories, even in new intersections that it has not been trained on. This\nis achieved by making use of the contravariant components of trajectories in\nthe curbside coordinate system, which ensures that the transformation of\ntrajectories across intersections is affine, regardless of the curbside\ngeometry. Our method is based on the Augmented Semi Nonnegative Sparse Coding\n(ASNSC) formulation and we use that as a baseline to show improvement in\nprediction performance on real pedestrian datasets collected at two\nintersections in Cambridge, with distinctly different curbside and crosswalk\ngeometries. We demonstrate a 7.2% improvement in prediction accuracy in the\ncase of same train and test intersections. Furthermore, we show a comparable\nprediction performance of TASNSC when trained and tested in different\nintersections with the baseline, trained and tested on the same intersection.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 13:19:45 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Jaipuria", "Nikita", ""], ["Habibi", "Golnaz", ""], ["How", "Jonathan P.", ""]]}, {"id": "1806.09453", "submitter": "Nikita Jaipuria", "authors": "Golnaz Habibi, Nikita Jaipuria, Jonathan P. How", "title": "Context-Aware Pedestrian Motion Prediction In Urban Intersections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel context-based approach for pedestrian motion\nprediction in crowded, urban intersections, with the additional flexibility of\nprediction in similar, but new, environments. Previously, Chen et. al. combined\nMarkovian-based and clustering-based approaches to learn motion primitives in a\ngrid-based world and subsequently predict pedestrian trajectories by modeling\nthe transition between learned primitives as a Gaussian Process (GP). This work\nextends that prior approach by incorporating semantic features from the\nenvironment (relative distance to curbside and status of pedestrian traffic\nlights) in the GP formulation for more accurate predictions of pedestrian\ntrajectories over the same timescale. We evaluate the new approach on\nreal-world data collected using one of the vehicles in the MIT Mobility On\nDemand fleet. The results show 12.5% improvement in prediction accuracy and a\n2.65 times reduction in Area Under the Curve (AUC), which is used as a metric\nto quantify the span of predicted set of trajectories, such that a lower AUC\ncorresponds to a higher level of confidence in the future direction of\npedestrian motion.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 13:45:57 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Habibi", "Golnaz", ""], ["Jaipuria", "Nikita", ""], ["How", "Jonathan P.", ""]]}, {"id": "1806.09455", "submitter": "Hector Geffner", "authors": "Tomas Geffner and Hector Geffner", "title": "Compact Policies for Fully-Observable Non-Deterministic Planning as SAT", "comments": null, "journal-ref": "Proc. ICAPS 2018", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully observable non-deterministic (FOND) planning is becoming increasingly\nimportant as an approach for computing proper policies in probabilistic\nplanning, extended temporal plans in LTL planning, and general plans in\ngeneralized planning. In this work, we introduce a SAT encoding for FOND\nplanning that is compact and can produce compact strong cyclic policies. Simple\nvariations of the encodings are also introduced for strong planning and for\nwhat we call, dual FOND planning, where some non-deterministic actions are\nassumed to be fair (e.g., probabilistic) and others unfair (e.g., adversarial).\nThe resulting FOND planners are compared empirically with existing planners\nover existing and new benchmarks. The notion of \"probabilistic interesting\nproblems\" is also revisited to yield a more comprehensive picture of the\nstrengths and limitations of current FOND planners and the proposed SAT\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 13:51:04 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Geffner", "Tomas", ""], ["Geffner", "Hector", ""]]}, {"id": "1806.09464", "submitter": "Ting Chen", "authors": "Ting Chen, Martin Renqiang Min, Yizhou Sun", "title": "Learning K-way D-dimensional Discrete Codes for Compact Embedding\n  Representations", "comments": "ICML 2018. arXiv admin note: text overlap with arXiv:1711.03067", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional embedding methods directly associate each symbol with a\ncontinuous embedding vector, which is equivalent to applying a linear\ntransformation based on a \"one-hot\" encoding of the discrete symbols. Despite\nits simplicity, such approach yields the number of parameters that grows\nlinearly with the vocabulary size and can lead to overfitting. In this work, we\npropose a much more compact K-way D-dimensional discrete encoding scheme to\nreplace the \"one-hot\" encoding. In the proposed \"KD encoding\", each symbol is\nrepresented by a $D$-dimensional code with a cardinality of $K$, and the final\nsymbol embedding vector is generated by composing the code embedding vectors.\nTo end-to-end learn semantically meaningful codes, we derive a relaxed discrete\noptimization approach based on stochastic gradient descent, which can be\ngenerally applied to any differentiable computational graph with an embedding\nlayer. In our experiments with various applications from natural language\nprocessing to graph convolutional networks, the total size of the embedding\nlayer can be reduced up to 98\\% while achieving similar or better performance.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jun 2018 18:59:05 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Chen", "Ting", ""], ["Min", "Martin Renqiang", ""], ["Sun", "Yizhou", ""]]}, {"id": "1806.09487", "submitter": "Pavel Surynek", "authors": "Pavel Surynek", "title": "Finding Optimal Solutions to Token Swapping by Conflict-based Search and\n  Reduction to SAT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study practical approaches to solving the token swapping (TSWAP) problem\noptimally in this short paper. In TSWAP, we are given an undirected graph with\ncolored vertices. A colored token is placed in each vertex. A pair of tokens\ncan be swapped between adjacent vertices. The goal is to perform a sequence of\nswaps so that token and vertex colors agree across the graph. The minimum\nnumber of swaps is required in the optimization variant of the problem. We\nobserved similarities between the TSWAP problem and multi-agent path finding\n(MAPF) where instead of tokens we have multiple agents that need to be moved\nfrom their current vertices to given unique target vertices. The difference\nbetween both problems consists in local conditions that state transitions\n(swaps/moves) must satisfy. We developed two algorithms for solving TSWAP\noptimally by adapting two different approaches to MAPF - CBS and MDD- SAT. This\nconstitutes the first attempt to design optimal solving algorithms for TSWAP.\nExperimental evaluation on various types of graphs shows that the reduction to\nSAT scales better than CBS in optimal TSWAP solving.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 14:28:09 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Surynek", "Pavel", ""]]}, {"id": "1806.09504", "submitter": "Arthur Colombini Gusm\\~ao", "authors": "Arthur Colombini Gusm\\~ao, Alvaro Henrique Chaim Correia, Glauber De\n  Bona, and Fabio Gagliardi Cozman", "title": "Interpreting Embedding Models of Knowledge Bases: A Pedagogical Approach", "comments": "presented at 2018 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2018), Stockholm, Sweden", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge bases are employed in a variety of applications from natural\nlanguage processing to semantic web search; alas, in practice their usefulness\nis hurt by their incompleteness. Embedding models attain state-of-the-art\naccuracy in knowledge base completion, but their predictions are notoriously\nhard to interpret. In this paper, we adapt \"pedagogical approaches\" (from the\nliterature on neural networks) so as to interpret embedding models by\nextracting weighted Horn rules from them. We show how pedagogical approaches\nhave to be adapted to take upon the large-scale relational aspects of knowledge\nbases and show experimentally their strengths and weaknesses.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 23:01:05 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Gusm\u00e3o", "Arthur Colombini", ""], ["Correia", "Alvaro Henrique Chaim", ""], ["De Bona", "Glauber", ""], ["Cozman", "Fabio Gagliardi", ""]]}, {"id": "1806.09506", "submitter": "Rita Gitik", "authors": "Rita Gitik", "title": "Optimal Seeding and Self-Reproduction from a Mathematical Point of View", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  P. Kabamba developed generation theory as a tool for studying\nself-reproducing systems. We provide an alternative definition of a generation\nsystem and give a complete solution to the problem of finding optimal seeds for\na finite self-replicating system. We also exhibit examples illustrating a\nconnection between self-replication and fixed-point theory.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 15:39:21 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Gitik", "Rita", ""]]}, {"id": "1806.09511", "submitter": "Jose Marcelino", "authors": "Jos\\'e Marcelino, Jo\\~ao Faria, Lu\\'is Ba\\'ia, Ricardo Gamelas Sousa", "title": "A Hierarchical Deep Learning Natural Language Parser for Fashion", "comments": "In Proceedings of KDD 2018 (KDD Workshop on AI for Fashion)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a hierarchical deep learning natural language parser for\nfashion. Our proposal intends not only to recognize fashion-domain entities but\nalso to expose syntactic and morphologic insights. We leverage the usage of an\narchitecture of specialist models, each one for a different task (from parsing\nto entity recognition). Such architecture renders a hierarchical model able to\ncapture the nuances of the fashion language. The natural language parser is\nable to deal with textual ambiguities which are left unresolved by our\ncurrently existing solution. Our empirical results establish a robust baseline,\nwhich justifies the use of hierarchical architectures of deep learning models\nwhile opening new research avenues to explore.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 14:57:52 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Marcelino", "Jos\u00e9", ""], ["Faria", "Jo\u00e3o", ""], ["Ba\u00eda", "Lu\u00eds", ""], ["Sousa", "Ricardo Gamelas", ""]]}, {"id": "1806.09514", "submitter": "No\\'e Tits", "authors": "Adaeze Adigwe, No\\'e Tits, Kevin El Haddad, Sarah Ostadabbas and\n  Thierry Dutoit", "title": "The Emotional Voices Database: Towards Controlling the Emotion Dimension\n  in Voice Generation Systems", "comments": "Submitted to SLSP 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a database of emotional speech intended to be\nopen-sourced and used for synthesis and generation purpose. It contains data\nfor male and female actors in English and a male actor in French. The database\ncovers 5 emotion classes so it could be suitable to build synthesis and voice\ntransformation systems with the potential to control the emotional dimension in\na continuous way. We show the data's efficiency by building a simple MLP system\nconverting neutral to angry speech style and evaluate it via a CMOS perception\ntest. Even though the system is a very simple one, the test show the efficiency\nof the data which is promising for future work.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 15:01:54 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Adigwe", "Adaeze", ""], ["Tits", "No\u00e9", ""], ["Haddad", "Kevin El", ""], ["Ostadabbas", "Sarah", ""], ["Dutoit", "Thierry", ""]]}, {"id": "1806.09597", "submitter": "Samuel L. Smith", "authors": "Samuel L. Smith, Daniel Duckworth, Semon Rezchikov, Quoc V. Le and\n  Jascha Sohl-Dickstein", "title": "Stochastic natural gradient descent draws posterior samples in function\n  space", "comments": "Workshop on Bayesian Deep Learning (NeurIPS 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has argued that stochastic gradient descent can approximate the\nBayesian uncertainty in model parameters near local minima. In this work we\ndevelop a similar correspondence for minibatch natural gradient descent (NGD).\nWe prove that for sufficiently small learning rates, if the model predictions\non the training set approach the true conditional distribution of labels given\ninputs, the stationary distribution of minibatch NGD approaches a Bayesian\nposterior near local minima. The temperature $T = \\epsilon N / (2B)$ is\ncontrolled by the learning rate $\\epsilon$, training set size $N$ and batch\nsize $B$. However minibatch NGD is not parameterisation invariant and it does\nnot sample a valid posterior away from local minima. We therefore propose a\nnovel optimiser, \"stochastic NGD\", which introduces the additional correction\nterms required to preserve both properties.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 17:47:42 GMT"}, {"version": "v2", "created": "Fri, 6 Jul 2018 01:10:14 GMT"}, {"version": "v3", "created": "Tue, 16 Oct 2018 15:04:58 GMT"}, {"version": "v4", "created": "Wed, 28 Nov 2018 18:07:24 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Smith", "Samuel L.", ""], ["Duckworth", "Daniel", ""], ["Rezchikov", "Semon", ""], ["Le", "Quoc V.", ""], ["Sohl-Dickstein", "Jascha", ""]]}, {"id": "1806.09605", "submitter": "Vivek Veeriah", "authors": "Vivek Veeriah, Junhyuk Oh, Satinder Singh", "title": "Many-Goals Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  All-goals updating exploits the off-policy nature of Q-learning to update all\npossible goals an agent could have from each transition in the world, and was\nintroduced into Reinforcement Learning (RL) by Kaelbling (1993). In prior work\nthis was mostly explored in small-state RL problems that allowed tabular\nrepresentations and where all possible goals could be explicitly enumerated and\nlearned separately. In this paper we empirically explore 3 different extensions\nof the idea of updating many (instead of all) goals in the context of RL with\ndeep neural networks (or DeepRL for short). First, in a direct adaptation of\nKaelbling's approach we explore if many-goals updating can be used to achieve\nmastery in non-tabular visual-observation domains. Second, we explore whether\nmany-goals updating can be used to pre-train a network to subsequently learn\nfaster and better on a single main task of interest. Third, we explore whether\nmany-goals updating can be used to provide auxiliary task updates in training a\nnetwork to learn faster and better on a single main task of interest. We\nprovide comparisons to baselines for each of the 3 extensions.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jun 2018 18:31:24 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Veeriah", "Vivek", ""], ["Oh", "Junhyuk", ""], ["Singh", "Satinder", ""]]}, {"id": "1806.09612", "submitter": "Arindam Chaudhuri AC", "authors": "Arindam Chaudhuri", "title": "Predictive Maintenance for Industrial IoT of Vehicle Fleets using\n  Hierarchical Modified Fuzzy Support Vector Machine", "comments": "Research work done at Samsung R & D Institute Delhi India", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Connected vehicle fleets are deployed worldwide in several industrial IoT\nscenarios. With the gradual increase of machines being controlled and managed\nthrough networked smart devices, the predictive maintenance potential grows\nrapidly. Predictive maintenance has the potential of optimizing uptime as well\nas performance such that time and labor associated with inspections and\npreventive maintenance are reduced. In order to understand the trends of\nvehicle faults with respect to important vehicle attributes viz mileage, age,\nvehicle type etc this problem is addressed through hierarchical modified fuzzy\nsupport vector machine (HMFSVM). The proposed method is compared with other\ncommonly used approaches like logistic regression, random forests and support\nvector machines. This helps better implementation of telematics data to ensure\npreventative management as part of the desired solution. The superiority of the\nproposed method is highlighted through several experimental results.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jun 2018 18:09:52 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Chaudhuri", "Arindam", ""]]}, {"id": "1806.09614", "submitter": "Olivier Sigaud", "authors": "Pierre Fournier, Olivier Sigaud, Mohamed Chetouani, Pierre-Yves\n  Oudeyer", "title": "Accuracy-based Curriculum Learning in Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate a new form of automated curriculum learning\nbased on adaptive selection of accuracy requirements, called accuracy-based\ncurriculum learning. Using a reinforcement learning agent based on the Deep\nDeterministic Policy Gradient algorithm and addressing the Reacher environment,\nwe first show that an agent trained with various accuracy requirements sampled\nrandomly learns more efficiently than when asked to be very accurate at all\ntimes. Then we show that adaptive selection of accuracy requirements, based on\na local measure of competence progress, automatically generates a curriculum\nwhere difficulty progressively increases, resulting in a better learning\nefficiency than sampling randomly.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 12:06:28 GMT"}, {"version": "v2", "created": "Fri, 21 Sep 2018 11:20:05 GMT"}], "update_date": "2018-09-24", "authors_parsed": [["Fournier", "Pierre", ""], ["Sigaud", "Olivier", ""], ["Chetouani", "Mohamed", ""], ["Oudeyer", "Pierre-Yves", ""]]}, {"id": "1806.09731", "submitter": "Jo\\~ao Correia", "authors": "Tiago Martins, Jo\\~ao Correia, Ernesto Costa, Penousal Machado", "title": "Evotype: Towards the Evolution of Type Stencils", "comments": "EvoMUSART 2018 Best paper", "journal-ref": null, "doi": "10.1007/978-3-319-77583-8_20", "report-no": null, "categories": "cs.NE cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typefaces are an essential resource employed by graphic designers. The\nincreasing demand for innovative type design work increases the need for good\ntechnological means to assist the designer in the creation of a typeface. We\npresent an evolutionary computation approach for the generation of type\nstencils to draw coherent glyphs for different characters. The proposed system\nemploys a Genetic Algorithm to evolve populations of type stencils. The\nevaluation of each candidate stencil uses a hill climbing algorithm to search\nthe best configurations to draw the target glyphs. We study the interplay\nbetween legibility, coherence and expressiveness, and show how our framework\ncan be used in practice.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 00:03:23 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Martins", "Tiago", ""], ["Correia", "Jo\u00e3o", ""], ["Costa", "Ernesto", ""], ["Machado", "Penousal", ""]]}, {"id": "1806.09748", "submitter": "Jong Chul Ye", "authors": "Eunhee Kang, Hyun Jung Koo, Dong Hyun Yang, Joon Bum Seo, and Jong\n  Chul Ye", "title": "Cycle Consistent Adversarial Denoising Network for Multiphase Coronary\n  CT Angiography", "comments": "This work is accepted in Medical Physics", "journal-ref": null, "doi": "10.1002/mp.13284", "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In coronary CT angiography, a series of CT images are taken at different\nlevels of radiation dose during the examination. Although this reduces the\ntotal radiation dose, the image quality during the low-dose phases is\nsignificantly degraded. To address this problem, here we propose a novel\nsemi-supervised learning technique that can remove the noises of the CT images\nobtained in the low-dose phases by learning from the CT images in the routine\ndose phases. Although a supervised learning approach is not possible due to the\ndifferences in the underlying heart structure in two phases, the images in the\ntwo phases are closely related so that we propose a cycle-consistent\nadversarial denoising network to learn the non-degenerate mapping between the\nlow and high dose cardiac phases. Experimental results showed that the proposed\nmethod effectively reduces the noise in the low-dose CT image while the\npreserving detailed texture and edge information. Moreover, thanks to the\ncyclic consistency and identity loss, the proposed network does not create any\nartificial features that are not present in the input images. Visual grading\nand quality evaluation also confirm that the proposed method provides\nsignificant improvement in diagnostic quality.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 01:17:51 GMT"}, {"version": "v2", "created": "Wed, 10 Oct 2018 12:14:42 GMT"}, {"version": "v3", "created": "Wed, 7 Nov 2018 14:21:14 GMT"}], "update_date": "2019-03-06", "authors_parsed": [["Kang", "Eunhee", ""], ["Koo", "Hyun Jung", ""], ["Yang", "Dong Hyun", ""], ["Seo", "Joon Bum", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1806.09769", "submitter": "Zheming Zhou", "authors": "Zheming Zhou, Zhiqiang Sui, Odest Chadwicke Jenkins", "title": "Plenoptic Monte Carlo Object Localization for Robot Grasping under\n  Layered Translucency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to fully function in human environments, robot perception will need\nto account for the uncertainty caused by translucent materials. Translucency\nposes several open challenges in the form of transparent objects (e.g.,\ndrinking glasses), refractive media (e.g., water), and diffuse partial\nocclusions (e.g., objects behind stained glass panels). This paper presents\nPlenoptic Monte Carlo Localization (PMCL) as a method for localizing object\nposes in the presence of translucency using plenoptic (light-field)\nobservations. We propose a new depth descriptor, the Depth Likelihood Volume\n(DLV), and its use within a Monte Carlo object localization algorithm. We\npresent results of localizing and manipulating objects with translucent\nmaterials and objects occluded by layers of translucency. Our PMCL\nimplementation uses observations from a Lytro first generation light field\ncamera to allow a Michigan Progress Fetch robot to perform grasping.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 02:41:52 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2018 23:39:02 GMT"}, {"version": "v3", "created": "Tue, 20 Nov 2018 00:38:57 GMT"}, {"version": "v4", "created": "Sun, 2 Dec 2018 01:32:41 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Zhou", "Zheming", ""], ["Sui", "Zhiqiang", ""], ["Jenkins", "Odest Chadwicke", ""]]}, {"id": "1806.09771", "submitter": "Zhengxing Chen", "authors": "Zhengxing Chen, Chris Amato, Truong-Huy Nguyen, Seth Cooper, Yizhou\n  Sun, Magy Seif El-Nasr", "title": "Q-DeckRec: A Fast Deck Recommendation System for Collectible Card Games", "comments": "CIG 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deck building is a crucial component in playing Collectible Card Games\n(CCGs). The goal of deck building is to choose a fixed-sized subset of cards\nfrom a large card pool, so that they work well together in-game against\nspecific opponents. Existing methods either lack flexibility to adapt to\ndifferent opponents or require large computational resources, still making them\nunsuitable for any real-time or large-scale application. We propose a new deck\nrecommendation system, named Q-DeckRec, which learns a deck search policy\nduring a training phase and uses it to solve deck building problem instances.\nOur experimental results demonstrate Q-DeckRec requires less computational\nresources to build winning-effective decks after a training phase compared to\nseveral baseline methods.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 02:55:16 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Chen", "Zhengxing", ""], ["Amato", "Chris", ""], ["Nguyen", "Truong-Huy", ""], ["Cooper", "Seth", ""], ["Sun", "Yizhou", ""], ["El-Nasr", "Magy Seif", ""]]}, {"id": "1806.09777", "submitter": "Raman Arora", "authors": "Poorya Mianjy, Raman Arora, Rene Vidal", "title": "On the Implicit Bias of Dropout", "comments": "17 pages, 3 figures, In Proceedings of the Thirty-fifth International\n  Conference on Machine Learning (ICML), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithmic approaches endow deep learning systems with implicit bias that\nhelps them generalize even in over-parametrized settings. In this paper, we\nfocus on understanding such a bias induced in learning through dropout, a\npopular technique to avoid overfitting in deep learning. For single\nhidden-layer linear neural networks, we show that dropout tends to make the\nnorm of incoming/outgoing weight vectors of all the hidden nodes equal. In\naddition, we provide a complete characterization of the optimization landscape\ninduced by dropout.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 03:08:21 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Mianjy", "Poorya", ""], ["Arora", "Raman", ""], ["Vidal", "Rene", ""]]}, {"id": "1806.09785", "submitter": "Richard Diehl Martinez", "authors": "Rooz Mahdavian, Richard Diehl Martinez", "title": "Theory of Machine Networks: A Case Study", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simplification of the Theory-of-Mind Network architecture, which\nfocuses on modeling complex, deterministic machines as a proxy for modeling\nnondeterministic, conscious entities. We then validate this architecture in the\ncontext of understanding engines, which, we argue, meet the required internal\nand external complexity to yield meaningful abstractions.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 04:13:25 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Mahdavian", "Rooz", ""], ["Martinez", "Richard Diehl", ""]]}, {"id": "1806.09787", "submitter": "Hamidreza Alvari", "authors": "Hamidreza Alvari, Paulo Shakarian", "title": "Causal Inference for Early Detection of Pathogenic Social Media Accounts", "comments": "Doctoral Consortium - 2018 International Conference on Social\n  Computing, Behavioral-Cultural Modeling & Prediction and Behavior\n  Representation in Modeling and Simulation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pathogenic social media accounts such as terrorist supporters exploit\ncommunities of supporters for conducting attacks on social media. Early\ndetection of PSM accounts is crucial as they are likely to be key users in\nmaking a harmful message \"viral\". This paper overviews my recent doctoral work\non utilizing causal inference to identify PSM accounts within a short time\nframe around their activity. The proposed scheme (1) assigns time-decay\ncausality scores to users, (2) applies a community detection-based algorithm to\ngroup of users sharing similar causality scores and finally (3) deploys a\nclassification algorithm to classify accounts. Unlike existing techniques that\nrequire network structure, cascade path, or content, our scheme relies solely\non action log of users.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 04:15:38 GMT"}, {"version": "v2", "created": "Fri, 3 Aug 2018 21:19:07 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Alvari", "Hamidreza", ""], ["Shakarian", "Paulo", ""]]}, {"id": "1806.09842", "submitter": "Pan Li", "authors": "Pan Li, Niao He, Olgica Milenkovic", "title": "Quadratic Decomposable Submodular Function Minimization", "comments": "A part of this work will be presented in NIPS2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new convex optimization problem, termed quadratic decomposable\nsubmodular function minimization. The problem is closely related to\ndecomposable submodular function minimization and arises in many learning on\ngraphs and hypergraphs settings, such as graph-based semi-supervised learning\nand PageRank. We approach the problem via a new dual strategy and describe an\nobjective that may be optimized via random coordinate descent (RCD) methods and\nprojections onto cones. We also establish the linear convergence rate of the\nRCD algorithm and develop efficient projection algorithms with provable\nperformance guarantees. Numerical experiments in semi-supervised learning on\nhypergraphs confirm the efficiency of the proposed algorithm and demonstrate\nthe significant improvements in prediction accuracy with respect to\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 08:47:05 GMT"}, {"version": "v2", "created": "Tue, 25 Sep 2018 07:27:42 GMT"}, {"version": "v3", "created": "Thu, 11 Oct 2018 16:24:23 GMT"}], "update_date": "2018-10-12", "authors_parsed": [["Li", "Pan", ""], ["He", "Niao", ""], ["Milenkovic", "Olgica", ""]]}, {"id": "1806.09935", "submitter": "Mohamed El Yafrani", "authors": "Marcella S. R. Martins, Mohamed El Yafrani, Roberto Santana, Myriam\n  Delgado, Ricardo L\\\"uders, Bela\\\"id Ahiod", "title": "On the performance of multi-objective estimation of distribution\n  algorithms for combinatorial problems", "comments": "Accepted in IEEE WCCI/CEC '2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fitness landscape analysis investigates features with a high influence on the\nperformance of optimization algorithms, aiming to take advantage of the\naddressed problem characteristics. In this work, a fitness landscape analysis\nusing problem features is performed for a Multi-objective Bayesian Optimization\nAlgorithm (mBOA) on instances of MNK-landscape problem for 2, 3, 5 and 8\nobjectives. We also compare the results of mBOA with those provided by NSGA-III\nthrough the analysis of their estimated runtime necessary to identify an\napproximation of the Pareto front. Moreover, in order to scrutinize the\nprobabilistic graphic model obtained by mBOA, the Pareto front is examined\naccording to a probabilistic view. The fitness landscape study shows that mBOA\nis moderately or loosely influenced by some problem features, according to a\nsimple and a multiple linear regression model, which is being proposed to\npredict the algorithms performance in terms of the estimated runtime. Besides,\nwe conclude that the analysis of the probabilistic graphic model produced at\nthe end of evolution can be useful to understand the convergence and diversity\nperformances of the proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jun 2018 23:15:57 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Martins", "Marcella S. R.", ""], ["Yafrani", "Mohamed El", ""], ["Santana", "Roberto", ""], ["Delgado", "Myriam", ""], ["L\u00fcders", "Ricardo", ""], ["Ahiod", "Bela\u00efd", ""]]}, {"id": "1806.09936", "submitter": "Riccardo Guidotti", "authors": "Dino Pedreschi, Fosca Giannotti, Riccardo Guidotti, Anna Monreale,\n  Luca Pappalardo, Salvatore Ruggieri, Franco Turini", "title": "Open the Black Box Data-Driven Explanation of Black Box Decision Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Black box systems for automated decision making, often based on machine\nlearning over (big) data, map a user's features into a class or a score without\nexposing the reasons why. This is problematic not only for lack of\ntransparency, but also for possible biases hidden in the algorithms, due to\nhuman prejudices and collection artifacts hidden in the training data, which\nmay lead to unfair or wrong decisions. We introduce the local-to-global\nframework for black box explanation, a novel approach with promising early\nresults, which paves the road for a wide spectrum of future developments along\nthree dimensions: (i) the language for expressing explanations in terms of\nhighly expressive logic-based rules, with a statistical and causal\ninterpretation; (ii) the inference of local explanations aimed at revealing the\nlogic of the decision adopted for a specific instance by querying and auditing\nthe black box in the vicinity of the target instance; (iii), the bottom-up\ngeneralization of the many local explanations into simple global ones, with\nalgorithms that optimize the quality and comprehensibility of explanations.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 12:14:44 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Pedreschi", "Dino", ""], ["Giannotti", "Fosca", ""], ["Guidotti", "Riccardo", ""], ["Monreale", "Anna", ""], ["Pappalardo", "Luca", ""], ["Ruggieri", "Salvatore", ""], ["Turini", "Franco", ""]]}, {"id": "1806.09954", "submitter": "Arthur Bit-Monnot", "authors": "Arthur Bit-Monnot", "title": "A Constraint-based Encoding for Domain-Independent Temporal Planning", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-98334-9_3", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general constraint-based encoding for domain-independent task\nplanning. Task planning is characterized by causal relationships expressed as\nconditions and effects of optional actions. Possible actions are typically\nrepresented by templates, where each template can be instantiated into a number\nof primitive actions. While most previous work for domain-independent task\nplanning has focused on primitive actions in a state-oriented view, our\nencoding uses a fully lifted representation at the level of action templates.\nIt follows a time-oriented view in the spirit of previous work in\nconstraint-based scheduling. As a result, the proposed encoding is simple and\ncompact as it grows with the number of actions in a solution plan rather than\nthe number of possible primitive actions. When solved with an SMT solver, we\nshow that the proposed encoding is slightly more efficient than\nstate-of-the-art methods on temporally constrained planning benchmarks while\nclearly outperforming other fully constraint-based approaches.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 12:58:59 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Bit-Monnot", "Arthur", ""]]}, {"id": "1806.09959", "submitter": "Arnaud Martin", "authors": "Manel Chehibi, Mouna Chebbah (LARODEC), Arnaud Martin (DRUID)", "title": "Independence of Sources in Social Networks", "comments": null, "journal-ref": "Information Processing and Management of Uncertainty in\n  Knowledge-Based Systems. Theory and Foundations - 17th International\n  Conference, IPMU, Jun 2018, Cadiz, Spain", "doi": null, "report-no": null, "categories": "cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online social networks are more and more studied. The links between users of\na social network are important and have to be well qualified in order to detect\ncommunities and find influencers for example. In this paper, we present an\napproach based on the theory of belief functions to estimate the degrees of\ncognitive independence between users in a social network. We experiment the\nproposed method on a large amount of data gathered from the Twitter social\nnetwork.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 13:10:41 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Chehibi", "Manel", "", "LARODEC"], ["Chebbah", "Mouna", "", "LARODEC"], ["Martin", "Arnaud", "", "DRUID"]]}, {"id": "1806.09997", "submitter": "Pierre Denis Mr.", "authors": "Pierre Denis", "title": "Probabilistic Inference Using Generators - The Statues Algorithm", "comments": "50 pages, incl. 3 appendices (v2: typos and minor corrections, added\n  appendix C with proof of correctness)", "journal-ref": null, "doi": "10.1007/978-3-030-52246-9_10", "report-no": null, "categories": "cs.AI cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present here a new probabilistic inference algorithm that gives exact\nresults in the domain of discrete probability distributions. This algorithm,\nnamed the Statues algorithm, calculates the marginal probability distribution\non probabilistic models defined as direct acyclic graphs. These models are made\nup of well-defined primitives that allow to express, in particular, joint\nprobability distributions, Bayesian networks, discrete Markov chains,\nconditioning and probabilistic arithmetic. The Statues algorithm relies on a\nvariable binding mechanism based on the generator construct, a special form of\ncoroutine; being related to the enumeration algorithm, this new algorithm\nbrings important improvements in terms of efficiency, which makes it valuable\nin regard to other exact marginalization algorithms. After introduction of\nseveral definitions, primitives and compositional rules, we present in details\nthe Statues algorithm. Then, we briefly discuss the interest of this algorithm\ncompared to others and we present possible extensions. Finally, we introduce\nLea and MicroLea, two Python libraries implementing the Statues algorithm,\nalong with several use cases. A proof of the correctness of the algorithm is\nprovided in appendix.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jun 2018 23:00:29 GMT"}, {"version": "v2", "created": "Thu, 2 Aug 2018 07:19:02 GMT"}], "update_date": "2020-07-08", "authors_parsed": [["Denis", "Pierre", ""]]}, {"id": "1806.10018", "submitter": "Enrico Malizia", "authors": "Thomas Lukasiewicz, Enrico Malizia", "title": "Complexity Results for Preference Aggregation over (m)CP-nets: Pareto\n  and Majority Voting", "comments": null, "journal-ref": "Artificial Intelligence, vol. 272, pp. 101-142, 2019", "doi": "10.1016/j.artint.2018.12.010", "report-no": null, "categories": "cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combinatorial preference aggregation has many applications in AI. Given the\nexponential nature of these preferences, compact representations are needed and\n($m$)CP-nets are among the most studied ones. Sequential and global voting are\ntwo ways to aggregate preferences over CP-nets. In the former, preferences are\naggregated feature-by-feature. Hence, when preferences have specific feature\ndependencies, sequential voting may exhibit voting paradoxes, i.e., it might\nselect sub-optimal outcomes. To avoid paradoxes in sequential voting, one has\noften assumed the $\\mathcal{O}$-legality restriction, which imposes a shared\ntopological order among all the CP-nets. On the contrary, in global voting,\nCP-nets are considered as a whole during preference aggregation. For this\nreason, global voting is immune from paradoxes, and there is no need to impose\nrestrictions over the CP-nets' topological structure. Sequential voting over\n$\\mathcal{O}$-legal CP-nets has extensively been investigated. On the other\nhand, global voting over non-$\\mathcal{O}$-legal CP-nets has not carefully been\nanalyzed, despite it was stated in the literature that a theoretical comparison\nbetween global and sequential voting was promising and a precise complexity\nanalysis for global voting has been asked for multiple times. In quite few\nworks, very partial results on the complexity of global voting over CP-nets\nhave been given. We start to fill this gap by carrying out a thorough\ncomplexity analysis of Pareto and majority global voting over not necessarily\n$\\mathcal{O}$-legal acyclic binary polynomially connected (m)CP-nets. We settle\nthese problems in the polynomial hierarchy, and some of them in PTIME or\nLOGSPACE, whereas EXPTIME was the previously known upper bound for most of\nthem. We show various tight lower bounds and matching upper bounds for problems\nthat up to date did not have any explicit non-obvious lower bound.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 14:33:15 GMT"}], "update_date": "2019-03-28", "authors_parsed": [["Lukasiewicz", "Thomas", ""], ["Malizia", "Enrico", ""]]}, {"id": "1806.10019", "submitter": "Zhang-Wei Hong", "authors": "Zhang-Wei Hong, Tsu-Jui Fu, Tzu-Yun Shann, Yi-Hsiang Chang, Chun-Yi\n  Lee", "title": "Adversarial Active Exploration for Inverse Dynamics Model Learning", "comments": "Published as a conference paper at CoRL 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an adversarial active exploration for inverse dynamics model\nlearning, a simple yet effective learning scheme that incentivizes exploration\nin an environment without any human intervention. Our framework consists of a\ndeep reinforcement learning (DRL) agent and an inverse dynamics model\ncontesting with each other. The former collects training samples for the\nlatter, with an objective to maximize the error of the latter. The latter is\ntrained with samples collected by the former, and generates rewards for the\nformer when it fails to predict the actual action taken by the former. In such\na competitive setting, the DRL agent learns to generate samples that the\ninverse dynamics model fails to predict correctly, while the inverse dynamics\nmodel learns to adapt to the challenging samples. We further propose a reward\nstructure that ensures the DRL agent to collect only moderately hard samples\nbut not overly hard ones that prevent the inverse model from predicting\neffectively. We evaluate the effectiveness of our method on several robotic arm\nand hand manipulation tasks against multiple baseline models. Experimental\nresults show that our method is comparable to those directly trained with\nexpert demonstrations, and superior to the other baselines even without any\nhuman priors.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 14:33:22 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2020 03:48:55 GMT"}], "update_date": "2020-03-18", "authors_parsed": [["Hong", "Zhang-Wei", ""], ["Fu", "Tsu-Jui", ""], ["Shann", "Tzu-Yun", ""], ["Chang", "Yi-Hsiang", ""], ["Lee", "Chun-Yi", ""]]}, {"id": "1806.10071", "submitter": "Alexander Peysakhovich", "authors": "Adam Lerer and Alexander Peysakhovich", "title": "Learning Existing Social Conventions via Observationally Augmented\n  Self-Play", "comments": "Published in AAAI-AIES2019 - Best Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order for artificial agents to coordinate effectively with people, they\nmust act consistently with existing conventions (e.g. how to navigate in\ntraffic, which language to speak, or how to coordinate with teammates). A\ngroup's conventions can be viewed as a choice of equilibrium in a coordination\ngame. We consider the problem of an agent learning a policy for a coordination\ngame in a simulated environment and then using this policy when it enters an\nexisting group. When there are multiple possible conventions we show that\nlearning a policy via multi-agent reinforcement learning (MARL) is likely to\nfind policies which achieve high payoffs at training time but fail to\ncoordinate with the real group into which the agent enters. We assume access to\na small number of samples of behavior from the true convention and show that we\ncan augment the MARL objective to help it find policies consistent with the\nreal group's convention. In three environments from the literature - traffic,\ncommunication, and team coordination - we observe that augmenting MARL with a\nsmall amount of imitation learning greatly increases the probability that the\nstrategy found by MARL fits well with the existing social convention. We show\nthat this works even in an environment where standard training methods very\nrarely find the true convention of the agent's partners.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 15:46:44 GMT"}, {"version": "v2", "created": "Tue, 4 Sep 2018 15:21:52 GMT"}, {"version": "v3", "created": "Wed, 13 Mar 2019 17:48:23 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Lerer", "Adam", ""], ["Peysakhovich", "Alexander", ""]]}, {"id": "1806.10095", "submitter": "Feng Liu", "authors": "Feng Liu, Yong Shi", "title": "Research on Artificial Intelligence Ethics Based on the Evolution of\n  Population Knowledge Base", "comments": "12 pages, 6 figures,1 table", "journal-ref": "Intelligence Science II. ICIS 2018.IFIP Advances in Information\n  and Communication Technology, vol 539. Springer, Cham", "doi": "10.1007/978-3-030-01313-4_48", "report-no": null, "categories": "cs.OH cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The unclear development direction of human society is a deep reason for that\nit is difficult to form a uniform ethical standard for human society and\nartificial intelligence. Since the 21st century, the latest advances in the\nInternet, brain science and artificial intelligence have brought new\ninspiration to the research on the development direction of human society.\nThrough the study of the Internet brain model, AI IQ evaluation, and the\nevolution of the brain, this paper proposes that the evolution of population\nknowledge base is the key for judging the development direction of human\nsociety, thereby discussing the standards and norms for the construction of\nartificial intelligence ethics.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jun 2018 17:01:27 GMT"}, {"version": "v2", "created": "Wed, 27 Jun 2018 05:27:36 GMT"}, {"version": "v3", "created": "Sun, 18 Nov 2018 04:03:39 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Liu", "Feng", ""], ["Shi", "Yong", ""]]}, {"id": "1806.10130", "submitter": "Zhengxing Chen", "authors": "Zhengxing Chen, Truong-Huy D Nguyen, Yuyu Xu, Chris Amato, Seth\n  Cooper, Yizhou Sun, Magy Seif El-Nasr", "title": "The Art of Drafting: A Team-Oriented Hero Recommendation System for\n  Multiplayer Online Battle Arena Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiplayer Online Battle Arena (MOBA) games have received increasing\npopularity recently. In a match of such games, players compete in two teams of\nfive, each controlling an in-game avatars, known as heroes, selected from a\nroster of more than 100. The selection of heroes, also known as pick or draft,\ntakes place before the match starts and alternates between the two teams until\neach player has selected one hero. Heroes are designed with different strengths\nand weaknesses to promote team cooperation in a game. Intuitively, heroes in a\nstrong team should complement each other's strengths and suppressing those of\nopponents. Hero drafting is therefore a challenging problem due to the complex\nhero-to-hero relationships to consider. In this paper, we propose a novel hero\nrecommendation system that suggests heroes to add to an existing team while\nmaximizing the team's prospect for victory. To that end, we model the drafting\nbetween two teams as a combinatorial game and use Monte Carlo Tree Search\n(MCTS) for estimating the values of hero combinations. Our empirical evaluation\nshows that hero teams drafted by our recommendation algorithm have\nsignificantly higher win rate against teams constructed by other baseline and\nstate-of-the-art strategies.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 17:59:09 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Chen", "Zhengxing", ""], ["Nguyen", "Truong-Huy D", ""], ["Xu", "Yuyu", ""], ["Amato", "Chris", ""], ["Cooper", "Seth", ""], ["Sun", "Yizhou", ""], ["El-Nasr", "Magy Seif", ""]]}, {"id": "1806.10131", "submitter": "Xiaoyang Wang", "authors": "Shujian Yu, Xiaoyang Wang, Jose C. Principe", "title": "Request-and-Reverify: Hierarchical Hypothesis Testing for Concept Drift\n  Detection with Expensive Labels", "comments": "Published as a conference paper at IJCAI 2018", "journal-ref": "Proceedings of the Twenty-Seventh International Joint Conference\n  on Artificial Intelligence (2018) 3033-3039", "doi": "10.24963/ijcai.2018/421", "report-no": "ITD-18-58133N", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One important assumption underlying common classification models is the\nstationarity of the data. However, in real-world streaming applications, the\ndata concept indicated by the joint distribution of feature and label is not\nstationary but drifting over time. Concept drift detection aims to detect such\ndrifts and adapt the model so as to mitigate any deterioration in the model's\npredictive performance. Unfortunately, most existing concept drift detection\nmethods rely on a strong and over-optimistic condition that the true labels are\navailable immediately for all already classified instances. In this paper, a\nnovel Hierarchical Hypothesis Testing framework with Request-and-Reverify\nstrategy is developed to detect concept drifts by requesting labels only when\nnecessary. Two methods, namely Hierarchical Hypothesis Testing with\nClassification Uncertainty (HHT-CU) and Hierarchical Hypothesis Testing with\nAttribute-wise \"Goodness-of-fit\" (HHT-AG), are proposed respectively under the\nnovel framework. In experiments with benchmark datasets, our methods\ndemonstrate overwhelming advantages over state-of-the-art unsupervised drift\ndetectors. More importantly, our methods even outperform DDM (the widely used\nsupervised drift detector) when we use significantly fewer labels.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 22:15:19 GMT"}, {"version": "v2", "created": "Thu, 28 Jun 2018 18:07:13 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Yu", "Shujian", ""], ["Wang", "Xiaoyang", ""], ["Principe", "Jose C.", ""]]}, {"id": "1806.10244", "submitter": "Nitin Yadav", "authors": "Nitin Yadav, Carsten Murawski, Sebastian Sardina, Peter Bossaerts", "title": "Phase transition in the knapsack problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine the phase transition phenomenon for the Knapsack problem from both\na computational and a human perspective. We first provide, via an empirical and\na theoretical analysis, a characterization of the phenomenon in terms of two\ninstance properties; normalised capacity and normalised profit. Then, we show\nevidence that average time spent by human decision makers in solving an\ninstance peaks near the phase transition. Given the ubiquity of the Knapsack\nproblem in every-day life, a better understanding of its structure can improve\nour understanding not only of computational techniques but also of human\nbehavior, including the use and development of heuristics and occurrence of\nbiases.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 23:23:26 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Yadav", "Nitin", ""], ["Murawski", "Carsten", ""], ["Sardina", "Sebastian", ""], ["Bossaerts", "Peter", ""]]}, {"id": "1806.10270", "submitter": "Kartik Ahuja", "authors": "Kartik Ahuja, William Zame, Mihaela van der Schaar", "title": "Optimal Piecewise Local-Linear Approximations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing works on \"black-box\" model interpretation use local-linear\napproximations to explain the predictions made for each data instance in terms\nof the importance assigned to the different features for arriving at the\nprediction. These works provide instancewise explanations and thus give a local\nview of the model. To be able to trust the model it is important to understand\nthe global model behavior and there are relatively fewer works which do the\nsame. Piecewise local-linear models provide a natural way to extend\nlocal-linear models to explain the global behavior of the model. In this work,\nwe provide a dynamic programming based framework to obtain piecewise\napproximations of the black-box model. We also provide provable fidelity, i.e.,\nhow well the explanations reflect the black-box model, guarantees. We carry out\nsimulations on synthetic and real datasets to show the utility of the proposed\napproach. At the end, we show that the ideas developed for our framework can\nalso be used to address the problem of clustering for one-dimensional data. We\ngive a polynomial time algorithm and prove that it achieves optimal clustering.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 02:10:56 GMT"}, {"version": "v2", "created": "Sun, 4 Nov 2018 18:58:51 GMT"}, {"version": "v3", "created": "Fri, 16 Aug 2019 05:21:26 GMT"}, {"version": "v4", "created": "Tue, 27 Aug 2019 16:30:31 GMT"}], "update_date": "2019-08-28", "authors_parsed": [["Ahuja", "Kartik", ""], ["Zame", "William", ""], ["van der Schaar", "Mihaela", ""]]}, {"id": "1806.10282", "submitter": "Haifeng Jin", "authors": "Haifeng Jin, Qingquan Song, Xia Hu", "title": "Auto-Keras: An Efficient Neural Architecture Search System", "comments": "The code of Auto-Keras is available at https://autokeras.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural architecture search (NAS) has been proposed to automatically tune deep\nneural networks, but existing search algorithms, e.g., NASNet, PNAS, usually\nsuffer from expensive computational cost. Network morphism, which keeps the\nfunctionality of a neural network while changing its neural architecture, could\nbe helpful for NAS by enabling more efficient training during the search. In\nthis paper, we propose a novel framework enabling Bayesian optimization to\nguide the network morphism for efficient neural architecture search. The\nframework develops a neural network kernel and a tree-structured acquisition\nfunction optimization algorithm to efficiently explores the search space.\nIntensive experiments on real-world benchmark datasets have been done to\ndemonstrate the superior performance of the developed framework over the\nstate-of-the-art methods. Moreover, we build an open-source AutoML system based\non our method, namely Auto-Keras. The system runs in parallel on CPU and GPU,\nwith an adaptive search strategy for different GPU memory limits.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 03:18:35 GMT"}, {"version": "v2", "created": "Wed, 3 Oct 2018 20:00:49 GMT"}, {"version": "v3", "created": "Tue, 26 Mar 2019 04:38:37 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Jin", "Haifeng", ""], ["Song", "Qingquan", ""], ["Hu", "Xia", ""]]}, {"id": "1806.10293", "submitter": "Alexander Irpan", "authors": "Dmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander\n  Herzog, Eric Jang, Deirdre Quillen, Ethan Holly, Mrinal Kalakrishnan, Vincent\n  Vanhoucke, Sergey Levine", "title": "QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic\n  Manipulation", "comments": "CoRL 2018 camera ready. 23 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of learning vision-based dynamic\nmanipulation skills using a scalable reinforcement learning approach. We study\nthis problem in the context of grasping, a longstanding challenge in robotic\nmanipulation. In contrast to static learning behaviors that choose a grasp\npoint and then execute the desired grasp, our method enables closed-loop\nvision-based control, whereby the robot continuously updates its grasp strategy\nbased on the most recent observations to optimize long-horizon grasp success.\nTo that end, we introduce QT-Opt, a scalable self-supervised vision-based\nreinforcement learning framework that can leverage over 580k real-world grasp\nattempts to train a deep neural network Q-function with over 1.2M parameters to\nperform closed-loop, real-world grasping that generalizes to 96% grasp success\non unseen objects. Aside from attaining a very high success rate, our method\nexhibits behaviors that are quite distinct from more standard grasping systems:\nusing only RGB vision-based perception from an over-the-shoulder camera, our\nmethod automatically learns regrasping strategies, probes objects to find the\nmost effective grasps, learns to reposition objects and perform other\nnon-prehensile pre-grasp manipulations, and responds dynamically to\ndisturbances and perturbations.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 04:34:30 GMT"}, {"version": "v2", "created": "Mon, 2 Jul 2018 19:08:00 GMT"}, {"version": "v3", "created": "Wed, 28 Nov 2018 02:40:54 GMT"}], "update_date": "2018-11-29", "authors_parsed": [["Kalashnikov", "Dmitry", ""], ["Irpan", "Alex", ""], ["Pastor", "Peter", ""], ["Ibarz", "Julian", ""], ["Herzog", "Alexander", ""], ["Jang", "Eric", ""], ["Quillen", "Deirdre", ""], ["Holly", "Ethan", ""], ["Kalakrishnan", "Mrinal", ""], ["Vanhoucke", "Vincent", ""], ["Levine", "Sergey", ""]]}, {"id": "1806.10322", "submitter": "Nicolas Berberich", "authors": "Nicolas Berberich, Klaus Diepold", "title": "The Virtuous Machine - Old Ethics for New Technology?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern AI and robotic systems are characterized by a high and ever-increasing\nlevel of autonomy. At the same time, their applications in fields such as\nautonomous driving, service robotics and digital personal assistants move\ncloser to humans. From the combination of both developments emerges the field\nof AI ethics which recognizes that the actions of autonomous machines entail\nmoral dimensions and tries to answer the question of how we can build moral\nmachines. In this paper we argue for taking inspiration from Aristotelian\nvirtue ethics by showing that it forms a suitable combination with modern AI\ndue to its focus on learning from experience. We furthermore propose that\nimitation learning from moral exemplars, a central concept in virtue ethics,\ncan solve the value alignment problem. Finally, we show that an intelligent\nsystem endowed with the virtues of temperance and friendship to humans would\nnot pose a control problem as it would not have the desire for limitless\nself-improvement.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 07:40:24 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Berberich", "Nicolas", ""], ["Diepold", "Klaus", ""]]}, {"id": "1806.10332", "submitter": "Chi-Hung Hsu", "authors": "Chi-Hung Hsu, Shu-Huan Chang, Jhao-Hong Liang, Hsin-Ping Chou,\n  Chun-Hao Liu, Shih-Chieh Chang, Jia-Yu Pan, Yu-Ting Chen, Wei Wei and\n  Da-Cheng Juan", "title": "MONAS: Multi-Objective Neural Architecture Search using Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies on neural architecture search have shown that automatically\ndesigned neural networks perform as good as expert-crafted architectures. While\nmost existing works aim at finding architectures that optimize the prediction\naccuracy, these architectures may have complexity and is therefore not suitable\nbeing deployed on certain computing environment (e.g., with limited power\nbudgets). We propose MONAS, a framework for Multi-Objective Neural\nArchitectural Search that employs reward functions considering both prediction\naccuracy and other important objectives (e.g., power consumption) when\nsearching for neural network architectures. Experimental results showed that,\ncompared to the state-ofthe-arts, models found by MONAS achieve comparable or\nbetter classification accuracy on computer vision applications, while\nsatisfying the additional objectives such as peak power.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 08:12:01 GMT"}, {"version": "v2", "created": "Mon, 3 Dec 2018 06:54:48 GMT"}], "update_date": "2018-12-04", "authors_parsed": [["Hsu", "Chi-Hung", ""], ["Chang", "Shu-Huan", ""], ["Liang", "Jhao-Hong", ""], ["Chou", "Hsin-Ping", ""], ["Liu", "Chun-Hao", ""], ["Chang", "Shih-Chieh", ""], ["Pan", "Jia-Yu", ""], ["Chen", "Yu-Ting", ""], ["Wei", "Wei", ""], ["Juan", "Da-Cheng", ""]]}, {"id": "1806.10449", "submitter": "Marco Valtorta", "authors": "Mohammad Ali Javidian and Marco Valtorta", "title": "A Proof of the Front-Door Adjustment Formula", "comments": "Six figures and an ancillary document consisting of 53 slides", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a proof of the the Front-Door adjustment formula using the\ndo-calculus.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jun 2018 21:01:03 GMT"}], "update_date": "2018-06-28", "authors_parsed": [["Javidian", "Mohammad Ali", ""], ["Valtorta", "Marco", ""]]}, {"id": "1806.10478", "submitter": "Tommaso Soru", "authors": "Tommaso Soru, Edgard Marx, Andr\\'e Valdestilhas, Diego Esteves, Diego\n  Moussallem, Gustavo Publio", "title": "Neural Machine Translation for Query Construction and Composition", "comments": "ICML workshop on Neural Abstract Machines & Program Induction v2\n  (NAMPI), extended abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on question answering with knowledge base has recently seen an\nincreasing use of deep architectures. In this extended abstract, we study the\napplication of the neural machine translation paradigm for question parsing. We\nemploy a sequence-to-sequence model to learn graph patterns in the SPARQL graph\nquery language and their compositions. Instead of inducing the programs through\nquestion-answer pairs, we expect a semi-supervised approach, where alignments\nbetween questions and queries are built through templates. We argue that the\ncoverage of language utterances can be expanded using late notable works in\nnatural language generation.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 13:40:49 GMT"}, {"version": "v2", "created": "Mon, 9 Jul 2018 14:25:46 GMT"}], "update_date": "2018-07-10", "authors_parsed": [["Soru", "Tommaso", ""], ["Marx", "Edgard", ""], ["Valdestilhas", "Andr\u00e9", ""], ["Esteves", "Diego", ""], ["Moussallem", "Diego", ""], ["Publio", "Gustavo", ""]]}, {"id": "1806.10518", "submitter": "Haris Gacanin", "authors": "Haris Gacanin", "title": "Autonomous Wireless Systems with Artificial Intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses technology and opportunities to embrace artificial\nintelligence (AI) in the design of autonomous wireless systems. We aim to\nprovide readers with motivation and general AI methodology of autonomous agents\nin the context of self-organization in real time by unifying knowledge\nmanagement with sensing, reasoning and active learning. We highlight\ndifferences between training-based methods for matching problems and\ntraining-free methods for environment-specific problems. Finally, we\nconceptually introduce the functions of an autonomous agent with knowledge\nmanagement.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 15:02:08 GMT"}, {"version": "v2", "created": "Tue, 21 May 2019 15:49:54 GMT"}], "update_date": "2019-05-22", "authors_parsed": [["Gacanin", "Haris", ""]]}, {"id": "1806.10561", "submitter": "Liangda Fang", "authors": "Liangda Fang and Kewen Wang and Zhe Wang and Ximing Wen", "title": "Knowledge Compilation in Multi-Agent Epistemic Logics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Epistemic logics are a primary formalism for multi-agent systems but major\nreasoning tasks in such epistemic logics are intractable, which impedes\napplications of multi-agent epistemic logics in automatic planning. Knowledge\ncompilation provides a promising way of resolving the intractability by\nidentifying expressive fragments of epistemic logics that are tractable for\nimportant reasoning tasks such as satisfiability and forgetting. The property\nof logical separability allows to decompose a formula into some of its\nsubformulas and thus modular algorithms for various reasoning tasks can be\ndeveloped. In this paper, by employing logical separability, we propose an\napproach to knowledge compilation for the logic Kn by defining a normal form\nSDNF. Among several novel results, we show that every epistemic formula can be\nequivalently compiled into a formula in SDNF, major reasoning tasks in SDNF are\ntractable, and formulas in SDNF enjoy the logical separability. Our results\nshed some lights on modular approaches to knowledge compilation. Furthermore,\nwe apply our results in the multi-agent epistemic planning. Finally, we extend\nthe above result to the logic K45n that is Kn extended by introspection axioms\n4 and 5.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 16:33:43 GMT"}, {"version": "v2", "created": "Thu, 28 Jun 2018 07:10:40 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Fang", "Liangda", ""], ["Wang", "Kewen", ""], ["Wang", "Zhe", ""], ["Wen", "Ximing", ""]]}, {"id": "1806.10574", "submitter": "Chaofan Chen", "authors": "Chaofan Chen, Oscar Li, Chaofan Tao, Alina Jade Barnett, Jonathan Su,\n  Cynthia Rudin", "title": "This Looks Like That: Deep Learning for Interpretable Image Recognition", "comments": "Chaofan Chen and Oscar Li contributed equally to this work. This work\n  has been accepted for spotlight presentation (top 3% of papers) at NeurIPS\n  2019", "journal-ref": "Advances in Neural Information Processing Systems 32 (NeurIPS\n  2019)", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When we are faced with challenging image classification tasks, we often\nexplain our reasoning by dissecting the image, and pointing out prototypical\naspects of one class or another. The mounting evidence for each of the classes\nhelps us make our final decision. In this work, we introduce a deep network\narchitecture -- prototypical part network (ProtoPNet), that reasons in a\nsimilar way: the network dissects the image by finding prototypical parts, and\ncombines evidence from the prototypes to make a final classification. The model\nthus reasons in a way that is qualitatively similar to the way ornithologists,\nphysicians, and others would explain to people on how to solve challenging\nimage classification tasks. The network uses only image-level labels for\ntraining without any annotations for parts of images. We demonstrate our method\non the CUB-200-2011 dataset and the Stanford Cars dataset. Our experiments show\nthat ProtoPNet can achieve comparable accuracy with its analogous\nnon-interpretable counterpart, and when several ProtoPNets are combined into a\nlarger network, it can achieve an accuracy that is on par with some of the\nbest-performing deep models. Moreover, ProtoPNet provides a level of\ninterpretability that is absent in other interpretable deep models.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 17:18:03 GMT"}, {"version": "v2", "created": "Fri, 30 Nov 2018 16:44:23 GMT"}, {"version": "v3", "created": "Wed, 19 Dec 2018 17:35:02 GMT"}, {"version": "v4", "created": "Fri, 22 Nov 2019 18:10:36 GMT"}, {"version": "v5", "created": "Sat, 28 Dec 2019 20:12:11 GMT"}], "update_date": "2020-01-01", "authors_parsed": [["Chen", "Chaofan", ""], ["Li", "Oscar", ""], ["Tao", "Chaofan", ""], ["Barnett", "Alina Jade", ""], ["Su", "Jonathan", ""], ["Rudin", "Cynthia", ""]]}, {"id": "1806.10698", "submitter": "Yura Perov N", "authors": "Salman Razzaki, Adam Baker, Yura Perov, Katherine Middleton, Janie\n  Baxter, Daniel Mullarkey, Davinder Sangar, Michael Taliercio, Mobasher Butt,\n  Azeem Majeed, Arnold DoRosario, Megan Mahoney, Saurabh Johri", "title": "A comparative study of artificial intelligence and human doctors for the\n  purpose of triage and diagnosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online symptom checkers have significant potential to improve patient care,\nhowever their reliability and accuracy remain variable. We hypothesised that an\nartificial intelligence (AI) powered triage and diagnostic system would compare\nfavourably with human doctors with respect to triage and diagnostic accuracy.\nWe performed a prospective validation study of the accuracy and safety of an AI\npowered triage and diagnostic system. Identical cases were evaluated by both an\nAI system and human doctors. Differential diagnoses and triage outcomes were\nevaluated by an independent judge, who was blinded from knowing the source (AI\nsystem or human doctor) of the outcomes. Independently of these cases,\nvignettes from publicly available resources were also assessed to provide a\nbenchmark to previous studies and the diagnostic component of the MRCGP exam.\nOverall we found that the Babylon AI powered Triage and Diagnostic System was\nable to identify the condition modelled by a clinical vignette with accuracy\ncomparable to human doctors (in terms of precision and recall). In addition, we\nfound that the triage advice recommended by the AI System was, on average,\nsafer than that of human doctors, when compared to the ranges of acceptable\ntriage provided by independent expert judges, with only a minimal reduction in\nappropriateness.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 21:18:37 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Razzaki", "Salman", ""], ["Baker", "Adam", ""], ["Perov", "Yura", ""], ["Middleton", "Katherine", ""], ["Baxter", "Janie", ""], ["Mullarkey", "Daniel", ""], ["Sangar", "Davinder", ""], ["Taliercio", "Michael", ""], ["Butt", "Mobasher", ""], ["Majeed", "Azeem", ""], ["DoRosario", "Arnold", ""], ["Mahoney", "Megan", ""], ["Johri", "Saurabh", ""]]}, {"id": "1806.10729", "submitter": "Niels Justesen", "authors": "Niels Justesen, Ruben Rodriguez Torrado, Philip Bontrager, Ahmed\n  Khalifa, Julian Togelius, Sebastian Risi", "title": "Illuminating Generalization in Deep Reinforcement Learning through\n  Procedural Level Generation", "comments": "Accepted to NeurIPS Deep RL Workshop 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning (RL) has shown impressive results in a variety of\ndomains, learning directly from high-dimensional sensory streams. However, when\nneural networks are trained in a fixed environment, such as a single level in a\nvideo game, they will usually overfit and fail to generalize to new levels.\nWhen RL models overfit, even slight modifications to the environment can result\nin poor agent performance. This paper explores how procedurally generated\nlevels during training can increase generality. We show that for some games\nprocedural level generation enables generalization to new levels within the\nsame distribution. Additionally, it is possible to achieve better performance\nwith less data by manipulating the difficulty of the levels in response to the\nperformance of the agent. The generality of the learned behaviors is also\nevaluated on a set of human-designed levels. The results suggest that the\nability to generalize to human-designed levels highly depends on the design of\nthe level generators. We apply dimensionality reduction and clustering\ntechniques to visualize the generators' distributions of levels and analyze to\nwhat degree they can produce levels similar to those designed by a human.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 01:16:11 GMT"}, {"version": "v2", "created": "Fri, 7 Sep 2018 14:41:25 GMT"}, {"version": "v3", "created": "Sun, 25 Nov 2018 17:54:57 GMT"}, {"version": "v4", "created": "Wed, 28 Nov 2018 11:52:17 GMT"}, {"version": "v5", "created": "Thu, 29 Nov 2018 18:10:13 GMT"}], "update_date": "2018-11-30", "authors_parsed": [["Justesen", "Niels", ""], ["Torrado", "Ruben Rodriguez", ""], ["Bontrager", "Philip", ""], ["Khalifa", "Ahmed", ""], ["Togelius", "Julian", ""], ["Risi", "Sebastian", ""]]}, {"id": "1806.10741", "submitter": "Rakshit Agrawal", "authors": "Rakshit Agrawal, Jack W. Stokes, Mady Marinescu, Karthik Selvaraj", "title": "Robust Neural Malware Detection Models for Emulation Sequence Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Malicious software, or malware, presents a continuously evolving challenge in\ncomputer security. These embedded snippets of code in the form of malicious\nfiles or hidden within legitimate files cause a major risk to systems with\ntheir ability to run malicious command sequences. Malware authors even use\npolymorphism to reorder these commands and create several malicious variations.\nHowever, if executed in a secure environment, one can perform early malware\ndetection on emulated command sequences.\n  The models presented in this paper leverage this sequential data derived via\nemulation in order to perform Neural Malware Detection. These models target the\ncore of the malicious operation by learning the presence and pattern of\nco-occurrence of malicious event actions from within these sequences. Our\nmodels can capture entire event sequences and be trained directly using the\nknown target labels. These end-to-end learning models are powered by two\ncommonly used structures - Long Short-Term Memory (LSTM) Networks and\nConvolutional Neural Networks (CNNs). Previously proposed sequential malware\nclassification models process no more than 200 events. Attackers can evade\ndetection by delaying any malicious activity beyond the beginning of the file.\nWe present specialized models that can handle extremely long sequences while\nsuccessfully performing malware detection in an efficient way. We present an\nimplementation of the Convoluted Partitioning of Long Sequences approach in\norder to tackle this vulnerability and operate on long sequences. We present\nour results on a large dataset consisting of 634,249 file sequences, with\nextremely long file sequences.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 02:28:18 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Agrawal", "Rakshit", ""], ["Stokes", "Jack W.", ""], ["Marinescu", "Mady", ""], ["Selvaraj", "Karthik", ""]]}, {"id": "1806.10755", "submitter": "Peter Sutor Jr.", "authors": "Peter Sutor Jr., Douglas Summers-Stay, and Yiannis Aloimonos", "title": "A Computational Theory for Life-Long Learning of Semantics", "comments": "Submitted and accepted to AGI 2018. Length 10 pages with 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic vectors are learned from data to express semantic relationships\nbetween elements of information, for the purpose of solving and informing\ndownstream tasks. Other models exist that learn to map and classify supervised\ndata. However, the two worlds of learning rarely interact to inform one another\ndynamically, whether across types of data or levels of semantics, in order to\nform a unified model. We explore the research problem of learning these vectors\nand propose a framework for learning the semantics of knowledge incrementally\nand online, across multiple mediums of data, via binary vectors. We discuss the\naspects of this framework to spur future research on this approach and problem.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 03:34:54 GMT"}, {"version": "v2", "created": "Sun, 22 Jul 2018 22:50:34 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Sutor", "Peter", "Jr."], ["Summers-Stay", "Douglas", ""], ["Aloimonos", "Yiannis", ""]]}, {"id": "1806.10758", "submitter": "Sara Hooker", "authors": "Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, Been Kim", "title": "A Benchmark for Interpretability Methods in Deep Neural Networks", "comments": "In NeurIPS 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an empirical measure of the approximate accuracy of feature\nimportance estimates in deep neural networks. Our results across several\nlarge-scale image classification datasets show that many popular\ninterpretability methods produce estimates of feature importance that are not\nbetter than a random designation of feature importance. Only certain ensemble\nbased approaches---VarGrad and SmoothGrad-Squared---outperform such a random\nassignment of importance. The manner of ensembling remains critical, we show\nthat some approaches do no better then the underlying method but carry a far\nhigher computational burden.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 03:46:57 GMT"}, {"version": "v2", "created": "Sat, 2 Mar 2019 21:55:38 GMT"}, {"version": "v3", "created": "Tue, 5 Nov 2019 02:25:30 GMT"}], "update_date": "2019-11-06", "authors_parsed": [["Hooker", "Sara", ""], ["Erhan", "Dumitru", ""], ["Kindermans", "Pieter-Jan", ""], ["Kim", "Been", ""]]}, {"id": "1806.10792", "submitter": "Kazeto Yamamoto", "authors": "Kazeto Yamamoto and Takashi Onishi and Yoshimasa Tsuruoka", "title": "Hierarchical Reinforcement Learning with Abductive Planning", "comments": "7 pages, 6 figures, ICML/IJCAI/AAMAS 2018 Workshop on Planning and\n  Learning (PAL-18)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the key challenges in applying reinforcement learning to real-life\nproblems is that the amount of train-and-error required to learn a good policy\nincreases drastically as the task becomes complex. One potential solution to\nthis problem is to combine reinforcement learning with automated symbol\nplanning and utilize prior knowledge on the domain. However, existing methods\nhave limitations in their applicability and expressiveness. In this paper we\npropose a hierarchical reinforcement learning method based on abductive\nsymbolic planning. The planner can deal with user-defined evaluation functions\nand is not based on the Herbrand theorem. Therefore it can utilize prior\nknowledge of the rewards and can work in a domain where the state space is\nunknown. We demonstrate empirically that our architecture significantly\nimproves learning efficiency with respect to the amount of training examples on\nthe evaluation domain, in which the state space is unknown and there exist\nmultiple goals.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 06:56:19 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Yamamoto", "Kazeto", ""], ["Onishi", "Takashi", ""], ["Tsuruoka", "Yoshimasa", ""]]}, {"id": "1806.10805", "submitter": "Pau Rodr\\'iguez L\\'opez", "authors": "Pau Rodr\\'iguez, Miguel A. Bautista, Jordi Gonz\\`alez, Sergio Escalera", "title": "Beyond One-hot Encoding: lower dimensional target embedding", "comments": "Published at Image and Vision Computing", "journal-ref": null, "doi": "10.1016/j.imavis.2018.04.004", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Target encoding plays a central role when learning Convolutional Neural\nNetworks. In this realm, One-hot encoding is the most prevalent strategy due to\nits simplicity. However, this so widespread encoding schema assumes a flat\nlabel space, thus ignoring rich relationships existing among labels that can be\nexploited during training. In large-scale datasets, data does not span the full\nlabel space, but instead lies in a low-dimensional output manifold. Following\nthis observation, we embed the targets into a low-dimensional space,\ndrastically improving convergence speed while preserving accuracy. Our\ncontribution is two fold: (i) We show that random projections of the label\nspace are a valid tool to find such lower dimensional embeddings, boosting\ndramatically convergence rates at zero computational cost; and (ii) we propose\na normalized eigenrepresentation of the class manifold that encodes the targets\nwith minimal information loss, improving the accuracy of random projections\nencoding while enjoying the same convergence rates. Experiments on CIFAR-100,\nCUB200-2011, Imagenet, and MIT Places demonstrate that the proposed approach\ndrastically improves convergence speed while reaching very competitive accuracy\nrates.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 07:34:14 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Rodr\u00edguez", "Pau", ""], ["Bautista", "Miguel A.", ""], ["Gonz\u00e0lez", "Jordi", ""], ["Escalera", "Sergio", ""]]}, {"id": "1806.10928", "submitter": "Bahare Fatemi", "authors": "Bahare Fatemi, Seyed Mehran Kazemi, David Poole", "title": "Record Linkage to Match Customer Names: A Probabilistic Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consider the following problem: given a database of records indexed by names\n(e.g., name of companies, restaurants, businesses, or universities) and a new\nname, determine whether the new name is in the database, and if so, which\nrecord it refers to. This problem is an instance of record linkage problem and\nis a challenging problem because people do not consistently use the official\nname, but use abbreviations, synonyms, different order of terms, different\nspelling of terms, short form of terms, and the name can contain typos or\nspacing issues. We provide a probabilistic model using relational logistic\nregression to find the probability of each record in the database being the\ndesired record for a given query and find the best record(s) with respect to\nthe probabilities. Building on term-matching and translational approaches for\nsearch, our model addresses many of the aforementioned challenges and provides\ngood results when existing baselines fail. Using the probabilities outputted by\nthe model, we can automate the search process for a portion of queries whose\ndesired documents get a probability higher than a trust threshold. We evaluate\nour model on a large real-world dataset from a telecommunications company and\ncompare it to several state-of-the-art baselines. The obtained results show\nthat our model is a promising probabilistic model for record linkage for names.\nWe also test if the knowledge learned by our model on one domain can be\neffectively transferred to a new domain. For this purpose, we test our model on\nan unseen test set from the business names of the secondString dataset.\nPromising results show that our model can be effectively applied to unseen\ndatasets. Finally, we study the sensitivity of our model to the statistics of\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jun 2018 19:26:47 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Fatemi", "Bahare", ""], ["Kazemi", "Seyed Mehran", ""], ["Poole", "David", ""]]}, {"id": "1806.11078", "submitter": "Yen-Chang Hsu", "authors": "Yen-Chang Hsu, Zhaoyang Lv, Joel Schlosser, Phillip Odom, Zsolt Kira", "title": "A probabilistic constrained clustering for transfer learning and image\n  category discovery", "comments": "CVPR 2018 Deep-Vision Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network-based clustering has recently gained popularity, and in\nparticular a constrained clustering formulation has been proposed to perform\ntransfer learning and image category discovery using deep learning. The core\nidea is to formulate a clustering objective with pairwise constraints that can\nbe used to train a deep clustering network; therefore the cluster assignments\nand their underlying feature representations are jointly optimized end-to-end.\nIn this work, we provide a novel clustering formulation to address scalability\nissues of previous work in terms of optimizing deeper networks and larger\namounts of categories. The proposed objective directly minimizes the negative\nlog-likelihood of cluster assignment with respect to the pairwise constraints,\nhas no hyper-parameters, and demonstrates improved scalability and performance\non both supervised learning and unsupervised transfer learning.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 16:49:19 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Hsu", "Yen-Chang", ""], ["Lv", "Zhaoyang", ""], ["Schlosser", "Joel", ""], ["Odom", "Phillip", ""], ["Kira", "Zsolt", ""]]}, {"id": "1806.11103", "submitter": "Marco Valtorta", "authors": "Mohammad Ali Javidian and Marco Valtorta", "title": "Comment on: Decomposition of structural learning about directed acyclic\n  graphs [1]", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an alternative proof concerning necessary and sufficient\nconditions to split the problem of searching for d-separators and building the\nskeleton of a DAG into small problems for every node of a separation tree T.\nThe proof is simpler than the original [1]. The same proof structure has been\nused in [2] for learning the structure of multivariate regression chain graphs\n(MVR CGs).\n", "versions": [{"version": "v1", "created": "Wed, 27 Jun 2018 19:37:33 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Javidian", "Mohammad Ali", ""], ["Valtorta", "Marco", ""]]}, {"id": "1806.11204", "submitter": "Brendan Juba", "authors": "Brendan Juba", "title": "Polynomial-time probabilistic reasoning with partial observations via\n  implicit learning in probability logics", "comments": "Presented in Eighth International Workshop on Statistical Relational\n  AI (STARAI 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard approaches to probabilistic reasoning require that one possesses an\nexplicit model of the distribution in question. But, the empirical learning of\nmodels of probability distributions from partial observations is a problem for\nwhich efficient algorithms are generally not known. In this work we consider\nthe use of bounded-degree fragments of the \"sum-of-squares\" logic as a\nprobability logic. Prior work has shown that we can decide refutability for\nsuch fragments in polynomial-time. We propose to use such fragments to answer\nqueries about whether a given probability distribution satisfies a given system\nof constraints and bounds on expected values. We show that in answering such\nqueries, such constraints and bounds can be implicitly learned from partial\nobservations in polynomial-time as well. It is known that this logic is capable\nof deriving many bounds that are useful in probabilistic analysis. We show here\nthat it furthermore captures useful polynomial-time fragments of resolution.\nThus, these fragments are also quite expressive.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jun 2018 21:33:34 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Juba", "Brendan", ""]]}, {"id": "1806.11298", "submitter": "Biqing Fang", "authors": "Xiao Huang, Biqing Fang, Hai Wan, Yongmei Liu", "title": "A General Multi-agent Epistemic Planner Based on Higher-order Belief\n  Change", "comments": "One of the authors think it's not appropriate to show this work there\n  days. Then we discussed, we want submit a new work and this one together\n  later", "journal-ref": "IJCAI. (2017) 1093-1101", "doi": "10.24963/ijcai.2017/152", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, multi-agent epistemic planning has received attention from\nboth dynamic logic and planning communities. Existing implementations of\nmulti-agent epistemic planning are based on compilation into classical planning\nand suffer from various limitations, such as generating only linear plans,\nrestriction to public actions, and incapability to handle disjunctive beliefs.\nIn this paper, we propose a general representation language for multi-agent\nepistemic planning where the initial KB and the goal, the preconditions and\neffects of actions can be arbitrary multi-agent epistemic formulas, and the\nsolution is an action tree branching on sensing results. To support efficient\nreasoning in the multi-agent KD45 logic, we make use of a normal form called\nalternating cover disjunctive formulas (ACDFs). We propose basic revision and\nupdate algorithms for ACDFs. We also handle static propositional common\nknowledge, which we call constraints. Based on our reasoning, revision and\nupdate algorithms, adapting the PrAO algorithm for contingent planning from the\nliterature, we implemented a multi-agent epistemic planner called MEPK. Our\nexperimental results show the viability of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 08:26:55 GMT"}, {"version": "v2", "created": "Tue, 14 Aug 2018 14:02:49 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Huang", "Xiao", ""], ["Fang", "Biqing", ""], ["Wan", "Hai", ""], ["Liu", "Yongmei", ""]]}, {"id": "1806.11304", "submitter": "Biqing Fang", "authors": "Liangda Fang, Hai Wan, Xianqiao Liu, Biqing Fang, Zhaorong Lai", "title": "Dependence in Propositional Logic: Formula-Formula Dependence and\n  Formula Forgetting -- Application to Belief Update and Conservative Extension", "comments": "We find a mistake in this version and we need a period of time to fix\n  it", "journal-ref": "AAAI. (2018) 1835-1844", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dependence is an important concept for many tasks in artificial intelligence.\nA task can be executed more efficiently by discarding something independent\nfrom the task. In this paper, we propose two novel notions of dependence in\npropositional logic: formula-formula dependence and formula forgetting. The\nfirst is a relation between formulas capturing whether a formula depends on\nanother one, while the second is an operation that returns the strongest\nconsequence independent of a formula. We also apply these two notions in two\nwell-known issues: belief update and conservative extension. Firstly, we define\na new update operator based on formula-formula dependence. Furthermore, we\nreduce conservative extension to formula forgetting.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 08:35:27 GMT"}, {"version": "v2", "created": "Wed, 12 Jun 2019 08:20:58 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Fang", "Liangda", ""], ["Wan", "Hai", ""], ["Liu", "Xianqiao", ""], ["Fang", "Biqing", ""], ["Lai", "Zhaorong", ""]]}, {"id": "1806.11338", "submitter": "Aswani Kumar Cherukuri Dr", "authors": "Ishwarya M S, Aswani Kumar Cherukuri", "title": "Quantum aspects of high dimensional formal representation of conceptual\n  spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human cognition is a complex process facilitated by the intricate\narchitecture of human brain. However, human cognition is often reduced to\nquantum theory based events in principle because of their correlative\nconjectures for the purpose of analysis for reciprocal understanding. In this\npaper, we begin our analysis of human cognition via formal methods and proceed\ntowards quantum theories. Human cognition often violate classic probabilities\non which formal representation of conceptual spaces are built. Further,\ngeometric representation of conceptual spaces proposed by Gardenfors discusses\nthe underlying content but lacks a systematic approach (Gardenfors, 2000; Kitto\net. al, 2012). However, the aforementioned views are not contradictory but\ndifferent perspective with a gap towards sufficient understanding of human\ncognitive process. A comprehensive and systematic approach to model a\nrelatively complex scenario can be addressed by vector space approach of\nconceptual spaces as discussed in literature. In this research, we have\nproposed an approach that uses both formal representation and Gardenfors\ngeometric approach. The proposed model of high dimensional formal\nrepresentation of conceptual space is mathematically analysed and inferred to\nexhibit quantum aspects. Also, the proposed model achieves cognition, in\nparticular, consciousness. We have demonstrated this process of achieving\nconsciousness with a constructive learning scenario. We have also proposed an\nalgorithm for conceptual scaling of a real world scenario under different\nquality dimensions to obtain a conceptual scale.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 10:35:18 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["S", "Ishwarya M", ""], ["Cherukuri", "Aswani Kumar", ""]]}, {"id": "1806.11379", "submitter": "Andrzej Banburski", "authors": "Tomaso Poggio, Qianli Liao, Brando Miranda, Andrzej Banburski, Xavier\n  Boix and Jack Hidary", "title": "Theory IIIb: Generalization in Deep Networks", "comments": "38 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A main puzzle of deep neural networks (DNNs) revolves around the apparent\nabsence of \"overfitting\", defined in this paper as follows: the expected error\ndoes not get worse when increasing the number of neurons or of iterations of\ngradient descent. This is surprising because of the large capacity demonstrated\nby DNNs to fit randomly labeled data and the absence of explicit\nregularization. Recent results by Srebro et al. provide a satisfying solution\nof the puzzle for linear networks used in binary classification. They prove\nthat minimization of loss functions such as the logistic, the cross-entropy and\nthe exp-loss yields asymptotic, \"slow\" convergence to the maximum margin\nsolution for linearly separable datasets, independently of the initial\nconditions. Here we prove a similar result for nonlinear multilayer DNNs near\nzero minima of the empirical loss. The result holds for exponential-type losses\nbut not for the square loss. In particular, we prove that the weight matrix at\neach layer of a deep network converges to a minimum norm solution up to a scale\nfactor (in the separable case). Our analysis of the dynamical system\ncorresponding to gradient descent of a multilayer network suggests a simple\ncriterion for ranking the generalization performance of different zero\nminimizers of the empirical loss.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 12:39:08 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Poggio", "Tomaso", ""], ["Liao", "Qianli", ""], ["Miranda", "Brando", ""], ["Banburski", "Andrzej", ""], ["Boix", "Xavier", ""], ["Hidary", "Jack", ""]]}, {"id": "1806.11391", "submitter": "Sebastijan Dumancic", "authors": "Sebastijan Dumancic, Alberto Garcia-Duran, Mathias Niepert", "title": "A Comparative Study of Distributional and Symbolic Paradigms for\n  Relational Learning", "comments": "corrected version: incorrect evaluation fixed; IJCAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world domains can be expressed as graphs and, more generally, as\nmulti-relational knowledge graphs. Though reasoning and learning with knowledge\ngraphs has traditionally been addressed by symbolic approaches, recent methods\nin (deep) representation learning has shown promising results for specialized\ntasks such as knowledge base completion. These approaches abandon the\ntraditional symbolic paradigm by replacing symbols with vectors in Euclidean\nspace. With few exceptions, symbolic and distributional approaches are explored\nin different communities and little is known about their respective strengths\nand weaknesses. In this work, we compare representation learning and relational\nlearning on various relational classification and clustering tasks and analyse\nthe complexity of the rules used implicitly by these approaches. Preliminary\nresults reveal possible indicators that could help in choosing one approach\nover the other for particular knowledge graphs.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 13:01:24 GMT"}, {"version": "v2", "created": "Mon, 2 Jul 2018 09:03:54 GMT"}, {"version": "v3", "created": "Thu, 31 Oct 2019 10:52:15 GMT"}, {"version": "v4", "created": "Tue, 24 Mar 2020 17:59:22 GMT"}], "update_date": "2020-03-25", "authors_parsed": [["Dumancic", "Sebastijan", ""], ["Garcia-Duran", "Alberto", ""], ["Niepert", "Mathias", ""]]}, {"id": "1806.11401", "submitter": "Amit Mishra", "authors": "Amit Kumar Mishra", "title": "WEBCA: Weakly-Electric-Fish Bioinspired Cognitive Architecture", "comments": "To be published in Annual Bio-Inspired Cognitive Architecture (BICA)\n  Conference 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuroethology has been an active field of study for more than a century now.\nOut of some of the most interesting species that has been studied so far,\nweakly electric fish is a fascinating one. It performs communication,\necho-location and inter-species detection efficiently with an interesting\nconfiguration of sensors, neu-rons and a simple brain. In this paper we propose\na cognitive architecture inspired by the way these fishes handle and process\ninformation. We believe that it is eas-ier to understand and mimic the neural\narchitectures of a simpler species than that of human. Hence, the proposed\narchitecture is expected to both help research in cognitive robotics and also\nhelp understand more complicated brains like that of human beings.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 13:22:37 GMT"}], "update_date": "2018-07-02", "authors_parsed": [["Mishra", "Amit Kumar", ""]]}, {"id": "1806.11463", "submitter": "Alejandro Pozas-Kerstjens", "authors": "Zhikuan Zhao, Alejandro Pozas-Kerstjens, Patrick Rebentrost, Peter\n  Wittek", "title": "Bayesian Deep Learning on a Quantum Computer", "comments": "11 pages, 3 figures. RevTeX 4.1. Code is available at\n  https://gitlab.com/apozas/bayesian-dl-quantum/ V3: Updated to match published\n  version", "journal-ref": "Quantum Machine Intelligence 1, 4 (2019)", "doi": "10.1007/s42484-019-00004-7", "report-no": null, "categories": "quant-ph cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian methods in machine learning, such as Gaussian processes, have great\nadvantages com-pared to other techniques. In particular, they provide estimates\nof the uncertainty associated with a prediction. Extending the Bayesian\napproach to deep architectures has remained a major challenge. Recent results\nconnected deep feedforward neural networks with Gaussian processes, allowing\ntraining without backpropagation. This connection enables us to leverage a\nquantum algorithm designed for Gaussian processes and develop a new algorithm\nfor Bayesian deep learning on quantum computers. The properties of the kernel\nmatrix in the Gaussian process ensure the efficient execution of the core\ncomponent of the protocol, quantum matrix inversion, providing an at least\npolynomial speedup over classical algorithms. Furthermore, we demonstrate the\nexecution of the algorithm on contemporary quantum computers and analyze its\nrobustness with respect to realistic noise models.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jun 2018 15:08:45 GMT"}, {"version": "v2", "created": "Mon, 9 Jul 2018 12:13:47 GMT"}, {"version": "v3", "created": "Fri, 17 May 2019 07:51:29 GMT"}], "update_date": "2019-05-20", "authors_parsed": [["Zhao", "Zhikuan", ""], ["Pozas-Kerstjens", "Alejandro", ""], ["Rebentrost", "Patrick", ""], ["Wittek", "Peter", ""]]}, {"id": "1806.11555", "submitter": "Marcelo Fernandes", "authors": "Matheus F. Torquato and Marcelo A. C. Fernandes", "title": "High-Performance Parallel Implementation of Genetic Algorithm on FPGA", "comments": "27 pages, 16 figures", "journal-ref": null, "doi": "10.1007/s00034-019-01037-w", "report-no": null, "categories": "cs.DC cs.AI cs.AR eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Genetic Algorithms (GAs) are used to solve search and optimization problems\nin which an optimal solution can be found using an iterative process with\nprobabilistic and non-deterministic transitions. However, depending on the\nproblem's nature, the time required to find a solution can be high in\nsequential machines due to the computational complexity of genetic algorithms.\nThis work proposes a parallel implementation of a genetic algorithm on\nfield-programmable gate array (FPGA). Optimization of the system's processing\ntime is the main goal of this project. Results associated with the processing\ntime and area occupancy (on FPGA) for various population sizes are analyzed.\nStudies concerning the accuracy of the GA response for the optimization of two\nvariables functions were also evaluated for the hardware implementation.\nHowever, the high-performance implementation proposes in this paper is able to\nwork with more variable from some adjustments on hardware architecture.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jun 2018 18:30:27 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Torquato", "Matheus F.", ""], ["Fernandes", "Marcelo A. C.", ""]]}]