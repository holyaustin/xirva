[{"id": "1203.0088", "submitter": "Panigrahy Rina", "authors": "Rina Panigrahy, Li Zhang", "title": "The Mind Grows Circuits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a vast supply of prior art that study models for mental processes.\nSome studies in psychology and philosophy approach it from an inner perspective\nin terms of experiences and percepts. Others such as neurobiology or\nconnectionist-machines approach it externally by viewing the mind as complex\ncircuit of neurons where each neuron is a primitive binary circuit. In this\npaper, we also model the mind as a place where a circuit grows, starting as a\ncollection of primitive components at birth and then builds up incrementally in\na bottom up fashion. A new node is formed by a simple composition of prior\nnodes when we undergo a repeated experience that can be described by that\ncomposition. Unlike neural networks, however, these circuits take \"concepts\" or\n\"percepts\" as inputs and outputs. Thus the growing circuits can be likened to a\ngrowing collection of lambda expressions that are built on top of one another\nin an attempt to compress the sensory input as a heuristic to bound its\nKolmogorov Complexity.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2012 04:20:03 GMT"}, {"version": "v2", "created": "Sat, 17 Mar 2012 19:32:10 GMT"}], "update_date": "2012-03-20", "authors_parsed": [["Panigrahy", "Rina", ""], ["Zhang", "Li", ""]]}, {"id": "1203.0202", "submitter": "Aleks Kissinger", "authors": "Aleks Kissinger", "title": "Pictures of Processes: Automated Graph Rewriting for Monoidal Categories\n  and Applications to Quantum Computing", "comments": "PhD Thesis. Passed examination. Minor corrections made and one\n  theorem added at the end of Chapter 5. 182 pages, ~300 figures. See full text\n  for unabridged abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CT cs.AI quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is about diagrammatic languages, how they can be represented, and\nwhat they in turn can be used to represent. More specifically, it focuses on\nrepresentations and applications of string diagrams. String diagrams are used\nto represent a collection of processes, depicted as \"boxes\" with multiple\n(typed) inputs and outputs, depicted as \"wires\". If we allow plugging input and\noutput wires together, we can intuitively represent complex compositions of\nprocesses, formalised as morphisms in a monoidal category.\n  [...] The first major contribution of this dissertation is the introduction\nof a discretised version of a string diagram called a string graph. String\ngraphs form a partial adhesive category, so they can be manipulated using\ndouble-pushout graph rewriting. Furthermore, we show how string graphs modulo a\nrewrite system can be used to construct free symmetric traced and compact\nclosed categories on a monoidal signature.\n  The second contribution is in the application of graphical languages to\nquantum information theory. We use a mixture of diagrammatic and algebraic\ntechniques to prove a new classification result for strongly complementary\nobservables. [...] We also introduce a graphical language for multipartite\nentanglement and illustrate a simple graphical axiom that distinguishes the two\nmaximally-entangled tripartite qubit states: GHZ and W. [...]\n  The third contribution is a description of two software tools developed in\npart by the author to implement much of the theoretical content described here.\nThe first tool is Quantomatic, a desktop application for building string graphs\nand graphical theories, as well as performing automated graph rewriting\nvisually. The second is QuantoCoSy, which performs fully automated,\nmodel-driven theory creation using a procedure called conjecture synthesis.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2012 14:45:09 GMT"}, {"version": "v2", "created": "Thu, 22 Mar 2012 10:57:10 GMT"}], "update_date": "2012-03-23", "authors_parsed": [["Kissinger", "Aleks", ""]]}, {"id": "1203.0220", "submitter": "Dov Gabbay", "authors": "Dov M. Gabbay", "title": "The Equational Approach to CF2 Semantics", "comments": "36 pages, version dated 15 February 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a family of new equational semantics for argumentation networks\nwhich can handle odd and even loops in a uniform manner. We offer one version\nof equational semantics which is equivalent to CF2 semantics, and a better\nversion which gives the same results as traditional Dung semantics for even\nloops but can still handle odd loops.\n", "versions": [{"version": "v1", "created": "Thu, 1 Mar 2012 15:45:31 GMT"}], "update_date": "2012-03-02", "authors_parsed": [["Gabbay", "Dov M.", ""]]}, {"id": "1203.0436", "submitter": "Rob Arthan", "authors": "Rob Arthan, Paulo Oliva", "title": "(Dual) Hoops Have Unique Halving", "comments": "17 pages, 5 figures, published as a chapter in the Bill McCune\n  Memorial Festschrift", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous logic extends the multi-valued Lukasiewicz logic by adding a\nhalving operator on propositions. This extension is designed to give a more\nsatisfactory model theory for continuous structures. The semantics of these\nlogics can be given using specialisations of algebraic structures known as\nhoops. As part of an investigation into the metatheory of propositional\ncontinuous logic, we were indebted to Prover9 for finding a proof of an\nimportant algebraic law.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2012 12:14:20 GMT"}, {"version": "v2", "created": "Mon, 14 Oct 2013 19:25:26 GMT"}], "update_date": "2013-10-15", "authors_parsed": [["Arthan", "Rob", ""], ["Oliva", "Paulo", ""]]}, {"id": "1203.0504", "submitter": "Martin Bachwerk", "authors": "Martin Bachwerk and Carl Vogel", "title": "Modelling Social Structures and Hierarchies in Language Evolution", "comments": "14 pages, 3 figures, 1 table. In proceedings of AI-2010, The\n  Thirtieth SGAI International Conference on Innovative Techniques and\n  Applications of Artificial Intelligence, Cambridge, England, UK, 14-16\n  December 2010", "journal-ref": "Research and Development in Intelligent Systems XXVII, 2011, pp.\n  49-62", "doi": "10.1007/978-0-85729-130-1_4", "report-no": null, "categories": "cs.CL cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Language evolution might have preferred certain prior social configurations\nover others. Experiments conducted with models of different social structures\n(varying subgroup interactions and the role of a dominant interlocutor) suggest\nthat having isolated agent groups rather than an interconnected agent is more\nadvantageous for the emergence of a social communication system. Distinctive\ngroups that are closely connected by communication yield systems less like\nnatural language than fully isolated groups inhabiting the same world.\nFurthermore, the addition of a dominant male who is asymmetrically favoured as\na hearer, and equally likely to be a speaker has no positive influence on the\ndisjoint groups.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2012 16:03:41 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Bachwerk", "Martin", ""], ["Vogel", "Carl", ""]]}, {"id": "1203.0512", "submitter": "Martin Bachwerk", "authors": "Martin Bachwerk and Carl Vogel", "title": "Establishing linguistic conventions in task-oriented primeval dialogue", "comments": "8 pages, 5 figures. In proceedings of the COST 2102 International\n  Conference, Budapest, Hungary, September 7-10, 2010, Revised Selected Papers", "journal-ref": "Analysis of Verbal and Nonverbal Communication and Enactment. The\n  Processing Issues. Lecture Notes in Computer Science, Volume 6800/2011, 2011,\n  pp. 48-55", "doi": "10.1007/978-3-642-25775-9_4", "report-no": null, "categories": "cs.CL cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we claim that language is likely to have emerged as a\nmechanism for coordinating the solution of complex tasks. To confirm this\nthesis, computer simulations are performed based on the coordination task\npresented by Garrod & Anderson (1987). The role of success in task-oriented\ndialogue is analytically evaluated with the help of performance measurements\nand a thorough lexical analysis of the emergent communication system.\nSimulation results confirm a strong effect of success mattering on both\nreliability and dispersion of linguistic conventions.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2012 16:25:13 GMT"}], "update_date": "2012-03-05", "authors_parsed": [["Bachwerk", "Martin", ""], ["Vogel", "Carl", ""]]}, {"id": "1203.0550", "submitter": "Afshin Rostamizadeh", "authors": "Corinna Cortes, Mehryar Mohri, Afshin Rostamizadeh", "title": "Algorithms for Learning Kernels Based on Centered Alignment", "comments": null, "journal-ref": "Journal of Machine Learning Research 13 (2012) 795-828", "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents new and effective algorithms for learning kernels. In\nparticular, as shown by our empirical results, these algorithms consistently\noutperform the so-called uniform combination solution that has proven to be\ndifficult to improve upon in the past, as well as other algorithms for learning\nkernels based on convex combinations of base kernels in both classification and\nregression. Our algorithms are based on the notion of centered alignment which\nis used as a similarity measure between kernels or kernel matrices. We present\na number of novel algorithmic, theoretical, and empirical results for learning\nkernels based on our notion of centered alignment. In particular, we describe\nefficient algorithms for learning a maximum alignment kernel by showing that\nthe problem can be reduced to a simple QP and discuss a one-stage algorithm for\nlearning both a kernel and a hypothesis based on that kernel using an\nalignment-based regularization. Our theoretical results include a novel\nconcentration bound for centered alignment between kernel matrices, the proof\nof the existence of effective predictors for kernels with high alignment, both\nfor classification and for regression, and the proof of stability-based\ngeneralization bounds for a broad family of algorithms for learning kernels\nbased on centered alignment. We also report the results of experiments with our\ncentered alignment-based algorithms in both classification and regression.\n", "versions": [{"version": "v1", "created": "Fri, 2 Mar 2012 19:20:42 GMT"}, {"version": "v2", "created": "Tue, 8 Apr 2014 18:30:21 GMT"}], "update_date": "2014-04-09", "authors_parsed": [["Cortes", "Corinna", ""], ["Mohri", "Mehryar", ""], ["Rostamizadeh", "Afshin", ""]]}, {"id": "1203.0648", "submitter": "Mark Levin", "authors": "Mark Sh. Levin", "title": "Towards Electronic Shopping of Composite Product", "comments": "10 pages, 20 figures, 17 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the paper, frameworks for electronic shopping of composite (modular)\nproducts are described: (a) multicriteria selection (product is considered as a\nwhole system, it is a traditional approach), (b) combinatorial synthesis\n(composition) of the product from its components, (c) aggregation of the\nproduct from several selected products/prototypes. The following product model\nis examined: (i) general tree-like structure, (ii) set of system\nparts/components (leaf nodes), (iii) design alternatives (DAs) for each\ncomponent, (iv) ordinal priorities for DAs, and (v) estimates of compatibility\nbetween DAs for different components. The combinatorial synthesis is realized\nas morphological design of a composite (modular) product or an extended\ncomposite product (e.g., product and support services as financial\ninstruments). Here the solving process is based on Hierarchical Morphological\nMulticriteria Design (HMMD): (i) multicriteria selection of alternatives for\nsystem parts, (ii) composing the selected alternatives into a resultant\ncombination (while taking into account ordinal quality of the alternatives\nabove and their compatibility). The aggregation framework is based on\nconsideration of aggregation procedures, for example: (i) addition procedure:\ndesign of a products substructure or an extended substructure ('kernel') and\naddition of elements, and (ii) design procedure: design of the composite\nsolution based on all elements of product superstructure. Applied numerical\nexamples (e.g., composite product, extended composite product, product repair\nplan, and product trajectory) illustrate the proposed approaches.\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2012 12:56:03 GMT"}], "update_date": "2012-03-06", "authors_parsed": [["Levin", "Mark Sh.", ""]]}, {"id": "1203.0656", "submitter": "Ahmed Maalel", "authors": "Ahmed Maalel and Habib Hadj-Mabrouk", "title": "Contribution of Case Based Reasoning (CBR) in the Exploitation of Return\n  of Experience. Application to Accident Scenarii in Railroad Transport", "comments": "Paper Award, 8 pages, 10 figures,3rd International Conference on\n  Information Systems and Economic Intelligence, 18-20 february 2010, Sousse,\n  Tunisia. http://siie2010.loria.fr/", "journal-ref": "Int. Conf. Info. Sys. Eco. Intelligence. (2010) 1-8", "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study is from a base of accident scenarii in rail transport (feedback) in\norder to develop a tool to share build and sustain knowledge and safety and\nsecondly to exploit the knowledge stored to prevent the reproduction of\naccidents / incidents. This tool should ultimately lead to the proposal of\nprevention and protection measures to minimize the risk level of a new\ntransport system and thus to improve safety. The approach to achieving this\ngoal largely depends on the use of artificial intelligence techniques and\nrarely the use of a method of automatic learning in order to develop a\nfeasibility model of a software tool based on case based reasoning (CBR) to\nexploit stored knowledge in order to create know-how that can help stimulate\ndomain experts in the task of analysis, evaluation and certification of a new\nsystem.\n", "versions": [{"version": "v1", "created": "Sat, 3 Mar 2012 14:41:27 GMT"}], "update_date": "2012-03-07", "authors_parsed": [["Maalel", "Ahmed", ""], ["Hadj-Mabrouk", "Habib", ""]]}, {"id": "1203.0697", "submitter": "Animashree Anandkumar", "authors": "A. Anandkumar, D. Hsu, F. Huang and S. M. Kakade", "title": "Learning High-Dimensional Mixtures of Graphical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider unsupervised estimation of mixtures of discrete graphical models,\nwhere the class variable corresponding to the mixture components is hidden and\neach mixture component over the observed variables can have a potentially\ndifferent Markov graph structure and parameters. We propose a novel approach\nfor estimating the mixture components, and our output is a tree-mixture model\nwhich serves as a good approximation to the underlying graphical model mixture.\nOur method is efficient when the union graph, which is the union of the Markov\ngraphs of the mixture components, has sparse vertex separators between any pair\nof observed variables. This includes tree mixtures and mixtures of bounded\ndegree graphs. For such models, we prove that our method correctly recovers the\nunion graph structure and the tree structures corresponding to\nmaximum-likelihood tree approximations of the mixture components. The sample\nand computational complexities of our method scale as $\\poly(p, r)$, for an\n$r$-component mixture of $p$-variate graphical models. We further extend our\nresults to the case when the union graph has sparse local separators between\nany pair of observed variables, such as mixtures of locally tree-like graphs,\nand the mixture components are in the regime of correlation decay.\n", "versions": [{"version": "v1", "created": "Sun, 4 Mar 2012 01:19:25 GMT"}, {"version": "v2", "created": "Sat, 30 Jun 2012 18:54:30 GMT"}], "update_date": "2012-07-03", "authors_parsed": [["Anandkumar", "A.", ""], ["Hsu", "D.", ""], ["Huang", "F.", ""], ["Kakade", "S. M.", ""]]}, {"id": "1203.0699", "submitter": "Joseph Y. Halpern", "authors": "Joseph Y. Halpern and Willemien Kets", "title": "Ambiguous Language and Differences in Beliefs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard models of multi-agent modal logic do not capture the fact that\ninformation is often ambiguous, and may be interpreted in different ways by\ndifferent agents. We propose a framework that can model this, and consider\ndifferent semantics that capture different assumptions about the agents'\nbeliefs regarding whether or not there is ambiguity. We consider the impact of\nambiguity on a seminal result in economics: Aumann's result saying that agents\nwith a common prior cannot agree to disagree. This result is known not to hold\nif agents do not have a common prior; we show that it also does not hold in the\npresence of ambiguity. We then consider the tradeoff between assuming a common\ninterpretation (i.e., no ambiguity) and a common prior (i.e., shared initial\nbeliefs).\n", "versions": [{"version": "v1", "created": "Sun, 4 Mar 2012 01:55:09 GMT"}], "update_date": "2012-03-06", "authors_parsed": [["Halpern", "Joseph Y.", ""], ["Kets", "Willemien", ""]]}, {"id": "1203.0876", "submitter": "Subhadip Basu", "authors": "Subhadip Basu, Nibaran Das, Ram Sarkar, Mahantapas Kundu, Mita\n  Nasipuri, Dipak Kumar Basu", "title": "An MLP based Approach for Recognition of Handwritten `Bangla' Numerals", "comments": null, "journal-ref": "Proc. 2nd Indian International Conference on Artificial\n  Intelligence, pp. 407-417, Dec. 2005, Pune", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The work presented here involves the design of a Multi Layer Perceptron (MLP)\nbased pattern classifier for recognition of handwritten Bangla digits using a\n76 element feature vector. Bangla is the second most popular script and\nlanguage in the Indian subcontinent and the fifth most popular language in the\nworld. The feature set developed for representing handwritten Bangla numerals\nhere includes 24 shadow features, 16 centroid features and 36 longest-run\nfeatures. On experimentation with a database of 6000 samples, the technique\nyields an average recognition rate of 96.67% evaluated after three-fold cross\nvalidation of results. It is useful for applications related to OCR of\nhandwritten Bangla Digit and can also be extended to include OCR of handwritten\ncharacters of Bangla alphabet.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2012 12:06:54 GMT"}], "update_date": "2012-03-06", "authors_parsed": [["Basu", "Subhadip", ""], ["Das", "Nibaran", ""], ["Sarkar", "Ram", ""], ["Kundu", "Mahantapas", ""], ["Nasipuri", "Mita", ""], ["Basu", "Dipak Kumar", ""]]}, {"id": "1203.0882", "submitter": "Subhadip Basu", "authors": "Subhadip Basu, Nibaran Das, Ram Sarkar, Mahantapas Kundu, Mita\n  Nasipuri, Dipak Kumar Basu", "title": "Handwritten Bangla Alphabet Recognition using an MLP Based Classifier", "comments": null, "journal-ref": "Proc. of the 2nd National Conf. on Computer Processing of Bangla,\n  pp. 285-291, Feb-2005, Dhaka", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The work presented here involves the design of a Multi Layer Perceptron (MLP)\nbased classifier for recognition of handwritten Bangla alphabet using a 76\nelement feature set Bangla is the second most popular script and language in\nthe Indian subcontinent and the fifth most popular language in the world. The\nfeature set developed for representing handwritten characters of Bangla\nalphabet includes 24 shadow features, 16 centroid features and 36 longest-run\nfeatures. Recognition performances of the MLP designed to work with this\nfeature set are experimentally observed as 86.46% and 75.05% on the samples of\nthe training and the test sets respectively. The work has useful application in\nthe development of a complete OCR system for handwritten Bangla text.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2012 12:22:23 GMT"}], "update_date": "2012-03-06", "authors_parsed": [["Basu", "Subhadip", ""], ["Das", "Nibaran", ""], ["Sarkar", "Ram", ""], ["Kundu", "Mahantapas", ""], ["Nasipuri", "Mita", ""], ["Basu", "Dipak Kumar", ""]]}, {"id": "1203.1007", "submitter": "Stephane Ross", "authors": "Stephane Ross, J. Andrew Bagnell", "title": "Agnostic System Identification for Model-Based Reinforcement Learning", "comments": "8 pages, published in ICML 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SY stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental problem in control is to learn a model of a system from\nobservations that is useful for controller synthesis. To provide good\nperformance guarantees, existing methods must assume that the real system is in\nthe class of models considered during learning. We present an iterative method\nwith strong guarantees even in the agnostic case where the system is not in the\nclass. In particular, we show that any no-regret online learning algorithm can\nbe used to obtain a near-optimal policy, provided some model achieves low\ntraining error and access to a good exploration distribution. Our approach\napplies to both discrete and continuous domains. We demonstrate its efficacy\nand scalability on a challenging helicopter domain from the literature.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2012 18:58:49 GMT"}, {"version": "v2", "created": "Tue, 3 Jul 2012 13:48:40 GMT"}], "update_date": "2012-07-04", "authors_parsed": [["Ross", "Stephane", ""], ["Bagnell", "J. Andrew", ""]]}, {"id": "1203.1021", "submitter": "Ahmed Maalel", "authors": "Ahmed Maalel, Habib Hadj mabrouk, Lassad Mejri and Henda Hajjami Ben\n  Ghezela", "title": "Development of an Ontology to Assist the Modeling of Accident Scenarii\n  \"Application on Railroad Transport \"", "comments": "7 pages, 9 figures, Journal of Computing (ISSN 2151-9617); Journal of\n  Computing, Volume 3, Issue 7, July 2011", "journal-ref": "J. of Computing. 3. 7. (2011) 1-8", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a world where communication and information sharing are at the heart of\nour business, the terminology needs are most pressing. It has become imperative\nto identify the terms used and defined in a consensual and coherent way while\npreserving linguistic diversity. To streamline and strengthen the process of\nacquisition, representation and exploitation of scenarii of train accidents, it\nis necessary to harmonize and standardize the terminology used by players in\nthe security field. The research aims to significantly improve analytical\nactivities and operations of the various safety studies, by tracking the error\nin system, hardware, software and human. This paper presents the contribution\nof ontology to modeling scenarii for rail accidents through a knowledge model\nbased on a generic ontology and domain ontology. After a detailed presentation\nof the state of the art material, this article presents the first results of\nthe developed model.\n", "versions": [{"version": "v1", "created": "Mon, 5 Mar 2012 19:45:43 GMT"}], "update_date": "2012-03-07", "authors_parsed": [["Maalel", "Ahmed", ""], ["mabrouk", "Habib Hadj", ""], ["Mejri", "Lassad", ""], ["Ghezela", "Henda Hajjami Ben", ""]]}, {"id": "1203.1095", "submitter": "Guido Tack", "authors": "Tom Schrijvers, Guido Tack, Pieter Wuille, Horst Samulowitz, Peter J.\n  Stuckey", "title": "Search Combinators", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to model search in a constraint solver can be an essential asset\nfor solving combinatorial problems. However, existing infrastructure for\ndefining search heuristics is often inadequate. Either modeling capabilities\nare extremely limited or users are faced with a general-purpose programming\nlanguage whose features are not tailored towards writing search heuristics. As\na result, major improvements in performance may remain unexplored.\n  This article introduces search combinators, a lightweight and\nsolver-independent method that bridges the gap between a conceptually simple\nmodeling language for search (high-level, functional and naturally\ncompositional) and an efficient implementation (low-level, imperative and\nhighly non-modular). By allowing the user to define application-tailored search\nstrategies from a small set of primitives, search combinators effectively\nprovide a rich domain-specific language (DSL) for modeling search to the user.\nRemarkably, this DSL comes at a low implementation cost to the developer of a\nconstraint solver.\n  The article discusses two modular implementation approaches and shows, by\nempirical evaluation, that search combinators can be implemented without\noverhead compared to a native, direct implementation in a constraint solver.\n", "versions": [{"version": "v1", "created": "Tue, 6 Mar 2012 03:59:34 GMT"}], "update_date": "2012-03-07", "authors_parsed": [["Schrijvers", "Tom", ""], ["Tack", "Guido", ""], ["Wuille", "Pieter", ""], ["Samulowitz", "Horst", ""], ["Stuckey", "Peter J.", ""]]}, {"id": "1203.1882", "submitter": "Ganti Meenakshi", "authors": "G.Meenakshi", "title": "Multi source feedback based performance appraisal system using Fuzzy\n  logic decision support system", "comments": "16 pages", "journal-ref": null, "doi": "10.5121/ijsc.2012.3108", "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In Multi-Source Feedback or 360 Degree Feedback, data on the performance of\nan individual are collected systematically from a number of stakeholders and\nare used for improving performance. The 360-Degree Feedback approach provides a\nconsistent management philosophy meeting the criterion outlined previously. The\n360-degree feedback appraisal process describes a human resource methodology\nthat is frequently used for both employee appraisal and employee development.\nUsed in employee performance appraisals, the 360-degree feedback methodology is\ndifferentiated from traditional, top-down appraisal methods in which the\nsupervisor responsible for the appraisal provides the majority of the data.\nInstead it seeks to use information gained from other sources to provide a\nfuller picture of employees' performances. Similarly, when this technique used\nin employee development it augments employees' perceptions of training needs\nwith those of the people with whom they interact. The 360-degree feedback based\nappraisal is a comprehensive method where in the feedback about the employee\ncomes from all the sources that come into contact with the employee on his/her\njob. The respondents for an employee can be her/his peers, managers,\nsubordinates team members, customers, suppliers and vendors. Hence anyone who\ncomes into contact with the employee, the 360 degree appraisal has four\ncomponents that include self-appraisal, superior's appraisal, subordinate's\nappraisal student's appraisal and peer's appraisal .The proposed system is an\nattempt to implement the 360 degree feedback based appraisal system in\nacademics especially engineering colleges.\n", "versions": [{"version": "v1", "created": "Thu, 8 Mar 2012 18:44:46 GMT"}], "update_date": "2012-03-09", "authors_parsed": [["Meenakshi", "G.", ""]]}, {"id": "1203.2200", "submitter": "Ryan Rossi", "authors": "Ryan Rossi, Brian Gallagher, Jennifer Neville, Keith Henderson", "title": "Role-Dynamics: Fast Mining of Large Dynamic Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To understand the structural dynamics of a large-scale social, biological or\ntechnological network, it may be useful to discover behavioral roles\nrepresenting the main connectivity patterns present over time. In this paper,\nwe propose a scalable non-parametric approach to automatically learn the\nstructural dynamics of the network and individual nodes. Roles may represent\nstructural or behavioral patterns such as the center of a star, peripheral\nnodes, or bridge nodes that connect different communities. Our novel approach\nlearns the appropriate structural role dynamics for any arbitrary network and\ntracks the changes over time. In particular, we uncover the specific global\nnetwork dynamics and the local node dynamics of a technological, communication,\nand social network. We identify interesting node and network patterns such as\nstationary and non-stationary roles, spikes/steps in role-memberships (perhaps\nindicating anomalies), increasing/decreasing role trends, among many others.\nOur results indicate that the nodes in each of these networks have distinct\nconnectivity patterns that are non-stationary and evolve considerably over\ntime. Overall, the experiments demonstrate the effectiveness of our approach\nfor fast mining and tracking of the dynamics in large networks. Furthermore,\nthe dynamic structural representation provides a basis for building more\nsophisticated models and tools that are fast for exploring large dynamic\nnetworks.\n", "versions": [{"version": "v1", "created": "Fri, 9 Mar 2012 22:45:34 GMT"}], "update_date": "2012-03-13", "authors_parsed": [["Rossi", "Ryan", ""], ["Gallagher", "Brian", ""], ["Neville", "Jennifer", ""], ["Henderson", "Keith", ""]]}, {"id": "1203.2315", "submitter": "Sergey Tarasenko", "authors": "Sergey Tarasenko", "title": "Modeling multistage decision processes with Reflexive Game Theory", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces application of Reflexive Game Theory to the matter of\nmultistage decision making processes. The idea behind is that each decision\nmaking session has certain parameters like \"when the session is taking place\",\n\"who are the group members to make decision\", \"how group members influence on\neach other\", etc. This study illustrates the consecutive or sequential decision\nmaking process, which consist of two stages. During the stage 1 decisions about\nthe parameters of the ultimate decision making are made. Then stage 2 is\nimplementation of Ultimate decision making itself. Since during stage 1 there\ncan be multiple decision sessions. In such a case it takes more than two\nsessions to make ultimate (final) decision. Therefore the overall process of\nultimate decision making becomes multistage decision making process consisting\nof consecutive decision making sessions.\n", "versions": [{"version": "v1", "created": "Sun, 11 Mar 2012 06:23:44 GMT"}], "update_date": "2014-11-19", "authors_parsed": [["Tarasenko", "Sergey", ""]]}, {"id": "1203.2556", "submitter": "Neeraj Gupta", "authors": "Neeraj Gupta, Rajiv Shekhar, Prem Kumar Kalra", "title": "A Probabilistic Transmission Expansion Planning Methodology based on\n  Roulette Wheel Selection and Social Welfare", "comments": "22 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new probabilistic methodology for transmission expansion planning (TEP)\nthat does not require a priori specification of new/additional transmission\ncapacities and uses the concept of social welfare has been proposed. Two new\nconcepts have been introduced in this paper: (i) roulette wheel methodology has\nbeen used to calculate the capacity of new transmission lines and (ii) load\nflow analysis has been used to calculate expected demand not served (EDNS). The\noverall methodology has been implemented on a modified IEEE 5-bus test system.\nSimulations show an important result: addition of only new transmission lines\nis not sufficient to minimize EDNS.\n", "versions": [{"version": "v1", "created": "Mon, 12 Mar 2012 17:13:32 GMT"}], "update_date": "2012-03-13", "authors_parsed": [["Gupta", "Neeraj", ""], ["Shekhar", "Rajiv", ""], ["Kalra", "Prem Kumar", ""]]}, {"id": "1203.2990", "submitter": "Yoshua Bengio", "authors": "Yoshua Bengio", "title": "Evolving Culture vs Local Minima", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a theory that relates difficulty of learning in deep architectures\nto culture and language. It is articulated around the following hypotheses: (1)\nlearning in an individual human brain is hampered by the presence of effective\nlocal minima; (2) this optimization difficulty is particularly important when\nit comes to learning higher-level abstractions, i.e., concepts that cover a\nvast and highly-nonlinear span of sensory configurations; (3) such high-level\nabstractions are best represented in brains by the composition of many levels\nof representation, i.e., by deep architectures; (4) a human brain can learn\nsuch high-level abstractions if guided by the signals produced by other humans,\nwhich act as hints or indirect supervision for these high-level abstractions;\nand (5), language and the recombination and optimization of mental concepts\nprovide an efficient evolutionary recombination operator, and this gives rise\nto rapid search in the space of communicable ideas that help humans build up\nbetter high-level internal representations of their world. These hypotheses put\ntogether imply that human culture and the evolution of ideas have been crucial\nto counter an optimization difficulty: this optimization difficulty would\notherwise make it very difficult for human brains to capture high-level\nknowledge of the world. The theory is grounded in experimental observations of\nthe difficulties of training deep artificial neural networks. Plausible\nconsequences of this theory for the efficiency of cultural evolutions are\nsketched.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2012 02:38:35 GMT"}, {"version": "v2", "created": "Thu, 29 Nov 2012 20:02:48 GMT"}], "update_date": "2012-11-30", "authors_parsed": [["Bengio", "Yoshua", ""]]}, {"id": "1203.3051", "submitter": "Nina Narodytska", "authors": "Nina Narodytska, Toby Walsh and Lirong Xia", "title": "Combining Voting Rules Together", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple method for combining together voting rules that performs\na run-off between the different winners of each voting rule. We prove that this\ncombinator has several good properties. For instance, even if just one of the\nbase voting rules has a desirable property like Condorcet consistency, the\ncombination inherits this property. In addition, we prove that combining voting\nrules together in this way can make finding a manipulation more computationally\ndifficult. Finally, we study the impact of this combinator on approximation\nmethods that find close to optimal manipulations.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2012 11:27:15 GMT"}], "update_date": "2012-03-15", "authors_parsed": [["Narodytska", "Nina", ""], ["Walsh", "Toby", ""], ["Xia", "Lirong", ""]]}, {"id": "1203.3227", "submitter": "Anton Loss V", "authors": "Anton Loss", "title": "Generalisation of language and knowledge models for corpus analysis", "comments": "13 pages, 2 figures, slightly unconventional", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper takes new look on language and knowledge modelling for corpus\nlinguistics. Using ideas of Chaitin, a line of argument is made against\nlanguage/knowledge separation in Natural Language Processing. A simplistic\nmodel, that generalises approaches to language and knowledge, is proposed. One\nof hypothetical consequences of this model is Strong AI.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2012 22:06:42 GMT"}], "update_date": "2012-03-16", "authors_parsed": [["Loss", "Anton", ""]]}, {"id": "1203.3376", "submitter": "Carlos Gershenson", "authors": "Bruce Edmonds and Carlos Gershenson", "title": "Learning, Social Intelligence and the Turing Test - why an\n  \"out-of-the-box\" Turing Machine will not pass the Turing Test", "comments": "10 pages, invited talk at Turing Centenary Conference CiE 2012,\n  special session on \"The Turing Test and Thinking Machines\"", "journal-ref": "Lecture Notes in Computer Science 7318 (2012) pp. 182-192", "doi": "10.1007/978-3-642-30870-3_18", "report-no": "CPM Report No.: 12-215", "categories": "cs.AI cs.LG nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Turing Test (TT) checks for human intelligence, rather than any putative\ngeneral intelligence. It involves repeated interaction requiring learning in\nthe form of adaption to the human conversation partner. It is a macro-level\npost-hoc test in contrast to the definition of a Turing Machine (TM), which is\na prior micro-level definition. This raises the question of whether learning is\njust another computational process, i.e. can be implemented as a TM. Here we\nargue that learning or adaption is fundamentally different from computation,\nthough it does involve processes that can be seen as computations. To\nillustrate this difference we compare (a) designing a TM and (b) learning a TM,\ndefining them for the purpose of the argument. We show that there is a\nwell-defined sequence of problems which are not effectively designable but are\nlearnable, in the form of the bounded halting problem. Some characteristics of\nhuman intelligence are reviewed including it's: interactive nature, learning\nabilities, imitative tendencies, linguistic ability and context-dependency. A\nstory that explains some of these is the Social Intelligence Hypothesis. If\nthis is broadly correct, this points to the necessity of a considerable period\nof acculturation (social learning in context) if an artificial intelligence is\nto pass the TT. Whilst it is always possible to 'compile' the results of\nlearning into a TM, this would not be a designed TM and would not be able to\ncontinually adapt (pass future TTs). We conclude three things, namely that: a\npurely \"designed\" TM will never pass the TT; that there is no such thing as a\ngeneral intelligence since it necessary involves learning; and that\nlearning/adaption and computation should be clearly distinguished.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 14:47:26 GMT"}], "update_date": "2012-10-10", "authors_parsed": [["Edmonds", "Bruce", ""], ["Gershenson", "Carlos", ""]]}, {"id": "1203.3464", "submitter": "Nimar S. Arora", "authors": "Nimar S. Arora, Rodrigo de Salvo Braz, Erik B. Sudderth, Stuart\n  Russell", "title": "Gibbs Sampling in Open-Universe Stochastic Languages", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-30-39", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Languages for open-universe probabilistic models (OUPMs) can represent\nsituations with an unknown number of objects and iden- tity uncertainty. While\nsuch cases arise in a wide range of important real-world appli- cations,\nexisting general purpose inference methods for OUPMs are far less efficient\nthan those available for more restricted lan- guages and model classes. This\npaper goes some way to remedying this deficit by in- troducing, and proving\ncorrect, a generaliza- tion of Gibbs sampling to partial worlds with possibly\nvarying model structure. Our ap- proach draws on and extends previous generic\nOUPM inference methods, as well as aux- iliary variable samplers for\nnonparametric mixture models. It has been implemented for BLOG, a well-known\nOUPM language. Combined with compile-time optimizations, the resulting\nalgorithm yields very substan- tial speedups over existing methods on sev- eral\ntest cases, and substantially improves the practicality of OUPM languages\ngenerally.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Arora", "Nimar S.", ""], ["Braz", "Rodrigo de Salvo", ""], ["Sudderth", "Erik B.", ""], ["Russell", "Stuart", ""]]}, {"id": "1203.3465", "submitter": "Raouia Ayachi", "authors": "Raouia Ayachi, Nahla Ben Amor, Salem Benferhat, Rolf Haenni", "title": "Compiling Possibilistic Networks: Alternative Approaches to\n  Possibilistic Inference", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-40-47", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Qualitative possibilistic networks, also known as min-based possibilistic\nnetworks, are important tools for handling uncertain information in the\npossibility theory frame- work. Despite their importance, only the junction\ntree adaptation has been proposed for exact reasoning with such networks. This\npaper explores alternative algorithms using compilation techniques. We first\npropose possibilistic adaptations of standard compilation-based probabilistic\nmethods. Then, we develop a new, purely possibilistic, method based on the\ntransformation of the initial network into a possibilistic base. A comparative\nstudy shows that this latter performs better than the possibilistic adap-\ntations of probabilistic methods. This result is also confirmed by experimental\nresults.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Ayachi", "Raouia", ""], ["Amor", "Nahla Ben", ""], ["Benferhat", "Salem", ""], ["Haenni", "Rolf", ""]]}, {"id": "1203.3466", "submitter": "Kim Bauters", "authors": "Kim Bauters, Steven Schockaert, Martine De Cock, Dirk Vermeir", "title": "Possibilistic Answer Set Programming Revisited", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-48-55", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Possibilistic answer set programming (PASP) extends answer set programming\n(ASP) by attaching to each rule a degree of certainty. While such an extension\nis important from an application point of view, existing semantics are not\nwell-motivated, and do not always yield intuitive results. To develop a more\nsuitable semantics, we first introduce a characterization of answer sets of\nclassical ASP programs in terms of possibilistic logic where an ASP program\nspecifies a set of constraints on possibility distributions. This\ncharacterization is then naturally generalized to define answer sets of PASP\nprograms. We furthermore provide a syntactic counterpart, leading to a\npossibilistic generalization of the well-known Gelfond-Lifschitz reduct, and we\nshow how our framework can readily be implemented using standard ASP solvers.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Bauters", "Kim", ""], ["Schockaert", "Steven", ""], ["De Cock", "Martine", ""], ["Vermeir", "Dirk", ""]]}, {"id": "1203.3467", "submitter": "Debarun Bhattacharjya", "authors": "Debarun Bhattacharjya, Ross D. Shachter", "title": "Three new sensitivity analysis methods for influence diagrams", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-56-64", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Performing sensitivity analysis for influence diagrams using the decision\ncircuit framework is particularly convenient, since the partial derivatives\nwith respect to every parameter are readily available [Bhattacharjya and\nShachter, 2007; 2008]. In this paper we present three non-linear sensitivity\nanalysis methods that utilize this partial derivative information and therefore\ndo not require re-evaluating the decision situation multiple times.\nSpecifically, we show how to efficiently compare strategies in decision\nsituations, perform sensitivity to risk aversion and compute the value of\nperfect hedging [Seyller, 2008].\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Bhattacharjya", "Debarun", ""], ["Shachter", "Ross D.", ""]]}, {"id": "1203.3469", "submitter": "Matthias Brocheler", "authors": "Matthias Brocheler, Lilyana Mihalkova, Lise Getoor", "title": "Probabilistic Similarity Logic", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-73-82", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning applications require the ability to learn from and\nreason about noisy multi-relational data. To address this, several effective\nrepresentations have been developed that provide both a language for expressing\nthe structural regularities of a domain, and principled support for\nprobabilistic inference. In addition to these two aspects, however, many\napplications also involve a third aspect-the need to reason about\nsimilarities-which has not been directly supported in existing frameworks. This\npaper introduces probabilistic similarity logic (PSL), a general-purpose\nframework for joint reasoning about similarity in relational domains that\nincorporates probabilistic reasoning about similarities and relational\nstructure in a principled way. PSL can integrate any existing domain-specific\nsimilarity measures and also supports reasoning about similarities between sets\nof entities. We provide efficient inference and learning techniques for PSL and\ndemonstrate its effectiveness both in common relational tasks and in settings\nthat require reasoning about similarity.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Brocheler", "Matthias", ""], ["Mihalkova", "Lilyana", ""], ["Getoor", "Lise", ""]]}, {"id": "1203.3470", "submitter": "Alan S. Carlin", "authors": "Alan S. Carlin, Nathan Schurr, Janusz Marecki", "title": "ALARMS: Alerting and Reasoning Management System for Next Generation\n  Aircraft Hazards", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-93-100", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Next Generation Air Transportation System will introduce new, advanced\nsensor technologies into the cockpit. With the introduction of such systems,\nthe responsibilities of the pilot are expected to dramatically increase. In the\nALARMS (ALerting And Reasoning Management System) project for NASA, we focus on\na key challenge of this environment, the quick and efficient handling of\naircraft sensor alerts. It is infeasible to alert the pilot on the state of all\nsubsystems at all times. Furthermore, there is uncertainty as to the true\nhazard state despite the evidence of the alerts, and there is uncertainty as to\nthe effect and duration of actions taken to address these alerts. This paper\nreports on the first steps in the construction of an application designed to\nhandle Next Generation alerts. In ALARMS, we have identified 60 different\naircraft subsystems and 20 different underlying hazards. In this paper, we show\nhow a Bayesian network can be used to derive the state of the underlying\nhazards, based on the sensor input. Then, we propose a framework whereby an\nautomated system can plan to address these hazards in cooperation with the\npilot, using a Time-Dependent Markov Process (TMDP). Different hazards and\npilot states will call for different alerting automation plans. We demonstrate\nthis emerging application of Bayesian networks and TMDPs to cockpit automation,\nfor a use case where a small number of hazards are present, and analyze the\nresulting alerting automation policies.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Carlin", "Alan S.", ""], ["Schurr", "Nathan", ""], ["Marecki", "Janusz", ""]]}, {"id": "1203.3471", "submitter": "Kamalika Chaudhuri", "authors": "Kamalika Chaudhuri, Yoav Freund, Daniel Hsu", "title": "An Online Learning-based Framework for Tracking", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-101-108", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the tracking problem, namely, estimating the hidden state of an\nobject over time, from unreliable and noisy measurements. The standard\nframework for the tracking problem is the generative framework, which is the\nbasis of solutions such as the Bayesian algorithm and its approximation, the\nparticle filters. However, these solutions can be very sensitive to model\nmismatches. In this paper, motivated by online learning, we introduce a new\nframework for tracking. We provide an efficient tracking algorithm for this\nframework. We provide experimental results comparing our algorithm to the\nBayesian algorithm on simulated data. Our experiments show that when there are\nslight model mismatches, our algorithm outperforms the Bayesian algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Chaudhuri", "Kamalika", ""], ["Freund", "Yoav", ""], ["Hsu", "Daniel", ""]]}, {"id": "1203.3473", "submitter": "Jaesik Choi", "authors": "Jaesik Choi, Eyal Amir, David J. Hill", "title": "Lifted Inference for Relational Continuous Models", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-126-134", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relational Continuous Models (RCMs) represent joint probability densities\nover attributes of objects, when the attributes have continuous domains. With\nrelational representations, they can model joint probability distributions over\nlarge numbers of variables compactly in a natural way. This paper presents a\nnew exact lifted inference algorithm for RCMs, thus it scales up to large\nmodels of real world applications. The algorithm applies to Relational Pairwise\nModels which are (relational) products of potentials of arity 2. Our algorithm\nis unique in two ways. First, it substantially improves the efficiency of\nlifted inference with variables of continuous domains. When a relational model\nhas Gaussian potentials, it takes only linear-time compared to cubic time of\nprevious methods. Second, it is the first exact inference algorithm which\nhandles RCMs in a lifted way. The algorithm is illustrated over an example from\neconometrics. Experimental results show that our algorithm outperforms both a\ngroundlevel inference algorithm and an algorithm built with previously-known\nlifted methods.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Choi", "Jaesik", ""], ["Amir", "Eyal", ""], ["Hill", "David J.", ""]]}, {"id": "1203.3474", "submitter": "Gabriel Corona", "authors": "Gabriel Corona, Francois Charpillet", "title": "Distribution over Beliefs for Memory Bounded Dec-POMDP Planning", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-135-142", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new point-based method for approximate planning in Dec-POMDP\nwhich outperforms the state-of-the-art approaches in terms of solution quality.\nIt uses a heuristic estimation of the prior probability of beliefs to choose a\nbounded number of policy trees: this choice is formulated as a combinatorial\noptimisation problem minimising the error induced by pruning.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Corona", "Gabriel", ""], ["Charpillet", "Francois", ""]]}, {"id": "1203.3477", "submitter": "Tom Erez", "authors": "Tom Erez, William D. Smart", "title": "A Scalable Method for Solving High-Dimensional Continuous POMDPs Using\n  Local Approximation", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-160-167", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partially-Observable Markov Decision Processes (POMDPs) are typically solved\nby finding an approximate global solution to a corresponding belief-MDP. In\nthis paper, we offer a new planning algorithm for POMDPs with continuous state,\naction and observation spaces. Since such domains have an inherent notion of\nlocality, we can find an approximate solution using local optimization methods.\nWe parameterize the belief distribution as a Gaussian mixture, and use the\nExtended Kalman Filter (EKF) to approximate the belief update. Since the EKF is\na first-order filter, we can marginalize over the observations analytically. By\nusing feedback control and state estimation during policy execution, we recover\na behavior that is effectively conditioned on incoming observations despite the\nunconditioned planning. Local optimization provides no guarantees of global\noptimality, but it allows us to tackle domains that are at least an order of\nmagnitude larger than the current state-of-the-art. We demonstrate the\nscalability of our algorithm by considering a simulated hand-eye coordination\ndomain with 16 continuous state dimensions and 6 continuous action dimensions.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Erez", "Tom", ""], ["Smart", "William D.", ""]]}, {"id": "1203.3478", "submitter": "Stefano Ermon", "authors": "Stefano Ermon, Jon Conrad, Carla P. Gomes, Bart Selman", "title": "Playing games against nature: optimal policies for renewable resource\n  allocation", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-168-176", "categories": "cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a class of Markov decision processes that arise as\na natural model for many renewable resource allocation problems. Upon extending\nresults from the inventory control literature, we prove that they admit a\nclosed form solution and we show how to exploit this structure to speed up its\ncomputation. We consider the application of the proposed framework to several\nproblems arising in very different domains, and as part of the ongoing effort\nin the emerging field of Computational Sustainability we discuss in detail its\napplication to the Northern Pacific Halibut marine fishery. Our approach is\napplied to a model based on real world data, obtaining a policy with a\nguaranteed lower bound on the utility function that is structurally very\ndifferent from the one currently employed.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Ermon", "Stefano", ""], ["Conrad", "Jon", ""], ["Gomes", "Carla P.", ""], ["Selman", "Bart", ""]]}, {"id": "1203.3479", "submitter": "Robin J. Evans", "authors": "Robin J. Evans, Thomas S. Richardson", "title": "Maximum likelihood fitting of acyclic directed mixed graphs to binary\n  data", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-177-184", "categories": "stat.ME cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acyclic directed mixed graphs, also known as semi-Markov models represent the\nconditional independence structure induced on an observed margin by a DAG model\nwith latent variables. In this paper we present the first method for fitting\nthese models to binary data using maximum likelihood estimation.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Evans", "Robin J.", ""], ["Richardson", "Thomas S.", ""]]}, {"id": "1203.3480", "submitter": "Xi Alice Gao", "authors": "Xi Alice Gao, Avi Pfeffer", "title": "Learning Game Representations from Data Using Rationality Constraints", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-185-192", "categories": "cs.GT cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While game theory is widely used to model strategic interactions, a natural\nquestion is where do the game representations come from? One answer is to learn\nthe representations from data. If one wants to learn both the payoffs and the\nplayers' strategies, a naive approach is to learn them both directly from the\ndata. This approach ignores the fact the players might be playing reasonably\ngood strategies, so there is a connection between the strategies and the data.\nThe main contribution of this paper is to make this connection while learning.\nWe formulate the learning problem as a weighted constraint satisfaction\nproblem, including constraints both for the fit of the payoffs and strategies\nto the data and the fit of the strategies to the payoffs. We use quantal\nresponse equilibrium as our notion of rationality for quantifying the latter\nfit. Our results show that incorporating rationality constraints can improve\nlearning when the amount of data is limited.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Gao", "Xi Alice", ""], ["Pfeffer", "Avi", ""]]}, {"id": "1203.3481", "submitter": "Robert Glaubius", "authors": "Robert Glaubius, Terry Tidwell, Christopher Gill, William D. Smart", "title": "Real-Time Scheduling via Reinforcement Learning", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-201-209", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cyber-physical systems, such as mobile robots, must respond adaptively to\ndynamic operating conditions. Effective operation of these systems requires\nthat sensing and actuation tasks are performed in a timely manner.\nAdditionally, execution of mission specific tasks such as imaging a room must\nbe balanced against the need to perform more general tasks such as obstacle\navoidance. This problem has been addressed by maintaining relative utilization\nof shared resources among tasks near a user-specified target level. Producing\noptimal scheduling strategies requires complete prior knowledge of task\nbehavior, which is unlikely to be available in practice. Instead, suitable\nscheduling strategies must be learned online through interaction with the\nsystem. We consider the sample complexity of reinforcement learning in this\ndomain, and demonstrate that while the problem state space is countably\ninfinite, we may leverage the problem's structure to guarantee efficient\nlearning.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Glaubius", "Robert", ""], ["Tidwell", "Terry", ""], ["Gill", "Christopher", ""], ["Smart", "William D.", ""]]}, {"id": "1203.3482", "submitter": "Vibhav Gogate", "authors": "Vibhav Gogate, Pedro Domingos", "title": "Formula-Based Probabilistic Inference", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-210-219", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computing the probability of a formula given the probabilities or weights\nassociated with other formulas is a natural extension of logical inference to\nthe probabilistic setting. Surprisingly, this problem has received little\nattention in the literature to date, particularly considering that it includes\nmany standard inference problems as special cases. In this paper, we propose\ntwo algorithms for this problem: formula decomposition and conditioning, which\nis an exact method, and formula importance sampling, which is an approximate\nmethod. The latter is, to our knowledge, the first application of model\ncounting to approximate probabilistic inference. Unlike conventional\nvariable-based algorithms, our algorithms work in the dual realm of logical\nformulas. Theoretically, we show that our algorithms can greatly improve\nefficiency by exploiting the structural information in the formulas.\nEmpirically, we show that they are indeed quite powerful, often achieving\nsubstantial performance gains over state-of-the-art schemes.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Gogate", "Vibhav", ""], ["Domingos", "Pedro", ""]]}, {"id": "1203.3484", "submitter": "Firas Hamze", "authors": "Firas Hamze, Nando de Freitas", "title": "Intracluster Moves for Constrained Discrete-Space MCMC", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-236-243", "categories": "stat.CO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of sampling from binary distributions with\nconstraints. In particular, it proposes an MCMC method to draw samples from a\ndistribution of the set of all states at a specified distance from some\nreference state. For example, when the reference state is the vector of zeros,\nthe algorithm can draw samples from a binary distribution with a constraint on\nthe number of active variables, say the number of 1's. We motivate the need for\nthis algorithm with examples from statistical physics and probabilistic\ninference. Unlike previous algorithms proposed to sample from binary\ndistributions with these constraints, the new algorithm allows for large moves\nin state space and tends to propose them such that they are energetically\nfavourable. The algorithm is demonstrated on three Boltzmann machines of\nvarying difficulty: A ferromagnetic Ising model (with positive potentials), a\nrestricted Boltzmann machine with learned Gabor-like filters as potentials, and\na challenging three-dimensional spin-glass (with positive and negative\npotentials).\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Hamze", "Firas", ""], ["de Freitas", "Nando", ""]]}, {"id": "1203.3487", "submitter": "Kalev Kask", "authors": "Kalev Kask, Rina Dechter, Andrew E. Gelfand", "title": "BEEM : Bucket Elimination with External Memory", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-268-276", "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major limitation of exact inference algorithms for probabilistic graphical\nmodels is their extensive memory usage, which often puts real-world problems\nout of their reach. In this paper we show how we can extend inference\nalgorithms, particularly Bucket Elimination, a special case of cluster (join)\ntree decomposition, to utilize disk memory. We provide the underlying ideas and\nshow promising empirical results of exactly solving large problems not solvable\nbefore.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Kask", "Kalev", ""], ["Dechter", "Rina", ""], ["Gelfand", "Andrew E.", ""]]}, {"id": "1203.3488", "submitter": "Kevin T. Kelly", "authors": "Kevin T. Kelly, Conor Mayo-Wilson", "title": "Causal Conclusions that Flip Repeatedly and Their Justification", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-277-285", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past two decades, several consistent procedures have been designed\nto infer causal conclusions from observational data. We prove that if the true\ncausal network might be an arbitrary, linear Gaussian network or a discrete\nBayes network, then every unambiguous causal conclusion produced by a\nconsistent method from non-experimental data is subject to reversal as the\nsample size increases any finite number of times. That result, called the\ncausal flipping theorem, extends prior results to the effect that causal\ndiscovery cannot be reliable on a given sample size. We argue that since\nrepeated flipping of causal conclusions is unavoidable in principle for\nconsistent methods, the best possible discovery methods are consistent methods\nthat retract their earlier conclusions no more than necessary. A series of\nsimulations of various methods across a wide range of sample sizes illustrates\nconcretely both the theorem and the principle of comparing methods in terms of\nretractions.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Kelly", "Kevin T.", ""], ["Mayo-Wilson", "Conor", ""]]}, {"id": "1203.3490", "submitter": "Akshat Kumar", "authors": "Akshat Kumar, Shlomo Zilberstein", "title": "Anytime Planning for Decentralized POMDPs using Expectation Maximization", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-294-301", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decentralized POMDPs provide an expressive framework for multi-agent\nsequential decision making. While fnite-horizon DECPOMDPs have enjoyed\nsignifcant success, progress remains slow for the infnite-horizon case mainly\ndue to the inherent complexity of optimizing stochastic controllers\nrepresenting agent policies. We present a promising new class of algorithms for\nthe infnite-horizon case, which recasts the optimization problem as inference\nin a mixture of DBNs. An attractive feature of this approach is the\nstraightforward adoption of existing inference techniques in DBNs for solving\nDEC-POMDPs and supporting richer representations such as factored or continuous\nstates and actions. We also derive the Expectation Maximization (EM) algorithm\nto optimize the joint policy represented as DBNs. Experiments on benchmark\ndomains show that EM compares favorably against the state-of-the-art solvers.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Kumar", "Akshat", ""], ["Zilberstein", "Shlomo", ""]]}, {"id": "1203.3493", "submitter": "Yijing Li", "authors": "Yijing Li, Prakash P. Shenoy", "title": "Solving Hybrid Influence Diagrams with Deterministic Variables", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-322-331", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a framework and an algorithm for solving hybrid influence\ndiagrams with discrete, continuous, and deterministic chance variables, and\ndiscrete and continuous decision variables. A continuous chance variable in an\ninfluence diagram is said to be deterministic if its conditional distributions\nhave zero variances. The solution algorithm is an extension of Shenoy's fusion\nalgorithm for discrete influence diagrams. We describe an extended\nShenoy-Shafer architecture for propagation of discrete, continuous, and utility\npotentials in hybrid influence diagrams that include deterministic chance\nvariables. The algorithm and framework are illustrated by solving two small\nexamples.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Li", "Yijing", ""], ["Shenoy", "Prakash P.", ""]]}, {"id": "1203.3498", "submitter": "Enrique Munoz de Cote", "authors": "Enrique Munoz de Cote, Archie C. Chapman, Adam M. Sykulski, Nicholas\n  R. Jennings", "title": "Automated Planning in Repeated Adversarial Games", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-376-383", "categories": "cs.GT cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Game theory's prescriptive power typically relies on full rationality and/or\nself-play interactions. In contrast, this work sets aside these fundamental\npremises and focuses instead on heterogeneous autonomous interactions between\ntwo or more agents. Specifically, we introduce a new and concise representation\nfor repeated adversarial (constant-sum) games that highlight the necessary\nfeatures that enable an automated planing agent to reason about how to score\nabove the game's Nash equilibrium, when facing heterogeneous adversaries. To\nthis end, we present TeamUP, a model-based RL algorithm designed for learning\nand planning such an abstraction. In essence, it is somewhat similar to R-max\nwith a cleverly engineered reward shaping that treats exploration as an\nadversarial optimization problem. In practice, it attempts to find an ally with\nwhich to tacitly collude (in more than two-player games) and then collaborates\non a joint plan of actions that can consistently score a high utility in\nadversarial repeated games. We use the inaugural Lemonade Stand Game Tournament\nto demonstrate the effectiveness of our approach, and find that TeamUP is the\nbest performing agent, demoting the Tournament's actual winning strategy into\nsecond place. In our experimental analysis, we show hat our strategy\nsuccessfully and consistently builds collaborations with many different\nheterogeneous (and sometimes very sophisticated) adversaries.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["de Cote", "Enrique Munoz", ""], ["Chapman", "Archie C.", ""], ["Sykulski", "Adam M.", ""], ["Jennings", "Nicholas R.", ""]]}, {"id": "1203.3499", "submitter": "Mathias Niepert", "authors": "Mathias Niepert", "title": "A Delayed Column Generation Strategy for Exact k-Bounded MAP Inference\n  in Markov Logic Networks", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-384-391", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper introduces k-bounded MAP inference, a parameterization of MAP\ninference in Markov logic networks. k-Bounded MAP states are MAP states with at\nmost k active ground atoms of hidden (non-evidence) predicates. We present a\nnovel delayed column generation algorithm and provide empirical evidence that\nthe algorithm efficiently computes k-bounded MAP states for meaningful\nreal-world graph matching problems. The underlying idea is that, instead of\nsolving one large optimization problem, it is often more efficient to tackle\nseveral small ones.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Niepert", "Mathias", ""]]}, {"id": "1203.3500", "submitter": "Farheen Omar", "authors": "Farheen Omar, Mathieu Sinn, Jakub Truszkowski, Pascal Poupart, James\n  Tung, Allen Caine", "title": "Comparative Analysis of Probabilistic Models for Activity Recognition\n  with an Instrumented Walker", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-392-400", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rollating walkers are popular mobility aids used by older adults to improve\nbalance control. There is a need to automatically recognize the activities\nperformed by walker users to better understand activity patterns, mobility\nissues and the context in which falls are more likely to happen. We design and\ncompare several techniques to recognize walker related activities. A\ncomprehensive evaluation with control subjects and walker users from a\nretirement community is presented.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Omar", "Farheen", ""], ["Sinn", "Mathieu", ""], ["Truszkowski", "Jakub", ""], ["Poupart", "Pascal", ""], ["Tung", "James", ""], ["Caine", "Allen", ""]]}, {"id": "1203.3502", "submitter": "Thorsten J. Ottosen", "authors": "Thorsten J. Ottosen, Finn Verner Jensen", "title": "The Cost of Troubleshooting Cost Clusters with Inside Information", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-409-416", "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision theoretical troubleshooting is about minimizing the expected cost of\nsolving a certain problem like repairing a complicated man-made device. In this\npaper we consider situations where you have to take apart some of the device to\nget access to certain clusters and actions. Specifically, we investigate\ntroubleshooting with independent actions in a tree of clusters where actions\ninside a cluster cannot be performed before the cluster is opened. The problem\nis non-trivial because there is a cost associated with opening and closing a\ncluster. Troubleshooting with independent actions and no clusters can be solved\nin O(n lg n) time (n being the number of actions) by the well-known \"P-over-C\"\nalgorithm due to Kadane and Simon, but an efficient and optimal algorithm for a\ntree cluster model has not yet been found. In this paper we describe a\n\"bottom-up P-over-C\" O(n lg n) time algorithm and show that it is optimal when\nthe clusters do not need to be closed to test whether the actions solved the\nproblem.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Ottosen", "Thorsten J.", ""], ["Jensen", "Finn Verner", ""]]}, {"id": "1203.3503", "submitter": "Judea Pearl", "authors": "Judea Pearl", "title": "On a Class of Bias-Amplifying Variables that Endanger Effect Estimates", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-417-424", "categories": "stat.ME cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note deals with a class of variables that, if conditioned on, tends to\namplify confounding bias in the analysis of causal effects. This class,\nindependently discovered by Bhattacharya and Vogt (2007) and Wooldridge (2009),\nincludes instrumental variables and variables that have greater influence on\ntreatment selection than on the outcome. We offer a simple derivation and an\nintuitive explanation of this phenomenon and then extend the analysis to non\nlinear models. We show that: 1. the bias-amplifying potential of instrumental\nvariables extends over to non-linear models, though not as sweepingly as in\nlinear models; 2. in non-linear models, conditioning on instrumental variables\nmay introduce new bias where none existed before; 3. in both linear and\nnon-linear models, instrumental variables have no effect on selection-induced\nbias.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Pearl", "Judea", ""]]}, {"id": "1203.3504", "submitter": "Judea Pearl", "authors": "Judea Pearl", "title": "On Measurement Bias in Causal Inference", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-425-432", "categories": "stat.ME cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of measurement errors in causal inference\nand highlights several algebraic and graphical methods for eliminating\nsystematic bias induced by such errors. In particulars, the paper discusses the\ncontrol of partially observable confounders in parametric and non parametric\nmodels and the computational problem of obtaining bias-free effect estimates in\nsuch models.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Pearl", "Judea", ""]]}, {"id": "1203.3505", "submitter": "Judea Pearl", "authors": "Judea Pearl, Azaria Paz", "title": "Confounding Equivalence in Causal Inference", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-433-441", "categories": "stat.ME cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper provides a simple test for deciding, from a given causal diagram,\nwhether two sets of variables have the same bias-reducing potential under\nadjustment. The test requires that one of the following two conditions holds:\neither (1) both sets are admissible (i.e., satisfy the back-door criterion) or\n(2) the Markov boundaries surrounding the manipulated variable(s) are identical\nin both sets. Applications to covariate selection and model testing are\ndiscussed.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Pearl", "Judea", ""], ["Paz", "Azaria", ""]]}, {"id": "1203.3508", "submitter": "Guilin Qi", "authors": "Guilin Qi, Jianfeng Du, Weiru Liu, David A. Bell", "title": "Merging Knowledge Bases in Possibilistic Logic by Lexicographic\n  Aggregation", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-458-465", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Belief merging is an important but difficult problem in Artificial\nIntelligence, especially when sources of information are pervaded with\nuncertainty. Many merging operators have been proposed to deal with this\nproblem in possibilistic logic, a weighted logic which is powerful for handling\ninconsistency and deal- ing with uncertainty. They often result in a\npossibilistic knowledge base which is a set of weighted formulas. Although\npossibilistic logic is inconsistency tolerant, it suers from the well-known\n\"drowning effect\". Therefore, we may still want to obtain a consistent possi-\nbilistic knowledge base as the result of merg- ing. In such a case, we argue\nthat it is not always necessary to keep weighted informa- tion after merging.\nIn this paper, we define a merging operator that maps a set of pos- sibilistic\nknowledge bases and a formula rep- resenting the integrity constraints to a\nclas- sical knowledge base by using lexicographic ordering. We show that it\nsatisfies nine pos- tulates that generalize basic postulates for propositional\nmerging given in [11]. These postulates capture the principle of minimal change\nin some sense. We then provide an algorithm for generating the resulting knowl-\nedge base of our merging operator. Finally, we discuss the compatibility of our\nmerging operator with propositional merging and es- tablish the advantage of\nour merging opera- tor over existing semantic merging operators in the\npropositional case.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Qi", "Guilin", ""], ["Du", "Jianfeng", ""], ["Liu", "Weiru", ""], ["Bell", "David A.", ""]]}, {"id": "1203.3509", "submitter": "Erik Quaeghebeur", "authors": "Erik Quaeghebeur", "title": "Characterizing the Set of Coherent Lower Previsions with a Finite Number\n  of Constraints or Vertices", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-466-473", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The standard coherence criterion for lower previsions is expressed using an\ninfinite number of linear constraints. For lower previsions that are\nessentially defined on some finite set of gambles on a finite possibility\nspace, we present a reformulation of this criterion that only uses a finite\nnumber of constraints. Any such lower prevision is coherent if it lies within\nthe convex polytope defined by these constraints. The vertices of this polytope\nare the extreme coherent lower previsions for the given set of gambles. Our\nreformulation makes it possible to compute them. We show how this is done and\nillustrate the procedure and its results.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Quaeghebeur", "Erik", ""]]}, {"id": "1203.3510", "submitter": "Michael Ramati", "authors": "Michael Ramati, Yuval Shahar", "title": "Irregular-Time Bayesian Networks", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-484-491", "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many fields observations are performed irregularly along time, due to\neither measurement limitations or lack of a constant immanent rate. While\ndiscrete-time Markov models (as Dynamic Bayesian Networks) introduce either\ninefficient computation or an information loss to reasoning about such\nprocesses, continuous-time Markov models assume either a discrete state space\n(as Continuous-Time Bayesian Networks), or a flat continuous state space (as\nstochastic differential equations). To address these problems, we present a new\nmodeling class called Irregular-Time Bayesian Networks (ITBNs), generalizing\nDynamic Bayesian Networks, allowing substantially more compact representations,\nand increasing the expressivity of the temporal dynamics. In addition, a\nglobally optimal solution is guaranteed when learning temporal systems,\nprovided that they are fully observed at the same irregularly spaced\ntime-points, and a semiparametric subclass of ITBNs is introduced to allow\nfurther adaptation to the irregular nature of the available data.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Ramati", "Michael", ""], ["Shahar", "Yuval", ""]]}, {"id": "1203.3512", "submitter": "Chris Russell", "authors": "Chris Russell, L'ubor Ladicky, Pushmeet Kohli, Philip H.S. Torr", "title": "Exact and Approximate Inference in Associative Hierarchical Networks\n  using Graph Cuts", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-501-508", "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov Networks are widely used through out computer vision and machine\nlearning. An important subclass are the Associative Markov Networks which are\nused in a wide variety of applications. For these networks a good approximate\nminimum cost solution can be found efficiently using graph cut based move\nmaking algorithms such as alpha-expansion. Recently a related model has been\nproposed, the associative hierarchical network, which provides a natural\ngeneralisation of the Associative Markov Network for higher order cliques (i.e.\nclique size greater than two). This method provides a good model for object\nclass segmentation problem in computer vision. Within this paper we briefly\ndescribe the associative hierarchical network and provide a computationally\nefficient method for approximate inference based on graph cuts. Our method\nperforms well for networks containing hundreds of thousand of variables, and\nhigher order potentials are defined over cliques containing tens of thousands\nof variables. Due to the size of these problems standard linear programming\ntechniques are inapplicable. We show that our method has a bound of 4 for the\nsolution of general associative hierarchical network with arbitrary clique size\nnoting that few results on bounds exist for the solution of labelling of Markov\nNetworks with higher order cliques.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Russell", "Chris", ""], ["Ladicky", "L'ubor", ""], ["Kohli", "Pushmeet", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1203.3513", "submitter": "Ross D. Shachter", "authors": "Ross D. Shachter, Debarun Bhattacharjya", "title": "Dynamic programming in in uence diagrams with decision circuits", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-509-516", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision circuits perform efficient evaluation of influence diagrams,\nbuilding on the ad- vances in arithmetic circuits for belief net- work\ninference [Darwiche, 2003; Bhattachar- jya and Shachter, 2007]. We show how\neven more compact decision circuits can be con- structed for dynamic\nprogramming in influ- ence diagrams with separable value functions and\nconditionally independent subproblems. Once a decision circuit has been\nconstructed based on the diagram's \"global\" graphical structure, it can be\ncompiled to exploit \"lo- cal\" structure for efficient evaluation and sen-\nsitivity analysis.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Shachter", "Ross D.", ""], ["Bhattacharjya", "Debarun", ""]]}, {"id": "1203.3515", "submitter": "Ilya Shpitser", "authors": "Ilya Shpitser, Tyler VanderWeele, James M. Robins", "title": "On the Validity of Covariate Adjustment for Estimating Causal Effects", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-527-536", "categories": "stat.ME cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying effects of actions (treatments) on outcome variables from\nobservational data and causal assumptions is a fundamental problem in causal\ninference. This identification is made difficult by the presence of confounders\nwhich can be related to both treatment and outcome variables. Confounders are\noften handled, both in theory and in practice, by adjusting for covariates, in\nother words considering outcomes conditioned on treatment and covariate values,\nweighed by probability of observing those covariate values. In this paper, we\ngive a complete graphical criterion for covariate adjustment, which we term the\nadjustment criterion, and derive some interesting corollaries of the\ncompleteness of this criterion.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Shpitser", "Ilya", ""], ["VanderWeele", "Tyler", ""], ["Robins", "James M.", ""]]}, {"id": "1203.3516", "submitter": "Aleksandr Simma", "authors": "Aleksandr Simma, Michael I. Jordan", "title": "Modeling Events with Cascades of Poisson Processes", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-546-555", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a probabilistic model of events in continuous time in which each\nevent triggers a Poisson process of successor events. The ensemble of observed\nevents is thereby modeled as a superposition of Poisson processes. Efficient\ninference is feasible under this model with an EM algorithm. Moreover, the EM\nalgorithm can be implemented as a distributed algorithm, permitting the model\nto be applied to very large datasets. We apply these techniques to the modeling\nof Twitter messages and the revision history of Wikipedia.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Simma", "Aleksandr", ""], ["Jordan", "Michael I.", ""]]}, {"id": "1203.3518", "submitter": "Jonathan Sorg", "authors": "Jonathan Sorg, Satinder Singh, Richard L. Lewis", "title": "Variance-Based Rewards for Approximate Bayesian Reinforcement Learning", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-564-571", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The explore{exploit dilemma is one of the central challenges in Reinforcement\nLearning (RL). Bayesian RL solves the dilemma by providing the agent with\ninformation in the form of a prior distribution over environments; however,\nfull Bayesian planning is intractable. Planning with the mean MDP is a common\nmyopic approximation of Bayesian planning. We derive a novel reward bonus that\nis a function of the posterior distribution over environments, which, when\nadded to the reward in planning with the mean MDP, results in an agent which\nexplores efficiently and effectively. Although our method is similar to\nexisting methods when given an uninformative or unstructured prior, unlike\nexisting methods, our method can exploit structured priors. We prove that our\nmethod results in a polynomial sample complexity and empirically demonstrate\nits advantages in a structured exploration task.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Sorg", "Jonathan", ""], ["Singh", "Satinder", ""], ["Lewis", "Richard L.", ""]]}, {"id": "1203.3519", "submitter": "Gerald Tesauro", "authors": "Gerald Tesauro, V T Rajan, Richard Segal", "title": "Bayesian Inference in Monte-Carlo Tree Search", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-580-588", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte-Carlo Tree Search (MCTS) methods are drawing great interest after\nyielding breakthrough results in computer Go. This paper proposes a Bayesian\napproach to MCTS that is inspired by distributionfree approaches such as UCT\n[13], yet significantly differs in important respects. The Bayesian framework\nallows potentially much more accurate (Bayes-optimal) estimation of node values\nand node uncertainties from a limited number of simulation trials. We further\npropose propagating inference in the tree via fast analytic Gaussian\napproximation methods: this can make the overhead of Bayesian inference\nmanageable in domains such as Go, while preserving high accuracy of\nexpected-value estimates. We find substantial empirical outperformance of UCT\nin an idealized bandit-tree test environment, where we can obtain valuable\ninsights by comparing with known ground truth. Additionally we rigorously prove\non-policy and off-policy convergence of the proposed methods.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Tesauro", "Gerald", ""], ["Rajan", "V T", ""], ["Segal", "Richard", ""]]}, {"id": "1203.3520", "submitter": "Jin Tian", "authors": "Jin Tian, Ru He, Lavanya Ram", "title": "Bayesian Model Averaging Using the k-best Bayesian Network Structures", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-589-597", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning Bayesian network structures from data. We\ndevelop an algorithm for finding the k-best Bayesian network structures. We\npropose to compute the posterior probabilities of hypotheses of interest by\nBayesian model averaging over the k-best Bayesian networks. We present\nempirical results on structural discovery over several real and synthetic data\nsets and show that the method outperforms the model selection method and the\nstate of-the-art MCMC methods.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Tian", "Jin", ""], ["He", "Ru", ""], ["Ram", "Lavanya", ""]]}, {"id": "1203.3525", "submitter": "Mark Voortman", "authors": "Mark Voortman, Denver Dash, Marek J. Druzdzel", "title": "Learning Why Things Change: The Difference-Based Causality Learner", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-641-650", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the Difference- Based Causality Learner (DBCL), an\nalgorithm for learning a class of discrete-time dynamic models that represents\nall causation across time by means of difference equations driving change in a\nsystem. We motivate this representation with real-world mechanical systems and\nprove DBCL's correctness for learning structure from time series data, an\nendeavour that is complicated by the existence of latent derivatives that have\nto be detected. We also prove that, under common assumptions for causal\ndiscovery, DBCL will identify the presence or absence of feedback loops, making\nthe model more useful for predicting the effects of manipulating variables when\nthe system is in equilibrium. We argue analytically and show empirically the\nadvantages of DBCL over vector autoregression (VAR) and Granger causality\nmodels as well as modified forms of Bayesian and constraintbased structure\ndiscovery algorithms. Finally, we show that our algorithm can discover causal\ndirections of alpha rhythms in human brains from EEG data.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Voortman", "Mark", ""], ["Dash", "Denver", ""], ["Druzdzel", "Marek J.", ""]]}, {"id": "1203.3526", "submitter": "Tomas Werner", "authors": "Tomas Werner", "title": "Primal View on Belief Propagation", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-651-657", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that fixed points of loopy belief propagation (BP) correspond to\nstationary points of the Bethe variational problem, where we minimize the Bethe\nfree energy subject to normalization and marginalization constraints.\nUnfortunately, this does not entirely explain BP because BP is a dual rather\nthan primal algorithm to solve the Bethe variational problem -- beliefs are\ninfeasible before convergence. Thus, we have no better understanding of BP than\nas an algorithm to seek for a common zero of a system of non-linear functions,\nnot explicitly related to each other. In this theoretical paper, we show that\nthese functions are in fact explicitly related -- they are the partial\nderivatives of a single function of reparameterizations. That means, BP seeks\nfor a stationary point of a single function, without any constraints. This\nfunction has a very natural form: it is a linear combination of local\nlog-partition functions, exactly as the Bethe entropy is the same linear\ncombination of local entropies.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Werner", "Tomas", ""]]}, {"id": "1203.3527", "submitter": "Jens Witkowski", "authors": "Jens Witkowski", "title": "Truthful Feedback for Sanctioning Reputation Mechanisms", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-658-665", "categories": "cs.GT cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For product rating environments, similar to that of Amazon Reviews, it has\nbeen shown that the truthful elicitation of feedback is possible through\nmechanisms which pay buyer reports contingent on the reports of other buyers.\nWe study whether similar mechanisms can be designed for reputation mechanisms\nat online auction sites where the buyers' experiences are partially determined\nby a strategic seller. We show that this is impossible for the basic setting.\nHowever, introducing a small prior belief that the seller is a cooperative\ncommitment player leads to a payment scheme with a truthful perfect Bayesian\nequilibrium.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Witkowski", "Jens", ""]]}, {"id": "1203.3528", "submitter": "Feng Wu", "authors": "Feng Wu, Shlomo Zilberstein, Xiaoping Chen", "title": "Rollout Sampling Policy Iteration for Decentralized POMDPs", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-666-673", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present decentralized rollout sampling policy iteration (DecRSPI) - a new\nalgorithm for multi-agent decision problems formalized as DEC-POMDPs. DecRSPI\nis designed to improve scalability and tackle problems that lack an explicit\nmodel. The algorithm uses Monte- Carlo methods to generate a sample of\nreachable belief states. Then it computes a joint policy for each belief state\nbased on the rollout estimations. A new policy representation allows us to\nrepresent solutions compactly. The key benefits of the algorithm are its linear\ntime complexity over the number of agents, its bounded memory usage and good\nsolution quality. It can solve larger problems that are intractable for\nexisting planning algorithms. Experimental results confirm the effectiveness\nand scalability of the approach.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Wu", "Feng", ""], ["Zilberstein", "Shlomo", ""], ["Chen", "Xiaoping", ""]]}, {"id": "1203.3529", "submitter": "Yan Yan", "authors": "Yan Yan, Romer Rosales, Glenn Fung, Jennifer Dy", "title": "Modeling Multiple Annotator Expertise in the Semi-Supervised Learning\n  Scenario", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-674-682", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning algorithms normally assume that there is at most one annotation or\nlabel per data point. However, in some scenarios, such as medical diagnosis and\non-line collaboration,multiple annotations may be available. In either case,\nobtaining labels for data points can be expensive and time-consuming (in some\ncircumstances ground-truth may not exist). Semi-supervised learning approaches\nhave shown that utilizing the unlabeled data is often beneficial in these\ncases. This paper presents a probabilistic semi-supervised model and algorithm\nthat allows for learning from both unlabeled and labeled data in the presence\nof multiple annotators. We assume that it is known what annotator labeled which\ndata points. The proposed approach produces annotator models that allow us to\nprovide (1) estimates of the true label and (2) annotator variable expertise\nfor both labeled and unlabeled data. We provide numerical comparisons under\nvarious scenarios and with respect to standard semi-supervised learning.\nExperiments showed that the presented approach provides clear advantages over\nmulti-annotator methods that do not use the unlabeled data and over methods\nthat do not use multi-labeler information.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Yan", "Yan", ""], ["Rosales", "Romer", ""], ["Fung", "Glenn", ""], ["Dy", "Jennifer", ""]]}, {"id": "1203.3531", "submitter": "Changhe Yuan", "authors": "Changhe Yuan, Xiaojian Wu, Eric A. Hansen", "title": "Solving Multistage Influence Diagrams using Branch-and-Bound Search", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-691-700", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A branch-and-bound approach to solving influ- ence diagrams has been\npreviously proposed in the literature, but appears to have never been\nimplemented and evaluated - apparently due to the difficulties of computing\neffective bounds for the branch-and-bound search. In this paper, we describe\nhow to efficiently compute effective bounds, and we develop a practical\nimplementa- tion of depth-first branch-and-bound search for influence diagram\nevaluation that outperforms existing methods for solving influence diagrams\nwith multiple stages.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Yuan", "Changhe", ""], ["Wu", "Xiaojian", ""], ["Hansen", "Eric A.", ""]]}, {"id": "1203.3535", "submitter": "Yu Zhang", "authors": "Yu Zhang, Bin Cao, Dit-Yan Yeung", "title": "Multi-Domain Collaborative Filtering", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-725-732", "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative filtering is an effective recommendation approach in which the\npreference of a user on an item is predicted based on the preferences of other\nusers with similar interests. A big challenge in using collaborative filtering\nmethods is the data sparsity problem which often arises because each user\ntypically only rates very few items and hence the rating matrix is extremely\nsparse. In this paper, we address this problem by considering multiple\ncollaborative filtering tasks in different domains simultaneously and\nexploiting the relationships between domains. We refer to it as a multi-domain\ncollaborative filtering (MCF) problem. To solve the MCF problem, we propose a\nprobabilistic framework which uses probabilistic matrix factorization to model\nthe rating problem in each domain and allows the knowledge to be adaptively\ntransferred across different domains by automatically learning the correlation\nbetween domains. We also introduce the link function for different domains to\ncorrect their biases. Experiments conducted on several real-world applications\ndemonstrate the effectiveness of our methods when compared with some\nrepresentative methods.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Zhang", "Yu", ""], ["Cao", "Bin", ""], ["Yeung", "Dit-Yan", ""]]}, {"id": "1203.3536", "submitter": "Yu Zhang", "authors": "Yu Zhang, Dit-Yan Yeung", "title": "A Convex Formulation for Learning Task Relationships in Multi-Task\n  Learning", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-733-742", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning is a learning paradigm which seeks to improve the\ngeneralization performance of a learning task with the help of some other\nrelated tasks. In this paper, we propose a regularization formulation for\nlearning the relationships between tasks in multi-task learning. This\nformulation can be viewed as a novel generalization of the regularization\nframework for single-task learning. Besides modeling positive task correlation,\nour method, called multi-task relationship learning (MTRL), can also describe\nnegative task correlation and identify outlier tasks based on the same\nunderlying principle. Under this regularization framework, the objective\nfunction of MTRL is convex. For efficiency, we use an alternating method to\nlearn the optimal model parameters for each task as well as the relationships\nbetween tasks. We study MTRL in the symmetric multi-task learning setting and\nthen generalize it to the asymmetric setting as well. We also study the\nrelationships between MTRL and some existing multi-task learning methods.\nExperiments conducted on a toy problem as well as several benchmark data sets\ndemonstrate the effectiveness of MTRL.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Zhang", "Yu", ""], ["Yeung", "Dit-Yan", ""]]}, {"id": "1203.3538", "submitter": "Emma Brunskill", "authors": "Emma Brunskill, Stuart Russell", "title": "RAPID: A Reachable Anytime Planner for Imprecisely-sensed Domains", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-83-92", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the intractability of generic optimal partially observable Markov\ndecision process planning, there exist important problems that have highly\nstructured models. Previous researchers have used this insight to construct\nmore efficient algorithms for factored domains, and for domains with\ntopological structure in the flat state dynamics model. In our work, motivated\nby findings from the education community relevant to automated tutoring, we\nconsider problems that exhibit a form of topological structure in the factored\ndynamics model. Our Reachable Anytime Planner for Imprecisely-sensed Domains\n(RAPID) leverages this structure to efficiently compute a good initial envelope\nof reachable states under the optimal MDP policy in time linear in the number\nof state variables. RAPID performs partially-observable planning over the\nlimited envelope of states, and slowly expands the state space considered as\ntime allows. RAPID performs well on a large tutoring-inspired problem\nsimulation with 122 state variables, corresponding to a flat state space of\nover 10^30 states.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:25:52 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Brunskill", "Emma", ""], ["Russell", "Stuart", ""]]}, {"id": "1203.3725", "submitter": "Richard Everitt", "authors": "Richard G. Everitt", "title": "Bayesian Parameter Estimation for Latent Markov Random Fields and Social\n  Networks", "comments": "26 pages, 2 figures, accepted in Journal of Computational and\n  Graphical Statistics (http://www.amstat.org/publications/jcgs.cfm)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cond-mat.stat-mech cs.AI cs.SI physics.data-an", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Undirected graphical models are widely used in statistics, physics and\nmachine vision. However Bayesian parameter estimation for undirected models is\nextremely challenging, since evaluation of the posterior typically involves the\ncalculation of an intractable normalising constant. This problem has received\nmuch attention, but very little of this has focussed on the important practical\ncase where the data consists of noisy or incomplete observations of the\nunderlying hidden structure. This paper specifically addresses this problem,\ncomparing two alternative methodologies. In the first of these approaches\nparticle Markov chain Monte Carlo (Andrieu et al., 2010) is used to efficiently\nexplore the parameter space, combined with the exchange algorithm (Murray et\nal., 2006) for avoiding the calculation of the intractable normalising constant\n(a proof showing that this combination targets the correct distribution in\nfound in a supplementary appendix online). This approach is compared with\napproximate Bayesian computation (Pritchard et al., 1999). Applications to\nestimating the parameters of Ising models and exponential random graphs from\nnoisy data are presented. Each algorithm used in the paper targets an\napproximation to the true posterior due to the use of MCMC to simulate from the\nlatent graphical model, in lieu of being able to do this exactly in general.\nThe supplementary appendix also describes the nature of the resulting\napproximation.\n", "versions": [{"version": "v1", "created": "Wed, 14 Mar 2012 07:31:15 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Everitt", "Richard G.", ""]]}, {"id": "1203.3764", "submitter": "Naveen Ashish", "authors": "Naveen Ashish, Antarip Biswas, Sumit Das, Saurav Nag and Rajiv Pratap", "title": "The Abzooba Smart Health Informatics Platform (SHIP) TM - From Patient\n  Experiences to Big Data to Insights", "comments": "3 pages", "journal-ref": null, "doi": null, "report-no": "ABZ-TR-2012-1", "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a technology to connect patients to information in the\nexperiences of other patients by using the power of structured big data. The\napproach, implemented in the Abzooba Smart Health Informatics Platform\n(SHIP),is to distill concepts of facts and expressions from conversations and\ndiscussions in health social media forums, and use those distilled concepts in\nconnecting patients to experiences and insights that are highly relevant to\nthem in particular. We envision our work, in progress, to provide new and\neffective tools to exploit the richness of content in social media in health\nfor outcomes research.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2012 16:59:03 GMT"}], "update_date": "2012-03-19", "authors_parsed": [["Ashish", "Naveen", ""], ["Biswas", "Antarip", ""], ["Das", "Sumit", ""], ["Nag", "Saurav", ""], ["Pratap", "Rajiv", ""]]}, {"id": "1203.3783", "submitter": "Gr\\'egoire Montavon", "authors": "Gr\\'egoire Montavon and Klaus-Robert M\\\"uller", "title": "Learning Feature Hierarchies with Centered Deep Boltzmann Machines", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-642-35289-8_33", "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Boltzmann machines are in principle powerful models for extracting the\nhierarchical structure of data. Unfortunately, attempts to train layers jointly\n(without greedy layer-wise pretraining) have been largely unsuccessful. We\npropose a modification of the learning algorithm that initially recenters the\noutput of the activation functions to zero. This modification leads to a better\nconditioned Hessian and thus makes learning easier. We test the algorithm on\nreal data and demonstrate that our suggestion, the centered deep Boltzmann\nmachine, learns a hierarchy of increasingly abstract representations and a\nbetter generative model of data.\n", "versions": [{"version": "v1", "created": "Fri, 16 Mar 2012 19:01:10 GMT"}], "update_date": "2012-12-19", "authors_parsed": [["Montavon", "Gr\u00e9goire", ""], ["M\u00fcller", "Klaus-Robert", ""]]}, {"id": "1203.3887", "submitter": "Animashree Anandkumar", "authors": "Animashree Anandkumar, Ragupathyraj Valluvan", "title": "Learning loopy graphical models with latent variables: Efficient methods\n  and guarantees", "comments": "Published in at http://dx.doi.org/10.1214/12-AOS1070 the Annals of\n  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Annals of Statistics 2013, Vol. 41, No. 2, 401-435", "doi": "10.1214/12-AOS1070", "report-no": "IMS-AOS-AOS1070", "categories": "stat.ML cs.AI cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of structure estimation in graphical models with latent variables\nis considered. We characterize conditions for tractable graph estimation and\ndevelop efficient methods with provable guarantees. We consider models where\nthe underlying Markov graph is locally tree-like, and the model is in the\nregime of correlation decay. For the special case of the Ising model, the\nnumber of samples $n$ required for structural consistency of our method scales\nas $n=\\Omega(\\theta_{\\min}^{-\\delta\\eta(\\eta+1)-2}\\log p)$, where p is the\nnumber of variables, $\\theta_{\\min}$ is the minimum edge potential, $\\delta$ is\nthe depth (i.e., distance from a hidden node to the nearest observed nodes),\nand $\\eta$ is a parameter which depends on the bounds on node and edge\npotentials in the Ising model. Necessary conditions for structural consistency\nunder any algorithm are derived and our method nearly matches the lower bound\non sample requirements. Further, the proposed method is practical to implement\nand provides flexibility to control the number of latent variables and the\ncycle lengths in the output graph.\n", "versions": [{"version": "v1", "created": "Sat, 17 Mar 2012 19:09:41 GMT"}, {"version": "v2", "created": "Sat, 30 Jun 2012 19:42:33 GMT"}, {"version": "v3", "created": "Mon, 16 Jul 2012 18:32:38 GMT"}, {"version": "v4", "created": "Mon, 22 Apr 2013 13:43:39 GMT"}], "update_date": "2013-04-23", "authors_parsed": [["Anandkumar", "Animashree", ""], ["Valluvan", "Ragupathyraj", ""]]}, {"id": "1203.4011", "submitter": "Raghuram Ramanujan", "authors": "Raghuram Ramanujan, Ashish Sabharwal, Bart Selman", "title": "Understanding Sampling Style Adversarial Search Methods", "comments": "Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty\n  in Artificial Intelligence (UAI2010)", "journal-ref": null, "doi": null, "report-no": "UAI-P-2010-PG-474-483", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  UCT has recently emerged as an exciting new adversarial reasoning technique\nbased on cleverly balancing exploration and exploitation in a Monte-Carlo\nsampling setting. It has been particularly successful in the game of Go but the\nreasons for its success are not well understood and attempts to replicate its\nsuccess in other domains such as Chess have failed. We provide an in-depth\nanalysis of the potential of UCT in domain-independent settings, in cases where\nheuristic values are available, and the effect of enhancing random playouts to\nmore informed playouts between two weak minimax players. To provide further\ninsights, we develop synthetic game tree instances and discuss interesting\nproperties of UCT, both empirically and analytically.\n", "versions": [{"version": "v1", "created": "Thu, 15 Mar 2012 11:17:56 GMT"}], "update_date": "2012-03-20", "authors_parsed": [["Ramanujan", "Raghuram", ""], ["Sabharwal", "Ashish", ""], ["Selman", "Bart", ""]]}, {"id": "1203.4184", "submitter": "Francisco Kitaura", "authors": "Francisco-Shu Kitaura", "title": "The Initial Conditions of the Universe from Constrained Simulations", "comments": "6 pages, 4 figures; accepted at MNRAS after minor corrections", "journal-ref": null, "doi": "10.1093/mnrasl/sls029", "report-no": null, "categories": "astro-ph.CO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I present a new approach to recover the primordial density fluctuations and\nthe cosmic web structure underlying a galaxy distribution. The method is based\non sampling Gaussian fields which are compatible with a galaxy distribution and\na structure formation model. This is achieved by splitting the inversion\nproblem into two Gibbs-sampling steps: the first being a Gaussianisation step\ntransforming a distribution of point sources at Lagrangian positions -which are\nnot a priori given- into a linear alias-free Gaussian field. This step is based\non Hamiltonian sampling with a Gaussian-Poisson model. The second step consists\non a likelihood comparison in which the set of matter tracers at the initial\nconditions is constrained on the galaxy distribution and the assumed structure\nformation model. For computational reasons second order Lagrangian Perturbation\nTheory is used. However, the presented approach is flexible to adopt any\nstructure formation model. A semi-analytic halo-model based galaxy mock catalog\nis taken to demonstrate that the recovered initial conditions are closely\nunbiased with respect to the actual ones from the corresponding N-body\nsimulation down to scales of a ~ 5 Mpc/h. The cross-correlation between them\nshows a substantial gain of information, being at k ~ 0.3 h/Mpc more than\ndoubled. In addition the initial conditions are extremely well Gaussian\ndistributed and the power-spectra follow the shape of the linear power-spectrum\nbeing very close to the actual one from the simulation down to scales of k ~ 1\nh/Mpc.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2012 17:42:25 GMT"}, {"version": "v2", "created": "Wed, 7 Nov 2012 14:21:37 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Kitaura", "Francisco-Shu", ""]]}, {"id": "1203.4287", "submitter": "Muhammad Islam", "authors": "Muhammad Asiful Islam, C. R. Ramakrishnan, I. V. Ramakrishnan", "title": "Parameter Learning in PRISM Programs with Continuous Random Variables", "comments": "7 pages. Main contribution: Learning algorithm. Inference appears in\n  http://arxiv.org/abs/1112.2681", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic Logic Programming (PLP), exemplified by Sato and Kameya's\nPRISM, Poole's ICL, De Raedt et al's ProbLog and Vennekens et al's LPAD,\ncombines statistical and logical knowledge representation and inference.\nInference in these languages is based on enumerative construction of proofs\nover logic programs. Consequently, these languages permit very limited use of\nrandom variables with continuous distributions. In this paper, we extend PRISM\nwith Gaussian random variables and linear equality constraints, and consider\nthe problem of parameter learning in the extended language. Many statistical\nmodels such as finite mixture models and Kalman filter can be encoded in\nextended PRISM. Our EM-based learning algorithm uses a symbolic inference\nprocedure that represents sets of derivations without enumeration. This permits\nus to learn the distribution parameters of extended PRISM programs with\ndiscrete as well as Gaussian variables. The learning algorithm naturally\ngeneralizes the ones used for PRISM and Hybrid Bayesian Networks.\n", "versions": [{"version": "v1", "created": "Mon, 19 Mar 2012 23:37:07 GMT"}], "update_date": "2012-03-21", "authors_parsed": [["Islam", "Muhammad Asiful", ""], ["Ramakrishnan", "C. R.", ""], ["Ramakrishnan", "I. V.", ""]]}, {"id": "1203.4345", "submitter": "Marc Deisenroth", "authors": "Marc Peter Deisenroth, Ryan Turner, Marco F. Huber, Uwe D. Hanebeck,\n  Carl Edward Rasmussen", "title": "Robust Filtering and Smoothing with Gaussian Processes", "comments": "7 pages, 1 figure, draft version of paper accepted at IEEE\n  Transactions on Automatic Control", "journal-ref": null, "doi": "10.1109/TAC.2011.2179426", "report-no": null, "categories": "cs.SY cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a principled algorithm for robust Bayesian filtering and smoothing\nin nonlinear stochastic dynamic systems when both the transition function and\nthe measurement function are described by non-parametric Gaussian process (GP)\nmodels. GPs are gaining increasing importance in signal processing, machine\nlearning, robotics, and control for representing unknown system functions by\nposterior probability distributions. This modern way of \"system identification\"\nis more robust than finding point estimates of a parametric function\nrepresentation. In this article, we present a principled algorithm for robust\nanalytic smoothing in GP dynamic systems, which are increasingly used in\nrobotics and control. Our numerical evaluations demonstrate the robustness of\nthe proposed approach in situations where other state-of-the-art Gaussian\nfilters and smoothers can fail.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2012 08:51:50 GMT"}], "update_date": "2012-08-13", "authors_parsed": [["Deisenroth", "Marc Peter", ""], ["Turner", "Ryan", ""], ["Huber", "Marco F.", ""], ["Hanebeck", "Uwe D.", ""], ["Rasmussen", "Carl Edward", ""]]}, {"id": "1203.4416", "submitter": "Guillaume Desjardins", "authors": "Guillaume Desjardins and Aaron Courville and Yoshua Bengio", "title": "On Training Deep Boltzmann Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deep Boltzmann machine (DBM) has been an important development in the\nquest for powerful \"deep\" probabilistic models. To date, simultaneous or joint\ntraining of all layers of the DBM has been largely unsuccessful with existing\ntraining methods. We introduce a simple regularization scheme that encourages\nthe weight vectors associated with each hidden unit to have similar norms. We\ndemonstrate that this regularization can be easily combined with standard\nstochastic maximum likelihood to yield an effective training strategy for the\nsimultaneous training of all layers of the deep Boltzmann machine.\n", "versions": [{"version": "v1", "created": "Tue, 20 Mar 2012 12:59:15 GMT"}], "update_date": "2012-03-21", "authors_parsed": [["Desjardins", "Guillaume", ""], ["Courville", "Aaron", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1203.4855", "submitter": "Shervan Fekri ershad", "authors": "Shervan Fekri Ershad", "title": "Texture Classification Approach Based on Combination of Edge &\n  Co-occurrence and Local Binary Pattern", "comments": "4 pages, 6 figures, 1 tables", "journal-ref": "Int'l Conf. IP, Comp. Vision, and Pattern Recognition, IPCV'11,\n  2011, pp. 626-629", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Texture classification is one of the problems which has been paid much\nattention on by computer scientists since late 90s. If texture classification\nis done correctly and accurately, it can be used in many cases such as Pattern\nrecognition, object tracking, and shape recognition. So far, there have been so\nmany methods offered to solve this problem. Near all these methods have tried\nto extract and define features to separate different labels of textures really\nwell. This article has offered an approach which has an overall process on the\nimages of textures based on Local binary pattern and Gray Level Co-occurrence\nmatrix and then by edge detection, and finally, extracting the statistical\nfeatures from the images would classify them. Although, this approach is a\ngeneral one and is could be used in different applications, the method has been\ntested on the stone texture and the results have been compared with some of the\nprevious approaches to prove the quality of proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 21 Mar 2012 23:33:30 GMT"}], "update_date": "2012-03-23", "authors_parsed": [["Ershad", "Shervan Fekri", ""]]}, {"id": "1203.5443", "submitter": "Martin Pelikan", "authors": "Martin Pelikan, Mark W. Hauschild, and Pier Luca Lanzi", "title": "Transfer Learning, Soft Distance-Based Bias, and the Hierarchical BOA", "comments": "Accepted at Parallel Problem Solving from Nature (PPSN XII), 10\n  pages. arXiv admin note: substantial text overlap with arXiv:1201.2241", "journal-ref": null, "doi": null, "report-no": "MEDAL Report No. 2012004", "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An automated technique has recently been proposed to transfer learning in the\nhierarchical Bayesian optimization algorithm (hBOA) based on distance-based\nstatistics. The technique enables practitioners to improve hBOA efficiency by\ncollecting statistics from probabilistic models obtained in previous hBOA runs\nand using the obtained statistics to bias future hBOA runs on similar problems.\nThe purpose of this paper is threefold: (1) test the technique on several\nclasses of NP-complete problems, including MAXSAT, spin glasses and minimum\nvertex cover; (2) demonstrate that the technique is effective even when\nprevious runs were done on problems of different size; (3) provide empirical\nevidence that combining transfer learning with other efficiency enhancement\ntechniques can often yield nearly multiplicative speedups.\n", "versions": [{"version": "v1", "created": "Sat, 24 Mar 2012 20:11:21 GMT"}, {"version": "v2", "created": "Thu, 21 Jun 2012 12:47:30 GMT"}], "update_date": "2012-06-22", "authors_parsed": [["Pelikan", "Martin", ""], ["Hauschild", "Mark W.", ""], ["Lanzi", "Pier Luca", ""]]}, {"id": "1203.5452", "submitter": "Nesrine Yahia Ben", "authors": "Nesrine Ben Yahia, Narj\\`es Bellamine and Henda Ben Ghezala", "title": "Modeling of Mixed Decision Making Process", "comments": "Keywords-collaborative knowledge management; mixed decision making;\n  dynamicity of actors; UML-G", "journal-ref": "In Proceedings of IEEE International Conference on Information\n  Technology and e-Services 2012, pp. 555-559 ISBN: 978-9938-9511-1-0", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Decision making whenever and wherever it is happened is key to organizations\nsuccess. In order to make correct decision, individuals, teams and\norganizations need both knowledge management (to manage content) and\ncollaboration (to manage group processes) to make that more effective and\nefficient. In this paper, we explain the knowledge management and collaboration\nconvergence. Then, we propose a formal description of mixed and multimodal\ndecision making (MDM) process where decision may be made by three possible\nmodes: individual, collective or hybrid. Finally, we explicit the MDM process\nbased on UML-G profile.\n", "versions": [{"version": "v1", "created": "Sat, 24 Mar 2012 22:18:36 GMT"}], "update_date": "2012-03-27", "authors_parsed": [["Yahia", "Nesrine Ben", ""], ["Bellamine", "Narj\u00e8s", ""], ["Ghezala", "Henda Ben", ""]]}, {"id": "1203.5532", "submitter": "Bruno Scherrer", "authors": "Bruno Scherrer (INRIA Lorraine - LORIA)", "title": "On the Use of Non-Stationary Policies for Infinite-Horizon Discounted\n  Markov Decision Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider infinite-horizon $\\gamma$-discounted Markov Decision Processes,\nfor which it is known that there exists a stationary optimal policy. We\nconsider the algorithm Value Iteration and the sequence of policies\n$\\pi_1,...,\\pi_k$ it implicitely generates until some iteration $k$. We provide\nperformance bounds for non-stationary policies involving the last $m$ generated\npolicies that reduce the state-of-the-art bound for the last stationary policy\n$\\pi_k$ by a factor $\\frac{1-\\gamma}{1-\\gamma^m}$. In particular, the use of\nnon-stationary policies allows to reduce the usual asymptotic performance\nbounds of Value Iteration with errors bounded by $\\epsilon$ at each iteration\nfrom $\\frac{\\gamma}{(1-\\gamma)^2}\\epsilon$ to\n$\\frac{\\gamma}{1-\\gamma}\\epsilon$, which is significant in the usual situation\nwhen $\\gamma$ is close to 1. Given Bellman operators that can only be computed\nwith some error $\\epsilon$, a surprising consequence of this result is that the\nproblem of \"computing an approximately optimal non-stationary policy\" is much\nsimpler than that of \"computing an approximately optimal stationary policy\",\nand even slightly simpler than that of \"approximately computing the value of\nsome fixed policy\", since this last problem only has a guarantee of\n$\\frac{1}{1-\\gamma}\\epsilon$.\n", "versions": [{"version": "v1", "created": "Sun, 25 Mar 2012 19:44:41 GMT"}, {"version": "v2", "created": "Fri, 30 Mar 2012 18:18:05 GMT"}], "update_date": "2012-04-02", "authors_parsed": [["Scherrer", "Bruno", "", "INRIA Lorraine - LORIA"]]}, {"id": "1203.6534", "submitter": "R\\'emy-Robert Joseph", "authors": "R\\'emy-Robert Joseph", "title": "Global preferential consistency for the topological sorting-based\n  maximal spanning tree problem", "comments": "12 pages, 7 figures, conference : Workshop on modeling and solving\n  problems with constraints (ECAI 2008-W31), Patras, Greece, 21 july 2008", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new type of fully computable problems, for DSS dedicated to\nmaximal spanning tree problems, based on deduction and choice: preferential\nconsistency problems. To show its interest, we describe a new compact\nrepresentation of preferences specific to spanning trees, identifying an\nefficient maximal spanning tree sub-problem. Next, we compare this problem with\nthe Pareto-based multiobjective one. And at last, we propose an efficient\nalgorithm solving the associated preferential consistency problem.\n", "versions": [{"version": "v1", "created": "Thu, 29 Mar 2012 14:30:38 GMT"}], "update_date": "2012-03-30", "authors_parsed": [["Joseph", "R\u00e9my-Robert", ""]]}, {"id": "1203.6716", "submitter": "Gopalakrishnan Tr Nair", "authors": "Dr T.R. Gopalakrishnan Nair, Meenakshi Malhotra", "title": "Creating Intelligent Linking for Information Threading in Knowledge\n  Networks", "comments": "5 Pages, 6 Figures, 2 Tables, India Conference (INDICON), 2011", "journal-ref": "India Conference (INDICON), 2011", "doi": "10.1109/INDCON.2011.6139335", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Informledge System (ILS) is a knowledge network with autonomous nodes and\nintelligent links that integrate and structure the pieces of knowledge. In this\npaper, we aim to put forward the link dynamics involved in intelligent\nprocessing of information in ILS. There has been advancement in knowledge\nmanagement field which involve managing information in databases from a single\ndomain. ILS works with information from multiple domains stored in distributed\nway in the autonomous nodes termed as Knowledge Network Node (KNN). Along with\nthe concept under consideration, KNNs store the processed information linking\nconcepts and processors leading to the appropriate processing of information.\n", "versions": [{"version": "v1", "created": "Fri, 30 Mar 2012 05:18:06 GMT"}], "update_date": "2012-04-02", "authors_parsed": [["Nair", "Dr T. R. Gopalakrishnan", ""], ["Malhotra", "Meenakshi", ""]]}]