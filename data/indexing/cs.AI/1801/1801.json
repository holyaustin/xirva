[{"id": "1801.00048", "submitter": "Daniel McNamee", "authors": "Daniel McNamee", "title": "Characterizing optimal hierarchical policy inference on graphs via\n  non-equilibrium thermodynamics", "comments": "NIPS 2017 Workshop on Hierarchical Reinforcement Learning. 8 pages, 1\n  figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.AI q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hierarchies are of fundamental interest in both stochastic optimal control\nand biological control due to their facilitation of a range of desirable\ncomputational traits in a control algorithm and the possibility that they may\nform a core principle of sensorimotor and cognitive control systems. However, a\ntheoretically justified construction of state-space hierarchies over all\nspatial resolutions and their evolution through a policy inference process\nremains elusive. Here, a formalism for deriving such normative representations\nof discrete Markov decision processes is introduced in the context of graphs.\nThe resulting hierarchies correspond to a hierarchical policy inference\nalgorithm approximating a discrete gradient flow between state-space trajectory\ndensities generated by the prior and optimal policies.\n", "versions": [{"version": "v1", "created": "Fri, 29 Dec 2017 22:19:16 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["McNamee", "Daniel", ""]]}, {"id": "1801.00056", "submitter": "Boris Belousov", "authors": "Boris Belousov, Jan Peters", "title": "f-Divergence constrained policy improvement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To ensure stability of learning, state-of-the-art generalized policy\niteration algorithms augment the policy improvement step with a trust region\nconstraint bounding the information loss. The size of the trust region is\ncommonly determined by the Kullback-Leibler (KL) divergence, which not only\ncaptures the notion of distance well but also yields closed-form solutions. In\nthis paper, we consider a more general class of f-divergences and derive the\ncorresponding policy update rules. The generic solution is expressed through\nthe derivative of the convex conjugate function to f and includes the KL\nsolution as a special case. Within the class of f-divergences, we further focus\non a one-parameter family of $\\alpha$-divergences to study effects of the\nchoice of divergence on policy improvement. Previously known as well as new\npolicy updates emerge for different values of $\\alpha$. We show that every type\nof policy update comes with a compatible policy evaluation resulting from the\nchosen f-divergence. Interestingly, the mean-squared Bellman error minimization\nis closely related to policy evaluation with the Pearson $\\chi^2$-divergence\npenalty, while the KL divergence results in the soft-max policy update and a\nlog-sum-exp critic. We carry out asymptotic analysis of the solutions for\ndifferent values of $\\alpha$ and demonstrate the effects of using different\ndivergence functions on a multi-armed bandit problem and on common standard\nreinforcement learning problems.\n", "versions": [{"version": "v1", "created": "Fri, 29 Dec 2017 23:07:26 GMT"}, {"version": "v2", "created": "Wed, 4 Apr 2018 11:36:12 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Belousov", "Boris", ""], ["Peters", "Jan", ""]]}, {"id": "1801.00085", "submitter": "Ruiyi Zhang", "authors": "Ruiyi Zhang, Chunyuan Li, Changyou Chen, Lawrence Carin", "title": "Learning Structural Weight Uncertainty for Sequential Decision-Making", "comments": "Accepted by AISTATS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning probability distributions on the weights of neural networks (NNs)\nhas recently proven beneficial in many applications. Bayesian methods, such as\nStein variational gradient descent (SVGD), offer an elegant framework to reason\nabout NN model uncertainty. However, by assuming independent Gaussian priors\nfor the individual NN weights (as often applied), SVGD does not impose prior\nknowledge that there is often structural information (dependence) among\nweights. We propose efficient posterior learning of structural weight\nuncertainty, within an SVGD framework, by employing matrix variate Gaussian\npriors on NN parameters. We further investigate the learned structural\nuncertainty in sequential decision-making problems, including contextual\nbandits and reinforcement learning. Experiments on several synthetic and real\ndatasets indicate the superiority of our model, compared with state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Sat, 30 Dec 2017 04:34:34 GMT"}, {"version": "v2", "created": "Mon, 2 Apr 2018 01:06:13 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Zhang", "Ruiyi", ""], ["Li", "Chunyuan", ""], ["Chen", "Changyou", ""], ["Carin", "Lawrence", ""]]}, {"id": "1801.00102", "submitter": "Yi Tay", "authors": "Yi Tay, Luu Anh Tuan, Siu Cheung Hui", "title": "Compare, Compress and Propagate: Enhancing Neural Architectures with\n  Alignment Factorization for Natural Language Inference", "comments": "EMNLP 2018 CRC and Update CAFE + ELMo result on SNLI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new deep learning architecture for Natural Language\nInference (NLI). Firstly, we introduce a new architecture where alignment pairs\nare compared, compressed and then propagated to upper layers for enhanced\nrepresentation learning. Secondly, we adopt factorization layers for efficient\nand expressive compression of alignment vectors into scalar features, which are\nthen used to augment the base word representations. The design of our approach\nis aimed to be conceptually simple, compact and yet powerful. We conduct\nexperiments on three popular benchmarks, SNLI, MultiNLI and SciTail, achieving\ncompetitive performance on all. A lightweight parameterization of our model\nalso enjoys a $\\approx 3$ times reduction in parameter size compared to the\nexisting state-of-the-art models, e.g., ESIM and DIIN, while maintaining\ncompetitive performance. Additionally, visual analysis shows that our\npropagated features are highly interpretable.\n", "versions": [{"version": "v1", "created": "Sat, 30 Dec 2017 08:54:16 GMT"}, {"version": "v2", "created": "Mon, 10 Sep 2018 14:38:47 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Tay", "Yi", ""], ["Tuan", "Luu Anh", ""], ["Hui", "Siu Cheung", ""]]}, {"id": "1801.00215", "submitter": "Simon Stiebellehner", "authors": "Simon Stiebellehner, Jun Wang, Shuai Yuan", "title": "Learning Continuous User Representations through Hybrid Filtering with\n  doc2vec", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Players in the online ad ecosystem are struggling to acquire the user data\nrequired for precise targeting. Audience look-alike modeling has the potential\nto alleviate this issue, but models' performance strongly depends on quantity\nand quality of available data. In order to maximize the predictive performance\nof our look-alike modeling algorithms, we propose two novel hybrid filtering\ntechniques that utilize the recent neural probabilistic language model\nalgorithm doc2vec. We apply these methods to data from a large mobile ad\nexchange and additional app metadata acquired from the Apple App store and\nGoogle Play store. First, we model mobile app users through their app usage\nhistories and app descriptions (user2vec). Second, we introduce context\nawareness to that model by incorporating additional user and app-related\nmetadata in model training (context2vec). Our findings are threefold: (1) the\nquality of recommendations provided by user2vec is notably higher than current\nstate-of-the-art techniques. (2) User representations generated through hybrid\nfiltering using doc2vec prove to be highly valuable features in supervised\nmachine learning models for look-alike modeling. This represents the first\napplication of hybrid filtering user models using neural probabilistic language\nmodels, specifically doc2vec, in look-alike modeling. (3) Incorporating context\nmetadata in the doc2vec model training process to introduce context awareness\nhas positive effects on performance and is superior to directly including the\ndata as features in the downstream supervised models.\n", "versions": [{"version": "v1", "created": "Sun, 31 Dec 2017 00:51:56 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Stiebellehner", "Simon", ""], ["Wang", "Jun", ""], ["Yuan", "Shuai", ""]]}, {"id": "1801.00218", "submitter": "Mateusz Krzysztof Tarkowski", "authors": "Mateusz K. Tarkowski, Tomasz P. Michalak, Talal Rahwan, Michael\n  Wooldridge", "title": "Game-theoretic Network Centrality: A Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Game-theoretic centrality is a flexible and sophisticated approach to\nidentify the most important nodes in a network. It builds upon the methods from\ncooperative game theory and network theory. The key idea is to treat nodes as\nplayers in a cooperative game, where the value of each coalition is determined\nby certain graph-theoretic properties. Using solution concepts from cooperative\ngame theory, it is then possible to measure how responsible each node is for\nthe worth of the network.\n  The literature on the topic is already quite large, and is scattered among\ngame-theoretic and computer science venues. We review the main game-theoretic\nnetwork centrality measures from both bodies of literature and organize them\ninto two categories: those that are more focused on the connectivity of nodes,\nand those that are more focused on the synergies achieved by nodes in groups.\nWe present and explain each centrality, with a focus on algorithms and\ncomplexity.\n", "versions": [{"version": "v1", "created": "Sun, 31 Dec 2017 01:10:39 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Tarkowski", "Mateusz K.", ""], ["Michalak", "Tomasz P.", ""], ["Rahwan", "Talal", ""], ["Wooldridge", "Michael", ""]]}, {"id": "1801.00345", "submitter": "Mehdi Maamar", "authors": "Christian Bessiere and Nadjib Lazaar and Yahia Lebbah and Mehdi Maamar", "title": "Users Constraints in Itemset Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering significant itemsets is one of the fundamental problems in data\nmining. It has recently been shown that constraint programming is a flexible\nway to tackle data mining tasks. With a constraint programming approach, we can\neasily express and efficiently answer queries with users constraints on items.\nHowever, in many practical cases it is possible that queries also express users\nconstraints on the dataset itself. For instance, asking for a particular\nitemset in a particular part of the dataset. This paper presents a general\nconstraint programming model able to handle any kind of query on the items or\nthe dataset for itemset mining.\n", "versions": [{"version": "v1", "created": "Sun, 31 Dec 2017 19:55:52 GMT"}, {"version": "v2", "created": "Thu, 8 Feb 2018 16:21:54 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Bessiere", "Christian", ""], ["Lazaar", "Nadjib", ""], ["Lebbah", "Yahia", ""], ["Maamar", "Mehdi", ""]]}, {"id": "1801.00356", "submitter": "Amit Sheth", "authors": "Amit Sheth, Utkarshani Jaimini, Hong Yung Yip", "title": "How will the Internet of Things enable Augmented Personalized Health?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet-of-Things (IoT) is profoundly redefining the way we create, consume,\nand share information. Health aficionados and citizens are increasingly using\nIoT technologies to track their sleep, food intake, activity, vital body\nsignals, and other physiological observations. This is complemented by IoT\nsystems that continuously collect health-related data from the environment and\ninside the living quarters. Together, these have created an opportunity for a\nnew generation of healthcare solutions. However, interpreting data to\nunderstand an individual's health is challenging. It is usually necessary to\nlook at that individual's clinical record and behavioral information, as well\nas social and environmental information affecting that individual. Interpreting\nhow well a patient is doing also requires looking at his adherence to\nrespective health objectives, application of relevant clinical knowledge and\nthe desired outcomes.\n  We resort to the vision of Augmented Personalized Healthcare (APH) to exploit\nthe extensive variety of relevant data and medical knowledge using Artificial\nIntelligence (AI) techniques to extend and enhance human health to presents\nvarious stages of augmented health management strategies: self-monitoring,\nself-appraisal, self-management, intervention, and disease progress tracking\nand prediction. kHealth technology, a specific incarnation of APH, and its\napplication to Asthma and other diseases are used to provide illustrations and\ndiscuss alternatives for technology-assisted health management. Several\nprominent efforts involving IoT and patient-generated health data (PGHD) with\nrespect converting multimodal data into actionable information (big data to\nsmart data) are also identified. Roles of three components in an evidence-based\nsemantic perception approach- Contextualization, Abstraction, and\nPersonalization are discussed.\n", "versions": [{"version": "v1", "created": "Sun, 31 Dec 2017 21:18:16 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Sheth", "Amit", ""], ["Jaimini", "Utkarshani", ""], ["Yip", "Hong Yung", ""]]}, {"id": "1801.00361", "submitter": "Jason Toy", "authors": "Jason Toy", "title": "SenseNet: 3D Objects Database and Tactile Simulator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of artificial intelligence research, as it relates from which to\nbiological senses has been focused on vision. The recent explosion of machine\nlearning and in particular, dee p learning, can be partially attributed to the\nrelease of high quality data sets for algorithm s from which to model the world\non. Thus, most of these datasets are comprised of images. We believe that\nfocusing on sensorimotor systems and tactile feedback will create algorithms\nthat better mimic human intelligence. Here we present SenseNet: a collection of\ntactile simulators and a large scale dataset of 3D objects for manipulation.\nSenseNet was created for the purpose of researching and training Artificial\nIntelligences (AIs) to interact with the environment via sensorimotor neural\nsystems and tactile feedback. We aim to accelerate that same explosion in image\nprocessing, but for the domain of tactile feedback and sensorimotor research.\nWe hope that SenseNet can offer researchers in both the machine learning and\ncomputational neuroscience communities brand new opportunities and avenues to\nexplore.\n", "versions": [{"version": "v1", "created": "Sun, 31 Dec 2017 21:50:15 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Toy", "Jason", ""]]}, {"id": "1801.00388", "submitter": "Walid Shalaby", "authors": "Walid Shalaby, Wlodek Zadrozny, and Hongxia Jin", "title": "Beyond Word Embeddings: Learning Entity and Concept Representations from\n  Large Scale Knowledge Bases", "comments": "arXiv admin note: text overlap with arXiv:1702.03342", "journal-ref": "Inf Retrieval J (2018)", "doi": "10.1007/s10791-018-9340-3", "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text representations using neural word embeddings have proven effective in\nmany NLP applications. Recent researches adapt the traditional word embedding\nmodels to learn vectors of multiword expressions (concepts/entities). However,\nthese methods are limited to textual knowledge bases (e.g., Wikipedia). In this\npaper, we propose a novel and simple technique for integrating the knowledge\nabout concepts from two large scale knowledge bases of different structure\n(Wikipedia and Probase) in order to learn concept representations. We adapt the\nefficient skip-gram model to seamlessly learn from the knowledge in Wikipedia\ntext and Probase concept graph. We evaluate our concept embedding models on two\ntasks: (1) analogical reasoning, where we achieve a state-of-the-art\nperformance of 91% on semantic analogies, (2) concept categorization, where we\nachieve a state-of-the-art performance on two benchmark datasets achieving\ncategorization accuracy of 100% on one and 98% on the other. Additionally, we\npresent a case study to evaluate our model on unsupervised argument type\nidentification for neural semantic parsing. We demonstrate the competitive\naccuracy of our unsupervised method and its ability to better generalize to out\nof vocabulary entity mentions compared to the tedious and error prone methods\nwhich depend on gazetteers and regular expressions.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jan 2018 03:43:30 GMT"}, {"version": "v2", "created": "Thu, 30 Aug 2018 21:54:56 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Shalaby", "Walid", ""], ["Zadrozny", "Wlodek", ""], ["Jin", "Hongxia", ""]]}, {"id": "1801.00453", "submitter": "Alex James Dr", "authors": "Akzharkyn Izbassarova, Aidana Irmanova, A. P. James", "title": "Automated rating of recorded classroom presentations using speech\n  analysis in kazakh", "comments": null, "journal-ref": "2017 International Conference on Advances in Computing,\n  Communications and Informatics (ICACCI), Udupi, 2017, pp. 393-397", "doi": "10.1109/ICACCI.2017.8125872", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective presentation skills can help to succeed in business, career and\nacademy. This paper presents the design of speech assessment during the oral\npresentation and the algorithm for speech evaluation based on criteria of\noptimal intonation. As the pace of the speech and its optimal intonation varies\nfrom language to language, developing an automatic identification of language\nduring the presentation is required. Proposed algorithm was tested with\npresentations delivered in Kazakh language. For testing purposes the features\nof Kazakh phonemes were extracted using MFCC and PLP methods and created a\nHidden Markov Model (HMM) [5], [5] of Kazakh phonemes. Kazakh vowel formants\nwere defined and the correlation between the deviation rate in fundamental\nfrequency and the liveliness of the speech to evaluate intonation of the\npresentation was analyzed. It was established that the threshold value between\nmonotone and dynamic speech is 0.16 and the error for intonation evaluation is\n19%.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jan 2018 14:56:56 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Izbassarova", "Akzharkyn", ""], ["Irmanova", "Aidana", ""], ["James", "A. P.", ""]]}, {"id": "1801.00512", "submitter": "Haik Manukian", "authors": "Haik Manukian, Fabio L. Traversa, Massimiliano Di Ventra", "title": "Accelerating Deep Learning with Memcomputing", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.ET cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Restricted Boltzmann machines (RBMs) and their extensions, called\n'deep-belief networks', are powerful neural networks that have found\napplications in the fields of machine learning and artificial intelligence. The\nstandard way to training these models resorts to an iterative unsupervised\nprocedure based on Gibbs sampling, called 'contrastive divergence' (CD), and\nadditional supervised tuning via back-propagation. However, this procedure has\nbeen shown not to follow any gradient and can lead to suboptimal solutions. In\nthis paper, we show an efficient alternative to CD by means of simulations of\ndigital memcomputing machines (DMMs). We test our approach on pattern\nrecognition using a modified version of the MNIST data set. DMMs sample\neffectively the vast phase space given by the model distribution of the RBM,\nand provide a very good approximation close to the optimum. This efficient\nsearch significantly reduces the number of pretraining iterations necessary to\nachieve a given level of accuracy, as well as a total performance gain over CD.\nIn fact, the acceleration of pretraining achieved by simulating DMMs is\ncomparable to, in number of iterations, the recently reported hardware\napplication of the quantum annealing method on the same network and data set.\nNotably, however, DMMs perform far better than the reported quantum annealing\nresults in terms of quality of the training. We also compare our method to\nadvances in supervised training, like batch-normalization and rectifiers, that\nwork to reduce the advantage of pretraining. We find that the memcomputing\nmethod still maintains a quality advantage ($>1\\%$ in accuracy, and a $20\\%$\nreduction in error rate) over these approaches. Furthermore, our method is\nagnostic about the connectivity of the network. Therefore, it can be extended\nto train full Boltzmann machines, and even deep networks at once.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jan 2018 21:27:11 GMT"}, {"version": "v2", "created": "Wed, 24 Jan 2018 01:33:19 GMT"}, {"version": "v3", "created": "Tue, 23 Oct 2018 19:23:11 GMT"}], "update_date": "2018-10-25", "authors_parsed": [["Manukian", "Haik", ""], ["Traversa", "Fabio L.", ""], ["Di Ventra", "Massimiliano", ""]]}, {"id": "1801.00602", "submitter": "Kai Qiao", "authors": "Kai Qiao, Chi Zhang, Linyuan Wang, Bin Yan, Jian Chen, Lei Zeng, Li\n  Tong", "title": "Accurate reconstruction of image stimuli from human fMRI based on the\n  decoding model with capsule network architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In neuroscience, all kinds of computation models were designed to answer the\nopen question of how sensory stimuli are encoded by neurons and conversely, how\nsensory stimuli can be decoded from neuronal activities. Especially, functional\nMagnetic Resonance Imaging (fMRI) studies have made many great achievements\nwith the rapid development of the deep network computation. However, comparing\nwith the goal of decoding orientation, position and object category from\nactivities in visual cortex, accurate reconstruction of image stimuli from\nhuman fMRI is a still challenging work. In this paper, the capsule network\n(CapsNet) architecture based visual reconstruction (CNAVR) method is developed\nto reconstruct image stimuli. The capsule means containing a group of neurons\nto perform the better organization of feature structure and representation,\ninspired by the structure of cortical mini column including several hundred\nneurons in primates. The high-level capsule features in the CapsNet includes\ndiverse features of image stimuli such as semantic class, orientation, location\nand so on. We used these features to bridge between human fMRI and image\nstimuli. We firstly employed the CapsNet to train the nonlinear mapping from\nimage stimuli to high-level capsule features, and from high-level capsule\nfeatures to image stimuli again in an end-to-end manner. After estimating the\nserviceability of each voxel by encoding performance to accomplish the\nselecting of voxels, we secondly trained the nonlinear mapping from\ndimension-decreasing fMRI data to high-level capsule features. Finally, we can\npredict the high-level capsule features with fMRI data, and reconstruct image\nstimuli with the CapsNet. We evaluated the proposed CNAVR method on the dataset\nof handwritten digital images, and exceeded about 10% than the accuracy of all\nexisting state-of-the-art methods on the structural similarity index (SSIM).\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 10:39:05 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Qiao", "Kai", ""], ["Zhang", "Chi", ""], ["Wang", "Linyuan", ""], ["Yan", "Bin", ""], ["Chen", "Jian", ""], ["Zeng", "Lei", ""], ["Tong", "Li", ""]]}, {"id": "1801.00631", "submitter": "Gary Marcus", "authors": "Gary Marcus", "title": "Deep Learning: A Critical Appraisal", "comments": "1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep learning has historical roots going back decades, neither the\nterm \"deep learning\" nor the approach was popular just over five years ago,\nwhen the field was reignited by papers such as Krizhevsky, Sutskever and\nHinton's now classic (2012) deep network model of Imagenet. What has the field\ndiscovered in the five subsequent years? Against a background of considerable\nprogress in areas such as speech recognition, image recognition, and game\nplaying, and considerable enthusiasm in the popular press, I present ten\nconcerns for deep learning, and suggest that deep learning must be supplemented\nby other techniques if we are to reach artificial general intelligence.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 12:49:35 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Marcus", "Gary", ""]]}, {"id": "1801.00688", "submitter": "Nicola Strisciuglio", "authors": "Nicola Strisciuglio", "title": "Learning audio and image representations with bio-inspired trainable\n  feature extractors", "comments": "Accepted for publication in the journal \"Eleectronic Letters on\n  Computer Vision and Image Understanding\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.SD eess.AS eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advancements in pattern recognition and signal processing concern the\nautomatic learning of data representations from labeled training samples.\nTypical approaches are based on deep learning and convolutional neural\nnetworks, which require large amount of labeled training samples. In this work,\nwe propose novel feature extractors that can be used to learn the\nrepresentation of single prototype samples in an automatic configuration\nprocess. We employ the proposed feature extractors in applications of audio and\nimage processing, and show their effectiveness on benchmark data sets.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 15:34:03 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Strisciuglio", "Nicola", ""]]}, {"id": "1801.00690", "submitter": "Yuval Tassa", "authors": "Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li, Diego\n  de Las Casas, David Budden, Abbas Abdolmaleki, Josh Merel, Andrew Lefrancq,\n  Timothy Lillicrap, Martin Riedmiller", "title": "DeepMind Control Suite", "comments": "24 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The DeepMind Control Suite is a set of continuous control tasks with a\nstandardised structure and interpretable rewards, intended to serve as\nperformance benchmarks for reinforcement learning agents. The tasks are written\nin Python and powered by the MuJoCo physics engine, making them easy to use and\nmodify. We include benchmarks for several learning algorithms. The Control\nSuite is publicly available at https://www.github.com/deepmind/dm_control . A\nvideo summary of all tasks is available at http://youtu.be/rAai4QzcYbs .\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 15:48:14 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Tassa", "Yuval", ""], ["Doron", "Yotam", ""], ["Muldal", "Alistair", ""], ["Erez", "Tom", ""], ["Li", "Yazhe", ""], ["Casas", "Diego de Las", ""], ["Budden", "David", ""], ["Abdolmaleki", "Abbas", ""], ["Merel", "Josh", ""], ["Lefrancq", "Andrew", ""], ["Lillicrap", "Timothy", ""], ["Riedmiller", "Martin", ""]]}, {"id": "1801.00702", "submitter": "Xinyang Deng", "authors": "Xinyang Deng and Wen Jiang", "title": "A total uncertainty measure for D numbers based on belief intervals", "comments": "14 pages, 2 figures. arXiv admin note: text overlap with\n  arXiv:1711.09186", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a generalization of Dempster-Shafer theory, the theory of D numbers is a\nnew theoretical framework for uncertainty reasoning. Measuring the uncertainty\nof knowledge or information represented by D numbers is an unsolved issue in\nthat theory. In this paper, inspired by distance based uncertainty measures for\nDempster-Shafer theory, a total uncertainty measure for a D number is proposed\nbased on its belief intervals. The proposed total uncertainty measure can\nsimultaneously capture the discord, and non-specificity, and non-exclusiveness\ninvolved in D numbers. And some basic properties of this total uncertainty\nmeasure, including range, monotonicity, generalized set consistency, are also\npresented.\n", "versions": [{"version": "v1", "created": "Mon, 25 Dec 2017 12:51:25 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Deng", "Xinyang", ""], ["Jiang", "Wen", ""]]}, {"id": "1801.00712", "submitter": "Luis Meira", "authors": "Luis A. A. Meira, Paulo S. Martins, Mauro Menzori, Guilherme A. Zeni", "title": "Multi-Objective Vehicle Routing Problem Applied to Large Scale Post\n  Office Deliveries", "comments": "arXiv admin note: text overlap with arXiv:1610.05402", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of optimization techniques in the combinatorial domain is large\nand diversified. Nevertheless, real-world based benchmarks for testing\nalgorithms are few. This work creates an extensible real-world mail delivery\nbenchmark to the Vehicle Routing Problem (VRP) in a planar graph embedded in\nthe 2D Euclidean space. Such problem is multi-objective on a roadmap with up to\n25 vehicles and 30,000 deliveries per day. Each instance models one generic day\nof mail delivery, allowing both comparison and validation of optimization\nalgorithms for routing problems. The benchmark may be extended to model other\nscenarios.\n", "versions": [{"version": "v1", "created": "Sat, 23 Dec 2017 14:23:22 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Meira", "Luis A. A.", ""], ["Martins", "Paulo S.", ""], ["Menzori", "Mauro", ""], ["Zeni", "Guilherme A.", ""]]}, {"id": "1801.00723", "submitter": "Pegah Karimi", "authors": "Pegah Karimi, Nicholas Davis, Kazjon Grace, Mary Lou Maher", "title": "Deep Learning for Identifying Potential Conceptual Shifts for\n  Co-creative Drawing", "comments": "This is an extended version of the paper presented at 31st Conference\n  on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\n  Workshop on Machine Learning for Creativity and Design", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system for identifying conceptual shifts between visual\ncategories, which will form the basis for a co-creative drawing system to help\nusers draw more creative sketches. The system recognizes human sketches and\nmatches them to structurally similar sketches from categories to which they do\nnot belong. This would allow a co-creative drawing system to produce an\nambiguous sketch that blends features from both categories.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 16:51:22 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Karimi", "Pegah", ""], ["Davis", "Nicholas", ""], ["Grace", "Kazjon", ""], ["Maher", "Mary Lou", ""]]}, {"id": "1801.00727", "submitter": "David Heckerman", "authors": "David Heckerman", "title": "Accounting for hidden common causes when inferring cause and effect from\n  observational data", "comments": "Presented at the NIPS workshop on causal inference (NIPS 2017), Long\n  Beach, CA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying causal relationships from observation data is difficult, in large\npart, due to the presence of hidden common causes. In some cases, where just\nthe right patterns of conditional independence and dependence lie in the\ndata---for example, Y-structures---it is possible to identify cause and effect.\nIn other cases, the analyst deliberately makes an uncertain assumption that\nhidden common causes are absent, and infers putative causal relationships to be\ntested in a randomized trial. Here, we consider a third approach, where there\nare sufficient clues in the data such that hidden common causes can be\ninferred.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 17:04:49 GMT"}, {"version": "v2", "created": "Wed, 3 Jan 2018 23:29:07 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Heckerman", "David", ""]]}, {"id": "1801.00743", "submitter": "Claudio Alexandre", "authors": "Claudio Alexandre and Jo\\~ao Balsa", "title": "Um Sistema Multiagente no Combate ao Braqueamento de Capitais", "comments": "17 pages, in Portuguese, 7 figures", "journal-ref": "RISTI, 25 12/2017", "doi": "10.17013/risti.25.1-17", "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Money laundering is a crime that makes it possible to finance other crimes,\nfor this reason, it is important for criminal organizations and their combat is\nprioritized by nations around the world. The anti-money laundering process has\nnot evolved as expected because it has prioritized only the signaling of\nsuspicious transactions. The constant increasing in the volume of transactions\nhas overloaded the indispensable human work of final evaluation of the\nsuspicions. This article presents a multiagent system that aims to go beyond\nthe capture of suspicious transactions, seeking to assist the human expert in\nthe analysis of suspicions. The agents created use data mining techniques to\ncreate transactional behavioral profiles; apply rules generated in learning\nprocess in conjunction with specific rules based on legal aspects and profiles\ncreated to capture suspicious transactions; and analyze these suspicious\ntransactions indicating to the human expert those that require more detailed\nanalysis.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 17:45:28 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Alexandre", "Claudio", ""], ["Balsa", "Jo\u00e3o", ""]]}, {"id": "1801.00815", "submitter": "Michael J. Kurtz", "authors": "Michael J. Kurtz", "title": "Advice from the Oracle: Really Intelligent Information Retrieval", "comments": "Author copy; published 25 years ago at the beginning of the\n  Astrophysics Data System; 2018 keywords added", "journal-ref": "In: Heck A., Murtagh F. (eds) Intelligent Information Retrieval:\n  The Case of Astronomy and Related Space Sciences. Astrophysics and Space\n  Science Library, vol 182. Springer, Dordrecht (1993)", "doi": "10.1007/978-0-585-33110-2_3", "report-no": null, "categories": "cs.AI astro-ph.IM physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What is \"intelligent\" information retrieval? Essentially this is asking what\nis intelligence, in this article I will attempt to show some of the aspects of\nhuman intelligence, as related to information retrieval. I will do this by the\ndevice of a semi-imaginary Oracle. Every Observatory has an oracle, someone who\nis a distinguished scientist, has great administrative responsibilities, acts\nas mentor to a number of less senior people, and as trusted advisor to even the\nmost accomplished scientists, and knows essentially everyone in the field. In\nan appendix I will present a brief summary of the Statistical Factor Space\nmethod for text indexing and retrieval, and indicate how it will be used in the\nAstrophysics Data System Abstract Service. 2018 Keywords: Personal Digital\nAssistant; Supervised Topic Models\n", "versions": [{"version": "v1", "created": "Tue, 2 Jan 2018 19:36:22 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Kurtz", "Michael J.", ""]]}, {"id": "1801.00984", "submitter": "Abdelkrime Aries", "authors": "Abdelkrime Aries, Djamel Eddine Zegour, Walid Khaled Hidouci", "title": "Sentence Object Notation: Multilingual sentence notation based on\n  Wordnet", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The representation of sentences is a very important task. It can be used as a\nway to exchange data inter-applications. One main characteristic, that a\nnotation must have, is a minimal size and a representative form. This can\nreduce the transfer time, and hopefully the processing time as well.\n  Usually, sentence representation is associated to the processed language. The\ngrammar of this language affects how we represent the sentence. To avoid\nlanguage-dependent notations, we have to come up with a new representation\nwhich don't use words, but their meanings. This can be done using a lexicon\nlike wordnet, instead of words we use their synsets. As for syntactic\nrelations, they have to be universal as much as possible.\n  Our new notation is called STON \"SenTences Object Notation\", which somehow\nhas similarities to JSON. It is meant to be minimal, representative and\nlanguage-independent syntactic representation. Also, we want it to be readable\nand easy to be created. This simplifies developing simple automatic generators\nand creating test banks manually. Its benefit is to be used as a medium between\ndifferent parts of applications like: text summarization, language translation,\netc. The notation is based on 4 languages: Arabic, English, Franch and\nJapanese; and there are some cases where these languages don't agree on one\nrepresentation. Also, given the diversity of grammatical structure of different\nworld languages, this annotation may fail for some languages which allows more\nfuture improvements.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 13:02:25 GMT"}, {"version": "v2", "created": "Wed, 10 Jan 2018 19:09:39 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Aries", "Abdelkrime", ""], ["Zegour", "Djamel Eddine", ""], ["Hidouci", "Walid Khaled", ""]]}, {"id": "1801.01000", "submitter": "Christopher Schulze", "authors": "Christopher Schulze, Marcus Schulze", "title": "ViZDoom: DRQN with Prioritized Experience Replay, Double-Q Learning, &\n  Snapshot Ensembling", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ViZDoom is a robust, first-person shooter reinforcement learning environment,\ncharacterized by a significant degree of latent state information. In this\npaper, double-Q learning and prioritized experience replay methods are tested\nunder a certain ViZDoom combat scenario using a competitive deep recurrent\nQ-network (DRQN) architecture. In addition, an ensembling technique known as\nsnapshot ensembling is employed using a specific annealed learning rate to\nobserve differences in ensembling efficacy under these two methods. Annealed\nlearning rates are important in general to the training of deep neural network\nmodels, as they shake up the status-quo and counter a model's tending towards\nlocal optima. While both variants show performance exceeding those of built-in\nAI agents of the game, the known stabilizing effects of double-Q learning are\nillustrated, and priority experience replay is again validated in its\nusefulness by showing immediate results early on in agent development, with the\ncaveat that value overestimation is accelerated in this case. In addition, some\nunique behaviors are observed to develop for priority experience replay (PER)\nand double-Q (DDQ) variants, and snapshot ensembling of both PER and DDQ proves\na valuable method for improving performance of the ViZDoom Marine.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 13:49:08 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Schulze", "Christopher", ""], ["Schulze", "Marcus", ""]]}, {"id": "1801.01102", "submitter": "Barathi Ganesh H B", "authors": "Barathi Ganesh HB", "title": "Social Media Analysis based on Semanticity of Streaming and Batch Data", "comments": "54 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Languages shared by people differ in different regions based on their\naccents, pronunciation and word usages. In this era sharing of language takes\nplace mainly through social media and blogs. Every second swing of such a micro\nposts exist which induces the need of processing those micro posts, in-order to\nextract knowledge out of it. Knowledge extraction differs with respect to the\napplication in which the research on cognitive science fed the necessities for\nthe same. This work further moves forward such a research by extracting\nsemantic information of streaming and batch data in applications like Named\nEntity Recognition and Author Profiling. In the case of Named Entity\nRecognition context of a single micro post has been utilized and context that\nlies in the pool of micro posts were utilized to identify the sociolect aspects\nof the author of those micro posts. In this work Conditional Random Field has\nbeen utilized to do the entity recognition and a novel approach has been\nproposed to find the sociolect aspects of the author (Gender, Age group).\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 18:23:41 GMT"}, {"version": "v2", "created": "Fri, 5 Jan 2018 05:40:59 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["HB", "Barathi Ganesh", ""]]}, {"id": "1801.01143", "submitter": "Tanya Khovanova", "authors": "Tanya Khovanova", "title": "Coins and Logic", "comments": "18 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.HO cs.AI math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish fun parallels between coin-weighing puzzles and\nknights-and-knaves puzzles.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 19:13:14 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Khovanova", "Tanya", ""]]}, {"id": "1801.01228", "submitter": "Aayush Gupta", "authors": "Aayush Gupta, Daniel Bessonov, Patrick Li", "title": "A Decision-theoretic Approach to Detection-based Target Search with a\n  UAV", "comments": "Published in IEEE IROS 2017. 6 pages", "journal-ref": "A. Gupta, D. Bessonov and P. Li, \"A decision-theoretic approach to\n  detection-based target search with a UAV,\" 2017 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS), Vancouver, BC, Canada,\n  2017, pp. 5304-5309", "doi": "10.1109/IROS.2017.8206423", "report-no": null, "categories": "cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Search and rescue missions and surveillance require finding targets in a\nlarge area. These tasks often use unmanned aerial vehicles (UAVs) with cameras\nto detect and move towards a target. However, common UAV approaches make two\nsimplifying assumptions. First, they assume that observations made from\ndifferent heights are deterministically correct. In practice, observations are\nnoisy, with the noise increasing as the height used for observations increases.\nSecond, they assume that a motion command executes correctly, which may not\nhappen due to wind and other environmental factors. To address these, we\npropose a sequential algorithm that determines actions in real time based on\nobservations, using partially observable Markov decision processes (POMDPs).\nOur formulation handles both observations and motion uncertainty and errors. We\nrun offline simulations and learn a policy. This policy is run on a UAV to find\nthe target efficiently. We employ a novel compact formulation to represent the\ncoordinates of the drone relative to the target coordinates. Our POMDP policy\nfinds the target up to 3.4 times faster when compared to a heuristic policy.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 02:29:19 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Gupta", "Aayush", ""], ["Bessonov", "Daniel", ""], ["Li", "Patrick", ""]]}, {"id": "1801.01253", "submitter": "Reinhard Heckel", "authors": "Reinhard Heckel and Max Simchowitz and Kannan Ramchandran and Martin\n  J. Wainwright", "title": "Approximate Ranking from Pairwise Comparisons", "comments": "AISTATS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common problem in machine learning is to rank a set of n items based on\npairwise comparisons. Here ranking refers to partitioning the items into sets\nof pre-specified sizes according to their scores, which includes identification\nof the top-k items as the most prominent special case. The score of a given\nitem is defined as the probability that it beats a randomly chosen other item.\nFinding an exact ranking typically requires a prohibitively large number of\ncomparisons, but in practice, approximate rankings are often adequate.\nAccordingly, we study the problem of finding approximate rankings from pairwise\ncomparisons. We analyze an active ranking algorithm that counts the number of\ncomparisons won, and decides whether to stop or which pair of items to compare\nnext, based on confidence intervals computed from the data collected in\nprevious steps. We show that this algorithm succeeds in recovering approximate\nrankings using a number of comparisons that is close to optimal up to\nlogarithmic factors. We also present numerical results, showing that in\npractice, approximation can drastically reduce the number of comparisons\nrequired to estimate a ranking.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 06:18:39 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Heckel", "Reinhard", ""], ["Simchowitz", "Max", ""], ["Ramchandran", "Kannan", ""], ["Wainwright", "Martin J.", ""]]}, {"id": "1801.01258", "submitter": "Jong Chul Ye", "authors": "Yoseob Han, Jingu Kang, and Jong Chul Ye", "title": "Deep Learning Reconstruction for 9-View Dual Energy CT Baggage Scanner", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For homeland and transportation security applications, 2D X-ray explosive\ndetection system (EDS) have been widely used, but they have limitations in\nrecognizing 3D shape of the hidden objects. Among various types of 3D computed\ntomography (CT) systems to address this issue, this paper is interested in a\nstationary CT using fixed X-ray sources and detectors. However, due to the\nlimited number of projection views, analytic reconstruction algorithms produce\nsevere streaking artifacts. Inspired by recent success of deep learning\napproach for sparse view CT reconstruction, here we propose a novel image and\nsinogram domain deep learning architecture for 3D reconstruction from very\nsparse view measurement. The algorithm has been tested with the real data from\na prototype 9-view dual energy stationary CT EDS carry-on baggage scanner\ndeveloped by GEMSS Medical Systems, Korea, which confirms the superior\nreconstruction performance over the existing approaches.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 06:35:53 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Han", "Yoseob", ""], ["Kang", "Jingu", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1801.01290", "submitter": "Tuomas Haarnoja", "authors": "Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, Sergey Levine", "title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement\n  Learning with a Stochastic Actor", "comments": "ICML 2018 Videos: sites.google.com/view/soft-actor-critic Code:\n  github.com/haarnoja/sac", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-free deep reinforcement learning (RL) algorithms have been demonstrated\non a range of challenging decision making and control tasks. However, these\nmethods typically suffer from two major challenges: very high sample complexity\nand brittle convergence properties, which necessitate meticulous hyperparameter\ntuning. Both of these challenges severely limit the applicability of such\nmethods to complex, real-world domains. In this paper, we propose soft\nactor-critic, an off-policy actor-critic deep RL algorithm based on the maximum\nentropy reinforcement learning framework. In this framework, the actor aims to\nmaximize expected reward while also maximizing entropy. That is, to succeed at\nthe task while acting as randomly as possible. Prior deep RL methods based on\nthis framework have been formulated as Q-learning methods. By combining\noff-policy updates with a stable stochastic actor-critic formulation, our\nmethod achieves state-of-the-art performance on a range of continuous control\nbenchmark tasks, outperforming prior on-policy and off-policy methods.\nFurthermore, we demonstrate that, in contrast to other off-policy algorithms,\nour approach is very stable, achieving very similar performance across\ndifferent random seeds.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 09:50:50 GMT"}, {"version": "v2", "created": "Wed, 8 Aug 2018 21:27:08 GMT"}], "update_date": "2018-08-10", "authors_parsed": [["Haarnoja", "Tuomas", ""], ["Zhou", "Aurick", ""], ["Abbeel", "Pieter", ""], ["Levine", "Sergey", ""]]}, {"id": "1801.01422", "submitter": "Louise Dennis Dr", "authors": "Louise Dennis and Michael Fisher", "title": "Practical Challenges in Explicit Ethical Machine Reasoning", "comments": "In proceedings International Conference on Artificial Intelligence\n  and Mathematics, Fort Lauderdale, Florida, FL. 3-5 January, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine implemented systems for ethical machine reasoning with a view to\nidentifying the practical challenges (as opposed to philosophical challenges)\nposed by the area. We identify a need for complex ethical machine reasoning not\nonly to be multi-objective, proactive, and scrutable but that it must draw on\nheterogeneous evidential reasoning. We also argue that, in many cases, it needs\nto operate in real time and be verifiable. We propose a general architecture\ninvolving a declarative ethical arbiter which draws upon multiple evidential\nreasoners each responsible for a particular ethical feature of the system's\nenvironment. We claim that this architecture enables some separation of\nconcerns among the practical challenges that ethical machine reasoning poses.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 16:19:33 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Dennis", "Louise", ""], ["Fisher", "Michael", ""]]}, {"id": "1801.01423", "submitter": "Joan Serr\\`a", "authors": "Joan Serr\\`a, D\\'idac Sur\\'is, Marius Miron, Alexandros Karatzoglou", "title": "Overcoming catastrophic forgetting with hard attention to the task", "comments": "Includes appendix. Accepted for ICML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Catastrophic forgetting occurs when a neural network loses the information\nlearned in a previous task after training on subsequent tasks. This problem\nremains a hurdle for artificial intelligence systems with sequential learning\ncapabilities. In this paper, we propose a task-based hard attention mechanism\nthat preserves previous tasks' information without affecting the current task's\nlearning. A hard attention mask is learned concurrently to every task, through\nstochastic gradient descent, and previous masks are exploited to condition such\nlearning. We show that the proposed mechanism is effective for reducing\ncatastrophic forgetting, cutting current rates by 45 to 80%. We also show that\nit is robust to different hyperparameter choices, and that it offers a number\nof monitoring capabilities. The approach features the possibility to control\nboth the stability and compactness of the learned knowledge, which we believe\nmakes it also attractive for online learning or network compression\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 16:22:22 GMT"}, {"version": "v2", "created": "Wed, 14 Feb 2018 09:01:55 GMT"}, {"version": "v3", "created": "Tue, 29 May 2018 09:00:04 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Serr\u00e0", "Joan", ""], ["Sur\u00eds", "D\u00eddac", ""], ["Miron", "Marius", ""], ["Karatzoglou", "Alexandros", ""]]}, {"id": "1801.01443", "submitter": "Wellington Pinheiro dos Santos", "authors": "Filipe Rolim Cordeiro, Wellington Pinheiro dos Santos, Abel\n  Guilhermino da Silva Filho", "title": "A semi-supervised fuzzy GrowCut algorithm to segment and classify\n  regions of interest of mammographic images", "comments": null, "journal-ref": "Expert Systems With Applications, 65 (2016), 116-126", "doi": "10.1016/j.eswa.2016.08.016", "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.NE eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  According to the World Health Organization, breast cancer is the most common\nform of cancer in women. It is the second leading cause of death among women\nround the world, becoming the most fatal form of cancer. Mammographic image\nsegmentation is a fundamental task to support image analysis and diagnosis,\ntaking into account shape analysis of mammary lesions and their borders.\nHowever, mammogram segmentation is a very hard process, once it is highly\ndependent on the types of mammary tissues. In this work we present a new\nsemi-supervised segmentation algorithm based on the modification of the GrowCut\nalgorithm to perform automatic mammographic image segmentation once a region of\ninterest is selected by a specialist. In our proposal, we used fuzzy Gaussian\nmembership functions to modify the evolution rule of the original GrowCut\nalgorithm, in order to estimate the uncertainty of a pixel being object or\nbackground. The main impact of the proposed method is the significant reduction\nof expert effort in the initialization of seed points of GrowCut to perform\naccurate segmentation, once it removes the need of selection of background\nseeds. We also constructed an automatic point selection process based on the\nsimulated annealing optimization method, avoiding the need of human\nintervention. The proposed approach was qualitatively compared with other\nstate-of-the-art segmentation techniques, considering the shape of segmented\nregions. In order to validate our proposal, we built an image classifier using\na classical multilayer perceptron. We used Zernike moments to extract segmented\nimage features. This analysis employed 685 mammograms from IRMA breast cancer\ndatabase, using fat and fibroid tissues. Results show that the proposed\ntechnique could achieve a classification rate of 91.28\\% for fat tissues,\nevidencing the feasibility of our approach.\n", "versions": [{"version": "v1", "created": "Sun, 3 Dec 2017 17:31:01 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Cordeiro", "Filipe Rolim", ""], ["Santos", "Wellington Pinheiro dos", ""], ["Filho", "Abel Guilhermino da Silva", ""]]}, {"id": "1801.01527", "submitter": "Martin Lackner", "authors": "Martin Lackner and Piotr Skowron", "title": "Utilitarian Welfare and Representation Guarantees of Approval-Based\n  Multiwinner Rules", "comments": "This work is based on the short paper titled \"A quantitative analysis\n  of multi-winner rules\" that appeared in the proceedings of the 28th\n  International Joint Conference on Artificial Intelligence (IJCAI 2019)", "journal-ref": "Artificial Intelligence, Volume 288, November 2020, 103366", "doi": "10.1016/j.artint.2020.103366", "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To choose a suitable multiwinner voting rule is a hard and ambiguous task.\nDepending on the context, it varies widely what constitutes the choice of an\n``optimal'' subset of alternatives. In this paper, we provide a quantitative\nanalysis of multiwinner voting rules using methods from the theory of\napproximation algorithms---we estimate how well multiwinner rules approximate\ntwo extreme objectives: a representation criterion defined via the Approval\nChamberlin--Courant rule and a utilitarian criterion defined via Multiwinner\nApproval Voting. With both theoretical and experimental methods, we classify\nmultiwinner rules in terms of their quantitative alignment with these two\nopposing objectives. Our results provide fundamental information about the\nnature of multiwinner rules and, in particular, about the necessary tradeoffs\nwhen choosing such a rule.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 19:54:23 GMT"}, {"version": "v2", "created": "Sat, 12 May 2018 13:05:06 GMT"}, {"version": "v3", "created": "Fri, 10 Jan 2020 12:51:33 GMT"}, {"version": "v4", "created": "Mon, 31 Aug 2020 07:45:19 GMT"}], "update_date": "2020-09-01", "authors_parsed": [["Lackner", "Martin", ""], ["Skowron", "Piotr", ""]]}, {"id": "1801.01596", "submitter": "Jiazhuo Wang", "authors": "Jiazhuo Wang, Jason Xu, Xuejun Wang", "title": "Combination of Hyperband and Bayesian Optimization for Hyperparameter\n  Optimization in Deep Learning", "comments": "preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has achieved impressive results on many problems. However, it\nrequires high degree of expertise or a lot of experience to tune well the\nhyperparameters, and such manual tuning process is likely to be biased.\nMoreover, it is not practical to try out as many different hyperparameter\nconfigurations in deep learning as in other machine learning scenarios, because\nevaluating each single hyperparameter configuration in deep learning would mean\ntraining a deep neural network, which usually takes quite long time. Hyperband\nalgorithm achieves state-of-the-art performance on various hyperparameter\noptimization problems in the field of deep learning. However, Hyperband\nalgorithm does not utilize history information of previous explored\nhyperparameter configurations, thus the solution found is suboptimal. We\npropose to combine Hyperband algorithm with Bayesian optimization (which does\nnot ignore history when sampling next trial configuration). Experimental\nresults show that our combination approach is superior to other hyperparameter\noptimization approaches including Hyperband algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 01:00:03 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Wang", "Jiazhuo", ""], ["Xu", "Jason", ""], ["Wang", "Xuejun", ""]]}, {"id": "1801.01604", "submitter": "Han Xiao Almighty", "authors": "Han Xiao", "title": "Intelligence Graph", "comments": "arXiv admin note: substantial text overlap with arXiv:1702.06247", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In fact, there exist three genres of intelligence architectures: logics (e.g.\n\\textit{Random Forest, A$^*$ Searching}), neurons (e.g. \\textit{CNN, LSTM}) and\nprobabilities (e.g. \\textit{Naive Bayes, HMM}), all of which are incompatible\nto each other. However, to construct powerful intelligence systems with various\nmethods, we propose the intelligence graph (short as \\textbf{\\textit{iGraph}}),\nwhich is composed by both of neural and probabilistic graph, under the\nframework of forward-backward propagation. By the paradigm of iGraph, we design\na recommendation model with semantic principle. First, the probabilistic\ndistributions of categories are generated from the embedding representations of\nusers/items, in the manner of neurons. Second, the probabilistic graph infers\nthe distributions of features, in the manner of probabilities. Last, for the\nrecommendation diversity, we perform an expectation computation then conduct a\nlogic judgment, in the manner of logics. Experimentally, we beat the\nstate-of-the-art baselines and verify our conclusions.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 01:29:00 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Xiao", "Han", ""]]}, {"id": "1801.01657", "submitter": "Olivier Cailloux", "authors": "Olivier Cailloux (LAMSADE), S\\'ebastien Destercke (Labex MS2T)", "title": "Reasons and Means to Model Preferences as Incomplete", "comments": null, "journal-ref": "11th International Conference on Scalable Uncertainty Management,\n  2017", "doi": "10.1007/978-3-319-67582-4_2", "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Literature involving preferences of artificial agents or human beings often\nassume their preferences can be represented using a complete transitive binary\nrelation. Much has been written however on different models of preferences. We\nreview some of the reasons that have been put forward to justify more complex\nmodeling, and review some of the techniques that have been proposed to obtain\nmodels of such preferences.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 07:46:28 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Cailloux", "Olivier", "", "LAMSADE"], ["Destercke", "S\u00e9bastien", "", "Labex MS2T"]]}, {"id": "1801.01681", "submitter": "Zhen Li", "authors": "Zhen Li, Deqing Zou, Shouhuai Xu, Xinyu Ou, Hai Jin, Sujuan Wang,\n  Zhijun Deng, Yuyi Zhong", "title": "VulDeePecker: A Deep Learning-Based System for Vulnerability Detection", "comments": null, "journal-ref": null, "doi": "10.14722/ndss.2018.23158", "report-no": null, "categories": "cs.CR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automatic detection of software vulnerabilities is an important research\nproblem. However, existing solutions to this problem rely on human experts to\ndefine features and often miss many vulnerabilities (i.e., incurring high false\nnegative rate). In this paper, we initiate the study of using deep\nlearning-based vulnerability detection to relieve human experts from the\ntedious and subjective task of manually defining features. Since deep learning\nis motivated to deal with problems that are very different from the problem of\nvulnerability detection, we need some guiding principles for applying deep\nlearning to vulnerability detection. In particular, we need to find\nrepresentations of software programs that are suitable for deep learning. For\nthis purpose, we propose using code gadgets to represent programs and then\ntransform them into vectors, where a code gadget is a number of (not\nnecessarily consecutive) lines of code that are semantically related to each\nother. This leads to the design and implementation of a deep learning-based\nvulnerability detection system, called Vulnerability Deep Pecker\n(VulDeePecker). In order to evaluate VulDeePecker, we present the first\nvulnerability dataset for deep learning approaches. Experimental results show\nthat VulDeePecker can achieve much fewer false negatives (with reasonable false\npositives) than other approaches. We further apply VulDeePecker to 3 software\nproducts (namely Xen, Seamonkey, and Libav) and detect 4 vulnerabilities, which\nare not reported in the National Vulnerability Database but were \"silently\"\npatched by the vendors when releasing later versions of these products; in\ncontrast, these vulnerabilities are almost entirely missed by the other\nvulnerability detection systems we experimented with.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 09:37:18 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Li", "Zhen", ""], ["Zou", "Deqing", ""], ["Xu", "Shouhuai", ""], ["Ou", "Xinyu", ""], ["Jin", "Hai", ""], ["Wang", "Sujuan", ""], ["Deng", "Zhijun", ""], ["Zhong", "Yuyi", ""]]}, {"id": "1801.01704", "submitter": "Sandeep Kumar Singh", "authors": "Javier Mata, Ignacio de Miguel, Ram\\'o n J. Dur\\'a n, Noem\\'i Merayo,\n  Sandeep Kumar Singh, Admela Jukan, Mohit Chamania", "title": "Artificial Intelligence (AI) Methods in Optical Networks: A\n  Comprehensive Survey", "comments": null, "journal-ref": "Optical Switching and Networking, Jan 2018", "doi": "10.1016/j.osn.2017.12.006", "report-no": null, "categories": "cs.AI cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Artificial intelligence (AI) is an extensive scientific discipline which\nenables computer systems to solve problems by emulating complex biological\nprocesses such as learning, reasoning and self-correction. This paper presents\na comprehensive review of the application of AI techniques for improving\nperformance of optical communication systems and networks. The use of AI-based\ntechniques is first studied in applications related to optical transmission,\nranging from the characterization and operation of network components to\nperformance monitoring, mitigation of nonlinearities, and quality of\ntransmission estimation. Then, applications related to optical network control\nand management are also reviewed, including topics like optical network\nplanning and operation in both transport and access networks. Finally, the\npaper also presents a summary of opportunities and challenges in optical\nnetworking where AI is expected to play a key role in the near future.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 10:51:55 GMT"}, {"version": "v2", "created": "Mon, 15 Jan 2018 11:54:34 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Mata", "Javier", ""], ["de Miguel", "Ignacio", ""], ["n", "Ram\u00f3 n J. Dur\u00e1", ""], ["Merayo", "Noem\u00ed", ""], ["Singh", "Sandeep Kumar", ""], ["Jukan", "Admela", ""], ["Chamania", "Mohit", ""]]}, {"id": "1801.01705", "submitter": "Martijn van Otterlo", "authors": "Martijn van Otterlo", "title": "Gatekeeping Algorithms with Human Ethical Bias: The ethics of algorithms\n  in archives, libraries and society", "comments": "Submitted (Nov 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the age of algorithms, I focus on the question of how to ensure algorithms\nthat will take over many of our familiar archival and library tasks, will\nbehave according to human ethical norms that have evolved over many years. I\nstart by characterizing physical archives in the context of related\ninstitutions such as libraries and museums. In this setting I analyze how\nethical principles, in particular about access to information, have been\nformalized and communicated in the form of ethical codes, or: codes of\nconducts. After that I describe two main developments: digitalization, in which\nphysical aspects of the world are turned into digital data, and\nalgorithmization, in which intelligent computer programs turn this data into\npredictions and decisions. Both affect interactions that were once physical but\nnow digital. In this new setting I survey and analyze the ethical aspects of\nalgorithms and how they shape a vision on the future of archivists and\nlibrarians, in the form of algorithmic documentalists, or: codementalists.\nFinally I outline a general research strategy, called IntERMEeDIUM, to obtain\nalgorithms that obey are human ethical values encoded in code of ethics.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 10:58:13 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["van Otterlo", "Martijn", ""]]}, {"id": "1801.01733", "submitter": "Purushottam Dixit", "authors": "Purushottam D. Dixit", "title": "Entropy production rate as a criterion for inconsistency in decision\n  theory", "comments": "To appear in Journal of Statistical Physics", "journal-ref": null, "doi": "10.1088/1742-5468/aac137", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Individual and group decisions are complex, often involving choosing an apt\nalternative from a multitude of options. Evaluating pairwise comparisons breaks\ndown such complex decision problems into tractable ones. Pairwise comparison\nmatrices (PCMs) are regularly used to solve multiple-criteria decision-making\n(MCDM) problems, for example, using Saaty's analytic hierarchy process (AHP)\nframework. However, there are two significant drawbacks of using PCMs. First,\nhumans evaluate PCMs in an inconsistent manner. Second, not all entries of a\nlarge PCM can be reliably filled by human decision makers. We address these two\nissues by first establishing a novel connection between PCMs and\ntime-irreversible Markov processes. Specifically, we show that every PCM\ninduces a family of dissipative maximum path entropy random walks (MERW) over\nthe set of alternatives. We show that only `consistent' PCMs correspond to\ndetailed balanced MERWs. We identify the non-equilibrium entropy production in\nthe induced MERWs as a metric of inconsistency of the underlying PCMs. Notably,\nthe entropy production satisfies all of the recently laid out criteria for\nreasonable consistency indices. We also propose an approach to use incompletely\nfilled PCMs in AHP. Potential future avenues are discussed as well.\n  keywords: analytic hierarchy process, markov chains, maximum entropy\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 12:06:56 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2018 14:18:07 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Dixit", "Purushottam D.", ""]]}, {"id": "1801.01768", "submitter": "Corina Florescu", "authors": "Corina Florescu and Wei Jin", "title": "Learning Feature Representations for Keyphrase Extraction", "comments": "To appear in AAAI 2018 Student Abstract and Poster Program", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In supervised approaches for keyphrase extraction, a candidate phrase is\nencoded with a set of hand-crafted features and machine learning algorithms are\ntrained to discriminate keyphrases from non-keyphrases. Although the\nmanually-designed features have shown to work well in practice, feature\nengineering is a difficult process that requires expert knowledge and normally\ndoes not generalize well. In this paper, we present SurfKE, a feature learning\nframework that exploits the text itself to automatically discover patterns that\nkeyphrases exhibit. Our model represents the document as a graph and\nautomatically learns feature representation of phrases. The proposed model\nobtains remarkable improvements in performance over strong baselines.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 14:36:31 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Florescu", "Corina", ""], ["Jin", "Wei", ""]]}, {"id": "1801.01788", "submitter": "Karl Schlechta", "authors": "Karl Schlechta", "title": "A Reliability Theory of Truth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our approach is basically a coherence approach, but we avoid the well-known\npitfalls of coherence theories of truth. Consistency is replaced by\nreliability, which expresses support and attack, and, in principle, every\ntheory (or agent, message) counts. At the same time, we do not require a\npriviledged access to \"reality\". A centerpiece of our approach is that we\nattribute reliability also to agents, messages, etc., so an unreliable source\nof information will be less important in future. Our ideas can also be extended\nto value systems, and even actions, e.g., of animals.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 14:51:47 GMT"}, {"version": "v2", "created": "Thu, 22 Feb 2018 16:57:31 GMT"}, {"version": "v3", "created": "Sun, 1 Apr 2018 14:28:01 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Schlechta", "Karl", ""]]}, {"id": "1801.01807", "submitter": "Fabricio de Franca Olivetti", "authors": "Fabricio Olivetti de Franca", "title": "A Greedy Search Tree Heuristic for Symbolic Regression", "comments": "30 pages, 7 figures, 3 tables, submitted to Information Science on\n  12/2016", "journal-ref": null, "doi": "10.1016/j.ins.2018.02.040", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symbolic Regression tries to find a mathematical expression that describes\nthe relationship of a set of explanatory variables to a measured variable. The\nmain objective is to find a model that minimizes the error and, optionally,\nthat also minimizes the expression size. A smaller expression can be seen as an\ninterpretable model considered a reliable decision model. This is often\nperformed with Genetic Programming which represents their solution as\nexpression trees. The shortcoming of this algorithm lies on this representation\nthat defines a rugged search space and contains expressions of any size and\ndifficulty. These pose as a challenge to find the optimal solution under\ncomputational constraints. This paper introduces a new data structure, called\nInteraction-Transformation (IT), that constrains the search space in order to\nexclude a region of larger and more complicated expressions. In order to test\nthis data structure, it was also introduced an heuristic called SymTree. The\nobtained results show evidence that SymTree are capable of obtaining the\noptimal solution whenever the target function is within the search space of the\nIT data structure and competitive results when it is not. Overall, the\nalgorithm found a good compromise between accuracy and simplicity for all the\ngenerated models.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jan 2018 18:30:38 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["de Franca", "Fabricio Olivetti", ""]]}, {"id": "1801.01814", "submitter": "Craig Larson", "authors": "N. Bushaw, C. E. Larson, N. Van Cleemput (and Summer 2017 Graph Brain\n  Project Workshop Participants)", "title": "Automated Conjecturing VII: The Graph Brain Project & Big Mathematics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Graph Brain Project is an experiment in how the use of automated\nmathematical discovery software, databases, large collaboration, and systematic\ninvestigation provide a model for how mathematical research might proceed in\nthe future.\n  Our Project began with the development of a program that can be used to\ngenerate invariant-relation and property-relation conjectures in many areas of\nmathematics. This program can produce conjectures which are not implied by\nexisting (published) theorems. Here we propose a new approach to push forward\nexisting mathematical research goals---using automated mathematical discovery\nsoftware. We suggest how to initiate and harness large-scale collaborative\nmathematics. We envision mathematical research labs similar to what exist in\nother sciences, new avenues for funding, new opportunities for training\nstudents, and a more efficient and effective use of published mathematical\nresearch.\n  And our experiment in graph theory can be imitated in many other areas of\nmathematical research. Big Mathematics is the idea of large, systematic,\ncollaborative research on problems of existing mathematical interest. What is\npossible when we put our skills, tools, and results together systematically?\n", "versions": [{"version": "v1", "created": "Thu, 28 Dec 2017 16:47:23 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Bushaw", "N.", "", "and Summer 2017 Graph Brain\n  Project Workshop Participants"], ["Larson", "C. E.", "", "and Summer 2017 Graph Brain\n  Project Workshop Participants"], ["Van Cleemput", "N.", "", "and Summer 2017 Graph Brain\n  Project Workshop Participants"]]}, {"id": "1801.01825", "submitter": "Danish Contractor", "authors": "Danish Contractor, Barun Patra, Mausam Singla, Parag Singla", "title": "Towards Understanding and Answering Multi-Sentence Recommendation\n  Questions on Tourism", "comments": null, "journal-ref": null, "doi": "10.1017/S1351324920000017", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the first system towards the novel task of answering complex\nmultisentence recommendation questions in the tourism domain. Our solution uses\na pipeline of two modules: question understanding and answering. For question\nunderstanding, we define an SQL-like query language that captures the semantic\nintent of a question; it supports operators like subset, negation, preference\nand similarity, which are often found in recommendation questions. We train and\ncompare traditional CRFs as well as bidirectional LSTM-based models for\nconverting a question to its semantic representation. We extend these models to\na semisupervised setting with partially labeled sequences gathered through\ncrowdsourcing. We find that our best model performs semi-supervised training of\nBiDiLSTM+CRF with hand-designed features and CCM(Chang et al., 2007)\nconstraints. Finally, in an end to end QA system, our answering component\nconverts our question representation into queries fired on underlying knowledge\nsources. Our experiments on two different answer corpora demonstrate that our\nsystem can significantly outperform baselines with up to 20 pt higher accuracy\nand 17 pt higher recall.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 16:38:05 GMT"}], "update_date": "2020-09-30", "authors_parsed": [["Contractor", "Danish", ""], ["Patra", "Barun", ""], ["Singla", "Mausam", ""], ["Singla", "Parag", ""]]}, {"id": "1801.01937", "submitter": "Seyedamin Pouriyeh", "authors": "Seyedamin Pouriyeh, Mehdi Allahyari, Krys Kochut, Hamid Reza Arabnia", "title": "A Comprehensive Survey of Ontology Summarization: Measures and Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Semantic Web is becoming a large scale framework that enables data to be\npublished, shared, and reused in the form of ontologies. The ontology which is\nconsidered as basic building block of semantic web consists of two layers\nincluding data and schema layer. With the current exponential development of\nontologies in both data size and complexity of schemas, ontology understanding\nwhich is playing an important role in different tasks such as ontology\nengineering, ontology learning, etc., is becoming more difficult. Ontology\nsummarization as a way to distill knowledge from an ontology and generate an\nabridge version to facilitate a better understanding is getting more attention\nrecently. There are various approaches available for ontology summarization\nwhich are focusing on different measures in order to produce a proper summary\nfor a given ontology. In this paper, we mainly focus on the common metrics\nwhich are using for ontology summarization and meet the state-of-the-art in\nontology summarization.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 22:53:41 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Pouriyeh", "Seyedamin", ""], ["Allahyari", "Mehdi", ""], ["Kochut", "Krys", ""], ["Arabnia", "Hamid Reza", ""]]}, {"id": "1801.01944", "submitter": "Nicholas Carlini", "authors": "Nicholas Carlini, David Wagner", "title": "Audio Adversarial Examples: Targeted Attacks on Speech-to-Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct targeted audio adversarial examples on automatic speech\nrecognition. Given any audio waveform, we can produce another that is over\n99.9% similar, but transcribes as any phrase we choose (recognizing up to 50\ncharacters per second of audio). We apply our white-box iterative\noptimization-based attack to Mozilla's implementation DeepSpeech end-to-end,\nand show it has a 100% success rate. The feasibility of this attack introduce a\nnew domain to study adversarial examples.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jan 2018 23:40:04 GMT"}, {"version": "v2", "created": "Fri, 30 Mar 2018 02:06:30 GMT"}], "update_date": "2018-04-02", "authors_parsed": [["Carlini", "Nicholas", ""], ["Wagner", "David", ""]]}, {"id": "1801.01957", "submitter": "Xiaodong He", "authors": "Heung-Yeung Shum, Xiaodong He, Di Li", "title": "From Eliza to XiaoIce: Challenges and Opportunities with Social Chatbots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational systems have come a long way since their inception in the\n1960s. After decades of research and development, we've seen progress from\nEliza and Parry in the 60's and 70's, to task-completion systems as in the\nDARPA Communicator program in the 2000s, to intelligent personal assistants\nsuch as Siri in the 2010s, to today's social chatbots like XiaoIce. Social\nchatbots' appeal lies not only in their ability to respond to users' diverse\nrequests, but also in being able to establish an emotional connection with\nusers. The latter is done by satisfying users' need for communication,\naffection, as well as social belonging. To further the advancement and adoption\nof social chatbots, their design must focus on user engagement and take both\nintellectual quotient (IQ) and emotional quotient (EQ) into account. Users\nshould want to engage with a social chatbot; as such, we define the success\nmetric for social chatbots as conversation-turns per session (CPS). Using\nXiaoIce as an illustrative example, we discuss key technologies in building\nsocial chatbots from core chat to visual awareness to skills. We also show how\nXiaoIce can dynamically recognize emotion and engage the user throughout long\nconversations with appropriate interpersonal responses. As we become the first\ngeneration of humans ever living with AI, we have a responsibility to design\nsocial chatbots to be both useful and empathetic, so they will become\nubiquitous and help society as a whole.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jan 2018 03:14:22 GMT"}, {"version": "v2", "created": "Fri, 9 Feb 2018 23:30:46 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Shum", "Heung-Yeung", ""], ["He", "Xiaodong", ""], ["Li", "Di", ""]]}, {"id": "1801.01968", "submitter": "Daichi Nishio", "authors": "Daichi Nishio and Satoshi Yamane", "title": "Faster Deep Q-learning using Neural Episodic Control", "comments": "6 pages, 6 figures, COMPSAC2018 short paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The research on deep reinforcement learning which estimates Q-value by deep\nlearning has been attracted the interest of researchers recently. In deep\nreinforcement learning, it is important to efficiently learn the experiences\nthat an agent has collected by exploring environment. We propose NEC2DQN that\nimproves learning speed of a poor sample efficiency algorithm such as DQN by\nusing good one such as NEC at the beginning of learning. We show it is able to\nlearn faster than Double DQN or N-step DQN in the experiments of Pong.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jan 2018 05:17:49 GMT"}, {"version": "v2", "created": "Mon, 15 Jan 2018 07:38:46 GMT"}, {"version": "v3", "created": "Thu, 1 Feb 2018 14:52:53 GMT"}, {"version": "v4", "created": "Sun, 3 Jun 2018 05:29:45 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Nishio", "Daichi", ""], ["Yamane", "Satoshi", ""]]}, {"id": "1801.01972", "submitter": "Ling Dong", "authors": "Zecang Gu and Ling Dong", "title": "Distance formulas capable of unifying Euclidian space and probability\n  space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For pattern recognition like image recognition, it has become clear that each\nmachine-learning dictionary data actually became data in probability space\nbelonging to Euclidean space. However, the distances in the Euclidean space and\nthe distances in the probability space are separated and ununified when machine\nlearning is introduced in the pattern recognition. There is still a problem\nthat it is impossible to directly calculate an accurate matching relation\nbetween the sampling data of the read image and the learned dictionary data. In\nthis research, we focused on the reason why the distance is changed and the\nextent of change when passing through the probability space from the original\nEuclidean distance among data belonging to multiple probability spaces\ncontaining Euclidean space. By finding the reason of the cause of the distance\nerror and finding the formula expressing the error quantitatively, a possible\ndistance formula to unify Euclidean space and probability space is found. Based\non the results of this research, the relationship between machine-learning\ndictionary data and sampling data was clearly understood for pattern\nrecognition. As a result, the calculation of collation among data and\nmachine-learning to compete mutually between data are cleared, and complicated\ncalculations became unnecessary. Finally, using actual pattern recognition\ndata, experimental demonstration of a possible distance formula to unify\nEuclidean space and probability space discovered by this research was carried\nout, and the effectiveness of the result was confirmed.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jan 2018 05:41:08 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Gu", "Zecang", ""], ["Dong", "Ling", ""]]}, {"id": "1801.02025", "submitter": "Justinas Miseikis", "authors": "Justinas Miseikis, Patrick Knobelreiter, Inka Brijacak, Saeed\n  Yahyanejad, Kyrre Glette, Ole Jakob Elle, Jim Torresen", "title": "Robot Localisation and 3D Position Estimation Using a Free-Moving Camera\n  and Cascaded Convolutional Neural Networks", "comments": "Submission for IEEE AIM 2018 conference, 7 pages, 7 figures, ROBIN\n  group, University of Oslo", "journal-ref": null, "doi": "10.1109/AIM.2018.8452236", "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many works in collaborative robotics and human-robot interaction focuses on\nidentifying and predicting human behaviour while considering the information\nabout the robot itself as given. This can be the case when sensors and the\nrobot are calibrated in relation to each other and often the reconfiguration of\nthe system is not possible, or extra manual work is required. We present a deep\nlearning based approach to remove the constraint of having the need for the\nrobot and the vision sensor to be fixed and calibrated in relation to each\nother. The system learns the visual cues of the robot body and is able to\nlocalise it, as well as estimate the position of robot joints in 3D space by\njust using a 2D color image. The method uses a cascaded convolutional neural\nnetwork, and we present the structure of the network, describe our own\ncollected dataset, explain the network training and achieved results. A fully\ntrained system shows promising results in providing an accurate mask of where\nthe robot is located and a good estimate of its joints positions in 3D. The\naccuracy is not good enough for visual servoing applications yet, however, it\ncan be sufficient for general safety and some collaborative tasks not requiring\nvery high precision. The main benefit of our method is the possibility of the\nvision sensor to move freely. This allows it to be mounted on moving objects,\nfor example, a body of the person or a mobile robot working in the same\nenvironment as the robots are operating in.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jan 2018 14:50:07 GMT"}, {"version": "v2", "created": "Wed, 30 May 2018 07:56:16 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Miseikis", "Justinas", ""], ["Knobelreiter", "Patrick", ""], ["Brijacak", "Inka", ""], ["Yahyanejad", "Saeed", ""], ["Glette", "Kyrre", ""], ["Elle", "Ole Jakob", ""], ["Torresen", "Jim", ""]]}, {"id": "1801.02068", "submitter": "Mircea Andrecut Dr", "authors": "M. Andrecut", "title": "On the inherent competition between valid and spurious inductive\n  inferences in Boolean data", "comments": "12 pages, 2 figures, Int. J. Mod. Phys. C, 2017", "journal-ref": null, "doi": "10.1142/S0129183117501467", "report-no": null, "categories": "physics.data-an cs.AI cs.LO q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inductive inference is the process of extracting general rules from specific\nobservations. This problem also arises in the analysis of biological networks,\nsuch as genetic regulatory networks, where the interactions are complex and the\nobservations are incomplete. A typical task in these problems is to extract\ngeneral interaction rules as combinations of Boolean covariates, that explain a\nmeasured response variable. The inductive inference process can be considered\nas an incompletely specified Boolean function synthesis problem. This\nincompleteness of the problem will also generate spurious inferences, which are\na serious threat to valid inductive inference rules. Using random Boolean data\nas a null model, here we attempt to measure the competition between valid and\nspurious inductive inference rules from a given data set. We formulate two\ngreedy search algorithms, which synthesize a given Boolean response variable in\na sparse disjunct normal form, and respectively a sparse generalized algebraic\nnormal form of the variables from the observation data, and we evaluate\nnumerically their performance.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jan 2018 19:06:15 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Andrecut", "M.", ""]]}, {"id": "1801.02193", "submitter": "Michal \\v{C}ertick\\'y", "authors": "Michal \\v{S}ustr, Jan Mal\\'y, Michal \\v{C}ertick\\'y", "title": "Multi-platform Version of StarCraft: Brood War in a Docker Container:\n  Technical Report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a dockerized version of a real-time strategy game StarCraft: Brood\nWar, commonly used as a domain for AI research, with a pre-installed collection\nof AI developement tools supporting all the major types of StarCraft bots. This\nprovides a convenient way to deploy StarCraft AIs on numerous hosts at once and\nacross multiple platforms despite limited OS support of StarCraft. In this\ntechnical report, we describe the design of our Docker images and present a few\nuse cases.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jan 2018 14:16:59 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["\u0160ustr", "Michal", ""], ["Mal\u00fd", "Jan", ""], ["\u010certick\u00fd", "Michal", ""]]}, {"id": "1801.02203", "submitter": "Richa Verma", "authors": "Prerna Agarwal, Richa Verma, Angshul Majumdar", "title": "Indian Regional Movie Dataset for Recommender Systems", "comments": "7 pages, 8 figures, open-source Indian movie rating dataset, metadata\n  of movies and users", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indian regional movie dataset is the first database of regional Indian\nmovies, users and their ratings. It consists of movies belonging to 18\ndifferent Indian regional languages and metadata of users with varying\ndemographics. Through this dataset, the diversity of Indian regional cinema and\nits huge viewership is captured. We analyze the dataset that contains roughly\n10K ratings of 919 users and 2,851 movies using some supervised and\nunsupervised collaborative filtering techniques like Probabilistic Matrix\nFactorization, Matrix Completion, Blind Compressed Sensing etc. The dataset\nconsists of metadata information of users like age, occupation, home state and\nknown languages. It also consists of metadata of movies like genre, language,\nrelease year and cast. India has a wide base of viewers which is evident by the\nlarge number of movies released every year and the huge box-office revenue.\nThis dataset can be used for designing recommendation systems for Indian users\nand regional movies, which do not, yet, exist. The dataset can be downloaded\nfrom \\href{https://goo.gl/EmTPv6}{https://goo.gl/EmTPv6}.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jan 2018 16:02:35 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Agarwal", "Prerna", ""], ["Verma", "Richa", ""], ["Majumdar", "Angshul", ""]]}, {"id": "1801.02209", "submitter": "Yi Wu", "authors": "Yi Wu, Yuxin Wu, Georgia Gkioxari, Yuandong Tian", "title": "Building Generalizable Agents with a Realistic and Rich 3D Environment", "comments": "updated with improved content and more experinemnts", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Teaching an agent to navigate in an unseen 3D environment is a challenging\ntask, even in the event of simulated environments. To generalize to unseen\nenvironments, an agent needs to be robust to low-level variations (e.g. color,\ntexture, object changes), and also high-level variations (e.g. layout changes\nof the environment). To improve overall generalization, all types of variations\nin the environment have to be taken under consideration via different level of\ndata augmentation steps. To this end, we propose House3D, a rich, extensible\nand efficient environment that contains 45,622 human-designed 3D scenes of\nvisually realistic houses, ranging from single-room studios to multi-storied\nhouses, equipped with a diverse set of fully labeled 3D objects, textures and\nscene layouts, based on the SUNCG dataset (Song et.al.). The diversity in\nHouse3D opens the door towards scene-level augmentation, while the label-rich\nnature of House3D enables us to inject pixel- & task-level augmentations such\nas domain randomization (Toubin et. al.) and multi-task training. Using a\nsubset of houses in House3D, we show that reinforcement learning agents trained\nwith an enhancement of different levels of augmentations perform much better in\nunseen environments than our baselines with raw RGB input by over 8% in terms\nof navigation success rate. House3D is publicly available at\nhttp://github.com/facebookresearch/House3D.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jan 2018 16:34:41 GMT"}, {"version": "v2", "created": "Sun, 8 Apr 2018 22:09:00 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Wu", "Yi", ""], ["Wu", "Yuxin", ""], ["Gkioxari", "Georgia", ""], ["Tian", "Yuandong", ""]]}, {"id": "1801.02243", "submitter": "Wanfeng Chen", "authors": "Catherine Xiao, Wanfeng Chen", "title": "Trading the Twitter Sentiment with Reinforcement Learning", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is to explore the possibility to use alternative data and\nartificial intelligence techniques to trade stocks. The efficacy of the daily\nTwitter sentiment on predicting the stock return is examined using machine\nlearning methods. Reinforcement learning(Q-learning) is applied to generate the\noptimal trading policy based on the sentiment signal. The predicting power of\nthe sentiment signal is more significant if the stock price is driven by the\nexpectation of the company growth and when the company has a major event that\ndraws the public attention. The optimal trading strategy based on reinforcement\nlearning outperforms the trading strategy based on the machine learning\nprediction.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jan 2018 20:26:00 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Xiao", "Catherine", ""], ["Chen", "Wanfeng", ""]]}, {"id": "1801.02268", "submitter": "Benjamin Spector", "authors": "Benjamin Spector, Serge Belongie", "title": "Sample-Efficient Reinforcement Learning through Transfer and\n  Architectural Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work in deep reinforcement learning has allowed algorithms to learn\ncomplex tasks such as Atari 2600 games just from the reward provided by the\ngame, but these algorithms presently require millions of training steps in\norder to learn, making them approximately five orders of magnitude slower than\nhumans. One reason for this is that humans build robust shared representations\nthat are applicable to collections of problems, making it much easier to\nassimilate new variants. This paper first introduces the idea of\nautomatically-generated game sets to aid in transfer learning research, and\nthen demonstrates the utility of shared representations by showing that models\ncan substantially benefit from the incorporation of relevant architectural\npriors. This technique affords a remarkable 50x positive transfer on a toy\nproblem-set.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jan 2018 23:13:29 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Spector", "Benjamin", ""], ["Belongie", "Serge", ""]]}, {"id": "1801.02270", "submitter": "David Rajaratnam", "authors": "Bernhard Hengst, Maurice Pagnucco, David Rajaratnam, Claude Sammut,\n  Michael Thielscher", "title": "Perceptual Context in Cognitive Hierarchies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cognition does not only depend on bottom-up sensor feature abstraction, but\nalso relies on contextual information being passed top-down. Context is higher\nlevel information that helps to predict belief states at lower levels. The main\ncontribution of this paper is to provide a formalisation of perceptual context\nand its integration into a new process model for cognitive hierarchies. Several\nsimple instantiations of a cognitive hierarchy are used to illustrate the role\nof context. Notably, we demonstrate the use context in a novel approach to\nvisually track the pose of rigid objects with just a 2D camera.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jan 2018 23:20:14 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Hengst", "Bernhard", ""], ["Pagnucco", "Maurice", ""], ["Rajaratnam", "David", ""], ["Sammut", "Claude", ""], ["Thielscher", "Michael", ""]]}, {"id": "1801.02281", "submitter": "Vatsal Mahajan", "authors": "Vatsal Mahajan", "title": "Winograd Schema - Knowledge Extraction Using Narrative Chains", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Winograd Schema Challenge (WSC) is a test of machine intelligence,\ndesigned to be an improvement on the Turing test. A Winograd Schema consists of\na sentence and a corresponding question. To successfully answer these\nquestions, one requires the use of commonsense knowledge and reasoning. This\nwork focuses on extracting common sense knowledge which can be used to generate\nanswers for the Winograd schema challenge. Common sense knowledge is extracted\nbased on events (or actions) and their participants; called Event-Based\nConditional Commonsense (ECC). I propose an approach using Narrative Event\nChains [Chambers et al., 2008] to extract ECC knowledge. These are stored in\ntemplates, to be later used for answering the WSC questions. This approach\nworks well with respect to a subset of WSC tasks.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 00:36:08 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Mahajan", "Vatsal", ""]]}, {"id": "1801.02334", "submitter": "Yunlong Mi", "authors": "Yunlong Mi, Yong Shi, and Jinhai Li", "title": "A generalized concept-cognitive learning: A machine learning viewpoint", "comments": "7 pages,3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Concept-cognitive learning (CCL) is a hot topic in recent years, and it has\nattracted much attention from the communities of formal concept analysis,\ngranular computing and cognitive computing. However, the relationship among\ncognitive computing (CC), concept-cognitive computing (CCC), CCL and\nconcept-cognitive learning model (CCLM) is not clearly described. To this end,\nwe first explain the relationship of CC, CCC, CCL and CCLM. Then, we propose a\ngeneralized concept-cognitive learning (GCCL) from the point of view of machine\nlearning. Finally, experiments on some data sets are conducted to verify the\nfeasibility of concept formation and concept-cognitive process of GCCL.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 08:16:57 GMT"}, {"version": "v2", "created": "Tue, 13 Feb 2018 12:49:41 GMT"}, {"version": "v3", "created": "Mon, 24 Dec 2018 02:48:26 GMT"}], "update_date": "2018-12-27", "authors_parsed": [["Mi", "Yunlong", ""], ["Shi", "Yong", ""], ["Li", "Jinhai", ""]]}, {"id": "1801.02471", "submitter": "Meysam Golmohammadi", "authors": "Meysam Golmohammadi, Saeedeh Ziyabari, Vinit Shah, Eva Von Weltin,\n  Christopher Campbell, Iyad Obeid and Joseph Picone", "title": "Gated Recurrent Networks for Seizure Detection", "comments": "Published in Dec 2017 publication In IEEE Signal Processing in\n  Medicine and Biology Symposium. Philadelphia, Pennsylvania, USA. arXiv admin\n  note: text overlap with arXiv:1712.09776", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNNs) with sophisticated units that implement a\ngating mechanism have emerged as powerful technique for modeling sequential\nsignals such as speech or electroencephalography (EEG). The latter is the focus\non this paper. A significant big data resource, known as the TUH EEG Corpus\n(TUEEG), has recently become available for EEG research, creating a unique\nopportunity to evaluate these recurrent units on the task of seizure detection.\nIn this study, we compare two types of recurrent units: long short-term memory\nunits (LSTM) and gated recurrent units (GRU). These are evaluated using a state\nof the art hybrid architecture that integrates Convolutional Neural Networks\n(CNNs) with RNNs. We also investigate a variety of initialization methods and\nshow that initialization is crucial since poorly initialized networks cannot be\ntrained. Furthermore, we explore regularization of these convolutional gated\nrecurrent networks to address the problem of overfitting. Our experiments\nrevealed that convolutional LSTM networks can achieve significantly better\nperformance than convolutional GRU networks. The convolutional LSTM\narchitecture with proper initialization and regularization delivers 30%\nsensitivity at 6 false alarms per 24 hours.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jan 2018 00:54:05 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Golmohammadi", "Meysam", ""], ["Ziyabari", "Saeedeh", ""], ["Shah", "Vinit", ""], ["Von Weltin", "Eva", ""], ["Campbell", "Christopher", ""], ["Obeid", "Iyad", ""], ["Picone", "Joseph", ""]]}, {"id": "1801.02668", "submitter": "Ting-Hao Huang", "authors": "Ting-Hao 'Kenneth' Huang and Joseph Chee Chang and Jeffrey P. Bigham", "title": "Evorus: A Crowd-powered Conversational Assistant Built to Automate\n  Itself Over Time", "comments": "10 pages. To appear in the Proceedings of the Conference on Human\n  Factors in Computing Systems 2018 (CHI'18)", "journal-ref": null, "doi": "10.1145/3173574.3173869", "report-no": null, "categories": "cs.HC cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowd-powered conversational assistants have been shown to be more robust\nthan automated systems, but do so at the cost of higher response latency and\nmonetary costs. A promising direction is to combine the two approaches for high\nquality, low latency, and low cost solutions. In this paper, we introduce\nEvorus, a crowd-powered conversational assistant built to automate itself over\ntime by (i) allowing new chatbots to be easily integrated to automate more\nscenarios, (ii) reusing prior crowd answers, and (iii) learning to\nautomatically approve response candidates. Our 5-month-long deployment with 80\nparticipants and 281 conversations shows that Evorus can automate itself\nwithout compromising conversation quality. Crowd-AI architectures have long\nbeen proposed as a way to reduce cost and latency for crowd-powered systems;\nEvorus demonstrates how automation can be introduced successfully in a deployed\nsystem. Its architecture allows future researchers to make further innovation\non the underlying automated components in the context of a deployed open domain\ndialog system.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 20:07:35 GMT"}, {"version": "v2", "created": "Wed, 10 Jan 2018 03:49:24 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Huang", "Ting-Hao 'Kenneth'", ""], ["Chang", "Joseph Chee", ""], ["Bigham", "Jeffrey P.", ""]]}, {"id": "1801.02805", "submitter": "Lex Fridman", "authors": "Lex Fridman, Jack Terwilliger, Benedikt Jenik", "title": "DeepTraffic: Crowdsourced Hyperparameter Tuning of Deep Reinforcement\n  Learning Systems for Multi-Agent Dense Traffic Navigation", "comments": "Neural Information Processing Systems (NIPS 2018) Deep Reinforcement\n  Learning Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a traffic simulation named DeepTraffic where the planning systems\nfor a subset of the vehicles are handled by a neural network as part of a\nmodel-free, off-policy reinforcement learning process. The primary goal of\nDeepTraffic is to make the hands-on study of deep reinforcement learning\naccessible to thousands of students, educators, and researchers in order to\ninspire and fuel the exploration and evaluation of deep Q-learning network\nvariants and hyperparameter configurations through large-scale, open\ncompetition. This paper investigates the crowd-sourced hyperparameter tuning of\nthe policy network that resulted from the first iteration of the DeepTraffic\ncompetition where thousands of participants actively searched through the\nhyperparameter space.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 05:56:15 GMT"}, {"version": "v2", "created": "Thu, 3 Jan 2019 01:36:43 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Fridman", "Lex", ""], ["Terwilliger", "Jack", ""], ["Jenik", "Benedikt", ""]]}, {"id": "1801.02827", "submitter": "Esra'a Alkafaween", "authors": "Esra'a O Alkafaween", "title": "Novel Methods for Enhancing the Performance of Genetic Algorithms", "comments": "88 pages, Master's thesis, Information Technology, Mutah university,\n  2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this thesis we propose new methods for crossover operator namely: cut on\nworst gene (COWGC), cut on worst L+R gene (COWLRGC) and Collision Crossovers.\nAnd also we propose several types of mutation operator such as: worst gene with\nrandom gene mutation (WGWRGM) , worst LR gene with random gene mutation\n(WLRGWRGM), worst gene with worst gene mutation (WGWWGM), worst gene with\nnearest neighbour mutation (WGWNNM), worst gene with the worst around the\nnearest neighbour mutation (WGWWNNM), worst gene inserted beside nearest\nneighbour mutation (WGIBNNM), random gene inserted beside nearest neighbour\nmutation (RGIBNNM), Swap worst gene locally mutation (SWGLM), Insert best\nrandom gene before worst gene mutation (IBRGBWGM) and Insert best random gene\nbefore random gene mutation (IBRGBRGM). In addition to proposing four selection\nstrategies, namely: select any crossover (SAC), select any mutation (SAM),\nselect best crossover (SBC) and select best mutation (SBM). The first two are\nbased on selection of the best crossover and mutation operator respectively,\nand the other two strategies randomly select any operator. So we investigate\nthe use of more than one crossover/mutation operator (based on the proposed\nstrategies) to enhance the performance of genetic algorithms. Our experiments,\nconducted on several Travelling Salesman Problems (TSP), show the superiority\nof some of the proposed methods in crossover and mutation over some of the\nwell-known crossover and mutation operators described in the literature. In\naddition, using any of the four strategies (SAC, SAM, SBC and SBM), found to be\nbetter than using one crossover/mutation operator in general, because those\nallow the GA to avoid local optima, or the so-called premature convergence.\nKeywords: GAs, Collision crossover, Multi crossovers, Multi mutations, TSP.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 07:37:23 GMT"}, {"version": "v2", "created": "Sun, 14 Jan 2018 06:27:03 GMT"}, {"version": "v3", "created": "Thu, 25 Jan 2018 18:08:15 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Alkafaween", "Esra'a O", ""]]}, {"id": "1801.02832", "submitter": "Carsten Eickhoff", "authors": "Ferenc Galk\\'o and Carsten Eickhoff", "title": "Biomedical Question Answering via Weighted Neural Network Passage\n  Retrieval", "comments": "To appear in ECIR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The amount of publicly available biomedical literature has been growing\nrapidly in recent years, yet question answering systems still struggle to\nexploit the full potential of this source of data. In a preliminary processing\nstep, many question answering systems rely on retrieval models for identifying\nrelevant documents and passages. This paper proposes a weighted cosine distance\nretrieval scheme based on neural network word embeddings. Our experiments are\nbased on publicly available data and tasks from the BioASQ biomedical question\nanswering challenge and demonstrate significant performance gains over a wide\nrange of state-of-the-art models.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 08:23:46 GMT"}], "update_date": "2018-01-10", "authors_parsed": [["Galk\u00f3", "Ferenc", ""], ["Eickhoff", "Carsten", ""]]}, {"id": "1801.02852", "submitter": "Tomasz Grel", "authors": "Igor Adamski, Robert Adamski, Tomasz Grel, Adam J\\k{e}drych, Kamil\n  Kaczmarek, Henryk Michalewski", "title": "Distributed Deep Reinforcement Learning: Learn how to play Atari games\n  in 21 minutes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a study in Distributed Deep Reinforcement Learning (DDRL) focused\non scalability of a state-of-the-art Deep Reinforcement Learning algorithm\nknown as Batch Asynchronous Advantage ActorCritic (BA3C). We show that using\nthe Adam optimization algorithm with a batch size of up to 2048 is a viable\nchoice for carrying out large scale machine learning computations. This,\ncombined with careful reexamination of the optimizer's hyperparameters, using\nsynchronous training on the node level (while keeping the local, single node\npart of the algorithm asynchronous) and minimizing the memory footprint of the\nmodel, allowed us to achieve linear scaling for up to 64 CPU nodes. This\ncorresponds to a training time of 21 minutes on 768 CPU cores, as opposed to 10\nhours when using a single node with 24 cores achieved by a baseline single-node\nimplementation.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 09:39:29 GMT"}, {"version": "v2", "created": "Mon, 9 Apr 2018 15:36:09 GMT"}], "update_date": "2018-04-10", "authors_parsed": [["Adamski", "Igor", ""], ["Adamski", "Robert", ""], ["Grel", "Tomasz", ""], ["J\u0119drych", "Adam", ""], ["Kaczmarek", "Kamil", ""], ["Michalewski", "Henryk", ""]]}, {"id": "1801.02940", "submitter": "Robert Rovetto", "authors": "Robert J. Rovetto", "title": "An Ontology for Satellite Databases", "comments": null, "journal-ref": "Earth Sci Inform (2017) 10: 417", "doi": "10.1007/s12145-017-0290-x", "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper demonstrates the development of ontology for satellite databases.\nFirst, I create a computational ontology for the Union of Concerned Scientists\n(UCS) Satellite Database (UCSSD for short), called the UCS Satellite Ontology\n(or UCSSO). Second, in developing UCSSO I show that The Space Situational\nAwareness Ontology (SSAO) (Rovetto and Kelso 2016)--an existing space domain\nreference ontology--and related ontology work by the author (Rovetto 2015,\n2016) can be used either (i) with a database-specific local ontology such as\nUCSSO, or (ii) in its stead. In case (i), local ontologies such as UCSSO can\nreuse SSAO terms, perform term mappings, or extend it. In case (ii), the\nauthor's orbital space ontology work, such as the SSAO, is usable by the UCSSD\nand organizations with other space object catalogs, as a reference ontology\nsuite providing a common semantically-rich domain model. The SSAO, UCSSO, and\nthe broader Orbital Space Environment Domain Ontology project is online at\nhttp://purl.org/space-ontology and GitHub. This ontology effort aims, in part,\nto provide accurate formal representations of the domain for various\napplications. Ontology engineering has the potential to facilitate the sharing\nand integration of satellite data from federated databases and sensors for\nsafer spaceflight.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jan 2018 16:59:49 GMT"}], "update_date": "2018-01-10", "authors_parsed": [["Rovetto", "Robert J.", ""]]}, {"id": "1801.03003", "submitter": "Lise Verlaet", "authors": "Lise Verlaet (LERASS), Sidonie Gallot (LERASS)", "title": "Between collective intelligence and semantic web : hypermediating sites.\n  Contribution to technologies of intelligence", "comments": null, "journal-ref": "EJDE - Electronic Journal of Digital Enterprise (ISSN: 1776-2960),\n  Academic e-Journal eJ.D.E. (www.scientifics.fr/ejde), 2013,\n  http://www.scientifics.fr/ejde/html/1776-2960%20R374.htm", "doi": null, "report-no": null, "categories": "cs.AI cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a new form of access to knowledge through what we\ncall \"hypermediator websites\". These hypermediator sites are intermediate\nbetween information devices that just scan the book culture and a \"real\"\nhypertext writing format.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 13:53:41 GMT"}], "update_date": "2018-01-10", "authors_parsed": [["Verlaet", "Lise", "", "LERASS"], ["Gallot", "Sidonie", "", "LERASS"]]}, {"id": "1801.03058", "submitter": "Imon Banerjee", "authors": "Imon Banerjee, Michael Francis Gensheimer, Douglas J. Wood, Solomon\n  Henry, Daniel Chang, Daniel L. Rubin", "title": "Abstract: Probabilistic Prognostic Estimates of Survival in Metastatic\n  Cancer Patients", "comments": null, "journal-ref": "AMIA Informatics Conference 2018", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a deep learning model - Probabilistic Prognostic Estimates of\nSurvival in Metastatic Cancer Patients (PPES-Met) for estimating short-term\nlife expectancy (3 months) of the patients by analyzing free-text clinical\nnotes in the electronic medical record, while maintaining the temporal visit\nsequence. In a single framework, we integrated semantic data mapping and neural\nembedding technique to produce a text processing method that extracts relevant\ninformation from heterogeneous types of clinical notes in an unsupervised\nmanner, and we designed a recurrent neural network to model the temporal\ndependency of the patient visits. The model was trained on a large dataset\n(10,293 patients) and validated on a separated dataset (1818 patients). Our\nmethod achieved an area under the ROC curve (AUC) of 0.89. To provide\nexplain-ability, we developed an interactive graphical tool that may improve\nphysician understanding of the basis for the model's predictions. The high\naccuracy and explain-ability of the PPES-Met model may enable our model to be\nused as a decision support tool to personalize metastatic cancer treatment and\nprovide valuable assistance to the physicians.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 17:51:12 GMT"}, {"version": "v2", "created": "Fri, 13 Jul 2018 23:56:14 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Banerjee", "Imon", ""], ["Gensheimer", "Michael Francis", ""], ["Wood", "Douglas J.", ""], ["Henry", "Solomon", ""], ["Chang", "Daniel", ""], ["Rubin", "Daniel L.", ""]]}, {"id": "1801.03132", "submitter": "Chen Wang", "authors": "Chen Wang, Suzhen Wang, Fuyan Shi, Zaixiang Wang", "title": "Robust Propensity Score Computation Method based on Machine Learning\n  with Label-corrupted Data", "comments": "26 pages, 4 figures, 8tables, to be submitted to peer-review journals\n  soon", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In biostatistics, propensity score is a common approach to analyze the\nimbalance of covariate and process confounding covariates to eliminate\ndifferences between groups. While there are an abundant amount of methods to\ncompute propensity score, a common issue of them is the corrupted labels in the\ndataset. For example, the data collected from the patients could contain\nsamples that are treated mistakenly, and the computing methods could\nincorporate them as a misleading information. In this paper, we propose a\nMachine Learning-based method to handle the problem. Specifically, we utilize\nthe fact that the majority of sample should be labeled with the correct\ninstance and design an approach to first cluster the data with spectral\nclustering and then sample a new dataset with a distribution processed from the\nclustering results. The propensity score is computed by Xgboost, and a\nmathematical justification of our method is provided in this paper. The\nexperimental results illustrate that xgboost propensity scores computing with\nthe data processed by our method could outperform the same method with original\ndata, and the advantages of our method increases as we add some artificial\ncorruptions to the dataset. Meanwhile, the implementation of xgboost to compute\npropensity score for multiple treatments is also a pioneering work in the area.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 20:35:31 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Wang", "Chen", ""], ["Wang", "Suzhen", ""], ["Shi", "Fuyan", ""], ["Wang", "Zaixiang", ""]]}, {"id": "1801.03137", "submitter": "Ben Parr", "authors": "Igor Gitman, Deepak Dilipkumar, Ben Parr", "title": "Convergence Analysis of Gradient Descent Algorithms with Proportional\n  Updates", "comments": "Source code (uses TensorFlow): https://github.com/bparr/lars", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of deep learning in recent years has brought with it increasingly\nclever optimization methods to deal with complex, non-linear loss functions.\nThese methods are often designed with convex optimization in mind, but have\nbeen shown to work well in practice even for the highly non-convex optimization\nassociated with neural networks. However, one significant drawback of these\nmethods when they are applied to deep learning is that the magnitude of the\nupdate step is sometimes disproportionate to the magnitude of the weights (much\nsmaller or larger), leading to training instabilities such as vanishing and\nexploding gradients. An idea to combat this issue is gradient descent with\nproportional updates. Gradient descent with proportional updates was introduced\nin 2017. It was independently developed by You et al (Layer-wise Adaptive Rate\nScaling (LARS) algorithm) and by Abu-El-Haija (PercentDelta algorithm). The\nbasic idea of both of these algorithms is to make each step of the gradient\ndescent proportional to the current weight norm and independent of the gradient\nmagnitude. It is common in the context of new optimization methods to prove\nconvergence or derive regret bounds under the assumption of Lipschitz\ncontinuity and convexity. However, even though LARS and PercentDelta were shown\nto work well in practice, there is no theoretical analysis of the convergence\nproperties of these algorithms. Thus it is not clear if the idea of gradient\ndescent with proportional updates is used in the optimal way, or if it could be\nimproved by using a different norm or specific learning rate schedule, for\nexample. Moreover, it is not clear if these algorithms can be extended to other\nproblems, besides neural networks. We attempt to answer these questions by\nestablishing the theoretical analysis of gradient descent with proportional\nupdates, and verifying this analysis with empirical examples.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 20:51:28 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Gitman", "Igor", ""], ["Dilipkumar", "Deepak", ""], ["Parr", "Ben", ""]]}, {"id": "1801.03138", "submitter": "Ben Parr", "authors": "Ben Parr", "title": "Deep In-GPU Experience Replay", "comments": "Source code (uses TensorFlow):\n  https://github.com/bparr/gpu-experience-replay", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experience replay allows a reinforcement learning agent to train on samples\nfrom a large amount of the most recent experiences. A simple in-RAM experience\nreplay stores these most recent experiences in a list in RAM, and then copies\nsampled batches to the GPU for training. I moved this list to the GPU, thus\ncreating an in-GPU experience replay, and a training step that no longer has\ninputs copied from the CPU. I trained an agent to play Super Smash Bros. Melee,\nusing internal game memory values as inputs and outputting controller button\npresses. A single state in Melee contains 27 floats, so the full experience\nreplay fits on a single GPU. For a batch size of 128, the in-GPU experience\nreplay trained twice as fast as the in-RAM experience replay. As far as I know,\nthis is the first in-GPU implementation of experience replay. Finally, I note a\nfew ideas for fitting the experience replay inside the GPU when the environment\nstate requires more memory.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 20:52:33 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Parr", "Ben", ""]]}, {"id": "1801.03143", "submitter": "Artit Wangperawong", "authors": "Artit Wangperawong, Kettip Kriangchaivech, Austin Lanari, Supui Lam,\n  Panthong Wangperawong", "title": "Comparing heterogeneous entities using artificial neural networks of\n  trainable weighted structural components and machine-learned activation\n  functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To compare entities of differing types and structural components, the\nartificial neural network paradigm was used to cross-compare structural\ncomponents between heterogeneous documents. Trainable weighted structural\ncomponents were input into machine-learned activation functions of the neurons.\nThe model was used for matching news articles and videos, where the inputs and\nactivation functions respectively consisted of term vectors and cosine\nsimilarity measures between the weighted structural components. The model was\ntested with different weights, achieving as high as 59.2% accuracy for matching\nvideos to news articles. A mobile application user interface for recommending\nrelated videos for news articles was developed to demonstrate consumer value,\nincluding its potential usefulness for cross-selling products from unrelated\ncategories.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 21:20:08 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Wangperawong", "Artit", ""], ["Kriangchaivech", "Kettip", ""], ["Lanari", "Austin", ""], ["Lam", "Supui", ""], ["Wangperawong", "Panthong", ""]]}, {"id": "1801.03150", "submitter": "Mathew Monfort", "authors": "Mathew Monfort, Alex Andonian, Bolei Zhou, Kandan Ramakrishnan, Sarah\n  Adel Bargal, Tom Yan, Lisa Brown, Quanfu Fan, Dan Gutfruend, Carl Vondrick,\n  Aude Oliva", "title": "Moments in Time Dataset: one million videos for event understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the Moments in Time Dataset, a large-scale human-annotated\ncollection of one million short videos corresponding to dynamic events\nunfolding within three seconds. Modeling the spatial-audio-temporal dynamics\neven for actions occurring in 3 second videos poses many challenges: meaningful\nevents do not include only people, but also objects, animals, and natural\nphenomena; visual and auditory events can be symmetrical in time (\"opening\" is\n\"closing\" in reverse), and either transient or sustained. We describe the\nannotation process of our dataset (each video is tagged with one action or\nactivity label among 339 different classes), analyze its scale and diversity in\ncomparison to other large-scale video datasets for action recognition, and\nreport results of several baseline models addressing separately, and jointly,\nthree modalities: spatial, temporal and auditory. The Moments in Time dataset,\ndesigned to have a large coverage and diversity of events in both visual and\nauditory modalities, can serve as a new challenge to develop models that scale\nto the level of complexity and abstract reasoning that a human processes on a\ndaily basis.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 21:46:38 GMT"}, {"version": "v2", "created": "Thu, 27 Dec 2018 15:25:18 GMT"}, {"version": "v3", "created": "Sat, 16 Feb 2019 13:20:03 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Monfort", "Mathew", ""], ["Andonian", "Alex", ""], ["Zhou", "Bolei", ""], ["Ramakrishnan", "Kandan", ""], ["Bargal", "Sarah Adel", ""], ["Yan", "Tom", ""], ["Brown", "Lisa", ""], ["Fan", "Quanfu", ""], ["Gutfruend", "Dan", ""], ["Vondrick", "Carl", ""], ["Oliva", "Aude", ""]]}, {"id": "1801.03160", "submitter": "Felix Lindner", "authors": "Felix Lindner and Martin Mose Bentzen", "title": "A Formalization of Kant's Second Formulation of the Categorical\n  Imperative", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a formalization and computational implementation of the second\nformulation of Kant's categorical imperative. This ethical principle requires\nan agent to never treat someone merely as a means but always also as an end.\nHere we interpret this principle in terms of how persons are causally affected\nby actions. We introduce Kantian causal agency models in which moral patients,\nactions, goals, and causal influence are represented, and we show how to\nformalize several readings of Kant's categorical imperative that correspond to\nKant's concept of strict and wide duties towards oneself and others. Stricter\nversions handle cases where an action directly causally affects oneself or\nothers, whereas the wide version maximizes the number of persons being treated\nas an end. We discuss limitations of our formalization by pointing to one of\nKant's cases that the machinery cannot handle in a satisfying way.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 22:23:21 GMT"}, {"version": "v2", "created": "Wed, 21 Mar 2018 12:23:33 GMT"}, {"version": "v3", "created": "Thu, 11 Jul 2019 13:22:27 GMT"}], "update_date": "2019-07-12", "authors_parsed": [["Lindner", "Felix", ""], ["Bentzen", "Martin Mose", ""]]}, {"id": "1801.03164", "submitter": "Justin Gottschlich", "authors": "Justin Gottschlich", "title": "Paranom: A Parallel Anomaly Dataset Generator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present Paranom, a parallel anomaly dataset generator. We\ndiscuss its design and provide brief experimental results demonstrating its\nusefulness in improving the classification correctness of LSTM-AD, a\nstate-of-the-art anomaly detection model.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 22:33:51 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Gottschlich", "Justin", ""]]}, {"id": "1801.03168", "submitter": "Justin Gottschlich", "authors": "Tae Jun Lee, Justin Gottschlich, Nesime Tatbul, Eric Metcalf, Stan\n  Zdonik", "title": "Greenhouse: A Zero-Positive Machine Learning System for Time-Series\n  Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This short paper describes our ongoing research on Greenhouse - a\nzero-positive machine learning system for time-series anomaly detection.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 22:44:21 GMT"}, {"version": "v2", "created": "Sun, 21 Jan 2018 22:05:31 GMT"}, {"version": "v3", "created": "Sun, 11 Feb 2018 22:32:31 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Lee", "Tae Jun", ""], ["Gottschlich", "Justin", ""], ["Tatbul", "Nesime", ""], ["Metcalf", "Eric", ""], ["Zdonik", "Stan", ""]]}, {"id": "1801.03175", "submitter": "Justin Gottschlich", "authors": "Tae Jun Lee, Justin Gottschlich, Nesime Tatbul, Eric Metcalf, Stan\n  Zdonik", "title": "Precision and Recall for Range-Based Anomaly Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical anomaly detection is principally concerned with point-based\nanomalies, anomalies that occur at a single data point. In this paper, we\npresent a new mathematical model to express range-based anomalies, anomalies\nthat occur over a range (or period) of time.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jan 2018 23:01:07 GMT"}, {"version": "v2", "created": "Sun, 21 Jan 2018 22:10:35 GMT"}, {"version": "v3", "created": "Sun, 11 Feb 2018 22:20:17 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Lee", "Tae Jun", ""], ["Gottschlich", "Justin", ""], ["Tatbul", "Nesime", ""], ["Metcalf", "Eric", ""], ["Zdonik", "Stan", ""]]}, {"id": "1801.03230", "submitter": "Sarfaraz Hussein", "authors": "Sarfaraz Hussein, Pujan Kandel, Candice W. Bolan, Michael B. Wallace,\n  and Ulas Bagci", "title": "Lung and Pancreatic Tumor Characterization in the Deep Learning Era:\n  Novel Supervised and Unsupervised Learning Approaches", "comments": "Accepted for publication in IEEE Transactions on Medical Imaging 2019", "journal-ref": null, "doi": "10.1109/TMI.2019.2894349", "report-no": null, "categories": "cs.CV cs.AI cs.LG q-bio.QM q-bio.TO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Risk stratification (characterization) of tumors from radiology images can be\nmore accurate and faster with computer-aided diagnosis (CAD) tools. Tumor\ncharacterization through such tools can also enable non-invasive cancer\nstaging, prognosis, and foster personalized treatment planning as a part of\nprecision medicine. In this study, we propose both supervised and unsupervised\nmachine learning strategies to improve tumor characterization. Our first\napproach is based on supervised learning for which we demonstrate significant\ngains with deep learning algorithms, particularly by utilizing a 3D\nConvolutional Neural Network and Transfer Learning. Motivated by the\nradiologists' interpretations of the scans, we then show how to incorporate\ntask dependent feature representations into a CAD system via a\ngraph-regularized sparse Multi-Task Learning (MTL) framework. In the second\napproach, we explore an unsupervised learning algorithm to address the limited\navailability of labeled training data, a common problem in medical imaging\napplications. Inspired by learning from label proportion (LLP) approaches in\ncomputer vision, we propose to use proportion-SVM for characterizing tumors. We\nalso seek the answer to the fundamental question about the goodness of \"deep\nfeatures\" for unsupervised tumor classification. We evaluate our proposed\nsupervised and unsupervised learning algorithms on two different tumor\ndiagnosis challenges: lung and pancreas with 1018 CT and 171 MRI scans,\nrespectively, and obtain the state-of-the-art sensitivity and specificity\nresults in both problems.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jan 2018 03:47:07 GMT"}, {"version": "v2", "created": "Sun, 29 Jul 2018 05:30:33 GMT"}, {"version": "v3", "created": "Fri, 18 Jan 2019 13:25:51 GMT"}], "update_date": "2019-01-21", "authors_parsed": [["Hussein", "Sarfaraz", ""], ["Kandel", "Pujan", ""], ["Bolan", "Candice W.", ""], ["Wallace", "Michael B.", ""], ["Bagci", "Ulas", ""]]}, {"id": "1801.03233", "submitter": "Mohammadreza Esfandiari", "authors": "Mohammadreza Esfandiari, Senjuti Basu Roy, Sihem Amer-Yahia", "title": "Eliciting Worker Preference for Task Completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current crowdsourcing platforms provide little support for worker feedback.\nWorkers are sometimes invited to post free text describing their experience and\npreferences in completing tasks. They can also use forums such as Turker\nNation1 to exchange preferences on tasks and requesters. In fact, crowdsourcing\nplatforms rely heavily on observing workers and inferring their preferences\nimplicitly. In this work, we believe that asking workers to indicate their\npreferences explicitly improve their experience in task completion and hence,\nthe quality of their contributions. Explicit elicitation can indeed help to\nbuild more accurate worker models for task completion that captures the\nevolving nature of worker preferences. We design a worker model whose accuracy\nis improved iteratively by requesting preferences for task factors such as\nrequired skills, task payment, and task relevance. We propose a generic\nframework, develop efficient solutions in realistic scenarios, and run\nextensive experiments that show the benefit of explicit preference elicitation\nover implicit ones with statistical significance.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jan 2018 03:55:09 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Esfandiari", "Mohammadreza", ""], ["Roy", "Senjuti Basu", ""], ["Amer-Yahia", "Sihem", ""]]}, {"id": "1801.03239", "submitter": "M Sadegh Riazi", "authors": "M. Sadegh Riazi and Christian Weinert and Oleksandr Tkachenko and\n  Ebrahim M. Songhori and Thomas Schneider and Farinaz Koushanfar", "title": "Chameleon: A Hybrid Secure Computation Framework for Machine Learning\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Chameleon, a novel hybrid (mixed-protocol) framework for secure\nfunction evaluation (SFE) which enables two parties to jointly compute a\nfunction without disclosing their private inputs. Chameleon combines the best\naspects of generic SFE protocols with the ones that are based upon additive\nsecret sharing. In particular, the framework performs linear operations in the\nring $\\mathbb{Z}_{2^l}$ using additively secret shared values and nonlinear\noperations using Yao's Garbled Circuits or the Goldreich-Micali-Wigderson\nprotocol. Chameleon departs from the common assumption of additive or linear\nsecret sharing models where three or more parties need to communicate in the\nonline phase: the framework allows two parties with private inputs to\ncommunicate in the online phase under the assumption of a third node generating\ncorrelated randomness in an offline phase. Almost all of the heavy\ncryptographic operations are precomputed in an offline phase which\nsubstantially reduces the communication overhead. Chameleon is both scalable\nand significantly more efficient than the ABY framework (NDSS'15) it is based\non. Our framework supports signed fixed-point numbers. In particular,\nChameleon's vector dot product of signed fixed-point numbers improves the\nefficiency of mining and classification of encrypted data for algorithms based\nupon heavy matrix multiplications. Our evaluation of Chameleon on a 5 layer\nconvolutional deep neural network shows 133x and 4.2x faster executions than\nMicrosoft CryptoNets (ICML'16) and MiniONN (CCS'17), respectively.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jan 2018 04:57:56 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Riazi", "M. Sadegh", ""], ["Weinert", "Christian", ""], ["Tkachenko", "Oleksandr", ""], ["Songhori", "Ebrahim M.", ""], ["Schneider", "Thomas", ""], ["Koushanfar", "Farinaz", ""]]}, {"id": "1801.03326", "submitter": "Kamil Ciosek", "authors": "Kamil Ciosek and Shimon Whiteson", "title": "Expected Policy Gradients for Reinforcement Learning", "comments": "36 pages, submitted for review to JMLR. This is an extended version\n  of our paper in the AAAI-18 conference (arXiv:1706.05374)", "journal-ref": "Journal of Machine Learning Research, Vol. 21, (52):1-51, 2020", "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose expected policy gradients (EPG), which unify stochastic policy\ngradients (SPG) and deterministic policy gradients (DPG) for reinforcement\nlearning. Inspired by expected sarsa, EPG integrates (or sums) across actions\nwhen estimating the gradient, instead of relying only on the action in the\nsampled trajectory. For continuous action spaces, we first derive a practical\nresult for Gaussian policies and quadratic critics and then extend it to a\nuniversal analytical method, covering a broad class of actors and critics,\nincluding Gaussian, exponential families, and policies with bounded support.\nFor Gaussian policies, we introduce an exploration method that uses covariance\nproportional to the matrix exponential of the scaled Hessian of the critic with\nrespect to the actions. For discrete action spaces, we derive a variant of EPG\nbased on softmax policies. We also establish a new general policy gradient\ntheorem, of which the stochastic and deterministic policy gradient theorems are\nspecial cases. Furthermore, we prove that EPG reduces the variance of the\ngradient estimates without requiring deterministic policies and with little\ncomputational overhead. Finally, we provide an extensive experimental\nevaluation of EPG and show that it outperforms existing approaches on multiple\nchallenging control domains.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jan 2018 11:59:59 GMT"}, {"version": "v2", "created": "Sat, 2 May 2020 07:20:30 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Ciosek", "Kamil", ""], ["Whiteson", "Shimon", ""]]}, {"id": "1801.03331", "submitter": "Craig Innes", "authors": "Craig Innes, Alex Lascarides, Stefano V Albrecht, Subramanian\n  Ramamoorthy, Benjamin Rosman", "title": "Reasoning about Unforeseen Possibilities During Policy Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for learning optimal policies in autonomous agents often assume that\nthe way the domain is conceptualised---its possible states and actions and\ntheir causal structure---is known in advance and does not change during\nlearning. This is an unrealistic assumption in many scenarios, because new\nevidence can reveal important information about what is possible, possibilities\nthat the agent was not aware existed prior to learning. We present a model of\nan agent which both discovers and learns to exploit unforeseen possibilities\nusing two sources of evidence: direct interaction with the world and\ncommunication with a domain expert. We use a combination of probabilistic and\nsymbolic reasoning to estimate all components of the decision problem,\nincluding its set of random variables and their causal dependencies. Agent\nsimulations show that the agent converges on optimal polices even when it\nstarts out unaware of factors that are critical to behaving optimally.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jan 2018 12:16:43 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Innes", "Craig", ""], ["Lascarides", "Alex", ""], ["Albrecht", "Stefano V", ""], ["Ramamoorthy", "Subramanian", ""], ["Rosman", "Benjamin", ""]]}, {"id": "1801.03354", "submitter": "Blai Bonet", "authors": "Wilmer Bandres, Blai Bonet, Hector Geffner", "title": "Planning with Pixels in (Almost) Real Time", "comments": "Published at AAAI-18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, width-based planning methods have been shown to yield\nstate-of-the-art results in the Atari 2600 video games. For this, the states\nwere associated with the (RAM) memory states of the simulator. In this work, we\nconsider the same planning problem but using the screen instead. By using the\nsame visual inputs, the planning results can be compared with those of humans\nand learning methods. We show that the planning approach, out of the box and\nwithout training, results in scores that compare well with those obtained by\nhumans and learning methods, and moreover, by developing an episodic, rollout\nversion of the IW(k) algorithm, we show that such scores can be obtained in\nalmost real time.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jan 2018 12:54:00 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Bandres", "Wilmer", ""], ["Bonet", "Blai", ""], ["Geffner", "Hector", ""]]}, {"id": "1801.03355", "submitter": "L\\'aszl\\'o Csat\\'o", "authors": "L\\'aszl\\'o Csat\\'o", "title": "Axiomatizations of inconsistency indices for triads", "comments": "12 pages", "journal-ref": "Annals of Operations Research, 280(1-2): 99-110, 2019", "doi": "10.1007/s10479-019-03312-0", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pairwise comparison matrices often exhibit inconsistency, therefore many\nindices have been suggested to measure their deviation from a consistent\nmatrix. A set of axioms has been proposed recently that is required to be\nsatisfied by any reasonable inconsistency index. This set seems to be not\nexhaustive as illustrated by an example, hence it is expanded by adding two new\nproperties. All axioms are considered on the set of triads, pairwise comparison\nmatrices with three alternatives, which is the simplest case of inconsistency.\nWe choose the logically independent properties and prove that they\ncharacterize, that is, uniquely determine the inconsistency ranking induced by\nmost inconsistency indices that coincide on this restricted domain. Since\ntriads play a prominent role in a number of inconsistency indices, our results\ncan also contribute to the measurement of inconsistency for pairwise comparison\nmatrices with more than three alternatives.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jan 2018 12:56:48 GMT"}, {"version": "v2", "created": "Thu, 14 Mar 2019 08:16:29 GMT"}], "update_date": "2020-05-28", "authors_parsed": [["Csat\u00f3", "L\u00e1szl\u00f3", ""]]}, {"id": "1801.03454", "submitter": "Ruth Fong", "authors": "Ruth Fong and Andrea Vedaldi", "title": "Net2Vec: Quantifying and Explaining how Concepts are Encoded by Filters\n  in Deep Neural Networks", "comments": "Camera-Ready for CVPR18; supplementary materials:\n  http://ruthcfong.github.io/files/net2vec_supps.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In an effort to understand the meaning of the intermediate representations\ncaptured by deep networks, recent papers have tried to associate specific\nsemantic concepts to individual neural network filter responses, where\ninteresting correlations are often found, largely by focusing on extremal\nfilter responses. In this paper, we show that this approach can favor\neasy-to-interpret cases that are not necessarily representative of the average\nbehavior of a representation.\n  A more realistic but harder-to-study hypothesis is that semantic\nrepresentations are distributed, and thus filters must be studied in\nconjunction. In order to investigate this idea while enabling systematic\nvisualization and quantification of multiple filter responses, we introduce the\nNet2Vec framework, in which semantic concepts are mapped to vectorial\nembeddings based on corresponding filter responses. By studying such\nembeddings, we are able to show that 1., in most cases, multiple filters are\nrequired to code for a concept, that 2., often filters are not concept specific\nand help encode multiple concepts, and that 3., compared to single filter\nactivations, filter embeddings are able to better characterize the meaning of a\nrepresentation and its relationship to other concepts.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jan 2018 17:01:36 GMT"}, {"version": "v2", "created": "Thu, 29 Mar 2018 11:00:02 GMT"}], "update_date": "2018-03-30", "authors_parsed": [["Fong", "Ruth", ""], ["Vedaldi", "Andrea", ""]]}, {"id": "1801.03526", "submitter": "Daniel Abolafia", "authors": "Daniel A. Abolafia, Mohammad Norouzi, Jonathan Shen, Rui Zhao, Quoc V.\n  Le", "title": "Neural Program Synthesis with Priority Queue Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of program synthesis in the presence of a reward\nfunction over the output of programs, where the goal is to find programs with\nmaximal rewards. We employ an iterative optimization scheme, where we train an\nRNN on a dataset of K best programs from a priority queue of the generated\nprograms so far. Then, we synthesize new programs and add them to the priority\nqueue by sampling from the RNN. We benchmark our algorithm, called priority\nqueue training (or PQT), against genetic algorithm and reinforcement learning\nbaselines on a simple but expressive Turing complete programming language\ncalled BF. Our experimental results show that our simple PQT algorithm\nsignificantly outperforms the baselines. By adding a program length penalty to\nthe reward function, we are able to synthesize short, human readable programs.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jan 2018 19:35:25 GMT"}, {"version": "v2", "created": "Fri, 23 Mar 2018 23:40:46 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Abolafia", "Daniel A.", ""], ["Norouzi", "Mohammad", ""], ["Shen", "Jonathan", ""], ["Zhao", "Rui", ""], ["Le", "Quoc V.", ""]]}, {"id": "1801.03604", "submitter": "Chandra Khatri", "authors": "Ashwin Ram, Rohit Prasad, Chandra Khatri, Anu Venkatesh, Raefer\n  Gabriel, Qing Liu, Jeff Nunn, Behnam Hedayatnia, Ming Cheng, Ashish Nagar,\n  Eric King, Kate Bland, Amanda Wartick, Yi Pan, Han Song, Sk Jayadevan, Gene\n  Hwang, Art Pettigrue", "title": "Conversational AI: The Science Behind the Alexa Prize", "comments": "18 pages, 5 figures, Alexa Prize Proceedings Paper\n  (https://developer.amazon.com/alexaprize/proceedings), Alexa Prize University\n  Competition to advance Conversational AI", "journal-ref": "Alexa.Prize.Proceedings\n  https://developer.amazon.com/alexaprize/proceedings accessed (2018)-01-01", "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CY cs.HC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational agents are exploding in popularity. However, much work remains\nin the area of social conversation as well as free-form conversation over a\nbroad range of domains and topics. To advance the state of the art in\nconversational AI, Amazon launched the Alexa Prize, a 2.5-million-dollar\nuniversity competition where sixteen selected university teams were challenged\nto build conversational agents, known as socialbots, to converse coherently and\nengagingly with humans on popular topics such as Sports, Politics,\nEntertainment, Fashion and Technology for 20 minutes. The Alexa Prize offers\nthe academic community a unique opportunity to perform research with a live\nsystem used by millions of users. The competition provided university teams\nwith real user conversational data at scale, along with the user-provided\nratings and feedback augmented with annotations by the Alexa team. This enabled\nteams to effectively iterate and make improvements throughout the competition\nwhile being evaluated in real-time through live user interactions. To build\ntheir socialbots, university teams combined state-of-the-art techniques with\nnovel strategies in the areas of Natural Language Understanding, Context\nModeling, Dialog Management, Response Generation, and Knowledge Acquisition. To\nsupport the efforts of participating teams, the Alexa Prize team made\nsignificant scientific and engineering investments to build and improve\nConversational Speech Recognition, Topic Tracking, Dialog Evaluation, Voice\nUser Experience, and tools for traffic management and scalability. This paper\noutlines the advances created by the university teams as well as the Alexa\nPrize team to achieve the common goal of solving the problem of Conversational\nAI.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 01:23:50 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Ram", "Ashwin", ""], ["Prasad", "Rohit", ""], ["Khatri", "Chandra", ""], ["Venkatesh", "Anu", ""], ["Gabriel", "Raefer", ""], ["Liu", "Qing", ""], ["Nunn", "Jeff", ""], ["Hedayatnia", "Behnam", ""], ["Cheng", "Ming", ""], ["Nagar", "Ashish", ""], ["King", "Eric", ""], ["Bland", "Kate", ""], ["Wartick", "Amanda", ""], ["Pan", "Yi", ""], ["Song", "Han", ""], ["Jayadevan", "Sk", ""], ["Hwang", "Gene", ""], ["Pettigrue", "Art", ""]]}, {"id": "1801.03612", "submitter": "Marco Cusumano-Towner", "authors": "Marco F. Cusumano-Towner, Vikash K. Mansinghka", "title": "Using probabilistic programs as proposals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo inference has asymptotic guarantees, but can be slow when using\ngeneric proposals. Handcrafted proposals that rely on user knowledge about the\nposterior distribution can be efficient, but are difficult to derive and\nimplement. This paper proposes to let users express their posterior knowledge\nin the form of proposal programs, which are samplers written in probabilistic\nprogramming languages. One strategy for writing good proposal programs is to\ncombine domain-specific heuristic algorithms with neural network models. The\nheuristics identify high probability regions, and the neural networks model the\nposterior uncertainty around the outputs of the algorithm. Proposal programs\ncan be used as proposal distributions in importance sampling and\nMetropolis-Hastings samplers without sacrificing asymptotic consistency, and\ncan be optimized offline using inference compilation. Support for optimizing\nand using proposal programs is easily implemented in a sampling-based\nprobabilistic programming runtime. The paper illustrates the proposed technique\nwith a proposal program that combines RANSAC and neural networks to accelerate\ninference in a Bayesian linear regression with outliers model.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 02:07:39 GMT"}, {"version": "v2", "created": "Sat, 13 Jan 2018 19:47:08 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Cusumano-Towner", "Marco F.", ""], ["Mansinghka", "Vikash K.", ""]]}, {"id": "1801.03622", "submitter": "Chandra Khatri", "authors": "Fenfei Guo, Angeliki Metallinou, Chandra Khatri, Anirudh Raju, Anu\n  Venkatesh, Ashwin Ram", "title": "Topic-based Evaluation for Conversational Bots", "comments": "10 Pages, 2 figures, 9 tables. NIPS 2017 Conversational AI workshop\n  paper.\n  http://alborz-geramifard.com/workshops/nips17-Conversational-AI/Main.html", "journal-ref": "Nips.Workshop.ConversationalAI 2017-12-08", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.HC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dialog evaluation is a challenging problem, especially for non task-oriented\ndialogs where conversational success is not well-defined. We propose to\nevaluate dialog quality using topic-based metrics that describe the ability of\na conversational bot to sustain coherent and engaging conversations on a topic,\nand the diversity of topics that a bot can handle. To detect conversation\ntopics per utterance, we adopt Deep Average Networks (DAN) and train a topic\nclassifier on a variety of question and query data categorized into multiple\ntopics. We propose a novel extension to DAN by adding a topic-word attention\ntable that allows the system to jointly capture topic keywords in an utterance\nand perform topic classification. We compare our proposed topic based metrics\nwith the ratings provided by users and show that our metrics both correlate\nwith and complement human judgment. Our analysis is performed on tens of\nthousands of real human-bot dialogs from the Alexa Prize competition and\nhighlights user expectations for conversational bots.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 03:20:02 GMT"}], "update_date": "2018-01-12", "authors_parsed": [["Guo", "Fenfei", ""], ["Metallinou", "Angeliki", ""], ["Khatri", "Chandra", ""], ["Raju", "Anirudh", ""], ["Venkatesh", "Anu", ""], ["Ram", "Ashwin", ""]]}, {"id": "1801.03625", "submitter": "Chandra Khatri", "authors": "Anu Venkatesh, Chandra Khatri, Ashwin Ram, Fenfei Guo, Raefer Gabriel,\n  Ashish Nagar, Rohit Prasad, Ming Cheng, Behnam Hedayatnia, Angeliki\n  Metallinou, Rahul Goel, Shaohua Yang, Anirudh Raju", "title": "On Evaluating and Comparing Open Domain Dialog Systems", "comments": "10 pages, 5 tables. NIPS 2017 Conversational AI workshop.\n  http://alborz-geramifard.com/workshops/nips17-Conversational-AI/Main.html", "journal-ref": "NIPS.Workshop.ConversationalAI 2017-12-08\n  http://alborz-geramifard.com/workshops/nips17-Conversational-AI/Main.html\n  accessed 2018-01-01", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.HC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational agents are exploding in popularity. However, much work remains\nin the area of non goal-oriented conversations, despite significant growth in\nresearch interest over recent years. To advance the state of the art in\nconversational AI, Amazon launched the Alexa Prize, a 2.5-million dollar\nuniversity competition where sixteen selected university teams built\nconversational agents to deliver the best social conversational experience.\nAlexa Prize provided the academic community with the unique opportunity to\nperform research with a live system used by millions of users. The subjectivity\nassociated with evaluating conversations is key element underlying the\nchallenge of building non-goal oriented dialogue systems. In this paper, we\npropose a comprehensive evaluation strategy with multiple metrics designed to\nreduce subjectivity by selecting metrics which correlate well with human\njudgement. The proposed metrics provide granular analysis of the conversational\nagents, which is not captured in human ratings. We show that these metrics can\nbe used as a reasonable proxy for human judgment. We provide a mechanism to\nunify the metrics for selecting the top performing agents, which has also been\napplied throughout the Alexa Prize competition. To our knowledge, to date it is\nthe largest setting for evaluating agents with millions of conversations and\nhundreds of thousands of ratings from users. We believe that this work is a\nstep towards an automatic evaluation process for conversational AIs.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 03:30:00 GMT"}, {"version": "v2", "created": "Wed, 26 Dec 2018 20:15:08 GMT"}], "update_date": "2018-12-31", "authors_parsed": [["Venkatesh", "Anu", ""], ["Khatri", "Chandra", ""], ["Ram", "Ashwin", ""], ["Guo", "Fenfei", ""], ["Gabriel", "Raefer", ""], ["Nagar", "Ashish", ""], ["Prasad", "Rohit", ""], ["Cheng", "Ming", ""], ["Hedayatnia", "Behnam", ""], ["Metallinou", "Angeliki", ""], ["Goel", "Rahul", ""], ["Yang", "Shaohua", ""], ["Raju", "Anirudh", ""]]}, {"id": "1801.03737", "submitter": "Stuart Armstrong", "authors": "Stuart Armstrong", "title": "Counterfactual equivalence for POMDPs, and underlying deterministic\n  environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partially Observable Markov Decision Processes (POMDPs) are rich environments\noften used in machine learning. But the issue of information and causal\nstructures in POMDPs has been relatively little studied. This paper presents\nthe concepts of equivalent and counterfactually equivalent POMDPs, where agents\ncannot distinguish which environment they are in though any observations and\nactions. It shows that any POMDP is counterfactually equivalent, for any finite\nnumber of turns, to a deterministic POMDP with all uncertainty concentrated\ninto the initial state. This allows a better understanding of POMDP\nuncertainty, information, and learning.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 12:40:59 GMT"}, {"version": "v2", "created": "Sun, 14 Jan 2018 12:56:00 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Armstrong", "Stuart", ""]]}, {"id": "1801.03825", "submitter": "Mohnish Dubey", "authors": "Mohnish Dubey, Debayan Banerjee, Debanjan Chaudhuri, Jens Lehmann", "title": "EARL: Joint Entity and Relation Linking for Question Answering over\n  Knowledge Graphs", "comments": "International Semantic Web Conference 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many question answering systems over knowledge graphs rely on entity and\nrelation linking components in order to connect the natural language input to\nthe underlying knowledge graph. Traditionally, entity linking and relation\nlinking have been performed either as dependent sequential tasks or as\nindependent parallel tasks. In this paper, we propose a framework called EARL,\nwhich performs entity linking and relation linking as a joint task. EARL\nimplements two different solution strategies for which we provide a comparative\nanalysis in this paper: The first strategy is a formalisation of the joint\nentity and relation linking tasks as an instance of the Generalised Travelling\nSalesman Problem (GTSP). In order to be computationally feasible, we employ\napproximate GTSP solvers. The second strategy uses machine learning in order to\nexploit the connection density between nodes in the knowledge graph. It relies\non three base features and re-ranking steps in order to predict entities and\nrelations. We compare the strategies and evaluate them on a dataset with 5000\nquestions. Both strategies significantly outperform the current\nstate-of-the-art approaches for entity and relation linking.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 15:40:31 GMT"}, {"version": "v2", "created": "Tue, 16 Jan 2018 12:28:56 GMT"}, {"version": "v3", "created": "Sat, 16 Jun 2018 08:21:59 GMT"}, {"version": "v4", "created": "Mon, 25 Jun 2018 14:00:37 GMT"}], "update_date": "2018-06-26", "authors_parsed": [["Dubey", "Mohnish", ""], ["Banerjee", "Debayan", ""], ["Chaudhuri", "Debanjan", ""], ["Lehmann", "Jens", ""]]}, {"id": "1801.03929", "submitter": "Lucas Bechberger", "authors": "Lucas Bechberger and Kai-Uwe K\\\"uhnberger", "title": "Formalized Conceptual Spaces with a Geometric Representation of\n  Correlations", "comments": "Published in the edited volume \"Conceptual Spaces: Elaborations and\n  Applications\". arXiv admin note: text overlap with arXiv:1706.06366,\n  arXiv:1707.02292, arXiv:1707.05165", "journal-ref": null, "doi": "10.1007/978-3-030-12800-5_3", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The highly influential framework of conceptual spaces provides a geometric\nway of representing knowledge. Instances are represented by points in a\nsimilarity space and concepts are represented by convex regions in this space.\nAfter pointing out a problem with the convexity requirement, we propose a\nformalization of conceptual spaces based on fuzzy star-shaped sets. Our\nformalization uses a parametric definition of concepts and extends the original\nframework by adding means to represent correlations between different domains\nin a geometric way. Moreover, we define various operations for our\nformalization, both for creating new concepts from old ones and for measuring\nrelations between concepts. We present an illustrative toy-example and sketch a\nresearch project on concept formation that is based on both our formalization\nand its implementation.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 08:37:58 GMT"}, {"version": "v2", "created": "Sat, 29 Jun 2019 06:35:31 GMT"}], "update_date": "2019-07-02", "authors_parsed": [["Bechberger", "Lucas", ""], ["K\u00fchnberger", "Kai-Uwe", ""]]}, {"id": "1801.03954", "submitter": "Glen Berseth", "authors": "Glen Berseth and Michiel van de Panne", "title": "Model-Based Action Exploration for Learning Dynamic Motion Skills", "comments": "7 pages, 7 figures, conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning has achieved great strides in solving challenging\nmotion control tasks. Recently, there has been significant work on methods for\nexploiting the data gathered during training, but there has been less work on\nhow to best generate the data to learn from. For continuous action domains, the\nmost common method for generating exploratory actions involves sampling from a\nGaussian distribution centred around the mean action output by a policy.\nAlthough these methods can be quite capable, they do not scale well with the\ndimensionality of the action space, and can be dangerous to apply on hardware.\nWe consider learning a forward dynamics model to predict the result,\n($x_{t+1}$), of taking a particular action, ($u$), given a specific observation\nof the state, ($x_{t}$). With this model we perform internal look-ahead\npredictions of outcomes and seek actions we believe have a reasonable chance of\nsuccess. This method alters the exploratory action space, thereby increasing\nlearning speed and enables higher quality solutions to difficult problems, such\nas robotic locomotion and juggling.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 19:05:38 GMT"}, {"version": "v2", "created": "Thu, 12 Apr 2018 03:56:02 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["Berseth", "Glen", ""], ["van de Panne", "Michiel", ""]]}, {"id": "1801.03968", "submitter": "Eisa Alanazi", "authors": "Eisa Alanazi, Malek Mouhoub, Sandra Zilles", "title": "The Complexity of Learning Acyclic Conditional Preference Networks", "comments": "57 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning of user preferences, as represented by, for example, Conditional\nPreference Networks (CP-nets), has become a core issue in AI research. Recent\nstudies investigate learning of CP-nets from randomly chosen examples or from\nmembership and equivalence queries. To assess the optimality of learning\nalgorithms as well as to better understand the combinatorial structure of\nclasses of CP-nets, it is helpful to calculate certain learning-theoretic\ninformation complexity parameters. This article focuses on the frequently\nstudied case of learning from so-called swap examples, which express\npreferences among objects that differ in only one attribute. It presents bounds\non or exact values of some well-studied information complexity parameters,\nnamely the VC dimension, the teaching dimension, and the recursive teaching\ndimension, for classes of acyclic CP-nets. We further provide algorithms that\nlearn tree-structured and general acyclic CP-nets from membership queries.\nUsing our results on complexity parameters, we assess the optimality of our\nalgorithms as well as that of another query learning algorithm for acyclic\nCP-nets presented in the literature. Our algorithms are near-optimal, and can,\nunder certain assumptions, be adapted to the case when the membership oracle is\nfaulty.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 19:56:43 GMT"}, {"version": "v2", "created": "Sat, 25 Aug 2018 20:06:48 GMT"}, {"version": "v3", "created": "Tue, 5 Feb 2019 17:45:09 GMT"}], "update_date": "2019-02-06", "authors_parsed": [["Alanazi", "Eisa", ""], ["Mouhoub", "Malek", ""], ["Zilles", "Sandra", ""]]}, {"id": "1801.03984", "submitter": "Mufti Mahmud", "authors": "Mufti Mahmud, M. Shamim Kaiser, M. Mostafizur Rahman, M. Arifur\n  Rahman, Antesar Shabut, Shamim Al-Mamun and Amir Hussain", "title": "A Brain-Inspired Trust Management Model to Assure Security in a Cloud\n  based IoT Framework for Neuroscience Applications", "comments": "17 pages, 10 figures, 2 tables", "journal-ref": "Cognitive Computation, 2018", "doi": "10.1007/s12559-018-9543-3", "report-no": null, "categories": "cs.CR cs.AI q-bio.NC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Rapid popularity of Internet of Things (IoT) and cloud computing permits\nneuroscientists to collect multilevel and multichannel brain data to better\nunderstand brain functions, diagnose diseases, and devise treatments. To ensure\nsecure and reliable data communication between end-to-end (E2E) devices\nsupported by current IoT and cloud infrastructure, trust management is needed\nat the IoT and user ends. This paper introduces a Neuro-Fuzzy based\nBrain-inspired trust management model (TMM) to secure IoT devices and relay\nnodes, and to ensure data reliability. The proposed TMM utilizes node\nbehavioral trust and data trust estimated using Adaptive Neuro-Fuzzy Inference\nSystem and weighted-additive methods respectively to assess the nodes\ntrustworthiness. In contrast to the existing fuzzy based TMMs, the NS2\nsimulation results confirm the robustness and accuracy of the proposed TMM in\nidentifying malicious nodes in the communication network. With the growing\nusage of cloud based IoT frameworks in Neuroscience research, integrating the\nproposed TMM into the existing infrastructure will assure secure and reliable\ndata communication among the E2E devices.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 20:45:01 GMT"}], "update_date": "2018-01-15", "authors_parsed": [["Mahmud", "Mufti", ""], ["Kaiser", "M. Shamim", ""], ["Rahman", "M. Mostafizur", ""], ["Rahman", "M. Arifur", ""], ["Shabut", "Antesar", ""], ["Al-Mamun", "Shamim", ""], ["Hussain", "Amir", ""]]}, {"id": "1801.04016", "submitter": "Judea Pearl", "authors": "Judea Pearl", "title": "Theoretical Impediments to Machine Learning With Seven Sparks from the\n  Causal Revolution", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": "R-475", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current machine learning systems operate, almost exclusively, in a\nstatistical, or model-free mode, which entails severe theoretical limits on\ntheir power and performance. Such systems cannot reason about interventions and\nretrospection and, therefore, cannot serve as the basis for strong AI. To\nachieve human level intelligence, learning machines need the guidance of a\nmodel of reality, similar to the ones used in causal inference tasks. To\ndemonstrate the essential role of such models, I will present a summary of\nseven tasks which are beyond reach of current machine learning systems and\nwhich have been accomplished using the tools of causal modeling.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jan 2018 23:37:48 GMT"}], "update_date": "2018-01-15", "authors_parsed": [["Pearl", "Judea", ""]]}, {"id": "1801.04099", "submitter": "Min Chen", "authors": "Min Chen, Stefanos Nikolaidis, Harold Soh, David Hsu, Siddhartha\n  Srinivasa", "title": "Trust-Aware Decision Making for Human-Robot Collaboration: Model\n  Learning and Planning", "comments": "Chen and Nikolaidis contributed equally to the work. Appeared In\n  Proceedings of 2018 ACM/IEEE International Conference on Human-Robot\n  Interaction, Chicago, IL, USA, (HRI 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trust in autonomy is essential for effective human-robot collaboration and\nuser adoption of autonomous systems such as robot assistants. This paper\nintroduces a computational model which integrates trust into robot\ndecision-making. Specifically, we learn from data a partially observable Markov\ndecision process (POMDP) with human trust as a latent variable. The trust-POMDP\nmodel provides a principled approach for the robot to (i) infer the trust of a\nhuman teammate through interaction, (ii) reason about the effect of its own\nactions on human trust, and (iii) choose actions that maximize team performance\nover the long term. We validated the model through human subject experiments on\na table-clearing task in simulation (201 participants) and with a real robot\n(20 participants). In our studies, the robot builds human trust by manipulating\nlow-risk objects first. Interestingly, the robot sometimes fails intentionally\nin order to modulate human trust and achieve the best team performance. These\nresults show that the trust-POMDP calibrates trust to improve human-robot team\nperformance over the long term. Further, they highlight that maximizing trust\nalone does not always lead to the best performance.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 09:28:28 GMT"}, {"version": "v2", "created": "Wed, 15 Aug 2018 03:21:49 GMT"}, {"version": "v3", "created": "Thu, 22 Nov 2018 05:14:00 GMT"}], "update_date": "2018-11-26", "authors_parsed": [["Chen", "Min", ""], ["Nikolaidis", "Stefanos", ""], ["Soh", "Harold", ""], ["Hsu", "David", ""], ["Srinivasa", "Siddhartha", ""]]}, {"id": "1801.04134", "submitter": "Jonas Rothfuss", "authors": "Jonas Rothfuss, Fabio Ferreira, Eren Erdal Aksoy, You Zhou, and Tamim\n  Asfour", "title": "Deep Episodic Memory: Encoding, Recalling, and Predicting Episodic\n  Experiences for Robot Action Execution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a novel deep neural network architecture for representing robot\nexperiences in an episodic-like memory which facilitates encoding, recalling,\nand predicting action experiences. Our proposed unsupervised deep episodic\nmemory model 1) encodes observed actions in a latent vector space and, based on\nthis latent encoding, 2) infers most similar episodes previously experienced,\n3) reconstructs original episodes, and 4) predicts future frames in an\nend-to-end fashion. Results show that conceptually similar actions are mapped\ninto the same region of the latent vector space. Based on these results, we\nintroduce an action matching and retrieval mechanism, benchmark its performance\non two large-scale action datasets, 20BN-something-something and ActivityNet\nand evaluate its generalization capability in a real-world scenario on a\nhumanoid robot.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 11:22:55 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 16:26:38 GMT"}, {"version": "v3", "created": "Sat, 14 Jul 2018 21:20:44 GMT"}], "update_date": "2018-07-17", "authors_parsed": [["Rothfuss", "Jonas", ""], ["Ferreira", "Fabio", ""], ["Aksoy", "Eren Erdal", ""], ["Zhou", "You", ""], ["Asfour", "Tamim", ""]]}, {"id": "1801.04170", "submitter": "Andrey Chistyakov", "authors": "Andrey Chistyakov", "title": "Multilayered Model of Speech", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human speech is the most important part of General Artificial Intelligence\nand subject of much research. The hypothesis proposed in this article provides\nexplanation of difficulties that modern science tackles in the field of human\nbrain simulation. The hypothesis is based on the author's conviction that the\nbrain of any given person has different ability to process and store\ninformation. Therefore, the approaches that are currently used to create\nGeneral Artificial Intelligence have to be altered.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 21:11:54 GMT"}, {"version": "v2", "created": "Mon, 10 Feb 2020 21:09:21 GMT"}], "update_date": "2020-02-12", "authors_parsed": [["Chistyakov", "Andrey", ""]]}, {"id": "1801.04271", "submitter": "Saifuddin Hitawala", "authors": "Saifuddin Hitawala", "title": "Comparative Study on Generative Adversarial Networks", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there have been tremendous advancements in the field of\nmachine learning. These advancements have been made through both academic as\nwell as industrial research. Lately, a fair amount of research has been\ndedicated to the usage of generative models in the field of computer vision and\nimage classification. These generative models have been popularized through a\nnew framework called Generative Adversarial Networks. Moreover, many modified\nversions of this framework have been proposed in the last two years. We study\nthe original model proposed by Goodfellow et al. as well as modifications over\nthe original model and provide a comparative analysis of these models.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 01:37:16 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Hitawala", "Saifuddin", ""]]}, {"id": "1801.04342", "submitter": "Forough Arabshahi", "authors": "Forough Arabshahi, Sameer Singh, Animashree Anandkumar", "title": "Combining Symbolic Expressions and Black-box Function Evaluations in\n  Neural Programs", "comments": "Published as a conference paper at the sixth International Conference\n  on Learning Representations (ICLR), 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural programming involves training neural networks to learn programs,\nmathematics, or logic from data. Previous works have failed to achieve good\ngeneralization performance, especially on problems and programs with high\ncomplexity or on large domains. This is because they mostly rely either on\nblack-box function evaluations that do not capture the structure of the\nprogram, or on detailed execution traces that are expensive to obtain, and\nhence the training data has poor coverage of the domain under consideration. We\npresent a novel framework that utilizes black-box function evaluations, in\nconjunction with symbolic expressions that define relationships between the\ngiven functions. We employ tree LSTMs to incorporate the structure of the\nsymbolic expression trees. We use tree encoding for numbers present in function\nevaluation data, based on their decimal representation. We present an\nevaluation benchmark for this task to demonstrate our proposed model combines\nsymbolic reasoning and function evaluation in a fruitful manner, obtaining high\naccuracies in our experiments. Our framework generalizes significantly better\nto expressions of higher depth and is able to fill partial equations with valid\ncompletions.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 22:24:42 GMT"}, {"version": "v2", "created": "Tue, 16 Jan 2018 22:45:02 GMT"}, {"version": "v3", "created": "Thu, 26 Apr 2018 23:36:01 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Arabshahi", "Forough", ""], ["Singh", "Sameer", ""], ["Anandkumar", "Animashree", ""]]}, {"id": "1801.04345", "submitter": "Nathalia Moraes Do Nascimento", "authors": "Nathalia Moraes do Nascimento and Carlos Jose Pereira de Lucena", "title": "Engineering Cooperative Smart Things based on Embodied Cognition", "comments": "IEEE 2017 NASA/ESA Conference on Adaptive Hardware and Systems (AHS)", "journal-ref": null, "doi": "10.1109/AHS.2017.8046366", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of the Internet of Things (IoT) is to transform any thing around us,\nsuch as a trash can or a street light, into a smart thing. A smart thing has\nthe ability of sensing, processing, communicating and/or actuating. In order to\nachieve the goal of a smart IoT application, such as minimizing waste\ntransportation costs or reducing energy consumption, the smart things in the\napplication scenario must cooperate with each other without a centralized\ncontrol. Inspired by known approaches to design swarm of cooperative and\nautonomous robots, we modeled our smart things based on the embodied cognition\nconcept. Each smart thing is a physical agent with a body composed of a\nmicrocontroller, sensors and actuators, and a brain that is represented by an\nartificial neural network. This type of agent is commonly called an embodied\nagent. The behavior of these embodied agents is autonomously configured through\nan evolutionary algorithm that is triggered according to the application\nperformance. To illustrate, we have designed three homogeneous prototypes for\nsmart street lights based on an evolved network. This application has shown\nthat the proposed approach results in a feasible way of modeling decentralized\nsmart things with self-developed and cooperative capabilities.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 22:36:34 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Nascimento", "Nathalia Moraes do", ""], ["de Lucena", "Carlos Jose Pereira", ""]]}, {"id": "1801.04346", "submitter": "Richard Kim", "authors": "Richard Kim, Max Kleiman-Weiner, Andres Abeliuk, Edmond Awad, Sohan\n  Dsouza, Josh Tenenbaum, Iyad Rahwan", "title": "A Computational Model of Commonsense Moral Decision Making", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new computational model of moral decision making, drawing on a\nrecent theory of commonsense moral learning via social dynamics. Our model\ndescribes moral dilemmas as a utility function that computes trade-offs in\nvalues over abstract moral dimensions, which provide interpretable parameter\nvalues when implemented in machine-led ethical decision-making. Moreover,\ncharacterizing the social structures of individuals and groups as a\nhierarchical Bayesian model, we show that a useful description of an\nindividual's moral values - as well as a group's shared values - can be\ninferred from a limited amount of observed data. Finally, we apply and evaluate\nour approach to data from the Moral Machine, a web application that collects\nhuman judgments on moral dilemmas involving autonomous vehicles.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 22:47:22 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Kim", "Richard", ""], ["Kleiman-Weiner", "Max", ""], ["Abeliuk", "Andres", ""], ["Awad", "Edmond", ""], ["Dsouza", "Sohan", ""], ["Tenenbaum", "Josh", ""], ["Rahwan", "Iyad", ""]]}, {"id": "1801.04378", "submitter": "AmirEmad Ghassami", "authors": "AmirEmad Ghassami, Sajad Khodadadian, Negar Kiyavash", "title": "Fairness in Supervised Learning: An Information Theoretic Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated decision making systems are increasingly being used in real-world\napplications. In these systems for the most part, the decision rules are\nderived by minimizing the training error on the available historical data.\nTherefore, if there is a bias related to a sensitive attribute such as gender,\nrace, religion, etc. in the data, say, due to cultural/historical\ndiscriminatory practices against a certain demographic, the system could\ncontinue discrimination in decisions by including the said bias in its decision\nrule. We present an information theoretic framework for designing fair\npredictors from data, which aim to prevent discrimination against a specified\nsensitive attribute in a supervised learning setting. We use equalized odds as\nthe criterion for discrimination, which demands that the prediction should be\nindependent of the protected attribute conditioned on the actual label. To\nensure fairness and generalization simultaneously, we compress the data to an\nauxiliary variable, which is used for the prediction task. This auxiliary\nvariable is chosen such that it is decontaminated from the discriminatory\nattribute in the sense of equalized odds. The final predictor is obtained by\napplying a Bayesian decision rule to the auxiliary variable.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jan 2018 04:03:04 GMT"}, {"version": "v2", "created": "Sun, 29 Jul 2018 21:49:01 GMT"}], "update_date": "2018-07-31", "authors_parsed": [["Ghassami", "AmirEmad", ""], ["Khodadadian", "Sajad", ""], ["Kiyavash", "Negar", ""]]}, {"id": "1801.04406", "submitter": "Lars Mescheder", "authors": "Lars Mescheder, Andreas Geiger, Sebastian Nowozin", "title": "Which Training Methods for GANs do actually Converge?", "comments": "conference", "journal-ref": "International Conference on Machine Learning 2018", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown local convergence of GAN training for absolutely\ncontinuous data and generator distributions. In this paper, we show that the\nrequirement of absolute continuity is necessary: we describe a simple yet\nprototypical counterexample showing that in the more realistic case of\ndistributions that are not absolutely continuous, unregularized GAN training is\nnot always convergent. Furthermore, we discuss regularization strategies that\nwere recently proposed to stabilize GAN training. Our analysis shows that GAN\ntraining with instance noise or zero-centered gradient penalties converges. On\nthe other hand, we show that Wasserstein-GANs and WGAN-GP with a finite number\nof discriminator updates per generator update do not always converge to the\nequilibrium point. We discuss these results, leading us to a new explanation\nfor the stability problems of GAN training. Based on our analysis, we extend\nour convergence results to more general GANs and prove local convergence for\nsimplified gradient penalties even if the generator and data distribution lie\non lower dimensional manifolds. We find these penalties to work well in\npractice and use them to learn high-resolution generative image models for a\nvariety of datasets with little hyperparameter tuning.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jan 2018 09:42:26 GMT"}, {"version": "v2", "created": "Mon, 19 Feb 2018 10:40:54 GMT"}, {"version": "v3", "created": "Mon, 11 Jun 2018 15:39:06 GMT"}, {"version": "v4", "created": "Tue, 31 Jul 2018 16:28:15 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Mescheder", "Lars", ""], ["Geiger", "Andreas", ""], ["Nowozin", "Sebastian", ""]]}, {"id": "1801.04486", "submitter": "Aaron Hertzmann", "authors": "Aaron Hertzmann", "title": "Can Computers Create Art?", "comments": "to appear in Arts, special issue on Machine as Artist (21st Century)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This essay discusses whether computers, using Artificial Intelligence (AI),\ncould create art. First, the history of technologies that automated aspects of\nart is surveyed, including photography and animation. In each case, there were\ninitial fears and denial of the technology, followed by a blossoming of new\ncreative and professional opportunities for artists. The current hype and\nreality of Artificial Intelligence (AI) tools for art making is then discussed,\ntogether with predictions about how AI tools will be used. It is then\nspeculated about whether it could ever happen that AI systems could be credited\nwith authorship of artwork. It is theorized that art is something created by\nsocial agents, and so computers cannot be credited with authorship of art in\nour current understanding. A few ways that this could change are also\nhypothesized.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jan 2018 21:04:13 GMT"}, {"version": "v2", "created": "Mon, 22 Jan 2018 03:37:22 GMT"}, {"version": "v3", "created": "Thu, 25 Jan 2018 06:25:17 GMT"}, {"version": "v4", "created": "Thu, 8 Feb 2018 19:23:07 GMT"}, {"version": "v5", "created": "Mon, 19 Mar 2018 06:32:02 GMT"}, {"version": "v6", "created": "Tue, 8 May 2018 03:45:56 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Hertzmann", "Aaron", ""]]}, {"id": "1801.04487", "submitter": "Benjamin Doerr", "authors": "Benjamin Doerr", "title": "Better Runtime Guarantees Via Stochastic Domination", "comments": "Significantly extended version of a paper that appeared in the\n  proceedings of EvoCOP 2018", "journal-ref": "Theoretical Computer Science, 773:115-137, 2019", "doi": "10.1016/j.tcs.2018.09.024", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Apart from few exceptions, the mathematical runtime analysis of evolutionary\nalgorithms is mostly concerned with expected runtimes. In this work, we argue\nthat stochastic domination is a notion that should be used more frequently in\nthis area. Stochastic domination allows to formulate much more informative\nperformance guarantees, it allows to decouple the algorithm analysis into the\ntrue algorithmic part of detecting a domination statement and the\nprobability-theoretical part of deriving the desired probabilistic guarantees\nfrom this statement, and it helps finding simpler and more natural proofs. As\nparticular results, we prove a fitness level theorem which shows that the\nruntime is dominated by a sum of independent geometric random variables, we\nprove the first tail bounds for several classic runtime problems, and we give a\nshort and natural proof for Witt's result that the runtime of any $(\\mu,p)$\nmutation-based algorithm on any function with unique optimum is subdominated by\nthe runtime of a variant of the \\oea on the \\onemax function. As side-products,\nwe determine the fastest unbiased (1+1) algorithm for the \\leadingones\nbenchmark problem, both in the general case and when restricted to static\nmutation operators, and we prove a Chernoff-type tail bound for sums of\nindependent coupon collector distributions.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jan 2018 21:30:09 GMT"}, {"version": "v2", "created": "Fri, 8 Jun 2018 13:06:08 GMT"}, {"version": "v3", "created": "Fri, 15 Jun 2018 09:10:45 GMT"}, {"version": "v4", "created": "Mon, 18 Jun 2018 16:21:46 GMT"}, {"version": "v5", "created": "Thu, 23 Aug 2018 06:37:03 GMT"}], "update_date": "2019-05-02", "authors_parsed": [["Doerr", "Benjamin", ""]]}, {"id": "1801.04520", "submitter": "Dipan Pal", "authors": "Dipan K. Pal, Marios Savvides", "title": "Non-Parametric Transformation Networks", "comments": "Preprint only", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  ConvNets, through their architecture, only enforce invariance to translation.\nIn this paper, we introduce a new class of deep convolutional architectures\ncalled Non-Parametric Transformation Networks (NPTNs) which can learn\n\\textit{general} invariances and symmetries directly from data. NPTNs are a\nnatural generalization of ConvNets and can be optimized directly using gradient\ndescent. Unlike almost all previous works in deep architectures, they make no\nassumption regarding the structure of the invariances present in the data and\nin that aspect are flexible and powerful. We also model ConvNets and NPTNs\nunder a unified framework called Transformation Networks (TN), which yields a\nbetter understanding of the connection between the two. We demonstrate the\nefficacy of NPTNs on data such as MNIST with extreme transformations and\nCIFAR10 where they outperform baselines, and further outperform several recent\nalgorithms on ETH-80. They do so while having the same number of parameters. We\nalso show that they are more effective than ConvNets in modelling symmetries\nand invariances from data, without the explicit knowledge of the added\narbitrary nuisance transformations. Finally, we replace ConvNets with NPTNs\nwithin Capsule Networks and show that this enables Capsule Nets to perform even\nbetter.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jan 2018 06:48:45 GMT"}, {"version": "v2", "created": "Fri, 19 Jan 2018 05:10:50 GMT"}, {"version": "v3", "created": "Wed, 14 Feb 2018 20:34:23 GMT"}, {"version": "v4", "created": "Sat, 12 May 2018 15:57:39 GMT"}, {"version": "v5", "created": "Sat, 19 May 2018 13:32:14 GMT"}, {"version": "v6", "created": "Sat, 8 Sep 2018 22:45:23 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Pal", "Dipan K.", ""], ["Savvides", "Marios", ""]]}, {"id": "1801.04541", "submitter": "Colin De Vrieze", "authors": "Colin de Vrieze, Shane Barratt, Daniel Tsai and Anant Sahai", "title": "Cooperative Multi-Agent Reinforcement Learning for Low-Level Wireless\n  Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Traditional radio systems are strictly co-designed on the lower levels of the\nOSI stack for compatibility and efficiency. Although this has enabled the\nsuccess of radio communications, it has also introduced lengthy standardization\nprocesses and imposed static allocation of the radio spectrum. Various\ninitiatives have been undertaken by the research community to tackle the\nproblem of artificial spectrum scarcity by both making frequency allocation\nmore dynamic and building flexible radios to replace the static ones. There is\nreason to believe that just as computer vision and control have been overhauled\nby the introduction of machine learning, wireless communication can also be\nimproved by utilizing similar techniques to increase the flexibility of\nwireless networks. In this work, we pose the problem of discovering low-level\nwireless communication schemes ex-nihilo between two agents in a fully\ndecentralized fashion as a reinforcement learning problem. Our proposed\napproach uses policy gradients to learn an optimal bi-directional communication\nscheme and shows surprisingly sophisticated and intelligent learning behavior.\nWe present the results of extensive experiments and an analysis of the fidelity\nof our approach.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jan 2018 12:05:12 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["de Vrieze", "Colin", ""], ["Barratt", "Shane", ""], ["Tsai", "Daniel", ""], ["Sahai", "Anant", ""]]}, {"id": "1801.04589", "submitter": "Konstantin B\\\"ottinger", "authors": "Konstantin B\\\"ottinger, Patrice Godefroid, Rishabh Singh", "title": "Deep Reinforcement Fuzzing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fuzzing is the process of finding security vulnerabilities in\ninput-processing code by repeatedly testing the code with modified inputs. In\nthis paper, we formalize fuzzing as a reinforcement learning problem using the\nconcept of Markov decision processes. This in turn allows us to apply\nstate-of-the-art deep Q-learning algorithms that optimize rewards, which we\ndefine from runtime properties of the program under test. By observing the\nrewards caused by mutating with a specific set of actions performed on an\ninitial program input, the fuzzing agent learns a policy that can next generate\nnew higher-reward inputs. We have implemented this new approach, and\npreliminary empirical evidence shows that reinforcement fuzzing can outperform\nbaseline random fuzzing.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jan 2018 17:46:17 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["B\u00f6ttinger", "Konstantin", ""], ["Godefroid", "Patrice", ""], ["Singh", "Rishabh", ""]]}, {"id": "1801.04590", "submitter": "Mehdi S. M. Sajjadi", "authors": "Mehdi S. M. Sajjadi and Raviteja Vemulapalli and Matthew Brown", "title": "Frame-Recurrent Video Super-Resolution", "comments": "Accepted at CVPR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in video super-resolution have shown that convolutional\nneural networks combined with motion compensation are able to merge information\nfrom multiple low-resolution (LR) frames to generate high-quality images.\nCurrent state-of-the-art methods process a batch of LR frames to generate a\nsingle high-resolution (HR) frame and run this scheme in a sliding window\nfashion over the entire video, effectively treating the problem as a large\nnumber of separate multi-frame super-resolution tasks. This approach has two\nmain weaknesses: 1) Each input frame is processed and warped multiple times,\nincreasing the computational cost, and 2) each output frame is estimated\nindependently conditioned on the input frames, limiting the system's ability to\nproduce temporally consistent results.\n  In this work, we propose an end-to-end trainable frame-recurrent video\nsuper-resolution framework that uses the previously inferred HR estimate to\nsuper-resolve the subsequent frame. This naturally encourages temporally\nconsistent results and reduces the computational cost by warping only one image\nin each step. Furthermore, due to its recurrent nature, the proposed method has\nthe ability to assimilate a large number of previous frames without increased\ncomputational demands. Extensive evaluations and comparisons with previous\nmethods validate the strengths of our approach and demonstrate that the\nproposed framework is able to significantly outperform the current state of the\nart.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jan 2018 17:53:53 GMT"}, {"version": "v2", "created": "Tue, 16 Jan 2018 12:28:58 GMT"}, {"version": "v3", "created": "Mon, 19 Mar 2018 00:38:35 GMT"}, {"version": "v4", "created": "Sun, 25 Mar 2018 17:24:01 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Sajjadi", "Mehdi S. M.", ""], ["Vemulapalli", "Raviteja", ""], ["Brown", "Matthew", ""]]}, {"id": "1801.04622", "submitter": "Vatsal Mahajan", "authors": "Vatsal Mahajan", "title": "Top k Memory Candidates in Memory Networks for Common Sense Reasoning", "comments": "3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Successful completion of reasoning task requires the agent to have relevant\nprior knowledge or some given context of the world dynamics. Usually, the\ninformation provided to the system for a reasoning task is just the query or\nsome supporting story, which is often not enough for common reasoning tasks.\nThe goal here is that, if the information provided along the question is not\nsufficient to correctly answer the question, the model should choose k most\nrelevant documents that can aid its inference process. In this work, the model\ndynamically selects top k most relevant memory candidates that can be used to\nsuccessfully solve reasoning tasks. Experiments were conducted on a subset of\nWinograd Schema Challenge (WSC) problems to show that the proposed model has\nthe potential for commonsense reasoning. The WSC is a test of machine\nintelligence, designed to be an improvement on the Turing test.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jan 2018 23:43:57 GMT"}, {"version": "v2", "created": "Fri, 15 Nov 2019 09:47:31 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Mahajan", "Vatsal", ""]]}, {"id": "1801.04701", "submitter": "Ao Zhang", "authors": "Ao Zhang, Nan Li, Jian Pu, Jun Wang, Junchi Yan, Hongyuan Zha", "title": "tau-FPL: Tolerance-Constrained Learning in Linear Time", "comments": "32 pages, 3 figures. This is an extended version of our paper\n  published in AAAI-18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a classifier with control on the false-positive rate plays a\ncritical role in many machine learning applications. Existing approaches either\nintroduce prior knowledge dependent label cost or tune parameters based on\ntraditional classifiers, which lack consistency in methodology because they do\nnot strictly adhere to the false-positive rate constraint. In this paper, we\npropose a novel scoring-thresholding approach, tau-False Positive Learning\n(tau-FPL) to address this problem. We show the scoring problem which takes the\nfalse-positive rate tolerance into accounts can be efficiently solved in linear\ntime, also an out-of-bootstrap thresholding method can transform the learned\nranking function into a low false-positive classifier. Both theoretical\nanalysis and experimental results show superior performance of the proposed\ntau-FPL over existing approaches.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jan 2018 08:56:49 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Zhang", "Ao", ""], ["Li", "Nan", ""], ["Pu", "Jian", ""], ["Wang", "Jun", ""], ["Yan", "Junchi", ""], ["Zha", "Hongyuan", ""]]}, {"id": "1801.04819", "submitter": "Matej Hoffmann", "authors": "Matej Hoffmann and Rolf Pfeifer", "title": "Robots as Powerful Allies for the Study of Embodied Cognition from the\n  Bottom Up", "comments": "22 pages, 3 figures", "journal-ref": "in A. Newen, L. de Bruin; & S. Gallagher, ed., 'The Oxford\n  Handbook 4e Cognition', Oxford University Press, pp. 841-862 (2018)", "doi": "10.1093/oxfordhb/9780198735410.013.45", "report-no": null, "categories": "cs.AI cs.RO q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large body of compelling evidence has been accumulated demonstrating that\nembodiment - the agent's physical setup, including its shape, materials,\nsensors and actuators - is constitutive for any form of cognition and as a\nconsequence, models of cognition need to be embodied. In contrast to methods\nfrom empirical sciences to study cognition, robots can be freely manipulated\nand virtually all key variables of their embodiment and control programs can be\nsystematically varied. As such, they provide an extremely powerful tool of\ninvestigation. We present a robotic bottom-up or developmental approach,\nfocusing on three stages: (a) low-level behaviors like walking and reflexes,\n(b) learning regularities in sensorimotor spaces, and (c) human-like cognition.\nWe also show that robotic based research is not only a productive path to\ndeepening our understanding of cognition, but that robots can strongly benefit\nfrom human-like cognition in order to become more autonomous, robust,\nresilient, and safe.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jan 2018 14:29:14 GMT"}, {"version": "v2", "created": "Wed, 1 Apr 2020 18:45:52 GMT"}], "update_date": "2020-04-03", "authors_parsed": [["Hoffmann", "Matej", ""], ["Pfeifer", "Rolf", ""]]}, {"id": "1801.04871", "submitter": "Pararth Shah", "authors": "Pararth Shah, Dilek Hakkani-T\\\"ur, Gokhan T\\\"ur, Abhinav Rastogi,\n  Ankur Bapna, Neha Nayak, Larry Heck", "title": "Building a Conversational Agent Overnight with Dialogue Self-Play", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Machines Talking To Machines (M2M), a framework combining\nautomation and crowdsourcing to rapidly bootstrap end-to-end dialogue agents\nfor goal-oriented dialogues in arbitrary domains. M2M scales to new tasks with\njust a task schema and an API client from the dialogue system developer, but it\nis also customizable to cater to task-specific interactions. Compared to the\nWizard-of-Oz approach for data collection, M2M achieves greater diversity and\ncoverage of salient dialogue flows while maintaining the naturalness of\nindividual utterances. In the first phase, a simulated user bot and a\ndomain-agnostic system bot converse to exhaustively generate dialogue\n\"outlines\", i.e. sequences of template utterances and their semantic parses. In\nthe second phase, crowd workers provide contextual rewrites of the dialogues to\nmake the utterances more natural while preserving their meaning. The entire\nprocess can finish within a few hours. We propose a new corpus of 3,000\ndialogues spanning 2 domains collected with M2M, and present comparisons with\npopular dialogue datasets on the quality and diversity of the surface forms and\ndialogue flows.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jan 2018 16:45:56 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Shah", "Pararth", ""], ["Hakkani-T\u00fcr", "Dilek", ""], ["T\u00fcr", "Gokhan", ""], ["Rastogi", "Abhinav", ""], ["Bapna", "Ankur", ""], ["Nayak", "Neha", ""], ["Heck", "Larry", ""]]}, {"id": "1801.05032", "submitter": "Feng-Lin Li", "authors": "Feng-Lin Li, Minghui Qiu, Haiqing Chen, Xiongwei Wang, Xing Gao, Jun\n  Huang, Juwei Ren, Zhongzhou Zhao, Weipeng Zhao, Lei Wang, Guwei Jin, Wei Chu", "title": "AliMe Assist: An Intelligent Assistant for Creating an Innovative\n  E-commerce Experience", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present AliMe Assist, an intelligent assistant designed for creating an\ninnovative online shopping experience in E-commerce. Based on question\nanswering (QA), AliMe Assist offers assistance service, customer service, and\nchatting service. It is able to take voice and text input, incorporate context\nto QA, and support multi-round interaction. Currently, it serves millions of\ncustomer questions per day and is able to address 85% of them. In this paper,\nwe demonstrate the system, present the underlying techniques, and share our\nexperience in dealing with real-world QA in the E-commerce field.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jan 2018 12:11:30 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Li", "Feng-Lin", ""], ["Qiu", "Minghui", ""], ["Chen", "Haiqing", ""], ["Wang", "Xiongwei", ""], ["Gao", "Xing", ""], ["Huang", "Jun", ""], ["Ren", "Juwei", ""], ["Zhao", "Zhongzhou", ""], ["Zhao", "Weipeng", ""], ["Wang", "Lei", ""], ["Jin", "Guwei", ""], ["Chu", "Wei", ""]]}, {"id": "1801.05075", "submitter": "Sina Mohseni", "authors": "Sina Mohseni and Jeremy E. Block and Eric D. Ragan", "title": "A Human-Grounded Evaluation Benchmark for Local Explanations of Machine\n  Learning", "comments": "Benchmark Available online at\n  https://github.com/SinaMohseni/ML-Interpretability-Evaluation-Benchmark", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in interpretable machine learning proposes different computational\nand human subject approaches to evaluate model saliency explanations. These\napproaches measure different qualities of explanations to achieve diverse goals\nin designing interpretable machine learning systems. In this paper, we propose\na human attention benchmark for image and text domains using multi-layer human\nattention masks aggregated from multiple human annotators. We then present an\nevaluation study to evaluate model saliency explanations obtained using\nGrad-cam and LIME techniques. We demonstrate our benchmark's utility for\nquantitative evaluation of model explanations by comparing it with human\nsubjective ratings and ground-truth single-layer segmentation masks\nevaluations. Our study results show that our threshold agnostic evaluation\nmethod with the human attention baseline is more effective than single-layer\nobject segmentation masks to ground truth. Our experiments also reveal user\nbiases in the subjective rating of model saliency explanations.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 00:14:43 GMT"}, {"version": "v2", "created": "Sun, 28 Jun 2020 21:05:07 GMT"}], "update_date": "2020-06-30", "authors_parsed": [["Mohseni", "Sina", ""], ["Block", "Jeremy E.", ""], ["Ragan", "Eric D.", ""]]}, {"id": "1801.05088", "submitter": "Chandra Khatri", "authors": "Chandra Khatri", "title": "Real-time Road Traffic Information Detection Through Social Media", "comments": "138 Pages, 21 Figures, 15 Tables. Masters Thesis in Computational\n  Science & Engineering Group @ Georgia Tech.\n  https://smartech.gatech.edu/bitstream/handle/1853/53889/KHATRI-THESIS-2015.pdf.\n  arXiv admin note: text overlap with arXiv:1703.03921 by other author", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.IR cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In current study, a mechanism to extract traffic related information such as\ncongestion and incidents from textual data from the internet is proposed. The\ncurrent source of data is Twitter. As the data being considered is extremely\nlarge in size automated models are developed to stream, download, and mine the\ndata in real-time. Furthermore, if any tweet has traffic related information\nthen the models should be able to infer and extract this data.\n  Currently, the data is collected only for United States and a total of\n120,000 geo-tagged traffic related tweets are extracted, while six million\ngeo-tagged non-traffic related tweets are retrieved and classification models\nare trained. Furthermore, this data is used for various kinds of spatial and\ntemporal analysis. A mechanism to calculate level of traffic congestion,\nsafety, and traffic perception for cities in U.S. is proposed. Traffic\ncongestion and safety rankings for the various urban areas are obtained and\nthen they are statistically validated with existing widely adopted rankings.\nTraffic perception depicts the attitude and perception of people towards the\ntraffic.\n  It is also seen that traffic related data when visualized spatially and\ntemporally provides the same pattern as the actual traffic flows for various\nurban areas. When visualized at the city level, it is clearly visible that the\nflow of tweets is similar to flow of vehicles and that the traffic related\ntweets are representative of traffic within the cities. With all the findings\nin current study, it is shown that significant amount of traffic related\ninformation can be extracted from Twitter and other sources on internet.\nFurthermore, Twitter and these data sources are freely available and are not\nbound by spatial and temporal limitations. That is, wherever there is a user\nthere is a potential for data.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 01:26:33 GMT"}], "update_date": "2018-01-20", "authors_parsed": [["Khatri", "Chandra", ""]]}, {"id": "1801.05151", "submitter": "Kai Qiao", "authors": "Chi Zhang, Kai Qiao, Linyuan Wang, Li Tong, Ying Zeng, Bin Yan", "title": "Constraint-free Natural Image Reconstruction from fMRI Signals Based on\n  Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, research on decoding brain activity based on functional\nmagnetic resonance imaging (fMRI) has made remarkable achievements. However,\nconstraint-free natural image reconstruction from brain activity is still a\nchallenge. The existing methods simplified the problem by using semantic prior\ninformation or just reconstructing simple images such as letters and digitals.\nWithout semantic prior information, we present a novel method to reconstruct\nnature images from fMRI signals of human visual cortex based on the computation\nmodel of convolutional neural network (CNN). Firstly, we extracted the units\noutput of viewed natural images in each layer of a pre-trained CNN as CNN\nfeatures. Secondly, we transformed image reconstruction from fMRI signals into\nthe problem of CNN feature visualizations by training a sparse linear\nregression to map from the fMRI patterns to CNN features. By iteratively\noptimization to find the matched image, whose CNN unit features become most\nsimilar to those predicted from the brain activity, we finally achieved the\npromising results for the challenging constraint-free natural image\nreconstruction. As there was no use of semantic prior information of the\nstimuli when training decoding model, any category of images (not constraint by\nthe training set) could be reconstructed theoretically. We found that the\nreconstructed images resembled the natural stimuli, especially in position and\nshape. The experimental results suggest that hierarchical visual features can\neffectively express the visual perception process of human brain.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 08:34:18 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Zhang", "Chi", ""], ["Qiao", "Kai", ""], ["Wang", "Linyuan", ""], ["Tong", "Li", ""], ["Zeng", "Ying", ""], ["Yan", "Bin", ""]]}, {"id": "1801.05156", "submitter": "Shumeet Baluja", "authors": "Shumeet Baluja", "title": "Empirical Explorations in Training Networks with Discrete Activations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present extensive experiments training and testing hidden units in deep\nnetworks that emit only a predefined, static, number of discretized values.\nThese units provide benefits in real-world deployment in systems in which\nmemory and/or computation may be limited. Additionally, they are particularly\nwell suited for use in large recurrent network models that require the\nmaintenance of large amounts of internal state in memory. Surprisingly, we find\nthat despite reducing the number of values that can be represented in the\noutput activations from $2^{32}-2^{64}$ to between 64 and 256, there is little\nto no degradation in network performance across a variety of different\nsettings. We investigate simple classification and regression tasks, as well as\nmemorization and compression problems. We compare the results with more\nstandard activations, such as tanh and relu. Unlike previous discretization\nstudies which often concentrate only on binary units, we examine the effects of\nvarying the number of allowed activation levels. Compared to existing\napproaches for discretization, the approach presented here is both conceptually\nand programatically simple, has no stochastic component, and allows the\ntraining, testing, and usage phases to be treated in exactly the same manner.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 08:47:18 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Baluja", "Shumeet", ""]]}, {"id": "1801.05159", "submitter": "Kamil Bennani-Smires", "authors": "Kamil Bennani-Smires, Claudiu Musat, Andreea Hossmann, Michael\n  Baeriswyl", "title": "GitGraph - Architecture Search Space Creation through Frequent\n  Computational Subgraph Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dramatic success of deep neural networks across multiple application\nareas often relies on experts painstakingly designing a network architecture\nspecific to each task. To simplify this process and make it more accessible, an\nemerging research effort seeks to automate the design of neural network\narchitectures, using e.g. evolutionary algorithms or reinforcement learning or\nsimple search in a constrained space of neural modules.\n  Considering the typical size of the search space (e.g. $10^{10}$ candidates\nfor a $10$-layer network) and the cost of evaluating a single candidate,\ncurrent architecture search methods are very restricted. They either rely on\nstatic pre-built modules to be recombined for the task at hand, or they define\na static hand-crafted framework within which they can generate new\narchitectures from the simplest possible operations.\n  In this paper, we relax these restrictions, by capitalizing on the collective\nwisdom contained in the plethora of neural networks published in online code\nrepositories. Concretely, we (a) extract and publish GitGraph, a corpus of\nneural architectures and their descriptions; (b) we create problem-specific\nneural architecture search spaces, implemented as a textual search mechanism\nover GitGraph; (c) we propose a method of identifying unique common subgraphs\nwithin the architectures solving each problem (e.g., image processing,\nreinforcement learning), that can then serve as modules in the newly created\nproblem specific neural search space.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 08:54:20 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Bennani-Smires", "Kamil", ""], ["Musat", "Claudiu", ""], ["Hossmann", "Andreea", ""], ["Baeriswyl", "Michael", ""]]}, {"id": "1801.05295", "submitter": "Mehdi Elahi", "authors": "Paolo Cremonesi and Chiara Francalanci and Alessandro Poli and Roberto\n  Pagano and Luca Mazzoni and Alberto Maggioni and Mehdi Elahi", "title": "Social Network based Short-Term Stock Trading System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI q-fin.ST", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel adaptive algorithm for the automated short-term\ntrading of financial instrument. The algorithm adopts a semantic sentiment\nanalysis technique to inspect the Twitter posts and to use them to predict the\nbehaviour of the stock market. Indeed, the algorithm is specifically developed\nto take advantage of both the sentiment and the past values of a certain\nfinancial instrument in order to choose the best investment decision. This\nallows the algorithm to ensure the maximization of the obtainable profits by\ntrading on the stock market. We have conducted an investment simulation and\ncompared the performance of our proposed with a well-known benchmark (DJTATO\nindex) and the optimal results, in which an investor knows in advance the\nfuture price of a product. The result shows that our approach outperforms the\nbenchmark and achieves the performance score close to the optimal result.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 15:26:46 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Cremonesi", "Paolo", ""], ["Francalanci", "Chiara", ""], ["Poli", "Alessandro", ""], ["Pagano", "Roberto", ""], ["Mazzoni", "Luca", ""], ["Maggioni", "Alberto", ""], ["Elahi", "Mehdi", ""]]}, {"id": "1801.05372", "submitter": "Thanh Lam Hoang", "authors": "Hoang Thanh Lam, Tran Ngoc Minh, Mathieu Sinn, Beat Buesser and Martin\n  Wistuba", "title": "Neural Feature Learning From Relational Database", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature engineering is one of the most important but most tedious tasks in\ndata science. This work studies automation of feature learning from relational\ndatabase. We first prove theoretically that finding the optimal features from\nrelational data for predictive tasks is NP-hard. We propose an efficient\nrule-based approach based on heuristics and a deep neural network to\nautomatically learn appropriate features from relational data. We benchmark our\napproaches in ensembles in past Kaggle competitions. Our new approach wins late\nmedals and beats the state-of-the-art solutions with significant margins. To\nthe best of our knowledge, this is the first time an automated data science\nsystem could win medals in Kaggle competitions with complex relational\ndatabase.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 17:18:29 GMT"}, {"version": "v2", "created": "Fri, 18 May 2018 14:41:52 GMT"}, {"version": "v3", "created": "Sun, 17 Jun 2018 07:28:31 GMT"}, {"version": "v4", "created": "Sat, 15 Jun 2019 05:25:25 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["Lam", "Hoang Thanh", ""], ["Minh", "Tran Ngoc", ""], ["Sinn", "Mathieu", ""], ["Buesser", "Beat", ""], ["Wistuba", "Martin", ""]]}, {"id": "1801.05394", "submitter": "Wei-Han Lee", "authors": "Wei-Han Lee, Jorge Ortiz, Bongjun Ko, Ruby Lee", "title": "Time Series Segmentation through Automatic Feature Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet of things (IoT) applications have become increasingly popular in\nrecent years, with applications ranging from building energy monitoring to\npersonal health tracking and activity recognition. In order to leverage these\ndata, automatic knowledge extraction - whereby we map from observations to\ninterpretable states and transitions - must be done at scale. As such, we have\nseen many recent IoT data sets include annotations with a human expert\nspecifying states, recorded as a set of boundaries and associated labels in a\ndata sequence. These data can be used to build automatic labeling algorithms\nthat produce labels as an expert would. Here, we refer to human-specified\nboundaries as breakpoints. Traditional changepoint detection methods only look\nfor statistically-detectable boundaries that are defined as abrupt variations\nin the generative parameters of a data sequence. However, we observe that\nbreakpoints occur on more subtle boundaries that are non-trivial to detect with\nthese statistical methods. In this work, we propose a new unsupervised\napproach, based on deep learning, that outperforms existing techniques and\nlearns the more subtle, breakpoint boundaries with a high accuracy. Through\nextensive experiments on various real-world data sets - including\nhuman-activity sensing data, speech signals, and electroencephalogram (EEG)\nactivity traces - we demonstrate the effectiveness of our algorithm for\npractical applications. Furthermore, we show that our approach achieves\nsignificantly better performance than previous methods.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 18:05:08 GMT"}, {"version": "v2", "created": "Fri, 26 Jan 2018 11:51:31 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Lee", "Wei-Han", ""], ["Ortiz", "Jorge", ""], ["Ko", "Bongjun", ""], ["Lee", "Ruby", ""]]}, {"id": "1801.05457", "submitter": "J. G. Wolff", "authors": "J Gerard Wolff", "title": "Solutions to problems with deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the several successes of deep learning systems, there are concerns\nabout their limitations, discussed most recently by Gary Marcus. This paper\ndiscusses Marcus's concerns and some others, together with solutions to several\nof these problems provided by the \"P theory of intelligence\" and its\nrealisation in the \"SP computer model\". The main advantages of the SP system\nare: relatively small requirements for data and the ability to learn from a\nsingle experience; the ability to model both hierarchical and non-hierarchical\nstructures; strengths in several kinds of reasoning, including `commonsense'\nreasoning; transparency in the representation of knowledge, and the provision\nof an audit trail for all processing; the likelihood that the SP system could\nnot be fooled into bizarre or eccentric recognition of stimuli, as deep\nlearning systems can be; the SP system provides a robust solution to the\nproblem of `catastrophic forgetting' in deep learning systems; the SP system\nprovides a theoretically-coherent solution to the problems of correcting over-\nand under-generalisations in learning, and learning correct structures despite\nerrors in data; unlike most research on deep learning, the SP programme of\nresearch draws extensively on research on human learning, perception, and\ncognition; and the SP programme of research has an overarching theory,\nsupported by evidence, something that is largely missing from research on deep\nlearning. In general, the SP system provides a much firmer foundation than deep\nlearning for the development of artificial general intelligence.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jan 2018 20:37:07 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Wolff", "J Gerard", ""]]}, {"id": "1801.05462", "submitter": "Arend Hintze", "authors": "Jory Schossau, Larissa Albantakis, Arend Hintze", "title": "The Role of Conditional Independence in the Evolution of Intelligent\n  Systems", "comments": "Original Abstract submitted to the GECCO conference 2017 Berlin", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Systems are typically made from simple components regardless of their\ncomplexity. While the function of each part is easily understood, higher order\nfunctions are emergent properties and are notoriously difficult to explain. In\nnetworked systems, both digital and biological, each component receives inputs,\nperforms a simple computation, and creates an output. When these components\nhave multiple outputs, we intuitively assume that the outputs are causally\ndependent on the inputs but are themselves independent of each other given the\nstate of their shared input. However, this intuition can be violated for\ncomponents with probabilistic logic, as these typically cannot be decomposed\ninto separate logic gates with one output each. This violation of conditional\nindependence on the past system state is equivalent to instantaneous\ninteraction --- the idea is that some information between the outputs is not\ncoming from the inputs and thus must have been created instantaneously. Here we\ncompare evolved artificial neural systems with and without instantaneous\ninteraction across several task environments. We show that systems without\ninstantaneous interactions evolve faster, to higher final levels of\nperformance, and require fewer logic components to create a densely connected\ncognitive machinery.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 19:43:13 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Schossau", "Jory", ""], ["Albantakis", "Larissa", ""], ["Hintze", "Arend", ""]]}, {"id": "1801.05500", "submitter": "Ursula Challita", "authors": "Ursula Challita and Walid Saad and Christian Bettstetter", "title": "Cellular-Connected UAVs over 5G: Deep Reinforcement Learning for\n  Interference Management", "comments": null, "journal-ref": "IEEE Transactions on Wireless Communications, 2019", "doi": "10.1109/TWC.2019.2900035", "report-no": null, "categories": "cs.IT cs.AI cs.GT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, an interference-aware path planning scheme for a network of\ncellular-connected unmanned aerial vehicles (UAVs) is proposed. In particular,\neach UAV aims at achieving a tradeoff between maximizing energy efficiency and\nminimizing both wireless latency and the interference level caused on the\nground network along its path. The problem is cast as a dynamic game among\nUAVs. To solve this game, a deep reinforcement learning algorithm, based on\necho state network (ESN) cells, is proposed. The introduced deep ESN\narchitecture is trained to allow each UAV to map each observation of the\nnetwork state to an action, with the goal of minimizing a sequence of\ntime-dependent utility functions. Each UAV uses ESN to learn its optimal path,\ntransmission power level, and cell association vector at different locations\nalong its path. The proposed algorithm is shown to reach a subgame perfect Nash\nequilibrium (SPNE) upon convergence. Moreover, an upper and lower bound for the\naltitude of the UAVs is derived thus reducing the computational complexity of\nthe proposed algorithm. Simulation results show that the proposed scheme\nachieves better wireless latency per UAV and rate per ground user (UE) while\nrequiring a number of steps that is comparable to a heuristic baseline that\nconsiders moving via the shortest distance towards the corresponding\ndestinations. The results also show that the optimal altitude of the UAVs\nvaries based on the ground network density and the UE data rate requirements\nand plays a vital role in minimizing the interference level on the ground UEs\nas well as the wireless transmission delay of the UAV.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 22:35:55 GMT"}], "update_date": "2019-05-03", "authors_parsed": [["Challita", "Ursula", ""], ["Saad", "Walid", ""], ["Bettstetter", "Christian", ""]]}, {"id": "1801.05532", "submitter": "Sungwoon Choi", "authors": "Sungwoon Choi, Heonseok Ha, Uiwon Hwang, Chanju Kim, Jung-Woo Ha,\n  Sungroh Yoon", "title": "Reinforcement Learning based Recommender System using Biclustering\n  Technique", "comments": "4 pages, 2 figures, IFUP2018(WSDM 2018 workshop)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recommender system aims to recommend items that a user is interested in\namong many items. The need for the recommender system has been expanded by the\ninformation explosion. Various approaches have been suggested for providing\nmeaningful recommendations to users. One of the proposed approaches is to\nconsider a recommender system as a Markov decision process (MDP) problem and\ntry to solve it using reinforcement learning (RL). However, existing RL-based\nmethods have an obvious drawback. To solve an MDP in a recommender system, they\nencountered a problem with the large number of discrete actions that bring RL\nto a larger class of problems. In this paper, we propose a novel RL-based\nrecommender system. We formulate a recommender system as a gridworld game by\nusing a biclustering technique that can reduce the state and action space\nsignificantly. Using biclustering not only reduces space but also improves the\nrecommendation quality effectively handling the cold-start problem. In\naddition, our approach can provide users with some explanation why the system\nrecommends certain items. Lastly, we examine the proposed algorithm on a\nreal-world dataset and achieve a better performance than the widely used\nrecommendation algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 03:03:58 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Choi", "Sungwoon", ""], ["Ha", "Heonseok", ""], ["Hwang", "Uiwon", ""], ["Kim", "Chanju", ""], ["Ha", "Jung-Woo", ""], ["Yoon", "Sungroh", ""]]}, {"id": "1801.05566", "submitter": "Jiaming Song", "authors": "Jiaming Song and Yuhuai Wu", "title": "An Empirical Analysis of Proximal Policy Optimization with\n  Kronecker-factored Natural Gradients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this technical report, we consider an approach that combines the PPO\nobjective and K-FAC natural gradient optimization, for which we call PPOKFAC.\nWe perform a range of empirical analysis on various aspects of the algorithm,\nsuch as sample complexity, training speed, and sensitivity to batch size and\ntraining epochs. We observe that PPOKFAC is able to outperform PPO in terms of\nsample complexity and speed in a range of MuJoCo environments, while being\nscalable in terms of batch size. In spite of this, it seems that adding more\nepochs is not necessarily helpful for sample efficiency, and PPOKFAC seems to\nbe worse than its A2C counterpart, ACKTR.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 06:09:09 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Song", "Jiaming", ""], ["Wu", "Yuhuai", ""]]}, {"id": "1801.05609", "submitter": "Lei Shu", "authors": "Lei Shu, Hu Xu, Bing Liu", "title": "Unseen Class Discovery in Open-world Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns open-world classification, where the classifier not only\nneeds to classify test examples into seen classes that have appeared in\ntraining but also reject examples from unseen or novel classes that have not\nappeared in training. Specifically, this paper focuses on discovering the\nhidden unseen classes of the rejected examples. Clearly, without prior\nknowledge this is difficult. However, we do have the data from the seen\ntraining classes, which can tell us what kind of similarity/difference is\nexpected for examples from the same class or from different classes. It is\nreasonable to assume that this knowledge can be transferred to the rejected\nexamples and used to discover the hidden unseen classes in them. This paper\naims to solve this problem. It first proposes a joint open classification model\nwith a sub-model for classifying whether a pair of examples belongs to the same\nor different classes. This sub-model can serve as a distance function for\nclustering to discover the hidden classes of the rejected examples.\nExperimental results show that the proposed model is highly promising.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 09:50:41 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Shu", "Lei", ""], ["Xu", "Hu", ""], ["Liu", "Bing", ""]]}, {"id": "1801.05643", "submitter": "Felix Martin Schuhknecht", "authors": "Ankur Sharma, Felix Martin Schuhknecht, Jens Dittrich", "title": "The Case for Automatic Database Administration using Deep Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Like any large software system, a full-fledged DBMS offers an overwhelming\namount of configuration knobs. These range from static initialisation\nparameters like buffer sizes, degree of concurrency, or level of replication to\ncomplex runtime decisions like creating a secondary index on a particular\ncolumn or reorganising the physical layout of the store. To simplify the\nconfiguration, industry grade DBMSs are usually shipped with various advisory\ntools, that provide recommendations for given workloads and machines. However,\nreality shows that the actual configuration, tuning, and maintenance is usually\nstill done by a human administrator, relying on intuition and experience.\nRecent work on deep reinforcement learning has shown very promising results in\nsolving problems, that require such a sense of intuition. For instance, it has\nbeen applied very successfully in learning how to play complicated games with\nenormous search spaces. Motivated by these achievements, in this work we\nexplore how deep reinforcement learning can be used to administer a DBMS.\nFirst, we will describe how deep reinforcement learning can be used to\nautomatically tune an arbitrary software system like a DBMS by defining a\nproblem environment. Second, we showcase our concept of NoDBA at the concrete\nexample of index selection and evaluate how well it recommends indexes for\ngiven workloads.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 12:51:01 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Sharma", "Ankur", ""], ["Schuhknecht", "Felix Martin", ""], ["Dittrich", "Jens", ""]]}, {"id": "1801.05644", "submitter": "Olivier Cailloux", "authors": "Olivier Cailloux and Yves Meinard", "title": "A formal framework for deliberated judgment", "comments": "This is the postprint version of the article published in Theory and\n  Decision. The text is identical, except for minor wording modifications", "journal-ref": null, "doi": "10.1007/s11238-019-09722-7", "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While the philosophical literature has extensively studied how decisions\nrelate to arguments, reasons and justifications, decision theory almost\nentirely ignores the latter notions and rather focuses on preference and\nbelief. In this article, we argue that decision theory can largely benefit from\nexplicitly taking into account the stance that decision-makers take towards\narguments and counter-arguments. To that end, we elaborate a formal framework\naiming to integrate the role of arguments and argumentation in decision theory\nand decision aid. We start from a decision situation, where an individual\nrequests decision support. In this context, we formally define, as a\ncommendable basis for decision-aid, this individual's deliberated judgment,\npopularized by Rawls. We explain how models of deliberated judgment can be\nvalidated empirically. We then identify conditions upon which the existence of\na valid model can be taken for granted, and analyze how these conditions can be\nrelaxed. We then explore the significance of our proposed framework for\ndecision aiding practice. We argue that our concept of deliberated judgment\nowes its normative credentials both to its normative foundations (the idea of\nrationality based on arguments) and to its reference to empirical reality (the\nstance that real, empirical individuals hold towards arguments and\ncounter-arguments, on due reflection). We then highlight that our framework\nopens promising avenues for future research involving both philosophical and\ndecision theoretic approaches, as well as empirical implementations.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 12:53:13 GMT"}, {"version": "v2", "created": "Thu, 7 Nov 2019 14:39:06 GMT"}], "update_date": "2019-11-11", "authors_parsed": [["Cailloux", "Olivier", ""], ["Meinard", "Yves", ""]]}, {"id": "1801.05667", "submitter": "Gary Marcus", "authors": "Gary Marcus", "title": "Innateness, AlphaZero, and Artificial Intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of innateness is rarely discussed in the context of artificial\nintelligence. When it is discussed, or hinted at, it is often the context of\ntrying to reduce the amount of innate machinery in a given system. In this\npaper, I consider as a test case a recent series of papers by Silver et al\n(Silver et al., 2017a) on AlphaGo and its successors that have been presented\nas an argument that a \"even in the most challenging of domains: it is possible\nto train to superhuman level, without human examples or guidance\", \"starting\ntabula rasa.\"\n  I argue that these claims are overstated, for multiple reasons. I close by\narguing that artificial intelligence needs greater attention to innateness, and\nI point to some proposals about what that innateness might look like.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 14:05:21 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Marcus", "Gary", ""]]}, {"id": "1801.05707", "submitter": "Fuyuan Xiao", "authors": "Fuyuan Xiao", "title": "Quantum dynamical mode (QDM): A possible extension of belief function", "comments": "37 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dempster-Shafer evidence theory has been widely used in various fields of\napplications, because of the flexibility and effectiveness in modeling\nuncertainties without prior information. Besides, it has been proven that the\nquantum theory has powerful capabilities of solving the decision making\nproblems, especially for modelling human decision and cognition. However, the\nclassical Dempster-Shafer evidence theory modelled by real numbers cannot be\nintegrated directly with the quantum theory modelled by complex numbers. So,\nhow can we establish a bridge of communications between the classical\nDempster-Shafer evidence theory and the quantum theory? To answer this\nquestion, a generalized Dempster-Shafer evidence theory is proposed in this\npaper. The main contribution in this study is that, unlike the existing\nevidence theory, a mass function in the generalized Dempster-Shafer evidence\ntheory is modelled by a complex number, called as a complex mass function. In\naddition, compared with the classical Dempster's combination rule, the\ncondition in terms of the conflict coefficient between two evidences K < 1 is\nreleased in the generalized Dempster's combination rule so that it is more\ngeneral and applicable than the classical Dempster's combination rule. When the\ncomplex mass function is degenerated from complex numbers to real numbers, the\ngeneralized Dempster's combination rule degenerates to the classical evidence\ntheory under the condition that the conflict coefficient between the evidences\nK is less than 1. Numerical examples are illustrated to show the efficiency of\nthe generalized Dempster-Shafer evidence theory. Finally, an application of an\nevidential quantum dynamical model is implemented by integrating the\ngeneralized Dempster-Shafer evidence theory with the quantum dynamical model.\nFrom the experimental results, it validates the feasibility and effectiveness\nof the proposed method.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jan 2018 06:22:37 GMT"}, {"version": "v2", "created": "Thu, 18 Jan 2018 13:55:10 GMT"}, {"version": "v3", "created": "Sun, 29 Apr 2018 04:24:25 GMT"}], "update_date": "2018-05-01", "authors_parsed": [["Xiao", "Fuyuan", ""]]}, {"id": "1801.05757", "submitter": "Zhiyuan Xu", "authors": "Zhiyuan Xu, Jian Tang, Jingsong Meng, Weiyi Zhang, Yanzhi Wang, Chi\n  Harold Liu and Dejun Yang", "title": "Experience-driven Networking: A Deep Reinforcement Learning based\n  Approach", "comments": "9 pages, 12 figures, paper is accepted as a conference paper at IEEE\n  Infocom 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern communication networks have become very complicated and highly\ndynamic, which makes them hard to model, predict and control. In this paper, we\ndevelop a novel experience-driven approach that can learn to well control a\ncommunication network from its own experience rather than an accurate\nmathematical model, just as a human learns a new skill (such as driving,\nswimming, etc). Specifically, we, for the first time, propose to leverage\nemerging Deep Reinforcement Learning (DRL) for enabling model-free control in\ncommunication networks; and present a novel and highly effective DRL-based\ncontrol framework, DRL-TE, for a fundamental networking problem: Traffic\nEngineering (TE). The proposed framework maximizes a widely-used utility\nfunction by jointly learning network environment and its dynamics, and making\ndecisions under the guidance of powerful Deep Neural Networks (DNNs). We\npropose two new techniques, TE-aware exploration and actor-critic-based\nprioritized experience replay, to optimize the general DRL framework\nparticularly for TE. To validate and evaluate the proposed framework, we\nimplemented it in ns-3, and tested it comprehensively with both representative\nand randomly generated network topologies. Extensive packet-level simulation\nresults show that 1) compared to several widely-used baseline methods, DRL-TE\nsignificantly reduces end-to-end delay and consistently improves the network\nutility, while offering better or comparable throughput; 2) DRL-TE is robust to\nnetwork changes; and 3) DRL-TE consistently outperforms a state-ofthe-art DRL\nmethod (for continuous control), Deep Deterministic Policy Gradient (DDPG),\nwhich, however, does not offer satisfying performance.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jan 2018 17:09:01 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Xu", "Zhiyuan", ""], ["Tang", "Jian", ""], ["Meng", "Jingsong", ""], ["Zhang", "Weiyi", ""], ["Wang", "Yanzhi", ""], ["Liu", "Chi Harold", ""], ["Yang", "Dejun", ""]]}, {"id": "1801.05853", "submitter": "Bo Wu", "authors": "Bo Wu, Wen-Huang Cheng, Yongdong Zhang, Tao Mei", "title": "Time Matters: Multi-scale Temporalization of Social Media Popularity", "comments": "accepted in ACM Multimedia 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The evolution of social media popularity exhibits rich temporality, i.e.,\npopularities change over time at various levels of temporal granularity. This\nis influenced by temporal variations of public attentions or user activities.\nFor example, popularity patterns of street snap on Flickr are observed to\ndepict distinctive fashion styles at specific time scales, such as season-based\nperiodic fluctuations for Trench Coat or one-off peak in days for Evening\nDress. However, this fact is often overlooked by existing research of\npopularity modeling. We present the first study to incorporate multiple\ntime-scale dynamics into predicting online popularity. We propose a novel\ncomputational framework in the paper, named Multi-scale Temporalization, for\nestimating popularity based on multi-scale decomposition and structural\nreconstruction in a tensor space of user, post, and time by joint low-rank\nconstraints. By considering the noise caused by context inconsistency, we\ndesign a data rearrangement step based on context aggregation as preprocessing\nto enhance contextual relevance of neighboring data in the tensor space. As a\nresult, our approach can leverage multiple levels of temporal characteristics\nand reduce the noise of data decomposition to improve modeling effectiveness.\nWe evaluate our approach on two large-scale Flickr image datasets with over 1.8\nmillion photos in total, for the task of popularity prediction. The results\nshow that our approach significantly outperforms state-of-the-art popularity\nprediction techniques, with a relative improvement of 10.9%-47.5% in terms of\nprediction accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 12 Dec 2017 05:51:47 GMT"}], "update_date": "2018-01-19", "authors_parsed": [["Wu", "Bo", ""], ["Cheng", "Wen-Huang", ""], ["Zhang", "Yongdong", ""], ["Mei", "Tao", ""]]}, {"id": "1801.05931", "submitter": "Vinod Kumar Chauhan", "authors": "Vinod Kumar Chauhan, Anuj Sharma and Kalpana Dahiya", "title": "Faster Learning by Reduction of Data Access Time", "comments": "80 figures, final journal version", "journal-ref": "Applied Intelligence, Springer, 2018", "doi": "10.1007/s10489-018-1235-x", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, the major challenge in machine learning is the Big Data challenge.\nThe big data problems due to large number of data points or large number of\nfeatures in each data point, or both, the training of models have become very\nslow. The training time has two major components: Time to access the data and\ntime to process (learn from) the data. So far, the research has focused only on\nthe second part, i.e., learning from the data. In this paper, we have proposed\none possible solution to handle the big data problems in machine learning. The\nidea is to reduce the training time through reducing data access time by\nproposing systematic sampling and cyclic/sequential sampling to select\nmini-batches from the dataset. To prove the effectiveness of proposed sampling\ntechniques, we have used Empirical Risk Minimization, which is commonly used\nmachine learning problem, for strongly convex and smooth case. The problem has\nbeen solved using SAG, SAGA, SVRG, SAAG-II and MBSGD (Mini-batched SGD), each\nusing two step determination techniques, namely, constant step size and\nbacktracking line search method. Theoretical results prove the same convergence\nfor systematic sampling, cyclic sampling and the widely used random sampling\ntechnique, in expectation. Experimental results with bench marked datasets\nprove the efficacy of the proposed sampling techniques and show up to six times\nfaster training.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jan 2018 04:31:40 GMT"}, {"version": "v2", "created": "Mon, 5 Feb 2018 09:18:20 GMT"}, {"version": "v3", "created": "Fri, 22 Jun 2018 07:38:28 GMT"}, {"version": "v4", "created": "Wed, 25 Jul 2018 04:27:05 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Chauhan", "Vinod Kumar", ""], ["Sharma", "Anuj", ""], ["Dahiya", "Kalpana", ""]]}, {"id": "1801.05950", "submitter": "Lindsey Kuper", "authors": "Lindsey Kuper, Guy Katz, Justin Gottschlich, Kyle Julian, Clark\n  Barrett, Mykel Kochenderfer", "title": "Toward Scalable Verification for Safety-Critical Deep Networks", "comments": "Accepted for presentation at SysML 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing use of deep neural networks for safety-critical applications,\nsuch as autonomous driving and flight control, raises concerns about their\nsafety and reliability. Formal verification can address these concerns by\nguaranteeing that a deep learning system operates as intended, but the state of\nthe art is limited to small systems. In this work-in-progress report we give an\noverview of our work on mitigating this difficulty, by pursuing two\ncomplementary directions: devising scalable verification techniques, and\nidentifying design choices that result in deep learning systems that are more\namenable to verification.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jan 2018 06:27:57 GMT"}, {"version": "v2", "created": "Fri, 2 Feb 2018 21:25:11 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Kuper", "Lindsey", ""], ["Katz", "Guy", ""], ["Gottschlich", "Justin", ""], ["Julian", "Kyle", ""], ["Barrett", "Clark", ""], ["Kochenderfer", "Mykel", ""]]}, {"id": "1801.06007", "submitter": "Pieter Gijsbers", "authors": "Pieter Gijsbers, Joaquin Vanschoren, Randal S. Olson", "title": "Layered TPOT: Speeding up Tree-based Pipeline Optimization", "comments": "Update to include a reference to Zutty et al. after it was brought to\n  our attention", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the demand for machine learning increasing, so does the demand for tools\nwhich make it easier to use. Automated machine learning (AutoML) tools have\nbeen developed to address this need, such as the Tree-Based Pipeline\nOptimization Tool (TPOT) which uses genetic programming to build optimal\npipelines. We introduce Layered TPOT, a modification to TPOT which aims to\ncreate pipelines equally good as the original, but in significantly less time.\nThis approach evaluates candidate pipelines on increasingly large subsets of\nthe data according to their fitness, using a modified evolutionary algorithm to\nallow for separate competition between pipelines trained on different sample\nsizes. Empirical evaluation shows that, on sufficiently large datasets, Layered\nTPOT indeed finds better models faster.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jan 2018 13:36:51 GMT"}, {"version": "v2", "created": "Mon, 12 Mar 2018 14:45:22 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Gijsbers", "Pieter", ""], ["Vanschoren", "Joaquin", ""], ["Olson", "Randal S.", ""]]}, {"id": "1801.06024", "submitter": "Gino Brunner", "authors": "Gino Brunner, Yuyi Wang, Roger Wattenhofer, Michael Weigelt", "title": "Natural Language Multitasking: Analyzing and Improving Syntactic\n  Saliency of Hidden Representations", "comments": "The 31st Annual Conference on Neural Information Processing (NIPS) -\n  Workshop on Learning Disentangled Features: from Perception to Control, Long\n  Beach, CA, December 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We train multi-task autoencoders on linguistic tasks and analyze the learned\nhidden sentence representations. The representations change significantly when\ntranslation and part-of-speech decoders are added. The more decoders a model\nemploys, the better it clusters sentences according to their syntactic\nsimilarity, as the representation space becomes less entangled. We explore the\nstructure of the representation space by interpolating between sentences, which\nyields interesting pseudo-English sentences, many of which have recognizable\nsyntactic structure. Lastly, we point out an interesting property of our\nmodels: The difference-vector between two sentences can be added to change a\nthird sentence with similar features in a meaningful way.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jan 2018 14:10:37 GMT"}], "update_date": "2018-01-19", "authors_parsed": [["Brunner", "Gino", ""], ["Wang", "Yuyi", ""], ["Wattenhofer", "Roger", ""], ["Weigelt", "Michael", ""]]}, {"id": "1801.06043", "submitter": "Sam Ganzfried", "authors": "Sam Ganzfried, Farzana Yusuf", "title": "Optimal Weighting for Exam Composition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A problem faced by many instructors is that of designing exams that\naccurately assess the abilities of the students. Typically these exams are\nprepared several days in advance, and generic question scores are used based on\nrough approximation of the question difficulty and length. For example, for a\nrecent class taught by the author, there were 30 multiple choice questions\nworth 3 points, 15 true/false with explanation questions worth 4 points, and 5\nanalytical exercises worth 10 points. We describe a novel framework where\nalgorithms from machine learning are used to modify the exam question weights\nin order to optimize the exam scores, using the overall class grade as a proxy\nfor a student's true ability. We show that significant error reduction can be\nobtained by our approach over standard weighting schemes, and we make several\nnew observations regarding the properties of the \"good\" and \"bad\" exam\nquestions that can have impact on the design of improved future evaluation\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 24 Dec 2017 05:35:47 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Ganzfried", "Sam", ""], ["Yusuf", "Farzana", ""]]}, {"id": "1801.06176", "submitter": "Xiujun Li", "authors": "Baolin Peng and Xiujun Li and Jianfeng Gao and Jingjing Liu and\n  Kam-Fai Wong and Shang-Yu Su", "title": "Deep Dyna-Q: Integrating Planning for Task-Completion Dialogue Policy\n  Learning", "comments": "11 pages, 8 figures, Accepted in ACL 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training a task-completion dialogue agent via reinforcement learning (RL) is\ncostly because it requires many interactions with real users. One common\nalternative is to use a user simulator. However, a user simulator usually lacks\nthe language complexity of human interlocutors and the biases in its design may\ntend to degrade the agent. To address these issues, we present Deep Dyna-Q,\nwhich to our knowledge is the first deep RL framework that integrates planning\nfor task-completion dialogue policy learning. We incorporate into the dialogue\nagent a model of the environment, referred to as the world model, to mimic real\nuser response and generate simulated experience. During dialogue policy\nlearning, the world model is constantly updated with real user experience to\napproach real user behavior, and in turn, the dialogue agent is optimized using\nboth real experience and simulated experience. The effectiveness of our\napproach is demonstrated on a movie-ticket booking task in both simulated and\nhuman-in-the-loop settings.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jan 2018 18:57:33 GMT"}, {"version": "v2", "created": "Sun, 13 May 2018 22:31:38 GMT"}, {"version": "v3", "created": "Wed, 23 May 2018 17:52:27 GMT"}], "update_date": "2018-05-24", "authors_parsed": [["Peng", "Baolin", ""], ["Li", "Xiujun", ""], ["Gao", "Jianfeng", ""], ["Liu", "Jingjing", ""], ["Wong", "Kam-Fai", ""], ["Su", "Shang-Yu", ""]]}, {"id": "1801.06294", "submitter": "Shaika Chowdhury", "authors": "Shaika Chowdhury, Chenwei Zhang and Philip S. Yu", "title": "Multi-Task Pharmacovigilance Mining from Social Media Posts", "comments": "Accepted in the research track of The Web Conference(WWW) 2018", "journal-ref": null, "doi": "10.1145/3178876.3186053", "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media has grown to be a crucial information source for\npharmacovigilance studies where an increasing number of people post adverse\nreactions to medical drugs that are previously unreported. Aiming to\neffectively monitor various aspects of Adverse Drug Reactions (ADRs) from\ndiversely expressed social medical posts, we propose a multi-task neural\nnetwork framework that learns several tasks associated with ADR monitoring with\ndifferent levels of supervisions collectively. Besides being able to correctly\nclassify ADR posts and accurately extract ADR mentions from online posts, the\nproposed framework is also able to further understand reasons for which the\ndrug is being taken, known as 'indication', from the given social media post. A\ncoverage-based attention mechanism is adopted in our framework to help the\nmodel properly identify 'phrasal' ADRs and Indications that are attentive to\nmultiple words in a post. Our framework is applicable in situations where\nlimited parallel data for different pharmacovigilance tasks are available.We\nevaluate the proposed framework on real-world Twitter datasets, where the\nproposed model outperforms the state-of-the-art alternatives of each individual\ntask consistently.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 05:04:21 GMT"}, {"version": "v2", "created": "Sun, 4 Feb 2018 02:07:45 GMT"}, {"version": "v3", "created": "Tue, 13 Feb 2018 06:07:22 GMT"}, {"version": "v4", "created": "Thu, 15 Feb 2018 03:03:56 GMT"}, {"version": "v5", "created": "Fri, 16 Feb 2018 18:43:05 GMT"}], "update_date": "2018-02-19", "authors_parsed": [["Chowdhury", "Shaika", ""], ["Zhang", "Chenwei", ""], ["Yu", "Philip S.", ""]]}, {"id": "1801.06316", "submitter": "Heliang Huang", "authors": "He-Liang Huang, Xi-Lin Wang, Peter P. Rohde, Yi-Han Luo, You-Wei Zhao,\n  Chang Liu, Li Li, Nai-Le Liu, Chao-Yang Lu, Jian-Wei Pan", "title": "Demonstration of Topological Data Analysis on a Quantum Processor", "comments": "Typos and minor corrections. For the first time, we have\n  experimentally demonstrated that quantum computing can analyze big data using\n  techniques from topology. Any comments are welcome", "journal-ref": "Optica 5(2),193(2018)", "doi": null, "report-no": null, "categories": "quant-ph cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topological data analysis offers a robust way to extract useful information\nfrom noisy, unstructured data by identifying its underlying structure.\nRecently, an efficient quantum algorithm was proposed [Lloyd, Garnerone,\nZanardi, Nat. Commun. 7, 10138 (2016)] for calculating Betti numbers of data\npoints -- topological features that count the number of topological holes of\nvarious dimensions in a scatterplot. Here, we implement a proof-of-principle\ndemonstration of this quantum algorithm by employing a six-photon quantum\nprocessor to successfully analyze the topological features of Betti numbers of\na network including three data points, providing new insights into data\nanalysis in the era of quantum computing.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 06:50:57 GMT"}, {"version": "v2", "created": "Wed, 18 Dec 2019 02:58:40 GMT"}], "update_date": "2019-12-19", "authors_parsed": [["Huang", "He-Liang", ""], ["Wang", "Xi-Lin", ""], ["Rohde", "Peter P.", ""], ["Luo", "Yi-Han", ""], ["Zhao", "You-Wei", ""], ["Liu", "Chang", ""], ["Li", "Li", ""], ["Liu", "Nai-Le", ""], ["Lu", "Chao-Yang", ""], ["Pan", "Jian-Wei", ""]]}, {"id": "1801.06349", "submitter": "Matei Mancas", "authors": "Matei Mancas, Christian Frisson, Jo\\\"elle Tilmanne, Nicolas\n  d'Alessandro, Petr Barborka, Furkan Bayansar, Francisco Bernard, Rebecca\n  Fiebrink, Alexis Heloir, Edgar Hemery, Sohaib Laraba, Alexis Moinet, Fabrizio\n  Nunnari, Thierry Ravet, Lo\\\"ic Reboursi\\`ere, Alvaro Sarasua, Micka\\\"el Tits,\n  No\\'e Tits, Fran\\c{c}ois Zaj\\'ega, Paolo Alborno, Ksenia Kolykhalova, Emma\n  Frid, Damiano Malafronte, Lisanne Huis in't Veld, H\\\"useyin Cakmak, Kevin El\n  Haddad, Nicolas Riche, Julien Leroy, Pierre Marighetto, Bekir Berker\n  T\\\"urker, Hossein Khaki, Roberto Pulisci, Emer Gilmartin, Fasih Haider,\n  K\\\"ubra Cengiz, Martin Sulir, Ilaria Torre, Shabbir Marzban, Ramazan\n  Yaz{\\i}c{\\i}, Furkan Burak B\\^agc{\\i}, Vedat Gazi K{\\i}l{\\i}, Hilal Sezer,\n  Sena B\\\"usra Yenge, Charles-Alexandre Delestage, Sylvie Leleu-Merviel, Muriel\n  Meyer-Chemenska, Daniel Schmitt, Willy Yvart, St\\'ephane Dupont, Ozan Can\n  Altiok, Ayseg\\\"ul Bumin, Ceren Dikmen, Ivan Giangreco, Silvan Heller, Emre\n  K\\\"ulah, Gueorgui Pironkov, Luca Rossetto, Yusuf Sahillioglu, Heiko Schuldt,\n  Omar Seddati, Yusuf Setinkaya, Metin Sezgin, Claudiu Tanase, Emre Toyan, Sean\n  Wood, Doguhan Yeke, Fran\\c{c}cois Rocca, Pierre-Henri De Deken, Alessandra\n  Bandrabur, Fabien Grisard, Axel Jean-Caurant, Vincent Courboulay, Radhwan Ben\n  Madhkour, Ambroise Moreau", "title": "Proceedings of eNTERFACE 2015 Workshop on Intelligent Interfaces", "comments": "159 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 11th Summer Workshop on Multimodal Interfaces eNTERFACE 2015 was hosted\nby the Numediart Institute of Creative Technologies of the University of Mons\nfrom August 10th to September 2015. During the four weeks, students and\nresearchers from all over the world came together in the Numediart Institute of\nthe University of Mons to work on eight selected projects structured around\nintelligent interfaces. Eight projects were selected and their reports are\nshown here.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 10:03:35 GMT"}], "update_date": "2018-01-22", "authors_parsed": [["Mancas", "Matei", ""], ["Frisson", "Christian", ""], ["Tilmanne", "Jo\u00eblle", ""], ["d'Alessandro", "Nicolas", ""], ["Barborka", "Petr", ""], ["Bayansar", "Furkan", ""], ["Bernard", "Francisco", ""], ["Fiebrink", "Rebecca", ""], ["Heloir", "Alexis", ""], ["Hemery", "Edgar", ""], ["Laraba", "Sohaib", ""], ["Moinet", "Alexis", ""], ["Nunnari", "Fabrizio", ""], ["Ravet", "Thierry", ""], ["Reboursi\u00e8re", "Lo\u00efc", ""], ["Sarasua", "Alvaro", ""], ["Tits", "Micka\u00ebl", ""], ["Tits", "No\u00e9", ""], ["Zaj\u00e9ga", "Fran\u00e7ois", ""], ["Alborno", "Paolo", ""], ["Kolykhalova", "Ksenia", ""], ["Frid", "Emma", ""], ["Malafronte", "Damiano", ""], ["Veld", "Lisanne Huis in't", ""], ["Cakmak", "H\u00fcseyin", ""], ["Haddad", "Kevin El", ""], ["Riche", "Nicolas", ""], ["Leroy", "Julien", ""], ["Marighetto", "Pierre", ""], ["T\u00fcrker", "Bekir Berker", ""], ["Khaki", "Hossein", ""], ["Pulisci", "Roberto", ""], ["Gilmartin", "Emer", ""], ["Haider", "Fasih", ""], ["Cengiz", "K\u00fcbra", ""], ["Sulir", "Martin", ""], ["Torre", "Ilaria", ""], ["Marzban", "Shabbir", ""], ["Yaz\u0131c\u0131", "Ramazan", ""], ["B\u00e2gc\u0131", "Furkan Burak", ""], ["K\u0131l\u0131", "Vedat Gazi", ""], ["Sezer", "Hilal", ""], ["Yenge", "Sena B\u00fcsra", ""], ["Delestage", "Charles-Alexandre", ""], ["Leleu-Merviel", "Sylvie", ""], ["Meyer-Chemenska", "Muriel", ""], ["Schmitt", "Daniel", ""], ["Yvart", "Willy", ""], ["Dupont", "St\u00e9phane", ""], ["Altiok", "Ozan Can", ""], ["Bumin", "Ayseg\u00fcl", ""], ["Dikmen", "Ceren", ""], ["Giangreco", "Ivan", ""], ["Heller", "Silvan", ""], ["K\u00fclah", "Emre", ""], ["Pironkov", "Gueorgui", ""], ["Rossetto", "Luca", ""], ["Sahillioglu", "Yusuf", ""], ["Schuldt", "Heiko", ""], ["Seddati", "Omar", ""], ["Setinkaya", "Yusuf", ""], ["Sezgin", "Metin", ""], ["Tanase", "Claudiu", ""], ["Toyan", "Emre", ""], ["Wood", "Sean", ""], ["Yeke", "Doguhan", ""], ["Rocca", "Fran\u00e7cois", ""], ["De Deken", "Pierre-Henri", ""], ["Bandrabur", "Alessandra", ""], ["Grisard", "Fabien", ""], ["Jean-Caurant", "Axel", ""], ["Courboulay", "Vincent", ""], ["Madhkour", "Radhwan Ben", ""], ["Moreau", "Ambroise", ""]]}, {"id": "1801.06481", "submitter": "Chen Liang", "authors": "Chen Liang, Jianbo Ye, Han Zhao, Bart Pursel, C. Lee Giles", "title": "Active Learning of Strict Partial Orders: A Case Study on Concept\n  Prerequisite Relations", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Strict partial order is a mathematical structure commonly seen in relational\ndata. One obstacle to extracting such type of relations at scale is the lack of\nlarge-scale labels for building effective data-driven solutions. We develop an\nactive learning framework for mining such relations subject to a strict order.\nOur approach incorporates relational reasoning not only in finding new\nunlabeled pairs whose labels can be deduced from an existing label set, but\nalso in devising new query strategies that consider the relational structure of\nlabels. Our experiments on concept prerequisite relations show our proposed\nframework can substantially improve the classification performance with the\nsame query budget compared to other baseline approaches.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 16:26:18 GMT"}], "update_date": "2018-01-22", "authors_parsed": [["Liang", "Chen", ""], ["Ye", "Jianbo", ""], ["Zhao", "Han", ""], ["Pursel", "Bart", ""], ["Giles", "C. Lee", ""]]}, {"id": "1801.06597", "submitter": "Yu Shi", "authors": "Yu Shi, Fangqiu Han, Xinwei He, Xinran He, Carl Yang, Jie Luo, Jiawei\n  Han", "title": "mvn2vec: Preservation and Collaboration in Multi-View Network Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-view networks are broadly present in real-world applications. In the\nmeantime, network embedding has emerged as an effective representation learning\napproach for networked data. Therefore, we are motivated to study the problem\nof multi-view network embedding with a focus on the optimization objectives\nthat are specific and important in embedding this type of network. In our\npractice of embedding real-world multi-view networks, we explicitly identify\ntwo such objectives, which we refer to as preservation and collaboration. The\nin-depth analysis of these two objectives is discussed throughout this paper.\nIn addition, the mvn2vec algorithms are proposed to (i) study how varied extent\nof preservation and collaboration can impact embedding learning and (ii)\nexplore the feasibility of achieving better embedding quality by modeling them\nsimultaneously. With experiments on a series of synthetic datasets, a\nlarge-scale internal Snapchat dataset, and two public datasets, we confirm the\nvalidity and importance of preservation and collaboration as two objectives for\nmulti-view network embedding. These experiments further demonstrate that better\nembedding can be obtained by simultaneously modeling the two objectives, while\nnot over-complicating the model or requiring additional supervision. The code\nand the processed datasets are available at\nhttp://yushi2.web.engr.illinois.edu/.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jan 2018 23:14:08 GMT"}, {"version": "v2", "created": "Tue, 30 Oct 2018 20:57:12 GMT"}, {"version": "v3", "created": "Sun, 3 Nov 2019 01:52:09 GMT"}], "update_date": "2019-11-05", "authors_parsed": [["Shi", "Yu", ""], ["Han", "Fangqiu", ""], ["He", "Xinwei", ""], ["He", "Xinran", ""], ["Yang", "Carl", ""], ["Luo", "Jie", ""], ["Han", "Jiawei", ""]]}, {"id": "1801.06620", "submitter": "Zoltan Toroczkai", "authors": "Botond Moln\\'ar, Melinda Varga, Zoltan Toroczkai, M\\'aria\n  Ercsey-Ravasz", "title": "A high-performance analog Max-SAT solver and its application to Ramsey\n  numbers", "comments": "33 pages, 13 figures. Version 2: added references, corrected SI\n  tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cond-mat.stat-mech cs.AI math.DS nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a continuous-time analog solver for MaxSAT, a quintessential\nclass of NP-hard discrete optimization problems, where the task is to find a\ntruth assignment for a set of Boolean variables satisfying the maximum number\nof given logical constraints. We show that the scaling of an invariant of the\nsolver's dynamics, the escape rate, as function of the number of unsatisfied\nclauses can predict the global optimum value, often well before reaching the\ncorresponding state. We demonstrate the performance of the solver on hard\nMaxSAT competition problems. We then consider the two-color Ramsey number\n$R(m,m)$ problem, translate it to SAT, and apply our algorithm to the still\nunknown $R(5,5)$. We find edge colorings without monochromatic 5-cliques for\ncomplete graphs up to 42 vertices, while on 43 vertices we find colorings with\nonly two monochromatic 5-cliques, the best coloring found so far, supporting\nthe conjecture that $R(5,5) = 43$.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jan 2018 03:54:22 GMT"}, {"version": "v2", "created": "Sun, 28 Jan 2018 21:18:21 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Moln\u00e1r", "Botond", ""], ["Varga", "Melinda", ""], ["Toroczkai", "Zoltan", ""], ["Ercsey-Ravasz", "M\u00e1ria", ""]]}, {"id": "1801.06635", "submitter": "Danping Liao", "authors": "Danping Liao, Siyu Chen, Yuntao Qian", "title": "Visualization of Hyperspectral Images Using Moving Least Squares", "comments": "arXiv admin note: text overlap with arXiv:1712.01657", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Displaying the large number of bands in a hyper spectral image on a\ntrichromatic monitor has been an active research topic. The visualized image\nshall convey as much information as possible form the original data and\nfacilitate image interpretation. Most existing methods display HSIs in false\ncolors which contradict with human's experience and expectation. In this paper,\nwe propose a nonlinear approach to visualize an input HSI with natural colors\nby taking advantage of a corresponding RGB image. Our approach is based on\nMoving Least Squares, an interpolation scheme for reconstructing a surface from\na set of control points, which in our case is a set of matching pixels between\nthe HSI and the corresponding RGB image. Based on MLS, the proposed method\nsolves for each spectral signature a unique transformation so that the non\nlinear structure of the HSI can be preserved. The matching pixels between a\npair of HSI and RGB image can be reused to display other HSIs captured b the\nsame imaging sensor with natural colors. Experiments show that the output image\nof the proposed method no only have natural colors but also maintain the visual\ninformation necessary for human analysis.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jan 2018 07:01:27 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Liao", "Danping", ""], ["Chen", "Siyu", ""], ["Qian", "Yuntao", ""]]}, {"id": "1801.06689", "submitter": "Alp M\\\"uyesser", "authors": "Necati Alp Muyesser and Kyle Dunovan and Timothy Verstynen", "title": "Learning model-based strategies in simple environments with hierarchical\n  q-networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Recent advances in deep learning have allowed artificial agents to rival\nhuman-level performance on a wide range of complex tasks; however, the ability\nof these networks to learn generalizable strategies remains a pressing\nchallenge. This critical limitation is due in part to two factors: the opaque\ninformation representation in deep neural networks and the complexity of the\ntask environments in which they are typically deployed. Here we propose a novel\nHierarchical Q-Network (HQN) motivated by theories of the hierarchical\norganization of the human prefrontal cortex, that attempts to identify lower\ndimensional patterns in the value landscape that can be exploited to construct\nan internal model of rules in simple environments. We draw on combinatorial\ngames, where there exists a single optimal strategy for winning that\ngeneralizes across other features of the game, to probe the strategy\ngeneralization of the HQN and other reinforcement learning (RL) agents using\nvariations of Wythoff's game. Traditional RL approaches failed to reach\nsatisfactory performance on variants of Wythoff's Game; however, the HQN\nlearned heuristic-like strategies that generalized across changes in board\nconfiguration. More importantly, the HQN allowed for transparent inspection of\nthe agent's internal model of the game following training. Our results show how\na biologically inspired hierarchical learner can facilitate learning abstract\nrules to promote robust and flexible action policies in simplified training\nenvironments with clearly delineated optimal strategies.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jan 2018 15:31:35 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Muyesser", "Necati Alp", ""], ["Dunovan", "Kyle", ""], ["Verstynen", "Timothy", ""]]}, {"id": "1801.06700", "submitter": "Iulian Vlad Serban", "authors": "Iulian V. Serban, Chinnadhurai Sankar, Mathieu Germain, Saizheng\n  Zhang, Zhouhan Lin, Sandeep Subramanian, Taesup Kim, Michael Pieper, Sarath\n  Chandar, Nan Rosemary Ke, Sai Rajeswar, Alexandre de Brebisson, Jose M. R.\n  Sotelo, Dendi Suhubdy, Vincent Michalski, Alexandre Nguyen, Joelle Pineau,\n  Yoshua Bengio", "title": "A Deep Reinforcement Learning Chatbot (Short Version)", "comments": "9 pages, 1 figure, 2 tables; presented at NIPS 2017, Conversational\n  AI: \"Today's Practice and Tomorrow's Potential\" Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present MILABOT: a deep reinforcement learning chatbot developed by the\nMontreal Institute for Learning Algorithms (MILA) for the Amazon Alexa Prize\ncompetition. MILABOT is capable of conversing with humans on popular small talk\ntopics through both speech and text. The system consists of an ensemble of\nnatural language generation and retrieval models, including neural network and\ntemplate-based models. By applying reinforcement learning to crowdsourced data\nand real-world user interactions, the system has been trained to select an\nappropriate response from the models in its ensemble. The system has been\nevaluated through A/B testing with real-world users, where it performed\nsignificantly better than other systems. The results highlight the potential of\ncoupling ensemble systems with deep reinforcement learning as a fruitful path\nfor developing real-world, open-domain conversational agents.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jan 2018 17:22:06 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Serban", "Iulian V.", ""], ["Sankar", "Chinnadhurai", ""], ["Germain", "Mathieu", ""], ["Zhang", "Saizheng", ""], ["Lin", "Zhouhan", ""], ["Subramanian", "Sandeep", ""], ["Kim", "Taesup", ""], ["Pieper", "Michael", ""], ["Chandar", "Sarath", ""], ["Ke", "Nan Rosemary", ""], ["Rajeswar", "Sai", ""], ["de Brebisson", "Alexandre", ""], ["Sotelo", "Jose M. R.", ""], ["Suhubdy", "Dendi", ""], ["Michalski", "Vincent", ""], ["Nguyen", "Alexandre", ""], ["Pineau", "Joelle", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1801.06740", "submitter": "Babatunde Akinkunmi", "authors": "Babatunde Opeoluwa Akinkunmi and Moyin Florence Babalola", "title": "Knowledge Representation for High-Level Norms and Violation Inference in\n  Logic Programming", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the knowledge Representation formalisms developed for representing\nprescriptive norms can be categorized as either suitable for representing\neither low level or high level norms.We argue that low level norm\nrepresentations do not advance the cause of autonomy in agents in the sense\nthat it is not the agent itself that determines the normative position it\nshould be at a particular time, on the account of a more general rule. In other\nwords an agent on some external system for a nitty gritty prescriptions of its\nobligations and prohibitions. On the other hand, high level norms which have an\nexplicit description of a norm's precondition and have some form of\nimplication, do not as they exist in the literature do not support generalized\ninferences about violation like low level norm representations do. This paper\npresents a logical formalism for the representation of high level norms in open\nsocieties that enable violation inferences that detail the situation in which\nthe norm violation took place and the identity of the norm violation. Norms are\nformalized as logic programs whose heads specify what an agent is obliged or\npermitted to do when a situation arises and within what time constraint of the\nsituation.Each norm is also assigned an identity using some reification scheme.\nThe body of each logic program describes the nature of the situation in which\nthe agent is expected to act or desist from acting. This kind of violation is\nnovel in the literature.\n", "versions": [{"version": "v1", "created": "Sat, 20 Jan 2018 23:31:14 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Akinkunmi", "Babatunde Opeoluwa", ""], ["Babalola", "Moyin Florence", ""]]}, {"id": "1801.06805", "submitter": "Weichang Wu", "authors": "Weichang Wu, Junchi Yan, Xiaokang Yang, Hongyuan Zha", "title": "Decoupled Learning for Factorial Marked Temporal Point Processes", "comments": "9 pages, 8 figures, submitted to TNNLS, 21 Jan, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the factorial marked temporal point process model and\npresents efficient learning methods. In conventional (multi-dimensional) marked\ntemporal point process models, event is often encoded by a single discrete\nvariable i.e. a marker. In this paper, we describe the factorial marked point\nprocesses whereby time-stamped event is factored into multiple markers.\nAccordingly the size of the infectivity matrix modeling the effect between\npairwise markers is in power order w.r.t. the number of the discrete marker\nspace. We propose a decoupled learning method with two learning procedures: i)\ndirectly solving the model based on two techniques: Alternating Direction\nMethod of Multipliers and Fast Iterative Shrinkage-Thresholding Algorithm; ii)\ninvolving a reformulation that transforms the original problem into a Logistic\nRegression model for more efficient learning. Moreover, a sparse group\nregularizer is added to identify the key profile features and event labels.\nEmpirical results on real world datasets demonstrate the efficiency of our\ndecoupled and reformulated method. The source code is available online.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jan 2018 11:13:29 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Wu", "Weichang", ""], ["Yan", "Junchi", ""], ["Yang", "Xiaokang", ""], ["Zha", "Hongyuan", ""]]}, {"id": "1801.06889", "submitter": "Fred Hohman", "authors": "Fred Hohman, Minsuk Kahng, Robert Pienta, Duen Horng Chau", "title": "Visual Analytics in Deep Learning: An Interrogative Survey for the Next\n  Frontiers", "comments": "Under review for IEEE Transactions on Visualization and Computer\n  Graphics (TVCG)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has recently seen rapid development and received significant\nattention due to its state-of-the-art performance on previously-thought hard\nproblems. However, because of the internal complexity and nonlinear structure\nof deep neural networks, the underlying decision making processes for why these\nmodels are achieving such performance are challenging and sometimes mystifying\nto interpret. As deep learning spreads across domains, it is of paramount\nimportance that we equip users of deep learning with tools for understanding\nwhen a model works correctly, when it fails, and ultimately how to improve its\nperformance. Standardized toolkits for building neural networks have helped\ndemocratize deep learning; visual analytics systems have now been developed to\nsupport model explanation, interpretation, debugging, and improvement. We\npresent a survey of the role of visual analytics in deep learning research,\nwhich highlights its short yet impactful history and thoroughly summarizes the\nstate-of-the-art using a human-centered interrogative framework, focusing on\nthe Five W's and How (Why, Who, What, How, When, and Where). We conclude by\nhighlighting research directions and open research problems. This survey helps\nresearchers and practitioners in both visual analytics and deep learning to\nquickly learn key aspects of this young and rapidly growing body of research,\nwhose impact spans a diverse range of domains.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jan 2018 20:13:07 GMT"}, {"version": "v2", "created": "Fri, 4 May 2018 01:09:33 GMT"}, {"version": "v3", "created": "Mon, 14 May 2018 04:59:24 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Hohman", "Fred", ""], ["Kahng", "Minsuk", ""], ["Pienta", "Robert", ""], ["Chau", "Duen Horng", ""]]}, {"id": "1801.06900", "submitter": "Liang Ding", "authors": "Liang Ding, Di Chang, Russell Malmberg, Aaron Martinez, David\n  Robinson, Matthew Wicker, Hongfei Yan, and Liming Cai", "title": "Efficient Learning of Optimal Markov Network Topology with k-Tree\n  Modeling", "comments": "18 pages main text, 2 pages appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The seminal work of Chow and Liu (1968) shows that approximation of a finite\nprobabilistic system by Markov trees can achieve the minimum information loss\nwith the topology of a maximum spanning tree. Our current paper generalizes the\nresult to Markov networks of tree width $\\leq k$, for every fixed $k\\geq 2$. In\nparticular, we prove that approximation of a finite probabilistic system with\nsuch Markov networks has the minimum information loss when the network topology\nis achieved with a maximum spanning $k$-tree. While constructing a maximum\nspanning $k$-tree is intractable for even $k=2$, we show that polynomial\nalgorithms can be ensured by a sufficient condition accommodated by many\nmeaningful applications. In particular, we prove an efficient algorithm for\nlearning the optimal topology of higher order correlations among random\nvariables that belong to an underlying linear structure.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jan 2018 22:16:48 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Ding", "Liang", ""], ["Chang", "Di", ""], ["Malmberg", "Russell", ""], ["Martinez", "Aaron", ""], ["Robinson", "David", ""], ["Wicker", "Matthew", ""], ["Yan", "Hongfei", ""], ["Cai", "Liming", ""]]}, {"id": "1801.06920", "submitter": "Girish Joshi", "authors": "Girish Joshi, Girish Chowdhary", "title": "Cross-Domain Transfer in Reinforcement Learning using Target Apprentice", "comments": "To appear as conference paper in ICRA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new approach to Transfer Learning (TL) in\nReinforcement Learning (RL) for cross-domain tasks. Many of the available\ntechniques approach the transfer architecture as a method of speeding up the\ntarget task learning. We propose to adapt and reuse the mapped source task\noptimal-policy directly in related domains. We show the optimal policy from a\nrelated source task can be near optimal in target domain provided an adaptive\npolicy accounts for the model error between target and source. The main benefit\nof this policy augmentation is generalizing policies across multiple related\ndomains without having to re-learn the new tasks. Our results show that this\narchitecture leads to better sample efficiency in the transfer, reducing sample\ncomplexity of target task learning to target apprentice learning.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 00:39:19 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Joshi", "Girish", ""], ["Chowdhary", "Girish", ""]]}, {"id": "1801.06975", "submitter": "Feng Li", "authors": "Feng Li, Sibo Yang, Huanhuan Huang, and Wei Wu", "title": "Extreme Learning Machine with Local Connections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with the sparsification of the input-hidden weights\nof ELM (Extreme Learning Machine). For ordinary feedforward neural networks,\nthe sparsification is usually done by introducing certain regularization\ntechnique into the learning process of the network. But this strategy can not\nbe applied for ELM, since the input-hidden weights of ELM are supposed to be\nrandomly chosen rather than to be learned. To this end, we propose a modified\nELM, called ELM-LC (ELM with local connections), which is designed for the\nsparsification of the input-hidden weights as follows: The hidden nodes and the\ninput nodes are divided respectively into several corresponding groups, and an\ninput node group is fully connected with its corresponding hidden node group,\nbut is not connected with any other hidden node group. As in the usual ELM, the\nhidden-input weights are randomly given, and the hidden-output weights are\nobtained through a least square learning. In the numerical simulations on some\nbenchmark problems, the new ELM-CL behaves better than the traditional ELM.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 06:54:22 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Li", "Feng", ""], ["Yang", "Sibo", ""], ["Huang", "Huanhuan", ""], ["Wu", "Wei", ""]]}, {"id": "1801.07150", "submitter": "Muhammad Abulaish", "authors": "Muhammad Abulaish, Jahiruddin", "title": "A Novel Weighted Distance Measure for Multi-Attributed Graph", "comments": null, "journal-ref": "Muhammad Abulaish and Jahiruddin, A Novel Weighted Distance\n  Measure for Multi-Attributed Graph, In Proceedings of the 10th Annual ACM\n  India Conference (COMPUTE), Bhopal, India, Nov. 16-18, 2017, pp. 1-9", "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.DS cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to exponential growth of complex data, graph structure has become\nincreasingly important to model various entities and their interactions, with\nmany interesting applications including, bioinformatics, social network\nanalysis, etc. Depending on the complexity of the data, the underlying graph\nmodel can be a simple directed/undirected and/or weighted/un-weighted graph to\na complex graph (aka multi-attributed graph) where vertices and edges are\nlabelled with multi-dimensional vectors. In this paper, we present a novel\nweighted distance measure based on weighted Euclidean norm which is defined as\na function of both vertex and edge attributes, and it can be used for various\ngraph analysis tasks including classification and cluster analysis. The\nproposed distance measure has flexibility to increase/decrease the weightage of\nedge labels while calculating the distance between vertex-pairs. We have also\nproposed a MAGDist algorithm, which reads multi-attributed graph stored in CSV\nfiles containing the list of vertex vectors and edge vectors, and calculates\nthe distance between each vertex-pair using the proposed weighted distance\nmeasure. Finally, we have proposed a multi-attributed similarity graph\ngeneration algorithm, MAGSim, which reads the output of MAGDist algorithm and\ngenerates a similarity graph that can be analysed using classification and\nclustering algorithms. The significance and accuracy of the proposed distance\nmeasure and algorithms is evaluated on Iris and Twitter data sets, and it is\nfound that the similarity graph generated by our proposed method yields better\nclustering results than the existing similarity graph generation methods.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 15:46:40 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Abulaish", "Muhammad", ""], ["Jahiruddin", "", ""]]}, {"id": "1801.07161", "submitter": "Laura Giordano", "authors": "Laura Giordano and Valentina Gliozzi", "title": "Reasoning about multiple aspects in DLs: Semantics and Closure\n  Construction", "comments": "arXiv admin note: text overlap with arXiv:1604.00301", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Starting from the observation that rational closure has the undesirable\nproperty of being an \"all or nothing\" mechanism, we here propose a\nmultipreferential semantics, which enriches the preferential semantics\nunderlying rational closure in order to separately deal with the inheritance of\ndifferent properties in an ontology with exceptions. We provide a\nmultipreference closure mechanism which is sound with respect to the\nmultipreference semantics.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jan 2018 20:50:48 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Giordano", "Laura", ""], ["Gliozzi", "Valentina", ""]]}, {"id": "1801.07215", "submitter": "Chao Yan", "authors": "Chao Yan, Bo Li, Yevgeniy Vorobeychik, Aron Laszka, Daniel Fabbri,\n  Bradley Malin", "title": "Get Your Workload in Order: Game Theoretic Prioritization of Database\n  Auditing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CR cs.DB cs.GT cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For enhancing the privacy protections of databases, where the increasing\namount of detailed personal data is stored and processed, multiple mechanisms\nhave been developed, such as audit logging and alert triggers, which notify\nadministrators about suspicious activities; however, the two main limitations\nin common are: 1) the volume of such alerts is often substantially greater than\nthe capabilities of resource-constrained organizations, and 2) strategic\nattackers may disguise their actions or carefully choosing which records they\ntouch, making incompetent the statistical detection models. For solving them,\nwe introduce a novel approach to database auditing that explicitly accounts for\nadversarial behavior by 1) prioritizing the order in which types of alerts are\ninvestigated and 2) providing an upper bound on how much resource to allocate\nfor each type. We model the interaction between a database auditor and\npotential attackers as a Stackelberg game in which the auditor chooses an\nauditing policy and attackers choose which records to target. A corresponding\napproach combining linear programming, column generation, and heuristic search\nis proposed to derive an auditing policy. For testing the policy-searching\nperformance, a publicly available credit card application dataset are adopted,\non which it shows that our methods produce high-quality mixed strategies as\ndatabase audit policies, and our general approach significantly outperforms\nnon-game-theoretic baselines.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 17:42:32 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Yan", "Chao", ""], ["Li", "Bo", ""], ["Vorobeychik", "Yevgeniy", ""], ["Laszka", "Aron", ""], ["Fabbri", "Daniel", ""], ["Malin", "Bradley", ""]]}, {"id": "1801.07226", "submitter": "Junhong Lin", "authors": "Junhong Lin and Volkan Cevher", "title": "Optimal Convergence for Distributed Learning with Stochastic Gradient\n  Methods and Spectral Algorithms", "comments": "53 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG math.FA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study generalization properties of distributed algorithms in the setting\nof nonparametric regression over a reproducing kernel Hilbert space (RKHS). We\nfirst investigate distributed stochastic gradient methods (SGM), with\nmini-batches and multi-passes over the data. We show that optimal\ngeneralization error bounds can be retained for distributed SGM provided that\nthe partition level is not too large. We then extend our results to\nspectral-regularization algorithms (SRA), including kernel ridge regression\n(KRR), kernel principal component analysis, and gradient methods. Our results\nare superior to the state-of-the-art theory. Particularly, our results show\nthat distributed SGM has a smaller theoretical computational complexity,\ncompared with distributed KRR and classic SGM. Moreover, even for\nnon-distributed SRA, they provide the first optimal, capacity-dependent\nconvergence rates, considering the case that the regression function may not be\nin the RKHS.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 18:14:11 GMT"}, {"version": "v2", "created": "Mon, 5 Nov 2018 15:21:59 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Lin", "Junhong", ""], ["Cevher", "Volkan", ""]]}, {"id": "1801.07229", "submitter": "Mark Levin", "authors": "Mark Sh. Levin", "title": "Combinatorial framework for planning in geological exploration", "comments": "14 pages, 15 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper describes combinatorial framework for planning of geological\nexploration for oil-gas fields. The suggested scheme of the geological\nexploration involves the following stages: (1) building of special 4-layer\ntree-like model (layer of geological exploration): productive layer, group of\nproductive layers, oil-gas field, oil-gas region (or group of the fields); (2)\ngenerations of local design (exploration) alternatives for each low-layer\ngeological objects: conservation, additional search, independent utilization,\njoint utilization; (3) multicriteria (i.e., multi-attribute) assessment of the\ndesign (exploration) alternatives and their interrelation (compatibility) and\nmapping if the obtained vector estimates into integrated ordinal scale; (4)\nhierarchical design ('bottom-up') of composite exploration plans for each\noil-gas field; (5) integration of the plans into region plans and (6)\naggregation of the region plans into a general exploration plan. Stages 2, 3,\n4, and 5 are based on hierarchical multicriteria morphological design (HMMD)\nmethod (assessment of ranking of alternatives, selection and composition of\nalternatives into composite alternatives). The composition problem is based on\nmorphological clique model. Aggregation of the obtained modular alternatives\n(stage 6) is based on detection of a alternatives 'kernel' and its extension by\naddition of elements (multiple choice model). In addition, the usage of\nmultiset estimates for alternatives is described as well. The alternative\nestimates are based on expert judgment. The suggested combinatorial planning\nmethodology is illustrated by numerical examples for geological exploration of\nYamal peninsula.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 18:22:42 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Levin", "Mark Sh.", ""]]}, {"id": "1801.07243", "submitter": "Jason  Weston", "authors": "Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela,\n  Jason Weston", "title": "Personalizing Dialogue Agents: I have a dog, do you have pets too?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chit-chat models are known to have several problems: they lack specificity,\ndo not display a consistent personality and are often not very captivating. In\nthis work we present the task of making chit-chat more engaging by conditioning\non profile information. We collect data and train models to (i) condition on\ntheir given profile information; and (ii) information about the person they are\ntalking to, resulting in improved dialogues, as measured by next utterance\nprediction. Since (ii) is initially unknown our model is trained to engage its\npartner with personal topics, and we show the resulting dialogue can be used to\npredict profile information about the interlocutors.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 18:58:18 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2018 23:08:17 GMT"}, {"version": "v3", "created": "Sun, 13 May 2018 15:53:46 GMT"}, {"version": "v4", "created": "Tue, 4 Sep 2018 04:48:01 GMT"}, {"version": "v5", "created": "Tue, 25 Sep 2018 18:55:07 GMT"}], "update_date": "2018-09-27", "authors_parsed": [["Zhang", "Saizheng", ""], ["Dinan", "Emily", ""], ["Urbanek", "Jack", ""], ["Szlam", "Arthur", ""], ["Kiela", "Douwe", ""], ["Weston", "Jason", ""]]}, {"id": "1801.07357", "submitter": "Yoav Artzi", "authors": "Claudia Yan, Dipendra Misra, Andrew Bennnett, Aaron Walsman, Yonatan\n  Bisk and Yoav Artzi", "title": "CHALET: Cornell House Agent Learning Environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present CHALET, a 3D house simulator with support for navigation and\nmanipulation. CHALET includes 58 rooms and 10 house configuration, and allows\nto easily create new house and room layouts. CHALET supports a range of common\nhousehold activities, including moving objects, toggling appliances, and\nplacing objects inside closeable containers. The environment and actions\navailable are designed to create a challenging domain to train and evaluate\nautonomous agents, including for tasks that combine language, vision, and\nplanning in a dynamic environment.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 00:22:25 GMT"}, {"version": "v2", "created": "Mon, 16 Sep 2019 21:13:22 GMT"}], "update_date": "2019-09-18", "authors_parsed": [["Yan", "Claudia", ""], ["Misra", "Dipendra", ""], ["Bennnett", "Andrew", ""], ["Walsman", "Aaron", ""], ["Bisk", "Yonatan", ""], ["Artzi", "Yoav", ""]]}, {"id": "1801.07384", "submitter": "Hugh Chen", "authors": "Hugh Chen, Scott Lundberg, Su-In Lee", "title": "Hybrid Gradient Boosting Trees and Neural Networks for Forecasting\n  Operating Room Data", "comments": "Presented at Machine Learning for Health Workshop: 31st Conference on\n  Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series data constitutes a distinct and growing problem in machine\nlearning. As the corpus of time series data grows larger, deep models that\nsimultaneously learn features and classify with these features can be\nintractable or suboptimal. In this paper, we present feature learning via long\nshort term memory (LSTM) networks and prediction via gradient boosting trees\n(XGB). Focusing on the consequential setting of electronic health record data,\nwe predict the occurrence of hypoxemia five minutes into the future based on\npast features. We make two observations: 1) long short term memory networks are\neffective at capturing long term dependencies based on a single feature and 2)\ngradient boosting trees are capable of tractably combining a large number of\nfeatures including static features like height and weight. With these\nobservations in mind, we generate features by performing \"supervised\"\nrepresentation learning with LSTM networks. Augmenting the original XGB model\nwith these features gives significantly better performance than either\nindividual method.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 03:18:14 GMT"}, {"version": "v2", "created": "Wed, 24 Jan 2018 02:11:40 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Chen", "Hugh", ""], ["Lundberg", "Scott", ""], ["Lee", "Su-In", ""]]}, {"id": "1801.07411", "submitter": "I-Chen Wu", "authors": "Wen-Jie Tseng, Jr-Chang Chen, I-Chen Wu, Tinghan Wei", "title": "Comparison Training for Computer Chinese Chess", "comments": "Submitted to IEEE Transaction on Games", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the application of comparison training (CT) for\nautomatic feature weight tuning, with the final objective of improving the\nevaluation functions used in Chinese chess programs. First, we propose an\nn-tuple network to extract features, since n-tuple networks require very little\nexpert knowledge through its large numbers of features, while simulta-neously\nallowing easy access. Second, we propose a novel evalua-tion method that\nincorporates tapered eval into CT. Experiments show that with the same features\nand the same Chinese chess program, the automatically tuned comparison training\nfeature weights achieved a win rate of 86.58% against the weights that were\nhand-tuned. The above trained version was then improved by adding additional\nfeatures, most importantly n-tuple features. This improved version achieved a\nwin rate of 81.65% against the trained version without additional features.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 07:09:26 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Tseng", "Wen-Jie", ""], ["Chen", "Jr-Chang", ""], ["Wu", "I-Chen", ""], ["Wei", "Tinghan", ""]]}, {"id": "1801.07440", "submitter": "Ildefons Magrans de Abril", "authors": "Ildefons Magrans de Abril and Ryota Kanai", "title": "Curiosity-driven reinforcement learning with homeostatic regulation", "comments": "Presented at the NIPS 2017 Workshop: Cognitively Informed Artificial\n  Intelligence: Insights From Natural Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a curiosity reward based on information theory principles and\nconsistent with the animal instinct to maintain certain critical parameters\nwithin a bounded range. Our experimental validation shows the added value of\nthe additional homeostatic drive to enhance the overall information gain of a\nreinforcement learning agent interacting with a complex environment using\ncontinuous actions. Our method builds upon two ideas: i) To take advantage of a\nnew Bellman-like equation of information gain and ii) to simplify the\ncomputation of the local rewards by avoiding the approximation of complex\ndistributions over continuous states and actions.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 08:52:22 GMT"}, {"version": "v2", "created": "Wed, 7 Feb 2018 02:27:12 GMT"}], "update_date": "2018-02-08", "authors_parsed": [["de Abril", "Ildefons Magrans", ""], ["Kanai", "Ryota", ""]]}, {"id": "1801.07537", "submitter": "Jannis Bulian", "authors": "Christian Buck, Jannis Bulian, Massimiliano Ciaramita, Wojciech\n  Gajewski, Andrea Gesmundo, Neil Houlsby, Wei Wang", "title": "Analyzing Language Learned by an Active Question Answering Agent", "comments": "Emergent Communication Workshop, NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the language learned by an agent trained with reinforcement\nlearning as a component of the ActiveQA system [Buck et al., 2017]. In\nActiveQA, question answering is framed as a reinforcement learning task in\nwhich an agent sits between the user and a black box question-answering system.\nThe agent learns to reformulate the user's questions to elicit the optimal\nanswers. It probes the system with many versions of a question that are\ngenerated via a sequence-to-sequence question reformulation model, then\naggregates the returned evidence to find the best answer. This process is an\ninstance of \\emph{machine-machine} communication. The question reformulation\nmodel must adapt its language to increase the quality of the answers returned,\nmatching the language of the question answering system. We find that the agent\ndoes not learn transformations that align with semantic intuitions but\ndiscovers through learning classical information retrieval techniques such as\ntf-idf re-weighting and stemming.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 13:50:11 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Buck", "Christian", ""], ["Bulian", "Jannis", ""], ["Ciaramita", "Massimiliano", ""], ["Gajewski", "Wojciech", ""], ["Gesmundo", "Andrea", ""], ["Houlsby", "Neil", ""], ["Wang", "Wei", ""]]}, {"id": "1801.07593", "submitter": "Margaret Mitchell", "authors": "Brian Hu Zhang, Blake Lemoine, Margaret Mitchell", "title": "Mitigating Unwanted Biases with Adversarial Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning is a tool for building models that accurately represent\ninput training data. When undesired biases concerning demographic groups are in\nthe training data, well-trained models will reflect those biases. We present a\nframework for mitigating such biases by including a variable for the group of\ninterest and simultaneously learning a predictor and an adversary. The input to\nthe network X, here text or census data, produces a prediction Y, such as an\nanalogy completion or income bracket, while the adversary tries to model a\nprotected variable Z, here gender or zip code.\n  The objective is to maximize the predictor's ability to predict Y while\nminimizing the adversary's ability to predict Z. Applied to analogy completion,\nthis method results in accurate predictions that exhibit less evidence of\nstereotyping Z. When applied to a classification task using the UCI Adult\n(Census) Dataset, it results in a predictive model that does not lose much\naccuracy while achieving very close to equality of odds (Hardt, et al., 2016).\nThe method is flexible and applicable to multiple definitions of fairness as\nwell as a wide range of gradient-based learning models, including both\nregression and classification tasks.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 06:45:23 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Zhang", "Brian Hu", ""], ["Lemoine", "Blake", ""], ["Mitchell", "Margaret", ""]]}, {"id": "1801.07648", "submitter": "Elie Aljalbout", "authors": "Elie Aljalbout, Vladimir Golkov, Yawar Siddiqui, Maximilian Strobel,\n  Daniel Cremers", "title": "Clustering with Deep Learning: Taxonomy and New Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering methods based on deep neural networks have proven promising for\nclustering real-world data because of their high representational power. In\nthis paper, we propose a systematic taxonomy of clustering methods that utilize\ndeep neural networks. We base our taxonomy on a comprehensive review of recent\nwork and validate the taxonomy in a case study. In this case study, we show\nthat the taxonomy enables researchers and practitioners to systematically\ncreate new clustering methods by selectively recombining and replacing distinct\naspects of previous methods with the goal of overcoming their individual\nlimitations. The experimental evaluation confirms this and shows that the\nmethod created for the case study achieves state-of-the-art clustering quality\nand surpasses it in some cases.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 16:41:03 GMT"}, {"version": "v2", "created": "Thu, 13 Sep 2018 19:41:22 GMT"}], "update_date": "2018-09-17", "authors_parsed": [["Aljalbout", "Elie", ""], ["Golkov", "Vladimir", ""], ["Siddiqui", "Yawar", ""], ["Strobel", "Maximilian", ""], ["Cremers", "Daniel", ""]]}, {"id": "1801.07653", "submitter": "Daniel Hornung", "authors": "Timm Fitschen, Alexander Schlemmer, Daniel Hornung, Henrik tom\n  W\\\"orden, Ulrich Parlitz, Stefan Luther", "title": "CaosDB - Research Data Management for Complex, Changing, and Automated\n  Research Workflows", "comments": null, "journal-ref": "Data 2019, 4(2), 83", "doi": "10.3390/data4020083", "report-no": null, "categories": "cs.DB cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Here we present CaosDB, a Research Data Management System (RDMS) designed to\nensure seamless integration of inhomogeneous data sources and repositories of\nlegacy data. Its primary purpose is the management of data from biomedical\nsciences, both from simulations and experiments during the complete research\ndata lifecycle. An RDMS for this domain faces particular challenges: Research\ndata arise in huge amounts, from a wide variety of sources, and traverse a\nhighly branched path of further processing. To be accepted by its users, an\nRDMS must be built around workflows of the scientists and practices and thus\nsupport changes in workflow and data structure. Nevertheless it should\nencourage and support the development and observation of standards and\nfurthermore facilitate the automation of data acquisition and processing with\nspecialized software. The storage data model of an RDMS must reflect these\ncomplexities with appropriate semantics and ontologies while offering simple\nmethods for finding, retrieving, and understanding relevant data. We show how\nCaosDB responds to these challenges and give an overview of the CaosDB Server,\nits data model and its easy-to-learn CaosDB Query Language. We briefly discuss\nthe status of the implementation, how we currently use CaosDB, and how we plan\nto use and extend it.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 16:46:18 GMT"}, {"version": "v2", "created": "Wed, 20 Mar 2019 12:46:10 GMT"}], "update_date": "2019-06-13", "authors_parsed": [["Fitschen", "Timm", ""], ["Schlemmer", "Alexander", ""], ["Hornung", "Daniel", ""], ["W\u00f6rden", "Henrik tom", ""], ["Parlitz", "Ulrich", ""], ["Luther", "Stefan", ""]]}, {"id": "1801.07654", "submitter": "Pablo Barros", "authors": "Pablo Barros, German I. Parisi, Di Fu, Xun Liu, and Stefan Wermter", "title": "Expectation Learning for Adaptive Crossmodal Stimuli Association", "comments": "3 pages 2017 EUCog meeting abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SD q-bio.NC stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The human brain is able to learn, generalize, and predict crossmodal stimuli.\nLearning by expectation fine-tunes crossmodal processing at different levels,\nthus enhancing our power of generalization and adaptation in highly dynamic\nenvironments. In this paper, we propose a deep neural architecture trained by\nusing expectation learning accounting for unsupervised learning tasks. Our\nlearning model exhibits a self-adaptable behavior, setting the first steps\ntowards the development of deep learning architectures for crossmodal stimuli\nassociation.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 16:47:32 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Barros", "Pablo", ""], ["Parisi", "German I.", ""], ["Fu", "Di", ""], ["Liu", "Xun", ""], ["Wermter", "Stefan", ""]]}, {"id": "1801.07674", "submitter": "James Davis", "authors": "James W. Davis, Christopher Menart, Muhammad Akbar, Roman Ilin", "title": "A Classification Refinement Strategy for Semantic Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on the observation that semantic segmentation errors are partially\npredictable, we propose a compact formulation using confusion statistics of the\ntrained classifier to refine (re-estimate) the initial pixel label hypotheses.\nThe proposed strategy is contingent upon computing the classifier confusion\nprobabilities for a given dataset and estimating a relevant prior on the object\nclasses present in the image to be classified. We provide a procedure to\nrobustly estimate the confusion probabilities and explore multiple prior\ndefinitions. Experiments are shown comparing performances on multiple\nchallenging datasets using different priors to improve a state-of-the-art\nsemantic segmentation classifier. This study demonstrates the potential to\nsignificantly improve semantic labeling and motivates future work for reliable\nlabel prior estimation from images.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 17:45:54 GMT"}], "update_date": "2018-01-24", "authors_parsed": [["Davis", "James W.", ""], ["Menart", "Christopher", ""], ["Akbar", "Muhammad", ""], ["Ilin", "Roman", ""]]}, {"id": "1801.07710", "submitter": "Vikram Mullachery", "authors": "Vikram Mullachery, Aniruddh Khera, Amir Husain", "title": "Bayesian Neural Networks", "comments": "arXiv admin note: text overlap with arXiv:1111.4246 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes and discusses Bayesian Neural Network (BNN). The paper\nshowcases a few different applications of them for classification and\nregression problems. BNNs are comprised of a Probabilistic Model and a Neural\nNetwork. The intent of such a design is to combine the strengths of Neural\nNetworks and Stochastic modeling. Neural Networks exhibit continuous function\napproximator capabilities. Stochastic models allow direct specification of a\nmodel with known interaction between parameters to generate data. During the\nprediction phase, stochastic models generate a complete posterior distribution\nand produce probabilistic guarantees on the predictions. Thus BNNs are a unique\ncombination of neural network and stochastic models with the stochastic model\nforming the core of this integration. BNNs can then produce probabilistic\nguarantees on it's predictions and also generate the distribution of parameters\nthat it has learnt from the observations. That means, in the parameter space,\none can deduce the nature and shape of the neural network's learnt parameters.\nThese two characteristics makes them highly attractive to theoreticians as well\nas practitioners. Recently there has been a lot of activity in this area, with\nthe advent of numerous probabilistic programming libraries such as: PyMC3,\nEdward, Stan etc. Further this area is rapidly gaining ground as a standard\nmachine learning approach for numerous problems\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 20:52:44 GMT"}, {"version": "v2", "created": "Tue, 30 Jan 2018 15:30:26 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Mullachery", "Vikram", ""], ["Khera", "Aniruddh", ""], ["Husain", "Amir", ""]]}, {"id": "1801.07729", "submitter": "Ahmed Elgammal", "authors": "Ahmed Elgammal and Marian Mazzone and Bingchen Liu and Diana Kim and\n  Mohamed Elhoseiny", "title": "The Shape of Art History in the Eyes of the Machine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  How does the machine classify styles in art? And how does it relate to art\nhistorians' methods for analyzing style? Several studies have shown the ability\nof the machine to learn and predict style categories, such as Renaissance,\nBaroque, Impressionism, etc., from images of paintings. This implies that the\nmachine can learn an internal representation encoding discriminative features\nthrough its visual analysis. However, such a representation is not necessarily\ninterpretable. We conducted a comprehensive study of several of the\nstate-of-the-art convolutional neural networks applied to the task of style\nclassification on 77K images of paintings, and analyzed the learned\nrepresentation through correlation analysis with concepts derived from art\nhistory. Surprisingly, the networks could place the works of art in a smooth\ntemporal arrangement mainly based on learning style labels, without any a\npriori knowledge of time of creation, the historical time and context of\nstyles, or relations between styles. The learned representations showed that\nthere are few underlying factors that explain the visual variations of style in\nart. Some of these factors were found to correlate with style patterns\nsuggested by Heinrich W\\\"olfflin (1846-1945). The learned representations also\nconsistently highlighted certain artists as the extreme distinctive\nrepresentative of their styles, which quantitatively confirms art historian\nobservations.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 19:05:21 GMT"}, {"version": "v2", "created": "Mon, 12 Feb 2018 10:00:28 GMT"}], "update_date": "2018-02-13", "authors_parsed": [["Elgammal", "Ahmed", ""], ["Mazzone", "Marian", ""], ["Liu", "Bingchen", ""], ["Kim", "Diana", ""], ["Elhoseiny", "Mohamed", ""]]}, {"id": "1801.07736", "submitter": "William Fedus", "authors": "William Fedus, Ian Goodfellow and Andrew M. Dai", "title": "MaskGAN: Better Text Generation via Filling in the______", "comments": "16 pages, ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural text generation models are often autoregressive language models or\nseq2seq models. These models generate text by sampling words sequentially, with\neach word conditioned on the previous word, and are state-of-the-art for\nseveral machine translation and summarization benchmarks. These benchmarks are\noften defined by validation perplexity even though this is not a direct measure\nof the quality of the generated text. Additionally, these models are typically\ntrained via maxi- mum likelihood and teacher forcing. These methods are\nwell-suited to optimizing perplexity but can result in poor sample quality\nsince generating text requires conditioning on sequences of words that may have\nnever been observed at training time. We propose to improve sample quality\nusing Generative Adversarial Networks (GANs), which explicitly train the\ngenerator to produce high quality samples and have shown a lot of success in\nimage generation. GANs were originally designed to output differentiable\nvalues, so discrete language generation is challenging for them. We claim that\nvalidation perplexity alone is not indicative of the quality of text generated\nby a model. We introduce an actor-critic conditional GAN that fills in missing\ntext conditioned on the surrounding context. We show qualitatively and\nquantitatively, evidence that this produces more realistic conditional and\nunconditional text samples compared to a maximum likelihood trained model.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 19:22:21 GMT"}, {"version": "v2", "created": "Wed, 28 Feb 2018 16:26:04 GMT"}, {"version": "v3", "created": "Thu, 1 Mar 2018 15:30:09 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Fedus", "William", ""], ["Goodfellow", "Ian", ""], ["Dai", "Andrew M.", ""]]}, {"id": "1801.07743", "submitter": "Pedro Saleiro", "authors": "Pedro Saleiro", "title": "Entity Retrieval and Text Mining for Online Reputation Monitoring", "comments": "PhD Thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online Reputation Monitoring (ORM) is concerned with the use of computational\ntools to measure the reputation of entities online, such as politicians or\ncompanies. In practice, current ORM methods are constrained to the generation\nof data analytics reports, which aggregate statistics of popularity and\nsentiment on social media. We argue that this format is too restrictive as end\nusers often like to have the flexibility to search for entity-centric\ninformation that is not available in predefined charts. As such, we propose the\ninclusion of entity retrieval capabilities as a first step towards the\nextension of current ORM capabilities. However, an entity's reputation is also\ninfluenced by the entity's relationships with other entities. Therefore, we\naddress the problem of Entity-Relationship (E-R) retrieval in which the goal is\nto search for multiple connected entities. This is a challenging problem which\ntraditional entity search systems cannot cope with. Besides E-R retrieval we\nalso believe ORM would benefit of text-based entity-centric prediction\ncapabilities, such as predicting entity popularity on social media based on\nnews events or the outcome of political surveys. However, none of these tasks\ncan provide useful results if there is no effective entity disambiguation and\nsentiment analysis tailored to the context of ORM. Consequently, this thesis\naddress two computational problems in Online Reputation Monitoring: Entity\nRetrieval and Text Mining. We researched and developed methods to extract,\nretrieve and predict entity-centric information spread across the Web.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 19:36:29 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Saleiro", "Pedro", ""]]}, {"id": "1801.07745", "submitter": "Justin Solomon", "authors": "Justin Solomon", "title": "Optimal Transport on Discrete Domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.AI cs.CG cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the matching of supply to demand in logistical problems, the\noptimal transport (or Monge--Kantorovich) problem involves the matching of\nprobability distributions defined over a geometric domain such as a surface or\nmanifold. In its most obvious discretization, optimal transport becomes a\nlarge-scale linear program, which typically is infeasible to solve efficiently\non triangle meshes, graphs, point clouds, and other domains encountered in\ngraphics and machine learning. Recent breakthroughs in numerical optimal\ntransport, however, enable scalability to orders-of-magnitude larger problems,\nsolvable in a fraction of a second. Here, we discuss advances in numerical\noptimal transport that leverage understanding of both discrete and smooth\naspects of the problem. State-of-the-art techniques in discrete optimal\ntransport combine insight from partial differential equations (PDE) with convex\nanalysis to reformulate, discretize, and optimize transportation problems. The\nend result is a set of theoretically-justified models suitable for domains with\nthousands or millions of vertices. Since numerical optimal transport is a\nrelatively new discipline, special emphasis is placed on identifying and\nexplaining open problems in need of mathematical insight and additional\nresearch.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 19:48:42 GMT"}, {"version": "v2", "created": "Tue, 1 May 2018 17:03:57 GMT"}], "update_date": "2018-05-02", "authors_parsed": [["Solomon", "Justin", ""]]}, {"id": "1801.07747", "submitter": "Vahid Yazdanpanah", "authors": "Vahid Yazdanpanah and Mehdi Dastani", "title": "Quantified Degrees of Group Responsibility (Extended Abstract)", "comments": "Presented in the 27th Belgian-Netherlands Conference on Artificial\n  Intelligence (BNAIC 2015), Hasselt, Belgium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper builds on an existing notion of group responsibility and proposes\ntwo ways to define the degree of group responsibility: structural and\nfunctional degrees of responsibility. These notions measure the potential\nresponsibilities of (agent) groups for avoiding a state of affairs. According\nto these notions, a degree of responsibility for a state of affairs can be\nassigned to a group of agents if, and to the extent that, the group has the\npotential to preclude the state of affairs.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 19:51:58 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Yazdanpanah", "Vahid", ""], ["Dastani", "Mehdi", ""]]}, {"id": "1801.07791", "submitter": "Yangyan Li", "authors": "Yangyan Li, Rui Bu, Mingchao Sun, Wei Wu, Xinhan Di, Baoquan Chen", "title": "PointCNN: Convolution On $\\mathcal{X}$-Transformed Points", "comments": "To be published in NIPS 2018, code available at\n  https://github.com/yangyanli/PointCNN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple and general framework for feature learning from point\nclouds. The key to the success of CNNs is the convolution operator that is\ncapable of leveraging spatially-local correlation in data represented densely\nin grids (e.g. images). However, point clouds are irregular and unordered, thus\ndirectly convolving kernels against features associated with the points, will\nresult in desertion of shape information and variance to point ordering. To\naddress these problems, we propose to learn an $\\mathcal{X}$-transformation\nfrom the input points, to simultaneously promote two causes. The first is the\nweighting of the input features associated with the points, and the second is\nthe permutation of the points into a latent and potentially canonical order.\nElement-wise product and sum operations of the typical convolution operator are\nsubsequently applied on the $\\mathcal{X}$-transformed features. The proposed\nmethod is a generalization of typical CNNs to feature learning from point\nclouds, thus we call it PointCNN. Experiments show that PointCNN achieves on\npar or better performance than state-of-the-art methods on multiple challenging\nbenchmark datasets and tasks.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jan 2018 22:07:21 GMT"}, {"version": "v2", "created": "Thu, 25 Jan 2018 11:45:08 GMT"}, {"version": "v3", "created": "Fri, 26 Jan 2018 02:11:12 GMT"}, {"version": "v4", "created": "Thu, 25 Oct 2018 01:33:31 GMT"}, {"version": "v5", "created": "Mon, 5 Nov 2018 09:31:45 GMT"}], "update_date": "2018-11-06", "authors_parsed": [["Li", "Yangyan", ""], ["Bu", "Rui", ""], ["Sun", "Mingchao", ""], ["Wu", "Wei", ""], ["Di", "Xinhan", ""], ["Chen", "Baoquan", ""]]}, {"id": "1801.07826", "submitter": "Susan Athey", "authors": "Susan Athey, David Blei, Robert Donnelly, Francisco Ruiz, and Tobias\n  Schmidt", "title": "Estimating Heterogeneous Consumer Preferences for Restaurants and Travel\n  Time Using Mobile Location Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "econ.EM cs.AI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper analyzes consumer choices over lunchtime restaurants using data\nfrom a sample of several thousand anonymous mobile phone users in the San\nFrancisco Bay Area. The data is used to identify users' approximate typical\nmorning location, as well as their choices of lunchtime restaurants. We build a\nmodel where restaurants have latent characteristics (whose distribution may\ndepend on restaurant observables, such as star ratings, food category, and\nprice range), each user has preferences for these latent characteristics, and\nthese preferences are heterogeneous across users. Similarly, each item has\nlatent characteristics that describe users' willingness to travel to the\nrestaurant, and each user has individual-specific preferences for those latent\ncharacteristics. Thus, both users' willingness to travel and their base utility\nfor each restaurant vary across user-restaurant pairs. We use a Bayesian\napproach to estimation. To make the estimation computationally feasible, we\nrely on variational inference to approximate the posterior distribution, as\nwell as stochastic gradient descent as a computational approach. Our model\nperforms better than more standard competing models such as multinomial logit\nand nested logit models, in part due to the personalization of the estimates.\nWe analyze how consumers re-allocate their demand after a restaurant closes to\nnearby restaurants versus more distant restaurants with similar\ncharacteristics, and we compare our predictions to actual outcomes. Finally, we\nshow how the model can be used to analyze counterfactual questions such as what\ntype of restaurant would attract the most consumers in a given location.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jan 2018 23:55:42 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Athey", "Susan", ""], ["Blei", "David", ""], ["Donnelly", "Robert", ""], ["Ruiz", "Francisco", ""], ["Schmidt", "Tobias", ""]]}, {"id": "1801.07863", "submitter": "Rediet Abebe", "authors": "Rediet Abebe, Jon Kleinberg, David Parkes, Charalampos E. Tsourakakis", "title": "Opinion Dynamics with Varying Susceptibility to Persuasion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.LG physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A long line of work in social psychology has studied variations in people's\nsusceptibility to persuasion -- the extent to which they are willing to modify\ntheir opinions on a topic. This body of literature suggests an interesting\nperspective on theoretical models of opinion formation by interacting parties\nin a network: in addition to considering interventions that directly modify\npeople's intrinsic opinions, it is also natural to consider interventions that\nmodify people's susceptibility to persuasion. In this work, we adopt a popular\nmodel for social opinion dynamics, and we formalize the opinion maximization\nand minimization problems where interventions happen at the level of\nsusceptibility.\n  We show that modeling interventions at the level of susceptibility lead to an\ninteresting family of new questions in network opinion dynamics. We find that\nthe questions are quite different depending on whether there is an overall\nbudget constraining the number of agents we can target or not. We give a\npolynomial-time algorithm for finding the optimal target-set to optimize the\nsum of opinions when there are no budget constraints on the size of the\ntarget-set. We show that this problem is NP-hard when there is a budget, and\nthat the objective function is neither submodular nor supermodular. Finally, we\npropose a heuristic for the budgeted opinion optimization and show its efficacy\nat finding target-sets that optimize the sum of opinions compared on real world\nnetworks, including a Twitter network with real opinion estimates.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 05:12:45 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Abebe", "Rediet", ""], ["Kleinberg", "Jon", ""], ["Parkes", "David", ""], ["Tsourakakis", "Charalampos E.", ""]]}, {"id": "1801.07964", "submitter": "Nadia Boukhelifa", "authors": "Nadia Boukhelifa, Anastasia Bezerianos and Evelyne Lutton", "title": "Evaluation of Interactive Machine Learning Systems", "comments": "20", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The evaluation of interactive machine learning systems remains a difficult\ntask. These systems learn from and adapt to the human, but at the same time,\nthe human receives feedback and adapts to the system. Getting a clear\nunderstanding of these subtle mechanisms of co-operation and co-adaptation is\nchallenging. In this chapter, we report on our experience in designing and\nevaluating various interactive machine learning applications from different\ndomains. We argue for coupling two types of validation: algorithm-centered\nanalysis, to study the computational behaviour of the system; and\nhuman-centered evaluation, to observe the utility and effectiveness of the\napplication for end-users. We use a visual analytics application for guided\nsearch, built using an interactive evolutionary approach, as an exemplar of our\nwork. Our observation is that human-centered design and evaluation complement\nalgorithmic analysis, and can play an important role in addressing the\n\"black-box\" effect of machine learning. Finally, we discuss research\nopportunities that require human-computer interaction methodologies, in order\nto support both the visible and hidden roles that humans play in interactive\nmachine learning.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 12:47:26 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Boukhelifa", "Nadia", ""], ["Bezerianos", "Anastasia", ""], ["Lutton", "Evelyne", ""]]}, {"id": "1801.07985", "submitter": "Tom Hanika", "authors": "Tom Hanika and Friedrich Martin Schneider and Gerd Stumme", "title": "Intrinsic Dimension of Geometric Data Sets", "comments": "v3: 33 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The curse of dimensionality is a phenomenon frequently observed in machine\nlearning (ML) and knowledge discovery (KD). There is a large body of literature\ninvestigating its origin and impact, using methods from mathematics as well as\nfrom computer science. Among the mathematical insights into data\ndimensionality, there is an intimate link between the dimension curse and the\nphenomenon of measure concentration, which makes the former accessible to\nmethods of geometric analysis. The present work provides a comprehensive study\nof the intrinsic geometry of a data set, based on Gromov's metric measure\ngeometry and Pestov's axiomatic approach to intrinsic dimension. In detail, we\ndefine a concept of geometric data set and introduce a metric as well as a\npartial order on the set of isomorphism classes of such data sets. Based on\nthese objects, we propose and investigate an axiomatic approach to the\nintrinsic dimension of geometric data sets and establish a concrete dimension\nfunction with the desired properties. Our model for data sets and their\nintrinsic dimension is computationally feasible and, moreover, adaptable to\nspecific ML/KD-algorithms, as illustrated by various experiments.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 13:49:37 GMT"}, {"version": "v2", "created": "Mon, 24 Dec 2018 22:54:02 GMT"}, {"version": "v3", "created": "Mon, 26 Oct 2020 12:42:25 GMT"}], "update_date": "2020-10-27", "authors_parsed": [["Hanika", "Tom", ""], ["Schneider", "Friedrich Martin", ""], ["Stumme", "Gerd", ""]]}, {"id": "1801.08092", "submitter": "Aditya Ganeshan Master", "authors": "Konda Reddy Mopuri, Aditya Ganeshan and R. Venkatesh Babu", "title": "Generalizable Data-free Objective for Crafting Universal Adversarial\n  Perturbations", "comments": "TPAMI | Repository: https://github.com/val-iisc/GD-UAP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models are susceptible to adversarial perturbations: small\nchanges to input that can cause large changes in output. It is also\ndemonstrated that there exist input-agnostic perturbations, called universal\nadversarial perturbations, which can change the inference of target model on\nmost of the data samples. However, existing methods to craft universal\nperturbations are (i) task specific, (ii) require samples from the training\ndata distribution, and (iii) perform complex optimizations. Additionally,\nbecause of the data dependence, fooling ability of the crafted perturbations is\nproportional to the available training data. In this paper, we present a novel,\ngeneralizable and data-free approaches for crafting universal adversarial\nperturbations. Independent of the underlying task, our objective achieves\nfooling via corrupting the extracted features at multiple layers. Therefore,\nthe proposed objective is generalizable to craft image-agnostic perturbations\nacross multiple vision tasks such as object recognition, semantic segmentation,\nand depth estimation. In the practical setting of black-box attack scenario\n(when the attacker does not have access to the target model and it's training\ndata), we show that our objective outperforms the data dependent objectives to\nfool the learned models. Further, via exploiting simple priors related to the\ndata distribution, our objective remarkably boosts the fooling ability of the\ncrafted perturbations. Significant fooling rates achieved by our objective\nemphasize that the current deep learning models are now at an increased risk,\nsince our objective generalizes across multiple tasks without the requirement\nof training data for crafting the perturbations. To encourage reproducible\nresearch, we have released the codes for our proposed algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 17:36:57 GMT"}, {"version": "v2", "created": "Thu, 21 Jun 2018 12:43:10 GMT"}, {"version": "v3", "created": "Tue, 24 Jul 2018 08:19:43 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Mopuri", "Konda Reddy", ""], ["Ganeshan", "Aditya", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1801.08094", "submitter": "Kui Zhao", "authors": "Kui Zhao, Yuechuan Li, Chi Zhang, Cheng Yang, Huan Xu", "title": "Adaptive Recurrent Neural Network Based on Mixture Layer", "comments": "7 pages, 3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Although Recurrent Neural Network (RNN) has been a powerful tool for modeling\nsequential data, its performance is inadequate when processing sequences with\nmultiple patterns. In this paper, we address this challenge by introducing a\nnovel mixture layer and constructing an adaptive RNN. The mixture layer\naugmented RNN (termed as M-RNN) partitions patterns in training sequences into\nseveral clusters and stores the principle patterns as prototype vectors of\ncomponents in a mixture model. By leveraging the mixture layer, the proposed\nmethod can adaptively update states according to the similarities between\nencoded inputs and prototype vectors, leading to a stronger capacity in\nassimilating sequences with multiple patterns. Moreover, our approach can be\nfurther extended by taking advantage of prior knowledge about data. Experiments\non both synthetic and real datasets demonstrate the effectiveness of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 17:38:48 GMT"}, {"version": "v2", "created": "Thu, 1 Feb 2018 09:19:59 GMT"}, {"version": "v3", "created": "Fri, 18 May 2018 09:45:30 GMT"}, {"version": "v4", "created": "Wed, 27 Feb 2019 01:54:38 GMT"}], "update_date": "2019-02-28", "authors_parsed": [["Zhao", "Kui", ""], ["Li", "Yuechuan", ""], ["Zhang", "Chi", ""], ["Yang", "Cheng", ""], ["Xu", "Huan", ""]]}, {"id": "1801.08116", "submitter": "Joel Leibo", "authors": "Joel Z. Leibo, Cyprien de Masson d'Autume, Daniel Zoran, David Amos,\n  Charles Beattie, Keith Anderson, Antonio Garc\\'ia Casta\\~neda, Manuel\n  Sanchez, Simon Green, Audrunas Gruslys, Shane Legg, Demis Hassabis, Matthew\n  M. Botvinick", "title": "Psychlab: A Psychology Laboratory for Deep Reinforcement Learning Agents", "comments": "28 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Psychlab is a simulated psychology laboratory inside the first-person 3D game\nworld of DeepMind Lab (Beattie et al. 2016). Psychlab enables implementations\nof classical laboratory psychological experiments so that they work with both\nhuman and artificial agents. Psychlab has a simple and flexible API that\nenables users to easily create their own tasks. As examples, we are releasing\nPsychlab implementations of several classical experimental paradigms including\nvisual search, change detection, random dot motion discrimination, and multiple\nobject tracking. We also contribute a study of the visual psychophysics of a\nspecific state-of-the-art deep reinforcement learning agent: UNREAL (Jaderberg\net al. 2016). This study leads to the surprising conclusion that UNREAL learns\nmore quickly about larger target stimuli than it does about smaller stimuli. In\nturn, this insight motivates a specific improvement in the form of a simple\nmodel of foveal vision that turns out to significantly boost UNREAL's\nperformance, both on Psychlab tasks, and on standard DeepMind Lab tasks. By\nopen-sourcing Psychlab we hope to facilitate a range of future such studies\nthat simultaneously advance deep reinforcement learning and improve its links\nwith cognitive science.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 18:31:51 GMT"}, {"version": "v2", "created": "Sun, 4 Feb 2018 19:29:12 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Leibo", "Joel Z.", ""], ["d'Autume", "Cyprien de Masson", ""], ["Zoran", "Daniel", ""], ["Amos", "David", ""], ["Beattie", "Charles", ""], ["Anderson", "Keith", ""], ["Casta\u00f1eda", "Antonio Garc\u00eda", ""], ["Sanchez", "Manuel", ""], ["Green", "Simon", ""], ["Gruslys", "Audrunas", ""], ["Legg", "Shane", ""], ["Hassabis", "Demis", ""], ["Botvinick", "Matthew M.", ""]]}, {"id": "1801.08175", "submitter": "Colm V. Gallagher", "authors": "Colm V. Gallagher, Kevin Leahy, Peter O'Donovan, Ken Bruton, Dominic\n  T.J. O'Sullivan", "title": "Development and application of a machine learning supported methodology\n  for measurement and verification (M&V) 2.0", "comments": "17 pages. Pre-print submitted to Energy and Buildings. This\n  manuscript version is made available under the CC-BY-NC-ND 4.0 licence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The foundations of all methodologies for the measurement and verification\n(M&V) of energy savings are based on the same five key principles: accuracy,\ncompleteness, conservatism, consistency and transparency. The most widely\naccepted methodologies tend to generalise M&V so as to ensure applicability\nacross the spectrum of energy conservation measures (ECM's). These do not\nprovide a rigid calculation procedure to follow. This paper aims to bridge the\ngap between high-level methodologies and the practical application of modelling\nalgorithms, with a focus on the industrial buildings sector. This is achieved\nwith the development of a novel, machine learning supported methodology for M&V\n2.0 which enables accurate quantification of savings.\n  A novel and computationally efficient feature selection algorithm and\npowerful machine learning regression algorithms are employed to maximise the\neffectiveness of available data. The baseline period energy consumption is\nmodelled using artificial neural networks, support vector machines, k-nearest\nneighbours and multiple ordinary least squares regression. Improved knowledge\ndiscovery and an expanded boundary of analysis allow more complex energy\nsystems be analysed, thus increasing the applicability of M&V. A case study in\na large biomedical manufacturing facility is used to demonstrate the\nmethodology's ability to accurately quantify the savings under real-world\nconditions. The ECM was found to result in 604,527 kWh of energy savings with\n57% uncertainty at a confidence interval of 68%. 20 baseline energy models are\ndeveloped using an exhaustive approach with the optimal model being used to\nquantify savings. The range of savings estimated with each model are presented\nand the acceptability of uncertainty is reviewed. The case study demonstrates\nthe ability of the methodology to perform M&V to an acceptable standard in\nchallenging circumstances.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 20:16:26 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Gallagher", "Colm V.", ""], ["Leahy", "Kevin", ""], ["O'Donovan", "Peter", ""], ["Bruton", "Ken", ""], ["O'Sullivan", "Dominic T. J.", ""]]}, {"id": "1801.08186", "submitter": "Licheng Yu", "authors": "Licheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal,\n  Tamara L.Berg", "title": "MAttNet: Modular Attention Network for Referring Expression\n  Comprehension", "comments": "Equation of word attention fixed; MAttNet+Grabcut results added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address referring expression comprehension: localizing an\nimage region described by a natural language expression. While most recent work\ntreats expressions as a single unit, we propose to decompose them into three\nmodular components related to subject appearance, location, and relationship to\nother objects. This allows us to flexibly adapt to expressions containing\ndifferent types of information in an end-to-end framework. In our model, which\nwe call the Modular Attention Network (MAttNet), two types of attention are\nutilized: language-based attention that learns the module weights as well as\nthe word/phrase attention that each module should focus on; and visual\nattention that allows the subject and relationship modules to focus on relevant\nimage components. Module weights combine scores from all three modules\ndynamically to output an overall score. Experiments show that MAttNet\noutperforms previous state-of-art methods by a large margin on both\nbounding-box-level and pixel-level comprehension tasks. Demo and code are\nprovided.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 20:54:26 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 15:40:51 GMT"}, {"version": "v3", "created": "Tue, 27 Mar 2018 02:45:55 GMT"}], "update_date": "2018-03-28", "authors_parsed": [["Yu", "Licheng", ""], ["Lin", "Zhe", ""], ["Shen", "Xiaohui", ""], ["Yang", "Jimei", ""], ["Lu", "Xin", ""], ["Bansal", "Mohit", ""], ["Berg", "Tamara L.", ""]]}, {"id": "1801.08212", "submitter": "Sergio Miguel Tome", "authors": "Sergio Miguel Tom\\'e", "title": "Multi-optional Many-sorted Past Present Future structures and its\n  description", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cognitive theory of true conditions (CTTC) is a proposal to describe the\nmodel-theoretic semantics of symbolic cognitive architectures and design the\nimplementation of cognitive abilities. The CTTC is formulated mathematically\nusing the multi-optional many-sorted past present future(MMPPF) structures.\nThis article defines mathematically the MMPPF structures and the formal\nlanguages proposed to describe them by the CTTC.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 21:59:15 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Tom\u00e9", "Sergio Miguel", ""]]}, {"id": "1801.08214", "submitter": "Devendra Singh Chaplot", "authors": "Devendra Singh Chaplot, Emilio Parisotto, Ruslan Salakhutdinov", "title": "Active Neural Localization", "comments": "Under Review at ICLR-18, 15 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Localization is the problem of estimating the location of an autonomous agent\nfrom an observation and a map of the environment. Traditional methods of\nlocalization, which filter the belief based on the observations, are\nsub-optimal in the number of steps required, as they do not decide the actions\ntaken by the agent. We propose \"Active Neural Localizer\", a fully\ndifferentiable neural network that learns to localize accurately and\nefficiently. The proposed model incorporates ideas of traditional\nfiltering-based localization methods, by using a structured belief of the state\nwith multiplicative interactions to propagate belief, and combines it with a\npolicy model to localize accurately while minimizing the number of steps\nrequired for localization. Active Neural Localizer is trained end-to-end with\nreinforcement learning. We use a variety of simulation environments for our\nexperiments which include random 2D mazes, random mazes in the Doom game engine\nand a photo-realistic environment in the Unreal game engine. The results on the\n2D environments show the effectiveness of the learned policy in an idealistic\nsetting while results on the 3D environments demonstrate the model's capability\nof learning the policy and perceptual model jointly from raw-pixel based RGB\nobservations. We also show that a model trained on random textures in the Doom\nenvironment generalizes well to a photo-realistic office space environment in\nthe Unreal engine.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jan 2018 22:06:55 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Chaplot", "Devendra Singh", ""], ["Parisotto", "Emilio", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1801.08287", "submitter": "Craig Sherstan", "authors": "Craig Sherstan, Brendan Bennett, Kenny Young, Dylan R. Ashley, Adam\n  White, Martha White, Richard S. Sutton", "title": "Directly Estimating the Variance of the {\\lambda}-Return Using\n  Temporal-Difference Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates estimating the variance of a temporal-difference\nlearning agent's update target. Most reinforcement learning methods use an\nestimate of the value function, which captures how good it is for the agent to\nbe in a particular state and is mathematically expressed as the expected sum of\ndiscounted future rewards (called the return). These values can be\nstraightforwardly estimated by averaging batches of returns using Monte Carlo\nmethods. However, if we wish to update the agent's value estimates during\nlearning--before terminal outcomes are observed--we must use a different\nestimation target called the {\\lambda}-return, which truncates the return with\nthe agent's own estimate of the value function. Temporal difference learning\nmethods estimate the expected {\\lambda}-return for each state, allowing these\nmethods to update online and incrementally, and in most cases achieve better\ngeneralization error and faster learning than Monte Carlo methods. Naturally\none could attempt to estimate higher-order moments of the {\\lambda}-return.\nThis paper is about estimating the variance of the {\\lambda}-return. Prior work\nhas shown that given estimates of the variance of the {\\lambda}-return,\nlearning systems can be constructed to (1) mitigate risk in action selection,\nand (2) automatically adapt the parameters of the learning process itself to\nimprove performance. Unfortunately, existing methods for estimating the\nvariance of the {\\lambda}-return are complex and not well understood\nempirically. We contribute a method for estimating the variance of the\n{\\lambda}-return directly using policy evaluation methods from reinforcement\nlearning. Our approach is significantly simpler than prior methods that\nindependently estimate the second moment of the {\\lambda}-return. Empirically\nour new approach behaves at least as well as existing approaches, but is\ngenerally more robust.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 06:48:14 GMT"}, {"version": "v2", "created": "Wed, 14 Feb 2018 17:00:05 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Sherstan", "Craig", ""], ["Bennett", "Brendan", ""], ["Young", "Kenny", ""], ["Ashley", "Dylan R.", ""], ["White", "Adam", ""], ["White", "Martha", ""], ["Sutton", "Richard S.", ""]]}, {"id": "1801.08295", "submitter": "Kui Yu", "authors": "Kui Yu, Lin Liu, Jiuyong Li", "title": "Discovering Markov Blanket from Multiple interventional Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of discovering the Markov blanket (MB) of\na target variable from multiple interventional datasets. Datasets attained from\ninterventional experiments contain richer causal information than passively\nobserved data (observational data) for MB discovery. However, almost all\nexisting MB discovery methods are designed for finding MBs from a single\nobservational dataset. To identify MBs from multiple interventional datasets,\nwe face two challenges: (1) unknown intervention variables; (2) nonidentical\ndata distributions. To tackle the challenges, we theoretically analyze (a)\nunder what conditions we can find the correct MB of a target variable, and (b)\nunder what conditions we can identify the causes of the target variable via\ndiscovering its MB. Based on the theoretical analysis, we propose a new\nalgorithm for discovering MBs from multiple interventional datasets, and\npresent the conditions/assumptions which assure the correctness of the\nalgorithm. To our knowledge, this work is the first to present the theoretical\nanalyses about the conditions for MB discovery in multiple interventional\ndatasets and the algorithm to find the MBs in relation to the conditions. Using\nbenchmark Bayesian networks and real-world datasets, the experiments have\nvalidated the effectiveness and efficiency of the proposed algorithm in the\npaper.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 07:34:41 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Yu", "Kui", ""], ["Liu", "Lin", ""], ["Li", "Jiuyong", ""]]}, {"id": "1801.08365", "submitter": "Vaishak Belle", "authors": "Vaishak Belle", "title": "Probabilistic Planning by Probabilistic Programming", "comments": "Article at AAAI-18 Workshop on Planning and Inference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated planning is a major topic of research in artificial intelligence,\nand enjoys a long and distinguished history. The classical paradigm assumes a\ndistinguished initial state, comprised of a set of facts, and is defined over a\nset of actions which change that state in one way or another. Planning in many\nreal-world settings, however, is much more involved: an agent's knowledge is\nalmost never simply a set of facts that are true, and actions that the agent\nintends to execute never operate the way they are supposed to. Thus,\nprobabilistic planning attempts to incorporate stochastic models directly into\nthe planning process. In this article, we briefly report on probabilistic\nplanning through the lens of probabilistic programming: a programming paradigm\nthat aims to ease the specification of structured probability distributions. In\nparticular, we provide an overview of the features of two systems, HYPE and\nALLEGRO, which emphasise different strengths of probabilistic programming that\nare particularly useful for complex modelling issues raised in probabilistic\nplanning. Among other things, with these systems, one can instantiate planning\nproblems with growing and shrinking state spaces, discrete and continuous\nprobability distributions, and non-unique prior distributions in a first-order\nsetting.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 11:47:42 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Belle", "Vaishak", ""]]}, {"id": "1801.08459", "submitter": "Jihyung Moon", "authors": "Jihyung Moon, Hyochang Yang, Sungzoon Cho", "title": "Finding ReMO (Related Memory Object): A Simple Neural Architecture for\n  Text based Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To solve the text-based question and answering task that requires relational\nreasoning, it is necessary to memorize a large amount of information and find\nout the question relevant information from the memory. Most approaches were\nbased on external memory and four components proposed by Memory Network. The\ndistinctive component among them was the way of finding the necessary\ninformation and it contributes to the performance. Recently, a simple but\npowerful neural network module for reasoning called Relation Network (RN) has\nbeen introduced. We analyzed RN from the view of Memory Network, and realized\nthat its MLP component is able to reveal the complicate relation between\nquestion and object pair. Motivated from it, we introduce which uses MLP to\nfind out relevant information on Memory Network architecture. It shows new\nstate-of-the-art results in jointly trained bAbI-10k story-based question\nanswering tasks and bAbI dialog-based question answering tasks.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 15:50:44 GMT"}, {"version": "v2", "created": "Fri, 26 Jan 2018 03:47:53 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Moon", "Jihyung", ""], ["Yang", "Hyochang", ""], ["Cho", "Sungzoon", ""]]}, {"id": "1801.08573", "submitter": "Weijian Zhang", "authors": "Weijian Zhang, Jonathan Deakin, Nicholas J. Higham, Shuaiqiang Wang", "title": "Etymo: A New Discovery Engine for AI Research", "comments": "7 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present Etymo (https://etymo.io), a discovery engine to facilitate\nartificial intelligence (AI) research and development. It aims to help readers\nnavigate a large number of AI-related papers published every week by using a\nnovel form of search that finds relevant papers and displays related papers in\na graphical interface. Etymo constructs and maintains an adaptive\nsimilarity-based network of research papers as an all-purpose knowledge graph\nfor ranking, recommendation, and visualisation. The network is constantly\nevolving and can learn from user feedback to adjust itself.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 19:22:54 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Zhang", "Weijian", ""], ["Deakin", "Jonathan", ""], ["Higham", "Nicholas J.", ""], ["Wang", "Shuaiqiang", ""]]}, {"id": "1801.08577", "submitter": "Jayanta Dutta", "authors": "Jayanta K Dutta, Jiayi Liu, Unmesh Kurup and Mohak Shah", "title": "Effective Building Block Design for Deep Convolutional Neural Networks\n  using Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has shown promising results on many machine learning tasks but\nDL models are often complex networks with large number of neurons and layers,\nand recently, complex layer structures known as building blocks. Finding the\nbest deep model requires a combination of finding both the right architecture\nand the correct set of parameters appropriate for that architecture. In\naddition, this complexity (in terms of layer types, number of neurons, and\nnumber of layers) also present problems with generalization since larger\nnetworks are easier to overfit to the data. In this paper, we propose a search\nframework for finding effective architectural building blocks for convolutional\nneural networks (CNN). Our approach is much faster at finding models that are\nclose to state-of-the-art in performance. In addition, the models discovered by\nour approach are also smaller than models discovered by similar techniques. We\nachieve these twin advantages by designing our search space in such a way that\nit searches over a reduced set of state-of-the-art building blocks for CNNs\nincluding residual block, inception block, inception-residual block, ResNeXt\nblock and many others. We apply this technique to generate models for multiple\nimage datasets and show that these models achieve performance comparable to\nstate-of-the-art (and even surpassing the state-of-the-art in one case). We\nalso show that learned models are transferable between datasets.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 19:40:44 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Dutta", "Jayanta K", ""], ["Liu", "Jiayi", ""], ["Kurup", "Unmesh", ""], ["Shah", "Mohak", ""]]}, {"id": "1801.08618", "submitter": "Amir Erfan Eshratifar", "authors": "Amir Erfan Eshratifar, Mohammad Saeed Abrishami, Massoud Pedram", "title": "JointDNN: An Efficient Training and Inference Engine for Intelligent\n  Mobile Cloud Computing Services", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning models are being deployed in many mobile intelligent\napplications. End-side services, such as intelligent personal assistants,\nautonomous cars, and smart home services often employ either simple local\nmodels on the mobile or complex remote models on the cloud. However, recent\nstudies have shown that partitioning the DNN computations between the mobile\nand cloud can increase the latency and energy efficiencies. In this paper, we\npropose an efficient, adaptive, and practical engine, JointDNN, for\ncollaborative computation between a mobile device and cloud for DNNs in both\ninference and training phase. JointDNN not only provides an energy and\nperformance efficient method of querying DNNs for the mobile side but also\nbenefits the cloud server by reducing the amount of its workload and\ncommunications compared to the cloud-only approach. Given the DNN architecture,\nwe investigate the efficiency of processing some layers on the mobile device\nand some layers on the cloud server. We provide optimization formulations at\nlayer granularity for forward- and backward-propagations in DNNs, which can\nadapt to mobile battery limitations and cloud server load constraints and\nquality of service. JointDNN achieves up to 18 and 32 times reductions on the\nlatency and mobile energy consumption of querying DNNs compared to the\nstatus-quo approaches, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 22:20:11 GMT"}, {"version": "v2", "created": "Tue, 4 Feb 2020 20:53:08 GMT"}], "update_date": "2020-02-06", "authors_parsed": [["Eshratifar", "Amir Erfan", ""], ["Abrishami", "Mohammad Saeed", ""], ["Pedram", "Massoud", ""]]}, {"id": "1801.08641", "submitter": "Kien Do", "authors": "Kien Do, Truyen Tran, Svetha Venkatesh", "title": "Knowledge Graph Embedding with Multiple Relation Projections", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graphs contain rich relational structures of the world, and thus\ncomplement data-driven machine learning in heterogeneous data. One of the most\neffective methods in representing knowledge graphs is to embed symbolic\nrelations and entities into continuous spaces, where relations are\napproximately linear translation between projected images of entities in the\nrelation space. However, state-of-the-art relation projection methods such as\nTransR, TransD or TransSparse do not model the correlation between relations,\nand thus are not scalable to complex knowledge graphs with thousands of\nrelations, both in computational demand and in statistical robustness. To this\nend we introduce TransF, a novel translation-based method which mitigates the\nburden of relation projection by explicitly modeling the basis subspaces of\nprojection matrices. As a result, TransF is far more light weight than the\nexisting projection methods, and is robust when facing a high number of\nrelations. Experimental results on the canonical link prediction task show that\nour proposed model outperforms competing rivals by a large margin and achieves\nstate-of-the-art performance. Especially, TransF improves by 9%/5% in the\nhead/tail entity prediction task for N-to-1/1-to-N relations over the best\nperforming translation-based method.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jan 2018 00:28:51 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Do", "Kien", ""], ["Tran", "Truyen", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1801.08650", "submitter": "Chang-Shing Lee", "authors": "Chang-Shing Lee, Mei-Hui Wang, Tzong-Xiang Huang, Li-Chung Chen,\n  Yung-Ching Huang, Sheng-Chi Yang, Chien-Hsun Tseng, Pi-Hsia Hung, and Naoyuki\n  Kubota", "title": "Ontology-based Fuzzy Markup Language Agent for Student and Robot\n  Co-Learning", "comments": "This paper is submitted to IEEE WCCI 2018 Conference for review", "journal-ref": null, "doi": "10.1109/FUZZ-IEEE.2018.8491610", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An intelligent robot agent based on domain ontology, machine learning\nmechanism, and Fuzzy Markup Language (FML) for students and robot co-learning\nis presented in this paper. The machine-human co-learning model is established\nto help various students learn the mathematical concepts based on their\nlearning ability and performance. Meanwhile, the robot acts as a teacher's\nassistant to co-learn with children in the class. The FML-based knowledge base\nand rule base are embedded in the robot so that the teachers can get feedback\nfrom the robot on whether students make progress or not. Next, we inferred\nstudents' learning performance based on learning content's difficulty and\nstudents' ability, concentration level, as well as teamwork sprit in the class.\nExperimental results show that learning with the robot is helpful for\ndisadvantaged and below-basic children. Moreover, the accuracy of the\nintelligent FML-based agent for student learning is increased after machine\nlearning mechanism.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jan 2018 02:04:01 GMT"}], "update_date": "2019-04-15", "authors_parsed": [["Lee", "Chang-Shing", ""], ["Wang", "Mei-Hui", ""], ["Huang", "Tzong-Xiang", ""], ["Chen", "Li-Chung", ""], ["Huang", "Yung-Ching", ""], ["Yang", "Sheng-Chi", ""], ["Tseng", "Chien-Hsun", ""], ["Hung", "Pi-Hsia", ""], ["Kubota", "Naoyuki", ""]]}, {"id": "1801.08757", "submitter": "Gal Dalal", "authors": "Gal Dalal, Krishnamurthy Dvijotham, Matej Vecerik, Todd Hester, Cosmin\n  Paduraru, Yuval Tassa", "title": "Safe Exploration in Continuous Action Spaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of deploying a reinforcement learning (RL) agent on a\nphysical system such as a datacenter cooling unit or robot, where critical\nconstraints must never be violated. We show how to exploit the typically smooth\ndynamics of these systems and enable RL algorithms to never violate constraints\nduring learning. Our technique is to directly add to the policy a safety layer\nthat analytically solves an action correction formulation per each state. The\nnovelty of obtaining an elegant closed-form solution is attained due to a\nlinearized model, learned on past trajectories consisting of arbitrary actions.\nThis is to mimic the real-world circumstances where data logs were generated\nwith a behavior policy that is implausible to describe mathematically; such\ncases render the known safety-aware off-policy methods inapplicable. We\ndemonstrate the efficacy of our approach on new representative physics-based\nenvironments, and prevail where reward shaping fails by maintaining zero\nconstraint violations.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jan 2018 11:11:18 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Dalal", "Gal", ""], ["Dvijotham", "Krishnamurthy", ""], ["Vecerik", "Matej", ""], ["Hester", "Todd", ""], ["Paduraru", "Cosmin", ""], ["Tassa", "Yuval", ""]]}, {"id": "1801.08829", "submitter": "Tadahiro Taniguchi", "authors": "Tadahiro Taniguchi, Emre Ugur, Matej Hoffmann, Lorenzo Jamone,\n  Takayuki Nagai, Benjamin Rosman, Toshihiko Matsuka, Naoto Iwahashi, Erhan\n  Oztop, Justus Piater, Florentin W\\\"org\\\"otter", "title": "Symbol Emergence in Cognitive Developmental Systems: a Survey", "comments": "23 pages, 6 figures. Submitted to IEEE Transactions on Cognitive and\n  Developmental Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans use signs, e.g., sentences in a spoken language, for communication and\nthought. Hence, symbol systems like language are crucial for our communication\nwith other agents and adaptation to our real-world environment. The symbol\nsystems we use in our human society adaptively and dynamically change over\ntime. In the context of artificial intelligence (AI) and cognitive systems, the\nsymbol grounding problem has been regarded as one of the central problems\nrelated to {\\it symbols}. However, the symbol grounding problem was originally\nposed to connect symbolic AI and sensorimotor information and did not consider\nmany interdisciplinary phenomena in human communication and dynamic symbol\nsystems in our society, which semiotics considered. In this paper, we focus on\nthe symbol emergence problem, addressing not only cognitive dynamics but also\nthe dynamics of symbol systems in society, rather than the symbol grounding\nproblem. We first introduce the notion of a symbol in semiotics from the\nhumanities, to leave the very narrow idea of symbols in symbolic AI.\nFurthermore, over the years, it became more and more clear that symbol\nemergence has to be regarded as a multifaceted problem. Therefore, secondly, we\nreview the history of the symbol emergence problem in different fields,\nincluding both biological and artificial systems, showing their mutual\nrelations. We summarize the discussion and provide an integrative viewpoint and\ncomprehensive overview of symbol emergence in cognitive systems. Additionally,\nwe describe the challenges facing the creation of cognitive systems that can be\npart of symbol emergence systems.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jan 2018 14:40:58 GMT"}, {"version": "v2", "created": "Wed, 11 Jul 2018 00:13:51 GMT"}], "update_date": "2018-07-12", "authors_parsed": [["Taniguchi", "Tadahiro", ""], ["Ugur", "Emre", ""], ["Hoffmann", "Matej", ""], ["Jamone", "Lorenzo", ""], ["Nagai", "Takayuki", ""], ["Rosman", "Benjamin", ""], ["Matsuka", "Toshihiko", ""], ["Iwahashi", "Naoto", ""], ["Oztop", "Erhan", ""], ["Piater", "Justus", ""], ["W\u00f6rg\u00f6tter", "Florentin", ""]]}, {"id": "1801.08841", "submitter": "Per-Arne Andersen", "authors": "Per-Arne Andersen, Morten Goodwin, Ole-Christoffer Granmo", "title": "FlashRL: A Reinforcement Learning Platform for Flash Games", "comments": "12 Pages, Proceedings of the 30th Norwegian Informatics Conference,\n  Oslo, Norway 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement Learning (RL) is a research area that has blossomed\ntremendously in recent years and has shown remarkable potential in among others\nsuccessfully playing computer games. However, there only exists a few game\nplatforms that provide diversity in tasks and state-space needed to advance RL\nalgorithms. The existing platforms offer RL access to Atari- and a few\nweb-based games, but no platform fully expose access to Flash games. This is\nunfortunate because applying RL to Flash games have potential to push the\nresearch of RL algorithms.\n  This paper introduces the Flash Reinforcement Learning platform (FlashRL)\nwhich attempts to fill this gap by providing an environment for thousands of\nFlash games on a novel platform for Flash automation. It opens up easy\nexperimentation with RL algorithms for Flash games, which has previously been\nchallenging. The platform shows excellent performance with as little as 5% CPU\nutilization on consumer hardware. It shows promising results for novel\nreinforcement learning algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jan 2018 15:12:31 GMT"}], "update_date": "2018-01-29", "authors_parsed": [["Andersen", "Per-Arne", ""], ["Goodwin", "Morten", ""], ["Granmo", "Ole-Christoffer", ""]]}, {"id": "1801.09061", "submitter": "Nick Bassiliades", "authors": "Nick Bassiliades", "title": "SWRL2SPIN: A tool for transforming SWRL rule bases in OWL ontologies to\n  object-oriented SPIN rules", "comments": "arXiv admin note: This version has been removed by arXiv\n  administrators due to copyright infringement. Previous versions available\n  arXiv:1801.09061v2 and arXiv:1801.09061v1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic Web Rule Language (SWRL) combines OWL (Web Ontology Language)\nontologies with Horn Logic rules of the Rule Markup Language (RuleML) family.\nBeing supported by ontology editors, rule engines and ontology reasoners, it\nhas become a very popular choice for developing rule-based applications on top\nof ontologies. However, SWRL is probably not go-ing to become a WWW Consortium\nstandard, prohibiting industrial acceptance. On the other hand, SPIN (SPARQL\nInferencing Notation) has become a de-facto industry standard to rep-resent\nSPARQL rules and constraints on Semantic Web models, building on the widespread\nacceptance of SPARQL (SPARQL Protocol and RDF Query Language). In this paper,\nwe ar-gue that the life of existing SWRL rule-based ontology applications can\nbe prolonged by con-verting them to SPIN. To this end, we have developed the\nSWRL2SPIN tool in Prolog that transforms SWRL rules into SPIN rules,\nconsidering the object-orientation of SPIN, i.e. linking rules to the\nappropriate ontology classes and optimizing them, as derived by analysing the\nrule conditions.\n", "versions": [{"version": "v1", "created": "Sat, 27 Jan 2018 09:36:22 GMT"}, {"version": "v2", "created": "Sat, 3 Feb 2018 09:33:33 GMT"}, {"version": "v3", "created": "Tue, 4 Dec 2018 07:31:21 GMT"}], "update_date": "2019-09-13", "authors_parsed": [["Bassiliades", "Nick", ""]]}, {"id": "1801.09251", "submitter": "Yi Tay", "authors": "Yi Tay, Luu Anh Tuan, Siu Cheung Hui", "title": "Multi-Pointer Co-Attention Networks for Recommendation", "comments": "Accepted to KDD 2018 (Research Track)", "journal-ref": null, "doi": "10.1145/3219819.3220086", "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recent state-of-the-art recommender systems such as D-ATT, TransNet and\nDeepCoNN exploit reviews for representation learning. This paper proposes a new\nneural architecture for recommendation with reviews. Our model operates on a\nmulti-hierarchical paradigm and is based on the intuition that not all reviews\nare created equal, i.e., only a select few are important. The importance,\nhowever, should be dynamically inferred depending on the current target. To\nthis end, we propose a review-by-review pointer-based learning scheme that\nextracts important reviews, subsequently matching them in a word-by-word\nfashion. This enables not only the most informative reviews to be utilized for\nprediction but also a deeper word-level interaction. Our pointer-based method\noperates with a novel gumbel-softmax based pointer mechanism that enables the\nincorporation of discrete vectors within differentiable neural architectures.\nOur pointer mechanism is co-attentive in nature, learning pointers which are\nco-dependent on user-item relationships. Finally, we propose a multi-pointer\nlearning scheme that learns to combine multiple views of interactions between\nuser and item. Overall, we demonstrate the effectiveness of our proposed model\nvia extensive experiments on \\textbf{24} benchmark datasets from Amazon and\nYelp. Empirical results show that our approach significantly outperforms\nexisting state-of-the-art, with up to 19% and 71% relative improvement when\ncompared to TransNet and DeepCoNN respectively. We study the behavior of our\nmulti-pointer learning mechanism, shedding light on evidence aggregation\npatterns in review-based recommender systems.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jan 2018 17:14:13 GMT"}, {"version": "v2", "created": "Thu, 21 Jun 2018 11:27:30 GMT"}], "update_date": "2018-06-22", "authors_parsed": [["Tay", "Yi", ""], ["Tuan", "Luu Anh", ""], ["Hui", "Siu Cheung", ""]]}, {"id": "1801.09271", "submitter": "Ying Liu", "authors": "Ning Liu and Ying Liu and Brent Logan and Zhiyuan Xu and Jian Tang and\n  Yanzhi Wang", "title": "Deep Reinforcement Learning for Dynamic Treatment Regimes on Medical\n  Registry Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the first deep reinforcement learning (DRL) framework to\nestimate the optimal Dynamic Treatment Regimes from observational medical data.\nThis framework is more flexible and adaptive for high dimensional action and\nstate spaces than existing reinforcement learning methods to model real-life\ncomplexity in heterogeneous disease progression and treatment choices, with the\ngoal of providing doctor and patients the data-driven personalized decision\nrecommendations. The proposed DRL framework comprises (i) a supervised learning\nstep to predict the most possible expert actions, and (ii) a deep reinforcement\nlearning step to estimate the long-term value function of Dynamic Treatment\nRegimes. Both steps depend on deep neural networks.\n  As a key motivational example, we have implemented the proposed framework on\na data set from the Center for International Bone Marrow Transplant Research\n(CIBMTR) registry database, focusing on the sequence of prevention and\ntreatments for acute and chronic graft versus host disease after\ntransplantation. In the experimental results, we have demonstrated promising\naccuracy in predicting human experts' decisions, as well as the high expected\nreward function in the DRL-based dynamic treatment regimes.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jan 2018 19:29:50 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Liu", "Ning", ""], ["Liu", "Ying", ""], ["Logan", "Brent", ""], ["Xu", "Zhiyuan", ""], ["Tang", "Jian", ""], ["Wang", "Yanzhi", ""]]}, {"id": "1801.09303", "submitter": "Nesreen Ahmed", "authors": "Ryan A. Rossi, Nesreen K. Ahmed, Eunyee Koh, Sungchul Kim, Anup Rao,\n  Yasin Abbasi Yadkori", "title": "HONE: Higher-Order Network Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a general framework for learning Higher-Order Network\nEmbeddings (HONE) from graph data based on network motifs. The HONE framework\nis highly expressive and flexible with many interchangeable components. The\nexperimental results demonstrate the effectiveness of learning higher-order\nnetwork representations. In all cases, HONE outperforms recent embedding\nmethods that are unable to capture higher-order structures with a mean relative\ngain in AUC of $19\\%$ (and up to $75\\%$ gain) across a wide variety of networks\nand embedding methods.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jan 2018 21:59:49 GMT"}, {"version": "v2", "created": "Wed, 30 May 2018 01:34:46 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Rossi", "Ryan A.", ""], ["Ahmed", "Nesreen K.", ""], ["Koh", "Eunyee", ""], ["Kim", "Sungchul", ""], ["Rao", "Anup", ""], ["Yadkori", "Yasin Abbasi", ""]]}, {"id": "1801.09317", "submitter": "Jason Pittman", "authors": "Jason M. Pittman and Courtney E. Soboleski", "title": "A Cyber Science Based Ontology for Artificial General Intelligence\n  Containment", "comments": "12 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of artificial general intelligence is considered by many to\nbe inevitable. What such intelligence does after becoming aware is not so\ncertain. To that end, research suggests that the likelihood of artificial\ngeneral intelligence becoming hostile to humans is significant enough to\nwarrant inquiry into methods to limit such potential. Thus, containment of\nartificial general intelligence is a timely and meaningful research topic.\nWhile there is limited research exploring possible containment strategies, such\nwork is bounded by the underlying field the strategies draw upon. Accordingly,\nwe set out to construct an ontology to describe necessary elements in any\nfuture containment technology. Using existing academic literature, we developed\na single domain ontology containing five levels, 32 codes, and 32 associated\ndescriptors. Further, we constructed ontology diagrams to demonstrate intended\nrelationships. We then identified humans, AGI, and the cyber world as novel\nagent objects necessary for future containment activities. Collectively, the\nwork addresses three critical gaps: (a) identifying and arranging fundamental\nconstructs; (b) situating AGI containment within cyber science; and (c)\ndeveloping scientific rigor within the field.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jan 2018 23:45:17 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Pittman", "Jason M.", ""], ["Soboleski", "Courtney E.", ""]]}, {"id": "1801.09343", "submitter": "Vishwanath Saragadam Raja Venkata", "authors": "Vishwanath Saragadam, Aswin C. Sankaranarayanan", "title": "KRISM --- Krylov Subspace-based Optical Computing of Hyperspectral\n  Images", "comments": "14 pages of main paper and 15 pages of supplementary material", "journal-ref": "Vishwanath Saragadam and Aswin C. Sankaranarayanan, \"KRISM ---\n  Krylov Subspace-based Optical Computing of Hyperspectral Images\", ACM Trans.\n  Graphics 38, 5 (2019), 148:1-14", "doi": "10.1145/3345553", "report-no": null, "categories": "eess.IV cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an adaptive imaging technique that optically computes a low-rank\napproximation of a scene's hyperspectral image, conceptualized as a matrix.\nCentral to the proposed technique is the optical implementation of two\nmeasurement operators: a spectrally-coded imager and a spatially-coded\nspectrometer. By iterating between the two operators, we show that the top\nsingular vectors and singular values of a hyperspectral image can be adaptively\nand optically computed with only a few iterations. We present an optical design\nthat uses pupil plane coding for implementing the two operations and show\nseveral compelling results using a lab prototype to demonstrate the\neffectiveness of the proposed hyperspectral imager.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jan 2018 12:10:38 GMT"}, {"version": "v2", "created": "Thu, 19 Jul 2018 03:11:34 GMT"}, {"version": "v3", "created": "Thu, 23 May 2019 05:38:39 GMT"}, {"version": "v4", "created": "Mon, 21 Oct 2019 18:17:40 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Saragadam", "Vishwanath", ""], ["Sankaranarayanan", "Aswin C.", ""]]}, {"id": "1801.09346", "submitter": "Barton Lee", "authors": "Barton E. Lee", "title": "Representing the Insincere: Strategically Robust Proportional\n  Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proportional representation (PR) is a fundamental principle of many\ndemocracies world-wide which employ PR-based voting rules to elect their\nrepresentatives. The normative properties of these voting rules however, are\noften only understood in the context of sincere voting.\n  In this paper we consider PR in the presence of strategic voters. We\nconstruct a voting rule such that for every preference profile there exists at\nleast one costly voting equilibrium satisfying PR with respect to voters'\nprivate and unrevealed preferences - such a voting rule is said to be\nstrategically robust. In contrast, a commonly applied voting rule is shown not\nbe strategically robust. Furthermore, we prove a limit on `how strategically\nrobust' a PR-based voting rule can be; we show that there is no PR-based voting\nrule which ensures that every equilibrium satisfies PR. Collectively, our\nresults highlight the possibility and limit of achieving PR in the presence of\nstrategic voters and a positive role for mechanisms, such as pre-election\npolls, which coordinate voter behaviour towards equilibria which satisfy PR.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 02:35:21 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Lee", "Barton E.", ""]]}, {"id": "1801.09354", "submitter": "Nayyar Zaidi", "authors": "Nayyar A. Zaidi, Geoffrey I. Webb, Francois Petitjean, Germain\n  Forestier", "title": "On the Inter-relationships among Drift rate, Forgetting rate,\n  Bias/variance profile and Error", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose two general and falsifiable hypotheses about expectations on\ngeneralization error when learning in the context of concept drift. One posits\nthat as drift rate increases, the forgetting rate that minimizes generalization\nerror will also increase and vice versa. The other posits that as a learner's\nforgetting rate increases, the bias/variance profile that minimizes\ngeneralization error will have lower variance and vice versa. These hypotheses\nlead to the concept of the sweet path, a path through the 3-d space of\nalternative drift rates, forgetting rates and bias/variance profiles on which\ngeneralization error will be minimized, such that slow drift is coupled with\nlow forgetting and low bias, while rapid drift is coupled with fast forgetting\nand low variance. We present experiments that support the existence of such a\nsweet path. We also demonstrate that simple learners that select appropriate\nforgetting rates and bias/variance profiles are highly competitive with the\nstate-of-the-art in incremental learners for concept drift on real-world drift\nproblems.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 03:35:55 GMT"}, {"version": "v2", "created": "Sun, 4 Feb 2018 01:18:03 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Zaidi", "Nayyar A.", ""], ["Webb", "Geoffrey I.", ""], ["Petitjean", "Francois", ""], ["Forestier", "Germain", ""]]}, {"id": "1801.09356", "submitter": "Ravi Kiran Sarvadevabhatla", "authors": "Ravi Kiran Sarvadevabhatla, Shiv Surya, Trisha Mittal and Venkatesh\n  Babu Radhakrishnan", "title": "Game of Sketches: Deep Recurrent Models of Pictionary-style Word\n  Guessing", "comments": "To be presented at AAAI-2018. Code, pre-trained models and dataset at\n  github.com/val-iisc/sketchguess", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability of intelligent agents to play games in human-like fashion is\npopularly considered a benchmark of progress in Artificial Intelligence.\nSimilarly, performance on multi-disciplinary tasks such as Visual Question\nAnswering (VQA) is considered a marker for gauging progress in Computer Vision.\nIn our work, we bring games and VQA together. Specifically, we introduce the\nfirst computational model aimed at Pictionary, the popular word-guessing social\ngame. We first introduce Sketch-QA, an elementary version of Visual Question\nAnswering task. Styled after Pictionary, Sketch-QA uses incrementally\naccumulated sketch stroke sequences as visual data. Notably, Sketch-QA involves\nasking a fixed question (\"What object is being drawn?\") and gathering\nopen-ended guess-words from human guessers. We analyze the resulting dataset\nand present many interesting findings therein. To mimic Pictionary-style\nguessing, we subsequently propose a deep neural model which generates\nguess-words in response to temporally evolving human-drawn sketches. Our model\neven makes human-like mistakes while guessing, thus amplifying the human\nmimicry factor. We evaluate our model on the large-scale guess-word dataset\ngenerated via Sketch-QA task and compare with various baselines. We also\nconduct a Visual Turing Test to obtain human impressions of the guess-words\ngenerated by humans and our model. Experimental results demonstrate the promise\nof our approach for Pictionary and similarly themed games.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 03:54:03 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Sarvadevabhatla", "Ravi Kiran", ""], ["Surya", "Shiv", ""], ["Mittal", "Trisha", ""], ["Radhakrishnan", "Venkatesh Babu", ""]]}, {"id": "1801.09373", "submitter": "M.Zubair Malik", "authors": "Muhammad Zubair Malik, Muhammad Nawaz, Nimrah Mustafa, Junaid Haroon\n  Siddiqui", "title": "Search Based Code Generation for Machine Learning Programs", "comments": "Search Based Software Engineering, Generating Machine Learning Code,\n  Partial Evaluation, Futamura Projection, Sketching", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning (ML) has revamped every domain of life as it provides\npowerful tools to build complex systems that learn and improve from experience\nand data. Our key insight is that to solve a machine learning problem, data\nscientists do not invent a new algorithm each time, but evaluate a range of\nexisting models with different configurations and select the best one. This\ntask is laborious, error-prone, and drains a large chunk of project budget and\ntime. In this paper we present a novel framework inspired by programming by\nSketching and Partial Evaluation to minimize human intervention in developing\nML solutions. We templatize machine learning algorithms to expose configuration\nchoices as holes to be searched. We share code and computation between\ndifferent algorithms, and only partially evaluate configuration space of\nalgorithms based on information gained from initial algorithm evaluations. We\nalso employ hierarchical and heuristic based pruning to reduce the search\nspace. Our initial findings indicate that our approach can generate highly\naccurate ML models. Interviews with data scientists show that they feel our\nframework can eliminate sources of common errors and significantly reduce\ndevelopment time.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 06:28:47 GMT"}, {"version": "v2", "created": "Tue, 6 Feb 2018 09:55:12 GMT"}], "update_date": "2018-02-07", "authors_parsed": [["Malik", "Muhammad Zubair", ""], ["Nawaz", "Muhammad", ""], ["Mustafa", "Nimrah", ""], ["Siddiqui", "Junaid Haroon", ""]]}, {"id": "1801.09466", "submitter": "Yannis Assael", "authors": "Nikolaos D. Goumagias, Dimitrios Hristu-Varsakelis, Yannis M. Assael", "title": "Using deep Q-learning to understand the tax evasion behavior of\n  risk-averse firms", "comments": "Preprint - accepted for publication in Expert Systems with\n  Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing tax policies that are effective in curbing tax evasion and maximize\nstate revenues requires a rigorous understanding of taxpayer behavior. This\nwork explores the problem of determining the strategy a self-interested,\nrisk-averse tax entity is expected to follow, as it \"navigates\" - in the\ncontext of a Markov Decision Process - a government-controlled tax environment\nthat includes random audits, penalties and occasional tax amnesties. Although\nsimplified versions of this problem have been previously explored, the mere\nassumption of risk-aversion (as opposed to risk-neutrality) raises the\ncomplexity of finding the optimal policy well beyond the reach of analytical\ntechniques. Here, we obtain approximate solutions via a combination of\nQ-learning and recent advances in Deep Reinforcement Learning. By doing so, we\ni) determine the tax evasion behavior expected of the taxpayer entity, ii)\ncalculate the degree of risk aversion of the \"average\" entity given empirical\nestimates of tax evasion, and iii) evaluate sample tax policies, in terms of\nexpected revenues. Our model can be useful as a testbed for \"in-vitro\" testing\nof tax policies, while our results lead to various policy recommendations.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 12:09:07 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Goumagias", "Nikolaos D.", ""], ["Hristu-Varsakelis", "Dimitrios", ""], ["Assael", "Yannis M.", ""]]}, {"id": "1801.09496", "submitter": "Gaurav Singh", "authors": "Gaurav Singh, James Thomas and John Shawe-Taylor", "title": "Improving Active Learning in Systematic Reviews", "comments": "10 pages, many figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.DL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Systematic reviews are essential to summarizing the results of different\nclinical and social science studies. The first step in a systematic review task\nis to identify all the studies relevant to the review. The task of identifying\nrelevant studies for a given systematic review is usually performed manually,\nand as a result, involves substantial amounts of expensive human resource.\nLately, there have been some attempts to reduce this manual effort using active\nlearning. In this work, we build upon some such existing techniques, and\nvalidate by experimenting on a larger and comprehensive dataset than has been\nattempted until now. Our experiments provide insights on the use of different\nfeature extraction models for different disciplines. More importantly, we\nidentify that a naive active learning based screening process is biased in\nfavour of selecting similar documents. We aimed to improve the performance of\nthe screening process using a novel active learning algorithm with success.\nAdditionally, we propose a mechanism to choose the best feature extraction\nmethod for a given review.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 13:26:48 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Singh", "Gaurav", ""], ["Thomas", "James", ""], ["Shawe-Taylor", "John", ""]]}, {"id": "1801.09547", "submitter": "Ramesh Ramasamy Pandi", "authors": "Songguang Ho, Sarat Chandra Nagavarapu, Ramesh Ramasamy Pandi and\n  Justin Dauwels", "title": "An Improved Tabu Search Heuristic for Static Dial-A-Ride Problem", "comments": "Journal Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-vehicle routing has become increasingly important with the rapid\ndevelopment of autonomous vehicle technology. Dial-a-ride problem, a variant of\nvehicle routing problem (VRP), deals with the allocation of customer requests\nto vehicles, scheduling the pick-up and drop-off times and the sequence of\nserving those requests by ensuring high customer satisfaction with minimized\ntravel cost. In this paper, we propose an improved tabu search (ITS) heuristic\nfor static dial-a-ride problem (DARP) with the objective of obtaining\nhigh-quality solutions in short time. Two new techniques, initialization\nheuristic, and time window adjustment are proposed to achieve faster\nconvergence to the global optimum. Various numerical experiments are conducted\nfor the proposed solution methodology using DARP test instances from the\nliterature and the convergence speed up is validated.\n", "versions": [{"version": "v1", "created": "Thu, 25 Jan 2018 09:46:42 GMT"}, {"version": "v2", "created": "Fri, 2 Feb 2018 09:09:06 GMT"}, {"version": "v3", "created": "Mon, 12 Feb 2018 12:45:01 GMT"}, {"version": "v4", "created": "Tue, 13 Feb 2018 07:35:18 GMT"}, {"version": "v5", "created": "Wed, 14 Feb 2018 04:13:03 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Ho", "Songguang", ""], ["Nagavarapu", "Sarat Chandra", ""], ["Pandi", "Ramesh Ramasamy", ""], ["Dauwels", "Justin", ""]]}, {"id": "1801.09573", "submitter": "Enkhtogtokh Togootogtokh", "authors": "Enkhtogtokh Togootogtokh, Amarzaya Amartuvshin", "title": "Deep Learning Approach for Very Similar Objects Recognition Application\n  on Chihuahua and Muffin Problem", "comments": "8 pages,4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We address the problem to tackle the very similar objects like Chihuahua or\nmuffin problem to recognize at least in human vision level. Our regular deep\nstructured machine learning still does not solve it. We saw many times for\nabout year in our community the problem. Today we proposed the state-of-the-art\nsolution for it. Our approach is quite tricky to get the very high accuracy. We\npropose the deep transfer learning method which could be tackled all this type\nof problems not limited to just Chihuahua or muffin problem. It is the best\nmethod to train with small data set not like require huge amount data.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 15:25:49 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Togootogtokh", "Enkhtogtokh", ""], ["Amartuvshin", "Amarzaya", ""]]}, {"id": "1801.09597", "submitter": "Per-Arne Andersen", "authors": "Per-Arne Andersen", "title": "Deep Reinforcement Learning using Capsules in Advanced Game Environments", "comments": "Master Thesis in Computer Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement Learning (RL) is a research area that has blossomed\ntremendously in recent years and has shown remarkable potential for artificial\nintelligence based opponents in computer games. This success is primarily due\nto vast capabilities of Convolutional Neural Networks (ConvNet), enabling\nalgorithms to extract useful information from noisy environments. Capsule\nNetwork (CapsNet) is a recent introduction to the Deep Learning algorithm group\nand has only barely begun to be explored. The network is an architecture for\nimage classification, with superior performance for classification of the MNIST\ndataset. CapsNets have not been explored beyond image classification.\n  This thesis introduces the use of CapsNet for Q-Learning based game\nalgorithms. To successfully apply CapsNet in advanced game play, three main\ncontributions follow. First, the introduction of four new game environments as\nframeworks for RL research with increasing complexity, namely Flash RL, Deep\nLine Wars, Deep RTS, and Deep Maze. These environments fill the gap between\nrelatively simple and more complex game environments available for RL research\nand are in the thesis used to test and explore the CapsNet behavior.\n  Second, the thesis introduces a generative modeling approach to produce\nartificial training data for use in Deep Learning models including CapsNets. We\nempirically show that conditional generative modeling can successfully generate\ngame data of sufficient quality to train a Deep Q-Network well.\n  Third, we show that CapsNet is a reliable architecture for Deep Q-Learning\nbased algorithms for game AI. A capsule is a group of neurons that determine\nthe presence of objects in the data and is in the literature shown to increase\nthe robustness of training and predictions while lowering the amount training\ndata needed. It should, therefore, be ideally suited for game plays.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 16:04:25 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Andersen", "Per-Arne", ""]]}, {"id": "1801.09626", "submitter": "Bhavya Kailkhura", "authors": "Aditya Vempaty, Bhavya Kailkhura, Pramod K. Varshney", "title": "Human-Machine Inference Networks For Smart Decision Making:\n  Opportunities and Challenges", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emerging paradigm of Human-Machine Inference Networks (HuMaINs) combines\ncomplementary cognitive strengths of humans and machines in an intelligent\nmanner to tackle various inference tasks and achieves higher performance than\neither humans or machines by themselves. While inference performance\noptimization techniques for human-only or sensor-only networks are quite\nmature, HuMaINs require novel signal processing and machine learning solutions.\nIn this paper, we present an overview of the HuMaINs architecture with a focus\non three main issues that include architecture design, inference algorithms\nincluding security/privacy challenges, and application areas/use cases.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 17:03:34 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Vempaty", "Aditya", ""], ["Kailkhura", "Bhavya", ""], ["Varshney", "Pramod K.", ""]]}, {"id": "1801.09780", "submitter": "Yue Wang", "authors": "Yue Wang, Swarat Chaudhuri, Lydia E. Kavraki", "title": "Bounded Policy Synthesis for POMDPs with Safe-Reachability Objectives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Planning robust executions under uncertainty is a fundamental challenge for\nbuilding autonomous robots. Partially Observable Markov Decision Processes\n(POMDPs) provide a standard framework for modeling uncertainty in many\napplications. In this work, we study POMDPs with safe-reachability objectives,\nwhich require that with a probability above some threshold, a goal state is\neventually reached while keeping the probability of visiting unsafe states\nbelow some threshold. This POMDP formulation is different from the traditional\nPOMDP models with optimality objectives and we show that in some cases, POMDPs\nwith safe-reachability objectives can provide a better guarantee of both safety\nand reachability than the existing POMDP models through an example. A key\nalgorithmic problem for POMDPs is policy synthesis, which requires reasoning\nover a vast space of beliefs (probability distributions). To address this\nchallenge, we introduce the notion of a goal-constrained belief space, which\nonly contains beliefs reachable from the initial belief under desired\nexecutions that can achieve the given safe-reachability objective. Our method\ncompactly represents this space over a bounded horizon using symbolic\nconstraints, and employs an incremental Satisfiability Modulo Theories (SMT)\nsolver to efficiently search for a valid policy over it. We evaluate our method\nusing a case study involving a partially observable robotic domain with\nuncertain obstacles. The results show that our method can synthesize policies\nover large belief spaces with a small number of SMT solver calls by focusing on\nthe goal-constrained belief space.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 22:06:40 GMT"}, {"version": "v2", "created": "Wed, 9 May 2018 03:42:54 GMT"}], "update_date": "2018-05-10", "authors_parsed": [["Wang", "Yue", ""], ["Chaudhuri", "Swarat", ""], ["Kavraki", "Lydia E.", ""]]}, {"id": "1801.09788", "submitter": "Nataliia Ruemmele", "authors": "Natalia Ruemmele, Yuriy Tyshetskiy, Alex Collins", "title": "Evaluating approaches for supervised semantic labeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relational data sources are still one of the most popular ways to store\nenterprise or Web data, however, the issue with relational schema is the lack\nof a well-defined semantic description. A common ontology provides a way to\nrepresent the meaning of a relational schema and can facilitate the integration\nof heterogeneous data sources within a domain. Semantic labeling is achieved by\nmapping attributes from the data sources to the classes and properties in the\nontology. We formulate this problem as a multi-class classification problem\nwhere previously labeled data sources are used to learn rules for labeling new\ndata sources. The majority of existing approaches for semantic labeling have\nfocused on data integration challenges such as naming conflicts and semantic\nheterogeneity. In addition, machine learning approaches typically have issues\naround class imbalance, lack of labeled instances and relative importance of\nattributes. To address these issues, we develop a new machine learning model\nwith engineered features as well as two deep learning models which do not\nrequire extensive feature engineering. We evaluate our new approaches with the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jan 2018 22:43:32 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Ruemmele", "Natalia", ""], ["Tyshetskiy", "Yuriy", ""], ["Collins", "Alex", ""]]}, {"id": "1801.09804", "submitter": "Kyongsik Yun", "authors": "Kyongsik Yun, Jessi Bustos, Thomas Lu", "title": "Predicting Rapid Fire Growth (Flashover) Using Conditional Generative\n  Adversarial Networks", "comments": "4 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A flashover occurs when a fire spreads very rapidly through crevices due to\nintense heat. Flashovers present one of the most frightening and challenging\nfire phenomena to those who regularly encounter them: firefighters.\nFirefighters' safety and lives often depend on their ability to predict\nflashovers before they occur. Typical pre-flashover fire characteristics\ninclude dark smoke, high heat, and rollover (\"angel fingers\") and can be\nquantified by color, size, and shape. Using a color video stream from a\nfirefighter's body camera, we applied generative adversarial neural networks\nfor image enhancement. The neural networks were trained to enhance very dark\nfire and smoke patterns in videos and monitor dynamic changes in smoke and fire\nareas. Preliminary tests with limited flashover training videos showed that we\npredicted a flashover as early as 55 seconds before it occurred.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 00:09:48 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Yun", "Kyongsik", ""], ["Bustos", "Jessi", ""], ["Lu", "Thomas", ""]]}, {"id": "1801.09808", "submitter": "Maruan Al-Shedivat", "authors": "Maruan Al-Shedivat, Avinava Dubey, Eric P. Xing", "title": "The Intriguing Properties of Model Explanations", "comments": "Interpretable ML Symposium, NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear approximations to the decision boundary of a complex model have become\none of the most popular tools for interpreting predictions. In this paper, we\nstudy such linear explanations produced either post-hoc by a few recent methods\nor generated along with predictions with contextual explanation networks\n(CENs). We focus on two questions: (i) whether linear explanations are always\nconsistent or can be misleading, and (ii) when integrated into the prediction\nprocess, whether and how explanations affect the performance of the model. Our\nanalysis sheds more light on certain properties of explanations produced by\ndifferent methods and suggests that learning models that explain and predict\njointly is often advantageous.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 00:16:45 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Al-Shedivat", "Maruan", ""], ["Dubey", "Avinava", ""], ["Xing", "Eric P.", ""]]}, {"id": "1801.09810", "submitter": "Maruan Al-Shedivat", "authors": "Maruan Al-Shedivat, Avinava Dubey, Eric P. Xing", "title": "Personalized Survival Prediction with Contextual Explanation Networks", "comments": "Machine Learning for Healthcare Workshop, NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate and transparent prediction of cancer survival times on the level of\nindividual patients can inform and improve patient care and treatment\npractices. In this paper, we design a model that concurrently learns to\naccurately predict patient-specific survival distributions and to explain its\npredictions in terms of patient attributes such as clinical tests or\nassessments. Our model is flexible and based on a recurrent network, can handle\nvarious modalities of data including temporal measurements, and yet constructs\nand uses simple explanations in the form of patient- and time-specific linear\nregression. For analysis, we use two publicly available datasets and show that\nour networks outperform a number of baselines in prediction while providing a\nway to inspect the reasons behind each prediction.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 00:21:14 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Al-Shedivat", "Maruan", ""], ["Dubey", "Avinava", ""], ["Xing", "Eric P.", ""]]}, {"id": "1801.09848", "submitter": "Ardavan Salehi Nobandegani", "authors": "Ardavan S. Nobandegani, Kevin da Silva Castanheira, A. Ross Otto,\n  Thomas R. Shultz", "title": "Over-representation of Extreme Events in Decision-Making: A Rational\n  Metacognitive Account", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Availability bias, manifested in the over-representation of extreme\neventualities in decision-making, is a well-known cognitive bias, and is\ngenerally taken as evidence of human irrationality. In this work, we present\nthe first rational, metacognitive account of the Availability bias, formally\narticulated at Marr's algorithmic level of analysis. Concretely, we present a\nnormative, metacognitive model of how a cognitive system should over-represent\nextreme eventualities, depending on the amount of time available at its\ndisposal for decision-making. Our model also accounts for two well-known\nframing effects in human decision-making under risk---the fourfold pattern of\nrisk preferences in outcome probability (Tversky & Kahneman, 1992) and in\noutcome magnitude (Markovitz, 1952)---thereby providing the first\nmetacognitively-rational basis for those effects. Empirical evidence,\nfurthermore, confirms an important prediction of our model. Surprisingly, our\nmodel is unimaginably robust with respect to its focal parameter. We discuss\nthe implications of our work for studies on human decision-making, and conclude\nby presenting a counterintuitive prediction of our model, which, if confirmed,\nwould have intriguing implications for human decision-making under risk. To our\nknowledge, our model is the first metacognitive, resource-rational process\nmodel of cognitive biases in decision-making.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 04:33:25 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Nobandegani", "Ardavan S.", ""], ["Castanheira", "Kevin da Silva", ""], ["Otto", "A. Ross", ""], ["Shultz", "Thomas R.", ""]]}, {"id": "1801.09854", "submitter": "Tathagata Chakraborti", "authors": "Tathagata Chakraborti and Subbarao Kambhampati", "title": "Algorithms for the Greater Good! On Mental Modeling and Acceptable\n  Symbiosis in Human-AI Collaboration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective collaboration between humans and AI-based systems requires\neffective modeling of the human in the loop, both in terms of the mental state\nas well as the physical capabilities of the latter. However, these models can\nalso open up pathways for manipulating and exploiting the human in the hopes of\nachieving some greater good, especially when the intent or values of the AI and\nthe human are not aligned or when they have an asymmetrical relationship with\nrespect to knowledge or computation power. In fact, such behavior does not\nnecessarily require any malicious intent but can rather be borne out of\ncooperative scenarios. It is also beyond simple misinterpretation of intents,\nas in the case of value alignment problems, and thus can be effectively\nengineered if desired. Such techniques already exist and pose several\nunresolved ethical and moral questions with regards to the design of autonomy.\nIn this paper, we illustrate some of these issues in a teaming scenario and\ninvestigate how they are perceived by participants in a thought experiment.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 05:23:28 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Chakraborti", "Tathagata", ""], ["Kambhampati", "Subbarao", ""]]}, {"id": "1801.09955", "submitter": "Toon Van Craenendonck", "authors": "Toon Van Craenendonck, Sebastijan Dumancic, Hendrik Blockeel", "title": "COBRA: A Fast and Simple Method for Active Clustering with Pairwise\n  Constraints", "comments": "Presented at IJCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering is inherently ill-posed: there often exist multiple valid\nclusterings of a single dataset, and without any additional information a\nclustering system has no way of knowing which clustering it should produce.\nThis motivates the use of constraints in clustering, as they allow users to\ncommunicate their interests to the clustering system. Active constraint-based\nclustering algorithms select the most useful constraints to query, aiming to\nproduce a good clustering using as few constraints as possible. We propose\nCOBRA, an active method that first over-clusters the data by running K-means\nwith a $K$ that is intended to be too large, and subsequently merges the\nresulting small clusters into larger ones based on pairwise constraints. In its\nmerging step, COBRA is able to keep the number of pairwise queries low by\nmaximally exploiting constraint transitivity and entailment. We experimentally\nshow that COBRA outperforms the state of the art in terms of clustering quality\nand runtime, without requiring the number of clusters in advance.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 12:30:50 GMT"}], "update_date": "2018-01-31", "authors_parsed": [["Van Craenendonck", "Toon", ""], ["Dumancic", "Sebastijan", ""], ["Blockeel", "Hendrik", ""]]}, {"id": "1801.10055", "submitter": "Blai Bonet", "authors": "Blai Bonet and Hector Geffner", "title": "Features, Projections, and Representation Change for Generalized\n  Planning", "comments": "Accepted in IJCAI-18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized planning is concerned with the characterization and computation\nof plans that solve many instances at once. In the standard formulation, a\ngeneralized plan is a mapping from feature or observation histories into\nactions, assuming that the instances share a common pool of features and\nactions. This assumption, however, excludes the standard relational planning\ndomains where actions and objects change across instances. In this work, we\nextend the standard formulation of generalized planning to such domains. This\nis achieved by projecting the actions over the features, resulting in a common\nset of abstract actions which can be tested for soundness and completeness, and\nwhich can be used for generating general policies such as \"if the gripper is\nempty, pick the clear block above x and place it on the table\" that achieve the\ngoal clear(x) in any Blocksworld instance. In this policy, \"pick the clear\nblock above x\" is an abstract action that may represent the action Unstack(a,\nb) in one situation and the action Unstack(b, c) in another. Transformations\nare also introduced for computing such policies by means of fully observable\nnon-deterministic (FOND) planners. The value of generalized representations for\nlearning general policies is also discussed.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 15:32:02 GMT"}, {"version": "v2", "created": "Tue, 15 May 2018 12:12:04 GMT"}, {"version": "v3", "created": "Thu, 31 May 2018 15:43:24 GMT"}, {"version": "v4", "created": "Thu, 14 Jun 2018 13:11:19 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Bonet", "Blai", ""], ["Geffner", "Hector", ""]]}, {"id": "1801.10186", "submitter": "Ardavan Salehi Nobandegani", "authors": "Ardavan S. Nobandegani, Ioannis N. Psaromiligkos", "title": "A Rational Distributed Process-level Account of Independence Judgment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is inconceivable how chaotic the world would look to humans, faced with\ninnumerable decisions a day to be made under uncertainty, had they been lacking\nthe capacity to distinguish the relevant from the irrelevant---a capacity which\ncomputationally amounts to handling probabilistic independence relations. The\nhighly parallel and distributed computational machinery of the brain suggests\nthat a satisfying process-level account of human independence judgment should\nalso mimic these features. In this work, we present the first rational,\ndistributed, message-passing, process-level account of independence judgment,\ncalled $\\mathcal{D}^\\ast$. Interestingly, $\\mathcal{D}^\\ast$ shows a curious,\nbut normatively-justified tendency for quick detection of dependencies,\nwhenever they hold. Furthermore, $\\mathcal{D}^\\ast$ outperforms all the\npreviously proposed algorithms in the AI literature in terms of worst-case\nrunning time, and a salient aspect of it is supported by recent work in\nneuroscience investigating possible implementations of Bayes nets at the neural\nlevel. $\\mathcal{D}^\\ast$ nicely exemplifies how the pursuit of cognitive\nplausibility can lead to the discovery of state-of-the-art algorithms with\nappealing properties, and its simplicity makes $\\mathcal{D}^\\ast$ potentially a\ngood candidate for pedagogical purposes.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jan 2018 19:42:45 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Nobandegani", "Ardavan S.", ""], ["Psaromiligkos", "Ioannis N.", ""]]}, {"id": "1801.10287", "submitter": "Ajin Joseph", "authors": "Ajin George Joseph and Shalabh Bhatnagar", "title": "An Incremental Off-policy Search in a Model-free Markov Decision Process\n  Using a Single Sample Path", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a modified version of the control problem in a\nmodel free Markov decision process (MDP) setting with large state and action\nspaces. The control problem most commonly addressed in the contemporary\nliterature is to find an optimal policy which maximizes the value function,\ni.e., the long run discounted reward of the MDP. The current settings also\nassume access to a generative model of the MDP with the hidden premise that\nobservations of the system behaviour in the form of sample trajectories can be\nobtained with ease from the model. In this paper, we consider a modified\nversion, where the cost function is the expectation of a non-convex function of\nthe value function without access to the generative model. Rather, we assume\nthat a sample trajectory generated using a priori chosen behaviour policy is\nmade available. In this restricted setting, we solve the modified control\nproblem in its true sense, i.e., to find the best possible policy given this\nlimited information. We propose a stochastic approximation algorithm based on\nthe well-known cross entropy method which is data (sample trajectory)\nefficient, stable, robust as well as computationally and storage efficient. We\nprovide a proof of convergence of our algorithm to a policy which is globally\noptimal relative to the behaviour policy. We also present experimental results\nto corroborate our claims and we demonstrate the superiority of the solution\nproduced by our algorithm compared to the state-of-the-art algorithms under\nappropriately chosen behaviour policy.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 02:53:34 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Joseph", "Ajin George", ""], ["Bhatnagar", "Shalabh", ""]]}, {"id": "1801.10291", "submitter": "Ajin Joseph", "authors": "Ajin George Joseph and Shalabh Bhatnagar", "title": "A Cross Entropy based Optimization Algorithm with Global Convergence\n  Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cross entropy (CE) method is a model based search method to solve\noptimization problems where the objective function has minimal structure. The\nMonte-Carlo version of the CE method employs the naive sample averaging\ntechnique which is inefficient, both computationally and space wise. We provide\na novel stochastic approximation version of the CE method, where the sample\naveraging is replaced with incremental geometric averaging. This approach can\nsave considerable computational and storage costs. Our algorithm is incremental\nin nature and possesses additional attractive features such as accuracy,\nstability, robustness and convergence to the global optimum for a particular\nclass of objective functions. We evaluate the algorithm on a variety of global\noptimization benchmark problems and the results obtained corroborate our\ntheoretical findings.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 03:53:47 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Joseph", "Ajin George", ""], ["Bhatnagar", "Shalabh", ""]]}, {"id": "1801.10437", "submitter": "L\\^e Nguy\\^en Hoang", "authors": "L\\^e Nguy\\^en Hoang and Rachid Guerraoui", "title": "Deep Learning Works in Practice. But Does it Work in Theory?", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning relies on a very specific kind of neural networks: those\nsuperposing several neural layers. In the last few years, deep learning\nachieved major breakthroughs in many tasks such as image analysis, speech\nrecognition, natural language processing, and so on. Yet, there is no\ntheoretical explanation of this success. In particular, it is not clear why the\ndeeper the network, the better it actually performs.\n  We argue that the explanation is intimately connected to a key feature of the\ndata collected from our surrounding universe to feed the machine learning\nalgorithms: large non-parallelizable logical depth. Roughly speaking, we\nconjecture that the shortest computational descriptions of the universe are\nalgorithms with inherently large computation times, even when a large number of\ncomputers are available for parallelization. Interestingly, this conjecture,\ncombined with the folklore conjecture in theoretical computer science that $ P\n\\neq NC$, explains the success of deep learning.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 13:12:30 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Hoang", "L\u00ea Nguy\u00ean", ""], ["Guerraoui", "Rachid", ""]]}, {"id": "1801.10459", "submitter": "Xiaoqin Zhang", "authors": "Xiaoqin Zhang, Huimin Ma", "title": "Pretraining Deep Actor-Critic Reinforcement Learning Algorithms With\n  Expert Demonstrations", "comments": "Added acknowledgements, modified references. 7 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pretraining with expert demonstrations have been found useful in speeding up\nthe training process of deep reinforcement learning algorithms since less\nonline simulation data is required. Some people use supervised learning to\nspeed up the process of feature learning, others pretrain the policies by\nimitating expert demonstrations. However, these methods are unstable and not\nsuitable for actor-critic reinforcement learning algorithms. Also, some\nexisting methods rely on the global optimum assumption, which is not true in\nmost scenarios. In this paper, we employ expert demonstrations in a\nactor-critic reinforcement learning framework, and meanwhile ensure that the\nperformance is not affected by the fact that expert demonstrations are not\nglobal optimal. We theoretically derive a method for computing policy gradients\nand value estimators with only expert demonstrations. Our method is\ntheoretically plausible for actor-critic reinforcement learning algorithms that\npretrains both policy and value functions. We apply our method to two of the\ntypical actor-critic reinforcement learning algorithms, DDPG and ACER, and\ndemonstrate with experiments that our method not only outperforms the RL\nalgorithms without pretraining process, but also is more simulation efficient.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 14:30:00 GMT"}, {"version": "v2", "created": "Fri, 9 Feb 2018 06:36:09 GMT"}], "update_date": "2018-02-12", "authors_parsed": [["Zhang", "Xiaoqin", ""], ["Ma", "Huimin", ""]]}, {"id": "1801.10467", "submitter": "Rahul Gupta", "authors": "Rahul Gupta, Aditya Kanade, Shirish Shevade", "title": "Deep Reinforcement Learning for Programming Language Correction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Novice programmers often struggle with the formal syntax of programming\nlanguages. To assist them, we design a novel programming language correction\nframework amenable to reinforcement learning. The framework allows an agent to\nmimic human actions for text navigation and editing. We demonstrate that the\nagent can be trained through self-exploration directly from the raw input, that\nis, program text itself, without any knowledge of the formal syntax of the\nprogramming language. We leverage expert demonstrations for one tenth of the\ntraining data to accelerate training. The proposed technique is evaluated on\n6975 erroneous C programs with typographic errors, written by students during\nan introductory programming course. Our technique fixes 14% more programs and\n29% more compiler error messages relative to those fixed by a state-of-the-art\ntool, DeepFix, which uses a fully supervised neural machine translation\napproach.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 14:48:41 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Gupta", "Rahul", ""], ["Kanade", "Aditya", ""], ["Shevade", "Shirish", ""]]}, {"id": "1801.10492", "submitter": "Charles Martin", "authors": "Charles P. Martin and Kai Olav Ellefsen and Jim Torresen", "title": "Deep Predictive Models in Interactive Music", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.HC cs.NE eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Musical performance requires prediction to operate instruments, to perform in\ngroups and to improvise. In this paper, we investigate how a number of digital\nmusical instruments (DMIs), including two of our own, have applied predictive\nmachine learning models that assist users by predicting unknown states of\nmusical processes. We characterise these predictions as focussed within a\nmusical instrument, at the level of individual performers, and between members\nof an ensemble. These models can connect to existing frameworks for DMI design\nand have parallels in the cognitive predictions of human musicians.\n  We discuss how recent advances in deep learning highlight the role of\nprediction in DMIs, by allowing data-driven predictive models with a long\nmemory of past states. The systems we review are used to motivate musical\nuse-cases where prediction is a necessary component, and to highlight a number\nof challenges for DMI designers seeking to apply deep predictive models in\ninteractive music systems of the future.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 15:30:32 GMT"}, {"version": "v2", "created": "Fri, 1 Jun 2018 08:48:26 GMT"}, {"version": "v3", "created": "Wed, 19 Dec 2018 22:16:26 GMT"}], "update_date": "2018-12-21", "authors_parsed": [["Martin", "Charles P.", ""], ["Ellefsen", "Kai Olav", ""], ["Torresen", "Jim", ""]]}, {"id": "1801.10495", "submitter": "Stefan L\\\"udtke", "authors": "Stefan L\\\"udtke, Max Schr\\\"oder, Sebastian Bader, Kristian Kersting,\n  Thomas Kirste", "title": "Lifted Filtering via Exchangeable Decomposition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a model for exact recursive Bayesian filtering based on lifted\nmultiset states. Combining multisets with lifting makes it possible to\nsimultaneously exploit multiple strategies for reducing inference complexity\nwhen compared to list-based grounded state representations. The core idea is to\nborrow the concept of Maximally Parallel Multiset Rewriting Systems and to\nenhance it by concepts from Rao-Blackwellization and Lifted Inference, giving a\nrepresentation of state distributions that enables efficient inference. In\nworlds where the random variables that define the system state are exchangeable\n-- where the identity of entities does not matter -- it automatically uses a\nrepresentation that abstracts from ordering (achieving an exponential reduction\nin complexity) -- and it automatically adapts when observations or system\ndynamics destroy exchangeability by breaking symmetry.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 15:37:13 GMT"}, {"version": "v2", "created": "Mon, 7 May 2018 06:19:58 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["L\u00fcdtke", "Stefan", ""], ["Schr\u00f6der", "Max", ""], ["Bader", "Sebastian", ""], ["Kersting", "Kristian", ""], ["Kirste", "Thomas", ""]]}, {"id": "1801.10545", "submitter": "Oscar Duarte", "authors": "Oscar Duarte and Sandra T\\'ellez", "title": "A family of OWA operators based on Faulhaber's formulas", "comments": "17 pages, 7 figures, 1 table, 1 algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we develop a new family of Ordered Weighted Averaging (OWA)\noperators. Weight vector is obtained from a desired orness of the operator.\nUsing Faulhaber's formulas we obtain direct and simple expressions for the\nweight vector without any iteration loop. With the exception of one weight, the\nremaining follow a straight line relation. As a result, a fast and robust\nalgorithm is developed. The resulting weight vector is suboptimal according\nwith the Maximum Entropy criterion, but it is very close to the optimal.\nComparisons are done with other procedures.\n", "versions": [{"version": "v1", "created": "Wed, 31 Jan 2018 16:51:34 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Duarte", "Oscar", ""], ["T\u00e9llez", "Sandra", ""]]}]