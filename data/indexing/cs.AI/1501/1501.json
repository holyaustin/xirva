[{"id": "1501.00601", "submitter": "Eray Ozkural", "authors": "Eray \\\"Ozkural", "title": "Ultimate Intelligence Part I: Physical Completeness and Objectivity of\n  Induction", "comments": "Under review at AGI-2015 conference. An early draft was submitted to\n  ALT-2014. This paper is now being split into two papers, one philosophical,\n  and one more technical. We intend that all installments of the paper series\n  will be on the arxiv", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose that Solomonoff induction is complete in the physical sense via\nseveral strong physical arguments. We also argue that Solomonoff induction is\nfully applicable to quantum mechanics. We show how to choose an objective\nreference machine for universal induction by defining a physical message\ncomplexity and physical message probability, and argue that this choice\ndissolves some well-known objections to universal induction. We also introduce\nmany more variants of physical message complexity based on energy and action,\nand discuss the ramifications of our proposals.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jan 2015 20:19:57 GMT"}, {"version": "v2", "created": "Sun, 5 Apr 2015 15:47:22 GMT"}, {"version": "v3", "created": "Thu, 9 Apr 2015 18:36:27 GMT"}], "update_date": "2015-04-13", "authors_parsed": [["\u00d6zkural", "Eray", ""]]}, {"id": "1501.00653", "submitter": "Souham Biswas", "authors": "Souham Biswas, Manisha J. Nene", "title": "Hostile Intent Identification by Movement Pattern Analysis: Using\n  Artificial Neural Networks", "comments": "To appear in IEEE Xplore as a part of the proceedings of the 3rd IEEE\n  International Conference on Parallel, Distributed and Grid Computing, 2014,\n  at Jaypee University of Information Technology, Solan", "journal-ref": null, "doi": "10.13140/2.1.4429.7281", "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent years, the problem of identifying suspicious behavior has\ngained importance and identifying this behavior using computational systems and\nautonomous algorithms is highly desirable in a tactical scenario. So far, the\nsolutions have been primarily manual which elicit human observation of entities\nto discern the hostility of the situation. To cater to this problem statement,\na number of fully automated and partially automated solutions exist. But, these\nsolutions lack the capability of learning from experiences and work in\nconjunction with human supervision which is extremely prone to error. In this\npaper, a generalized methodology to predict the hostility of a given object\nbased on its movement patterns is proposed which has the ability to learn and\nis based upon the mechanism of humans of learning from experiences. The\nmethodology so proposed has been implemented in a computer simulation. The\nresults show that the posited methodology has the potential to be applied in\nreal world tactical scenarios.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jan 2015 09:07:45 GMT"}], "update_date": "2015-01-06", "authors_parsed": [["Biswas", "Souham", ""], ["Nene", "Manisha J.", ""]]}, {"id": "1501.01086", "submitter": "Vasanthakumar Soundararajan", "authors": "Vasanthakumar Soundararajan", "title": "A Novel Design of a Parallel Machine Learnt Generational Garbage\n  Collector", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Generational Garbage collection involves organizing the heap into\ndifferent divisions of memory space in-order to filter long-lived objects from\nshort-lived objects through moving the surviving object of each generation\nGarbage Collection cycle to another memory space updating its age and\nreclaiming space from the dead ones. The problem in this method is that the\nlonger an object is alive during its initial generations the longer the garbage\ncollector will have to deal with it by checking for its reachability from the\nroot and promoting it to other space divisions where as the ultimate goal of\nthe Garbage Collector is to reclaim memory from unreachable objects at a\nminimal time possible. This paper is a proposal of a method where the lifetime\nof every object getting into the heap will be predicted and will be placed in\nheap accordingly for the garbage collector to deal more with reclaiming space\nfrom dead objects and less in promoting the live ones to the higher level.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jan 2015 06:04:45 GMT"}], "update_date": "2015-01-07", "authors_parsed": [["Soundararajan", "Vasanthakumar", ""]]}, {"id": "1501.01178", "submitter": "Benjamin Negrevergne", "authors": "Benjamin Negrevergne and Tias Guns", "title": "Constraint-based sequence mining using constraint programming", "comments": "In Integration of AI and OR Techniques in Constraint Programming\n  (CPAIOR), 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of constraint-based sequence mining is to find sequences of symbols\nthat are included in a large number of input sequences and that satisfy some\nconstraints specified by the user. Many constraints have been proposed in the\nliterature, but a general framework is still missing. We investigate the use of\nconstraint programming as general framework for this task. We first identify\nfour categories of constraints that are applicable to sequence mining. We then\npropose two constraint programming formulations. The first formulation\nintroduces a new global constraint called exists-embedding. This formulation is\nthe most efficient but does not support one type of constraint. To support such\nconstraints, we develop a second formulation that is more general but incurs\nmore overhead. Both formulations can use the projected database technique used\nin specialised algorithms. Experiments demonstrate the flexibility towards\nconstraint-based settings and compare the approach to existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jan 2015 13:47:24 GMT"}, {"version": "v2", "created": "Thu, 8 Jan 2015 13:50:53 GMT"}, {"version": "v3", "created": "Wed, 25 Feb 2015 16:31:27 GMT"}], "update_date": "2015-02-26", "authors_parsed": [["Negrevergne", "Benjamin", ""], ["Guns", "Tias", ""]]}, {"id": "1501.01239", "submitter": "Han Zhao", "authors": "Han Zhao, Mazen Melibari and Pascal Poupart", "title": "On the Relationship between Sum-Product Networks and Bayesian Networks", "comments": "Full version of the same paper to appear at ICML-2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we establish some theoretical connections between Sum-Product\nNetworks (SPNs) and Bayesian Networks (BNs). We prove that every SPN can be\nconverted into a BN in linear time and space in terms of the network size. The\nkey insight is to use Algebraic Decision Diagrams (ADDs) to compactly represent\nthe local conditional probability distributions at each node in the resulting\nBN by exploiting context-specific independence (CSI). The generated BN has a\nsimple directed bipartite graphical structure. We show that by applying the\nVariable Elimination algorithm (VE) to the generated BN with ADD\nrepresentations, we can recover the original SPN where the SPN can be viewed as\na history record or caching of the VE inference process. To help state the\nproof clearly, we introduce the notion of {\\em normal} SPN and present a\ntheoretical analysis of the consistency and decomposability properties. We\nconclude the paper with some discussion of the implications of the proof and\nestablish a connection between the depth of an SPN and a lower bound of the\ntree-width of its corresponding BN.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jan 2015 17:14:11 GMT"}, {"version": "v2", "created": "Thu, 30 Apr 2015 18:15:12 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["Zhao", "Han", ""], ["Melibari", "Mazen", ""], ["Poupart", "Pascal", ""]]}, {"id": "1501.01252", "submitter": "Juan-Manuel Torres-Moreno", "authors": "Mayeul Mathias, Assema Moussa, Fen Zhou, Juan-Manuel Torres-Moreno,\n  Marie-Sylvie Poli, Didier Josselin, Marc El-B\\`eze, Andr\\'ea Carneiro\n  Linhares, Francoise Rigat", "title": "Optimisation using Natural Language Processing: Personalized Tour\n  Recommendation for Museums", "comments": "8 pages, 4 figures; Proceedings of the 2014 Federated Conference on\n  Computer Science and Information Systems pp. 439-446", "journal-ref": null, "doi": "10.15439/2014F336", "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new method to provide personalized tour recommendation\nfor museum visits. It combines an optimization of preference criteria of\nvisitors with an automatic extraction of artwork importance from museum\ninformation based on Natural Language Processing using textual energy. This\nproject includes researchers from computer and social sciences. Some results\nare obtained with numerical experiments. They show that our model clearly\nimproves the satisfaction of the visitor who follows the proposed tour. This\nwork foreshadows some interesting outcomes and applications about on-demand\npersonalized visit of museums in a very near future.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jan 2015 17:58:43 GMT"}], "update_date": "2015-01-07", "authors_parsed": [["Mathias", "Mayeul", ""], ["Moussa", "Assema", ""], ["Zhou", "Fen", ""], ["Torres-Moreno", "Juan-Manuel", ""], ["Poli", "Marie-Sylvie", ""], ["Josselin", "Didier", ""], ["El-B\u00e8ze", "Marc", ""], ["Linhares", "Andr\u00e9a Carneiro", ""], ["Rigat", "Francoise", ""]]}, {"id": "1501.01432", "submitter": "Kuang Zhou", "authors": "Kuang Zhou (IRISA), Arnaud Martin (IRISA), Quan Pan", "title": "Evidential-EM Algorithm Applied to Progressively Censored Observations", "comments": null, "journal-ref": "15th International Conference on Information Processing and\n  Management of Uncertainty in Knowledge-Based Systems, Jul 2014, Montpellier,\n  France. pp.180 - 189", "doi": "10.1007/978-3-319-08852-5_19", "report-no": null, "categories": "cs.AI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evidential-EM (E2M) algorithm is an effective approach for computing maximum\nlikelihood estimations under finite mixture models, especially when there is\nuncertain information about data. In this paper we present an extension of the\nE2M method in a particular case of incom-plete data, where the loss of\ninformation is due to both mixture models and censored observations. The prior\nuncertain information is expressed by belief functions, while the\npseudo-likelihood function is derived based on imprecise observations and prior\nknowledge. Then E2M method is evoked to maximize the generalized likelihood\nfunction to obtain the optimal estimation of parameters. Numerical examples\nshow that the proposed method could effectively integrate the uncertain prior\ninfor-mation with the current imprecise knowledge conveyed by the observed\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jan 2015 10:27:45 GMT"}], "update_date": "2015-01-08", "authors_parsed": [["Zhou", "Kuang", "", "IRISA"], ["Martin", "Arnaud", "", "IRISA"], ["Pan", "Quan", ""]]}, {"id": "1501.01457", "submitter": "Inaki Fernandez", "authors": "I\\~naki Fern\\'andez P\\'erez (INRIA Nancy - Grand Est / LORIA), Amine\n  Boumaza (INRIA Nancy - Grand Est / LORIA), Fran\\c{c}ois Charpillet (INRIA\n  Nancy - Grand Est / LORIA)", "title": "Comparison of Selection Methods in On-line Distributed Evolutionary\n  Robotics", "comments": null, "journal-ref": "ALIFE 14, Jul 2014, New York, United States. Artificial Life 14 in\n  Complex Adaptive Systems, MIT Press, Artificial Life 14", "doi": "10.7551/978-0-262-32621-6-ch046", "report-no": null, "categories": "cs.AI cs.MA cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the impact of selection methods in the context of\non-line on-board distributed evolutionary algorithms. We propose a variant of\nthe mEDEA algorithm in which we add a selection operator, and we apply it in a\ntaskdriven scenario. We evaluate four selection methods that induce different\nintensity of selection pressure in a multi-robot navigation with obstacle\navoidance task and a collective foraging task. Experiments show that a small\nintensity of selection pressure is sufficient to rapidly obtain good\nperformances on the tasks at hand. We introduce different measures to compare\nthe selection methods, and show that the higher the selection pressure, the\nbetter the performances obtained, especially for the more challenging food\nforaging task.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jan 2015 12:11:27 GMT"}], "update_date": "2015-01-08", "authors_parsed": [["P\u00e9rez", "I\u00f1aki Fern\u00e1ndez", "", "INRIA Nancy - Grand Est / LORIA"], ["Boumaza", "Amine", "", "INRIA Nancy - Grand Est / LORIA"], ["Charpillet", "Fran\u00e7ois", "", "INRIA\n  Nancy - Grand Est / LORIA"]]}, {"id": "1501.01460", "submitter": "Kuang Zhou", "authors": "Kuang Zhou (IRISA), Arnaud Martin (IRISA), Quan Pan, Zhun-Ga Liu", "title": "Median evidential c-means algorithm and its application to community\n  detection", "comments": null, "journal-ref": "Knowledge-Based Systems, Elsevier, 2015, 74, pp.69 - 88", "doi": "10.1016/j.knosys.2014.11.010", "report-no": null, "categories": "cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Median clustering is of great value for partitioning relational data. In this\npaper, a new prototype-based clustering method, called Median Evidential\nC-Means (MECM), which is an extension of median c-means and median fuzzy\nc-means on the theoretical framework of belief functions is proposed. The\nmedian variant relaxes the restriction of a metric space embedding for the\nobjects but constrains the prototypes to be in the original data set. Due to\nthese properties, MECM could be applied to graph clustering problems. A\ncommunity detection scheme for social networks based on MECM is investigated\nand the obtained credal partitions of graphs, which are more refined than crisp\nand fuzzy ones, enable us to have a better understanding of the graph\nstructures. An initial prototype-selection scheme based on evidential\nsemi-centrality is presented to avoid local premature convergence and an\nevidential modularity function is defined to choose the optimal number of\ncommunities. Finally, experiments in synthetic and real data sets illustrate\nthe performance of MECM and show its difference to other methods.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jan 2015 12:16:50 GMT"}], "update_date": "2015-01-08", "authors_parsed": [["Zhou", "Kuang", "", "IRISA"], ["Martin", "Arnaud", "", "IRISA"], ["Pan", "Quan", ""], ["Liu", "Zhun-Ga", ""]]}, {"id": "1501.01501", "submitter": "Adam Barker", "authors": "Chris Schneider, Adam Barker and Simon Dobson", "title": "Autonomous Fault Detection in Self-Healing Systems using Restricted\n  Boltzmann Machines", "comments": "Published and presented in the 11th IEEE International Conference and\n  Workshops on Engineering of Autonomic and Autonomous Systems (EASe 2014)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomously detecting and recovering from faults is one approach for\nreducing the operational complexity and costs associated with managing\ncomputing environments. We present a novel methodology for autonomously\ngenerating investigation leads that help identify systems faults, and extends\nour previous work in this area by leveraging Restricted Boltzmann Machines\n(RBMs) and contrastive divergence learning to analyse changes in historical\nfeature data. This allows us to heuristically identify the root cause of a\nfault, and demonstrate an improvement to the state of the art by showing\nfeature data can be predicted heuristically beyond a single instance to include\nentire sequences of information.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jan 2015 14:18:25 GMT"}], "update_date": "2015-01-08", "authors_parsed": [["Schneider", "Chris", ""], ["Barker", "Adam", ""], ["Dobson", "Simon", ""]]}, {"id": "1501.01576", "submitter": "arXiv Admin", "authors": "Mohammad Tafaghodi, Meysam Ghaffari, Alimohammad Latif, Seyed Rasoul\n  Mousavi", "title": "Improving image watermarking based on Tabu search by Chaos", "comments": "This paper has been withdrawn by arXiv. arXiv admin note: author list\n  truncated due to disputed authorship and content", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the fast development of communication and multimedia technology, the\nrights of the owners of multimedia products is vulnerable to the unauthorized\ncopies and watermarking is one of the best known methods for proving the\nownership of a product. In this paper we prosper the previous watermarking\nmethod which was based on Tabu search by Chaos. The modification applied in the\npermutation step of watermarking and the initial population generation of the\nTabu search. We analyze our method on some well known images and experimental\nresults shows the improvement in the quality and speed of the proposed\nwatermarking method.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jan 2015 17:59:15 GMT"}, {"version": "v2", "created": "Sun, 21 Jun 2015 14:46:54 GMT"}, {"version": "v3", "created": "Wed, 16 Sep 2015 17:17:32 GMT"}], "update_date": "2015-09-17", "authors_parsed": [["Tafaghodi", "Mohammad", ""], ["Ghaffari", "Meysam", ""], ["Latif", "Alimohammad", ""], ["Mousavi", "Seyed Rasoul", ""]]}, {"id": "1501.02036", "submitter": "EPTCS", "authors": "Fernando S\\'aenz-P\\'erez (Universidad Complutense de Madrid)", "title": "Improving the Deductive System DES with Persistence by Using SQL DBMS's", "comments": "In Proceedings PROLE 2014, arXiv:1501.01693", "journal-ref": "EPTCS 173, 2015, pp. 100-114", "doi": "10.4204/EPTCS.173.8", "report-no": null, "categories": "cs.LO cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents how persistent predicates have been included in the\nin-memory deductive system DES by relying on external SQL database management\nsystems. We introduce how persistence is supported from a user-point of view\nand the possible applications the system opens up, as the deductive expressive\npower is projected to relational databases. Also, we describe how it is\npossible to intermix computations of the deductive engine and the external\ndatabase, explaining its implementation and some optimizations. Finally, a\nperformance analysis is undertaken, comparing the system with current\nrelational database systems.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jan 2015 04:00:41 GMT"}], "update_date": "2015-01-12", "authors_parsed": [["S\u00e1enz-P\u00e9rez", "Fernando", "", "Universidad Complutense de Madrid"]]}, {"id": "1501.02192", "submitter": "Mohammad Alhawarat Dr.", "authors": "M. Alhawarat, T. Olde Scheper and N.T. Crook", "title": "Investigation of a chaotic spiking neuron model", "comments": null, "journal-ref": "International Journal of Computer Applications 99(17):1-8, August\n  2014", "doi": "10.5120/17462-8258", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chaos provides many interesting properties that can be used to achieve\ncomputational tasks. Such properties are sensitivity to initial conditions,\nspace filling, control and synchronization. Chaotic neural models have been\ndevised to exploit such properties. In this paper, a chaotic spiking neuron\nmodel is investigated experimentally. This investigation is performed to\nunderstand the dynamic behaviours of the model.\n  The aim of this research is to investigate the dynamics of the nonlinear\ndynamic state neuron (NDS) experimentally. The experimental approach has\nrevealed some quantitative and qualitative properties of the NDS model such as\nthe control mechanism, the reset mechanism, and the way the model may exhibit\ndynamic behaviours in phase space. It is shown experimentally in this paper\nthat both the reset mechanism and the self-feed back control mechanism are\nimportant for the NDS model to work and to stabilise to one of the large number\nof available unstable periodic orbits (UPOs) that are embedded in its\nattractor. The experimental investigation suggests that the internal dynamics\nof the NDS neuron provide a rich set of dynamic behaviours that can be\ncontrolled and stabilised. These wide range of dynamic behaviours may be\nexploited to carry out information processing tasks.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jan 2015 16:20:42 GMT"}], "update_date": "2015-01-12", "authors_parsed": [["Alhawarat", "M.", ""], ["Scheper", "T. Olde", ""], ["Crook", "N. T.", ""]]}, {"id": "1501.02315", "submitter": "Panos Toulis", "authors": "Panagiotis (Panos) Toulis, David C. Parkes", "title": "Long-term causal effects via behavioral game theory", "comments": "30th Conference on Neural Information Processing Systems (NIPS'16)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Planned experiments are the gold standard in reliably comparing the causal\neffect of switching from a baseline policy to a new policy. One critical\nshortcoming of classical experimental methods, however, is that they typically\ndo not take into account the dynamic nature of response to policy changes. For\ninstance, in an experiment where we seek to understand the effects of a new ad\npricing policy on auction revenue, agents may adapt their bidding in response\nto the experimental pricing changes. Thus, causal effects of the new pricing\npolicy after such adaptation period, the {\\em long-term causal effects}, are\nnot captured by the classical methodology even though they clearly are more\nindicative of the value of the new policy. Here, we formalize a framework to\ndefine and estimate long-term causal effects of policy changes in multiagent\neconomies. Central to our approach is behavioral game theory, which we leverage\nto formulate the ignorability assumptions that are necessary for causal\ninference. Under such assumptions we estimate long-term causal effects through\na latent space approach, where a behavioral model of how agents act conditional\non their latent behaviors is combined with a temporal model of how behaviors\nevolve over time.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jan 2015 07:06:43 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2015 03:10:48 GMT"}, {"version": "v3", "created": "Wed, 10 Jun 2015 20:20:48 GMT"}, {"version": "v4", "created": "Fri, 16 Oct 2015 17:50:51 GMT"}, {"version": "v5", "created": "Tue, 20 Oct 2015 17:58:47 GMT"}, {"version": "v6", "created": "Tue, 27 Oct 2015 01:08:18 GMT"}, {"version": "v7", "created": "Wed, 2 Nov 2016 21:26:47 GMT"}, {"version": "v8", "created": "Fri, 4 Nov 2016 02:18:51 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Panagiotis", "", "", "Panos"], ["Toulis", "", ""], ["Parkes", "David C.", ""]]}, {"id": "1501.02527", "submitter": "Nicholas Locascio", "authors": "Harini Suresh, Nicholas Locascio", "title": "Autodetection and Classification of Hidden Cultural City Districts from\n  Yelp Reviews", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models are a way to discover underlying themes in an otherwise\nunstructured collection of documents. In this study, we specifically used the\nLatent Dirichlet Allocation (LDA) topic model on a dataset of Yelp reviews to\nclassify restaurants based off of their reviews. Furthermore, we hypothesize\nthat within a city, restaurants can be grouped into similar \"clusters\" based on\nboth location and similarity. We used several different clustering methods,\nincluding K-means Clustering and a Probabilistic Mixture Model, in order to\nuncover and classify districts, both well-known and hidden (i.e. cultural areas\nlike Chinatown or hearsay like \"the best street for Italian restaurants\")\nwithin a city. We use these models to display and label different clusters on a\nmap. We also introduce a topic similarity heatmap that displays the similarity\ndistribution in a city to a new restaurant.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jan 2015 03:10:01 GMT"}], "update_date": "2015-01-13", "authors_parsed": [["Suresh", "Harini", ""], ["Locascio", "Nicholas", ""]]}, {"id": "1501.02560", "submitter": "Kuang Zhou", "authors": "Wiem Maalel (IRISA), Kuang Zhou (IRISA), Arnaud Martin (IRISA), Zied\n  Elouedi", "title": "Belief Hierarchical Clustering", "comments": null, "journal-ref": "3rd International Conference on Belief Functions, Sep 2014,\n  Oxford, United Kingdom. pp.68 - 76", "doi": "10.1007/978-3-319-11191-9_8", "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the data mining field many clustering methods have been proposed, yet\nstandard versions do not take into account uncertain databases. This paper\ndeals with a new approach to cluster uncertain data by using a hierarchical\nclustering defined within the belief function framework. The main objective of\nthe belief hierarchical clustering is to allow an object to belong to one or\nseveral clusters. To each belonging, a degree of belief is associated, and\nclusters are combined based on the pignistic properties. Experiments with real\nuncertain data show that our proposed method can be considered as a propitious\ntool.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jan 2015 07:55:41 GMT"}], "update_date": "2015-01-13", "authors_parsed": [["Maalel", "Wiem", "", "IRISA"], ["Zhou", "Kuang", "", "IRISA"], ["Martin", "Arnaud", "", "IRISA"], ["Elouedi", "Zied", ""]]}, {"id": "1501.02629", "submitter": "Aur\\'elien Bellet", "authors": "St\\'ephan Cl\\'emen\\c{c}on, Aur\\'elien Bellet, Igor Colin", "title": "Scaling-up Empirical Risk Minimization: Optimization of Incomplete\n  U-statistics", "comments": "To appear in Journal of Machine Learning Research. 34 pages. v2:\n  minor correction to Theorem 4 and its proof, added 1 reference. v3: typo\n  corrected in Proposition 3. v4: improved presentation, added experiments on\n  model selection for clustering, fixed minor typos", "journal-ref": "Journal of Machine Learning Research 17(76):1-36, 2016", "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a wide range of statistical learning problems such as ranking, clustering\nor metric learning among others, the risk is accurately estimated by\n$U$-statistics of degree $d\\geq 1$, i.e. functionals of the training data with\nlow variance that take the form of averages over $k$-tuples. From a\ncomputational perspective, the calculation of such statistics is highly\nexpensive even for a moderate sample size $n$, as it requires averaging\n$O(n^d)$ terms. This makes learning procedures relying on the optimization of\nsuch data functionals hardly feasible in practice. It is the major goal of this\npaper to show that, strikingly, such empirical risks can be replaced by\ndrastically computationally simpler Monte-Carlo estimates based on $O(n)$ terms\nonly, usually referred to as incomplete $U$-statistics, without damaging the\n$O_{\\mathbb{P}}(1/\\sqrt{n})$ learning rate of Empirical Risk Minimization (ERM)\nprocedures. For this purpose, we establish uniform deviation results describing\nthe error made when approximating a $U$-process by its incomplete version under\nappropriate complexity assumptions. Extensions to model selection, fast rate\nsituations and various sampling techniques are also considered, as well as an\napplication to stochastic gradient descent for ERM. Finally, numerical examples\nare displayed in order to provide strong empirical evidence that the approach\nwe promote largely surpasses more naive subsampling techniques.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jan 2015 12:58:45 GMT"}, {"version": "v2", "created": "Tue, 14 Jul 2015 22:42:22 GMT"}, {"version": "v3", "created": "Wed, 21 Oct 2015 12:42:17 GMT"}, {"version": "v4", "created": "Tue, 19 Apr 2016 06:30:09 GMT"}], "update_date": "2019-01-25", "authors_parsed": [["Cl\u00e9men\u00e7on", "St\u00e9phan", ""], ["Bellet", "Aur\u00e9lien", ""], ["Colin", "Igor", ""]]}, {"id": "1501.02662", "submitter": "Renato Fabbri", "authors": "Renato Fabbri, Henrique Parra Parra Filho, Rodrigo Bandeira de Luna,\n  Ricardo Augusto Poppi Martins, Flor Karina Mamani Amanqui, Dilvan de Abreu\n  Moreira, Osvaldo Novais de Oliveira Junior", "title": "Social Participation Ontology: community documentation, enhancements and\n  use examples", "comments": "See ancillary for table of terms, OPS code and figures. Further\n  information is at https://github.com/ttm/ops", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Participatory democracy advances in virtually all governments and especially\nin South America which exhibits a mixed culture and social predisposition. This\narticle presents the \"Social Participation Ontology\" (OPS from the Brazilian\nname \\emph{Ontologia de Participa\\c{c}\\~ao Social}) implemented in compliance\nwith the Web Ontology Language standard (OWL) for fostering social\nparticipation, specially in virtual platforms. The entities and links of OPS\nwere defined based on an extensive collaboration of specialists. It is shown\nthat OPS is instrumental for information retrieval from the contents of the\nportal, both in terms of the actors (at various levels) as well as mechanisms\nand activities. Significantly, OPS is linked to other OWL ontologies as an\nupper ontology and via FOAF and BFO as higher upper ontologies, which yields\nsound organization and access of knowledge and data. In order to illustrate the\nusefulness of OPS, we present results on ontological expansion and integration\nwith other ontologies and data. Ongoing work involves further adoption of OPS\nby the official Brazilian federal portal for social participation and NGO s,\nand further linkage to other ontologies for social participation.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jan 2015 14:33:23 GMT"}, {"version": "v2", "created": "Tue, 13 Jan 2015 03:10:57 GMT"}, {"version": "v3", "created": "Mon, 30 Oct 2017 18:28:50 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Fabbri", "Renato", ""], ["Filho", "Henrique Parra Parra", ""], ["de Luna", "Rodrigo Bandeira", ""], ["Martins", "Ricardo Augusto Poppi", ""], ["Amanqui", "Flor Karina Mamani", ""], ["Moreira", "Dilvan de Abreu", ""], ["Junior", "Osvaldo Novais de Oliveira", ""]]}, {"id": "1501.02732", "submitter": "Ilya Goldin", "authors": "April Galyardt and Ilya Goldin", "title": "Predicting Performance During Tutoring with Models of Recent Performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In educational technology and learning sciences, there are multiple uses for\na predictive model of whether a student will perform a task correctly or not.\nFor example, an intelligent tutoring system may use such a model to estimate\nwhether or not a student has mastered a skill. We analyze the significance of\ndata recency in making such predictions, i.e., asking whether relatively more\nrecent observations of a student's performance matter more than relatively\nolder observations. We develop a new Recent-Performance Factors Analysis model\nthat takes data recency into account. The new model significantly improves\npredictive accuracy over both existing logistic-regression performance models\nand over novel baseline models in evaluations on real-world and synthetic\ndatasets. As a secondary contribution, we demonstrate how the widely used\ncross-validation with 0-1 loss is inferior to AIC and to cross-validation with\nL1 prediction error loss as a measure of model performance.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jan 2015 17:39:53 GMT"}], "update_date": "2015-01-13", "authors_parsed": [["Galyardt", "April", ""], ["Goldin", "Ilya", ""]]}, {"id": "1501.03093", "submitter": "Vojtech Forejt", "authors": "Tom\\'a\\v{s} Br\\'azdil, Krishnendu Chatterjee, Vojt\\v{e}ch Forejt, and\n  Anton\\'in Ku\\v{c}era", "title": "MultiGain: A controller synthesis tool for MDPs with multiple\n  mean-payoff objectives", "comments": "Extended version for a TACAS 2015 tool demo paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present MultiGain, a tool to synthesize strategies for Markov decision\nprocesses (MDPs) with multiple mean-payoff objectives. Our models are described\nin PRISM, and our tool uses the existing interface and simulator of PRISM. Our\ntool extends PRISM by adding novel algorithms for multiple mean-payoff\nobjectives, and also provides features such as (i)~generating strategies and\nexploring them for simulation, and checking them with respect to other\nproperties; and (ii)~generating an approximate Pareto curve for two mean-payoff\nobjectives. In addition, we present a new practical algorithm for the analysis\nof MDPs with multiple mean-payoff objectives under memoryless strategies.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jan 2015 18:04:46 GMT"}], "update_date": "2015-01-14", "authors_parsed": [["Br\u00e1zdil", "Tom\u00e1\u0161", ""], ["Chatterjee", "Krishnendu", ""], ["Forejt", "Vojt\u011bch", ""], ["Ku\u010dera", "Anton\u00edn", ""]]}, {"id": "1501.03302", "submitter": "Mateusz Malinowski", "authors": "Mateusz Malinowski and Mario Fritz", "title": "Hard to Cheat: A Turing Test based on Answering Questions about Images", "comments": "Presented in AAAI-15 Workshop: Beyond the Turing Test", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Progress in language and image understanding by machines has sparkled the\ninterest of the research community in more open-ended, holistic tasks, and\nrefueled an old AI dream of building intelligent machines. We discuss a few\nprominent challenges that characterize such holistic tasks and argue for\n\"question answering about images\" as a particular appealing instance of such a\nholistic task. In particular, we point out that it is a version of a Turing\nTest that is likely to be more robust to over-interpretations and contrast it\nwith tasks like grounding and generation of descriptions. Finally, we discuss\ntools to measure progress in this field.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jan 2015 10:38:43 GMT"}, {"version": "v2", "created": "Thu, 15 Jan 2015 10:18:54 GMT"}], "update_date": "2015-01-16", "authors_parsed": [["Malinowski", "Mateusz", ""], ["Fritz", "Mario", ""]]}, {"id": "1501.03784", "submitter": "Denis Kleyko", "authors": "Denis Kleyko, Evgeny Osipov, Alexander Senior, Asad I. Khan and Y.\n  Ahmet \\c{S}ekercio\\u{g}lu", "title": "Holographic Graph Neuron: a Bio-Inspired Architecture for Pattern\n  Processing", "comments": "9 pages, 13 figures", "journal-ref": "IEEE Transactions on Neural Networks and Learning Systems 28\n  (2017) 1250 - 1262", "doi": "10.1109/TNNLS.2016.2535338", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article proposes the use of Vector Symbolic Architectures for\nimplementing Hierarchical Graph Neuron, an architecture for memorizing patterns\nof generic sensor stimuli. The adoption of a Vector Symbolic representation\nensures a one-layered design for the approach, while maintaining the previously\nreported properties and performance characteristics of Hierarchical Graph\nNeuron, and also improving the noise resistance of the architecture. The\nproposed architecture enables a linear (with respect to the number of stored\nentries) time search for an arbitrary sub-pattern.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jan 2015 19:25:32 GMT"}], "update_date": "2019-08-13", "authors_parsed": [["Kleyko", "Denis", ""], ["Osipov", "Evgeny", ""], ["Senior", "Alexander", ""], ["Khan", "Asad I.", ""], ["\u015eekercio\u011flu", "Y. Ahmet", ""]]}, {"id": "1501.03959", "submitter": "Kamil Ciosek", "authors": "Kamil Ciosek and David Silver", "title": "Value Iteration with Options and State Aggregation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a way of solving Markov Decision Processes that combines\nstate abstraction and temporal abstraction. Specifically, we combine state\naggregation with the options framework and demonstrate that they work well\ntogether and indeed it is only after one combines the two that the full benefit\nof each is realized. We introduce a hierarchical value iteration algorithm\nwhere we first coarsely solve subgoals and then use these approximate solutions\nto exactly solve the MDP. This algorithm solved several problems faster than\nvanilla value iteration.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jan 2015 12:02:51 GMT"}], "update_date": "2015-01-19", "authors_parsed": [["Ciosek", "Kamil", ""], ["Silver", "David", ""]]}, {"id": "1501.04177", "submitter": "Andrea Schaerf", "authors": "Sara Ceschia, Nguyen Thi Thanh Dang, Patrick De Causmaecker, Stefaan\n  Haspeslagh, Andrea Schaerf", "title": "Second International Nurse Rostering Competition (INRC-II) --- Problem\n  Description and Rules ---", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we provide all information to participate to the Second\nInternational Nurse Rostering Competition (INRC-II). First, we describe the\nproblem formulation, which, differently from INRC-I, is a multi-stage\nprocedure. Second, we illustrate all the necessary infrastructure do be used\ntogether with the participant's solver, including the testbed, the file\nformats, and the validation/simulation tools. Finally, we state the rules of\nthe competition. All update-to-date information about the competition is\navailable at http://mobiz.vives.be/inrc2/.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jan 2015 09:06:08 GMT"}], "update_date": "2015-01-20", "authors_parsed": [["Ceschia", "Sara", ""], ["Dang", "Nguyen Thi Thanh", ""], ["De Causmaecker", "Patrick", ""], ["Haspeslagh", "Stefaan", ""], ["Schaerf", "Andrea", ""]]}, {"id": "1501.04242", "submitter": "Hector Zenil", "authors": "Nicolas Gauvrit, Hector Zenil, Jesper Tegn\\'er", "title": "The Information-theoretic and Algorithmic Approach to Human, Animal and\n  Artificial Cognition", "comments": "22 pages. Forthcoming in Gordana Dodig-Crnkovic and Raffaela\n  Giovagnoli (eds). Representation and Reality: Humans, Animals and Machines,\n  Springer Verlag", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We survey concepts at the frontier of research connecting artificial, animal\nand human cognition to computation and information processing---from the Turing\ntest to Searle's Chinese Room argument, from Integrated Information Theory to\ncomputational and algorithmic complexity. We start by arguing that passing the\nTuring test is a trivial computational problem and that its pragmatic\ndifficulty sheds light on the computational nature of the human mind more than\nit does on the challenge of artificial intelligence. We then review our\nproposed algorithmic information-theoretic measures for quantifying and\ncharacterizing cognition in various forms. These are capable of accounting for\nknown biases in human behavior, thus vindicating a computational algorithmic\nview of cognition as first suggested by Turing, but this time rooted in the\nconcept of algorithmic probability, which in turn is based on computational\nuniversality while being independent of computational model, and which has the\nvirtue of being predictive and testable as a model theory of cognitive\nbehavior.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jan 2015 22:55:48 GMT"}, {"version": "v2", "created": "Fri, 23 Jan 2015 23:55:43 GMT"}, {"version": "v3", "created": "Tue, 27 Jan 2015 01:23:36 GMT"}, {"version": "v4", "created": "Wed, 28 Jan 2015 15:51:30 GMT"}, {"version": "v5", "created": "Thu, 24 Dec 2015 13:54:22 GMT"}], "update_date": "2015-12-25", "authors_parsed": [["Gauvrit", "Nicolas", ""], ["Zenil", "Hector", ""], ["Tegn\u00e9r", "Jesper", ""]]}, {"id": "1501.04346", "submitter": "Divyanshu Vats", "authors": "Andrew S. Lan and Divyanshu Vats and Andrew E. Waters and Richard G.\n  Baraniuk", "title": "Mathematical Language Processing: Automatic Grading and Feedback for\n  Open Response Mathematical Questions", "comments": "ACM Conference on Learning at Scale, March 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While computer and communication technologies have provided effective means\nto scale up many aspects of education, the submission and grading of\nassessments such as homework assignments and tests remains a weak link. In this\npaper, we study the problem of automatically grading the kinds of open response\nmathematical questions that figure prominently in STEM (science, technology,\nengineering, and mathematics) courses. Our data-driven framework for\nmathematical language processing (MLP) leverages solution data from a large\nnumber of learners to evaluate the correctness of their solutions, assign\npartial-credit scores, and provide feedback to each learner on the likely\nlocations of any errors. MLP takes inspiration from the success of natural\nlanguage processing for text data and comprises three main steps. First, we\nconvert each solution to an open response mathematical question into a series\nof numerical features. Second, we cluster the features from several solutions\nto uncover the structures of correct, partially correct, and incorrect\nsolutions. We develop two different clustering approaches, one that leverages\ngeneric clustering algorithms and one based on Bayesian nonparametrics. Third,\nwe automatically grade the remaining (potentially large number of) solutions\nbased on their assigned cluster and one instructor-provided grade per cluster.\nAs a bonus, we can track the cluster assignment of each step of a multistep\nsolution and determine when it departs from a cluster of correct solutions,\nwhich enables us to indicate the likely locations of errors to learners. We\ntest and validate MLP on real-world MOOC data to demonstrate how it can\nsubstantially reduce the human effort required in large-scale educational\nplatforms.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jan 2015 20:50:39 GMT"}], "update_date": "2015-01-20", "authors_parsed": [["Lan", "Andrew S.", ""], ["Vats", "Divyanshu", ""], ["Waters", "Andrew E.", ""], ["Baraniuk", "Richard G.", ""]]}, {"id": "1501.04358", "submitter": "Paul Smaldino", "authors": "Paul E. Smaldino", "title": "Does Learning Imply a Decrease in the Entropy of Behavior?", "comments": "14 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shannon's information entropy measures of the uncertainty of an event's\noutcome. If learning about a system reflects a decrease in uncertainty, then a\nplausible intuition is that learning should be accompanied by a decrease in the\nentropy of the organism's actions and/or perceptual states. To address whether\nthis intuition is valid, I examined an artificial organism -- a simple robot --\nthat learned to navigate in an arena and analyzed the entropy of the outcome\nvariables action, state, and reward. Entropy did indeed decrease in the initial\nstages of learning, but two factors complicated the scenario: (1) the\nintroduction of new options discovered during the learning process and (2) the\nshifting patterns of perceptual and environmental states resulting from changes\nto the robot's learned movement strategies. These factors lead to a subsequent\nincrease in entropy as the agent learned. I end with a discussion of the\nutility of information-based characterizations of learning.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jan 2015 21:49:22 GMT"}, {"version": "v2", "created": "Sun, 25 Jan 2015 01:13:52 GMT"}, {"version": "v3", "created": "Thu, 19 Feb 2015 01:16:18 GMT"}], "update_date": "2015-02-20", "authors_parsed": [["Smaldino", "Paul E.", ""]]}, {"id": "1501.04370", "submitter": "Ru He", "authors": "Ru He, Jin Tian, Huaiqing Wu", "title": "Structure Learning in Bayesian Networks of Moderate Size by Efficient\n  Sampling", "comments": "51 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the Bayesian model averaging approach to learning Bayesian network\nstructures (DAGs) from data. We develop new algorithms including the first\nalgorithm that is able to efficiently sample DAGs according to the exact\nstructure posterior. The DAG samples can then be used to construct estimators\nfor the posterior of any feature. We theoretically prove good properties of our\nestimators and empirically show that our estimators considerably outperform the\nestimators from the previous state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jan 2015 01:32:43 GMT"}], "update_date": "2015-01-20", "authors_parsed": [["He", "Ru", ""], ["Tian", "Jin", ""], ["Wu", "Huaiqing", ""]]}, {"id": "1501.04413", "submitter": "Masayuki Ohzeki", "authors": "Masayuki Ohzeki", "title": "Statistical-mechanical analysis of pre-training and fine tuning in deep\n  learning", "comments": "13 pages and 2 figures, to appear in JPSJ", "journal-ref": null, "doi": "10.7566/JPSJ.84.034003", "report-no": null, "categories": "stat.ML cond-mat.dis-nn cond-mat.stat-mech cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a statistical-mechanical analysis of deep learning.\nWe elucidate some of the essential components of deep learning---pre-training\nby unsupervised learning and fine tuning by supervised learning. We formulate\nthe extraction of features from the training data as a margin criterion in a\nhigh-dimensional feature-vector space. The self-organized classifier is then\nsupplied with small amounts of labelled data, as in deep learning. Although we\nemploy a simple single-layer perceptron model, rather than directly analyzing a\nmulti-layer neural network, we find a nontrivial phase transition that is\ndependent on the number of unlabelled data in the generalization error of the\nresultant classifier. In this sense, we evaluate the efficacy of the\nunsupervised learning component of deep learning. The analysis is performed by\nthe replica method, which is a sophisticated tool in statistical mechanics. We\nvalidate our result in the manner of deep learning, using a simple iterative\nalgorithm to learn the weight vector on the basis of belief propagation.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jan 2015 07:24:21 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Ohzeki", "Masayuki", ""]]}, {"id": "1501.04684", "submitter": "Razvan Ranca", "authors": "Razvan Ranca, Zoubin Ghahramani", "title": "Slice Sampling for Probabilistic Programming", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the first, general purpose, slice sampling inference engine for\nprobabilistic programs. This engine is released as part of StocPy, a new\nTuring-Complete probabilistic programming language, available as a Python\nlibrary. We present a transdimensional generalisation of slice sampling which\nis necessary for the inference engine to work on traces with different numbers\nof random variables. We show that StocPy compares favourably to other PPLs in\nterms of flexibility and usability, and that slice sampling can outperform\npreviously introduced inference methods. Our experiments include a logistic\nregression, HMM, and Bayesian Neural Net.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jan 2015 00:24:14 GMT"}], "update_date": "2015-01-21", "authors_parsed": [["Ranca", "Razvan", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1501.04786", "submitter": "Arnaud Martin", "authors": "Mouna Chebbah (IRISA), Mouloud Kharoune (IRISA), Arnaud Martin\n  (IRISA), Boutheina Ben Yaghlane", "title": "Consid{\\'e}rant la d{\\'e}pendance dans la th{\\'e}orie des fonctions de\n  croyance", "comments": "in French", "journal-ref": "Revue des Nouvelles Technologies Informatiques (RNTI), 2014,\n  Fouille de donn{\\'e}es complexes, RNTI-E-27, pp.43-64", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose to learn sources independence in order to choose\nthe appropriate type of combination rules when aggregating their beliefs. Some\ncombination rules are used with the assumption of their sources independence\nwhereas others combine beliefs of dependent sources. Therefore, the choice of\nthe combination rule depends on the independence of sources involved in the\ncombination. In this paper, we propose also a measure of independence, positive\nand negative dependence to integrate in mass functions before the combinaision\nwith the independence assumption.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jan 2015 12:48:41 GMT"}], "update_date": "2015-01-21", "authors_parsed": [["Chebbah", "Mouna", "", "IRISA"], ["Kharoune", "Mouloud", "", "IRISA"], ["Martin", "Arnaud", "", "IRISA"], ["Yaghlane", "Boutheina Ben", ""]]}, {"id": "1501.04792", "submitter": "Arnaud Martin", "authors": "Jean-Christophe Dubois (IRISA), Yolande Le Gall (IRISA), Arnaud Martin\n  (IRISA)", "title": "Designing a Belief Function-Based Accessibility Indicator to Improve Web\n  Browsing for Disabled People", "comments": null, "journal-ref": "Belief 2014, Sep 2014, Oxford, United Kingdom. Lecture Notes in\n  Artificial Intelligence, Lecture Notes in Computer Science, Vol. 8764, pp.134\n  - 142, Belief Functions: Theory and Applications", "doi": "10.1007/978-3-319-11191-9_15", "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this study is to provide an accessibility measure of\nweb-pages, in order to draw disabled users to the pages that have been designed\nto be ac-cessible to them. Our approach is based on the theory of belief\nfunctions, using data which are supplied by reports produced by automatic web\ncontent assessors that test the validity of criteria defined by the WCAG 2.0\nguidelines proposed by the World Wide Web Consortium (W3C) organization. These\ntools detect errors with gradual degrees of certainty and their results do not\nalways converge. For these reasons, to fuse information coming from the\nreports, we choose to use an information fusion framework which can take into\naccount the uncertainty and imprecision of infor-mation as well as divergences\nbetween sources. Our accessibility indicator covers four categories of\ndeficiencies. To validate the theoretical approach in this context, we propose\nan evaluation completed on a corpus of 100 most visited French news websites,\nand 2 evaluation tools. The results obtained illustrate the interest of our\naccessibility indicator.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jan 2015 12:53:27 GMT"}], "update_date": "2015-01-21", "authors_parsed": [["Dubois", "Jean-Christophe", "", "IRISA"], ["Gall", "Yolande Le", "", "IRISA"], ["Martin", "Arnaud", "", "IRISA"]]}, {"id": "1501.04795", "submitter": "Arnaud Martin", "authors": "Salma Ben Dhaou (IRISA), Mouloud Kharoune (IRISA), Arnaud Martin\n  (IRISA), Boutheina Ben Yaghlane", "title": "Belief Approach for Social Networks", "comments": null, "journal-ref": "Belief 2014, Sep 2014, Oxford, United Kingdom. Lecture Notes in\n  Artificial Intelligence, Lecture Notes in Computer Science, Vol. 8764,\n  pp.115-123, Belief Functions: Theory and Applications", "doi": "10.1007/978-3-319-11191-9_13", "report-no": null, "categories": "cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, social networks became essential in information exchange between\nindividuals. Indeed, as users of these networks, we can send messages to other\npeople according to the links connecting us. Moreover, given the large volume\nof exchanged messages, detecting the true nature of the received message\nbecomes a challenge. For this purpose, it is interesting to consider this new\ntendency with reasoning under uncertainty by using the theory of belief\nfunctions. In this paper, we tried to model a social network as being a network\nof fusion of information and determine the true nature of the received message\nin a well-defined node by proposing a new model: the belief social network.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jan 2015 13:01:01 GMT"}], "update_date": "2015-01-21", "authors_parsed": [["Dhaou", "Salma Ben", "", "IRISA"], ["Kharoune", "Mouloud", "", "IRISA"], ["Martin", "Arnaud", "", "IRISA"], ["Yaghlane", "Boutheina Ben", ""]]}, {"id": "1501.04796", "submitter": "Pierre-Yves Oudeyer", "authors": "Pierre-Yves Oudeyer (INRIA Bordeaux - Sud-Ouest)", "title": "What do we learn about development from baby robots?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding infant development is one of the greatest scientific challenges\nof contemporary science. A large source of difficulty comes from the fact that\nthe development of skills in infants results from the interactions of multiple\nmechanisms at multiple spatio-temporal scales. The concepts of \"innate\" or\n\"acquired\" are not any more adequate tools for explanations, which call for a\nshift from reductionist to systemic accounts. To address this challenge,\nbuilding and experimenting with robots modeling the growing infant brain and\nbody is crucial. Systemic explanations of pattern formation in sensorimotor,\ncognitive and social development, viewed as a complex dynamical system, require\nthe use of formal models based on mathematics, algorithms and robots.\nFormulating hypothesis about development using such models, and exploring them\nthrough experiments, allows us to consider in detail the interaction between\nmany mechanisms and parameters. This complements traditional experimental\nmethods in psychology and neuroscience where only a few variables can be\nstudied at the same time. Furthermore, the use of robots is of particular\nimportance. The laws of physics generate everywhere around us spontaneous\npatterns in the inorganic world. They also strongly impact the living, and in\nparticular constrain and guide infant development through the properties of its\n(changing) body in interaction with the physical environment. Being able to\nconsider the body as an experimental variable, something that can be\nsystematically changed in order to study the impact on skill formation, has\nbeen a dream to many developmental scientists. This is today becoming possible\nwith developmental robotics.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jan 2015 13:03:26 GMT"}], "update_date": "2015-01-21", "authors_parsed": [["Oudeyer", "Pierre-Yves", "", "INRIA Bordeaux - Sud-Ouest"]]}, {"id": "1501.04832", "submitter": "Robert Jeansoulin", "authors": "Robert Jeansoulin", "title": "Big Data: How Geo-information Helped Shape the Future of Data\n  Engineering", "comments": "Conference \"AutoCarto 6\", revisited 30 years later in a\n  \"Retrospective book\", edited by Barry Wellar, the same chair as the original\n  conference. 12 pages, 6 figures. see: AutoCarto Six Retrospective, 2013.\n  ISBN: 978-0-9921435-0-3, pages 190-201", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Very large data sets are the common rule in automated mapping, GIS, remote\nsensing, and what we can name geo-information. Indeed, in 1983 Landsat was\nalready delivering gigabytes of data, and other sensors were in orbit or ready\nfor launch, and a tantamount of cartographic data was being digitized. The\nretrospective paper revisits several issues that geo-information sciences had\nto face from the early stages on, including: structure ( to bring some\nstructure to the data registered from a sampled signal, metadata); processing\n(huge amounts of data for big computers and fast algorithms); uncertainty (the\nkinds of errors, their quantification); consistency (when merging different\nsources of data is logically allowed, and meaningful); ontologies (clear and\nagreed shared definitions, if any kind of decision should be based upon them).\nAll these issues are the background of Internet queries, and the underlying\ntechnology has been shaped during those years when geo-information engineering\nemerged.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jan 2015 14:57:11 GMT"}], "update_date": "2015-01-21", "authors_parsed": [["Jeansoulin", "Robert", ""]]}, {"id": "1501.05031", "submitter": "Samantha Leung", "authors": "Brad Gulko and Samantha Leung", "title": "Maximin Safety: When Failing to Lose is Preferable to Trying to Win", "comments": "14 pages", "journal-ref": "ECSQARU 2013: 254-265", "doi": null, "report-no": null, "categories": "cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new decision rule, \\emph{maximin safety}, that seeks to maintain\na large margin from the worst outcome, in much the same way minimax regret\nseeks to minimize distance from the best. We argue that maximin safety is\nvaluable both descriptively and normatively. Descriptively, maximin safety\nexplains the well-known \\emph{decoy effect}, in which the introduction of a\ndominated option changes preferences among the other options. Normatively, we\nprovide an axiomatization that characterizes preferences induced by maximin\nsafety, and show that maximin safety shares much of the same behavioral basis\nwith minimax regret.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jan 2015 01:01:28 GMT"}], "update_date": "2015-01-22", "authors_parsed": [["Gulko", "Brad", ""], ["Leung", "Samantha", ""]]}, {"id": "1501.05215", "submitter": "Peter Thwaites", "authors": "Peter A. Thwaites and Jim Q. Smith", "title": "A Separation Theorem for Chain Event Graphs", "comments": "39 pages, 10 figures. Submitted to Electronic Journal of Statistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian Networks (BNs) are popular graphical models for the representation\nof statistical problems embodying dependence relationships between a number of\nvariables. Much of this popularity is due to the d-separation theorem of Pearl\nand Lauritzen, which allows an analyst to identify the conditional independence\nstatements that a model of the problem embodies using only the topology of the\ngraph. However for many problems the complete model dependence structure cannot\nbe depicted by a BN. The Chain Event Graph (CEG) was introduced for these types\nof problem. In this paper we introduce a separation theorem for CEGs, analogous\nto the d-separation theorem for BNs, which likewise allows an analyst to\nidentify the conditional independence structure of their model from the\ntopology of the graph.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jan 2015 16:14:27 GMT"}], "update_date": "2015-01-22", "authors_parsed": [["Thwaites", "Peter A.", ""], ["Smith", "Jim Q.", ""]]}, {"id": "1501.05272", "submitter": "Arnaud Martin", "authors": "Imen Ouled Dlala (IRISA), Dorra Attiaoui (IRISA), Arnaud Martin\n  (IRISA), Boutheina Ben Yaghlane", "title": "Trolls Identification within an Uncertain Framework", "comments": "International Conference on Tools with Artificial Intelligence -\n  ICTAI , Nov 2014, Limassol, Cyprus", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The web plays an important role in people's social lives since the emergence\nof Web 2.0. It facilitates the interaction between users, gives them the\npossibility to freely interact, share and collaborate through social networks,\nonline communities forums, blogs, wikis and other online collaborative media.\nHowever, an other side of the web is negatively taken such as posting\ninflammatory messages. Thus, when dealing with the online communities forums,\nthe managers seek to always enhance the performance of such platforms. In fact,\nto keep the serenity and prohibit the disturbance of the normal atmosphere,\nmanagers always try to novice users against these malicious persons by posting\nsuch message (DO NOT FEED TROLLS). But, this kind of warning is not enough to\nreduce this phenomenon. In this context we propose a new approach for detecting\nmalicious people also called 'Trolls' in order to allow community managers to\ntake their ability to post online. To be more realistic, our proposal is\ndefined within an uncertain framework. Based on the assumption consisting on\nthe trolls' integration in the successful discussion threads, we try to detect\nthe presence of such malicious users. Indeed, this method is based on a\nconflict measure of the belief function theory applied between the different\nmessages of the thread. In order to show the feasibility and the result of our\napproach, we test it in different simulated data.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jan 2015 19:34:23 GMT"}], "update_date": "2015-01-22", "authors_parsed": [["Dlala", "Imen Ouled", "", "IRISA"], ["Attiaoui", "Dorra", "", "IRISA"], ["Martin", "Arnaud", "", "IRISA"], ["Yaghlane", "Boutheina Ben", ""]]}, {"id": "1501.05290", "submitter": "Bernardo Gon\\c{c}alves", "authors": "Bernardo Gon\\c{c}alves", "title": "Managing large-scale scientific hypotheses as uncertain and\n  probabilistic data", "comments": "145 pages, 61 figures, 1 table. PhD thesis, National Laboratory for\n  Scientific Computing (LNCC), Brazil, February 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In view of the paradigm shift that makes science ever more data-driven, in\nthis thesis we propose a synthesis method for encoding and managing large-scale\ndeterministic scientific hypotheses as uncertain and probabilistic data.\n  In the form of mathematical equations, hypotheses symmetrically relate\naspects of the studied phenomena. For computing predictions, however,\ndeterministic hypotheses can be abstracted as functions. We build upon Simon's\nnotion of structural equations in order to efficiently extract the (so-called)\ncausal ordering between variables, implicit in a hypothesis structure (set of\nmathematical equations).\n  We show how to process the hypothesis predictive structure effectively\nthrough original algorithms for encoding it into a set of functional\ndependencies (fd's) and then performing causal reasoning in terms of acyclic\npseudo-transitive reasoning over fd's. Such reasoning reveals important causal\ndependencies implicit in the hypothesis predictive data and guide our synthesis\nof a probabilistic database. Like in the field of graphical models in AI, such\na probabilistic database should be normalized so that the uncertainty arisen\nfrom competing hypotheses is decomposed into factors and propagated properly\nonto predictive data by recovering its joint probability distribution through a\nlossless join. That is motivated as a design-theoretic principle for\ndata-driven hypothesis management and predictive analytics.\n  The method is applicable to both quantitative and qualitative deterministic\nhypotheses and demonstrated in realistic use cases from computational science.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jan 2015 20:46:23 GMT"}, {"version": "v2", "created": "Thu, 12 Feb 2015 20:52:29 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["Gon\u00e7alves", "Bernardo", ""]]}, {"id": "1501.05426", "submitter": "Arnaud Martin", "authors": "Siwar Jendoubi (IRISA), Arnaud Martin (IRISA), Ludovic Li\\'etard\n  (IRISA), Boutheina Ben Yaghlane", "title": "Classification of Message Spreading in a Heterogeneous Social Network", "comments": null, "journal-ref": "International Conference on Information Processing and Management\n  of Uncertainty in Knowledge-Based Systems (IPMU), Jul 2014, Montpellier,\n  France. pp.66 - 75", "doi": "10.1007/978-3-319-08855-6_8", "report-no": null, "categories": "cs.SI cs.AI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, social networks such as Twitter, Facebook and LinkedIn become\nincreasingly popular. In fact, they introduced new habits, new ways of\ncommunication and they collect every day several information that have\ndifferent sources. Most existing research works fo-cus on the analysis of\nhomogeneous social networks, i.e. we have a single type of node and link in the\nnetwork. However, in the real world, social networks offer several types of\nnodes and links. Hence, with a view to preserve as much information as\npossible, it is important to consider so-cial networks as heterogeneous and\nuncertain. The goal of our paper is to classify the social message based on its\nspreading in the network and the theory of belief functions. The proposed\nclassifier interprets the spread of messages on the network, crossed paths and\ntypes of links. We tested our classifier on a real word network that we\ncollected from Twitter, and our experiments show the performance of our belief\nclassifier.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jan 2015 08:46:46 GMT"}], "update_date": "2015-01-23", "authors_parsed": [["Jendoubi", "Siwar", "", "IRISA"], ["Martin", "Arnaud", "", "IRISA"], ["Li\u00e9tard", "Ludovic", "", "IRISA"], ["Yaghlane", "Boutheina Ben", ""]]}, {"id": "1501.05530", "submitter": "Arnaud Martin", "authors": "Siwar Jendoubi (IRISA), Boutheina Ben Yaghlane, Arnaud Martin (IRISA)", "title": "Belief Hidden Markov Model for speech recognition", "comments": null, "journal-ref": "International Conference on Modeling, Simulation and Applied\n  Optimization (ICMSAO), Apr 2013, Hammamet, Tunisia. pp.1 - 6", "doi": "10.1109/ICMSAO.2013.6552563", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech Recognition searches to predict the spoken words automatically. These\nsystems are known to be very expensive because of using several pre-recorded\nhours of speech. Hence, building a model that minimizes the cost of the\nrecognizer will be very interesting. In this paper, we present a new approach\nfor recognizing speech based on belief HMMs instead of proba-bilistic HMMs.\nExperiments shows that our belief recognizer is insensitive to the lack of the\ndata and it can be trained using only one exemplary of each acoustic unit and\nit gives a good recognition rates. Consequently, using the belief HMM\nrecognizer can greatly minimize the cost of these systems.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jan 2015 15:20:28 GMT"}], "update_date": "2015-01-23", "authors_parsed": [["Jendoubi", "Siwar", "", "IRISA"], ["Yaghlane", "Boutheina Ben", "", "IRISA"], ["Martin", "Arnaud", "", "IRISA"]]}, {"id": "1501.05612", "submitter": "Arnaud Martin", "authors": "Anthony Fiche, Jean-Christophe Cexus, Arnaud Martin (IRISA), Ali\n  Khenchaf", "title": "Features modeling with an $\\alpha$-stable distribution: Application to\n  pattern recognition based on continuous belief functions", "comments": null, "journal-ref": "Information Fusion, Elsevier, 2013, 14, pp.504 - 520", "doi": "10.1016/j.inffus.2013.02.004", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to show the interest in fitting features with an\n$\\alpha$-stable distribution to classify imperfect data. The supervised pattern\nrecognition is thus based on the theory of continuous belief functions, which\nis a way to consider imprecision and uncertainty of data. The distributions of\nfeatures are supposed to be unimodal and estimated by a single Gaussian and\n$\\alpha$-stable model. Experimental results are first obtained from synthetic\ndata by combining two features of one dimension and by considering a vector of\ntwo features. Mass functions are calculated from plausibility functions by\nusing the generalized Bayes theorem. The same study is applied to the automatic\nclassification of three types of sea floor (rock, silt and sand) with features\nacquired by a mono-beam echo-sounder. We evaluate the quality of the\n$\\alpha$-stable model and the Gaussian model by analyzing qualitative results,\nusing a Kolmogorov-Smirnov test (K-S test), and quantitative results with\nclassification rates. The performances of the belief classifier are compared\nwith a Bayesian approach.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jan 2015 19:55:58 GMT"}], "update_date": "2015-01-23", "authors_parsed": [["Fiche", "Anthony", "", "IRISA"], ["Cexus", "Jean-Christophe", "", "IRISA"], ["Martin", "Arnaud", "", "IRISA"], ["Khenchaf", "Ali", ""]]}, {"id": "1501.05613", "submitter": "Arnaud Martin", "authors": "Jungyeul Park (IRISA), Mouna Chebbah (IRISA), Siwar Jendoubi (IRISA),\n  Arnaud Martin (IRISA)", "title": "Second-Order Belief Hidden Markov Models", "comments": null, "journal-ref": "Belief 2014, Sep 2014, Oxford, United Kingdom. pp.284 - 293", "doi": "10.1007/978-3-319-11191-9_31", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hidden Markov Models (HMMs) are learning methods for pattern recognition. The\nprobabilistic HMMs have been one of the most used techniques based on the\nBayesian model. First-order probabilistic HMMs were adapted to the theory of\nbelief functions such that Bayesian probabilities were replaced with mass\nfunctions. In this paper, we present a second-order Hidden Markov Model using\nbelief functions. Previous works in belief HMMs have been focused on the\nfirst-order HMMs. We extend them to the second-order model.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jan 2015 19:56:34 GMT"}], "update_date": "2015-01-23", "authors_parsed": [["Park", "Jungyeul", "", "IRISA"], ["Chebbah", "Mouna", "", "IRISA"], ["Jendoubi", "Siwar", "", "IRISA"], ["Martin", "Arnaud", "", "IRISA"]]}, {"id": "1501.05614", "submitter": "Arnaud Martin", "authors": "Mouloud Kharoune (IRISA), Arnaud Martin (IRISA)", "title": "Int{\\'e}gration d'une mesure d'ind{\\'e}pendance pour la fusion\n  d'informations", "comments": "in French, appears in Atelier Fouille de donn{\\'e}es complexes,\n  Extraction et Gestion des Connaissances (EGC), Jan 2013, Toulouse, France.\n  arXiv admin note: substantial text overlap with arXiv:1501.04786", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many information sources are considered into data fusion in order to improve\nthe decision in terms of uncertainty and imprecision. For each technique used\nfor data fusion, the asumption on independance is usually made. We propose in\nthis article an approach to take into acount an independance measure befor to\nmake the combination of information in the context of the theory of belief\nfunctions.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jan 2015 19:57:59 GMT"}], "update_date": "2015-01-23", "authors_parsed": [["Kharoune", "Mouloud", "", "IRISA"], ["Martin", "Arnaud", "", "IRISA"]]}, {"id": "1501.05677", "submitter": "David Tolpin", "authors": "David Tolpin, Jan Willem van de Meent, Brooks Paige, Frank Wood", "title": "Output-Sensitive Adaptive Metropolis-Hastings for Probabilistic Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an adaptive output-sensitive Metropolis-Hastings algorithm for\nprobabilistic models expressed as programs, Adaptive Lightweight\nMetropolis-Hastings (AdLMH). The algorithm extends Lightweight\nMetropolis-Hastings (LMH) by adjusting the probabilities of proposing random\nvariables for modification to improve convergence of the program output. We\nshow that AdLMH converges to the correct equilibrium distribution and compare\nconvergence of AdLMH to that of LMH on several test problems to highlight\ndifferent aspects of the adaptation scheme. We observe consistent improvement\nin convergence on the test problems.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jan 2015 22:42:36 GMT"}, {"version": "v2", "created": "Tue, 5 May 2015 20:46:01 GMT"}], "update_date": "2015-05-07", "authors_parsed": [["Tolpin", "David", ""], ["van de Meent", "Jan Willem", ""], ["Paige", "Brooks", ""], ["Wood", "Frank", ""]]}, {"id": "1501.05724", "submitter": "Arnaud Martin", "authors": "Amira Essaid, Arnaud Martin (IRISA), Gr\\'egory Smits, Boutheina Ben\n  Yaghlane", "title": "Uncertainty in Ontology Matching: A Decision Rule-Based Approach", "comments": null, "journal-ref": "International Conference on Information Processing and Management\n  of Uncertainty in Knowledge-Based Systems (IPMU), Jul 2014, Montpellier,\n  France. pp.46 - 55", "doi": "10.1007/978-3-319-08795-5_6", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Considering the high heterogeneity of the ontologies pub-lished on the web,\nontology matching is a crucial issue whose aim is to establish links between an\nentity of a source ontology and one or several entities from a target ontology.\nPerfectible similarity measures, consid-ered as sources of information, are\ncombined to establish these links. The theory of belief functions is a powerful\nmathematical tool for combining such uncertain information. In this paper, we\nintroduce a decision pro-cess based on a distance measure to identify the best\npossible matching entities for a given source entity.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jan 2015 07:17:37 GMT"}], "update_date": "2015-01-26", "authors_parsed": [["Essaid", "Amira", "", "IRISA"], ["Martin", "Arnaud", "", "IRISA"], ["Smits", "Gr\u00e9gory", ""], ["Yaghlane", "Boutheina Ben", ""]]}, {"id": "1501.05882", "submitter": "Anand Subramanian D.Sc.", "authors": "Anand Subramanian, Katyanne Farias", "title": "Efficient local search limitation strategy for single machine total\n  weighted tardiness scheduling with sequence-dependent setup times", "comments": "32 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper concerns the single machine total weighted tardiness scheduling\nwith sequence-dependent setup times, usually referred as $1|s_{ij}|\\sum\nw_jT_j$. In this $\\mathcal{NP}$-hard problem, each job has an associated\nprocessing time, due date and a weight. For each pair of jobs $i$ and $j$,\nthere may be a setup time before starting to process $j$ in case this job is\nscheduled immediately after $i$. The objective is to determine a schedule that\nminimizes the total weighted tardiness, where the tardiness of a job is equal\nto its completion time minus its due date, in case the job is completely\nprocessed only after its due date, and is equal to zero otherwise. Due to its\ncomplexity, this problem is most commonly solved by heuristics. The aim of this\nwork is to develop a simple yet effective limitation strategy that speeds up\nthe local search procedure without a significant loss in the solution quality.\nSuch strategy consists of a filtering mechanism that prevents unpromising moves\nto be evaluated. The proposed strategy has been embedded in a local search\nbased metaheuristic from the literature and tested in classical benchmark\ninstances. Computational experiments revealed that the limitation strategy\nenabled the metaheuristic to be extremely competitive when compared to other\nalgorithms from the literature, since it allowed the use of a large number of\nneighborhood structures without a significant increase in the CPU time and,\nconsequently, high quality solutions could be achieved in a matter of seconds.\nIn addition, we analyzed the effectiveness of the proposed strategy in two\nother well-known metaheuristics. Further experiments were also carried out on\nbenchmark instances of problem $1|s_{ij}|\\sum T_j$.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jan 2015 17:20:50 GMT"}, {"version": "v2", "created": "Mon, 26 Jan 2015 02:41:26 GMT"}, {"version": "v3", "created": "Mon, 30 Nov 2015 21:13:24 GMT"}], "update_date": "2015-12-02", "authors_parsed": [["Subramanian", "Anand", ""], ["Farias", "Katyanne", ""]]}, {"id": "1501.05917", "submitter": "Igor Subbotin", "authors": "Igor Yakov Subbotin", "title": "On Generalized Rectangular Fuzzy Model for Assessment", "comments": "arXiv admin note: text overlap with arXiv:1404.7279 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article is dedicated to the analysis of the existing models for\nassessment based of the fuzzy logic centroid technique. A new Generalized\nRectangular Model were developed. Some generalizations of the existing models\nare offered.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jan 2015 19:54:57 GMT"}], "update_date": "2015-01-26", "authors_parsed": [["Subbotin", "Igor Yakov", ""]]}, {"id": "1501.05940", "submitter": "Taoufik Rachad", "authors": "T. Rachad, J. Boutahar and S. El ghazi", "title": "A New Efficient Method for Calculating Similarity Between Web Services", "comments": "7 pages, 4 figures, 8 tables, International Journal of Advanced\n  Computer Science and Applications (IJACSA),Vol. 5, No. 8, 2014", "journal-ref": null, "doi": "10.14569/IJACSA.2014.050809", "report-no": null, "categories": "cs.AI cs.CL cs.IR cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web services allow communication between heterogeneous systems in a\ndistributed environment. Their enormous success and their increased use led to\nthe fact that thousands of Web services are present on the Internet. This\nsignificant number of Web services which not cease to increase has led to\nproblems of the difficulty in locating and classifying web services, these\nproblems are encountered mainly during the operations of web services discovery\nand substitution. Traditional ways of search based on keywords are not\nsuccessful in this context, their results do not support the structure of Web\nservices and they consider in their search only the identifiers of the web\nservice description language (WSDL) interface elements. The methods based on\nsemantics (WSDLS, OWLS, SAWSDL...) which increase the WSDL description of a Web\nservice with a semantic description allow raising partially this problem, but\ntheir complexity and difficulty delays their adoption in real cases. Measuring\nthe similarity between the web services interfaces is the most suitable\nsolution for this kind of problems, it will classify available web services so\nas to know those that best match the searched profile and those that do not\nmatch. Thus, the main goal of this work is to study the degree of similarity\nbetween any two web services by offering a new method that is more effective\nthan existing works.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jan 2015 22:18:45 GMT"}], "update_date": "2015-01-27", "authors_parsed": [["Rachad", "T.", ""], ["Boutahar", "J.", ""], ["ghazi", "S. El", ""]]}, {"id": "1501.05973", "submitter": "Ashish Kapoor", "authors": "Ashish Kapoor, E. Paxon Frady, Stefanie Jegelka, William B. Kristan\n  and Eric Horvitz", "title": "Inferring and Learning from Neuronal Correspondences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and study methods for inferring and learning from\ncorrespondences among neurons. The approach enables alignment of data from\ndistinct multiunit studies of nervous systems. We show that the methods for\ninferring correspondences combine data effectively from cross-animal studies to\nmake joint inferences about behavioral decision making that are not possible\nwith the data from a single animal. We focus on data collection, machine\nlearning, and prediction in the representative and long-studied invertebrate\nnervous system of the European medicinal leech. Acknowledging the computational\nintractability of the general problem of identifying correspondences among\nneurons, we introduce efficient computational procedures for matching neurons\nacross animals. The methods include techniques that adjust for missing cells or\nadditional cells in the different data sets that may reflect biological or\nexperimental variation. The methods highlight the value harnessing inference\nand learning in new kinds of computational microscopes for multiunit\nneurobiological studies.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jan 2015 22:29:13 GMT"}, {"version": "v2", "created": "Tue, 27 Jan 2015 08:23:24 GMT"}], "update_date": "2015-01-28", "authors_parsed": [["Kapoor", "Ashish", ""], ["Frady", "E. Paxon", ""], ["Jegelka", "Stefanie", ""], ["Kristan", "William B.", ""], ["Horvitz", "Eric", ""]]}, {"id": "1501.06206", "submitter": "Radhakrishnan Delhibabu", "authors": "Radhakrishnan Delhibabu", "title": "Dynamics of Belief: Abduction, Horn Knowledge Base And Database Updates", "comments": "arXiv admin note: substantial text overlap with arXiv:1411.2499,\n  arXiv:1405.2642, arXiv:1407.3512, arXiv:1301.5154", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.DB", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The dynamics of belief and knowledge is one of the major components of any\nautonomous system that should be able to incorporate new pieces of information.\nIn order to apply the rationality result of belief dynamics theory to various\npractical problems, it should be generalized in two respects: first it should\nallow a certain part of belief to be declared as immutable; and second, the\nbelief state need not be deductively closed. Such a generalization of belief\ndynamics, referred to as base dynamics, is presented in this paper, along with\nthe concept of a generalized revision algorithm for knowledge bases (Horn or\nHorn logic with stratified negation). We show that knowledge base dynamics has\nan interesting connection with kernel change via hitting set and abduction. In\nthis paper, we show how techniques from disjunctive logic programming can be\nused for efficient (deductive) database updates. The key idea is to transform\nthe given database together with the update request into a disjunctive\n(datalog) logic program and apply disjunctive techniques (such as minimal model\nreasoning) to solve the original update problem. The approach extends and\nintegrates standard techniques for efficient query answering and integrity\nchecking. The generation of a hitting set is carried out through a hyper\ntableaux calculus and magic set that is focused on the goal of minimality. The\npresent paper provides a comparative study of view update algorithms in\nrational approach. For, understand the basic concepts with abduction, we\nprovide an abductive framework for knowledge base dynamics. Finally, we\ndemonstrate how belief base dynamics can provide an axiomatic characterization\nfor insertion a view atom to the database. We give a quick overview of the main\noperators for belief change, in particular, belief update versus database\nupdate.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jan 2015 20:48:53 GMT"}, {"version": "v2", "created": "Tue, 27 Jan 2015 09:59:49 GMT"}], "update_date": "2015-01-28", "authors_parsed": [["Delhibabu", "Radhakrishnan", ""]]}, {"id": "1501.06595", "submitter": "Sahin Geyik", "authors": "Sahin Cem Geyik, Ali Dasdan, Kuang-Chih Lee", "title": "User Clustering in Online Advertising via Topic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the domain of online advertising, our aim is to serve the best ad to a\nuser who visits a certain webpage, to maximize the chance of a desired action\nto be performed by this user after seeing the ad. While it is possible to\ngenerate a different prediction model for each user to tell if he/she will act\non a given ad, the prediction result typically will be quite unreliable with\nhuge variance, since the desired actions are extremely sparse, and the set of\nusers is huge (hundreds of millions) and extremely volatile, i.e., a lot of new\nusers are introduced everyday, or are no longer valid. In this paper we aim to\nimprove the accuracy in finding users who will perform the desired action, by\nassigning each user to a cluster, where the number of clusters is much smaller\nthan the number of users (in the order of hundreds). Each user will fall into\nthe same cluster with another user if their event history are similar. For this\npurpose, we modify the probabilistic latent semantic analysis (pLSA) model by\nassuming the independence of the user and the cluster id, given the history of\nevents. This assumption helps us to identify a cluster of a new user without\nre-clustering all the users. We present the details of the algorithm we\nemployed as well as the distributed implementation on Hadoop, and some initial\nresults on the clusters that were generated by the algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jan 2015 21:44:08 GMT"}, {"version": "v2", "created": "Tue, 24 Feb 2015 02:49:32 GMT"}], "update_date": "2015-02-25", "authors_parsed": [["Geyik", "Sahin Cem", ""], ["Dasdan", "Ali", ""], ["Lee", "Kuang-Chih", ""]]}, {"id": "1501.06705", "submitter": "Arnaud Martin", "authors": "Dorra Attiaoui (IRISA), Pierre-Emmanuel Dor\\'e, Arnaud Martin (IRISA),\n  Boutheina Ben Yaghlane", "title": "Inclusion within Continuous Belief Functions", "comments": "International Conference on Information Fusion - (FUSION 2013), Jul\n  2013, Istanbul, Turkey", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Defining and modeling the relation of inclusion between continuous belief\nfunction may be considered as an important operation in order to study their\nbehaviors. Within this paper we will propose and present two forms of\ninclusion: The strict and the partial one. In order to develop this relation,\nwe will study the case of consonant belief function. To do so, we will simulate\nnormal distributions allowing us to model and analyze these relations. Based on\nthat, we will determine the parameters influencing and characterizing the two\nforms of inclusion.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jan 2015 09:23:23 GMT"}], "update_date": "2015-01-28", "authors_parsed": [["Attiaoui", "Dorra", "", "IRISA"], ["Dor\u00e9", "Pierre-Emmanuel", "", "IRISA"], ["Martin", "Arnaud", "", "IRISA"], ["Yaghlane", "Boutheina Ben", ""]]}, {"id": "1501.06727", "submitter": "Jose M. Pe\\~na", "authors": "Jose M. Pe\\~na", "title": "Factorization, Inference and Parameter Learning in Discrete AMP Chain\n  Graphs", "comments": null, "journal-ref": "Proceedings of the 13th European Conference on Symbolic and\n  Quantitative Approaches to Reasoning under Uncertainty (ECSQARU 2015),\n  Lecture Notes in Artificial Intelligence 9161, 335-345", "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address some computational issues that may hinder the use of AMP chain\ngraphs in practice. Specifically, we show how a discrete probability\ndistribution that satisfies all the independencies represented by an AMP chain\ngraph factorizes according to it. We show how this factorization makes it\npossible to perform inference and parameter learning efficiently, by adapting\nexisting algorithms for Markov and Bayesian networks. Finally, we turn our\nattention to another issue that may hinder the use of AMP CGs, namely the lack\nof an intuitive interpretation of their edges. We provide one such\ninterpretation.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jan 2015 10:28:19 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2015 15:10:19 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Pe\u00f1a", "Jose M.", ""]]}, {"id": "1501.06769", "submitter": "Jan-Willem van de Meent", "authors": "Jan-Willem van de Meent and Hongseok Yang and Vikash Mansinghka and\n  Frank Wood", "title": "Particle Gibbs with Ancestor Sampling for Probabilistic Programs", "comments": "9 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Particle Markov chain Monte Carlo techniques rank among current\nstate-of-the-art methods for probabilistic program inference. A drawback of\nthese techniques is that they rely on importance resampling, which results in\ndegenerate particle trajectories and a low effective sample size for variables\nsampled early in a program. We here develop a formalism to adapt ancestor\nresampling, a technique that mitigates particle degeneracy, to the\nprobabilistic programming setting. We present empirical results that\ndemonstrate nontrivial performance gains.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jan 2015 14:16:59 GMT"}, {"version": "v2", "created": "Wed, 28 Jan 2015 18:26:55 GMT"}, {"version": "v3", "created": "Mon, 2 Feb 2015 19:42:52 GMT"}, {"version": "v4", "created": "Wed, 4 Feb 2015 16:12:05 GMT"}, {"version": "v5", "created": "Mon, 9 Feb 2015 23:49:26 GMT"}], "update_date": "2015-02-11", "authors_parsed": [["van de Meent", "Jan-Willem", ""], ["Yang", "Hongseok", ""], ["Mansinghka", "Vikash", ""], ["Wood", "Frank", ""]]}, {"id": "1501.07008", "submitter": "Arnaud Martin", "authors": "Amira Essaid (IRISA), Arnaud Martin (IRISA), Gr\\'egory Smits,\n  Boutheina Ben Yaghlane", "title": "A Distance-Based Decision in the Credal Level", "comments": null, "journal-ref": "International Conference on Artificial Intelligence and Symbolic\n  Computation (AISC 2014), Dec 2014, Sevilla, Spain. pp.147 - 156", "doi": "10.1007/978-3-319-13770-4_13", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Belief function theory provides a flexible way to combine information\nprovided by different sources. This combination is usually followed by a\ndecision making which can be handled by a range of decision rules. Some rules\nhelp to choose the most likely hypothesis. Others allow that a decision is made\non a set of hypotheses. In [6], we proposed a decision rule based on a distance\nmeasure. First, in this paper, we aim to demonstrate that our proposed decision\nrule is a particular case of the rule proposed in [4]. Second, we give\nexperiments showing that our rule is able to decide on a set of hypotheses.\nSome experiments are handled on a set of mass functions generated randomly,\nothers on real databases.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jan 2015 07:24:12 GMT"}], "update_date": "2015-01-29", "authors_parsed": [["Essaid", "Amira", "", "IRISA"], ["Martin", "Arnaud", "", "IRISA"], ["Smits", "Gr\u00e9gory", ""], ["Yaghlane", "Boutheina Ben", ""]]}, {"id": "1501.07250", "submitter": "Alejandro Torre\\~no", "authors": "Alejandro Torre\\~no, Eva Onaindia, \\'Oscar Sapena", "title": "FMAP: Distributed Cooperative Multi-Agent Planning", "comments": "21 pages, 11 figures", "journal-ref": "Applied Intelligence, Volume 41, Issue 2, pp. 606-626, Year 2014", "doi": "10.1007/s10489-014-0540-2", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes FMAP (Forward Multi-Agent Planning), a fully-distributed\nmulti-agent planning method that integrates planning and coordination. Although\nFMAP is specifically aimed at solving problems that require cooperation among\nagents, the flexibility of the domain-independent planning model allows FMAP to\ntackle multi-agent planning tasks of any type. In FMAP, agents jointly explore\nthe plan space by building up refinement plans through a complete and flexible\nforward-chaining partial-order planner. The search is guided by $h_{DTG}$, a\nnovel heuristic function that is based on the concepts of Domain Transition\nGraph and frontier state and is optimized to evaluate plans in distributed\nenvironments. Agents in FMAP apply an advanced privacy model that allows them\nto adequately keep private information while communicating only the data of the\nrefinement plans that is relevant to each of the participating agents.\nExperimental results show that FMAP is a general-purpose approach that\nefficiently solves tightly-coupled domains that have specialized agents and\ncooperative goals as well as loosely-coupled problems. Specifically, the\nempirical evaluation shows that FMAP outperforms current MAP systems at solving\ncomplex planning tasks that are adapted from the International Planning\nCompetition benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jan 2015 19:38:35 GMT"}], "update_date": "2015-01-30", "authors_parsed": [["Torre\u00f1o", "Alejandro", ""], ["Onaindia", "Eva", ""], ["Sapena", "\u00d3scar", ""]]}, {"id": "1501.07256", "submitter": "Alejandro Torre\\~no", "authors": "Alejandro Torre\\~no, Eva Onaindia, \\'Oscar Sapena", "title": "An approach to multi-agent planning with incomplete information", "comments": "6 pages, 2 figures", "journal-ref": "20th European Conference of Artificial Intelligence (ECAI 2012),\n  Volume 242, pp. 762-767, Year 2012", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-agent planning (MAP) approaches have been typically conceived for\nindependent or loosely-coupled problems to enhance the benefits of distributed\nplanning between autonomous agents as solving this type of problems require\nless coordination between the agents' sub-plans. However, when it comes to\ntightly-coupled agents' tasks, MAP has been relegated in favour of centralized\napproaches and little work has been done in this direction. In this paper, we\npresent a general-purpose MAP capable to efficiently handle planning problems\nwith any level of coupling between agents. We propose a cooperative refinement\nplanning approach, built upon the partial-order planning paradigm, that allows\nagents to work with incomplete information and to have incomplete views of the\nworld, i.e. being ignorant of other agents' information, as well as maintaining\ntheir own private information. We show various experiments to compare the\nperformance of our system with a distributed CSP-based MAP approach over a\nsuite of problems.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jan 2015 20:02:14 GMT"}], "update_date": "2015-01-30", "authors_parsed": [["Torre\u00f1o", "Alejandro", ""], ["Onaindia", "Eva", ""], ["Sapena", "\u00d3scar", ""]]}, {"id": "1501.07423", "submitter": "Alejandro Torre\\~no", "authors": "Alejandro Torre\\~no, Eva Onaindia, \\'Oscar Sapena", "title": "A Flexible Coupling Approach to Multi-Agent Planning under Incomplete\n  Information", "comments": "40 pages, 10 figures", "journal-ref": "Knowledge and Information Systems, Volume 38, Issue 1, pp.\n  141-178, Year 2014", "doi": "10.1007/s10115-012-0569-7", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-agent planning (MAP) approaches are typically oriented at solving\nloosely-coupled problems, being ineffective to deal with more complex,\nstrongly-related problems. In most cases, agents work under complete\ninformation, building complete knowledge bases. The present article introduces\na general-purpose MAP framework designed to tackle problems of any coupling\nlevels under incomplete information. Agents in our MAP model are partially\nunaware of the information managed by the rest of agents and share only the\ncritical information that affects other agents, thus maintaining a distributed\nvision of the task.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jan 2015 11:56:41 GMT"}], "update_date": "2015-01-30", "authors_parsed": [["Torre\u00f1o", "Alejandro", ""], ["Onaindia", "Eva", ""], ["Sapena", "\u00d3scar", ""]]}]