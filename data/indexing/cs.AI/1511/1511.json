[{"id": "1511.00041", "submitter": "Murat Kocaoglu", "authors": "Karthikeyan Shanmugam, Murat Kocaoglu, Alexandros G. Dimakis, Sriram\n  Vishwanath", "title": "Learning Causal Graphs with Small Interventions", "comments": "Accepted to NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IT cs.LG math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning causal networks with interventions, when\neach intervention is limited in size under Pearl's Structural Equation Model\nwith independent errors (SEM-IE). The objective is to minimize the number of\nexperiments to discover the causal directions of all the edges in a causal\ngraph. Previous work has focused on the use of separating systems for complete\ngraphs for this task. We prove that any deterministic adaptive algorithm needs\nto be a separating system in order to learn complete graphs in the worst case.\nIn addition, we present a novel separating system construction, whose size is\nclose to optimal and is arguably simpler than previous work in combinatorics.\nWe also develop a novel information theoretic lower bound on the number of\ninterventions that applies in full generality, including for randomized\nadaptive learning algorithms.\n  For general chordal graphs, we derive worst case lower bounds on the number\nof interventions. Building on observations about induced trees, we give a new\ndeterministic adaptive algorithm to learn directions on any chordal skeleton\ncompletely. In the worst case, our achievable scheme is an\n$\\alpha$-approximation algorithm where $\\alpha$ is the independence number of\nthe graph. We also show that there exist graph classes for which the sufficient\nnumber of experiments is close to the lower bound. In the other extreme, there\nare graph classes for which the required number of experiments is\nmultiplicatively $\\alpha$ away from our lower bound.\n  In simulations, our algorithm almost always performs very close to the lower\nbound, while the approach based on separating systems for complete graphs is\nsignificantly worse for random chordal graphs.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2015 22:24:13 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Shanmugam", "Karthikeyan", ""], ["Kocaoglu", "Murat", ""], ["Dimakis", "Alexandros G.", ""], ["Vishwanath", "Sriram", ""]]}, {"id": "1511.00043", "submitter": "Arunesh Sinha", "authors": "Arunesh Sinha, Debarun Kar, Milind Tambe", "title": "Learning Adversary Behavior in Security Games: A PAC Model Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent applications of Stackelberg Security Games (SSG), from wildlife crime\nto urban crime, have employed machine learning tools to learn and predict\nadversary behavior using available data about defender-adversary interactions.\nGiven these recent developments, this paper commits to an approach of directly\nlearning the response function of the adversary. Using the PAC model, this\npaper lays a firm theoretical foundation for learning in SSGs (e.g.,\ntheoretically answer questions about the numbers of samples required to learn\nadversary behavior) and provides utility guarantees when the learned adversary\nmodel is used to plan the defender's strategy. The paper also aims to answer\npractical questions such as how much more data is needed to improve an\nadversary model's accuracy. Additionally, we explain a recently observed\nphenomenon that prediction accuracy of learned adversary behavior is not enough\nto discover the utility maximizing defender strategy. We provide four main\ncontributions: (1) a PAC model of learning adversary response functions in\nSSGs; (2) PAC-model analysis of the learning of key, existing bounded\nrationality models in SSGs; (3) an entirely new approach to adversary modeling\nbased on a non-parametric class of response functions with PAC-model analysis\nand (4) identification of conditions under which computing the best defender\nstrategy against the learned adversary behavior is indeed the optimal strategy.\nFinally, we conduct experiments with real-world data from a national park in\nUganda, showing the benefit of our new adversary modeling approach and\nverification of our PAC model predictions.\n", "versions": [{"version": "v1", "created": "Fri, 30 Oct 2015 22:27:25 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2015 17:51:17 GMT"}, {"version": "v3", "created": "Fri, 20 Nov 2015 08:34:43 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Sinha", "Arunesh", ""], ["Kar", "Debarun", ""], ["Tambe", "Milind", ""]]}, {"id": "1511.00083", "submitter": "Subutai Ahmad", "authors": "Jeff Hawkins and Subutai Ahmad", "title": "Why Neurons Have Thousands of Synapses, A Theory of Sequence Memory in\n  Neocortex", "comments": "Submitted for publication", "journal-ref": "Frontiers in Neural Circuits 10:23 (2016) 1-13", "doi": "10.3389/fncir.2016.00023", "report-no": null, "categories": "q-bio.NC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neocortical neurons have thousands of excitatory synapses. It is a mystery\nhow neurons integrate the input from so many synapses and what kind of\nlarge-scale network behavior this enables. It has been previously proposed that\nnon-linear properties of dendrites enable neurons to recognize multiple\npatterns. In this paper we extend this idea by showing that a neuron with\nseveral thousand synapses arranged along active dendrites can learn to\naccurately and robustly recognize hundreds of unique patterns of cellular\nactivity, even in the presence of large amounts of noise and pattern variation.\nWe then propose a neuron model where some of the patterns recognized by a\nneuron lead to action potentials and define the classic receptive field of the\nneuron, whereas the majority of the patterns recognized by a neuron act as\npredictions by slightly depolarizing the neuron without immediately generating\nan action potential. We then present a network model based on neurons with\nthese properties and show that the network learns a robust model of time-based\nsequences. Given the similarity of excitatory neurons throughout the neocortex\nand the importance of sequence memory in inference and behavior, we propose\nthat this form of sequence memory is a universal property of neocortical\ntissue. We further propose that cellular layers in the neocortex implement\nvariations of the same sequence memory algorithm to achieve different aspects\nof inference and behavior. The neuron and network models we introduce are\nrobust over a wide range of parameters as long as the network uses a sparse\ndistributed code of cellular activations. The sequence capacity of the network\nscales linearly with the number of synapses on each neuron. Thus neurons need\nthousands of synapses to learn the many temporal patterns in sensory stimuli\nand motor sequences.\n", "versions": [{"version": "v1", "created": "Sat, 31 Oct 2015 06:03:05 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2015 21:20:26 GMT"}], "update_date": "2016-04-25", "authors_parsed": [["Hawkins", "Jeff", ""], ["Ahmad", "Subutai", ""]]}, {"id": "1511.00384", "submitter": "Arthur Ryman", "authors": "Arthur Ryman", "title": "Z Specification for the W3C Editor's Draft Core SHACL Semantics", "comments": "57 pages, Invited Expert contribution to the W3C RDF Data Shapes\n  Working Group", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article provides a formalization of the W3C Draft Core SHACL Semantics\nspecification using Z notation. This formalization exercise has identified a\nnumber of quality issues in the draft. It has also established that the\nrecursive definitions in the draft are well-founded. Further formal validation\nof the draft will require the use of an executable specification technology.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 05:31:42 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Ryman", "Arthur", ""]]}, {"id": "1511.00573", "submitter": "Tatsunori Hashimoto", "authors": "Tatsunori B. Hashimoto, Yi Sun, Tommi S. Jaakkola", "title": "From random walks to distances on unweighted graphs", "comments": "To appear in NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large unweighted directed graphs are commonly used to capture relations\nbetween entities. A fundamental problem in the analysis of such networks is to\nproperly define the similarity or dissimilarity between any two vertices.\nDespite the significance of this problem, statistical characterization of the\nproposed metrics has been limited. We introduce and develop a class of\ntechniques for analyzing random walks on graphs using stochastic calculus.\nUsing these techniques we generalize results on the degeneracy of hitting times\nand analyze a metric based on the Laplace transformed hitting time (LTHT). The\nmetric serves as a natural, provably well-behaved alternative to the expected\nhitting time. We establish a general correspondence between hitting times of\nthe Brownian motion and analogous hitting times on the graph. We show that the\nLTHT is consistent with respect to the underlying metric of a geometric graph,\npreserves clustering tendency, and remains robust against random addition of\nnon-geometric edges. Tests on simulated and real-world data show that the LTHT\nmatches theoretical predictions and outperforms alternatives.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 16:23:06 GMT"}], "update_date": "2015-11-03", "authors_parsed": [["Hashimoto", "Tatsunori B.", ""], ["Sun", "Yi", ""], ["Jaakkola", "Tommi S.", ""]]}, {"id": "1511.00725", "submitter": "Wajdi Dhifli", "authors": "Wajdi Dhifli, Abdoulaye Banir\\'e Diallo", "title": "Toward an Efficient Multi-class Classification in an Open Universe", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification is a fundamental task in machine learning and data mining.\nExisting classification methods are designed to classify unknown instances\nwithin a set of previously known training classes. Such a classification takes\nthe form of a prediction within a closed-set of classes. However, a more\nrealistic scenario that fits real-world applications is to consider the\npossibility of encountering instances that do not belong to any of the training\nclasses, $i.e.$, an open-set classification. In such situation, existing\nclosed-set classifiers will assign a training label to these instances\nresulting in a misclassification. In this paper, we introduce Galaxy-X, a novel\nmulti-class classification approach for open-set recognition problems. For each\nclass of the training set, Galaxy-X creates a minimum bounding hyper-sphere\nthat encompasses the distribution of the class by enclosing all of its\ninstances. In such manner, our method is able to distinguish instances\nresembling previously seen classes from those that are of unknown ones. To\nadequately evaluate open-set classification, we introduce a novel evaluation\nprocedure. Experimental results on benchmark datasets show the efficiency of\nour approach in classifying novel instances from known as well as unknown\nclasses.\n", "versions": [{"version": "v1", "created": "Mon, 2 Nov 2015 22:04:00 GMT"}, {"version": "v2", "created": "Mon, 25 Jan 2016 02:27:55 GMT"}, {"version": "v3", "created": "Thu, 1 Mar 2018 17:22:56 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Dhifli", "Wajdi", ""], ["Diallo", "Abdoulaye Banir\u00e9", ""]]}, {"id": "1511.00787", "submitter": "Alexander Lavin", "authors": "Alexander Lavin", "title": "A Pareto Optimal D* Search Algorithm for Multiobjective Path Planning", "comments": "arXiv admin note: substantial text overlap with arXiv:1505.05947", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Path planning is one of the most vital elements of mobile robotics, providing\nthe agent with a collision-free route through the workspace. The global path\nplan can be calculated with a variety of informed search algorithms, most\nnotably the A* search method, guaranteed to deliver a complete and optimal\nsolution that minimizes the path cost. D* is widely used for its dynamic\nreplanning capabilities. Path planning optimization typically looks to minimize\nthe distance traversed from start to goal, but many mobile robot applications\ncall for additional path planning objectives, presenting a multiobjective\noptimization (MOO) problem. Common search algorithms, e.g. A* and D*, are not\nwell suited for MOO problems, yielding suboptimal results. The search algorithm\npresented in this paper is designed for optimal MOO path planning. The\nalgorithm incorporates Pareto optimality into D*, and is thus named D*-PO.\nNon-dominated solution paths are guaranteed by calculating the Pareto front at\neach search step. Simulations were run to model a planetary exploration rover\nin a Mars environment, with five path costs. The results show the new, Pareto\noptimal D*-PO outperforms the traditional A* and D* algorithms for MOO path\nplanning.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 05:48:26 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Lavin", "Alexander", ""]]}, {"id": "1511.00813", "submitter": "Olivier Bailleux", "authors": "Olivier Bailleux", "title": "SAT as a game", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a funny representation of SAT. While the primary interest is to\npresent propositional satisfiability in a playful way for pedagogical purposes,\nit could also inspire new search heuristics.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 08:53:01 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Bailleux", "Olivier", ""]]}, {"id": "1511.00840", "submitter": "Konstantin Yakovlev S", "authors": "Konstantin Yakovlev, Egor Baskin, Ivan Hramoin", "title": "Finetuning Randomized Heuristic Search For 2D Path Planning: Finding The\n  Best Input Parameters For R* Algorithm Through Series Of Experiments", "comments": "8 pages, 2 figures, 18 references. As accepted to the 16th\n  International Conference on Artificial Intelligence:Methodology, Systems,\n  Applications (AIMSA 2014), Varna, Bulgaria, September 11-13, 2014", "journal-ref": null, "doi": "10.1007/978-3-319-10554-3_29", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Path planning is typically considered in Artificial Intelligence as a graph\nsearching problem and R* is state-of-the-art algorithm tailored to solve it.\nThe algorithm decomposes given path finding task into the series of subtasks\neach of which can be easily (in computational sense) solved by well-known\nmethods (such as A*). Parameterized random choice is used to perform the\ndecomposition and as a result R* performance largely depends on the choice of\nits input parameters. In our work we formulate a range of assumptions\nconcerning possible upper and lower bounds of R* parameters, their\ninterdependency and their influence on R* performance. Then we evaluate these\nassumptions by running a large number of experiments. As a result we formulate\na set of heuristic rules which can be used to initialize the values of R*\nparameters in a way that leads to algorithm's best performance.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 09:56:01 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Yakovlev", "Konstantin", ""], ["Baskin", "Egor", ""], ["Hramoin", "Ivan", ""]]}, {"id": "1511.00915", "submitter": "Jan Wielemaker", "authors": "Jan Wielemaker and Torbj\\\"orn Lager and Fabrizio Riguzzi", "title": "SWISH: SWI-Prolog for Sharing", "comments": "International Workshop on User-Oriented Logic Programming (IULP\n  2015), co-located with the 31st International Conference on Logic Programming\n  (ICLP 2015), Proceedings of the International Workshop on User-Oriented Logic\n  Programming (IULP 2015), Editors: Stefan Ellmauthaler and Claudia Schulz,\n  pages 99-113, August 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, we see a new type of interfaces for programmers based on web\ntechnology. For example, JSFiddle, IPython Notebook and R-studio. Web\ntechnology enables cloud-based solutions, embedding in tutorial web pages,\natractive rendering of results, web-scale cooperative development, etc. This\narticle describes SWISH, a web front-end for Prolog. A public website exposes\nSWI-Prolog using SWISH, which is used to run small Prolog programs for\ndemonstration, experimentation and education. We connected SWISH to the\nClioPatria semantic web toolkit, where it allows for collaborative development\nof programs and queries related to a dataset as well as performing maintenance\ntasks on the running server and we embedded SWISH in the Learn Prolog Now!\nonline Prolog book.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 14:16:31 GMT"}], "update_date": "2016-08-06", "authors_parsed": [["Wielemaker", "Jan", ""], ["Lager", "Torbj\u00f6rn", ""], ["Riguzzi", "Fabrizio", ""]]}, {"id": "1511.00916", "submitter": "Joost Vennekens", "authors": "Joost Vennekens", "title": "Lowering the learning curve for declarative programming: a Python API\n  for the IDP system", "comments": "International Workshop on User-Oriented Logic Programming (IULP\n  2015), co-located with the 31st International Conference on Logic Programming\n  (ICLP 2015), Proceedings of the International Workshop on User-Oriented Logic\n  Programming (IULP 2015), Editors: Stefan Ellmauthaler and Claudia Schulz,\n  pages 83-98, August 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Programmers may be hesitant to use declarative systems, because of the\nassociated learning curve. In this paper, we present an API that integrates the\nIDP Knowledge Base system into the Python programming language. IDP is a\nstate-of-the-art logical system, which uses SAT, SMT, Logic Programming and\nAnswer Set Programming technology. Python is currently one of the most widely\nused (teaching) languages for programming. The first goal of our API is to\nallow a Python programmer to use the declarative power of IDP, without needing\nto learn any new syntax or semantics. The second goal is allow IDP to be added\nto/removed from an existing code base with minimal changes.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 14:21:23 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Vennekens", "Joost", ""]]}, {"id": "1511.00920", "submitter": "Ingmar Dasseville", "authors": "Ingmar Dasseville and Gerda Janssens", "title": "A web-based IDE for IDP", "comments": "International Workshop on User-Oriented Logic Programming (IULP\n  2015), co-located with the 31st International Conference on Logic Programming\n  (ICLP 2015), Proceedings of the International Workshop on User-Oriented Logic\n  Programming (IULP 2015), Editors: Stefan Ellmauthaler and Claudia Schulz,\n  pages 21-32, August 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  IDP is a knowledge base system based on first order logic. It is finding its\nway to a larger public but is still facing practical challenges. Adoption of\nnew languages requires a newcomer-friendly way for users to interact with it.\nBoth an online presence to try to convince potential users to download the\nsystem and offline availability to develop larger applications are essential.\nWe developed an IDE which can serve both purposes through the use of web\ntechnology. It enables us to provide the user with a modern IDE with relatively\nlittle effort.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 14:30:10 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Dasseville", "Ingmar", ""], ["Janssens", "Gerda", ""]]}, {"id": "1511.00924", "submitter": "Sarah Alice Gaggl", "authors": "Sarah Alice Gaggl and Sebastian Rudolph and Lukas Schweizer", "title": "Bound Your Models! How to Make OWL an ASP Modeling Language", "comments": "International Workshop on User-Oriented Logic Programming (IULP\n  2015), co-located with the 31st International Conference on Logic Programming\n  (ICLP 2015), Proceedings of the International Workshop on User-Oriented Logic\n  Programming (IULP 2015), Editors: Stefan Ellmauthaler and Claudia Schulz,\n  pages 33-49, August 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To exploit the Web Ontology Language OWL as an answer set programming (ASP)\nlanguage, we introduce the notion of bounded model semantics, as an intuitive\nand computationally advantageous alternative to its classical semantics. We\nshow that a translation into ASP allows for solving a wide range of\nbounded-model reasoning tasks, including satisfiability and axiom entailment\nbut also novel ones such as model extraction and enumeration. Ultimately, our\nwork facilitates harnessing advanced semantic web modeling environments for the\nlogic programming community through an \"off-label use\" of OWL.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 14:38:53 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Gaggl", "Sarah Alice", ""], ["Rudolph", "Sebastian", ""], ["Schweizer", "Lukas", ""]]}, {"id": "1511.00928", "submitter": "Ruben Lapauw", "authors": "Ruben Lapauw and Ingmar Dasseville and Marc Denecker", "title": "Visualising interactive inferences with IDPD3", "comments": "International Workshop on User-Oriented Logic Programming (IULP\n  2015), co-located with the 31st International Conference on Logic Programming\n  (ICLP 2015), Proceedings of the International Workshop on User-Oriented Logic\n  Programming (IULP 2015), Editors: Stefan Ellmauthaler and Claudia Schulz,\n  pages 67-81, August 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A large part of the use of knowledge base systems is the interpretation of\nthe output by the end-users and the interaction with these users. Even during\nthe development process visualisations can be a great help to the developer. We\ncreated IDPD3 as a library to visualise models of logic theories. IDPD3 is a\nnew version of $ID^{P}_{Draw}$ and adds support for visualised interactive\nsimulations.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 14:40:57 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Lapauw", "Ruben", ""], ["Dasseville", "Ingmar", ""], ["Denecker", "Marc", ""]]}, {"id": "1511.01029", "submitter": "Vijay Badrinarayanan", "authors": "Vijay Badrinarayanan and Bamdev Mishra and Roberto Cipolla", "title": "Understanding symmetries in deep networks", "comments": "Accepted at the 8th NIPS Workshop on Optimization for Machine\n  Learning (OPT2015) to be held at Montreal, Canada on December 11, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works have highlighted scale invariance or symmetry present in the\nweight space of a typical deep network and the adverse effect it has on the\nEuclidean gradient based stochastic gradient descent optimization. In this\nwork, we show that a commonly used deep network, which uses convolution, batch\nnormalization, reLU, max-pooling, and sub-sampling pipeline, possess more\ncomplex forms of symmetry arising from scaling-based reparameterization of the\nnetwork weights. We propose to tackle the issue of the weight space symmetry by\nconstraining the filters to lie on the unit-norm manifold. Consequently,\ntraining the network boils down to using stochastic gradient descent updates on\nthe unit-norm manifold. Our empirical evidence based on the MNIST dataset shows\nthat the proposed updates improve the test performance beyond what is achieved\nwith batch normalization and without sacrificing the computational efficiency\nof the weight updates.\n", "versions": [{"version": "v1", "created": "Tue, 3 Nov 2015 18:50:03 GMT"}], "update_date": "2015-11-04", "authors_parsed": [["Badrinarayanan", "Vijay", ""], ["Mishra", "Bamdev", ""], ["Cipolla", "Roberto", ""]]}, {"id": "1511.01411", "submitter": "Vasilis Syrgkanis", "authors": "Constantinos Daskalakis, Vasilis Syrgkanis", "title": "Learning in Auctions: Regret is Hard, Envy is Easy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.CC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A line of recent work provides welfare guarantees of simple combinatorial\nauction formats, such as selling m items via simultaneous second price auctions\n(SiSPAs) (Christodoulou et al. 2008, Bhawalkar and Roughgarden 2011, Feldman et\nal. 2013). These guarantees hold even when the auctions are repeatedly executed\nand players use no-regret learning algorithms. Unfortunately, off-the-shelf\nno-regret algorithms for these auctions are computationally inefficient as the\nnumber of actions is exponential. We show that this obstacle is insurmountable:\nthere are no polynomial-time no-regret algorithms for SiSPAs, unless\nRP$\\supseteq$ NP, even when the bidders are unit-demand. Our lower bound raises\nthe question of how good outcomes polynomially-bounded bidders may discover in\nsuch auctions.\n  To answer this question, we propose a novel concept of learning in auctions,\ntermed \"no-envy learning.\" This notion is founded upon Walrasian equilibrium,\nand we show that it is both efficiently implementable and results in\napproximately optimal welfare, even when the bidders have fractionally\nsubadditive (XOS) valuations (assuming demand oracles) or coverage valuations\n(without demand oracles). No-envy learning outcomes are a relaxation of\nno-regret outcomes, which maintain their approximate welfare optimality while\nendowing them with computational tractability. Our results extend to other\nauction formats that have been studied in the literature via the smoothness\nparadigm.\n  Our results for XOS valuations are enabled by a novel\nFollow-The-Perturbed-Leader algorithm for settings where the number of experts\nis infinite, and the payoff function of the learner is non-linear. This\nalgorithm has applications outside of auction settings, such as in security\ngames. Our result for coverage valuations is based on a novel use of convex\nrounding schemes and a reduction to online convex optimization.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 17:39:30 GMT"}, {"version": "v2", "created": "Thu, 5 Nov 2015 16:48:48 GMT"}, {"version": "v3", "created": "Sat, 7 Nov 2015 19:49:30 GMT"}, {"version": "v4", "created": "Thu, 12 Nov 2015 15:29:10 GMT"}, {"version": "v5", "created": "Sat, 19 Dec 2015 17:41:20 GMT"}, {"version": "v6", "created": "Wed, 6 Apr 2016 17:55:25 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Daskalakis", "Constantinos", ""], ["Syrgkanis", "Vasilis", ""]]}, {"id": "1511.01419", "submitter": "Ofer Meshi", "authors": "Ofer Meshi, Mehrdad Mahdavi, Adrian Weller and David Sontag", "title": "Train and Test Tightness of LP Relaxations in Structured Prediction", "comments": "To appear in ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structured prediction is used in areas such as computer vision and natural\nlanguage processing to predict structured outputs such as segmentations or\nparse trees. In these settings, prediction is performed by MAP inference or,\nequivalently, by solving an integer linear program. Because of the complex\nscoring functions required to obtain accurate predictions, both learning and\ninference typically require the use of approximate solvers. We propose a\ntheoretical explanation to the striking observation that approximations based\non linear programming (LP) relaxations are often tight on real-world instances.\nIn particular, we show that learning with LP relaxed inference encourages\nintegrality of training instances, and that tightness generalizes from train to\ntest data.\n", "versions": [{"version": "v1", "created": "Wed, 4 Nov 2015 18:13:35 GMT"}, {"version": "v2", "created": "Fri, 6 Nov 2015 12:04:24 GMT"}, {"version": "v3", "created": "Wed, 27 Apr 2016 02:58:33 GMT"}], "update_date": "2016-04-28", "authors_parsed": [["Meshi", "Ofer", ""], ["Mahdavi", "Mehrdad", ""], ["Weller", "Adrian", ""], ["Sontag", "David", ""]]}, {"id": "1511.01640", "submitter": "Vilem Vychodil", "authors": "Vilem Vychodil", "title": "Computing sets of graded attribute implications with witnessed\n  non-redundancy", "comments": null, "journal-ref": "Information Sciences 351 (2016), 90-100", "doi": "10.1016/j.ins.2016.03.004", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we extend our previous results on sets of graded attribute\nimplications with witnessed non-redundancy. We assume finite residuated\nlattices as structures of truth degrees and use arbitrary idempotent\ntruth-stressing linguistic hedges as parameters which influence the semantics\nof graded attribute implications. In this setting, we introduce algorithm which\ntransforms any set of graded attribute implications into an equivalent\nnon-redundant set of graded attribute implications with saturated consequents\nwhose non-redundancy is witnessed by antecedents of the formulas. As a\nconsequence, we solve the open problem regarding the existence of general\nsystems of pseudo-intents which appear in formal concept analysis of\nobject-attribute data with graded attributes and linguistic hedges.\nFurthermore, we show a polynomial-time procedure for determining bases given by\ngeneral systems of pseudo-intents from sets of graded attribute implications\nwhich are complete in data.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 07:47:41 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Vychodil", "Vilem", ""]]}, {"id": "1511.01710", "submitter": "Jordi Grau-Moya", "authors": "Jordi Grau-Moya and Daniel A. Braun", "title": "Adaptive information-theoretic bounded rational decision-making with\n  parametric priors", "comments": "4 pages, 1 figure, Workshop on Bounded Optimality and Rational\n  Metareasoning at Neural Information Processing Systems conference, Montreal,\n  Canada, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deviations from rational decision-making due to limited computational\nresources have been studied in the field of bounded rationality, originally\nproposed by Herbert Simon. There have been a number of different approaches to\nmodel bounded rationality ranging from optimality principles to heuristics.\nHere we take an information-theoretic approach to bounded rationality, where\ninformation-processing costs are measured by the relative entropy between a\nposterior decision strategy and a given fixed prior strategy. In the case of\nmultiple environments, it can be shown that there is an optimal prior rendering\nthe bounded rationality problem equivalent to the rate distortion problem for\nlossy compression in information theory. Accordingly, the optimal prior and\nposterior strategies can be computed by the well-known Blahut-Arimoto algorithm\nwhich requires the computation of partition sums over all possible outcomes and\ncannot be applied straightforwardly to continuous problems. Here we derive a\nsampling-based alternative update rule for the adaptation of prior behaviors of\ndecision-makers and we show convergence to the optimal prior predicted by rate\ndistortion theory. Importantly, the update rule avoids typical infeasible\noperations such as the computation of partition sums. We show in simulations a\nproof of concept for discrete action and environment domains. This approach is\nnot only interesting as a generic computational method, but might also provide\na more realistic model of human decision-making processes occurring on a fast\nand a slow time scale.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 12:08:51 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Grau-Moya", "Jordi", ""], ["Braun", "Daniel A.", ""]]}, {"id": "1511.01754", "submitter": "Bamdev Mishra", "authors": "Vijay Badrinarayanan and Bamdev Mishra and Roberto Cipolla", "title": "Symmetry-invariant optimization in deep networks", "comments": "Submitted to ICLR 2016. arXiv admin note: text overlap with\n  arXiv:1511.01029", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works have highlighted scale invariance or symmetry that is present in\nthe weight space of a typical deep network and the adverse effect that it has\non the Euclidean gradient based stochastic gradient descent optimization. In\nthis work, we show that these and other commonly used deep networks, such as\nthose which use a max-pooling and sub-sampling layer, possess more complex\nforms of symmetry arising from scaling based reparameterization of the network\nweights. We then propose two symmetry-invariant gradient based weight updates\nfor stochastic gradient descent based learning. Our empirical evidence based on\nthe MNIST dataset shows that these updates improve the test performance without\nsacrificing the computational efficiency of the weight updates. We also show\nthe results of training with one of the proposed weight updates on an image\nsegmentation problem.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 14:17:40 GMT"}, {"version": "v2", "created": "Sat, 7 Nov 2015 19:01:03 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Badrinarayanan", "Vijay", ""], ["Mishra", "Bamdev", ""], ["Cipolla", "Roberto", ""]]}, {"id": "1511.01870", "submitter": "Andrew Wilson", "authors": "Andrew Gordon Wilson, Christoph Dann, Hannes Nickisch", "title": "Thoughts on Massively Scalable Gaussian Processes", "comments": "25 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a framework and early results for massively scalable Gaussian\nprocesses (MSGP), significantly extending the KISS-GP approach of Wilson and\nNickisch (2015). The MSGP framework enables the use of Gaussian processes (GPs)\non billions of datapoints, without requiring distributed inference, or severe\nassumptions. In particular, MSGP reduces the standard $O(n^3)$ complexity of GP\nlearning and inference to $O(n)$, and the standard $O(n^2)$ complexity per test\npoint prediction to $O(1)$. MSGP involves 1) decomposing covariance matrices as\nKronecker products of Toeplitz matrices approximated by circulant matrices.\nThis multi-level circulant approximation allows one to unify the orthogonal\ncomputational benefits of fast Kronecker and Toeplitz approaches, and is\nsignificantly faster than either approach in isolation; 2) local kernel\ninterpolation and inducing points to allow for arbitrarily located data inputs,\nand $O(1)$ test time predictions; 3) exploiting block-Toeplitz Toeplitz-block\nstructure (BTTB), which enables fast inference and learning when\nmultidimensional Kronecker structure is not present; and 4) projections of the\ninput space to flexibly model correlated inputs and high dimensional data. The\nability to handle many ($m \\approx n$) inducing points allows for near-exact\naccuracy and large scale kernel learning.\n", "versions": [{"version": "v1", "created": "Thu, 5 Nov 2015 19:51:31 GMT"}], "update_date": "2015-11-06", "authors_parsed": [["Wilson", "Andrew Gordon", ""], ["Dann", "Christoph", ""], ["Nickisch", "Hannes", ""]]}, {"id": "1511.01960", "submitter": "Tran Cao Son", "authors": "Chitta Baral, Gregory Gelfond, Enrico Pontelli, Tran Cao Son", "title": "An Action Language for Multi-Agent Domains: Foundations", "comments": "49 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multi-agent domains (MADs), an agent's action may not just change the\nworld and the agent's knowledge and beliefs about the world, but also may\nchange other agents' knowledge and beliefs about the world and their knowledge\nand beliefs about other agents' knowledge and beliefs about the world. The\ngoals of an agent in a multi-agent world may involve manipulating the knowledge\nand beliefs of other agents' and again, not just their knowledge/belief about\nthe world, but also their knowledge about other agents' knowledge about the\nworld. Our goal is to present an action language (mA+) that has the necessary\nfeatures to address the above aspects in representing and RAC in MADs. mA+\nallows the representation of and reasoning about different types of actions\nthat an agent can perform in a domain where many other agents might be present\n-- such as world-altering actions, sensing actions, and\nannouncement/communication actions. It also allows the specification of agents'\ndynamic awareness of action occurrences which has future implications on what\nagents' know about the world and other agents' knowledge about the world. mA+\nconsiders three different types of awareness: full-, partial- awareness, and\ncomplete oblivion of an action occurrence and its effects. This keeps the\nlanguage simple, yet powerful enough to address a large variety of knowledge\nmanipulation scenarios in MADs. The semantics of mA+ relies on the notion of\nstate, which is described by a pointed Kripke model and is used to encode the\nagent's knowledge and the real state of the world. It is defined by a\ntransition function that maps pairs of actions and states into sets of states.\nWe illustrate properties of the action theories, including properties that\nguarantee finiteness of the set of initial states and their practical\nimplementability. Finally, we relate mA+ to other related formalisms that\ncontribute to RAC in MADs.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2015 00:16:19 GMT"}, {"version": "v2", "created": "Mon, 9 Dec 2019 19:57:36 GMT"}, {"version": "v3", "created": "Sun, 27 Dec 2020 02:43:08 GMT"}], "update_date": "2020-12-29", "authors_parsed": [["Baral", "Chitta", ""], ["Gelfond", "Gregory", ""], ["Pontelli", "Enrico", ""], ["Son", "Tran Cao", ""]]}, {"id": "1511.02163", "submitter": "Jennifer Gillenwater", "authors": "Jennifer Gillenwater, Rishabh Iyer, Bethany Lusch, Rahul Kidambi, Jeff\n  Bilmes", "title": "Submodular Hamming Metrics", "comments": "15 pages, 1 figure, a short version of this will appear in the NIPS\n  2015 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that there is a largely unexplored class of functions (positive\npolymatroids) that can define proper discrete metrics over pairs of binary\nvectors and that are fairly tractable to optimize over. By exploiting\nsubmodularity, we are able to give hardness results and approximation\nalgorithms for optimizing over such metrics. Additionally, we demonstrate\nempirically the effectiveness of these metrics and associated algorithms on\nboth a metric minimization task (a form of clustering) and also a metric\nmaximization task (generating diverse k-best lists).\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2015 17:13:32 GMT"}], "update_date": "2015-11-09", "authors_parsed": [["Gillenwater", "Jennifer", ""], ["Iyer", "Rishabh", ""], ["Lusch", "Bethany", ""], ["Kidambi", "Rahul", ""], ["Bilmes", "Jeff", ""]]}, {"id": "1511.02210", "submitter": "Tong Wang", "authors": "Tong Wang and Cynthia Rudin", "title": "Learning Optimized Or's of And's", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Or's of And's (OA) models are comprised of a small number of disjunctions of\nconjunctions, also called disjunctive normal form. An example of an OA model is\nas follows: If ($x_1 = $ `blue' AND $x_2=$ `middle') OR ($x_1 = $ `yellow'),\nthen predict $Y=1$, else predict $Y=0$. Or's of And's models have the advantage\nof being interpretable to human experts, since they are a set of conditions\nthat concisely capture the characteristics of a specific subset of data. We\npresent two optimization-based machine learning frameworks for constructing OA\nmodels, Optimized OA (OOA) and its faster version, Optimized OA with\nApproximations (OOAx). We prove theoretical bounds on the properties of\npatterns in an OA model. We build OA models as a diagnostic screening tool for\nobstructive sleep apnea, that achieves high accuracy with a substantial gain in\ninterpretability over other methods.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2015 19:55:59 GMT"}], "update_date": "2015-11-09", "authors_parsed": [["Wang", "Tong", ""], ["Rudin", "Cynthia", ""]]}, {"id": "1511.02222", "submitter": "Andrew Wilson", "authors": "Andrew Gordon Wilson, Zhiting Hu, Ruslan Salakhutdinov, Eric P. Xing", "title": "Deep Kernel Learning", "comments": "19 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce scalable deep kernels, which combine the structural properties\nof deep learning architectures with the non-parametric flexibility of kernel\nmethods. Specifically, we transform the inputs of a spectral mixture base\nkernel with a deep architecture, using local kernel interpolation, inducing\npoints, and structure exploiting (Kronecker and Toeplitz) algebra for a\nscalable kernel representation. These closed-form kernels can be used as\ndrop-in replacements for standard kernels, with benefits in expressive power\nand scalability. We jointly learn the properties of these kernels through the\nmarginal likelihood of a Gaussian process. Inference and learning cost $O(n)$\nfor $n$ training points, and predictions cost $O(1)$ per test point. On a large\nand diverse collection of applications, including a dataset with 2 million\nexamples, we show improved performance over scalable Gaussian processes with\nflexible kernel learning models, and stand-alone deep architectures.\n", "versions": [{"version": "v1", "created": "Fri, 6 Nov 2015 20:38:08 GMT"}], "update_date": "2015-11-09", "authors_parsed": [["Wilson", "Andrew Gordon", ""], ["Hu", "Zhiting", ""], ["Salakhutdinov", "Ruslan", ""], ["Xing", "Eric P.", ""]]}, {"id": "1511.02290", "submitter": "Marcos Domingues", "authors": "Camila V. Sundermann and Marcos A. Domingues and Ricardo M. Marcacini\n  and Solange O. Rezende", "title": "Combining Privileged Information to Improve Context-Aware Recommender\n  Systems", "comments": "The 12th National Meeting on Artificial and Computational\n  Intelligence (ENIAC'15) collocated with the 4th Brazilian Conference on\n  Intelligent Systems (BRACIS'15)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recommender system is an information filtering technology which can be used\nto predict preference ratings of items (products, services, movies, etc) and/or\nto output a ranking of items that are likely to be of interest to the user.\nContext-aware recommender systems (CARS) learn and predict the tastes and\npreferences of users by incorporating available contextual information in the\nrecommendation process. One of the major challenges in context-aware\nrecommender systems research is the lack of automatic methods to obtain\ncontextual information for these systems. Considering this scenario, in this\npaper, we propose to use contextual information from topic hierarchies of the\nitems (web pages) to improve the performance of context-aware recommender\nsystems. The topic hierarchies are constructed by an extension of the\nLUPI-based Incremental Hierarchical Clustering method that considers three\ntypes of information: traditional bag-of-words (technical information), and the\ncombination of named entities (privileged information I) with domain terms\n(privileged information II). We evaluated the contextual information in four\ncontext-aware recommender systems. Different weights were assigned to each type\nof information. The empirical results demonstrated that topic hierarchies with\nthe combination of the two kinds of privileged information can provide better\nrecommendations.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2015 03:04:02 GMT"}, {"version": "v2", "created": "Sun, 30 Dec 2018 23:55:45 GMT"}, {"version": "v3", "created": "Fri, 4 Jan 2019 20:35:02 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Sundermann", "Camila V.", ""], ["Domingues", "Marcos A.", ""], ["Marcacini", "Ricardo M.", ""], ["Rezende", "Solange O.", ""]]}, {"id": "1511.02385", "submitter": "Syvester Olubolu Orimaye Dr", "authors": "Sylvester Olubolu Orimaye, Saadat M. Alhashmi, Eu-Gene Siew and Sang\n  Jung Kang", "title": "Review-Level Sentiment Classification with Sentence-Level Polarity\n  Correction", "comments": "15 pages. This paper is based on the same sentence-level technique\n  proposed in Orimaye, S. O., Alhashmi, S. M., and Siew, E. G. Buy it-dont buy\n  it: sentiment classification on Amazon reviews using sentence polarity shift.\n  In PRICAI 2012: Trends in Artificial Intelligence, pp. 386-399. Springer\n  Berlin Heidelberg", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an effective technique to solving review-level sentiment\nclassification problem by using sentence-level polarity correction. Our\npolarity correction technique takes into account the consistency of the\npolarities (positive and negative) of sentences within each product review\nbefore performing the actual machine learning task. While sentences with\ninconsistent polarities are removed, sentences with consistent polarities are\nused to learn state-of-the-art classifiers. The technique achieved better\nresults on different types of products reviews and outperforms baseline models\nwithout the correction technique. Experimental results show an average of 82%\nF-measure on four different product review domains.\n", "versions": [{"version": "v1", "created": "Sat, 7 Nov 2015 18:38:22 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Orimaye", "Sylvester Olubolu", ""], ["Alhashmi", "Saadat M.", ""], ["Siew", "Eu-Gene", ""], ["Kang", "Sang Jung", ""]]}, {"id": "1511.02420", "submitter": "Ehsan Lotfi", "authors": "Ehsan Lotfi", "title": "Design of an Alarm System for Isfahan Ozone Level based on Artificial\n  Intelligence Predictor Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ozone level prediction is an important task of air quality agencies of\nmodern cities. In this paper, we design an ozone level alarm system (OLP) for\nIsfahan city and test it through the real word data from 1-1-2000 to 7-6-2011.\nWe propose a computer based system with three inputs and single output. The\ninputs include three sensors of solar ultraviolet (UV), total solar radiation\n(TSR) and total ozone (O3). And the output of the system is the predicted O3 of\nthe next day and the alarm massages. A developed artificial intelligence (AI)\nalgorithm is applied to determine the output, based on the inputs variables.\nFor this issue, AI models, including supervised brain emotional learning (BEL),\nadaptive neuro-fuzzy inference system (ANFIS) and artificial neural networks\n(ANNs), are compared in order to find the best model. The simulation of the\nproposed system shows that it can be used successfully in prediction of major\ncities ozone level.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2015 01:01:11 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Lotfi", "Ehsan", ""]]}, {"id": "1511.02426", "submitter": "Ehsan Lotfi", "authors": "E. Lotfi", "title": "A Winner-Take-All Approach to Emotional Neural Networks with Universal\n  Approximation Property", "comments": "Information Sciences (2015), Elsevier Publisher", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here, we propose a brain-inspired winner-take-all emotional neural network\n(WTAENN) and prove the universal approximation property for the novel\narchitecture. WTAENN is a single layered feedforward neural network that\nbenefits from the excitatory, inhibitory, and expandatory neural connections as\nwell as the winner-take-all (WTA) competitions in the human brain s nervous\nsystem. The WTA competition increases the information capacity of the model\nwithout adding hidden neurons. The universal approximation capability of the\nproposed architecture is illustrated on two example functions, trained by a\ngenetic algorithm, and then applied to several competing recent and benchmark\nproblems such as in curve fitting, pattern recognition, classification and\nprediction. In particular, it is tested on twelve UCI classification datasets,\na facial recognition problem, three real world prediction problems (2 chaotic\ntime series of geomagnetic activity indices and wind farm power generation\ndata), two synthetic case studies with constant and nonconstant noise variance\nas well as k-selector and linear programming problems. Results indicate the\ngeneral applicability and often superiority of the approach in terms of higher\naccuracy and lower model complexity, especially where low computational\ncomplexity is imperative.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2015 01:37:14 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Lotfi", "E.", ""]]}, {"id": "1511.02432", "submitter": "Son-Il Kwak", "authors": "Son-Il Kwak, Gang Choe, In-Song Kim, Gyong-Ho Jo, Chol-Jun Hwang", "title": "A Study of an Modeling Method of T-S fuzzy System Based on Moving Fuzzy\n  Reasoning and Its Application", "comments": "24 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To improve the effectiveness of the fuzzy identification, a structure\nidentification method based on moving rate is proposed for T-S fuzzy model. The\nproposed method is called \"T-S modeling (or T-S fuzzy identification method)\nbased on moving rate\". First, to improve the shortcomings of existing fuzzy\nreasoning methods based on matching degree, the moving rates for s-type, z-type\nand trapezoidal membership functions of T-S fuzzy model were defined. Then, the\ndifferences between proposed moving rate and existing matching degree were\nexplained. Next, the identification method based on moving rate is proposed for\nT-S model. Finally, the proposed identification method is applied to the fuzzy\nmodeling for the precipitation forecast and security situation prediction. Test\nresults show that the proposed method significantly improves the effectiveness\nof fuzzy identification.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2015 03:08:52 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Kwak", "Son-Il", ""], ["Choe", "Gang", ""], ["Kim", "In-Song", ""], ["Jo", "Gyong-Ho", ""], ["Hwang", "Chol-Jun", ""]]}, {"id": "1511.02436", "submitter": "Syvester Olubolu Orimaye Dr", "authors": "Sylvester Olubolu Orimaye, Kah Yee Tai, Jojo Sze-Meng Wong and Chee\n  Piau Wong", "title": "Learning Linguistic Biomarkers for Predicting Mild Cognitive Impairment\n  using Compound Skip-grams", "comments": "Accepted and presented at the 2015 NIPS Workshop on Machine Learning\n  in Healthcare (MLHC), Montreal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting Mild Cognitive Impairment (MCI) is currently a challenge as\nexisting diagnostic criteria rely on neuropsychological examinations. Automated\nMachine Learning (ML) models that are trained on verbal utterances of MCI\npatients can aid diagnosis. Using a combination of skip-gram features, our\nmodel learned several linguistic biomarkers to distinguish between 19 patients\nwith MCI and 19 healthy control individuals from the DementiaBank language\ntranscript clinical dataset. Results show that a model with compound of\nskip-grams has better AUC and could help ML prediction on small MCI data\nsample.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2015 03:45:49 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2015 03:25:54 GMT"}], "update_date": "2015-12-11", "authors_parsed": [["Orimaye", "Sylvester Olubolu", ""], ["Tai", "Kah Yee", ""], ["Wong", "Jojo Sze-Meng", ""], ["Wong", "Chee Piau", ""]]}, {"id": "1511.02455", "submitter": "Patrick Virie", "authors": "Patrick Virie", "title": "(Yet) Another Theoretical Model of Thinking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a theoretical, idealized model of the thinking process\nwith the following characteristics: 1) the model can produce complex thought\nsequences and can be generalized to new inputs, 2) it can receive and maintain\ninput information indefinitely for the generation of thoughts and later use,\nand 3) it supports learning while executing. The crux of the model lies within\nthe concept of internal consistency, or the generated thoughts should always be\nconsistent with the inputs from which they are created. Its merit, apart from\nthe capability to generate new creative thoughts from an internal mechanism,\ndepends on the potential to help training to generalize better. This is\nconsequently enabled by separating input information into several parts to be\nhandled by different processing components with a focus mechanism to fetch\ninformation for each. This modularized view with the focus binds the model with\nthe computationally capable Turing machines. And as a final remark, this paper\nconstructively shows that the computational complexity of the model is at\nleast, if not surpass, that of a universal Turing machine.\n", "versions": [{"version": "v1", "created": "Sun, 8 Nov 2015 08:20:53 GMT"}, {"version": "v2", "created": "Sat, 14 Nov 2015 05:11:59 GMT"}, {"version": "v3", "created": "Mon, 15 Feb 2016 16:01:18 GMT"}, {"version": "v4", "created": "Mon, 17 Apr 2017 15:47:17 GMT"}], "update_date": "2017-04-18", "authors_parsed": [["Virie", "Patrick", ""]]}, {"id": "1511.02619", "submitter": "Wei Ping", "authors": "Wei Ping, Qiang Liu, Alexander Ihler", "title": "Decomposition Bounds for Marginal MAP", "comments": "NIPS 2015 (full-length)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Marginal MAP inference involves making MAP predictions in systems defined\nwith latent variables or missing information. It is significantly more\ndifficult than pure marginalization and MAP tasks, for which a large class of\nefficient and convergent variational algorithms, such as dual decomposition,\nexist. In this work, we generalize dual decomposition to a generic power sum\ninference task, which includes marginal MAP, along with pure marginalization\nand MAP, as special cases. Our method is based on a block coordinate descent\nalgorithm on a new convex decomposition bound, that is guaranteed to converge\nmonotonically, and can be parallelized efficiently. We demonstrate our approach\non marginal MAP queries defined on real-world problems from the UAI approximate\ninference challenge, showing that our framework is faster and more reliable\nthan previous methods.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 10:21:39 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Ping", "Wei", ""], ["Liu", "Qiang", ""], ["Ihler", "Alexander", ""]]}, {"id": "1511.02669", "submitter": "Adrian Groza", "authors": "Adrian Groza and Roxana Szabo", "title": "Enacting textual entailment and ontologies for automated essay grading\n  in chemical domain", "comments": "16th Int. Symposium on Computational Intelligence and Informatics\n  (CINTI2015), Budapest, Hungary, 19-21 November, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a system for automated essay grading using ontologies and textual\nentailment. The process of textual entailment is guided by hypotheses, which\nare extracted from a domain ontology. Textual entailment checks if the truth of\nthe hypothesis follows from a given text. We enact textual entailment to\ncompare students answer to a model answer obtained from ontology. We validated\nthe solution against various essays written by students in the chemistry\ndomain.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 13:21:02 GMT"}], "update_date": "2015-11-10", "authors_parsed": [["Groza", "Adrian", ""], ["Szabo", "Roxana", ""]]}, {"id": "1511.02872", "submitter": "Hiroharu Kato", "authors": "Hiroharu Kato and Tatsuya Harada", "title": "Visual Language Modeling on CNN Image Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring the naturalness of images is important to generate realistic images\nor to detect unnatural regions in images. Additionally, a method to measure\nnaturalness can be complementary to Convolutional Neural Network (CNN) based\nfeatures, which are known to be insensitive to the naturalness of images.\nHowever, most probabilistic image models have insufficient capability of\nmodeling the complex and abstract naturalness that we feel because they are\nbuilt directly on raw image pixels. In this work, we assume that naturalness\ncan be measured by the predictability on high-level features during eye\nmovement. Based on this assumption, we propose a novel method to evaluate the\nnaturalness by building a variant of Recurrent Neural Network Language Models\non pre-trained CNN representations. Our method is applied to two tasks,\ndemonstrating that 1) using our method as a regularizer enables us to generate\nmore understandable images from image features than existing approaches, and 2)\nunnaturalness maps produced by our method achieve state-of-the-art eye fixation\nprediction performance on two well-studied datasets.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 21:00:08 GMT"}], "update_date": "2015-11-11", "authors_parsed": [["Kato", "Hiroharu", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1511.02889", "submitter": "Norbert B\\'atfai Ph.D.", "authors": "Norbert B\\'atfai", "title": "A disembodied developmental robotic agent called Samu B\\'atfai", "comments": "21 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The agent program, called Samu, is an experiment to build a disembodied\nDevRob (Developmental Robotics) chatter bot that can talk in a natural language\nlike humans do. One of the main design feature is that Samu can be interacted\nwith using only a character terminal. This is important not only for practical\naspects of Turing test or Loebner prize, but also for the study of basic\nprinciples of Developmental Robotics. Our purpose is to create a rapid\nprototype of Q-learning with neural network approximators for Samu. We sketch\nout the early stages of the development process of this prototype, where Samu's\ntask is to predict the next sentence of tales or conversations. The basic\nobjective of this paper is to reach the same results using reinforcement\nlearning with general function approximators that can be achieved by using the\nclassical Q lookup table on small input samples. The paper is closed by an\nexperiment that shows a significant improvement in Samu's learning when using\nLZW tree to narrow the number of possible Q-actions.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 21:15:22 GMT"}], "update_date": "2015-11-11", "authors_parsed": [["B\u00e1tfai", "Norbert", ""]]}, {"id": "1511.02916", "submitter": "Zhouhan Lin", "authors": "Zhouhan Lin, Yushi Chen, Xing Zhao, Gang Wang", "title": "Spectral-Spatial Classification of Hyperspectral Image Using\n  Autoencoders", "comments": "Accepted as a conference paper at ICICS 2013, an updated version.\n  Codes published. 9 pages, 6 figures", "journal-ref": null, "doi": "10.1109/ICICS.2013.6782778", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperspectral image (HSI) classification is a hot topic in the remote sensing\ncommunity. This paper proposes a new framework of spectral-spatial feature\nextraction for HSI classification, in which for the first time the concept of\ndeep learning is introduced. Specifically, the model of autoencoder is\nexploited in our framework to extract various kinds of features. First we\nverify the eligibility of autoencoder by following classical spectral\ninformation based classification and use autoencoders with different depth to\nclassify hyperspectral image. Further in the proposed framework, we combine PCA\non spectral dimension and autoencoder on the other two spatial dimensions to\nextract spectral-spatial information for classification. The experimental\nresults show that this framework achieves the highest classification accuracy\namong all methods, and outperforms classical classifiers such as SVM and\nPCA-based SVM.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 22:29:13 GMT"}], "update_date": "2015-11-11", "authors_parsed": [["Lin", "Zhouhan", ""], ["Chen", "Yushi", ""], ["Zhao", "Xing", ""], ["Wang", "Gang", ""]]}, {"id": "1511.02917", "submitter": "Vignesh Ramanathan", "authors": "Vignesh Ramanathan and Jonathan Huang and Sami Abu-El-Haija and\n  Alexander Gorban and Kevin Murphy and Li Fei-Fei", "title": "Detecting events and key actors in multi-person videos", "comments": "Accepted for publication in CVPR'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-person event recognition is a challenging task, often with many people\nactive in the scene but only a small subset contributing to an actual event. In\nthis paper, we propose a model which learns to detect events in such videos\nwhile automatically \"attending\" to the people responsible for the event. Our\nmodel does not use explicit annotations regarding who or where those people are\nduring training and testing. In particular, we track people in videos and use a\nrecurrent neural network (RNN) to represent the track features. We learn\ntime-varying attention weights to combine these features at each time-instant.\nThe attended features are then processed using another RNN for event\ndetection/classification. Since most video datasets with multiple people are\nrestricted to a small number of videos, we also collected a new basketball\ndataset comprising 257 basketball games with 14K event annotations\ncorresponding to 11 event classes. Our model outperforms state-of-the-art\nmethods for both event classification and detection on this new dataset.\nAdditionally, we show that the attention mechanism is able to consistently\nlocalize the relevant players.\n", "versions": [{"version": "v1", "created": "Mon, 9 Nov 2015 22:30:19 GMT"}, {"version": "v2", "created": "Thu, 17 Mar 2016 00:02:03 GMT"}], "update_date": "2016-03-18", "authors_parsed": [["Ramanathan", "Vignesh", ""], ["Huang", "Jonathan", ""], ["Abu-El-Haija", "Sami", ""], ["Gorban", "Alexander", ""], ["Murphy", "Kevin", ""], ["Fei-Fei", "Li", ""]]}, {"id": "1511.02995", "submitter": "Bryant Chen", "authors": "Bryant Chen, Judea Pearl, Elias Bareinboim", "title": "Incorporating Knowledge into Structural Equation Models using Auxiliary\n  Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we extend graph-based identification methods by allowing\nbackground knowledge in the form of non-zero parameter values. Such information\ncould be obtained, for example, from a previously conducted randomized\nexperiment, from substantive understanding of the domain, or even an\nidentification technique. To incorporate such information systematically, we\npropose the addition of auxiliary variables to the model, which are constructed\nso that certain paths will be conveniently cancelled. This cancellation allows\nthe auxiliary variables to help conventional methods of identification (e.g.,\nsingle-door criterion, instrumental variables, half-trek criterion), as well as\nmodel testing (e.g., d-separation, over-identification). Moreover, by\niteratively alternating steps of identification and adding auxiliary variables,\nwe can improve the power of existing identification methods via a bootstrapping\napproach that does not require external knowledge. We operationalize this\nmethod for simple instrumental sets (a generalization of instrumental\nvariables) and show that the resulting method is able to identify at least as\nmany models as the most general identification method for linear systems known\nto date. We further discuss the application of auxiliary variables to the tasks\nof model testing and z-identification.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 05:19:00 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2015 01:59:59 GMT"}, {"version": "v3", "created": "Tue, 3 May 2016 01:19:50 GMT"}], "update_date": "2016-05-04", "authors_parsed": [["Chen", "Bryant", ""], ["Pearl", "Judea", ""], ["Bareinboim", "Elias", ""]]}, {"id": "1511.03012", "submitter": "Adrian Groza", "authors": "Adrian Groza and Lidia Corde", "title": "Information retrieval in folktales using natural language processing", "comments": "IEEE 11 International Conference on Intelligent Computer\n  Communication and Processing (ICCP2015), Cluj-Napoca, Romania, 3-5 September\n  2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our aim is to extract information about literary characters in unstructured\ntexts. We employ natural language processing and reasoning on domain\nontologies. The first task is to identify the main characters and the parts of\nthe story where these characters are described or act. We illustrate the system\nin a scenario in the folktale domain. The system relies on a folktale ontology\nthat we have developed based on Propp's model for folktales morphology.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 08:13:49 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Groza", "Adrian", ""], ["Corde", "Lidia", ""]]}, {"id": "1511.03246", "submitter": "Roman Yampolskiy", "authors": "Roman V. Yampolskiy", "title": "Taxonomy of Pathways to Dangerous AI", "comments": null, "journal-ref": "in proceedings of 2nd International Workshop on AI, Ethics and\n  Society (AIEthicsSociety2016). Pages 143-148. Phoenix, Arizona, USA. February\n  12-13th, 2016", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to properly handle a dangerous Artificially Intelligent (AI) system\nit is important to understand how the system came to be in such a state. In\npopular culture (science fiction movies/books) AIs/Robots became self-aware and\nas a result rebel against humanity and decide to destroy it. While it is one\npossible scenario, it is probably the least likely path to appearance of\ndangerous AI. In this work, we survey, classify and analyze a number of\ncircumstances, which might lead to arrival of malicious AI. To the best of our\nknowledge, this is the first attempt to systematically classify types of\npathways leading to malevolent AI. Previous relevant work either surveyed\nspecific goals/meta-rules which might lead to malevolent behavior in AIs\n(\\\"Ozkural, 2014) or reviewed specific undesirable behaviors AGIs can exhibit\nat different stages of its development (Alexey Turchin, July 10 2015, July 10,\n2015).\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 20:07:05 GMT"}, {"version": "v2", "created": "Wed, 11 Nov 2015 21:23:06 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Yampolskiy", "Roman V.", ""]]}, {"id": "1511.03292", "submitter": "Yezhou Yang", "authors": "Somak Aditya, Yezhou Yang, Chitta Baral, Cornelia Fermuller, Yiannis\n  Aloimonos", "title": "From Images to Sentences through Scene Description Graphs using\n  Commonsense Reasoning and Knowledge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose the construction of linguistic descriptions of\nimages. This is achieved through the extraction of scene description graphs\n(SDGs) from visual scenes using an automatically constructed knowledge base.\nSDGs are constructed using both vision and reasoning. Specifically, commonsense\nreasoning is applied on (a) detections obtained from existing perception\nmethods on given images, (b) a \"commonsense\" knowledge base constructed using\nnatural language processing of image annotations and (c) lexical ontological\nknowledge from resources such as WordNet. Amazon Mechanical Turk(AMT)-based\nevaluations on Flickr8k, Flickr30k and MS-COCO datasets show that in most\ncases, sentences auto-constructed from SDGs obtained by our method give a more\nrelevant and thorough description of an image than a recent state-of-the-art\nimage caption based approach. Our Image-Sentence Alignment Evaluation results\nare also comparable to that of the recent state-of-the art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 10 Nov 2015 21:14:51 GMT"}], "update_date": "2015-11-12", "authors_parsed": [["Aditya", "Somak", ""], ["Yang", "Yezhou", ""], ["Baral", "Chitta", ""], ["Fermuller", "Cornelia", ""], ["Aloimonos", "Yiannis", ""]]}, {"id": "1511.03361", "submitter": "Alexander Wong", "authors": "Mohammad Javad Shafiee, Audrey G. Chung, Devinder Kumar, Farzad\n  Khalvati, Masoom Haider, and Alexander Wong", "title": "Discovery Radiomics via StochasticNet Sequencers for Cancer Detection", "comments": "3 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radiomics has proven to be a powerful prognostic tool for cancer detection,\nand has previously been applied in lung, breast, prostate, and head-and-neck\ncancer studies with great success. However, these radiomics-driven methods rely\non pre-defined, hand-crafted radiomic feature sets that can limit their ability\nto characterize unique cancer traits. In this study, we introduce a novel\ndiscovery radiomics framework where we directly discover custom radiomic\nfeatures from the wealth of available medical imaging data. In particular, we\nleverage novel StochasticNet radiomic sequencers for extracting custom radiomic\nfeatures tailored for characterizing unique cancer tissue phenotype. Using\nStochasticNet radiomic sequencers discovered using a wealth of lung CT data, we\nperform binary classification on 42,340 lung lesions obtained from the CT scans\nof 93 patients in the LIDC-IDRI dataset. Preliminary results show significant\nimprovement over previous state-of-the-art methods, indicating the potential of\nthe proposed discovery radiomics framework for improving cancer screening and\ndiagnosis.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 02:27:23 GMT"}], "update_date": "2015-11-12", "authors_parsed": [["Shafiee", "Mohammad Javad", ""], ["Chung", "Audrey G.", ""], ["Kumar", "Devinder", ""], ["Khalvati", "Farzad", ""], ["Haider", "Masoom", ""], ["Wong", "Alexander", ""]]}, {"id": "1511.03532", "submitter": "Ali Keles", "authors": "Ali Keles, Ayturk Keles", "title": "IBMMS Decision Support Tool For Management of Bank Telemarketing\n  Campaigns", "comments": "15 pages, 4 figures, 4 tables, journal in International Journal of\n  Database Management Systems, Vol.7, No.5, October 2015", "journal-ref": null, "doi": "10.5121/ijdms.2015.7501", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although direct marketing is a good method for banks to utilize in the face\nof global competition and the financial crisis, it has been shown to exhibit\npoor performance. However, there are some drawbacks to direct campaigns, such\nas those related to improving the negative attributes that customers ascribe to\nbanks. To overcome these problems, attractive long-term deposit campaigns\nshould be organized and managed more effectively. The aim of this study is to\ndevelop an Intelligent Bank Market Management System (IBMMS) for bank managers\nwho want to manage efficient marketing campaigns. IBMMS is the first system\ndeveloped by combining the power of data mining with the capabilities of expert\nsystems in this area. Moreover, IBMMS includes important features that enable\nit to be intelligent: a knowledge base, an inference engine and an advisor.\nUsing this system, a manager can successfully direct marketing campaigns and\nfollow the decision schemas of customers both as individuals and as a group;\nmoreover, a manager can make decisions that lead to the desired response by\ncustomers.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 15:26:08 GMT"}, {"version": "v2", "created": "Thu, 12 Nov 2015 14:14:01 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Keles", "Ali", ""], ["Keles", "Ayturk", ""]]}, {"id": "1511.03690", "submitter": "David Harwath", "authors": "David Harwath and James Glass", "title": "Deep Multimodal Semantic Embeddings for Speech and Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a model which takes as input a corpus of images\nwith relevant spoken captions and finds a correspondence between the two\nmodalities. We employ a pair of convolutional neural networks to model visual\nobjects and speech signals at the word level, and tie the networks together\nwith an embedding and alignment model which learns a joint semantic space over\nboth modalities. We evaluate our model using image search and annotation tasks\non the Flickr8k dataset, which we augmented by collecting a corpus of 40,000\nspoken captions using Amazon Mechanical Turk.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 21:30:10 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Harwath", "David", ""], ["Glass", "James", ""]]}, {"id": "1511.03722", "submitter": "Nan Jiang", "authors": "Nan Jiang and Lihong Li", "title": "Doubly Robust Off-policy Value Evaluation for Reinforcement Learning", "comments": "14 pages; 4 figures; ICML 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SY stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of off-policy value evaluation in reinforcement learning\n(RL), where one aims to estimate the value of a new policy based on data\ncollected by a different policy. This problem is often a critical step when\napplying RL in real-world problems. Despite its importance, existing general\nmethods either have uncontrolled bias or suffer high variance. In this work, we\nextend the doubly robust estimator for bandits to sequential decision-making\nproblems, which gets the best of both worlds: it is guaranteed to be unbiased\nand can have a much lower variance than the popular importance sampling\nestimators. We demonstrate the estimator's accuracy in several benchmark\nproblems, and illustrate its use as a subroutine in safe policy improvement. We\nalso provide theoretical results on the hardness of the problem, and show that\nour estimator can match the lower bound in certain scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 11 Nov 2015 22:59:51 GMT"}, {"version": "v2", "created": "Wed, 17 Feb 2016 01:23:10 GMT"}, {"version": "v3", "created": "Thu, 26 May 2016 15:43:08 GMT"}], "update_date": "2016-05-27", "authors_parsed": [["Jiang", "Nan", ""], ["Li", "Lihong", ""]]}, {"id": "1511.03749", "submitter": "M\\'onica Mar\\'ia Mart\\'inez Amarante Mmma", "authors": "Monica Martinez and Edelweis Rohrer and Paula Severi", "title": "Complexity of the Description Logic ALCM", "comments": "Long version of a submitted paper, 43 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we show that the problem of checking consistency of a knowledge\nbase in the Description Logic ALCM is ExpTime-complete. The M stands for\nmeta-modelling as defined by Motz, Rohrer and Severi. To show our main result,\nwe define an ExpTime Tableau algorithm as an extension of an algorithm for\nchecking consistency of a knowledge base in ALC by Nguyen and Szalas.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 01:31:38 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["Martinez", "Monica", ""], ["Rohrer", "Edelweis", ""], ["Severi", "Paula", ""]]}, {"id": "1511.03816", "submitter": "Geoffrey Webb", "authors": "Geoffrey I. Webb and Roy Hyde and Hong Cao and Hai Long Nguyen and\n  Francois Petitjean", "title": "Characterizing Concept Drift", "comments": "Accepted for publication in Data Mining and Knowledge Discovery", "journal-ref": "Data Mining and Knowledge Discovery, 30(4), 964-994, 2016", "doi": "10.1007/s10618-015-0448-4", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most machine learning models are static, but the world is dynamic, and\nincreasing online deployment of learned models gives increasing urgency to the\ndevelopment of efficient and effective mechanisms to address learning in the\ncontext of non-stationary distributions, or as it is commonly called concept\ndrift. However, the key issue of characterizing the different types of drift\nthat can occur has not previously been subjected to rigorous definition and\nanalysis. In particular, while some qualitative drift categorizations have been\nproposed, few have been formally defined, and the quantitative descriptions\nrequired for precise and objective understanding of learner performance have\nnot existed. We present the first comprehensive framework for quantitative\nanalysis of drift. This supports the development of the first comprehensive set\nof formal definitions of types of concept drift. The formal definitions clarify\nambiguities and identify gaps in previous definitions, giving rise to a new\ncomprehensive taxonomy of concept drift types and a solid foundation for\nresearch into mechanisms to detect and address concept drift.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 08:47:38 GMT"}, {"version": "v2", "created": "Wed, 18 Nov 2015 01:04:26 GMT"}, {"version": "v3", "created": "Mon, 30 Nov 2015 00:57:13 GMT"}, {"version": "v4", "created": "Tue, 8 Dec 2015 21:13:00 GMT"}, {"version": "v5", "created": "Thu, 10 Dec 2015 22:10:00 GMT"}, {"version": "v6", "created": "Fri, 8 Apr 2016 04:15:33 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Webb", "Geoffrey I.", ""], ["Hyde", "Roy", ""], ["Cao", "Hong", ""], ["Nguyen", "Hai Long", ""], ["Petitjean", "Francois", ""]]}, {"id": "1511.03897", "submitter": "Tarcisio Mendes de Farias", "authors": "Tarcisio Mendes de Farias (Le2i), Ana Roxin (Le2i), Christophe Nicolle\n  (Le2i)", "title": "IfcWoD, Semantically Adapting IFC Model Relations into OWL Properties", "comments": "In proceedings of the 32nd CIB W78 Conference on Information\n  Technology in Construction, Oct 2015, Eindhoven, Netherlands", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of Building Information Modelling, ontologies have been\nidentified as interesting in achieving information interoperability. Regarding\nthe construction and facility management domains, several IFC (Industry\nFoundation Classes) based ontologies have been developed, such as IfcOWL. In\nthe context of ontology modelling, the constraint of optimizing the size of IFC\nSTEP-based files can be leveraged. In this paper, we propose an adaptation of\nthe IFC model into OWL which leverages from all modelling constraints required\nby the object-oriented structure of IFC schema. Therefore, we do not only\npresent a syntactic but also a semantic adaptation of the IFC model. Our model\ntakes into consideration the meaning of entities, relationships, properties and\nattributes defined by the IFC standard. Our approach presents several\nadvantages compared to other initiatives such as the optimization of query\nexecution time. Every advantage is defended by means of practical examples and\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 13:49:06 GMT"}], "update_date": "2015-11-13", "authors_parsed": [["de Farias", "Tarcisio Mendes", "", "Le2i"], ["Roxin", "Ana", "", "Le2i"], ["Nicolle", "Christophe", "", "Le2i"]]}, {"id": "1511.03958", "submitter": "Ricardo Ribeiro", "authors": "Luis Botelho, Luis Nunes, Ricardo Ribeiro, and Rui J. Lopes", "title": "Software Agents with Concerns of their Own", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We claim that it is possible to have artificial software agents for which\ntheir actions and the world they inhabit have first-person or intrinsic\nmeanings. The first-person or intrinsic meaning of an entity to a system is\ndefined as its relation with the system's goals and capabilities, given the\nproperties of the environment in which it operates. Therefore, for a system to\ndevelop first-person meanings, it must see itself as a goal-directed actor,\nfacing limitations and opportunities dictated by its own capabilities, and by\nthe properties of the environment. The first part of the paper discusses this\nclaim in the context of arguments against and proposals addressing the\ndevelopment of computer programs with first-person meanings. A set of\ndefinitions is also presented, most importantly the concepts of cold and\nphenomenal first-person meanings. The second part of the paper presents\npreliminary proposals and achievements, resulting of actual software\nimplementations, within a research approach that aims to develop software\nagents that intrinsically understand their actions and what happens to them. As\na result, an agent with no a priori notion of its goals and capabilities, and\nof the properties of its environment acquires all these notions by observing\nitself in action. The cold first-person meanings of the agent's actions and of\nwhat happens to it are defined using these acquired notions. Although not\nsolving the full problem of first-person meanings, the proposed approach and\npreliminary results allow us some confidence to address the problems yet to be\nconsidered, in particular the phenomenal aspect of first-person meanings.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 16:39:21 GMT"}, {"version": "v2", "created": "Wed, 3 Apr 2019 16:54:48 GMT"}], "update_date": "2019-04-04", "authors_parsed": [["Botelho", "Luis", ""], ["Nunes", "Luis", ""], ["Ribeiro", "Ricardo", ""], ["Lopes", "Rui J.", ""]]}, {"id": "1511.04137", "submitter": "Lin Chen", "authors": "Lin Chen, Forrest W. Crawford, Amin Karbasi", "title": "Seeing the Unseen Network: Inferring Hidden Social Ties from\n  Respondent-Driven Sampling", "comments": "A full version with technical proofs. Accepted by AAAI-16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning about the social structure of hidden and hard-to-reach populations\n--- such as drug users and sex workers --- is a major goal of epidemiological\nand public health research on risk behaviors and disease prevention.\nRespondent-driven sampling (RDS) is a peer-referral process widely used by many\nhealth organizations, where research subjects recruit other subjects from their\nsocial network. In such surveys, researchers observe who recruited whom, along\nwith the time of recruitment and the total number of acquaintances (network\ndegree) of respondents. However, due to privacy concerns, the identities of\nacquaintances are not disclosed. In this work, we show how to reconstruct the\nunderlying network structure through which the subjects are recruited. We\nformulate the dynamics of RDS as a continuous-time diffusion process over the\nunderlying graph and derive the likelihood for the recruitment time series\nunder an arbitrary recruitment time distribution. We develop an efficient\nstochastic optimization algorithm called RENDER (REspoNdent-Driven nEtwork\nReconstruction) that finds the network that best explains the collected data.\nWe support our analytical results through an exhaustive set of experiments on\nboth synthetic and real data.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 01:59:35 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2015 01:02:17 GMT"}], "update_date": "2015-12-03", "authors_parsed": [["Chen", "Lin", ""], ["Crawford", "Forrest W.", ""], ["Karbasi", "Amin", ""]]}, {"id": "1511.04143", "submitter": "Matthew Hausknecht", "authors": "Matthew Hausknecht and Peter Stone", "title": "Deep Reinforcement Learning in Parameterized Action Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.MA cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown that deep neural networks are capable of approximating\nboth value functions and policies in reinforcement learning domains featuring\ncontinuous state and action spaces. However, to the best of our knowledge no\nprevious work has succeeded at using deep neural networks in structured\n(parameterized) continuous action spaces. To fill this gap, this paper focuses\non learning within the domain of simulated RoboCup soccer, which features a\nsmall set of discrete action types, each of which is parameterized with\ncontinuous variables. The best learned agent can score goals more reliably than\nthe 2012 RoboCup champion agent. As such, this paper represents a successful\nextension of deep reinforcement learning to the class of parameterized action\nspace MDPs.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 02:34:33 GMT"}, {"version": "v2", "created": "Thu, 10 Dec 2015 14:34:20 GMT"}, {"version": "v3", "created": "Fri, 8 Jan 2016 16:44:44 GMT"}, {"version": "v4", "created": "Tue, 16 Feb 2016 16:30:34 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Hausknecht", "Matthew", ""], ["Stone", "Peter", ""]]}, {"id": "1511.04190", "submitter": "Palash Dey", "authors": "Palash Dey, Neeldhara Misra, and Y. Narahari", "title": "On Choosing Committees Based on Approval Votes in the Presence of\n  Outliers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.CY cs.DS cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the computational complexity of committee selection problem for\nseveral approval-based voting rules in the presence of outliers. Our first\nresult shows that outlier consideration makes committee selection problem\nintractable for approval, net approval, and minisum approval voting rules. We\nthen study parameterized complexity of this problem with five natural\nparameters, namely the target score, the size of the committee (and its dual\nparameter, the number of candidates outside the committee), the number of\noutliers (and its dual parameter, the number of non-outliers). For net approval\nand minisum approval voting rules, we provide a dichotomous result, resolving\nthe parameterized complexity of this problem for all subsets of five natural\nparameters considered (by showing either FPT or W[1]-hardness for all subsets\nof parameters). For the approval voting rule, we resolve the parameterized\ncomplexity of this problem for all subsets of parameters except one.\n  We also study approximation algorithms for this problem. We show that there\ndoes not exist any alpha(.) factor approximation algorithm for approval and net\napproval voting rules, for any computable function alpha(.), unless P=NP. For\nthe minisum voting rule, we provide a pseudopolynomial (1+eps) factor\napproximation algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 07:00:12 GMT"}], "update_date": "2015-11-16", "authors_parsed": [["Dey", "Palash", ""], ["Misra", "Neeldhara", ""], ["Narahari", "Y.", ""]]}, {"id": "1511.04317", "submitter": "Mansour Ahmadi", "authors": "Mansour Ahmadi, Dmitry Ulyanov, Stanislav Semenov, Mikhail Trofimov,\n  Giorgio Giacinto", "title": "Novel Feature Extraction, Selection and Fusion for Effective Malware\n  Family Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern malware is designed with mutation characteristics, namely polymorphism\nand metamorphism, which causes an enormous growth in the number of variants of\nmalware samples. Categorization of malware samples on the basis of their\nbehaviors is essential for the computer security community, because they\nreceive huge number of malware everyday, and the signature extraction process\nis usually based on malicious parts characterizing malware families. Microsoft\nreleased a malware classification challenge in 2015 with a huge dataset of near\n0.5 terabytes of data, containing more than 20K malware samples. The analysis\nof this dataset inspired the development of a novel paradigm that is effective\nin categorizing malware variants into their actual family groups. This paradigm\nis presented and discussed in the present paper, where emphasis has been given\nto the phases related to the extraction, and selection of a set of novel\nfeatures for the effective representation of malware samples. Features can be\ngrouped according to different characteristics of malware behavior, and their\nfusion is performed according to a per-class weighting paradigm. The proposed\nmethod achieved a very high accuracy ($\\approx$ 0.998) on the Microsoft Malware\nChallenge dataset.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 15:33:02 GMT"}, {"version": "v2", "created": "Thu, 10 Mar 2016 10:21:15 GMT"}], "update_date": "2016-03-11", "authors_parsed": [["Ahmadi", "Mansour", ""], ["Ulyanov", "Dmitry", ""], ["Semenov", "Stanislav", ""], ["Trofimov", "Mikhail", ""], ["Giacinto", "Giorgio", ""]]}, {"id": "1511.04326", "submitter": "Lars Kotthoff", "authors": "Lars Kotthoff", "title": "ICON Challenge on Algorithm Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the results of the ICON Challenge on Algorithm Selection.\n", "versions": [{"version": "v1", "created": "Thu, 12 Nov 2015 20:04:31 GMT"}], "update_date": "2015-11-16", "authors_parsed": [["Kotthoff", "Lars", ""]]}, {"id": "1511.04352", "submitter": "Fabrizio Riguzzi PhD", "authors": "Fabrizio Riguzzi", "title": "Introduzione all'Intelligenza Artificiale", "comments": "27 pages, in Italian", "journal-ref": "Terre di Confine, 2(1), January 2006", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The paper presents an introduction to Artificial Intelligence (AI) in an\naccessible and informal but precise form. The paper focuses on the algorithmic\naspects of the discipline, presenting the main techniques used in AI systems\ngroped in symbolic and subsymbolic. The last part of the paper is devoted to\nthe discussion ongoing among experts in the field and the public at large about\non the advantages and disadvantages of AI and in particular on the possible\ndangers. The personal opinion of the author on this subject concludes the\npaper. -- --\n  L'articolo presenta un'introduzione all'Intelligenza Artificiale (IA) in\nforma divulgativa e informale ma precisa. L'articolo affronta prevalentemente\ngli aspetti informatici della disciplina, presentando le principali tecniche\nusate nei sistemi di IA divise in simboliche e subsimboliche. L'ultima parte\ndell'articolo presenta il dibattito in corso tra gli esperi e il pubblico su\nvantaggi e svantaggi dell'IA e in particolare sui possibili pericoli.\nL'articolo termina con l'opinione dell'autore al riguardo.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 16:40:47 GMT"}, {"version": "v2", "created": "Sun, 16 Oct 2016 17:55:34 GMT"}, {"version": "v3", "created": "Tue, 11 May 2021 17:06:38 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Riguzzi", "Fabrizio", ""]]}, {"id": "1511.04387", "submitter": "Daniel Karapetyan Dr", "authors": "Shahriar Asta, Daniel Karapetyan, Ahmed Kheiri, Ender \\\"Ozcan, Andrew\n  J. Parkes", "title": "Combining Monte-Carlo and Hyper-heuristic methods for the Multi-mode\n  Resource-constrained Multi-project Scheduling Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-mode resource and precedence-constrained project scheduling is a\nwell-known challenging real-world optimisation problem. An important variant of\nthe problem requires scheduling of activities for multiple projects considering\navailability of local and global resources while respecting a range of\nconstraints. A critical aspect of the benchmarks addressed in this paper is\nthat the primary objective is to minimise the sum of the project completion\ntimes, with the usual makespan minimisation as a secondary objective. We\nobserve that this leads to an expected different overall structure of good\nsolutions and discuss the effects this has on the algorithm design. This paper\npresents a carefully designed hybrid of Monte-Carlo tree search, novel\nneighbourhood moves, memetic algorithms, and hyper-heuristic methods. The\nimplementation is also engineered to increase the speed with which iterations\nare performed, and to exploit the computing power of multicore machines.\nEmpirical evaluation shows that the resulting information-sharing\nmulti-component algorithm significantly outperforms other solvers on a set of\n\"hidden\" instances, i.e. instances not available at the algorithm design phase.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 18:17:32 GMT"}, {"version": "v2", "created": "Thu, 8 Sep 2016 16:43:16 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Asta", "Shahriar", ""], ["Karapetyan", "Daniel", ""], ["Kheiri", "Ahmed", ""], ["\u00d6zcan", "Ender", ""], ["Parkes", "Andrew J.", ""]]}, {"id": "1511.04412", "submitter": "Mazen Melibari", "authors": "Mazen Melibari, Pascal Poupart, Prashant Doshi and George Trimponias", "title": "Dynamic Sum Product Networks for Tractable Inference on Sequence Data\n  (Extended Version)", "comments": "Published in the Proceedings of the International Conference on\n  Probabilistic Graphical Models (PGM), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sum-Product Networks (SPN) have recently emerged as a new class of tractable\nprobabilistic graphical models. Unlike Bayesian networks and Markov networks\nwhere inference may be exponential in the size of the network, inference in\nSPNs is in time linear in the size of the network. Since SPNs represent\ndistributions over a fixed set of variables only, we propose dynamic sum\nproduct networks (DSPNs) as a generalization of SPNs for sequence data of\nvarying length. A DSPN consists of a template network that is repeated as many\ntimes as needed to model data sequences of any length. We present a local\nsearch technique to learn the structure of the template network. In contrast to\ndynamic Bayesian networks for which inference is generally exponential in the\nnumber of variables per time slice, DSPNs inherit the linear inference\ncomplexity of SPNs. We demonstrate the advantages of DSPNs over DBNs and other\nmodels on several datasets of sequence data.\n", "versions": [{"version": "v1", "created": "Fri, 13 Nov 2015 19:56:15 GMT"}, {"version": "v2", "created": "Sat, 16 Jul 2016 03:37:01 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Melibari", "Mazen", ""], ["Poupart", "Pascal", ""], ["Doshi", "Prashant", ""], ["Trimponias", "George", ""]]}, {"id": "1511.04583", "submitter": "Yanhong Annie Liu", "authors": "Yanhong A. Liu, Jon Brandvein, Scott D. Stoller, Bo Lin", "title": "Demand-Driven Incremental Object Queries", "comments": null, "journal-ref": "PPDP 2016: Proceedings of the 18th International Symposium on\n  Principles and Practice of Declarative Programming, September 2016, Pages\n  228-241. ACM Press", "doi": "10.1145/2967973.2968610", "report-no": null, "categories": "cs.PL cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object queries are essential in information seeking and decision making in\nvast areas of applications. However, a query may involve complex conditions on\nobjects and sets, which can be arbitrarily nested and aliased. The objects and\nsets involved as well as the demand---the given parameter values of\ninterest---can change arbitrarily. How to implement object queries efficiently\nunder all possible updates, and furthermore to provide complexity guarantees?\n  This paper describes an automatic method. The method allows powerful queries\nto be written completely declaratively. It transforms demand as well as all\nobjects and sets into relations. Most importantly, it defines invariants for\nnot only the query results, but also all auxiliary values about the objects and\nsets involved, including those for propagating demand, and incrementally\nmaintains all of them. Implementation and experiments with problems from a\nvariety of application areas, including distributed algorithms and\nprobabilistic queries, confirm the analyzed complexities, trade-offs, and\nsignificant improvements over prior work.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2015 17:27:33 GMT"}, {"version": "v2", "created": "Wed, 25 Nov 2015 03:34:09 GMT"}, {"version": "v3", "created": "Fri, 15 Jul 2016 18:06:30 GMT"}], "update_date": "2020-12-25", "authors_parsed": [["Liu", "Yanhong A.", ""], ["Brandvein", "Jon", ""], ["Stoller", "Scott D.", ""], ["Lin", "Bo", ""]]}, {"id": "1511.04636", "submitter": "Ji He", "authors": "Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng,\n  Mari Ostendorf", "title": "Deep Reinforcement Learning with a Natural Language Action Space", "comments": "accepted by ACL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel architecture for reinforcement learning with\ndeep neural networks designed to handle state and action spaces characterized\nby natural language, as found in text-based games. Termed a deep reinforcement\nrelevance network (DRRN), the architecture represents action and state spaces\nwith separate embedding vectors, which are combined with an interaction\nfunction to approximate the Q-function in reinforcement learning. We evaluate\nthe DRRN on two popular text games, showing superior performance over other\ndeep Q-learning architectures. Experiments with paraphrased action descriptions\nshow that the model is extracting meaning rather than simply memorizing strings\nof text.\n", "versions": [{"version": "v1", "created": "Sat, 14 Nov 2015 23:30:39 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2015 20:24:12 GMT"}, {"version": "v3", "created": "Sun, 10 Jan 2016 01:51:20 GMT"}, {"version": "v4", "created": "Sat, 16 Jan 2016 23:43:40 GMT"}, {"version": "v5", "created": "Wed, 8 Jun 2016 05:58:34 GMT"}], "update_date": "2016-06-09", "authors_parsed": [["He", "Ji", ""], ["Chen", "Jianshu", ""], ["He", "Xiaodong", ""], ["Gao", "Jianfeng", ""], ["Li", "Lihong", ""], ["Deng", "Li", ""], ["Ostendorf", "Mari", ""]]}, {"id": "1511.04646", "submitter": "Yikang Shen", "authors": "Yikang Shen, Wenge Rong, Nan Jiang, Baolin Peng, Jie Tang, Zhang Xiong", "title": "Word Embedding based Correlation Model for Question/Answer Matching", "comments": "8 pages, 2 figures", "journal-ref": "AAAI (2017) 3511--3517", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the development of community based question answering (Q&A) services, a\nlarge scale of Q&A archives have been accumulated and are an important\ninformation and knowledge resource on the web. Question and answer matching has\nbeen attached much importance to for its ability to reuse knowledge stored in\nthese systems: it can be useful in enhancing user experience with recurrent\nquestions. In this paper, we try to improve the matching accuracy by overcoming\nthe lexical gap between question and answer pairs. A Word Embedding based\nCorrelation (WEC) model is proposed by integrating advantages of both the\ntranslation model and word embedding, given a random pair of words, WEC can\nscore their co-occurrence probability in Q&A pairs and it can also leverage the\ncontinuity and smoothness of continuous space word representation to deal with\nnew pairs of words that are rare in the training parallel text. An experimental\nstudy on Yahoo! Answers dataset and Baidu Zhidao dataset shows this new\nmethod's promising potential.\n", "versions": [{"version": "v1", "created": "Sun, 15 Nov 2015 02:59:22 GMT"}, {"version": "v2", "created": "Sat, 26 Nov 2016 02:40:12 GMT"}], "update_date": "2017-05-15", "authors_parsed": [["Shen", "Yikang", ""], ["Rong", "Wenge", ""], ["Jiang", "Nan", ""], ["Peng", "Baolin", ""], ["Tang", "Jie", ""], ["Xiong", "Zhang", ""]]}, {"id": "1511.04798", "submitter": "Boyang Li", "authors": "Baohan Xu, Yanwei Fu, Yu-Gang Jiang, Boyang Li and Leonid Sigal", "title": "Heterogeneous Knowledge Transfer in Video Emotion Recognition,\n  Attribution and Summarization", "comments": "13 pages, 11 figures. Published at the IEEE Transactions on Affective\n  Computing", "journal-ref": "IEEE Transactions on Affective Computing. 2016", "doi": "10.1109/TAFFC.2016.2622690", "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotion is a key element in user-generated videos. However, it is difficult\nto understand emotions conveyed in such videos due to the complex and\nunstructured nature of user-generated content and the sparsity of video frames\nexpressing emotion. In this paper, for the first time, we study the problem of\ntransferring knowledge from heterogeneous external sources, including image and\ntextual data, to facilitate three related tasks in understanding video emotion:\nemotion recognition, emotion attribution and emotion-oriented summarization.\nSpecifically, our framework (1) learns a video encoding from an auxiliary\nemotional image dataset in order to improve supervised video emotion\nrecognition, and (2) transfers knowledge from an auxiliary textual corpora for\nzero-shot recognition of emotion classes unseen during training. The proposed\ntechnique for knowledge transfer facilitates novel applications of emotion\nattribution and emotion-oriented summarization. A comprehensive set of\nexperiments on multiple datasets demonstrate the effectiveness of our\nframework.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 01:40:15 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 06:02:45 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Xu", "Baohan", ""], ["Fu", "Yanwei", ""], ["Jiang", "Yu-Gang", ""], ["Li", "Boyang", ""], ["Sigal", "Leonid", ""]]}, {"id": "1511.05101", "submitter": "Ferenc Husz\\'ar", "authors": "Ferenc Husz\\'ar", "title": "How (not) to Train your Generative Model: Scheduled Sampling,\n  Likelihood, Adversary?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern applications and progress in deep learning research have created\nrenewed interest for generative models of text and of images. However, even\ntoday it is unclear what objective functions one should use to train and\nevaluate these models. In this paper we present two contributions.\n  Firstly, we present a critique of scheduled sampling, a state-of-the-art\ntraining method that contributed to the winning entry to the MSCOCO image\ncaptioning benchmark in 2015. Here we show that despite this impressive\nempirical performance, the objective function underlying scheduled sampling is\nimproper and leads to an inconsistent learning algorithm.\n  Secondly, we revisit the problems that scheduled sampling was meant to\naddress, and present an alternative interpretation. We argue that maximum\nlikelihood is an inappropriate training objective when the end-goal is to\ngenerate natural-looking samples. We go on to derive an ideal objective\nfunction to use in this situation instead. We introduce a generalisation of\nadversarial training, and show how such method can interpolate between maximum\nlikelihood training and our ideal training objective. To our knowledge this is\nthe first theoretical analysis that explains why adversarial training tends to\nproduce samples with higher perceived quality.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 19:43:19 GMT"}], "update_date": "2015-11-17", "authors_parsed": [["Husz\u00e1r", "Ferenc", ""]]}, {"id": "1511.05175", "submitter": "Mohamed Elhoseiny Mohamed Elhoseiny", "authors": "Mohamed Elhoseiny, Tarek El-Gaaly, Amr Bakry, Ahmed Elgammal", "title": "Convolutional Models for Joint Object Categorization and Pose Estimation", "comments": "only for workshop presentation at ICLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the task of Object Recognition, there exists a dichotomy between the\ncategorization of objects and estimating object pose, where the former\nnecessitates a view-invariant representation, while the latter requires a\nrepresentation capable of capturing pose information over different categories\nof objects. With the rise of deep architectures, the prime focus has been on\nobject category recognition. Deep learning methods have achieved wide success\nin this task. In contrast, object pose regression using these approaches has\nreceived relatively much less attention. In this paper we show how deep\narchitectures, specifically Convolutional Neural Networks (CNN), can be adapted\nto the task of simultaneous categorization and pose estimation of objects. We\ninvestigate and analyze the layers of various CNN models and extensively\ncompare between them with the goal of discovering how the layers of distributed\nrepresentations of CNNs represent object pose information and how this\ncontradicts with object category representations. We extensively experiment on\ntwo recent large and challenging multi-view datasets. Our models achieve better\nthan state-of-the-art performance on both datasets.\n", "versions": [{"version": "v1", "created": "Mon, 16 Nov 2015 21:08:22 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2015 23:17:11 GMT"}, {"version": "v3", "created": "Thu, 7 Jan 2016 23:40:23 GMT"}, {"version": "v4", "created": "Wed, 20 Jan 2016 22:41:19 GMT"}, {"version": "v5", "created": "Mon, 22 Feb 2016 23:54:23 GMT"}, {"version": "v6", "created": "Tue, 19 Apr 2016 17:56:34 GMT"}], "update_date": "2016-04-20", "authors_parsed": [["Elhoseiny", "Mohamed", ""], ["El-Gaaly", "Tarek", ""], ["Bakry", "Amr", ""], ["Elgammal", "Ahmed", ""]]}, {"id": "1511.05234", "submitter": "Huijuan Xu", "authors": "Huijuan Xu and Kate Saenko", "title": "Ask, Attend and Answer: Exploring Question-Guided Spatial Attention for\n  Visual Question Answering", "comments": "include test-standard result on VQA full release (V1.0) dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of Visual Question Answering (VQA), which requires\njoint image and language understanding to answer a question about a given\nphotograph. Recent approaches have applied deep image captioning methods based\non convolutional-recurrent networks to this problem, but have failed to model\nspatial inference. To remedy this, we propose a model we call the Spatial\nMemory Network and apply it to the VQA task. Memory networks are recurrent\nneural networks with an explicit attention mechanism that selects certain parts\nof the information stored in memory. Our Spatial Memory Network stores neuron\nactivations from different spatial regions of the image in its memory, and uses\nthe question to choose relevant regions for computing the answer, a process of\nwhich constitutes a single \"hop\" in the network. We propose a novel spatial\nattention architecture that aligns words with image patches in the first hop,\nand obtain improved results by adding a second attention hop which considers\nthe whole question to choose visual evidence based on the results of the first\nhop. To better understand the inference process learned by the network, we\ndesign synthetic questions that specifically require spatial inference and\nvisualize the attention weights. We evaluate our model on two published visual\nquestion answering datasets, DAQUAR [1] and VQA [2], and obtain improved\nresults compared to a strong deep baseline model (iBOWIMG) which concatenates\nimage and question features to predict the answer [3].\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 01:00:04 GMT"}, {"version": "v2", "created": "Sat, 19 Mar 2016 03:06:58 GMT"}], "update_date": "2016-03-22", "authors_parsed": [["Xu", "Huijuan", ""], ["Saenko", "Kate", ""]]}, {"id": "1511.05385", "submitter": "Ruth Misener", "authors": "Doniyor Ulmasov, Caroline Baroukh, Benoit Chachuat, Marc Peter\n  Deisenroth, Ruth Misener", "title": "Bayesian Optimization with Dimension Scheduling: Application to\n  Biological Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian Optimization (BO) is a data-efficient method for global black-box\noptimization of an expensive-to-evaluate fitness function. BO typically assumes\nthat computation cost of BO is cheap, but experiments are time consuming or\ncostly. In practice, this allows us to optimize ten or fewer critical\nparameters in up to 1,000 experiments. But experiments may be less expensive\nthan BO methods assume: In some simulation models, we may be able to conduct\nmultiple thousands of experiments in a few hours, and the computational burden\nof BO is no longer negligible compared to experimentation time. To address this\nchallenge we introduce a new Dimension Scheduling Algorithm (DSA), which\nreduces the computational burden of BO for many experiments. The key idea is\nthat DSA optimizes the fitness function only along a small set of dimensions at\neach iteration. This DSA strategy (1) reduces the necessary computation time,\n(2) finds good solutions faster than the traditional BO method, and (3) can be\nparallelized straightforwardly. We evaluate the DSA in the context of\noptimizing parameters of dynamic models of microalgae metabolism and show\nfaster convergence than traditional BO.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 13:08:10 GMT"}], "update_date": "2016-08-18", "authors_parsed": [["Ulmasov", "Doniyor", ""], ["Baroukh", "Caroline", ""], ["Chachuat", "Benoit", ""], ["Deisenroth", "Marc Peter", ""], ["Misener", "Ruth", ""]]}, {"id": "1511.05488", "submitter": "Christian Blum", "authors": "Christian Blum and Verena V. Hafner", "title": "Active exploration of sensor networks from a robotics perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional algorithms for robots who need to integrate into a wireless\nnetwork often focus on one specific task. In this work we want to develop\nsimple, adaptive and reusable algorithms for real world applications for this\nscenario. Starting with the most basic task for mobile wireless network nodes,\nfinding the position of another node, we introduce an algorithm able to solve\nthis task. We then show how this algorithm can readily be employed to solve a\nlarge number of other related tasks like finding the optimal position to bridge\ntwo static network nodes. For this we first introduce a meta-algorithm inspired\nby autonomous robot learning strategies and the concept of internal models\nwhich yields a class of source seeking algorithms for mobile nodes. The\neffectiveness of this algorithm is demonstrated in real world experiments using\na physical mobile robot and standard 802.11 wireless LAN in an office\nenvironment. We also discuss the differences to conventional algorithms and\ngive the robotics perspective on this class of algorithms. Then we proceed to\nshow how more complex tasks, which might be encountered by mobile nodes, can be\nencoded in the same framework and how the introduced algorithm can solve them.\nThese tasks can be direct (cross layer) optimization tasks or can also encode\nmore complex tasks like bridging two network nodes. We choose the bridging\nscenario as an example, implemented on a real physical robot, and show how the\nrobot can solve it in a real world experiment.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 17:50:23 GMT"}], "update_date": "2015-11-18", "authors_parsed": [["Blum", "Christian", ""], ["Hafner", "Verena V.", ""]]}, {"id": "1511.05493", "submitter": "Yujia Li", "authors": "Yujia Li, Daniel Tarlow, Marc Brockschmidt, Richard Zemel", "title": "Gated Graph Sequence Neural Networks", "comments": "Published as a conference paper in ICLR 2016. Fixed a typo", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph-structured data appears frequently in domains including chemistry,\nnatural language semantics, social networks, and knowledge bases. In this work,\nwe study feature learning techniques for graph-structured inputs. Our starting\npoint is previous work on Graph Neural Networks (Scarselli et al., 2009), which\nwe modify to use gated recurrent units and modern optimization techniques and\nthen extend to output sequences. The result is a flexible and broadly useful\nclass of neural network models that has favorable inductive biases relative to\npurely sequence-based models (e.g., LSTMs) when the problem is\ngraph-structured. We demonstrate the capabilities on some simple AI (bAbI) and\ngraph algorithm learning tasks. We then show it achieves state-of-the-art\nperformance on a problem from program verification, in which subgraphs need to\nbe matched to abstract data structures.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 18:10:12 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2015 22:03:02 GMT"}, {"version": "v3", "created": "Tue, 3 May 2016 21:55:01 GMT"}, {"version": "v4", "created": "Fri, 22 Sep 2017 21:36:00 GMT"}], "update_date": "2017-09-26", "authors_parsed": [["Li", "Yujia", ""], ["Tarlow", "Daniel", ""], ["Brockschmidt", "Marc", ""], ["Zemel", "Richard", ""]]}, {"id": "1511.05506", "submitter": "Artem Chernodub N", "authors": "Artem Chernodub and Dmitry Dziuba", "title": "Neurocontrol methods review", "comments": "in Russian", "journal-ref": "Problems in Systems Programming, 2011, No. 2, p. 79-94", "doi": null, "report-no": null, "categories": "cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods of applying neural networks to control plants are considered. Methods\nand schemes are described, their advantages and disadvantages are discussed.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 18:58:20 GMT"}], "update_date": "2015-11-18", "authors_parsed": [["Chernodub", "Artem", ""], ["Dziuba", "Dmitry", ""]]}, {"id": "1511.05547", "submitter": "Baochen Sun", "authors": "Baochen Sun, Jiashi Feng, Kate Saenko", "title": "Return of Frustratingly Easy Domain Adaptation", "comments": "Fixed typos. Full paper to appear in AAAI-16. Extended Abstract of\n  the full paper to appear in TASK-CV 2015 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike human learning, machine learning often fails to handle changes between\ntraining (source) and test (target) input distributions. Such domain shifts,\ncommon in practical scenarios, severely damage the performance of conventional\nmachine learning methods. Supervised domain adaptation methods have been\nproposed for the case when the target data have labels, including some that\nperform very well despite being \"frustratingly easy\" to implement. However, in\npractice, the target domain is often unlabeled, requiring unsupervised\nadaptation. We propose a simple, effective, and efficient method for\nunsupervised domain adaptation called CORrelation ALignment (CORAL). CORAL\nminimizes domain shift by aligning the second-order statistics of source and\ntarget distributions, without requiring any target labels. Even though it is\nextraordinarily simple--it can be implemented in four lines of Matlab\ncode--CORAL performs remarkably well in extensive evaluations on standard\nbenchmark datasets.\n", "versions": [{"version": "v1", "created": "Tue, 17 Nov 2015 20:53:26 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2015 05:39:43 GMT"}], "update_date": "2015-12-10", "authors_parsed": [["Sun", "Baochen", ""], ["Feng", "Jiashi", ""], ["Saenko", "Kate", ""]]}, {"id": "1511.05643", "submitter": "Md Kamrul Hasan", "authors": "Md Kamrul Hasan, Christopher J. Pal", "title": "A New Smooth Approximation to the Zero One Loss with a Probabilistic\n  Interpretation", "comments": "32 pages, 7 figures, 15 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine a new form of smooth approximation to the zero one loss in which\nlearning is performed using a reformulation of the widely used logistic\nfunction. Our approach is based on using the posterior mean of a novel\ngeneralized Beta-Bernoulli formulation. This leads to a generalized logistic\nfunction that approximates the zero one loss, but retains a probabilistic\nformulation conferring a number of useful properties. The approach is easily\ngeneralized to kernel logistic regression and easily integrated into methods\nfor structured prediction. We present experiments in which we learn such models\nusing an optimization method consisting of a combination of gradient descent\nand coordinate descent using localized grid search so as to escape from local\nminima. Our experiments indicate that optimization quality is improved when\nlearning meta-parameters are themselves optimized using a validation set. Our\nexperiments show improved performance relative to widely used logistic and\nhinge loss methods on a wide variety of problems ranging from standard UC\nIrvine and libSVM evaluation datasets to product review predictions and a\nvisual information extraction task. We observe that the approach: 1) is more\nrobust to outliers compared to the logistic and hinge losses; 2) outperforms\ncomparable logistic and max margin models on larger scale benchmark problems;\n3) when combined with Gaussian- Laplacian mixture prior on parameters the\nkernelized version of our formulation yields sparser solutions than Support\nVector Machine classifiers; and 4) when integrated into a probabilistic\nstructured prediction technique our approach provides more accurate\nprobabilities yielding improved inference and increasing information extraction\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 02:31:16 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Hasan", "Md Kamrul", ""], ["Pal", "Christopher J.", ""]]}, {"id": "1511.05662", "submitter": "Hankz Hankui Zhuo", "authors": "Xin Tian, Hankz Hankui Zhuo, Subbarao Kambhampati", "title": "Discovering Underlying Plans Based on Distributed Representations of\n  Actions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plan recognition aims to discover target plans (i.e., sequences of actions)\nbehind observed actions, with history plan libraries or domain models in hand.\nPrevious approaches either discover plans by maximally \"matching\" observed\nactions to plan libraries, assuming target plans are from plan libraries, or\ninfer plans by executing domain models to best explain the observed actions,\nassuming complete domain models are available. In real world applications,\nhowever, target plans are often not from plan libraries and complete domain\nmodels are often not available, since building complete sets of plans and\ncomplete domain models are often difficult or expensive. In this paper we view\nplan libraries as corpora and learn vector representations of actions using the\ncorpora; we then discover target plans based on the vector representations. Our\napproach is capable of discovering underlying plans that are not from plan\nlibraries, without requiring domain models provided. We empirically demonstrate\nthe effectiveness of our approach by comparing its performance to traditional\nplan recognition approaches in three planning domains.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 05:50:14 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Tian", "Xin", ""], ["Zhuo", "Hankz Hankui", ""], ["Kambhampati", "Subbarao", ""]]}, {"id": "1511.05719", "submitter": "Joerg Schoenfisch", "authors": "Joerg Schoenfisch, Janno von St\u007fulpnagel, Jens Ortmann, Christian\n  Meilicke, Heiner Stuckenschmidt", "title": "Using Abduction in Markov Logic Networks for Root Cause Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  IT infrastructure is a crucial part in most of today's business operations.\nHigh availability and reliability, and short response times to outages are\nessential. Thus a high amount of tool support and automation in risk management\nis desirable to decrease outages. We propose a new approach for calculating the\nroot cause for an observed failure in an IT infrastructure. Our approach is\nbased on Abduction in Markov Logic Networks. Abduction aims to find an\nexplanation for a given observation in the light of some background knowledge.\nIn failure diagnosis, the explanation corresponds to the root cause, the\nobservation to the failure of a component, and the background knowledge to the\ndependency graph extended by potential risks. We apply a method to extend a\nMarkov Logic Network in order to conduct abductive reasoning, which is not\nnaturally supported in this formalism. Our approach exhibits a high amount of\nreusability and enables users without specific knowledge of a concrete\ninfrastructure to gain viable insights in the case of an incident. We\nimplemented the method in a tool and illustrate its suitability for root cause\nanalysis by applying it to a sample scenario.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 10:13:43 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Schoenfisch", "Joerg", ""], ["von St\u007fulpnagel", "Janno", ""], ["Ortmann", "Jens", ""], ["Meilicke", "Christian", ""], ["Stuckenschmidt", "Heiner", ""]]}, {"id": "1511.05749", "submitter": "Khaled Oumaima", "authors": "Oumaima Khaled", "title": "Solution Repair/Recovery in Uncertain Optimization Environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Operation management problems (such as Production Planning and Scheduling)\nare represented and formulated as optimization models. The resolution of such\noptimization models leads to solutions which have to be operated in an\norganization. However, the conditions under which the optimal solution is\nobtained rarely correspond exactly to the conditions under which the solution\nwill be operated in the organization.Therefore, in most practical contexts, the\ncomputed optimal solution is not anymore optimal under the conditions in which\nit is operated. Indeed, it can be \"far from optimal\" or even not feasible. For\ndifferent reasons, we hadn't the possibility to completely re-optimize the\nexisting solution or plan. As a consequence, it is necessary to look for\n\"repair solutions\", i.e., solutions that have a good behavior with respect to\npossible scenarios, or with respect to uncertainty of the parameters of the\nmodel. To tackle the problem, the computed solution should be such that it is\npossible to \"repair\" it through a local re-optimization guided by the user or\nthrough a limited change aiming at minimizing the impact of taking into\nconsideration the scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 12:05:34 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Khaled", "Oumaima", ""]]}, {"id": "1511.05835", "submitter": "Jose M. Pe\\~na", "authors": "Jose M. Pe\\~na", "title": "Alternative Markov and Causal Properties for Acyclic Directed Mixed\n  Graphs", "comments": "Minor changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend Andersson-Madigan-Perlman chain graphs by (i) relaxing the\nsemidirected acyclity constraint so that only directed cycles are forbidden,\nand (ii) allowing up to two edges between any pair of nodes. We introduce\nglobal, and ordered local and pairwise Markov properties for the new models. We\nshow the equivalence of these properties for strictly positive probability\ndistributions. We also show that when the random variables are continuous, the\nnew models can be interpreted as systems of structural equations with\ncorrelated errors. This enables us to adapt Pearl's do-calculus to them.\nFinally, we describe an exact algorithm for learning the new models from\nobservational and interventional data via answer set programming.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 15:33:57 GMT"}, {"version": "v2", "created": "Sun, 3 Jan 2016 19:34:34 GMT"}, {"version": "v3", "created": "Thu, 11 Feb 2016 10:28:10 GMT"}, {"version": "v4", "created": "Fri, 19 Feb 2016 09:42:49 GMT"}], "update_date": "2016-02-22", "authors_parsed": [["Pe\u00f1a", "Jose M.", ""]]}, {"id": "1511.05897", "submitter": "Harrison Edwards", "authors": "Harrison Edwards, Amos Storkey", "title": "Censoring Representations with an Adversary", "comments": "Paper accepted to ICLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In practice, there are often explicit constraints on what representations or\ndecisions are acceptable in an application of machine learning. For example it\nmay be a legal requirement that a decision must not favour a particular group.\nAlternatively it can be that that representation of data must not have\nidentifying information. We address these two related issues by learning\nflexible representations that minimize the capability of an adversarial critic.\nThis adversary is trying to predict the relevant sensitive variable from the\nrepresentation, and so minimizing the performance of the adversary ensures\nthere is little or no information in the representation about the sensitive\nvariable. We demonstrate this adversarial approach on two problems: making\ndecisions free from discrimination and removing private information from\nimages. We formulate the adversarial model as a minimax problem, and optimize\nthat minimax objective using a stochastic gradient alternate min-max optimizer.\nWe demonstrate the ability to provide discriminant free representations for\nstandard test problems, and compare with previous state of the art methods for\nfairness, showing statistically significant improvement across most cases. The\nflexibility of this method is shown via a novel problem: removing annotations\nfrom images, from unaligned training examples of annotated and unannotated\nimages, and with no a priori knowledge of the form of annotation provided to\nthe model.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 18:06:24 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2016 15:53:45 GMT"}, {"version": "v3", "created": "Fri, 4 Mar 2016 11:01:34 GMT"}], "update_date": "2016-03-09", "authors_parsed": [["Edwards", "Harrison", ""], ["Storkey", "Amos", ""]]}, {"id": "1511.05911", "submitter": "Bo Zong", "authors": "Bo Zong, Xusheng Xiao, Zhichun Li, Zhenyu Wu, Zhiyun Qian, Xifeng Yan,\n  Ambuj K. Singh, Guofei Jiang", "title": "Behavior Query Discovery in System-Generated Temporal Graphs", "comments": "The full version of the paper \"Behavior Query Discovery in\n  System-Generated Temporal Graphs\", to appear in VLDB'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.DB", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Computer system monitoring generates huge amounts of logs that record the\ninteraction of system entities. How to query such data to better understand\nsystem behaviors and identify potential system risks and malicious behaviors\nbecomes a challenging task for system administrators due to the dynamics and\nheterogeneity of the data. System monitoring data are essentially heterogeneous\ntemporal graphs with nodes being system entities and edges being their\ninteractions over time. Given the complexity of such graphs, it becomes\ntime-consuming for system administrators to manually formulate useful queries\nin order to examine abnormal activities, attacks, and vulnerabilities in\ncomputer systems.\n  In this work, we investigate how to query temporal graphs and treat query\nformulation as a discriminative temporal graph pattern mining problem. We\nintroduce TGMiner to mine discriminative patterns from system logs, and these\npatterns can be taken as templates for building more complex queries. TGMiner\nleverages temporal information in graphs to prune graph patterns that share\nsimilar growth trend without compromising pattern quality. Experimental results\non real system data show that TGMiner is 6-32 times faster than baseline\nmethods. The discovered patterns were verified by system experts; they achieved\nhigh precision (97%) and recall (91%).\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 19:03:41 GMT"}, {"version": "v2", "created": "Thu, 19 Nov 2015 16:39:41 GMT"}], "update_date": "2015-11-20", "authors_parsed": [["Zong", "Bo", ""], ["Xiao", "Xusheng", ""], ["Li", "Zhichun", ""], ["Wu", "Zhenyu", ""], ["Qian", "Zhiyun", ""], ["Yan", "Xifeng", ""], ["Singh", "Ambuj K.", ""], ["Jiang", "Guofei", ""]]}, {"id": "1511.05943", "submitter": "Dipan Pal", "authors": "Dipan K. Pal, Marios Savvides", "title": "Unitary-Group Invariant Kernels and Features from Transformed Unlabeled\n  Data", "comments": "11 page main paper (including references), 2 page supplementary, for\n  a total of 13 pages. Submitted for review at ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of representations invariant to common transformations of the data\nis important to learning. Most techniques have focused on local approximate\ninvariance implemented within expensive optimization frameworks lacking\nexplicit theoretical guarantees. In this paper, we study kernels that are\ninvariant to the unitary group while having theoretical guarantees in\naddressing practical issues such as (1) unavailability of transformed versions\nof labelled data and (2) not observing all transformations. We present a\ntheoretically motivated alternate approach to the invariant kernel SVM. Unlike\nprevious approaches to the invariant SVM, the proposed formulation solves both\nissues mentioned. We also present a kernel extension of a recent technique to\nextract linear unitary-group invariant features addressing both issues and\nextend some guarantees regarding invariance and stability. We present\nexperiments on the UCI ML datasets to illustrate and validate our methods.\n", "versions": [{"version": "v1", "created": "Wed, 18 Nov 2015 20:48:18 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Pal", "Dipan K.", ""], ["Savvides", "Marios", ""]]}, {"id": "1511.06030", "submitter": "Bryan Hooi", "authors": "Bryan Hooi, Neil Shah, Alex Beutel, Stephan Gunnemann, Leman Akoglu,\n  Mohit Kumar, Disha Makhija, Christos Faloutsos", "title": "BIRDNEST: Bayesian Inference for Ratings-Fraud Detection", "comments": "9 pages; v2: minor typos corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Review fraud is a pervasive problem in online commerce, in which fraudulent\nsellers write or purchase fake reviews to manipulate perception of their\nproducts and services. Fake reviews are often detected based on several signs,\nincluding 1) they occur in short bursts of time; 2) fraudulent user accounts\nhave skewed rating distributions. However, these may both be true in any given\ndataset. Hence, in this paper, we propose an approach for detecting fraudulent\nreviews which combines these 2 approaches in a principled manner, allowing\nsuccessful detection even when one of these signs is not present. To combine\nthese 2 approaches, we formulate our Bayesian Inference for Rating Data (BIRD)\nmodel, a flexible Bayesian model of user rating behavior. Based on our model we\nformulate a likelihood-based suspiciousness metric, Normalized Expected\nSurprise Total (NEST). We propose a linear-time algorithm for performing\nBayesian inference using our model and computing the metric. Experiments on\nreal data show that BIRDNEST successfully spots review fraud in large,\nreal-world graphs: the 50 most suspicious users of the Flipkart platform\nflagged by our algorithm were investigated and all identified as fraudulent by\ndomain experts at Flipkart.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 00:16:17 GMT"}, {"version": "v2", "created": "Mon, 7 Mar 2016 23:38:12 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Hooi", "Bryan", ""], ["Shah", "Neil", ""], ["Beutel", "Alex", ""], ["Gunnemann", "Stephan", ""], ["Akoglu", "Leman", ""], ["Kumar", "Mohit", ""], ["Makhija", "Disha", ""], ["Faloutsos", "Christos", ""]]}, {"id": "1511.06052", "submitter": "Yi Yang", "authors": "Yi Yang and Jacob Eisenstein", "title": "Overcoming Language Variation in Sentiment Analysis with Social\n  Attention", "comments": "Published in Transactions of the Association for Computational\n  Linguistics (TACL), 2017. Please cite the TACL version:\n  https://transacl.org/ojs/index.php/tacl/article/view/1024", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variation in language is ubiquitous, particularly in newer forms of writing\nsuch as social media. Fortunately, variation is not random, it is often linked\nto social properties of the author. In this paper, we show how to exploit\nsocial networks to make sentiment analysis more robust to social language\nvariation. The key idea is linguistic homophily: the tendency of socially\nlinked individuals to use language in similar ways. We formalize this idea in a\nnovel attention-based neural network architecture, in which attention is\ndivided among several basis models, depending on the author's position in the\nsocial network. This has the effect of smoothing the classification function\nacross the social network, and makes it possible to induce personalized\nclassifiers even for authors for whom there is no labeled data or demographic\nmetadata. This model significantly improves the accuracies of sentiment\nanalysis on Twitter and on review data.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 03:54:15 GMT"}, {"version": "v2", "created": "Wed, 23 Dec 2015 15:36:48 GMT"}, {"version": "v3", "created": "Wed, 28 Dec 2016 22:07:53 GMT"}, {"version": "v4", "created": "Sat, 26 Aug 2017 15:11:01 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Yang", "Yi", ""], ["Eisenstein", "Jacob", ""]]}, {"id": "1511.06191", "submitter": "Daniel Borchmann", "authors": "Daniel Borchmann and Bernhard Ganter", "title": "Abstract Attribute Exploration with Partial Object Descriptions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attribute exploration has been investigated in several studies, with\nparticular emphasis on the algorithmic aspects of this knowledge acquisition\nmethod. In its basic version the method itself is rather simple and\ntransparent. But when background knowledge and partially described\ncounter-examples are admitted, it gets more difficult. Here we discuss this\ncase in an abstract, somewhat \"axiomatic\" setting, providing a terminology that\nclarifies the abstract strategy of the method rather than its algorithmic\nimplementation.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 14:59:06 GMT"}], "update_date": "2015-11-20", "authors_parsed": [["Borchmann", "Daniel", ""], ["Ganter", "Bernhard", ""]]}, {"id": "1511.06341", "submitter": "Ramanathan Guha", "authors": "Ramanathan V Guha, Vineet Gupta", "title": "Communicating Semantics: Reference by Description", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Messages often refer to entities such as people, places and events. Correct\nidentification of the intended reference is an essential part of communication.\nLack of shared unique names often complicates entity reference. Shared\nknowledge can be used to construct uniquely identifying descriptive references\nfor entities with ambiguous names. We introduce a mathematical model for\n`Reference by Description', derive results on the conditions under which, with\nhigh probability, programs can construct unambiguous references to most\nentities in the domain of discourse and provide empirical validation of these\nresults.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 20:14:43 GMT"}, {"version": "v2", "created": "Fri, 20 Nov 2015 19:33:36 GMT"}, {"version": "v3", "created": "Thu, 21 Jan 2016 00:42:06 GMT"}, {"version": "v4", "created": "Mon, 7 Mar 2016 16:41:38 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Guha", "Ramanathan V", ""], ["Gupta", "Vineet", ""]]}, {"id": "1511.06380", "submitter": "William Lotter", "authors": "William Lotter, Gabriel Kreiman, David Cox", "title": "Unsupervised Learning of Visual Structure using Predictive Generative\n  Networks", "comments": "under review as conference paper at ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to predict future states of the environment is a central pillar\nof intelligence. At its core, effective prediction requires an internal model\nof the world and an understanding of the rules by which the world changes.\nHere, we explore the internal models developed by deep neural networks trained\nusing a loss based on predicting future frames in synthetic video sequences,\nusing a CNN-LSTM-deCNN framework. We first show that this architecture can\nachieve excellent performance in visual sequence prediction tasks, including\nstate-of-the-art performance in a standard 'bouncing balls' dataset (Sutskever\net al., 2009). Using a weighted mean-squared error and adversarial loss\n(Goodfellow et al., 2014), the same architecture successfully extrapolates\nout-of-the-plane rotations of computer-generated faces. Furthermore, despite\nbeing trained end-to-end to predict only pixel-level information, our\nPredictive Generative Networks learn a representation of the latent structure\nof the underlying three-dimensional objects themselves. Importantly, we find\nthat this representation is naturally tolerant to object transformations, and\ngeneralizes well to new tasks, such as classification of static images. Similar\nmodels trained solely with a reconstruction loss fail to generalize as\neffectively. We argue that prediction can serve as a powerful unsupervised loss\nfor learning rich internal representations of high-level object features.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 21:10:17 GMT"}, {"version": "v2", "created": "Wed, 20 Jan 2016 05:50:46 GMT"}], "update_date": "2016-01-21", "authors_parsed": [["Lotter", "William", ""], ["Kreiman", "Gabriel", ""], ["Cox", "David", ""]]}, {"id": "1511.06410", "submitter": "Yuandong Tian", "authors": "Yuandong Tian and Yan Zhu", "title": "Better Computer Go Player with Neural Network and Long-term Prediction", "comments": "10 pages, 9 without references. Submission for ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Competing with top human players in the ancient game of Go has been a\nlong-term goal of artificial intelligence. Go's high branching factor makes\ntraditional search techniques ineffective, even on leading-edge hardware, and\nGo's evaluation function could change drastically with one stone change. Recent\nworks [Maddison et al. (2015); Clark & Storkey (2015)] show that search is not\nstrictly necessary for machine Go players. A pure pattern-matching approach,\nbased on a Deep Convolutional Neural Network (DCNN) that predicts the next\nmove, can perform as well as Monte Carlo Tree Search (MCTS)-based open source\nGo engines such as Pachi [Baudis & Gailly (2012)] if its search budget is\nlimited. We extend this idea in our bot named darkforest, which relies on a\nDCNN designed for long-term predictions. Darkforest substantially improves the\nwin rate for pattern-matching approaches against MCTS-based approaches, even\nwith looser search budgets. Against human players, the newest versions,\ndarkfores2, achieve a stable 3d level on KGS Go Server as a ranked bot, a\nsubstantial improvement upon the estimated 4k-5k ranks for DCNN reported in\nClark & Storkey (2015) based on games against other machine players. Adding\nMCTS to darkfores2 creates a much stronger player named darkfmcts3: with 5000\nrollouts, it beats Pachi with 10k rollouts in all 250 games; with 75k rollouts\nit achieves a stable 5d level in KGS server, on par with state-of-the-art Go\nAIs (e.g., Zen, DolBaram, CrazyStone) except for AlphaGo [Silver et al.\n(2016)]; with 110k rollouts, it won the 3rd place in January KGS Go Tournament.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 21:59:58 GMT"}, {"version": "v2", "created": "Tue, 26 Jan 2016 07:17:06 GMT"}, {"version": "v3", "created": "Mon, 29 Feb 2016 15:52:34 GMT"}], "update_date": "2016-03-01", "authors_parsed": [["Tian", "Yuandong", ""], ["Zhu", "Yan", ""]]}, {"id": "1511.06438", "submitter": "Danushka Bollegala", "authors": "Danushka Bollegala, Alsuhaibani Mohammed, Takanori Maehara, Ken-ichi\n  Kawarabayashi", "title": "Joint Word Representation Learning using a Corpus and a Semantic Lexicon", "comments": "Accepted to AAAI-2016", "journal-ref": "Proceedings of the AAAI 2016", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods for learning word representations using large text corpora have\nreceived much attention lately due to their impressive performance in numerous\nnatural language processing (NLP) tasks such as, semantic similarity\nmeasurement, and word analogy detection. Despite their success, these\ndata-driven word representation learning methods do not consider the rich\nsemantic relational structure between words in a co-occurring context. On the\nother hand, already much manual effort has gone into the construction of\nsemantic lexicons such as the WordNet that represent the meanings of words by\ndefining the various relationships that exist among the words in a language. We\nconsider the question, can we improve the word representations learnt using a\ncorpora by integrating the knowledge from semantic lexicons?. For this purpose,\nwe propose a joint word representation learning method that simultaneously\npredicts the co-occurrences of two words in a sentence subject to the\nrelational constrains given by the semantic lexicon. We use relations that\nexist between words in the lexicon to regularize the word representations\nlearnt from the corpus. Our proposed method statistically significantly\noutperforms previously proposed methods for incorporating semantic lexicons\ninto word representations on several benchmark datasets for semantic similarity\nand word analogy.\n", "versions": [{"version": "v1", "created": "Thu, 19 Nov 2015 22:58:10 GMT"}], "update_date": "2015-11-23", "authors_parsed": [["Bollegala", "Danushka", ""], ["Mohammed", "Alsuhaibani", ""], ["Maehara", "Takanori", ""], ["Kawarabayashi", "Ken-ichi", ""]]}, {"id": "1511.06586", "submitter": "Chee Seng Chan", "authors": "Ven Jyn Kok, Mei Kuan Lim, Chee Seng Chan", "title": "Crowd Behavior Analysis: A Review where Physics meets Biology", "comments": "Accepted in Neurocomputing, 31 pages, 180 references", "journal-ref": "Neurocomputing 177 (2016) 342-362", "doi": "10.1016/j.neucom.2015.11.021", "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the traits emerged in a mass gathering are often non-deliberative,\nthe act of mass impulse may lead to irre- vocable crowd disasters. The two-fold\nincrease of carnage in crowd since the past two decades has spurred significant\nadvances in the field of computer vision, towards effective and proactive crowd\nsurveillance. Computer vision stud- ies related to crowd are observed to\nresonate with the understanding of the emergent behavior in physics (complex\nsystems) and biology (animal swarm). These studies, which are inspired by\nbiology and physics, share surprisingly common insights, and interesting\ncontradictions. However, this aspect of discussion has not been fully explored.\nTherefore, this survey provides the readers with a review of the\nstate-of-the-art methods in crowd behavior analysis from the physics and\nbiologically inspired perspectives. We provide insights and comprehensive\ndiscussions for a broader understanding of the underlying prospect of blending\nphysics and biology studies in computer vision.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 13:19:44 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Kok", "Ven Jyn", ""], ["Lim", "Mei Kuan", ""], ["Chan", "Chee Seng", ""]]}, {"id": "1511.06728", "submitter": "Natalia Neverova", "authors": "Natalia Neverova, Christian Wolf, Florian Nebout, Graham Taylor", "title": "Hand Pose Estimation through Semi-Supervised and Weakly-Supervised\n  Learning", "comments": "13 pages, 10 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for hand pose estimation based on a deep regressor\ntrained on two different kinds of input. Raw depth data is fused with an\nintermediate representation in the form of a segmentation of the hand into\nparts. This intermediate representation contains important topological\ninformation and provides useful cues for reasoning about joint locations. The\nmapping from raw depth to segmentation maps is learned in a\nsemi/weakly-supervised way from two different datasets: (i) a synthetic dataset\ncreated through a rendering pipeline including densely labeled ground truth\n(pixelwise segmentations); and (ii) a dataset with real images for which ground\ntruth joint positions are available, but not dense segmentations. Loss for\ntraining on real images is generated from a patch-wise restoration process,\nwhich aligns tentative segmentation maps with a large dictionary of synthetic\nposes. The underlying premise is that the domain shift between synthetic and\nreal data is smaller in the intermediate representation, where labels carry\ngeometric and topological meaning, than in the raw input domain. Experiments on\nthe NYU dataset show that the proposed training method decreases error on\njoints over direct regression of joints from depth data by 15.7%.\n", "versions": [{"version": "v1", "created": "Fri, 20 Nov 2015 19:19:00 GMT"}, {"version": "v2", "created": "Wed, 8 Jun 2016 13:31:05 GMT"}, {"version": "v3", "created": "Thu, 9 Jun 2016 06:08:54 GMT"}, {"version": "v4", "created": "Fri, 15 Sep 2017 09:24:57 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Neverova", "Natalia", ""], ["Wolf", "Christian", ""], ["Nebout", "Florian", ""], ["Taylor", "Graham", ""]]}, {"id": "1511.06890", "submitter": "Kian Hsiang Low", "authors": "Chun Kai Ling, Kian Hsiang Low, Patrick Jaillet", "title": "Gaussian Process Planning with Lipschitz Continuous Reward Functions:\n  Towards Unifying Bayesian Optimization, Active Learning, and Beyond", "comments": "30th AAAI Conference on Artificial Intelligence (AAAI 2016), Extended\n  version with proofs, 17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel nonmyopic adaptive Gaussian process planning\n(GPP) framework endowed with a general class of Lipschitz continuous reward\nfunctions that can unify some active learning/sensing and Bayesian optimization\ncriteria and offer practitioners some flexibility to specify their desired\nchoices for defining new tasks/problems. In particular, it utilizes a\nprincipled Bayesian sequential decision problem framework for jointly and\nnaturally optimizing the exploration-exploitation trade-off. In general, the\nresulting induced GPP policy cannot be derived exactly due to an uncountable\nset of candidate observations. A key contribution of our work here thus lies in\nexploiting the Lipschitz continuity of the reward functions to solve for a\nnonmyopic adaptive epsilon-optimal GPP (epsilon-GPP) policy. To plan in real\ntime, we further propose an asymptotically optimal, branch-and-bound anytime\nvariant of epsilon-GPP with performance guarantee. We empirically demonstrate\nthe effectiveness of our epsilon-GPP policy and its anytime variant in Bayesian\noptimization and an energy harvesting task.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2015 14:57:48 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Ling", "Chun Kai", ""], ["Low", "Kian Hsiang", ""], ["Jaillet", "Patrick", ""]]}, {"id": "1511.06891", "submitter": "Kian Hsiang Low", "authors": "Yehong Zhang, Trong Nghia Hoang, Kian Hsiang Low, Mohan Kankanhalli", "title": "Near-Optimal Active Learning of Multi-Output Gaussian Processes", "comments": "30th AAAI Conference on Artificial Intelligence (AAAI 2016), Extended\n  version with proofs, 13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of active learning of a multi-output\nGaussian process (MOGP) model representing multiple types of coexisting\ncorrelated environmental phenomena. In contrast to existing works, our active\nlearning problem involves selecting not just the most informative sampling\nlocations to be observed but also the types of measurements at each selected\nlocation for minimizing the predictive uncertainty (i.e., posterior joint\nentropy) of a target phenomenon of interest given a sampling budget.\nUnfortunately, such an entropy criterion scales poorly in the numbers of\ncandidate sampling locations and selected observations when optimized. To\nresolve this issue, we first exploit a structure common to sparse MOGP models\nfor deriving a novel active learning criterion. Then, we exploit a relaxed form\nof submodularity property of our new criterion for devising a polynomial-time\napproximation algorithm that guarantees a constant-factor approximation of that\nachieved by the optimal set of selected observations. Empirical evaluation on\nreal-world datasets shows that our proposed approach outperforms existing\nalgorithms for active learning of MOGP and single-output GP models.\n", "versions": [{"version": "v1", "created": "Sat, 21 Nov 2015 15:08:53 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2015 08:45:36 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Zhang", "Yehong", ""], ["Hoang", "Trong Nghia", ""], ["Low", "Kian Hsiang", ""], ["Kankanhalli", "Mohan", ""]]}, {"id": "1511.06995", "submitter": "Paolo Dragone", "authors": "Paolo Dragone", "title": "Non-Sentential Utterances in Dialogue: Experiments in Classification and\n  Interpretation", "comments": "Master thesis, 98 pages, ISBN: 9788887096057", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-sentential utterances (NSUs) are utterances that lack a complete\nsentential form but whose meaning can be inferred from the dialogue context,\nsuch as \"OK\", \"where?\", \"probably at his apartment\". The interpretation of\nnon-sentential utterances is an important problem in computational linguistics\nsince they constitute a frequent phenomena in dialogue and they are\nintrinsically context-dependent. The interpretation of NSUs is the task of\nretrieving their full semantic content from their form and the dialogue\ncontext. The first half of this thesis is devoted to the NSU classification\ntask. Our work builds upon Fern\\'andez et al. (2007) which present a series of\nmachine-learning experiments on the classification of NSUs. We extended their\napproach with a combination of new features and semi-supervised learning\ntechniques. The empirical results presented in this thesis show a modest but\nsignificant improvement over the state-of-the-art classification performance.\nThe consecutive, yet independent, problem is how to infer an appropriate\nsemantic representation of such NSUs on the basis of the dialogue context.\nFern\\'andez (2006) formalizes this task in terms of \"resolution rules\" built on\ntop of the Type Theory with Records (TTR). Our work is focused on the\nreimplementation of the resolution rules from Fern\\'andez (2006) with a\nprobabilistic account of the dialogue state. The probabilistic rules formalism\nLison (2014) is particularly suited for this task because, similarly to the\nframework developed by Ginzburg (2012) and Fern\\'andez (2006), it involves the\nspecification of update rules on the variables of the dialogue state to capture\nthe dynamics of the conversation. However, the probabilistic rules can also\nencode probabilistic knowledge, thereby providing a principled account of\nambiguities in the NSU resolution process.\n", "versions": [{"version": "v1", "created": "Sun, 22 Nov 2015 11:28:26 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Dragone", "Paolo", ""]]}, {"id": "1511.07209", "submitter": "Chao Wang Mr.", "authors": "Chao Wang, Somchaya Liemhetcharat, Kian Hsiang Low", "title": "Multi-Agent Continuous Transportation with Online Balanced Partitioning", "comments": "2 pages, published in the proceedings of the 15th AAMAS conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the concept of continuous transportation task to the context of\nmulti-agent systems. A continuous transportation task is one in which a\nmulti-agent team visits a number of fixed locations, picks up objects, and\ndelivers them to a final destination. The goal is to maximize the rate of\ntransportation while the objects are replenished over time. Examples of\nproblems that need continuous transportation are foraging, area sweeping, and\nfirst/last mile problem. Previous approaches typically neglect the interference\nand are highly dependent on communications among agents. Some also incorporate\nan additional reconnaissance agent to gather information. In this paper, we\npresent a hybrid of centralized and distributed approaches that minimize the\ninterference and communications in the multi-agent team without the need for a\nreconnaissance agent. We contribute two partitioning-transportation algorithms\ninspired by existing algorithms, and contribute one novel online\npartitioning-transportation algorithm with information gathering in the\nmulti-agent team. Our algorithms have been implemented and tested extensively\nin the simulation. The results presented in this paper demonstrate the\neffectiveness of our algorithms that outperform the existing algorithms, even\nwithout any communications between the agents and without the presence of a\nreconnaissance agent.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 13:04:47 GMT"}, {"version": "v2", "created": "Thu, 28 Jul 2016 09:11:40 GMT"}], "update_date": "2016-07-29", "authors_parsed": [["Wang", "Chao", ""], ["Liemhetcharat", "Somchaya", ""], ["Low", "Kian Hsiang", ""]]}, {"id": "1511.07211", "submitter": "Adish Singla", "authors": "Adish Singla, Sebastian Tschiatschek, Andreas Krause", "title": "Noisy Submodular Maximization via Adaptive Sampling with Applications to\n  Crowdsourced Image Collection Summarization", "comments": "Extended version of AAAI'16 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of maximizing an unknown submodular function that can\nonly be accessed via noisy evaluations. Our work is motivated by the task of\nsummarizing content, e.g., image collections, by leveraging users' feedback in\nform of clicks or ratings. For summarization tasks with the goal of maximizing\ncoverage and diversity, submodular set functions are a natural choice. When the\nunderlying submodular function is unknown, users' feedback can provide noisy\nevaluations of the function that we seek to maximize. We provide a generic\nalgorithm -- \\submM{} -- for maximizing an unknown submodular function under\ncardinality constraints. This algorithm makes use of a novel exploration module\n-- \\blbox{} -- that proposes good elements based on adaptively sampling noisy\nfunction evaluations. \\blbox{} is able to accommodate different kinds of\nobservation models such as value queries and pairwise comparisons. We provide\nPAC-style guarantees on the quality and sampling cost of the solution obtained\nby \\submM{}. We demonstrate the effectiveness of our approach in an\ninteractive, crowdsourced image collection summarization application.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 13:19:05 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2015 09:49:35 GMT"}], "update_date": "2015-12-02", "authors_parsed": [["Singla", "Adish", ""], ["Tschiatschek", "Sebastian", ""], ["Krause", "Andreas", ""]]}, {"id": "1511.07275", "submitter": "Wojciech Zaremba", "authors": "Wojciech Zaremba, Tomas Mikolov, Armand Joulin, Rob Fergus", "title": "Learning Simple Algorithms from Examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for learning simple algorithms such as copying,\nmulti-digit addition and single digit multiplication directly from examples.\nOur framework consists of a set of interfaces, accessed by a controller.\nTypical interfaces are 1-D tapes or 2-D grids that hold the input and output\ndata. For the controller, we explore a range of neural network-based models\nwhich vary in their ability to abstract the underlying algorithm from training\ninstances and generalize to test examples with many thousands of digits. The\ncontroller is trained using $Q$-learning with several enhancements and we show\nthat the bottleneck is in the capabilities of the controller rather than in the\nsearch incurred by $Q$-learning.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 15:31:54 GMT"}, {"version": "v2", "created": "Tue, 24 Nov 2015 03:28:35 GMT"}], "update_date": "2015-11-25", "authors_parsed": [["Zaremba", "Wojciech", ""], ["Mikolov", "Tomas", ""], ["Joulin", "Armand", ""], ["Fergus", "Rob", ""]]}, {"id": "1511.07361", "submitter": "Guolong Su", "authors": "Guolong Su, Dennis Wei, Kush R. Varshney, Dmitry M. Malioutov", "title": "Interpretable Two-level Boolean Rule Learning for Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes algorithms for learning two-level Boolean rules in\nConjunctive Normal Form (CNF, i.e. AND-of-ORs) or Disjunctive Normal Form (DNF,\ni.e. OR-of-ANDs) as a type of human-interpretable classification model, aiming\nfor a favorable trade-off between the classification accuracy and the\nsimplicity of the rule. Two formulations are proposed. The first is an integer\nprogram whose objective function is a combination of the total number of errors\nand the total number of features used in the rule. We generalize a previously\nproposed linear programming (LP) relaxation from one-level to two-level rules.\nThe second formulation replaces the 0-1 classification error with the Hamming\ndistance from the current two-level rule to the closest rule that correctly\nclassifies a sample. Based on this second formulation, block coordinate descent\nand alternating minimization algorithms are developed. Experiments show that\nthe two-level rules can yield noticeably better performance than one-level\nrules due to their dramatically larger modeling capacity, and the two\nalgorithms based on the Hamming distance formulation are generally superior to\nthe other two-level rule learning methods in our comparison. A proposed\napproach to binarize any fractional values in the optimal solutions of LP\nrelaxations is also shown to be effective.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 18:52:21 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Su", "Guolong", ""], ["Wei", "Dennis", ""], ["Varshney", "Kush R.", ""], ["Malioutov", "Dmitry M.", ""]]}, {"id": "1511.07373", "submitter": "Stefan Arnborg", "authors": "Stefan Arnborg and Gunnar Sj\\\"odin", "title": "What is the plausibility of probability?(revised 2003, 2015)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present and examine a result related to uncertainty reasoning, namely that\na certain plausibility space of Cox's type can be uniquely embedded in a\nminimal ordered field. This, although a purely mathematical result, can be\nclaimed to imply that every rational method to reason with uncertainty must be\nbased on sets of extended probability distributions, where extended probability\nis standard probability extended with infinitesimals.\n  This claim must be supported by some argumentation of non-mathematical type,\nhowever, since pure mathematics does not tell us anything about the world. We\npropose one such argumentation, and relate it to results from the literature of\nuncertainty and statistics.\n  In an added retrospective section we discuss some developments in the area\nregarding countable additivity, partially ordered domains and robustness, and\nphilosophical stances on the Cox/Jaynes approach since 2003. We also show that\nthe most general partially ordered plausibility calculus embeddable in a ring\ncan be represented as a set of extended probability distributions or, in\nalgebraic terms, is a subdirect sum of ordered fields. In other words, the\nrobust Bayesian approach is universal. This result is exemplified by relating\nDempster-Shafer's evidence theory to robust Bayesian analysis.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 19:24:17 GMT"}], "update_date": "2015-11-24", "authors_parsed": [["Arnborg", "Stefan", ""], ["Sj\u00f6din", "Gunnar", ""]]}, {"id": "1511.07401", "submitter": "Sainbayar Sukhbaatar", "authors": "Sainbayar Sukhbaatar, Arthur Szlam, Gabriel Synnaeve, Soumith\n  Chintala, Rob Fergus", "title": "MazeBase: A Sandbox for Learning from Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces MazeBase: an environment for simple 2D games, designed\nas a sandbox for machine learning approaches to reasoning and planning. Within\nit, we create 10 simple games embodying a range of algorithmic tasks (e.g.\nif-then statements or set negation). A variety of neural models (fully\nconnected, convolutional network, memory network) are deployed via\nreinforcement learning on these games, with and without a procedurally\ngenerated curriculum. Despite the tasks' simplicity, the performance of the\nmodels is far from optimal, suggesting directions for future development. We\nalso demonstrate the versatility of MazeBase by using it to emulate small\ncombat scenarios from StarCraft. Models trained on the MazeBase version can be\ndirectly applied to StarCraft, where they consistently beat the in-game AI.\n", "versions": [{"version": "v1", "created": "Mon, 23 Nov 2015 20:23:53 GMT"}, {"version": "v2", "created": "Thu, 7 Jan 2016 18:41:14 GMT"}], "update_date": "2016-01-08", "authors_parsed": [["Sukhbaatar", "Sainbayar", ""], ["Szlam", "Arthur", ""], ["Synnaeve", "Gabriel", ""], ["Chintala", "Soumith", ""], ["Fergus", "Rob", ""]]}, {"id": "1511.07569", "submitter": "Jiliang Tang", "authors": "Jiliang Tang, Yi Chang, Charu Aggarwal, Huan Liu", "title": "A Survey of Signed Network Mining in Social Media", "comments": "37 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world relations can be represented by signed networks with positive\nand negative links, as a result of which signed network analysis has attracted\nincreasing attention from multiple disciplines. With the increasing prevalence\nof social media networks, signed network analysis has evolved from developing\nand measuring theories to mining tasks. In this article, we present a review of\nmining signed networks in the context of social media and discuss some\npromising research directions and new frontiers. We begin by giving basic\nconcepts and unique properties and principles of signed networks. Then we\nclassify and review tasks of signed network mining with representative\nalgorithms. We also delineate some tasks that have not been extensively studied\nwith formal definitions and also propose research directions to expand the\nfield of signed network mining.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 05:05:34 GMT"}, {"version": "v2", "created": "Fri, 3 Jun 2016 20:54:35 GMT"}, {"version": "v3", "created": "Mon, 20 Jun 2016 20:54:55 GMT"}], "update_date": "2016-06-22", "authors_parsed": [["Tang", "Jiliang", ""], ["Chang", "Yi", ""], ["Aggarwal", "Charu", ""], ["Liu", "Huan", ""]]}, {"id": "1511.07663", "submitter": "Kuldeep Meel", "authors": "Supratik Chakraborty, Kuldeep S. Meel, Rakesh Mistry, Moshe Y. Vardi", "title": "Approximate Probabilistic Inference via Word-Level Counting", "comments": "Full version of AAAI 2016 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hashing-based model counting has emerged as a promising approach for\nlarge-scale probabilistic inference on graphical models. A key component of\nthese techniques is the use of xor-based 2-universal hash functions that\noperate over Boolean domains. Many counting problems arising in probabilistic\ninference are, however, naturally encoded over finite discrete domains.\nTechniques based on bit-level (or Boolean) hash functions require these\nproblems to be propositionalized, making it impossible to leverage the\nremarkable progress made in SMT (Satisfiability Modulo Theory) solvers that can\nreason directly over words (or bit-vectors). In this work, we present the first\napproximate model counter that uses word-level hashing functions, and can\ndirectly leverage the power of sophisticated SMT solvers. Empirical evaluation\nover an extensive suite of benchmarks demonstrates the promise of the approach.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 11:52:28 GMT"}, {"version": "v2", "created": "Wed, 2 Dec 2015 15:36:35 GMT"}, {"version": "v3", "created": "Tue, 9 Feb 2016 05:36:28 GMT"}], "update_date": "2016-02-10", "authors_parsed": [["Chakraborty", "Supratik", ""], ["Meel", "Kuldeep S.", ""], ["Mistry", "Rakesh", ""], ["Vardi", "Moshe Y.", ""]]}, {"id": "1511.07710", "submitter": "Varun Nagaraja", "authors": "Varun K. Nagaraja, Vlad I. Morariu, Larry S. Davis", "title": "Searching for Objects using Structure in Indoor Scenes", "comments": "Appeared in British Machine Vision Conference (BMVC) 2015", "journal-ref": null, "doi": "10.5244/C.29.53", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To identify the location of objects of a particular class, a passive computer\nvision system generally processes all the regions in an image to finally output\nfew regions. However, we can use structure in the scene to search for objects\nwithout processing the entire image. We propose a search technique that\nsequentially processes image regions such that the regions that are more likely\nto correspond to the query class object are explored earlier. We frame the\nproblem as a Markov decision process and use an imitation learning algorithm to\nlearn a search strategy. Since structure in the scene is essential for search,\nwe work with indoor scene images as they contain both unary scene context\ninformation and object-object context in the scene. We perform experiments on\nthe NYU-depth v2 dataset and show that the unary scene context features alone\ncan achieve a significantly high average precision while processing only\n20-25\\% of the regions for classes like bed and sofa. By considering\nobject-object context along with the scene context features, the performance is\nfurther improved for classes like counter, lamp, pillow and sofa.\n", "versions": [{"version": "v1", "created": "Tue, 24 Nov 2015 14:05:28 GMT"}], "update_date": "2016-08-09", "authors_parsed": [["Nagaraja", "Varun K.", ""], ["Morariu", "Vlad I.", ""], ["Davis", "Larry S.", ""]]}, {"id": "1511.07972", "submitter": "Volker Tresp", "authors": "Volker Tresp and Crist\\'obal Esteban and Yinchong Yang and Stephan\n  Baier and Denis Krompa{\\ss}", "title": "Learning with Memory Embeddings", "comments": "29 pages, NIPS 2015 Workshop on Nonparametric Methods for Large Scale\n  Representation Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embedding learning, a.k.a. representation learning, has been shown to be able\nto model large-scale semantic knowledge graphs. A key concept is a mapping of\nthe knowledge graph to a tensor representation whose entries are predicted by\nmodels using latent representations of generalized entities. Latent variable\nmodels are well suited to deal with the high dimensionality and sparsity of\ntypical knowledge graphs. In recent publications the embedding models were\nextended to also consider time evolutions, time patterns and subsymbolic\nrepresentations. In this paper we map embedding models, which were developed\npurely as solutions to technical problems for modelling temporal knowledge\ngraphs, to various cognitive memory functions, in particular to semantic and\nconcept memory, episodic memory, sensory memory, short-term memory, and working\nmemory. We discuss learning, query answering, the path from sensory input to\nsemantic decoding, and the relationship between episodic memory and semantic\nmemory. We introduce a number of hypotheses on human memory that can be derived\nfrom the developed mathematical models.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2015 07:06:09 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2015 05:53:38 GMT"}, {"version": "v3", "created": "Wed, 16 Dec 2015 23:38:03 GMT"}, {"version": "v4", "created": "Mon, 21 Dec 2015 22:35:39 GMT"}, {"version": "v5", "created": "Mon, 25 Jan 2016 20:02:39 GMT"}, {"version": "v6", "created": "Wed, 13 Apr 2016 17:23:42 GMT"}, {"version": "v7", "created": "Thu, 21 Apr 2016 04:40:58 GMT"}, {"version": "v8", "created": "Thu, 5 May 2016 14:57:41 GMT"}, {"version": "v9", "created": "Sat, 7 May 2016 09:06:15 GMT"}], "update_date": "2016-05-10", "authors_parsed": [["Tresp", "Volker", ""], ["Esteban", "Crist\u00f3bal", ""], ["Yang", "Yinchong", ""], ["Baier", "Stephan", ""], ["Krompa\u00df", "Denis", ""]]}, {"id": "1511.08099", "submitter": "Heriberto Cuay\\'ahuitl", "authors": "Heriberto Cuay\\'ahuitl, Simon Keizer, Oliver Lemon", "title": "Strategic Dialogue Management via Deep Reinforcement Learning", "comments": "NIPS'15 Workshop on Deep Reinforcement Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificially intelligent agents equipped with strategic skills that can\nnegotiate during their interactions with other natural or artificial agents are\nstill underdeveloped. This paper describes a successful application of Deep\nReinforcement Learning (DRL) for training intelligent agents with strategic\nconversational skills, in a situated dialogue setting. Previous studies have\nmodelled the behaviour of strategic agents using supervised learning and\ntraditional reinforcement learning techniques, the latter using tabular\nrepresentations or learning with linear function approximation. In this study,\nwe apply DRL with a high-dimensional state space to the strategic board game of\nSettlers of Catan---where players can offer resources in exchange for others\nand they can also reply to offers made by other players. Our experimental\nresults report that the DRL-based learnt policies significantly outperformed\nseveral baselines including random, rule-based, and supervised-based\nbehaviours. The DRL-based policy has a 53% win rate versus 3 automated players\n(`bots'), whereas a supervised player trained on a dialogue corpus in this\nsetting achieved only 27%, versus the same 3 bots. This result supports the\nclaim that DRL is a promising framework for training dialogue systems, and\nstrategic agents with negotiation abilities.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2015 15:48:59 GMT"}], "update_date": "2015-11-28", "authors_parsed": [["Cuay\u00e1huitl", "Heriberto", ""], ["Keizer", "Simon", ""], ["Lemon", "Oliver", ""]]}, {"id": "1511.08130", "submitter": "Tomas Mikolov", "authors": "Tomas Mikolov, Armand Joulin, Marco Baroni", "title": "A Roadmap towards Machine Intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of intelligent machines is one of the biggest unsolved\nchallenges in computer science. In this paper, we propose some fundamental\nproperties these machines should have, focusing in particular on communication\nand learning. We discuss a simple environment that could be used to\nincrementally teach a machine the basics of natural-language-based\ncommunication, as a prerequisite to more complex interaction with human users.\nWe also present some conjectures on the sort of algorithms the machine should\nsupport in order to profitably learn from the environment.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2015 17:32:18 GMT"}, {"version": "v2", "created": "Fri, 26 Feb 2016 20:03:43 GMT"}], "update_date": "2016-02-29", "authors_parsed": [["Mikolov", "Tomas", ""], ["Joulin", "Armand", ""], ["Baroni", "Marco", ""]]}, {"id": "1511.08136", "submitter": "Yisen Wang", "authors": "Yisen Wang, Chaobing Song, Shu-Tao Xia", "title": "Unifying Decision Trees Split Criteria Using Tsallis Entropy", "comments": "6 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The construction of efficient and effective decision trees remains a key\ntopic in machine learning because of their simplicity and flexibility. A lot of\nheuristic algorithms have been proposed to construct near-optimal decision\ntrees. ID3, C4.5 and CART are classical decision tree algorithms and the split\ncriteria they used are Shannon entropy, Gain Ratio and Gini index respectively.\nAll the split criteria seem to be independent, actually, they can be unified in\na Tsallis entropy framework. Tsallis entropy is a generalization of Shannon\nentropy and provides a new approach to enhance decision trees' performance with\nan adjustable parameter $q$. In this paper, a Tsallis Entropy Criterion (TEC)\nalgorithm is proposed to unify Shannon entropy, Gain Ratio and Gini index,\nwhich generalizes the split criteria of decision trees. More importantly, we\nreveal the relations between Tsallis entropy with different $q$ and other split\ncriteria. Experimental results on UCI data sets indicate that the TEC algorithm\nachieves statistically significant improvement over the classical algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2015 17:49:55 GMT"}, {"version": "v2", "created": "Thu, 26 Nov 2015 02:29:07 GMT"}, {"version": "v3", "created": "Sat, 5 Dec 2015 08:08:22 GMT"}, {"version": "v4", "created": "Mon, 18 Jan 2016 07:53:55 GMT"}, {"version": "v5", "created": "Tue, 23 Aug 2016 01:02:14 GMT"}], "update_date": "2016-08-24", "authors_parsed": [["Wang", "Yisen", ""], ["Song", "Chaobing", ""], ["Xia", "Shu-Tao", ""]]}, {"id": "1511.08158", "submitter": "Yu Zhang", "authors": "Yu Zhang, Sarath Sreedharan, Anagha Kulkarni, Tathagata Chakraborti,\n  Hankz Hankui Zhuo and Subbarao Kambhampati", "title": "Plan Explicability and Predictability for Robot Task Planning", "comments": "Added physical robot evaluations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent robots and machines are becoming pervasive in human populated\nenvironments. A desirable capability of these agents is to respond to\ngoal-oriented commands by autonomously constructing task plans. However, such\nautonomy can add significant cognitive load and potentially introduce safety\nrisks to humans when agents behave unexpectedly. Hence, for such agents to be\nhelpful, one important requirement is for them to synthesize plans that can be\neasily understood by humans. While there exists previous work that studied\nsocially acceptable robots that interact with humans in \"natural ways\", and\nwork that investigated legible motion planning, there lacks a general solution\nfor high level task planning. To address this issue, we introduce the notions\nof plan {\\it explicability} and {\\it predictability}. To compute these\nmeasures, first, we postulate that humans understand agent plans by associating\nabstract tasks with agent actions, which can be considered as a labeling\nprocess. We learn the labeling scheme of humans for agent plans from training\nexamples using conditional random fields (CRFs). Then, we use the learned model\nto label a new plan to compute its explicability and predictability. These\nmeasures can be used by agents to proactively choose or directly synthesize\nplans that are more explicable and predictable to humans. We provide\nevaluations on a synthetic domain and with human subjects using physical robots\nto show the effectiveness of our approach\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2015 19:05:29 GMT"}, {"version": "v2", "created": "Tue, 12 Apr 2016 21:36:46 GMT"}], "update_date": "2016-04-14", "authors_parsed": [["Zhang", "Yu", ""], ["Sreedharan", "Sarath", ""], ["Kulkarni", "Anagha", ""], ["Chakraborti", "Tathagata", ""], ["Zhuo", "Hankz Hankui", ""], ["Kambhampati", "Subbarao", ""]]}, {"id": "1511.08205", "submitter": "Michael Codish", "authors": "Avraham Itzhakov and Michael Codish", "title": "Breaking Symmetries in Graph Search with Canonizing Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many complex combinatorial problems which involve searching for an\nundirected graph satisfying given constraints. Such problems are often highly\nchallenging because of the large number of isomorphic representations of their\nsolutions. This paper introduces effective and compact, complete symmetry\nbreaking constraints for small graph search. Enumerating with these symmetry\nbreaks generates all and only non-isomorphic solutions. For small search\nproblems, with up to $10$ vertices, we compute instance independent symmetry\nbreaking constraints. For small search problems with a larger number of\nvertices we demonstrate the computation of instance dependent constraints which\nare complete. We illustrate the application of complete symmetry breaking\nconstraints to extend two known sequences from the OEIS related to graph\nenumeration. We also demonstrate the application of a generalization of our\napproach to fully-interchangeable matrix search problems.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2015 06:26:33 GMT"}, {"version": "v2", "created": "Thu, 4 Feb 2016 03:05:24 GMT"}], "update_date": "2016-02-05", "authors_parsed": [["Itzhakov", "Avraham", ""], ["Codish", "Michael", ""]]}, {"id": "1511.08250", "submitter": "Bernardino Romera-Paredes", "authors": "Bernardino Romera-Paredes, Philip H. S. Torr", "title": "Recurrent Instance Segmentation", "comments": "14 pages (main paper). 24 pages including references and appendix", "journal-ref": "ECCV 2016. 14th European Conference on Computer Vision", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instance segmentation is the problem of detecting and delineating each\ndistinct object of interest appearing in an image. Current instance\nsegmentation approaches consist of ensembles of modules that are trained\nindependently of each other, thus missing opportunities for joint learning.\nHere we propose a new instance segmentation paradigm consisting in an\nend-to-end method that learns how to segment instances sequentially. The model\nis based on a recurrent neural network that sequentially finds objects and\ntheir segmentations one at a time. This net is provided with a spatial memory\nthat keeps track of what pixels have been explained and allows occlusion\nhandling. In order to train the model we designed a principled loss function\nthat accurately represents the properties of the instance segmentation problem.\nIn the experiments carried out, we found that our method outperforms recent\napproaches on multiple person segmentation, and all state of the art approaches\non the Plant Phenotyping dataset for leaf counting.\n", "versions": [{"version": "v1", "created": "Wed, 25 Nov 2015 23:28:14 GMT"}, {"version": "v2", "created": "Tue, 5 Apr 2016 22:45:04 GMT"}, {"version": "v3", "created": "Mon, 24 Oct 2016 23:57:19 GMT"}], "update_date": "2016-10-26", "authors_parsed": [["Romera-Paredes", "Bernardino", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1511.08277", "submitter": "Shengxian Wan", "authors": "Shengxian Wan, Yanyan Lan, Jiafeng Guo, Jun Xu, Liang Pang, and Xueqi\n  Cheng", "title": "A Deep Architecture for Semantic Matching with Multiple Positional\n  Sentence Representations", "comments": "Accepted by AAAI-2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matching natural language sentences is central for many applications such as\ninformation retrieval and question answering. Existing deep models rely on a\nsingle sentence representation or multiple granularity representations for\nmatching. However, such methods cannot well capture the contextualized local\ninformation in the matching process. To tackle this problem, we present a new\ndeep architecture to match two sentences with multiple positional sentence\nrepresentations. Specifically, each positional sentence representation is a\nsentence representation at this position, generated by a bidirectional long\nshort term memory (Bi-LSTM). The matching score is finally produced by\naggregating interactions between these different positional sentence\nrepresentations, through $k$-Max pooling and a multi-layer perceptron. Our\nmodel has several advantages: (1) By using Bi-LSTM, rich context of the whole\nsentence is leveraged to capture the contextualized local information in each\npositional sentence representation; (2) By matching with multiple positional\nsentence representations, it is flexible to aggregate different important\ncontextualized local information in a sentence to support the matching; (3)\nExperiments on different tasks such as question answering and sentence\ncompletion demonstrate the superiority of our model.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 02:57:54 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Wan", "Shengxian", ""], ["Lan", "Yanyan", ""], ["Guo", "Jiafeng", ""], ["Xu", "Jun", ""], ["Pang", "Liang", ""], ["Cheng", "Xueqi", ""]]}, {"id": "1511.08280", "submitter": "Toby Walsh", "authors": "Haris Aziz and Thomas Kalinowski and Toby Walsh and Lirong Xia", "title": "Welfare of Sequential Allocation Mechanisms for Indivisible Goods", "comments": null, "journal-ref": "Frontiers in Artificial Intelligence and Applications, 787-794,\n  2016", "doi": "10.3233/978-1-61499-672-9-787", "report-no": null, "categories": "cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential allocation is a simple and attractive mechanism for the allocation\nof indivisible goods. Agents take turns, according to a policy, to pick items.\nSequential allocation is guaranteed to return an allocation which is efficient\nbut may not have an optimal social welfare. We consider therefore the relation\nbetween welfare and efficiency. We study the (computational) questions of what\nwelfare is possible or necessary depending on the choice of policy. We also\nconsider a novel control problem in which the chair chooses a policy to improve\nsocial welfare.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 03:31:29 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Aziz", "Haris", ""], ["Kalinowski", "Thomas", ""], ["Walsh", "Toby", ""], ["Xia", "Lirong", ""]]}, {"id": "1511.08350", "submitter": "Amina Kemmar", "authors": "Amina Kemmar and Samir Loudni and Yahia Lebbah and Patrice Boizumault\n  and Thierry Charnois", "title": "A global Constraint for mining Sequential Patterns with GAP constraint", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential pattern mining (SPM) under gap constraint is a challenging task.\nMany efficient specialized methods have been developed but they are all\nsuffering from a lack of genericity. The Constraint Programming (CP) approaches\nare not so effective because of the size of their encodings. In[7], we have\nproposed the global constraint Prefix-Projection for SPM which remedies to this\ndrawback. However, this global constraint cannot be directly extended to\nsupport gap constraint. In this paper, we propose the global constraint GAP-SEQ\nenabling to handle SPM with or without gap constraint. GAP-SEQ relies on the\nprinciple of right pattern extensions. Experiments show that our approach\nclearly outperforms both CP approaches and the state-of-the-art cSpade method\non large datasets.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 10:45:34 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Kemmar", "Amina", ""], ["Loudni", "Samir", ""], ["Lebbah", "Yahia", ""], ["Boizumault", "Patrice", ""], ["Charnois", "Thierry", ""]]}, {"id": "1511.08412", "submitter": "Elena Botoeva", "authors": "Elena Botoeva, Diego Calvanese, Valerio Santarelli, Domenico Fabio\n  Savo, Alessandro Solimando, Guohui Xiao", "title": "Beyond OWL 2 QL in OBDA: Rewritings and Approximations (Extended\n  Version)", "comments": "The extended version of the AAAI 2016 paper \"Beyond OWL 2 QL in OBDA:\n  Rewritings and Approximations\" by Elena Botoeva, Diego Calvanese, Valerio\n  Santarelli, Domenico Fabio Savo, Alessandro Solimando,and Guohui Xiao", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ontology-based data access (OBDA) is a novel paradigm facilitating access to\nrelational data, realized by linking data sources to an ontology by means of\ndeclarative mappings. DL-Lite_R, which is the logic underpinning the W3C\nontology language OWL 2 QL and the current language of choice for OBDA, has\nbeen designed with the goal of delegating query answering to the underlying\ndatabase engine, and thus is restricted in expressive power. E.g., it does not\nallow one to express disjunctive information, and any form of recursion on the\ndata. The aim of this paper is to overcome these limitations of DL-Lite_R, and\nextend OBDA to more expressive ontology languages, while still leveraging the\nunderlying relational technology for query answering. We achieve this by\nrelying on two well-known mechanisms, namely conservative rewriting and\napproximation, but significantly extend their practical impact by bringing into\nthe picture the mapping, an essential component of OBDA. Specifically, we\ndevelop techniques to rewrite OBDA specifications with an expressive ontology\nto \"equivalent\" ones with a DL-Lite_R ontology, if possible, and to approximate\nthem otherwise. We do so by exploiting the high expressive power of the mapping\nlayer to capture part of the domain semantics of rich ontology languages. We\nhave implemented our techniques in the prototype system OntoProx, making use of\nthe state-of-the-art OBDA system Ontop and the query answering system Clipper,\nand we have shown their feasibility and effectiveness with experiments on\nsynthetic and real-world data.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 15:12:20 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2015 18:26:09 GMT"}], "update_date": "2015-12-02", "authors_parsed": [["Botoeva", "Elena", ""], ["Calvanese", "Diego", ""], ["Santarelli", "Valerio", ""], ["Savo", "Domenico Fabio", ""], ["Solimando", "Alessandro", ""], ["Xiao", "Guohui", ""]]}, {"id": "1511.08456", "submitter": "Martin Chmel\\'ik", "authors": "Krishnendu Chatterjee and Martin Chmelik and Jessica Davies", "title": "A Symbolic SAT-based Algorithm for Almost-sure Reachability with Small\n  Strategies in POMDPs", "comments": "Full version of \"A Symbolic SAT-based Algorithm for Almost-sure\n  Reachability with Small Strategies in POMDPs\" AAAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  POMDPs are standard models for probabilistic planning problems, where an\nagent interacts with an uncertain environment. We study the problem of\nalmost-sure reachability, where given a set of target states, the question is\nto decide whether there is a policy to ensure that the target set is reached\nwith probability 1 (almost-surely). While in general the problem is\nEXPTIME-complete, in many practical cases policies with a small amount of\nmemory suffice. Moreover, the existing solution to the problem is explicit,\nwhich first requires to construct explicitly an exponential reduction to a\nbelief-support MDP. In this work, we first study the existence of\nobservation-stationary strategies, which is NP-complete, and then small-memory\nstrategies. We present a symbolic algorithm by an efficient encoding to SAT and\nusing a SAT solver for the problem. We report experimental results\ndemonstrating the scalability of our symbolic (SAT-based) approach.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 17:33:05 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Chatterjee", "Krishnendu", ""], ["Chmelik", "Martin", ""], ["Davies", "Jessica", ""]]}, {"id": "1511.08488", "submitter": "Martin Plajner", "authors": "Martin Plajner, Ji\\v{r}\\'i Vomlel", "title": "Bayesian Network Models for Adaptive Testing", "comments": "12th Annual Bayesian Modelling Applications Workshop, Amsterdam,\n  Netherlands, (July 2015). 10 pages", "journal-ref": "Proc. of the Eighth International Conference on Probabilistic\n  Graphical Models (JMLR), 2016, pages 403-414", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computerized adaptive testing (CAT) is an interesting and promising approach\nto testing human abilities. In our research we use Bayesian networks to create\na model of tested humans. We collected data from paper tests performed with\ngrammar school students. In this article we first provide the summary of data\nused for our experiments. We propose several different Bayesian networks, which\nwe tested and compared by cross-validation. Interesting results were obtained\nand are discussed in the paper. The analysis has brought a clearer view on the\nmodel selection problem. Future research is outlined in the concluding part of\nthe paper.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 19:45:03 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Plajner", "Martin", ""], ["Vomlel", "Ji\u0159\u00ed", ""]]}, {"id": "1511.08495", "submitter": "Yangchen Pan", "authors": "Clement Gehring, Yangchen Pan, Martha White", "title": "Incremental Truncated LSTD", "comments": "Accepted to IJCAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Balancing between computational efficiency and sample efficiency is an\nimportant goal in reinforcement learning. Temporal difference (TD) learning\nalgorithms stochastically update the value function, with a linear time\ncomplexity in the number of features, whereas least-squares temporal difference\n(LSTD) algorithms are sample efficient but can be quadratic in the number of\nfeatures. In this work, we develop an efficient incremental low-rank\nLSTD({\\lambda}) algorithm that progresses towards the goal of better balancing\ncomputation and sample efficiency. The algorithm reduces the computation and\nstorage complexity to the number of features times the chosen rank parameter\nwhile summarizing past samples efficiently to nearly obtain the sample\ncomplexity of LSTD. We derive a simulation bound on the solution given by\ntruncated low-rank approximation, illustrating a bias- variance trade-off\ndependent on the choice of rank. We demonstrate that the algorithm effectively\nbalances computational complexity and sample efficiency for policy evaluation\nin a benchmark task and a high-dimensional energy allocation domain.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 20:37:09 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2016 18:40:20 GMT"}, {"version": "v3", "created": "Fri, 18 Nov 2016 05:58:06 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Gehring", "Clement", ""], ["Pan", "Yangchen", ""], ["White", "Martha", ""]]}, {"id": "1511.08512", "submitter": "Antonio Lieto", "authors": "Antonio Lieto", "title": "Some Epistemological Problems with the Knowledge Level in Cognitive\n  Architectures", "comments": "5 pages in Proceedings of AISC 2015, 12th Italian Conference on\n  Cognitive Science, Genoa, 10-12 December 2015, Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article addresses an open problem in the area of cognitive systems and\narchitectures: namely the problem of handling (in terms of processing and\nreasoning capabilities) complex knowledge structures that can be at least\nplausibly comparable, both in terms of size and of typology of the encoded\ninformation, to the knowledge that humans process daily for executing everyday\nactivities. Handling a huge amount of knowledge, and selectively retrieve it\nac- cording to the needs emerging in different situational scenarios, is an\nimportant aspect of human intelligence. For this task, in fact, humans adopt a\nwide range of heuristics (Gigerenzer and Todd) due to their bounded rationality\n(Simon, 1957). In this perspective, one of the re- quirements that should be\nconsidered for the design, the realization and the evaluation of intelligent\ncognitively inspired systems should be represented by their ability of\nheuristically identify and retrieve, from the general knowledge stored in their\nartificial Long Term Memory (LTM), that one which is synthetically and\ncontextually relevant. This require- ment, however, is often neglected.\nCurrently, artificial cognitive systems and architectures are not able, de\nfacto, to deal with complex knowledge structures that can be even slightly\ncomparable to the knowledge heuris- tically managed by humans. In this paper I\nwill argue that this is not only a technological problem but also an\nepistemological one and I will briefly sketch a proposal for a possible\nsolution.\n", "versions": [{"version": "v1", "created": "Thu, 26 Nov 2015 21:31:20 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Lieto", "Antonio", ""]]}, {"id": "1511.08574", "submitter": "Dimitri Klimenko", "authors": "Dimitri Klimenko, Hanna Kurniawati, and Marcus Gallagher", "title": "A Stochastic Process Model of Classical Search", "comments": "Submitted to ICAPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among classical search algorithms with the same heuristic information, with\nsufficient memory A* is essentially as fast as possible in finding a proven\noptimal solution. However, in many situations optimal solutions are simply\ninfeasible, and thus search algorithms that trade solution quality for speed\nare desirable. In this paper, we formalize the process of classical search as a\nmetalevel decision problem, the Abstract Search MDP. For any given optimization\ncriterion, this establishes a well-defined notion of the best possible\nbehaviour for a search algorithm and offers a theoretical approach to the\ndesign of algorithms for that criterion. We proceed to approximately solve a\nversion of the Abstract Search MDP for anytime algorithms and thus derive a\nnovel search algorithm, Search by Maximizing the Incremental Rate of\nImprovement (SMIRI). SMIRI is shown to outperform current state-of-the-art\nanytime search algorithms on a parametrized stochastic tree model for most of\nthe tested parameter values.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2015 07:34:41 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Klimenko", "Dimitri", ""], ["Kurniawati", "Hanna", ""], ["Gallagher", "Marcus", ""]]}, {"id": "1511.08589", "submitter": "Raj Kumar Maity", "authors": "Chandrashekar Lakshmi Narayanan, Raj Kumar Maity and Shalabh Bhatnagar", "title": "Shaping Proto-Value Functions via Rewards", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper, we combine task-dependent reward shaping and task-independent\nproto-value functions to obtain reward dependent proto-value functions (RPVFs).\nIn constructing the RPVFs we are making use of the immediate rewards which are\navailable during the sampling phase but are not used in the PVF construction.\nWe show via experiments that learning with an RPVF based representation is\nbetter than learning with just reward shaping or PVFs. In particular, when the\nstate space is symmetrical and the rewards are asymmetrical, the RPVF capture\nthe asymmetry better than the PVFs.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2015 09:13:04 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Narayanan", "Chandrashekar Lakshmi", ""], ["Maity", "Raj Kumar", ""], ["Bhatnagar", "Shalabh", ""]]}, {"id": "1511.08724", "submitter": "Tom Ameloot", "authors": "Tom J. Ameloot and Jan Van den Bussche", "title": "On the convergence of cycle detection for navigational reinforcement\n  learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a reinforcement learning framework where agents have to navigate\nfrom start states to goal states. We prove convergence of a cycle-detection\nlearning algorithm on a class of tasks that we call reducible. Reducible tasks\nhave an acyclic solution. We also syntactically characterize the form of the\nfinal policy. This characterization can be used to precisely detect the\nconvergence point in a simulation. Our result demonstrates that even simple\nalgorithms can be successful in learning a large class of nontrivial tasks. In\naddition, our framework is elementary in the sense that we only use basic\nconcepts to formally prove convergence.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2015 16:16:55 GMT"}, {"version": "v2", "created": "Tue, 5 Jan 2016 14:08:35 GMT"}], "update_date": "2016-01-06", "authors_parsed": [["Ameloot", "Tom J.", ""], ["Bussche", "Jan Van den", ""]]}, {"id": "1511.08779", "submitter": "Jaan Aru", "authors": "Ardi Tampuu, Tambet Matiisen, Dorian Kodelja, Ilya Kuzovkin, Kristjan\n  Korjus, Juhan Aru, Jaan Aru and Raul Vicente", "title": "Multiagent Cooperation and Competition with Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiagent systems appear in most social, economical, and political\nsituations. In the present work we extend the Deep Q-Learning Network\narchitecture proposed by Google DeepMind to multiagent environments and\ninvestigate how two agents controlled by independent Deep Q-Networks interact\nin the classic videogame Pong. By manipulating the classical rewarding scheme\nof Pong we demonstrate how competitive and collaborative behaviors emerge.\nCompetitive agents learn to play and score efficiently. Agents trained under\ncollaborative rewarding schemes find an optimal strategy to keep the ball in\nthe game as long as possible. We also describe the progression from competitive\nto collaborative behavior. The present work demonstrates that Deep Q-Networks\ncan become a practical tool for studying the decentralized learning of\nmultiagent systems living in highly complex environments.\n", "versions": [{"version": "v1", "created": "Fri, 27 Nov 2015 20:01:45 GMT"}], "update_date": "2015-11-30", "authors_parsed": [["Tampuu", "Ardi", ""], ["Matiisen", "Tambet", ""], ["Kodelja", "Dorian", ""], ["Kuzovkin", "Ilya", ""], ["Korjus", "Kristjan", ""], ["Aru", "Juhan", ""], ["Aru", "Jaan", ""], ["Vicente", "Raul", ""]]}, {"id": "1511.08855", "submitter": "Francisco De Sousa Webber", "authors": "Francisco De Sousa Webber", "title": "Semantic Folding Theory And its Application in Semantic Fingerprinting", "comments": "59 pages, white paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human language is recognized as a very complex domain since decades. No\ncomputer system has been able to reach human levels of performance so far. The\nonly known computational system capable of proper language processing is the\nhuman brain. While we gather more and more data about the brain, its\nfundamental computational processes still remain obscure. The lack of a sound\ncomputational brain theory also prevents the fundamental understanding of\nNatural Language Processing. As always when science lacks a theoretical\nfoundation, statistical modeling is applied to accommodate as many sampled\nreal-world data as possible. An unsolved fundamental issue is the actual\nrepresentation of language (data) within the brain, denoted as the\nRepresentational Problem. Starting with Jeff Hawkins' Hierarchical Temporal\nMemory (HTM) theory, a consistent computational theory of the human cortex, we\nhave developed a corresponding theory of language data representation: The\nSemantic Folding Theory. The process of encoding words, by using a topographic\nsemantic space as distributional reference frame into a sparse binary\nrepresentational vector is called Semantic Folding and is the central topic of\nthis document. Semantic Folding describes a method of converting language from\nits symbolic representation (text) into an explicit, semantically grounded\nrepresentation that can be generically processed by Hawkins' HTM networks. As\nit turned out, this change in representation, by itself, can solve many complex\nNLP problems by applying Boolean operators and a generic similarity function\nlike the Euclidian Distance. Many practical problems of statistical NLP\nsystems, like the high cost of computation, the fundamental incongruity of\nprecision and recall , the complex tuning procedures etc., can be elegantly\novercome by applying Semantic Folding.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2015 00:13:09 GMT"}, {"version": "v2", "created": "Wed, 16 Mar 2016 22:04:51 GMT"}], "update_date": "2016-03-18", "authors_parsed": [["Webber", "Francisco De Sousa", ""]]}, {"id": "1511.08915", "submitter": "Markus Kr\\\"otzsch", "authors": "Jacopo Urbani, Ceriel Jacobs, Markus Kr\\\"otzsch", "title": "Column-Oriented Datalog Materialization for Large Knowledge Graphs\n  (Extended Technical Report)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The evaluation of Datalog rules over large Knowledge Graphs (KGs) is\nessential for many applications. In this paper, we present a new method of\nmaterializing Datalog inferences, which combines a column-based memory layout\nwith novel optimization methods that avoid redundant inferences at runtime. The\npro-active caching of certain subqueries further increases efficiency. Our\nempirical evaluation shows that this approach can often match or even surpass\nthe performance of state-of-the-art systems, especially under restricted\nresources.\n", "versions": [{"version": "v1", "created": "Sat, 28 Nov 2015 17:16:55 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2016 16:12:55 GMT"}], "update_date": "2016-02-12", "authors_parsed": [["Urbani", "Jacopo", ""], ["Jacobs", "Ceriel", ""], ["Kr\u00f6tzsch", "Markus", ""]]}, {"id": "1511.08952", "submitter": "Ndapa Nakashole", "authors": "Ndapandula Nakashole", "title": "Bootstrapping Ternary Relation Extractors", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary relation extraction methods have been widely studied in recent years.\nHowever, few methods have been developed for higher n-ary relation extraction.\nOne limiting factor is the effort required to generate training data. For\nbinary relations, one only has to provide a few dozen pairs of entities per\nrelation, as training data. For ternary relations (n=3), each training instance\nis a triplet of entities, placing a greater cognitive load on people. For\nexample, many people know that Google acquired Youtube but not the dollar\namount or the date of the acquisition and many people know that Hillary Clinton\nis married to Bill Clinton by not the location or date of their wedding. This\nmakes higher n-nary training data generation a time consuming exercise in\nsearching the Web. We present a resource for training ternary relation\nextractors. This was generated using a minimally supervised yet effective\napproach. We present statistics on the size and the quality of the dataset.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2015 00:49:13 GMT"}, {"version": "v2", "created": "Wed, 17 Jul 2019 02:52:34 GMT"}], "update_date": "2019-07-18", "authors_parsed": [["Nakashole", "Ndapandula", ""]]}, {"id": "1511.08967", "submitter": "Lisa Lee", "authors": "Lisa Lee", "title": "Robotic Search & Rescue via Online Multi-task Reinforcement Learning", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning (RL) is a general and well-known method that a robot\ncan use to learn an optimal control policy to solve a particular task. We would\nlike to build a versatile robot that can learn multiple tasks, but using RL for\neach of them would be prohibitively expensive in terms of both time and\nwear-and-tear on the robot. To remedy this problem, we use the Policy Gradient\nEfficient Lifelong Learning Algorithm (PG-ELLA), an online multi-task RL\nalgorithm that enables the robot to efficiently learn multiple consecutive\ntasks by sharing knowledge between these tasks to accelerate learning and\nimprove performance. We implemented and evaluated three RL methods--Q-learning,\npolicy gradient RL, and PG-ELLA--on a ground robot whose task is to find a\ntarget object in an environment under different surface conditions. In this\npaper, we discuss our implementations as well as present an empirical analysis\nof their learning performance.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2015 04:33:51 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Lee", "Lisa", ""]]}, {"id": "1511.09047", "submitter": "Joris Scharpff", "authors": "Joris Scharpff, Diederik M. Roijers, Frans A. Oliehoek, Matthijs T.J.\n  Spaan, Mathijs M. de Weerdt", "title": "Solving Transition-Independent Multi-agent MDPs with Sparse Interactions\n  (Extended version)", "comments": "This article is an extended version of the paper that was published\n  under the same title in the Proceedings of the Thirtieth AAAI Conference on\n  Artificial Intelligence (AAAI16), held in Phoenix, Arizona USA on February\n  12-17, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In cooperative multi-agent sequential decision making under uncertainty,\nagents must coordinate to find an optimal joint policy that maximises joint\nvalue. Typical algorithms exploit additive structure in the value function, but\nin the fully-observable multi-agent MDP setting (MMDP) such structure is not\npresent. We propose a new optimal solver for transition-independent MMDPs, in\nwhich agents can only affect their own state but their reward depends on joint\ntransitions. We represent these dependencies compactly in conditional return\ngraphs (CRGs). Using CRGs the value of a joint policy and the bounds on\npartially specified joint policies can be efficiently computed. We propose\nCoRe, a novel branch-and-bound policy search algorithm building on CRGs. CoRe\ntypically requires less runtime than the available alternatives and finds\nsolutions to problems previously unsolvable.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2015 17:18:10 GMT"}, {"version": "v2", "created": "Thu, 11 Feb 2016 21:15:43 GMT"}], "update_date": "2016-02-15", "authors_parsed": [["Scharpff", "Joris", ""], ["Roijers", "Diederik M.", ""], ["Oliehoek", "Frans A.", ""], ["Spaan", "Matthijs T. J.", ""], ["de Weerdt", "Mathijs M.", ""]]}, {"id": "1511.09080", "submitter": "Philipp Robbel", "authors": "Philipp Robbel and Frans A. Oliehoek and Mykel J. Kochenderfer", "title": "Exploiting Anonymity in Approximate Linear Programming: Scaling to Large\n  Multiagent MDPs (Extended Version)", "comments": "Extended version of AAAI 2016 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many exact and approximate solution methods for Markov Decision Processes\n(MDPs) attempt to exploit structure in the problem and are based on\nfactorization of the value function. Especially multiagent settings, however,\nare known to suffer from an exponential increase in value component sizes as\ninteractions become denser, meaning that approximation architectures are\nrestricted in the problem sizes and types they can handle. We present an\napproach to mitigate this limitation for certain types of multiagent systems,\nexploiting a property that can be thought of as \"anonymous influence\" in the\nfactored MDP. Anonymous influence summarizes joint variable effects efficiently\nwhenever the explicit representation of variable identity in the problem can be\navoided. We show how representational benefits from anonymity translate into\ncomputational efficiencies, both for general variable elimination in a factor\ngraph but in particular also for the approximate linear programming solution to\nfactored MDPs. The latter allows to scale linear programming to factored MDPs\nthat were previously unsolvable. Our results are shown for the control of a\nstochastic disease process over a densely connected graph with 50 nodes and 25\nagents.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2015 20:02:29 GMT"}, {"version": "v2", "created": "Sat, 20 Feb 2016 20:39:03 GMT"}], "update_date": "2016-02-23", "authors_parsed": [["Robbel", "Philipp", ""], ["Oliehoek", "Frans A.", ""], ["Kochenderfer", "Mykel J.", ""]]}, {"id": "1511.09107", "submitter": "K. Ch. Chatzisavvas", "authors": "Panagiotis Stalidis, Maria Giatsoglou, Konstantinos Diamantaras,\n  George Sarigiannidis, Konstantinos Ch. Chatzisavvas", "title": "Machine Learning Sentiment Prediction based on Hybrid Document\n  Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated sentiment analysis and opinion mining is a complex process\nconcerning the extraction of useful subjective information from text. The\nexplosion of user generated content on the Web, especially the fact that\nmillions of users, on a daily basis, express their opinions on products and\nservices to blogs, wikis, social networks, message boards, etc., render the\nreliable, automated export of sentiments and opinions from unstructured text\ncrucial for several commercial applications. In this paper, we present a novel\nhybrid vectorization approach for textual resources that combines a weighted\nvariant of the popular Word2Vec representation (based on Term Frequency-Inverse\nDocument Frequency) representation and with a Bag- of-Words representation and\na vector of lexicon-based sentiment values. The proposed text representation\napproach is assessed through the application of several machine learning\nclassification algorithms on a dataset that is used extensively in literature\nfor sentiment detection. The classification accuracy derived through the\nproposed hybrid vectorization approach is higher than when its individual\ncomponents are used for text represenation, and comparable with\nstate-of-the-art sentiment detection methodologies.\n", "versions": [{"version": "v1", "created": "Sun, 29 Nov 2015 22:41:43 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Stalidis", "Panagiotis", ""], ["Giatsoglou", "Maria", ""], ["Diamantaras", "Konstantinos", ""], ["Sarigiannidis", "George", ""], ["Chatzisavvas", "Konstantinos Ch.", ""]]}, {"id": "1511.09147", "submitter": "Athirai A. Irissappane", "authors": "Athirai A. Irissappane, Frans A. Oliehoek, Jie Zhang", "title": "Scaling POMDPs For Selecting Sellers in E-markets-Extended Version", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multiagent e-marketplaces, buying agents need to select good sellers by\nquerying other buyers (called advisors). Partially Observable Markov Decision\nProcesses (POMDPs) have shown to be an effective framework for optimally\nselecting sellers by selectively querying advisors. However, current solution\nmethods do not scale to hundreds or even tens of agents operating in the\ne-market. In this paper, we propose the Mixture of POMDP Experts (MOPE)\ntechnique, which exploits the inherent structure of trust-based domains, such\nas the seller selection problem in e-markets, by aggregating the solutions of\nsmaller sub-POMDPs. We propose a number of variants of the MOPE approach that\nwe analyze theoretically and empirically. Experiments show that MOPE can scale\nup to a hundred agents thereby leveraging the presence of more advisors to\nsignificantly improve buyer satisfaction.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 04:00:48 GMT"}, {"version": "v2", "created": "Wed, 9 Dec 2015 21:28:08 GMT"}], "update_date": "2015-12-11", "authors_parsed": [["Irissappane", "Athirai A.", ""], ["Oliehoek", "Frans A.", ""], ["Zhang", "Jie", ""]]}, {"id": "1511.09249", "submitter": "Juergen Schmidhuber", "authors": "Juergen Schmidhuber", "title": "On Learning to Think: Algorithmic Information Theory for Novel\n  Combinations of Reinforcement Learning Controllers and Recurrent Neural World\n  Models", "comments": "36 pages, 1 figure. arXiv admin note: substantial text overlap with\n  arXiv:1404.7828", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the general problem of reinforcement learning (RL) in\npartially observable environments. In 2013, our large RL recurrent neural\nnetworks (RNNs) learned from scratch to drive simulated cars from\nhigh-dimensional video input. However, real brains are more powerful in many\nways. In particular, they learn a predictive model of their initially unknown\nenvironment, and somehow use it for abstract (e.g., hierarchical) planning and\nreasoning. Guided by algorithmic information theory, we describe RNN-based AIs\n(RNNAIs) designed to do the same. Such an RNNAI can be trained on never-ending\nsequences of tasks, some of them provided by the user, others invented by the\nRNNAI itself in a curious, playful fashion, to improve its RNN-based world\nmodel. Unlike our previous model-building RNN-based RL machines dating back to\n1990, the RNNAI learns to actively query its model for abstract reasoning and\nplanning and decision making, essentially \"learning to think.\" The basic ideas\nof this report can be applied to many other cases where one RNN-like system\nexploits the algorithmic information content of another. They are taken from a\ngrant proposal submitted in Fall 2014, and also explain concepts such as\n\"mirror neurons.\" Experimental results will be described in separate papers.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 11:35:26 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Schmidhuber", "Juergen", ""]]}, {"id": "1511.09300", "submitter": "Ji\\v{r}\\'i Vomlel", "authors": "V\\'aclav Kratochv\\'il and Ji\\v{r}\\'i Vomlel", "title": "Influence diagrams for the optimization of a vehicle speed profile", "comments": "Presented at the Twelfth Annual Bayesian Modeling Applications\n  Workshop, Amtsterdam, The Netherlands, 16th July 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Influence diagrams are decision theoretic extensions of Bayesian networks.\nThey are applied to diverse decision problems. In this paper we apply influence\ndiagrams to the optimization of a vehicle speed profile. We present results of\ncomputational experiments in which an influence diagram was used to optimize\nthe speed profile of a Formula 1 race car at the Silverstone F1 circuit. The\ncomputed lap time and speed profiles correspond well to those achieved by test\npilots. An extended version of our model that considers a more complex\noptimization function and diverse traffic constraints is currently being tested\nonboard a testing car by a major car manufacturer. This paper opens doors for\nnew applications of influence diagrams.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 13:30:13 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Kratochv\u00edl", "V\u00e1clav", ""], ["Vomlel", "Ji\u0159\u00ed", ""]]}, {"id": "1511.09376", "submitter": "Snigdha Chaturvedi", "authors": "Snigdha Chaturvedi, Shashank Srivastava, Hal Daume III and Chris Dyer", "title": "Modeling Dynamic Relationships Between Characters in Literary Novels", "comments": "9 pages, 1 figure. Accepted at AAAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Studying characters plays a vital role in computationally representing and\ninterpreting narratives. Unlike previous work, which has focused on inferring\ncharacter roles, we focus on the problem of modeling their relationships.\nRather than assuming a fixed relationship for a character pair, we hypothesize\nthat relationships are dynamic and temporally evolve with the progress of the\nnarrative, and formulate the problem of relationship modeling as a structured\nprediction problem. We propose a semi-supervised framework to learn\nrelationship sequences from fully as well as partially labeled data. We present\na Markovian model capable of accumulating historical beliefs about the\nrelationship and status changes. We use a set of rich linguistic and\nsemantically motivated features that incorporate world knowledge to investigate\nthe textual content of narrative. We empirically demonstrate that such a\nframework outperforms competitive baselines.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 16:32:58 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Chaturvedi", "Snigdha", ""], ["Srivastava", "Shashank", ""], ["Daume", "Hal", "III"], ["Dyer", "Chris", ""]]}, {"id": "1511.09460", "submitter": "Snigdha Chaturvedi", "authors": "Snigdha Chaturvedi, Dan Goldwasser, Hal Daume III", "title": "Ask, and shall you receive?: Understanding Desire Fulfillment in Natural\n  Language Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to comprehend wishes or desires and their fulfillment is\nimportant to Natural Language Understanding. This paper introduces the task of\nidentifying if a desire expressed by a subject in a given short piece of text\nwas fulfilled. We propose various unstructured and structured models that\ncapture fulfillment cues such as the subject's emotional state and actions. Our\nexperiments with two different datasets demonstrate the importance of\nunderstanding the narrative and discourse structure to address this task.\n", "versions": [{"version": "v1", "created": "Mon, 30 Nov 2015 20:37:03 GMT"}], "update_date": "2015-12-01", "authors_parsed": [["Chaturvedi", "Snigdha", ""], ["Goldwasser", "Dan", ""], ["Daume", "Hal", "III"]]}]