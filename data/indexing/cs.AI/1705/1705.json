[{"id": "1705.00047", "submitter": "Renaud Hartert", "authors": "Renaud Hartert", "title": "Kiwi - A Minimalist CP Solver", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kiwi is a minimalist and extendable Constraint Programming (CP) solver\nspecifically designed for education. The particularities of Kiwi stand in its\ngeneric trailing state restoration mechanism and its modulable use of\nvariables. By developing Kiwi, the author does not aim to provide an\nalternative to full featured constraint solvers but rather to provide readers\nwith a basic architecture that will (hopefully) help them to understand the\ncore mechanisms hidden under the hood of constraint solvers, to develop their\nown extended constraint solver, or to test innovative ideas.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 19:34:19 GMT"}, {"version": "v2", "created": "Tue, 2 May 2017 01:18:47 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Hartert", "Renaud", ""]]}, {"id": "1705.00094", "submitter": "Marcos Cardinot", "authors": "Marcos Cardinot, Colm O'Riordan and Josephine Griffith", "title": "The Impact of Coevolution and Abstention on the Emergence of Cooperation", "comments": "To appear at Studies in Computational Intelligence (SCI), Springer,\n  2017", "journal-ref": "Studies in Computational Intelligence, 2019, vol 792. Springer", "doi": "10.1007/978-3-319-99283-9_6", "report-no": null, "categories": "cs.GT cs.AI cs.MA cs.NE math.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper explores the Coevolutionary Optional Prisoner's Dilemma (COPD)\ngame, which is a simple model to coevolve game strategy and link weights of\nagents playing the Optional Prisoner's Dilemma game. We consider a population\nof agents placed in a lattice grid with boundary conditions. A number of Monte\nCarlo simulations are performed to investigate the impacts of the COPD game on\nthe emergence of cooperation. Results show that the coevolutionary rules enable\ncooperators to survive and even dominate, with the presence of abstainers in\nthe population playing a key role in the protection of cooperators against\nexploitation from defectors. We observe that in adverse conditions such as when\nthe initial population of abstainers is too scarce/abundant, or when the\ntemptation to defect is very high, cooperation has no chance of emerging.\nHowever, when the simple coevolutionary rules are applied, cooperators\nflourish.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 23:15:11 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Cardinot", "Marcos", ""], ["O'Riordan", "Colm", ""], ["Griffith", "Josephine", ""]]}, {"id": "1705.00106", "submitter": "Xinya Du", "authors": "Xinya Du, Junru Shao and Claire Cardie", "title": "Learning to Ask: Neural Question Generation for Reading Comprehension", "comments": "Accepted to ACL 2017, 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study automatic question generation for sentences from text passages in\nreading comprehension. We introduce an attention-based sequence learning model\nfor the task and investigate the effect of encoding sentence- vs.\nparagraph-level information. In contrast to all previous work, our model does\nnot rely on hand-crafted rules or a sophisticated NLP pipeline; it is instead\ntrainable end-to-end via sequence-to-sequence learning. Automatic evaluation\nresults show that our system significantly outperforms the state-of-the-art\nrule-based system. In human evaluations, questions generated by our system are\nalso rated as being more natural (i.e., grammaticality, fluency) and as more\ndifficult to answer (in terms of syntactic and lexical divergence from the\noriginal text and reasoning needed to answer).\n", "versions": [{"version": "v1", "created": "Sat, 29 Apr 2017 01:08:48 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Du", "Xinya", ""], ["Shao", "Junru", ""], ["Cardie", "Claire", ""]]}, {"id": "1705.00154", "submitter": "Masataro Asai", "authors": "Masataro Asai, Alex Fukunaga", "title": "Classical Planning in Deep Latent Space: Bridging the\n  Subsymbolic-Symbolic Boundary", "comments": "This is an extended manuscript of the paper accepted in AAAI-18. The\n  contents of AAAI-18 paper itself is significantly extended from what has been\n  published in Arxiv or previous workshops. Over half of the paper describing\n  (2) is new. Additionally, this manuscript contains the supplemental materials\n  of AAAI-18 submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current domain-independent, classical planners require symbolic models of the\nproblem domain and instance as input, resulting in a knowledge acquisition\nbottleneck. Meanwhile, although deep learning has achieved significant success\nin many fields, the knowledge is encoded in a subsymbolic representation which\nis incompatible with symbolic systems such as planners. We propose LatPlan, an\nunsupervised architecture combining deep learning and classical planning. Given\nonly an unlabeled set of image pairs showing a subset of transitions allowed in\nthe environment (training inputs), and a pair of images representing the\ninitial and the goal states (planning inputs), LatPlan finds a plan to the goal\nstate in a symbolic latent space and returns a visualized plan execution. The\ncontribution of this paper is twofold: (1) State Autoencoder, which finds a\npropositional state representation of the environment using a Variational\nAutoencoder. It generates a discrete latent vector from the images, based on\nwhich a PDDL model can be constructed and then solved by an off-the-shelf\nplanner. (2) Action Autoencoder / Discriminator, a neural architecture which\njointly finds the action symbols and the implicit action models\n(preconditions/effects), and provides a successor function for the implicit\ngraph search. We evaluate LatPlan using image-based versions of 3 planning\ndomains: 8-puzzle, Towers of Hanoi and LightsOut.\n", "versions": [{"version": "v1", "created": "Sat, 29 Apr 2017 08:22:29 GMT"}, {"version": "v2", "created": "Thu, 9 Nov 2017 04:09:36 GMT"}, {"version": "v3", "created": "Sun, 3 Dec 2017 12:19:39 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Asai", "Masataro", ""], ["Fukunaga", "Alex", ""]]}, {"id": "1705.00211", "submitter": "Babatunde Akinkunmi", "authors": "B.O. Akinkunmi", "title": "A Partitioning Algorithm for Detecting Eventuality Coincidence in\n  Temporal Double recurrence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A logical theory of regular double or multiple recurrence of eventualities,\nwhich are regular patterns of occurrences that are repeated, in time, has been\ndeveloped within the context of temporal reasoning that enabled reasoning about\nthe problem of coincidence. i.e. if two complex eventualities, or eventuality\nsequences consisting respectively of component eventualities x0, x1,....,xr and\ny0, y1, ..,ys both recur over an interval k and all eventualities are of fixed\ndurations, is there a subinterval of k over which the occurrence xp and yq for\np between 1 and r and q between 1 and s coincide. We present the ideas behind a\nnew algorithm for detecting the coincidence of eventualities xp and yq within a\ncycle of the double recurrence of x and y. The algorithm is based on the novel\nconcept of gcd partitions that requires the partitioning of each of the\nincidences of both x and y into eventuality sequences each of which components\nhave a duration that is equal to the greatest common divisor of the durations\nof x and y. The worst case running time of the partitioning algorithm is linear\nin the maximum of the duration of x and that of y, while the worst case running\ntime of an algorithm exploring a complete cycle is quadratic in the durations\nof x and y. Hence the partitioning algorithm works faster than the cyclical\nexploration in the worst case.\n", "versions": [{"version": "v1", "created": "Sat, 29 Apr 2017 16:31:58 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Akinkunmi", "B. O.", ""]]}, {"id": "1705.00303", "submitter": "Beishui Liao", "authors": "Beishui Liao and Leendert van der Torre", "title": "Defense semantics of argumentation: encoding reasons for accepting\n  arguments", "comments": "14 pages, first submitted on April 30, 2017; 16 pages, revised in\n  terms of the comments from MIREL2017 on August 03, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we show how the defense relation among abstract arguments can\nbe used to encode the reasons for accepting arguments. After introducing a\nnovel notion of defenses and defense graphs, we propose a defense semantics\ntogether with a new notion of defense equivalence of argument graphs, and\ncompare defense equivalence with standard equivalence and strong equivalence,\nrespectively. Then, based on defense semantics, we define two kinds of reasons\nfor accepting arguments, i.e., direct reasons and root reasons, and a notion of\nroot equivalence of argument graphs. Finally, we show how the notion of root\nequivalence can be used in argumentation summarization.\n", "versions": [{"version": "v1", "created": "Sun, 30 Apr 2017 12:06:28 GMT"}, {"version": "v2", "created": "Wed, 2 Aug 2017 15:46:21 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Liao", "Beishui", ""], ["van der Torre", "Leendert", ""]]}, {"id": "1705.00321", "submitter": "Ganbin Zhou", "authors": "Ganbin Zhou, Ping Luo, Rongyu Cao, Yijun Xiao, Fen Lin, Bo Chen, Qing\n  He", "title": "Tree-Structured Neural Machine for Linguistics-Aware Sentence Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different from other sequential data, sentences in natural language are\nstructured by linguistic grammars. Previous generative conversational models\nwith chain-structured decoder ignore this structure in human language and might\ngenerate plausible responses with less satisfactory relevance and fluency. In\nthis study, we aim to incorporate the results from linguistic analysis into the\nprocess of sentence generation for high-quality conversation generation.\nSpecifically, we use a dependency parser to transform each response sentence\ninto a dependency tree and construct a training corpus of sentence-tree pairs.\nA tree-structured decoder is developed to learn the mapping from a sentence to\nits tree, where different types of hidden states are used to depict the local\ndependencies from an internal tree node to its children. For training\nacceleration, we propose a tree canonicalization method, which transforms trees\ninto equivalent ternary trees. Then, with a proposed tree-structured search\nmethod, the model is able to generate the most probable responses in the form\nof dependency trees, which are finally flattened into sequences as the system\noutput. Experimental results demonstrate that the proposed X2Tree framework\noutperforms baseline methods over 11.15% increase of acceptance ratio.\n", "versions": [{"version": "v1", "created": "Sun, 30 Apr 2017 15:09:10 GMT"}, {"version": "v2", "created": "Sat, 6 May 2017 14:38:10 GMT"}, {"version": "v3", "created": "Tue, 21 Nov 2017 05:31:23 GMT"}, {"version": "v4", "created": "Wed, 3 Jan 2018 07:35:19 GMT"}], "update_date": "2018-01-04", "authors_parsed": [["Zhou", "Ganbin", ""], ["Luo", "Ping", ""], ["Cao", "Rongyu", ""], ["Xiao", "Yijun", ""], ["Lin", "Fen", ""], ["Chen", "Bo", ""], ["He", "Qing", ""]]}, {"id": "1705.00335", "submitter": "Silvio Amir", "authors": "Silvio Amir, Glen Coppersmith, Paula Carvalho, M\\'ario J. Silva, Byron\n  C. Wallace", "title": "Quantifying Mental Health from Social Media with Neural User Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mental illnesses adversely affect a significant proportion of the population\nworldwide. However, the methods traditionally used for estimating and\ncharacterizing the prevalence of mental health conditions are time-consuming\nand expensive. Consequently, best-available estimates concerning the prevalence\nof mental health conditions are often years out of date. Automated approaches\nto supplement these survey methods with broad, aggregated information derived\nfrom social media content provides a potential means for near real-time\nestimates at scale. These may, in turn, provide grist for supporting,\nevaluating and iteratively improving upon public health programs and\ninterventions.\n  We propose a novel model for automated mental health status quantification\nthat incorporates user embeddings. This builds upon recent work exploring\nrepresentation learning methods that induce embeddings by leveraging social\nmedia post histories. Such embeddings capture latent characteristics of\nindividuals (e.g., political leanings) and encode a soft notion of homophily.\nIn this paper, we investigate whether user embeddings learned from twitter post\nhistories encode information that correlates with mental health statuses. To\nthis end, we estimated user embeddings for a set of users known to be affected\nby depression and post-traumatic stress disorder (PTSD), and for a set of\ndemographically matched `control' users. We then evaluated these embeddings\nwith respect to: (i) their ability to capture homophilic relations with respect\nto mental health status; and (ii) the performance of downstream mental health\nprediction models based on these features. Our experimental results demonstrate\nthat the user embeddings capture similarities between users with respect to\nmental conditions, and are predictive of mental health.\n", "versions": [{"version": "v1", "created": "Sun, 30 Apr 2017 16:12:28 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Amir", "Silvio", ""], ["Coppersmith", "Glen", ""], ["Carvalho", "Paula", ""], ["Silva", "M\u00e1rio J.", ""], ["Wallace", "Byron C.", ""]]}, {"id": "1705.00341", "submitter": "Chris Martens", "authors": "Ryan Alexander, Chris Martens", "title": "Deriving Quests from Open World Mechanics", "comments": "To appear at Foundations of Digital Games (FDG) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open world games present players with more freedom than games with linear\nprogression structures. However, without clearly-defined objectives, they often\nleave players without a sense of purpose. Most of the time, quests and\nobjectives are hand-authored and overlaid atop an open world's mechanics. But\nwhat if they could be generated organically from the gameplay itself? The goal\nof our project was to develop a model of the mechanics in Minecraft that could\nbe used to determine the ideal placement of objectives in an open world\nsetting. We formalized the game logic of Minecraft in terms of logical rules\nthat can be manipulated in two ways: they may be executed to generate graphs\nrepresentative of the player experience when playing an open world game with\nlittle developer direction; and they may be statically analyzed to determine\ndependency orderings, feedback loops, and bottlenecks. These analyses may then\nbe used to place achievements on gameplay actions algorithmically.\n", "versions": [{"version": "v1", "created": "Sun, 30 Apr 2017 16:47:57 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Alexander", "Ryan", ""], ["Martens", "Chris", ""]]}, {"id": "1705.00561", "submitter": "Richard Oentaryo", "authors": "Ferdian Thung, Richard J. Oentaryo, David Lo, Yuan Tian", "title": "WebAPIRec: Recommending Web APIs to Software Projects via Personalized\n  Ranking", "comments": "IEEE Transactions on Emerging Topics in Computational Intelligence,\n  2017", "journal-ref": "IEEE Transactions on Emerging Topics in Computational Intelligence\n  2017", "doi": "10.1109/TETCI.2017.2699222", "report-no": null, "categories": "cs.IR cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Application programming interfaces (APIs) offer a plethora of functionalities\nfor developers to reuse without reinventing the wheel. Identifying the\nappropriate APIs given a project requirement is critical for the success of a\nproject, as many functionalities can be reused to achieve faster development.\nHowever, the massive number of APIs would often hinder the developers' ability\nto quickly find the right APIs. In this light, we propose a new, automated\napproach called WebAPIRec that takes as input a project profile and outputs a\nranked list of {web} APIs that can be used to implement the project. At its\nheart, WebAPIRec employs a personalized ranking model that ranks web APIs\nspecific (personalized) to a project. Based on the historical data of {web} API\nusages, WebAPIRec learns a model that minimizes the incorrect ordering of web\nAPIs, i.e., when a used {web} API is ranked lower than an unused (or a\nnot-yet-used) web API. We have evaluated our approach on a dataset comprising\n9,883 web APIs and 4,315 web application projects from ProgrammableWeb with\npromising results. For 84.0% of the projects, WebAPIRec is able to successfully\nreturn correct APIs that are used to implement the projects in the top-5\npositions. This is substantially better than the recommendations provided by\nProgrammableWeb's native search functionality. WebAPIRec also outperforms\nMcMillan et al.'s application search engine and popularity-based\nrecommendation.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 15:28:20 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Thung", "Ferdian", ""], ["Oentaryo", "Richard J.", ""], ["Lo", "David", ""], ["Tian", "Yuan", ""]]}, {"id": "1705.00594", "submitter": "Randal Olson", "authors": "Randal S. Olson, Moshe Sipper, William La Cava, Sharon Tartarone,\n  Steven Vitale, Weixuan Fu, Patryk Orzechowski, Ryan J. Urbanowicz, John H.\n  Holmes, Jason H. Moore", "title": "A System for Accessible Artificial Intelligence", "comments": "14 pages, 5 figures, submitted to Genetic Programming Theory and\n  Practice 2017 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While artificial intelligence (AI) has become widespread, many commercial AI\nsystems are not yet accessible to individual researchers nor the general public\ndue to the deep knowledge of the systems required to use them. We believe that\nAI has matured to the point where it should be an accessible technology for\neveryone. We present an ongoing project whose ultimate goal is to deliver an\nopen source, user-friendly AI system that is specialized for machine learning\nanalysis of complex data in the biomedical and health care domains. We discuss\nhow genetic programming can aid in this endeavor, and highlight specific\nexamples where genetic programming has automated machine learning analyses in\nprevious projects.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 17:11:48 GMT"}, {"version": "v2", "created": "Thu, 10 Aug 2017 17:14:14 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Olson", "Randal S.", ""], ["Sipper", "Moshe", ""], ["La Cava", "William", ""], ["Tartarone", "Sharon", ""], ["Vitale", "Steven", ""], ["Fu", "Weixuan", ""], ["Orzechowski", "Patryk", ""], ["Urbanowicz", "Ryan J.", ""], ["Holmes", "John H.", ""], ["Moore", "Jason H.", ""]]}, {"id": "1705.00597", "submitter": "Xiaofeng Zhang", "authors": "Zhaocai Sun, William K. Cheung, Xiaofeng Zhang, Jun Yang", "title": "Towards well-specified semi-supervised model-based classifiers via\n  structural adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Semi-supervised learning plays an important role in large-scale machine\nlearning. Properly using additional unlabeled data (largely available nowadays)\noften can improve the machine learning accuracy. However, if the machine\nlearning model is misspecified for the underlying true data distribution, the\nmodel performance could be seriously jeopardized. This issue is known as model\nmisspecification. To address this issue, we focus on generative models and\npropose a criterion to detect the onset of model misspecification by measuring\nthe performance difference between models obtained using supervised and\nsemi-supervised learning. Then, we propose to automatically modify the\ngenerative models during model training to achieve an unbiased generative\nmodel. Rigorous experiments were carried out to evaluate the proposed method\nusing two image classification data sets PASCAL VOC'07 and MIR Flickr. Our\nproposed method has been demonstrated to outperform a number of\nstate-of-the-art semi-supervised learning approaches for the classification\ntask.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 17:26:53 GMT"}], "update_date": "2017-05-02", "authors_parsed": [["Sun", "Zhaocai", ""], ["Cheung", "William K.", ""], ["Zhang", "Xiaofeng", ""], ["Yang", "Jun", ""]]}, {"id": "1705.00673", "submitter": "Prasanna Parthasarathi", "authors": "Hoai Phuoc Truong, Prasanna Parthasarathi, Joelle Pineau", "title": "MACA: A Modular Architecture for Conversational Agents", "comments": "The architecture needs to be tested further. Sorry for the\n  inconvenience. We should be putting up the paper up soon", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a software architecture designed to ease the implementation of\ndialogue systems. The Modular Architecture for Conversational Agents (MACA)\nuses a plug-n-play style that allows quick prototyping, thereby facilitating\nthe development of new techniques and the reproduction of previous work. The\narchitecture separates the domain of the conversation from the agent's dialogue\nstrategy, and as such can be easily extended to multiple domains. MACA provides\ntools to host dialogue agents on Amazon Mechanical Turk (mTurk) for data\ncollection and allows processing of other sources of training data. The current\nversion of the framework already incorporates several domains and existing\ndialogue strategies from the recent literature.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 19:18:04 GMT"}, {"version": "v2", "created": "Wed, 3 May 2017 01:20:26 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Truong", "Hoai Phuoc", ""], ["Parthasarathi", "Prasanna", ""], ["Pineau", "Joelle", ""]]}, {"id": "1705.00697", "submitter": "Luis Argerich", "authors": "Juan Andr\\'es Laura, Gabriel Masi, Luis Argerich", "title": "From Imitation to Prediction, Data Compression vs Recurrent Neural\n  Networks for Natural Language Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent studies [1][13][12] Recurrent Neural Networks were used for\ngenerative processes and their surprising performance can be explained by their\nability to create good predictions. In addition, data compression is also based\non predictions. What the problem comes down to is whether a data compressor\ncould be used to perform as well as recurrent neural networks in natural\nlanguage processing tasks. If this is possible,then the problem comes down to\ndetermining if a compression algorithm is even more intelligent than a neural\nnetwork in specific tasks related to human language. In our journey we\ndiscovered what we think is the fundamental difference between a Data\nCompression Algorithm and a Recurrent Neural Network.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 20:23:13 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Laura", "Juan Andr\u00e9s", ""], ["Masi", "Gabriel", ""], ["Argerich", "Luis", ""]]}, {"id": "1705.00732", "submitter": "Erisa Karafili", "authors": "Erisa Karafili, Antonis C. Kakas, Nikolaos I. Spanoudakis, Emil C.\n  Lupu", "title": "Argumentation-based Security for Social Good", "comments": "Paper presented at the AAAI Spring Symposium 2017, 7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increase of connectivity and the impact it has in every day life is\nraising new and existing security problems that are becoming important for\nsocial good. We introduce two particular problems: cyber attack attribution and\nregulatory data sharing. For both problems, decisions about which rules to\napply, should be taken under incomplete and context dependent information. The\nsolution we propose is based on argumentation reasoning, that is a well suited\ntechnique for implementing decision making mechanisms under conflicting and\nincomplete information. Our proposal permits us to identify the attacker of a\ncyber attack and decide the regulation rule that should be used while using and\nsharing data. We illustrate our solution through concrete examples.\n", "versions": [{"version": "v1", "created": "Mon, 1 May 2017 22:41:38 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Karafili", "Erisa", ""], ["Kakas", "Antonis C.", ""], ["Spanoudakis", "Nikolaos I.", ""], ["Lupu", "Emil C.", ""]]}, {"id": "1705.00930", "submitter": "Tseng-Hung Chen", "authors": "Tseng-Hung Chen, Yuan-Hong Liao, Ching-Yao Chuang, Wan-Ting Hsu,\n  Jianlong Fu, Min Sun", "title": "Show, Adapt and Tell: Adversarial Training of Cross-domain Image\n  Captioner", "comments": "ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Impressive image captioning results are achieved in domains with plenty of\ntraining image and sentence pairs (e.g., MSCOCO). However, transferring to a\ntarget domain with significant domain shifts but no paired training data\n(referred to as cross-domain image captioning) remains largely unexplored. We\npropose a novel adversarial training procedure to leverage unpaired data in the\ntarget domain. Two critic networks are introduced to guide the captioner,\nnamely domain critic and multi-modal critic. The domain critic assesses whether\nthe generated sentences are indistinguishable from sentences in the target\ndomain. The multi-modal critic assesses whether an image and its generated\nsentence are a valid pair. During training, the critics and captioner act as\nadversaries -- captioner aims to generate indistinguishable sentences, whereas\ncritics aim at distinguishing them. The assessment improves the captioner\nthrough policy gradient updates. During inference, we further propose a novel\ncritic-based planning method to select high-quality sentences without\nadditional supervision (e.g., tags). To evaluate, we use MSCOCO as the source\ndomain and four other datasets (CUB-200-2011, Oxford-102, TGIF, and Flickr30k)\nas the target domains. Our method consistently performs well on all datasets.\nIn particular, on CUB-200-2011, we achieve 21.8% CIDEr-D improvement after\nadaptation. Utilizing critics during inference further gives another 4.5%\nboost.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 12:06:54 GMT"}, {"version": "v2", "created": "Mon, 14 Aug 2017 15:54:32 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Chen", "Tseng-Hung", ""], ["Liao", "Yuan-Hong", ""], ["Chuang", "Ching-Yao", ""], ["Hsu", "Wan-Ting", ""], ["Fu", "Jianlong", ""], ["Sun", "Min", ""]]}, {"id": "1705.00969", "submitter": "Babatunde Akinkunmi", "authors": "B.O. Akinkunmi", "title": "The Problem of Coincidence in A Theory of Temporal Multiple Recurrence", "comments": "arXiv admin note: substantial text overlap with arXiv:1705.00211", "journal-ref": "Journal of Applied Logic, 15:46-48, May 2016", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logical theories have been developed which have allowed temporal reasoning\nabout eventualities (a la Galton) such as states, processes, actions, events,\nprocesses and complex eventualities such as sequences and recurrences of other\neventualities. This paper presents the problem of coincidence within the\nframework of a first order logical theory formalising temporal multiple\nrecurrence of two sequences of fixed duration eventualities and presents a\nsolution to it The coincidence problem is described as: if two complex\neventualities (or eventuality sequences) consisting respectively of component\neventualities x0, x1,....,xr and y0, y1, ..,ys both recur over an interval k\nand all eventualities are of fixed durations, is there a sub-interval of k over\nwhich the incidence xt and yu for t between 0..r and s between 0..s coincide.\nThe solution presented here formalises the intuition that a solution can be\nfound by temporal projection over a cycle of the multiple recurrence of both\nsequences.\n", "versions": [{"version": "v1", "created": "Sat, 29 Apr 2017 16:54:18 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Akinkunmi", "B. O.", ""]]}, {"id": "1705.01013", "submitter": "Wen Jiang", "authors": "Zichang He and Wen Jiang", "title": "Quantum Mechanical Approach to Modelling Reliability of Sensor Reports", "comments": "13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dempster-Shafer evidence theory is wildly applied in multi-sensor data\nfusion. However, lots of uncertainty and interference exist in practical\nsituation, especially in the battle field. It is still an open issue to model\nthe reliability of sensor reports. Many methods are proposed based on the\nrelationship among collected data. In this letter, we proposed a quantum\nmechanical approach to evaluate the reliability of sensor reports, which is\nbased on the properties of a sensor itself. The proposed method is used to\nmodify the combining of evidences.\n", "versions": [{"version": "v1", "created": "Mon, 17 Apr 2017 01:22:15 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["He", "Zichang", ""], ["Jiang", "Wen", ""]]}, {"id": "1705.01040", "submitter": "Chih-Hong Cheng", "authors": "Chih-Hong Cheng, Georg N\\\"uhrenberg, Harald Ruess", "title": "Maximum Resilience of Artificial Neural Networks", "comments": "Timestamp research work conducted in the project. version 2: fix some\n  typos, rephrase the definition, and add some more existing work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deployment of Artificial Neural Networks (ANNs) in safety-critical\napplications poses a number of new verification and certification challenges.\nIn particular, for ANN-enabled self-driving vehicles it is important to\nestablish properties about the resilience of ANNs to noisy or even maliciously\nmanipulated sensory input. We are addressing these challenges by defining\nresilience properties of ANN-based classifiers as the maximal amount of input\nor sensor perturbation which is still tolerated. This problem of computing\nmaximal perturbation bounds for ANNs is then reduced to solving mixed integer\noptimization problems (MIP). A number of MIP encoding heuristics are developed\nfor drastically reducing MIP-solver runtimes, and using parallelization of\nMIP-solvers results in an almost linear speed-up in the number (up to a certain\nlimit) of computing cores in our experiments. We demonstrate the effectiveness\nand scalability of our approach by means of computing maximal resilience bounds\nfor a number of ANN benchmark sets ranging from typical image recognition\nscenarios to the autonomous maneuvering of robots.\n", "versions": [{"version": "v1", "created": "Fri, 28 Apr 2017 12:04:02 GMT"}, {"version": "v2", "created": "Wed, 5 Jul 2017 11:27:46 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Cheng", "Chih-Hong", ""], ["N\u00fchrenberg", "Georg", ""], ["Ruess", "Harald", ""]]}, {"id": "1705.01076", "submitter": "Rafa{\\l} Skinderowicz", "authors": "Rafa{\\l} Skinderowicz", "title": "An improved Ant Colony System for the Sequential Ordering Problem", "comments": "30 pages, 8 tables, 11 figures", "journal-ref": null, "doi": "10.1016/j.cor.2017.04.012", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is not rare that the performance of one metaheuristic algorithm can be\nimproved by incorporating ideas taken from another. In this article we present\nhow Simulated Annealing (SA) can be used to improve the efficiency of the Ant\nColony System (ACS) and Enhanced ACS when solving the Sequential Ordering\nProblem (SOP). Moreover, we show how the very same ideas can be applied to\nimprove the convergence of a dedicated local search, i.e. the SOP-3-exchange\nalgorithm. A statistical analysis of the proposed algorithms both in terms of\nfinding suitable parameter values and the quality of the generated solutions is\npresented based on a series of computational experiments conducted on SOP\ninstances from the well-known TSPLIB and SOPLIB2006 repositories. The proposed\nACS-SA and EACS-SA algorithms often generate solutions of better quality than\nthe ACS and EACS, respectively. Moreover, the EACS-SA algorithm combined with\nthe proposed SOP-3-exchange-SA local search was able to find 10 new best\nsolutions for the SOP instances from the SOPLIB2006 repository, thus improving\nthe state-of-the-art results as known from the literature. Overall, the best\nknown or improved solutions were found in 41 out of 48 cases.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 17:17:26 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Skinderowicz", "Rafa\u0142", ""]]}, {"id": "1705.01080", "submitter": "Jialin Liu Ph.D", "authors": "Kamolwan Kunanusont, Raluca D. Gaina, Jialin Liu, Diego Perez-Liebana,\n  Simon M. Lucas", "title": "The N-Tuple Bandit Evolutionary Algorithm for Automatic Game Improvement", "comments": "8 pages, 9 figure, 2 tables, CEC2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a new evolutionary algorithm that is especially well\nsuited to AI-Assisted Game Design. The approach adopted in this paper is to use\nobservations of AI agents playing the game to estimate the game's quality. Some\nof best agents for this purpose are General Video Game AI agents, since they\ncan be deployed directly on a new game without game-specific tuning; these\nagents tend to be based on stochastic algorithms which give robust but noisy\nresults and tend to be expensive to run. This motivates the main contribution\nof the paper: the development of the novel N-Tuple Bandit Evolutionary\nAlgorithm, where a model is used to estimate the fitness of unsampled points\nand a bandit approach is used to balance exploration and exploitation of the\nsearch space. Initial results on optimising a Space Battle game variant suggest\nthat the algorithm offers far more robust results than the Random Mutation Hill\nClimber and a Biased Mutation variant, which are themselves known to offer\ncompetitive performance across a range of problems. Subjective observations are\nalso given by human players on the nature of the evolved games, which indicate\na preference towards games generated by the N-Tuple algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 18 Mar 2017 09:10:09 GMT"}], "update_date": "2017-05-03", "authors_parsed": [["Kunanusont", "Kamolwan", ""], ["Gaina", "Raluca D.", ""], ["Liu", "Jialin", ""], ["Perez-Liebana", "Diego", ""], ["Lucas", "Simon M.", ""]]}, {"id": "1705.01172", "submitter": "Gavin Rens", "authors": "Gavin Rens and Thomas Meyer", "title": "Imagining Probabilistic Belief Change as Imaging (Technical Report)", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imaging is a form of probabilistic belief change which could be employed for\nboth revision and update. In this paper, we propose a new framework for\nprobabilistic belief change based on imaging, called Expected Distance Imaging\n(EDI). EDI is sufficiently general to define Bayesian conditioning and other\nforms of imaging previously defined in the literature. We argue that, and\ninvestigate how, EDI can be used for both revision and update. EDI's definition\ndepends crucially on a weight function whose properties are studied and whose\neffect on belief change operations is analysed. Finally, four EDI\ninstantiations are proposed, two for revision and two for update, and\nprobabilistic rationality postulates are suggested for their analysis.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 20:50:59 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Rens", "Gavin", ""], ["Meyer", "Thomas", ""]]}, {"id": "1705.01187", "submitter": "Akansel Cosgun", "authors": "Akansel Cosgun, Lichao Ma, Jimmy Chiu, Jiawei Huang, Mahmut Demir,\n  Alexandre Miranda Anon, Thang Lian, Hasan Tafish, Samir Al-Stouhi", "title": "Towards Full Automated Drive in Urban Environments: A Demonstration in\n  GoMentum Station, California", "comments": "Accepted to Intelligent Vehicles Conference (IV 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Each year, millions of motor vehicle traffic accidents all over the world\ncause a large number of fatalities, injuries and significant material loss.\nAutomated Driving (AD) has potential to drastically reduce such accidents. In\nthis work, we focus on the technical challenges that arise from AD in urban\nenvironments. We present the overall architecture of an AD system and describe\nin detail the perception and planning modules. The AD system, built on a\nmodified Acura RLX, was demonstrated in a course in GoMentum Station in\nCalifornia. We demonstrated autonomous handling of 4 scenarios: traffic lights,\ncross-traffic at intersections, construction zones and pedestrians. The AD\nvehicle displayed safe behavior and performed consistently in repeated\ndemonstrations with slight variations in conditions. Overall, we completed 44\nruns, encompassing 110km of automated driving with only 3 cases where the\ndriver intervened the control of the vehicle, mostly due to error in GPS\npositioning. Our demonstration showed that robust and consistent behavior in\nurban scenarios is possible, yet more investigation is necessary for full scale\nroll-out on public roads.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 22:04:59 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Cosgun", "Akansel", ""], ["Ma", "Lichao", ""], ["Chiu", "Jimmy", ""], ["Huang", "Jiawei", ""], ["Demir", "Mahmut", ""], ["Anon", "Alexandre Miranda", ""], ["Lian", "Thang", ""], ["Tafish", "Hasan", ""], ["Al-Stouhi", "Samir", ""]]}, {"id": "1705.01196", "submitter": "Akansel Cosgun", "authors": "David Isele, Reza Rahimi, Akansel Cosgun, Kaushik Subramanian, Kikuo\n  Fujimura", "title": "Navigating Occluded Intersections with Autonomous Vehicles using Deep\n  Reinforcement Learning", "comments": "IEEE International Conference on Robotics and Automation (ICRA 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Providing an efficient strategy to navigate safely through unsignaled\nintersections is a difficult task that requires determining the intent of other\ndrivers. We explore the effectiveness of Deep Reinforcement Learning to handle\nintersection problems. Using recent advances in Deep RL, we are able to learn\npolicies that surpass the performance of a commonly-used heuristic approach in\nseveral metrics including task completion time and goal success rate and have\nlimited ability to generalize. We then explore a system's ability to learn\nactive sensing behaviors to enable navigating safely in the case of occlusions.\nOur analysis, provides insight into the intersection handling problem, the\nsolutions learned by the network point out several shortcomings of current\nrule-based methods, and the failures of our current deep reinforcement learning\nsystem point to future research directions.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 22:57:36 GMT"}, {"version": "v2", "created": "Tue, 27 Feb 2018 03:09:02 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Isele", "David", ""], ["Rahimi", "Reza", ""], ["Cosgun", "Akansel", ""], ["Subramanian", "Kaushik", ""], ["Fujimura", "Kikuo", ""]]}, {"id": "1705.01197", "submitter": "Akansel Cosgun", "authors": "David Isele, Akansel Cosgun, Kikuo Fujimura", "title": "Analyzing Knowledge Transfer in Deep Q-Networks for Autonomously\n  Handling Multiple Intersections", "comments": "Submitted to IEEE International Conference on Intelligent\n  Transportation Systems (ITSC 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze how the knowledge to autonomously handle one type of intersection,\nrepresented as a Deep Q-Network, translates to other types of intersections\n(tasks). We view intersection handling as a deep reinforcement learning\nproblem, which approximates the state action Q function as a deep neural\nnetwork. Using a traffic simulator, we show that directly copying a network\ntrained for one type of intersection to another type of intersection decreases\nthe success rate. We also show that when a network that is pre-trained on Task\nA and then is fine-tuned on a Task B, the resulting network not only performs\nbetter on the Task B than an network exclusively trained on Task A, but also\nretained knowledge on the Task A. Finally, we examine a lifelong learning\nsetting, where we train a single network on five different types of\nintersections sequentially and show that the resulting network exhibited\ncatastrophic forgetting of knowledge on previous tasks. This result suggests a\nneed for a long-term memory component to preserve knowledge.\n", "versions": [{"version": "v1", "created": "Tue, 2 May 2017 23:05:56 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Isele", "David", ""], ["Cosgun", "Akansel", ""], ["Fujimura", "Kikuo", ""]]}, {"id": "1705.01208", "submitter": "Hanqing Zhu", "authors": "Ashis Pati, Kantwon Rogers and Hanqing Zhu", "title": "A Rule-Based Computational Model of Cognitive Arithmetic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cognitive arithmetic studies the mental processes used in solving math\nproblems. This area of research explores the retrieval mechanisms and\nstrategies used by people during a common cognitive task. Past research has\nshown that human performance in arithmetic operations is correlated to the\nnumerical size of the problem. Past research on cognitive arithmetic has\npinpointed this trend to either retrieval strength, error checking, or\nstrategy-based approaches when solving equations. This paper describes a\nrule-based computational model that performs the four major arithmetic\noperations (addition, subtraction, multiplication and division) on two\noperands. We then evaluated our model to probe its validity in representing the\nprevailing concepts observed in psychology experiments from the related works.\nThe experiments specifically explore the problem size effect, an\nactivation-based model for fact retrieval, backup strategies when retrieval\nfails, and finally optimization strategies when faced with large operands. From\nour experimental results, we concluded that our model's response times were\ncomparable to results observed when people performed similar tasks during\npsychology experiments. The fit of our model in reproducing these results and\nincorporating accuracy into our model are discussed.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 00:28:26 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Pati", "Ashis", ""], ["Rogers", "Kantwon", ""], ["Zhu", "Hanqing", ""]]}, {"id": "1705.01209", "submitter": "Gan Sun", "authors": "Gan Sun, Yang Cong, Ji Liu and Xiaowei Xu", "title": "Lifelong Metric Learning", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The state-of-the-art online learning approaches are only capable of learning\nthe metric for predefined tasks. In this paper, we consider lifelong learning\nproblem to mimic \"human learning\", i.e., endowing a new capability to the\nlearned metric for a new task from new online samples and incorporating\nprevious experiences and knowledge. Therefore, we propose a new metric learning\nframework: lifelong metric learning (LML), which only utilizes the data of the\nnew task to train the metric model while preserving the original capabilities.\nMore specifically, the proposed LML maintains a common subspace for all learned\nmetrics, named lifelong dictionary, transfers knowledge from the common\nsubspace to each new metric task with task-specific idiosyncrasy, and redefines\nthe common subspace over time to maximize performance across all metric tasks.\nFor model optimization, we apply online passive aggressive optimization\nalgorithm to solve the proposed LML framework, where the lifelong dictionary\nand task-specific partition are optimized alternatively and consecutively.\nFinally, we evaluate our approach by analyzing several multi-task metric\nlearning datasets. Extensive experimental results demonstrate effectiveness and\nefficiency of the proposed framework.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 00:31:55 GMT"}, {"version": "v2", "created": "Mon, 12 Jun 2017 15:09:20 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Sun", "Gan", ""], ["Cong", "Yang", ""], ["Liu", "Ji", ""], ["Xu", "Xiaowei", ""]]}, {"id": "1705.01228", "submitter": "EPTCS", "authors": "Alessandro Coglio (Kestrel Institute), Matt Kaufmann (Department of\n  Computer Science, The University of Texas at Austin), Eric W. Smith (Kestrel\n  Institute)", "title": "A Versatile, Sound Tool for Simplifying Definitions", "comments": "In Proceedings ACL2Workshop 2017, arXiv:1705.00766", "journal-ref": "EPTCS 249, 2017, pp. 61-77", "doi": "10.4204/EPTCS.249.5", "report-no": null, "categories": "cs.PL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a tool, simplify-defun, that transforms the definition of a given\nfunction into a simplified definition of a new function, providing a proof\nchecked by ACL2 that the old and new functions are equivalent. When appropriate\nit also generates termination and guard proofs for the new function. We explain\nhow the tool is engineered so that these proofs will succeed. Examples\nillustrate its utility, in particular for program transformation in synthesis\nand verification.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 01:49:29 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Coglio", "Alessandro", "", "Kestrel Institute"], ["Kaufmann", "Matt", "", "Department of\n  Computer Science, The University of Texas at Austin"], ["Smith", "Eric W.", "", "Kestrel\n  Institute"]]}, {"id": "1705.01320", "submitter": "R\\\"udiger Ehlers", "authors": "Ruediger Ehlers", "title": "Formal Verification of Piece-Wise Linear Feed-Forward Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for the verification of feed-forward neural networks\nin which all nodes have a piece-wise linear activation function. Such networks\nare often used in deep learning and have been shown to be hard to verify for\nmodern satisfiability modulo theory (SMT) and integer linear programming (ILP)\nsolvers.\n  The starting point of our approach is the addition of a global linear\napproximation of the overall network behavior to the verification problem that\nhelps with SMT-like reasoning over the network behavior. We present a\nspecialized verification algorithm that employs this approximation in a search\nprocess in which it infers additional node phases for the non-linear nodes in\nthe network from partial node phase assignments, similar to unit propagation in\nclassical SAT solving. We also show how to infer additional conflict clauses\nand safe node fixtures from the results of the analysis steps performed during\nthe search. The resulting approach is evaluated on collision avoidance and\nhandwritten digit recognition case studies.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 09:13:10 GMT"}, {"version": "v2", "created": "Thu, 18 May 2017 07:32:33 GMT"}, {"version": "v3", "created": "Wed, 2 Aug 2017 09:21:18 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Ehlers", "Ruediger", ""]]}, {"id": "1705.01399", "submitter": "Leonardo Anjoletto Ferreira", "authors": "Leonardo A. Ferreira, Reinaldo A. C. Bianchi, Paulo E. Santos, Ramon\n  Lopez de Mantaras", "title": "Answer Set Programming for Non-Stationary Markov Decision Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-stationary domains, where unforeseen changes happen, present a challenge\nfor agents to find an optimal policy for a sequential decision making problem.\nThis work investigates a solution to this problem that combines Markov Decision\nProcesses (MDP) and Reinforcement Learning (RL) with Answer Set Programming\n(ASP) in a method we call ASP(RL). In this method, Answer Set Programming is\nused to find the possible trajectories of an MDP, from where Reinforcement\nLearning is applied to learn the optimal policy of the problem. Results show\nthat ASP(RL) is capable of efficiently finding the optimal solution of an MDP\nrepresenting non-stationary domains.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 13:13:51 GMT"}], "update_date": "2017-05-04", "authors_parsed": [["Ferreira", "Leonardo A.", ""], ["Bianchi", "Reinaldo A. C.", ""], ["Santos", "Paulo E.", ""], ["de Mantaras", "Ramon Lopez", ""]]}, {"id": "1705.01485", "submitter": "Marco Todescato", "authors": "Marco Todescato, Andrea Carron, Ruggero Carli, Gianluigi Pillonetto,\n  Luca Schenato", "title": "Efficient Spatio-Temporal Gaussian Regression via Kalman Filtering", "comments": "26 pages, 12 figures. Submitted to IEEE Transactions on Pattern\n  Analysis and Machine Intelligence", "journal-ref": null, "doi": "10.1016/j.automatica.2020.109032", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we study the non-parametric reconstruction of spatio-temporal\ndynamical Gaussian processes (GPs) via GP regression from sparse and noisy\ndata. GPs have been mainly applied to spatial regression where they represent\none of the most powerful estimation approaches also thanks to their universal\nrepresenting properties. Their extension to dynamical processes has been\ninstead elusive so far since classical implementations lead to unscalable\nalgorithms. We then propose a novel procedure to address this problem by\ncoupling GP regression and Kalman filtering. In particular, assuming space/time\nseparability of the covariance (kernel) of the process and rational time\nspectrum, we build a finite-dimensional discrete-time state-space process\nrepresentation amenable of Kalman filtering. With sampling over a finite set of\nfixed spatial locations, our major finding is that the Kalman filter state at\ninstant $t_k$ represents a sufficient statistic to compute the minimum variance\nestimate of the process at any $t \\geq t_k$ over the entire spatial domain.\nThis result can be interpreted as a novel Kalman representer theorem for\ndynamical GPs. We then extend the study to situations where the set of spatial\ninput locations can vary over time. The proposed algorithms are finally tested\non both synthetic and real field data, also providing comparisons with standard\nGP and truncated GP regression techniques.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 15:49:38 GMT"}], "update_date": "2020-10-06", "authors_parsed": [["Todescato", "Marco", ""], ["Carron", "Andrea", ""], ["Carli", "Ruggero", ""], ["Pillonetto", "Gianluigi", ""], ["Schenato", "Luca", ""]]}, {"id": "1705.01681", "submitter": "Francisco L\\'opez-Ramos", "authors": "Francisco L\\'opez-Ramos, Armando Guarnaschelli, Jos\\'e-Fernando\n  Camacho-Vallejo, Laura Hervert-Escobar, Rosa G. Gonz\\'alez-Ram\\'irez", "title": "Tramp Ship Scheduling Problem with Berth Allocation Considerations and\n  Time-dependent Constraints", "comments": "16 pages, 3 figures, 5 tables, proceedings paper of Mexican\n  International Conference on Artificial Intelligence (MICAI) 2016", "journal-ref": null, "doi": null, "report-no": "Accepted manuscript id 47", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a model for the Tramp Ship Scheduling problem including\nberth allocation considerations, motivated by a real case of a shipping\ncompany. The aim is to determine the travel schedule for each vessel\nconsidering multiple docking and multiple time windows at the berths. This work\nis innovative due to the consideration of both spatial and temporal attributes\nduring the scheduling process. The resulting model is formulated as a\nmixed-integer linear programming problem, and a heuristic method to deal with\nmultiple vessel schedules is also presented. Numerical experimentation is\nperformed to highlight the benefits of the proposed approach and the\napplicability of the heuristic. Conclusions and recommendations for further\nresearch are provided.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 02:49:26 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["L\u00f3pez-Ramos", "Francisco", ""], ["Guarnaschelli", "Armando", ""], ["Camacho-Vallejo", "Jos\u00e9-Fernando", ""], ["Hervert-Escobar", "Laura", ""], ["Gonz\u00e1lez-Ram\u00edrez", "Rosa G.", ""]]}, {"id": "1705.01736", "submitter": "Yu Cheng", "authors": "Yu Cheng, Shaddin Dughmi, David Kempe", "title": "Of the People: Voting Is More Effective with Representative Candidates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In light of the classic impossibility results of Arrow and Gibbard and\nSatterthwaite regarding voting with ordinal rules, there has been recent\ninterest in characterizing how well common voting rules approximate the social\noptimum. In order to quantify the quality of approximation, it is natural to\nconsider the candidates and voters as embedded within a common metric space,\nand to ask how much further the chosen candidate is from the population as\ncompared to the socially optimal one. We use this metric preference model to\nexplore a fundamental and timely question: does the social welfare of a\npopulation improve when candidates are representative of the population? If so,\nthen by how much, and how does the answer depend on the complexity of the\nmetric space?\n  We restrict attention to the most fundamental and common social choice\nsetting: a population of voters, two independently drawn candidates, and a\nmajority rule election. When candidates are not representative of the\npopulation, it is known that the candidate selected by the majority rule can be\nthrice as far from the population as the socially optimal one. We examine how\nthis ratio improves when candidates are drawn independently from the population\nof voters. Our results are two-fold: When the metric is a line, the ratio\nimproves from $3$ to $4-2\\sqrt{2}$, roughly $1.1716$; this bound is tight. When\nthe metric is arbitrary, we show a lower bound of $1.5$ and a constant upper\nbound strictly better than $2$ on the approximation ratio of the majority rule.\n  The positive result depends in part on the assumption that candidates are\nindependent and identically distributed. However, we show that independence\nalone is not enough to achieve the upper bound: even when candidates are drawn\nindependently, if the population of candidates can be different from the\nvoters, then an upper bound of $2$ on the approximation is tight.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 08:30:46 GMT"}, {"version": "v2", "created": "Sat, 26 Aug 2017 09:23:41 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Cheng", "Yu", ""], ["Dughmi", "Shaddin", ""], ["Kempe", "David", ""]]}, {"id": "1705.01813", "submitter": "Cheng-Hao Deng", "authors": "Cheng-Hao Deng and Wan-Lei Zhao", "title": "Fast k-means based on KNN Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the era of big data, k-means clustering has been widely adopted as a basic\nprocessing tool in various contexts. However, its computational cost could be\nprohibitively high as the data size and the cluster number are large. It is\nwell known that the processing bottleneck of k-means lies in the operation of\nseeking closest centroid in each iteration. In this paper, a novel solution\ntowards the scalability issue of k-means is presented. In the proposal, k-means\nis supported by an approximate k-nearest neighbors graph. In the k-means\niteration, each data sample is only compared to clusters that its nearest\nneighbors reside. Since the number of nearest neighbors we consider is much\nless than k, the processing cost in this step becomes minor and irrelevant to\nk. The processing bottleneck is therefore overcome. The most interesting thing\nis that k-nearest neighbor graph is constructed by iteratively calling the fast\n$k$-means itself. Comparing with existing fast k-means variants, the proposed\nalgorithm achieves hundreds to thousands times speed-up while maintaining high\nclustering quality. As it is tested on 10 million 512-dimensional data, it\ntakes only 5.2 hours to produce 1 million clusters. In contrast, to fulfill the\nsame scale of clustering, it would take 3 years for traditional k-means.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 12:27:28 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Deng", "Cheng-Hao", ""], ["Zhao", "Wan-Lei", ""]]}, {"id": "1705.01817", "submitter": "Christoph Schwering", "authors": "Christoph Schwering", "title": "A Reasoning System for a First-Order Logic of Limited Belief", "comments": "22 pages, 0 figures, Twenty-sixth International Joint Conference on\n  Artificial Intelligence (IJCAI-17)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logics of limited belief aim at enabling computationally feasible reasoning\nin highly expressive representation languages. These languages are often\ndialects of first-order logic with a weaker form of logical entailment that\nkeeps reasoning decidable or even tractable. While a number of such logics have\nbeen proposed in the past, they tend to remain for theoretical analysis only\nand their practical relevance is very limited. In this paper, we aim to go\nbeyond the theory. Building on earlier work by Liu, Lakemeyer, and Levesque, we\ndevelop a logic of limited belief that is highly expressive while remaining\ndecidable in the first-order and tractable in the propositional case and\nexhibits some characteristics that make it attractive for an implementation. We\nintroduce a reasoning system that employs this logic as representation language\nand present experimental results that showcase the benefit of limited belief.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 12:39:27 GMT"}], "update_date": "2017-05-05", "authors_parsed": [["Schwering", "Christoph", ""]]}, {"id": "1705.01968", "submitter": "Josua Krause", "authors": "Josua Krause, Aritra Dasgupta, Jordan Swartz, Yindalon\n  Aphinyanaphongs, Enrico Bertini", "title": "A Workflow for Visual Diagnostics of Binary Classifiers using\n  Instance-Level Explanations", "comments": "Published at IEEE Conference on Visual Analytics Science and\n  Technology (IEEE VAST 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human-in-the-loop data analysis applications necessitate greater transparency\nin machine learning models for experts to understand and trust their decisions.\nTo this end, we propose a visual analytics workflow to help data scientists and\ndomain experts explore, diagnose, and understand the decisions made by a binary\nclassifier. The approach leverages \"instance-level explanations\", measures of\nlocal feature relevance that explain single instances, and uses them to build a\nset of visual representations that guide the users in their investigation. The\nworkflow is based on three main visual representations and steps: one based on\naggregate statistics to see how data distributes across correct / incorrect\ndecisions; one based on explanations to understand which features are used to\nmake these decisions; and one based on raw data, to derive insights on\npotential root causes for the observed patterns. The workflow is derived from a\nlong-term collaboration with a group of machine learning and healthcare\nprofessionals who used our method to make sense of machine learning models they\ndeveloped. The case study from this collaboration demonstrates that the\nproposed workflow helps experts derive useful knowledge about the model and the\nphenomena it describes, thus experts can generate useful hypotheses on how a\nmodel can be improved.\n", "versions": [{"version": "v1", "created": "Thu, 4 May 2017 18:24:38 GMT"}, {"version": "v2", "created": "Fri, 21 Jul 2017 17:34:37 GMT"}, {"version": "v3", "created": "Sun, 1 Oct 2017 22:24:17 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Krause", "Josua", ""], ["Dasgupta", "Aritra", ""], ["Swartz", "Jordan", ""], ["Aphinyanaphongs", "Yindalon", ""], ["Bertini", "Enrico", ""]]}, {"id": "1705.02175", "submitter": "Nikos Katzouris", "authors": "Nikos Katzouris, Alexander Artikis, Georgios Paliouras", "title": "Distributed Online Learning of Event Definitions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logic-based event recognition systems infer occurrences of events in time\nusing a set of event definitions in the form of first-order rules. The Event\nCalculus is a temporal logic that has been used as a basis in event recognition\napplications, providing among others, direct connections to machine learning,\nvia Inductive Logic Programming (ILP). OLED is a recently proposed ILP system\nthat learns event definitions in the form of Event Calculus theories, in a\nsingle pass over a data stream. In this work we present a version of OLED that\nallows for distributed, online learning. We evaluate our approach on a\nbenchmark activity recognition dataset and show that we can significantly\nreduce training times, exchanging minimal information between processing nodes.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 11:40:11 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Katzouris", "Nikos", ""], ["Artikis", "Alexander", ""], ["Paliouras", "Georgios", ""]]}, {"id": "1705.02210", "submitter": "Cheng-Hao Cai", "authors": "Cheng-Hao Cai", "title": "SLDR-DL: A Framework for SLD-Resolution with Deep Learning", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an SLD-resolution technique based on deep learning.\nThis technique enables neural networks to learn from old and successful\nresolution processes and to use learnt experiences to guide new resolution\nprocesses. An implementation of this technique is named SLDR-DL. It includes a\nProlog library of deep feedforward neural networks and some essential functions\nof resolution. In the SLDR-DL framework, users can define logical rules in the\nform of definite clauses and teach neural networks to use the rules in\nreasoning processes.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 13:32:54 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Cai", "Cheng-Hao", ""]]}, {"id": "1705.02212", "submitter": "Michel Besserve", "authors": "Michel Besserve, Naji Shajarisales, Bernhard Sch\\\"olkopf and Dominik\n  Janzing", "title": "Group invariance principles for causal generative models", "comments": "16 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The postulate of independence of cause and mechanism (ICM) has recently led\nto several new causal discovery algorithms. The interpretation of independence\nand the way it is utilized, however, varies across these methods. Our aim in\nthis paper is to propose a group theoretic framework for ICM to unify and\ngeneralize these approaches. In our setting, the cause-mechanism relationship\nis assessed by comparing it against a null hypothesis through the application\nof random generic group transformations. We show that the group theoretic view\nprovides a very general tool to study the structure of data generating\nmechanisms with direct applications to machine learning.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 13:34:16 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Besserve", "Michel", ""], ["Shajarisales", "Naji", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Janzing", "Dominik", ""]]}, {"id": "1705.02245", "submitter": "Neil Lawrence", "authors": "Neil D. Lawrence", "title": "Data Readiness Levels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Application of models to data is fraught. Data-generating collaborators often\nonly have a very basic understanding of the complications of collating,\nprocessing and curating data. Challenges include: poor data collection\npractices, missing values, inconvenient storage mechanisms, intellectual\nproperty, security and privacy. All these aspects obstruct the sharing and\ninterconnection of data, and the eventual interpretation of data through\nmachine learning or other approaches. In project reporting, a major challenge\nis in encapsulating these problems and enabling goals to be built around the\nprocessing of data. Project overruns can occur due to failure to account for\nthe amount of time required to curate and collate. But to understand these\nfailures we need to have a common language for assessing the readiness of a\nparticular data set. This position paper proposes the use of data readiness\nlevels: it gives a rough outline of three stages of data preparedness and\nspeculates on how formalisation of these levels into a common language for data\nreadiness could facilitate project management.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 14:53:56 GMT"}], "update_date": "2017-05-08", "authors_parsed": [["Lawrence", "Neil D.", ""]]}, {"id": "1705.02426", "submitter": "Hanxiao Liu", "authors": "Hanxiao Liu, Yuexin Wu, Yiming Yang", "title": "Analogical Inference for Multi-Relational Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale multi-relational embedding refers to the task of learning the\nlatent representations for entities and relations in large knowledge graphs. An\neffective and scalable solution for this problem is crucial for the true\nsuccess of knowledge-based inference in a broad range of applications. This\npaper proposes a novel framework for optimizing the latent representations with\nrespect to the \\textit{analogical} properties of the embedded entities and\nrelations. By formulating the learning objective in a differentiable fashion,\nour model enjoys both theoretical power and computational scalability, and\nsignificantly outperformed a large number of representative baseline methods on\nbenchmark datasets. Furthermore, the model offers an elegant unification of\nseveral well-known methods in multi-relational embedding, which can be proven\nto be special instantiations of our framework.\n", "versions": [{"version": "v1", "created": "Sat, 6 May 2017 01:40:28 GMT"}, {"version": "v2", "created": "Thu, 6 Jul 2017 16:58:24 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Liu", "Hanxiao", ""], ["Wu", "Yuexin", ""], ["Yang", "Yiming", ""]]}, {"id": "1705.02476", "submitter": "Mahardhika Pratama Dr", "authors": "Mahardhika Pratama", "title": "PANFIS++: A Generalized Approach to Evolving Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of evolving intelligent system (EIS) provides an effective avenue\nfor data stream mining because it is capable of coping with two prominent\nissues: online learning and rapidly changing environments. We note at least\nthree uncharted territories of existing EISs: data uncertainty, temporal system\ndynamic, redundant data streams. This book chapter aims at delivering a\nconcrete solution of this problem with the algorithmic development of a novel\nlearning algorithm, namely PANFIS++. PANFIS++ is a generalized version of the\nPANFIS by putting forward three important components: 1) An online active\nlearning scenario is developed to overcome redundant data streams. This module\nallows to actively select data streams for the training process, thereby\nexpediting execution time and enhancing generalization performance, 2) PANFIS++\nis built upon an interval type-2 fuzzy system environment, which incorporates\nthe so-called footprint of uncertainty. This component provides a degree of\ntolerance for data uncertainty. 3) PANFIS++ is structured under a recurrent\nnetwork architecture with a self-feedback loop. This is meant to tackle the\ntemporal system dynamic. The efficacy of the PANFIS++ has been numerically\nvalidated through numerous real-world and synthetic case studies, where it\ndelivers the highest predictive accuracy while retaining the lowest complexity.\n", "versions": [{"version": "v1", "created": "Sat, 6 May 2017 12:02:15 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Pratama", "Mahardhika", ""]]}, {"id": "1705.02477", "submitter": "Mahardhika Pratama Dr", "authors": "Mahardhika Pratama, Eric Dimla, Chow Yin Lai, Edwin Lughofer", "title": "Metacognitive Learning Approach for Online Tool Condition Monitoring", "comments": null, "journal-ref": null, "doi": "10.1007/s10845-017-1348-9", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As manufacturing processes become increasingly automated, so should tool\ncondition monitoring (TCM) as it is impractical to have human workers monitor\nthe state of the tools continuously. Tool condition is crucial to ensure the\ngood quality of products: Worn tools affect not only the surface quality but\nalso the dimensional accuracy, which means higher reject rate of the products.\nTherefore, there is an urgent need to identify tool failures before it occurs\non the fly. While various versions of intelligent tool condition monitoring\nhave been proposed, most of them suffer from a cognitive nature of traditional\nmachine learning algorithms. They focus on the how to learn process without\npaying attention to other two crucial issues: what to learn, and when to learn.\nThe what to learn and the when to learn provide self regulating mechanisms to\nselect the training samples and to determine time instants to train a model. A\nnovel tool condition monitoring approach based on a psychologically plausible\nconcept, namely the metacognitive scaffolding theory, is proposed and built\nupon a recently published algorithm, recurrent classifier (rClass). The\nlearning process consists of three phases: what to learn, how to learn, when to\nlearn and makes use of a generalized recurrent network structure as a cognitive\ncomponent. Experimental studies with real-world manufacturing data streams were\nconducted where rClass demonstrated the highest accuracy while retaining the\nlowest complexity over its counterparts.\n", "versions": [{"version": "v1", "created": "Sat, 6 May 2017 12:16:16 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Pratama", "Mahardhika", ""], ["Dimla", "Eric", ""], ["Lai", "Chow Yin", ""], ["Lughofer", "Edwin", ""]]}, {"id": "1705.02518", "submitter": "Subhabrata Mukherjee", "authors": "Subhabrata Mukherjee, Kashyap Popat, Gerhard Weikum", "title": "Exploring Latent Semantic Factors to Find Useful Product Reviews", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online reviews provided by consumers are a valuable asset for e-Commerce\nplatforms, influencing potential consumers in making purchasing decisions.\nHowever, these reviews are of varying quality, with the useful ones buried deep\nwithin a heap of non-informative reviews. In this work, we attempt to\nautomatically identify review quality in terms of its helpfulness to the end\nconsumers. In contrast to previous works in this domain exploiting a variety of\nsyntactic and community-level features, we delve deep into the semantics of\nreviews as to what makes them useful, providing interpretable explanation for\nthe same. We identify a set of consistency and semantic factors, all from the\ntext, ratings, and timestamps of user-generated reviews, making our approach\ngeneralizable across all communities and domains. We explore review semantics\nin terms of several latent factors like the expertise of its author, his\njudgment about the fine-grained facets of the underlying product, and his\nwriting style. These are cast into a Hidden Markov Model -- Latent Dirichlet\nAllocation (HMM-LDA) based model to jointly infer: (i) reviewer expertise, (ii)\nitem facets, and (iii) review helpfulness. Large-scale experiments on five\nreal-world datasets from Amazon show significant improvement over\nstate-of-the-art baselines in predicting and ranking useful reviews.\n", "versions": [{"version": "v1", "created": "Sat, 6 May 2017 19:21:48 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Mukherjee", "Subhabrata", ""], ["Popat", "Kashyap", ""], ["Weikum", "Gerhard", ""]]}, {"id": "1705.02519", "submitter": "Subhabrata Mukherjee", "authors": "Subhabrata Mukherjee, Hemank Lamba, Gerhard Weikum", "title": "Item Recommendation with Evolving User Preferences and Experience", "comments": null, "journal-ref": null, "doi": "10.1109/ICDM.2015.111", "report-no": null, "categories": "cs.AI cs.CL cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current recommender systems exploit user and item similarities by\ncollaborative filtering. Some advanced methods also consider the temporal\nevolution of item ratings as a global background process. However, all prior\nmethods disregard the individual evolution of a user's experience level and how\nthis is expressed in the user's writing in a review community. In this paper,\nwe model the joint evolution of user experience, interest in specific item\nfacets, writing style, and rating behavior. This way we can generate individual\nrecommendations that take into account the user's maturity level (e.g.,\nrecommending art movies rather than blockbusters for a cinematography expert).\nAs only item ratings and review texts are observables, we capture the user's\nexperience and interests in a latent model learned from her reviews, vocabulary\nand writing style. We develop a generative HMM-LDA model to trace user\nevolution, where the Hidden Markov Model (HMM) traces her latent experience\nprogressing over time -- with solely user reviews and ratings as observables\nover time. The facets of a user's interest are drawn from a Latent Dirichlet\nAllocation (LDA) model derived from her reviews, as a function of her (again\nlatent) experience level. In experiments with five real-world datasets, we show\nthat our model improves the rating prediction over state-of-the-art baselines,\nby a substantial margin. We also show, in a use-case study, that our model\nperforms well in the assessment of user experience levels.\n", "versions": [{"version": "v1", "created": "Sat, 6 May 2017 19:22:41 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Mukherjee", "Subhabrata", ""], ["Lamba", "Hemank", ""], ["Weikum", "Gerhard", ""]]}, {"id": "1705.02522", "submitter": "Subhabrata Mukherjee", "authors": "Subhabrata Mukherjee, Gerhard Weikum, Cristian Danescu-Niculescu-Mizil", "title": "People on Drugs: Credibility of User Statements in Health Communities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online health communities are a valuable source of information for patients\nand physicians. However, such user-generated resources are often plagued by\ninaccuracies and misinformation. In this work we propose a method for\nautomatically establishing the credibility of user-generated medical statements\nand the trustworthiness of their authors by exploiting linguistic cues and\ndistant supervision from expert sources. To this end we introduce a\nprobabilistic graphical model that jointly learns user trustworthiness,\nstatement credibility, and language objectivity. We apply this methodology to\nthe task of extracting rare or unknown side-effects of medical drugs --- this\nbeing one of the problems where large scale non-expert data has the potential\nto complement expert medical knowledge. We show that our method can reliably\nextract side-effects and filter out false statements, while identifying\ntrustworthy users that are likely to contribute valuable medical information.\n", "versions": [{"version": "v1", "created": "Sat, 6 May 2017 19:38:33 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Mukherjee", "Subhabrata", ""], ["Weikum", "Gerhard", ""], ["Danescu-Niculescu-Mizil", "Cristian", ""]]}, {"id": "1705.02553", "submitter": "Kamyar Azizzadenesheli Ph.D.", "authors": "Kamyar Azizzadenesheli, Alessandro Lazaric, Animashree Anandkumar", "title": "Experimental results : Reinforcement Learning of POMDPs using Spectral\n  Methods", "comments": "30th Conference on Neural Information Processing Systems (NIPS 2016),\n  Barcelona, Spain", "journal-ref": "NIPS-DeepRL-Workshop-2016Barcelona", "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new reinforcement learning algorithm for partially observable\nMarkov decision processes (POMDP) based on spectral decomposition methods.\nWhile spectral methods have been previously employed for consistent learning of\n(passive) latent variable models such as hidden Markov models, POMDPs are more\nchallenging since the learner interacts with the environment and possibly\nchanges the future observations in the process. We devise a learning algorithm\nrunning through epochs, in each epoch we employ spectral techniques to learn\nthe POMDP parameters from a trajectory generated by a fixed policy. At the end\nof the epoch, an optimization oracle returns the optimal memoryless planning\npolicy which maximizes the expected reward based on the estimated POMDP model.\nWe prove an order-optimal regret bound with respect to the optimal memoryless\npolicy and efficient scaling with respect to the dimensionality of observation\nand action spaces.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 02:49:10 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Azizzadenesheli", "Kamyar", ""], ["Lazaric", "Alessandro", ""], ["Anandkumar", "Animashree", ""]]}, {"id": "1705.02620", "submitter": "Wen Jiang", "authors": "Dong Wu, Xiang Liu, Feng Xue, Hanqing Zheng, Yehang Shou, Wen Jiang", "title": "A New Medical Diagnosis Method Based on Z-Numbers", "comments": "24 pages, 9 figures, 13 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to handle uncertainty in medical diagnosis is an open issue. In this\npaper, a new decision making methodology based on Z-numbers is presented.\nFirstly, the experts' opinions are represented by Z-numbers. Z-number is an\nordered pair of fuzzy numbers denoted as Z = (A, B). Then, a new method for\nranking fuzzy numbers is proposed. And based on the proposed fuzzy number\nranking method, a novel method is presented to transform the Z-numbers into\nBasic Probability Assignment (BPA). As a result, the information from different\nsources is combined by the Dempster' combination rule. The final decision\nmaking is more reasonable due to the advantage of information fusion. Finally,\ntwo experiments, risk analysis and medical diagnosis, are illustrated to show\nthe efficiency of the proposed methodology.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 13:29:53 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Wu", "Dong", ""], ["Liu", "Xiang", ""], ["Xue", "Feng", ""], ["Zheng", "Hanqing", ""], ["Shou", "Yehang", ""], ["Jiang", "Wen", ""]]}, {"id": "1705.02636", "submitter": "Xiang Jiang", "authors": "Xiang Jiang, Erico N de Souza, Ahmad Pesaranghader, Baifan Hu, Daniel\n  L. Silver and Stan Matwin", "title": "TrajectoryNet: An Embedded GPS Trajectory Representation for Point-based\n  Classification Using Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding and discovering knowledge from GPS (Global Positioning System)\ntraces of human activities is an essential topic in mobility-based urban\ncomputing. We propose TrajectoryNet-a neural network architecture for\npoint-based trajectory classification to infer real world human transportation\nmodes from GPS traces. To overcome the challenge of capturing the underlying\nlatent factors in the low-dimensional and heterogeneous feature space imposed\nby GPS data, we develop a novel representation that embeds the original feature\nspace into another space that can be understood as a form of basis expansion.\nWe also enrich the feature space via segment-based information and use Maxout\nactivations to improve the predictive power of Recurrent Neural Networks\n(RNNs). We achieve over 98% classification accuracy when detecting four types\nof transportation modes, outperforming existing models without additional\nsensory data or location-based prior knowledge.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 15:40:08 GMT"}, {"version": "v2", "created": "Wed, 30 Aug 2017 15:06:43 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Jiang", "Xiang", ""], ["de Souza", "Erico N", ""], ["Pesaranghader", "Ahmad", ""], ["Hu", "Baifan", ""], ["Silver", "Daniel L.", ""], ["Matwin", "Stan", ""]]}, {"id": "1705.02667", "submitter": "Subhabrata Mukherjee", "authors": "Subhabrata Mukherjee, Gerhard Weikum", "title": "People on Media: Jointly Identifying Credible News and Trustworthy\n  Citizen Journalists in Online Communities", "comments": null, "journal-ref": null, "doi": "10.1145/2806416.2806537", "report-no": null, "categories": "cs.AI cs.CL cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Media seems to have become more partisan, often providing a biased coverage\nof news catering to the interest of specific groups. It is therefore essential\nto identify credible information content that provides an objective narrative\nof an event. News communities such as digg, reddit, or newstrust offer\nrecommendations, reviews, quality ratings, and further insights on journalistic\nworks. However, there is a complex interaction between different factors in\nsuch online communities: fairness and style of reporting, language clarity and\nobjectivity, topical perspectives (like political viewpoint), expertise and\nbias of community members, and more. This paper presents a model to\nsystematically analyze the different interactions in a news community between\nusers, news, and sources. We develop a probabilistic graphical model that\nleverages this joint interaction to identify 1) highly credible news articles,\n2) trustworthy news sources, and 3) expert users who perform the role of\n\"citizen journalists\" in the community. Our method extends CRF models to\nincorporate real-valued ratings, as some communities have very fine-grained\nscales that cannot be easily discretized without losing information. To the\nbest of our knowledge, this paper is the first full-fledged analysis of\ncredibility, trust, and expertise in news communities.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 17:41:31 GMT"}, {"version": "v2", "created": "Tue, 9 May 2017 16:40:16 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Mukherjee", "Subhabrata", ""], ["Weikum", "Gerhard", ""]]}, {"id": "1705.02668", "submitter": "Subhabrata Mukherjee", "authors": "Subhabrata Mukherjee, Sourav Dutta, Gerhard Weikum", "title": "Credible Review Detection with Limited Information using Consistency\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online reviews provide viewpoints on the strengths and shortcomings of\nproducts/services, influencing potential customers' purchasing decisions.\nHowever, the proliferation of non-credible reviews -- either fake (promoting/\ndemoting an item), incompetent (involving irrelevant aspects), or biased --\nentails the problem of identifying credible reviews. Prior works involve\nclassifiers harnessing rich information about items/users -- which might not be\nreadily available in several domains -- that provide only limited\ninterpretability as to why a review is deemed non-credible. This paper presents\na novel approach to address the above issues. We utilize latent topic models\nleveraging review texts, item ratings, and timestamps to derive consistency\nfeatures without relying on item/user histories, unavailable for \"long-tail\"\nitems/users. We develop models, for computing review credibility scores to\nprovide interpretable evidence for non-credible reviews, that are also\ntransferable to other domains -- addressing the scarcity of labeled data.\nExperiments on real-world datasets demonstrate improvements over\nstate-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 17:43:01 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Mukherjee", "Subhabrata", ""], ["Dutta", "Sourav", ""], ["Weikum", "Gerhard", ""]]}, {"id": "1705.02669", "submitter": "Subhabrata Mukherjee", "authors": "Subhabrata Mukherjee, Stephan Guennemann, Gerhard Weikum", "title": "Item Recommendation with Continuous Experience Evolution of Users using\n  Brownian Motion", "comments": null, "journal-ref": null, "doi": "10.1145/2939672.2939780", "report-no": null, "categories": "cs.AI cs.CL cs.IR cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online review communities are dynamic as users join and leave, adopt new\nvocabulary, and adapt to evolving trends. Recent work has shown that\nrecommender systems benefit from explicit consideration of user experience.\nHowever, prior work assumes a fixed number of discrete experience levels,\nwhereas in reality users gain experience and mature continuously over time.\nThis paper presents a new model that captures the continuous evolution of user\nexperience, and the resulting language model in reviews and other posts. Our\nmodel is unsupervised and combines principles of Geometric Brownian Motion,\nBrownian Motion, and Latent Dirichlet Allocation to trace a smooth temporal\nprogression of user experience and language model respectively. We develop\npractical algorithms for estimating the model parameters from data and for\ninference with our model (e.g., to recommend items). Extensive experiments with\nfive real-world datasets show that our model not only fits data better than\ndiscrete-model baselines, but also outperforms state-of-the-art methods for\npredicting item ratings.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 17:46:43 GMT"}, {"version": "v2", "created": "Tue, 9 May 2017 06:47:51 GMT"}, {"version": "v3", "created": "Wed, 9 Aug 2017 17:56:00 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Mukherjee", "Subhabrata", ""], ["Guennemann", "Stephan", ""], ["Weikum", "Gerhard", ""]]}, {"id": "1705.02670", "submitter": "Jessica Hamrick", "authors": "Jessica B. Hamrick, Andrew J. Ballard, Razvan Pascanu, Oriol Vinyals,\n  Nicolas Heess, Peter W. Battaglia", "title": "Metacontrol for Adaptive Imagination-Based Optimization", "comments": "Published as a conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many machine learning systems are built to solve the hardest examples of a\nparticular task, which often makes them large and expensive to run---especially\nwith respect to the easier examples, which might require much less computation.\nFor an agent with a limited computational budget, this \"one-size-fits-all\"\napproach may result in the agent wasting valuable computation on easy examples,\nwhile not spending enough on hard examples. Rather than learning a single,\nfixed policy for solving all instances of a task, we introduce a metacontroller\nwhich learns to optimize a sequence of \"imagined\" internal simulations over\npredictive models of the world in order to construct a more informed, and more\neconomical, solution. The metacontroller component is a model-free\nreinforcement learning agent, which decides both how many iterations of the\noptimization procedure to run, as well as which model to consult on each\niteration. The models (which we call \"experts\") can be state transition models,\naction-value functions, or any other mechanism that provides information useful\nfor solving the task, and can be learned on-policy or off-policy in parallel\nwith the metacontroller. When the metacontroller, controller, and experts were\ntrained with \"interaction networks\" (Battaglia et al., 2016) as expert models,\nour approach was able to solve a challenging decision-making problem under\ncomplex non-linear dynamics. The metacontroller learned to adapt the amount of\ncomputation it performed to the difficulty of the task, and learned how to\nchoose which experts to consult by factoring in both their reliability and\nindividual computational resource costs. This allowed the metacontroller to\nachieve a lower overall cost (task loss plus computational cost) than more\ntraditional fixed policy approaches. These results demonstrate that our\napproach is a powerful framework for using...\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 17:48:14 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Hamrick", "Jessica B.", ""], ["Ballard", "Andrew J.", ""], ["Pascanu", "Razvan", ""], ["Vinyals", "Oriol", ""], ["Heess", "Nicolas", ""], ["Battaglia", "Peter W.", ""]]}, {"id": "1705.02687", "submitter": "Seyed Sajjadi", "authors": "Seyed Sajjadi, Bruce Shapiro, Christopher McKinlay, Allen Sarkisyan,\n  Carol Shubin, Efunwande Osoba", "title": "Finding Bottlenecks: Predicting Student Attrition with Unsupervised\n  Classifier", "comments": "7 pages, 10 figures, Finding Bottlenecks: Predicting Student\n  Attrition with Unsupervised Classifiers, IEEE, IntelliSys 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CY cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With pressure to increase graduation rates and reduce time to degree in\nhigher education, it is important to identify at-risk students early. Automated\nearly warning systems are therefore highly desirable. In this paper, we use\nunsupervised clustering techniques to predict the graduation status of declared\nmajors in five departments at California State University Northridge (CSUN),\nbased on a minimal number of lower division courses in each major. In addition,\nwe use the detected clusters to identify hidden bottleneck courses.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 19:45:49 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Sajjadi", "Seyed", ""], ["Shapiro", "Bruce", ""], ["McKinlay", "Christopher", ""], ["Sarkisyan", "Allen", ""], ["Shubin", "Carol", ""], ["Osoba", "Efunwande", ""]]}, {"id": "1705.02689", "submitter": "Seyed Sajjadi", "authors": "Seyed A Sajjadi, Danial Moazen, Ani Nahapetian", "title": "AirDraw: Leveraging Smart Watch Motion Sensors for Mobile Human Computer\n  Interactions", "comments": "6 pages, AirDraw, Leveraging Smart Watch Motion Sensors for Mobile\n  Human Computer Interactions : IEEE, CCNC 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wearable computing is one of the fastest growing technologies today. Smart\nwatches are poised to take over at least of half the wearable devices market in\nthe near future. Smart watch screen size, however, is a limiting factor for\ngrowth, as it restricts practical text input. On the other hand, wearable\ndevices have some features, such as consistent user interaction and hands-free,\nheads-up operations, which pave the way for gesture recognition methods of text\nentry. This paper proposes a new text input method for smart watches, which\nutilizes motion sensor data and machine learning approaches to detect letters\nwritten in the air by a user. This method is less computationally intensive and\nless expensive when compared to computer vision approaches. It is also not\naffected by lighting factors, which limit computer vision solutions. The\nAirDraw system prototype developed to test this approach is presented.\nAdditionally, experimental results close to 71% accuracy are presented.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 19:58:54 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Sajjadi", "Seyed A", ""], ["Moazen", "Danial", ""], ["Nahapetian", "Ani", ""]]}, {"id": "1705.02694", "submitter": "Amol Patwardhan", "authors": "Amol S Patwardhan and Gerald M Knapp", "title": "Multimodal Affect Analysis for Product Feedback Assessment", "comments": "10 pages, ISERC 2013, IIE Annual Conference. Proceedings. Institute\n  of Industrial Engineers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consumers often react expressively to products such as food samples, perfume,\njewelry, sunglasses, and clothing accessories. This research discusses a\nmultimodal affect recognition system developed to classify whether a consumer\nlikes or dislikes a product tested at a counter or kiosk, by analyzing the\nconsumer's facial expression, body posture, hand gestures, and voice after\ntesting the product. A depth-capable camera and microphone system - Kinect for\nWindows - is utilized. An emotion identification engine has been developed to\nanalyze the images and voice to determine affective state of the customer. The\nimage is segmented using skin color and adaptive threshold. Face, body and\nhands are detected using the Haar cascade classifier. Canny edges are\nidentified and the lip, body and hand contours are extracted using spatial\nfiltering. Edge count and orientation around the mouth, cheeks, eyes,\nshoulders, fingers and the location of the edges are used as features.\nClassification is done by an emotion template mapping algorithm and training a\nclassifier using support vector machines. The real-time performance, accuracy\nand feasibility for multimodal affect recognition in feedback assessment are\nevaluated.\n", "versions": [{"version": "v1", "created": "Sun, 7 May 2017 20:39:35 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Patwardhan", "Amol S", ""], ["Knapp", "Gerald M", ""]]}, {"id": "1705.02772", "submitter": "Toshiki Nakamura", "authors": "Toshiki Nakamura, Anna Zhu, Keiji Yanai and Seiichi Uchida", "title": "Scene Text Eraser", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The character information in natural scene images contains various personal\ninformation, such as telephone numbers, home addresses, etc. It is a high risk\nof leakage the information if they are published. In this paper, we proposed a\nscene text erasing method to properly hide the information via an inpainting\nconvolutional neural network (CNN) model. The input is a scene text image, and\nthe output is expected to be text erased image with all the character regions\nfilled up the colors of the surrounding background pixels. This work is\naccomplished by a CNN model through convolution to deconvolution with\ninterconnection process. The training samples and the corresponding inpainting\nimages are considered as teaching signals for training. To evaluate the text\nerasing performance, the output images are detected by a novel scene text\ndetection method. Subsequently, the same measurement on text detection is\nutilized for testing the images in benchmark dataset ICDAR2013. Compared with\ndirect text detection way, the scene text erasing process demonstrates a\ndrastically decrease on the precision, recall and f-score. That proves the\neffectiveness of proposed method for erasing the text in natural scene images.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 08:28:34 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Nakamura", "Toshiki", ""], ["Zhu", "Anna", ""], ["Yanai", "Keiji", ""], ["Uchida", "Seiichi", ""]]}, {"id": "1705.02843", "submitter": "Alex Fukunaga", "authors": "Satoru Horie, Alex Fukunaga", "title": "Block-Parallel IDA* for GPUs (Extended Manuscript)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate GPU-based parallelization of Iterative-Deepening A* (IDA*). We\nshow that straightforward thread-based parallelization techniques which were\npreviously proposed for massively parallel SIMD processors perform poorly due\nto warp divergence and load imbalance. We propose Block-Parallel IDA* (BPIDA*),\nwhich assigns the search of a subtree to a block (a group of threads with\naccess to fast shared memory) rather than a thread. On the 15-puzzle, BPIDA* on\na NVIDIA GRID K520 with 1536 CUDA cores achieves a speedup of 4.98 compared to\na highly optimized sequential IDA* implementation on a Xeon E5-2670 core.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 12:11:36 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Horie", "Satoru", ""], ["Fukunaga", "Alex", ""]]}, {"id": "1705.02894", "submitter": "Jong Chul Ye", "authors": "Jae Hyun Lim and Jong Chul Ye", "title": "Geometric GAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cond-mat.dis-nn cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Nets (GANs) represent an important milestone for\neffective generative models, which has inspired numerous variants seemingly\ndifferent from each other. One of the main contributions of this paper is to\nreveal a unified geometric structure in GAN and its variants. Specifically, we\nshow that the adversarial generative model training can be decomposed into\nthree geometric steps: separating hyperplane search, discriminator parameter\nupdate away from the separating hyperplane, and the generator update along the\nnormal vector direction of the separating hyperplane. This geometric intuition\nreveals the limitations of the existing approaches and leads us to propose a\nnew formulation called geometric GAN using SVM separating hyperplane that\nmaximizes the margin. Our theoretical analysis shows that the geometric GAN\nconverges to a Nash equilibrium between the discriminator and generator. In\naddition, extensive numerical results show that the superior performance of\ngeometric GAN.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 14:32:33 GMT"}, {"version": "v2", "created": "Tue, 9 May 2017 01:12:28 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Lim", "Jae Hyun", ""], ["Ye", "Jong Chul", ""]]}, {"id": "1705.02908", "submitter": "Yangqiu Song", "authors": "Yangqiu Song and Dan Roth", "title": "Machine Learning with World Knowledge: The Position and Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning has become pervasive in multiple domains, impacting a wide\nvariety of applications, such as knowledge discovery and data mining, natural\nlanguage processing, information retrieval, computer vision, social and health\ninformatics, ubiquitous computing, etc. Two essential problems of machine\nlearning are how to generate features and how to acquire labels for machines to\nlearn. Particularly, labeling large amount of data for each domain-specific\nproblem can be very time consuming and costly. It has become a key obstacle in\nmaking learning protocols realistic in applications. In this paper, we will\ndiscuss how to use the existing general-purpose world knowledge to enhance\nmachine learning processes, by enriching the features or reducing the labeling\nwork. We start from the comparison of world knowledge with domain-specific\nknowledge, and then introduce three key problems in using world knowledge in\nlearning processes, i.e., explicit and implicit feature representation,\ninference for knowledge linking and disambiguation, and learning with direct or\nindirect supervision. Finally we discuss the future directions of this research\ntopic.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 15:06:32 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Song", "Yangqiu", ""], ["Roth", "Dan", ""]]}, {"id": "1705.02955", "submitter": "Noam Brown", "authors": "Noam Brown and Tuomas Sandholm", "title": "Safe and Nested Subgame Solving for Imperfect-Information Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In imperfect-information games, the optimal strategy in a subgame may depend\non the strategy in other, unreached subgames. Thus a subgame cannot be solved\nin isolation and must instead consider the strategy for the entire game as a\nwhole, unlike perfect-information games. Nevertheless, it is possible to first\napproximate a solution for the whole game and then improve it by solving\nindividual subgames. This is referred to as subgame solving. We introduce\nsubgame-solving techniques that outperform prior methods both in theory and\npractice. We also show how to adapt them, and past subgame-solving techniques,\nto respond to opponent actions that are outside the original action\nabstraction; this significantly outperforms the prior state-of-the-art\napproach, action translation. Finally, we show that subgame solving can be\nrepeated as the game progresses down the game tree, leading to far lower\nexploitability. These techniques were a key component of Libratus, the first AI\nto defeat top humans in heads-up no-limit Texas hold'em poker.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 16:27:48 GMT"}, {"version": "v2", "created": "Fri, 26 May 2017 21:44:39 GMT"}, {"version": "v3", "created": "Thu, 16 Nov 2017 23:12:15 GMT"}], "update_date": "2017-11-20", "authors_parsed": [["Brown", "Noam", ""], ["Sandholm", "Tuomas", ""]]}, {"id": "1705.03078", "submitter": "Toby Pereira", "authors": "Toby Pereira", "title": "An Anthropic Argument against the Future Existence of Superintelligent\n  Artificial Intelligence", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper uses anthropic reasoning to argue for a reduced likelihood that\nsuperintelligent AI will come into existence in the future. To make this\nargument, a new principle is introduced: the Super-Strong Self-Sampling\nAssumption (SSSSA), building on the Self-Sampling Assumption (SSA) and the\nStrong Self-Sampling Assumption (SSSA). SSA uses as its sample the relevant\nobservers, whereas SSSA goes further by using observer-moments. SSSSA goes\nfurther still and weights each sample proportionally, according to the size of\na mind in cognitive terms. SSSSA is required for human observer-samples to be\ntypical, given by how much non-human animals outnumber humans. Given SSSSA, the\nassumption that humans experience typical observer-samples relies on a future\nwhere superintelligent AI does not dominate, which in turn reduces the\nlikelihood of it being created at all.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 20:37:45 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Pereira", "Toby", ""]]}, {"id": "1705.03127", "submitter": "Stefan Jansen", "authors": "Stefan Jansen", "title": "Word and Phrase Translation with word2vec", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word and phrase tables are key inputs to machine translations, but costly to\nproduce. New unsupervised learning methods represent words and phrases in a\nhigh-dimensional vector space, and these monolingual embeddings have been shown\nto encode syntactic and semantic relationships between language elements. The\ninformation captured by these embeddings can be exploited for bilingual\ntranslation by learning a transformation matrix that allows matching relative\npositions across two monolingual vector spaces. This method aims to identify\nhigh-quality candidates for word and phrase translation more cost-effectively\nfrom unlabeled data.\n  This paper expands the scope of previous attempts of bilingual translation to\nfour languages (English, German, Spanish, and French). It shows how to process\nthe source data, train a neural network to learn the high-dimensional\nembeddings for individual languages and expands the framework for testing their\nquality beyond the English language. Furthermore, it shows how to learn\nbilingual transformation matrices and obtain candidates for word and phrase\ntranslation, and assess their quality.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 00:09:38 GMT"}, {"version": "v2", "created": "Wed, 10 May 2017 06:04:24 GMT"}, {"version": "v3", "created": "Thu, 11 May 2017 02:18:47 GMT"}, {"version": "v4", "created": "Tue, 24 Apr 2018 15:39:41 GMT"}], "update_date": "2018-04-25", "authors_parsed": [["Jansen", "Stefan", ""]]}, {"id": "1705.03175", "submitter": "Esh Vckay", "authors": "Esh Vckay and Debasish Ghose", "title": "Emotional Metaheuristics For in-situ Foraging Using Sensor Constrained\n  Robot Swarms", "comments": "2009 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems, workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new social animal inspired emotional swarm intelligence\ntechnique. This technique is used to solve a variant of the popular collective\nrobots problem called foraging. We show with a simulation study how simple\ninteraction rules based on sensations like hunger and loneliness can lead to\nglobally coherent emergent behavior which allows sensor constrained robots to\nsolve the given problem\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 04:42:03 GMT"}, {"version": "v2", "created": "Thu, 27 Jun 2019 06:43:30 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Vckay", "Esh", ""], ["Ghose", "Debasish", ""]]}, {"id": "1705.03176", "submitter": "Esh Vckay", "authors": "Esh Vckay, Mansimar Aneja, Dipti Deodhare", "title": "Solving a Path Planning Problem in a Partially Known Environment using a\n  Swarm Algorithm", "comments": "IEEE International Symposium on Measurements and Control in Robotics.\n  2008", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a path planning strategy for an Autonomous Ground Vehicle\n(AGV) navigating in a partially known environment. Global path planning is\nperformed by first using a spatial database of the region to be traversed\ncontaining selected attributes such as height data and soil information from a\nsuitable spatial database. The database is processed using a biomimetic swarm\nalgorithm that is inspired by the nest building strategies followed by\ntermites. Local path planning is performed online utilizing information\nregarding contingencies that affect the safe navigation of the AGV from various\nsensors. The simulation discussed has been implemented on the open source\nPlayer-Stage-Gazebo platform.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 04:46:47 GMT"}, {"version": "v2", "created": "Thu, 27 Jun 2019 06:29:08 GMT"}], "update_date": "2019-06-28", "authors_parsed": [["Vckay", "Esh", ""], ["Aneja", "Mansimar", ""], ["Deodhare", "Dipti", ""]]}, {"id": "1705.03260", "submitter": "Joshua Peterson", "authors": "Joshua C. Peterson, Thomas L. Griffiths", "title": "Evidence for the size principle in semantic and perceptual domains", "comments": "6 pages, 4 figures, To appear in the Proceedings of the 39th Annual\n  Conference of the Cognitive Science Society", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shepard's Universal Law of Generalization offered a compelling case for the\nfirst physics-like law in cognitive science that should hold for all\nintelligent agents in the universe. Shepard's account is based on a rational\nBayesian model of generalization, providing an answer to the question of why\nsuch a law should emerge. Extending this account to explain how humans use\nmultiple examples to make better generalizations requires an additional\nassumption, called the size principle: hypotheses that pick out fewer objects\nshould make a larger contribution to generalization. The degree to which this\nprinciple warrants similarly law-like status is far from conclusive. Typically,\nevaluating this principle has not been straightforward, requiring additional\nassumptions. We present a new method for evaluating the size principle that is\nmore direct, and apply this method to a diverse array of datasets. Our results\nprovide support for the broad applicability of the size principle.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 10:21:49 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Peterson", "Joshua C.", ""], ["Griffiths", "Thomas L.", ""]]}, {"id": "1705.03290", "submitter": "Iiris Sundin", "authors": "Iiris Sundin, Tomi Peltola, Muntasir Mamun Majumder, Pedram Daee,\n  Marta Soare, Homayun Afrabandpey, Caroline Heckman, Samuel Kaski and Pekka\n  Marttinen", "title": "Improving drug sensitivity predictions in precision medicine through\n  active expert knowledge elicitation", "comments": "24 pages, 3 figures", "journal-ref": null, "doi": "10.1093/bioinformatics/bty257", "report-no": null, "categories": "cs.AI cs.HC cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting the efficacy of a drug for a given individual, using\nhigh-dimensional genomic measurements, is at the core of precision medicine.\nHowever, identifying features on which to base the predictions remains a\nchallenge, especially when the sample size is small. Incorporating expert\nknowledge offers a promising alternative to improve a prediction model, but\ncollecting such knowledge is laborious to the expert if the number of candidate\nfeatures is very large. We introduce a probabilistic model that can incorporate\nexpert feedback about the impact of genomic measurements on the sensitivity of\na cancer cell for a given drug. We also present two methods to intelligently\ncollect this feedback from the expert, using experimental design and\nmulti-armed bandit models. In a multiple myeloma blood cancer data set (n=51),\nexpert knowledge decreased the prediction error by 8%. Furthermore, the\nintelligent approaches can be used to reduce the workload of feedback\ncollection to less than 30% on average compared to a naive approach.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 12:04:33 GMT"}], "update_date": "2019-01-08", "authors_parsed": [["Sundin", "Iiris", ""], ["Peltola", "Tomi", ""], ["Majumder", "Muntasir Mamun", ""], ["Daee", "Pedram", ""], ["Soare", "Marta", ""], ["Afrabandpey", "Homayun", ""], ["Heckman", "Caroline", ""], ["Kaski", "Samuel", ""], ["Marttinen", "Pekka", ""]]}, {"id": "1705.03303", "submitter": "Niek Tax", "authors": "Niek Tax, Xixi Lu, Natalia Sidorova, Dirk Fahland, Wil M. P. van der\n  Aalst", "title": "The Imprecisions of Precision Measures in Process Mining", "comments": null, "journal-ref": "Information Processing Letters, 135 (2018), 1-8", "doi": "10.1016/j.ipl.2018.01.013", "report-no": null, "categories": "cs.DB cs.AI cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In process mining, precision measures are used to quantify how much a process\nmodel overapproximates the behavior seen in an event log. Although several\nmeasures have been proposed throughout the years, no research has been done to\nvalidate whether these measures achieve the intended aim of quantifying\nover-approximation in a consistent way for all models and logs. This paper\nfills this gap by postulating a number of axioms for quantifying precision\nconsistently for any log and any model. Further, we show through\ncounter-examples that none of the existing measures consistently quantifies\nprecision.\n", "versions": [{"version": "v1", "created": "Wed, 3 May 2017 11:50:45 GMT"}, {"version": "v2", "created": "Tue, 16 May 2017 18:44:59 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Tax", "Niek", ""], ["Lu", "Xixi", ""], ["Sidorova", "Natalia", ""], ["Fahland", "Dirk", ""], ["van der Aalst", "Wil M. P.", ""]]}, {"id": "1705.03352", "submitter": "Vaclav Kratochvil", "authors": "Ji\\v{r}ina Vejnarov\\'a, V\\'aclav Kratochv\\'il", "title": "Composition of Credal Sets via Polyhedral Geometry", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently introduced composition operator for credal sets is an analogy of\nsuch operators in probability, possibility, evidence and valuation-based\nsystems theories. It was designed to construct multidimensional models (in the\nframework of credal sets) from a system of low- dimensional credal sets. In\nthis paper we study its potential from the computational point of view\nutilizing methods of polyhedral geometry.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 14:46:44 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Vejnarov\u00e1", "Ji\u0159ina", ""], ["Kratochv\u00edl", "V\u00e1clav", ""]]}, {"id": "1705.03381", "submitter": "Nicolas Maudet", "authors": "Leila Amgoud, Elise Bonzon, Marco Correia, Jorge Cruz, J\\'er\\^ome\n  Delobelle, S\\'ebastien Konieczny, Jo\\~ao Leite, Alexis Martin, Nicolas\n  Maudet, Srdjan Vesic", "title": "A note on the uniqueness of models in social abstract argumentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social abstract argumentation is a principled way to assign values to\nconflicting (weighted) arguments. In this note we discuss the important\nproperty of the uniqueness of the model.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 15:18:13 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Amgoud", "Leila", ""], ["Bonzon", "Elise", ""], ["Correia", "Marco", ""], ["Cruz", "Jorge", ""], ["Delobelle", "J\u00e9r\u00f4me", ""], ["Konieczny", "S\u00e9bastien", ""], ["Leite", "Jo\u00e3o", ""], ["Martin", "Alexis", ""], ["Maudet", "Nicolas", ""], ["Vesic", "Srdjan", ""]]}, {"id": "1705.03392", "submitter": "Hans van Ditmarsch", "authors": "Philippe Balbiani, Hans van Ditmarsch, Sa\\'ul Fern\\'andez Gonz\\'alez", "title": "Asynchronous Announcements", "comments": "Originally presented at workshop Strategic Reasoning 2017 Liverpool", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC cs.LO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose a multi-agent epistemic logic of asynchronous announcements, where\ntruthful announcements are publicly sent but individually received by agents,\nand in the order in which they were sent. Additional to epistemic modalities\nthe logic contains dynamic modalities for making announcements and for\nreceiving them. What an agent believes is a function of her initial uncertainty\nand of the announcements she has received. Beliefs need not be truthful,\nbecause announcements already made may not yet have been received. As\nannouncements are true when sent, certain message sequences can be ruled out,\njust like inconsistent cuts in distributed computing.\n  We provide a complete axiomatization for this \\emph{asynchronous announcement\nlogic} (AA). It is a reduction system that also demonstrates that any formula\nin $AA$ is equivalent to one without dynamic modalities, just as for public\nannouncement logic. A detailed example modelling message exchanging processes\nin distributed computing in $AA$ closes our investigation.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 15:30:47 GMT"}, {"version": "v2", "created": "Sat, 13 May 2017 14:23:20 GMT"}, {"version": "v3", "created": "Fri, 27 Sep 2019 10:41:12 GMT"}, {"version": "v4", "created": "Mon, 18 Jan 2021 17:06:43 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Balbiani", "Philippe", ""], ["van Ditmarsch", "Hans", ""], ["Gonz\u00e1lez", "Sa\u00fal Fern\u00e1ndez", ""]]}, {"id": "1705.03451", "submitter": "Rui L. Lopes", "authors": "Alipio Jorge, German Larrazabal, Pablo Guillen, Rui L. Lopes", "title": "Proceedings of the Workshop on Data Mining for Oil and Gas", "comments": null, "journal-ref": null, "doi": "10.13140/RG.2.2.16408.39681", "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The process of exploring and exploiting Oil and Gas (O&G) generates a lot of\ndata that can bring more efficiency to the industry. The opportunities for\nusing data mining techniques in the \"digital oil-field\" remain largely\nunexplored or uncharted. With the high rate of data expansion, companies are\nscrambling to develop ways to develop near-real-time predictive analytics, data\nmining and machine learning capabilities, and are expanding their data storage\ninfrastructure and resources. With these new goals, come the challenges of\nmanaging data growth, integrating intelligence tools, and analyzing the data to\nglean useful insights. Oil and Gas companies need data solutions to\neconomically extract value from very large volumes of a wide variety of data\ngenerated from exploration, well drilling and production devices and sensors.\n  Data mining for oil and gas industry throughout the lifecycle of the\nreservoir includes the following roles: locating hydrocarbons, managing\ngeological data, drilling and formation evaluation, well construction, well\ncompletion, and optimizing production through the life of the oil field. For\neach of these phases during the lifecycle of oil field, data mining play a\nsignificant role. Based on which phase were talking about, knowledge creation\nthrough scientific models, data analytics and machine learning, a effective,\nproductive, and on demand data insight is critical for decision making within\nthe organization.\n  The significant challenges posed by this complex and economically vital field\njustify a meeting of data scientists that are willing to share their experience\nand knowledge. Thus, the Worskhop on Data Mining for Oil and Gas (DM4OG) aims\nto provide a quality forum for researchers that work on the significant\nchallenges arising from the synergy between data science, machine learning, and\nthe modeling and optimization problems in the O&G industry.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 17:55:15 GMT"}, {"version": "v2", "created": "Fri, 26 May 2017 13:17:17 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Jorge", "Alipio", ""], ["Larrazabal", "German", ""], ["Guillen", "Pablo", ""], ["Lopes", "Rui L.", ""]]}, {"id": "1705.03454", "submitter": "Mihail Eric", "authors": "Matthew Lamm and Mihail Eric", "title": "The Pragmatics of Indirect Commands in Collaborative Discourse", "comments": "International Conference on Computational Semantics 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Today's artificial assistants are typically prompted to perform tasks through\ndirect, imperative commands such as \\emph{Set a timer} or \\emph{Pick up the\nbox}. However, to progress toward more natural exchanges between humans and\nthese assistants, it is important to understand the way non-imperative\nutterances can indirectly elicit action of an addressee. In this paper, we\ninvestigate command types in the setting of a grounded, collaborative game. We\nfocus on a less understood family of utterances for eliciting agent action,\nlocatives like \\emph{The chair is in the other room}, and demonstrate how these\nutterances indirectly command in specific game state contexts. Our work shows\nthat models with domain-specific grounding can effectively realize the\npragmatic reasoning that is necessary for more robust natural language\ninteraction.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 19:34:23 GMT"}, {"version": "v2", "created": "Mon, 4 Sep 2017 19:04:40 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Lamm", "Matthew", ""], ["Eric", "Mihail", ""]]}, {"id": "1705.03455", "submitter": "Ankur Bapna", "authors": "Ankur Bapna, Gokhan Tur, Dilek Hakkani-Tur, Larry Heck", "title": "Sequential Dialogue Context Modeling for Spoken Language Understanding", "comments": "8 + 2 pages, Updated 10/17: Updated typos in abstract, Updated 07/07:\n  Updated Title, abstract and few minor changes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spoken Language Understanding (SLU) is a key component of goal oriented\ndialogue systems that would parse user utterances into semantic frame\nrepresentations. Traditionally SLU does not utilize the dialogue history beyond\nthe previous system turn and contextual ambiguities are resolved by the\ndownstream components. In this paper, we explore novel approaches for modeling\ndialogue context in a recurrent neural network (RNN) based language\nunderstanding system. We propose the Sequential Dialogue Encoder Network, that\nallows encoding context from the dialogue history in chronological order. We\ncompare the performance of our proposed architecture with two context models,\none that uses just the previous turn context and another that encodes dialogue\ncontext in a memory network, but loses the order of utterances in the dialogue\nhistory. Experiments with a multi-domain dialogue dataset demonstrate that the\nproposed architecture results in reduced semantic frame error rates.\n", "versions": [{"version": "v1", "created": "Mon, 8 May 2017 20:57:30 GMT"}, {"version": "v2", "created": "Thu, 11 May 2017 00:21:13 GMT"}, {"version": "v3", "created": "Fri, 7 Jul 2017 21:35:02 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Bapna", "Ankur", ""], ["Tur", "Gokhan", ""], ["Hakkani-Tur", "Dilek", ""], ["Heck", "Larry", ""]]}, {"id": "1705.03520", "submitter": "Jaeyoung Lee", "authors": "Jaeyoung Lee and Richard S. Sutton", "title": "Policy Iterations for Reinforcement Learning Problems in Continuous Time\n  and Space -- Fundamental Theory and Methods", "comments": "To appear in Automatica. All the Appendices are provided", "journal-ref": "Automatica vol. 126, 109421 (2021)", "doi": "10.1016/j.automatica.2020.109421", "report-no": null, "categories": "cs.AI cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Policy iteration (PI) is a recursive process of policy evaluation and\nimprovement for solving an optimal decision-making/control problem, or in other\nwords, a reinforcement learning (RL) problem. PI has also served as the\nfundamental for developing RL methods. In this paper, we propose two PI\nmethods, called differential PI (DPI) and integral PI (IPI), and their\nvariants, for a general RL framework in continuous time and space (CTS), where\nthe environment is modeled by a system of ordinary differential equations\n(ODEs). The proposed methods inherit the current ideas of PI in classical RL\nand optimal control and theoretically support the existing RL algorithms in\nCTS: TD-learning and value-gradient-based (VGB) greedy policy update. We also\nprovide case studies including 1) discounted RL and 2) optimal control tasks.\nFundamental mathematical properties -- admissibility, uniqueness of the\nsolution to the Bellman equation (BE), monotone improvement, convergence, and\noptimality of the solution to the Hamilton-Jacobi-Bellman equation (HJBE) --\nare all investigated in-depth and improved from the existing theory, along with\nthe general and case studies. Finally, the proposed ones are simulated with an\ninverted-pendulum model and their model-based and partially model-free\nimplementations to support the theory and further investigate them beyond.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 20:01:34 GMT"}, {"version": "v2", "created": "Sat, 31 Oct 2020 16:19:02 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Lee", "Jaeyoung", ""], ["Sutton", "Richard S.", ""]]}, {"id": "1705.03550", "submitter": "Vincenzo Lomonaco", "authors": "Vincenzo Lomonaco and Davide Maltoni", "title": "CORe50: a New Dataset and Benchmark for Continuous Object Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous/Lifelong learning of high-dimensional data streams is a\nchallenging research problem. In fact, fully retraining models each time new\ndata become available is infeasible, due to computational and storage issues,\nwhile na\\\"ive incremental strategies have been shown to suffer from\ncatastrophic forgetting. In the context of real-world object recognition\napplications (e.g., robotic vision), where continuous learning is crucial, very\nfew datasets and benchmarks are available to evaluate and compare emerging\ntechniques. In this work we propose a new dataset and benchmark CORe50,\nspecifically designed for continuous object recognition, and introduce baseline\napproaches for different continuous learning scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 21:32:19 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Lomonaco", "Vincenzo", ""], ["Maltoni", "Davide", ""]]}, {"id": "1705.03562", "submitter": "Steven Hansen", "authors": "Steven Stenberg Hansen", "title": "Deep Episodic Value Iteration for Model-based Meta-Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new deep meta reinforcement learner, which we call Deep Episodic\nValue Iteration (DEVI). DEVI uses a deep neural network to learn a similarity\nmetric for a non-parametric model-based reinforcement learning algorithm. Our\nmodel is trained end-to-end via back-propagation. Despite being trained using\nthe model-free Q-learning objective, we show that DEVI's model-based internal\nstructure provides `one-shot' transfer to changes in reward and transition\nstructure, even for tasks with very high-dimensional state spaces.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 22:59:18 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Hansen", "Steven Stenberg", ""]]}, {"id": "1705.03597", "submitter": "Yan  Li", "authors": "Yan Li, Zhaohan Sun", "title": "Solving Multi-Objective MDP with Lexicographic Preference: An\n  application to stochastic planning with multiple quantile objective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In most common settings of Markov Decision Process (MDP), an agent evaluate a\npolicy based on expectation of (discounted) sum of rewards. However in many\napplications this criterion might not be suitable from two perspective: first,\nin risk aversion situation expectation of accumulated rewards is not robust\nenough, this is the case when distribution of accumulated reward is heavily\nskewed; another issue is that many applications naturally take several\nobjective into consideration when evaluating a policy, for instance in\nautonomous driving an agent needs to balance speed and safety when choosing\nappropriate decision. In this paper, we consider evaluating a policy based on a\nsequence of quantiles it induces on a set of target states, our idea is to\nreformulate the original problem into a multi-objective MDP problem with\nlexicographic preference naturally defined. For computation of finding an\noptimal policy, we proposed an algorithm \\textbf{FLMDP} that could solve\ngeneral multi-objective MDP with lexicographic reward preference.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 03:13:30 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Li", "Yan", ""], ["Sun", "Zhaohan", ""]]}, {"id": "1705.03669", "submitter": "Rui L. Lopes", "authors": "Rui L. Lopes, Al\\'ipio Jorge", "title": "Mind the Gap: A Well Log Data Analysis", "comments": "Part of DM4OG 2017 proceedings (arXiv:1705.03451)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main task in oil and gas exploration is to gain an understanding of the\ndistribution and nature of rocks and fluids in the subsurface. Well logs are\nrecords of petro-physical data acquired along a borehole, providing direct\ninformation about what is in the subsurface. The data collected by logging\nwells can have significant economic consequences, due to the costs inherent to\ndrilling wells, and the potential return of oil deposits. In this paper, we\ndescribe preliminary work aimed at building a general framework for well log\nprediction.\n  First, we perform a descriptive and exploratory analysis of the gaps in the\nneutron porosity logs of more than a thousand wells in the North Sea. Then, we\ngenerate artificial gaps in the neutron logs that reflect the statistics\ncollected before. Finally, we compare Artificial Neural Networks, Random\nForests, and three algorithms of Linear Regression in the prediction of missing\ngaps on a well-by-well basis.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 09:27:16 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Lopes", "Rui L.", ""], ["Jorge", "Al\u00edpio", ""]]}, {"id": "1705.03751", "submitter": "Gagan Madan", "authors": "Gagan Madan", "title": "A Survey of Distant Supervision Methods using PGMs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relation Extraction refers to the task of populating a database with tuples\nof the form $r(e_1, e_2)$, where $r$ is a relation and $e_1$, $e_2$ are\nentities. Distant supervision is one such technique which tries to\nautomatically generate training examples based on an existing KB such as\nFreebase. This paper is a survey of some of the techniques in distant\nsupervision which primarily rely on Probabilistic Graphical Models (PGMs).\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 13:19:38 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Madan", "Gagan", ""]]}, {"id": "1705.03773", "submitter": "Jiyuan Zhang", "authors": "Jiyuan Zhang, Yang Feng, Dong Wang, Yang Wang, Andrew Abel, Shiyue\n  Zhang, Andi Zhang", "title": "Flexible and Creative Chinese Poetry Generation Using Neural Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been shown that Chinese poems can be successfully generated by\nsequence-to-sequence neural models, particularly with the attention mechanism.\nA potential problem of this approach, however, is that neural models can only\nlearn abstract rules, while poem generation is a highly creative process that\ninvolves not only rules but also innovations for which pure statistical models\nare not appropriate in principle. This work proposes a memory-augmented neural\nmodel for Chinese poem generation, where the neural model and the augmented\nmemory work together to balance the requirements of linguistic accordance and\naesthetic innovation, leading to innovative generations that are still\nrule-compliant. In addition, it is found that the memory mechanism provides\ninteresting flexibility that can be used to generate poems with different\nstyles.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 13:55:53 GMT"}], "update_date": "2017-05-11", "authors_parsed": [["Zhang", "Jiyuan", ""], ["Feng", "Yang", ""], ["Wang", "Dong", ""], ["Wang", "Yang", ""], ["Abel", "Andrew", ""], ["Zhang", "Shiyue", ""], ["Zhang", "Andi", ""]]}, {"id": "1705.03821", "submitter": "Djallel Bouneffouf", "authors": "Djallel Bouneffouf, Irina Rish, Guillermo A. Cecchi, Raphael Feraud", "title": "Context Attentive Bandits: Contextual Bandit with Restricted Context", "comments": "IJCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a novel formulation of the multi-armed bandit model, which we\ncall the contextual bandit with restricted context, where only a limited number\nof features can be accessed by the learner at every iteration. This novel\nformulation is motivated by different online problems arising in clinical\ntrials, recommender systems and attention modeling. Herein, we adapt the\nstandard multi-armed bandit algorithm known as Thompson Sampling to take\nadvantage of our restricted context setting, and propose two novel algorithms,\ncalled the Thompson Sampling with Restricted Context(TSRC) and the Windows\nThompson Sampling with Restricted Context(WTSRC), for handling stationary and\nnonstationary environments, respectively. Our empirical results demonstrate\nadvantages of the proposed approaches on several real-life datasets\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 15:32:36 GMT"}, {"version": "v2", "created": "Wed, 7 Jun 2017 18:40:28 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Bouneffouf", "Djallel", ""], ["Rish", "Irina", ""], ["Cecchi", "Guillermo A.", ""], ["Feraud", "Raphael", ""]]}, {"id": "1705.03865", "submitter": "Akshay Gupta", "authors": "Akshay Kumar Gupta", "title": "Survey of Visual Question Answering: Datasets and Techniques", "comments": "10 pages, 3 figures, 3 tables Added references, corrected typos, made\n  references less wordy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual question answering (or VQA) is a new and exciting problem that\ncombines natural language processing and computer vision techniques. We present\na survey of the various datasets and models that have been used to tackle this\ntask. The first part of the survey details the various datasets for VQA and\ncompares them along some common factors. The second part of this survey details\nthe different approaches for VQA, classified into four types: non-deep learning\nmodels, deep learning models without attention, deep learning models with\nattention, and other models which do not fit into the first three. Finally, we\ncompare the performances of these approaches and provide some directions for\nfuture work.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 17:30:17 GMT"}, {"version": "v2", "created": "Thu, 11 May 2017 06:46:52 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Gupta", "Akshay Kumar", ""]]}, {"id": "1705.03916", "submitter": "Tiep Le", "authors": "Tiep Le, Tran Cao Son, Enrico Pontelli, and William Yeoh", "title": "Solving Distributed Constraint Optimization Problems Using Logic\n  Programming", "comments": "Under consideration in Theory and Practice of Logic Programming\n  (TPLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the use of Answer Set Programming (ASP) in solving\nDistributed Constraint Optimization Problems (DCOPs). The paper provides the\nfollowing novel contributions: (1) It shows how one can formulate DCOPs as\nlogic programs; (2) It introduces ASP-DPOP, the first DCOP algorithm that is\nbased on logic programming; (3) It experimentally shows that ASP-DPOP can be up\nto two orders of magnitude faster than DPOP (its imperative programming\ncounterpart) as well as solve some problems that DPOP fails to solve, due to\nmemory limitations; and (4) It demonstrates the applicability of ASP in a wide\narray of multi-agent problems currently modeled as DCOPs. Under consideration\nin Theory and Practice of Logic Programming (TPLP).\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 18:32:01 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Le", "Tiep", ""], ["Son", "Tran Cao", ""], ["Pontelli", "Enrico", ""], ["Yeoh", "William", ""]]}, {"id": "1705.04119", "submitter": "Jin-Kao Hao", "authors": "Yangming Zhou, Jin-Kao Hao, Fred Glover", "title": "Memetic search for identifying critical nodes in sparse graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Critical node problems involve identifying a subset of critical nodes from an\nundirected graph whose removal results in optimizing a pre-defined measure over\nthe residual graph. As useful models for a variety of practical applications,\nthese problems are computational challenging. In this paper, we study the\nclassic critical node problem (CNP) and introduce an effective memetic\nalgorithm for solving CNP. The proposed algorithm combines a double\nbackbone-based crossover operator (to generate promising offspring solutions),\na component-based neighborhood search procedure (to find high-quality local\noptima) and a rank-based pool updating strategy (to guarantee a healthy\npopulation). Specially, the component-based neighborhood search integrates two\nkey techniques, i.e., two-phase node exchange strategy and node weighting\nscheme. The double backbone-based crossover extends the idea of general\nbackbone-based crossovers. Extensive evaluations on 42 synthetic and real-world\nbenchmark instances show that the proposed algorithm discovers 21 new upper\nbounds and matches 18 previous best-known upper bounds. We also demonstrate the\nrelevance of our algorithm for effectively solving a variant of the classic\nCNP, called the cardinality-constrained critical node problem. Finally, we\ninvestigate the usefulness of each key algorithmic component.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 11:43:30 GMT"}, {"version": "v2", "created": "Sat, 7 Oct 2017 13:15:03 GMT"}], "update_date": "2017-10-10", "authors_parsed": [["Zhou", "Yangming", ""], ["Hao", "Jin-Kao", ""], ["Glover", "Fred", ""]]}, {"id": "1705.04146", "submitter": "Wang Ling", "authors": "Wang Ling, Dani Yogatama, Chris Dyer, Phil Blunsom", "title": "Program Induction by Rationale Generation : Learning to Solve and\n  Explain Algebraic Word Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving algebraic word problems requires executing a series of arithmetic\noperations---a program---to obtain a final answer. However, since programs can\nbe arbitrarily complicated, inducing them directly from question-answer pairs\nis a formidable challenge. To make this task more feasible, we solve these\nproblems by generating answer rationales, sequences of natural language and\nhuman-readable mathematical expressions that derive the final answer through a\nseries of small steps. Although rationales do not explicitly specify programs,\nthey provide a scaffolding for their structure via intermediate milestones. To\nevaluate our approach, we have created a new 100,000-sample dataset of\nquestions, answers and rationales. Experimental results show that indirect\nsupervision of program learning via answer rationales is a promising strategy\nfor inducing arithmetic programs.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 13:04:47 GMT"}, {"version": "v2", "created": "Tue, 10 Oct 2017 14:57:26 GMT"}, {"version": "v3", "created": "Mon, 23 Oct 2017 16:45:03 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Ling", "Wang", ""], ["Yogatama", "Dani", ""], ["Dyer", "Chris", ""], ["Blunsom", "Phil", ""]]}, {"id": "1705.04185", "submitter": "Sina Ghiassian", "authors": "Sina Ghiassian, Banafsheh Rafiee, Richard S. Sutton", "title": "A First Empirical Study of Emphatic Temporal Difference Learning", "comments": "5 pages, Accepted to NIPS Continual Learning and Deep Networks\n  workshop, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present the first empirical study of the emphatic\ntemporal-difference learning algorithm (ETD), comparing it with conventional\ntemporal-difference learning, in particular, with linear TD(0), on on-policy\nand off-policy variations of the Mountain Car problem. The initial motivation\nfor developing ETD was that it has good convergence properties under off-policy\ntraining (Sutton, Mahmood and White 2016), but it is also a new algorithm for\nthe on-policy case. In both our on-policy and off-policy experiments, we found\nthat each method converged to a characteristic asymptotic level of error, with\nETD better than TD(0). TD(0) achieved a still lower error level temporarily\nbefore falling back to its higher asymptote, whereas ETD never showed this kind\nof \"bounce\". In the off-policy case (in which TD(0) is not guaranteed to\nconverge), ETD was significantly slower.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 13:52:52 GMT"}, {"version": "v2", "created": "Fri, 12 May 2017 16:49:38 GMT"}], "update_date": "2017-05-15", "authors_parsed": [["Ghiassian", "Sina", ""], ["Rafiee", "Banafsheh", ""], ["Sutton", "Richard S.", ""]]}, {"id": "1705.04282", "submitter": "Amanda Song", "authors": "Amanda Song, Linjie Li, Chad Atalla, Garrison Cottrell", "title": "Learning to see people like people", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans make complex inferences on faces, ranging from objective properties\n(gender, ethnicity, expression, age, identity, etc) to subjective judgments\n(facial attractiveness, trustworthiness, sociability, friendliness, etc). While\nthe objective aspects of face perception have been extensively studied,\nrelatively fewer computational models have been developed for the social\nimpressions of faces. Bridging this gap, we develop a method to predict human\nimpressions of faces in 40 subjective social dimensions, using deep\nrepresentations from state-of-the-art neural networks. We find that model\nperformance grows as the human consensus on a face trait increases, and that\nmodel predictions outperform human groups in correlation with human averages.\nThis illustrates the learnability of subjective social perception of faces,\nespecially when there is high human consensus. Our system can be used to decide\nwhich photographs from a personal collection will make the best impression. The\nresults are significant for the field of social robotics, demonstrating that\nrobots can learn the subjective judgments defining the underlying fabric of\nhuman interaction.\n", "versions": [{"version": "v1", "created": "Fri, 5 May 2017 05:47:15 GMT"}], "update_date": "2017-05-12", "authors_parsed": [["Song", "Amanda", ""], ["Li", "Linjie", ""], ["Atalla", "Chad", ""], ["Cottrell", "Garrison", ""]]}, {"id": "1705.04351", "submitter": "Rachit Dubey", "authors": "Rachit Dubey and Thomas L. Griffiths", "title": "A rational analysis of curiosity", "comments": "Conference paper in CogSci 2017", "journal-ref": "39th Annual Conference of the Cognitive Science Society (CogSci),\n  2017", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a rational analysis of curiosity, proposing that people's\ncuriosity is driven by seeking stimuli that maximize their ability to make\nappropriate responses in the future. This perspective offers a way to unify\nprevious theories of curiosity into a single framework. Experimental results\nconfirm our model's predictions, showing how the relationship between curiosity\nand confidence can change significantly depending on the nature of the\nenvironment. Please refer to https://psyarxiv.com/wg5m6/ for a more updated\nversion of this manuscript with a more detailed modeling section with extensive\nexperiments.\n", "versions": [{"version": "v1", "created": "Thu, 11 May 2017 18:54:10 GMT"}, {"version": "v2", "created": "Sat, 1 Aug 2020 05:15:12 GMT"}], "update_date": "2020-08-04", "authors_parsed": [["Dubey", "Rachit", ""], ["Griffiths", "Thomas L.", ""]]}, {"id": "1705.04448", "submitter": "TonTon Huang", "authors": "TonTon Hsien-De Huang, and Hung-Yu Kao", "title": "R2-D2: ColoR-inspired Convolutional NeuRal Network (CNN)-based AndroiD\n  Malware Detections", "comments": "Verison 2018/11/15, IEEE BigData 2018, Seattle, WA, USA, Dec 10-13,\n  2018. (Accepted)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The influence of Deep Learning on image identification and natural language\nprocessing has attracted enormous attention globally. The convolution neural\nnetwork that can learn without prior extraction of features fits well in\nresponse to the rapid iteration of Android malware. The traditional solution\nfor detecting Android malware requires continuous learning through\npre-extracted features to maintain high performance of identifying the malware.\nIn order to reduce the manpower of feature engineering prior to the condition\nof not to extract pre-selected features, we have developed a coloR-inspired\nconvolutional neuRal networks (CNN)-based AndroiD malware Detection (R2-D2)\nsystem. The system can convert the bytecode of classes.dex from Android archive\nfile to rgb color code and store it as a color image with fixed size. The color\nimage is input to the convolutional neural network for automatic feature\nextraction and training. The data was collected from Jan. 2017 to Aug 2017.\nDuring the period of time, we have collected approximately 2 million of benign\nand malicious Android apps for our experiments with the help from our research\npartner Leopard Mobile Inc. Our experiment results demonstrate that the\nproposed system has accurate security analysis on contracts. Furthermore, we\nkeep our research results and experiment materials on http://R2D2.TWMAN.ORG.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 06:28:12 GMT"}, {"version": "v2", "created": "Fri, 21 Jul 2017 11:56:51 GMT"}, {"version": "v3", "created": "Fri, 1 Sep 2017 16:10:48 GMT"}, {"version": "v4", "created": "Tue, 5 Dec 2017 09:08:15 GMT"}, {"version": "v5", "created": "Thu, 15 Nov 2018 10:47:38 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Huang", "TonTon Hsien-De", ""], ["Kao", "Hung-Yu", ""]]}, {"id": "1705.04524", "submitter": "Peng Su", "authors": "Peng Su, Xiao-Rong Ding, Yuan-Ting Zhang, Jing Liu, Fen Miao, Ni Zhao", "title": "Long-term Blood Pressure Prediction with Deep Recurrent Neural Networks", "comments": "To appear in IEEE BHI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing methods for arterial blood pressure (BP) estimation directly map the\ninput physiological signals to output BP values without explicitly modeling the\nunderlying temporal dependencies in BP dynamics. As a result, these models\nsuffer from accuracy decay over a long time and thus require frequent\ncalibration. In this work, we address this issue by formulating BP estimation\nas a sequence prediction problem in which both the input and target are\ntemporal sequences. We propose a novel deep recurrent neural network (RNN)\nconsisting of multilayered Long Short-Term Memory (LSTM) networks, which are\nincorporated with (1) a bidirectional structure to access larger-scale context\ninformation of input sequence, and (2) residual connections to allow gradients\nin deep RNN to propagate more effectively. The proposed deep RNN model was\ntested on a static BP dataset, and it achieved root mean square error (RMSE) of\n3.90 and 2.66 mmHg for systolic BP (SBP) and diastolic BP (DBP) prediction\nrespectively, surpassing the accuracy of traditional BP prediction models. On a\nmulti-day BP dataset, the deep RNN achieved RMSE of 3.84, 5.25, 5.80 and 5.81\nmmHg for the 1st day, 2nd day, 4th day and 6th month after the 1st day SBP\nprediction, and 1.80, 4.78, 5.0, 5.21 mmHg for corresponding DBP prediction,\nrespectively, which outperforms all previous models with notable improvement.\nThe experimental results suggest that modeling the temporal dependencies in BP\ndynamics significantly improves the long-term BP prediction accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 11:53:26 GMT"}, {"version": "v2", "created": "Tue, 27 Jun 2017 15:45:23 GMT"}, {"version": "v3", "created": "Sun, 14 Jan 2018 13:56:46 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Su", "Peng", ""], ["Ding", "Xiao-Rong", ""], ["Zhang", "Yuan-Ting", ""], ["Liu", "Jing", ""], ["Miao", "Fen", ""], ["Zhao", "Ni", ""]]}, {"id": "1705.04530", "submitter": "Arindam Bhattacharya", "authors": "Arindam Bhattacharya", "title": "A Survey of Question Answering for Math and Science Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Turing test was long considered the measure for artificial intelligence. But\nwith the advances in AI, it has proved to be insufficient measure. We can now\naim to mea- sure machine intelligence like we measure human intelligence. One\nof the widely accepted measure of intelligence is standardized math and science\ntest. In this paper, we explore the progress we have made towards the goal of\nmaking a machine smart enough to pass the standardized test. We see the\nchallenges and opportunities posed by the domain, and note that we are quite\nsome ways from actually making a system as smart as a even a middle school\nscholar.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 15:28:37 GMT"}], "update_date": "2017-05-15", "authors_parsed": [["Bhattacharya", "Arindam", ""]]}, {"id": "1705.04569", "submitter": "Max Ostrowski", "authors": "Mutsunori Banbara and Benjamin Kaufmann and Max Ostrowski and Torsten\n  Schaub", "title": "Clingcon: The Next Generation", "comments": "Under consideration in Theory and Practice of Logic Programming\n  (TPLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present the third generation of the constraint answer set system clingcon,\ncombining Answer Set Programming (ASP) with finite domain constraint processing\n(CP). While its predecessors rely on a black-box approach to hybrid solving by\nintegrating the CP solver gecode, the new clingcon system pursues a lazy\napproach using dedicated constraint propagators to extend propagation in the\nunderlying ASP solver clasp. No extension is needed for parsing and grounding\nclingcon's hybrid modeling language since both can be accommodated by the new\ngeneric theory handling capabilities of the ASP grounder gringo. As a whole,\nclingcon 3 is thus an extension of the ASP system clingo 5, which itself relies\non the grounder gringo and the solver clasp. The new approach of clingcon\noffers a seamless integration of CP propagation into ASP solving that benefits\nfrom the whole spectrum of clasp's reasoning modes, including for instance\nmulti-shot solving and advanced optimization techniques. This is accomplished\nby a lazy approach that unfolds the representation of constraints and adds it\nto that of the logic program only when needed. Although the unfolding is\nusually dictated by the constraint propagators during solving, it can already\nbe partially (or even totally) done during preprocessing. Moreover, clingcon's\nconstraint preprocessing and propagation incorporate several well established\nCP techniques that greatly improve its performance. We demonstrate this via an\nextensive empirical evaluation contrasting, first, the various techniques in\nthe context of CSP solving and, second, the new clingcon system with other\nhybrid ASP systems. Under consideration in Theory and Practice of Logic\nProgramming (TPLP)\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 13:57:31 GMT"}], "update_date": "2017-05-15", "authors_parsed": [["Banbara", "Mutsunori", ""], ["Kaufmann", "Benjamin", ""], ["Ostrowski", "Max", ""], ["Schaub", "Torsten", ""]]}, {"id": "1705.04662", "submitter": "Karl Ni", "authors": "Cory Stephenson, Patrick Callier, Abhinav Ganesh, Karl Ni", "title": "Monaural Audio Speaker Separation with Source Contrastive Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose an algorithm to separate simultaneously speaking persons from each\nother, the \"cocktail party problem\", using a single microphone. Our approach\ninvolves a deep recurrent neural networks regression to a vector space that is\ndescriptive of independent speakers. Such a vector space can embed empirically\ndetermined speaker characteristics and is optimized by distinguishing between\nspeaker masks. We call this technique source-contrastive estimation. The\nmethodology is inspired by negative sampling, which has seen success in natural\nlanguage processing, where an embedding is learned by correlating and\nde-correlating a given input vector with output weights. Although the matrix\ndetermined by the output weights is dependent on a set of known speakers, we\nonly use the input vectors during inference. Doing so will ensure that source\nseparation is explicitly speaker-independent. Our approach is similar to recent\ndeep neural network clustering and permutation-invariant training research; we\nuse weighted spectral features and masks to augment individual speaker\nfrequencies while filtering out other speakers. We avoid, however, the severe\ncomputational burden of other approaches with our technique. Furthermore, by\ntraining a vector space rather than combinations of different speakers or\ndifferences thereof, we avoid the so-called permutation problem during\ntraining. Our algorithm offers an intuitive, computationally efficient response\nto the cocktail party problem, and most importantly boasts better empirical\nperformance than other current techniques.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 17:23:02 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Stephenson", "Cory", ""], ["Callier", "Patrick", ""], ["Ganesh", "Abhinav", ""], ["Ni", "Karl", ""]]}, {"id": "1705.04665", "submitter": "Richard Valenzano", "authors": "Richard Anthony Valenzano and Danniel Sihui Yang", "title": "A Formal Characterization of the Local Search Topology of the Gap\n  Heuristic", "comments": "Technical report providing proofs of statements appearing in a \"An\n  Analysis and Enhancement of the Gap Heuristic for the Pancake Puzzle\" by\n  Richard Anthony Valenzano and Danniel Yang. This paper appeared at the 2017\n  Symposium on Combinatorial Search", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The pancake puzzle is a classic optimization problem that has become a\nstandard benchmark for heuristic search algorithms. In this paper, we provide\nfull proofs regarding the local search topology of the gap heuristic for the\npancake puzzle. First, we show that in any non-goal state in which there is no\nmove that will decrease the number of gaps, there is a move that will keep the\nnumber of gaps constant. We then classify any state in which the number of gaps\ncannot be decreased in a single action into two groups: those requiring 2\nactions to decrease the number of gaps, and those which require 3 actions to\ndecrease the number of gaps.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 17:28:43 GMT"}], "update_date": "2017-05-15", "authors_parsed": [["Valenzano", "Richard Anthony", ""], ["Yang", "Danniel Sihui", ""]]}, {"id": "1705.04712", "submitter": "Denis Ponomaryov", "authors": "Denis Ponomaryov and Mikhail Soutchanski", "title": "Progression of Decomposed Local-Effect Action Theories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many tasks related to reasoning about consequences of a logical theory, it\nis desirable to decompose the theory into a number of weakly-related or\nindependent components. However, a theory may represent knowledge that is\nsubject to change, as a result of executing actions that have effects on some\nof the initial properties mentioned in the theory. Having once computed a\ndecomposition of a theory, it is advantageous to know whether a decomposition\nhas to be computed again in the newly-changed theory (obtained from taking into\naccount changes resulting from execution of an action). In the paper, we\naddress this problem in the scope of the situation calculus, where a change of\nan initial theory is related to the notion of progression. Progression provides\na form of forward reasoning; it relies on forgetting values of those\nproperties, which are subject to change, and computing new values for them. We\nconsider decomposability and inseparability, two component properties known\nfrom the literature, and contribute by 1) studying the conditions when these\nproperties are preserved and 2) when they are lost wrt progression and the\nrelated operation of forgetting. To show the latter, we demonstrate the\nboundaries using a number of negative examples. To show the former, we identify\ncases when these properties are preserved under forgetting and progression of\ninitial theories in local-effect basic action theories of the situation\ncalculus. Our paper contributes to bridging two different communities in\nKnowledge Representation, namely research on modularity and research on\nreasoning about actions.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 18:36:21 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Ponomaryov", "Denis", ""], ["Soutchanski", "Mikhail", ""]]}, {"id": "1705.04719", "submitter": "Denis Ponomaryov", "authors": "Yevgeny Kazakov and Denis Ponomaryov", "title": "On the Complexity of Semantic Integration of OWL Ontologies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new mechanism for integration of OWL ontologies using semantic\nimport relations. In contrast to the standard OWL importing, we do not require\nall axioms of the imported ontologies to be taken into account for reasoning\ntasks, but only their logical implications over a chosen signature. This\nproperty comes natural in many ontology integration scenarios, especially when\nthe number of ontologies is large. In this paper, we study the complexity of\nreasoning over ontologies with semantic import relations and establish a range\nof tight complexity bounds for various fragments of OWL.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 18:54:16 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Kazakov", "Yevgeny", ""], ["Ponomaryov", "Denis", ""]]}, {"id": "1705.04724", "submitter": "Wei Li", "authors": "Wei Li, Xiatian Zhu, Shaogang Gong", "title": "Person Re-Identification by Deep Joint Learning of Multi-Loss\n  Classification", "comments": "Accepted by IJCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing person re-identification (re-id) methods rely mostly on either\nlocalised or global feature representation alone. This ignores their joint\nbenefit and mutual complementary effects. In this work, we show the advantages\nof jointly learning local and global features in a Convolutional Neural Network\n(CNN) by aiming to discover correlated local and global features in different\ncontext. Specifically, we formulate a method for joint learning of local and\nglobal feature selection losses designed to optimise person re-id when using\nonly generic matching metrics such as the L2 distance. We design a novel CNN\narchitecture for Jointly Learning Multi-Loss (JLML) of local and global\ndiscriminative feature optimisation subject concurrently to the same re-id\nlabelled information. Extensive comparative evaluations demonstrate the\nadvantages of this new JLML model for person re-id over a wide range of\nstate-of-the-art re-id methods on five benchmarks (VIPeR, GRID, CUHK01, CUHK03,\nMarket-1501).\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 19:18:07 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 01:04:56 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Li", "Wei", ""], ["Zhu", "Xiatian", ""], ["Gong", "Shaogang", ""]]}, {"id": "1705.04885", "submitter": "Jose Fontanari", "authors": "Jos\\'e F. Fontanari", "title": "Awareness improves problem-solving performance", "comments": null, "journal-ref": "Cogn Syst Res, 45C (2017) 52-58", "doi": "10.1016/j.cogsys.2017.05.003", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The brain's self-monitoring of activities, including internal activities -- a\nfunctionality that we refer to as awareness -- has been suggested as a key\nelement of consciousness. Here we investigate whether the presence of an\ninner-eye-like process (monitor) that supervises the activities of a number of\nsubsystems (operative agents) engaged in the solution of a problem can improve\nthe problem-solving efficiency of the system. The problem is to find the global\nmaximum of a NK fitness landscape and the performance is measured by the time\nrequired to find that maximum. The operative agents explore blindly the fitness\nlandscape and the monitor provides them with feedback on the quality (fitness)\nof the proposed solutions. This feedback is then used by the operative agents\nto bias their searches towards the fittest regions of the landscape. We find\nthat a weak feedback between the monitor and the operative agents improves the\nperformance of the system, regardless of the difficulty of the problem, which\nis gauged by the number of local maxima in the landscape. For easy problems\n(i.e., landscapes without local maxima), the performance improves monotonically\nas the feedback strength increases, but for difficult problems, there is an\noptimal value of the feedback strength beyond which the system performance\ndegrades very rapidly.\n", "versions": [{"version": "v1", "created": "Sat, 13 May 2017 20:40:24 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Fontanari", "Jos\u00e9 F.", ""]]}, {"id": "1705.04970", "submitter": "Shunji Umetani", "authors": "Shunji Umetani, Masanao Arakawa, Mutsunori Yagiura", "title": "Relaxation heuristics for the set multicover problem with generalized\n  upper bound constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an extension of the set covering problem (SCP) introducing\n(i)~multicover and (ii)~generalized upper bound (GUB)~constraints. For the\nconventional SCP, the pricing method has been introduced to reduce the size of\ninstances, and several efficient heuristic algorithms based on such reduction\ntechniques have been developed to solve large-scale instances. However, GUB\nconstraints often make the pricing method less effective, because they often\nprevent solutions from containing highly evaluated variables together. To\novercome this problem, we develop heuristic algorithms to reduce the size of\ninstances, in which new evaluation schemes of variables are introduced taking\naccount of GUB constraints. We also develop an efficient implementation of a\n2-flip neighborhood local search algorithm that reduces the number of\ncandidates in the neighborhood without sacrificing the solution quality. In\norder to guide the search to visit a wide variety of good solutions, we also\nintroduce a path relinking method that generates new solutions by combining two\nor more solutions obtained so far. According to computational comparison on\nbenchmark instances, the proposed method succeeds in selecting a small number\nof promising variables properly and performs quite effectively even for\nlarge-scale instances having hard GUB constraints.\n", "versions": [{"version": "v1", "created": "Sun, 14 May 2017 14:35:54 GMT"}, {"version": "v2", "created": "Sat, 6 Jan 2018 03:05:53 GMT"}], "update_date": "2018-01-09", "authors_parsed": [["Umetani", "Shunji", ""], ["Arakawa", "Masanao", ""], ["Yagiura", "Mutsunori", ""]]}, {"id": "1705.05035", "submitter": "Luke Metz", "authors": "Luke Metz, Julian Ibarz, Navdeep Jaitly, James Davidson", "title": "Discrete Sequential Prediction of Continuous Actions for Deep RL", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has long been assumed that high dimensional continuous control problems\ncannot be solved effectively by discretizing individual dimensions of the\naction space due to the exponentially large number of bins over which policies\nwould have to be learned. In this paper, we draw inspiration from the recent\nsuccess of sequence-to-sequence models for structured prediction problems to\ndevelop policies over discretized spaces. Central to this method is the\nrealization that complex functions over high dimensional spaces can be modeled\nby neural networks that predict one dimension at a time. Specifically, we show\nhow Q-values and policies over continuous spaces can be modeled using a next\nstep prediction model over discretized dimensions. With this parameterization,\nit is possible to both leverage the compositional structure of action spaces\nduring learning, as well as compute maxima over action spaces (approximately).\nOn a simple example task we demonstrate empirically that our method can perform\nglobal search, which effectively gets around the local optimization issues that\nplague DDPG. We apply the technique to off-policy (Q-learning) methods and show\nthat our method can achieve the state-of-the-art for off-policy methods on\nseveral continuous control tasks.\n", "versions": [{"version": "v1", "created": "Sun, 14 May 2017 22:53:13 GMT"}, {"version": "v2", "created": "Sat, 16 Jun 2018 03:11:46 GMT"}, {"version": "v3", "created": "Sat, 8 Jun 2019 00:56:16 GMT"}], "update_date": "2019-06-11", "authors_parsed": [["Metz", "Luke", ""], ["Ibarz", "Julian", ""], ["Jaitly", "Navdeep", ""], ["Davidson", "James", ""]]}, {"id": "1705.05065", "submitter": "Shital Shah", "authors": "Shital Shah, Debadeepta Dey, Chris Lovett, Ashish Kapoor", "title": "AirSim: High-Fidelity Visual and Physical Simulation for Autonomous\n  Vehicles", "comments": "Accepted for Field and Service Robotics conference 2017 (FSR 2017)", "journal-ref": null, "doi": null, "report-no": "MSR-TR-2017-9", "categories": "cs.RO cs.AI cs.CV cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing and testing algorithms for autonomous vehicles in real world is an\nexpensive and time consuming process. Also, in order to utilize recent advances\nin machine intelligence and deep learning we need to collect a large amount of\nannotated training data in a variety of conditions and environments. We present\na new simulator built on Unreal Engine that offers physically and visually\nrealistic simulations for both of these goals. Our simulator includes a physics\nengine that can operate at a high frequency for real-time hardware-in-the-loop\n(HITL) simulations with support for popular protocols (e.g. MavLink). The\nsimulator is designed from the ground up to be extensible to accommodate new\ntypes of vehicles, hardware platforms and software protocols. In addition, the\nmodular design enables various components to be easily usable independently in\nother projects. We demonstrate the simulator by first implementing a quadrotor\nas an autonomous vehicle and then experimentally comparing the software\ncomponents with real-world flights.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 04:06:22 GMT"}, {"version": "v2", "created": "Tue, 18 Jul 2017 05:30:28 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Shah", "Shital", ""], ["Dey", "Debadeepta", ""], ["Lovett", "Chris", ""], ["Kapoor", "Ashish", ""]]}, {"id": "1705.05088", "submitter": "Patrick Speicher", "authors": "Patrick Speicher, Marcel Steinmetz, J\\\"org Hoffmann, Michael Backes,\n  Robert K\\\"unnemann", "title": "Towards Automated Network Mitigation Analysis (extended)", "comments": "Cleaned up presentation to focus on mitigation analysis in simulated\n  pentesting. Stackelberg planning in a more general scope and the algorithm\n  proposed in v1 are extended and discussed in more detail in Speicher et.al.:\n  Stackelberg Planning: Towards Effective Leader-Follower State Space Search,\n  AAAI'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Penetration testing is a well-established practical concept for the\nidentification of potentially exploitable security weaknesses and an important\ncomponent of a security audit. Providing a holistic security assessment for\nnetworks consisting of several hundreds hosts is hardly feasible though without\nsome sort of mechanization. Mitigation, prioritizing counter-measures subject\nto a given budget, currently lacks a solid theoretical understanding and is\nhence more art than science. In this work, we propose the first approach for\nconducting comprehensive what-if analyses in order to reason about mitigation\nin a conceptually well-founded manner. To evaluate and compare mitigation\nstrategies, we use simulated penetration testing, i.e., automated\nattack-finding, based on a network model to which a subset of a given set of\nmitigation actions, e.g., changes to the network topology, system updates,\nconfiguration changes etc. is applied. Using Stackelberg planning, we determine\noptimal combinations that minimize the maximal attacker success (similar to a\nStackelberg game), and thus provide a well-founded basis for a holistic\nmitigation strategy. We show that these Stackelberg planning models can largely\nbe derived from network scan, public vulnerability databases and manual\ninspection with various degrees of automation and detail, and we simulate\nmitigation analysis on networks of different size and vulnerability.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 07:05:34 GMT"}, {"version": "v2", "created": "Fri, 4 Jan 2019 12:38:33 GMT"}], "update_date": "2019-01-07", "authors_parsed": [["Speicher", "Patrick", ""], ["Steinmetz", "Marcel", ""], ["Hoffmann", "J\u00f6rg", ""], ["Backes", "Michael", ""], ["K\u00fcnnemann", "Robert", ""]]}, {"id": "1705.05098", "submitter": "Lahari Poddar", "authors": "Lahari Poddar, Wynne Hsu, Mong Li Lee", "title": "Quantifying Aspect Bias in Ordinal Ratings using a Bayesian Approach", "comments": "Accepted for publication in IJCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  User opinions expressed in the form of ratings can influence an individual's\nview of an item. However, the true quality of an item is often obfuscated by\nuser biases, and it is not obvious from the observed ratings the importance\ndifferent users place on different aspects of an item. We propose a\nprobabilistic modeling of the observed aspect ratings to infer (i) each user's\naspect bias and (ii) latent intrinsic quality of an item. We model multi-aspect\nratings as ordered discrete data and encode the dependency between different\naspects by using a latent Gaussian structure. We handle the\nGaussian-Categorical non-conjugacy using a stick-breaking formulation coupled\nwith P\\'{o}lya-Gamma auxiliary variable augmentation for a simple, fully\nBayesian inference. On two real world datasets, we demonstrate the predictive\nability of our model and its effectiveness in learning explainable user biases\nto provide insights towards a more reliable product quality estimation.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 07:35:59 GMT"}, {"version": "v2", "created": "Wed, 24 May 2017 08:47:24 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Poddar", "Lahari", ""], ["Hsu", "Wynne", ""], ["Lee", "Mong Li", ""]]}, {"id": "1705.05108", "submitter": "Liangli Zhen", "authors": "Liangli Zhen, Dezhong Peng, Wei Wang, Xin Yao", "title": "Kernel Truncated Regression Representation for Robust Subspace\n  Clustering", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subspace clustering aims to group data points into multiple clusters of which\neach corresponds to one subspace. Most existing subspace clustering approaches\nassume that input data lie on linear subspaces. In practice, however, this\nassumption usually does not hold. To achieve nonlinear subspace clustering, we\npropose a novel method, called kernel truncated regression representation. Our\nmethod consists of the following four steps: 1) projecting the input data into\na hidden space, where each data point can be linearly represented by other data\npoints; 2) calculating the linear representation coefficients of the data\nrepresentations in the hidden space; 3) truncating the trivial coefficients to\nachieve robustness and block-diagonality; and 4) executing the graph cutting\noperation on the coefficient matrix by solving a graph Laplacian problem. Our\nmethod has the advantages of a closed-form solution and the capacity of\nclustering data points that lie on nonlinear subspaces. The first advantage\nmakes our method efficient in handling large-scale datasets, and the second one\nenables the proposed method to conquer the nonlinear subspace clustering\nchallenge. Extensive experiments on six benchmarks demonstrate the\neffectiveness and the efficiency of the proposed method in comparison with\ncurrent state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 08:16:34 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 15:33:56 GMT"}, {"version": "v3", "created": "Fri, 27 Mar 2020 09:16:24 GMT"}], "update_date": "2020-03-30", "authors_parsed": [["Zhen", "Liangli", ""], ["Peng", "Dezhong", ""], ["Wang", "Wei", ""], ["Yao", "Xin", ""]]}, {"id": "1705.05116", "submitter": "Fangyi Zhang", "authors": "Fangyi Zhang, J\\\"urgen Leitner, Michael Milford, Peter I. Corke", "title": "Tuning Modular Networks with Weighted Losses for Hand-Eye Coordination", "comments": "2 pages, to appear in the Deep Learning for Robotic Vision (DLRV)\n  Workshop in CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an end-to-end fine-tuning method to improve hand-eye\ncoordination in modular deep visuo-motor policies (modular networks) where each\nmodule is trained independently. Benefiting from weighted losses, the\nfine-tuning method significantly improves the performance of the policies for a\nrobotic planar reaching task.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 08:57:27 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Zhang", "Fangyi", ""], ["Leitner", "J\u00fcrgen", ""], ["Milford", "Michael", ""], ["Corke", "Peter I.", ""]]}, {"id": "1705.05172", "submitter": "Thomas Moerland", "authors": "Thomas M. Moerland, Joost Broekens, Catholijn M. Jonker", "title": "Emotion in Reinforcement Learning Agents and Robots: A Survey", "comments": "To be published in Machine Learning Journal", "journal-ref": "Machine Learning 2017", "doi": "10.1007/s10994-017-5666-0", "report-no": null, "categories": "cs.LG cs.AI cs.HC cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article provides the first survey of computational models of emotion in\nreinforcement learning (RL) agents. The survey focuses on agent/robot emotions,\nand mostly ignores human user emotions. Emotions are recognized as functional\nin decision-making by influencing motivation and action selection. Therefore,\ncomputational emotion models are usually grounded in the agent's decision\nmaking architecture, of which RL is an important subclass. Studying emotions in\nRL-based agents is useful for three research fields. For machine learning (ML)\nresearchers, emotion models may improve learning efficiency. For the\ninteractive ML and human-robot interaction (HRI) community, emotions can\ncommunicate state and enhance user investment. Lastly, it allows affective\nmodelling (AM) researchers to investigate their emotion theories in a\nsuccessful AI agent class. This survey provides background on emotion theory\nand RL. It systematically addresses 1) from what underlying dimensions (e.g.,\nhomeostasis, appraisal) emotions can be derived and how these can be modelled\nin RL-agents, 2) what types of emotions have been derived from these\ndimensions, and 3) how these emotions may either influence the learning\nefficiency of the agent or be useful as social signals. We also systematically\ncompare evaluation criteria, and draw connections to important RL sub-domains\nlike (intrinsic) motivation and model-based RL. In short, this survey provides\nboth a practical overview for engineers wanting to implement emotions in their\nRL agents, and identifies challenges and directions for future emotion-RL\nresearch.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 11:49:56 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Moerland", "Thomas M.", ""], ["Broekens", "Joost", ""], ["Jonker", "Catholijn M.", ""]]}, {"id": "1705.05206", "submitter": "Chen Zhang", "authors": "Chen Zhang, Hao Wang, Yingcai Wu", "title": "ResumeVis: A Visual Analytics System to Discover Semantic Information in\n  Semi-structured Resume Data", "comments": null, "journal-ref": null, "doi": "10.1145/3230707", "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Massive public resume data emerging on the WWW indicates individual-related\ncharacteristics in terms of profile and career experiences. Resume Analysis\n(RA) provides opportunities for many applications, such as talent seeking and\nevaluation. Existing RA studies based on statistical analyzing have primarily\nfocused on talent recruitment by identifying explicit attributes. However, they\nfailed to discover the implicit semantic information, i.e., individual career\nprogress patterns and social-relations, which are vital to comprehensive\nunderstanding of career development. Besides, how to visualize them for better\nhuman cognition is also challenging. To tackle these issues, we propose a\nvisual analytics system ResumeVis to mine and visualize resume data. Firstly, a\ntext-mining based approach is presented to extract semantic information. Then,\na set of visualizations are devised to represent the semantic information in\nmultiple perspectives. By interactive exploration on ResumeVis performed by\ndomain experts, the following tasks can be accomplished: to trace individual\ncareer evolving trajectory; to mine latent social-relations among individuals;\nand to hold the full picture of massive resumes' collective mobility. Case\nstudies with over 2500 online officer resumes demonstrate the effectiveness of\nour system. We provide a demonstration video.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 13:13:47 GMT"}], "update_date": "2018-12-06", "authors_parsed": [["Zhang", "Chen", ""], ["Wang", "Hao", ""], ["Wu", "Yingcai", ""]]}, {"id": "1705.05249", "submitter": "Cedric Nugteren", "authors": "Cedric Nugteren", "title": "CLBlast: A Tuned OpenCL BLAS Library", "comments": "Conference paper in: IWOCL '18, the International Workshop on OpenCL", "journal-ref": null, "doi": "10.1145/3204919.3204924", "report-no": null, "categories": "cs.MS cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces CLBlast, an open-source BLAS library providing optimized\nOpenCL routines to accelerate dense linear algebra for a wide variety of\ndevices. It is targeted at machine learning and HPC applications and thus\nprovides a fast matrix-multiplication routine (GEMM) to accelerate the core of\nmany applications (e.g. deep learning, iterative solvers, astrophysics,\ncomputational fluid dynamics, quantum chemistry). CLBlast has five main\nadvantages over other OpenCL BLAS libraries: 1) it is optimized for and tested\non a large variety of OpenCL devices including less commonly used devices such\nas embedded and low-power GPUs, 2) it can be explicitly tuned for specific\nproblem-sizes on specific hardware platforms, 3) it can perform operations in\nhalf-precision floating-point FP16 saving bandwidth, time and energy, 4) it has\nan optional CUDA back-end, 5) and it can combine multiple operations in a\nsingle batched routine, accelerating smaller problems significantly. This paper\ndescribes the library and demonstrates the advantages of CLBlast experimentally\nfor different use-cases on a wide variety of OpenCL hardware.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 17:16:59 GMT"}, {"version": "v2", "created": "Fri, 27 Apr 2018 09:10:16 GMT"}], "update_date": "2018-04-30", "authors_parsed": [["Nugteren", "Cedric", ""]]}, {"id": "1705.05254", "submitter": "Yanjing Wang", "authors": "Raul Fervari, Andreas Herzig, Yanjun Li, Yanjing Wang", "title": "Strategically knowing how", "comments": "an earlier version of the paper to appear in IJCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a single-agent logic of goal-directed knowing how\nextending the standard epistemic logic of knowing that with a new knowing how\noperator. The semantics of the new operator is based on the idea that knowing\nhow to achieve $\\phi$ means that there exists a (uniform) strategy such that\nthe agent knows that it can make sure $\\phi$. We give an intuitive\naxiomatization of our logic and prove the soundness, completeness, and\ndecidability of the logic. The crucial axioms relating knowing that and knowing\nhow illustrate our understanding of knowing how in this setting. This logic can\nbe used in representing both knowledge-that and knowledge-how.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 14:12:16 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Fervari", "Raul", ""], ["Herzig", "Andreas", ""], ["Li", "Yanjun", ""], ["Wang", "Yanjing", ""]]}, {"id": "1705.05316", "submitter": "Minas Dasygenis Dr.", "authors": "Minas Dasygenis and Kostas Stergiou", "title": "Exploiting the Pruning Power of Strong Local Consistencies Through\n  Parallelization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local consistencies stronger than arc consistency have received a lot of\nattention since the early days of CSP research. %because of the strong pruning\nthey can achieve. However, they have not been widely adopted by CSP solvers.\nThis is because applying such consistencies can sometimes result in\nconsiderably smaller search tree sizes and therefore in important speed-ups,\nbut in other cases the search space reduction may be small, causing severe run\ntime penalties. Taking advantage of recent advances in parallelization, we\npropose a novel approach for the application of strong local consistencies\n(SLCs) that can improve their performance by largely preserving the speed-ups\nthey offer in cases where they are successful, and eliminating the run time\npenalties in cases where they are unsuccessful. This approach is presented in\nthe form of two search algorithms. Both algorithms consist of a master search\nprocess, which is a typical CSP solver, and a number of slave processes, with\neach one implementing a SLC method. The first algorithm runs the different SLCs\nsynchronously at each node of the search tree explored in the master process,\nwhile the second one can run them asynchronously at different nodes of the\nsearch tree. Experimental results demonstrate the benefits of the proposed\nmethod.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 16:28:00 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Dasygenis", "Minas", ""], ["Stergiou", "Kostas", ""]]}, {"id": "1705.05326", "submitter": "Michael Huth", "authors": "Paul Beaumont and Michael Huth", "title": "Constrained Bayesian Networks: Theory, Optimization, and Applications", "comments": "43 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop the theory and practice of an approach to modelling and\nprobabilistic inference in causal networks that is suitable when\napplication-specific or analysis-specific constraints should inform such\ninference or when little or no data for the learning of causal network\nstructure or probability values at nodes are available. Constrained Bayesian\nNetworks generalize a Bayesian Network such that probabilities can be symbolic,\narithmetic expressions and where the meaning of the network is constrained by\nfinitely many formulas from the theory of the reals. A formal semantics for\nconstrained Bayesian Networks over first-order logic of the reals is given,\nwhich enables non-linear and non-convex optimisation algorithms that rely on\ndecision procedures for this logic, and supports the composition of several\nconstrained Bayesian Networks. A non-trivial case study in arms control, where\nfew or no data are available to assess the effectiveness of an arms inspection\nprocess, evaluates our approach. An open-access prototype implementation of\nthese foundations and their algorithms uses the SMT solver Z3 as decision\nprocedure, leverages an open-source package for Bayesian inference to symbolic\ncomputation, and is evaluated experimentally.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 16:48:12 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Beaumont", "Paul", ""], ["Huth", "Michael", ""]]}, {"id": "1705.05363", "submitter": "Deepak Pathak", "authors": "Deepak Pathak, Pulkit Agrawal, Alexei A. Efros, Trevor Darrell", "title": "Curiosity-driven Exploration by Self-supervised Prediction", "comments": "In ICML 2017. Website at https://pathak22.github.io/noreward-rl/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world scenarios, rewards extrinsic to the agent are extremely\nsparse, or absent altogether. In such cases, curiosity can serve as an\nintrinsic reward signal to enable the agent to explore its environment and\nlearn skills that might be useful later in its life. We formulate curiosity as\nthe error in an agent's ability to predict the consequence of its own actions\nin a visual feature space learned by a self-supervised inverse dynamics model.\nOur formulation scales to high-dimensional continuous state spaces like images,\nbypasses the difficulties of directly predicting pixels, and, critically,\nignores the aspects of the environment that cannot affect the agent. The\nproposed approach is evaluated in two environments: VizDoom and Super Mario\nBros. Three broad settings are investigated: 1) sparse extrinsic reward, where\ncuriosity allows for far fewer interactions with the environment to reach the\ngoal; 2) exploration with no extrinsic reward, where curiosity pushes the agent\nto explore more efficiently; and 3) generalization to unseen scenarios (e.g.\nnew levels of the same game) where the knowledge gained from earlier experience\nhelps the agent explore new places much faster than starting from scratch. Demo\nvideo and code available at https://pathak22.github.io/noreward-rl/\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 17:56:22 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Pathak", "Deepak", ""], ["Agrawal", "Pulkit", ""], ["Efros", "Alexei A.", ""], ["Darrell", "Trevor", ""]]}, {"id": "1705.05394", "submitter": "David Held", "authors": "David Held, Zoe McCarthy, Michael Zhang, Fred Shentu, Pieter Abbeel", "title": "Probabilistically Safe Policy Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although learning-based methods have great potential for robotics, one\nconcern is that a robot that updates its parameters might cause large amounts\nof damage before it learns the optimal policy. We formalize the idea of safe\nlearning in a probabilistic sense by defining an optimization problem: we\ndesire to maximize the expected return while keeping the expected damage below\na given safety limit. We study this optimization for the case of a robot\nmanipulator with safety-based torque limits. We would like to ensure that the\ndamage constraint is maintained at every step of the optimization and not just\nat convergence. To achieve this aim, we introduce a novel method which predicts\nhow modifying the torque limit, as well as how updating the policy parameters,\nmight affect the robot's safety. We show through a number of experiments that\nour approach allows the robot to improve its performance while ensuring that\nthe expected damage constraint is not violated during the learning process.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 18:02:17 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Held", "David", ""], ["McCarthy", "Zoe", ""], ["Zhang", "Michael", ""], ["Shentu", "Fred", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1705.05396", "submitter": "Avi Pfeffer", "authors": "Avi Pfeffer", "title": "Learning Probabilistic Programs Using Backpropagation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic modeling enables combining domain knowledge with learning from\ndata, thereby supporting learning from fewer training instances than purely\ndata-driven methods. However, learning probabilistic models is difficult and\nhas not achieved the level of performance of methods such as deep neural\nnetworks on many tasks. In this paper, we attempt to address this issue by\npresenting a method for learning the parameters of a probabilistic program\nusing backpropagation. Our approach opens the possibility to building deep\nprobabilistic programming models that are trained in a similar way to neural\nnetworks.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 18:07:31 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Pfeffer", "Avi", ""]]}, {"id": "1705.05427", "submitter": "Nan Jiang", "authors": "Kareem Amin, Nan Jiang, Satinder Singh", "title": "Repeated Inverse Reinforcement Learning", "comments": "The first two authors contributed equally to this work. The paper\n  appears in NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel repeated Inverse Reinforcement Learning problem: the\nagent has to act on behalf of a human in a sequence of tasks and wishes to\nminimize the number of tasks that it surprises the human by acting suboptimally\nwith respect to how the human would have acted. Each time the human is\nsurprised, the agent is provided a demonstration of the desired behavior by the\nhuman. We formalize this problem, including how the sequence of tasks is\nchosen, in a few different ways and provide some foundational results.\n", "versions": [{"version": "v1", "created": "Mon, 15 May 2017 20:06:35 GMT"}, {"version": "v2", "created": "Thu, 18 May 2017 19:32:27 GMT"}, {"version": "v3", "created": "Sat, 4 Nov 2017 00:38:19 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Amin", "Kareem", ""], ["Jiang", "Nan", ""], ["Singh", "Satinder", ""]]}, {"id": "1705.05515", "submitter": "GyongIl Ryang", "authors": "Jon JaeGyong, Mun JongHui, Ryang GyongIl", "title": "A Method for Determining Weights of Criterias and Alternative of Fuzzy\n  Group Decision Making Problem", "comments": "12 pages, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we constructed a model to determine weights of criterias and\npresented a solution for determining the optimal alternative by using the\nconstructed model and relationship analysis between criterias in fuzzy group\ndecision-making problem with different forms of preference information of\ndecision makers on criterias.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 03:10:56 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["JaeGyong", "Jon", ""], ["JongHui", "Mun", ""], ["GyongIl", "Ryang", ""]]}, {"id": "1705.05524", "submitter": "Dieterich Lawson", "authors": "Dieterich Lawson, Chung-Cheng Chiu, George Tucker, Colin Raffel, Kevin\n  Swersky, Navdeep Jaitly", "title": "Learning Hard Alignments with Variational Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has recently been significant interest in hard attention models for\ntasks such as object recognition, visual captioning and speech recognition.\nHard attention can offer benefits over soft attention such as decreased\ncomputational cost, but training hard attention models can be difficult because\nof the discrete latent variables they introduce. Previous work used REINFORCE\nand Q-learning to approach these issues, but those methods can provide\nhigh-variance gradient estimates and be slow to train. In this paper, we tackle\nthe problem of learning hard attention for a sequential task using variational\ninference methods, specifically the recently introduced VIMCO and NVIL.\nFurthermore, we propose a novel baseline that adapts VIMCO to this setting. We\ndemonstrate our method on a phoneme recognition task in clean and noisy\nenvironments and show that our method outperforms REINFORCE, with the\ndifference being greater for a more complicated task.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 04:30:56 GMT"}, {"version": "v2", "created": "Wed, 1 Nov 2017 19:08:18 GMT"}], "update_date": "2017-11-03", "authors_parsed": [["Lawson", "Dieterich", ""], ["Chiu", "Chung-Cheng", ""], ["Tucker", "George", ""], ["Raffel", "Colin", ""], ["Swersky", "Kevin", ""], ["Jaitly", "Navdeep", ""]]}, {"id": "1705.05551", "submitter": "Katsunari Shibata", "authors": "Katsunari Shibata and Yuki Goto", "title": "New Reinforcement Learning Using a Chaotic Neural Network for Emergence\n  of \"Thinking\" - \"Exploration\" Grows into \"Thinking\" through Learning -", "comments": "The Multi-disciplinary Conference on Reinforcement Learning and\n  Decision Making (RLDM) 2017, 5 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Expectation for the emergence of higher functions is getting larger in the\nframework of end-to-end reinforcement learning using a recurrent neural\nnetwork. However, the emergence of \"thinking\" that is a typical higher function\nis difficult to realize because \"thinking\" needs non fixed-point, flow-type\nattractors with both convergence and transition dynamics. Furthermore, in order\nto introduce \"inspiration\" or \"discovery\" in \"thinking\", not completely random\nbut unexpected transition should be also required.\n  By analogy to \"chaotic itinerancy\", we have hypothesized that \"exploration\"\ngrows into \"thinking\" through learning by forming flow-type attractors on\nchaotic random-like dynamics. It is expected that if rational dynamics are\nlearned in a chaotic neural network (ChNN), coexistence of rational state\ntransition, inspiration-like state transition and also random-like exploration\nfor unknown situation can be realized.\n  Based on the above idea, we have proposed new reinforcement learning using a\nChNN as an actor. The positioning of exploration is completely different from\nthe conventional one. The chaotic dynamics inside the ChNN produces exploration\nfactors by itself. Since external random numbers for stochastic action\nselection are not used, exploration factors cannot be isolated from the output.\nTherefore, the learning method is also completely different from the\nconventional one.\n  At each non-feedback connection, one variable named causality trace takes in\nand maintains the input through the connection according to the change in its\noutput. Using the trace and TD error, the weight is updated.\n  In this paper, as the result of a recent simple task to see whether the new\nlearning works or not, it is shown that a robot with two wheels and two visual\nsensors reaches a target while avoiding an obstacle after learning though there\nare still many rooms for improvement.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 06:54:04 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Shibata", "Katsunari", ""], ["Goto", "Yuki", ""]]}, {"id": "1705.05637", "submitter": "Jakub Kowalski", "authors": "Bartosz Kostka, Jaroslaw Kwiecien, Jakub Kowalski, Pawel Rychlikowski", "title": "Text-based Adventures of the Golovin AI Agent", "comments": null, "journal-ref": null, "doi": "10.1109/CIG.2017.8080433", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The domain of text-based adventure games has been recently established as a\nnew challenge of creating the agent that is both able to understand natural\nlanguage, and acts intelligently in text-described environments.\n  In this paper, we present our approach to tackle the problem. Our agent,\nnamed Golovin, takes advantage of the limited game domain. We use genre-related\ncorpora (including fantasy books and decompiled games) to create language\nmodels suitable to this domain. Moreover, we embed mechanisms that allow us to\nspecify, and separately handle, important tasks as fighting opponents, managing\ninventory, and navigating on the game map.\n  We validated usefulness of these mechanisms, measuring agent's performance on\nthe set of 50 interactive fiction games. Finally, we show that our agent plays\non a level comparable to the winner of the last year Text-Based Adventure AI\nCompetition.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 10:55:08 GMT"}], "update_date": "2019-04-02", "authors_parsed": [["Kostka", "Bartosz", ""], ["Kwiecien", "Jaroslaw", ""], ["Kowalski", "Jakub", ""], ["Rychlikowski", "Pawel", ""]]}, {"id": "1705.05681", "submitter": "Brijnesh Jain", "authors": "Brijnesh J. Jain and David Schultz", "title": "Optimal Warping Paths are unique for almost every Pair of Time Series", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Update rules for learning in dynamic time warping spaces are based on optimal\nwarping paths between parameter and input time series. In general, optimal\nwarping paths are not unique resulting in adverse effects in theory and\npractice. Under the assumption of squared error local costs, we show that no\ntwo warping paths have identical costs almost everywhere in a measure-theoretic\nsense. Two direct consequences of this result are: (i) optimal warping paths\nare unique almost everywhere, and (ii) the set of all pairs of time series with\nmultiple equal-cost warping paths coincides with the union of exponentially\nmany zero sets of quadratic forms. One implication of the proposed results is\nthat typical distance-based cost functions such as the k-means objective are\ndifferentiable almost everywhere and can be minimized by subgradient methods.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 12:41:25 GMT"}, {"version": "v2", "created": "Fri, 2 Mar 2018 11:18:36 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Jain", "Brijnesh J.", ""], ["Schultz", "David", ""]]}, {"id": "1705.05720", "submitter": "Rui Meng", "authors": "Rui Meng, Hao Xin, Lei Chen, Yangqiu Song", "title": "Subjective Knowledge Acquisition and Enrichment Powered By Crowdsourcing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge bases (KBs) have attracted increasing attention due to its great\nsuccess in various areas, such as Web and mobile search.Existing KBs are\nrestricted to objective factual knowledge, such as city population or fruit\nshape, whereas,subjective knowledge, such as big city, which is commonly\nmentioned in Web and mobile queries, has been neglected. Subjective knowledge\ndiffers from objective knowledge in that it has no documented or observed\nground truth. Instead, the truth relies on people's dominant opinion. Thus, we\ncan use the crowdsourcing technique to get opinion from the crowd. In our work,\nwe propose a system, called crowdsourced subjective knowledge acquisition\n(CoSKA),for subjective knowledge acquisition powered by crowdsourcing and\nexisting KBs. The acquired knowledge can be used to enrich existing KBs in the\nsubjective dimension which bridges the gap between existing objective knowledge\nand subjective queries.The main challenge of CoSKA is the conflict between\nlarge scale knowledge facts and limited crowdsourcing resource. To address this\nchallenge, in this work, we define knowledge inference rules and then select\nthe seed knowledge judiciously for crowdsourcing to maximize the inference\npower under the resource constraint. Our experimental results on real knowledge\nbase and crowdsourcing platform verify the effectiveness of CoSKA system.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 14:25:02 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Meng", "Rui", ""], ["Xin", "Hao", ""], ["Chen", "Lei", ""], ["Song", "Yangqiu", ""]]}, {"id": "1705.05735", "submitter": "Johan Ugander", "authors": "Jon Kleinberg, Sendhil Mullainathan, Johan Ugander", "title": "Comparison-Based Choices", "comments": "20 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A broad range of on-line behaviors are mediated by interfaces in which people\nmake choices among sets of options. A rich and growing line of work in the\nbehavioral sciences indicate that human choices follow not only from the\nutility of alternatives, but also from the choice set in which alternatives are\npresented. In this work we study comparison-based choice functions, a simple\nbut surprisingly rich class of functions capable of exhibiting so-called\nchoice-set effects. Motivated by the challenge of predicting complex choices,\nwe study the query complexity of these functions in a variety of settings. We\nconsider settings that allow for active queries or passive observation of a\nstream of queries, and give analyses both at the granularity of individuals or\npopulations that might exhibit heterogeneous choice behavior. Our main result\nis that any comparison-based choice function in one dimension can be inferred\nas efficiently as a basic maximum or minimum choice function across many query\ncontexts, suggesting that choice-set effects need not entail any fundamental\nalgorithmic barriers to inference. We also introduce a class of choice\nfunctions we call distance-comparison-based functions, and briefly discuss the\nanalysis of such functions. The framework we outline provides intriguing\nconnections between human choice behavior and a range of questions in the\ntheory of sorting.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 14:44:13 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Kleinberg", "Jon", ""], ["Mullainathan", "Sendhil", ""], ["Ugander", "Johan", ""]]}, {"id": "1705.05742", "submitter": "Rakshit Trivedi", "authors": "Rakshit Trivedi, Hanjun Dai, Yichen Wang, Le Song", "title": "Know-Evolve: Deep Temporal Reasoning for Dynamic Knowledge Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of large scale event data with time stamps has given rise to\ndynamically evolving knowledge graphs that contain temporal information for\neach edge. Reasoning over time in such dynamic knowledge graphs is not yet well\nunderstood. To this end, we present Know-Evolve, a novel deep evolutionary\nknowledge network that learns non-linearly evolving entity representations over\ntime. The occurrence of a fact (edge) is modeled as a multivariate point\nprocess whose intensity function is modulated by the score for that fact\ncomputed based on the learned entity embeddings. We demonstrate significantly\nimproved performance over various relational learning approaches on two large\nscale real-world datasets. Further, our method effectively predicts occurrence\nor recurrence time of a fact which is novel compared to prior reasoning\napproaches in multi-relational setting.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 14:53:02 GMT"}, {"version": "v2", "created": "Wed, 17 May 2017 04:54:07 GMT"}, {"version": "v3", "created": "Wed, 21 Jun 2017 05:21:46 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Trivedi", "Rakshit", ""], ["Dai", "Hanjun", ""], ["Wang", "Yichen", ""], ["Song", "Le", ""]]}, {"id": "1705.05756", "submitter": "Witold Rudnicki", "authors": "Krzysztof Mnich and Witold R. Rudnicki", "title": "All-relevant feature selection using multidimensional filters with\n  exhaustive search", "comments": "27 pages, 11 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a method for identification of the informative variables\nin the information system with discrete decision variables. It is targeted\nspecifically towards discovery of the variables that are non-informative when\nconsidered alone, but are informative when the synergistic interactions between\nmultiple variables are considered. To this end, the mutual entropy of all\npossible k-tuples of variables with decision variable is computed. Then, for\neach variable the maximal information gain due to interactions with other\nvariables is obtained. For non-informative variables this quantity conforms to\nthe well known statistical distributions. This allows for discerning truly\ninformative variables from non-informative ones. For demonstration of the\napproach, the method is applied to several synthetic datasets that involve\ncomplex multidimensional interactions between variables. It is capable of\nidentifying most important informative variables, even in the case when the\ndimensionality of the analysis is smaller than the true dimensionality of the\nproblem. What is more, the high sensitivity of the algorithm allows for\ndetection of the influence of nuisance variables on the response variable.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 15:11:10 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Mnich", "Krzysztof", ""], ["Rudnicki", "Witold R.", ""]]}, {"id": "1705.05765", "submitter": "Akshay Soni", "authors": "Jeya Balaji Balasubramanian, Akshay Soni, Yashar Mehdad, Nikolay\n  Laptev", "title": "Online Article Ranking as a Constrained, Dynamic, Multi-Objective\n  Optimization Problem", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The content ranking problem in a social news website, is typically a function\nthat maximizes a scalar metric of interest like dwell-time. However, like in\nmost real-world applications we are interested in more than one metric---for\ninstance simultaneously maximizing click-through rate, monetization metrics,\ndwell-time---and also satisfy the traffic requirements promised to different\npublishers. All this needs to be done on online data and under the settings\nwhere the objective function and the constraints can dynamically change; this\ncould happen if for instance new publishers are added, some contracts are\nadjusted, or if some contracts are over.\n  In this paper, we formulate this problem as a constrained, dynamic,\nmulti-objective optimization problem. We propose a novel framework that extends\na successful genetic optimization algorithm, NSGA-II, to solve this online,\ndata-driven problem. We design the modules of NSGA-II to suit our problem. We\nevaluate optimization performance using Hypervolume and introduce a confidence\ninterval metric for assessing the practicality of a solution. We demonstrate\nthe application of this framework on a real-world Article Ranking problem. We\nobserve that we make considerable improvements in both time and performance\nover a brute-force baseline technique that is currently in production.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 15:27:57 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Balasubramanian", "Jeya Balaji", ""], ["Soni", "Akshay", ""], ["Mehdad", "Yashar", ""], ["Laptev", "Nikolay", ""]]}, {"id": "1705.05769", "submitter": "Varun Ojha", "authors": "Varun Kumar Ojha, Vaclav Snasel, Ajith Abraham", "title": "Multiobjective Programming for Type-2 Hierarchical Fuzzy Inference Trees", "comments": null, "journal-ref": "IEEE Transactions on Fuzzy Systems 2017", "doi": "10.1109/TFUZZ.2017.2698399", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a design of hierarchical fuzzy inference tree (HFIT). An\nHFIT produces an optimum treelike structure, i.e., a natural hierarchical\nstructure that accommodates simplicity by combining several low-dimensional\nfuzzy inference systems (FISs). Such a natural hierarchical structure provides\na high degree of approximation accuracy. The construction of HFIT takes place\nin two phases. Firstly, a nondominated sorting based multiobjective genetic\nprogramming (MOGP) is applied to obtain a simple tree structure (a low\ncomplexity model) with a high accuracy. Secondly, the differential evolution\nalgorithm is applied to optimize the obtained tree's parameters. In the derived\ntree, each node acquires a different input's combination, where the\nevolutionary process governs the input's combination. Hence, HFIT nodes are\nheterogeneous in nature, which leads to a high diversity among the rules\ngenerated by the HFIT. Additionally, the HFIT provides an automatic feature\nselection because it uses MOGP for the tree's structural optimization that\naccepts inputs only relevant to the knowledge contained in data. The HFIT was\nstudied in the context of both type-1 and type-2 FISs, and its performance was\nevaluated through six application problems. Moreover, the proposed\nmultiobjective HFIT was compared both theoretically and empirically with\nrecently proposed FISs methods from the literature, such as McIT2FIS,\nTSCIT2FNN, SIT2FNN, RIT2FNS-WB, eT2FIS, MRIT2NFS, IT2FNN-SVR, etc. From the\nobtained results, it was found that the HFIT provided less complex and highly\naccurate models compared to the models produced by the most of other methods.\nHence, the proposed HFIT is an efficient and competitive alternative to the\nother FISs for function approximation and feature selection.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 15:34:19 GMT"}], "update_date": "2017-05-17", "authors_parsed": [["Ojha", "Varun Kumar", ""], ["Snasel", "Vaclav", ""], ["Abraham", "Ajith", ""]]}, {"id": "1705.05785", "submitter": "Sebastijan Dumancic", "authors": "Sebastijan Duman\\v{c}i\\'c and Hendrik Blockeel", "title": "Demystifying Relational Latent Representations", "comments": "12 pages, 8 figures; accepted to ILP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latent features learned by deep learning approaches have proven to be a\npowerful tool for machine learning. They serve as a data abstraction that makes\nlearning easier by capturing regularities in data explicitly. Their benefits\nmotivated their adaptation to relational learning context. In our previous\nwork, we introduce an approach that learns relational latent features by means\nof clustering instances and their relations. The major drawback of latent\nrepresentations is that they are often black-box and difficult to interpret.\nThis work addresses these issues and shows that (1) latent features created by\nclustering are interpretable and capture interesting properties of data; (2)\nthey identify local regions of instances that match well with the label, which\npartially explains their benefit; and (3) although the number of latent\nfeatures generated by this approach is large, often many of them are highly\nredundant and can be removed without hurting performance much.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 16:06:59 GMT"}, {"version": "v2", "created": "Thu, 28 Sep 2017 13:46:17 GMT"}, {"version": "v3", "created": "Fri, 29 Sep 2017 07:36:32 GMT"}], "update_date": "2017-10-02", "authors_parsed": [["Duman\u010di\u0107", "Sebastijan", ""], ["Blockeel", "Hendrik", ""]]}, {"id": "1705.05884", "submitter": "Babak Toghiani-Rizi", "authors": "Babak Toghiani-Rizi, Christofer Lind, Maria Svensson, Marcus Windmark", "title": "Static Gesture Recognition using Leap Motion", "comments": "Results based on a study conducted during the course Intelligent\n  Interactive Systems at Uppsala University, spring 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report, an automated bartender system was developed for making orders\nin a bar using hand gestures. The gesture recognition of the system was\ndeveloped using Machine Learning techniques, where the model was trained to\nclassify gestures using collected data. The final model used in the system\nreached an average accuracy of 95%. The system raised ethical concerns both in\nterms of user interaction and having such a system in a real world scenario,\nbut it could initially work as a complement to a real bartender.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 19:38:20 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Toghiani-Rizi", "Babak", ""], ["Lind", "Christofer", ""], ["Svensson", "Maria", ""], ["Windmark", "Marcus", ""]]}, {"id": "1705.05935", "submitter": "Ricard Sole", "authors": "Ricard Sole", "title": "Rise of the humanbot", "comments": "3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The accelerated path of technological development, particularly at the\ninterface between hardware and biology has been suggested as evidence for\nfuture major technological breakthroughs associated to our potential to\novercome biological constraints. This includes the potential of becoming\nimmortal, having expanded cognitive capacities thanks to hardware implants or\nthe creation of intelligent machines. Here I argue that several relevant\nevolutionary and structural constraints might prevent achieving most (if not\nall) these innovations. Instead, the coming future will bring novelties that\nwill challenge many other aspects of our life and that can be seen as other\nfeasible singularities. One particularly important one has to do with the\nevolving interactions between humans and non-intelligent robots capable of\nlearning and communication. Here I argue that a long term interaction can lead\nto a new class of \"agent\" (the humanbot). The way shared memories get tangled\nover time will inevitably have important consequences for both sides of the\npair, whose identity as separated entities might become blurred and ultimately\nvanish. Understanding such hybrid systems requires a second-order neuroscience\napproach while posing serious conceptual challenges, including the definition\nof consciousness.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 21:46:17 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Sole", "Ricard", ""]]}, {"id": "1705.05983", "submitter": "Chien-Ping Lu", "authors": "Chien-Ping Lu", "title": "AI, Native Supercomputing and The Revival of Moore's Law", "comments": "17 pages, 13 figures; to be published in IEEE APSIPA Transaction on\n  Signal and Information Processing as an invited paper on Industrial\n  Technology Advances", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on Alan Turing's proposition on AI and computing machinery, which\nshaped Computing as we know it today, the new AI computing machinery should\ncomprise a universal computer and a universal learning machine. The later\nshould understand linear algebra natively to overcome the slowdown of Moore's\nlaw. In such a universal learnig machine, a computing unit does not need to\nkeep the legacy of a universal computing core. The data can be distributed to\nthe computing units, and the results can be collected from them through\nCollective Streaming, reminiscent of Collective Communication in\nSupercomputing. It is not necessary to use a GPU-like deep memory hierarchy,\nnor a TPU-like fine-grain mesh.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 02:15:27 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 16:30:39 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Lu", "Chien-Ping", ""]]}, {"id": "1705.05986", "submitter": "Srinivasan Parthasarathy", "authors": "Yanjie Fu, Charu Aggarwal, Srinivasan Parthasarathy, Deepak S. Turaga,\n  Hui Xiong", "title": "REMIX: Automated Exploration for Interactive Outlier Detection", "comments": "To appear in KDD 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outlier detection is the identification of points in a dataset that do not\nconform to the norm. Outlier detection is highly sensitive to the choice of the\ndetection algorithm and the feature subspace used by the algorithm. Extracting\ndomain-relevant insights from outliers needs systematic exploration of these\nchoices since diverse outlier sets could lead to complementary insights. This\nchallenge is especially acute in an interactive setting, where the choices must\nbe explored in a time-constrained manner. In this work, we present REMIX, the\nfirst system to address the problem of outlier detection in an interactive\nsetting. REMIX uses a novel mixed integer programming (MIP) formulation for\nautomatically selecting and executing a diverse set of outlier detectors within\na time limit. This formulation incorporates multiple aspects such as (i) an\nupper limit on the total execution time of detectors (ii) diversity in the\nspace of algorithms and features, and (iii) meta-learning for evaluating the\ncost and utility of detectors. REMIX provides two distinct ways for the analyst\nto consume its results: (i) a partitioning of the detectors explored by REMIX\ninto perspectives through low-rank non-negative matrix factorization; each\nperspective can be easily visualized as an intuitive heatmap of experiments\nversus outliers, and (ii) an ensembled set of outliers which combines outlier\nscores from all detectors. We demonstrate the benefits of REMIX through\nextensive empirical validation on real-world data.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 02:17:48 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Fu", "Yanjie", ""], ["Aggarwal", "Charu", ""], ["Parthasarathy", "Srinivasan", ""], ["Turaga", "Deepak S.", ""], ["Xiong", "Hui", ""]]}, {"id": "1705.06058", "submitter": "Marius Lindauer", "authors": "Katharina Eggensperger, Marius Lindauer, Frank Hutter", "title": "Pitfalls and Best Practices in Algorithm Configuration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Good parameter settings are crucial to achieve high performance in many areas\nof artificial intelligence (AI), such as propositional satisfiability solving,\nAI planning, scheduling, and machine learning (in particular deep learning).\nAutomated algorithm configuration methods have recently received much attention\nin the AI community since they replace tedious, irreproducible and error-prone\nmanual parameter tuning and can lead to new state-of-the-art performance.\nHowever, practical applications of algorithm configuration are prone to several\n(often subtle) pitfalls in the experimental design that can render the\nprocedure ineffective. We identify several common issues and propose best\npractices for avoiding them. As one possibility for automatically handling as\nmany of these as possible, we also propose a tool called GenericWrapper4AC.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 09:07:46 GMT"}, {"version": "v2", "created": "Thu, 11 Oct 2018 11:24:19 GMT"}, {"version": "v3", "created": "Thu, 28 Mar 2019 10:25:29 GMT"}], "update_date": "2019-03-29", "authors_parsed": [["Eggensperger", "Katharina", ""], ["Lindauer", "Marius", ""], ["Hutter", "Frank", ""]]}, {"id": "1705.06243", "submitter": "Jaeyong Sung", "authors": "Jaeyong Sung, J. Kenneth Salisbury, Ashutosh Saxena", "title": "Learning to Represent Haptic Feedback for Partially-Observable Tasks", "comments": "IEEE International Conference on Robotics and Automation (ICRA), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sense of touch, being the earliest sensory system to develop in a human\nbody [1], plays a critical part of our daily interaction with the environment.\nIn order to successfully complete a task, many manipulation interactions\nrequire incorporating haptic feedback. However, manually designing a feedback\nmechanism can be extremely challenging. In this work, we consider manipulation\ntasks that need to incorporate tactile sensor feedback in order to modify a\nprovided nominal plan. To incorporate partial observation, we present a new\nframework that models the task as a partially observable Markov decision\nprocess (POMDP) and learns an appropriate representation of haptic feedback\nwhich can serve as the state for a POMDP model. The model, that is parametrized\nby deep recurrent neural networks, utilizes variational Bayes methods to\noptimize the approximate posterior. Finally, we build on deep Q-learning to be\nable to select the optimal action in each state without access to a simulator.\nWe test our model on a PR2 robot for multiple tasks of turning a knob until it\nclicks.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 16:21:56 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Sung", "Jaeyong", ""], ["Salisbury", "J. Kenneth", ""], ["Saxena", "Ashutosh", ""]]}, {"id": "1705.06273", "submitter": "Franck Dernoncourt", "authors": "Ji Young Lee, Franck Dernoncourt, Peter Szolovits", "title": "Transfer Learning for Named-Entity Recognition with Neural Networks", "comments": "The first two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent approaches based on artificial neural networks (ANNs) have shown\npromising results for named-entity recognition (NER). In order to achieve high\nperformances, ANNs need to be trained on a large labeled dataset. However,\nlabels might be difficult to obtain for the dataset on which the user wants to\nperform NER: label scarcity is particularly pronounced for patient note\nde-identification, which is an instance of NER. In this work, we analyze to\nwhat extent transfer learning may address this issue. In particular, we\ndemonstrate that transferring an ANN model trained on a large labeled dataset\nto another dataset with a limited number of labels improves upon the\nstate-of-the-art results on two different datasets for patient note\nde-identification.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 17:45:15 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Lee", "Ji Young", ""], ["Dernoncourt", "Franck", ""], ["Szolovits", "Peter", ""]]}, {"id": "1705.06338", "submitter": "Bibek Behera", "authors": "Bibek Behera, Manoj Joshi, Abhilash KK, Mohammad Ansari Ismail", "title": "Distributed Vector Representation Of Shopping Items, The Customer And\n  Shopping Cart To Build A Three Fold Recommendation System", "comments": "Cicling 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The main idea of this paper is to represent shopping items through vectors\nbecause these vectors act as the base for building em- beddings for customers\nand shopping carts. Also, these vectors are input to the mathematical models\nthat act as either a recommendation engine or help in targeting potential\ncustomers. We have used exponential family embeddings as the tool to construct\ntwo basic vectors - product embeddings and context vectors. Using the basic\nvectors, we build combined embeddings, trip embeddings and customer embeddings.\nCombined embeddings mix linguistic properties of product names with their\nshopping patterns. The customer embeddings establish an understand- ing of the\nbuying pattern of customers in a group and help in building customer profile.\nFor example a customer profile can represent customers frequently buying\npet-food. Identifying such profiles can help us bring out offers and discounts.\nSimilarly, trip embeddings are used to build trip profiles. People happen to\nbuy similar set of products in a trip and hence their trip embeddings can be\nused to predict the next product they would like to buy. This is a novel\ntechnique and the first of its kind to make recommendation using product, trip\nand customer embeddings.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 20:28:14 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Behera", "Bibek", ""], ["Joshi", "Manoj", ""], ["KK", "Abhilash", ""], ["Ismail", "Mohammad Ansari", ""]]}, {"id": "1705.06342", "submitter": "Thommen Karimpanal George", "authors": "Thommen George Karimpanal, Erik Wilhelm", "title": "Identification and Off-Policy Learning of Multiple Objectives Using\n  Adaptive Clustering", "comments": "Accepted in Neurocomputing: Special Issue on Multiobjective\n  Reinforcement Learning: Theory and Applications, 24 pages, 6 figures", "journal-ref": "Neurocomputing 263, 39-47, 2017", "doi": "10.1016/j.neucom.2017.04.074", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a methodology that enables an agent to make\nefficient use of its exploratory actions by autonomously identifying possible\nobjectives in its environment and learning them in parallel. The identification\nof objectives is achieved using an online and unsupervised adaptive clustering\nalgorithm. The identified objectives are learned (at least partially) in\nparallel using Q-learning. Using a simulated agent and environment, it is shown\nthat the converged or partially converged value function weights resulting from\noff-policy learning can be used to accumulate knowledge about multiple\nobjectives without any additional exploration. We claim that the proposed\napproach could be useful in scenarios where the objectives are initially\nunknown or in real world scenarios where exploration is typically a time and\nenergy intensive process. The implications and possible extensions of this work\nare also briefly discussed.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 20:55:15 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Karimpanal", "Thommen George", ""], ["Wilhelm", "Erik", ""]]}, {"id": "1705.06366", "submitter": "Carlos Florensa", "authors": "Carlos Florensa, David Held, Xinyang Geng, Pieter Abbeel", "title": "Automatic Goal Generation for Reinforcement Learning Agents", "comments": "Accepted at ICML 2018, Proceedings of the 35th International\n  Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning is a powerful technique to train an agent to perform a\ntask. However, an agent that is trained using reinforcement learning is only\ncapable of achieving the single task that is specified via its reward function.\nSuch an approach does not scale well to settings in which an agent needs to\nperform a diverse set of tasks, such as navigating to varying positions in a\nroom or moving objects to varying locations. Instead, we propose a method that\nallows an agent to automatically discover the range of tasks that it is capable\nof performing. We use a generator network to propose tasks for the agent to try\nto achieve, specified as goal states. The generator network is optimized using\nadversarial training to produce tasks that are always at the appropriate level\nof difficulty for the agent. Our method thus automatically produces a\ncurriculum of tasks for the agent to learn. We show that, by using this\nframework, an agent can efficiently and automatically learn to perform a wide\nset of tasks without requiring any prior knowledge of its environment. Our\nmethod can also learn to achieve tasks with sparse rewards, which traditionally\npose significant challenges.\n", "versions": [{"version": "v1", "created": "Wed, 17 May 2017 23:05:46 GMT"}, {"version": "v2", "created": "Wed, 31 May 2017 17:46:36 GMT"}, {"version": "v3", "created": "Mon, 4 Jun 2018 07:43:29 GMT"}, {"version": "v4", "created": "Tue, 17 Jul 2018 16:25:52 GMT"}, {"version": "v5", "created": "Mon, 23 Jul 2018 09:25:37 GMT"}], "update_date": "2018-07-24", "authors_parsed": [["Florensa", "Carlos", ""], ["Held", "David", ""], ["Geng", "Xinyang", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1705.06390", "submitter": "Jaroslaw Zola", "authors": "Subhadeep Karan and Jaroslaw Zola", "title": "Scalable Exact Parent Sets Identification in Bayesian Networks Learning\n  with Apache Spark", "comments": null, "journal-ref": null, "doi": "10.1109/HiPC.2017.00014", "report-no": null, "categories": "cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Machine Learning, the parent set identification problem is to find a set\nof random variables that best explain selected variable given the data and some\npredefined scoring function. This problem is a critical component to structure\nlearning of Bayesian networks and Markov blankets discovery, and thus has many\npractical applications, ranging from fraud detection to clinical decision\nsupport. In this paper, we introduce a new distributed memory approach to the\nexact parent sets assignment problem. To achieve scalability, we derive\ntheoretical bounds to constraint the search space when MDL scoring function is\nused, and we reorganize the underlying dynamic programming such that the\ncomputational density is increased and fine-grain synchronization is\neliminated. We then design efficient realization of our approach in the Apache\nSpark platform. Through experimental results, we demonstrate that the method\nmaintains strong scalability on a 500-core standalone Spark cluster, and it can\nbe used to efficiently process data sets with 70 variables, far beyond the\nreach of the currently available solutions.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 01:50:04 GMT"}, {"version": "v2", "created": "Tue, 24 Oct 2017 20:24:01 GMT"}], "update_date": "2019-01-09", "authors_parsed": [["Karan", "Subhadeep", ""], ["Zola", "Jaroslaw", ""]]}, {"id": "1705.06431", "submitter": "Rami Daknama", "authors": "Rami Daknama and Elisabeth Kraus", "title": "Vehicle Routing with Drones", "comments": "24 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.AI math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a package service model where trucks as well as drones can\ndeliver packages. Drones can travel on trucks or fly; but while flying, drones\ncan only carry one package at a time and have to return to a truck to charge\nafter each delivery. We present a heuristic algorithm to solve the problem of\nfinding a good schedule for all drones and trucks. The algorithm is based on\ntwo nested local searches, thus the definition of suitable neighbourhoods of\nsolutions is crucial for the algorithm. Empirical tests show that our algorithm\nperforms significantly better than a natural Greedy algorithm. Moreover, the\nsavings compared to solutions without drones turn out to be substantial,\nsuggesting that delivery systems might considerably benefit from using drones\nin addition to trucks.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 06:29:56 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Daknama", "Rami", ""], ["Kraus", "Elisabeth", ""]]}, {"id": "1705.06460", "submitter": "Mahardhika Pratama Dr", "authors": "Mahardhika Pratama, Witold Pedrycz, Edwin Lughofer", "title": "Evolving Ensemble Fuzzy Classifier", "comments": "this paper has been published by IEEE Transactions on Fuzzy Systems", "journal-ref": "IEEE Transactions on Fuzzy Systems, 2018", "doi": "10.1109/TFUZZ.2018.2796099", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The concept of ensemble learning offers a promising avenue in learning from\ndata streams under complex environments because it addresses the bias and\nvariance dilemma better than its single model counterpart and features a\nreconfigurable structure, which is well suited to the given context. While\nvarious extensions of ensemble learning for mining non-stationary data streams\ncan be found in the literature, most of them are crafted under a static base\nclassifier and revisits preceding samples in the sliding window for a\nretraining step. This feature causes computationally prohibitive complexity and\nis not flexible enough to cope with rapidly changing environments. Their\ncomplexities are often demanding because it involves a large collection of\noffline classifiers due to the absence of structural complexities reduction\nmechanisms and lack of an online feature selection mechanism. A novel evolving\nensemble classifier, namely Parsimonious Ensemble pENsemble, is proposed in\nthis paper. pENsemble differs from existing architectures in the fact that it\nis built upon an evolving classifier from data streams, termed Parsimonious\nClassifier pClass. pENsemble is equipped by an ensemble pruning mechanism,\nwhich estimates a localized generalization error of a base classifier. A\ndynamic online feature selection scenario is integrated into the pENsemble.\nThis method allows for dynamic selection and deselection of input features on\nthe fly. pENsemble adopts a dynamic ensemble structure to output a final\nclassification decision where it features a novel drift detection scenario to\ngrow the ensemble structure. The efficacy of the pENsemble has been numerically\ndemonstrated through rigorous numerical studies with dynamic and evolving data\nstreams where it delivers the most encouraging performance in attaining a\ntradeoff between accuracy and complexity.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 08:19:41 GMT"}, {"version": "v2", "created": "Sat, 7 Dec 2019 20:26:52 GMT"}], "update_date": "2019-12-10", "authors_parsed": [["Pratama", "Mahardhika", ""], ["Pedrycz", "Witold", ""], ["Lughofer", "Edwin", ""]]}, {"id": "1705.06564", "submitter": "J\\\"org P\\\"uhrer", "authors": "Johannes Oetsch, J\\\"org P\\\"uhrer, Hans Tompits", "title": "Stepwise Debugging of Answer-Set Programs", "comments": "Under consideration in Theory and Practice of Logic Programming\n  (TPLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a stepping methodology for answer-set programming (ASP) that\nallows for debugging answer-set programs and is based on the stepwise\napplication of rules. Similar to debugging in imperative languages, where the\nbehaviour of a program is observed during a step-by-step execution, stepping\nfor ASP allows for observing the effects that rule applications have in the\ncomputation of an answer set. While the approach is inspired from debugging in\nimperative programming, it is conceptually different to stepping in other\nparadigms due to non-determinism and declarativity that are inherent to ASP. In\nparticular, unlike statements in an imperative program that are executed\nfollowing a strict control flow, there is no predetermined order in which to\nconsider rules in ASP during a computation. In our approach, the user is free\nto decide which rule to consider active in the next step following his or her\nintuition. This way, one can focus on interesting parts of the debugging search\nspace. Bugs are detected during stepping by revealing differences between the\nactual semantics of the program and the expectations of the user. As a solid\nformal basis for stepping, we develop a framework of computations for\nanswer-set programs. For fully supporting different solver languages, we build\nour framework on an abstract ASP language that is sufficiently general to\ncapture different solver languages. To this end, we make use of abstract\nconstraints as an established abstraction for popular language constructs such\nas aggregates. Stepping has been implemented in SeaLion, an integrated\ndevelopment environment for ASP. We illustrate stepping using an example\nscenario and discuss the stepping plugin of SeaLion. Moreover, we elaborate on\nmethodological aspects and the embedding of stepping in the ASP development\nprocess.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 13:02:19 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Oetsch", "Johannes", ""], ["P\u00fchrer", "J\u00f6rg", ""], ["Tompits", "Hans", ""]]}, {"id": "1705.06573", "submitter": "Pontus Svenson", "authors": "Magnus J\\\"andel, Pontus Svenson, Niclas Wadstr\\\"omer", "title": "Online learnability of Statistical Relational Learning in anomaly\n  detection", "comments": "8 pages. Author contact xpontus@gmail.com", "journal-ref": "Proc 15th Int Conf Information Fusion (2012)", "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical Relational Learning (SRL) methods for anomaly detection are\nintroduced via a security-related application. Operational requirements for\nonline learning stability are outlined and compared to mathematical definitions\nas applied to the learning process of a representative SRL method - Bayesian\nLogic Programs (BLP). Since a formal proof of online stability appears to be\nimpossible, tentative common sense requirements are formulated and tested by\ntheoretical and experimental analysis of a simple and analytically tractable\nBLP model. It is found that learning algorithms in initial stages of online\nlearning can lock on unstable false predictors that nevertheless comply with\nour tentative stability requirements and thus masquerade as bona fide\nsolutions. The very expressiveness of SRL seems to cause significant stability\nissues in settings with many variables and scarce data. We conclude that\nreliable anomaly detection with SRL-methods requires monitoring by an\noverarching framework that may involve a comprehensive context knowledge base\nor human supervision.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 13:14:43 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["J\u00e4ndel", "Magnus", ""], ["Svenson", "Pontus", ""], ["Wadstr\u00f6mer", "Niclas", ""]]}, {"id": "1705.06578", "submitter": "Wen Jiang", "authors": "Zichang He and Wen Jiang", "title": "An evidential Markov decision making model", "comments": "38 pages, 7 figures. arXiv admin note: text overlap with\n  arXiv:1703.02386", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI math.DS math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sure thing principle and the law of total probability are basic laws in\nclassic probability theory. A disjunction fallacy leads to the violation of\nthese two classical laws. In this paper, an Evidential Markov (EM) decision\nmaking model based on Dempster-Shafer (D-S) evidence theory and Markov\nmodelling is proposed to address this issue and model the real human\ndecision-making process. In an evidential framework, the states are extended by\nintroducing an uncertain state which represents the hesitance of a decision\nmaker. The classical Markov model can not produce the disjunction effect, which\nassumes that a decision has to be certain at one time. However, the state is\nallowed to be uncertain in the EM model before the final decision is made. An\nextra uncertainty degree parameter is defined by a belief entropy, named Deng\nentropy, to assignment the basic probability assignment of the uncertain state,\nwhich is the key to predict the disjunction effect. A classical categorization\ndecision-making experiment is used to illustrate the effectiveness and validity\nof EM model. The disjunction effect can be well predicted and the free\nparameters are less compared with the existing models.\n", "versions": [{"version": "v1", "created": "Wed, 10 May 2017 07:59:41 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["He", "Zichang", ""], ["Jiang", "Wen", ""]]}, {"id": "1705.06694", "submitter": "Tommy Nilsson", "authors": "Kevin K. Bowden, Tommy Nilsson, Christine P. Spencer, Kubra Cengiz,\n  Alexandru Ghitulescu, Jelte B. van Waterschoot", "title": "I Probe, Therefore I Am: Designing a Virtual Journalist with Human\n  Emotions", "comments": "eNTERFACE16 proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By utilizing different communication channels, such as verbal language,\ngestures or facial expressions, virtually embodied interactive humans hold a\nunique potential to bridge the gap between human-computer interaction and\nactual interhuman communication. The use of virtual humans is consequently\nbecoming increasingly popular in a wide range of areas where such a natural\ncommunication might be beneficial, including entertainment, education, mental\nhealth research and beyond. Behind this development lies a series of\ntechnological advances in a multitude of disciplines, most notably natural\nlanguage processing, computer vision, and speech synthesis. In this paper we\ndiscuss a Virtual Human Journalist, a project employing a number of novel\nsolutions from these disciplines with the goal to demonstrate their viability\nby producing a humanoid conversational agent capable of naturally eliciting and\nreacting to information from a human user. A set of qualitative and\nquantitative evaluation sessions demonstrated the technical feasibility of the\nsystem whilst uncovering a number of deficits in its capacity to engage users\nin a way that would be perceived as natural and emotionally engaging. We argue\nthat naturalness should not always be seen as a desirable goal and suggest that\ndeliberately suppressing the naturalness of virtual human interactions, such as\nby altering its personality cues, might in some cases yield more desirable\nresults.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 17:01:10 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Bowden", "Kevin K.", ""], ["Nilsson", "Tommy", ""], ["Spencer", "Christine P.", ""], ["Cengiz", "Kubra", ""], ["Ghitulescu", "Alexandru", ""], ["van Waterschoot", "Jelte B.", ""]]}, {"id": "1705.06709", "submitter": "Zhuolin Jiang", "authors": "Zhuolin Jiang, Viktor Rozgic, Sancar Adali", "title": "Learning Spatiotemporal Features for Infrared Action Recognition with 3D\n  Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infrared (IR) imaging has the potential to enable more robust action\nrecognition systems compared to visible spectrum cameras due to lower\nsensitivity to lighting conditions and appearance variability. While the action\nrecognition task on videos collected from visible spectrum imaging has received\nmuch attention, action recognition in IR videos is significantly less explored.\nOur objective is to exploit imaging data in this modality for the action\nrecognition task. In this work, we propose a novel two-stream 3D convolutional\nneural network (CNN) architecture by introducing the discriminative code layer\nand the corresponding discriminative code loss function. The proposed network\nprocesses IR image and the IR-based optical flow field sequences. We pretrain\nthe 3D CNN model on the visible spectrum Sports-1M action dataset and finetune\nit on the Infrared Action Recognition (InfAR) dataset. To our best knowledge,\nthis is the first application of the 3D CNN to action recognition in the IR\ndomain. We conduct an elaborate analysis of different fusion schemes (weighted\naverage, single and double-layer neural nets) applied to different 3D CNN\noutputs. Experimental results demonstrate that our approach can achieve\nstate-of-the-art average precision (AP) performances on the InfAR dataset: (1)\nthe proposed two-stream 3D CNN achieves the best reported 77.5% AP, and (2) our\n3D CNN model applied to the optical flow fields achieves the best reported\nsingle stream 75.42% AP.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 17:26:34 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Jiang", "Zhuolin", ""], ["Rozgic", "Viktor", ""], ["Adali", "Sancar", ""]]}, {"id": "1705.06715", "submitter": "Suleiman Yerima", "authors": "Feng Yao, Suleiman Y. Yerima, BooJoong Kang, Sakir Sezer", "title": "Continuous Implicit Authentication for Mobile Devices based on Adaptive\n  Neuro-Fuzzy Inference System", "comments": "International Conference on Cyber Security and Protection of Digital\n  Services (Cyber Security 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As mobile devices have become indispensable in modern life, mobile security\nis becoming much more important. Traditional password or PIN-like\npoint-of-entry security measures score low on usability and are vulnerable to\nbrute force and other types of attacks. In order to improve mobile security, an\nadaptive neuro-fuzzy inference system(ANFIS)-based implicit authentication\nsystem is proposed in this paper to provide authentication in a continuous and\ntransparent manner.To illustrate the applicability and capability of ANFIS in\nour implicit authentication system, experiments were conducted on behavioural\ndata collected for up to 12 weeks from different Android users. The ability of\nthe ANFIS-based system to detect an adversary is also tested with scenarios\ninvolving an attacker with varying levels of knowledge. The results demonstrate\nthat ANFIS is a feasible and efficient approach for implicit authentication\nwith an average of 95% user recognition rate. Moreover, the use of ANFIS-based\nsystem for implicit authentication significantly reduces manual tuning and\nconfiguration tasks due to its selflearning capability.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 17:38:06 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Yao", "Feng", ""], ["Yerima", "Suleiman Y.", ""], ["Kang", "BooJoong", ""], ["Sezer", "Sakir", ""]]}, {"id": "1705.06769", "submitter": "Nat Dilokthanakul", "authors": "Nat Dilokthanakul, Christos Kaplanis, Nick Pawlowski, Murray Shanahan", "title": "Feature Control as Intrinsic Motivation for Hierarchical Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": "10.1109/TNNLS.2019.2891792", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of sparse rewards is one of the hardest challenges in\ncontemporary reinforcement learning. Hierarchical reinforcement learning (HRL)\ntackles this problem by using a set of temporally-extended actions, or options,\neach of which has its own subgoal. These subgoals are normally handcrafted for\nspecific tasks. Here, though, we introduce a generic class of subgoals with\nbroad applicability in the visual domain. Underlying our approach (in common\nwith work using \"auxiliary tasks\") is the hypothesis that the ability to\ncontrol aspects of the environment is an inherently useful skill to have. We\nincorporate such subgoals in an end-to-end hierarchical reinforcement learning\nsystem and test two variants of our algorithm on a number of games from the\nAtari suite. We highlight the advantage of our approach in one of the hardest\ngames -- Montezuma's revenge -- for which the ability to handle sparse rewards\nis key. Our agent learns several times faster than the current state-of-the-art\nHRL agent in this game, reaching a similar level of performance. UPDATE\n22/11/17: We found that a standard A3C agent with a simple shaped reward, i.e.\nextrinsic reward + feature control intrinsic reward, has comparable performance\nto our agent in Montezuma Revenge. In light of the new experiments performed,\nthe advantage of our HRL approach can be attributed more to its ability to\nlearn useful features from intrinsic rewards rather than its ability to explore\nand reuse abstracted skills with hierarchical components. This has led us to a\nnew conclusion about the result.\n", "versions": [{"version": "v1", "created": "Thu, 18 May 2017 19:00:43 GMT"}, {"version": "v2", "created": "Wed, 22 Nov 2017 15:56:19 GMT"}], "update_date": "2020-01-14", "authors_parsed": [["Dilokthanakul", "Nat", ""], ["Kaplanis", "Christos", ""], ["Pawlowski", "Nick", ""], ["Shanahan", "Murray", ""]]}, {"id": "1705.06840", "submitter": "Nicholas Mattei", "authors": "Jing Wu Lian, Nicholas Mattei, Renee Noble, and Toby Walsh", "title": "The Conference Paper Assignment Problem: Using Order Weighted Averages\n  to Assign Indivisible Goods", "comments": "3 Figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the common academic problem of allocating papers to referees for\nconference reviewing we propose a novel mechanism for solving the assignment\nproblem when we have a two sided matching problem with preferences from one\nside (the agents/reviewers) over the other side (the objects/papers) and both\nsides have capacity constraints. The assignment problem is a fundamental\nproblem in both computer science and economics with application in many areas\nincluding task and resource allocation. We draw inspiration from multi-criteria\ndecision making and voting and use order weighted averages (OWAs) to propose a\nnovel and flexible class of algorithms for the assignment problem. We show an\nalgorithm for finding a $\\Sigma$-OWA assignment in polynomial time, in contrast\nto the NP-hardness of finding an egalitarian assignment. Inspired by this\nsetting we observe an interesting connection between our model and the classic\nproportional multi-winner election problem in social choice.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 00:51:53 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Lian", "Jing Wu", ""], ["Mattei", "Nicholas", ""], ["Noble", "Renee", ""], ["Walsh", "Toby", ""]]}, {"id": "1705.06927", "submitter": "Mark Kaminski", "authors": "Mark Kaminski, Bernardo Cuenca Grau, Egor V. Kostylev, Boris Motik and\n  Ian Horrocks", "title": "Foundations of Declarative Data Analysis Using Limit Datalog Programs", "comments": "23 pages; full version of a paper accepted at IJCAI-17; v2 fixes some\n  typos and improves the acknowledgments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by applications in declarative data analysis, we study\n$\\mathit{Datalog}_{\\mathbb{Z}}$---an extension of positive Datalog with\narithmetic functions over integers. This language is known to be undecidable,\nso we propose two fragments. In $\\mathit{limit}~\\mathit{Datalog}_{\\mathbb{Z}}$\npredicates are axiomatised to keep minimal/maximal numeric values, allowing us\nto show that fact entailment is coNExpTime-complete in combined, and\ncoNP-complete in data complexity. Moreover, an additional $\\mathit{stability}$\nrequirement causes the complexity to drop to ExpTime and PTime, respectively.\nFinally, we show that stable $\\mathit{Datalog}_{\\mathbb{Z}}$ can express many\nuseful data analysis tasks, and so our results provide a sound foundation for\nthe development of advanced information systems.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 10:42:12 GMT"}, {"version": "v2", "created": "Sun, 12 Nov 2017 13:14:13 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Kaminski", "Mark", ""], ["Grau", "Bernardo Cuenca", ""], ["Kostylev", "Egor V.", ""], ["Motik", "Boris", ""], ["Horrocks", "Ian", ""]]}, {"id": "1705.06936", "submitter": "Tomasz Grel", "authors": "Robert Adamski, Tomasz Grel, Maciej Klimek and Henryk Michalewski", "title": "Atari games and Intel processors", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-319-75931-9_1", "report-no": null, "categories": "cs.DC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The asynchronous nature of the state-of-the-art reinforcement learning\nalgorithms such as the Asynchronous Advantage Actor-Critic algorithm, makes\nthem exceptionally suitable for CPU computations. However, given the fact that\ndeep reinforcement learning often deals with interpreting visual information, a\nlarge part of the train and inference time is spent performing convolutions. In\nthis work we present our results on learning strategies in Atari games using a\nConvolutional Neural Network, the Math Kernel Library and TensorFlow 0.11rc0\nmachine learning framework. We also analyze effects of asynchronous\ncomputations on the convergence of reinforcement learning algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 11:19:45 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Adamski", "Robert", ""], ["Grel", "Tomasz", ""], ["Klimek", "Maciej", ""], ["Michalewski", "Henryk", ""]]}, {"id": "1705.07086", "submitter": "Emmanouil Antonios Platanios", "authors": "Emmanouil A. Platanios, Hoifung Poon, Tom M. Mitchell, Eric Horvitz", "title": "Estimating Accuracy from Unlabeled Data: A Probabilistic Logic Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an efficient method to estimate the accuracy of classifiers using\nonly unlabeled data. We consider a setting with multiple classification\nproblems where the target classes may be tied together through logical\nconstraints. For example, a set of classes may be mutually exclusive, meaning\nthat a data instance can belong to at most one of them. The proposed method is\nbased on the intuition that: (i) when classifiers agree, they are more likely\nto be correct, and (ii) when the classifiers make a prediction that violates\nthe constraints, at least one classifier must be making an error. Experiments\non four real-world data sets produce accuracy estimates within a few percent of\nthe true accuracy, using solely unlabeled data. Our models also outperform\nexisting state-of-the-art solutions in both estimating accuracies, and\ncombining multiple classifier outputs. The results emphasize the utility of\nlogical constraints in estimating accuracy, thus validating our intuition.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 16:52:52 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Platanios", "Emmanouil A.", ""], ["Poon", "Hoifung", ""], ["Mitchell", "Tom M.", ""], ["Horvitz", "Eric", ""]]}, {"id": "1705.07095", "submitter": "Ondrej Kuzelka", "authors": "Ondrej Kuzelka, Jesse Davis, Steven Schockaert", "title": "Induction of Interpretable Possibilistic Logic Theories from Relational\n  Data", "comments": "Longer version of a paper appearing in IJCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of Statistical Relational Learning (SRL) is concerned with learning\nprobabilistic models from relational data. Learned SRL models are typically\nrepresented using some kind of weighted logical formulas, which make them\nconsiderably more interpretable than those obtained by e.g. neural networks. In\npractice, however, these models are often still difficult to interpret\ncorrectly, as they can contain many formulas that interact in non-trivial ways\nand weights do not always have an intuitive meaning. To address this, we\npropose a new SRL method which uses possibilistic logic to encode relational\nmodels. Learned models are then essentially stratified classical theories,\nwhich explicitly encode what can be derived with a given level of certainty.\nCompared to Markov Logic Networks (MLNs), our method is faster and produces\nconsiderably more interpretable models.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 17:12:07 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Kuzelka", "Ondrej", ""], ["Davis", "Jesse", ""], ["Schockaert", "Steven", ""]]}, {"id": "1705.07105", "submitter": "Charalampos Nikolaou", "authors": "Charalampos Nikolaou and Egor V. Kostylev and George Konstantinidis\n  and Mark Kaminski and Bernardo Cuenca Grau and Ian Horrocks", "title": "The Bag Semantics of Ontology-Based Data Access", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ontology-based data access (OBDA) is a popular approach for integrating and\nquerying multiple data sources by means of a shared ontology. The ontology is\nlinked to the sources using mappings, which assign views over the data to\nontology predicates. Motivated by the need for OBDA systems supporting\ndatabase-style aggregate queries, we propose a bag semantics for OBDA, where\nduplicate tuples in the views defined by the mappings are retained, as is the\ncase in standard databases. We show that bag semantics makes conjunctive query\nanswering in OBDA coNP-hard in data complexity. To regain tractability, we\nconsider a rather general class of queries and show its rewritability to a\ngeneralisation of the relational calculus to bags.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 17:33:28 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Nikolaou", "Charalampos", ""], ["Kostylev", "Egor V.", ""], ["Konstantinidis", "George", ""], ["Kaminski", "Mark", ""], ["Grau", "Bernardo Cuenca", ""], ["Horrocks", "Ian", ""]]}, {"id": "1705.07114", "submitter": "Pooyan Jamshidi", "authors": "Hamid Arabnejad, Claus Pahl, Pooyan Jamshidi, Giovani Estrada", "title": "A Comparison of Reinforcement Learning Techniques for Fuzzy Cloud\n  Auto-Scaling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A goal of cloud service management is to design self-adaptable auto-scaler to\nreact to workload fluctuations and changing the resources assigned. The key\nproblem is how and when to add/remove resources in order to meet agreed\nservice-level agreements. Reducing application cost and guaranteeing\nservice-level agreements (SLAs) are two critical factors of dynamic controller\ndesign. In this paper, we compare two dynamic learning strategies based on a\nfuzzy logic system, which learns and modifies fuzzy scaling rules at runtime. A\nself-adaptive fuzzy logic controller is combined with two reinforcement\nlearning (RL) approaches: (i) Fuzzy SARSA learning (FSL) and (ii) Fuzzy\nQ-learning (FQL). As an off-policy approach, Q-learning learns independent of\nthe policy currently followed, whereas SARSA as an on-policy always\nincorporates the actual agent's behavior and leads to faster learning. Both\napproaches are implemented and compared in their advantages and disadvantages,\nhere in the OpenStack cloud platform. We demonstrate that both auto-scaling\napproaches can handle various load traffic situations, sudden and periodic, and\ndelivering resources on demand while reducing operating costs and preventing\nSLA violations. The experimental results demonstrate that FSL and FQL have\nacceptable performance in terms of adjusted number of virtual machine targeted\nto optimize SLA compliance and response time.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 17:56:42 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Arabnejad", "Hamid", ""], ["Pahl", "Claus", ""], ["Jamshidi", "Pooyan", ""], ["Estrada", "Giovani", ""]]}, {"id": "1705.07120", "submitter": "Jakub Tomczak Ph.D.", "authors": "Jakub M. Tomczak and Max Welling", "title": "VAE with a VampPrior", "comments": "16 pages, final version, AISTATS 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many different methods to train deep generative models have been introduced\nin the past. In this paper, we propose to extend the variational auto-encoder\n(VAE) framework with a new type of prior which we call \"Variational Mixture of\nPosteriors\" prior, or VampPrior for short. The VampPrior consists of a mixture\ndistribution (e.g., a mixture of Gaussians) with components given by\nvariational posteriors conditioned on learnable pseudo-inputs. We further\nextend this prior to a two layer hierarchical model and show that this\narchitecture with a coupled prior and posterior, learns significantly better\nmodels. The model also avoids the usual local optima issues related to useless\nlatent dimensions that plague VAEs. We provide empirical studies on six\ndatasets, namely, static and binary MNIST, OMNIGLOT, Caltech 101 Silhouettes,\nFrey Faces and Histopathology patches, and show that applying the hierarchical\nVampPrior delivers state-of-the-art results on all datasets in the unsupervised\npermutation invariant setting and the best results or comparable to SOTA\nmethods for the approach with convolutional networks.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 10:07:00 GMT"}, {"version": "v2", "created": "Mon, 24 Jul 2017 14:14:08 GMT"}, {"version": "v3", "created": "Mon, 21 Aug 2017 12:21:47 GMT"}, {"version": "v4", "created": "Fri, 13 Oct 2017 17:54:28 GMT"}, {"version": "v5", "created": "Mon, 26 Feb 2018 15:23:53 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Tomczak", "Jakub M.", ""], ["Welling", "Max", ""]]}, {"id": "1705.07177", "submitter": "Mikael Henaff", "authors": "Mikael Henaff, William F. Whitney, Yann LeCun", "title": "Model-Based Planning with Discrete and Continuous Actions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action planning using learned and differentiable forward models of the world\nis a general approach which has a number of desirable properties, including\nimproved sample complexity over model-free RL methods, reuse of learned models\nacross different tasks, and the ability to perform efficient gradient-based\noptimization in continuous action spaces. However, this approach does not apply\nstraightforwardly when the action space is discrete. In this work, we show that\nit is in fact possible to effectively perform planning via backprop in discrete\naction spaces, using a simple paramaterization of the actions vectors on the\nsimplex combined with input noise when training the forward model. Our\nexperiments show that this approach can match or outperform model-free RL and\ndiscrete planning methods on gridworld navigation tasks in terms of performance\nand/or planning time while using limited environment interactions, and can\nadditionally be used to perform model-based control in a challenging new task\nwhere the action space combines discrete and continuous actions. We furthermore\npropose a policy distillation approach which yields a fast policy network which\ncan be used at inference time, removing the need for an iterative planning\nprocedure.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 20:38:49 GMT"}, {"version": "v2", "created": "Wed, 4 Apr 2018 06:34:26 GMT"}], "update_date": "2018-04-05", "authors_parsed": [["Henaff", "Mikael", ""], ["Whitney", "William F.", ""], ["LeCun", "Yann", ""]]}, {"id": "1705.07215", "submitter": "Naveen Kodali", "authors": "Naveen Kodali, Jacob Abernethy, James Hays, Zsolt Kira", "title": "On Convergence and Stability of GANs", "comments": "Analysis of convergence and mode collapse by studying GAN training\n  process as regret minimization. Some new results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.GT cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose studying GAN training dynamics as regret minimization, which is in\ncontrast to the popular view that there is consistent minimization of a\ndivergence between real and generated distributions. We analyze the convergence\nof GAN training from this new point of view to understand why mode collapse\nhappens. We hypothesize the existence of undesirable local equilibria in this\nnon-convex game to be responsible for mode collapse. We observe that these\nlocal equilibria often exhibit sharp gradients of the discriminator function\naround some real data points. We demonstrate that these degenerate local\nequilibria can be avoided with a gradient penalty scheme called DRAGAN. We show\nthat DRAGAN enables faster training, achieves improved stability with fewer\nmode collapses, and leads to generator networks with better modeling\nperformance across a variety of architectures and objective functions.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 22:41:56 GMT"}, {"version": "v2", "created": "Wed, 24 May 2017 15:13:01 GMT"}, {"version": "v3", "created": "Thu, 25 May 2017 00:51:40 GMT"}, {"version": "v4", "created": "Fri, 27 Oct 2017 21:47:51 GMT"}, {"version": "v5", "created": "Sun, 10 Dec 2017 15:24:13 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Kodali", "Naveen", ""], ["Abernethy", "Jacob", ""], ["Hays", "James", ""], ["Kira", "Zsolt", ""]]}, {"id": "1705.07224", "submitter": "Marco Cusumano-Towner", "authors": "Marco F. Cusumano-Towner, Vikash K. Mansinghka", "title": "AIDE: An algorithm for measuring the accuracy of probabilistic inference\n  algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate probabilistic inference algorithms are central to many fields.\nExamples include sequential Monte Carlo inference in robotics, variational\ninference in machine learning, and Markov chain Monte Carlo inference in\nstatistics. A key problem faced by practitioners is measuring the accuracy of\nan approximate inference algorithm on a specific data set. This paper\nintroduces the auxiliary inference divergence estimator (AIDE), an algorithm\nfor measuring the accuracy of approximate inference algorithms. AIDE is based\non the observation that inference algorithms can be treated as probabilistic\nmodels and the random variables used within the inference algorithm can be\nviewed as auxiliary variables. This view leads to a new estimator for the\nsymmetric KL divergence between the approximating distributions of two\ninference algorithms. The paper illustrates application of AIDE to algorithms\nfor inference in regression, hidden Markov, and Dirichlet process mixture\nmodels. The experiments show that AIDE captures the qualitative behavior of a\nbroad class of inference algorithms and can detect failure modes of inference\nalgorithms that are missed by standard heuristics.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 23:48:11 GMT"}, {"version": "v2", "created": "Sat, 4 Nov 2017 16:09:58 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Cusumano-Towner", "Marco F.", ""], ["Mansinghka", "Vikash K.", ""]]}, {"id": "1705.07226", "submitter": "Tjitze Rienstra", "authors": "Tjitze Rienstra", "title": "RankPL: A Qualitative Probabilistic Programming Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce RankPL, a modeling language that can be thought of\nas a qualitative variant of a probabilistic programming language with a\nsemantics based on Spohn's ranking theory. Broadly speaking, RankPL can be used\nto represent and reason about processes that exhibit uncertainty expressible by\ndistinguishing \"normal\" from\" surprising\" events. RankPL allows (iterated)\nrevision of rankings over alternative program states and supports various types\nof reasoning, including abduction and causal inference. We present the\nlanguage, its denotational semantics, and a number of practical examples. We\nalso discuss an implementation of RankPL that is available for download.\n", "versions": [{"version": "v1", "created": "Fri, 19 May 2017 23:58:26 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Rienstra", "Tjitze", ""]]}, {"id": "1705.07262", "submitter": "Daniel Hein", "authors": "Daniel Hein, Steffen Udluft, Michel Tokic, Alexander Hentschel, Thomas\n  A. Runkler, Volkmar Sterzing", "title": "Batch Reinforcement Learning on the Industrial Benchmark: First\n  Experiences", "comments": null, "journal-ref": "2017 International Joint Conference on Neural Networks (IJCNN),\n  Anchorage, AK, 2017, pp. 4214-4221", "doi": "10.1109/IJCNN.2017.7966389", "report-no": null, "categories": "cs.LG cs.AI cs.NE cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Particle Swarm Optimization Policy (PSO-P) has been recently introduced\nand proven to produce remarkable results on interacting with academic\nreinforcement learning benchmarks in an off-policy, batch-based setting. To\nfurther investigate the properties and feasibility on real-world applications,\nthis paper investigates PSO-P on the so-called Industrial Benchmark (IB), a\nnovel reinforcement learning (RL) benchmark that aims at being realistic by\nincluding a variety of aspects found in industrial applications, like\ncontinuous state and action spaces, a high dimensional, partially observable\nstate space, delayed effects, and complex stochasticity. The experimental\nresults of PSO-P on IB are compared to results of closed-form control policies\nderived from the model-based Recurrent Control Neural Network (RCNN) and the\nmodel-free Neural Fitted Q-Iteration (NFQ). Experiments show that PSO-P is not\nonly of interest for academic benchmarks, but also for real-world industrial\napplications, since it also yielded the best performing policy in our IB\nsetting. Compared to other well established RL techniques, PSO-P produced\noutstanding results in performance and robustness, requiring only a relatively\nlow amount of effort in finding adequate parameters or making complex design\ndecisions.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 05:31:52 GMT"}, {"version": "v2", "created": "Thu, 27 Jul 2017 15:34:21 GMT"}], "update_date": "2018-01-26", "authors_parsed": [["Hein", "Daniel", ""], ["Udluft", "Steffen", ""], ["Tokic", "Michel", ""], ["Hentschel", "Alexander", ""], ["Runkler", "Thomas A.", ""], ["Sterzing", "Volkmar", ""]]}, {"id": "1705.07267", "submitter": "Jiatao Gu", "authors": "Jiatao Gu, Yong Wang, Kyunghyun Cho and Victor O.K. Li", "title": "Search Engine Guided Non-Parametric Neural Machine Translation", "comments": "Accepted by AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we extend an attention-based neural machine translation (NMT)\nmodel by allowing it to access an entire training set of parallel sentence\npairs even after training. The proposed approach consists of two stages. In the\nfirst stage--retrieval stage--, an off-the-shelf, black-box search engine is\nused to retrieve a small subset of sentence pairs from a training set given a\nsource sentence. These pairs are further filtered based on a fuzzy matching\nscore based on edit distance. In the second stage--translation stage--, a novel\ntranslation model, called translation memory enhanced NMT (TM-NMT), seamlessly\nuses both the source sentence and a set of retrieved sentence pairs to perform\nthe translation. Empirical evaluation on three language pairs (En-Fr, En-De,\nand En-Es) shows that the proposed approach significantly outperforms the\nbaseline approach and the improvement is more significant when more relevant\nsentence pairs were retrieved.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 06:53:09 GMT"}, {"version": "v2", "created": "Thu, 8 Mar 2018 08:15:24 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Gu", "Jiatao", ""], ["Wang", "Yong", ""], ["Cho", "Kyunghyun", ""], ["Li", "Victor O. K.", ""]]}, {"id": "1705.07269", "submitter": "Sahil Sharma", "authors": "Sahil Sharma, Aravind Suresh, Rahul Ramesh, Balaraman Ravindran", "title": "Learning to Factor Policies and Action-Value Functions: Factored Action\n  Space Representations for Deep Reinforcement learning", "comments": "11 pages + 7 pages appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Reinforcement Learning (DRL) methods have performed well in an\nincreasing numbering of high-dimensional visual decision making domains. Among\nall such visual decision making problems, those with discrete action spaces\noften tend to have underlying compositional structure in the said action space.\nSuch action spaces often contain actions such as go left, go up as well as go\ndiagonally up and left (which is a composition of the former two actions). The\nrepresentations of control policies in such domains have traditionally been\nmodeled without exploiting this inherent compositional structure in the action\nspaces. We propose a new learning paradigm, Factored Action space\nRepresentations (FAR) wherein we decompose a control policy learned using a\nDeep Reinforcement Learning Algorithm into independent components, analogous to\ndecomposing a vector in terms of some orthogonal basis vectors. This\narchitectural modification of the control policy representation allows the\nagent to learn about multiple actions simultaneously, while executing only one\nof them. We demonstrate that FAR yields considerable improvements on top of two\nDRL algorithms in Atari 2600: FARA3C outperforms A3C (Asynchronous Advantage\nActor Critic) in 9 out of 14 tasks and FARAQL outperforms AQL (Asynchronous\nn-step Q-Learning) in 9 out of 13 tasks.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 07:18:40 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Sharma", "Sahil", ""], ["Suresh", "Aravind", ""], ["Ramesh", "Rahul", ""], ["Ravindran", "Balaraman", ""]]}, {"id": "1705.07325", "submitter": "Yu Wang", "authors": "Yu Wang, Aniket Chakrabarti, David Sivakoff, Srinivasan Parthasarathy", "title": "Fast Change Point Detection on Dynamic Social Networks", "comments": "IJCAI'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of real world problems in many domains (e.g. sociology, biology,\npolitical science and communication networks) can be modeled as dynamic\nnetworks with nodes representing entities of interest and edges representing\ninteractions among the entities at different points in time. A common\nrepresentation for such models is the snapshot model - where a network is\ndefined at logical time-stamps. An important problem under this model is change\npoint detection. In this work we devise an effective and efficient\nthree-step-approach for detecting change points in dynamic networks under the\nsnapshot model. Our algorithm achieves up to 9X speedup over the\nstate-of-the-art while improving quality on both synthetic and real world\nnetworks.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 16:08:01 GMT"}, {"version": "v2", "created": "Sun, 4 Jun 2017 16:18:36 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Wang", "Yu", ""], ["Chakrabarti", "Aniket", ""], ["Sivakoff", "David", ""], ["Parthasarathy", "Srinivasan", ""]]}, {"id": "1705.07339", "submitter": "Jin-Kao Hao", "authors": "Yi Zhou and Jin-Kao Hao", "title": "Combining tabu search and graph reduction to solve the maximum balanced\n  biclique problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Maximum Balanced Biclique Problem is a well-known graph model with\nrelevant applications in diverse domains. This paper introduces a novel\nalgorithm, which combines an effective constraint-based tabu search procedure\nand two dedicated graph reduction techniques. We verify the effectiveness of\nthe algorithm on 30 classical random benchmark graphs and 25 very large\nreal-life sparse graphs from the popular Koblenz Network Collection (KONECT).\nThe results show that the algorithm improves the best-known results (new lower\nbounds) for 10 classical benchmarks and obtains the optimal solutions for 14\nKONECT instances.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 17:47:31 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Zhou", "Yi", ""], ["Hao", "Jin-Kao", ""]]}, {"id": "1705.07343", "submitter": "Kijung Shin", "authors": "Kijung Shin, Euiwoong Lee, Dhivya Eswaran, Ariel D. Procaccia", "title": "Why You Should Charge Your Friends for Borrowing Your Stuff", "comments": "to be published in 26th International Joint Conference on Artificial\n  Intelligence (IJCAI-17)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT cs.MA cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider goods that can be shared with k-hop neighbors (i.e., the set of\nnodes within k hops from an owner) on a social network. We examine incentives\nto buy such a good by devising game-theoretic models where each node decides\nwhether to buy the good or free ride. First, we find that social inefficiency,\nspecifically excessive purchase of the good, occurs in Nash equilibria. Second,\nthe social inefficiency decreases as k increases and thus a good can be shared\nwith more nodes. Third, and most importantly, the social inefficiency can also\nbe significantly reduced by charging free riders an access cost and paying it\nto owners, leading to the conclusion that organizations and system designers\nshould impose such a cost. These findings are supported by our theoretical\nanalysis in terms of the price of anarchy and the price of stability; and by\nsimulations based on synthetic and real social networks.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 18:28:38 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Shin", "Kijung", ""], ["Lee", "Euiwoong", ""], ["Eswaran", "Dhivya", ""], ["Procaccia", "Ariel D.", ""]]}, {"id": "1705.07347", "submitter": "Xiuyuan Lu", "authors": "Xiuyuan Lu, Benjamin Van Roy", "title": "Ensemble Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thompson sampling has emerged as an effective heuristic for a broad range of\nonline decision problems. In its basic form, the algorithm requires computing\nand sampling from a posterior distribution over models, which is tractable only\nfor simple special cases. This paper develops ensemble sampling, which aims to\napproximate Thompson sampling while maintaining tractability even in the face\nof complex models such as neural networks. Ensemble sampling dramatically\nexpands on the range of applications for which Thompson sampling is viable. We\nestablish a theoretical basis that supports the approach and present\ncomputational results that offer further insight.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 19:36:36 GMT"}, {"version": "v2", "created": "Wed, 5 Jul 2017 21:11:12 GMT"}, {"version": "v3", "created": "Wed, 22 Nov 2017 21:48:42 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Lu", "Xiuyuan", ""], ["Van Roy", "Benjamin", ""]]}, {"id": "1705.07368", "submitter": "James Foulds", "authors": "James Foulds", "title": "Mixed Membership Word Embeddings for Computational Social Science", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings improve the performance of NLP systems by revealing the\nhidden structural relationships between words. Despite their success in many\napplications, word embeddings have seen very little use in computational social\nscience NLP tasks, presumably due to their reliance on big data, and to a lack\nof interpretability. I propose a probabilistic model-based word embedding\nmethod which can recover interpretable embeddings, without big data. The key\ninsight is to leverage mixed membership modeling, in which global\nrepresentations are shared, but individual entities (i.e. dictionary words) are\nfree to use these representations to uniquely differing degrees. I show how to\ntrain the model using a combination of state-of-the-art training techniques for\nword embeddings and topic models. The experimental results show an improvement\nin predictive language modeling of up to 63% in MRR over the skip-gram, and\ndemonstrate that the representations are beneficial for supervised learning. I\nillustrate the interpretability of the models with computational social science\ncase studies on State of the Union addresses and NIPS articles.\n", "versions": [{"version": "v1", "created": "Sat, 20 May 2017 23:45:54 GMT"}, {"version": "v2", "created": "Thu, 25 May 2017 03:12:35 GMT"}, {"version": "v3", "created": "Tue, 20 Feb 2018 00:34:49 GMT"}], "update_date": "2018-02-21", "authors_parsed": [["Foulds", "James", ""]]}, {"id": "1705.07381", "submitter": "Luis Pineda", "authors": "Luis Pineda and Shlomo Zilberstein", "title": "Generalizing the Role of Determinization in Probabilistic Planning", "comments": null, "journal-ref": null, "doi": null, "report-no": "UM-CS-2017-006", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The stochastic shortest path problem (SSP) is a highly expressive model for\nprobabilistic planning. The computational hardness of SSPs has sparked interest\nin determinization-based planners that can quickly solve large problems.\nHowever, existing methods employ a simplistic approach to determinization. In\nparticular, they ignore the possibility of tailoring the determinization to the\nspecific characteristics of the target domain. In this work we examine this\nquestion, by showing that learning a good determinization for a planning domain\ncan be done efficiently and can improve performance. Moreover, we show how to\ndirectly incorporate probabilistic reasoning into the planning problem when a\ngood determinization is not sufficient by itself. Based on these insights, we\nintroduce a planner, FF-LAO*, that outperforms state-of-the-art probabilistic\nplanners on several well-known competition benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 21 May 2017 02:39:02 GMT"}, {"version": "v2", "created": "Sat, 29 Jul 2017 14:25:10 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Pineda", "Luis", ""], ["Zilberstein", "Shlomo", ""]]}, {"id": "1705.07429", "submitter": "Sergey Paramonov", "authors": "Sergey Paramonov, Christian Bessiere, Anton Dries, Luc De Raedt", "title": "Sketched Answer Set Programming", "comments": "15 pages, 11 figures; to appear in ICTAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answer Set Programming (ASP) is a powerful modeling formalism for\ncombinatorial problems. However, writing ASP models is not trivial. We propose\na novel method, called Sketched Answer Set Programming (SkASP), aiming at\nsupporting the user in resolving this issue. The user writes an ASP program\nwhile marking uncertain parts open with question marks. In addition, the user\nprovides a number of positive and negative examples of the desired program\nbehaviour. The sketched model is rewritten into another ASP program, which is\nsolved by traditional methods. As a result, the user obtains a functional and\nreusable ASP program modelling her problem. We evaluate our approach on 21 well\nknown puzzles and combinatorial problems inspired by Karp's 21 NP-complete\nproblems and demonstrate a use-case for a database application based on ASP.\n", "versions": [{"version": "v1", "created": "Sun, 21 May 2017 11:03:53 GMT"}, {"version": "v2", "created": "Wed, 22 Aug 2018 09:52:51 GMT"}], "update_date": "2018-08-23", "authors_parsed": [["Paramonov", "Sergey", ""], ["Bessiere", "Christian", ""], ["Dries", "Anton", ""], ["De Raedt", "Luc", ""]]}, {"id": "1705.07445", "submitter": "Sahil Sharma", "authors": "Sahil Sharma, Girish Raguvir J, Srivatsan Ramesh, Balaraman Ravindran", "title": "Learning to Mix n-Step Returns: Generalizing lambda-Returns for Deep\n  Reinforcement Learning", "comments": "10 pages + 9 page appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement Learning (RL) can model complex behavior policies for\ngoal-directed sequential decision making tasks. A hallmark of RL algorithms is\nTemporal Difference (TD) learning: value function for the current state is\nmoved towards a bootstrapped target that is estimated using next state's value\nfunction. $\\lambda$-returns generalize beyond 1-step returns and strike a\nbalance between Monte Carlo and TD learning methods. While lambda-returns have\nbeen extensively studied in RL, they haven't been explored a lot in Deep RL.\nThis paper's first contribution is an exhaustive benchmarking of\nlambda-returns. Although mathematically tractable, the use of exponentially\ndecaying weighting of n-step returns based targets in lambda-returns is a\nrather ad-hoc design choice. Our second major contribution is that we propose a\ngeneralization of lambda-returns called Confidence-based Autodidactic Returns\n(CAR), wherein the RL agent learns the weighting of the n-step returns in an\nend-to-end manner. This allows the agent to learn to decide how much it wants\nto weigh the n-step returns based targets. In contrast, lambda-returns restrict\nRL agents to use an exponentially decaying weighting scheme. Autodidactic\nreturns can be used for improving any RL algorithm which uses TD learning. We\nempirically demonstrate that using sophisticated weighted mixtures of\nmulti-step returns (like CAR and lambda-returns) considerably outperforms the\nuse of n-step returns. We perform our experiments on the Asynchronous Advantage\nActor Critic (A3C) algorithm in the Atari 2600 domain.\n", "versions": [{"version": "v1", "created": "Sun, 21 May 2017 12:47:37 GMT"}, {"version": "v2", "created": "Sun, 5 Nov 2017 06:36:53 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Sharma", "Sahil", ""], ["J", "Girish Raguvir", ""], ["Ramesh", "Srivatsan", ""], ["Ravindran", "Balaraman", ""]]}, {"id": "1705.07460", "submitter": "Min Xu", "authors": "Min Xu", "title": "Experience enrichment based task independent reward model", "comments": "4 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For most reinforcement learning approaches, the learning is performed by\nmaximizing an accumulative reward that is expectedly and manually defined for\nspecific tasks. However, in real world, rewards are emergent phenomena from the\ncomplex interactions between agents and environments. In this paper, we propose\nan implicit generic reward model for reinforcement learning. Unlike those\nrewards that are manually defined for specific tasks, such implicit reward is\ntask independent. It only comes from the deviation from the agents' previous\nexperiences.\n", "versions": [{"version": "v1", "created": "Sun, 21 May 2017 15:19:20 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Xu", "Min", ""]]}, {"id": "1705.07461", "submitter": "Nir Levine", "authors": "Nir Levine, Tom Zahavy, Daniel J. Mankowitz, Aviv Tamar, Shie Mannor", "title": "Shallow Updates for Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning (DRL) methods such as the Deep Q-Network (DQN)\nhave achieved state-of-the-art results in a variety of challenging,\nhigh-dimensional domains. This success is mainly attributed to the power of\ndeep neural networks to learn rich domain representations for approximating the\nvalue function or policy. Batch reinforcement learning methods with linear\nrepresentations, on the other hand, are more stable and require less hyper\nparameter tuning. Yet, substantial feature engineering is necessary to achieve\ngood results. In this work we propose a hybrid approach -- the Least Squares\nDeep Q-Network (LS-DQN), which combines rich feature representations learned by\na DRL algorithm with the stability of a linear least squares method. We do this\nby periodically re-training the last hidden layer of a DRL network with a batch\nleast squares update. Key to our approach is a Bayesian regularization term for\nthe least squares update, which prevents over-fitting to the more recent data.\nWe tested LS-DQN on five Atari games and demonstrate significant improvement\nover vanilla DQN and Double-DQN. We also investigated the reasons for the\nsuperior performance of our method. Interestingly, we found that the\nperformance improvement can be attributed to the large batch size used by the\nLS method when optimizing the last layer.\n", "versions": [{"version": "v1", "created": "Sun, 21 May 2017 15:20:15 GMT"}, {"version": "v2", "created": "Thu, 2 Nov 2017 19:00:40 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Levine", "Nir", ""], ["Zahavy", "Tom", ""], ["Mankowitz", "Daniel J.", ""], ["Tamar", "Aviv", ""], ["Mannor", "Shie", ""]]}, {"id": "1705.07477", "submitter": "Tianyang Li", "authors": "Tianyang Li, Liu Liu, Anastasios Kyrillidis, Constantine Caramanis", "title": "Statistical inference using SGD", "comments": "To appear in AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for frequentist statistical inference in\n$M$-estimation problems, based on stochastic gradient descent (SGD) with a\nfixed step size: we demonstrate that the average of such SGD sequences can be\nused for statistical inference, after proper scaling. An intuitive analysis\nusing the Ornstein-Uhlenbeck process suggests that such averages are\nasymptotically normal. From a practical perspective, our SGD-based inference\nprocedure is a first order method, and is well-suited for large scale problems.\nTo show its merits, we apply it to both synthetic and real datasets, and\ndemonstrate that its accuracy is comparable to classical statistical methods,\nwhile requiring potentially far less computation.\n", "versions": [{"version": "v1", "created": "Sun, 21 May 2017 17:01:51 GMT"}, {"version": "v2", "created": "Sun, 19 Nov 2017 20:59:52 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Li", "Tianyang", ""], ["Liu", "Liu", ""], ["Kyrillidis", "Anastasios", ""], ["Caramanis", "Constantine", ""]]}, {"id": "1705.07558", "submitter": "Mark Levin", "authors": "Mark Sh. Levin", "title": "Note on Evolution and Forecasting of Requirements: Communications\n  Example", "comments": "8 pages, 8 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combinatorial evolution and forecasting of system requirements is examined.\nThe morphological model is used for a hierarchical requirements system (i.e.,\nsystem parts, design alternatives for the system parts, ordinal estimates for\nthe alternatives). A set of system changes involves changes of the system\nstructure, component alternatives and their estimates. The composition process\nof the forecast is based on combinatorial synthesis (knapsack problem, multiple\nchoice problem, hierarchical morphological design). An illustrative numerical\nexample for four-phase evolution and forecasting of requirements to\ncommunications is described.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 05:22:53 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Levin", "Mark Sh.", ""]]}, {"id": "1705.07615", "submitter": "John Aslanides", "authors": "John Aslanides", "title": "AIXIjs: A Software Demo for General Reinforcement Learning", "comments": "Masters thesis. Australian National University, October 2016. 97 pp", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reinforcement learning is a general and powerful framework with which to\nstudy and implement artificial intelligence. Recent advances in deep learning\nhave enabled RL algorithms to achieve impressive performance in restricted\ndomains such as playing Atari video games (Mnih et al., 2015) and, recently,\nthe board game Go (Silver et al., 2016). However, we are still far from\nconstructing a generally intelligent agent. Many of the obstacles and open\nquestions are conceptual: What does it mean to be intelligent? How does one\nexplore and learn optimally in general, unknown environments? What, in fact,\ndoes it mean to be optimal in the general sense? The universal Bayesian agent\nAIXI (Hutter, 2005) is a model of a maximally intelligent agent, and plays a\ncentral role in the sub-field of general reinforcement learning (GRL).\nRecently, AIXI has been shown to be flawed in important ways; it doesn't\nexplore enough to be asymptotically optimal (Orseau, 2010), and it can perform\npoorly with certain priors (Leike and Hutter, 2015). Several variants of AIXI\nhave been proposed to attempt to address these shortfalls: among them are\nentropy-seeking agents (Orseau, 2011), knowledge-seeking agents (Orseau et al.,\n2013), Bayes with bursts of exploration (Lattimore, 2013), MDL agents (Leike,\n2016a), Thompson sampling (Leike et al., 2016), and optimism (Sunehag and\nHutter, 2015). We present AIXIjs, a JavaScript implementation of these GRL\nagents. This implementation is accompanied by a framework for running\nexperiments against various environments, similar to OpenAI Gym (Brockman et\nal., 2016), and a suite of interactive demos that explore different properties\nof the agents, similar to REINFORCEjs (Karpathy, 2015). We use AIXIjs to\npresent numerous experiments illustrating fundamental properties of, and\ndifferences between, these agents.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 08:56:54 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Aslanides", "John", ""]]}, {"id": "1705.07798", "submitter": "Gergely Neu", "authors": "Gergely Neu and Anders Jonsson and Vicen\\c{c} G\\'omez", "title": "A unified view of entropy-regularized Markov decision processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general framework for entropy-regularized average-reward\nreinforcement learning in Markov decision processes (MDPs). Our approach is\nbased on extending the linear-programming formulation of policy optimization in\nMDPs to accommodate convex regularization functions. Our key result is showing\nthat using the conditional entropy of the joint state-action distributions as\nregularization yields a dual optimization problem closely resembling the\nBellman optimality equations. This result enables us to formalize a number of\nstate-of-the-art entropy-regularized reinforcement learning algorithms as\napproximate variants of Mirror Descent or Dual Averaging, and thus to argue\nabout the convergence properties of these methods. In particular, we show that\nthe exact version of the TRPO algorithm of Schulman et al. (2015) actually\nconverges to the optimal policy, while the entropy-regularized policy gradient\nmethods of Mnih et al. (2016) may fail to converge to a fixed point. Finally,\nwe illustrate empirically the effects of using various regularization\ntechniques on learning performance in a simple reinforcement learning setup.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 15:06:25 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Neu", "Gergely", ""], ["Jonsson", "Anders", ""], ["G\u00f3mez", "Vicen\u00e7", ""]]}, {"id": "1705.07830", "submitter": "Jannis Bulian", "authors": "Christian Buck, Jannis Bulian, Massimiliano Ciaramita, Wojciech\n  Gajewski, Andrea Gesmundo, Neil Houlsby, Wei Wang", "title": "Ask the Right Questions: Active Question Reformulation with\n  Reinforcement Learning", "comments": null, "journal-ref": "Sixth International Conference on Learning Representations (ICLR),\n  2018", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We frame Question Answering (QA) as a Reinforcement Learning task, an\napproach that we call Active Question Answering. We propose an agent that sits\nbetween the user and a black box QA system and learns to reformulate questions\nto elicit the best possible answers. The agent probes the system with,\npotentially many, natural language reformulations of an initial question and\naggregates the returned evidence to yield the best answer. The reformulation\nsystem is trained end-to-end to maximize answer quality using policy gradient.\nWe evaluate on SearchQA, a dataset of complex questions extracted from\nJeopardy!. The agent outperforms a state-of-the-art base model, playing the\nrole of the environment, and other benchmarks. We also analyze the language\nthat the agent has learned while interacting with the question answering\nsystem. We find that successful question reformulations look quite different\nfrom natural language paraphrases. The agent is able to discover non-trivial\nreformulation strategies that resemble classic information retrieval techniques\nsuch as term re-weighting (tf-idf) and stemming.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 16:19:21 GMT"}, {"version": "v2", "created": "Wed, 12 Jul 2017 12:21:14 GMT"}, {"version": "v3", "created": "Fri, 2 Mar 2018 16:02:24 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Buck", "Christian", ""], ["Bulian", "Jannis", ""], ["Ciaramita", "Massimiliano", ""], ["Gajewski", "Wojciech", ""], ["Gesmundo", "Andrea", ""], ["Houlsby", "Neil", ""], ["Wang", "Wei", ""]]}, {"id": "1705.07874", "submitter": "Scott Lundberg", "authors": "Scott Lundberg and Su-In Lee", "title": "A Unified Approach to Interpreting Model Predictions", "comments": "To appear in NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding why a model makes a certain prediction can be as crucial as the\nprediction's accuracy in many applications. However, the highest accuracy for\nlarge modern datasets is often achieved by complex models that even experts\nstruggle to interpret, such as ensemble or deep learning models, creating a\ntension between accuracy and interpretability. In response, various methods\nhave recently been proposed to help users interpret the predictions of complex\nmodels, but it is often unclear how these methods are related and when one\nmethod is preferable over another. To address this problem, we present a\nunified framework for interpreting predictions, SHAP (SHapley Additive\nexPlanations). SHAP assigns each feature an importance value for a particular\nprediction. Its novel components include: (1) the identification of a new class\nof additive feature importance measures, and (2) theoretical results showing\nthere is a unique solution in this class with a set of desirable properties.\nThe new class unifies six existing methods, notable because several recent\nmethods in the class lack the proposed desirable properties. Based on insights\nfrom this unification, we present new methods that show improved computational\nperformance and/or better consistency with human intuition than previous\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 17:38:10 GMT"}, {"version": "v2", "created": "Sat, 25 Nov 2017 03:53:32 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Lundberg", "Scott", ""], ["Lee", "Su-In", ""]]}, {"id": "1705.07904", "submitter": "Chris Donahue", "authors": "Chris Donahue, Zachary C. Lipton, Akshay Balsubramani, Julian McAuley", "title": "Semantically Decomposing the Latent Spaces of Generative Adversarial\n  Networks", "comments": "Published as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new algorithm for training generative adversarial networks that\njointly learns latent codes for both identities (e.g. individual humans) and\nobservations (e.g. specific photographs). By fixing the identity portion of the\nlatent codes, we can generate diverse images of the same subject, and by fixing\nthe observation portion, we can traverse the manifold of subjects while\nmaintaining contingent aspects such as lighting and pose. Our algorithm\nfeatures a pairwise training scheme in which each sample from the generator\nconsists of two images with a common identity code. Corresponding samples from\nthe real dataset consist of two distinct photographs of the same subject. In\norder to fool the discriminator, the generator must produce pairs that are\nphotorealistic, distinct, and appear to depict the same individual. We augment\nboth the DCGAN and BEGAN approaches with Siamese discriminators to facilitate\npairwise training. Experiments with human judges and an off-the-shelf face\nverification system demonstrate our algorithm's ability to generate convincing,\nidentity-matched photographs.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 18:00:02 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 18:00:05 GMT"}, {"version": "v3", "created": "Thu, 22 Feb 2018 19:36:33 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Donahue", "Chris", ""], ["Lipton", "Zachary C.", ""], ["Balsubramani", "Akshay", ""], ["McAuley", "Julian", ""]]}, {"id": "1705.07961", "submitter": "Irina Georgescu", "authors": "Irina Georgescu", "title": "Compatible extensions and consistent closures: a fuzzy approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper $\\ast$--compatible extensions of fuzzy relations are studied,\ngeneralizing some results obtained by Duggan in case of crisp relations. From\nthis general result are obtained as particular cases fuzzy versions of some\nimportant extension theorems for crisp relations (Szpilrajn, Hansson,\nSuzumura). Two notions of consistent closure of a fuzzy relation are\nintroduced.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 19:27:19 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Georgescu", "Irina", ""]]}, {"id": "1705.07962", "submitter": "Tony Beltramelli", "authors": "Tony Beltramelli", "title": "pix2code: Generating Code from a Graphical User Interface Screenshot", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Transforming a graphical user interface screenshot created by a designer into\ncomputer code is a typical task conducted by a developer in order to build\ncustomized software, websites, and mobile applications. In this paper, we show\nthat deep learning methods can be leveraged to train a model end-to-end to\nautomatically generate code from a single input image with over 77% of accuracy\nfor three different platforms (i.e. iOS, Android and web-based technologies).\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 19:32:20 GMT"}, {"version": "v2", "created": "Tue, 19 Sep 2017 11:27:47 GMT"}], "update_date": "2017-09-20", "authors_parsed": [["Beltramelli", "Tony", ""]]}, {"id": "1705.07996", "submitter": "Neil Lawrence", "authors": "Neil D. Lawrence", "title": "Living Together: Mind and Machine Intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we consider the nature of the machine intelligences we have\ncreated in the context of our human intelligence. We suggest that the\nfundamental difference between human and machine intelligence comes down to\n\\emph{embodiment factors}. We define embodiment factors as the ratio between an\nentity's ability to communicate information vs compute information. We\nspeculate on the role of embodiment factors in driving our own intelligence and\nconsciousness. We briefly review dual process models of cognition and cast\nmachine intelligence within that framework, characterising it as a dominant\nSystem Zero, which can drive behaviour through interfacing with us\nsubconsciously. Driven by concerns about the consequence of such a system we\nsuggest prophylactic courses of action that could be considered. Our main\nconclusion is that it is \\emph{not} sentient intelligence we should fear but\n\\emph{non-sentient} intelligence.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 20:49:43 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Lawrence", "Neil D.", ""]]}, {"id": "1705.08039", "submitter": "Maximilian Nickel", "authors": "Maximilian Nickel, Douwe Kiela", "title": "Poincar\\'e Embeddings for Learning Hierarchical Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation learning has become an invaluable approach for learning from\nsymbolic data such as text and graphs. However, while complex symbolic datasets\noften exhibit a latent hierarchical structure, state-of-the-art methods\ntypically learn embeddings in Euclidean vector spaces, which do not account for\nthis property. For this purpose, we introduce a new approach for learning\nhierarchical representations of symbolic data by embedding them into hyperbolic\nspace -- or more precisely into an n-dimensional Poincar\\'e ball. Due to the\nunderlying hyperbolic geometry, this allows us to learn parsimonious\nrepresentations of symbolic data by simultaneously capturing hierarchy and\nsimilarity. We introduce an efficient algorithm to learn the embeddings based\non Riemannian optimization and show experimentally that Poincar\\'e embeddings\noutperform Euclidean embeddings significantly on data with latent hierarchies,\nboth in terms of representation capacity and in terms of generalization\nability.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 23:14:36 GMT"}, {"version": "v2", "created": "Fri, 26 May 2017 17:40:55 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Nickel", "Maximilian", ""], ["Kiela", "Douwe", ""]]}, {"id": "1705.08044", "submitter": "Nariman Farsad", "authors": "Nariman Farsad and Andrea Goldsmith", "title": "Detection Algorithms for Communication Systems Using Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design and analysis of communication systems typically rely on the\ndevelopment of mathematical models that describe the underlying communication\nchannel, which dictates the relationship between the transmitted and the\nreceived signals. However, in some systems, such as molecular communication\nsystems where chemical signals are used for transfer of information, it is not\npossible to accurately model this relationship. In these scenarios, because of\nthe lack of mathematical channel models, a completely new approach to design\nand analysis is required. In this work, we focus on one important aspect of\ncommunication systems, the detection algorithms, and demonstrate that by\nborrowing tools from deep learning, it is possible to train detectors that\nperform well, without any knowledge of the underlying channel models. We\nevaluate these algorithms using experimental data that is collected by a\nchemical communication platform, where the channel model is unknown and\ndifficult to model analytically. We show that deep learning algorithms perform\nsignificantly better than a simple detector that was used in previous works,\nwhich also did not assume any knowledge of the channel.\n", "versions": [{"version": "v1", "created": "Mon, 22 May 2017 23:47:47 GMT"}, {"version": "v2", "created": "Mon, 31 Jul 2017 02:43:27 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Farsad", "Nariman", ""], ["Goldsmith", "Andrea", ""]]}, {"id": "1705.08142", "submitter": "Sebastian Ruder", "authors": "Sebastian Ruder, Joachim Bingel, Isabelle Augenstein, Anders\n  S{\\o}gaard", "title": "Latent Multi-task Architecture Learning", "comments": "To appear in Proceedings of AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning (MTL) allows deep neural networks to learn from related\ntasks by sharing parameters with other networks. In practice, however, MTL\ninvolves searching an enormous space of possible parameter sharing\narchitectures to find (a) the layers or subspaces that benefit from sharing,\n(b) the appropriate amount of sharing, and (c) the appropriate relative weights\nof the different task losses. Recent work has addressed each of the above\nproblems in isolation. In this work we present an approach that learns a latent\nmulti-task architecture that jointly addresses (a)--(c). We present experiments\non synthetic data and data from OntoNotes 5.0, including four different tasks\nand seven different domains. Our extension consistently outperforms previous\napproaches to learning latent architectures for multi-task problems and\nachieves up to 15% average error reductions over common approaches to MTL.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 08:58:09 GMT"}, {"version": "v2", "created": "Tue, 16 Jan 2018 14:05:37 GMT"}, {"version": "v3", "created": "Mon, 19 Nov 2018 10:30:52 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Ruder", "Sebastian", ""], ["Bingel", "Joachim", ""], ["Augenstein", "Isabelle", ""], ["S\u00f8gaard", "Anders", ""]]}, {"id": "1705.08200", "submitter": "Chaoyang Song", "authors": "Fang Wan and Chaoyang Song", "title": "Logical Learning Through a Hybrid Neural Network with Auxiliary Inputs", "comments": "11 pages, 9 figures, 4 tables", "journal-ref": "Front. Robot. AI, 30 July 2018", "doi": "10.3389/frobt.2018.00086", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human reasoning process is seldom a one-way process from an input leading\nto an output. Instead, it often involves a systematic deduction by ruling out\nother possible outcomes as a self-checking mechanism. In this paper, we\ndescribe the design of a hybrid neural network for logical learning that is\nsimilar to the human reasoning through the introduction of an auxiliary input,\nnamely the indicators, that act as the hints to suggest logical outcomes. We\ngenerate these indicators by digging into the hidden information buried\nunderneath the original training data for direct or indirect suggestions. We\nused the MNIST data to demonstrate the design and use of these indicators in a\nconvolutional neural network. We trained a series of such hybrid neural\nnetworks with variations of the indicators. Our results show that these hybrid\nneural networks are very robust in generating logical outcomes with inherently\nhigher prediction accuracy than the direct use of the original input and output\nin apparent models. Such improved predictability with reassured logical\nconfidence is obtained through the exhaustion of all possible indicators to\nrule out all illogical outcomes, which is not available in the apparent models.\nOur logical learning process can effectively cope with the unknown unknowns\nusing a full exploitation of all existing knowledge available for learning. The\ndesign and implementation of the hints, namely the indicators, become an\nessential part of artificial intelligence for logical learning. We also\nintroduce an ongoing application setup for this hybrid neural network in an\nautonomous grasping robot, namely as_DeepClaw, aiming at learning an optimized\ngrasping pose through logical learning.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 12:11:30 GMT"}], "update_date": "2020-03-10", "authors_parsed": [["Wan", "Fang", ""], ["Song", "Chaoyang", ""]]}, {"id": "1705.08218", "submitter": "Xiaojian Wu", "authors": "Xiaojian Wu, Yexiang Xue, Bart Selman, Carla P. Gomes", "title": "XOR-Sampling for Network Design with Correlated Stochastic Events", "comments": "In Proceedings of the Twenty-sixth International Joint Conference on\n  Artificial Intelligence (IJCAI-17). The first two authors contribute equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many network optimization problems can be formulated as stochastic network\ndesign problems in which edges are present or absent stochastically.\nFurthermore, protective actions can guarantee that edges will remain present.\nWe consider the problem of finding the optimal protection strategy under a\nbudget limit in order to maximize some connectivity measurements of the\nnetwork. Previous approaches rely on the assumption that edges are independent.\nIn this paper, we consider a more realistic setting where multiple edges are\nnot independent due to natural disasters or regional events that make the\nstates of multiple edges stochastically correlated. We use Markov Random Fields\nto model the correlation and define a new stochastic network design framework.\nWe provide a novel algorithm based on Sample Average Approximation (SAA)\ncoupled with a Gibbs or XOR sampler. The experimental results on real road\nnetwork data show that the policies produced by SAA with the XOR sampler have\nhigher quality and lower variance compared to SAA with Gibbs sampler.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 12:50:36 GMT"}, {"version": "v2", "created": "Wed, 24 May 2017 01:38:57 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Wu", "Xiaojian", ""], ["Xue", "Yexiang", ""], ["Selman", "Bart", ""], ["Gomes", "Carla P.", ""]]}, {"id": "1705.08245", "submitter": "Vincent Huang", "authors": "Vincent Huang, Tobias Ley, Martha Vlachou-Konchylaki, Wenfeng Hu", "title": "Enhanced Experience Replay Generation for Efficient Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applying deep reinforcement learning (RL) on real systems suffers from slow\ndata sampling. We propose an enhanced generative adversarial network (EGAN) to\ninitialize an RL agent in order to achieve faster learning. The EGAN utilizes\nthe relation between states and actions to enhance the quality of data samples\ngenerated by a GAN. Pre-training the agent with the EGAN shows a steeper\nlearning curve with a 20% improvement of training time in the beginning of\nlearning, compared to no pre-training, and an improvement compared to training\nwith GAN by about 5% with smaller variations. For real time systems with sparse\nand slow data sampling the EGAN could be used to speed up the early phases of\nthe training process.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 13:36:00 GMT"}, {"version": "v2", "created": "Mon, 29 May 2017 14:24:08 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Huang", "Vincent", ""], ["Ley", "Tobias", ""], ["Vlachou-Konchylaki", "Martha", ""], ["Hu", "Wenfeng", ""]]}, {"id": "1705.08320", "submitter": "Svetlin Penkov", "authors": "Svetlin Penkov and Subramanian Ramamoorthy", "title": "Explaining Transition Systems through Program Induction", "comments": "submitted to Neural Information Processing Systems 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explaining and reasoning about processes which underlie observed black-box\nphenomena enables the discovery of causal mechanisms, derivation of suitable\nabstract representations and the formulation of more robust predictions. We\npropose to learn high level functional programs in order to represent abstract\nmodels which capture the invariant structure in the observed data. We introduce\nthe $\\pi$-machine (program-induction machine) -- an architecture able to induce\ninterpretable LISP-like programs from observed data traces. We propose an\noptimisation procedure for program learning based on backpropagation, gradient\ndescent and A* search. We apply the proposed method to three problems: system\nidentification of dynamical systems, explaining the behaviour of a DQN agent\nand learning by demonstration in a human-robot interaction scenario. Our\nexperimental results show that the $\\pi$-machine can efficiently induce\ninterpretable programs from individual data traces.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 14:38:28 GMT"}], "update_date": "2017-07-27", "authors_parsed": [["Penkov", "Svetlin", ""], ["Ramamoorthy", "Subramanian", ""]]}, {"id": "1705.08369", "submitter": "Talha Qaiser", "authors": "Talha Qaiser, Abhik Mukherjee, Chaitanya Reddy Pb, Sai Dileep\n  Munugoti, Vamsi Tallam, Tomi Pitk\\\"aaho, Taina Lehtim\\\"aki, Thomas Naughton,\n  Matt Berseth, An\\'ibal Pedraza, Ramakrishnan Mukundan, Matthew Smith, Abhir\n  Bhalerao, Erik Rodner, Marcel Simon, Joachim Denzler, Chao-Hui Huang, Gloria\n  Bueno, David Snead, Ian Ellis, Mohammad Ilyas, Nasir Rajpoot", "title": "Her2 Challenge Contest: A Detailed Assessment of Automated Her2 Scoring\n  Algorithms in Whole Slide Images of Breast Cancer Tissues", "comments": null, "journal-ref": null, "doi": "10.1111/his.13333", "report-no": null, "categories": "cs.CV cs.AI q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evaluating expression of the Human epidermal growth factor receptor 2 (Her2)\nby visual examination of immunohistochemistry (IHC) on invasive breast cancer\n(BCa) is a key part of the diagnostic assessment of BCa due to its recognised\nimportance as a predictive and prognostic marker in clinical practice. However,\nvisual scoring of Her2 is subjective and consequently prone to inter-observer\nvariability. Given the prognostic and therapeutic implications of Her2 scoring,\na more objective method is required. In this paper, we report on a recent\nautomated Her2 scoring contest, held in conjunction with the annual PathSoc\nmeeting held in Nottingham in June 2016, aimed at systematically comparing and\nadvancing the state-of-the-art Artificial Intelligence (AI) based automated\nmethods for Her2 scoring. The contest dataset comprised of digitised whole\nslide images (WSI) of sections from 86 cases of invasive breast carcinoma\nstained with both Haematoxylin & Eosin (H&E) and IHC for Her2. The contesting\nalgorithms automatically predicted scores of the IHC slides for an unseen\nsubset of the dataset and the predicted scores were compared with the 'ground\ntruth' (a consensus score from at least two experts). We also report on a\nsimple Man vs Machine contest for the scoring of Her2 and show that the\nautomated methods could beat the pathology experts on this contest dataset.\nThis paper presents a benchmark for comparing the performance of automated\nalgorithms for scoring of Her2. It also demonstrates the enormous potential of\nautomated algorithms in assisting the pathologist with objective IHC scoring.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 15:36:04 GMT"}, {"version": "v2", "created": "Wed, 24 May 2017 08:47:33 GMT"}, {"version": "v3", "created": "Mon, 24 Jul 2017 21:39:53 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Qaiser", "Talha", ""], ["Mukherjee", "Abhik", ""], ["Pb", "Chaitanya Reddy", ""], ["Munugoti", "Sai Dileep", ""], ["Tallam", "Vamsi", ""], ["Pitk\u00e4aho", "Tomi", ""], ["Lehtim\u00e4ki", "Taina", ""], ["Naughton", "Thomas", ""], ["Berseth", "Matt", ""], ["Pedraza", "An\u00edbal", ""], ["Mukundan", "Ramakrishnan", ""], ["Smith", "Matthew", ""], ["Bhalerao", "Abhir", ""], ["Rodner", "Erik", ""], ["Simon", "Marcel", ""], ["Denzler", "Joachim", ""], ["Huang", "Chao-Hui", ""], ["Bueno", "Gloria", ""], ["Snead", "David", ""], ["Ellis", "Ian", ""], ["Ilyas", "Mohammad", ""], ["Rajpoot", "Nasir", ""]]}, {"id": "1705.08395", "submitter": "Ari Seff", "authors": "Ari Seff, Alex Beatson, Daniel Suo, Han Liu", "title": "Continual Learning in Generative Adversarial Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developments in deep generative models have allowed for tractable learning of\nhigh-dimensional data distributions. While the employed learning procedures\ntypically assume that training data is drawn i.i.d. from the distribution of\ninterest, it may be desirable to model distinct distributions which are\nobserved sequentially, such as when different classes are encountered over\ntime. Although conditional variations of deep generative models permit multiple\ndistributions to be modeled by a single network in a disentangled fashion, they\nare susceptible to catastrophic forgetting when the distributions are\nencountered sequentially. In this paper, we adapt recent work in reducing\ncatastrophic forgetting to the task of training generative adversarial networks\non a sequence of distinct distributions, enabling continual generative\nmodeling.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 16:27:19 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Seff", "Ari", ""], ["Beatson", "Alex", ""], ["Suo", "Daniel", ""], ["Liu", "Han", ""]]}, {"id": "1705.08417", "submitter": "Viktoriya Krakovna", "authors": "Tom Everitt, Victoria Krakovna, Laurent Orseau, Marcus Hutter, Shane\n  Legg", "title": "Reinforcement Learning with a Corrupted Reward Channel", "comments": "A shorter version of this report was accepted to IJCAI 2017 AI and\n  Autonomy track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  No real-world reward function is perfect. Sensory errors and software bugs\nmay result in RL agents observing higher (or lower) rewards than they should.\nFor example, a reinforcement learning agent may prefer states where a sensory\nerror gives it the maximum reward, but where the true reward is actually small.\nWe formalise this problem as a generalised Markov Decision Problem called\nCorrupt Reward MDP. Traditional RL methods fare poorly in CRMDPs, even under\nstrong simplifying assumptions and when trying to compensate for the possibly\ncorrupt rewards. Two ways around the problem are investigated. First, by giving\nthe agent richer data, such as in inverse reinforcement learning and\nsemi-supervised reinforcement learning, reward corruption stemming from\nsystematic sensory errors may sometimes be completely managed. Second, by using\nrandomisation to blunt the agent's optimisation, reward corruption can be\npartially managed under some assumptions.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 17:06:56 GMT"}, {"version": "v2", "created": "Sat, 19 Aug 2017 05:01:16 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Everitt", "Tom", ""], ["Krakovna", "Victoria", ""], ["Orseau", "Laurent", ""], ["Hutter", "Marcus", ""], ["Legg", "Shane", ""]]}, {"id": "1705.08426", "submitter": "Shufang Zhu", "authors": "Shufang Zhu, Lucas M. Tabajara, Jianwen Li, Geguang Pu, Moshe Y. Vardi", "title": "Symbolic LTLf Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LTLf synthesis is the process of finding a strategy that satisfies a linear\ntemporal specification over finite traces. An existing solution to this problem\nrelies on a reduction to a DFA game. In this paper, we propose a symbolic\nframework for LTLf synthesis based on this technique, by performing the\ncomputation over a representation of the DFA as a boolean formula rather than\nas an explicit graph. This approach enables strategy generation by utilizing\nthe mechanism of boolean synthesis. We implement this symbolic synthesis method\nin a tool called Syft, and demonstrate by experiments on scalable benchmarks\nthat the symbolic approach scales better than the explicit one.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 17:18:18 GMT"}, {"version": "v2", "created": "Thu, 21 Sep 2017 15:51:01 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Zhu", "Shufang", ""], ["Tabajara", "Lucas M.", ""], ["Li", "Jianwen", ""], ["Pu", "Geguang", ""], ["Vardi", "Moshe Y.", ""]]}, {"id": "1705.08439", "submitter": "Thomas Anthony", "authors": "Thomas Anthony, Zheng Tian, David Barber", "title": "Thinking Fast and Slow with Deep Learning and Tree Search", "comments": "v1 to v2: - Add a value function in MCTS - Some MCTS hyper-parameters\n  changed - Repetition of experiments: improved accuracy and errors shown.\n  (note the reduction in effect size for the tpt/cat experiment) - Results from\n  a longer training run, including changes in expert strength in training -\n  Comparison to MoHex. v3: clarify independence of ExIt and AG0. v4: see\n  appendix E", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequential decision making problems, such as structured prediction, robotic\ncontrol, and game playing, require a combination of planning policies and\ngeneralisation of those plans. In this paper, we present Expert Iteration\n(ExIt), a novel reinforcement learning algorithm which decomposes the problem\ninto separate planning and generalisation tasks. Planning new policies is\nperformed by tree search, while a deep neural network generalises those plans.\nSubsequently, tree search is improved by using the neural network policy to\nguide search, increasing the strength of new plans. In contrast, standard deep\nReinforcement Learning algorithms rely on a neural network not only to\ngeneralise plans, but to discover them too. We show that ExIt outperforms\nREINFORCE for training a neural network to play the board game Hex, and our\nfinal tree search agent, trained tabula rasa, defeats MoHex 1.0, the most\nrecent Olympiad Champion player to be publicly released.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 17:48:51 GMT"}, {"version": "v2", "created": "Sat, 4 Nov 2017 17:37:18 GMT"}, {"version": "v3", "created": "Fri, 10 Nov 2017 10:01:16 GMT"}, {"version": "v4", "created": "Sun, 3 Dec 2017 10:56:00 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Anthony", "Thomas", ""], ["Tian", "Zheng", ""], ["Barber", "David", ""]]}, {"id": "1705.08440", "submitter": "Mieczys{\\l}aw K{\\l}opotek", "authors": "M.Michalewicz, S.T.Wierzcho\\'n, M.A. K{\\l}opotek", "title": "Knowledge Acquisition, Representation \\& Manipulation in Decision\n  Support Systems", "comments": "Intelligent Information Systems Proceedings of a Workshop held in\n  August\\'ow, Poland, 7-11 June, 1993, pages 210- 238", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a methodology and discuss some implementation issues\nfor a project on statistical/expert approach to data analysis and knowledge\nacquisition. We discuss some general assumptions underlying the project.\nFurther, the requirements for a user-friendly computer assistant are specified\nalong with the nature of tools aiding the researcher. Next we show some aspects\nof belief network approach and Dempster-Shafer (DST) methodology introduced in\npractice to system SEAD. Specifically we present the application of DS\nmethodology to belief revision problem. Further a concept of an interface to\nprobabilistic and DS belief networks enabling a user to understand the\ncommunication with a belief network based reasoning system is presented\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 17:51:58 GMT"}], "update_date": "2017-05-24", "authors_parsed": [["Michalewicz", "M.", ""], ["Wierzcho\u0144", "S. T.", ""], ["K\u0142opotek", "M. A.", ""]]}, {"id": "1705.08475", "submitter": "Matthias Hein", "authors": "Matthias Hein, Maksym Andriushchenko", "title": "Formal Guarantees on the Robustness of a Classifier against Adversarial\n  Manipulation", "comments": "final version accepted at NIPS 2017, fixed bug in implementation of\n  Cross-Lipschitz regularization and lower bound computation, now results are\n  better", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work has shown that state-of-the-art classifiers are quite brittle, in\nthe sense that a small adversarial change of an originally with high confidence\ncorrectly classified input leads to a wrong classification again with high\nconfidence. This raises concerns that such classifiers are vulnerable to\nattacks and calls into question their usage in safety-critical systems. We show\nin this paper for the first time formal guarantees on the robustness of a\nclassifier by giving instance-specific lower bounds on the norm of the input\nmanipulation required to change the classifier decision. Based on this analysis\nwe propose the Cross-Lipschitz regularization functional. We show that using\nthis form of regularization in kernel methods resp. neural networks improves\nthe robustness of the classifier without any loss in prediction performance.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 18:48:20 GMT"}, {"version": "v2", "created": "Sun, 5 Nov 2017 20:58:09 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Hein", "Matthias", ""], ["Andriushchenko", "Maksym", ""]]}, {"id": "1705.08488", "submitter": "Denis Newman-Griffis", "authors": "Denis Newman-Griffis and Eric Fosler-Lussier", "title": "Second-Order Word Embeddings from Nearest Neighbor Topological Features", "comments": "Submitted to NIPS 2017. (8 pages + 4 reference)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce second-order vector representations of words, induced from\nnearest neighborhood topological features in pre-trained contextual word\nembeddings. We then analyze the effects of using second-order embeddings as\ninput features in two deep natural language processing models, for named entity\nrecognition and recognizing textual entailment, as well as a linear model for\nparaphrase recognition. Surprisingly, we find that nearest neighbor information\nalone is sufficient to capture most of the performance benefits derived from\nusing pre-trained word embeddings. Furthermore, second-order embeddings are\nable to handle highly heterogeneous data better than first-order\nrepresentations, though at the cost of some specificity. Additionally,\naugmenting contextual embeddings with second-order information further improves\nmodel performance in some cases. Due to variance in the random initializations\nof word embeddings, utilizing nearest neighbor features from multiple\nfirst-order embedding samples can also contribute to downstream performance\ngains. Finally, we identify intriguing characteristics of second-order\nembedding spaces for further research, including much higher density and\ndifferent semantic interpretations of cosine similarity.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 19:12:05 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Newman-Griffis", "Denis", ""], ["Fosler-Lussier", "Eric", ""]]}, {"id": "1705.08492", "submitter": "Yan Zhao", "authors": "Yan Zhao, Xiao Fang, and David Simchi-Levi", "title": "Uplift Modeling with Multiple Treatments and General Response Types", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized experiments have been used to assist decision-making in many\nareas. They help people select the optimal treatment for the test population\nwith certain statistical guarantee. However, subjects can show significant\nheterogeneity in response to treatments. The problem of customizing treatment\nassignment based on subject characteristics is known as uplift modeling,\ndifferential response analysis, or personalized treatment learning in\nliterature. A key feature for uplift modeling is that the data is unlabeled. It\nis impossible to know whether the chosen treatment is optimal for an individual\nsubject because response under alternative treatments is unobserved. This\npresents a challenge to both the training and the evaluation of uplift models.\nIn this paper we describe how to obtain an unbiased estimate of the key\nperformance metric of an uplift model, the expected response. We present a new\nuplift algorithm which creates a forest of randomized trees. The trees are\nbuilt with a splitting criterion designed to directly optimize their uplift\nperformance based on the proposed evaluation method. Both the evaluation method\nand the algorithm apply to arbitrary number of treatments and general response\ntypes. Experimental results on synthetic data and industry-provided data show\nthat our algorithm leads to significant performance improvement over other\napplicable methods.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 19:20:18 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Zhao", "Yan", ""], ["Fang", "Xiao", ""], ["Simchi-Levi", "David", ""]]}, {"id": "1705.08500", "submitter": "Yonatan Geifman", "authors": "Yonatan Geifman, Ran El-Yaniv", "title": "Selective Classification for Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selective classification techniques (also known as reject option) have not\nyet been considered in the context of deep neural networks (DNNs). These\ntechniques can potentially significantly improve DNNs prediction performance by\ntrading-off coverage. In this paper we propose a method to construct a\nselective classifier given a trained neural network. Our method allows a user\nto set a desired risk level. At test time, the classifier rejects instances as\nneeded, to grant the desired risk (with high probability). Empirical results\nover CIFAR and ImageNet convincingly demonstrate the viability of our method,\nwhich opens up possibilities to operate DNNs in mission-critical applications.\nFor example, using our method an unprecedented 2% error in top-5 ImageNet\nclassification can be guaranteed with probability 99.9%, and almost 60% test\ncoverage.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 19:43:56 GMT"}, {"version": "v2", "created": "Thu, 1 Jun 2017 14:10:34 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Geifman", "Yonatan", ""], ["El-Yaniv", "Ran", ""]]}, {"id": "1705.08508", "submitter": "Yihui He", "authors": "Yihui He, Xiaobo Ma, Xiapu Luo, Jianfeng Li, Mengchen Zhao, Bo An,\n  Xiaohong Guan", "title": "Vehicle Traffic Driven Camera Placement for Better Metropolis Security\n  Surveillance", "comments": "IEEE Intelligent Systems", "journal-ref": null, "doi": "10.1109/MIS.2018.223110904", "report-no": null, "categories": "cs.CY cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Security surveillance is one of the most important issues in smart cities,\nespecially in an era of terrorism. Deploying a number of (video) cameras is a\ncommon surveillance approach. Given the never-ending power offered by vehicles\nto metropolises, exploiting vehicle traffic to design camera placement\nstrategies could potentially facilitate security surveillance. This article\nconstitutes the first effort toward building the linkage between vehicle\ntraffic and security surveillance, which is a critical problem for smart\ncities. We expect our study could influence the decision making of surveillance\ncamera placement, and foster more research of principled ways of security\nsurveillance beneficial to our physical-world life. Code has been made publicly\navailable.\n", "versions": [{"version": "v1", "created": "Sat, 1 Apr 2017 09:27:25 GMT"}, {"version": "v2", "created": "Sat, 25 Nov 2017 09:09:01 GMT"}, {"version": "v3", "created": "Tue, 5 Dec 2017 00:24:03 GMT"}, {"version": "v4", "created": "Mon, 20 Aug 2018 02:26:57 GMT"}], "update_date": "2018-08-21", "authors_parsed": [["He", "Yihui", ""], ["Ma", "Xiaobo", ""], ["Luo", "Xiapu", ""], ["Li", "Jianfeng", ""], ["Zhao", "Mengchen", ""], ["An", "Bo", ""], ["Guan", "Xiaohong", ""]]}, {"id": "1705.08509", "submitter": "Pouria Amirian Dr.", "authors": "Pouria Amirian, Anahid Basiri, Jeremy Morley", "title": "Predictive Analytics for Enhancing Travel Time Estimation in Navigation\n  Apps of Apple, Google, and Microsoft", "comments": null, "journal-ref": null, "doi": "10.1145/3003965.3003976", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The explosive growth of the location-enabled devices coupled with the\nincreasing use of Internet services has led to an increasing awareness of the\nimportance and usage of geospatial information in many applications. The\nnavigation apps (often called Maps), use a variety of available data sources to\ncalculate and predict the travel time as well as several options for routing in\npublic transportation, car or pedestrian modes. This paper evaluates the\npedestrian mode of Maps apps in three major smartphone operating systems\n(Android, iOS and Windows Phone). In the paper, we will show that the Maps apps\non iOS, Android and Windows Phone in pedestrian mode, predict travel time\nwithout learning from the individual's movement profile. In addition, we will\nexemplify that those apps suffer from a specific data quality issue which\nrelates to the absence of information about location and type of pedestrian\ncrossings. Finally, we will illustrate learning from movement profile of\nindividuals using various predictive analytics models to improve the accuracy\nof travel time estimation.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 19:54:19 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Amirian", "Pouria", ""], ["Basiri", "Anahid", ""], ["Morley", "Jeremy", ""]]}, {"id": "1705.08520", "submitter": "Horst Samulowitz", "authors": "Gonzalo Diaz, Achille Fokoue, Giacomo Nannicini, Horst Samulowitz", "title": "An effective algorithm for hyperparameter optimization of neural\n  networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge in designing neural network (NN) systems is to determine\nthe best structure and parameters for the network given the data for the\nmachine learning problem at hand. Examples of parameters are the number of\nlayers and nodes, the learning rates, and the dropout rates. Typically, these\nparameters are chosen based on heuristic rules and manually fine-tuned, which\nmay be very time-consuming, because evaluating the performance of a single\nparametrization of the NN may require several hours. This paper addresses the\nproblem of choosing appropriate parameters for the NN by formulating it as a\nbox-constrained mathematical optimization problem, and applying a\nderivative-free optimization tool that automatically and effectively searches\nthe parameter space. The optimization tool employs a radial basis function\nmodel of the objective function (the prediction accuracy of the NN) to\naccelerate the discovery of configurations yielding high accuracy. Candidate\nconfigurations explored by the algorithm are trained to a small number of\nepochs, and only the most promising candidates receive full training. The\nperformance of the proposed methodology is assessed on benchmark sets and in\nthe context of predicting drug-drug interactions, showing promising results.\nThe optimization tool used in this paper is open-source.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 20:17:44 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Diaz", "Gonzalo", ""], ["Fokoue", "Achille", ""], ["Nannicini", "Giacomo", ""], ["Samulowitz", "Horst", ""]]}, {"id": "1705.08551", "submitter": "Felix Berkenkamp", "authors": "Felix Berkenkamp, Matteo Turchetta, Angela P. Schoellig, Andreas\n  Krause", "title": "Safe Model-based Reinforcement Learning with Stability Guarantees", "comments": "Proc. of Neural Information Processing Systems (NIPS), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning is a powerful paradigm for learning optimal policies\nfrom experimental data. However, to find optimal policies, most reinforcement\nlearning algorithms explore all possible actions, which may be harmful for\nreal-world systems. As a consequence, learning algorithms are rarely applied on\nsafety-critical systems in the real world. In this paper, we present a learning\nalgorithm that explicitly considers safety, defined in terms of stability\nguarantees. Specifically, we extend control-theoretic results on Lyapunov\nstability verification and show how to use statistical models of the dynamics\nto obtain high-performance control policies with provable stability\ncertificates. Moreover, under additional regularity assumptions in terms of a\nGaussian process prior, we prove that one can effectively and safely collect\ndata in order to learn about the dynamics and thus both improve control\nperformance and expand the safe region of the state space. In our experiments,\nwe show how the resulting algorithm can safely optimize a neural network policy\non a simulated inverted pendulum, without the pendulum ever falling down.\n", "versions": [{"version": "v1", "created": "Tue, 23 May 2017 22:20:08 GMT"}, {"version": "v2", "created": "Tue, 7 Nov 2017 10:14:13 GMT"}, {"version": "v3", "created": "Mon, 13 Nov 2017 18:49:54 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Berkenkamp", "Felix", ""], ["Turchetta", "Matteo", ""], ["Schoellig", "Angela P.", ""], ["Krause", "Andreas", ""]]}, {"id": "1705.08584", "submitter": "Wei-Cheng Chang", "authors": "Chun-Liang Li, Wei-Cheng Chang, Yu Cheng, Yiming Yang, Barnab\\'as\n  P\\'oczos", "title": "MMD GAN: Towards Deeper Understanding of Moment Matching Network", "comments": "In the Proceedings of Thirty-first Annual Conference on Neural\n  Information Processing Systems (NIPS 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative moment matching network (GMMN) is a deep generative model that\ndiffers from Generative Adversarial Network (GAN) by replacing the\ndiscriminator in GAN with a two-sample test based on kernel maximum mean\ndiscrepancy (MMD). Although some theoretical guarantees of MMD have been\nstudied, the empirical performance of GMMN is still not as competitive as that\nof GAN on challenging and large benchmark datasets. The computational\nefficiency of GMMN is also less desirable in comparison with GAN, partially due\nto its requirement for a rather large batch size during the training. In this\npaper, we propose to improve both the model expressiveness of GMMN and its\ncomputational efficiency by introducing adversarial kernel learning techniques,\nas the replacement of a fixed Gaussian kernel in the original GMMN. The new\napproach combines the key ideas in both GMMN and GAN, hence we name it MMD GAN.\nThe new distance measure in MMD GAN is a meaningful loss that enjoys the\nadvantage of weak topology and can be optimized via gradient descent with\nrelatively small batch sizes. In our evaluation on multiple benchmark datasets,\nincluding MNIST, CIFAR- 10, CelebA and LSUN, the performance of MMD-GAN\nsignificantly outperforms GMMN, and is competitive with other representative\nGAN works.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 02:20:29 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 17:05:42 GMT"}, {"version": "v3", "created": "Mon, 27 Nov 2017 14:04:35 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Li", "Chun-Liang", ""], ["Chang", "Wei-Cheng", ""], ["Cheng", "Yu", ""], ["Yang", "Yiming", ""], ["P\u00f3czos", "Barnab\u00e1s", ""]]}, {"id": "1705.08690", "submitter": "Hanul Shin", "authors": "Hanul Shin, Jung Kwon Lee, Jaehong Kim, Jiwon Kim", "title": "Continual Learning with Deep Generative Replay", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attempts to train a comprehensive artificial intelligence capable of solving\nmultiple tasks have been impeded by a chronic problem called catastrophic\nforgetting. Although simply replaying all previous data alleviates the problem,\nit requires large memory and even worse, often infeasible in real world\napplications where the access to past data is limited. Inspired by the\ngenerative nature of hippocampus as a short-term memory system in primate\nbrain, we propose the Deep Generative Replay, a novel framework with a\ncooperative dual model architecture consisting of a deep generative model\n(\"generator\") and a task solving model (\"solver\"). With only these two models,\ntraining data for previous tasks can easily be sampled and interleaved with\nthose for a new task. We test our methods in several sequential learning\nsettings involving image classification tasks.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 10:37:38 GMT"}, {"version": "v2", "created": "Sat, 9 Sep 2017 15:31:38 GMT"}, {"version": "v3", "created": "Tue, 12 Dec 2017 02:14:21 GMT"}], "update_date": "2017-12-13", "authors_parsed": [["Shin", "Hanul", ""], ["Lee", "Jung Kwon", ""], ["Kim", "Jaehong", ""], ["Kim", "Jiwon", ""]]}, {"id": "1705.08804", "submitter": "Sirui Yao", "authors": "Sirui Yao, Bert Huang", "title": "Beyond Parity: Fairness Objectives for Collaborative Filtering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study fairness in collaborative-filtering recommender systems, which are\nsensitive to discrimination that exists in historical data. Biased data can\nlead collaborative-filtering methods to make unfair predictions for users from\nminority groups. We identify the insufficiency of existing fairness metrics and\npropose four new metrics that address different forms of unfairness. These\nfairness metrics can be optimized by adding fairness terms to the learning\nobjective. Experiments on synthetic and real data show that our new metrics can\nbetter measure fairness than the baseline, and that the fairness objectives\neffectively help reduce unfairness.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 14:52:06 GMT"}, {"version": "v2", "created": "Thu, 30 Nov 2017 21:11:25 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Yao", "Sirui", ""], ["Huang", "Bert", ""]]}, {"id": "1705.08807", "submitter": "Owain Evans", "authors": "Katja Grace, John Salvatier, Allan Dafoe, Baobao Zhang, Owain Evans", "title": "When Will AI Exceed Human Performance? Evidence from AI Experts", "comments": "Accepted by Journal of Artificial Intelligence Research (AI and\n  Society Track). Minor update to refer to related work (page 5)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in artificial intelligence (AI) will transform modern life by\nreshaping transportation, health, science, finance, and the military. To adapt\npublic policy, we need to better anticipate these advances. Here we report the\nresults from a large survey of machine learning researchers on their beliefs\nabout progress in AI. Researchers predict AI will outperform humans in many\nactivities in the next ten years, such as translating languages (by 2024),\nwriting high-school essays (by 2026), driving a truck (by 2027), working in\nretail (by 2031), writing a bestselling book (by 2049), and working as a\nsurgeon (by 2053). Researchers believe there is a 50% chance of AI\noutperforming humans in all tasks in 45 years and of automating all human jobs\nin 120 years, with Asian respondents expecting these dates much sooner than\nNorth Americans. These results will inform discussion amongst researchers and\npolicymakers about anticipating and managing trends in AI.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 15:00:20 GMT"}, {"version": "v2", "created": "Tue, 30 May 2017 21:36:40 GMT"}, {"version": "v3", "created": "Thu, 3 May 2018 20:14:21 GMT"}], "update_date": "2018-05-07", "authors_parsed": [["Grace", "Katja", ""], ["Salvatier", "John", ""], ["Dafoe", "Allan", ""], ["Zhang", "Baobao", ""], ["Evans", "Owain", ""]]}, {"id": "1705.08844", "submitter": "Rodrigo Toro Icarte", "authors": "Rodrigo Toro Icarte, Jorge A. Baier, Cristian Ruz, Alvaro Soto", "title": "How a General-Purpose Commonsense Ontology can Improve Performance of\n  Learning-Based Image Retrieval", "comments": "Accepted in IJCAI-17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The knowledge representation community has built general-purpose ontologies\nwhich contain large amounts of commonsense knowledge over relevant aspects of\nthe world, including useful visual information, e.g.: \"a ball is used by a\nfootball player\", \"a tennis player is located at a tennis court\". Current\nstate-of-the-art approaches for visual recognition do not exploit these\nrule-based knowledge sources. Instead, they learn recognition models directly\nfrom training examples. In this paper, we study how general-purpose\nontologies---specifically, MIT's ConceptNet ontology---can improve the\nperformance of state-of-the-art vision systems. As a testbed, we tackle the\nproblem of sentence-based image retrieval. Our retrieval approach incorporates\nknowledge from ConceptNet on top of a large pool of object detectors derived\nfrom a deep learning technique. In our experiments, we show that ConceptNet can\nimprove performance on a common benchmark dataset. Key to our performance is\nthe use of the ESPGAME dataset to select visually relevant relations from\nConceptNet. Consequently, a main conclusion of this work is that\ngeneral-purpose commonsense ontologies improve performance on visual reasoning\ntasks when properly filtered to select meaningful visual relations.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 16:22:53 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Icarte", "Rodrigo Toro", ""], ["Baier", "Jorge A.", ""], ["Ruz", "Cristian", ""], ["Soto", "Alvaro", ""]]}, {"id": "1705.08850", "submitter": "Abhishek Kumar", "authors": "Abhishek Kumar, Prasanna Sattigeri, P. Thomas Fletcher", "title": "Semi-supervised Learning with GANs: Manifold Invariance with Improved\n  Inference", "comments": "NIPS 2017 accepted version, including appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised learning methods using Generative Adversarial Networks (GANs)\nhave shown promising empirical success recently. Most of these methods use a\nshared discriminator/classifier which discriminates real examples from fake\nwhile also predicting the class label. Motivated by the ability of the GANs\ngenerator to capture the data manifold well, we propose to estimate the tangent\nspace to the data manifold using GANs and employ it to inject invariances into\nthe classifier. In the process, we propose enhancements over existing methods\nfor learning the inverse mapping (i.e., the encoder) which greatly improves in\nterms of semantic similarity of the reconstructed sample with the input sample.\nWe observe considerable empirical gains in semi-supervised learning over\nbaselines, particularly in the cases when the number of labeled examples is\nlow. We also provide insights into how fake examples influence the\nsemi-supervised learning procedure.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 16:35:37 GMT"}, {"version": "v2", "created": "Tue, 5 Dec 2017 18:34:17 GMT"}], "update_date": "2018-06-13", "authors_parsed": [["Kumar", "Abhishek", ""], ["Sattigeri", "Prasanna", ""], ["Fletcher", "P. Thomas", ""]]}, {"id": "1705.08868", "submitter": "Aditya Grover", "authors": "Aditya Grover, Manik Dhar, Stefano Ermon", "title": "Flow-GAN: Combining Maximum Likelihood and Adversarial Learning in\n  Generative Models", "comments": "AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial learning of probabilistic models has recently emerged as a\npromising alternative to maximum likelihood. Implicit models such as generative\nadversarial networks (GAN) often generate better samples compared to explicit\nmodels trained by maximum likelihood. Yet, GANs sidestep the characterization\nof an explicit density which makes quantitative evaluations challenging. To\nbridge this gap, we propose Flow-GANs, a generative adversarial network for\nwhich we can perform exact likelihood evaluation, thus supporting both\nadversarial and maximum likelihood training. When trained adversarially,\nFlow-GANs generate high-quality samples but attain extremely poor\nlog-likelihood scores, inferior even to a mixture model memorizing the training\ndata; the opposite is true when trained by maximum likelihood. Results on MNIST\nand CIFAR-10 demonstrate that hybrid training can attain high held-out\nlikelihoods while retaining visual fidelity in the generated samples.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 17:11:25 GMT"}, {"version": "v2", "created": "Wed, 3 Jan 2018 21:47:01 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Grover", "Aditya", ""], ["Dhar", "Manik", ""], ["Ermon", "Stefano", ""]]}, {"id": "1705.08926", "submitter": "Gregory Farquhar", "authors": "Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas\n  Nardelli, Shimon Whiteson", "title": "Counterfactual Multi-Agent Policy Gradients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cooperative multi-agent systems can be naturally used to model many real\nworld problems, such as network packet routing and the coordination of\nautonomous vehicles. There is a great need for new reinforcement learning\nmethods that can efficiently learn decentralised policies for such systems. To\nthis end, we propose a new multi-agent actor-critic method called\ncounterfactual multi-agent (COMA) policy gradients. COMA uses a centralised\ncritic to estimate the Q-function and decentralised actors to optimise the\nagents' policies. In addition, to address the challenges of multi-agent credit\nassignment, it uses a counterfactual baseline that marginalises out a single\nagent's action, while keeping the other agents' actions fixed. COMA also uses a\ncritic representation that allows the counterfactual baseline to be computed\nefficiently in a single forward pass. We evaluate COMA in the testbed of\nStarCraft unit micromanagement, using a decentralised variant with significant\npartial observability. COMA significantly improves average performance over\nother multi-agent actor-critic methods in this setting, and the best performing\nagents are competitive with state-of-the-art centralised controllers that get\naccess to the full state.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 18:52:17 GMT"}, {"version": "v2", "created": "Thu, 14 Dec 2017 14:50:34 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Foerster", "Jakob", ""], ["Farquhar", "Gregory", ""], ["Afouras", "Triantafyllos", ""], ["Nardelli", "Nantas", ""], ["Whiteson", "Shimon", ""]]}, {"id": "1705.08927", "submitter": "Davide Venturelli", "authors": "Davide Venturelli, Minh Do, Eleanor Rieffel, Jeremy Frank", "title": "Compiling quantum circuits to realistic hardware architectures using\n  temporal planners", "comments": "updated manuscript, more planners and results", "journal-ref": "2017 Quantum Sci. Technol. - also related to proceedings of IJCAI\n  2017, and ICAPS SPARK Workshop 2017", "doi": "10.1088/2058-9565/aaa331", "report-no": null, "categories": "quant-ph cs.AI cs.ET cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To run quantum algorithms on emerging gate-model quantum hardware, quantum\ncircuits must be compiled to take into account constraints on the hardware. For\nnear-term hardware, with only limited means to mitigate decoherence, it is\ncritical to minimize the duration of the circuit. We investigate the\napplication of temporal planners to the problem of compiling quantum circuits\nto newly emerging quantum hardware. While our approach is general, we focus on\ncompiling to superconducting hardware architectures with nearest neighbor\nconstraints. Our initial experiments focus on compiling Quantum Alternating\nOperator Ansatz (QAOA) circuits whose high number of commuting gates allow\ngreat flexibility in the order in which the gates can be applied. That freedom\nmakes it more challenging to find optimal compilations but also means there is\na greater potential win from more optimized compilation than for less flexible\ncircuits. We map this quantum circuit compilation problem to a temporal\nplanning problem, and generated a test suite of compilation problems for QAOA\ncircuits of various sizes to a realistic hardware architecture. We report\ncompilation results from several state-of-the-art temporal planners on this\ntest set. This early empirical evaluation demonstrates that temporal planning\nis a viable approach to quantum circuit compilation.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 18:52:43 GMT"}, {"version": "v2", "created": "Thu, 21 Dec 2017 10:41:42 GMT"}], "update_date": "2017-12-22", "authors_parsed": [["Venturelli", "Davide", ""], ["Do", "Minh", ""], ["Rieffel", "Eleanor", ""], ["Frank", "Jeremy", ""]]}, {"id": "1705.08961", "submitter": "Roni Stern", "authors": "Roni Stern and Brendan Juba", "title": "Efficient, Safe, and Probably Approximately Complete Learning of Action\n  Models", "comments": null, "journal-ref": "International Joint Conference on Artificial Intelligence (IJCAI)\n  2017", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we explore the theoretical boundaries of planning in a setting\nwhere no model of the agent's actions is given. Instead of an action model, a\nset of successfully executed plans are given and the task is to generate a plan\nthat is safe, i.e., guaranteed to achieve the goal without failing. To this\nend, we show how to learn a conservative model of the world in which actions\nare guaranteed to be applicable. This conservative model is then given to an\noff-the-shelf classical planner, resulting in a plan that is guaranteed to\nachieve the goal. However, this reduction from a model-free planning to a\nmodel-based planning is not complete: in some cases a plan will not be found\neven when such exists. We analyze the relation between the number of observed\nplans and the likelihood that our conservative approach will indeed fail to\nsolve a solvable problem. Our analysis show that the number of trajectories\nneeded scales gracefully.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 20:38:52 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Stern", "Roni", ""], ["Juba", "Brendan", ""]]}, {"id": "1705.08968", "submitter": "Artur Garcez", "authors": "Ivan Donadello, Luciano Serafini, Artur d'Avila Garcez", "title": "Logic Tensor Networks for Semantic Image Interpretation", "comments": "14 pages, 2 figures, IJCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic Image Interpretation (SII) is the task of extracting structured\nsemantic descriptions from images. It is widely agreed that the combined use of\nvisual data and background knowledge is of great importance for SII. Recently,\nStatistical Relational Learning (SRL) approaches have been developed for\nreasoning under uncertainty and learning in the presence of data and rich\nknowledge. Logic Tensor Networks (LTNs) are an SRL framework which integrates\nneural networks with first-order fuzzy logic to allow (i) efficient learning\nfrom noisy data in the presence of logical constraints, and (ii) reasoning with\nlogical formulas describing general properties of the data. In this paper, we\ndevelop and apply LTNs to two of the main tasks of SII, namely, the\nclassification of an image's bounding boxes and the detection of the relevant\npart-of relations between objects. To the best of our knowledge, this is the\nfirst successful application of SRL to such SII tasks. The proposed approach is\nevaluated on a standard image processing benchmark. Experiments show that the\nuse of background knowledge in the form of logical constraints can improve the\nperformance of purely data-driven approaches, including the state-of-the-art\nFast Region-based Convolutional Neural Networks (Fast R-CNN). Moreover, we show\nthat the use of logical background knowledge adds robustness to the learning\nsystem when errors are present in the labels of the training data.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 21:34:14 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Donadello", "Ivan", ""], ["Serafini", "Luciano", ""], ["Garcez", "Artur d'Avila", ""]]}, {"id": "1705.08982", "submitter": "Shuai Xiao", "authors": "Shuai Xiao, Junchi Yan, Stephen M. Chu, Xiaokang Yang, Hongyuan Zha", "title": "Modeling The Intensity Function Of Point Process Via Recurrent Neural\n  Networks", "comments": "Accepted at Thirty-First AAAI Conference on Artificial Intelligence\n  (AAAI17)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event sequence, asynchronously generated with random timestamp, is ubiquitous\namong applications. The precise and arbitrary timestamp can carry important\nclues about the underlying dynamics, and has lent the event data fundamentally\ndifferent from the time-series whereby series is indexed with fixed and equal\ntime interval. One expressive mathematical tool for modeling event is point\nprocess. The intensity functions of many point processes involve two\ncomponents: the background and the effect by the history. Due to its inherent\nspontaneousness, the background can be treated as a time series while the other\nneed to handle the history events. In this paper, we model the background by a\nRecurrent Neural Network (RNN) with its units aligned with time series indexes\nwhile the history effect is modeled by another RNN whose units are aligned with\nasynchronous events to capture the long-range dynamics. The whole model with\nevent type and timestamp prediction output layers can be trained end-to-end.\nOur approach takes an RNN perspective to point process, and models its\nbackground and history effect. For utility, our method allows a black-box\ntreatment for modeling the intensity which is often a pre-defined parametric\nform in point processes. Meanwhile end-to-end training opens the venue for\nreusing existing rich techniques in deep network for point process modeling. We\napply our model to the predictive maintenance problem using a log dataset by\nmore than 1000 ATMs from a global bank headquartered in North America.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 22:23:14 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Xiao", "Shuai", ""], ["Yan", "Junchi", ""], ["Chu", "Stephen M.", ""], ["Yang", "Xiaokang", ""], ["Zha", "Hongyuan", ""]]}, {"id": "1705.08997", "submitter": "Farhan Tejani", "authors": "Himanshu Sahni, Saurabh Kumar, Farhan Tejani, Yannick Schroecker,\n  Charles Isbell", "title": "State Space Decomposition and Subgoal Creation for Transfer in Deep\n  Reinforcement Learning", "comments": "5 pages, 6 figures; 3rd Multidisciplinary Conference on Reinforcement\n  Learning and Decision Making (RLDM 2017), Ann Arbor, Michigan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Typical reinforcement learning (RL) agents learn to complete tasks specified\nby reward functions tailored to their domain. As such, the policies they learn\ndo not generalize even to similar domains. To address this issue, we develop a\nframework through which a deep RL agent learns to generalize policies from\nsmaller, simpler domains to more complex ones using a recurrent attention\nmechanism. The task is presented to the agent as an image and an instruction\nspecifying the goal. This meta-controller guides the agent towards its goal by\ndesigning a sequence of smaller subtasks on the part of the state space within\nthe attention, effectively decomposing it. As a baseline, we consider a setup\nwithout attention as well. Our experiments show that the meta-controller learns\nto create subgoals within the attention.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 23:19:44 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["Sahni", "Himanshu", ""], ["Kumar", "Saurabh", ""], ["Tejani", "Farhan", ""], ["Schroecker", "Yannick", ""], ["Isbell", "Charles", ""]]}, {"id": "1705.09011", "submitter": "Han Zhao", "authors": "Han Zhao, Zhenyao Zhu, Junjie Hu, Adam Coates, Geoff Gordon", "title": "Principled Hybrids of Generative and Discriminative Domain Adaptation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a probabilistic framework for domain adaptation that blends both\ngenerative and discriminative modeling in a principled way. Under this\nframework, generative and discriminative models correspond to specific choices\nof the prior over parameters. This provides us a very general way to\ninterpolate between generative and discriminative extremes through different\nchoices of priors. By maximizing both the marginal and the conditional\nlog-likelihoods, models derived from this framework can use both labeled\ninstances from the source domain as well as unlabeled instances from both\nsource and target domains. Under this framework, we show that the popular\nreconstruction loss of autoencoder corresponds to an upper bound of the\nnegative marginal log-likelihoods of unlabeled instances, where marginal\ndistributions are given by proper kernel density estimations. This provides a\nway to interpret the empirical success of autoencoders in domain adaptation and\nsemi-supervised learning. We instantiate our framework using neural networks,\nand build a concrete model, DAuto. Empirically, we demonstrate the\neffectiveness of DAuto on text, image and speech datasets, showing that it\noutperforms related competitors when domain adaptation is possible.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 01:02:16 GMT"}, {"version": "v2", "created": "Fri, 27 Oct 2017 19:29:31 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Zhao", "Han", ""], ["Zhu", "Zhenyao", ""], ["Hu", "Junjie", ""], ["Coates", "Adam", ""], ["Gordon", "Geoff", ""]]}, {"id": "1705.09026", "submitter": "Bert Huang", "authors": "Walid Chaabene and Bert Huang", "title": "Best-Choice Edge Grafting for Efficient Structure Learning of Markov\n  Random Fields", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incremental methods for structure learning of pairwise Markov random fields\n(MRFs), such as grafting, improve scalability by avoiding inference over the\nentire feature space in each optimization step. Instead, inference is performed\nover an incrementally grown active set of features. In this paper, we address\nkey computational bottlenecks that current incremental techniques still suffer\nby introducing best-choice edge grafting, an incremental, structured method\nthat activates edges as groups of features in a streaming setting. The method\nuses a reservoir of edges that satisfy an activation condition, approximating\nthe search for the optimal edge to activate. It also reorganizes the search\nspace using search-history and structure heuristics. Experiments show a\nsignificant speedup for structure learning and a controllable trade-off between\nthe speed and quality of learning.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 02:28:53 GMT"}, {"version": "v2", "created": "Sun, 20 May 2018 01:28:19 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Chaabene", "Walid", ""], ["Huang", "Bert", ""]]}, {"id": "1705.09045", "submitter": "Ashley Edwards", "authors": "Ashley D. Edwards, Srijan Sood, and Charles L. Isbell Jr", "title": "Cross-Domain Perceptual Reward Functions", "comments": "A shorter version of this paper was accepted to RLDM\n  (http://rldm.org/rldm2017/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In reinforcement learning, we often define goals by specifying rewards within\ndesirable states. One problem with this approach is that we typically need to\nredefine the rewards each time the goal changes, which often requires some\nunderstanding of the solution in the agents environment. When humans are\nlearning to complete tasks, we regularly utilize alternative sources that guide\nour understanding of the problem. Such task representations allow one to\nspecify goals on their own terms, thus providing specifications that can be\nappropriately interpreted across various environments. This motivates our own\nwork, in which we represent goals in environments that are different from the\nagents. We introduce Cross-Domain Perceptual Reward (CDPR) functions, learned\nrewards that represent the visual similarity between an agents state and a\ncross-domain goal image. We report results for learning the CDPRs with a deep\nneural network and using them to solve two tasks with deep reinforcement\nlearning.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 04:54:36 GMT"}, {"version": "v2", "created": "Wed, 7 Jun 2017 15:44:37 GMT"}, {"version": "v3", "created": "Tue, 25 Jul 2017 15:40:28 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Edwards", "Ashley D.", ""], ["Sood", "Srijan", ""], ["Isbell", "Charles L.", "Jr"]]}, {"id": "1705.09058", "submitter": "Yihui He", "authors": "Yihui He, Ming Xiang", "title": "An Empirical Analysis of Approximation Algorithms for the Euclidean\n  Traveling Salesman Problem", "comments": "4 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With applications to many disciplines, the traveling salesman problem (TSP)\nis a classical computer science optimization problem with applications to\nindustrial engineering, theoretical computer science, bioinformatics, and\nseveral other disciplines. In recent years, there have been a plethora of novel\napproaches for approximate solutions ranging from simplistic greedy to\ncooperative distributed algorithms derived from artificial intelligence. In\nthis paper, we perform an evaluation and analysis of cornerstone algorithms for\nthe Euclidean TSP. We evaluate greedy, 2-opt, and genetic algorithms. We use\nseveral datasets as input for the algorithms including a small dataset, a\nmediumsized dataset representing cities in the United States, and a synthetic\ndataset consisting of 200 cities to test algorithm scalability. We discover\nthat the greedy and 2-opt algorithms efficiently calculate solutions for\nsmaller datasets. Genetic algorithm has the best performance for optimality for\nmedium to large datasets, but generally have longer runtime. Our\nimplementations is public available.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 06:21:39 GMT"}], "update_date": "2017-05-26", "authors_parsed": [["He", "Yihui", ""], ["Xiang", "Ming", ""]]}, {"id": "1705.09207", "submitter": "Yang Liu", "authors": "Yang Liu and Mirella Lapata", "title": "Learning Structured Text Representations", "comments": "change to one-based indexing, published in Transactions of the\n  Association for Computational Linguistics (TACL),\n  https://transacl.org/ojs/index.php/tacl/article/view/1185/280", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on learning structure-aware document representations\nfrom data without recourse to a discourse parser or additional annotations.\nDrawing inspiration from recent efforts to empower neural networks with a\nstructural bias, we propose a model that can encode a document while\nautomatically inducing rich structural dependencies. Specifically, we embed a\ndifferentiable non-projective parsing algorithm into a neural model and use\nattention mechanisms to incorporate the structural biases. Experimental\nevaluation across different tasks and datasets shows that the proposed model\nachieves state-of-the-art results on document modeling tasks while inducing\nintermediate structures which are both interpretable and meaningful.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 14:54:07 GMT"}, {"version": "v2", "created": "Thu, 20 Jul 2017 23:14:58 GMT"}, {"version": "v3", "created": "Thu, 14 Sep 2017 20:42:25 GMT"}, {"version": "v4", "created": "Sat, 3 Feb 2018 13:31:40 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Liu", "Yang", ""], ["Lapata", "Mirella", ""]]}, {"id": "1705.09218", "submitter": "Mohamed Siala Dr", "authors": "Begum Genc, Mohamed Siala, Barry O'Sullivan, Gilles Simonin", "title": "Finding Robust Solutions to Stable Marriage", "comments": "IJCAI 2017 proceedings", "journal-ref": null, "doi": "10.24963/ijcai.2017/88", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the notion of robustness in stable matching problems. We first\ndefine robustness by introducing (a,b)-supermatches. An $(a,b)$-supermatch is a\nstable matching in which if $a$ pairs break up it is possible to find another\nstable matching by changing the partners of those $a$ pairs and at most $b$\nother pairs. In this context, we define the most robust stable matching as a\n$(1,b)$-supermatch where b is minimum. We show that checking whether a given\nstable matching is a $(1,b)$-supermatch can be done in polynomial time. Next,\nwe use this procedure to design a constraint programming model, a local search\napproach, and a genetic algorithm to find the most robust stable matching. Our\nempirical evaluation on large instances show that local search outperforms the\nother approaches.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 07:49:52 GMT"}, {"version": "v2", "created": "Sun, 20 Aug 2017 12:25:42 GMT"}, {"version": "v3", "created": "Fri, 27 Oct 2017 13:53:56 GMT"}], "update_date": "2017-10-30", "authors_parsed": [["Genc", "Begum", ""], ["Siala", "Mohamed", ""], ["O'Sullivan", "Barry", ""], ["Simonin", "Gilles", ""]]}, {"id": "1705.09231", "submitter": "Swarat Chaudhuri", "authors": "Rohan Mukherjee, Dipak Chaudhari, Matthew Amodio, Thomas Reps, Swarat\n  Chaudhuri, Chris Jermaine", "title": "Neural Attribute Grammars for Semantics-Guided Program Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing deep models for code tend to be trained on syntactic program\nrepresentations. We present an alternative, called Neural Attribute Grammars,\nthat exposes the semantics of the target language to the training procedure\nusing an attribute grammar. During training, our model learns to replicate the\nrelationship between the syntactic rules used to construct a program, and the\nsemantic attributes (for example, symbol tables) constructed from the context\nin which the rules are fired. We implement the approach as a system for\nconditional generation of Java programs modulo eleven natural requirements. Our\nexperiments show that the system generates constraint-abiding programs with\nsignificantly higher frequency than a baseline model trained on syntactic\nprogram representations, and also in terms of generation accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 15:35:16 GMT"}, {"version": "v2", "created": "Wed, 31 May 2017 04:00:12 GMT"}, {"version": "v3", "created": "Sat, 20 Feb 2021 20:32:07 GMT"}], "update_date": "2021-02-23", "authors_parsed": [["Mukherjee", "Rohan", ""], ["Chaudhari", "Dipak", ""], ["Amodio", "Matthew", ""], ["Reps", "Thomas", ""], ["Chaudhuri", "Swarat", ""], ["Jermaine", "Chris", ""]]}, {"id": "1705.09279", "submitter": "Chris J. Maddison", "authors": "Chris J. Maddison, Dieterich Lawson, George Tucker, Nicolas Heess,\n  Mohammad Norouzi, Andriy Mnih, Arnaud Doucet, Yee Whye Teh", "title": "Filtering Variational Objectives", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When used as a surrogate objective for maximum likelihood estimation in\nlatent variable models, the evidence lower bound (ELBO) produces\nstate-of-the-art results. Inspired by this, we consider the extension of the\nELBO to a family of lower bounds defined by a particle filter's estimator of\nthe marginal likelihood, the filtering variational objectives (FIVOs). FIVOs\ntake the same arguments as the ELBO, but can exploit a model's sequential\nstructure to form tighter bounds. We present results that relate the tightness\nof FIVO's bound to the variance of the particle filter's estimator by\nconsidering the generic case of bounds defined as log-transformed likelihood\nestimators. Experimentally, we show that training with FIVO results in\nsubstantial improvements over training the same model architecture with the\nELBO on sequential data.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 17:52:41 GMT"}, {"version": "v2", "created": "Mon, 28 Aug 2017 17:58:51 GMT"}, {"version": "v3", "created": "Sun, 12 Nov 2017 20:38:13 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Maddison", "Chris J.", ""], ["Lawson", "Dieterich", ""], ["Tucker", "George", ""], ["Heess", "Nicolas", ""], ["Norouzi", "Mohammad", ""], ["Mnih", "Andriy", ""], ["Doucet", "Arnaud", ""], ["Teh", "Yee Whye", ""]]}, {"id": "1705.09328", "submitter": "Gabriele Farina", "authors": "Gabriele Farina and John P. Dickerson and Tuomas Sandholm", "title": "Operation Frames and Clubs in Kidney Exchange", "comments": "Published at IJCAI-17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A kidney exchange is a centrally-administered barter market where patients\nswap their willing yet incompatible donors. Modern kidney exchanges use\n2-cycles, 3-cycles, and chains initiated by non-directed donors (altruists who\nare willing to give a kidney to anyone) as the means for swapping.\n  We propose significant generalizations to kidney exchange. We allow more than\none donor to donate in exchange for their desired patient receiving a kidney.\nWe also allow for the possibility of a donor willing to donate if any of a\nnumber of patients receive kidneys. Furthermore, we combine these notions and\ngeneralize them. The generalization is to exchange among organ clubs, where a\nclub is willing to donate organs outside the club if and only if the club\nreceives organs from outside the club according to given specifications. We\nprove that unlike in the standard model, the uncapped clearing problem is\nNP-complete.\n  We also present the notion of operation frames that can be used to sequence\nthe operations across batches, and present integer programming formulations for\nthe market clearing problems for these new types of organ exchanges.\n  Experiments show that in the single-donation setting, operation frames\nimprove planning by 34%--51%. Allowing up to two donors to donate in exchange\nfor one kidney donated to their designated patient yields a further increase in\nsocial welfare.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 18:58:58 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Farina", "Gabriele", ""], ["Dickerson", "John P.", ""], ["Sandholm", "Tuomas", ""]]}, {"id": "1705.09349", "submitter": "Pavel Naumov", "authors": "Pavel Naumov and Jia Tao", "title": "Together We Know How to Achieve: An Epistemic Logic of Know-How", "comments": "An extended abstract of this paper will appear in Proceedings of 16th\n  conference on Theoretical Aspects of Rationality and Knowledge (TARK-17),\n  Liverpool, United Kingdom, July 24-26, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT cs.LO cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The existence of a coalition strategy to achieve a goal does not necessarily\nmean that the coalition has enough information to know how to follow the\nstrategy. Neither does it mean that the coalition knows that such a strategy\nexists. The article studies an interplay between the distributed knowledge,\ncoalition strategies, and coalition \"know-how\" strategies. The main technical\nresult is a sound and complete trimodal logical system that describes the\nproperties of this interplay.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 20:22:16 GMT"}, {"version": "v2", "created": "Thu, 15 Jun 2017 18:59:15 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["Naumov", "Pavel", ""], ["Tao", "Jia", ""]]}, {"id": "1705.09359", "submitter": "Niek Tax", "authors": "Niek Tax, Emin Alasgarov, Natalia Sidorova, Wil M.P. van der Aalst,\n  Reinder Haakma", "title": "Generating Time-Based Label Refinements to Discover More Precise Process\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Process mining is a research field focused on the analysis of event data with\nthe aim of extracting insights related to dynamic behavior. Applying process\nmining techniques on data from smart home environments has the potential to\nprovide valuable insights in (un)healthy habits and to contribute to ambient\nassisted living solutions. Finding the right event labels to enable the\napplication of process mining techniques is however far from trivial, as simply\nusing the triggering sensor as the label for sensor events results in\nuninformative models that allow for too much behavior (overgeneralizing).\nRefinements of sensor level event labels suggested by domain experts have been\nshown to enable discovery of more precise and insightful process models.\nHowever, there exists no automated approach to generate refinements of event\nlabels in the context of process mining. In this paper we propose a framework\nfor the automated generation of label refinements based on the time attribute\nof events, allowing us to distinguish behaviourally different instances of the\nsame event type based on their time attribute. We show on a case study with\nreal life smart home event data that using automatically generated refined\nlabels in process discovery, we can find more specific, and therefore more\ninsightful, process models. We observe that one label refinement could have an\neffect on the usefulness of other label refinements when used together.\nTherefore, we explore four strategies to generate useful combinations of\nmultiple label refinements and evaluate those on three real life smart home\nevent logs.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 21:01:20 GMT"}, {"version": "v2", "created": "Tue, 31 Oct 2017 16:22:43 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Tax", "Niek", ""], ["Alasgarov", "Emin", ""], ["Sidorova", "Natalia", ""], ["van der Aalst", "Wil M. P.", ""], ["Haakma", "Reinder", ""]]}, {"id": "1705.09382", "submitter": "Vahan Huroyan", "authors": "Vahan Huroyan and Gilad Lerman", "title": "Distributed Robust Subspace Recovery", "comments": null, "journal-ref": "SIAM J. Sci. Comput. 40 (2018) A3067-A3090", "doi": "10.1137/17M1131659", "report-no": null, "categories": "math.NA cs.AI cs.DC cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose distributed solutions to the problem of Robust Subspace Recovery\n(RSR). Our setting assumes a huge dataset in an ad hoc network without a\ncentral processor, where each node has access only to one chunk of the dataset.\nFurthermore, part of the whole dataset lies around a low-dimensional subspace\nand the other part is composed of outliers that lie away from that subspace.\nThe goal is to recover the underlying subspace for the whole dataset, without\ntransferring the data itself between the nodes. We first apply the\nConsensus-Based Gradient method to the Geometric Median Subspace algorithm for\nRSR. For this purpose, we propose an iterative solution for the local dual\nminimization problem and establish its r-linear convergence. We then explain\nhow to distributedly implement the Reaper and Fast Median Subspace algorithms\nfor RSR. The proposed algorithms display competitive performance on both\nsynthetic and real data.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 22:22:51 GMT"}, {"version": "v2", "created": "Sat, 3 Mar 2018 21:31:02 GMT"}, {"version": "v3", "created": "Wed, 4 Jul 2018 21:56:51 GMT"}], "update_date": "2018-11-07", "authors_parsed": [["Huroyan", "Vahan", ""], ["Lerman", "Gilad", ""]]}, {"id": "1705.09391", "submitter": "Panagiotis Mandros", "authors": "Panagiotis Mandros, Mario Boley, Jilles Vreeken", "title": "Discovering Reliable Approximate Functional Dependencies", "comments": "Accepted: In Proceedings of the ACM SIGKDD Conference on Knowledge\n  Discovery and Data Mining (KDD), August 13-17, 2017, Halifax, NS, Canada", "journal-ref": null, "doi": "10.1145/3097983.3098062", "report-no": null, "categories": "cs.DB cs.AI cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a database and a target attribute of interest, how can we tell whether\nthere exists a functional, or approximately functional dependence of the target\non any set of other attributes in the data? How can we reliably, without bias\nto sample size or dimensionality, measure the strength of such a dependence?\nAnd, how can we efficiently discover the optimal or $\\alpha$-approximate\ntop-$k$ dependencies? These are exactly the questions we answer in this paper.\n  As we want to be agnostic on the form of the dependence, we adopt an\ninformation-theoretic approach, and construct a reliable, bias correcting score\nthat can be efficiently computed. Moreover, we give an effective optimistic\nestimator of this score, by which for the first time we can mine the\napproximate functional dependencies from data with guarantees of optimality.\nEmpirical evaluation shows that the derived score achieves a good bias for\nvariance trade-off, can be used within an efficient discovery algorithm, and\nindeed discovers meaningful dependencies. Most important, it remains reliable\nin the face of data sparsity.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 23:00:46 GMT"}, {"version": "v2", "created": "Sun, 18 Jun 2017 18:18:55 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Mandros", "Panagiotis", ""], ["Boley", "Mario", ""], ["Vreeken", "Jilles", ""]]}, {"id": "1705.09436", "submitter": "Daksh Varshneya", "authors": "Daksh Varshneya, G. Srinivasaraghavan", "title": "Human Trajectory Prediction using Spatially aware Deep Attention Models", "comments": "10 pages, 5 figures, Submitted to 31st Conference on Neural\n  Information Processing Systems (NIPS 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trajectory Prediction of dynamic objects is a widely studied topic in the\nfield of artificial intelligence. Thanks to a large number of applications like\npredicting abnormal events, navigation system for the blind, etc. there have\nbeen many approaches to attempt learning patterns of motion directly from data\nusing a wide variety of techniques ranging from hand-crafted features to\nsophisticated deep learning models for unsupervised feature learning. All these\napproaches have been limited by problems like inefficient features in the case\nof hand crafted features, large error propagation across the predicted\ntrajectory and no information of static artefacts around the dynamic moving\nobjects. We propose an end to end deep learning model to learn the motion\npatterns of humans using different navigational modes directly from data using\nthe much popular sequence to sequence model coupled with a soft attention\nmechanism. We also propose a novel approach to model the static artefacts in a\nscene and using these to predict the dynamic trajectories. The proposed method,\ntested on trajectories of pedestrians, consistently outperforms previously\nproposed state of the art approaches on a variety of large scale data sets. We\nalso show how our architecture can be naturally extended to handle multiple\nmodes of movement (say pedestrians, skaters, bikers and buses) simultaneously.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 05:37:36 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Varshneya", "Daksh", ""], ["Srinivasaraghavan", "G.", ""]]}, {"id": "1705.09439", "submitter": "Kosetsu Tsukuda", "authors": "Kosetsu Tsukuda, Masataka Goto", "title": "Taste or Addiction?: Using Play Logs to Infer Song Selection Motivation", "comments": "Accepted by The 21st Pacific-Asia Conference on Knowledge Discovery\n  and Data Mining (PAKDD 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online music services are increasing in popularity. They enable us to analyze\npeople's music listening behavior based on play logs. Although it is known that\npeople listen to music based on topic (e.g., rock or jazz), we assume that when\na user is addicted to an artist, s/he chooses the artist's songs regardless of\ntopic. Based on this assumption, in this paper, we propose a probabilistic\nmodel to analyze people's music listening behavior. Our main contributions are\nthree-fold. First, to the best of our knowledge, this is the first study\nmodeling music listening behavior by taking into account the influence of\naddiction to artists. Second, by using real-world datasets of play logs, we\nshowed the effectiveness of our proposed model. Third, we carried out\nqualitative experiments and showed that taking addiction into account enables\nus to analyze music listening behavior from a new viewpoint in terms of how\npeople listen to music according to the time of day, how an artist's songs are\nlistened to by people, etc. We also discuss the possibility of applying the\nanalysis results to applications such as artist similarity computation and song\nrecommendation.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 05:54:20 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Tsukuda", "Kosetsu", ""], ["Goto", "Masataka", ""]]}, {"id": "1705.09515", "submitter": "Yannick Esteve", "authors": "Edwin Simonnet (LIUM), Sahar Ghannay (LIUM), Nathalie Camelin (LIUM),\n  Yannick Est\\`eve (LIUM), Renato De Mori (LIA)", "title": "ASR error management for improving spoken language understanding", "comments": "Interspeech 2017, Aug 2017, Stockholm, Sweden. 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of automatic speech recognition (ASR) error\ndetection and their use for improving spoken language understanding (SLU)\nsystems. In this study, the SLU task consists in automatically extracting, from\nASR transcriptions , semantic concepts and concept/values pairs in a e.g\ntouristic information system. An approach is proposed for enriching the set of\nsemantic labels with error specific labels and by using a recently proposed\nneural approach based on word embeddings to compute well calibrated ASR\nconfidence measures. Experimental results are reported showing that it is\npossible to decrease significantly the Concept/Value Error Rate with a state of\nthe art system, outperforming previously published results performance on the\nsame experimental data. It also shown that combining an SLU approach based on\nconditional random fields with a neural encoder/decoder attention based\narchitecture , it is possible to effectively identifying confidence islands and\nuncertain semantic output segments useful for deciding appropriate error\nhandling actions by the dialogue manager strategy .\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 10:34:24 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Simonnet", "Edwin", "", "LIUM"], ["Ghannay", "Sahar", "", "LIUM"], ["Camelin", "Nathalie", "", "LIUM"], ["Est\u00e8ve", "Yannick", "", "LIUM"], ["De Mori", "Renato", "", "LIA"]]}, {"id": "1705.09545", "submitter": "Mark Lewis", "authors": "Fred Glover, Mark Lewis, Gary Kochenberger", "title": "Logical and Inequality Implications for Reducing the Size and Complexity\n  of Quadratic Unconstrained Binary Optimization Problems", "comments": "30 pages + 6 pages of Appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The quadratic unconstrained binary optimization (QUBO) problem arises in\ndiverse optimization applications ranging from Ising spin problems to classical\nproblems in graph theory and binary discrete optimization. The use of\npreprocessing to transform the graph representing the QUBO problem into a\nsmaller equivalent graph is important for improving solution quality and time\nfor both exact and metaheuristic algorithms and is a step towards mapping large\nscale QUBO to hardware graphs used in quantum annealing computers. In an\nearlier paper (Lewis and Glover, 2016) a set of rules was introduced that\nachieved significant QUBO reductions as verified through computational testing.\nHere this work is extended with additional rules that provide further\nreductions that succeed in exactly solving 10% of the benchmark QUBO problems.\nAn algorithm and associated data structures to efficiently implement the entire\nset of rules is detailed and computational experiments are reported that\ndemonstrate their efficacy.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 11:59:49 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Glover", "Fred", ""], ["Lewis", "Mark", ""], ["Kochenberger", "Gary", ""]]}, {"id": "1705.09552", "submitter": "Seyed-Mohsen Moosavi-Dezfooli", "authors": "Alhussein Fawzi, Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard,\n  Stefano Soatto", "title": "Classification regions of deep neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this paper is to analyze the geometric properties of deep neural\nnetwork classifiers in the input space. We specifically study the topology of\nclassification regions created by deep networks, as well as their associated\ndecision boundary. Through a systematic empirical investigation, we show that\nstate-of-the-art deep nets learn connected classification regions, and that the\ndecision boundary in the vicinity of datapoints is flat along most directions.\nWe further draw an essential connection between two seemingly unrelated\nproperties of deep networks: their sensitivity to additive perturbations in the\ninputs, and the curvature of their decision boundary. The directions where the\ndecision boundary is curved in fact remarkably characterize the directions to\nwhich the classifier is the most vulnerable. We finally leverage a fundamental\nasymmetry in the curvature of the decision boundary of deep nets, and propose a\nmethod to discriminate between original images, and images perturbed with small\nadversarial examples. We show the effectiveness of this purely geometric\napproach for detecting small adversarial perturbations in images, and for\nrecovering the labels of perturbed images.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 12:38:48 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Fawzi", "Alhussein", ""], ["Moosavi-Dezfooli", "Seyed-Mohsen", ""], ["Frossard", "Pascal", ""], ["Soatto", "Stefano", ""]]}, {"id": "1705.09554", "submitter": "Seyed-Mohsen Moosavi-Dezfooli", "authors": "Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, Pascal\n  Frossard, Stefano Soatto", "title": "Robustness of classifiers to universal perturbations: a geometric\n  perspective", "comments": "Published at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep networks have recently been shown to be vulnerable to universal\nperturbations: there exist very small image-agnostic perturbations that cause\nmost natural images to be misclassified by such classifiers. In this paper, we\npropose the first quantitative analysis of the robustness of classifiers to\nuniversal perturbations, and draw a formal link between the robustness to\nuniversal perturbations, and the geometry of the decision boundary.\nSpecifically, we establish theoretical bounds on the robustness of classifiers\nunder two decision boundary models (flat and curved models). We show in\nparticular that the robustness of deep networks to universal perturbations is\ndriven by a key property of their curvature: there exists shared directions\nalong which the decision boundary of deep networks is systematically positively\ncurved. Under such conditions, we prove the existence of small universal\nperturbations. Our analysis further provides a novel geometric method for\ncomputing universal perturbations, in addition to explaining their properties.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 12:42:55 GMT"}, {"version": "v2", "created": "Mon, 1 Mar 2021 20:45:00 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Moosavi-Dezfooli", "Seyed-Mohsen", ""], ["Fawzi", "Alhussein", ""], ["Fawzi", "Omar", ""], ["Frossard", "Pascal", ""], ["Soatto", "Stefano", ""]]}, {"id": "1705.09558", "submitter": "Andrew Wilson", "authors": "Yunus Saatchi, Andrew Gordon Wilson", "title": "Bayesian GAN", "comments": "Updated to the version that appears at Advances in Neural Information\n  Processing Systems 30 (NIPS), 2017", "journal-ref": "Advances in Neural Information Processing Systems 30 (NIPS), 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) can implicitly learn rich\ndistributions over images, audio, and data which are hard to model with an\nexplicit likelihood. We present a practical Bayesian formulation for\nunsupervised and semi-supervised learning with GANs. Within this framework, we\nuse stochastic gradient Hamiltonian Monte Carlo to marginalize the weights of\nthe generator and discriminator networks. The resulting approach is\nstraightforward and obtains good performance without any standard interventions\nsuch as feature matching, or mini-batch discrimination. By exploring an\nexpressive posterior over the parameters of the generator, the Bayesian GAN\navoids mode-collapse, produces interpretable and diverse candidate samples, and\nprovides state-of-the-art quantitative results for semi-supervised learning on\nbenchmarks including SVHN, CelebA, and CIFAR-10, outperforming DCGAN,\nWasserstein GANs, and DCGAN ensembles.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 12:47:56 GMT"}, {"version": "v2", "created": "Mon, 29 May 2017 07:54:47 GMT"}, {"version": "v3", "created": "Wed, 8 Nov 2017 17:52:21 GMT"}], "update_date": "2017-11-09", "authors_parsed": [["Saatchi", "Yunus", ""], ["Wilson", "Andrew Gordon", ""]]}, {"id": "1705.09580", "submitter": "Agostino Capponi", "authors": "Agostino Capponi and Reza Ghanadan and Matt Stern", "title": "Risk-Sensitive Cooperative Games for Human-Machine Systems", "comments": "15 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous systems can substantially enhance a human's efficiency and\neffectiveness in complex environments. Machines, however, are often unable to\nobserve the preferences of the humans that they serve. Despite the fact that\nthe human's and machine's objectives are aligned, asymmetric information, along\nwith heterogeneous sensitivities to risk by the human and machine, make their\njoint optimization process a game with strategic interactions. We propose a\nframework based on risk-sensitive dynamic games; the human seeks to optimize\nher risk-sensitive criterion according to her true preferences, while the\nmachine seeks to adaptively learn the human's preferences and at the same time\nprovide a good service to the human. We develop a class of performance measures\nfor the proposed framework based on the concept of regret. We then evaluate\ntheir dependence on the risk-sensitivity and the degree of uncertainty. We\npresent applications of our framework to self-driving taxis, and robo-financial\nadvising.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 13:35:28 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Capponi", "Agostino", ""], ["Ghanadan", "Reza", ""], ["Stern", "Matt", ""]]}, {"id": "1705.09644", "submitter": "AmirEmad Ghassami", "authors": "AmirEmad Ghassami, Saber Salehkaleybar, Negar Kiyavash, Kun Zhang", "title": "Learning Causal Structures Using Regression Invariance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study causal inference in a multi-environment setting, in which the\nfunctional relations for producing the variables from their direct causes\nremain the same across environments, while the distribution of exogenous noises\nmay vary. We introduce the idea of using the invariance of the functional\nrelations of the variables to their causes across a set of environments. We\ndefine a notion of completeness for a causal inference algorithm in this\nsetting and prove the existence of such algorithm by proposing the baseline\nalgorithm. Additionally, we present an alternate algorithm that has\nsignificantly improved computational and sample complexity compared to the\nbaseline algorithm. The experiment results show that the proposed algorithm\noutperforms the other existing algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 16:34:13 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Ghassami", "AmirEmad", ""], ["Salehkaleybar", "Saber", ""], ["Kiyavash", "Negar", ""], ["Zhang", "Kun", ""]]}, {"id": "1705.09650", "submitter": "Qin Lin", "authors": "Xiaoran Liu and Qin Lin and Sicco Verwer and Dmitri Jarnikov", "title": "Anomaly Detection in a Digital Video Broadcasting System Using Timed\n  Automata", "comments": "This paper has been accepted by the Thirty-Second Annual ACM/IEEE\n  Symposium on Logic in Computer Science (LICS) Workshop on Learning and\n  Automata (LearnAut)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.FL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on detecting anomalies in a digital video broadcasting\n(DVB) system from providers' perspective. We learn a probabilistic\ndeterministic real timed automaton profiling benign behavior of encryption\ncontrol in the DVB control access system. This profile is used as a one-class\nclassifier. Anomalous items in a testing sequence are detected when the\nsequence is not accepted by the learned model.\n", "versions": [{"version": "v1", "created": "Wed, 24 May 2017 09:26:49 GMT"}], "update_date": "2017-05-29", "authors_parsed": [["Liu", "Xiaoran", ""], ["Lin", "Qin", ""], ["Verwer", "Sicco", ""], ["Jarnikov", "Dmitri", ""]]}, {"id": "1705.09684", "submitter": "Han Zhao", "authors": "Han Zhao, Shanghang Zhang, Guanhang Wu, Jo\\~ao P. Costeira, Jos\\'e M.\n  F. Moura, Geoffrey J. Gordon", "title": "Multiple Source Domain Adaptation with Adversarial Training of Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While domain adaptation has been actively researched in recent years, most\ntheoretical results and algorithms focus on the single-source-single-target\nadaptation setting. Naive application of such algorithms on multiple source\ndomain adaptation problem may lead to suboptimal solutions. As a step toward\nbridging the gap, we propose a new generalization bound for domain adaptation\nwhen there are multiple source domains with labeled instances and one target\ndomain with unlabeled instances. Compared with existing bounds, the new bound\ndoes not require expert knowledge about the target distribution, nor the\noptimal combination rule for multisource domains. Interestingly, our theory\nalso leads to an efficient learning strategy using adversarial neural networks:\nwe show how to interpret it as learning feature representations that are\ninvariant to the multiple domain shifts while still being discriminative for\nthe learning task. To this end, we propose two models, both of which we call\nmultisource domain adversarial networks (MDANs): the first model optimizes\ndirectly our bound, while the second model is a smoothed approximation of the\nfirst one, leading to a more data-efficient and task-adaptive model. The\noptimization tasks of both models are minimax saddle point problems that can be\noptimized by adversarial training. To demonstrate the effectiveness of MDANs,\nwe conduct extensive experiments showing superior adaptation performance on\nthree real-world datasets: sentiment analysis, digit classification, and\nvehicle counting.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 19:10:56 GMT"}, {"version": "v2", "created": "Fri, 27 Oct 2017 22:10:46 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Zhao", "Han", ""], ["Zhang", "Shanghang", ""], ["Wu", "Guanhang", ""], ["Costeira", "Jo\u00e3o P.", ""], ["Moura", "Jos\u00e9 M. F.", ""], ["Gordon", "Geoffrey J.", ""]]}, {"id": "1705.09783", "submitter": "Zihang Dai", "authors": "Zihang Dai, Zhilin Yang, Fan Yang, William W. Cohen, Ruslan\n  Salakhutdinov", "title": "Good Semi-supervised Learning that Requires a Bad GAN", "comments": "NIPS 2017 camera ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semi-supervised learning methods based on generative adversarial networks\n(GANs) obtained strong empirical results, but it is not clear 1) how the\ndiscriminator benefits from joint training with a generator, and 2) why good\nsemi-supervised classification performance and a good generator cannot be\nobtained at the same time. Theoretically, we show that given the discriminator\nobjective, good semisupervised learning indeed requires a bad generator, and\npropose the definition of a preferred generator. Empirically, we derive a novel\nformulation based on our analysis that substantially improves over feature\nmatching GANs, obtaining state-of-the-art results on multiple benchmark\ndatasets.\n", "versions": [{"version": "v1", "created": "Sat, 27 May 2017 07:53:53 GMT"}, {"version": "v2", "created": "Wed, 21 Jun 2017 07:25:43 GMT"}, {"version": "v3", "created": "Fri, 3 Nov 2017 17:18:43 GMT"}], "update_date": "2017-11-06", "authors_parsed": [["Dai", "Zihang", ""], ["Yang", "Zhilin", ""], ["Yang", "Fan", ""], ["Cohen", "William W.", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1705.09786", "submitter": "Ryota Tomioka", "authors": "Alexander L. Gaunt, Matthew A. Johnson, Maik Riechert, Daniel Tarlow,\n  Ryota Tomioka, Dimitrios Vytiniotis, Sam Webster", "title": "AMPNet: Asynchronous Model-Parallel Training for Dynamic Neural Networks", "comments": "17 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  New types of machine learning hardware in development and entering the market\nhold the promise of revolutionizing deep learning in a manner as profound as\nGPUs. However, existing software frameworks and training algorithms for deep\nlearning have yet to evolve to fully leverage the capability of the new wave of\nsilicon. We already see the limitations of existing algorithms for models that\nexploit structured input via complex and instance-dependent control flow, which\nprohibits minibatching. We present an asynchronous model-parallel (AMP)\ntraining algorithm that is specifically motivated by training on networks of\ninterconnected devices. Through an implementation on multi-core CPUs, we show\nthat AMP training converges to the same accuracy as conventional synchronous\ntraining algorithms in a similar number of epochs, but utilizes the available\nhardware more efficiently even for small minibatch sizes, resulting in\nsignificantly shorter overall training times. Our framework opens the door for\nscaling up a new class of deep learning models that cannot be efficiently\ntrained today.\n", "versions": [{"version": "v1", "created": "Sat, 27 May 2017 08:10:40 GMT"}, {"version": "v2", "created": "Sun, 11 Jun 2017 19:51:03 GMT"}, {"version": "v3", "created": "Thu, 22 Jun 2017 17:34:30 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Gaunt", "Alexander L.", ""], ["Johnson", "Matthew A.", ""], ["Riechert", "Maik", ""], ["Tarlow", "Daniel", ""], ["Tomioka", "Ryota", ""], ["Vytiniotis", "Dimitrios", ""], ["Webster", "Sam", ""]]}, {"id": "1705.09811", "submitter": "Torsten Schaub", "authors": "Martin Gebser and Roland Kaminski and Benjamin Kaufmann and Torsten\n  Schaub", "title": "Multi-shot ASP solving with clingo", "comments": "Under consideration for publication in Theory and Practice of Logic\n  Programming (TPLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new flexible paradigm of grounding and solving in Answer Set\nProgramming (ASP), which we refer to as multi-shot ASP solving, and present its\nimplementation in the ASP system clingo.\n  Multi-shot ASP solving features grounding and solving processes that deal\nwith continuously changing logic programs. In doing so, they remain operative\nand accommodate changes in a seamless way. For instance, such processes allow\nfor advanced forms of search, as in optimization or theory solving, or\ninteraction with an environment, as in robotics or query-answering. Common to\nthem is that the problem specification evolves during the reasoning process,\neither because data or constraints are added, deleted, or replaced. This\nevolutionary aspect adds another dimension to ASP since it brings about state\nchanging operations. We address this issue by providing an operational\nsemantics that characterizes grounding and solving processes in multi-shot ASP\nsolving. This characterization provides a semantic account of grounder and\nsolver states along with the operations manipulating them.\n  The operative nature of multi-shot solving avoids redundancies in relaunching\ngrounder and solver programs and benefits from the solver's learning\ncapacities. clingo accomplishes this by complementing ASP's declarative input\nlanguage with control capacities. On the declarative side, a new directive\nallows for structuring logic programs into named and parameterizable\nsubprograms. The grounding and integration of these subprograms into the\nsolving process is completely modular and fully controllable from the\nprocedural side. To this end, clingo offers a new application programming\ninterface that is conveniently accessible via scripting languages.\n", "versions": [{"version": "v1", "created": "Sat, 27 May 2017 11:52:40 GMT"}, {"version": "v2", "created": "Tue, 20 Mar 2018 16:43:53 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Gebser", "Martin", ""], ["Kaminski", "Roland", ""], ["Kaufmann", "Benjamin", ""], ["Schaub", "Torsten", ""]]}, {"id": "1705.09844", "submitter": "Mark Lewis", "authors": "Mark Lewis, Fred Glover", "title": "Quadratic Unconstrained Binary Optimization Problem Preprocessing:\n  Theory and Empirical Analysis", "comments": "Benchmark problems used are available from the first author", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Quadratic Unconstrained Binary Optimization problem (QUBO) has become a\nunifying model for representing a wide range of combinatorial optimization\nproblems, and for linking a variety of disciplines that face these problems. A\nnew class of quantum annealing computer that maps QUBO onto a physical qubit\nnetwork structure with specific size and edge density restrictions is\ngenerating a growing interest in ways to transform the underlying QUBO\nstructure into an equivalent graph having fewer nodes and edges. In this paper\nwe present rules for reducing the size of the QUBO matrix by identifying\nvariables whose value at optimality can be predetermined. We verify that the\nreductions improve both solution quality and time to solution and, in the case\nof metaheuristic methods where optimal solutions cannot be guaranteed, the\nquality of solutions obtained within reasonable time limits.\n  We discuss the general QUBO structural characteristics that can take\nadvantage of these reduction techniques and perform careful experimental design\nand analysis to identify and quantify the specific characteristics most\naffecting reduction. The rules make it possible to dramatically improve\nsolution times on a new set of problems using both the exact Cplex solver and a\ntabu search metaheuristic.\n", "versions": [{"version": "v1", "created": "Sat, 27 May 2017 17:09:56 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Lewis", "Mark", ""], ["Glover", "Fred", ""]]}, {"id": "1705.09879", "submitter": "Patrick Rodler", "authors": "Patrick Rodler and Wolfgang Schmid and Konstantin Schekotihin", "title": "Inexpensive Cost-Optimized Measurement Proposal for Sequential\n  Model-Based Diagnosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we present strategies for (optimal) measurement selection in\nmodel-based sequential diagnosis. In particular, assuming a set of leading\ndiagnoses being given, we show how queries (sets of measurements) can be\ncomputed and optimized along two dimensions: expected number of queries and\ncost per query. By means of a suitable decoupling of two optimizations and a\nclever search space reduction the computations are done without any inference\nengine calls. For the full search space, we give a method requiring only a\npolynomial number of inferences and guaranteeing query properties existing\nmethods cannot provide. Evaluation results using real-world problems indicate\nthat the new method computes (virtually) optimal queries instantly\nindependently of the size and complexity of the considered diagnosis problems.\n", "versions": [{"version": "v1", "created": "Sun, 28 May 2017 00:47:29 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Rodler", "Patrick", ""], ["Schmid", "Wolfgang", ""], ["Schekotihin", "Konstantin", ""]]}, {"id": "1705.09922", "submitter": "Ole-Christoffer Granmo", "authors": "Ole-Christoffer Granmo", "title": "Bayesian Unification of Gradient and Bandit-based Learning for\n  Accelerated Global Optimisation", "comments": "15th IEEE International Conference on Machine Learning and\n  Applications (ICMLA 2016)", "journal-ref": null, "doi": "10.1109/ICMLA.2016.0044", "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bandit based optimisation has a remarkable advantage over gradient based\napproaches due to their global perspective, which eliminates the danger of\ngetting stuck at local optima. However, for continuous optimisation problems or\nproblems with a large number of actions, bandit based approaches can be\nhindered by slow learning. Gradient based approaches, on the other hand,\nnavigate quickly in high-dimensional continuous spaces through local\noptimisation, following the gradient in fine grained steps. Yet, apart from\nbeing susceptible to local optima, these schemes are less suited for online\nlearning due to their reliance on extensive trial-and-error before the optimum\ncan be identified. In this paper, we propose a Bayesian approach that unifies\nthe above two paradigms in one single framework, with the aim of combining\ntheir advantages. At the heart of our approach we find a stochastic linear\napproximation of the function to be optimised, where both the gradient and\nvalues of the function are explicitly captured. This allows us to learn from\nboth noisy function and gradient observations, and predict these properties\nacross the action space to support optimisation. We further propose an\naccompanying bandit driven exploration scheme that uses Bayesian credible\nbounds to trade off exploration against exploitation. Our empirical results\ndemonstrate that by unifying bandit and gradient based learning, one obtains\nconsistently improved performance across a wide spectrum of problem\nenvironments. Furthermore, even when gradient feedback is unavailable, the\nflexibility of our model, including gradient prediction, still allows us\noutperform competing approaches, although with a smaller margin. Due to the\npervasiveness of bandit based optimisation, our scheme opens up for improved\nperformance both in meta-optimisation and in applications where gradient\nrelated information is readily available.\n", "versions": [{"version": "v1", "created": "Sun, 28 May 2017 09:55:11 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Granmo", "Ole-Christoffer", ""]]}, {"id": "1705.09970", "submitter": "Steven Holtzen", "authors": "Steven Holtzen and Todd Millstein and Guy Van den Broeck", "title": "Probabilistic Program Abstractions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstraction is a fundamental tool for reasoning about complex systems.\nProgram abstraction has been utilized to great effect for analyzing\ndeterministic programs. At the heart of program abstraction is the relationship\nbetween a concrete program, which is difficult to analyze, and an abstract\nprogram, which is more tractable. Program abstractions, however, are typically\nnot probabilistic. We generalize non-deterministic program abstractions to\nprobabilistic program abstractions by explicitly quantifying the\nnon-deterministic choices. Our framework upgrades key definitions and\nproperties of abstractions to the probabilistic context. We also discuss\npreliminary ideas for performing inference on probabilistic abstractions and\ngeneral probabilistic programs.\n", "versions": [{"version": "v1", "created": "Sun, 28 May 2017 17:53:01 GMT"}, {"version": "v2", "created": "Fri, 14 Jul 2017 15:46:25 GMT"}], "update_date": "2017-07-17", "authors_parsed": [["Holtzen", "Steven", ""], ["Millstein", "Todd", ""], ["Broeck", "Guy Van den", ""]]}, {"id": "1705.09990", "submitter": "Smitha Milli", "authors": "Smitha Milli, Dylan Hadfield-Menell, Anca Dragan, Stuart Russell", "title": "Should Robots be Obedient?", "comments": "Accepted to IJCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intuitively, obedience -- following the order that a human gives -- seems\nlike a good property for a robot to have. But, we humans are not perfect and we\nmay give orders that are not best aligned to our preferences. We show that when\na human is not perfectly rational then a robot that tries to infer and act\naccording to the human's underlying preferences can always perform better than\na robot that simply follows the human's literal order. Thus, there is a\ntradeoff between the obedience of a robot and the value it can attain for its\nowner. We investigate how this tradeoff is impacted by the way the robot infers\nthe human's preferences, showing that some methods err more on the side of\nobedience than others. We then analyze how performance degrades when the robot\nhas a misspecified model of the features that the human cares about or the\nlevel of rationality of the human. Finally, we study how robots can start\ndetecting such model misspecification. Overall, our work suggests that there\nmight be a middle ground in which robots intelligently decide when to obey\nhuman orders, but err on the side of obedience.\n", "versions": [{"version": "v1", "created": "Sun, 28 May 2017 20:51:19 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Milli", "Smitha", ""], ["Hadfield-Menell", "Dylan", ""], ["Dragan", "Anca", ""], ["Russell", "Stuart", ""]]}, {"id": "1705.10044", "submitter": "Ryuta Arisaka", "authors": "Ryuta Arisaka, Ken Satoh", "title": "Abstract Argumentation / Persuasion / Dynamics", "comments": "Arisaka R., Satoh K. (2018) Abstract Argumentation / Persuasion /\n  Dynamics. In: Miller T., Oren N., Sakurai Y., Noda I., Savarimuthu B., Cao\n  Son T. (eds) PRIMA 2018: Principles and Practice of Multi-Agent Systems.\n  PRIMA 2018. Lecture Notes in Computer Science, vol 11224. Springer, Cham", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The act of persuasion, a key component in rhetoric argumentation, may be\nviewed as a dynamics modifier. We extend Dung's frameworks with acts of\npersuasion among agents, and consider interactions among attack, persuasion and\ndefence that have been largely unheeded so far. We characterise basic notions\nof admissibilities in this framework, and show a way of enriching them through,\neffectively, CTL (computation tree logic) encoding, which also permits\nimportation of the theoretical results known to the logic into our\nargumentation frameworks. Our aim is to complement the growing interest in\ncoordination of static and dynamic argumentation.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 06:14:56 GMT"}, {"version": "v2", "created": "Fri, 2 Jun 2017 05:37:28 GMT"}, {"version": "v3", "created": "Wed, 7 Nov 2018 08:28:40 GMT"}], "update_date": "2018-11-08", "authors_parsed": [["Arisaka", "Ryuta", ""], ["Satoh", "Ken", ""]]}, {"id": "1705.10092", "submitter": "Mingming Li", "authors": "Mingming Li, Rui Jiang, Shuzhi Sam Ge, Tong Heng Lee", "title": "Role Playing Learning for Socially Concomitant Mobile Robot Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present the Role Playing Learning (RPL) scheme for a mobile\nrobot to navigate socially with its human companion in populated environments.\nNeural networks (NN) are constructed to parameterize a stochastic policy that\ndirectly maps sensory data collected by the robot to its velocity outputs,\nwhile respecting a set of social norms. An efficient simulative learning\nenvironment is built with maps and pedestrians trajectories collected from a\nnumber of real-world crowd data sets. In each learning iteration, a robot\nequipped with the NN policy is created virtually in the learning environment to\nplay itself as a companied pedestrian and navigate towards a goal in a socially\nconcomitant manner. Thus, we call this process Role Playing Learning, which is\nformulated under a reinforcement learning (RL) framework. The NN policy is\noptimized end-to-end using Trust Region Policy Optimization (TRPO), with\nconsideration of the imperfectness of robot's sensor measurements. Simulative\nand experimental results are provided to demonstrate the efficacy and\nsuperiority of our method.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 09:42:36 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Li", "Mingming", ""], ["Jiang", "Rui", ""], ["Ge", "Shuzhi Sam", ""], ["Lee", "Tong Heng", ""]]}, {"id": "1705.10119", "submitter": "Jiaxin Shi", "authors": "Jiaxin Shi, Shengyang Sun, Jun Zhu", "title": "Kernel Implicit Variational Inference", "comments": "Published as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in variational inference has paid much attention to the\nflexibility of variational posteriors. One promising direction is to use\nimplicit distributions, i.e., distributions without tractable densities as the\nvariational posterior. However, existing methods on implicit posteriors still\nface challenges of noisy estimation and computational infeasibility when\napplied to models with high-dimensional latent variables. In this paper, we\npresent a new approach named Kernel Implicit Variational Inference that\naddresses these challenges. As far as we know, for the first time implicit\nvariational inference is successfully applied to Bayesian neural networks,\nwhich shows promising results on both regression and classification tasks.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 11:11:35 GMT"}, {"version": "v2", "created": "Thu, 22 Feb 2018 13:49:00 GMT"}, {"version": "v3", "created": "Fri, 23 Feb 2018 15:45:59 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Shi", "Jiaxin", ""], ["Sun", "Shengyang", ""], ["Zhu", "Jun", ""]]}, {"id": "1705.10201", "submitter": "Leigh Sheneman", "authors": "Leigh Sheneman and Arend Hintze", "title": "Machine Learned Learning Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  There are two common approaches for optimizing the performance of a machine:\ngenetic algorithms and machine learning. A genetic algorithm is applied over\nmany generations whereas machine learning works by applying feedback until the\nsystem meets a performance threshold. Though these are methods that typically\noperate separately, we combine evolutionary adaptation and machine learning\ninto one approach. Our focus is on machines that can learn during their\nlifetime, but instead of equipping them with a machine learning algorithm we\naim to let them evolve their ability to learn by themselves. We use evolvable\nnetworks of probabilistic and deterministic logic gates, known as Markov\nBrains, as our computational model organism. The ability of Markov Brains to\nlearn is augmented by a novel adaptive component that can change its\ncomputational behavior based on feedback. We show that Markov Brains can indeed\nevolve to incorporate these feedback gates to improve their adaptability to\nvariable environments. By combining these two methods, we now also implemented\na computational model that can be used to study the evolution of learning.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 14:07:33 GMT"}, {"version": "v2", "created": "Thu, 31 Aug 2017 15:53:28 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Sheneman", "Leigh", ""], ["Hintze", "Arend", ""]]}, {"id": "1705.10202", "submitter": "Niek Tax", "authors": "Niek Tax, Natalia Sidorova, Reinder Haakma, Wil M.P. van der Aalst", "title": "Mining Process Model Descriptions of Daily Life through Event\n  Abstraction", "comments": "arXiv admin note: substantial text overlap with arXiv:1606.07283", "journal-ref": "Studies in Computational Intelligence, 751 (2017) 83-104", "doi": "10.1007/978-3-319-69266-1_5", "report-no": null, "categories": "cs.LG cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Process mining techniques focus on extracting insight in processes from event\nlogs. Process mining has the potential to provide valuable insights in\n(un)healthy habits and to contribute to ambient assisted living solutions when\napplied on data from smart home environments. However, events recorded in smart\nhome environments are on the level of sensor triggers, at which process\ndiscovery algorithms produce overgeneralizing process models that allow for too\nmuch behavior and that are difficult to interpret for human experts. We show\nthat abstracting the events to a higher-level interpretation can enable\ndiscovery of more precise and more comprehensible models. We present a\nframework for the extraction of features that can be used for abstraction with\nsupervised learning methods that is based on the XES IEEE standard for event\nlogs. This framework can automatically abstract sensor-level events to their\ninterpretation at the human activity level, after training it on training data\nfor which both the sensor and human activity events are known. We demonstrate\nour abstraction framework on three real-life smart home event logs and show\nthat the process models that can be discovered after abstraction are more\nprecise indeed.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 20:32:56 GMT"}], "update_date": "2018-01-11", "authors_parsed": [["Tax", "Niek", ""], ["Sidorova", "Natalia", ""], ["Haakma", "Reinder", ""], ["van der Aalst", "Wil M. P.", ""]]}, {"id": "1705.10217", "submitter": "Javier \\'Alvez", "authors": "Javier \\'Alvez and Paqui Lucio and German Rigau", "title": "Black-box Testing of First-Order Logic Ontologies Using WordNet", "comments": "59 pages,14 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Intelligence aims to provide computer programs with commonsense\nknowledge to reason about our world. This paper offers a new practical approach\ntowards automated commonsense reasoning with first-order logic (FOL)\nontologies. We propose a new black-box testing methodology of FOL SUMO-based\nontologies by exploiting WordNet and its mapping into SUMO. Our proposal\nincludes a method for the (semi-)automatic creation of a very large benchmark\nof competency questions and a procedure for its automated evaluation by using\nautomated theorem provers (ATPs). Applying different quality criteria, our\ntesting proposal enables a successful evaluation of a) the competency of\nseveral translations of SUMO into FOL and b) the performance of various\nautomated ATPs. Finally, we also provide a fine-grained and complete analysis\nof the commonsense reasoning competency of current FOL SUMO-based ontologies.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 14:41:20 GMT"}, {"version": "v2", "created": "Thu, 22 Mar 2018 13:28:14 GMT"}, {"version": "v3", "created": "Fri, 23 Mar 2018 14:43:13 GMT"}], "update_date": "2018-03-26", "authors_parsed": [["\u00c1lvez", "Javier", ""], ["Lucio", "Paqui", ""], ["Rigau", "German", ""]]}, {"id": "1705.10219", "submitter": "Javier \\'Alvez", "authors": "Javier \\'Alvez and Montserrat Hermo and Paqui Lucio and German Rigau", "title": "Automatic White-Box Testing of First-Order Logic Ontologies", "comments": "38 pages, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Formal ontologies are axiomatizations in a logic-based formalism. The\ndevelopment of formal ontologies, and their important role in the Semantic Web\narea, is generating considerable research on the use of automated reasoning\ntechniques and tools that help in ontology engineering. One of the main aims is\nto refine and to improve axiomatizations for enabling automated reasoning tools\nto efficiently infer reliable information. Defects in the axiomatization can\nnot only cause wrong inferences, but can also hinder the inference of expected\ninformation, either by increasing the computational cost of, or even\npreventing, the inference. In this paper, we introduce a novel, fully automatic\nwhite-box testing framework for first-order logic ontologies. Our methodology\nis based on the detection of inference-based redundancies in the given\naxiomatization. The application of the proposed testing method is fully\nautomatic since a) the automated generation of tests is guided only by the\nsyntax of axioms and b) the evaluation of tests is performed by automated\ntheorem provers. Our proposal enables the detection of defects and serves to\ncertify the grade of suitability --for reasoning purposes-- of every axiom. We\nformally define the set of tests that are generated from any axiom and prove\nthat every test is logically related to redundancies in the axiom from which\nthe test has been generated. We have implemented our method and used this\nimplementation to automatically detect several non-trivial defects that were\nhidden in various first-order logic ontologies. Throughout the paper we provide\nillustrative examples of these defects, explain how they were found, and how\neach proof --given by an automated theorem-prover-- provides useful hints on\nthe nature of each defect. Additionally, by correcting all the detected\ndefects, we have obtained an improved version of one of the tested ontologies:\nAdimen-SUMO.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 14:42:48 GMT"}, {"version": "v2", "created": "Tue, 26 Jun 2018 19:23:02 GMT"}, {"version": "v3", "created": "Wed, 30 Jan 2019 08:14:56 GMT"}], "update_date": "2019-01-31", "authors_parsed": [["\u00c1lvez", "Javier", ""], ["Hermo", "Montserrat", ""], ["Lucio", "Paqui", ""], ["Rigau", "German", ""]]}, {"id": "1705.10279", "submitter": "Sudeep Pillai", "authors": "Sudeep Pillai, John J. Leonard", "title": "Towards Visual Ego-motion Learning in Robots", "comments": "Conference paper; Submitted to IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS) 2017, Vancouver CA; 8 pages, 8 figures,\n  2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many model-based Visual Odometry (VO) algorithms have been proposed in the\npast decade, often restricted to the type of camera optics, or the underlying\nmotion manifold observed. We envision robots to be able to learn and perform\nthese tasks, in a minimally supervised setting, as they gain more experience.\nTo this end, we propose a fully trainable solution to visual ego-motion\nestimation for varied camera optics. We propose a visual ego-motion learning\narchitecture that maps observed optical flow vectors to an ego-motion density\nestimate via a Mixture Density Network (MDN). By modeling the architecture as a\nConditional Variational Autoencoder (C-VAE), our model is able to provide\nintrospective reasoning and prediction for ego-motion induced scene-flow.\nAdditionally, our proposed model is especially amenable to bootstrapped\nego-motion learning in robots where the supervision in ego-motion estimation\nfor a particular camera sensor can be obtained from standard navigation-based\nsensor fusion strategies (GPS/INS and wheel-odometry fusion). Through\nexperiments, we show the utility of our proposed approach in enabling the\nconcept of self-supervised learning for visual ego-motion estimation in\nautonomous robots.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 16:25:50 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["Pillai", "Sudeep", ""], ["Leonard", "John J.", ""]]}, {"id": "1705.10301", "submitter": "Maruan Al-Shedivat", "authors": "Maruan Al-Shedivat, Avinava Dubey, Eric P. Xing", "title": "Contextual Explanation Networks", "comments": "48 pages, 18 figures, to appear in JMLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern learning algorithms excel at producing accurate but complex models of\nthe data. However, deploying such models in the real-world requires extra care:\nwe must ensure their reliability, robustness, and absence of undesired biases.\nThis motivates the development of models that are equally accurate but can be\nalso easily inspected and assessed beyond their predictive performance. To this\nend, we introduce contextual explanation networks (CEN)---a class of\narchitectures that learn to predict by generating and utilizing intermediate,\nsimplified probabilistic models. Specifically, CENs generate parameters for\nintermediate graphical models which are further used for prediction and play\nthe role of explanations. Contrary to the existing post-hoc model-explanation\ntools, CENs learn to predict and to explain simultaneously. Our approach offers\ntwo major advantages: (i) for each prediction valid, instance-specific\nexplanation is generated with no computational overhead and (ii) prediction via\nexplanation acts as a regularizer and boosts performance in data-scarce\nsettings. We analyze the proposed framework theoretically and experimentally.\nOur results on image and text classification and survival analysis tasks\ndemonstrate that CENs are not only competitive with the state-of-the-art\nmethods but also offer additional insights behind each prediction, that can be\nvaluable for decision support. We also show that while post-hoc methods may\nproduce misleading explanations in certain cases, CENs are consistent and allow\nto detect such cases systematically.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 17:39:51 GMT"}, {"version": "v2", "created": "Tue, 30 Jan 2018 00:06:02 GMT"}, {"version": "v3", "created": "Tue, 18 Dec 2018 22:33:40 GMT"}, {"version": "v4", "created": "Wed, 9 Sep 2020 14:20:44 GMT"}], "update_date": "2020-09-10", "authors_parsed": [["Al-Shedivat", "Maruan", ""], ["Dubey", "Avinava", ""], ["Xing", "Eric P.", ""]]}, {"id": "1705.10308", "submitter": "Mieczys{\\l}aw K{\\l}opotek", "authors": "Mieczys{\\l}aw K{\\l}opotek", "title": "Learning Belief Network Structure From Data under Causal Insufficiency", "comments": "A short version of this paper appeared in [Klopotek:94m] M.A.\n  K{\\l}opotek: Learning Belief Network Structure From Data under Causal\n  Insufficiency. [in:] F. Bergadano, L.DeRaed Eds.: Machine Learning ECML-94 ,\n  Proc. 13th European Conference on Machine Learning, Catania, Italy, 6-8 April\n  1994, Lecture Notes in Artificial Intelligence 784, Springer-Verlag, 1994,\n  pp. 379-382", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though a belief network (a representation of the joint probability\ndistribution, see [3]) and a causal network (a representation of causal\nrelationships [14]) are intended to mean different things, they are closely\nrelated. Both assume an underlying dag (directed acyclic graph) structure of\nrelations among variables and if Markov condition and faithfulness condition\n[15] are met, then a causal network is in fact a belief network. The difference\ncomes to appearance when we recover belief network and causal network structure\nfrom data.\n  A causal network structure may be impossible to recover completely from data\nas not all directions of causal links may be uniquely determined [15].\nFortunately, if we deal with causally sufficient sets of variables (that is\nwhenever significant influence variables are not omitted from observation),\nthen there exists the possibility to identify the family of belief networks a\ncausal network belongs to [16]. Regrettably, to our knowledge, a similar result\nis not directly known for causally insufficient sets of variables. Spirtes,\nGlymour and Scheines developed a CI algorithm to handle this situation, but it\nleaves some important questions open.\n  The big open question is whether or not the bidirectional edges (that is\nindications of a common cause) are the only ones necessary to develop a belief\nnetwork out of the product of CI, or must there be some other hidden variables\nadded (e.g. by guessing). This paper is devoted to settling this question.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 17:58:13 GMT"}], "update_date": "2017-05-30", "authors_parsed": [["K\u0142opotek", "Mieczys\u0142aw", ""]]}, {"id": "1705.10342", "submitter": "Thomas Lukasiewicz", "authors": "Patrick Hohenecker and Thomas Lukasiewicz", "title": "Deep Learning for Ontology Reasoning", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a novel approach to ontology reasoning that is based\non deep learning rather than logic-based formal reasoning. To this end, we\nintroduce a new model for statistical relational learning that is built upon\ndeep recursive neural networks, and give experimental evidence that it can\neasily compete with, or even outperform, existing logic-based reasoners on the\ntask of ontology reasoning. More precisely, we compared our implemented system\nwith one of the best logic-based ontology reasoners at present, RDFox, on a\nnumber of large standard benchmark datasets, and found that our system attained\nhigh reasoning quality, while being up to two orders of magnitude faster.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 18:17:52 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Hohenecker", "Patrick", ""], ["Lukasiewicz", "Thomas", ""]]}, {"id": "1705.10422", "submitter": "Guan-Horng Liu", "authors": "Guan-Horng Liu, Avinash Siravuru, Sai Prabhakar, Manuela Veloso,\n  George Kantor", "title": "Learning End-to-end Multimodal Sensor Policies for Autonomous Navigation", "comments": "to be published in Conference on Robot Learning (CoRL), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multisensory polices are known to enhance both state estimation and target\ntracking. However, in the space of end-to-end sensorimotor control, this\nmulti-sensor outlook has received limited attention. Moreover, systematic ways\nto make policies robust to partial sensor failure are not well explored. In\nthis work, we propose a specific customization of Dropout, called\n\\textit{Sensor Dropout}, to improve multisensory policy robustness and handle\npartial failure in the sensor-set. We also introduce an additional auxiliary\nloss on the policy network in order to reduce variance in the band of potential\nmulti- and uni-sensory policies to reduce jerks during policy switching\ntriggered by an abrupt sensor failure or deactivation/activation. Finally,\nthrough the visualization of gradients, we show that the learned policies are\nconditioned on the same latent states representation despite having diverse\nobservations spaces - a hallmark of true sensor-fusion. Simulation results of\nthe multisensory policy, as visualized in TORCS racing game, can be seen here:\nhttps://youtu.be/QAK2lcXjNZc.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 00:52:24 GMT"}, {"version": "v2", "created": "Wed, 1 Nov 2017 02:30:51 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Liu", "Guan-Horng", ""], ["Siravuru", "Avinash", ""], ["Prabhakar", "Sai", ""], ["Veloso", "Manuela", ""], ["Kantor", "George", ""]]}, {"id": "1705.10432", "submitter": "Hamid Mirzaei Buini", "authors": "Hamid Mirzaei, Tony Givargis", "title": "Fine-grained acceleration control for autonomous intersection management\n  using deep reinforcement learning", "comments": "Accepted in IEEE Smart World Congress 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.RO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in combining deep learning and Reinforcement Learning have\nshown a promising path for designing new control agents that can learn optimal\npolicies for challenging control tasks. These new methods address the main\nlimitations of conventional Reinforcement Learning methods such as customized\nfeature engineering and small action/state space dimension requirements. In\nthis paper, we leverage one of the state-of-the-art Reinforcement Learning\nmethods, known as Trust Region Policy Optimization, to tackle intersection\nmanagement for autonomous vehicles. We show that using this method, we can\nperform fine-grained acceleration control of autonomous vehicles in a grid\nstreet plan to achieve a global design objective.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 02:04:29 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Mirzaei", "Hamid", ""], ["Givargis", "Tony", ""]]}, {"id": "1705.10443", "submitter": "Victor Silva", "authors": "Victor do Nascimento Silva and Luiz Chaimowicz", "title": "MOBA: a New Arena for Game AI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Games have always been popular testbeds for Artificial Intelligence (AI). In\nthe last decade, we have seen the rise of the Multiple Online Battle Arena\n(MOBA) games, which are the most played games nowadays. In spite of this, there\nare few works that explore MOBA as a testbed for AI Research. In this paper we\npresent and discuss the main features and opportunities offered by MOBA games\nto Game AI Research. We describe the various challenges faced along the game\nand also propose a discrete model that can be used to better understand and\nexplore the game. With this, we aim to encourage the use of MOBA as a novel\nresearch platform for Game AI.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 03:12:03 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Silva", "Victor do Nascimento", ""], ["Chaimowicz", "Luiz", ""]]}, {"id": "1705.10480", "submitter": "Gianluca Cima", "authors": "Gianluca Cima", "title": "Preliminary results on Ontology-based Open Data Publishing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the current interest in Open Data publishing, a formal and\ncomprehensive methodology supporting an organization in deciding which data to\npublish and carrying out precise procedures for publishing high-quality data,\nis still missing. In this paper we argue that the Ontology-based Data\nManagement paradigm can provide a formal basis for a principled approach to\npublish high quality, semantically annotated Open Data. We describe two main\napproaches to using an ontology for this endeavor, and then we present some\ntechnical results on one of the approaches, called bottom-up, where the\nspecification of the data to be published is given in terms of the sources, and\nspecific techniques allow deriving suitable annotations for interpreting the\npublished data under the light of the ontology.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 07:16:45 GMT"}, {"version": "v2", "created": "Thu, 13 Jul 2017 13:40:52 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Cima", "Gianluca", ""]]}, {"id": "1705.10557", "submitter": "John Aslanides", "authors": "John Aslanides, Jan Leike, Marcus Hutter", "title": "Universal Reinforcement Learning Algorithms: Survey and Experiments", "comments": "8 pages, 6 figures, Twenty-sixth International Joint Conference on\n  Artificial Intelligence (IJCAI-17)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many state-of-the-art reinforcement learning (RL) algorithms typically assume\nthat the environment is an ergodic Markov Decision Process (MDP). In contrast,\nthe field of universal reinforcement learning (URL) is concerned with\nalgorithms that make as few assumptions as possible about the environment. The\nuniversal Bayesian agent AIXI and a family of related URL algorithms have been\ndeveloped in this setting. While numerous theoretical optimality results have\nbeen proven for these agents, there has been no empirical investigation of\ntheir behavior to date. We present a short and accessible survey of these URL\nalgorithms under a unified notation and framework, along with results of some\nexperiments that qualitatively illustrate some properties of the resulting\npolicies, and their relative performance on partially-observable gridworld\nenvironments. We also present an open-source reference implementation of the\nalgorithms which we hope will facilitate further understanding of, and\nexperimentation with, these ideas.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 11:41:00 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Aslanides", "John", ""], ["Leike", "Jan", ""], ["Hutter", "Marcus", ""]]}, {"id": "1705.10694", "submitter": "Andreas Veit", "authors": "David Rolnick, Andreas Veit, Serge Belongie, Nir Shavit", "title": "Deep Learning is Robust to Massive Label Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks trained on large supervised datasets have led to\nimpressive results in image classification and other tasks. However,\nwell-annotated datasets can be time-consuming and expensive to collect, lending\nincreased interest to larger but noisy datasets that are more easily obtained.\nIn this paper, we show that deep neural networks are capable of generalizing\nfrom training data for which true labels are massively outnumbered by incorrect\nlabels. We demonstrate remarkably high test performance after training on\ncorrupted data from MNIST, CIFAR, and ImageNet. For example, on MNIST we obtain\ntest accuracy above 90 percent even after each clean training example has been\ndiluted with 100 randomly-labeled examples. Such behavior holds across multiple\npatterns of label noise, even when erroneous labels are biased towards\nconfusing classes. We show that training in this regime requires a significant\nbut manageable increase in dataset size that is related to the factor by which\ncorrect labels have been diluted. Finally, we provide an analysis of our\nresults that shows how increasing noise decreases the effective batch size.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 15:10:51 GMT"}, {"version": "v2", "created": "Wed, 31 May 2017 02:02:56 GMT"}, {"version": "v3", "created": "Mon, 26 Feb 2018 16:51:57 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Rolnick", "David", ""], ["Veit", "Andreas", ""], ["Belongie", "Serge", ""], ["Shavit", "Nir", ""]]}, {"id": "1705.10701", "submitter": "I-Chen Wu", "authors": "Ti-Rong Wu, I-Chen Wu, Guan-Wun Chen, Ting-han Wei, Tung-Yi Lai,\n  Hung-Chun Wu and Li-Cheng Lan", "title": "Multi-Labelled Value Networks for Computer Go", "comments": "This version was also submitted to IEEE TCIAIG on May 30, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new approach to a novel value network architecture for\nthe game Go, called a multi-labelled (ML) value network. In the ML value\nnetwork, different values (win rates) are trained simultaneously for different\nsettings of komi, a compensation given to balance the initiative of playing\nfirst. The ML value network has three advantages, (a) it outputs values for\ndifferent komi, (b) it supports dynamic komi, and (c) it lowers the mean\nsquared error (MSE). This paper also proposes a new dynamic komi method to\nimprove game-playing strength. This paper also performs experiments to\ndemonstrate the merits of the architecture. First, the MSE of the ML value\nnetwork is generally lower than the value network alone. Second, the program\nbased on the ML value network wins by a rate of 67.6% against the program based\non the value network alone. Third, the program with the proposed dynamic komi\nmethod significantly improves the playing strength over the baseline that does\nnot use dynamic komi, especially for handicap games. To our knowledge, up to\ndate, no handicap games have been played openly by programs using value\nnetworks. This paper provides these programs with a useful approach to playing\nhandicap games.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 15:23:32 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Wu", "Ti-Rong", ""], ["Wu", "I-Chen", ""], ["Chen", "Guan-Wun", ""], ["Wei", "Ting-han", ""], ["Lai", "Tung-Yi", ""], ["Wu", "Hung-Chun", ""], ["Lan", "Li-Cheng", ""]]}, {"id": "1705.10720", "submitter": "Stuart Armstrong", "authors": "Stuart Armstrong and Benjamin Levinstein", "title": "Low Impact Artificial Intelligences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many goals for an AI that could become dangerous if the AI becomes\nsuperintelligent or otherwise powerful. Much work on the AI control problem has\nbeen focused on constructing AI goals that are safe even for such AIs. This\npaper looks at an alternative approach: defining a general concept of `low\nimpact'. The aim is to ensure that a powerful AI which implements low impact\nwill not modify the world extensively, even if it is given a simple or\ndangerous goal. The paper proposes various ways of defining and grounding low\nimpact, and discusses methods for ensuring that the AI can still be allowed to\nhave a (desired) impact despite the restriction. The end of the paper addresses\nknown issues with this approach and avenues for future research.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 16:15:16 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Armstrong", "Stuart", ""], ["Levinstein", "Benjamin", ""]]}, {"id": "1705.10726", "submitter": "Naveen Sundar Govindarajulu", "authors": "Naveen Sundar Govindarajulu, Selmer Bringsjord", "title": "Strength Factors: An Uncertainty System for a Quantified Modal Logic", "comments": "Presented on August 20, 2017 at the Logical Foundations for\n  Uncertainty and Machine Learning Workshop @ IJCAI 2017 in Melbourne,\n  Australia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new system S for handling uncertainty in a quantified modal\nlogic (first-order modal logic). The system is based on both probability theory\nand proof theory. The system is derived from Chisholm's epistemology. We\nconcretize Chisholm's system by grounding his undefined and primitive (i.e.\nfoundational) concept of reasonablenes in probability and proof theory. S can\nbe useful in systems that have to interact with humans and provide\njustifications for their uncertainty. As a demonstration of the system, we\napply the system to provide a solution to the lottery paradox. Another\nadvantage of the system is that it can be used to provide uncertainty values\nfor counterfactual statements. Counterfactuals are statements that an agent\nknows for sure are false. Among other cases, counterfactuals are useful when\nsystems have to explain their actions to users. Uncertainties for\ncounterfactuals fall out naturally from our system.\n  Efficient reasoning in just simple first-order logic is a hard problem.\nResolution-based first-order reasoning systems have made significant progress\nover the last several decades in building systems that have solved non-trivial\ntasks (even unsolved conjectures in mathematics). We present a sketch of a\nnovel algorithm for reasoning that extends first-order resolution.\n  Finally, while there have been many systems of uncertainty for propositional\nlogics, first-order logics and propositional modal logics, there has been very\nlittle work in building systems of uncertainty for first-order modal logics.\nThe work described below is in progress; and once finished will address this\nlack.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 16:24:18 GMT"}, {"version": "v2", "created": "Mon, 28 May 2018 06:07:18 GMT"}], "update_date": "2018-05-29", "authors_parsed": [["Govindarajulu", "Naveen Sundar", ""], ["Bringsjord", "Selmer", ""]]}, {"id": "1705.10742", "submitter": "Martin Jaggi", "authors": "Tina Fang, Martin Jaggi, Katerina Argyraki", "title": "Generating Steganographic Text with LSTMs", "comments": "ACL 2017 Student Research Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CR cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motivated by concerns for user privacy, we design a steganographic system\n(\"stegosystem\") that enables two users to exchange encrypted messages without\nan adversary detecting that such an exchange is taking place. We propose a new\nlinguistic stegosystem based on a Long Short-Term Memory (LSTM) neural network.\nWe demonstrate our approach on the Twitter and Enron email datasets and show\nthat it yields high-quality steganographic text while significantly improving\ncapacity (encrypted bits per word) relative to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 16:52:48 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Fang", "Tina", ""], ["Jaggi", "Martin", ""], ["Argyraki", "Katerina", ""]]}, {"id": "1705.10744", "submitter": "Ondrej Bajgar", "authors": "Rudolf Kadlec, Ondrej Bajgar and Jan Kleindienst", "title": "Knowledge Base Completion: Baselines Strike Back", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many papers have been published on the knowledge base completion task in the\npast few years. Most of these introduce novel architectures for relation\nlearning that are evaluated on standard datasets such as FB15k and WN18. This\npaper shows that the accuracy of almost all models published on the FB15k can\nbe outperformed by an appropriately tuned baseline - our reimplementation of\nthe DistMult model. Our findings cast doubt on the claim that the performance\nimprovements of recent models are due to architectural changes as opposed to\nhyper-parameter tuning or different training objectives. This should prompt\nfuture research to re-consider how the performance of models is evaluated and\nreported.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 16:54:19 GMT"}], "update_date": "2017-05-31", "authors_parsed": [["Kadlec", "Rudolf", ""], ["Bajgar", "Ondrej", ""], ["Kleindienst", "Jan", ""]]}, {"id": "1705.10786", "submitter": "Hamidreza Alvari", "authors": "Hamidreza Alvari, Paulo Shakarian, J.E. Kelly Snyder", "title": "Semi-Supervised Learning for Detecting Human Trafficking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human trafficking is one of the most atrocious crimes and among the\nchallenging problems facing law enforcement which demands attention of global\nmagnitude. In this study, we leverage textual data from the website \"Backpage\"-\nused for classified advertisement- to discern potential patterns of human\ntrafficking activities which manifest online and identify advertisements of\nhigh interest to law enforcement. Due to the lack of ground truth, we rely on a\nhuman analyst from law enforcement, for hand-labeling a small portion of the\ncrawled data. We extend the existing Laplacian SVM and present S3VM-R, by\nadding a regularization term to exploit exogenous information embedded in our\nfeature space in favor of the task at hand. We train the proposed method using\nlabeled and unlabeled data and evaluate it on a fraction of the unlabeled data,\nherein referred to as unseen data, with our expert's further verification.\nResults from comparisons between our method and other semi-supervised and\nsupervised approaches on the labeled data demonstrate that our learner is\neffective in identifying advertisements of high interest to law enforcement\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 05:51:53 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Alvari", "Hamidreza", ""], ["Shakarian", "Paulo", ""], ["Snyder", "J. E. Kelly", ""]]}, {"id": "1705.10834", "submitter": "Thommen George Karimpanal", "authors": "Thommen George Karimpanal, Roland Bouffanais", "title": "Experience Replay Using Transition Sequences", "comments": "23 pages, 6 figures", "journal-ref": "Frontiers in Neurorobotics, 2018", "doi": "10.3389/fnbot.2018.00032", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experience replay is one of the most commonly used approaches to improve the\nsample efficiency of reinforcement learning algorithms. In this work, we\npropose an approach to select and replay sequences of transitions in order to\naccelerate the learning of a reinforcement learning agent in an off-policy\nsetting. In addition to selecting appropriate sequences, we also artificially\nconstruct transition sequences using information gathered from previous\nagent-environment interactions. These sequences, when replayed, allow value\nfunction information to trickle down to larger sections of the\nstate/state-action space, thereby making the most of the agent's experience. We\ndemonstrate our approach on modified versions of standard reinforcement\nlearning tasks such as the mountain car and puddle world problems and\nempirically show that it enables better learning of value functions as compared\nto other forms of experience replay. Further, we briefly discuss some of the\npossible extensions to this work, as well as applications and situations where\nthis approach could be particularly useful.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 19:24:09 GMT"}, {"version": "v2", "created": "Fri, 13 Sep 2019 01:13:39 GMT"}], "update_date": "2019-09-16", "authors_parsed": [["Karimpanal", "Thommen George", ""], ["Bouffanais", "Roland", ""]]}, {"id": "1705.10868", "submitter": "Hang Ma", "authors": "Hang Ma, Jiaoyang Li, T. K. Satish Kumar, Sven Koenig", "title": "Lifelong Multi-Agent Path Finding for Online Pickup and Delivery Tasks", "comments": "In AAMAS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multi-agent path-finding (MAPF) problem has recently received a lot of\nattention. However, it does not capture important characteristics of many\nreal-world domains, such as automated warehouses, where agents are constantly\nengaged with new tasks. In this paper, we therefore study a lifelong version of\nthe MAPF problem, called the multi-agent pickup and delivery (MAPD) problem. In\nthe MAPD problem, agents have to attend to a stream of delivery tasks in an\nonline setting. One agent has to be assigned to each delivery task. This agent\nhas to first move to a given pickup location and then to a given delivery\nlocation while avoiding collisions with other agents. We present two decoupled\nMAPD algorithms, Token Passing (TP) and Token Passing with Task Swaps (TPTS).\nTheoretically, we show that they solve all well-formed MAPD instances, a\nrealistic subclass of MAPD instances. Experimentally, we compare them against a\ncentralized strawman MAPD algorithm without this guarantee in a simulated\nwarehouse system. TP can easily be extended to a fully distributed MAPD\nalgorithm and is the best choice when real-time computation is of primary\nconcern since it remains efficient for MAPD instances with hundreds of agents\nand tasks. TPTS requires limited communication among agents and balances well\nbetween TP and the centralized MAPD algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 21:08:16 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Ma", "Hang", ""], ["Li", "Jiaoyang", ""], ["Kumar", "T. K. Satish", ""], ["Koenig", "Sven", ""]]}, {"id": "1705.10882", "submitter": "David Rolnick", "authors": "David Rolnick, Yaron Meirovitch, Toufiq Parag, Hanspeter Pfister,\n  Viren Jain, Jeff W. Lichtman, Edward S. Boyden, Nir Shavit", "title": "Morphological Error Detection in 3D Segmentations", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning algorithms for connectomics rely upon localized classification,\nrather than overall morphology. This leads to a high incidence of erroneously\nmerged objects. Humans, by contrast, can easily detect such errors by acquiring\nintuition for the correct morphology of objects. Biological neurons have\ncomplicated and variable shapes, which are challenging to learn, and merge\nerrors take a multitude of different forms. We present an algorithm, MergeNet,\nthat shows 3D ConvNets can, in fact, detect merge errors from high-level\nneuronal morphology. MergeNet follows unsupervised training and operates across\ndatasets. We demonstrate the performance of MergeNet both on a variety of\nconnectomics data and on a dataset created from merged MNIST images.\n", "versions": [{"version": "v1", "created": "Tue, 30 May 2017 22:25:44 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Rolnick", "David", ""], ["Meirovitch", "Yaron", ""], ["Parag", "Toufiq", ""], ["Pfister", "Hanspeter", ""], ["Jain", "Viren", ""], ["Lichtman", "Jeff W.", ""], ["Boyden", "Edward S.", ""], ["Shavit", "Nir", ""]]}, {"id": "1705.10898", "submitter": "Jerry Lonlac", "authors": "Jerry Lonlac and Engelbert Mephu Nguifo", "title": "Towards Learned Clauses Database Reduction Strategies Based on Dominance\n  Relationship", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clause Learning is one of the most important components of a conflict driven\nclause learning (CDCL) SAT solver that is effective on industrial instances.\nSince the number of learned clauses is proved to be exponential in the worse\ncase, it is necessary to identify the most relevant clauses to maintain and\ndelete the irrelevant ones. As reported in the literature, several learned\nclauses deletion strategies have been proposed. However the diversity in both\nthe number of clauses to be removed at each step of reduction and the results\nobtained with each strategy creates confusion to determine which criterion is\nbetter. Thus, the problem to select which learned clauses are to be removed\nduring the search step remains very challenging. In this paper, we propose a\nnovel approach to identify the most relevant learned clauses without favoring\nor excluding any of the proposed measures, but by adopting the notion of\ndominance relationship among those measures. Our approach bypasses the problem\nof the diversity of results and reaches a compromise between the assessments of\nthese measures. Furthermore, the proposed approach also avoids another\nnon-trivial problem which is the amount of clauses to be deleted at each\nreduction of the learned clause database.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 00:05:26 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Lonlac", "Jerry", ""], ["Nguifo", "Engelbert Mephu", ""]]}, {"id": "1705.10899", "submitter": "Son Tran", "authors": "Son N. Tran", "title": "Propositional Knowledge Representation and Reasoning in Restricted\n  Boltzmann Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  While knowledge representation and reasoning are considered the keys for\nhuman-level artificial intelligence, connectionist networks have been shown\nsuccessful in a broad range of applications due to their capacity for robust\nlearning and flexible inference under uncertainty. The idea of representing\nsymbolic knowledge in connectionist networks has been well-received and\nattracted much attention from research community as this can establish a\nfoundation for integration of scalable learning and sound reasoning. In\nprevious work, there exist a number of approaches that map logical inference\nrules with feed-forward propagation of artificial neural networks (ANN).\nHowever, the discriminative structure of an ANN requires the separation of\ninput/output variables which makes it difficult for general reasoning where any\nvariables should be inferable. Other approaches address this issue by employing\ngenerative models such as symmetric connectionist networks, however, they are\ndifficult and convoluted. In this paper we propose a novel method to represent\npropositional formulas in restricted Boltzmann machines which is less complex,\nespecially in the cases of logical implications and Horn clauses. An\nintegration system is then developed and evaluated in real datasets which shows\npromising results.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 00:24:16 GMT"}, {"version": "v2", "created": "Thu, 1 Jun 2017 00:19:24 GMT"}, {"version": "v3", "created": "Tue, 29 May 2018 04:44:31 GMT"}], "update_date": "2018-05-30", "authors_parsed": [["Tran", "Son N.", ""]]}, {"id": "1705.10915", "submitter": "Emily Denton", "authors": "Emily Denton, Vighnesh Birodkar", "title": "Unsupervised Learning of Disentangled Representations from Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new model DrNET that learns disentangled image representations\nfrom video. Our approach leverages the temporal coherence of video and a novel\nadversarial loss to learn a representation that factorizes each frame into a\nstationary part and a temporally varying component. The disentangled\nrepresentation can be used for a range of tasks. For example, applying a\nstandard LSTM to the time-vary components enables prediction of future frames.\nWe evaluate our approach on a range of synthetic and real videos, demonstrating\nthe ability to coherently generate hundreds of steps into the future.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 02:12:19 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Denton", "Emily", ""], ["Birodkar", "Vighnesh", ""]]}, {"id": "1705.10929", "submitter": "Sandeep Subramanian", "authors": "Sai Rajeswar, Sandeep Subramanian, Francis Dutil, Christopher Pal,\n  Aaron Courville", "title": "Adversarial Generation of Natural Language", "comments": "11 pages, 3 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative Adversarial Networks (GANs) have gathered a lot of attention from\nthe computer vision community, yielding impressive results for image\ngeneration. Advances in the adversarial generation of natural language from\nnoise however are not commensurate with the progress made in generating images,\nand still lag far behind likelihood based methods. In this paper, we take a\nstep towards generating natural language with a GAN objective alone. We\nintroduce a simple baseline that addresses the discrete output space problem\nwithout relying on gradient estimators and show that it is able to achieve\nstate-of-the-art results on a Chinese poem generation dataset. We present\nquantitative results on generating sentences from context-free and\nprobabilistic context-free grammars, and qualitative language modeling results.\nA conditional version is also described that can generate sequences conditioned\non sentence characteristics.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 03:06:39 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Rajeswar", "Sai", ""], ["Subramanian", "Sandeep", ""], ["Dutil", "Francis", ""], ["Pal", "Christopher", ""], ["Courville", "Aaron", ""]]}, {"id": "1705.10993", "submitter": "Julien Perez", "authors": "Julien Perez and Tomi Silander", "title": "Non-Markovian Control with Gated End-to-End Memory Policy Networks", "comments": "11 pages, 1 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partially observable environments present an important open challenge in the\ndomain of sequential control learning with delayed rewards. Despite numerous\nattempts during the two last decades, the majority of reinforcement learning\nalgorithms and associated approximate models, applied to this context, still\nassume Markovian state transitions. In this paper, we explore the use of a\nrecently proposed attention-based model, the Gated End-to-End Memory Network,\nfor sequential control. We call the resulting model the Gated End-to-End Memory\nPolicy Network. More precisely, we use a model-free value-based algorithm to\nlearn policies for partially observed domains using this memory-enhanced neural\nnetwork. This model is end-to-end learnable and it features unbounded memory.\nIndeed, because of its attention mechanism and associated non-parametric\nmemory, the proposed model allows us to define an attention mechanism over the\nobservation stream unlike recurrent models. We show encouraging results that\nillustrate the capability of our attention-based model in the context of the\ncontinuous-state non-stationary control problem of stock trading. We also\npresent an OpenAI Gym environment for simulated stock exchange and explain its\nrelevance as a benchmark for the field of non-Markovian decision process\nlearning.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 09:00:44 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Perez", "Julien", ""], ["Silander", "Tomi", ""]]}, {"id": "1705.10998", "submitter": "Vitaly Kurin", "authors": "Vitaly Kurin, Sebastian Nowozin, Katja Hofmann, Lucas Beyer, Bastian\n  Leibe", "title": "The Atari Grand Challenge Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in Reinforcement Learning (RL), fueled by its combination,\nwith Deep Learning has enabled impressive results in learning to interact with\ncomplex virtual environments, yet real-world applications of RL are still\nscarce. A key limitation is data efficiency, with current state-of-the-art\napproaches requiring millions of training samples. A promising way to tackle\nthis problem is to augment RL with learning from human demonstrations. However,\nhuman demonstration data is not yet readily available. This hinders progress in\nthis direction. The present work addresses this problem as follows. We (i)\ncollect and describe a large dataset of human Atari 2600 replays -- the largest\nand most diverse such data set publicly released to date, (ii) illustrate an\nexample use of this dataset by analyzing the relation between demonstration\nquality and imitation learning performance, and (iii) outline possible research\ndirections that are opened up by our work.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 09:08:36 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Kurin", "Vitaly", ""], ["Nowozin", "Sebastian", ""], ["Hofmann", "Katja", ""], ["Beyer", "Lucas", ""], ["Leibe", "Bastian", ""]]}, {"id": "1705.11040", "submitter": "Tim Rockt\\\"aschel", "authors": "Tim Rockt\\\"aschel and Sebastian Riedel", "title": "End-to-End Differentiable Proving", "comments": "NIPS 2017 camera-ready, NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce neural networks for end-to-end differentiable proving of queries\nto knowledge bases by operating on dense vector representations of symbols.\nThese neural networks are constructed recursively by taking inspiration from\nthe backward chaining algorithm as used in Prolog. Specifically, we replace\nsymbolic unification with a differentiable computation on vector\nrepresentations of symbols using a radial basis function kernel, thereby\ncombining symbolic reasoning with learning subsymbolic vector representations.\nBy using gradient descent, the resulting neural network can be trained to infer\nfacts from a given incomplete knowledge base. It learns to (i) place\nrepresentations of similar symbols in close proximity in a vector space, (ii)\nmake use of such similarities to prove queries, (iii) induce logical rules, and\n(iv) use provided and induced logical rules for multi-hop reasoning. We\ndemonstrate that this architecture outperforms ComplEx, a state-of-the-art\nneural link prediction model, on three out of four benchmark knowledge bases\nwhile at the same time inducing interpretable function-free first-order logic\nrules.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 11:40:57 GMT"}, {"version": "v2", "created": "Mon, 4 Dec 2017 00:24:04 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Rockt\u00e4schel", "Tim", ""], ["Riedel", "Sebastian", ""]]}, {"id": "1705.11122", "submitter": "Qizhe Xie", "authors": "Qizhe Xie, Zihang Dai, Yulun Du, Eduard Hovy, Graham Neubig", "title": "Controllable Invariance through Adversarial Feature Learning", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning meaningful representations that maintain the content necessary for a\nparticular task while filtering away detrimental variations is a problem of\ngreat interest in machine learning. In this paper, we tackle the problem of\nlearning representations invariant to a specific factor or trait of data. The\nrepresentation learning process is formulated as an adversarial minimax game.\nWe analyze the optimal equilibrium of such a game and find that it amounts to\nmaximizing the uncertainty of inferring the detrimental factor given the\nrepresentation while maximizing the certainty of making task-specific\npredictions. On three benchmark tasks, namely fair and bias-free\nclassification, language-independent generation, and lighting-independent image\nclassification, we show that the proposed framework induces an invariant\nrepresentation, and leads to better generalization evidenced by the improved\nperformance.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 14:57:33 GMT"}, {"version": "v2", "created": "Mon, 6 Nov 2017 17:47:33 GMT"}, {"version": "v3", "created": "Mon, 29 Jan 2018 00:59:58 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Xie", "Qizhe", ""], ["Dai", "Zihang", ""], ["Du", "Yulun", ""], ["Hovy", "Eduard", ""], ["Neubig", "Graham", ""]]}, {"id": "1705.11190", "submitter": "Xerxes D. Arsiwalla", "authors": "Xerxes D. Arsiwalla, Ricard Sole, Clement Moulin-Frier, Ivan Herreros,\n  Marti Sanchez-Fibla, Paul Verschure", "title": "The Morphospace of Consciousness", "comments": "23 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cond-mat.dis-nn cs.AI physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We construct a complexity-based morphospace to study systems-level properties\nof conscious & intelligent systems. The axes of this space label 3 complexity\ntypes: autonomous, cognitive & social. Given recent proposals to synthesize\nconsciousness, a generic complexity-based conceptualization provides a useful\nframework for identifying defining features of conscious & synthetic systems.\nBased on current clinical scales of consciousness that measure cognitive\nawareness and wakefulness, we take a perspective on how contemporary\nartificially intelligent machines & synthetically engineered life forms measure\non these scales. It turns out that awareness & wakefulness can be associated to\ncomputational & autonomous complexity respectively. Subsequently, building on\ninsights from cognitive robotics, we examine the function that consciousness\nserves, & argue the role of consciousness as an evolutionary game-theoretic\nstrategy. This makes the case for a third type of complexity for describing\nconsciousness: social complexity. Having identified these complexity types,\nallows for a representation of both, biological & synthetic systems in a common\nmorphospace. A consequence of this classification is a taxonomy of possible\nconscious machines. We identify four types of consciousness, based on\nembodiment: (i) biological consciousness, (ii) synthetic consciousness, (iii)\ngroup consciousness (resulting from group interactions), & (iv) simulated\nconsciousness (embodied by virtual agents within a simulated reality). This\ntaxonomy helps in the investigation of comparative signatures of consciousness\nacross domains, in order to highlight design principles necessary to engineer\nconscious machines. This is particularly relevant in the light of recent\ndevelopments at the crossroads of cognitive neuroscience, biomedical\nengineering, artificial intelligence & biomimetics.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 17:45:39 GMT"}, {"version": "v2", "created": "Thu, 8 Jun 2017 17:42:04 GMT"}, {"version": "v3", "created": "Sat, 24 Nov 2018 23:05:40 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Arsiwalla", "Xerxes D.", ""], ["Sole", "Ricard", ""], ["Moulin-Frier", "Clement", ""], ["Herreros", "Ivan", ""], ["Sanchez-Fibla", "Marti", ""], ["Verschure", "Paul", ""]]}]