[{"id": "2104.00008", "submitter": "Dan Roberts", "authors": "Daniel A. Roberts", "title": "Why is AI hard and Physics simple?", "comments": "written for a special issue of Machine Learning: Science and\n  Technology as an invited perspective piece", "journal-ref": null, "doi": null, "report-no": "MIT-CTP/5269", "categories": "hep-th cs.AI cs.LG physics.hist-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss why AI is hard and why physics is simple. We discuss how physical\nintuition and the approach of theoretical physics can be brought to bear on the\nfield of artificial intelligence and specifically machine learning. We suggest\nthat the underlying project of machine learning and the underlying project of\nphysics are strongly coupled through the principle of sparsity, and we call\nupon theoretical physicists to work on AI as physicists. As a first step in\nthat direction, we discuss an upcoming book on the principles of deep learning\ntheory that attempts to realize this approach.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 18:00:01 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Roberts", "Daniel A.", ""]]}, {"id": "2104.00053", "submitter": "Ryan Hoque", "authors": "Ryan Hoque, Ashwin Balakrishna, Carl Putterman, Michael Luo, Daniel S.\n  Brown, Daniel Seita, Brijen Thananjeyan, Ellen Novoseller, Ken Goldberg", "title": "LazyDAgger: Reducing Context Switching in Interactive Imitation Learning", "comments": "IEEE CASE 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Corrective interventions while a robot is learning to automate a task provide\nan intuitive method for a human supervisor to assist the robot and convey\ninformation about desired behavior. However, these interventions can impose\nsignificant burden on a human supervisor, as each intervention interrupts other\nwork the human is doing, incurs latency with each context switch between\nsupervisor and autonomous control, and requires time to perform. We present\nLazyDAgger, which extends the interactive imitation learning (IL) algorithm\nSafeDAgger to reduce context switches between supervisor and autonomous\ncontrol. We find that LazyDAgger improves the performance and robustness of the\nlearned policy during both learning and execution while limiting burden on the\nsupervisor. Simulation experiments suggest that LazyDAgger can reduce context\nswitches by an average of 60% over SafeDAgger on 3 continuous control tasks\nwhile maintaining state-of-the-art policy performance. In physical fabric\nmanipulation experiments with an ABB YuMi robot, LazyDAgger reduces context\nswitches by 60% while achieving a 60% higher success rate than SafeDAgger at\nexecution time.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 18:22:53 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 21:47:14 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Hoque", "Ryan", ""], ["Balakrishna", "Ashwin", ""], ["Putterman", "Carl", ""], ["Luo", "Michael", ""], ["Brown", "Daniel S.", ""], ["Seita", "Daniel", ""], ["Thananjeyan", "Brijen", ""], ["Novoseller", "Ellen", ""], ["Goldberg", "Ken", ""]]}, {"id": "2104.00060", "submitter": "Jingkai Chen", "authors": "Jingkai Chen, Yuening Zhang, Cheng Fang, Brian C. Williams", "title": "Generalized Conflict-directed Search for Optimal Ordering Problems", "comments": "Accepted at SOCS2021. 9 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving planning and scheduling problems for multiple tasks with highly\ncoupled state and temporal constraints is notoriously challenging. An appealing\napproach to effectively decouple the problem is to judiciously order the events\nsuch that decisions can be made over sequences of tasks. As many problems\nencountered in practice are over-constrained, we must instead find relaxed\nsolutions in which certain requirements are dropped. This motivates a\nformulation of optimality with respect to the costs of relaxing constraints and\nthe problem of finding an optimal ordering under which this relaxing cost is\nminimum. In this paper, we present Generalized Conflict-directed Ordering\n(GCDO), a branch-and-bound ordering method that generates an optimal total\norder of events by leveraging the generalized conflicts of both inconsistency\nand suboptimality from sub-solvers for cost estimation and solution space\npruning. Due to its ability to reason over generalized conflicts, GCDO is much\nmore efficient in finding high-quality total orders than the previous\nconflict-directed approach CDITO. We demonstrate this by benchmarking on\ntemporal network configuration problems, which involves managing networks over\ntime and makes necessary tradeoffs between network flows against CDITO and\nMixed Integer-Linear Programing (MILP). Our algorithm is able to solve two\norders of magnitude more benchmark problems to optimality and twice the\nproblems compared to CDITO and MILP within a runtime limit, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 18:46:48 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Chen", "Jingkai", ""], ["Zhang", "Yuening", ""], ["Fang", "Cheng", ""], ["Williams", "Brian C.", ""]]}, {"id": "2104.00093", "submitter": "Lea A. Shanley", "authors": "Lea A. Shanley, Lucy Fortson, Tanya Berger-Wolf, Kevin Crowston, and\n  Pietro Michelucci", "title": "Imagine All the People: Citizen Science, Artificial Intelligence, and\n  Computational Research", "comments": "A Computing Community Consortium (CCC) white paper, 6 pages", "journal-ref": null, "doi": null, "report-no": "ccc2021whitepaper_2", "categories": "cs.CY cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning, artificial intelligence, and deep learning have advanced\nsignificantly over the past decade. Nonetheless, humans possess unique\nabilities such as creativity, intuition, context and abstraction, analytic\nproblem solving, and detecting unusual events. To successfully tackle pressing\nscientific and societal challenges, we need the complementary capabilities of\nboth humans and machines. The Federal Government could accelerate its\npriorities on multiple fronts through judicious integration of citizen science\nand crowdsourcing with artificial intelligence (AI), Internet of Things (IoT),\nand cloud strategies.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 20:21:13 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 15:30:34 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Shanley", "Lea A.", ""], ["Fortson", "Lucy", ""], ["Berger-Wolf", "Tanya", ""], ["Crowston", "Kevin", ""], ["Michelucci", "Pietro", ""]]}, {"id": "2104.00096", "submitter": "Katie Siek", "authors": "Christina N. Harrington, Ben Jelen, Amanda Lazar, Aqueasha\n  Martin-Hammond, Alisha Pradhan, Blaine Reeder, and Katie Siek", "title": "Taking Stock of the Present and Future of Smart Technologies for Older\n  Adults and Caregivers", "comments": "A Computing Community Consortium (CCC) white paper, 6 pages", "journal-ref": null, "doi": null, "report-no": "ccc2021whitepaper_3", "categories": "cs.CY cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Technology has the opportunity to assist older adults as they age in place,\ncoordinate caregiving resources, and meet unmet needs through access to\nresources. Currently, older adults use consumer technologies to support\neveryday life, however these technologies are not always accessible or as\nuseful as they can be. Indeed, industry has attempted to create smart home\ntechnologies with older adults as a target user group, however these solutions\nare often more focused on the technical aspects and are short lived. In this\npaper, we advocate for older adults being involved in the design process - from\ninitial ideation to product development to deployment. We encourage federally\nfunded researchers and industry to create compensated, diverse older adult\nadvisory boards to address stereotypes about aging while ensuring their needs\nare considered.\n  We envision artificial intelligence systems that augment resources instead of\nreplacing them - especially in under-resourced communities. Older adults rely\non their caregiver networks and community organizations for social, emotional,\nand physical support; thus, AI should be used to coordinate resources better\nand lower the burden of connecting with these resources. Although\nsociotechnical smart systems can help identify needs of older adults, the lack\nof affordable research infrastructure and translation of findings into consumer\ntechnology perpetuates inequities in designing for diverse older adults. In\naddition, there is a disconnect between the creation of smart sensing systems\nand creating understandable, actionable data for older adults and caregivers to\nutilize. We ultimately advocate for a well-coordinated research effort across\nthe United States that connects older adults, caregivers, community\norganizations, and researchers together to catalyze innovative and practical\nresearch for all stakeholders.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 20:28:38 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Harrington", "Christina N.", ""], ["Jelen", "Ben", ""], ["Lazar", "Amanda", ""], ["Martin-Hammond", "Aqueasha", ""], ["Pradhan", "Alisha", ""], ["Reeder", "Blaine", ""], ["Siek", "Katie", ""]]}, {"id": "2104.00099", "submitter": "Hudson Bruno", "authors": "Hudson M. S. Bruno and Esther L. Colombini", "title": "LIFT-SLAM: a deep-learning feature-based monocular visual SLAM method", "comments": "30 pages, Published in Neurocomputing", "journal-ref": null, "doi": "10.1016/j.neucom.2021.05.027", "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Simultaneous Localization and Mapping (SLAM) problem addresses the\npossibility of a robot to localize itself in an unknown environment and\nsimultaneously build a consistent map of this environment. Recently, cameras\nhave been successfully used to get the environment's features to perform SLAM,\nwhich is referred to as visual SLAM (VSLAM). However, classical VSLAM\nalgorithms can be easily induced to fail when either the motion of the robot or\nthe environment is too challenging. Although new approaches based on Deep\nNeural Networks (DNNs) have achieved promising results in VSLAM, they still are\nunable to outperform traditional methods. To leverage the robustness of deep\nlearning to enhance traditional VSLAM systems, we propose to combine the\npotential of deep learning-based feature descriptors with the traditional\ngeometry-based VSLAM, building a new VSLAM system called LIFT-SLAM. Experiments\nconducted on KITTI and Euroc datasets show that deep learning can be used to\nimprove the performance of traditional VSLAM systems, as the proposed approach\nwas able to achieve results comparable to the state-of-the-art while being\nrobust to sensorial noise. We enhance the proposed VSLAM pipeline by avoiding\nparameter tuning for specific datasets with an adaptive approach while\nevaluating how transfer learning can affect the quality of the features\nextracted.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 20:35:10 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 13:44:22 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Bruno", "Hudson M. S.", ""], ["Colombini", "Esther L.", ""]]}, {"id": "2104.00106", "submitter": "Aviral Joshi", "authors": "Aviral Joshi, Chengzhi Huang, Har Simrat Singh", "title": "Zero-Shot Language Transfer vs Iterative Back Translation for\n  Unsupervised Machine Translation", "comments": "7 pages, 2 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work focuses on comparing different solutions for machine translation on\nlow resource language pairs, namely, with zero-shot transfer learning and\nunsupervised machine translation. We discuss how the data size affects the\nperformance of both unsupervised MT and transfer learning. Additionally we also\nlook at how the domain of the data affects the result of unsupervised MT. The\ncode to all the experiments performed in this project are accessible on Github.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 20:47:19 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Joshi", "Aviral", ""], ["Huang", "Chengzhi", ""], ["Singh", "Har Simrat", ""]]}, {"id": "2104.00158", "submitter": "Nikos Katzouris", "authors": "Nikos Katzouris, Alexander Artikis and Georgios Paliouras", "title": "Online Learning Probabilistic Event Calculus Theories in Answer Set\n  Programming", "comments": "Under consideration in Theory and Practice of Logic Programming\n  (TPLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Complex Event Recognition (CER) systems detect event occurrences in streaming\ntime-stamped input using predefined event patterns. Logic-based approaches are\nof special interest in CER, since, via Statistical Relational AI, they combine\nuncertainty-resilient reasoning with time and change, with machine learning,\nthus alleviating the cost of manual event pattern authoring. We present a\nsystem based on Answer Set Programming (ASP), capable of probabilistic\nreasoning with complex event patterns in the form of weighted rules in the\nEvent Calculus, whose structure and weights are learnt online. We compare our\nASP-based implementation with a Markov Logic-based one and with a number of\nstate-of-the-art batch learning algorithms on CER datasets for activity\nrecognition, maritime surveillance and fleet management. Our results\ndemonstrate the superiority of our novel approach, both in terms of efficiency\nand predictive performance. This paper is under consideration for publication\nin Theory and Practice of Logic Programming (TPLP).\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 23:16:29 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Katzouris", "Nikos", ""], ["Artikis", "Alexander", ""], ["Paliouras", "Georgios", ""]]}, {"id": "2104.00163", "submitter": "Faraz Torabi", "authors": "Faraz Torabi, Garrett Warnell and Peter Stone", "title": "DEALIO: Data-Efficient Adversarial Learning for Imitation from\n  Observation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In imitation learning from observation IfO, a learning agent seeks to imitate\na demonstrating agent using only observations of the demonstrated behavior\nwithout access to the control signals generated by the demonstrator. Recent\nmethods based on adversarial imitation learning have led to state-of-the-art\nperformance on IfO problems, but they typically suffer from high sample\ncomplexity due to a reliance on data-inefficient, model-free reinforcement\nlearning algorithms. This issue makes them impractical to deploy in real-world\nsettings, where gathering samples can incur high costs in terms of time,\nenergy, and risk. In this work, we hypothesize that we can incorporate ideas\nfrom model-based reinforcement learning with adversarial methods for IfO in\norder to increase the data efficiency of these methods without sacrificing\nperformance. Specifically, we consider time-varying linear Gaussian policies,\nand propose a method that integrates the linear-quadratic regulator with path\nintegral policy improvement into an existing adversarial IfO framework. The\nresult is a more data-efficient IfO algorithm with better performance, which we\nshow empirically in four simulation domains: using far fewer interactions with\nthe environment, the proposed method exhibits similar or better performance\nthan the existing technique.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 23:46:32 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Torabi", "Faraz", ""], ["Warnell", "Garrett", ""], ["Stone", "Peter", ""]]}, {"id": "2104.00164", "submitter": "Christine Sinoquet Ms", "authors": "Fatoumata Dama, Christine Sinoquet", "title": "Analysis and modeling to forecast in time series: a systematic review", "comments": "65 pages (including 9 pages with bibliographic references), 14\n  figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper surveys state-of-the-art methods and models dedicated to time\nseries analysis and modeling, with the final aim of prediction. This review\naims to offer a structured and comprehensive view of the full process flow, and\nencompasses time series decomposition, stationary tests, modeling and\nforecasting. Besides, to meet didactic purposes, a unified presentation has\nbeen adopted throughout this survey, to present decomposition frameworks on the\none hand and linear and nonlinear time series models on the other hand. First,\nwe decrypt the relationships between stationarity and linearity, and further\nexamine the main classes of methods used to test for weak stationarity. Next,\nthe main frameworks for time series decomposition are presented in a unified\nway: depending on the time series, a more or less complex decomposition scheme\nseeks to obtain nonstationary effects (the deterministic components) and a\nremaining stochastic component. An appropriate modeling of the latter is a\ncritical step to guarantee prediction accuracy. We then present three popular\nlinear models, together with two more flexible variants of the latter. A step\nfurther in model complexity, and still in a unified way, we present five major\nnonlinear models used for time series. Amongst nonlinear models, artificial\nneural networks hold a place apart as deep learning has recently gained\nconsiderable attention. A whole section is therefore dedicated to time series\nforecasting relying on deep learning approaches. A final section provides a\nlist of R and Python implementations for the methods, models and tests\npresented throughout this review. In this document, our intention is to bring\nsufficient in-depth knowledge, while covering a broad range of models and\nforecasting methods: this compilation spans from well-established conventional\napproaches to more recent adaptations of deep learning to time series\nforecasting.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 23:48:46 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Dama", "Fatoumata", ""], ["Sinoquet", "Christine", ""]]}, {"id": "2104.00170", "submitter": "Robik Shrestha", "authors": "Robik Shrestha, Kushal Kafle and Christopher Kanan", "title": "An Investigation of Critical Issues in Bias Mitigation Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A critical problem in deep learning is that systems learn inappropriate\nbiases, resulting in their inability to perform well on minority groups. This\nhas led to the creation of multiple algorithms that endeavor to mitigate bias.\nHowever, it is not clear how effective these methods are. This is because study\nprotocols differ among papers, systems are tested on datasets that fail to test\nmany forms of bias, and systems have access to hidden knowledge or are tuned\nspecifically to the test set. To address this, we introduce an improved\nevaluation protocol, sensible metrics, and a new dataset, which enables us to\nask and answer critical questions about bias mitigation algorithms. We evaluate\nseven state-of-the-art algorithms using the same network architecture and\nhyperparameter selection policy across three benchmark datasets. We introduce a\nnew dataset called Biased MNIST that enables assessment of robustness to\nmultiple bias sources. We use Biased MNIST and a visual question answering\n(VQA) benchmark to assess robustness to hidden biases. Rather than only tuning\nto the test set distribution, we study robustness across different tuning\ndistributions, which is critical because for many applications the test\ndistribution may not be known during development. We find that algorithms\nexploit hidden biases, are unable to scale to multiple forms of bias, and are\nhighly sensitive to the choice of tuning set. Based on our findings, we implore\nthe community to adopt more rigorous assessment of future bias mitigation\nmethods. All data, code, and results are publicly available at:\nhttps://github.com/erobic/bias-mitigators.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 00:14:45 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Shrestha", "Robik", ""], ["Kafle", "Kushal", ""], ["Kanan", "Christopher", ""]]}, {"id": "2104.00190", "submitter": "Wail Gueaieb", "authors": "Mohammed Abouheaf, Wail Gueaieb, Md. Suruz Miah, Davide Spinello", "title": "Trajectory Tracking of Underactuated Sea Vessels With Uncertain\n  Dynamics: An Integral Reinforcement Learning Approach", "comments": null, "journal-ref": "IEEE International Conference on Systems, Man, and Cybernetics\n  (SMC), Toronto, ON, Canada, 2020, pp. 1866-1871", "doi": "10.1109/SMC42975.2020.9283399", "report-no": null, "categories": "eess.SY cs.AI cs.LG cs.RO cs.SY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Underactuated systems like sea vessels have degrees of motion that are\ninsufficiently matched by a set of independent actuation forces. In addition,\nthe underlying trajectory-tracking control problems grow in complexity in order\nto decide the optimal rudder and thrust control signals. This enforces several\ndifficult-to-solve constraints that are associated with the error dynamical\nequations using classical optimal tracking and adaptive control approaches. An\nonline machine learning mechanism based on integral reinforcement learning is\nproposed to find a solution for a class of nonlinear tracking problems with\npartial prior knowledge of the system dynamics. The actuation forces are\ndecided using innovative forms of temporal difference equations relevant to the\nvessel's surge and angular velocities. The solution is implemented using an\nonline value iteration process which is realized by employing means of the\nadaptive critics and gradient descent approaches. The adaptive learning\nmechanism exhibited well-functioning and interactive features in react to\ndifferent desired reference-tracking scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 01:41:49 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Abouheaf", "Mohammed", ""], ["Gueaieb", "Wail", ""], ["Miah", "Md. Suruz", ""], ["Spinello", "Davide", ""]]}, {"id": "2104.00199", "submitter": "Wail Gueaieb", "authors": "Ning Wang, Mohammed Abouheaf, Wail Gueaieb", "title": "Data-Driven Optimized Tracking Control Heuristic for MIMO Structures: A\n  Balance System Case Study", "comments": null, "journal-ref": "IEEE International Conference on Systems, Man, and Cybernetics\n  (SMC), Toronto, ON, Canada, 2020, pp. 2365-2370", "doi": "10.1109/SMC42975.2020.9283038", "report-no": null, "categories": "eess.SY cs.AI cs.LG cs.SY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A data-driven computational heuristic is proposed to control MIMO systems\nwithout prior knowledge of their dynamics. The heuristic is illustrated on a\ntwo-input two-output balance system. It integrates a self-adjusting nonlinear\nthreshold accepting heuristic with a neural network to compromise between the\ndesired transient and steady state characteristics of the system while\noptimizing a dynamic cost function. The heuristic decides on the control gains\nof multiple interacting PID control loops. The neural network is trained upon\noptimizing a weighted-derivative like objective cost function. The performance\nof the developed mechanism is compared with another controller that employs a\ncombined PID-Riccati approach. One of the salient features of the proposed\ncontrol schemes is that they do not require prior knowledge of the system\ndynamics. However, they depend on a known region of stability for the control\ngains to be used as a search space by the optimization algorithm. The control\nmechanism is validated using different optimization criteria which address\ndifferent design requirements.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 02:00:20 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Wang", "Ning", ""], ["Abouheaf", "Mohammed", ""], ["Gueaieb", "Wail", ""]]}, {"id": "2104.00203", "submitter": "Marina Haliem", "authors": "Marina Haliem, Vaneet Aggarwal and Bharat Bhargava", "title": "AdaPool: A Diurnal-Adaptive Fleet Management Framework using Model-Free\n  Deep Reinforcement Learning and Change Point Detection", "comments": "arXiv admin note: text overlap with arXiv:2010.01755", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an adaptive model-free deep reinforcement approach that\ncan recognize and adapt to the diurnal patterns in the ride-sharing environment\nwith car-pooling. Deep Reinforcement Learning (RL) suffers from catastrophic\nforgetting due to being agnostic to the timescale of changes in the\ndistribution of experiences. Although RL algorithms are guaranteed to converge\nto optimal policies in Markov decision processes (MDPs), this only holds in the\npresence of static environments. However, this assumption is very restrictive.\nIn many real-world problems like ride-sharing, traffic control, etc., we are\ndealing with highly dynamic environments, where RL methods yield only\nsub-optimal decisions. To mitigate this problem in highly dynamic environments,\nwe (1) adopt an online Dirichlet change point detection (ODCP) algorithm to\ndetect the changes in the distribution of experiences, (2) develop a Deep Q\nNetwork (DQN) agent that is capable of recognizing diurnal patterns and making\ninformed dispatching decisions according to the changes in the underlying\nenvironment. Rather than fixing patterns by time of week, the proposed approach\nautomatically detects that the MDP has changed, and uses the results of the new\nmodel. In addition to the adaptation logic in dispatching, this paper also\nproposes a dynamic, demand-aware vehicle-passenger matching and route planning\nframework that dynamically generates optimal routes for each vehicle based on\nonline demand, vehicle capacities, and locations. Evaluation on New York City\nTaxi public dataset shows the effectiveness of our approach in improving the\nfleet utilization, where less than 50% of the fleet are utilized to serve the\ndemand of up to 90% of the requests, while maximizing profits and minimizing\nidle times.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 02:14:01 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 15:42:28 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Haliem", "Marina", ""], ["Aggarwal", "Vaneet", ""], ["Bhargava", "Bharat", ""]]}, {"id": "2104.00273", "submitter": "Pengliang Ji", "authors": "Pengliang Ji, Li Ruan, Yunzhi Xue, Limin Xiao, Qian Dong", "title": "Perspective, Survey and Trends: Public Driving Datasets and Toolsets for\n  Autonomous Driving Virtual Test", "comments": "6 pages, 4 figures. Accepted to 24th IEEE International Conference on\n  Intelligent Transportation - ITSC2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Owing to the merits of early safety and reliability guarantee, autonomous\ndriving virtual testing has recently gains increasing attention compared with\nclosed-loop testing in real scenarios. Although the availability and quality of\nautonomous driving datasets and toolsets are the premise to diagnose the\nautonomous driving system bottlenecks and improve the system performance, due\nto the diversity and privacy of the datasets and toolsets, collecting and\nfeaturing the perspective and quality of them become not only time-consuming\nbut also increasingly challenging. This paper first proposes a Systematic\nLiterature review approach for Autonomous driving tests (SLA), then presents an\noverview of existing publicly available datasets and toolsets from 2000 to\n2020. Quantitative findings with the scenarios concerned, perspectives and\ntrend inferences and suggestions with 35 automated driving test tool sets and\n70 test data sets are also presented. To the best of our knowledge, we are the\nfirst to perform such recent empirical survey on both the datasets and toolsets\nusing a SLA based survey approach. Our multifaceted analyses and new findings\nnot only reveal insights that we believe are useful for system designers,\npractitioners and users, but also can promote more researches on a systematic\nsurvey analysis in autonomous driving surveys on dataset and toolsets.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 06:17:01 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 02:50:15 GMT"}, {"version": "v3", "created": "Tue, 20 Apr 2021 01:24:34 GMT"}, {"version": "v4", "created": "Wed, 30 Jun 2021 09:01:52 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Ji", "Pengliang", ""], ["Ruan", "Li", ""], ["Xue", "Yunzhi", ""], ["Xiao", "Limin", ""], ["Dong", "Qian", ""]]}, {"id": "2104.00290", "submitter": "Thamme Gowda", "authors": "Thamme Gowda, Zhao Zhang, Chris A Mattmann, Jonathan May", "title": "Many-to-English Machine Translation Tools, Data, and Pretrained Models", "comments": "To-appear: ACL 2021 System Demonstrations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While there are more than 7000 languages in the world, most translation\nresearch efforts have targeted a few high-resource languages. Commercial\ntranslation systems support only one hundred languages or fewer, and do not\nmake these models available for transfer to low resource languages. In this\nwork, we present useful tools for machine translation research: MTData,\nNLCodec, and RTG. We demonstrate their usefulness by creating a multilingual\nneural machine translation model capable of translating from 500 source\nlanguages to English. We make this multilingual model readily downloadable and\nusable as a service, or as a parent model for transfer-learning to even\nlower-resource languages.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 06:55:12 GMT"}, {"version": "v2", "created": "Thu, 1 Jul 2021 19:40:00 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Gowda", "Thamme", ""], ["Zhang", "Zhao", ""], ["Mattmann", "Chris A", ""], ["May", "Jonathan", ""]]}, {"id": "2104.00299", "submitter": "Hojung Lee", "authors": "Hojung Lee, Jong-Seok Lee", "title": "Students are the Best Teacher: Exit-Ensemble Distillation with\n  Multi-Exits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a novel knowledge distillation-based learning method to\nimprove the classification performance of convolutional neural networks (CNNs)\nwithout a pre-trained teacher network, called exit-ensemble distillation. Our\nmethod exploits the multi-exit architecture that adds auxiliary classifiers\n(called exits) in the middle of a conventional CNN, through which early\ninference results can be obtained. The idea of our method is to train the\nnetwork using the ensemble of the exits as the distillation target, which\ngreatly improves the classification performance of the overall network. Our\nmethod suggests a new paradigm of knowledge distillation; unlike the\nconventional notion of distillation where teachers only teach students, we show\nthat students can also help other students and even the teacher to learn\nbetter. Experimental results demonstrate that our method achieves significant\nimprovement of classification performance on various popular CNN architectures\n(VGG, ResNet, ResNeXt, WideResNet, etc.). Furthermore, the proposed method can\nexpedite the convergence of learning with improved stability. Our code will be\navailable on Github.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 07:10:36 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 01:13:20 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Lee", "Hojung", ""], ["Lee", "Jong-Seok", ""]]}, {"id": "2104.00308", "submitter": "Rongjie Li", "authors": "Rongjie Li, Songyang Zhang, Bo Wan, Xuming He", "title": "Bipartite Graph Network with Adaptive Message Passing for Unbiased Scene\n  Graph Generation", "comments": "Accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scene graph generation is an important visual understanding task with a broad\nrange of vision applications. Despite recent tremendous progress, it remains\nchallenging due to the intrinsic long-tailed class distribution and large\nintra-class variation. To address these issues, we introduce a novel\nconfidence-aware bipartite graph neural network with adaptive message\npropagation mechanism for unbiased scene graph generation. In addition, we\npropose an efficient bi-level data resampling strategy to alleviate the\nimbalanced data distribution problem in training our graph network. Our\napproach achieves superior or competitive performance over previous methods on\nseveral challenging datasets, including Visual Genome, Open Images V4/V6,\ndemonstrating its effectiveness and generality.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 07:30:14 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 03:33:36 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Li", "Rongjie", ""], ["Zhang", "Songyang", ""], ["Wan", "Bo", ""], ["He", "Xuming", ""]]}, {"id": "2104.00312", "submitter": "Ningyu Zhang", "authors": "Luoqiu Li, Xiang Chen, Ningyu Zhang, Shumin Deng, Xin Xie, Chuanqi\n  Tan, Mosha Chen, Fei Huang, Huajun Chen", "title": "Normal vs. Adversarial: Salience-based Analysis of Adversarial Samples\n  for Relation Extraction", "comments": "Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent neural-based relation extraction approaches, though achieving\npromising improvement on benchmark datasets, have reported their vulnerability\ntowards adversarial attacks. Thus far, efforts mostly focused on generating\nadversarial samples or defending adversarial attacks, but little is known about\nthe difference between normal and adversarial samples. In this work, we take\nthe first step to leverage the salience-based method to analyze those\nadversarial samples. We observe that salience tokens have a direct correlation\nwith adversarial perturbations. We further find the adversarial perturbations\nare either those tokens not existing in the training set or superficial cues\nassociated with relation labels. To some extent, our approach unveils the\ncharacters against adversarial samples. We release an open-source testbed,\n\"DiagnoseAdv\".\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 07:36:04 GMT"}, {"version": "v2", "created": "Sun, 11 Apr 2021 07:39:20 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Li", "Luoqiu", ""], ["Chen", "Xiang", ""], ["Zhang", "Ningyu", ""], ["Deng", "Shumin", ""], ["Xie", "Xin", ""], ["Tan", "Chuanqi", ""], ["Chen", "Mosha", ""], ["Huang", "Fei", ""], ["Chen", "Huajun", ""]]}, {"id": "2104.00317", "submitter": "Phong Tran", "authors": "Phong Tran and Anh Tran and Quynh Phung and Minh Hoai", "title": "Explore Image Deblurring via Blur Kernel Space", "comments": "Accepted to CVPR'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a method to encode the blur operators of an arbitrary\ndataset of sharp-blur image pairs into a blur kernel space. Assuming the\nencoded kernel space is close enough to in-the-wild blur operators, we propose\nan alternating optimization algorithm for blind image deblurring. It\napproximates an unseen blur operator by a kernel in the encoded space and\nsearches for the corresponding sharp image. Unlike recent deep-learning-based\nmethods, our system can handle unseen blur kernel, while avoiding using\ncomplicated handcrafted priors on the blur operator often found in classical\nmethods. Due to the method's design, the encoded kernel space is fully\ndifferentiable, thus can be easily adopted in deep neural network models.\nMoreover, our method can be used for blur synthesis by transferring existing\nblur operators from a given dataset into a new domain. Finally, we provide\nexperimental results to confirm the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 07:52:53 GMT"}, {"version": "v2", "created": "Sat, 3 Apr 2021 12:58:29 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Tran", "Phong", ""], ["Tran", "Anh", ""], ["Phung", "Quynh", ""], ["Hoai", "Minh", ""]]}, {"id": "2104.00336", "submitter": "Nayeon Lee", "authors": "Nayeon Lee, Yejin Bang, Andrea Madotto, Pascale Fung", "title": "Mitigating Media Bias through Neutral Article Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Media bias can lead to increased political polarization, and thus, the need\nfor automatic mitigation methods is growing. Existing mitigation work displays\narticles from multiple news outlets to provide diverse news coverage, but\nwithout neutralizing the bias inherent in each of the displayed articles.\nTherefore, we propose a new task, a single neutralized article generation out\nof multiple biased articles, to facilitate more efficient access to balanced\nand unbiased information. In this paper, we compile a new dataset NeuWS, define\nan automatic evaluation metric, and provide baselines and multiple analyses to\nserve as a solid starting point for the proposed task. Lastly, we obtain a\nhuman evaluation to demonstrate the alignment between our metric and human\njudgment.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 08:37:26 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Lee", "Nayeon", ""], ["Bang", "Yejin", ""], ["Madotto", "Andrea", ""], ["Fung", "Pascale", ""]]}, {"id": "2104.00351", "submitter": "Kacper Kania", "authors": "Kacper Kania, Marek Kowalski, Tomasz Trzci\\'nski", "title": "TrajeVAE -- Controllable Human Motion Generation from Trajectories", "comments": "Animations used in the paper can be found at\n  https://kacperkan.github.io/trajevae-supplementary/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The generation of plausible and controllable 3D human motion animations is a\nlong-standing problem that often requires a manual intervention of skilled\nartists. Existing machine learning approaches try to semi-automate this process\nby allowing the user to input partial information about the future movement.\nHowever, they are limited in two significant ways: they either base their pose\nprediction on past prior frames with no additional control over the future\nposes or allow the user to input only a single trajectory that precludes\nfine-grained control over the output. To mitigate these two issues, we\nreformulate the problem of future pose prediction into pose completion in space\nand time where trajectories are represented as poses with missing joints. We\nshow that such a framework can generalize to other neural networks designed for\nfuture pose prediction. Once trained in this framework, a model is capable of\npredicting sequences from any number of trajectories. To leverage this notion,\nwe propose a novel transformer-like architecture, TrajeVAE, that provides a\nversatile framework for 3D human animation. We demonstrate that TrajeVAE\noutperforms trajectory-based reference approaches and methods that base their\npredictions on past poses in terms of accuracy. We also show that it can\npredict reasonable future poses even if provided only with an initial pose.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 09:12:48 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Kania", "Kacper", ""], ["Kowalski", "Marek", ""], ["Trzci\u0144ski", "Tomasz", ""]]}, {"id": "2104.00358", "submitter": "Daniel Buschek", "authors": "Daniel Buschek, Lukas Mecke, Florian Lehmann, Hai Dang", "title": "Nine Potential Pitfalls when Designing Human-AI Co-Creative Systems", "comments": "8 pages, 2 figures, 1 table, HAI-GEN Workshop at IUI'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This position paper examines potential pitfalls on the way towards achieving\nhuman-AI co-creation with generative models in a way that is beneficial to the\nusers' interests. In particular, we collected a set of nine potential pitfalls,\nbased on the literature and our own experiences as researchers working at the\nintersection of HCI and AI. We illustrate each pitfall with examples and\nsuggest ideas for addressing it. Reflecting on all pitfalls, we discuss and\nconclude with implications for future research directions. With this\ncollection, we hope to contribute to a critical and constructive discussion on\nthe roles of humans and AI in co-creative interactions, with an eye on related\nassumptions and potential side-effects for creative practices and beyond.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 09:27:30 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Buschek", "Daniel", ""], ["Mecke", "Lukas", ""], ["Lehmann", "Florian", ""], ["Dang", "Hai", ""]]}, {"id": "2104.00362", "submitter": "Martin K\\\"appel", "authors": "Martin K\\\"appel, Stefan Jablonski, Stefan Sch\\\"onig", "title": "Evaluating Predictive Business Process Monitoring Approaches on Small\n  Event Logs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive business process monitoring is concerned with the prediction how a\nrunning process instance will unfold up to its completion at runtime. Most of\nthe proposed approaches rely on a wide number of different machine learning\n(ML) techniques. In the last years numerous comparative studies, reviews, and\nbenchmarks of such approaches where published and revealed that they can be\nsuccessfully applied for different prediction targets. ML techniques require a\nqualitatively and quantitatively sufficient data set. However, there are many\nsituations in business process management (BPM) where only a quantitatively\ninsufficient data set is available. The problem of insufficient data in the\ncontext of BPM is still neglected. Hence, none of the comparative studies or\nbenchmarks investigates the performance of predictive business process\nmonitoring techniques in environments with small data sets. In this paper an\nevaluation framework for comparing existing approaches with regard to their\nsuitability for small data sets is developed and exemplarily applied to\nstate-of-the-art approaches in predictive business process monitoring.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 09:36:04 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 06:40:02 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["K\u00e4ppel", "Martin", ""], ["Jablonski", "Stefan", ""], ["Sch\u00f6nig", "Stefan", ""]]}, {"id": "2104.00387", "submitter": "Agnese Chiatti", "authors": "Agnese Chiatti, Gianluca Bardaro, Enrico Motta, Enrico Daga", "title": "Commonsense Spatial Reasoning for Visually Intelligent Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Service robots are expected to reliably make sense of complex, fast-changing\nenvironments. From a cognitive standpoint, they need the appropriate reasoning\ncapabilities and background knowledge required to exhibit human-like Visual\nIntelligence. In particular, our prior work has shown that the ability to\nreason about spatial relations between objects in the world is a key\nrequirement for the development of Visually Intelligent Agents. In this paper,\nwe present a framework for commonsense spatial reasoning which is tailored to\nreal-world robotic applications. Differently from prior approaches to\nqualitative spatial reasoning, the proposed framework is robust to variations\nin the robot's viewpoint and object orientation. The spatial relations in the\nproposed framework are also mapped to the types of commonsense predicates used\nto describe typical object configurations in English. In addition, we also show\nhow this formally-defined framework can be implemented in a concrete spatial\ndatabase.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 10:43:50 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Chiatti", "Agnese", ""], ["Bardaro", "Gianluca", ""], ["Motta", "Enrico", ""], ["Daga", "Enrico", ""]]}, {"id": "2104.00405", "submitter": "Vincenzo Lomonaco PhD", "authors": "Vincenzo Lomonaco, Lorenzo Pellegrini, Andrea Cossu, Antonio Carta,\n  Gabriele Graffieti, Tyler L. Hayes, Matthias De Lange, Marc Masana, Jary\n  Pomponi, Gido van de Ven, Martin Mundt, Qi She, Keiland Cooper, Jeremy\n  Forest, Eden Belouadah, Simone Calderara, German I. Parisi, Fabio Cuzzolin,\n  Andreas Tolias, Simone Scardapane, Luca Antiga, Subutai Amhad, Adrian\n  Popescu, Christopher Kanan, Joost van de Weijer, Tinne Tuytelaars, Davide\n  Bacciu, Davide Maltoni", "title": "Avalanche: an End-to-End Library for Continual Learning", "comments": "Official Website: https://avalanche.continualai.org", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning continually from non-stationary data streams is a long-standing goal\nand a challenging problem in machine learning. Recently, we have witnessed a\nrenewed and fast-growing interest in continual learning, especially within the\ndeep learning community. However, algorithmic solutions are often difficult to\nre-implement, evaluate and port across different settings, where even results\non standard benchmarks are hard to reproduce. In this work, we propose\nAvalanche, an open-source end-to-end library for continual learning research\nbased on PyTorch. Avalanche is designed to provide a shared and collaborative\ncodebase for fast prototyping, training, and reproducible evaluation of\ncontinual learning algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 11:31:46 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Lomonaco", "Vincenzo", ""], ["Pellegrini", "Lorenzo", ""], ["Cossu", "Andrea", ""], ["Carta", "Antonio", ""], ["Graffieti", "Gabriele", ""], ["Hayes", "Tyler L.", ""], ["De Lange", "Matthias", ""], ["Masana", "Marc", ""], ["Pomponi", "Jary", ""], ["van de Ven", "Gido", ""], ["Mundt", "Martin", ""], ["She", "Qi", ""], ["Cooper", "Keiland", ""], ["Forest", "Jeremy", ""], ["Belouadah", "Eden", ""], ["Calderara", "Simone", ""], ["Parisi", "German I.", ""], ["Cuzzolin", "Fabio", ""], ["Tolias", "Andreas", ""], ["Scardapane", "Simone", ""], ["Antiga", "Luca", ""], ["Amhad", "Subutai", ""], ["Popescu", "Adrian", ""], ["Kanan", "Christopher", ""], ["van de Weijer", "Joost", ""], ["Tuytelaars", "Tinne", ""], ["Bacciu", "Davide", ""], ["Maltoni", "Davide", ""]]}, {"id": "2104.00409", "submitter": "Parfait Atchade", "authors": "Parfait Atchade-Adelomou, Daniel Casado-Fauli, Elisabet\n  Golobardes-Ribe and Xavier Vilasis-Cardona", "title": "quantum Case-Based Reasoning (qCBR)", "comments": "14 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.ET cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Case-Based Reasoning (CBR) is an artificial intelligence approach to\nproblem-solving with a good record of success. This article proposes using\nQuantum Computing to improve some of the key processes of CBR defining so a\nQuantum Case-Based Reasoning (qCBR) paradigm. The focus is set on designing and\nimplementing a qCBR based on the variational principle that improves its\nclassical counterpart in terms of average accuracy, scalability and tolerance\nto overlapping. A comparative study of the proposed qCBR with a classic CBR is\nperformed for the case of the Social Workers' Problem as a sample of a\ncombinatorial optimization problem with overlapping. The algorithm's quantum\nfeasibility is modelled with docplex and tested on IBMQ computers, and\nexperimented on the Qibo framework.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 11:34:22 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Atchade-Adelomou", "Parfait", ""], ["Casado-Fauli", "Daniel", ""], ["Golobardes-Ribe", "Elisabet", ""], ["Vilasis-Cardona", "Xavier", ""]]}, {"id": "2104.00415", "submitter": "Amir Zandieh", "authors": "Amir Zandieh", "title": "Learning with Neural Tangent Kernels in Near Input Sparsity Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Neural Tangent Kernel (NTK) characterizes the behavior of infinitely wide\nneural nets trained under least squares loss by gradient descent. However,\ndespite its importance, the super-quadratic runtime of kernel methods limits\nthe use of NTK in large-scale learning tasks. To accelerate kernel machines\nwith NTK, we propose a near input sparsity time algorithm that maps the input\ndata to a randomized low-dimensional feature space so that the inner product of\nthe transformed data approximates their NTK evaluation. Our transformation\nworks by sketching the polynomial expansions of arc-cosine kernels.\nFurthermore, we propose a feature map for approximating the convolutional\ncounterpart of the NTK, which can transform any image using a runtime that is\nonly linear in the number of pixels. We show that in standard large-scale\nregression and classification tasks a linear regressor trained on our features\noutperforms trained Neural Nets and Nystrom approximation of NTK kernel.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 11:56:58 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 07:04:09 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Zandieh", "Amir", ""]]}, {"id": "2104.00426", "submitter": "Mingxuan Niu", "authors": "Yuka Takeishi, Mingxuan Niu, Jing Luo, Zhong Jin, Xinyu Yang", "title": "WakaVT: A Sequential Variational Transformer for Waka Generation", "comments": "This paper has been submitted to Neural Processing Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Poetry generation has long been a challenge for artificial intelligence. In\nthe scope of Japanese poetry generation, many researchers have paid attention\nto Haiku generation, but few have focused on Waka generation. To further\nexplore the creative potential of natural language generation systems in\nJapanese poetry creation, we propose a novel Waka generation model, WakaVT,\nwhich automatically produces Waka poems given user-specified keywords. Firstly,\nan additive mask-based approach is presented to satisfy the form constraint.\nSecondly, the structures of Transformer and variational autoencoder are\nintegrated to enhance the quality of generated content. Specifically, to obtain\nnovelty and diversity, WakaVT employs a sequence of latent variables, which\neffectively captures word-level variability in Waka data. To improve linguistic\nquality in terms of fluency, coherence, and meaningfulness, we further propose\nthe fused multilevel self-attention mechanism, which properly models the\nhierarchical linguistic structure of Waka. To the best of our knowledge, we are\nthe first to investigate Waka generation with models based on Transformer\nand/or variational autoencoder. Both objective and subjective evaluation\nresults demonstrate that our model outperforms baselines significantly.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 12:14:41 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Takeishi", "Yuka", ""], ["Niu", "Mingxuan", ""], ["Luo", "Jing", ""], ["Jin", "Zhong", ""], ["Yang", "Xinyu", ""]]}, {"id": "2104.00428", "submitter": "Emile van Krieken", "authors": "Emile van Krieken, Jakub M. Tomczak, Annette ten Teije", "title": "Storchastic: A Framework for General Stochastic Automatic\n  Differentiation", "comments": "28 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modelers use automatic differentiation (AD) of computation graphs to\nimplement complex Deep Learning models without defining gradient computations.\nStochastic AD extends AD to stochastic computation graphs with sampling steps,\nwhich arise when modelers handle the intractable expectations common in\nReinforcement Learning and Variational Inference. However, current methods for\nstochastic AD are limited: They are either only applicable to continuous random\nvariables and differentiable functions, or can only use simple but high\nvariance score-function estimators. To overcome these limitations, we introduce\nStorchastic, a new framework for AD of stochastic computation graphs.\nStorchastic allows the modeler to choose from a wide variety of gradient\nestimation methods at each sampling step, to optimally reduce the variance of\nthe gradient estimates. Furthermore, Storchastic is provably unbiased for\nestimation of any-order gradients, and generalizes variance reduction\ntechniques to higher-order gradient estimates. Finally, we implement\nStorchastic as a PyTorch library.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 12:19:54 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 14:13:16 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["van Krieken", "Emile", ""], ["Tomczak", "Jakub M.", ""], ["Teije", "Annette ten", ""]]}, {"id": "2104.00442", "submitter": "Sai Rajeswar Mudumba", "authors": "Sai Rajeswar, Cyril Ibrahim, Nitin Surya, Florian Golemo, David\n  Vazquez, Aaron Courville, Pedro O. Pinheiro", "title": "Touch-based Curiosity for Sparse-Reward Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Robots in many real-world settings have access to force/torque sensors in\ntheir gripper and tactile sensing is often necessary in tasks that involve\ncontact-rich motion. In this work, we leverage surprise from mismatches in\ntouch feedback to guide exploration in hard sparse-reward reinforcement\nlearning tasks. Our approach, Touch-based Curiosity (ToC), learns what visible\nobjects interactions are supposed to \"feel\" like. We encourage exploration by\nrewarding interactions where the expectation and the experience don't match. In\nour proposed method, an initial task-independent exploration phase is followed\nby an on-task learning phase, in which the original interactions are relabeled\nwith on-task rewards. We test our approach on a range of touch-intensive robot\narm tasks (e.g. pushing objects, opening doors), which we also release as part\nof this work. Across multiple experiments in a simulated setting, we\ndemonstrate that our method is able to learn these difficult tasks through\nsparse reward and curiosity alone. We compare our cross-modal approach to\nsingle-modality (touch- or vision-only) approaches as well as other\ncuriosity-based methods and find that our method performs better and is more\nsample-efficient.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 12:49:29 GMT"}, {"version": "v2", "created": "Sat, 26 Jun 2021 04:55:32 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Rajeswar", "Sai", ""], ["Ibrahim", "Cyril", ""], ["Surya", "Nitin", ""], ["Golemo", "Florian", ""], ["Vazquez", "David", ""], ["Courville", "Aaron", ""], ["Pinheiro", "Pedro O.", ""]]}, {"id": "2104.00452", "submitter": "Jo\\v{z}e Ro\\v{z}anec", "authors": "Jo\\v{z}e M. Ro\\v{z}anec and Dunja Mladeni\\'c", "title": "Semantic XAI for contextualized demand forecasting explanations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper proposes a novel architecture for explainable AI based on semantic\ntechnologies and AI. We tailor the architecture for the domain of demand\nforecasting and validate it on a real-world case study. The provided\nexplanations combine concepts describing features relevant to a particular\nforecast, related media events, and metadata regarding external datasets of\ninterest. The knowledge graph provides concepts that convey feature information\nat a higher abstraction level. By using them, explanations do not expose\nsensitive details regarding the demand forecasting models. The explanations\nalso emphasize actionable dimensions where suitable. We link domain knowledge,\nforecasted values, and forecast explanations in a Knowledge Graph. The ontology\nand dataset we developed for this use case are publicly available for further\nresearch.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 13:08:53 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Ro\u017eanec", "Jo\u017ee M.", ""], ["Mladeni\u0107", "Dunja", ""]]}, {"id": "2104.00479", "submitter": "Celia Cintas", "authors": "Celia Cintas, Payel Das, Brian Quanz, Skyler Speakman, Victor\n  Akinwande, Pin-Yu Chen", "title": "Towards creativity characterization of generative models via group-based\n  subset scanning", "comments": "Synthetic Data Generation Workshop at ICLR'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep generative models, such as Variational Autoencoders (VAEs), have been\nemployed widely in computational creativity research. However, such models\ndiscourage out-of-distribution generation to avoid spurious sample generation,\nlimiting their creativity. Thus, incorporating research on human creativity\ninto generative deep learning techniques presents an opportunity to make their\noutputs more compelling and human-like. As we see the emergence of generative\nmodels directed to creativity research, a need for machine learning-based\nsurrogate metrics to characterize creative output from these models is\nimperative. We propose group-based subset scanning to quantify, detect, and\ncharacterize creative processes by detecting a subset of anomalous\nnode-activations in the hidden layers of generative models. Our experiments on\noriginal, typically decoded, and \"creatively decoded\" (Das et al 2020) image\ndatasets reveal that the proposed subset scores distribution is more useful for\ndetecting creative processes in the activation space rather than the pixel\nspace. Further, we found that creative samples generate larger subsets of\nanomalies than normal or non-creative samples across datasets. The node\nactivations highlighted during the creative decoding process are different from\nthose responsible for normal sample generation.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 14:07:49 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 11:49:07 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Cintas", "Celia", ""], ["Das", "Payel", ""], ["Quanz", "Brian", ""], ["Speakman", "Skyler", ""], ["Akinwande", "Victor", ""], ["Chen", "Pin-Yu", ""]]}, {"id": "2104.00487", "submitter": "Jianjin Xu", "authors": "Jianjin Xu, Changxi Zheng", "title": "Linear Semantics in Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generative Adversarial Networks (GANs) are able to generate high-quality\nimages, but it remains difficult to explicitly specify the semantics of\nsynthesized images. In this work, we aim to better understand the semantic\nrepresentation of GANs, and thereby enable semantic control in GAN's generation\nprocess. Interestingly, we find that a well-trained GAN encodes image semantics\nin its internal feature maps in a surprisingly simple way: a linear\ntransformation of feature maps suffices to extract the generated image\nsemantics. To verify this simplicity, we conduct extensive experiments on\nvarious GANs and datasets; and thanks to this simplicity, we are able to learn\na semantic segmentation model for a trained GAN from a small number (e.g., 8)\nof labeled images. Last but not least, leveraging our findings, we propose two\nfew-shot image editing approaches, namely Semantic-Conditional Sampling and\nSemantic Image Editing. Given a trained GAN and as few as eight semantic\nannotations, the user is able to generate diverse images subject to a\nuser-provided semantic layout, and control the synthesized image semantics. We\nhave made the code publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 14:18:48 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Xu", "Jianjin", ""], ["Zheng", "Changxi", ""]]}, {"id": "2104.00488", "submitter": "Jun Fu", "authors": "Jun Fu, Wei Zhou, Zhibo Chen", "title": "Bayesian Graph Convolutional Network for Traffic Prediction", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, adaptive graph convolutional network based traffic prediction\nmethods, learning a latent graph structure from traffic data via various\nattention-based mechanisms, have achieved impressive performance. However, they\nare still limited to find a better description of spatial relationships between\ntraffic conditions due to: (1) ignoring the prior of the observed topology of\nthe road network; (2) neglecting the presence of negative spatial\nrelationships; and (3) lacking investigation on uncertainty of the graph\nstructure. In this paper, we propose a Bayesian Graph Convolutional Network\n(BGCN) framework to alleviate these issues. Under this framework, the graph\nstructure is viewed as a random realization from a parametric generative model,\nand its posterior is inferred using the observed topology of the road network\nand traffic data. Specifically, the parametric generative model is comprised of\ntwo parts: (1) a constant adjacency matrix which discovers potential spatial\nrelationships from the observed physical connections between roads using a\nBayesian approach; (2) a learnable adjacency matrix that learns a global shared\nspatial correlations from traffic data in an end-to-end fashion and can model\nnegative spatial correlations. The posterior of the graph structure is then\napproximated by performing Monte Carlo dropout on the parametric graph\nstructure. We verify the effectiveness of our method on five real-world\ndatasets, and the experimental results demonstrate that BGCN attains superior\nperformance compared with state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 14:19:37 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Fu", "Jun", ""], ["Zhou", "Wei", ""], ["Chen", "Zhibo", ""]]}, {"id": "2104.00513", "submitter": "Jingsong Wang", "authors": "Jingsong Wang, Yuxuan He, Chunyu Zhao, Qijie Shao, Wei-Wei Tu, Tom Ko,\n  Hung-yi Lee, Lei Xie", "title": "Auto-KWS 2021 Challenge: Task, Datasets, and Baselines", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Auto-KWS 2021 challenge calls for automated machine learning (AutoML)\nsolutions to automate the process of applying machine learning to a customized\nkeyword spotting task. Compared with other keyword spotting tasks, Auto-KWS\nchallenge has the following three characteristics: 1) The challenge focuses on\nthe problem of customized keyword spotting, where the target device can only be\nawakened by an enrolled speaker with his specified keyword. The speaker can use\nany language and accent to define his keyword. 2) All dataset of the challenge\nis recorded in realistic environment. It is to simulate different user\nscenarios. 3) Auto-KWS is a \"code competition\", where participants need to\nsubmit AutoML solutions, then the platform automatically runs the enrollment\nand prediction steps with the submitted code.This challenge aims at promoting\nthe development of a more personalized and flexible keyword spotting system.\nTwo baseline systems are provided to all participants as references.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 14:56:48 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Wang", "Jingsong", ""], ["He", "Yuxuan", ""], ["Zhao", "Chunyu", ""], ["Shao", "Qijie", ""], ["Tu", "Wei-Wei", ""], ["Ko", "Tom", ""], ["Lee", "Hung-yi", ""], ["Xie", "Lei", ""]]}, {"id": "2104.00526", "submitter": "Partha Mitra", "authors": "Partha P Mitra", "title": "Fitting Elephants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Textbook wisdom advocates for smooth function fits and implies that\ninterpolation of noisy data should lead to poor generalization. A related\nheuristic is that fitting parameters should be fewer than measurements (Occam's\nRazor). Surprisingly, contemporary machine learning (ML) approaches, cf. deep\nnets (DNNs), generalize well despite interpolating noisy data. This may be\nunderstood via Statistically Consistent Interpolation (SCI), i.e. data\ninterpolation techniques that generalize optimally for big data. In this\narticle we elucidate SCI using the weighted interpolating nearest neighbors\n(wiNN) algorithm, which adds singular weight functions to kNN (k-nearest\nneighbors). This shows that data interpolation can be a valid ML strategy for\nbig data. SCI clarifies the relation between two ways of modeling natural\nphenomena: the rationalist approach (strong priors) of theoretical physics with\nfew parameters and the empiricist (weak priors) approach of modern ML with more\nparameters than data. SCI shows that the purely empirical approach can\nsuccessfully predict. However data interpolation does not provide theoretical\ninsights, and the training data requirements may be prohibitive. Complex animal\nbrains are between these extremes, with many parameters, but modest training\ndata, and with prior structure encoded in species-specific mesoscale circuitry.\nThus, modern ML provides a distinct epistemological approach different both\nfrom physical theories and animal brains.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 05:50:39 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Mitra", "Partha P", ""]]}, {"id": "2104.00540", "submitter": "Bhaskar Ramasubramanian", "authors": "Bhaskar Ramasubramanian, Luyao Niu, Andrew Clark, Radha Poovendran", "title": "Reinforcement Learning Beyond Expectation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SY eess.SY math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The inputs and preferences of human users are important considerations in\nsituations where these users interact with autonomous cyber or cyber-physical\nsystems. In these scenarios, one is often interested in aligning behaviors of\nthe system with the preferences of one or more human users. Cumulative prospect\ntheory (CPT) is a paradigm that has been empirically shown to model a tendency\nof humans to view gains and losses differently. In this paper, we consider a\nsetting where an autonomous agent has to learn behaviors in an unknown\nenvironment. In traditional reinforcement learning, these behaviors are learned\nthrough repeated interactions with the environment by optimizing an expected\nutility. In order to endow the agent with the ability to closely mimic the\nbehavior of human users, we optimize a CPT-based cost. We introduce the notion\nof the CPT-value of an action taken in a state, and establish the convergence\nof an iterative dynamic programming-based approach to estimate this quantity.\nWe develop two algorithms to enable agents to learn policies to optimize the\nCPT-vale, and evaluate these algorithms in environments where a target state\nhas to be reached while avoiding obstacles. We demonstrate that behaviors of\nthe agent learned using these algorithms are better aligned with that of a\nhuman user who might be placed in the same environment, and is significantly\nimproved over a baseline that optimizes an expected utility.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 20:35:25 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Ramasubramanian", "Bhaskar", ""], ["Niu", "Luyao", ""], ["Clark", "Andrew", ""], ["Poovendran", "Radha", ""]]}, {"id": "2104.00558", "submitter": "Constantine Lignos", "authors": "Jonne S\\\"alev\\\"a and Constantine Lignos", "title": "Mining Wikidata for Name Resources for African Languages", "comments": "Accepted at the EACL 2021 AfricaNLP workshop (non-archival)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work supports further development of language technology for the\nlanguages of Africa by providing a Wikidata-derived resource of name lists\ncorresponding to common entity types (person, location, and organization).\nWhile we are not the first to mine Wikidata for name lists, our approach\nemphasizes scalability and replicability and addresses data quality issues for\nlanguages that do not use Latin scripts. We produce lists containing\napproximately 1.9 million names across 28 African languages. We describe the\ndata, the process used to produce it, and its limitations, and provide the\nsoftware and data for public use. Finally, we discuss the ethical\nconsiderations of producing this resource and others of its kind.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 15:34:53 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["S\u00e4lev\u00e4", "Jonne", ""], ["Lignos", "Constantine", ""]]}, {"id": "2104.00563", "submitter": "Roger Girgis", "authors": "Roger Girgis, Florian Golemo, Felipe Codevilla, Jim Aldon D'Souza,\n  Martin Weiss, Samira Ebrahimi Kahou, Felix Heide, Christopher Pal", "title": "Autobots: Latent Variable Sequential Set Transformers", "comments": "21 pages, 15 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Robust multi-agent trajectory prediction is essential for the safe control of\nrobots and vehicles that interact with humans. Many existing methods treat\nsocial and temporal information separately and therefore fall short of\nmodelling the joint future trajectories of all agents in a socially consistent\nway. To address this, we propose a new class of Latent Variable Sequential Set\nTransformers which autoregressively model multi-agent trajectories. We refer to\nthese architectures as \"AutoBots\". AutoBots model the contents of sets (e.g.\nrepresenting the properties of agents in a scene) over time and employ\nmulti-head self-attention blocks over these sequences of sets to encode the\nsociotemporal relationships between the different actors of a scene. This\nproduces either the trajectory of one ego-agent or a distribution over the\nfuture trajectories for all agents under consideration. Our approach works for\ngeneral sequences of sets and we provide illustrative experiments modelling the\nsequential structure of the multiple strokes that make up symbols in the\nOmniglot data. For the single-agent prediction case, we validate our model on\nthe NuScenes motion prediction task and achieve competitive results on the\nglobal leaderboard. In the multi-agent forecasting setting, we validate our\nmodel on TrajNet. We find that our method outperforms physical extrapolation\nand recurrent network baselines and generates scene-consistent trajectories.\n", "versions": [{"version": "v1", "created": "Fri, 19 Feb 2021 18:53:26 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 19:06:13 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Girgis", "Roger", ""], ["Golemo", "Florian", ""], ["Codevilla", "Felipe", ""], ["D'Souza", "Jim Aldon", ""], ["Weiss", "Martin", ""], ["Kahou", "Samira Ebrahimi", ""], ["Heide", "Felix", ""], ["Pal", "Christopher", ""]]}, {"id": "2104.00564", "submitter": "Mauro Martini", "authors": "Mauro Martini, Vittorio Mazzia, Aleem Khaliq, Marcello Chiaberge", "title": "Domain-Adversarial Training of Self-Attention Based Networks for Land\n  Cover Classification using Multi-temporal Sentinel-2 Satellite Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The increasing availability of large-scale remote sensing labeled data has\nprompted researchers to develop increasingly precise and accurate data-driven\nmodels for land cover and crop classification (LC&CC). Moreover, with the\nintroduction of self-attention and introspection mechanisms, deep learning\napproaches have shown promising results in processing long temporal sequences\nin the multi-spectral domain with a contained computational request.\nNevertheless, most practical applications cannot rely on labeled data, and in\nthe field, surveys are a time consuming solution that poses strict limitations\nto the number of collected samples. Moreover, atmospheric conditions and\nspecific geographical region characteristics constitute a relevant domain gap\nthat does not allow direct applicability of a trained model on the available\ndataset to the area of interest. In this paper, we investigate adversarial\ntraining of deep neural networks to bridge the domain discrepancy between\ndistinct geographical zones. In particular, we perform a thorough analysis of\ndomain adaptation applied to challenging multi-spectral, multi-temporal data,\naccurately highlighting the advantages of adapting state-of-the-art\nself-attention based models for LC&CC to different target zones where labeled\ndata are not available. Extensive experimentation demonstrated significant\nperformance and generalization gain in applying domain-adversarial training to\nsource and target regions with marked dissimilarities between the distribution\nof extracted features.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 15:45:17 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 14:30:42 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Martini", "Mauro", ""], ["Mazzia", "Vittorio", ""], ["Khaliq", "Aleem", ""], ["Chiaberge", "Marcello", ""]]}, {"id": "2104.00606", "submitter": "Jessica Zosa Forde", "authors": "Jessica Zosa Forde, A. Feder Cooper, Kweku Kwegyir-Aggrey, Chris De Sa\n  and Michael Littman", "title": "Model Selection's Disparate Impact in Real-World Deep Learning\n  Applications", "comments": "Accepted to the Science and Engineering of Deep Learning Workshop,\n  ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Algorithmic fairness has emphasized the role of biased data in automated\ndecision outcomes. Recently, there has been a shift in attention to sources of\nbias that implicate fairness in other stages in the ML pipeline. We contend\nthat one source of such bias, human preferences in model selection, remains\nunder-explored in terms of its role in disparate impact across demographic\ngroups. Using a deep learning model trained on real-world medical imaging data,\nwe verify our claim empirically and argue that choice of metric for model\ncomparison can significantly bias model selection outcomes.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 16:37:01 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Forde", "Jessica Zosa", ""], ["Cooper", "A. Feder", ""], ["Kwegyir-Aggrey", "Kweku", ""], ["De Sa", "Chris", ""], ["Littman", "Michael", ""]]}, {"id": "2104.00615", "submitter": "Roman Popovych", "authors": "Alex Bihlo and Roman O. Popovych", "title": "Physics-informed neural networks for the shallow-water equations on the\n  sphere", "comments": "20 pages, 7 figures, 3 tables, minor corrections", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.AI cs.LG cs.NA math.NA physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the use of physics-informed neural networks for solving the\nshallow-water equations on the sphere. Physics-informed neural networks are\ntrained to satisfy the differential equations along with the prescribed initial\nand boundary data, and thus can be seen as an alternative approach to solving\ndifferential equations compared to traditional numerical approaches such as\nfinite difference, finite volume or spectral methods. We discuss the training\ndifficulties of physics-informed neural networks for the shallow-water\nequations on the sphere and propose a simple multi-model approach to tackle\ntest cases of comparatively long time intervals. We illustrate the abilities of\nthe method by solving the most prominent test cases proposed by Williamson et\nal. [J. Comput. Phys. 102, 211-224, 1992].\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 16:47:40 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 07:31:22 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Bihlo", "Alex", ""], ["Popovych", "Roman O.", ""]]}, {"id": "2104.00620", "submitter": "Karush Suri", "authors": "Karush Suri, Xiao Qi Shi, Konstantinos Plataniotis, Yuri Lawryshyn", "title": "TradeR: Practical Deep Hierarchical Reinforcement Learning for Trade\n  Execution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.TR cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Advances in Reinforcement Learning (RL) span a wide variety of applications\nwhich motivate development in this area. While application tasks serve as\nsuitable benchmarks for real world problems, RL is seldomly used in practical\nscenarios consisting of abrupt dynamics. This allows one to rethink the problem\nsetup in light of practical challenges. We present Trade Execution using\nReinforcement Learning (TradeR) which aims to address two such practical\nchallenges of catastrophy and surprise minimization by formulating trading as a\nreal-world hierarchical RL problem. Through this lens, TradeR makes use of\nhierarchical RL to execute trade bids on high frequency real market experiences\ncomprising of abrupt price variations during the 2019 fiscal year COVID19 stock\nmarket crash. The framework utilizes an energy-based scheme in conjunction with\nsurprise value function for estimating and minimizing surprise. In a\nlarge-scale study of 35 stock symbols from the S&P500 index, TradeR\ndemonstrates robustness to abrupt price changes and catastrophic losses while\nmaintaining profitable outcomes. We hope that our work serves as a motivating\nexample for application of RL to practical problems.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2021 19:52:52 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Suri", "Karush", ""], ["Shi", "Xiao Qi", ""], ["Plataniotis", "Konstantinos", ""], ["Lawryshyn", "Yuri", ""]]}, {"id": "2104.00624", "submitter": "Minsu Kang", "authors": "Minsu Kang, Jihyun Lee, Simin Kim and Injung Kim", "title": "Fast DCTTS: Efficient Deep Convolutional Text-to-Speech", "comments": "5 pages, 1 figure, to be published in IEEE International Conference\n  on Acoustics, Speech and Signal Processing (ICASSP) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose an end-to-end speech synthesizer, Fast DCTTS, that synthesizes\nspeech in real time on a single CPU thread. The proposed model is composed of a\ncarefully-tuned lightweight network designed by applying multiple network\nreduction and fidelity improvement techniques. In addition, we propose a novel\ngroup highway activation that can compromise between computational efficiency\nand the regularization effect of the gating mechanism. As well, we introduce a\nnew metric called Elastic mel-cepstral distortion (EMCD) to measure the\nfidelity of the output mel-spectrogram. In experiments, we analyze the effect\nof the acceleration techniques on speed and speech quality. Compared with the\nbaseline model, the proposed model exhibits improved MOS from 2.62 to 2.74 with\nonly 1.76% computation and 2.75% parameters. The speed on a single CPU thread\nwas improved by 7.45 times, which is fast enough to produce mel-spectrogram in\nreal time without GPU.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 17:08:01 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Kang", "Minsu", ""], ["Lee", "Jihyun", ""], ["Kim", "Simin", ""], ["Kim", "Injung", ""]]}, {"id": "2104.00631", "submitter": "Joshua Gruenstein", "authors": "Joshua Gruenstein, Tao Chen, Neel Doshi, Pulkit Agrawal", "title": "Residual Model Learning for Microrobot Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A majority of microrobots are constructed using compliant materials that are\ndifficult to model analytically, limiting the utility of traditional\nmodel-based controllers. Challenges in data collection on microrobots and large\nerrors between simulated models and real robots make current model-based\nlearning and sim-to-real transfer methods difficult to apply. We propose a\nnovel framework residual model learning (RML) that leverages approximate models\nto substantially reduce the sample complexity associated with learning an\naccurate robot model. We show that using RML, we can learn a model of the\nHarvard Ambulatory MicroRobot (HAMR) using just 12 seconds of passively\ncollected interaction data. The learned model is accurate enough to be\nleveraged as \"proxy-simulator\" for learning walking and turning behaviors using\nmodel-free reinforcement learning algorithms. RML provides a general framework\nfor learning from extremely small amounts of interaction data, and our\nexperiments with HAMR clearly demonstrate that RML substantially outperforms\nexisting techniques.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 17:22:50 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Gruenstein", "Joshua", ""], ["Chen", "Tao", ""], ["Doshi", "Neel", ""], ["Agrawal", "Pulkit", ""]]}, {"id": "2104.00639", "submitter": "Rafel Palliser Sans", "authors": "Rafel Palliser-Sans, Albert Rial-Farr\\`as", "title": "HLE-UPC at SemEval-2021 Task 5: Multi-Depth DistilBERT for Toxic Spans\n  Detection", "comments": "7 pages, SemEval-2021 Workshop, ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents our submission to SemEval-2021 Task 5: Toxic Spans\nDetection. The purpose of this task is to detect the spans that make a text\ntoxic, which is a complex labour for several reasons. Firstly, because of the\nintrinsic subjectivity of toxicity, and secondly, due to toxicity not always\ncoming from single words like insults or offends, but sometimes from whole\nexpressions formed by words that may not be toxic individually. Following this\nidea of focusing on both single words and multi-word expressions, we study the\nimpact of using a multi-depth DistilBERT model, which uses embeddings from\ndifferent layers to estimate the final per-token toxicity. Our quantitative\nresults show that using information from multiple depths boosts the performance\nof the model. Finally, we also analyze our best model qualitatively.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 17:37:38 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 11:05:54 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Palliser-Sans", "Rafel", ""], ["Rial-Farr\u00e0s", "Albert", ""]]}, {"id": "2104.00640", "submitter": "James Thorne", "authors": "James Thorne, Max Glockner, Gisela Vallejo, Andreas Vlachos, Iryna\n  Gurevych", "title": "Evidence-based Verification for Real World Information Needs", "comments": "Code and Data\n  https://github.com/CambridgeNLIP/verification-real-world-info-needs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Claim verification is the task of predicting the veracity of written\nstatements against evidence. Previous large-scale datasets model the task as\nclassification, ignoring the need to retrieve evidence, or are constructed for\nresearch purposes, and may not be representative of real-world needs. In this\npaper, we introduce a novel claim verification dataset with instances derived\nfrom search-engine queries, yielding 10,987 claims annotated with evidence that\nrepresent real-world information needs. For each claim, we annotate evidence\nfrom full Wikipedia articles with both section and sentence-level granularity.\nOur annotation allows comparison between two complementary approaches to\nverification: stance classification, and evidence extraction followed by\nentailment recognition. In our comprehensive evaluation, we find no significant\ndifference in accuracy between these two approaches. This enables systems to\nuse evidence extraction to summarize a rationale for an end-user while\nmaintaining the accuracy when predicting a claim's veracity. With challenging\nclaims and evidence documents containing hundreds of sentences, our dataset\npresents interesting challenges that are not captured in previous work --\nevidenced through transfer learning experiments. We release code and data to\nsupport further research on this task.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 17:40:08 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Thorne", "James", ""], ["Glockner", "Max", ""], ["Vallejo", "Gisela", ""], ["Vlachos", "Andreas", ""], ["Gurevych", "Iryna", ""]]}, {"id": "2104.00676", "submitter": "Zhiqiang Shen", "authors": "Zhiqiang Shen and Zechun Liu and Dejia Xu and Zitian Chen and\n  Kwang-Ting Cheng and Marios Savvides", "title": "Is Label Smoothing Truly Incompatible with Knowledge Distillation: An\n  Empirical Study", "comments": "ICLR 2021. Project page:\n  http://zhiqiangshen.com/projects/LS_and_KD/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work aims to empirically clarify a recently discovered perspective that\nlabel smoothing is incompatible with knowledge distillation. We begin by\nintroducing the motivation behind on how this incompatibility is raised, i.e.,\nlabel smoothing erases relative information between teacher logits. We provide\na novel connection on how label smoothing affects distributions of semantically\nsimilar and dissimilar classes. Then we propose a metric to quantitatively\nmeasure the degree of erased information in sample's representation. After\nthat, we study its one-sidedness and imperfection of the incompatibility view\nthrough massive analyses, visualizations and comprehensive experiments on Image\nClassification, Binary Networks, and Neural Machine Translation. Finally, we\nbroadly discuss several circumstances wherein label smoothing will indeed lose\nits effectiveness. Project page:\nhttp://zhiqiangshen.com/projects/LS_and_KD/index.html.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 17:59:12 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Shen", "Zhiqiang", ""], ["Liu", "Zechun", ""], ["Xu", "Dejia", ""], ["Chen", "Zitian", ""], ["Cheng", "Kwang-Ting", ""], ["Savvides", "Marios", ""]]}, {"id": "2104.00677", "submitter": "Ajay Jain", "authors": "Ajay Jain and Matthew Tancik and Pieter Abbeel", "title": "Putting NeRF on a Diet: Semantically Consistent Few-Shot View Synthesis", "comments": "Project website: https://www.ajayj.com/dietnerf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present DietNeRF, a 3D neural scene representation estimated from a few\nimages. Neural Radiance Fields (NeRF) learn a continuous volumetric\nrepresentation of a scene through multi-view consistency, and can be rendered\nfrom novel viewpoints by ray casting. While NeRF has an impressive ability to\nreconstruct geometry and fine details given many images, up to 100 for\nchallenging 360{\\deg} scenes, it often finds a degenerate solution to its image\nreconstruction objective when only a few input views are available. To improve\nfew-shot quality, we propose DietNeRF. We introduce an auxiliary semantic\nconsistency loss that encourages realistic renderings at novel poses. DietNeRF\nis trained on individual scenes to (1) correctly render given input views from\nthe same pose, and (2) match high-level semantic attributes across different,\nrandom poses. Our semantic loss allows us to supervise DietNeRF from arbitrary\nposes. We extract these semantics using a pre-trained visual encoder such as\nCLIP, a Vision Transformer trained on hundreds of millions of diverse\nsingle-view, 2D photographs mined from the web with natural language\nsupervision. In experiments, DietNeRF improves the perceptual quality of\nfew-shot view synthesis when learned from scratch, can render novel views with\nas few as one observed image when pre-trained on a multi-view dataset, and\nproduces plausible completions of completely unobserved regions.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 17:59:31 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Jain", "Ajay", ""], ["Tancik", "Matthew", ""], ["Abbeel", "Pieter", ""]]}, {"id": "2104.00682", "submitter": "Christoph Feichtenhofer", "authors": "Bo Xiong, Haoqi Fan, Kristen Grauman, Christoph Feichtenhofer", "title": "Multiview Pseudo-Labeling for Semi-supervised Learning from Video", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a multiview pseudo-labeling approach to video learning, a novel\nframework that uses complementary views in the form of appearance and motion\ninformation for semi-supervised learning in video. The complementary views help\nobtain more reliable pseudo-labels on unlabeled video, to learn stronger video\nrepresentations than from purely supervised data. Though our method capitalizes\non multiple views, it nonetheless trains a model that is shared across\nappearance and motion input and thus, by design, incurs no additional\ncomputation overhead at inference time. On multiple video recognition datasets,\nour method substantially outperforms its supervised counterpart, and compares\nfavorably to previous work on standard benchmarks in self-supervised video\nrepresentation learning.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 17:59:48 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Xiong", "Bo", ""], ["Fan", "Haoqi", ""], ["Grauman", "Kristen", ""], ["Feichtenhofer", "Christoph", ""]]}, {"id": "2104.00698", "submitter": "Anssi Kanervisto", "authors": "Dylan Ashley, Anssi Kanervisto, Brendan Bennett", "title": "Back to Square One: Superhuman Performance in Chutes and Ladders Through\n  Deep Neural Networks and Tree Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present AlphaChute: a state-of-the-art algorithm that achieves superhuman\nperformance in the ancient game of Chutes and Ladders. We prove that our\nalgorithm converges to the Nash equilibrium in constant time, and therefore is\n-- to the best of our knowledge -- the first such formal solution to this game.\nSurprisingly, despite all this, our implementation of AlphaChute remains\nrelatively straightforward due to domain-specific adaptations. We provide the\nsource code for AlphaChute here in our Appendix.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 18:08:55 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Ashley", "Dylan", ""], ["Kanervisto", "Anssi", ""], ["Bennett", "Brendan", ""]]}, {"id": "2104.00705", "submitter": "Qing He", "authors": "Qing He, Zhiping Xiu, Thilo Koehler, Jilong Wu", "title": "Multi-rate attention architecture for fast streamable Text-to-speech\n  spectrum modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Typical high quality text-to-speech (TTS) systems today use a two-stage\narchitecture, with a spectrum model stage that generates spectral frames and a\nvocoder stage that generates the actual audio. High-quality spectrum models\nusually incorporate the encoder-decoder architecture with self-attention or\nbi-directional long short-term (BLSTM) units. While these models can produce\nhigh quality speech, they often incur O($L$) increase in both latency and\nreal-time factor (RTF) with respect to input length $L$. In other words, longer\ninputs leads to longer delay and slower synthesis speed, limiting its use in\nreal-time applications. In this paper, we propose a multi-rate attention\narchitecture that breaks the latency and RTF bottlenecks by computing a compact\nrepresentation during encoding and recurrently generating the attention vector\nin a streaming manner during decoding. The proposed architecture achieves high\naudio quality (MOS of 4.31 compared to groundtruth 4.48), low latency, and low\nRTF at the same time. Meanwhile, both latency and RTF of the proposed system\nstay constant regardless of input lengths, making it ideal for real-time\napplications.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 18:15:30 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["He", "Qing", ""], ["Xiu", "Zhiping", ""], ["Koehler", "Thilo", ""], ["Wu", "Jilong", ""]]}, {"id": "2104.00721", "submitter": "Zaharah A. Bukhsh", "authors": "Zaharah A. Bukhsh, Aaqib Saeed, Remco M. Dijkman", "title": "ProcessTransformer: Predictive Business Process Monitoring with\n  Transformer Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Predictive business process monitoring focuses on predicting future\ncharacteristics of a running process using event logs. The foresight into\nprocess execution promises great potentials for efficient operations, better\nresource management, and effective customer services. Deep learning-based\napproaches have been widely adopted in process mining to address the\nlimitations of classical algorithms for solving multiple problems, especially\nthe next event and remaining-time prediction tasks. Nevertheless, designing a\ndeep neural architecture that performs competitively across various tasks is\nchallenging as existing methods fail to capture long-range dependencies in the\ninput sequences and perform poorly for lengthy process traces. In this paper,\nwe propose ProcessTransformer, an approach for learning high-level\nrepresentations from event logs with an attention-based network. Our model\nincorporates long-range memory and relies on a self-attention mechanism to\nestablish dependencies between a multitude of event sequences and corresponding\noutputs. We evaluate the applicability of our technique on nine real event\nlogs. We demonstrate that the transformer-based model outperforms several\nbaselines of prior techniques by obtaining on average above 80% accuracy for\nthe task of predicting the next activity. Our method also perform\ncompetitively, compared to baselines, for the tasks of predicting event time\nand remaining time of a running case\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 18:58:46 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Bukhsh", "Zaharah A.", ""], ["Saeed", "Aaqib", ""], ["Dijkman", "Remco M.", ""]]}, {"id": "2104.00722", "submitter": "Avoy Datta", "authors": "Heejung W. Chung, Avoy Datta, Chris Waites", "title": "GABO: Graph Augmentations with Bi-level Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Data augmentation refers to a wide range of techniques for improving model\ngeneralization by augmenting training examples. Oftentimes such methods require\ndomain knowledge about the dataset at hand, spawning a plethora of recent\nliterature surrounding automated techniques for data augmentation. In this work\nwe apply one such method, bilevel optimization, to tackle the problem of graph\nclassification on the ogbg-molhiv dataset. Our best performing augmentation\nachieved a test ROCAUC score of 77.77 % with a GIN+virtual classifier, which\nmakes it the most effective augmenter for this classifier on the leaderboard.\nThis framework combines a GIN layer augmentation generator with a bias\ntransformation and outperforms the same classifier augmented using the\nstate-of-the-art FLAG augmentation.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 19:00:17 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Chung", "Heejung W.", ""], ["Datta", "Avoy", ""], ["Waites", "Chris", ""]]}, {"id": "2104.00739", "submitter": "Gopal P. Sarma", "authors": "Gopal Sarma, James Koppel, Gregory Malecha, Patrick Schultz, Eric\n  Drexler, Ramana Kumar, Cody Roux, and Philip Zucker", "title": "Formal Methods for the Informal Engineer: Workshop Recommendations", "comments": "6 pages", "journal-ref": null, "doi": "10.31219/osf.io/t4qs8", "report-no": null, "categories": "cs.SE cs.AI cs.LG cs.PL q-bio.OT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Formal Methods for the Informal Engineer (FMIE) was a workshop held at the\nBroad Institute of MIT and Harvard in 2021 to explore the potential role of\nverified software in the biomedical software ecosystem. The motivation for\norganizing FMIE was the recognition that the life sciences and medicine are\nundergoing a transition from being passive consumers of software and AI/ML\ntechnologies to fundamental drivers of new platforms, including those which\nwill need to be mission and safety-critical. Drawing on conversations leading\nup to and during the workshop, we make five concrete recommendations to help\nsoftware leaders organically incorporate tools, techniques, and perspectives\nfrom formal methods into their project planning and development trajectories.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 19:22:42 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Sarma", "Gopal", ""], ["Koppel", "James", ""], ["Malecha", "Gregory", ""], ["Schultz", "Patrick", ""], ["Drexler", "Eric", ""], ["Kumar", "Ramana", ""], ["Roux", "Cody", ""], ["Zucker", "Philip", ""]]}, {"id": "2104.00743", "submitter": "Tanmay Gupta", "authors": "Tanmay Gupta, Amita Kamath, Aniruddha Kembhavi and Derek Hoiem", "title": "Towards General Purpose Vision Systems", "comments": "Project page: https://prior.allenai.org/projects/gpv", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A special purpose learning system assumes knowledge of admissible tasks at\ndesign time. Adapting such a system to unforeseen tasks requires architecture\nmanipulation such as adding an output head for each new task or dataset. In\nthis work, we propose a task-agnostic vision-language system that accepts an\nimage and a natural language task description and outputs bounding boxes,\nconfidences, and text. The system supports a wide range of vision tasks such as\nclassification, localization, question answering, captioning, and more. We\nevaluate the system's ability to learn multiple skills simultaneously, to\nperform tasks with novel skill-concept combinations, and to learn new skills\nefficiently and without forgetting.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 19:35:21 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Gupta", "Tanmay", ""], ["Kamath", "Amita", ""], ["Kembhavi", "Aniruddha", ""], ["Hoiem", "Derek", ""]]}, {"id": "2104.00807", "submitter": "Hilmi Enes Egilmez", "authors": "Ankitesh K. Singh, Hilmi E. Egilmez, Reza Pourreza, Muhammed Coban,\n  Marta Karczewicz, Taco S. Cohen", "title": "A Combined Deep Learning based End-to-End Video Coding Architecture for\n  YUV Color Space", "comments": "5 pages, submitted to as a conference paper. arXiv admin note: text\n  overlap with arXiv:2103.01760", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the existing deep learning based end-to-end video coding (DLEC)\narchitectures are designed specifically for RGB color format, yet the video\ncoding standards, including H.264/AVC, H.265/HEVC and H.266/VVC developed over\npast few decades, have been designed primarily for YUV 4:2:0 format, where the\nchrominance (U and V) components are subsampled to achieve superior compression\nperformances considering the human visual system. While a broad number of\npapers on DLEC compare these two distinct coding schemes in RGB domain, it is\nideal to have a common evaluation framework in YUV 4:2:0 domain for a more fair\ncomparison. This paper introduces a new DLEC architecture for video coding to\neffectively support YUV 4:2:0 and compares its performance against the HEVC\nstandard under a common evaluation framework. The experimental results on YUV\n4:2:0 video sequences show that the proposed architecture can outperform HEVC\nin intra-frame coding, however inter-frame coding is not as efficient on\ncontrary to the RGB coding results reported in recent papers.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 23:41:06 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Singh", "Ankitesh K.", ""], ["Egilmez", "Hilmi E.", ""], ["Pourreza", "Reza", ""], ["Coban", "Muhammed", ""], ["Karczewicz", "Marta", ""], ["Cohen", "Taco S.", ""]]}, {"id": "2104.00835", "submitter": "Juntao Wang Mr", "authors": "Yiling Chen, Alon Eden, Juntao Wang", "title": "Cursed yet Satisfied Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI econ.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In real life auctions, a widely observed phenomenon is the winner's curse --\nthe winner's high bid implies that the winner often over-estimates the value of\nthe good for sale, resulting in an incurred negative utility. The seminal work\nof Eyster and Rabin [Econometrica'05] introduced a behavioral model aimed to\nexplain this observed anomaly. We term agents who display this bias \"cursed\nagents\". We adopt their model in the interdependent value setting, and aim to\ndevise mechanisms that prevent the cursed agents from obtaining negative\nutility. We design mechanisms that are cursed ex-post IC, that is, incentivize\nagents to bid their true signal even though they are cursed, while ensuring\nthat the outcome is individually rational -- the price the agents pay is no\nmore than the agents' true value.\n  Since the agents might over-estimate the good's value, such mechanisms might\nrequire the seller to make positive transfers to the agents to prevent agents\nfrom over-paying. For revenue maximization, we give the optimal deterministic\nand anonymous mechanism. For welfare maximization, we require ex-post budget\nbalance (EPBB), as positive transfers might lead to negative revenue. We\npropose a masking operation that takes any deterministic mechanism, and imposes\nthat the seller would not make positive transfers, enforcing EPBB. We show that\nin typical settings, EPBB implies that the mechanism cannot make any positive\ntransfers, implying that applying the masking operation on the fully efficient\nmechanism results in a socially optimal EPBB mechanism. This further implies\nthat if the valuation function is the maximum of agents' signals, the optimal\nEPBB mechanism obtains zero welfare. In contrast, we show that for sum-concave\nvaluations, which include weighted-sum valuations and l_p-norms, the welfare\noptimal EPBB mechanism obtains half of the optimal welfare as the number of\nagents grows large.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 01:15:53 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 01:47:12 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Chen", "Yiling", ""], ["Eden", "Alon", ""], ["Wang", "Juntao", ""]]}, {"id": "2104.00850", "submitter": "Loris Nanni", "authors": "Alessandra Lumini, Loris Nanni and Gianluca Maguolo", "title": "Deep ensembles based on Stochastic Activation Selection for Polyp\n  Segmentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semantic segmentation has a wide array of applications ranging from\nmedical-image analysis, scene understanding, autonomous driving and robotic\nnavigation. This work deals with medical image segmentation and in particular\nwith accurate polyp detection and segmentation during colonoscopy examinations.\nSeveral convolutional neural network architectures have been proposed to\neffectively deal with this task and with the problem of segmenting objects at\ndifferent scale input. The basic architecture in image segmentation consists of\nan encoder and a decoder: the first uses convolutional filters to extract\nfeatures from the image, the second is responsible for generating the final\noutput. In this work, we compare some variant of the DeepLab architecture\nobtained by varying the decoder backbone. We compare several decoder\narchitectures, including ResNet, Xception, EfficentNet, MobileNet and we\nperturb their layers by substituting ReLU activation layers with other\nfunctions. The resulting methods are used to create deep ensembles which are\nshown to be very effective. Our experimental evaluations show that our best\nensemble produces good segmentation results by achieving high evaluation scores\nwith a dice coefficient of 0.884, and a mean Intersection over Union (mIoU) of\n0.818 for the Kvasir-SEG dataset. To improve reproducibility and research\nefficiency the MATLAB source code used for this research is available at\nGitHub: https://github.com/LorisNanni.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 02:07:37 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 21:31:56 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Lumini", "Alessandra", ""], ["Nanni", "Loris", ""], ["Maguolo", "Gianluca", ""]]}, {"id": "2104.00853", "submitter": "Hao Peng", "authors": "Hao Peng, Jianxin Li, Yangqiu Song, Renyu Yang, Rajiv Ranjan, Philip\n  S. Yu, Lifang He", "title": "Streaming Social Event Detection and Evolution Discovery in\n  Heterogeneous Information Networks", "comments": "Accepted by TKDD 2021. arXiv admin note: text overlap with\n  arXiv:1906.04580", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Events are happening in real-world and real-time, which can be planned and\norganized for occasions, such as social gatherings, festival celebrations,\ninfluential meetings or sports activities. Social media platforms generate a\nlot of real-time text information regarding public events with different\ntopics. However, mining social events is challenging because events typically\nexhibit heterogeneous texture and metadata are often ambiguous. In this paper,\nwe first design a novel event-based meta-schema to characterize the semantic\nrelatedness of social events and then build an event-based heterogeneous\ninformation network (HIN) integrating information from external knowledge base.\nSecond, we propose a novel Pairwise Popularity Graph Convolutional Network,\nnamed as PP-GCN, based on weighted meta-path instance similarity and textual\nsemantic representation as inputs, to perform fine-grained social event\ncategorization and learn the optimal weights of meta-paths in different tasks.\nThird, we propose a streaming social event detection and evolution discovery\nframework for HINs based on meta-path similarity search, historical information\nabout meta-paths, and heterogeneous DBSCAN clustering method. Comprehensive\nexperiments on real-world streaming social text data are conducted to compare\nvarious social event detection and evolution discovery algorithms. Experimental\nresults demonstrate that our proposed framework outperforms other alternative\nsocial event detection and evolution discovery techniques.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 02:13:10 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Peng", "Hao", ""], ["Li", "Jianxin", ""], ["Song", "Yangqiu", ""], ["Yang", "Renyu", ""], ["Ranjan", "Rajiv", ""], ["Yu", "Philip S.", ""], ["He", "Lifang", ""]]}, {"id": "2104.00859", "submitter": "Hanna Kurniawati", "authors": "Jimy Cai Huang and Hanna Kurniawati", "title": "An NCAP-like Safety Indicator for Self-Driving Cars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a mechanism to assess the safety of autonomous cars. It\nassesses the car's safety in scenarios where the car must avoid collision with\nan adversary. Core to this mechanism is a safety measure, called Safe-Kamikaze\nDistance (SKD), which computes the average similarity between sets of safe\nadversary's trajectories and kamikaze trajectories close to the safe\ntrajectories. The kamikaze trajectories are generated based on planning under\nuncertainty techniques, namely the Partially Observable Markov Decision\nProcesses, to account for the partially observed car policy from the point of\nview of the adversary. We found that SKD is inversely proportional to the upper\nbound on the probability that a small deformation changes a collision-free\ntrajectory of the adversary into a colliding one. We perform systematic tests\non a scenario where the adversary is a pedestrian crossing a single-lane road\nin front of the car being assessed --which is, one of the scenarios in the\nEuro-NCAP's Vulnerable Road User (VRU) tests on Autonomous Emergency Braking.\nSimulation results on assessing cars with basic controllers and a test on a\nMachine-Learning controller using a high-fidelity simulator indicates promising\nresults for SKD to measure the safety of autonomous cars. Moreover, the time\ntaken for each simulation test is under 11 seconds, enabling a sufficient\nstatistics to compute SKD from simulation to be generated on a quad-core\ndesktop in less than 25 minutes.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 02:39:53 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Huang", "Jimy Cai", ""], ["Kurniawati", "Hanna", ""]]}, {"id": "2104.00862", "submitter": "Lianghua Huang Dr.", "authors": "Lianghua Huang, Yu Liu, Bin Wang, Pan Pan, Yinghui Xu, Rong Jin", "title": "Self-supervised Video Representation Learning by Context and Motion\n  Decoupling", "comments": "Accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key challenge in self-supervised video representation learning is how to\neffectively capture motion information besides context bias. While most\nexisting works implicitly achieve this with video-specific pretext tasks (e.g.,\npredicting clip orders, time arrows, and paces), we develop a method that\nexplicitly decouples motion supervision from context bias through a carefully\ndesigned pretext task. Specifically, we take the keyframes and motion vectors\nin compressed videos (e.g., in H.264 format) as the supervision sources for\ncontext and motion, respectively, which can be efficiently extracted at over\n500 fps on the CPU. Then we design two pretext tasks that are jointly\noptimized: a context matching task where a pairwise contrastive loss is cast\nbetween video clip and keyframe features; and a motion prediction task where\nclip features, passed through an encoder-decoder network, are used to estimate\nmotion features in a near future. These two tasks use a shared video backbone\nand separate MLP heads. Experiments show that our approach improves the quality\nof the learned video representation over previous works, where we obtain\nabsolute gains of 16.0% and 11.1% in video retrieval recall on UCF101 and\nHMDB51, respectively. Moreover, we find the motion prediction to be a strong\nregularization for video networks, where using it as an auxiliary task improves\nthe accuracy of action recognition with a margin of 7.4%~13.8%.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 02:47:34 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Huang", "Lianghua", ""], ["Liu", "Yu", ""], ["Wang", "Bin", ""], ["Pan", "Pan", ""], ["Xu", "Yinghui", ""], ["Jin", "Rong", ""]]}, {"id": "2104.00871", "submitter": "Jay Kumar", "authors": "Khwaja Mutahir Ahmad, Gang He, Wenxin Yu, Xiaochuan Xu, Jay Kumar,\n  Muhammad Asim Saleem", "title": "A Survey on Semi-parametric Machine Learning Technique for Time Series\n  Forecasting", "comments": "44 pages, 8 figures, journal paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Intelligence (AI) has recently shown its capabilities for almost\nevery field of life. Machine Learning, which is a subset of AI, is a `HOT'\ntopic for researchers. Machine Learning outperforms other classical forecasting\ntechniques in almost all-natural applications. It is a crucial part of modern\nresearch. As per this statement, Modern Machine Learning algorithms are hungry\nfor big data. Due to the small datasets, the researchers may not prefer to use\nMachine Learning algorithms. To tackle this issue, the main purpose of this\nsurvey is to illustrate, demonstrate related studies for significance of a\nsemi-parametric Machine Learning framework called Grey Machine Learning (GML).\nThis kind of framework is capable of handling large datasets as well as small\ndatasets for time series forecasting likely outcomes. This survey presents a\ncomprehensive overview of the existing semi-parametric machine learning\ntechniques for time series forecasting. In this paper, a primer survey on the\nGML framework is provided for researchers. To allow an in-depth understanding\nfor the readers, a brief description of Machine Learning, as well as various\nforms of conventional grey forecasting models are discussed. Moreover, a brief\ndescription on the importance of GML framework is presented.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 03:26:20 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Ahmad", "Khwaja Mutahir", ""], ["He", "Gang", ""], ["Yu", "Wenxin", ""], ["Xu", "Xiaochuan", ""], ["Kumar", "Jay", ""], ["Saleem", "Muhammad Asim", ""]]}, {"id": "2104.00872", "submitter": "Matvey Soloviev", "authors": "Matvey Soloviev, Joseph Y. Halpern", "title": "Security Properties as Nested Causal Statements", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CR cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thinking in terms of causality helps us structure how different parts of a\nsystem depend on each other, and how interventions on one part of a system may\nresult in changes to other parts. Therefore, formal models of causality are an\nattractive tool for reasoning about security, which concerns itself with\nsafeguarding properties of a system against interventions that may be\nmalicious. As we show, many security properties are naturally expressed as\nnested causal statements: not only do we consider what caused a particular\nundesirable effect, but we also consider what caused this causal relationship\nitself to hold. We present a natural way to extend the Halpern-Pearl (HP)\nframework for causality to capture such nested causal statements. This\nextension adds expressivity, enabling the HP framework to distinguish between\ncausal scenarios that it could not previously naturally tell apart. We moreover\nrevisit some design decisions of the HP framework that were made with\nnon-nested causal statements in mind, such as the choice to treat specific\nvalues of causal variables as opposed to the variables themselves as causes,\nand may no longer be appropriate for nested ones.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 03:29:00 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Soloviev", "Matvey", ""], ["Halpern", "Joseph Y.", ""]]}, {"id": "2104.00878", "submitter": "Yantian Zha", "authors": "Yantian Zha, Siddhant Bhambri and Lin Guan", "title": "Contrastively Learning Visual Attention as Affordance Cues from\n  Demonstrations for Robotic Grasping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Conventional works that learn grasping affordance from demonstrations need to\nexplicitly predict grasping configurations, such as gripper approaching angles\nor grasping preshapes. Classic motion planners could then sample trajectories\nby using such predicted configurations. In this work, our goal is instead to\nintegrate the two objectives of affordance discovery and affordance-aware\npolicy learning in an end-to-end imitation learning framework based on deep\nneural networks. From a psychological perspective, there is a close association\nbetween attention and affordance. Therefore, with an end-to-end neural network,\nwe propose to learn affordance cues as visual attention that serves as a useful\nindicating signal of how a demonstrator accomplishes tasks. To achieve this, we\npropose a contrastive learning framework that consists of a Siamese encoder and\na trajectory decoder. We further introduce a coupled triplet loss to encourage\nthe discovered affordance cues to be more affordance-relevant. Our experimental\nresults demonstrate that our model with the coupled triplet loss achieves the\nhighest grasping success rate.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 04:18:53 GMT"}, {"version": "v2", "created": "Sun, 11 Apr 2021 15:27:52 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Zha", "Yantian", ""], ["Bhambri", "Siddhant", ""], ["Guan", "Lin", ""]]}, {"id": "2104.00889", "submitter": "Mayank Patwari", "authors": "Wooram Kang, Mayank Patwari", "title": "Low Dose Helical CBCT denoising by using domain filtering with deep\n  reinforcement learning", "comments": "Research project report. 5 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Cone Beam Computed Tomography(CBCT) is a now known method to conduct CT\nimaging. Especially, The Low Dose CT imaging is one of possible options to\nprotect organs of patients when conducting CT imaging. Therefore Low Dose CT\nimaging can be an alternative instead of Standard dose CT imaging. However Low\nDose CT imaging has a fundamental issue with noises within results compared to\nStandard Dose CT imaging. Currently, there are lots of attempts to erase the\nnoises. Most of methods with artificial intelligence have many parameters and\nunexplained layers or a kind of black-box methods. Therefore, our research has\npurposes related to these issues. Our approach has less parameters than usual\nmethods by having Iterative learn-able bilateral filtering approach with Deep\nreinforcement learning. And we applied The Iterative learn-able filtering\napproach with deep reinforcement learning to sinograms and reconstructed volume\ndomains. The method and the results of the method can be much more explainable\nthan The other black box AI approaches. And we applied the method to Helical\nCone Beam Computed Tomography(CBCT), which is the recent CBCT trend. We tested\nthis method with on 2 abdominal scans(L004, L014) from Mayo Clinic TCIA\ndataset. The results and the performances of our approach overtake the results\nof the other previous methods.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 05:28:04 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Kang", "Wooram", ""], ["Patwari", "Mayank", ""]]}, {"id": "2104.00896", "submitter": "Vineeth Rakesh", "authors": "Vineeth Rakesh, Swayambhoo Jain", "title": "Efficacy of Bayesian Neural Networks in Active Learning", "comments": "Published at CVPR Workshop on Learning From Limited or Imperfect Data\n  (LLID) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Obtaining labeled data for machine learning tasks can be prohibitively\nexpensive. Active learning mitigates this issue by exploring the unlabeled data\nspace and prioritizing the selection of data that can best improve the model\nperformance. A common approach to active learning is to pick a small sample of\ndata for which the model is most uncertain. In this paper, we explore the\nefficacy of Bayesian neural networks for active learning, which naturally\nmodels uncertainty by learning distribution over the weights of neural\nnetworks. By performing a comprehensive set of experiments, we show that\nBayesian neural networks are more efficient than ensemble based techniques in\ncapturing uncertainty. Our findings also reveal some key drawbacks of the\nensemble techniques, which was recently shown to be more effective than Monte\nCarlo dropouts.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 06:02:11 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 22:19:53 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Rakesh", "Vineeth", ""], ["Jain", "Swayambhoo", ""]]}, {"id": "2104.00912", "submitter": "Frederic Le Mouel", "authors": "Michael Puentes (UIS), Diana Novoa, John Delgado Nivia (UTS), Carlos\n  Barrios Hern\\'andez (UIS), Oscar Carrillo (DYNAMID, CPE), Fr\\'ed\\'eric Le\n  Mou\\\"el (DYNAMID)", "title": "Datacentric analysis to reduce pedestrians accidents: A case study in\n  Colombia", "comments": null, "journal-ref": "International Conference on Sustainable Smart Cities and\n  Territories (SSCt2021), Apr 2021, Doha, Qatar", "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since 2012, in a case-study in Bucaramanga-Colombia, 179 pedestrians died in\ncar accidents, and another 2873 pedestrians were injured. Each day, at least\none passerby is involved in a tragedy. Knowing the causes to decrease accidents\nis crucial, and using system-dynamics to reproduce the collisions' events is\ncritical to prevent further accidents. This work implements simulations to save\nlives by reducing the city's accidental rate and suggesting new safety policies\nto implement. Simulation's inputs are video recordings in some areas of the\ncity. Deep Learning analysis of the images results in the segmentation of the\ndifferent objects in the scene, and an interaction model identifies the primary\nreasons which prevail in the pedestrians or vehicles' behaviours. The first and\nmost efficient safety policy to implement-validated by our simulations-would be\nto build speed bumps in specific places before the crossings reducing the\naccident rate by 80%.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 06:59:50 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Puentes", "Michael", "", "UIS"], ["Novoa", "Diana", "", "UTS"], ["Nivia", "John Delgado", "", "UTS"], ["Hern\u00e1ndez", "Carlos Barrios", "", "UIS"], ["Carrillo", "Oscar", "", "DYNAMID, CPE"], ["Mou\u00ebl", "Fr\u00e9d\u00e9ric Le", "", "DYNAMID"]]}, {"id": "2104.00929", "submitter": "Changying Hao", "authors": "Changying Hao, Liang Pang, Yanyan Lan, Yan Wang, Jiafeng Guo, Xueqi\n  Cheng", "title": "Sketch and Customize: A Counterfactual Story Generator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recent text generation models are easy to generate relevant and fluent text\nfor the given text, while lack of causal reasoning ability when we change some\nparts of the given text. Counterfactual story rewriting is a recently proposed\ntask to test the causal reasoning ability for text generation models, which\nrequires a model to predict the corresponding story ending when the condition\nis modified to a counterfactual one. Previous works have shown that the\ntraditional sequence-to-sequence model cannot well handle this problem, as it\noften captures some spurious correlations between the original and\ncounterfactual endings, instead of the causal relations between conditions and\nendings. To address this issue, we propose a sketch-and-customize generation\nmodel guided by the causality implicated in the conditions and endings. In the\nsketch stage, a skeleton is extracted by removing words which are conflict to\nthe counterfactual condition, from the original ending. In the customize stage,\na generation model is used to fill proper words in the skeleton under the\nguidance of the counterfactual condition. In this way, the obtained\ncounterfactual ending is both relevant to the original ending and consistent\nwith the counterfactual condition. Experimental results show that the proposed\nmodel generates much better endings, as compared with the traditional\nsequence-to-sequence model.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 08:14:22 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Hao", "Changying", ""], ["Pang", "Liang", ""], ["Lan", "Yanyan", ""], ["Wang", "Yan", ""], ["Guo", "Jiafeng", ""], ["Cheng", "Xueqi", ""]]}, {"id": "2104.00948", "submitter": "Angelo Salatino", "authors": "Angelo A. Salatino, Francesco Osborne, Thiviyan Thanapalasingam,\n  Enrico Motta", "title": "The CSO Classifier: Ontology-Driven Detection of Research Topics in\n  Scholarly Articles", "comments": "Conference paper at TPDL 2019", "journal-ref": "In Digital Libraries for Open Knowledge. LNCS, vol 11799.\n  Springer, Cham (2019)", "doi": "10.1007/978-3-030-30760-8_26", "report-no": null, "categories": "cs.IR cs.AI cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Classifying research papers according to their research topics is an\nimportant task to improve their retrievability, assist the creation of smart\nanalytics, and support a variety of approaches for analysing and making sense\nof the research environment. In this paper, we present the CSO Classifier, a\nnew unsupervised approach for automatically classifying research papers\naccording to the Computer Science Ontology (CSO), a comprehensive ontology of\nre-search areas in the field of Computer Science. The CSO Classifier takes as\ninput the metadata associated with a research paper (title, abstract, keywords)\nand returns a selection of research concepts drawn from the ontology. The\napproach was evaluated on a gold standard of manually annotated articles\nyielding a significant improvement over alternative methods.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 09:02:32 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Salatino", "Angelo A.", ""], ["Osborne", "Francesco", ""], ["Thanapalasingam", "Thiviyan", ""], ["Motta", "Enrico", ""]]}, {"id": "2104.00950", "submitter": "Thomas Rojat", "authors": "Thomas Rojat, Rapha\\\"el Puget, David Filliat, Javier Del Ser, Rodolphe\n  Gelin, and Natalia D\\'iaz-Rodr\\'iguez", "title": "Explainable Artificial Intelligence (XAI) on TimeSeries Data: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most of state of the art methods applied on time series consist of deep\nlearning methods that are too complex to be interpreted. This lack of\ninterpretability is a major drawback, as several applications in the real world\nare critical tasks, such as the medical field or the autonomous driving field.\nThe explainability of models applied on time series has not gather much\nattention compared to the computer vision or the natural language processing\nfields. In this paper, we present an overview of existing explainable AI (XAI)\nmethods applied on time series and illustrate the type of explanations they\nproduce. We also provide a reflection on the impact of these explanation\nmethods to provide confidence and trust in the AI systems.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 09:14:00 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Rojat", "Thomas", ""], ["Puget", "Rapha\u00ebl", ""], ["Filliat", "David", ""], ["Del Ser", "Javier", ""], ["Gelin", "Rodolphe", ""], ["D\u00edaz-Rodr\u00edguez", "Natalia", ""]]}, {"id": "2104.00983", "submitter": "Jo\\v{z}e Ro\\v{z}anec", "authors": "Jo\\v{z}e M. Ro\\v{z}anec, Patrik Zajec, Klemen Kenda, Inna Novalija,\n  Bla\\v{z} Fortuna, Dunja Mladeni\\'c, Entso Veliou, Dimitrios Papamartzivanos,\n  Thanassis Giannetsos, Sofia Anna Menesidou, Rub\\'en Alonso, Nino Cauli, Diego\n  Reforgiato Recupero, Dimosthenis Kyriazis, Georgios Sofianidis, Spyros\n  Theodoropoulos and John Soldatos", "title": "STARdom: an architecture for trusted and secure human-centered\n  manufacturing systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a lack of a single architecture specification that addresses the\nneeds of trusted and secure Artificial Intelligence systems with humans in the\nloop, such as human-centered manufacturing systems at the core of the evolution\ntowards Industry 5.0. To realize this, we propose an architecture that\nintegrates forecasts, Explainable Artificial Intelligence, supports collecting\nusers' feedback, and uses Active Learning and Simulated Reality to enhance\nforecasts and provide decision-making recommendations. The architecture\nsecurity is addressed as a general concern. We align the proposed architecture\nwith the Big Data Value Association Reference Architecture Model. We tailor it\nfor the domain of demand forecasting and validate it on a real-world case\nstudy.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 11:00:20 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Ro\u017eanec", "Jo\u017ee M.", ""], ["Zajec", "Patrik", ""], ["Kenda", "Klemen", ""], ["Novalija", "Inna", ""], ["Fortuna", "Bla\u017e", ""], ["Mladeni\u0107", "Dunja", ""], ["Veliou", "Entso", ""], ["Papamartzivanos", "Dimitrios", ""], ["Giannetsos", "Thanassis", ""], ["Menesidou", "Sofia Anna", ""], ["Alonso", "Rub\u00e9n", ""], ["Cauli", "Nino", ""], ["Recupero", "Diego Reforgiato", ""], ["Kyriazis", "Dimosthenis", ""], ["Sofianidis", "Georgios", ""], ["Theodoropoulos", "Spyros", ""], ["Soldatos", "John", ""]]}, {"id": "2104.01008", "submitter": "Hugo Cisneros", "authors": "Hugo Cisneros, Josef Sivic, Tomas Mikolov", "title": "Visualizing computation in large-scale cellular automata", "comments": null, "journal-ref": "Artificial Life Conference Proceedings 2020 (pp. 239-247). MIT\n  Press", "doi": "10.1162/isal_a_00277", "report-no": null, "categories": "nlin.CG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emergent processes in complex systems such as cellular automata can perform\ncomputations of increasing complexity, and could possibly lead to artificial\nevolution. Such a feat would require scaling up current simulation sizes to\nallow for enough computational capacity. Understanding complex computations\nhappening in cellular automata and other systems capable of emergence poses\nmany challenges, especially in large-scale systems. We propose methods for\ncoarse-graining cellular automata based on frequency analysis of cell states,\nclustering and autoencoders. These innovative techniques facilitate the\ndiscovery of large-scale structure formation and complexity analysis in those\nsystems. They emphasize interesting behaviors in elementary cellular automata\nwhile filtering out background patterns. Moreover, our methods reduce large 2D\nautomata to smaller sizes and enable identifying systems that behave\ninterestingly at multiple scales.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 08:14:15 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Cisneros", "Hugo", ""], ["Sivic", "Josef", ""], ["Mikolov", "Tomas", ""]]}, {"id": "2104.01024", "submitter": "Burak Turhan", "authors": "Seyedrebvar Hosseini and Burak Turhan", "title": "A Comparison of Similarity Based Instance Selection Methods for Cross\n  Project Defect Prediction", "comments": "The 36th ACM/SIGAPP Symposium on Applied Computing (SAC'21), 10 pages", "journal-ref": null, "doi": "10.1145/3412841.3442020", "report-no": null, "categories": "cs.LG cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context: Previous studies have shown that training data instance selection\nbased on nearest neighborhood (NN) information can lead to better performance\nin cross project defect prediction (CPDP) by reducing heterogeneity in training\ndatasets. However, neighborhood calculation is computationally expensive and\napproximate methods such as Locality Sensitive Hashing (LSH) can be as\neffective as exact methods. Aim: We aim at comparing instance selection methods\nfor CPDP, namely LSH, NN-filter, and Genetic Instance Selection (GIS). Method:\nWe conduct experiments with five base learners, optimizing their hyper\nparameters, on 13 datasets from PROMISE repository in order to compare the\nperformance of LSH with benchmark instance selection methods NN-Filter and GIS.\nResults: The statistical tests show six distinct groups for F-measure\nperformance. The top two group contains only LSH and GIS benchmarks whereas the\nbottom two groups contain only NN-Filter variants. LSH and GIS favor recall\nmore than precision. In fact, for precision performance only three\nsignificantly distinct groups are detected by the tests where the top group is\ncomprised of NN-Filter variants only. Recall wise, 16 different groups are\nidentified where the top three groups contain only LSH methods, four of the\nnext six are GIS only and the bottom five contain only NN-Filter. Finally,\nNN-Filter benchmarks never outperform the LSH counterparts with the same base\nlearner, tuned or non-tuned. Further, they never even belong to the same rank\ngroup, meaning that LSH is always significantly better than NN-Filter with the\nsame learner and settings. Conclusions: The increase in performance and the\ndecrease in computational overhead and runtime make LSH a promising approach.\nHowever, the performance of LSH is based on high recall and in environments\nwhere precision is considered more important NN-Filter should be considered.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 12:50:44 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Hosseini", "Seyedrebvar", ""], ["Turhan", "Burak", ""]]}, {"id": "2104.01032", "submitter": "Sheng Huang", "authors": "Zeyu Wang and Sheng Huang and Zhongxin Liu and Meng Yan and Xin Xia\n  and Bei Wang and Dan Yang", "title": "Plot2API: Recommending Graphic API from Plot via Semantic Parsing Guided\n  Neural Network", "comments": "Accepted by SANER2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Plot-based Graphic API recommendation (Plot2API) is an unstudied but\nmeaningful issue, which has several important applications in the context of\nsoftware engineering and data visualization, such as the plotting guidance of\nthe beginner, graphic API correlation analysis, and code conversion for\nplotting. Plot2API is a very challenging task, since each plot is often\nassociated with multiple APIs and the appearances of the graphics drawn by the\nsame API can be extremely varied due to the different settings of the\nparameters. Additionally, the samples of different APIs also suffer from\nextremely imbalanced. Considering the lack of technologies in Plot2API, we\npresent a novel deep multi-task learning approach named Semantic Parsing Guided\nNeural Network (SPGNN) which translates the Plot2API issue as a multi-label\nimage classification and an image semantic parsing tasks for the solution. In\nSPGNN, the recently advanced Convolutional Neural Network (CNN) named\nEfficientNet is employed as the backbone network for API recommendation.\nMeanwhile, a semantic parsing module is complemented to exploit the semantic\nrelevant visual information in feature learning and eliminate the\nappearance-relevant visual information which may confuse the\nvisual-information-based API recommendation. Moreover, the recent data\naugmentation technique named random erasing is also applied for alleviating the\nimbalance of API categories. We collect plots with the graphic APIs used to\ndrawn them from Stack Overflow, and release three new Plot2API datasets\ncorresponding to the graphic APIs of R and Python programming languages for\nevaluating the effectiveness of Plot2API techniques. Extensive experimental\nresults not only demonstrate the superiority of our method over the recent deep\nlearning baselines but also show the practicability of our method in the\nrecommendation of graphic APIs.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 13:08:56 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Wang", "Zeyu", ""], ["Huang", "Sheng", ""], ["Liu", "Zhongxin", ""], ["Yan", "Meng", ""], ["Xia", "Xin", ""], ["Wang", "Bei", ""], ["Yang", "Dan", ""]]}, {"id": "2104.01040", "submitter": "Igor Halperin", "authors": "Igor Halperin", "title": "Distributional Offline Continuous-Time Reinforcement Learning with\n  Neural Physics-Informed PDEs (SciPhy RL for DOCTR-L)", "comments": "24 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI physics.comp-ph q-fin.CP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper addresses distributional offline continuous-time reinforcement\nlearning (DOCTR-L) with stochastic policies for high-dimensional optimal\ncontrol. A soft distributional version of the classical Hamilton-Jacobi-Bellman\n(HJB) equation is given by a semilinear partial differential equation (PDE).\nThis `soft HJB equation' can be learned from offline data without assuming that\nthe latter correspond to a previous optimal or near-optimal policy. A\ndata-driven solution of the soft HJB equation uses methods of Neural PDEs and\nPhysics-Informed Neural Networks developed in the field of Scientific Machine\nLearning (SciML). The suggested approach, dubbed `SciPhy RL', thus reduces\nDOCTR-L to solving neural PDEs from data. Our algorithm called Deep DOCTR-L\nconverts offline high-dimensional data into an optimal policy in one step by\nreducing it to supervised learning, instead of relying on value iteration or\npolicy iteration methods. The method enables a computable approach to the\nquality control of obtained policies in terms of both their expected returns\nand uncertainties about their values.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 13:22:14 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Halperin", "Igor", ""]]}, {"id": "2104.01063", "submitter": "Raghvendra Mall", "authors": "Raghvendra Mall, Shameem A. Parambath, Han Yufei, Ting Yu and Sanjay\n  Chawla", "title": "Permutation-Invariant Subgraph Discovery", "comments": "8 pages, 4 Figures, 2 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce Permutation and Structured Perturbation Inference (PSPI), a new\nproblem formulation that abstracts many graph matching tasks that arise in\nsystems biology. PSPI can be viewed as a robust formulation of the permutation\ninference or graph matching, where the objective is to find a permutation\nbetween two graphs under the assumption that a set of edges may have undergone\na perturbation due to an underlying cause. For example, suppose there are two\ngene regulatory networks X and Y from a diseased and normal tissue\nrespectively. Then, the PSPI problem can be used to detect if there has been a\nstructural change between the two networks which can serve as a signature of\nthe disease. Besides the new problem formulation, we propose an ADMM algorithm\n(STEPD) to solve a relaxed version of the PSPI problem. An extensive case study\non comparative gene regulatory networks (GRNs) is used to demonstrate that\nSTEPD is able to accurately infer structured perturbations and thus provides a\ntool for computational biologists to identify novel prognostic signatures. A\nspectral analysis confirms that STEPD can recover small clique-like\nperturbations making it a useful tool for detecting permutation-invariant\nchanges in graphs.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 14:28:21 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Mall", "Raghvendra", ""], ["Parambath", "Shameem A.", ""], ["Yufei", "Han", ""], ["Yu", "Ting", ""], ["Chawla", "Sanjay", ""]]}, {"id": "2104.01066", "submitter": "Jacob Taylor PhD", "authors": "Rafael Kaufmann, Pranav Gupta, Jacob Taylor", "title": "An active inference model of collective intelligence", "comments": "32 pages, 10 figures, manuscript under review", "journal-ref": null, "doi": "10.3390/e23070830", "report-no": null, "categories": "cs.SI cs.AI cs.MA cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To date, formal models of collective intelligence have lacked a plausible\nmathematical description of the relationship between local-scale interactions\nbetween highly autonomous sub-system components (individuals) and global-scale\nbehavior of the composite system (the collective). In this paper we use the\nActive Inference Formulation (AIF), a framework for explaining the behavior of\nany non-equilibrium steady state system at any scale, to posit a minimal\nagent-based model that simulates the relationship between local\nindividual-level interaction and collective intelligence (operationalized as\nsystem-level performance). We explore the effects of providing baseline AIF\nagents (Model 1) with specific cognitive capabilities: Theory of Mind (Model\n2); Goal Alignment (Model 3), and Theory of Mind with Goal Alignment (Model 4).\nThese stepwise transitions in sophistication of cognitive ability are motivated\nby the types of advancements plausibly required for an AIF agent to persist and\nflourish in an environment populated by other AIF agents, and have also\nrecently been shown to map naturally to canonical steps in human cognitive\nability. Illustrative results show that stepwise cognitive transitions increase\nsystem performance by providing complementary mechanisms for alignment between\nagents' local and global optima. Alignment emerges endogenously from the\ndynamics of interacting AIF agents themselves, rather than being imposed\nexogenously by incentives to agents' behaviors (contra existing computational\nmodels of collective intelligence) or top-down priors for collective behavior\n(contra existing multiscale simulations of AIF). These results shed light on\nthe types of generic information-theoretic patterns conducive to collective\nintelligence in human and other complex adaptive systems.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 14:32:01 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Kaufmann", "Rafael", ""], ["Gupta", "Pranav", ""], ["Taylor", "Jacob", ""]]}, {"id": "2104.01082", "submitter": "Adeyinka K. Akanbi MR", "authors": "Adeyinka Akanbi", "title": "ESTemd: A Distributed Processing Framework for Environmental Monitoring\n  based on Apache Kafka Streaming Engine", "comments": "12 pages", "journal-ref": null, "doi": "10.1145/3445945.3445949", "report-no": null, "categories": "cs.DC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Distributed networks and real-time systems are becoming the most important\ncomponents for the new computer age, the Internet of Things (IoT), with huge\ndata streams or data sets generated from sensors and data generated from\nexisting legacy systems. The data generated offers the ability to measure,\ninfer and understand environmental indicators, from delicate ecologies and\nnatural resources to urban environments. This can be achieved through the\nanalysis of the heterogeneous data sources (structured and unstructured). In\nthis paper, we propose a distributed framework Event STream Processing Engine\nfor Environmental Monitoring Domain (ESTemd) for the application of stream\nprocessing on heterogeneous environmental data. Our work in this area\ndemonstrates the useful role big data techniques can play in an environmental\ndecision support system, early warning and forecasting systems. The proposed\nframework addresses the challenges of data heterogeneity from heterogeneous\nsystems and real time processing of huge environmental datasets through a\npublish/subscribe method via a unified data pipeline with the application of\nApache Kafka for real time analytics.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 15:04:15 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Akanbi", "Adeyinka", ""]]}, {"id": "2104.01083", "submitter": "Mark Anderson", "authors": "Mark Anderson and Carlos G\\'omez-Rodr\\'iguez", "title": "What Taggers Fail to Learn, Parsers Need the Most", "comments": "Due to be published in the proceedings of the 23rd Nordic Conference\n  on Computational Linguistics (NoDaLiDa 2021). Previously rejected at the 2021\n  Conference of the European Chapter of the Association for Computational\n  Linguistics (EACL 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an error analysis of neural UPOS taggers to evaluate why using\ngold standard tags has such a large positive contribution to parsing\nperformance while using predicted UPOS tags either harms performance or offers\na negligible improvement. We evaluate what neural dependency parsers implicitly\nlearn about word types and how this relates to the errors taggers make to\nexplain the minimal impact using predicted tags has on parsers. We also present\na short analysis on what contexts result in reductions in tagging performance.\nWe then mask UPOS tags based on errors made by taggers to tease away the\ncontribution of UPOS tags which taggers succeed and fail to classify correctly\nand the impact of tagging errors.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 15:04:56 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Anderson", "Mark", ""], ["G\u00f3mez-Rodr\u00edguez", "Carlos", ""]]}, {"id": "2104.01105", "submitter": "Saeedeh Shekarpour", "authors": "Sunday C. Ngwobia and Saeedeh Shekarpour and Faisal Alshargi", "title": "Capturing Knowledge of Emerging Entities From Extended Search Snippets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Google and other search engines feature the entity search by representing a\nknowledge card summarizing related facts about the user-supplied entity.\nHowever, the knowledge card is limited to certain entities that have a Wiki\npage or an entry in encyclopedias such as Freebase. The current encyclopedias\nare limited to highly popular entities, which are far fewer compared with the\nemerging entities. Despite the availability of knowledge about the emerging\nentities on the search results, yet there are no approaches to capture,\nabstract, summerize, fuse, and validate fragmented pieces of knowledge about\nthem. Thus, in this paper, we develop approaches to capture two types of\nknowledge about the emerging entities from a corpus extended from top-n search\nsnippets of a given emerging entity. The first kind of knowledge identifies the\nrole(s) of the emerging entity as, e.g., who is s/he? The second kind captures\nthe entities closely associated with the emerging entity. As the testbed, we\nconsidered a collection of 20 emerging entities and 20 popular entities as the\nground truth. Our approach is an unsupervised approach based on text analysis\nand entity embeddings. Our experimental studies show promising results as the\naccuracy of more than $87\\%$ for recognizing entities and $75\\%$ for ranking\nthem. Besides $87\\%$ of the entailed types were recognizable. Our testbed and\nsource code is available on Github\nhttps://github.com/sunnyUD/research_source_code.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 15:34:54 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Ngwobia", "Sunday C.", ""], ["Shekarpour", "Saeedeh", ""], ["Alshargi", "Faisal", ""]]}, {"id": "2104.01113", "submitter": "Satvik Garg", "authors": "Satvik Garg", "title": "Drug Recommendation System based on Sentiment Analysis of Drug Reviews\n  using Machine Learning", "comments": "7 pages, 8 figures, 2021 11th International Conference on Cloud\n  Computing, Data Science & Engineering (Confluence)", "journal-ref": null, "doi": "10.1109/Confluence51648.2021.9377188", "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since coronavirus has shown up, inaccessibility of legitimate clinical\nresources is at its peak, like the shortage of specialists, healthcare workers,\nlack of proper equipment and medicines. The entire medical fraternity is in\ndistress, which results in numerous individuals demise. Due to unavailability,\npeople started taking medication independently without appropriate\nconsultation, making the health condition worse than usual. As of late, machine\nlearning has been valuable in numerous applications, and there is an increase\nin innovative work for automation. This paper intends to present a drug\nrecommender system that can drastically reduce specialists heap. In this\nresearch, we build a medicine recommendation system that uses patient reviews\nto predict the sentiment using various vectorization processes like Bow, TFIDF,\nWord2Vec, and Manual Feature Analysis, which can help recommend the top drug\nfor a given disease by different classification algorithms. The predicted\nsentiments were evaluated by precision, recall, f1score, accuracy, and AUC\nscore. The results show that classifier LinearSVC using TFIDF vectorization\noutperforms all other models with 93% accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 10:11:18 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 03:19:32 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Garg", "Satvik", ""]]}, {"id": "2104.01190", "submitter": "Fang Li", "authors": "Fang Li, Huaduo Wang, Gopal Gupta", "title": "grASP: A Graph Based ASP-Solver and Justification System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Answer set programming (ASP) is a popular nonmonotonic-logic based paradigm\nfor knowledge representation and solving combinatorial problems. Computing the\nanswer set of an ASP program is NP-hard in general, and researchers have been\ninvesting significant effort to speed it up. The majority of current ASP\nsolvers employ SAT solver-like technology to find these answer sets. As a\nresult, justification for why a literal is in the answer set is hard to\nproduce. There are dependency graph based approaches to find answer sets, but\ndue to the representational limitations of dependency graphs, such approaches\nare limited. We propose a novel dependency graph-based approach for finding\nanswer sets in which conjunction of goals is explicitly represented as a node\nwhich allows arbitrary answer set programs to be uniformly represented. Our\nrepresentation preserves causal relationships allowing for justification for\neach literal in the answer set to be elegantly found. Performance results from\nan implementation are also reported. Our work paves the way for computing\nanswer sets without grounding a program.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 18:16:20 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Li", "Fang", ""], ["Wang", "Huaduo", ""], ["Gupta", "Gopal", ""]]}, {"id": "2104.01193", "submitter": "Ana Ozaki", "authors": "Ana Ozaki", "title": "Learning Description Logic Ontologies. Five Approaches. Where Do They\n  Stand?", "comments": null, "journal-ref": "KI Kunstliche Intelligenz (2020) 34 317-327", "doi": "10.1007/s13218-020-00656-9", "report-no": null, "categories": "cs.AI cs.LG cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The quest for acquiring a formal representation of the knowledge of a domain\nof interest has attracted researchers with various backgrounds into a diverse\nfield called ontology learning. We highlight classical machine learning and\ndata mining approaches that have been proposed for (semi-)automating the\ncreation of description logic (DL) ontologies. These are based on association\nrule mining, formal concept analysis, inductive logic programming,\ncomputational learning theory, and neural networks. We provide an overview of\neach approach and how it has been adapted for dealing with DL ontologies.\nFinally, we discuss the benefits and limitations of each of them for learning\nDL ontologies.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 18:36:45 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Ozaki", "Ana", ""]]}, {"id": "2104.01231", "submitter": "Theodoros Tsiligkaridis", "authors": "Athanasios Tsiligkaridis, Theodoros Tsiligkaridis", "title": "Misclassification-Aware Gaussian Smoothing improves Robustness against\n  Domain Shifts", "comments": "16 pages, submitted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks achieve high prediction accuracy when the train and test\ndistributions coincide. However, in practice various types of corruptions can\ndeviate from this setup and performance can be heavily degraded. There have\nbeen only a few methods to address generalization in presence of unexpected\ndomain shifts observed during deployment. In this paper, a\nmisclassification-aware Gaussian smoothing approach is presented to improve the\nrobustness of image classifiers against a variety of corruptions while\nmaintaining clean accuracy. The intuition behind our proposed\nmisclassification-aware objective is revealed through bounds on the local loss\ndeviation in the small-noise regime. When our method is coupled with additional\ndata augmentations, it is empirically shown to improve upon the\nstate-of-the-art in robustness and uncertainty calibration on several image\nclassification tasks.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 20:25:53 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Tsiligkaridis", "Athanasios", ""], ["Tsiligkaridis", "Theodoros", ""]]}, {"id": "2104.01233", "submitter": "Ravikiran Mane", "authors": "Ravikiran Mane, Effie Chew, Karen Chua, Kai Keng Ang, Neethu Robinson,\n  A. P. Vinod, Seong-Whan Lee, Cuntai Guan", "title": "FBCNet: A Multi-view Convolutional Neural Network for Brain-Computer\n  Interface", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.OH cs.AI cs.LG eess.SP", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Lack of adequate training samples and noisy high-dimensional features are key\nchallenges faced by Motor Imagery (MI) decoding algorithms for\nelectroencephalogram (EEG) based Brain-Computer Interface (BCI). To address\nthese challenges, inspired from neuro-physiological signatures of MI, this\npaper proposes a novel Filter-Bank Convolutional Network (FBCNet) for MI\nclassification. FBCNet employs a multi-view data representation followed by\nspatial filtering to extract spectro-spatially discriminative features. This\nmultistage approach enables efficient training of the network even when limited\ntraining data is available. More significantly, in FBCNet, we propose a novel\nVariance layer that effectively aggregates the EEG time-domain information.\nWith this design, we compare FBCNet with state-of-the-art (SOTA) BCI algorithm\non four MI datasets: The BCI competition IV dataset 2a (BCIC-IV-2a), the\nOpenBMI dataset, and two large datasets from chronic stroke patients. The\nresults show that, by achieving 76.20% 4-class classification accuracy, FBCNet\nsets a new SOTA for BCIC-IV-2a dataset. On the other three datasets, FBCNet\nyields up to 8% higher binary classification accuracies. Additionally, using\nexplainable AI techniques we present one of the first reports about the\ndifferences in discriminative EEG features between healthy subjects and stroke\npatients. Also, the FBCNet source code is available at\nhttps://github.com/ravikiran-mane/FBCNet.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 08:27:01 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Mane", "Ravikiran", ""], ["Chew", "Effie", ""], ["Chua", "Karen", ""], ["Ang", "Kai Keng", ""], ["Robinson", "Neethu", ""], ["Vinod", "A. P.", ""], ["Lee", "Seong-Whan", ""], ["Guan", "Cuntai", ""]]}, {"id": "2104.01252", "submitter": "Benjamin Kahl", "authors": "Benjamin Kahl", "title": "Autonomous Driving Data Chain & Interfaces", "comments": "5 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent developments in autonomous driving technology have proven that map\ndata may be used, not only for general routing purposes, but also for to\nenhance and complement common sensor data. This document reviews the most\ncommonly used interfaces and formats at each step of a selfhealing map data\nchain.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 21:39:12 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Kahl", "Benjamin", ""]]}, {"id": "2104.01264", "submitter": "Qingyun Dou", "authors": "Qingyun Dou, Yiting Lu, Potsawee Manakul, Xixin Wu, Mark J. F. Gales", "title": "Attention Forcing for Machine Translation", "comments": "arXiv admin note: text overlap with arXiv:1909.12289", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Auto-regressive sequence-to-sequence models with attention mechanisms have\nachieved state-of-the-art performance in various tasks including Text-To-Speech\n(TTS) and Neural Machine Translation (NMT). The standard training approach,\nteacher forcing, guides a model with the reference output history. At inference\nstage, the generated output history must be used. This mismatch can impact\nperformance. However, it is highly challenging to train the model using the\ngenerated output. Several approaches have been proposed to address this\nproblem, normally by selectively using the generated output history. To make\ntraining stable, these approaches often require a heuristic schedule or an\nauxiliary classifier. This paper introduces attention forcing for NMT. This\napproach guides the model with the generated output history and reference\nattention, and can reduce the training-inference mismatch without a schedule or\na classifier. Attention forcing has been successful in TTS, but its application\nto NMT is more challenging, due to the discrete and multi-modal nature of the\noutput space. To tackle this problem, this paper adds a selection scheme to\nvanilla attention forcing, which automatically selects a suitable training\napproach for each pair of training data. Experiments show that attention\nforcing can improve the overall translation quality and the diversity of the\ntranslations.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 22:33:42 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Dou", "Qingyun", ""], ["Lu", "Yiting", ""], ["Manakul", "Potsawee", ""], ["Wu", "Xixin", ""], ["Gales", "Mark J. F.", ""]]}, {"id": "2104.01266", "submitter": "Kenneth Holstein", "authors": "Kenneth Holstein and Vincent Aleven", "title": "Designing for human-AI complementarity in K-12 education", "comments": "To appear in AI Magazine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent work has explored how complementary strengths of humans and artificial\nintelligence (AI) systems might be productively combined. However, successful\nforms of human-AI partnership have rarely been demonstrated in real-world\nsettings. We present the iterative design and evaluation of Lumilo, smart\nglasses that help teachers help their students in AI-supported classrooms by\npresenting real-time analytics about students' learning, metacognition, and\nbehavior. Results from a field study conducted in K-12 classrooms indicate that\nstudents learn more when teachers and AI tutors work together during class. We\ndiscuss implications of this research for the design of human-AI partnerships.\nWe argue for more participatory approaches to research and design in this area,\nin which practitioners and other stakeholders are deeply, meaningfully involved\nthroughout the process. Furthermore, we advocate for theory-building and for\nprincipled approaches to the study of human-AI decision-making in real-world\ncontexts.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 22:38:50 GMT"}, {"version": "v2", "created": "Sat, 10 Jul 2021 22:54:50 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Holstein", "Kenneth", ""], ["Aleven", "Vincent", ""]]}, {"id": "2104.01271", "submitter": "C.-H. Huck Yang", "authors": "Chao-Han Huck Yang, Sabato Marco Siniscalchi, Chin-Hui Lee", "title": "PATE-AAE: Incorporating Adversarial Autoencoder into Private Aggregation\n  of Teacher Ensembles for Spoken Command Classification", "comments": "Accepted to Interspeech 2021", "journal-ref": "Proc. Interspeech 2021", "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.LG cs.NE eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We propose using an adversarial autoencoder (AAE) to replace generative\nadversarial network (GAN) in the private aggregation of teacher ensembles\n(PATE), a solution for ensuring differential privacy in speech applications.\nThe AAE architecture allows us to obtain good synthetic speech leveraging upon\na discriminative training of latent vectors. Such synthetic speech is used to\nbuild a privacy-preserving classifier when non-sensitive data is not\nsufficiently available in the public domain. This classifier follows the PATE\nscheme that uses an ensemble of noisy outputs to label the synthetic samples\nand guarantee $\\varepsilon$-differential privacy (DP) on its derived\nclassifiers. Our proposed framework thus consists of an AAE-based generator and\na PATE-based classifier (PATE-AAE). Evaluated on the Google Speech Commands\nDataset Version II, the proposed PATE-AAE improves the average classification\naccuracy by +$2.11\\%$ and +$6.60\\%$, respectively, when compared with\nalternative privacy-preserving solutions, namely PATE-GAN and DP-GAN, while\nmaintaining a strong level of privacy target at $\\varepsilon$=0.01 with a fixed\n$\\delta$=10$^{-5}$.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 23:10:57 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 06:09:42 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Yang", "Chao-Han Huck", ""], ["Siniscalchi", "Sabato Marco", ""], ["Lee", "Chin-Hui", ""]]}, {"id": "2104.01328", "submitter": "Niko S\\\"underhauf", "authors": "Dimity Miller, Niko S\\\"underhauf, Michael Milford and Feras Dayoub", "title": "Uncertainty for Identifying Open-Set Errors in Visual Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deployed into an open world, object detectors are prone to a type of false\npositive detection termed open-set errors. We propose GMM-Det, a real-time\nmethod for extracting epistemic uncertainty from object detectors to identify\nand reject open-set errors. GMM-Det trains the detector to produce a structured\nlogit space that is modelled with class-specific Gaussian Mixture Models. At\ntest time, open-set errors are identified by their low log-probability under\nall Gaussian Mixture Models. We test two common detector architectures, Faster\nR-CNN and RetinaNet, across three varied datasets spanning robotics and\ncomputer vision. Our results show that GMM-Det consistently outperforms\nexisting uncertainty techniques for identifying and rejecting open-set\ndetections, especially at the low-error-rate operating point required for\nsafety-critical applications. GMM-Det maintains object detection performance,\nand introduces only minimal computational overhead. We also introduce a\nmethodology for converting existing object detection datasets into specific\nopen-set datasets to consistently evaluate open-set performance in object\ndetection. Code for GMM-Det and the dataset methodology will be made publicly\navailable.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 07:12:31 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Miller", "Dimity", ""], ["S\u00fcnderhauf", "Niko", ""], ["Milford", "Michael", ""], ["Dayoub", "Feras", ""]]}, {"id": "2104.01353", "submitter": "YoungJin Heo", "authors": "Young-Jin Heo, Young-Ju Choi, Young-Woon Lee, Byung-Gyu Kim", "title": "Deepfake Detection Scheme Based on Vision Transformer and Distillation", "comments": "7 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deepfake is the manipulated video made with a generative deep learning\ntechnique such as Generative Adversarial Networks (GANs) or Auto Encoder that\nanyone can utilize. Recently, with the increase of Deepfake videos, some\nclassifiers consisting of the convolutional neural network that can distinguish\nfake videos as well as deepfake datasets have been actively created. However,\nthe previous studies based on the CNN structure have the problem of not only\noverfitting, but also considerable misjudging fake video as real ones. In this\npaper, we propose a Vision Transformer model with distillation methodology for\ndetecting fake videos. We design that a CNN features and patch-based\npositioning model learns to interact with all positions to find the artifact\nregion for solving false negative problem. Through comparative analysis on\nDeepfake Detection (DFDC) Dataset, we verify that the proposed scheme with\npatch embedding as input outperforms the state-of-the-art using the combined\nCNN features. Without ensemble technique, our model obtains 0.978 of AUC and\n91.9 of f1 score, while previous SOTA model yields 0.972 of AUC and 90.6 of f1\nscore on the same condition.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 09:13:05 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Heo", "Young-Jin", ""], ["Choi", "Young-Ju", ""], ["Lee", "Young-Woon", ""], ["Kim", "Byung-Gyu", ""]]}, {"id": "2104.01396", "submitter": "Ekaterina Komendantskaya Dr", "authors": "Marco Casadio, Matthew Daggitt, Ekaterina Komendantskaya, Wen Kokke,\n  Daniel Kienitz, Rob Stewart", "title": "Property-driven Training: All You (N)Ever Wanted to Know About", "comments": "10 pages, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are known for their ability to detect general patterns in\nnoisy data. This makes them a popular tool for perception components in complex\nAI systems. Paradoxically, they are also known for being vulnerable to\nadversarial attacks. In response, various methods such as adversarial training,\ndata-augmentation and Lipschitz robustness training have been proposed as means\nof improving their robustness. However, as this paper explores, these training\nmethods each optimise for a different definition of robustness. We perform an\nin-depth comparison of these different definitions, including their\nrelationship, assumptions, interpretability and verifiability after training.\nWe also look at constraint-driven training, a general approach designed to\nencode arbitrary constraints, and show that not all of these definitions are\ndirectly encodable. Finally we perform experiments to compare the applicability\nand efficacy of the training methods at ensuring the network obeys these\ndifferent definitions. These results highlight that even the encoding of such a\nsimple piece of knowledge such as robustness in neural network training is\nfraught with difficult choices and pitfalls.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 13:06:06 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Casadio", "Marco", ""], ["Daggitt", "Matthew", ""], ["Komendantskaya", "Ekaterina", ""], ["Kokke", "Wen", ""], ["Kienitz", "Daniel", ""], ["Stewart", "Rob", ""]]}, {"id": "2104.01409", "submitter": "Myeonghun Jeong", "authors": "Myeonghun Jeong, Hyeongju Kim, Sung Jun Cheon, Byoung Jin Choi, and\n  Nam Soo Kim", "title": "Diff-TTS: A Denoising Diffusion Model for Text-to-Speech", "comments": "Submitted to INTERSPEECH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.AI cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although neural text-to-speech (TTS) models have attracted a lot of attention\nand succeeded in generating human-like speech, there is still room for\nimprovements to its naturalness and architectural efficiency. In this work, we\npropose a novel non-autoregressive TTS model, namely Diff-TTS, which achieves\nhighly natural and efficient speech synthesis. Given the text, Diff-TTS\nexploits a denoising diffusion framework to transform the noise signal into a\nmel-spectrogram via diffusion time steps. In order to learn the mel-spectrogram\ndistribution conditioned on the text, we present a likelihood-based\noptimization method for TTS. Furthermore, to boost up the inference speed, we\nleverage the accelerated sampling method that allows Diff-TTS to generate raw\nwaveforms much faster without significantly degrading perceptual quality.\nThrough experiments, we verified that Diff-TTS generates 28 times faster than\nthe real-time with a single NVIDIA 2080Ti GPU.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 13:53:19 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Jeong", "Myeonghun", ""], ["Kim", "Hyeongju", ""], ["Cheon", "Sung Jun", ""], ["Choi", "Byoung Jin", ""], ["Kim", "Nam Soo", ""]]}, {"id": "2104.01494", "submitter": "Aly El Gamal", "authors": "Rehana Mahfuz, Rajeev Sahay, Aly El Gamal", "title": "Mitigating Gradient-based Adversarial Attacks via Denoising and\n  Compression", "comments": "13 pages, 2 figures, 9 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient-based adversarial attacks on deep neural networks pose a serious\nthreat, since they can be deployed by adding imperceptible perturbations to the\ntest data of any network, and the risk they introduce cannot be assessed\nthrough the network's original training performance. Denoising and\ndimensionality reduction are two distinct methods that have been independently\ninvestigated to combat such attacks. While denoising offers the ability to\ntailor the defense to the specific nature of the attack, dimensionality\nreduction offers the advantage of potentially removing previously unseen\nperturbations, along with reducing the training time of the network being\ndefended. We propose strategies to combine the advantages of these two defense\nmechanisms. First, we propose the cascaded defense, which involves denoising\nfollowed by dimensionality reduction. To reduce the training time of the\ndefense for a small trade-off in performance, we propose the hidden layer\ndefense, which involves feeding the output of the encoder of a denoising\nautoencoder into the network. Further, we discuss how adaptive attacks against\nthese defenses could become significantly weak when an alternative defense is\nused, or when no defense is used. In this light, we propose a new metric to\nevaluate a defense which measures the sensitivity of the adaptive attack to\nmodifications in the defense. Finally, we present a guideline for building an\nordered repertoire of defenses, a.k.a. a defense infrastructure, that adjusts\nto limited computational resources in presence of uncertainty about the attack\nstrategy.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 22:57:01 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Mahfuz", "Rehana", ""], ["Sahay", "Rajeev", ""], ["Gamal", "Aly El", ""]]}, {"id": "2104.01506", "submitter": "Tasmia Tasrin", "authors": "Tasmia Tasrin, Md Sultan Al Nahian, Habarakadage Perera and Brent\n  Harrison", "title": "Influencing Reinforcement Learning through Natural Language Guidance", "comments": "7 pages, 6 figures, The 34th International FLAIRS Conference, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive reinforcement learning agents use human feedback or instruction\nto help them learn in complex environments. Often, this feedback comes in the\nform of a discrete signal that is either positive or negative. While\ninformative, this information can be difficult to generalize on its own. In\nthis work, we explore how natural language advice can be used to provide a\nricher feedback signal to a reinforcement learning agent by extending policy\nshaping, a well-known Interactive reinforcement learning technique. Usually\npolicy shaping employs a human feedback policy to help an agent to learn more\nabout how to achieve its goal. In our case, we replace this human feedback\npolicy with policy generated based on natural language advice. We aim to\ninspect if the generated natural language reasoning provides support to a deep\nreinforcement learning agent to decide its actions successfully in any given\nenvironment. So, we design our model with three networks: first one is the\nexperience driven, next is the advice generator and third one is the advice\ndriven. While the experience driven reinforcement learning agent chooses its\nactions being influenced by the environmental reward, the advice driven neural\nnetwork with generated feedback by the advice generator for any new state\nselects its actions to assist the reinforcement learning agent to better policy\nshaping.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 00:23:39 GMT"}, {"version": "v2", "created": "Sun, 11 Apr 2021 02:21:32 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Tasrin", "Tasmia", ""], ["Nahian", "Md Sultan Al", ""], ["Perera", "Habarakadage", ""], ["Harrison", "Brent", ""]]}, {"id": "2104.01521", "submitter": "Omid Tarkhaneh", "authors": "Omid Tarkhaneh, Neda Alipour, Amirahmad Chapnevis, Haifeng Shen", "title": "Golden Tortoise Beetle Optimizer: A Novel Nature-Inspired Meta-heuristic\n  Algorithm for Engineering Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG cs.NA math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a novel nature-inspired meta-heuristic algorithm called\nthe Golden Tortoise Beetle Optimizer (GTBO) to solve optimization problems. It\nmimics golden tortoise beetle's behavior of changing colors to attract opposite\nsex for mating and its protective strategy that uses a kind of anal fork to\ndeter predators. The algorithm is modeled based on the beetle's dual\nattractiveness and survival strategy to generate new solutions for optimization\nproblems. To measure its performance, the proposed GTBO is compared with five\nother nature-inspired evolutionary algorithms on 24 well-known benchmark\nfunctions investigating the trade-off between exploration and exploitation,\nlocal optima avoidance, and convergence towards the global optima is\nstatistically significant. We particularly applied GTBO to two well-known\nengineering problems including the welded beam design problem and the gear\ntrain design problem. The results demonstrate that the new algorithm is more\nefficient than the five baseline algorithms for both problems. A sensitivity\nanalysis is also performed to reveal different impacts of the algorithm's key\ncontrol parameters and operators on GTBO's performance.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 02:29:47 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Tarkhaneh", "Omid", ""], ["Alipour", "Neda", ""], ["Chapnevis", "Amirahmad", ""], ["Shen", "Haifeng", ""]]}, {"id": "2104.01542", "submitter": "Zhenyu Jiang", "authors": "Zhenyu Jiang, Yifeng Zhu, Maxwell Svetlik, Kuan Fang, Yuke Zhu", "title": "Synergies Between Affordance and Geometry: 6-DoF Grasp Detection via\n  Implicit Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grasp detection in clutter requires the robot to reason about the 3D scene\nfrom incomplete and noisy perception. In this work, we draw insight that 3D\nreconstruction and grasp learning are two intimately connected tasks, both of\nwhich require a fine-grained understanding of local geometry details. We thus\npropose to utilize the synergies between grasp affordance and 3D reconstruction\nthrough multi-task learning of a shared representation. Our model takes\nadvantage of deep implicit functions, a continuous and memory-efficient\nrepresentation, to enable differentiable training of both tasks. We train the\nmodel on self-supervised grasp trials data in simulation. Evaluation is\nconducted on a clutter removal task, where the robot clears cluttered objects\nby grasping them one at a time. The experimental results in simulation and on\nthe real robot have demonstrated that the use of implicit neural\nrepresentations and joint learning of grasp affordance and 3D reconstruction\nhave led to state-of-the-art grasping results. Our method outperforms baselines\nby over 10% in terms of grasp success rate. Additional results and videos can\nbe found at https://sites.google.com/view/rpl-giga2021\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 05:46:37 GMT"}, {"version": "v2", "created": "Wed, 21 Jul 2021 14:39:06 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Jiang", "Zhenyu", ""], ["Zhu", "Yifeng", ""], ["Svetlik", "Maxwell", ""], ["Fang", "Kuan", ""], ["Zhu", "Yuke", ""]]}, {"id": "2104.01543", "submitter": "Esha Singh", "authors": "Esha Singh, Anu Bompelli, Ruyuan Wan, Jiang Bian, Serguei Pakhomov,\n  and Rui Zhang", "title": "A Conversational Agent System for Dietary Supplements Use", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Dietary supplements (DS) have been widely used by consumers, but the\ninformation around the efficacy and safety of DS is disparate or incomplete,\nthus creating barriers for consumers to find information effectively.\nConversational agent (CA) systems have been applied to the healthcare domain,\nbut there is no such a system to answer consumers regarding DS use, although\nwidespread use of DS. In this study, we develop the first CA system for DS use\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 05:47:04 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 22:16:49 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Singh", "Esha", ""], ["Bompelli", "Anu", ""], ["Wan", "Ruyuan", ""], ["Bian", "Jiang", ""], ["Pakhomov", "Serguei", ""], ["Zhang", "Rui", ""]]}, {"id": "2104.01549", "submitter": "Antony Thomas", "authors": "Hossein Karami and Antony Thomas and Fulvio Mastrogiovanni", "title": "A Task-Motion Planning Framework Using Iteratively Deepened AND/OR Graph\n  Networks", "comments": "Submitted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an approach for Task-Motion Planning (TMP) using Iterative\nDeepened AND/OR Graph Networks (TMP-IDAN) that uses an AND/OR graph network\nbased novel abstraction for compactly representing the task-level states and\nactions. While retrieving a target object from clutter, the number of object\nre-arrangements required to grasp the target is not known ahead of time. To\naddress this challenge, in contrast to traditional AND/OR graph-based planners,\nwe grow the AND/OR graph online until the target grasp is feasible and thereby\nobtain a network of AND/OR graphs. The AND/OR graph network allows faster\ncomputations than traditional task planners. We validate our approach and\nevaluate its capabilities using a Baxter robot and a state-of-the-art robotics\nsimulator in several challenging non-trivial cluttered table-top scenarios. The\nexperiments show that our approach is readily scalable to increasing number of\nobjects and different degrees of clutter.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 07:06:52 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Karami", "Hossein", ""], ["Thomas", "Antony", ""], ["Mastrogiovanni", "Fulvio", ""]]}, {"id": "2104.01563", "submitter": "Abhishek Mittal", "authors": "Abhishek Mittal, Ashutosh Modi", "title": "ReCAM@IITK at SemEval-2021 Task 4: BERT and ALBERT based Ensemble for\n  Abstract Word Prediction", "comments": "Accepted at SemEval 2021 Task 4, 8 Pages (7 Pages main content + 1\n  pages for references)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper describes our system for Task 4 of SemEval-2021: Reading\nComprehension of Abstract Meaning (ReCAM). We participated in all subtasks\nwhere the main goal was to predict an abstract word missing from a statement.\nWe fine-tuned the pre-trained masked language models namely BERT and ALBERT and\nused an Ensemble of these as our submitted system on Subtask 1\n(ReCAM-Imperceptibility) and Subtask 2 (ReCAM-Nonspecificity). For Subtask 3\n(ReCAM-Intersection), we submitted the ALBERT model as it gives the best\nresults. We tried multiple approaches and found that Masked Language\nModeling(MLM) based approach works the best.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 08:22:19 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Mittal", "Abhishek", ""], ["Modi", "Ashutosh", ""]]}, {"id": "2104.01568", "submitter": "Geon Yeong Park", "authors": "Geon Yeong Park, Sang Wan Lee", "title": "Information-theoretic regularization for Multi-source Domain Adaptation", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Adversarial learning strategy has demonstrated remarkable performance in\ndealing with single-source Domain Adaptation (DA) problems, and it has recently\nbeen applied to Multi-source DA (MDA) problems. Although most existing MDA\nstrategies rely on a multiple domain discriminator setting, its effect on the\nlatent space representations has been poorly understood. Here we adopt an\ninformation-theoretic approach to identify and resolve the potential adverse\neffect of the multiple domain discriminators on MDA: disintegration of\ndomain-discriminative information, limited computational scalability, and a\nlarge variance in the gradient of the loss during training. We examine the\nabove issues by situating adversarial DA in the context of information\nregularization. This also provides a theoretical justification for using a\nsingle and unified domain discriminator. Based on this idea, we implement a\nnovel neural architecture called a Multi-source Information-regularized\nAdaptation Networks (MIAN). Large-scale experiments demonstrate that MIAN,\ndespite its structural simplicity, reliably and significantly outperforms other\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 09:11:35 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Park", "Geon Yeong", ""], ["Lee", "Sang Wan", ""]]}, {"id": "2104.01628", "submitter": "Marouen Kachroudi", "authors": "Marouen Kachroudi", "title": "Revisiting Indirect Ontology Alignment : New Challenging Issues in\n  Cross-Lingual Context", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ontology alignment process is overwhelmingly cited in Knowledge Engineering\nas a key mechanism aimed at bypassing heterogeneity and reconciling various\ndata sources, represented by ontologies, i.e., the the Semantic Web\ncornerstone. In such infrastructures and environments, it is inconceivable to\nassume that all ontologies covering a particular domain of knowledge are\naligned in pairs. Moreover, the high performance of alignment approaches is\nclosely related to two factors, i.e., time consumption and machine resource\nlimitations. Thus, good quality alignments are valuable and it would be\nappropriate to exploit them. Based on this observation, this article introduces\na new method of indirect alignment of ontologies in a cross-lingual context.\nIndeed, the proposed method deals with alignments of multilingual ontologies\nand implements an indirect ontology alignment strategy based on a composition\nand reuse of effective direct alignments. The trigger of the proposed method\nprocess is based on alignment algebra which governs the semantics composition\nof relationships and confidence values. The obtained results, after a thorough\nand detailed experiment are very encouraging and highlight many positive\naspects about the new proposed method.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 15:21:09 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Kachroudi", "Marouen", ""]]}, {"id": "2104.01632", "submitter": "Rui Liu", "authors": "Rui Liu, Siddharth Bhatia, Bryan Hooi", "title": "Isconna: Streaming Anomaly Detection with Frequency and Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An edge stream is a common form of presentation of dynamic networks. It can\nevolve with time, with new types of nodes or edges being continuously added.\nExisting methods for anomaly detection rely on edge occurrence counts or\ncompare pattern snippets found in historical records. In this work, we propose\nIsconna, which focuses on both the frequency and the pattern of edge records.\nThe burst detection component targets anomalies between individual timestamps,\nwhile the pattern detection component highlights anomalies across segments of\ntimestamps. These two components together produce three intermediate scores,\nwhich are aggregated into the final anomaly score. Isconna does not actively\nexplore or maintain pattern snippets; it instead measures the consecutive\npresence and absence of edge records. Isconna is an online algorithm, it does\nnot keep the original information of edge records; only statistical values are\nmaintained in a few count-min sketches (CMS). Isconna's space complexity\n$O(rc)$ is determined by two user-specific parameters, the size of CMSs. In\nworst case, Isconna's time complexity can be up to $O(rc)$, but it can be\namortized in practice. Experiments show that Isconna outperforms five\nstate-of-the-art frequency- and/or pattern-based baselines on six real-world\ndatasets with up to 20 million edge records.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 15:42:14 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Liu", "Rui", ""], ["Bhatia", "Siddharth", ""], ["Hooi", "Bryan", ""]]}, {"id": "2104.01634", "submitter": "Mohammad Mahdi Kamani", "authors": "Mohammad Mahdi Kamani, Rana Forsati, James Z. Wang, Mehrdad Mahdavi", "title": "Pareto Efficient Fairness in Supervised Learning: From Extraction to\n  Tracing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As algorithmic decision-making systems are becoming more pervasive, it is\ncrucial to ensure such systems do not become mechanisms of unfair\ndiscrimination on the basis of gender, race, ethnicity, religion, etc.\nMoreover, due to the inherent trade-off between fairness measures and accuracy,\nit is desirable to learn fairness-enhanced models without significantly\ncompromising the accuracy. In this paper, we propose Pareto efficient Fairness\n(PEF) as a suitable fairness notion for supervised learning, that can ensure\nthe optimal trade-off between overall loss and other fairness criteria. The\nproposed PEF notion is definition-agnostic, meaning that any well-defined\nnotion of fairness can be reduced to the PEF notion. To efficiently find a PEF\nclassifier, we cast the fairness-enhanced classification as a bilevel\noptimization problem and propose a gradient-based method that can guarantee the\nsolution belongs to the Pareto frontier with provable guarantees for convex and\nnon-convex objectives. We also generalize the proposed algorithmic solution to\nextract and trace arbitrary solutions from the Pareto frontier for a given\npreference over accuracy and fairness measures. This approach is generic and\ncan be generalized to any multicriteria optimization problem to trace points on\nthe Pareto frontier curve, which is interesting by its own right. We\nempirically demonstrate the effectiveness of the PEF solution and the extracted\nPareto frontier on real-world datasets compared to state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 15:49:35 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Kamani", "Mohammad Mahdi", ""], ["Forsati", "Rana", ""], ["Wang", "James Z.", ""], ["Mahdavi", "Mehrdad", ""]]}, {"id": "2104.01662", "submitter": "Lokesh Krishna", "authors": "Lokesh Krishna, Utkarsh A. Mishra, Guillermo A. Castillo, Ayonga\n  Hereid, Shishir Kolathaya", "title": "Learning Linear Policies for Robust Bipedal Locomotion on Terrains with\n  Varying Slopes", "comments": "7 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, with a view toward deployment of light-weight control\nframeworks for bipedal walking robots, we realize end-foot trajectories that\nare shaped by a single linear feedback policy. We learn this policy via a\nmodel-free and a gradient-free learning algorithm, Augmented Random Search\n(ARS), in the two robot platforms Rabbit and Digit. Our contributions are\ntwo-fold: a) By using torso and support plane orientation as inputs, we achieve\nrobust walking on slopes of up to 20 degrees in simulation. b) We demonstrate\nadditional behaviors like walking backwards, stepping-in-place, and recovery\nfrom external pushes of up to 120 N. The end result is a robust and a fast\nfeedback control law for bipedal walking on terrains with varying slopes.\nTowards the end, we also provide preliminary results of hardware transfer to\nDigit.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 18:50:58 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Krishna", "Lokesh", ""], ["Mishra", "Utkarsh A.", ""], ["Castillo", "Guillermo A.", ""], ["Hereid", "Ayonga", ""], ["Kolathaya", "Shishir", ""]]}, {"id": "2104.01678", "submitter": "Timoth\\'ee Lesort", "authors": "Timoth\\'ee Lesort, Massimo Caccia, Irina Rish", "title": "Understanding Continual Learning Settings with Data Distribution Drift\n  Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Classical machine learning algorithms often assume that the data are drawn\ni.i.d. from a stationary probability distribution. Recently, continual learning\nemerged as a rapidly growing area of machine learning where this assumption is\nrelaxed, namely, where the data distribution is non-stationary, i.e., changes\nover time. However, data distribution drifts may interfere with the learning\nprocess and erase previously learned knowledge; thus, continual learning\nalgorithms must include specialized mechanisms to deal with such distribution\ndrifts. A distribution drift may change the class labels distribution, the\ninput distribution, or both. Moreover, distribution drifts might be abrupt or\ngradual. In this paper, we aim to identify and categorize different types of\ndata distribution drifts and potential assumptions about them, to better\ncharacterize various continual-learning scenarios. Moreover, we propose to use\nthe distribution drift framework to provide more precise definitions of several\nterms commonly used in the continual learning field.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 19:48:16 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Lesort", "Timoth\u00e9e", ""], ["Caccia", "Massimo", ""], ["Rish", "Irina", ""]]}, {"id": "2104.01703", "submitter": "Hyounghun Kim", "authors": "Hyounghun Kim, Abhay Zala, Graham Burri, Mohit Bansal", "title": "FixMyPose: Pose Correctional Captioning and Retrieval", "comments": "AAAI 2021 (18 pages, 16 figures; webpage:\n  https://fixmypose-unc.github.io/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interest in physical therapy and individual exercises such as yoga/dance has\nincreased alongside the well-being trend. However, such exercises are hard to\nfollow without expert guidance (which is impossible to scale for personalized\nfeedback to every trainee remotely). Thus, automated pose correction systems\nare required more than ever, and we introduce a new captioning dataset named\nFixMyPose to address this need. We collect descriptions of correcting a\n\"current\" pose to look like a \"target\" pose (in both English and Hindi). The\ncollected descriptions have interesting linguistic properties such as\negocentric relations to environment objects, analogous references, etc.,\nrequiring an understanding of spatial relations and commonsense knowledge about\npostures. Further, to avoid ML biases, we maintain a balance across characters\nwith diverse demographics, who perform a variety of movements in several\ninterior environments (e.g., homes, offices). From our dataset, we introduce\nthe pose-correctional-captioning task and its reverse target-pose-retrieval\ntask. During the correctional-captioning task, models must generate\ndescriptions of how to move from the current to target pose image, whereas in\nthe retrieval task, models should select the correct target pose given the\ninitial pose and correctional description. We present strong cross-attention\nbaseline models (uni/multimodal, RL, multilingual) and also show that our\nbaselines are competitive with other models when evaluated on other\nimage-difference datasets. We also propose new task-specific metrics\n(object-match, body-part-match, direction-match) and conduct human evaluation\nfor more reliable evaluation, and we demonstrate a large human-model\nperformance gap suggesting room for promising future work. To verify the\nsim-to-real transfer of our FixMyPose dataset, we collect a set of real images\nand show promising performance on these images.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 21:45:44 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Kim", "Hyounghun", ""], ["Zala", "Abhay", ""], ["Burri", "Graham", ""], ["Bansal", "Mohit", ""]]}, {"id": "2104.01709", "submitter": "Amit Verma Dr.", "authors": "Amit Verma and Mark Lewis", "title": "Constraint Programming to Discover One-Flip Local Optima of Quadratic\n  Unconstrained Binary Optimization Problems", "comments": "Benchmark problems used are available from the first author", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DM math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The broad applicability of Quadratic Unconstrained Binary Optimization (QUBO)\nconstitutes a general-purpose modeling framework for combinatorial optimization\nproblems and are a required format for gate array and quantum annealing\ncomputers. QUBO annealers as well as other solution approaches benefit from\nstarting with a diverse set of solutions with local optimality an additional\nbenefit. This paper presents a new method for generating a set of one-flip\nlocal optima leveraging constraint programming. Further, as demonstrated in\nexperimental testing, analysis of the solution set allows the generation of\nsoft constraints to help guide the optimization process.\n", "versions": [{"version": "v1", "created": "Sun, 4 Apr 2021 22:55:25 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Verma", "Amit", ""], ["Lewis", "Mark", ""]]}, {"id": "2104.01745", "submitter": "Pingping Zhang Dr", "authors": "Xuehu Liu and Pingping Zhang and Chenyang Yu and Huchuan Lu and\n  Xuesheng Qian and Xiaoyun Yang", "title": "A Video Is Worth Three Views: Trigeminal Transformers for Video-based\n  Person Re-identification", "comments": "This work includes 10 pages, 5 figures and 4 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video-based person re-identification (Re-ID) aims to retrieve video sequences\nof the same person under non-overlapping cameras. Previous methods usually\nfocus on limited views, such as spatial, temporal or spatial-temporal view,\nwhich lack of the observations in different feature domains. To capture richer\nperceptions and extract more comprehensive video representations, in this paper\nwe propose a novel framework named Trigeminal Transformers (TMT) for\nvideo-based person Re-ID. More specifically, we design a trigeminal feature\nextractor to jointly transform raw video data into spatial, temporal and\nspatial-temporal domain. Besides, inspired by the great success of vision\ntransformer, we introduce the transformer structure for video-based person\nRe-ID. In our work, three self-view transformers are proposed to exploit the\nrelationships between local features for information enhancement in spatial,\ntemporal and spatial-temporal domains. Moreover, a cross-view transformer is\nproposed to aggregate the multi-view features for comprehensive video\nrepresentations. The experimental results indicate that our approach can\nachieve better performance than other state-of-the-art approaches on public\nRe-ID benchmarks. We will release the code for model reproduction.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 02:50:16 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Liu", "Xuehu", ""], ["Zhang", "Pingping", ""], ["Yu", "Chenyang", ""], ["Lu", "Huchuan", ""], ["Qian", "Xuesheng", ""], ["Yang", "Xiaoyun", ""]]}, {"id": "2104.01747", "submitter": "Sanjai Narain", "authors": "Sanjai Narain, Emily Mak, Dana Chee, Brendan Englot, Kishore\n  Pochiraju, Niraj K. Jha, Karthik Narayan", "title": "Fast Design Space Exploration of Nonlinear Systems: Part I", "comments": "14 pages, 26 figures. arXiv admin note: text overlap with\n  arXiv:2010.09842", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  System design tools are often only available as blackboxes with complex\nnonlinear relationships between inputs and outputs. Blackboxes typically run in\nthe forward direction: for a given design as input they compute an output\nrepresenting system behavior. Most cannot be run in reverse to produce an input\nfrom requirements on output. Thus, finding a design satisfying a requirement is\noften a trial-and-error process without assurance of optimality. Finding\ndesigns concurrently satisfying multiple requirements is harder because designs\nsatisfying individual requirements may conflict with each other. Compounding\nthe hardness are the facts that blackbox evaluations can be expensive and\nsometimes fail to produce an output due to non-convergence of underlying\nnumerical algorithms. This paper presents CNMA (Constrained optimization with\nNeural networks, MILP solvers and Active Learning), a new optimization method\nfor blackboxes. It is conservative in the number of blackbox evaluations. Any\ndesigns it finds are guaranteed to satisfy all requirements. It is resilient to\nthe failure of blackboxes to compute outputs. It tries to sample only the part\nof the design space relevant to solving the design problem, leveraging the\npower of neural networks, MILPs, and a new learning-from-failure feedback loop.\nThe paper also presents parallel CNMA that improves the efficiency and quality\nof solutions over the sequential version, and tries to steer it away from local\noptima. CNMA's performance is evaluated for seven nonlinear design problems of\n8 (2 problems), 10, 15, 36 and 60 real-valued dimensions and one with 186\nbinary dimensions. It is shown that CNMA improves the performance of stable,\noff-the-shelf implementations of Bayesian Optimization and Nelder Mead and\nRandom Search by 1%-87% for a given fixed time and function evaluation budget.\nNote, that these implementations did not always return solutions.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 02:59:45 GMT"}, {"version": "v2", "created": "Sun, 11 Apr 2021 04:04:23 GMT"}, {"version": "v3", "created": "Thu, 27 May 2021 20:09:36 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Narain", "Sanjai", ""], ["Mak", "Emily", ""], ["Chee", "Dana", ""], ["Englot", "Brendan", ""], ["Pochiraju", "Kishore", ""], ["Jha", "Niraj K.", ""], ["Narayan", "Karthik", ""]]}, {"id": "2104.01757", "submitter": "Keenan Venuti", "authors": "Keenan Venuti", "title": "Predicting Mergers and Acquisitions using Graph-based Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The graph data structure is a staple in mathematics, yet graph-based machine\nlearning is a relatively green field within the domain of data science. Recent\nadvances in graph-based ML and open source implementations of relevant\nalgorithms are allowing researchers to apply methods created in academia to\nreal-world datasets. The goal of this project was to utilize a popular graph\nmachine learning framework, GraphSAGE, to predict mergers and acquisitions\n(M&A) of enterprise companies. The results were promising, as the model\npredicted with 81.79% accuracy on a validation dataset. Given the abundance of\ndata sources and algorithmic decision making within financial data science,\ngraph-based machine learning offers a performant, yet non-traditional approach\nto generating alpha.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 03:49:45 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Venuti", "Keenan", ""]]}, {"id": "2104.01778", "submitter": "Yuan Gong", "authors": "Yuan Gong, Yu-An Chung, James Glass", "title": "AST: Audio Spectrogram Transformer", "comments": "Accepted at Interspeech 2021. Code at\n  https://github.com/YuanGongND/ast", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the past decade, convolutional neural networks (CNNs) have been widely\nadopted as the main building block for end-to-end audio classification models,\nwhich aim to learn a direct mapping from audio spectrograms to corresponding\nlabels. To better capture long-range global context, a recent trend is to add a\nself-attention mechanism on top of the CNN, forming a CNN-attention hybrid\nmodel. However, it is unclear whether the reliance on a CNN is necessary, and\nif neural networks purely based on attention are sufficient to obtain good\nperformance in audio classification. In this paper, we answer the question by\nintroducing the Audio Spectrogram Transformer (AST), the first\nconvolution-free, purely attention-based model for audio classification. We\nevaluate AST on various audio classification benchmarks, where it achieves new\nstate-of-the-art results of 0.485 mAP on AudioSet, 95.6% accuracy on ESC-50,\nand 98.1% accuracy on Speech Commands V2.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 05:26:29 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 20:29:37 GMT"}, {"version": "v3", "created": "Thu, 8 Jul 2021 20:16:28 GMT"}], "update_date": "2021-07-12", "authors_parsed": [["Gong", "Yuan", ""], ["Chung", "Yu-An", ""], ["Glass", "James", ""]]}, {"id": "2104.01787", "submitter": "Jeong Min Lee", "authors": "Jeong Min Lee and Milos Hauskrecht", "title": "Neural Clinical Event Sequence Prediction through Personalized Online\n  Adaptive Learning", "comments": "Accepted at 19th International Conference on Artificial Intelligence\n  in Medicine (AIME 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Clinical event sequences consist of thousands of clinical events that\nrepresent records of patient care in time. Developing accurate prediction\nmodels for such sequences is of a great importance for defining representations\nof a patient state and for improving patient care. One important challenge of\nlearning a good predictive model of clinical sequences is patient-specific\nvariability. Based on underlying clinical complications, each patient's\nsequence may consist of different sets of clinical events. However,\npopulation-based models learned from such sequences may not accurately predict\npatient-specific dynamics of event sequences. To address the problem, we\ndevelop a new adaptive event sequence prediction framework that learns to\nadjust its prediction for individual patients through an online model update.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 06:22:56 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 03:02:09 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Lee", "Jeong Min", ""], ["Hauskrecht", "Milos", ""]]}, {"id": "2104.01791", "submitter": "Saikat Dutta", "authors": "Sourya Dipta Das, Ayan Basak, Saikat Dutta", "title": "A Heuristic-driven Uncertainty based Ensemble Framework for Fake News\n  Detection in Tweets and News Articles", "comments": "submitted to Neurocomputing. arXiv admin note: substantial text\n  overlap with arXiv:2101.03545", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The significance of social media has increased manifold in the past few\ndecades as it helps people from even the most remote corners of the world to\nstay connected. With the advent of technology, digital media has become more\nrelevant and widely used than ever before and along with this, there has been a\nresurgence in the circulation of fake news and tweets that demand immediate\nattention. In this paper, we describe a novel Fake News Detection system that\nautomatically identifies whether a news item is \"real\" or \"fake\", as an\nextension of our work in the CONSTRAINT COVID-19 Fake News Detection in English\nchallenge. We have used an ensemble model consisting of pre-trained models\nfollowed by a statistical feature fusion network , along with a novel heuristic\nalgorithm by incorporating various attributes present in news items or tweets\nlike source, username handles, URL domains and authors as statistical feature.\nOur proposed framework have also quantified reliable predictive uncertainty\nalong with proper class output confidence level for the classification task. We\nhave evaluated our results on the COVID-19 Fake News dataset and FakeNewsNet\ndataset to show the effectiveness of the proposed algorithm on detecting fake\nnews in short news content as well as in news articles. We obtained a best\nF1-score of 0.9892 on the COVID-19 dataset, and an F1-score of 0.9073 on the\nFakeNewsNet dataset.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 06:35:30 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Das", "Sourya Dipta", ""], ["Basak", "Ayan", ""], ["Dutta", "Saikat", ""]]}, {"id": "2104.01813", "submitter": "Yan Xu", "authors": "Yan Xu, Yongliang Cheng", "title": "Semi-supervised Variational Temporal Convolutional Network for IoT\n  Communication Multi-anomaly Detection", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The consumer Internet of Things (IoT) have developed in recent years. Mass\nIoT devices are constructed to build a huge communications network. But these\ndevices are insecure in reality, it means that the communications network are\nexposed by the attacker. Moreover, the IoT communication network also faces\nwith variety of sudden errors. Therefore, it easily leads to that is vulnerable\nwith the threat of attacker and system failure. The severe situation of IoT\ncommunication network motivates the development of new techniques to\nautomatically detect multi-anomaly. In this paper, we propose SS-VTCN, a\nsemi-supervised network for IoT multiple anomaly detection that works well\neffectively for IoT communication network. SS-VTCN is designed to capture the\nnormal patterns of the IoT traffic data based on the distribution whether it is\nlabeled or not by learning their representations with key techniques such as\nVariational Autoencoders and Temporal Convolutional Network. This network can\nuse the encode data to predict preliminary result, and reconstruct input data\nto determine anomalies by the representations. Extensive evaluation experiments\nbased on a benchmark dataset and a real consumer smart home dataset demonstrate\nthat SS-VTCN is more suitable than supervised and unsupervised method with\nbetter performance when compared other state-of-art semi-supervised method.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 08:51:24 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Xu", "Yan", ""], ["Cheng", "Yongliang", ""]]}, {"id": "2104.01828", "submitter": "Esmaeil Delfarazpahlevanloo", "authors": "Ruben Becker, Gianlorenzo D'Angelo, Esmaeil Delfaraz and Hugo Gilbert", "title": "When Can Liquid Democracy Unveil the Truth?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we investigate the so-called ODP-problem that has been\nformulated by Caragiannis and Micha [10]. Here, we are in a setting with two\nelection alternatives out of which one is assumed to be correct. In ODP, the\ngoal is to organise the delegations in the social network in order to maximize\nthe probability that the correct alternative, referred to as ground truth, is\nelected. While the problem is known to be computationally hard, we strengthen\nexisting hardness results by providing a novel strong approximation hardness\nresult: For any positive constant $C$, we prove that, unless $P=NP$, there is\nno polynomial-time algorithm for ODP that achieves an approximation guarantee\nof $\\alpha \\ge (\\ln n)^{-C}$, where $n$ is the number of voters. The reduction\ndesigned for this result uses poorly connected social networks in which some\nvoters suffer from misinformation. Interestingly, under some hypothesis on\neither the accuracies of voters or the connectivity of the network, we obtain a\npolynomial-time $1/2$-approximation algorithm. This observation proves formally\nthat the connectivity of the social network is a key feature for the efficiency\nof the liquid democracy paradigm. Lastly, we run extensive simulations and\nobserve that simple algorithms (working either in a centralized or\ndecentralized way) outperform direct democracy on a large class of instances.\nOverall, our contributions yield new insights on the question in which\nsituations liquid democracy can be beneficial.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 09:45:14 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Becker", "Ruben", ""], ["D'Angelo", "Gianlorenzo", ""], ["Delfaraz", "Esmaeil", ""], ["Gilbert", "Hugo", ""]]}, {"id": "2104.01832", "submitter": "Chaoqun Wang", "authors": "Chaoqun Wang, Xuejin Chen, Shaobo Min, Xiaoyan Sun, Houqiang Li", "title": "Task-Independent Knowledge Makes for Transferable Representations for\n  Generalized Zero-Shot Learning", "comments": "Accepted at AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generalized Zero-Shot Learning (GZSL) targets recognizing new categories by\nlearning transferable image representations. Existing methods find that, by\naligning image representations with corresponding semantic labels, the\nsemantic-aligned representations can be transferred to unseen categories.\nHowever, supervised by only seen category labels, the learned semantic\nknowledge is highly task-specific, which makes image representations biased\ntowards seen categories. In this paper, we propose a novel Dual-Contrastive\nEmbedding Network (DCEN) that simultaneously learns task-specific and\ntask-independent knowledge via semantic alignment and instance discrimination.\nFirst, DCEN leverages task labels to cluster representations of the same\nsemantic category by cross-modal contrastive learning and exploring\nsemantic-visual complementarity. Besides task-specific knowledge, DCEN then\nintroduces task-independent knowledge by attracting representations of\ndifferent views of the same image and repelling representations of different\nimages. Compared to high-level seen category supervision, this instance\ndiscrimination supervision encourages DCEN to capture low-level visual\nknowledge, which is less biased toward seen categories and alleviates the\nrepresentation bias. Consequently, the task-specific and task-independent\nknowledge jointly make for transferable representations of DCEN, which obtains\naveraged 4.1% improvement on four public benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 10:05:48 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Wang", "Chaoqun", ""], ["Chen", "Xuejin", ""], ["Min", "Shaobo", ""], ["Sun", "Xiaoyan", ""], ["Li", "Houqiang", ""]]}, {"id": "2104.01854", "submitter": "Mohammad Azangoo", "authors": "Seppo Sierla (1), Mohammad Azangoo (1), Alexander Fay (2), Valeriy\n  Vyatkin (1 and 3), and Nikolaos Papakonstantinou (4) ((1) Department of\n  Electrical Engineering and Automation, Aalto University, Espoo, Finland, (2)\n  Department of Automation Engineering, Helmut Schmidt University, Hamburg,\n  Germany, (3) Department of Computer Science, Electrical and Space\n  Engineering, Lule{\\aa} University of Technology, Lule{\\aa}, Sweden, (4) VTT\n  Technical Research Centre of Finland Ltd, Espoo, Finland)", "title": "Integrating 2D and 3D Digital Plant Information Towards Automatic\n  Generation of Digital Twins", "comments": "8 pages, 13 figures", "journal-ref": null, "doi": "10.1109/ISIE45063.2020.9152371", "report-no": null, "categories": "eess.SY cs.AI cs.CV cs.IT cs.SE cs.SY math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ongoing standardization in Industry 4.0 supports tool vendor neutral\nrepresentations of Piping and Instrumentation diagrams as well as 3D pipe\nrouting. However, a complete digital plant model requires combining these two\nrepresentations. 3D pipe routing information is essential for building any\naccurate first-principles process simulation model. Piping and instrumentation\ndiagrams are the primary source for control loops. In order to automatically\nintegrate these information sources to a unified digital plant model, it is\nnecessary to develop algorithms for identifying corresponding elements such as\ntanks and pumps from piping and instrumentation diagrams and 3D CAD models. One\napproach is to raise these two information sources to a common level of\nabstraction and to match them at this level of abstraction. Graph matching is a\npotential technique for this purpose. This article focuses on automatic\ngeneration of the graphs as a prerequisite to graph matching. Algorithms for\nthis purpose are proposed and validated with a case study. The paper concludes\nwith a discussion of further research needed to reprocess the generated graphs\nin order to enable effective matching.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 11:07:05 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Sierla", "Seppo", "", "1 and 3"], ["Azangoo", "Mohammad", "", "1 and 3"], ["Fay", "Alexander", "", "1 and 3"], ["Vyatkin", "Valeriy", "", "1 and 3"], ["Papakonstantinou", "Nikolaos", ""]]}, {"id": "2104.01865", "submitter": "Thimal Kempitiya", "authors": "Thimal Kempitiya, Seppo Sierla, Daswin De Silva, Matti Yli-Ojanpera,\n  Damminda Alahakoon, Valeriy Vyatkin", "title": "An Artificial Intelligence Framework for Bidding Optimization with\n  Uncertainty in Multiple Frequency Reserve Markets", "comments": null, "journal-ref": "Applied Energy, Volume 280, 15 December 2020, 115918", "doi": "10.1016/j.apenergy.2020.115918", "report-no": null, "categories": "cs.AI cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The global ambitions of a carbon-neutral society necessitate a stable and\nrobust smart grid that capitalises on frequency reserves of renewable energy.\nFrequency reserves are resources that adjust power production or consumption in\nreal time to react to a power grid frequency deviation. Revenue generation\nmotivates the availability of these resources for managing such deviations.\nHowever, limited research has been conducted on data-driven decisions and\noptimal bidding strategies for trading such capacities in multiple frequency\nreserves markets. We address this limitation by making the following research\ncontributions. Firstly, a generalised model is designed based on an extensive\nstudy of critical characteristics of global frequency reserves markets.\nSecondly, three bidding strategies are proposed, based on this market model, to\ncapitalise on price peaks in multi-stage markets. Two strategies are proposed\nfor non-reschedulable loads, in which case the bidding strategy aims to select\nthe market with the highest anticipated price, and the third bidding strategy\nfocuses on rescheduling loads to hours on which highest reserve market prices\nare anticipated. The third research contribution is an Artificial Intelligence\n(AI) based bidding optimization framework that implements these three\nstrategies, with novel uncertainty metrics that supplement data-driven price\nprediction. Finally, the framework is evaluated empirically using a case study\nof multiple frequency reserves markets in Finland. The results from this\nevaluation confirm the effectiveness of the proposed bidding strategies and the\nAI-based bidding optimization framework in terms of cumulative revenue\ngeneration, leading to an increased availability of frequency reserves.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 12:04:29 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Kempitiya", "Thimal", ""], ["Sierla", "Seppo", ""], ["De Silva", "Daswin", ""], ["Yli-Ojanpera", "Matti", ""], ["Alahakoon", "Damminda", ""], ["Vyatkin", "Valeriy", ""]]}, {"id": "2104.01910", "submitter": "Yuanpeng He", "authors": "Yuanpeng He", "title": "Combining conflicting ordinal quantum evidences utilizing individual\n  reliability", "comments": "44 pages, 20 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to combine uncertain information from different sources has been a hot\ntopic for years. However, with respect to ordinal quantum evidences contained\nin information, there is no any referable work which is able to provide a\nsolution to this kind of problem. Besides, the method to dispel uncertainty of\nquantum information is still an open issue. Therefore, in this paper, a\nspecially designed method is designed to provide an excellent method which\nimproves the combination of ordinal quantum evidences reasonably and reduce the\neffects brought by uncertainty contained in quantum information simultaneously.\nBesides, some actual applications are provided to verify the correctness and\nvalidity of the proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 13:18:38 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["He", "Yuanpeng", ""]]}, {"id": "2104.01939", "submitter": "Quanlin Chen", "authors": "Quanlin Chen", "title": "NQMIX: Non-monotonic Value Function Factorization for Deep Multi-Agent\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-agent value-based approaches recently make great progress, especially\nvalue decomposition methods. However, there are still a lot of limitations in\nvalue function factorization. In VDN, the joint action-value function is the\nsum of per-agent action-value function while the joint action-value function of\nQMIX is the monotonic mixing of per-agent action-value function. To some\nextent, QTRAN reduces the limitation of joint action-value functions that can\nbe represented, but it has unsatisfied performance in complex tasks. In this\npaper, in order to extend the class of joint value functions that can be\nrepresented, we propose a novel actor-critic method called NQMIX. NQMIX\nintroduces an off-policy policy gradient on QMIX and modify its network\narchitecture, which can remove the monotonicity constraint of QMIX and\nimplement a non-monotonic value function factorization for the joint\naction-value function. In addition, NQMIX takes the state-value as the learning\ntarget, which overcomes the problem in QMIX that the learning target is\noverestimated. Furthermore, NQMIX can be extended to continuous action space\nsettings by introducing deterministic policy gradient on itself. Finally, we\nevaluate our actor-critic methods on SMAC domain, and show that it has a\nstronger performance than COMA and QMIX on complex maps with heterogeneous\nagent types. In addition, our ablation results show that our modification of\nmixer is effective.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 14:56:09 GMT"}, {"version": "v2", "created": "Sun, 18 Apr 2021 05:10:30 GMT"}, {"version": "v3", "created": "Mon, 21 Jun 2021 14:29:54 GMT"}, {"version": "v4", "created": "Tue, 13 Jul 2021 11:13:45 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Chen", "Quanlin", ""]]}, {"id": "2104.01955", "submitter": "Dhivya Chandrasekaran", "authors": "Dhivya Chandrasekaran and Vijay Mago", "title": "Automating Transfer Credit Assessment in Student Mobility -- A Natural\n  Language Processing-based Approach", "comments": "13 pages and 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Student mobility or academic mobility involves students moving between\ninstitutions during their post-secondary education, and one of the challenging\ntasks in this process is to assess the transfer credits to be offered to the\nincoming student. In general, this process involves domain experts comparing\nthe learning outcomes of the courses, to decide on offering transfer credits to\nthe incoming students. This manual implementation is not only labor-intensive\nbut also influenced by undue bias and administrative complexity. The proposed\nresearch article focuses on identifying a model that exploits the advancements\nin the field of Natural Language Processing (NLP) to effectively automate this\nprocess. Given the unique structure, domain specificity, and complexity of\nlearning outcomes (LOs), a need for designing a tailor-made model arises. The\nproposed model uses a clustering-inspired methodology based on knowledge-based\nsemantic similarity measures to assess the taxonomic similarity of LOs and a\ntransformer-based semantic similarity model to assess the semantic similarity\nof the LOs. The similarity between LOs is further aggregated to form course to\ncourse similarity. Due to the lack of quality benchmark datasets, a new\nbenchmark dataset containing seven course-to-course similarity measures is\nproposed. Understanding the inherent need for flexibility in the\ndecision-making process the aggregation part of the model offers tunable\nparameters to accommodate different scenarios. While providing an efficient\nmodel to assess the similarity between courses with existing resources, this\nresearch work steers future research attempts to apply NLP in the field of\narticulation in an ideal direction by highlighting the persisting research\ngaps.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 15:14:59 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Chandrasekaran", "Dhivya", ""], ["Mago", "Vijay", ""]]}, {"id": "2104.01966", "submitter": "Martin Garriga", "authors": "Damian Andrew Tamburri, Willem-Jan Van den Heuvel, Martin Garriga", "title": "DataOps for Societal Intelligence: a Data Pipeline for Labor Market\n  Skills Extraction and Matching", "comments": null, "journal-ref": "2020 IEEE 21st International Conference on Information Reuse and\n  Integration for Data Science (IRI), Las Vegas, NV, USA, 2020, pp. 391-394", "doi": "10.1109/IRI49571.2020.00063", "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Big Data analytics supported by AI algorithms can support skills localization\nand retrieval in the context of a labor market intelligence problem. We\nformulate and solve this problem through specific DataOps models, blending data\nsources from administrative and technical partners in several countries into\ncooperation, creating shared knowledge to support policy and decision-making.\nWe then focus on the critical task of skills extraction from resumes and\nvacancies featuring state-of-the-art machine learning models. We showcase\npreliminary results with applied machine learning on real data from the\nemployment agencies of the Netherlands and the Flemish region in Belgium. The\nfinal goal is to match these skills to standard ontologies of skills, jobs and\noccupations.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 15:37:25 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Tamburri", "Damian Andrew", ""], ["Heuvel", "Willem-Jan Van den", ""], ["Garriga", "Martin", ""]]}, {"id": "2104.01978", "submitter": "Haoqi Li", "authors": "Haoqi Li, Yelin Kim, Cheng-Hao Kuo, Shrikanth Narayanan", "title": "Acted vs. Improvised: Domain Adaptation for Elicitation Approaches in\n  Audio-Visual Emotion Recognition", "comments": "paper accepted by INTERSPEECH2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Key challenges in developing generalized automatic emotion recognition\nsystems include scarcity of labeled data and lack of gold-standard references.\nEven for the cues that are labeled as the same emotion category, the\nvariability of associated expressions can be high depending on the elicitation\ncontext e.g., emotion elicited during improvised conversations vs. acted\nsessions with predefined scripts. In this work, we regard the emotion\nelicitation approach as domain knowledge, and explore domain transfer learning\ntechniques on emotional utterances collected under different emotion\nelicitation approaches, particularly with limited labeled target samples. Our\nemotion recognition model combines the gradient reversal technique with an\nentropy loss function as well as the softlabel loss, and the experiment results\nshow that domain transfer learning methods can be employed to alleviate the\ndomain mismatch between different elicitation approaches. Our work provides new\ninsights into emotion data collection, particularly the impact of its\nelicitation strategies, and the importance of domain adaptation in emotion\nrecognition aiming for generalized systems.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 15:59:31 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 14:42:04 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Li", "Haoqi", ""], ["Kim", "Yelin", ""], ["Kuo", "Cheng-Hao", ""], ["Narayanan", "Shrikanth", ""]]}, {"id": "2104.01980", "submitter": "Fahad Alhasoun", "authors": "Fahad Alhasoun, Sarah Alnegheimish, Joshua Tenenbaum", "title": "Probabilistic Programming Bots in Intuitive Physics Game Play", "comments": "Shorter version to appear in AAAI-21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent findings suggest that humans deploy cognitive mechanism of physics\nsimulation engines to simulate the physics of objects. We propose a framework\nfor bots to deploy probabilistic programming tools for interacting with\nintuitive physics environments. The framework employs a physics simulation in a\nprobabilistic way to infer about moves performed by an agent in a setting\ngoverned by Newtonian laws of motion. However, methods of probabilistic\nprograms can be slow in such setting due to their need to generate many\nsamples. We complement the model with a model-free approach to aid the sampling\nprocedures in becoming more efficient through learning from experience during\ngame playing. We present an approach where combining model-free approaches (a\nconvolutional neural network in our model) and model-based approaches\n(probabilistic physics simulation) is able to achieve what neither could alone.\nThis way the model outperforms an all model-free or all model-based approach.\nWe discuss a case study showing empirical results of the performance of the\nmodel on the game of Flappy Bird.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 16:14:41 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Alhasoun", "Fahad", ""], ["Alnegheimish", "Sarah", ""], ["Tenenbaum", "Joshua", ""]]}, {"id": "2104.02032", "submitter": "Kolawole Ogunsina", "authors": "Kolawole Ogunsina and Wendy A. Okolo", "title": "Artificial Neural Network Modeling for Airline Disruption Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Since the 1970s, most airlines have incorporated computerized support for\nmanaging disruptions during flight schedule execution. However, existing\nplatforms for airline disruption management (ADM) employ monolithic system\ndesign methods that rely on the creation of specific rules and requirements\nthrough explicit optimization routines, before a system that meets the\nspecifications is designed. Thus, current platforms for ADM are unable to\nreadily accommodate additional system complexities resulting from the\nintroduction of new capabilities, such as the introduction of unmanned aerial\nsystems (UAS), operations and infrastructure, to the system. To this end, we\nuse historical data on airline scheduling and operations recovery to develop a\nsystem of artificial neural networks (ANNs), which describe a predictive\ntransfer function model (PTFM) for promptly estimating the recovery impact of\ndisruption resolutions at separate phases of flight schedule execution during\nADM. Furthermore, we provide a modular approach for assessing and executing the\nPTFM by employing a parallel ensemble method to develop generative routines\nthat amalgamate the system of ANNs. Our modular approach ensures that current\nindustry standards for tardiness in flight schedule execution during ADM are\nsatisfied, while accurately estimating appropriate time-based performance\nmetrics for the separate phases of flight schedule execution.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 17:37:24 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 13:24:05 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Ogunsina", "Kolawole", ""], ["Okolo", "Wendy A.", ""]]}, {"id": "2104.02096", "submitter": "Zhiyuan Fang", "authors": "Zhiyuan Fang, Jianfeng Wang, Xiaowei Hu, Lijuan Wang, Yezhou Yang,\n  Zicheng Liu", "title": "Compressing Visual-linguistic Model via Knowledge Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite exciting progress in pre-training for visual-linguistic (VL)\nrepresentations, very few aspire to a small VL model. In this paper, we study\nknowledge distillation (KD) to effectively compress a transformer-based large\nVL model into a small VL model. The major challenge arises from the\ninconsistent regional visual tokens extracted from different detectors of\nTeacher and Student, resulting in the misalignment of hidden representations\nand attention distributions. To address the problem, we retrain and adapt the\nTeacher by using the same region proposals from Student's detector while the\nfeatures are from Teacher's own object detector. With aligned network inputs,\nthe adapted Teacher is capable of transferring the knowledge through the\nintermediate representations. Specifically, we use the mean square error loss\nto mimic the attention distribution inside the transformer block and present a\ntoken-wise noise contrastive loss to align the hidden state by contrasting with\nnegative representations stored in a sample queue. To this end, we show that\nour proposed distillation significantly improves the performance of small VL\nmodels on image captioning and visual question answering tasks. It reaches\n120.8 in CIDEr score on COCO captioning, an improvement of 5.1 over its\nnon-distilled counterpart; and an accuracy of 69.8 on VQA 2.0, a 0.8 gain from\nthe baseline. Our extensive experiments and ablations confirm the effectiveness\nof VL distillation in both pre-training and fine-tuning stages.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 18:02:17 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Fang", "Zhiyuan", ""], ["Wang", "Jianfeng", ""], ["Hu", "Xiaowei", ""], ["Wang", "Lijuan", ""], ["Yang", "Yezhou", ""], ["Liu", "Zicheng", ""]]}, {"id": "2104.02115", "submitter": "Hadeel Al-Negheimish", "authors": "Hadeel Al-Negheimish, Pranava Madhyastha, Alessandra Russo", "title": "Discrete Reasoning Templates for Natural Language Understanding", "comments": "Published at EACL 2021 SRW", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reasoning about information from multiple parts of a passage to derive an\nanswer is an open challenge for reading-comprehension models. In this paper, we\npresent an approach that reasons about complex questions by decomposing them to\nsimpler subquestions that can take advantage of single-span extraction\nreading-comprehension models, and derives the final answer according to\ninstructions in a predefined reasoning template. We focus on subtraction-based\narithmetic questions and evaluate our approach on a subset of the DROP dataset.\nWe show that our approach is competitive with the state-of-the-art while being\ninterpretable and requires little supervision\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 18:56:56 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Al-Negheimish", "Hadeel", ""], ["Madhyastha", "Pranava", ""], ["Russo", "Alessandra", ""]]}, {"id": "2104.02137", "submitter": "Hongming Zhang", "authors": "Hongming Zhang, Xin Liu, Haojie Pan, Haowen Ke, Jiefu Ou, Tianqing\n  Fang, Yangqiu Song", "title": "ASER: Towards Large-scale Commonsense Knowledge Acquisition via\n  Higher-order Selectional Preference over Eventualities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Commonsense knowledge acquisition and reasoning have long been a core\nartificial intelligence problem. However, in the past, there has been a lack of\nscalable methods to collect commonsense knowledge. In this paper, we propose to\ndevelop principles for collecting commonsense knowledge based on selectional\npreference. We generalize the definition of selectional preference from one-hop\nlinguistic syntactic relations to higher-order relations over linguistic\ngraphs. Unlike previous commonsense knowledge definition (e.g., ConceptNet),\nthe selectional preference (SP) knowledge only relies on statistical\ndistribution over linguistic graphs, which can be efficiently and accurately\nacquired from the unlabeled corpus with modern tools. Following this principle,\nwe develop a large-scale eventuality (a linguistic term covering activity,\nstate, and event)-based knowledge graph ASER, where each eventuality is\nrepresented as a dependency graph, and the relation between them is a discourse\nrelation defined in shallow discourse parsing. The higher-order selectional\npreference over collected linguistic graphs reflects various kinds of\ncommonsense knowledge. Moreover, motivated by the observation that humans\nunderstand events by abstracting the observed events to a higher level and can\nthus transferring their knowledge to new events, we propose a conceptualization\nmodule to significantly boost the coverage of ASER. In total, ASER contains 438\nmillion eventualities and 648 million edges between eventualities. After\nconceptualization with Probase, a selectional preference based concept-instance\nrelational knowledge base, our concept graph contains 15 million conceptualized\neventualities and 224 million edges between them. Detailed analysis is provided\nto demonstrate its quality. All the collected data, APIs, and tools are\navailable at https://github.com/HKUST-KnowComp/ASER.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 20:23:46 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Zhang", "Hongming", ""], ["Liu", "Xin", ""], ["Pan", "Haojie", ""], ["Ke", "Haowen", ""], ["Ou", "Jiefu", ""], ["Fang", "Tianqing", ""], ["Song", "Yangqiu", ""]]}, {"id": "2104.02144", "submitter": "Abolfazl Farahani", "authors": "Abolfazl Farahani, Behrouz Pourshojae, Khaled Rasheed, Hamid R.\n  Arabnia", "title": "A Concise Review of Transfer Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The availability of abundant labeled data in recent years led the researchers\nto introduce a methodology called transfer learning, which utilizes existing\ndata in situations where there are difficulties in collecting new annotated\ndata. Transfer learning aims to boost the performance of a target learner by\napplying another related source data. In contrast to the traditional machine\nlearning and data mining techniques, which assume that the training and testing\ndata lie from the same feature space and distribution, transfer learning can\nhandle situations where there is a discrepancy between domains and\ndistributions. These characteristics give the model the potential to utilize\nthe available related source data and extend the underlying knowledge to the\ntarget task achieving better performance. This survey paper aims to give a\nconcise review of traditional and current transfer learning settings, existing\nchallenges, and related approaches.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 20:34:55 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Farahani", "Abolfazl", ""], ["Pourshojae", "Behrouz", ""], ["Rasheed", "Khaled", ""], ["Arabnia", "Hamid R.", ""]]}, {"id": "2104.02155", "submitter": "Samuel Henrique Silva", "authors": "Samuel Henrique Silva, Arun Das, Ian Scarff, Peyman Najafirad", "title": "Adaptive Clustering of Robust Semantic Representations for Adversarial\n  Image Purification", "comments": "11 pages, 5 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep Learning models are highly susceptible to adversarial manipulations that\ncan lead to catastrophic consequences. One of the most effective methods to\ndefend against such disturbances is adversarial training but at the cost of\ngeneralization of unseen attacks and transferability across models. In this\npaper, we propose a robust defense against adversarial attacks, which is model\nagnostic and generalizable to unseen adversaries. Initially, with a baseline\nmodel, we extract the latent representations for each class and adaptively\ncluster the latent representations that share a semantic similarity. We obtain\nthe distributions for the clustered latent representations and from their\noriginating images, we learn semantic reconstruction dictionaries (SRD). We\nadversarially train a new model constraining the latent space representation to\nminimize the distance between the adversarial latent representation and the\ntrue cluster distribution. To purify the image, we decompose the input into low\nand high-frequency components. The high-frequency component is reconstructed\nbased on the most adequate SRD from the clean dataset. In order to evaluate the\nmost adequate SRD, we rely on the distance between robust latent\nrepresentations and semantic cluster distributions. The output is a purified\nimage with no perturbation. Image purification on CIFAR-10 and ImageNet-10\nusing our proposed method improved the accuracy by more than 10% compared to\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 21:07:04 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 15:22:42 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Silva", "Samuel Henrique", ""], ["Das", "Arun", ""], ["Scarff", "Ian", ""], ["Najafirad", "Peyman", ""]]}, {"id": "2104.02199", "submitter": "Mehdi Sadi", "authors": "Kaniz Mishty, Mehdi Sadi", "title": "Designing Efficient and High-performance AI Accelerators with Customized\n  STT-MRAM", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we demonstrate the design of efficient and high-performance\nAI/Deep Learning accelerators with customized STT-MRAM and a reconfigurable\ncore. Based on model-driven detailed design space exploration, we present the\ndesign methodology of an innovative scratchpad-assisted on-chip STT-MRAM based\nbuffer system for high-performance accelerators. Using analytically derived\nexpression of memory occupancy time of AI model weights and activation maps,\nthe volatility of STT-MRAM is adjusted with process and temperature variation\naware scaling of thermal stability factor to optimize the retention time,\nenergy, read/write latency, and area of STT-MRAM. From the analysis of modern\nAI workloads and accelerator implementation in 14nm technology, we verify the\nefficacy of our designed AI accelerator with STT-MRAM STT-AI. Compared to an\nSRAM-based implementation, the STT-AI accelerator achieves 75% area and 3%\npower savings at iso-accuracy. Furthermore, with a relaxed bit error rate and\nnegligible AI accuracy trade-off, the designed STT-AI Ultra accelerator\nachieves 75.4%, and 3.5% savings in area and power, respectively over regular\nSRAM-based accelerators.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 00:34:06 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Mishty", "Kaniz", ""], ["Sadi", "Mehdi", ""]]}, {"id": "2104.02206", "submitter": "Mengmi Zhang", "authors": "Mengmi Zhang, Rohil Badkundri, Morgan B. Talbot, Gabriel Kreiman", "title": "Hypothesis-driven Stream Learning with Augmented Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stream learning refers to the ability to acquire and transfer knowledge\nacross a continuous stream of data without forgetting and without repeated\npasses over the data. A common way to avoid catastrophic forgetting is to\nintersperse new examples with replays of old examples stored as image pixels or\nreproduced by generative models. Here, we considered stream learning in image\nclassification tasks and proposed a novel hypotheses-driven Augmented Memory\nNetwork, which efficiently consolidates previous knowledge with a limited\nnumber of hypotheses in the augmented memory and replays relevant hypotheses to\navoid catastrophic forgetting. The advantages of hypothesis-driven replay over\nimage pixel replay and generative replay are two-fold. First, hypothesis-based\nknowledge consolidation avoids redundant information in the image pixel space\nand makes memory usage more efficient. Second, hypotheses in the augmented\nmemory can be re-used for learning new tasks, improving generalization and\ntransfer learning ability. We evaluated our method on three stream learning\nobject recognition datasets. Our method performs comparably well or better than\nSOTA methods, while offering more efficient memory usage. All source code and\ndata are publicly available https://github.com/kreimanlab/AugMem.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 00:53:01 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 01:25:33 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Zhang", "Mengmi", ""], ["Badkundri", "Rohil", ""], ["Talbot", "Morgan B.", ""], ["Kreiman", "Gabriel", ""]]}, {"id": "2104.02214", "submitter": "Basheer Qolomany", "authors": "Ghezlane Halhoul Merabet, Mohamed Essaaidi, Mohamed Ben Haddou,\n  Basheer Qolomany, Junaid Qadir, Muhammad Anan, Ala Al-Fuqaha, Mohamed Riduan\n  Abid, Driss Benhaddou", "title": "Intelligent Building Control Systems for Thermal Comfort and\n  Energy-Efficiency: A Systematic Review of Artificial Intelligence-Assisted\n  Techniques", "comments": "arXiv admin note: text overlap with arXiv:2006.12559", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building operations represent a significant percentage of the total primary\nenergy consumed in most countries due to the proliferation of Heating,\nVentilation and Air-Conditioning (HVAC) installations in response to the\ngrowing demand for improved thermal comfort. Reducing the associated energy\nconsumption while maintaining comfortable conditions in buildings are\nconflicting objectives and represent a typical optimization problem that\nrequires intelligent system design. Over the last decade, different\nmethodologies based on the Artificial Intelligence (AI) techniques have been\ndeployed to find the sweet spot between energy use in HVAC systems and suitable\nindoor comfort levels to the occupants. This paper performs a comprehensive and\nan in-depth systematic review of AI-based techniques used for building control\nsystems by assessing the outputs of these techniques, and their implementations\nin the reviewed works, as well as investigating their abilities to improve the\nenergy-efficiency, while maintaining thermal comfort conditions. This enables a\nholistic view of (1) the complexities of delivering thermal comfort to users\ninside buildings in an energy-efficient way, and (2) the associated\nbibliographic material to assist researchers and experts in the field in\ntackling such a challenge. Among the 20 AI tools developed for both energy\nconsumption and comfort control, functions such as identification and\nrecognition patterns, optimization, predictive control. Based on the findings\nof this work, the application of AI technology in building control is a\npromising area of research and still an ongoing, i.e., the performance of\nAI-based control is not yet completely satisfactory. This is mainly due in part\nto the fact that these algorithms usually need a large amount of high-quality\nreal-world data, which is lacking in the building or, more precisely, the\nenergy sector.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 01:04:28 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Merabet", "Ghezlane Halhoul", ""], ["Essaaidi", "Mohamed", ""], ["Haddou", "Mohamed Ben", ""], ["Qolomany", "Basheer", ""], ["Qadir", "Junaid", ""], ["Anan", "Muhammad", ""], ["Al-Fuqaha", "Ala", ""], ["Abid", "Mohamed Riduan", ""], ["Benhaddou", "Driss", ""]]}, {"id": "2104.02215", "submitter": "Mengmi Zhang", "authors": "Philipp Bomatter, Mengmi Zhang, Dimitar Karev, Spandan Madan, Claire\n  Tseng, Gabriel Kreiman", "title": "When Pigs Fly: Contextual Reasoning in Synthetic and Natural Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context is of fundamental importance to both human and machine vision -- an\nobject in the air is more likely to be an airplane, than a pig. The rich notion\nof context incorporates several aspects including physics rules, statistical\nco-occurrences, and relative object sizes, among others. While previous works\nhave crowd-sourced out-of-context photographs from the web to study scene\ncontext, controlling the nature and extent of contextual violations has been an\nextremely daunting task. Here we introduce a diverse, synthetic Out-of-Context\nDataset (OCD) with fine-grained control over scene context. By leveraging a 3D\nsimulation engine, we systematically control the gravity, object co-occurrences\nand relative sizes across 36 object categories in a virtual household\nenvironment. We then conduct a series of experiments to gain insights into the\nimpact of contextual cues on both human and machine vision using OCD. First, we\nconduct psycho-physics experiments to establish a human benchmark for\nout-of-context recognition, and then compare it with state-of-the-art computer\nvision models to quantify the gap between the two. Finally, we propose a\ncontext-aware recognition transformer model, fusing object and contextual\ninformation via multi-head attention. Our model captures useful information for\ncontextual reasoning, enabling human-level performance and significantly better\nrobustness in out-of-context conditions compared to baseline models across OCD\nand other existing out-of-context natural image datasets. All source code and\ndata are publicly available https://github.com/kreimanlab/WhenPigsFlyContext.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 01:05:34 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Bomatter", "Philipp", ""], ["Zhang", "Mengmi", ""], ["Karev", "Dimitar", ""], ["Madan", "Spandan", ""], ["Tseng", "Claire", ""], ["Kreiman", "Gabriel", ""]]}, {"id": "2104.02226", "submitter": "Boyuan Chen", "authors": "Boyuan Chen, Yu Li, Sunand Raghupathi, Hod Lipson", "title": "Beyond Categorical Label Representations for Image Classification", "comments": "International Conference on Learning Representations (ICLR 2021).\n  Project page is at\n  \\url{https://www.creativemachineslab.com/label-representation.html}", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We find that the way we choose to represent data labels can have a profound\neffect on the quality of trained models. For example, training an image\nclassifier to regress audio labels rather than traditional categorical\nprobabilities produces a more reliable classification. This result is\nsurprising, considering that audio labels are more complex than simpler\nnumerical probabilities or text. We hypothesize that high dimensional, high\nentropy label representations are generally more useful because they provide a\nstronger error signal. We support this hypothesis with evidence from various\nlabel representations including constant matrices, spectrograms, shuffled\nspectrograms, Gaussian mixtures, and uniform random matrices of various\ndimensionalities. Our experiments reveal that high dimensional, high entropy\nlabels achieve comparable accuracy to text (categorical) labels on the standard\nimage classification task, but features learned through our label\nrepresentations exhibit more robustness under various adversarial attacks and\nbetter effectiveness with a limited amount of training data. These results\nsuggest that label representation may play a more important role than\npreviously thought. The project website is at\n\\url{https://www.creativemachineslab.com/label-representation.html}.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 01:31:04 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Chen", "Boyuan", ""], ["Li", "Yu", ""], ["Raghupathi", "Sunand", ""], ["Lipson", "Hod", ""]]}, {"id": "2104.02233", "submitter": "Seyed Hamed Fatemi Langroudi", "authors": "Hamed F. Langroudi, Vedant Karia, Tej Pandit, Dhireesha Kudithipudi", "title": "TENT: Efficient Quantization of Neural Networks on the tiny Edge with\n  Tapered FixEd PoiNT", "comments": "poster presented at the first tinyML Research Symposium, March 26,\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this research, we propose a new low-precision framework, TENT, to leverage\nthe benefits of a tapered fixed-point numerical format in TinyML models. We\nintroduce a tapered fixed-point quantization algorithm that matches the\nnumerical format's dynamic range and distribution to that of the deep neural\nnetwork model's parameter distribution at each layer. An accelerator\narchitecture for the tapered fixed-point with TENT framework is proposed.\nResults show that the accuracy on classification tasks improves up to ~31 %\nwith an energy overhead of ~17-30 % as compared to fixed-point, for ConvNet and\nResNet-18 models.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 01:54:32 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Langroudi", "Hamed F.", ""], ["Karia", "Vedant", ""], ["Pandit", "Tej", ""], ["Kudithipudi", "Dhireesha", ""]]}, {"id": "2104.02245", "submitter": "Wang Xin", "authors": "Xin Wang, Yang Zhao, Tangwen Yang, Qiuqi Ruan", "title": "Multi-Scale Context Aggregation Network with Attention-Guided for Crowd\n  Counting", "comments": null, "journal-ref": "ICSP2020", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Crowd counting aims to predict the number of people and generate the density\nmap in the image. There are many challenges, including varying head scales, the\ndiversity of crowd distribution across images and cluttered backgrounds. In\nthis paper, we propose a multi-scale context aggregation network (MSCANet)\nbased on single-column encoder-decoder architecture for crowd counting, which\nconsists of an encoder based on a dense context-aware module (DCAM) and a\nhierarchical attention-guided decoder. To handle the issue of scale variation,\nwe construct the DCAM to aggregate multi-scale contextual information by\ndensely connecting the dilated convolution with varying receptive fields. The\nproposed DCAM can capture rich contextual information of crowd areas due to its\nlong-range receptive fields and dense scale sampling. Moreover, to suppress the\nbackground noise and generate a high-quality density map, we adopt a\nhierarchical attention-guided mechanism in the decoder. This helps to integrate\nmore useful spatial information from shallow feature maps of the encoder by\nintroducing multiple supervision based on semantic attention module (SAM).\nExtensive experiments demonstrate that the proposed approach achieves better\nperformance than other similar state-of-the-art methods on three challenging\nbenchmark datasets for crowd counting. The code is available at\nhttps://github.com/KingMV/MSCANet\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 02:24:06 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Wang", "Xin", ""], ["Zhao", "Yang", ""], ["Yang", "Tangwen", ""], ["Ruan", "Qiuqi", ""]]}, {"id": "2104.02271", "submitter": "Yang Yang", "authors": "Yang Yang, Yuanhao Liu, Hengyue Liang, Xibai Lou, Changhyun Choi", "title": "Attribute-Based Robotic Grasping with One-Grasp Adaptation", "comments": "Accepted to the IEEE International Conference on Robotics and\n  Automation (ICRA 2021). Project page:\n  https://sites.google.com/umn.edu/attributes-grasping", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robotic grasping is one of the most fundamental robotic manipulation tasks\nand has been actively studied. However, how to quickly teach a robot to grasp a\nnovel target object in clutter remains challenging. This paper attempts to\ntackle the challenge by leveraging object attributes that facilitate\nrecognition, grasping, and quick adaptation. In this work, we introduce an\nend-to-end learning method of attribute-based robotic grasping with one-grasp\nadaptation capability. Our approach fuses the embeddings of a workspace image\nand a query text using a gated-attention mechanism and learns to predict\ninstance grasping affordances. Besides, we utilize object persistence before\nand after grasping to learn a joint metric space of visual and textual\nattributes. Our model is self-supervised in a simulation that only uses basic\nobjects of various colors and shapes but generalizes to novel objects and\nreal-world scenes. We further demonstrate that our model is capable of adapting\nto novel objects with only one grasp data and improving instance grasping\nperformance significantly. Experimental results in both simulation and the real\nworld demonstrate that our approach achieves over 80\\% instance grasping\nsuccess rate on unknown objects, which outperforms several baselines by large\nmargins.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 03:40:46 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Yang", "Yang", ""], ["Liu", "Yuanhao", ""], ["Liang", "Hengyue", ""], ["Lou", "Xibai", ""], ["Choi", "Changhyun", ""]]}, {"id": "2104.02284", "submitter": "Ningyu Zhang", "authors": "Luoqiu Li, Zhen Bi, Hongbin Ye, Shumin Deng, Hui Chen, Huaixiao Tou", "title": "Text-guided Legal Knowledge Graph Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed the prosperity of legal artificial intelligence\nwith the development of technologies. In this paper, we propose a novel legal\napplication of legal provision prediction (LPP), which aims to predict the\nrelated legal provisions of affairs. We formulate this task as a challenging\nknowledge graph completion problem, which requires not only text understanding\nbut also graph reasoning. To this end, we propose a novel text-guided graph\nreasoning approach. We collect amounts of real-world legal provision data from\nthe Guangdong government service website and construct a legal dataset called\nLegalLPP. Extensive experimental results on the dataset show that our approach\nachieves better performance compared with baselines. The code and dataset are\navailable in \\url{https://github.com/zjunlp/LegalPP} for reproducibility.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 04:42:56 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 13:52:49 GMT"}, {"version": "v3", "created": "Wed, 28 Jul 2021 06:34:51 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Li", "Luoqiu", ""], ["Bi", "Zhen", ""], ["Ye", "Hongbin", ""], ["Deng", "Shumin", ""], ["Chen", "Hui", ""], ["Tou", "Huaixiao", ""]]}, {"id": "2104.02287", "submitter": "Wesley Holliday", "authors": "Matthew Harrison-Trainor, Wesley H. Holliday, and Thomas F. Icard III", "title": "Preferential Structures for Comparative Probabilistic Reasoning", "comments": "Postprint of AAAI 2017 paper, corrected to include a distinguished\n  set of states in Definitions 2-3 and 5 (resp. before Theorem 3) to match the\n  appropriate special case of the semantics of Holliday and Icard 2013 (resp.\n  van der Hoek 1996)", "journal-ref": "AAAI Conference on Artificial Intelligence, 2017, pp. 1135-1141", "doi": null, "report-no": null, "categories": "cs.AI cs.LO math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Qualitative and quantitative approaches to reasoning about uncertainty can\nlead to different logical systems for formalizing such reasoning, even when the\nlanguage for expressing uncertainty is the same. In the case of reasoning about\nrelative likelihood, with statements of the form $\\varphi\\succsim\\psi$\nexpressing that $\\varphi$ is at least as likely as $\\psi$, a standard\nqualitative approach using preordered preferential structures yields a\ndramatically different logical system than a quantitative approach using\nprobability measures. In fact, the standard preferential approach validates\nprinciples of reasoning that are incorrect from a probabilistic point of view.\nHowever, in this paper we show that a natural modification of the preferential\napproach yields exactly the same logical system as a probabilistic\napproach--not using single probability measures, but rather sets of probability\nmeasures. Thus, the same preferential structures used in the study of\nnon-monotonic logics and belief revision may be used in the study of\ncomparative probabilistic reasoning based on imprecise probabilities.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 05:00:20 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Harrison-Trainor", "Matthew", ""], ["Holliday", "Wesley H.", ""], ["Icard", "Thomas F.", "III"]]}, {"id": "2104.02297", "submitter": "Rui Wang", "authors": "Rui Wang, Xiaoqian Wang, David I. Inouye", "title": "Shapley Explanation Networks", "comments": "26 pages, 11 figures, accepted to ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Shapley values have become one of the most popular feature attribution\nexplanation methods. However, most prior work has focused on post-hoc Shapley\nexplanations, which can be computationally demanding due to its exponential\ntime complexity and preclude model regularization based on Shapley explanations\nduring training. Thus, we propose to incorporate Shapley values themselves as\nlatent representations in deep models thereby making Shapley explanations\nfirst-class citizens in the modeling paradigm. This intrinsic explanation\napproach enables layer-wise explanations, explanation regularization of the\nmodel during training, and fast explanation computation at test time. We define\nthe Shapley transform that transforms the input into a Shapley representation\ngiven a specific function. We operationalize the Shapley transform as a neural\nnetwork module and construct both shallow and deep networks, called ShapNets,\nby composing Shapley modules. We prove that our Shallow ShapNets compute the\nexact Shapley values and our Deep ShapNets maintain the missingness and\naccuracy properties of Shapley values. We demonstrate on synthetic and\nreal-world datasets that our ShapNets enable layer-wise Shapley explanations,\nnovel Shapley regularizations during training, and fast computation while\nmaintaining reasonable performance. Code is available at\nhttps://github.com/inouye-lab/ShapleyExplanationNetworks.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 05:42:12 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Wang", "Rui", ""], ["Wang", "Xiaoqian", ""], ["Inouye", "David I.", ""]]}, {"id": "2104.02303", "submitter": "Tomasz Kryjak", "authors": "Dominika Przewlocka-Rus, Marcin Kowalczyk, Tomasz Kryjak", "title": "Exploration of Hardware Acceleration Methods for an XNOR Traffic Signs\n  Classifier", "comments": "12 pages, 2 figures, 6 tables. Submitted for the CORES 2021\n  conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning algorithms are a key component of many state-of-the-art vision\nsystems, especially as Convolutional Neural Networks (CNN) outperform most\nsolutions in the sense of accuracy. To apply such algorithms in real-time\napplications, one has to address the challenges of memory and computational\ncomplexity. To deal with the first issue, we use networks with reduced\nprecision, specifically a binary neural network (also known as XNOR). To\nsatisfy the computational requirements, we propose to use highly parallel and\nlow-power FPGA devices. In this work, we explore the possibility of\naccelerating XNOR networks for traffic sign classification. The trained binary\nnetworks are implemented on the ZCU 104 development board, equipped with a Zynq\nUltraScale+ MPSoC device using two different approaches. Firstly, we propose a\ncustom HDL accelerator for XNOR networks, which enables the inference with\nalmost 450 fps. Even better results are obtained with the second method - the\nXilinx FINN accelerator - enabling to process input images with around 550\nframe rate. Both approaches provide over 96% accuracy on the test set.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 06:01:57 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Przewlocka-Rus", "Dominika", ""], ["Kowalczyk", "Marcin", ""], ["Kryjak", "Tomasz", ""]]}, {"id": "2104.02322", "submitter": "Mehrdad Khani", "authors": "Mehrdad Khani, Vibhaalakshmi Sivaraman, Mohammad Alizadeh", "title": "Efficient Video Compression via Content-Adaptive Super-Resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video compression is a critical component of Internet video delivery. Recent\nwork has shown that deep learning techniques can rival or outperform\nhuman-designed algorithms, but these methods are significantly less compute and\npower-efficient than existing codecs. This paper presents a new approach that\naugments existing codecs with a small, content-adaptive super-resolution model\nthat significantly boosts video quality. Our method, SRVC, encodes video into\ntwo bitstreams: (i) a content stream, produced by compressing downsampled\nlow-resolution video with the existing codec, (ii) a model stream, which\nencodes periodic updates to a lightweight super-resolution neural network\ncustomized for short segments of the video. SRVC decodes the video by passing\nthe decompressed low-resolution video frames through the (time-varying)\nsuper-resolution model to reconstruct high-resolution video frames. Our results\nshow that to achieve the same PSNR, SRVC requires 16% of the bits-per-pixel of\nH.265 in slow mode, and 2% of the bits-per-pixel of DVC, a recent deep\nlearning-based video compression scheme. SRVC runs at 90 frames per second on a\nNVIDIA V100 GPU.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 07:01:06 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Khani", "Mehrdad", ""], ["Sivaraman", "Vibhaalakshmi", ""], ["Alizadeh", "Mohammad", ""]]}, {"id": "2104.02324", "submitter": "Tianning Yuan", "authors": "Tianning Yuan (1), Fang Wan (1), Mengying Fu (1), Jianzhuang Liu (2),\n  Songcen Xu (2), Xiangyang Ji (3), Qixiang Ye (1) ((1) University of Chinese\n  Academy of Sciences, Beijing, China, (2) Noah's Ark Lab, Huawei Technologies,\n  Shenzhen, China, (3) Tsinghua University, Beijing, China)", "title": "Multiple instance active learning for object detection", "comments": "10 pages, 7 figures, 5 tables. Code is available at\n  https://github.com/yuantn/MI-AOD", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the substantial progress of active learning for image recognition,\nthere still lacks an instance-level active learning method specified for object\ndetection. In this paper, we propose Multiple Instance Active Object Detection\n(MI-AOD), to select the most informative images for detector training by\nobserving instance-level uncertainty. MI-AOD defines an instance uncertainty\nlearning module, which leverages the discrepancy of two adversarial instance\nclassifiers trained on the labeled set to predict instance uncertainty of the\nunlabeled set. MI-AOD treats unlabeled images as instance bags and feature\nanchors in images as instances, and estimates the image uncertainty by\nre-weighting instances in a multiple instance learning (MIL) fashion. Iterative\ninstance uncertainty learning and re-weighting facilitate suppressing noisy\ninstances, toward bridging the gap between instance uncertainty and image-level\nuncertainty. Experiments validate that MI-AOD sets a solid baseline for\ninstance-level active learning. On commonly used object detection datasets,\nMI-AOD outperforms state-of-the-art methods with significant margins,\nparticularly when the labeled sets are small. Code is available at\nhttps://github.com/yuantn/MI-AOD.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 07:03:38 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Yuan", "Tianning", ""], ["Wan", "Fang", ""], ["Fu", "Mengying", ""], ["Liu", "Jianzhuang", ""], ["Xu", "Songcen", ""], ["Ji", "Xiangyang", ""], ["Ye", "Qixiang", ""]]}, {"id": "2104.02357", "submitter": "Chen Ju", "authors": "Chen Ju, Peisen Zhao, Siheng Chen, Ya Zhang, Xiaoyun Zhang, Qi Tian", "title": "Adaptive Mutual Supervision for Weakly-Supervised Temporal Action\n  Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly-supervised temporal action localization aims to localize actions in\nuntrimmed videos with only video-level action category labels. Most of previous\nmethods ignore the incompleteness issue of Class Activation Sequences (CAS),\nsuffering from trivial localization results. To solve this issue, we introduce\nan adaptive mutual supervision framework (AMS) with two branches, where the\nbase branch adopts CAS to localize the most discriminative action regions,\nwhile the supplementary branch localizes the less discriminative action regions\nthrough a novel adaptive sampler. The adaptive sampler dynamically updates the\ninput of the supplementary branch with a sampling weight sequence negatively\ncorrelated with the CAS from the base branch, thereby prompting the\nsupplementary branch to localize the action regions underestimated by the base\nbranch. To promote mutual enhancement between these two branches, we construct\nmutual location supervision. Each branch leverages location pseudo-labels\ngenerated from the other branch as localization supervision. By alternately\noptimizing the two branches in multiple iterations, we progressively complete\naction regions. Extensive experiments on THUMOS14 and ActivityNet1.2\ndemonstrate that the proposed AMS method significantly outperforms the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 08:31:10 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Ju", "Chen", ""], ["Zhao", "Peisen", ""], ["Chen", "Siheng", ""], ["Zhang", "Ya", ""], ["Zhang", "Xiaoyun", ""], ["Tian", "Qi", ""]]}, {"id": "2104.02361", "submitter": "Yiming Li", "authors": "Yiming Li, Tongqing Zhai, Yong Jiang, Zhifeng Li, Shu-Tao Xia", "title": "Backdoor Attack in the Physical World", "comments": "This work was done when Yiming Li was an intern at Tencent AI Lab,\n  supported by the Tencent Rhino-Bird Elite Training Program (2020). This is a\n  6-pages short version of our ongoing work, `Rethinking the Trigger of\n  Backdoor Attack' (arXiv:2004.04692). It is accepted by the non-archival ICLR\n  2021 workshop on Robust and Reliable Machine Learning in the Real World.\n  arXiv admin note: substantial text overlap with arXiv:2004.04692", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Backdoor attack intends to inject hidden backdoor into the deep neural\nnetworks (DNNs), such that the prediction of infected models will be\nmaliciously changed if the hidden backdoor is activated by the attacker-defined\ntrigger. Currently, most existing backdoor attacks adopted the setting of\nstatic trigger, $i.e.,$ triggers across the training and testing images follow\nthe same appearance and are located in the same area. In this paper, we revisit\nthis attack paradigm by analyzing trigger characteristics. We demonstrate that\nthis attack paradigm is vulnerable when the trigger in testing images is not\nconsistent with the one used for training. As such, those attacks are far less\neffective in the physical world, where the location and appearance of the\ntrigger in the digitized image may be different from that of the one used for\ntraining. Moreover, we also discuss how to alleviate such vulnerability. We\nhope that this work could inspire more explorations on backdoor properties, to\nhelp the design of more advanced backdoor attack and defense methods.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 08:37:33 GMT"}, {"version": "v2", "created": "Sat, 24 Apr 2021 16:40:13 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Li", "Yiming", ""], ["Zhai", "Tongqing", ""], ["Jiang", "Yong", ""], ["Li", "Zhifeng", ""], ["Xia", "Shu-Tao", ""]]}, {"id": "2104.02395", "submitter": "M Tanveer PhD", "authors": "M.A. Ganaie (1) and Minghui Hu (2) and M. Tanveer*(1) and P.N.\n  Suganthan*(2) (* Corresponding Author (1) Department of Mathematics, Indian\n  Institute of Technology Indore, Simrol, Indore, 453552, India (2) School of\n  Electrical & Electronic Engineering, Nanyang Technological University,\n  Singapore)", "title": "Ensemble deep learning: A review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensemble learning combines several individual models to obtain better\ngeneralization performance. Currently, deep learning models with multilayer\nprocessing architecture is showing better performance as compared to the\nshallow or traditional classification models. Deep ensemble learning models\ncombine the advantages of both the deep learning models as well as the ensemble\nlearning such that the final model has better generalization performance. This\npaper reviews the state-of-art deep ensemble models and hence serves as an\nextensive summary for the researchers. The ensemble models are broadly\ncategorised into ensemble models like bagging, boosting and stacking, negative\ncorrelation based deep ensemble models, explicit/implicit ensembles,\nhomogeneous /heterogeneous ensemble, decision fusion strategies, unsupervised,\nsemi-supervised, reinforcement learning and online/incremental, multilabel\nbased deep ensemble models. Application of deep ensemble models in different\ndomains is also briefly discussed. Finally, we conclude this paper with some\nfuture recommendations and research directions.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 09:56:29 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Ganaie", "M. A.", ""], ["Hu", "Minghui", ""], ["Tanveer*", "M.", ""], ["Suganthan*", "P. N.", ""]]}, {"id": "2104.02425", "submitter": "Kashif Ahmad", "authors": "Senthil Kumar Jagatheesaperumal, Mohamed Rahouti, Kashif Ahmad, Ala\n  Al-Fuqaha, Mohsen Guizani", "title": "The Duo of Artificial Intelligence and Big Data for Industry 4.0: Review\n  of Applications, Techniques, Challenges, and Future Research Directions", "comments": "33 pages, 10 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The increasing need for economic, safe, and sustainable smart manufacturing\ncombined with novel technological enablers, has paved the way for Artificial\nIntelligence (AI) and Big Data in support of smart manufacturing. This implies\na substantial integration of AI, Industrial Internet of Things (IIoT),\nRobotics, Big data, Blockchain, 5G communications, in support of smart\nmanufacturing and the dynamical processes in modern industries. In this paper,\nwe provide a comprehensive overview of different aspects of AI and Big Data in\nIndustry 4.0 with a particular focus on key applications, techniques, the\nconcepts involved, key enabling technologies, challenges, and research\nperspective towards deployment of Industry 5.0. In detail, we highlight and\nanalyze how the duo of AI and Big Data is helping in different applications of\nIndustry 4.0. We also highlight key challenges in a successful deployment of AI\nand Big Data methods in smart industries with a particular emphasis on\ndata-related issues, such as availability, bias, auditing, management,\ninterpretability, communication, and different adversarial attacks and security\nissues. In a nutshell, we have explored the significance of AI and Big data\ntowards Industry 4.0 applications through panoramic reviews and discussions. We\nbelieve, this work will provide a baseline for future research in the domain.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 11:08:02 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 10:59:47 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Jagatheesaperumal", "Senthil Kumar", ""], ["Rahouti", "Mohamed", ""], ["Ahmad", "Kashif", ""], ["Al-Fuqaha", "Ala", ""], ["Guizani", "Mohsen", ""]]}, {"id": "2104.02443", "submitter": "Ahmed Elnaggar", "authors": "Ahmed Elnaggar, Wei Ding, Llion Jones, Tom Gibbs, Tamas Feher,\n  Christoph Angerer, Silvia Severini, Florian Matthes and Burkhard Rost", "title": "CodeTrans: Towards Cracking the Language of Silicon's Code Through\n  Self-Supervised Deep Learning and High Performance Computing", "comments": "28 pages, 6 tables and 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.CL cs.LG cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, a growing number of mature natural language processing\napplications make people's life more convenient. Such applications are built by\nsource code - the language in software engineering. However, the applications\nfor understanding source code language to ease the software engineering process\nare under-researched. Simultaneously, the transformer model, especially its\ncombination with transfer learning, has been proven to be a powerful technique\nfor natural language processing tasks. These breakthroughs point out a\npromising direction for process source code and crack software engineering\ntasks. This paper describes CodeTrans - an encoder-decoder transformer model\nfor tasks in the software engineering domain, that explores the effectiveness\nof encoder-decoder transformer models for six software engineering tasks,\nincluding thirteen sub-tasks. Moreover, we have investigated the effect of\ndifferent training strategies, including single-task learning, transfer\nlearning, multi-task learning, and multi-task learning with fine-tuning.\nCodeTrans outperforms the state-of-the-art models on all the tasks. To expedite\nfuture works in the software engineering domain, we have published our\npre-trained models of CodeTrans. https://github.com/agemagician/CodeTrans\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 11:57:12 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 06:51:32 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Elnaggar", "Ahmed", ""], ["Ding", "Wei", ""], ["Jones", "Llion", ""], ["Gibbs", "Tom", ""], ["Feher", "Tamas", ""], ["Angerer", "Christoph", ""], ["Severini", "Silvia", ""], ["Matthes", "Florian", ""], ["Rost", "Burkhard", ""]]}, {"id": "2104.02459", "submitter": "Andr\\'e Artelt", "authors": "Andr\\'e Artelt, Fabian Hinder, Valerie Vaquet, Robert Feldhans,\n  Barbara Hammer", "title": "Contrastive Explanations for Explaining Model Adaptations", "comments": "Fix some typos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many decision making systems deployed in the real world are not static - a\nphenomenon known as model adaptation takes place over time. The need for\ntransparency and interpretability of AI-based decision models is widely\naccepted and thus have been worked on extensively. Usually, explanation methods\nassume a static system that has to be explained. Explaining non-static systems\nis still an open research question, which poses the challenge how to explain\nmodel adaptations. In this contribution, we propose and (empirically) evaluate\na framework for explaining model adaptations by contrastive explanations. We\nalso propose a method for automatically finding regions in data space that are\naffected by a given model adaptation and thus should be explained.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 12:35:23 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 07:09:30 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Artelt", "Andr\u00e9", ""], ["Hinder", "Fabian", ""], ["Vaquet", "Valerie", ""], ["Feldhans", "Robert", ""], ["Hammer", "Barbara", ""]]}, {"id": "2104.02464", "submitter": "Prerit Terway", "authors": "Prerit Terway, Kenza Hamidouche, and Niraj K. Jha", "title": "Fast Design Space Exploration of Nonlinear Systems: Part II", "comments": "14 pages, 24 figures. arXiv admin note: substantial text overlap with\n  arXiv:2009.10214", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.AI cs.LG cs.NE cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nonlinear system design is often a multi-objective optimization problem\ninvolving search for a design that satisfies a number of predefined\nconstraints. The design space is typically very large since it includes all\npossible system architectures with different combinations of components\ncomposing each architecture. In this article, we address nonlinear system\ndesign space exploration through a two-step approach encapsulated in a\nframework called Fast Design Space Exploration of Nonlinear Systems (ASSENT).\nIn the first step, we use a genetic algorithm to search for system\narchitectures that allow discrete choices for component values or else only\ncomponent values for a fixed architecture. This step yields a coarse design\nsince the system may or may not meet the target specifications. In the second\nstep, we use an inverse design to search over a continuous space and fine-tune\nthe component values with the goal of improving the value of the objective\nfunction. We use a neural network to model the system response. The neural\nnetwork is converted into a mixed-integer linear program for active learning to\nsample component values efficiently. We illustrate the efficacy of ASSENT on\nproblems ranging from nonlinear system design to design of electrical circuits.\nExperimental results show that ASSENT achieves the same or better value of the\nobjective function compared to various other optimization techniques for\nnonlinear system design by up to 54%. We improve sample efficiency by 6-10x\ncompared to reinforcement learning based synthesis of electrical circuits.\n", "versions": [{"version": "v1", "created": "Mon, 5 Apr 2021 16:11:50 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 19:35:15 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Terway", "Prerit", ""], ["Hamidouche", "Kenza", ""], ["Jha", "Niraj K.", ""]]}, {"id": "2104.02468", "submitter": "Sanghoon Myung", "authors": "Sanghoon Myung, Hyunjae Jang, Byungseon Choi, Jisu Ryu, Hyuk Kim, Sang\n  Wuk Park, Changwook Jeong and Dae Sin Kim", "title": "A Novel Approach for Semiconductor Etching Process with Inductive Biases", "comments": "5 pages; accepted to NeurIPS 2020 Workshop on Interpretable Inductive\n  Biases and Physically Structured Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG physics.comp-ph physics.plasm-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The etching process is one of the most important processes in semiconductor\nmanufacturing. We have introduced the state-of-the-art deep learning model to\npredict the etching profiles. However, the significant problems violating\nphysics have been found through various techniques such as explainable\nartificial intelligence and representation of prediction uncertainty. To\naddress this problem, this paper presents a novel approach to apply the\ninductive biases for etching process. We demonstrate that our approach fits the\nmeasurement faster than physical simulator while following the physical\nbehavior. Our approach would bring a new opportunity for better etching process\nwith higher accuracy and lower cost.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 12:51:52 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Myung", "Sanghoon", ""], ["Jang", "Hyunjae", ""], ["Choi", "Byungseon", ""], ["Ryu", "Jisu", ""], ["Kim", "Hyuk", ""], ["Park", "Sang Wuk", ""], ["Jeong", "Changwook", ""], ["Kim", "Dae Sin", ""]]}, {"id": "2104.02478", "submitter": "Rui Song", "authors": "Rui Song and Fausto Giunchiglia and Ke Zhao and Hao Xu", "title": "Topological Regularization for Graph Neural Networks Augmentation", "comments": "20 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The complexity and non-Euclidean structure of graph data hinder the\ndevelopment of data augmentation methods similar to those in computer vision.\nIn this paper, we propose a feature augmentation method for graph nodes based\non topological regularization, in which topological structure information is\nintroduced into end-to-end model. Specifically, we first obtain topology\nembedding of nodes through unsupervised representation learning method based on\nrandom walk. Then, the topological embedding as additional features and the\noriginal node features are input into a dual graph neural network for\npropagation, and two different high-order neighborhood representations of nodes\nare obtained. On this basis, we propose a regularization technique to bridge\nthe differences between the two different node representations, eliminate the\nadverse effects caused by the topological features of graphs directly used, and\ngreatly improve the performance. We have carried out extensive experiments on a\nlarge number of datasets to prove the effectiveness of our model.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 01:37:44 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Song", "Rui", ""], ["Giunchiglia", "Fausto", ""], ["Zhao", "Ke", ""], ["Xu", "Hao", ""]]}, {"id": "2104.02484", "submitter": "Petr Marek", "authors": "Petr Marek, Vishal Ishwar Naik, Vincent Auvray, Anuj Goyal", "title": "OodGAN: Generative Adversarial Network for Out-of-Domain Data Generation", "comments": "NAACL 2021 Industry track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Detecting an Out-of-Domain (OOD) utterance is crucial for a robust dialog\nsystem. Most dialog systems are trained on a pool of annotated OOD data to\nachieve this goal. However, collecting the annotated OOD data for a given\ndomain is an expensive process. To mitigate this issue, previous works have\nproposed generative adversarial networks (GAN) based models to generate OOD\ndata for a given domain automatically. However, these proposed models do not\nwork directly with the text. They work with the text's latent space instead,\nenforcing these models to include components responsible for encoding text into\nlatent space and decoding it back, such as auto-encoder. These components\nincrease the model complexity, making it difficult to train. We propose OodGAN,\na sequential generative adversarial network (SeqGAN) based model for OOD data\ngeneration. Our proposed model works directly on the text and hence eliminates\nthe need to include an auto-encoder. OOD data generated using OodGAN model\noutperforms state-of-the-art in OOD detection metrics for ROSTD (67% relative\nimprovement in FPR 0.95) and OSQ datasets (28% relative improvement in FPR\n0.95) (Zheng et al., 2020).\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 13:08:39 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Marek", "Petr", ""], ["Naik", "Vishal Ishwar", ""], ["Auvray", "Vincent", ""], ["Goyal", "Anuj", ""]]}, {"id": "2104.02491", "submitter": "Abdullah Sad{\\i}k Sat{\\i}r", "authors": "A. Sadik Satir, Umut Demir, Gulay Goktas Sever, N. Kemal Ure", "title": "Nonlinear Model Based Guidance with Deep Learning Based Target\n  Trajectory Prediction Against Aerial Agile Attack Patterns", "comments": "Accepted for the 2021 American Control Conference (ACC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.AI cs.SY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose a novel missile guidance algorithm that combines\ndeep learning based trajectory prediction with nonlinear model predictive\ncontrol. Although missile guidance and threat interception is a well-studied\nproblem, existing algorithms' performance degrades significantly when the\ntarget is pulling high acceleration attack maneuvers while rapidly changing its\ndirection. We argue that since most threats execute similar attack maneuvers,\nthese nonlinear trajectory patterns can be processed with modern machine\nlearning methods to build high accuracy trajectory prediction algorithms. We\ntrain a long short-term memory network (LSTM) based on a class of simulated\nstructured agile attack patterns, then combine this predictor with quadratic\nprogramming based nonlinear model predictive control (NMPC). Our method, named\nnonlinear model based predictive control with target acceleration predictions\n(NMPC-TAP), significantly outperforms compared approaches in terms of miss\ndistance, for the scenarios where the target/threat is executing agile\nmaneuvers.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 13:20:36 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Satir", "A. Sadik", ""], ["Demir", "Umut", ""], ["Sever", "Gulay Goktas", ""], ["Ure", "N. Kemal", ""]]}, {"id": "2104.02532", "submitter": "Tal Feldman", "authors": "Tal Feldman and Ashley Peake", "title": "End-To-End Bias Mitigation: Removing Gender Bias in Deep Learning", "comments": "9 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning models have been deployed across many different aspects of\nsociety, often in situations that affect social welfare. Although these models\noffer streamlined solutions to large problems, they may contain biases and\ntreat groups or individuals unfairly based on protected attributes such as\ngender. In this paper, we introduce several examples of machine learning gender\nbias in practice followed by formalizations of fairness. We provide a survey of\nfairness research by detailing influential pre-processing, in-processing, and\npost-processing bias mitigation algorithms. We then propose an end-to-end bias\nmitigation framework, which employs a fusion of pre-, in-, and post-processing\nmethods to leverage the strengths of each individual technique. We test this\nmethod, along with the standard techniques we review, on a deep neural network\nto analyze bias mitigation in a deep learning setting. We find that our\nend-to-end bias mitigation framework outperforms the baselines with respect to\nseveral fairness metrics, suggesting its promise as a method for improving\nfairness. As society increasingly relies on artificial intelligence to help in\ndecision-making, addressing gender biases present in deep learning models is\nimperative. To provide readers with the tools to assess the fairness of machine\nlearning models and mitigate the biases present in them, we discuss multiple\nopen source packages for fairness in AI.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 14:11:16 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 02:32:06 GMT"}, {"version": "v3", "created": "Mon, 21 Jun 2021 01:48:24 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Feldman", "Tal", ""], ["Peake", "Ashley", ""]]}, {"id": "2104.02541", "submitter": "Nicoletta Risi", "authors": "Nicoletta Risi, Enrico Calabrese, Giacomo Indiveri", "title": "Instantaneous Stereo Depth Estimation of Real-World Stimuli with a\n  Neuromorphic Stereo-Vision Setup", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The stereo-matching problem, i.e., matching corresponding features in two\ndifferent views to reconstruct depth, is efficiently solved in biology. Yet, it\nremains the computational bottleneck for classical machine vision approaches.\nBy exploiting the properties of event cameras, recently proposed Spiking Neural\nNetwork (SNN) architectures for stereo vision have the potential of simplifying\nthe stereo-matching problem. Several solutions that combine event cameras with\nspike-based neuromorphic processors already exist. However, they are either\nsimulated on digital hardware or tested on simplified stimuli. In this work, we\nuse the Dynamic Vision Sensor 3D Human Pose Dataset (DHP19) to validate a\nbrain-inspired event-based stereo-matching architecture implemented on a\nmixed-signal neuromorphic processor with real-world data. Our experiments show\nthat this SNN architecture, composed of coincidence detectors and disparity\nsensitive neurons, is able to provide a coarse estimate of the input disparity\ninstantaneously, thereby detecting the presence of a stimulus moving in depth\nin real-time.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 14:31:23 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Risi", "Nicoletta", ""], ["Calabrese", "Enrico", ""], ["Indiveri", "Giacomo", ""]]}, {"id": "2104.02545", "submitter": "Xugui Zhou", "authors": "Xugui Zhou, Bulbul Ahmed, James H. Aylor, Philip Asare, Homa Alemzadeh", "title": "Data-driven Design of Context-aware Monitors for Hazard Prediction in\n  Artificial Pancreas Systems", "comments": "13 pages, 9 figures, to appear in the 51st IEEE/IFIP International\n  Conference on Dependable Systems and Networks (DSN 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Medical Cyber-physical Systems (MCPS) are vulnerable to accidental or\nmalicious faults that can target their controllers and cause safety hazards and\nharm to patients. This paper proposes a combined model and data-driven approach\nfor designing context-aware monitors that can detect early signs of hazards and\nmitigate them in MCPS. We present a framework for formal specification of\nunsafe system context using Signal Temporal Logic (STL) combined with an\noptimization method for patient-specific refinement of STL formulas based on\nreal or simulated faulty data from the closed-loop system for the generation of\nmonitor logic. We evaluate our approach in simulation using two\nstate-of-the-art closed-loop Artificial Pancreas Systems (APS). The results\nshow the context-aware monitor achieves up to 1.4 times increase in average\nhazard prediction accuracy (F1-score) over several baseline monitors, reduces\nfalse-positive and false-negative rates, and enables hazard mitigation with a\n54% success rate while decreasing the average risk for patients.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 14:36:33 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 05:22:04 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Zhou", "Xugui", ""], ["Ahmed", "Bulbul", ""], ["Aylor", "James H.", ""], ["Asare", "Philip", ""], ["Alemzadeh", "Homa", ""]]}, {"id": "2104.02548", "submitter": "Meghna P Ayyar", "authors": "Meghna P Ayyar, Jenny Benois-Pineau, Akka Zemmari", "title": "White Box Methods for Explanations of Convolutional Neural Networks in\n  Image Classification Tasks", "comments": "Submitted to Journal of Electronic Imaging (JEI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, deep learning has become prevalent to solve applications\nfrom multiple domains. Convolutional Neural Networks (CNNs) particularly have\ndemonstrated state of the art performance for the task of image classification.\nHowever, the decisions made by these networks are not transparent and cannot be\ndirectly interpreted by a human. Several approaches have been proposed to\nexplain to understand the reasoning behind a prediction made by a network. In\nthis paper, we propose a topology of grouping these methods based on their\nassumptions and implementations. We focus primarily on white box methods that\nleverage the information of the internal architecture of a network to explain\nits decision. Given the task of image classification and a trained CNN, this\nwork aims to provide a comprehensive and detailed overview of a set of methods\nthat can be used to create explanation maps for a particular image, that assign\nan importance score to each pixel of the image based on its contribution to the\ndecision of the network. We also propose a further classification of the white\nbox methods based on their implementations to enable better comparisons and\nhelp researchers find methods best suited for different scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 14:40:00 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Ayyar", "Meghna P", ""], ["Benois-Pineau", "Jenny", ""], ["Zemmari", "Akka", ""]]}, {"id": "2104.02563", "submitter": "Stefan Mengel", "authors": "Stefan Mengel, Friedrich Slivovsky", "title": "Proof Complexity of Symbolic QBF Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce and investigate symbolic proof systems for Quantified Boolean\nFormulas (QBF) operating on Ordered Binary Decision Diagrams (OBDDs). These\nsystems capture QBF solvers that perform symbolic quantifier elimination, and\nas such admit short proofs of formulas of bounded path-width and quantifier\ncomplexity. As a consequence, we obtain exponential separations from standard\nclausal proof systems, specifically (long-distance) QU-Resolution and IR-Calc.\n  We further develop a lower bound technique for symbolic QBF proof systems\nbased on strategy extraction that lifts known lower bounds from communication\ncomplexity. This allows us to derive strong lower bounds against symbolic QBF\nproof systems that are independent of the variable ordering of the underlying\nOBDDs, and that hold even if the proof system is allowed access to an\nNP-oracle.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 15:01:56 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Mengel", "Stefan", ""], ["Slivovsky", "Friedrich", ""]]}, {"id": "2104.02573", "submitter": "AKM Bahalul Haque", "authors": "Shahriar Rahman, Shazzadur Rahman and A K M Bahalul Haque", "title": "Prediction of Solar Radiation Using Artificial Neural Network", "comments": "Published as open access, 12 pages, 13 images and 2 tables", "journal-ref": "Journal of Physics: Conference Series , 2021", "doi": "10.1088/1742-6596/1767/1/012041", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most solar applications and systems can be reliably used to generate\nelectricity and power in many homes and offices. Recently, there is an increase\nin many solar required systems that can be found not only in electricity\ngeneration but other applications such as solar distillation, water heating,\nheating of buildings, meteorology and producing solar conversion energy.\nPrediction of solar radiation is very significant in order to accomplish the\npreviously mentioned objectives. In this paper, the main target is to present\nan algorithm that can be used to predict an hourly activity of solar radiation.\nUsing a dataset that consists of temperature of air, time, humidity, wind\nspeed, atmospheric pressure, direction of wind and solar radiation data, an\nArtificial Neural Network (ANN) model is constructed to effectively forecast\nsolar radiation using the available weather forecast data. Two models are\ncreated to efficiently create a system capable of interpreting patterns through\nsupervised learning data and predict the correct amount of radiation present in\nthe atmosphere. The results of the two statistical indicators: Mean Absolute\nError (MAE) and Mean Squared Error (MSE) are performed and compared with\nobserved and predicted data. These two models were able to generate efficient\npredictions with sufficient performance accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 20:41:27 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Rahman", "Shahriar", ""], ["Rahman", "Shazzadur", ""], ["Haque", "A K M Bahalul", ""]]}, {"id": "2104.02577", "submitter": "Dongha Lee", "authors": "Dongha Lee, Seonghyeon Lee, Hwanjo Yu", "title": "Learnable Dynamic Temporal Pooling for Time Series Classification", "comments": "AAAI 2021. 7 pages + references (2 pages). 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the increase of available time series data, predicting their class\nlabels has been one of the most important challenges in a wide range of\ndisciplines. Recent studies on time series classification show that\nconvolutional neural networks (CNN) achieved the state-of-the-art performance\nas a single classifier. In this work, pointing out that the global pooling\nlayer that is usually adopted by existing CNN classifiers discards the temporal\ninformation of high-level features, we present a dynamic temporal pooling (DTP)\ntechnique that reduces the temporal size of hidden representations by\naggregating the features at the segment-level. For the partition of a whole\nseries into multiple segments, we utilize dynamic time warping (DTW) to align\neach time point in a temporal order with the prototypical features of the\nsegments, which can be optimized simultaneously with the network parameters of\nCNN classifiers. The DTP layer combined with a fully-connected layer helps to\nextract further discriminative features considering their temporal position\nwithin an input time series. Extensive experiments on both univariate and\nmultivariate time series datasets show that our proposed pooling significantly\nimproves the classification performance.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 08:58:44 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Lee", "Dongha", ""], ["Lee", "Seonghyeon", ""], ["Yu", "Hwanjo", ""]]}, {"id": "2104.02578", "submitter": "Ilona Kulikovskikh Dr.", "authors": "Ilona Kulikovskikh", "title": "Neurons learn slower than they think", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies revealed complex convergence dynamics in gradient-based\nmethods, which has been little understood so far. Changing the step size to\nbalance between high convergence rate and small generalization error may not be\nsufficient: maximizing the test accuracy usually requires a larger learning\nrate than minimizing the training loss. To explore the dynamic bounds of\nconvergence rate, this study introduces \\textit{differential capability} into\nan optimization process, which measures whether the test accuracy increases as\nfast as a model approaches the decision boundary in a classification problem.\nThe convergence analysis showed that: 1) a higher convergence rate leads to\nslower capability growth; 2) a lower convergence rate results in faster\ncapability growth and decay; 3) regulating a convergence rate in either\ndirection reduces differential capability.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 09:09:52 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Kulikovskikh", "Ilona", ""]]}, {"id": "2104.02580", "submitter": "David Pastor-Escuredo", "authors": "David Pastor-Escuredo", "title": "Future of work: ethics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.HC econ.GN q-fin.EC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Work must be reshaped in the upcoming new era characterized by new challenges\nand the presence of new technologies and computational tools. Over-automation\nseems to be the driver of the digitalization process. Substitution is the\nparadigm leading Artificial Intelligence and robotics development against human\ncognition. Digital technology should be designed to enhance human skills and\nmake more productive use of human cognition and capacities. Digital technology\nis characterized also by scalability because of its easy and inexpensive\ndeployment. Thus, automation can lead to the absence of jobs and scalable\nnegative impact in human development and the performance of business. A look at\ndigitalization from the lens of Sustainable Development Goals can tell us how\ndigitalization impact in different sectors and areas considering society as a\ncomplex interconnected system. Here, reflections on how AI and Data impact\nfuture of work and sustainable development are provided grounded on an ethical\ncore that comprises human-level principles and also systemic principles.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 15:20:30 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Pastor-Escuredo", "David", ""]]}, {"id": "2104.02605", "submitter": "Zejun Li", "authors": "Zejun Li, Zhongyu Wei, Zhihao Fan, Haijun Shan, Xuanjing Huang", "title": "An Unsupervised Sampling Approach for Image-Sentence Matching Using\n  Document-Level Structural Information", "comments": "To be published in AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on the problem of unsupervised image-sentence\nmatching. Existing research explores to utilize document-level structural\ninformation to sample positive and negative instances for model training.\nAlthough the approach achieves positive results, it introduces a sampling bias\nand fails to distinguish instances with high semantic similarity. To alleviate\nthe bias, we propose a new sampling strategy to select additional\nintra-document image-sentence pairs as positive or negative samples.\nFurthermore, to recognize the complex pattern in intra-document samples, we\npropose a Transformer based model to capture fine-grained features and\nimplicitly construct a graph for each document, where concepts in a document\nare introduced to bridge the representation learning of images and sentences in\nthe context of a document. Experimental results show the effectiveness of our\napproach to alleviate the bias and learn well-aligned multimodal\nrepresentations.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 05:43:29 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Li", "Zejun", ""], ["Wei", "Zhongyu", ""], ["Fan", "Zhihao", ""], ["Shan", "Haijun", ""], ["Huang", "Xuanjing", ""]]}, {"id": "2104.02611", "submitter": "Linchao He", "authors": "Linchao He, Mengting Luo, Dejun Zhang, Xiao Yang, Hu Chen and Yi Zhang", "title": "PointShuffleNet: Learning Non-Euclidean Features with Homotopy\n  Equivalence and Mutual Information", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Point cloud analysis is still a challenging task due to the disorder and\nsparsity of samplings of their geometric structures from 3D sensors. In this\npaper, we introduce the homotopy equivalence relation (HER) to make the neural\nnetworks learn the data distribution from a high-dimension manifold. A shuffle\noperation is adopted to construct HER for its randomness and zero-parameter. In\naddition, inspired by prior works, we propose a local mutual information\nregularizer (LMIR) to cut off the trivial path that leads to a classification\nerror from HER. LMIR utilizes mutual information to measure the distance\nbetween the original feature and HER transformed feature and learns common\nfeatures in a contrastive learning scheme. Thus, we combine HER and LMIR to\ngive our model the ability to learn non-Euclidean features from a\nhigh-dimension manifold. This is named the non-Euclidean feature learner.\nFurthermore, we propose a new heuristics and efficiency point sampling\nalgorithm named ClusterFPS to obtain approximate uniform sampling but at faster\nspeed. ClusterFPS uses a cluster algorithm to divide a point cloud into several\nclusters and deploy the farthest point sampling algorithm on each cluster in\nparallel. By combining the above methods, we propose a novel point cloud\nanalysis neural network called PointShuffleNet (PSN), which shows great promise\nin point cloud classification and segmentation. Extensive experiments show that\nour PSN achieves state-of-the-art results on ModelNet40, ShapeNet and S3DIS\nwith high efficiency. Theoretically, we provide mathematical analysis toward\nunderstanding of what the data distribution HER has developed and why LMIR can\ndrop the trivial path by maximizing mutual information implicitly.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 03:01:16 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["He", "Linchao", ""], ["Luo", "Mengting", ""], ["Zhang", "Dejun", ""], ["Yang", "Xiao", ""], ["Chen", "Hu", ""], ["Zhang", "Yi", ""]]}, {"id": "2104.02617", "submitter": "Diego Gragnaniello", "authors": "Diego Gragnaniello, Davide Cozzolino, Francesco Marra, Giovanni Poggi,\n  Luisa Verdoliva", "title": "Are GAN generated images easy to detect? A critical analysis of the\n  state-of-the-art", "comments": "7 pages, 5 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The advent of deep learning has brought a significant improvement in the\nquality of generated media. However, with the increased level of photorealism,\nsynthetic media are becoming hardly distinguishable from real ones, raising\nserious concerns about the spread of fake or manipulated information over the\nInternet. In this context, it is important to develop automated tools to\nreliably and timely detect synthetic media. In this work, we analyze the\nstate-of-the-art methods for the detection of synthetic images, highlighting\nthe key ingredients of the most successful approaches, and comparing their\nperformance over existing generative architectures. We will devote special\nattention to realistic and challenging scenarios, like media uploaded on social\nnetworks or generated by new and unseen architectures, analyzing the impact of\nsuitable augmentation and training strategies on the detectors' generalization\nability.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 15:54:26 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Gragnaniello", "Diego", ""], ["Cozzolino", "Davide", ""], ["Marra", "Francesco", ""], ["Poggi", "Giovanni", ""], ["Verdoliva", "Luisa", ""]]}, {"id": "2104.02621", "submitter": "Zhenhua Chen", "authors": "Zhenhua Chen, Xiwen Li, Qian Lou, David Crandall", "title": "How to Accelerate Capsule Convolutions in Capsule Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  How to improve the efficiency of routing procedures in CapsNets has been\nstudied a lot. However, the efficiency of capsule convolutions has largely been\nneglected. Capsule convolution, which uses capsules rather than neurons as the\nbasic computation unit, makes it incompatible with current deep learning\nframeworks' optimization solution. As a result, capsule convolutions are\nusually very slow with these frameworks. We observe that capsule convolutions\ncan be considered as the operations of `multiplication of multiple small\nmatrics' plus tensor-based combination. Based on this observation, we develop\ntwo acceleration schemes with CUDA APIs and test them on a custom CapsNet. The\nresult shows that our solution achieves a 4X acceleration.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 15:57:49 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Chen", "Zhenhua", ""], ["Li", "Xiwen", ""], ["Lou", "Qian", ""], ["Crandall", "David", ""]]}, {"id": "2104.02640", "submitter": "TrungTin Nguyen", "authors": "TrungTin Nguyen, Hien Duy Nguyen, Faicel Chamroukhi and Florence\n  Forbes", "title": "A non-asymptotic penalization criterion for model selection in mixture\n  of experts models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.AI cs.LG stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture of experts (MoE) is a popular class of models in statistics and\nmachine learning that has sustained attention over the years, due to its\nflexibility and effectiveness. We consider the Gaussian-gated localized MoE\n(GLoME) regression model for modeling heterogeneous data. This model poses\nchallenging questions with respect to the statistical estimation and model\nselection problems, including feature selection, both from the computational\nand theoretical points of view. We study the problem of estimating the number\nof components of the GLoME model, in a penalized maximum likelihood estimation\nframework. We provide a lower bound on the penalty that ensures a weak oracle\ninequality is satisfied by our estimator. To support our theoretical result, we\nperform numerical experiments on simulated and real data, which illustrate the\nperformance of our finite-sample oracle inequality.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 16:24:55 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Nguyen", "TrungTin", ""], ["Nguyen", "Hien Duy", ""], ["Chamroukhi", "Faicel", ""], ["Forbes", "Florence", ""]]}, {"id": "2104.02646", "submitter": "Krishna Murthy Jatavallabhula", "authors": "Krishna Murthy Jatavallabhula and Miles Macklin and Florian Golemo and\n  Vikram Voleti and Linda Petrini and Martin Weiss and Breandan Considine and\n  Jerome Parent-Levesque and Kevin Xie and Kenny Erleben and Liam Paull and\n  Florian Shkurti and Derek Nowrouzezahrai and Sanja Fidler", "title": "gradSim: Differentiable simulation for system identification and\n  visuomotor control", "comments": "ICLR 2021. Project page (and a dynamic web version of the article):\n  https://gradsim.github.io", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We consider the problem of estimating an object's physical properties such as\nmass, friction, and elasticity directly from video sequences. Such a system\nidentification problem is fundamentally ill-posed due to the loss of\ninformation during image formation. Current solutions require precise 3D labels\nwhich are labor-intensive to gather, and infeasible to create for many systems\nsuch as deformable solids or cloth. We present gradSim, a framework that\novercomes the dependence on 3D supervision by leveraging differentiable\nmultiphysics simulation and differentiable rendering to jointly model the\nevolution of scene dynamics and image formation. This novel combination enables\nbackpropagation from pixels in a video sequence through to the underlying\nphysical attributes that generated them. Moreover, our unified computation\ngraph -- spanning from the dynamics and through the rendering process --\nenables learning in challenging visuomotor control tasks, without relying on\nstate-based (3D) supervision, while obtaining performance competitive to or\nbetter than techniques that rely on precise 3D labels.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 16:32:01 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Jatavallabhula", "Krishna Murthy", ""], ["Macklin", "Miles", ""], ["Golemo", "Florian", ""], ["Voleti", "Vikram", ""], ["Petrini", "Linda", ""], ["Weiss", "Martin", ""], ["Considine", "Breandan", ""], ["Parent-Levesque", "Jerome", ""], ["Xie", "Kevin", ""], ["Erleben", "Kenny", ""], ["Paull", "Liam", ""], ["Shkurti", "Florian", ""], ["Nowrouzezahrai", "Derek", ""], ["Fidler", "Sanja", ""]]}, {"id": "2104.02656", "submitter": "Vinod Kumar Kurmi", "authors": "Vinod K Kurmi, Vipul Bajaj, Badri N Patro, K S Venkatesh, Vinay P\n  Namboodiri, Preethi Jyothi", "title": "Collaborative Learning to Generate Audio-Video Jointly", "comments": "ICASSP 2021 (Accepted)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.MM cs.SD eess.AS eess.IV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  There have been a number of techniques that have demonstrated the generation\nof multimedia data for one modality at a time using GANs, such as the ability\nto generate images, videos, and audio. However, so far, the task of multi-modal\ngeneration of data, specifically for audio and videos both, has not been\nsufficiently well-explored. Towards this, we propose a method that demonstrates\nthat we are able to generate naturalistic samples of video and audio data by\nthe joint correlated generation of audio and video modalities. The proposed\nmethod uses multiple discriminators to ensure that the audio, video, and the\njoint output are also indistinguishable from real-world samples. We present a\ndataset for this task and show that we are able to generate realistic samples.\nThis method is validated using various standard metrics such as Inception\nScore, Frechet Inception Distance (FID) and through human evaluation.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 01:00:51 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Kurmi", "Vinod K", ""], ["Bajaj", "Vipul", ""], ["Patro", "Badri N", ""], ["Venkatesh", "K S", ""], ["Namboodiri", "Vinay P", ""], ["Jyothi", "Preethi", ""]]}, {"id": "2104.02661", "submitter": "Tarindu Jayatilaka", "authors": "Haritha Jayasinghe, Tarindu Jayatilaka, Ravin Gunawardena,\n  Uthayasanker Thayasivam", "title": "Data-Driven Simulation of Ride-Hailing Services using Imitation and\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The rapid growth of ride-hailing platforms has created a highly competitive\nmarket where businesses struggle to make profits, demanding the need for better\noperational strategies. However, real-world experiments are risky and expensive\nfor these platforms as they deal with millions of users daily. Thus, a need\narises for a simulated environment where they can predict users' reactions to\nchanges in the platform-specific parameters such as trip fares and incentives.\nBuilding such a simulation is challenging, as these platforms exist within\ndynamic environments where thousands of users regularly interact with one\nanother. This paper presents a framework to mimic and predict user,\nspecifically driver, behaviors in ride-hailing services. We use a data-driven\nhybrid reinforcement learning and imitation learning approach for this. First,\nthe agent utilizes behavioral cloning to mimic driver behavior using a\nreal-world data set. Next, reinforcement learning is applied on top of the\npre-trained agents in a simulated environment, to allow them to adapt to\nchanges in the platform. Our framework provides an ideal playground for\nride-hailing platforms to experiment with platform-specific parameters to\npredict drivers' behavioral patterns.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 16:49:26 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Jayasinghe", "Haritha", ""], ["Jayatilaka", "Tarindu", ""], ["Gunawardena", "Ravin", ""], ["Thayasivam", "Uthayasanker", ""]]}, {"id": "2104.02687", "submitter": "Medhini Narasimhan", "authors": "Medhini Narasimhan, Shiry Ginosar, Andrew Owens, Alexei A. Efros,\n  Trevor Darrell", "title": "Strumming to the Beat: Audio-Conditioned Contrastive Video Textures", "comments": "Project website at https://medhini.github.io/audio_video_textures/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a non-parametric approach for infinite video texture synthesis\nusing a representation learned via contrastive learning. We take inspiration\nfrom Video Textures, which showed that plausible new videos could be generated\nfrom a single one by stitching its frames together in a novel yet consistent\norder. This classic work, however, was constrained by its use of hand-designed\ndistance metrics, limiting its use to simple, repetitive videos. We draw on\nrecent techniques from self-supervised learning to learn this distance metric,\nallowing us to compare frames in a manner that scales to more challenging\ndynamics, and to condition on other data, such as audio. We learn\nrepresentations for video frames and frame-to-frame transition probabilities by\nfitting a video-specific model trained using contrastive learning. To\nsynthesize a texture, we randomly sample frames with high transition\nprobabilities to generate diverse temporally smooth videos with novel sequences\nand transitions. The model naturally extends to an audio-conditioned setting\nwithout requiring any finetuning. Our model outperforms baselines on human\nperceptual scores, can handle a diverse range of input videos, and can combine\nsemantic and audio-visual cues in order to synthesize videos that synchronize\nwell with an audio signal.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 17:24:57 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Narasimhan", "Medhini", ""], ["Ginosar", "Shiry", ""], ["Owens", "Andrew", ""], ["Efros", "Alexei A.", ""], ["Darrell", "Trevor", ""]]}, {"id": "2104.02704", "submitter": "Canwen Xu", "authors": "Canwen Xu and Wangchunshu Zhou and Tao Ge and Ke Xu and Julian McAuley\n  and Furu Wei", "title": "Blow the Dog Whistle: A Chinese Dataset for Cant Understanding with\n  Common Sense and World Knowledge", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cant is important for understanding advertising, comedies and dog-whistle\npolitics. However, computational research on cant is hindered by a lack of\navailable datasets. In this paper, we propose a large and diverse Chinese\ndataset for creating and understanding cant from a computational linguistics\nperspective. We formulate a task for cant understanding and provide both\nquantitative and qualitative analysis for tested word embedding similarity and\npretrained language models. Experiments suggest that such a task requires deep\nlanguage understanding, common sense, and world knowledge and thus can be a\ngood testbed for pretrained language models and help models perform better on\nother tasks. The code is available at https://github.com/JetRunner/dogwhistle.\nThe data and leaderboard are available at\nhttps://competitions.codalab.org/competitions/30451.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 17:55:43 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 18:09:33 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Xu", "Canwen", ""], ["Zhou", "Wangchunshu", ""], ["Ge", "Tao", ""], ["Xu", "Ke", ""], ["McAuley", "Julian", ""], ["Wei", "Furu", ""]]}, {"id": "2104.02726", "submitter": "Giorgio Franceschelli", "authors": "Giorgio Franceschelli and Mirco Musolesi", "title": "Creativity and Machine Learning: A Survey", "comments": "25 pages, 3 figures, 2 tables; uppercase typos corrected; paragraph\n  about char-RNN and folk-RNN revised", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a growing interest in the area of machine learning and creativity.\nThis survey presents an overview of the history and the state of the art of\ncomputational creativity theories, machine learning techniques, including\ngenerative deep learning, and corresponding automatic evaluation methods. After\npresenting a critical discussion of the key contributions in this area, we\noutline the current research challenges and emerging opportunities in this\nfield.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 18:00:06 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 18:00:03 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Franceschelli", "Giorgio", ""], ["Musolesi", "Mirco", ""]]}, {"id": "2104.02737", "submitter": "Suhail Alsalehi", "authors": "Suhail Alsalehi, Noushin Mehdipour, Ezio Bartocci and Calin Belta", "title": "Neural Network-based Control for Multi-Agent Systems from\n  Spatio-Temporal Specifications", "comments": "8 pages. Submitted to the CDC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.AI cs.FL cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework for solving control synthesis problems for multi-agent\nnetworked systems required to satisfy spatio-temporal specifications. We use\nSpatio-Temporal Reach and Escape Logic (STREL) as a specification language. For\nthis logic, we define smooth quantitative semantics, which captures the degree\nof satisfaction of a formula by a multi-agent team. We use the novel\nquantitative semantics to map control synthesis problems with STREL\nspecifications to optimization problems and propose a combination of heuristic\nand gradient-based methods to solve such problems. As this method might not\nmeet the requirements of a real-time implementation, we develop a machine\nlearning technique that uses the results of the off-line optimizations to train\na neural network that gives the control inputs at current states. We illustrate\nthe effectiveness of the proposed framework by applying it to a model of a\nrobotic team required to satisfy a spatial-temporal specification under\ncommunication constraints.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 18:08:09 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Alsalehi", "Suhail", ""], ["Mehdipour", "Noushin", ""], ["Bartocci", "Ezio", ""], ["Belta", "Calin", ""]]}, {"id": "2104.02749", "submitter": "Pranjal Singh Rajput", "authors": "Pranjal Singh Rajput, Yeshwanth Napolean, Jan van Gemert", "title": "Heuristics2Annotate: Efficient Annotation of Large-Scale Marathon\n  Dataset For Bounding Box Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Annotating a large-scale in-the-wild person re-identification dataset\nespecially of marathon runners is a challenging task. The variations in the\nscenarios such as camera viewpoints, resolution, occlusion, and illumination\nmake the problem non-trivial. Manually annotating bounding boxes in such\nlarge-scale datasets is cost-inefficient. Additionally, due to crowdedness and\nocclusion in the videos, aligning the identity of runners across multiple\ndisjoint cameras is a challenge. We collected a novel large-scale in-the-wild\nvideo dataset of marathon runners. The dataset consists of hours of recording\nof thousands of runners captured using 42 hand-held smartphone cameras and\ncovering real-world scenarios. Due to the presence of crowdedness and occlusion\nin the videos, the annotation of runners becomes a challenging task. We propose\na new scheme for tackling the challenges in the annotation of such large\ndataset. Our technique reduces the overall cost of annotation in terms of time\nas well as budget. We demonstrate performing fps analysis to reduce the effort\nand time of annotation. We investigate several annotation methods for\nefficiently generating tight bounding boxes. Our results prove that\ninterpolating bounding boxes between keyframes is the most efficient method of\nbounding box generation amongst several other methods and is 3x times faster\nthan the naive baseline method. We introduce a novel way of aligning the\nidentity of runners in disjoint cameras. Our inter-camera alignment tool\nintegrated with the state-of-the-art person re-id system proves to be\nsufficient and effective in the alignment of the runners across multiple\ncameras with non-overlapping views. Our proposed framework of annotation\nreduces the annotation cost of the dataset by a factor of 16x, also effectively\naligning 93.64% of the runners in the cross-camera setting.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 19:08:31 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Rajput", "Pranjal Singh", ""], ["Napolean", "Yeshwanth", ""], ["van Gemert", "Jan", ""]]}, {"id": "2104.02756", "submitter": "Fran\\c{c}ois Mercier", "authors": "Fran\\c{c}ois Mercier", "title": "Efficient transfer learning for NLP with ELECTRA", "comments": "Submission for ML Reproducibility Challenge 2020", "journal-ref": "Machine Learning Reproducibility Challenge 2020", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Clark et al. [2020] claims that the ELECTRA approach is highly efficient in\nNLP performances relative to computation budget. As such, this reproducibility\nstudy focus on this claim, summarized by the following question: Can we use\nELECTRA to achieve close to SOTA performances for NLP in low-resource settings,\nin term of compute cost?\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 19:34:36 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Mercier", "Fran\u00e7ois", ""]]}, {"id": "2104.02768", "submitter": "Jacob Pfau", "authors": "Jacob Pfau, Albert T. Young, Jerome Wei, Maria L. Wei, Michael J.\n  Keiser", "title": "Robust Semantic Interpretability: Revisiting Concept Activation Vectors", "comments": "ICML WHI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpretability methods for image classification assess model\ntrustworthiness by attempting to expose whether the model is systematically\nbiased or attending to the same cues as a human would. Saliency methods for\nfeature attribution dominate the interpretability literature, but these methods\ndo not address semantic concepts such as the textures, colors, or genders of\nobjects within an image. Our proposed Robust Concept Activation Vectors (RCAV)\nquantifies the effects of semantic concepts on individual model predictions and\non model behavior as a whole. RCAV calculates a concept gradient and takes a\ngradient ascent step to assess model sensitivity to the given concept. By\ngeneralizing previous work on concept activation vectors to account for model\nnon-linearity, and by introducing stricter hypothesis testing, we show that\nRCAV yields interpretations which are both more accurate at the image level and\nrobust at the dataset level. RCAV, like saliency methods, supports the\ninterpretation of individual predictions. To evaluate the practical use of\ninterpretability methods as debugging tools, and the scientific use of\ninterpretability methods for identifying inductive biases (e.g. texture over\nshape), we construct two datasets and accompanying metrics for realistic\nbenchmarking of semantic interpretability methods. Our benchmarks expose the\nimportance of counterfactual augmentation and negative controls for quantifying\nthe practical usability of interpretability methods.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 20:14:59 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Pfau", "Jacob", ""], ["Young", "Albert T.", ""], ["Wei", "Jerome", ""], ["Wei", "Maria L.", ""], ["Keiser", "Michael J.", ""]]}, {"id": "2104.02784", "submitter": "Karl-Philipp Kortmann", "authors": "Karl-Philipp Kortmann, Moritz Fehsenfeld and Mark Wielitzka", "title": "Autoencoder-based Representation Learning from Heterogeneous\n  Multivariate Time Series Data of Mechatronic Systems", "comments": "A later version of this paper in German language was submitted to VDI\n  Mechatronic Tagung 2021 and will be published in the conference proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI eess.SP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Sensor and control data of modern mechatronic systems are often available as\nheterogeneous time series with different sampling rates and value ranges.\nSuitable classification and regression methods from the field of supervised\nmachine learning already exist for predictive tasks, for example in the context\nof condition monitoring, but their performance scales strongly with the number\nof labeled training data. Their provision is often associated with high effort\nin the form of person-hours or additional sensors. In this paper, we present a\nmethod for unsupervised feature extraction using autoencoder networks that\nspecifically addresses the heterogeneous nature of the database and reduces the\namount of labeled training data required compared to existing methods. Three\npublic datasets of mechatronic systems from different application domains are\nused to validate the results.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 21:04:27 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 12:39:35 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Kortmann", "Karl-Philipp", ""], ["Fehsenfeld", "Moritz", ""], ["Wielitzka", "Mark", ""]]}, {"id": "2104.02821", "submitter": "Cristian Canton Ferrer", "authors": "Caner Hazirbas, Joanna Bitton, Brian Dolhansky, Jacqueline Pan, Albert\n  Gordo, Cristian Canton Ferrer", "title": "Towards measuring fairness in AI: the Casual Conversations dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel dataset to help researchers evaluate their\ncomputer vision and audio models for accuracy across a diverse set of age,\ngenders, apparent skin tones and ambient lighting conditions. Our dataset is\ncomposed of 3,011 subjects and contains over 45,000 videos, with an average of\n15 videos per person. The videos were recorded in multiple U.S. states with a\ndiverse set of adults in various age, gender and apparent skin tone groups. A\nkey feature is that each subject agreed to participate for their likenesses to\nbe used. Additionally, our age and gender annotations are provided by the\nsubjects themselves. A group of trained annotators labeled the subjects'\napparent skin tone using the Fitzpatrick skin type scale. Moreover, annotations\nfor videos recorded in low ambient lighting are also provided. As an\napplication to measure robustness of predictions across certain attributes, we\nprovide a comprehensive study on the top five winners of the DeepFake Detection\nChallenge (DFDC). Experimental evaluation shows that the winning models are\nless performant on some specific groups of people, such as subjects with darker\nskin tones and thus may not generalize to all people. In addition, we also\nevaluate the state-of-the-art apparent age and gender classification methods.\nOur experiments provides a through analysis on these models in terms of fair\ntreatment of people from various backgrounds.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 22:48:22 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Hazirbas", "Caner", ""], ["Bitton", "Joanna", ""], ["Dolhansky", "Brian", ""], ["Pan", "Jacqueline", ""], ["Gordo", "Albert", ""], ["Ferrer", "Cristian Canton", ""]]}, {"id": "2104.02841", "submitter": "Lifeng Fan", "authors": "Lifeng Fan, Shuwen Qiu, Zilong Zheng, Tao Gao, Song-Chun Zhu, Yixin\n  Zhu", "title": "Learning Triadic Belief Dynamics in Nonverbal Communication from Videos", "comments": "CVPR2021, Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans possess a unique social cognition capability; nonverbal communication\ncan convey rich social information among agents. In contrast, such crucial\nsocial characteristics are mostly missing in the existing scene understanding\nliterature. In this paper, we incorporate different nonverbal communication\ncues (e.g., gaze, human poses, and gestures) to represent, model, learn, and\ninfer agents' mental states from pure visual inputs. Crucially, such a mental\nrepresentation takes the agent's belief into account so that it represents what\nthe true world state is and infers the beliefs in each agent's mental state,\nwhich may differ from the true world states. By aggregating different beliefs\nand true world states, our model essentially forms \"five minds\" during the\ninteractions between two agents. This \"five minds\" model differs from prior\nworks that infer beliefs in an infinite recursion; instead, agents' beliefs are\nconverged into a \"common mind\". Based on this representation, we further devise\na hierarchical energy-based model that jointly tracks and predicts all five\nminds. From this new perspective, a social event is interpreted by a series of\nnonverbal communication and belief dynamics, which transcends the classic\nkeyframe video summary. In the experiments, we demonstrate that using such a\nsocial account provides a better video summary on videos with rich social\ninteractions compared with state-of-the-art keyframe video summary methods.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 00:52:04 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Fan", "Lifeng", ""], ["Qiu", "Shuwen", ""], ["Zheng", "Zilong", ""], ["Gao", "Tao", ""], ["Zhu", "Song-Chun", ""], ["Zhu", "Yixin", ""]]}, {"id": "2104.02844", "submitter": "Stas Tiomkin", "authors": "Philippe Hansen-Estruch, Wenling Shang, Lerrel Pinto, Pieter Abbeel,\n  Stas Tiomkin", "title": "GEM: Group Enhanced Model for Learning Dynamical Control Systems", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.AI cs.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning the dynamics of a physical system wherein an autonomous agent\noperates is an important task. Often these systems present apparent geometric\nstructures. For instance, the trajectories of a robotic manipulator can be\nbroken down into a collection of its transitional and rotational motions, fully\ncharacterized by the corresponding Lie groups and Lie algebras. In this work,\nwe take advantage of these structures to build effective dynamical models that\nare amenable to sample-based learning. We hypothesize that learning the\ndynamics on a Lie algebra vector space is more effective than learning a direct\nstate transition model. To verify this hypothesis, we introduce the Group\nEnhanced Model (GEM). GEMs significantly outperform conventional transition\nmodels on tasks of long-term prediction, planning, and model-based\nreinforcement learning across a diverse suite of standard continuous-control\nenvironments, including Walker, Hopper, Reacher, Half-Cheetah, Inverted\nPendulums, Ant, and Humanoid. Furthermore, plugging GEM into existing state of\nthe art systems enhances their performance, which we demonstrate on the PETS\nsystem. This work sheds light on a connection between learning of dynamics and\nLie group properties, which opens doors for new research directions and\npractical applications along this direction. Our code is publicly available at:\nhttps://tinyurl.com/GEMMBRL.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 01:08:18 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Hansen-Estruch", "Philippe", ""], ["Shang", "Wenling", ""], ["Pinto", "Lerrel", ""], ["Abbeel", "Pieter", ""], ["Tiomkin", "Stas", ""]]}, {"id": "2104.02847", "submitter": "Ashwin Raju", "authors": "Ashwin Raju, Shun Miao, Chi-Tung Cheng, Le Lu, Mei Han, Jing Xiao,\n  Chien-Hung Liao, Junzhou Huang and Adam P. Harrison", "title": "Deep Implicit Statistical Shape Models for 3D Medical Image Delineation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  3D delineation of anatomical structures is a cardinal goal in medical imaging\nanalysis. Prior to deep learning, statistical shape models that imposed\nanatomical constraints and produced high quality surfaces were a core\ntechnology. Prior to deep learning, statistical shape models that imposed\nanatomical constraints and produced high quality surfaces were a core\ntechnology. Today fully-convolutional networks (FCNs), while dominant, do not\noffer these capabilities. We present deep implicit statistical shape models\n(DISSMs), a new approach to delineation that marries the representation power\nof convolutional neural networks (CNNs) with the robustness of SSMs. DISSMs use\na deep implicit surface representation to produce a compact and descriptive\nshape latent space that permits statistical models of anatomical variance. To\nreliably fit anatomically plausible shapes to an image, we introduce a novel\nrigid and non-rigid pose estimation pipeline that is modelled as a Markov\ndecision process(MDP). We outline a training regime that includes inverted\nepisodic training and a deep realization of marginal space learning (MSL).\nIntra-dataset experiments on the task of pathological liver segmentation\ndemonstrate that DISSMs can perform more robustly than three leading FCN\nmodels, including nnU-Net: reducing the mean Hausdorff distance (HD) by\n7.7-14.3mm and improving the worst case Dice-Sorensen coefficient (DSC) by\n1.2-2.3%. More critically, cross-dataset experiments on a dataset directly\nreflecting clinical deployment scenarios demonstrate that DISSMs improve the\nmean DSC and HD by 3.5-5.9% and 12.3-24.5mm, respectively, and the worst-case\nDSC by 5.4-7.3%. These improvements are over and above any benefits from\nrepresenting delineations with high-quality surface.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 01:15:06 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Raju", "Ashwin", ""], ["Miao", "Shun", ""], ["Cheng", "Chi-Tung", ""], ["Lu", "Le", ""], ["Han", "Mei", ""], ["Xiao", "Jing", ""], ["Liao", "Chien-Hung", ""], ["Huang", "Junzhou", ""], ["Harrison", "Adam P.", ""]]}, {"id": "2104.02850", "submitter": "Jin Liu", "authors": "Jin Liu, Peng Chen, Tao Liang, Zhaoxing Li, Cai Yu, Shuqiao Zou, Jiao\n  Dai, Jizhong Han", "title": "LI-Net: Large-Pose Identity-Preserving Face Reenactment Network", "comments": "IEEE International Conference on Multimedia and Expo(ICME) 2021 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Face reenactment is a challenging task, as it is difficult to maintain\naccurate expression, pose and identity simultaneously. Most existing methods\ndirectly apply driving facial landmarks to reenact source faces and ignore the\nintrinsic gap between two identities, resulting in the identity mismatch issue.\nBesides, they neglect the entanglement of expression and pose features when\nencoding driving faces, leading to inaccurate expressions and visual artifacts\non large-pose reenacted faces. To address these problems, we propose a\nLarge-pose Identity-preserving face reenactment network, LI-Net. Specifically,\nthe Landmark Transformer is adopted to adjust driving landmark images, which\naims to narrow the identity gap between driving and source landmark images.\nThen the Face Rotation Module and the Expression Enhancing Generator decouple\nthe transformed landmark image into pose and expression features, and reenact\nthose attributes separately to generate identity-preserving faces with accurate\nexpressions and poses. Both qualitative and quantitative experimental results\ndemonstrate the superiority of our method.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 01:41:21 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Liu", "Jin", ""], ["Chen", "Peng", ""], ["Liang", "Tao", ""], ["Li", "Zhaoxing", ""], ["Yu", "Cai", ""], ["Zou", "Shuqiao", ""], ["Dai", "Jiao", ""], ["Han", "Jizhong", ""]]}, {"id": "2104.02894", "submitter": "Zhaoyi Wan", "authors": "Zhaoyi Wan, Haoran Chen, Jielei Zhang, Wentao Jiang, Cong Yao, Jiebo\n  Luo", "title": "Facial Attribute Transformers for Precise and Robust Makeup Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we address the problem of makeup transfer, which aims at\ntransplanting the makeup from the reference face to the source face while\npreserving the identity of the source. Existing makeup transfer methods have\nmade notable progress in generating realistic makeup faces, but do not perform\nwell in terms of color fidelity and spatial transformation. To tackle these\nissues, we propose a novel Facial Attribute Transformer (FAT) and its variant\nSpatial FAT for high-quality makeup transfer. Drawing inspirations from the\nTransformer in NLP, FAT is able to model the semantic correspondences and\ninteractions between the source face and reference face, and then precisely\nestimate and transfer the facial attributes. To further facilitate shape\ndeformation and transformation of facial parts, we also integrate thin plate\nsplines (TPS) into FAT, thus creating Spatial FAT, which is the first method\nthat can transfer geometric attributes in addition to color and texture.\nExtensive qualitative and quantitative experiments demonstrate the\neffectiveness and superiority of our proposed FATs in the following aspects:\n(1) ensuring high-fidelity color transfer; (2) allowing for geometric\ntransformation of facial parts; (3) handling facial variations (such as poses\nand shadows) and (4) supporting high-resolution face generation.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 03:39:02 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Wan", "Zhaoyi", ""], ["Chen", "Haoran", ""], ["Zhang", "Jielei", ""], ["Jiang", "Wentao", ""], ["Yao", "Cong", ""], ["Luo", "Jiebo", ""]]}, {"id": "2104.02925", "submitter": "Alexandre Thiery", "authors": "Rahul Rahaman, Atin Ghosh and Alexandre H. Thiery", "title": "Pretrained equivariant features improve unsupervised landmark discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Locating semantically meaningful landmark points is a crucial component of a\nlarge number of computer vision pipelines. Because of the small number of\navailable datasets with ground truth landmark annotations, it is important to\ndesign robust unsupervised and semi-supervised methods for landmark detection.\n  Many of the recent unsupervised learning methods rely on the equivariance\nproperties of landmarks to synthetic image deformations. Our work focuses on\nsuch widely used methods and sheds light on its core problem, its inability to\nproduce equivariant intermediate convolutional features. This finding leads us\nto formulate a two-step unsupervised approach that overcomes this challenge by\nfirst learning powerful pixel-based features and then use the pre-trained\nfeatures to learn a landmark detector by the traditional equivariance method.\nOur method produces state-of-the-art results in several challenging landmark\ndetection datasets such as the BBC Pose dataset and the Cat-Head dataset. It\nperforms comparably on a range of other benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 05:42:11 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Rahaman", "Rahul", ""], ["Ghosh", "Atin", ""], ["Thiery", "Alexandre H.", ""]]}, {"id": "2104.02932", "submitter": "Tingyi Wanyan", "authors": "Tingyi Wanyan, Jing Zhang, Ying Ding, Ariful Azad, Zhangyang Wang,\n  Benjamin S Glicksberg", "title": "Bootstrapping Your Own Positive Sample: Contrastive Learning With\n  Electronic Health Record Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electronic Health Record (EHR) data has been of tremendous utility in\nArtificial Intelligence (AI) for healthcare such as predicting future clinical\nevents. These tasks, however, often come with many challenges when using\nclassical machine learning models due to a myriad of factors including class\nimbalance and data heterogeneity (i.e., the complex intra-class variances). To\naddress some of these research gaps, this paper leverages the exciting\ncontrastive learning framework and proposes a novel contrastive regularized\nclinical classification model. The contrastive loss is found to substantially\naugment EHR-based prediction: it effectively characterizes the\nsimilar/dissimilar patterns (by its \"push-and-pull\" form), meanwhile mitigating\nthe highly skewed class distribution by learning more balanced feature spaces\n(as also echoed by recent findings). In particular, when naively exporting the\ncontrastive learning to the EHR data, one hurdle is in generating positive\nsamples, since EHR data is not as amendable to data augmentation as image data.\nTo this end, we have introduced two unique positive sampling strategies\nspecifically tailored for EHR data: a feature-based positive sampling that\nexploits the feature space neighborhood structure to reinforce the feature\nlearning; and an attribute-based positive sampling that incorporates\npre-generated patient similarity metrics to define the sample proximity. Both\nsampling approaches are designed with an awareness of unique high intra-class\nvariance in EHR data. Our overall framework yields highly competitive\nexperimental results in predicting the mortality risk on real-world COVID-19\nEHR data with a total of 5,712 patients admitted to a large, urban health\nsystem. Specifically, our method reaches a high AUROC prediction score of\n0.959, which outperforms other baselines and alternatives: cross-entropy(0.873)\nand focal loss(0.931).\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 06:02:04 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Wanyan", "Tingyi", ""], ["Zhang", "Jing", ""], ["Ding", "Ying", ""], ["Azad", "Ariful", ""], ["Wang", "Zhangyang", ""], ["Glicksberg", "Benjamin S", ""]]}, {"id": "2104.02934", "submitter": "Jiayang Cheng", "authors": "Jiayang Cheng, Haiyun Jiang, Deqing Yang, Yanghua Xiao", "title": "A Question-answering Based Framework for Relation Extraction Validation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relation extraction is an important task in knowledge acquisition and text\nunderstanding. Existing works mainly focus on improving relation extraction by\nextracting effective features or designing reasonable model structures.\nHowever, few works have focused on how to validate and correct the results\ngenerated by the existing relation extraction models. We argue that validation\nis an important and promising direction to further improve the performance of\nrelation extraction. In this paper, we explore the possibility of using\nquestion answering as validation. Specifically, we propose a novel\nquestion-answering based framework to validate the results from relation\nextraction models. Our proposed framework can be easily applied to existing\nrelation classifiers without any additional information. We conduct extensive\nexperiments on the popular NYT dataset to evaluate the proposed framework, and\nobserve consistent improvements over five strong baselines.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 06:08:36 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Cheng", "Jiayang", ""], ["Jiang", "Haiyun", ""], ["Yang", "Deqing", ""], ["Xiao", "Yanghua", ""]]}, {"id": "2104.02938", "submitter": "Ini Oguntola", "authors": "Ini Oguntola, Dana Hughes, Katia Sycara", "title": "Deep Interpretable Models of Theory of Mind", "comments": "RO-MAN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When developing AI systems that interact with humans, it is essential to\ndesign both a system that can understand humans, and a system that humans can\nunderstand. Most deep network based agent-modeling approaches are 1) not\ninterpretable and 2) only model external behavior, ignoring internal mental\nstates, which potentially limits their capability for assistance,\ninterventions, discovering false beliefs, etc. To this end, we develop an\ninterpretable modular neural framework for modeling the intentions of other\nobserved entities. We demonstrate the efficacy of our approach with experiments\non data from human participants on a search and rescue task in Minecraft, and\nshow that incorporating interpretability can significantly increase predictive\nperformance under the right conditions.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 06:18:58 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 18:22:30 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Oguntola", "Ini", ""], ["Hughes", "Dana", ""], ["Sycara", "Katia", ""]]}, {"id": "2104.02959", "submitter": "Badr AlKhamissi", "authors": "Badr AlKhamissi, Muhammad ElNokrashy, Michael Spranger", "title": "The Emergence of Abstract and Episodic Neurons in Episodic Meta-RL", "comments": "This work was accepted at the Learning to Learn Workshop (ICLR 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this work, we analyze the reinstatement mechanism introduced by Ritter et\nal. (2018) to reveal two classes of neurons that emerge in the agent's working\nmemory (an epLSTM cell) when trained using episodic meta-RL on an episodic\nvariant of the Harlow visual fixation task. Specifically, Abstract neurons\nencode knowledge shared across tasks, while Episodic neurons carry information\nrelevant for a specific episode's task.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 07:25:52 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["AlKhamissi", "Badr", ""], ["ElNokrashy", "Muhammad", ""], ["Spranger", "Michael", ""]]}, {"id": "2104.02971", "submitter": "Jiashuo Yu", "authors": "Jiashuo Yu, Ying Cheng, Rui Feng", "title": "MPN: Multimodal Parallel Network for Audio-Visual Event Localization", "comments": "IEEE International Conference on Multimedia and Expo (ICME) 2021 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Audio-visual event localization aims to localize an event that is both\naudible and visible in the wild, which is a widespread audio-visual scene\nanalysis task for unconstrained videos. To address this task, we propose a\nMultimodal Parallel Network (MPN), which can perceive global semantics and\nunmixed local information parallelly. Specifically, our MPN framework consists\nof a classification subnetwork to predict event categories and a localization\nsubnetwork to predict event boundaries. The classification subnetwork is\nconstructed by the Multimodal Co-attention Module (MCM) and obtains global\ncontexts. The localization subnetwork consists of Multimodal Bottleneck\nAttention Module (MBAM), which is designed to extract fine-grained\nsegment-level contents. Extensive experiments demonstrate that our framework\nachieves the state-of-the-art performance both in fully supervised and weakly\nsupervised settings on the Audio-Visual Event (AVE) dataset.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 07:44:22 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Yu", "Jiashuo", ""], ["Cheng", "Ying", ""], ["Feng", "Rui", ""]]}, {"id": "2104.02973", "submitter": "Pierre Gutierrez", "authors": "Antoine Cordier, Deepan Das, and Pierre Gutierrez", "title": "Active learning using weakly supervised signals for quality inspection", "comments": "8 pages, 3 Figures, QCAV 2021 conference (proceedings published in\n  SPIE)", "journal-ref": null, "doi": "10.1117/12.2586595", "report-no": null, "categories": "cs.CV cs.AI cs.LO stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Because manufacturing processes evolve fast, and since production visual\naspect can vary significantly on a daily basis, the ability to rapidly update\nmachine vision based inspection systems is paramount. Unfortunately, supervised\nlearning of convolutional neural networks requires a significant amount of\nannotated images for being able to learn effectively from new data.\nAcknowledging the abundance of continuously generated images coming from the\nproduction line and the cost of their annotation, we demonstrate it is possible\nto prioritize and accelerate the annotation process. In this work, we develop a\nmethodology for learning actively, from rapidly mined, weakly (i.e. partially)\nannotated data, enabling a fast, direct feedback from the operators on the\nproduction line and tackling a big machine vision weakness: false positives. We\nalso consider the problem of covariate shift, which arises inevitably due to\nchanging conditions during data acquisition. In that regard, we show\ndomain-adversarial training to be an efficient way to address this issue.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 07:49:07 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Cordier", "Antoine", ""], ["Das", "Deepan", ""], ["Gutierrez", "Pierre", ""]]}, {"id": "2104.02979", "submitter": "Xudong Li", "authors": "Xudong Li, Li Feng, Lei Li, Chen Wang", "title": "Few-Shot Meta-Learning on Point Cloud for Semantic Segmentation", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The promotion of construction robots can solve the problem of human resource\nshortage and improve the quality of decoration. To help the construction robots\nobtain environmental information, we need to use 3D point cloud, which is\nwidely used in robotics, autonomous driving, and so on. With a good\nunderstanding of environmental information, construction robots can work\nbetter. However, the dynamic changes of 3D point cloud data may bring\ndifficulties for construction robots to understand environmental information,\nsuch as when construction robots renovate houses. The paper proposes a semantic\nsegmentation method for point cloud based on meta-learning. The method includes\na basic learning module and a meta-learning module. The basic learning module\nis responsible for learning data features and evaluating the model, while the\nmeta-learning module is responsible for updating the parameters of the model\nand improving the model generalization ability. In our work, we pioneered the\nmethod of producing datasets for meta-learning in 3D scenes, as well as\ndemonstrated that the Model-Agnostic Meta-Learning (MAML) algorithm can be\napplied to process 3D point cloud data. At the same time, experiments show that\nour method can allow the model to be quickly applied to new environments with a\nfew samples. Our method has important applications.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 08:06:08 GMT"}, {"version": "v2", "created": "Sun, 11 Apr 2021 11:38:37 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Li", "Xudong", ""], ["Feng", "Li", ""], ["Li", "Lei", ""], ["Wang", "Chen", ""]]}, {"id": "2104.02980", "submitter": "Pierre Gutierrez", "authors": "Pierre Gutierrez, Maria Luschkova, Antoine Cordier, Mustafa Shukor,\n  Mona Schappert, and Tim Dahmen", "title": "Synthetic training data generation for deep learning based quality\n  inspection", "comments": "8 pages, 4 figures, to be published in QCAV 2021 conference,\n  proceedings will by published by SPIE", "journal-ref": null, "doi": "10.1117/12.2586824", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deep learning is now the gold standard in computer vision-based quality\ninspection systems. In order to detect defects, supervised learning is often\nutilized, but necessitates a large amount of annotated images, which can be\ncostly: collecting, cleaning, and annotating the data is tedious and limits the\nspeed at which a system can be deployed as everything the system must detect\nneeds to be observed first. This can impede the inspection of rare defects,\nsince very few samples can be collected by the manufacturer. In this work, we\nfocus on simulations to solve this issue. We first present a generic simulation\npipeline to render images of defective or healthy (non defective) parts. As\nmetallic parts can be highly textured with small defects like holes, we design\na texture scanning and generation method. We assess the quality of the\ngenerated images by training deep learning networks and by testing them on real\ndata from a manufacturer. We demonstrate that we can achieve encouraging\nresults on real defect detection using purely simulated data. Additionally, we\nare able to improve global performances by concatenating simulated and real\ndata, showing that simulations can complement real images to boost\nperformances. Lastly, using domain adaptation techniques helps improving\nslightly our final results.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 08:07:57 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Gutierrez", "Pierre", ""], ["Luschkova", "Maria", ""], ["Cordier", "Antoine", ""], ["Shukor", "Mustafa", ""], ["Schappert", "Mona", ""], ["Dahmen", "Tim", ""]]}, {"id": "2104.02995", "submitter": "Qingqing Long", "authors": "Qingqing Long, Yilun Jin, Yi Wu, Guojie Song", "title": "Theoretically Improving Graph Neural Networks via Anonymous Walk Graph\n  Kernels", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph neural networks (GNNs) have achieved tremendous success in graph\nmining. However, the inability of GNNs to model substructures in graphs remains\na significant drawback. Specifically, message-passing GNNs (MPGNNs), as the\nprevailing type of GNNs, have been theoretically shown unable to distinguish,\ndetect or count many graph substructures. While efforts have been paid to\ncomplement the inability, existing works either rely on pre-defined\nsubstructure sets, thus being less flexible, or are lacking in theoretical\ninsights. In this paper, we propose GSKN, a GNN model with a theoretically\nstronger ability to distinguish graph structures. Specifically, we design GSKN\nbased on anonymous walks (AWs), flexible substructure units, and derive it upon\nfeature mappings of graph kernels (GKs). We theoretically show that GSKN\nprovably extends the 1-WL test, and hence the maximally powerful MPGNNs from\nboth graph-level and node-level viewpoints. Correspondingly, various\nexperiments are leveraged to evaluate GSKN, where GSKN outperforms a wide range\nof baselines, endorsing the analysis.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 08:50:34 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Long", "Qingqing", ""], ["Jin", "Yilun", ""], ["Wu", "Yi", ""], ["Song", "Guojie", ""]]}, {"id": "2104.02997", "submitter": "Stefan Edelkamp", "authors": "Stefan Edelkamp", "title": "On the Power of Refined Skat Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Skat is a fascinating combinatorial card game, show-casing many of the\nintrinsic challenges for modern AI systems such as cooperative and adversarial\nbehaviors (among the players), randomness (in the deal), and partial knowledge\n(due to hidden cards). Given the larger number of tricks and higher degree of\nuncertainty, reinforcement learning is less effective compared to classical\nboard games like Chess and Go. As within the game of Bridge, in Skat we have a\nbidding and trick-taking stage. Prior to the trick-taking and as part of the\nbidding process, one phase in the game is to select two skat cards, whose\nquality may influence subsequent playing performance drastically. This paper\nlooks into different skat selection strategies. Besides predicting the\nprobability of winning and other hand strength functions we propose hard\nexpert-rules and a scoring functions based on refined skat evaluation features.\nExperiments emphasize the impact of the refined skat putting algorithm on the\nplaying performance of the bots, especially for AI bidding and AI game\nselection.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 08:54:58 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Edelkamp", "Stefan", ""]]}, {"id": "2104.03006", "submitter": "Albert Zeyer", "authors": "Albert Zeyer, Andr\\'e Merboldt, Wilfried Michel, Ralf Schl\\\"uter,\n  Hermann Ney", "title": "Librispeech Transducer Model with Internal Language Model Prior\n  Correction", "comments": "accepted at Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present our transducer model on Librispeech. We study variants to include\nan external language model (LM) with shallow fusion and subtract an estimated\ninternal LM. This is justified by a Bayesian interpretation where the\ntransducer model prior is given by the estimated internal LM. The subtraction\nof the internal LM gives us over 14% relative improvement over normal shallow\nfusion. Our transducer has a separate probability distribution for the\nnon-blank labels which allows for easier combination with the external LM, and\neasier estimation of the internal LM. We additionally take care of including\nthe end-of-sentence (EOS) probability of the external LM in the last blank\nprobability which further improves the performance. All our code and setups are\npublished.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 09:18:56 GMT"}, {"version": "v2", "created": "Sun, 13 Jun 2021 00:09:16 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Zeyer", "Albert", ""], ["Merboldt", "Andr\u00e9", ""], ["Michel", "Wilfried", ""], ["Schl\u00fcter", "Ralf", ""], ["Ney", "Hermann", ""]]}, {"id": "2104.03042", "submitter": "Akhil Mathur", "authors": "Akhil Mathur, Daniel J. Beutel, Pedro Porto Buarque de Gusm\\~ao,\n  Javier Fernandez-Marques, Taner Topal, Xinchi Qiu, Titouan Parcollet, Yan\n  Gao, Nicholas D. Lane", "title": "On-device Federated Learning with Flower", "comments": "Accepted at the 2nd On-device Intelligence Workshop @ MLSys 2021.\n  arXiv admin note: substantial text overlap with arXiv:2007.14390", "journal-ref": "On-device Intelligence Workshop at the Fourth Conference on\n  Machine Learning and Systems (MLSys), April 9, 2021", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Federated Learning (FL) allows edge devices to collaboratively learn a shared\nprediction model while keeping their training data on the device, thereby\ndecoupling the ability to do machine learning from the need to store data in\nthe cloud. Despite the algorithmic advancements in FL, the support for\non-device training of FL algorithms on edge devices remains poor. In this\npaper, we present an exploration of on-device FL on various smartphones and\nembedded devices using the Flower framework. We also evaluate the system costs\nof on-device FL and discuss how this quantification could be used to design\nmore efficient FL algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 10:42:14 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Mathur", "Akhil", ""], ["Beutel", "Daniel J.", ""], ["de Gusm\u00e3o", "Pedro Porto Buarque", ""], ["Fernandez-Marques", "Javier", ""], ["Topal", "Taner", ""], ["Qiu", "Xinchi", ""], ["Parcollet", "Titouan", ""], ["Gao", "Yan", ""], ["Lane", "Nicholas D.", ""]]}, {"id": "2104.03047", "submitter": "Chi Zhang", "authors": "Chi Zhang, Nan Song, Guosheng Lin, Yun Zheng, Pan Pan, Yinghui Xu", "title": "Few-Shot Incremental Learning with Continually Evolved Classifiers", "comments": "Accpeted to CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot class-incremental learning (FSCIL) aims to design machine learning\nalgorithms that can continually learn new concepts from a few data points,\nwithout forgetting knowledge of old classes. The difficulty lies in that\nlimited data from new classes not only lead to significant overfitting issues\nbut also exacerbate the notorious catastrophic forgetting problems. Moreover,\nas training data come in sequence in FSCIL, the learned classifier can only\nprovide discriminative information in individual sessions, while FSCIL requires\nall classes to be involved for evaluation. In this paper, we address the FSCIL\nproblem from two aspects. First, we adopt a simple but effective decoupled\nlearning strategy of representations and classifiers that only the classifiers\nare updated in each incremental session, which avoids knowledge forgetting in\nthe representations. By doing so, we demonstrate that a pre-trained backbone\nplus a non-parametric class mean classifier can beat state-of-the-art methods.\nSecond, to make the classifiers learned on individual sessions applicable to\nall classes, we propose a Continually Evolved Classifier (CEC) that employs a\ngraph model to propagate context information between classifiers for\nadaptation. To enable the learning of CEC, we design a pseudo incremental\nlearning paradigm that episodically constructs a pseudo incremental learning\ntask to optimize the graph parameters by sampling data from the base dataset.\nExperiments on three popular benchmark datasets, including CIFAR100,\nminiImageNet, and Caltech-USCD Birds-200-2011 (CUB200), show that our method\nsignificantly outperforms the baselines and sets new state-of-the-art results\nwith remarkable advantages.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 10:54:51 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Zhang", "Chi", ""], ["Song", "Nan", ""], ["Lin", "Guosheng", ""], ["Zheng", "Yun", ""], ["Pan", "Pan", ""], ["Xu", "Yinghui", ""]]}, {"id": "2104.03059", "submitter": "Thomas Unterthiner", "authors": "Jean-Baptiste Cordonnier, Aravindh Mahendran, Alexey Dosovitskiy, Dirk\n  Weissenborn, Jakob Uszkoreit, Thomas Unterthiner", "title": "Differentiable Patch Selection for Image Recognition", "comments": "Accepted to IEEE/CVF Conference on Computer Vision and Pattern\n  Recognition (CVPR) 2021. Code available at\n  https://github.com/google-research/google-research/tree/master/ptopk_patch_selection/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural Networks require large amounts of memory and compute to process high\nresolution images, even when only a small part of the image is actually\ninformative for the task at hand. We propose a method based on a differentiable\nTop-K operator to select the most relevant parts of the input to efficiently\nprocess high resolution images. Our method may be interfaced with any\ndownstream neural network, is able to aggregate information from different\npatches in a flexible way, and allows the whole model to be trained end-to-end\nusing backpropagation. We show results for traffic sign recognition,\ninter-patch relationship reasoning, and fine-grained recognition without using\nobject/part bounding box annotations during training.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 11:15:51 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Cordonnier", "Jean-Baptiste", ""], ["Mahendran", "Aravindh", ""], ["Dosovitskiy", "Alexey", ""], ["Weissenborn", "Dirk", ""], ["Uszkoreit", "Jakob", ""], ["Unterthiner", "Thomas", ""]]}, {"id": "2104.03090", "submitter": "Firoj Alam", "authors": "Firoj Alam, Umair Qazi, Muhammad Imran, Ferda Ofli", "title": "HumAID: Human-Annotated Disaster Incidents Data from Twitter with Deep\n  Learning Benchmarks", "comments": "Accepted in ICWSM-2021, Twitter datasets, Textual content, Natural\n  disasters, Crisis Informatics, Benchmarks, Transformers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.LG cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Social networks are widely used for information consumption and\ndissemination, especially during time-critical events such as natural\ndisasters. Despite its significantly large volume, social media content is\noften too noisy for direct use in any application. Therefore, it is important\nto filter, categorize, and concisely summarize the available content to\nfacilitate effective consumption and decision-making. To address such issues\nautomatic classification systems have been developed using supervised modeling\napproaches, thanks to the earlier efforts on creating labeled datasets.\nHowever, existing datasets are limited in different aspects (e.g., size,\ncontains duplicates) and less suitable to support more advanced and data-hungry\ndeep learning models. In this paper, we present a new large-scale dataset with\n~77K human-labeled tweets, sampled from a pool of ~24 million tweets across 19\ndisaster events that happened between 2016 and 2019. Moreover, we propose a\ndata collection and sampling pipeline, which is important for social media data\nsampling for human annotation. We report multiclass classification results\nusing classic and deep learning (fastText and transformer) based models to set\nthe ground for future studies. The dataset and associated resources are\npublicly available. https://crisisnlp.qcri.org/humaid_dataset.html\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 12:29:36 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 09:12:11 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Alam", "Firoj", ""], ["Qazi", "Umair", ""], ["Imran", "Muhammad", ""], ["Ofli", "Ferda", ""]]}, {"id": "2104.03111", "submitter": "Christopher Dance", "authors": "Jinyoung Choi, Christopher R. Dance, Jung-eun Kim, Seulbin Hwang,\n  Kyung-sik Park", "title": "Risk-Conditioned Distributional Soft Actor-Critic for Risk-Sensitive\n  Navigation", "comments": "ICRA 2021. For associated videos, see https://europe.naverlabs.com/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern navigation algorithms based on deep reinforcement learning (RL) show\npromising efficiency and robustness. However, most deep RL algorithms operate\nin a risk-neutral manner, making no special attempt to shield users from\nrelatively rare but serious outcomes, even if such shielding might cause little\nloss of performance. Furthermore, such algorithms typically make no provisions\nto ensure safety in the presence of inaccuracies in the models on which they\nwere trained, beyond adding a cost-of-collision and some domain randomization\nwhile training, in spite of the formidable complexity of the environments in\nwhich they operate. In this paper, we present a novel distributional RL\nalgorithm that not only learns an uncertainty-aware policy, but can also change\nits risk measure without expensive fine-tuning or retraining. Our method shows\nsuperior performance and safety over baselines in partially-observed navigation\ntasks. We also demonstrate that agents trained using our method can adapt their\npolicies to a wide range of risk measures at run-time.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 13:23:53 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 11:18:10 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Choi", "Jinyoung", ""], ["Dance", "Christopher R.", ""], ["Kim", "Jung-eun", ""], ["Hwang", "Seulbin", ""], ["Park", "Kyung-sik", ""]]}, {"id": "2104.03149", "submitter": "Corentin Dancette", "authors": "Corentin Dancette, Remi Cadene, Damien Teney, Matthieu Cord", "title": "Beyond Question-Based Biases: Assessing Multimodal Shortcut Learning in\n  Visual Question Answering", "comments": "Code is available at https://github.com/cdancette/detect-shortcuts", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce an evaluation methodology for visual question answering (VQA) to\nbetter diagnose cases of shortcut learning. These cases happen when a model\nexploits spurious statistical regularities to produce correct answers but does\nnot actually deploy the desired behavior. There is a need to identify possible\nshortcuts in a dataset and assess their use before deploying a model in the\nreal world. The research community in VQA has focused exclusively on\nquestion-based shortcuts, where a model might, for example, answer \"What is the\ncolor of the sky\" with \"blue\" by relying mostly on the question-conditional\ntraining prior and give little weight to visual evidence. We go a step further\nand consider multimodal shortcuts that involve both questions and images. We\nfirst identify potential shortcuts in the popular VQA v2 training set by mining\ntrivial predictive rules such as co-occurrences of words and visual elements.\nWe then create VQA-CE, a new evaluation set made of CounterExamples i.e.\nquestions where the mined rules lead to incorrect answers. We use this new\nevaluation in a large-scale study of existing models. We demonstrate that even\nstate-of-the-art models perform poorly and that existing techniques to reduce\nbiases are largely ineffective in this context. Our findings suggest that past\nwork on question-based biases in VQA has only addressed one facet of a complex\nissue. The code for our method is available at\nhttps://github.com/cdancette/detect-shortcuts\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 14:28:22 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 09:53:36 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Dancette", "Corentin", ""], ["Cadene", "Remi", ""], ["Teney", "Damien", ""], ["Cord", "Matthieu", ""]]}, {"id": "2104.03150", "submitter": "Eren Aksoy", "authors": "Cristofer Englund and Eren Erdal Aksoy and Fernando Alonso-Fernandez\n  and Martin Daniel Cooney and Sepideh Pashami and Bjorn Astrand", "title": "AI perspectives in Smart Cities and Communities to enable road vehicle\n  automation and smart traffic control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smart Cities and Communities (SCC) constitute a new paradigm in urban\ndevelopment. SCC ideates on a data-centered society aiming at improving\nefficiency by automating and optimizing activities and utilities. Information\nand communication technology along with internet of things enables data\ncollection and with the help of artificial intelligence (AI) situation\nawareness can be obtained to feed the SCC actors with enriched knowledge. This\npaper describes AI perspectives in SCC and gives an overview of AI-based\ntechnologies used in traffic to enable road vehicle automation and smart\ntraffic control. Perception, Smart Traffic Control and Driver Modelling are\ndescribed along with open research challenges and standardization to help\nintroduce advanced driver assistance systems and automated vehicle\nfunctionality in traffic. To fully realize the potential of SCC, to create a\nholistic view on a city level, the availability of data from different\nstakeholders is need. Further, though AI technologies provide accurate\npredictions and classifications there is an ambiguity regarding the correctness\nof their outputs. This can make it difficult for the human operator to trust\nthe system. Today there are no methods that can be used to match function\nrequirements with the level of detail in data annotation in order to train an\naccurate model. Another challenge related to trust is explainability, while the\nmodels have difficulties explaining how they come to a certain conclusion it is\ndifficult for humans to trust it.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 14:31:08 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 10:46:01 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Englund", "Cristofer", ""], ["Aksoy", "Eren Erdal", ""], ["Alonso-Fernandez", "Fernando", ""], ["Cooney", "Martin Daniel", ""], ["Pashami", "Sepideh", ""], ["Astrand", "Bjorn", ""]]}, {"id": "2104.03154", "submitter": "Lucas Schott", "authors": "Lucas Schott, Manon C\\'esaire, Hatem Hajri, Sylvain Lamprier", "title": "Improving Robustness of Deep Reinforcement Learning Agents: Environment\n  Attacks based on Critic Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To improve policy robustness of deep reinforcement learning agents, a line of\nrecent works focus on producing disturbances of the environment. Existing\napproaches of the literature to generate meaningful disturbances of the\nenvironment are adversarial reinforcement learning methods. These methods set\nthe problem as a two-player game between the protagonist agent, which learns to\nperform a task in an environment, and the adversary agent, which learns to\ndisturb the protagonist via modifications of the considered environment. Both\nprotagonist and adversary are trained with deep reinforcement learning\nalgorithms. Alternatively, we propose in this paper to build on gradient-based\nadversarial attacks, usually used for classification tasks for instance, that\nwe apply on the critic network of the protagonist to identify efficient\ndisturbances of the environment. Rather than learning an attacker policy, which\nusually reveals as very complex and unstable, we leverage the knowledge of the\ncritic network of the protagonist, to dynamically complexify the task at each\nstep of the learning process. We show that our method, while being faster and\nlighter, leads to significantly better improvements in policy robustness than\nexisting methods of the literature.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 14:37:23 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Schott", "Lucas", ""], ["C\u00e9saire", "Manon", ""], ["Hajri", "Hatem", ""], ["Lamprier", "Sylvain", ""]]}, {"id": "2104.03169", "submitter": "Boubakr Nour", "authors": "Afaf Taik and Boubakr Nour and Soumaya Cherkaoui", "title": "Empowering Prosumer Communities in Smart Grid with Wireless\n  Communications and Federated Edge Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AI cs.NI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The exponential growth of distributed energy resources is enabling the\ntransformation of traditional consumers in the smart grid into prosumers. Such\ntransition presents a promising opportunity for sustainable energy trading.\nYet, the integration of prosumers in the energy market imposes new\nconsiderations in designing unified and sustainable frameworks for efficient\nuse of the power and communication infrastructure. Furthermore, several issues\nneed to be tackled to adequately promote the adoption of decentralized\nrenewable-oriented systems, such as communication overhead, data privacy,\nscalability, and sustainability. In this article, we present the different\naspects and challenges to be addressed for building efficient energy trading\nmarkets in relation to communication and smart decision-making. Accordingly, we\npropose a multi-level pro-decision framework for prosumer communities to\nachieve collective goals. Since the individual decisions of prosumers are\nmainly driven by individual self-sufficiency goals, the framework prioritizes\nthe individual prosumers' decisions and relies on 5G wireless network for fast\ncoordination among community members. In fact, each prosumer predicts energy\nproduction and consumption to make proactive trading decisions as a response to\ncollective-level requests. Moreover, the collaboration of the community is\nfurther extended by including the collaborative training of prediction models\nusing Federated Learning, assisted by edge servers and prosumer home-area\nequipment. In addition to preserving prosumers' privacy, we show through\nevaluations that training prediction models using Federated Learning yields\nhigh accuracy for different energy resources while reducing the communication\noverhead.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 14:57:57 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Taik", "Afaf", ""], ["Nour", "Boubakr", ""], ["Cherkaoui", "Soumaya", ""]]}, {"id": "2104.03189", "submitter": "Tunazzina Islam", "authors": "Tunazzina Islam, Dan Goldwasser", "title": "Analysis of Twitter Users' Lifestyle Choices using Joint Embedding Model", "comments": "accepted at 15th International AAAI Conference on Web and Social\n  Media (ICWSM-2021), 12 pages. Minor changes for camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiview representation learning of data can help construct coherent and\ncontextualized users' representations on social media. This paper suggests a\njoint embedding model, incorporating users' social and textual information to\nlearn contextualized user representations used for understanding their\nlifestyle choices. We apply our model to tweets related to two lifestyle\nactivities, `Yoga' and `Keto diet' and use it to analyze users' activity type\nand motivation. We explain the data collection and annotation process in detail\nand provide an in-depth analysis of users from different classes based on their\nTwitter content. Our experiments show that our model results in performance\nimprovements in both domains.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 15:29:36 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2021 15:36:19 GMT"}, {"version": "v3", "created": "Tue, 4 May 2021 18:14:32 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Islam", "Tunazzina", ""], ["Goldwasser", "Dan", ""]]}, {"id": "2104.03207", "submitter": "Soheyla Amirian", "authors": "Soheyla Amirian, Abolfazl Farahani, Hamid R. Arabnia, Khaled Rasheed,\n  Thiab R. Taha", "title": "The Use of Video Captioning for Fostering Physical Activity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Video Captioning is considered to be one of the most challenging problems in\nthe field of computer vision. Video Captioning involves the combination of\ndifferent deep learning models to perform object detection, action detection,\nand localization by processing a sequence of image frames. It is crucial to\nconsider the sequence of actions in a video in order to generate a meaningful\ndescription of the overall action event. A reliable, accurate, and real-time\nvideo captioning method can be used in many applications. However, this paper\nfocuses on one application: video captioning for fostering and facilitating\nphysical activities. In broad terms, the work can be considered to be assistive\ntechnology. Lack of physical activity appears to be increasingly widespread in\nmany nations due to many factors, the most important being the convenience that\ntechnology has provided in workplaces. The adopted sedentary lifestyle is\nbecoming a significant public health issue. Therefore, it is essential to\nincorporate more physical movements into our daily lives. Tracking one's daily\nphysical activities would offer a base for comparison with activities performed\nin subsequent days. With the above in mind, this paper proposes a video\ncaptioning framework that aims to describe the activities in a video and\nestimate a person's daily physical activity level. This framework could\npotentially help people trace their daily movements to reduce an inactive\nlifestyle's health risks. The work presented in this paper is still in its\ninfancy. The initial steps of the application are outlined in this paper. Based\non our preliminary research, this project has great merit.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 15:52:48 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Amirian", "Soheyla", ""], ["Farahani", "Abolfazl", ""], ["Arabnia", "Hamid R.", ""], ["Rasheed", "Khaled", ""], ["Taha", "Thiab R.", ""]]}, {"id": "2104.03226", "submitter": "Satvik Garg", "authors": "Satvik Garg and Himanshu Jindal", "title": "Evaluation of Time Series Forecasting Models for Estimation of PM2.5\n  Levels in Air", "comments": "8 pages, This paper is accepted and presented in the IEEE 6th I2CT\n  2021 conference. The final version of this paper will appear in the\n  conference proceedings", "journal-ref": null, "doi": "10.1109/I2CT51068.2021.9418215", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Air contamination in urban areas has risen consistently over the past few\nyears. Due to expanding industrialization and increasing concentration of toxic\ngases in the climate, the air is getting more poisonous step by step at an\nalarming rate. Since the arrival of the Coronavirus pandemic, it is getting\nmore critical to lessen air contamination to reduce its impact. The specialists\nand environmentalists are making a valiant effort to gauge air contamination\nlevels. However, its genuinely unpredictable to mimic subatomic communication\nin the air, which brings about off base outcomes. There has been an ascent in\nusing machine learning and deep learning models to foresee the results on time\nseries data. This study adopts ARIMA, FBProphet, and deep learning models such\nas LSTM, 1D CNN, to estimate the concentration of PM2.5 in the environment. Our\npredicted results convey that all adopted methods give comparative outcomes in\nterms of average root mean squared error. However, the LSTM outperforms all\nother models with reference to mean absolute percentage error.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 16:24:39 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Garg", "Satvik", ""], ["Jindal", "Himanshu", ""]]}, {"id": "2104.03252", "submitter": "Pieter Robberechts", "authors": "Maaike Van Roy, Pieter Robberechts, Wen-Chi Yang, Luc De Raedt, Jesse\n  Davis", "title": "Leaving Goals on the Pitch: Evaluating Decision Making in Soccer", "comments": null, "journal-ref": "2021 MIT Sloan Sports Analytics Conference", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Analysis of the popular expected goals (xG) metric in soccer has determined\nthat a (slightly) smaller number of high-quality attempts will likely yield\nmore goals than a slew of low-quality ones. This observation has driven a\nchange in shooting behavior. Teams are passing up on shots from outside the\npenalty box, in the hopes of generating a better shot closer to goal later on.\nThis paper evaluates whether this decrease in long-distance shots is warranted.\nTherefore, we propose a novel generic framework to reason about decision-making\nin soccer by combining techniques from machine learning and artificial\nintelligence (AI). First, we model how a team has behaved offensively over the\ncourse of two seasons by learning a Markov Decision Process (MDP) from event\nstream data. Second, we use reasoning techniques arising from the AI literature\non verification to each team's MDP. This allows us to reason about the efficacy\nof certain potential decisions by posing counterfactual questions to the MDP.\nOur key conclusion is that teams would score more goals if they shot more often\nfrom outside the penalty box in a small number of team-specific locations. The\nproposed framework can easily be extended and applied to analyze other aspects\nof the game.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 16:56:31 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Van Roy", "Maaike", ""], ["Robberechts", "Pieter", ""], ["Yang", "Wen-Chi", ""], ["De Raedt", "Luc", ""], ["Davis", "Jesse", ""]]}, {"id": "2104.03279", "submitter": "Philipp Seidl", "authors": "Philipp Seidl, Philipp Renz, Natalia Dyubankova, Paulo Neves, Jonas\n  Verhoeven, Marwin Segler, J\\\"org K. Wegner, Sepp Hochreiter, G\\\"unter\n  Klambauer", "title": "Modern Hopfield Networks for Few- and Zero-Shot Reaction Template\n  Prediction", "comments": "14 pages + 12 pages appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI q-bio.BM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding synthesis routes for molecules of interest is an essential step in\nthe discovery of new drugs and materials. To find such routes,\ncomputer-assisted synthesis planning (CASP) methods are employed which rely on\na model of chemical reactivity. In this study, we model single-step\nretrosynthesis in a template-based approach using modern Hopfield networks\n(MHNs). We adapt MHNs to associate different modalities, reaction templates and\nmolecules, which allows the model to leverage structural information about\nreaction templates. This approach significantly improves the performance of\ntemplate relevance prediction, especially for templates with few or zero\ntraining examples. With inference speed several times faster than that of\nbaseline methods, we improve predictive performance for top-k exact match\naccuracy for $\\mathrm{k}\\geq5$ in the retrosynthesis benchmark USPTO-50k.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 17:35:00 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 15:57:01 GMT"}, {"version": "v3", "created": "Tue, 15 Jun 2021 13:24:02 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Seidl", "Philipp", ""], ["Renz", "Philipp", ""], ["Dyubankova", "Natalia", ""], ["Neves", "Paulo", ""], ["Verhoeven", "Jonas", ""], ["Segler", "Marwin", ""], ["Wegner", "J\u00f6rg K.", ""], ["Hochreiter", "Sepp", ""], ["Klambauer", "G\u00fcnter", ""]]}, {"id": "2104.03309", "submitter": "Aayush Bansal", "authors": "Zhiqiu Lin and Deva Ramanan and Aayush Bansal", "title": "Streaming Self-Training via Domain-Agnostic Unlabeled Images", "comments": "Project Page: https://www.cs.cmu.edu/~aayushb/SST/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present streaming self-training (SST) that aims to democratize the process\nof learning visual recognition models such that a non-expert user can define a\nnew task depending on their needs via a few labeled examples and minimal domain\nknowledge. Key to SST are two crucial observations: (1) domain-agnostic\nunlabeled images enable us to learn better models with a few labeled examples\nwithout any additional knowledge or supervision; and (2) learning is a\ncontinuous process and can be done by constructing a schedule of learning\nupdates that iterates between pre-training on novel segments of the streams of\nunlabeled data, and fine-tuning on the small and fixed labeled dataset. This\nallows SST to overcome the need for a large number of domain-specific labeled\nand unlabeled examples, exorbitant computational resources, and\ndomain/task-specific knowledge. In this setting, classical semi-supervised\napproaches require a large amount of domain-specific labeled and unlabeled\nexamples, immense resources to process data, and expert knowledge of a\nparticular task. Due to these reasons, semi-supervised learning has been\nrestricted to a few places that can house required computational and human\nresources. In this work, we overcome these challenges and demonstrate our\nfindings for a wide range of visual recognition tasks including fine-grained\nimage classification, surface normal estimation, and semantic segmentation. We\nalso demonstrate our findings for diverse domains including medical, satellite,\nand agricultural imagery, where there does not exist a large amount of labeled\nor unlabeled data.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 17:58:39 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Lin", "Zhiqiu", ""], ["Ramanan", "Deva", ""], ["Bansal", "Aayush", ""]]}, {"id": "2104.03311", "submitter": "Chuang Gan", "authors": "Zhiao Huang, Yuanming Hu, Tao Du, Siyuan Zhou, Hao Su, Joshua B.\n  Tenenbaum, Chuang Gan", "title": "PlasticineLab: A Soft-Body Manipulation Benchmark with Differentiable\n  Physics", "comments": "Accepted to ICLR 2021 as a spotlight presentation. Project page:\n  http://plasticinelab.csail.mit.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.GR cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Simulated virtual environments serve as one of the main driving forces behind\ndeveloping and evaluating skill learning algorithms. However, existing\nenvironments typically only simulate rigid body physics. Additionally, the\nsimulation process usually does not provide gradients that might be useful for\nplanning and control optimizations. We introduce a new differentiable physics\nbenchmark called PasticineLab, which includes a diverse collection of soft body\nmanipulation tasks. In each task, the agent uses manipulators to deform the\nplasticine into the desired configuration. The underlying physics engine\nsupports differentiable elastic and plastic deformation using the DiffTaichi\nsystem, posing many under-explored challenges to robotic agents. We evaluate\nseveral existing reinforcement learning (RL) methods and gradient-based methods\non this benchmark. Experimental results suggest that 1) RL-based approaches\nstruggle to solve most of the tasks efficiently; 2) gradient-based approaches,\nby optimizing open-loop control sequences with the built-in differentiable\nphysics engine, can rapidly find a solution within tens of iterations, but\nstill fall short on multi-stage tasks that require long-term planning. We\nexpect that PlasticineLab will encourage the development of novel algorithms\nthat combine differentiable physics and RL for more complex physics-based skill\nlearning tasks.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 17:59:23 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Huang", "Zhiao", ""], ["Hu", "Yuanming", ""], ["Du", "Tao", ""], ["Zhou", "Siyuan", ""], ["Su", "Hao", ""], ["Tenenbaum", "Joshua B.", ""], ["Gan", "Chuang", ""]]}, {"id": "2104.03337", "submitter": "Soheyla Amirian", "authors": "Soheyla Amirian, Khaled Rasheed, Thiab R. Taha, Hamid R. Arabnia", "title": "Automatic Generation of Descriptive Titles for Video Clips Using Deep\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Over the last decade, the use of Deep Learning in many applications produced\nresults that are comparable to and in some cases surpassing human expert\nperformance. The application domains include diagnosing diseases, finance,\nagriculture, search engines, robot vision, and many others. In this paper, we\nare proposing an architecture that utilizes image/video captioning methods and\nNatural Language Processing systems to generate a title and a concise abstract\nfor a video. Such a system can potentially be utilized in many application\ndomains, including, the cinema industry, video search engines, security\nsurveillance, video databases/warehouses, data centers, and others. The\nproposed system functions and operates as followed: it reads a video;\nrepresentative image frames are identified and selected; the image frames are\ncaptioned; NLP is applied to all generated captions together with text\nsummarization; and finally, a title and an abstract are generated for the\nvideo. All functions are performed automatically. Preliminary results are\nprovided in this paper using publicly available datasets. This paper is not\nconcerned about the efficiency of the system at the execution time. We hope to\nbe able to address execution efficiency issues in our subsequent publications.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 18:14:18 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Amirian", "Soheyla", ""], ["Rasheed", "Khaled", ""], ["Taha", "Thiab R.", ""], ["Arabnia", "Hamid R.", ""]]}, {"id": "2104.03349", "submitter": "Kolawole Ogunsina", "authors": "Kolawole Ogunsina and Daniel DeLaurentis", "title": "Enabling Integration and Interaction for Decentralized Artificial\n  Intelligence in Airline Disruption Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Airline disruption management traditionally seeks to address three problem\ndimensions: aircraft scheduling, crew scheduling, and passenger scheduling, in\nthat order. However, current efforts have, at most, only addressed the first\ntwo problem dimensions concurrently and do not account for the propagative\neffects that uncertain scheduling outcomes in one dimension can have on another\ndimension. In addition, existing approaches for airline disruption management\ninclude human specialists who decide on necessary corrective actions for\nairline schedule disruptions on the day of operation. However, human\nspecialists are limited in their ability to process copious amounts of\ninformation imperative for making robust decisions that simultaneously address\nall problem dimensions during disruption management. Therefore, there is a need\nto augment the decision-making capabilities of a human specialist with\nquantitative and qualitative tools that can rationalize complex interactions\namongst all dimensions in airline disruption management, and provide objective\ninsights to the specialists in the airline operations control center. To that\neffect, we provide a discussion and demonstration of an agnostic and systematic\nparadigm for enabling expeditious simultaneously-integrated recovery of all\nproblem dimensions during airline disruption management, through an intelligent\nmulti-agent system that employs principles from artificial intelligence and\ndistributed ledger technology.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 18:59:02 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Ogunsina", "Kolawole", ""], ["DeLaurentis", "Daniel", ""]]}, {"id": "2104.03372", "submitter": "Benjamin Doerr", "authors": "Benjamin Doerr and Timo K\\\"otzing", "title": "Lower Bounds from Fitness Levels Made Easy", "comments": "Extended version of a paper appearing in the proceedings of GECCO\n  2021", "journal-ref": null, "doi": "10.1145/3449639.3459352", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the first and easy to use techniques for proving run time bounds for\nevolutionary algorithms is the so-called method of fitness levels by Wegener.\nIt uses a partition of the search space into a sequence of levels which are\ntraversed by the algorithm in increasing order, possibly skipping levels. An\neasy, but often strong upper bound for the run time can then be derived by\nadding the reciprocals of the probabilities to leave the levels (or upper\nbounds for these). Unfortunately, a similarly effective method for proving\nlower bounds has not yet been established. The strongest such method, proposed\nby Sudholt (2013), requires a careful choice of the viscosity parameters\n$\\gamma_{i,j}$, $0 \\le i < j \\le n$.\n  In this paper we present two new variants of the method, one for upper and\none for lower bounds. Besides the level leaving probabilities, they only rely\non the probabilities that levels are visited at all. We show that these can be\ncomputed or estimated without greater difficulties and apply our method to\nreprove the following known results in an easy and natural way. (i) The precise\nrun time of the (1+1) EA on \\textsc{LeadingOnes}. (ii) A lower bound for the\nrun time of the (1+1) EA on \\textsc{OneMax}, tight apart from an $O(n)$ term.\n(iii) A lower bound for the run time of the (1+1) EA on long $k$-paths. We also\nprove a tighter lower bound for the run time of the (1+1) EA on jump functions\nby showing that, regardless of the jump size, only with probability $O(2^{-n})$\nthe algorithm can avoid to jump over the valley of low fitness.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 19:50:53 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 09:54:13 GMT"}, {"version": "v3", "created": "Wed, 28 Apr 2021 08:18:18 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Doerr", "Benjamin", ""], ["K\u00f6tzing", "Timo", ""]]}, {"id": "2104.03404", "submitter": "Nicholas Guttenberg", "authors": "Nicholas Guttenberg, Marek Rosa", "title": "Bootstrapping of memetic from genetic evolution via inter-agent\n  selection pressures", "comments": "9 pages, 3 figures, submitted to ALife 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We create an artificial system of agents (attention-based neural networks)\nwhich selectively exchange messages with each-other in order to study the\nemergence of memetic evolution and how memetic evolutionary pressures interact\nwith genetic evolution of the network weights. We observe that the ability of\nagents to exert selection pressures on each-other is essential for memetic\nevolution to bootstrap itself into a state which has both high-fidelity\nreplication of memes, as well as continuing production of new memes over time.\nHowever, in this system there is very little interaction between this memetic\n'ecology' and underlying tasks driving individual fitness - the emergent meme\nlayer appears to be neither helpful nor harmful to agents' ability to learn to\nsolve tasks. Sourcecode for these experiments is available at\nhttps://github.com/GoodAI/memes\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 21:31:05 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Guttenberg", "Nicholas", ""], ["Rosa", "Marek", ""]]}, {"id": "2104.03414", "submitter": "Youngeun Kim", "authors": "Youngeun Kim, Yeshwanth Venkatesha and Priyadarshini Panda", "title": "PrivateSNN: Fully Privacy-Preserving Spiking Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  How can we bring both privacy and energy-efficiency to a neural system on\nedge devices? In this paper, we propose PrivateSNN, which aims to build\nlow-power Spiking Neural Networks (SNNs) from a pre-trained ANN model without\nleaking sensitive information contained in a dataset. Here, we tackle two types\nof leakage problems: 1) Data leakage caused when the networks access real\ntraining data during an ANN-SNN conversion process. 2) Class leakage is the\nconcept of leakage caused when class-related features can be reconstructed from\nnetwork parameters. In order to address the data leakage issue, we generate\nsynthetic images from the pre-trained ANNs and convert ANNs to SNNs using\ngenerated images. However, converted SNNs are still vulnerable with respect to\nthe class leakage since the weight parameters have the same (or scaled) value\nwith respect to ANN parameters. Therefore, we encrypt SNN weights by training\nSNNs with a temporal spike-based learning rule. Updating weight parameters with\ntemporal data makes networks difficult to be interpreted in the spatial domain.\nWe observe that the encrypted PrivateSNN can be implemented not only without\nthe huge performance drop (less than ~5%) but also with significant\nenergy-efficiency gain (about x60 compared to the standard ANN). We conduct\nextensive experiments on various datasets including CIFAR10, CIFAR100, and\nTinyImageNet, highlighting the importance of privacy-preserving SNN training.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 22:14:02 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Kim", "Youngeun", ""], ["Venkatesha", "Yeshwanth", ""], ["Panda", "Priyadarshini", ""]]}, {"id": "2104.03435", "submitter": "Sethuraman Sankaran", "authors": "Sethuraman Sankaran, David Yang, Ser-Nam Lim", "title": "Multimodal Fusion Refiner Networks", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Tasks that rely on multi-modal information typically include a fusion module\nthat combines information from different modalities. In this work, we develop a\nRefiner Fusion Network (ReFNet) that enables fusion modules to combine strong\nunimodal representation with strong multimodal representations. ReFNet combines\nthe fusion network with a decoding/defusing module, which imposes a\nmodality-centric responsibility condition. This approach addresses a big gap in\nexisting multimodal fusion frameworks by ensuring that both unimodal and fused\nrepresentations are strongly encoded in the latent fusion space. We demonstrate\nthat the Refiner Fusion Network can improve upon performance of powerful\nbaseline fusion modules such as multimodal transformers. The refiner network\nenables inducing graphical representations of the fused embeddings in the\nlatent space, which we prove under certain conditions and is supported by\nstrong empirical results in the numerical experiments. These graph structures\nare further strengthened by combining the ReFNet with a Multi-Similarity\ncontrastive loss function. The modular nature of Refiner Fusion Network lends\nitself to be combined with different fusion architectures easily, and in\naddition, the refiner step can be applied for pre-training on unlabeled\ndatasets, thus leveraging unsupervised data towards improving performance. We\ndemonstrate the power of Refiner Fusion Networks on three datasets, and further\nshow that they can maintain performance with only a small fraction of labeled\ndata.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 00:02:01 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Sankaran", "Sethuraman", ""], ["Yang", "David", ""], ["Lim", "Ser-Nam", ""]]}, {"id": "2104.03483", "submitter": "Q.Vera Liao", "authors": "Q. Vera Liao, Milena Pribi\\'c, Jaesik Han, Sarah Miller, Daby Sow", "title": "Question-Driven Design Process for Explainable AI User Experiences", "comments": "working paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A pervasive design issue of AI systems is their explainability--how to\nprovide appropriate information to help users understand the AI. The technical\nfield of explainable AI (XAI) has produced a rich toolbox of techniques.\nDesigners are now tasked with the challenges of how to select the most suitable\nXAI techniques and translate them into UX solutions. Informed by our previous\nwork studying design challenges around XAI UX, this work proposes a design\nprocess to tackle these challenges. We review our and related prior work to\nidentify requirements that the process should fulfill, and accordingly, propose\na Question-Driven Design Process that grounds the user needs, choices of XAI\ntechniques, design, and evaluation of XAI UX all in the user questions. We\nprovide a mapping guide between prototypical user questions and exemplars of\nXAI techniques to reframe the technical space of XAI, also serving as boundary\nobjects to support collaboration between designers and AI engineers. We\ndemonstrate it with a use case of designing XAI for healthcare adverse events\nprediction, and discuss lessons learned for tackling design challenges of AI\nsystems.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 02:51:36 GMT"}, {"version": "v2", "created": "Sun, 9 May 2021 01:17:55 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Liao", "Q. Vera", ""], ["Pribi\u0107", "Milena", ""], ["Han", "Jaesik", ""], ["Miller", "Sarah", ""], ["Sow", "Daby", ""]]}, {"id": "2104.03488", "submitter": "Loris Nanni", "authors": "Loris Nanni, Stefano Ghidoni, Sheryl Brahnam", "title": "Deep Features for training Support Vector Machine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Features play a crucial role in computer vision. Initially designed to detect\nsalient elements by means of handcrafted algorithms, features are now often\nlearned by different layers in Convolutional Neural Networks (CNNs). This paper\ndevelops a generic computer vision system based on features extracted from\ntrained CNNs. Multiple learned features are combined into a single structure to\nwork on different image classification tasks. The proposed system was\nexperimentally derived by testing several approaches for extracting features\nfrom the inner layers of CNNs and using them as inputs to SVMs that are then\ncombined by sum rule. Dimensionality reduction techniques are used to reduce\nthe high dimensionality of inner layers. The resulting vision system is shown\nto significantly boost the performance of standard CNNs across a large and\ndiverse collection of image data sets. An ensemble of different topologies\nusing the same approach obtains state-of-the-art results on a virus data set.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 03:13:09 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 22:29:51 GMT"}], "update_date": "2021-06-30", "authors_parsed": [["Nanni", "Loris", ""], ["Ghidoni", "Stefano", ""], ["Brahnam", "Sheryl", ""]]}, {"id": "2104.03501", "submitter": "Jiaxin Li", "authors": "Jiaxin Li, Gim Hee Lee", "title": "DeepI2P: Image-to-Point Cloud Registration via Deep Classification", "comments": "CVPR 2021. Main paper and supplementary materials", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents DeepI2P: a novel approach for cross-modality registration\nbetween an image and a point cloud. Given an image (e.g. from a rgb-camera) and\na general point cloud (e.g. from a 3D Lidar scanner) captured at different\nlocations in the same scene, our method estimates the relative rigid\ntransformation between the coordinate frames of the camera and Lidar. Learning\ncommon feature descriptors to establish correspondences for the registration is\ninherently challenging due to the lack of appearance and geometric correlations\nacross the two modalities. We circumvent the difficulty by converting the\nregistration problem into a classification and inverse camera projection\noptimization problem. A classification neural network is designed to label\nwhether the projection of each point in the point cloud is within or beyond the\ncamera frustum. These labeled points are subsequently passed into a novel\ninverse camera projection solver to estimate the relative pose. Extensive\nexperimental results on Oxford Robotcar and KITTI datasets demonstrate the\nfeasibility of our approach. Our source code is available at\nhttps://github.com/lijx10/DeepI2P\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 04:27:32 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Li", "Jiaxin", ""], ["Lee", "Gim Hee", ""]]}, {"id": "2104.03503", "submitter": "Zhiwei Xu", "authors": "Zhiwei Xu, Bin Zhang, Yunpeng Bai, Dapeng Li, Guoliang Fan", "title": "Learning to Coordinate via Multiple Graph Neural Networks", "comments": "12 pages, 4 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The collaboration between agents has gradually become an important topic in\nmulti-agent systems. The key is how to efficiently solve the credit assignment\nproblems. This paper introduces MGAN for collaborative multi-agent\nreinforcement learning, a new algorithm that combines graph convolutional\nnetworks and value-decomposition methods. MGAN learns the representation of\nagents from different perspectives through multiple graph networks, and\nrealizes the proper allocation of attention between all agents. We show the\namazing ability of the graph network in representation learning by visualizing\nthe output of the graph network, and therefore improve interpretability for the\nactions of each agent in the multi-agent system.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 04:33:00 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Xu", "Zhiwei", ""], ["Zhang", "Bin", ""], ["Bai", "Yunpeng", ""], ["Li", "Dapeng", ""], ["Fan", "Guoliang", ""]]}, {"id": "2104.03525", "submitter": "Seo Taek Kong", "authors": "Seo Taek Kong, Soomin Jeon, Jaewon Lee, Hongseok Lee, Kyu-Hwan Jung", "title": "Relieving the Plateau: Active Semi-Supervised Learning for a Better\n  Landscape", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning (DL) relies on massive amounts of labeled data, and improving\nits labeled sample-efficiency remains one of the most important problems since\nits advent. Semi-supervised learning (SSL) leverages unlabeled data that are\nmore accessible than their labeled counterparts. Active learning (AL) selects\nunlabeled instances to be annotated by a human-in-the-loop in hopes of better\nperformance with less labeled data. Given the accessible pool of unlabeled data\nin pool-based AL, it seems natural to use SSL when training and AL to update\nthe labeled set; however, algorithms designed for their combination remain\nlimited. In this work, we first prove that convergence of gradient descent on\nsufficiently wide ReLU networks can be expressed in terms of their Gram matrix'\neigen-spectrum. Equipped with a few theoretical insights, we propose\nconvergence rate control (CRC), an AL algorithm that selects unlabeled data to\nimprove the problem conditioning upon inclusion to the labeled set, by\nformulating an acquisition step in terms of improving training dynamics.\nExtensive experiments show that SSL algorithms coupled with CRC can achieve\nhigh performance using very few labeled data.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 06:03:59 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Kong", "Seo Taek", ""], ["Jeon", "Soomin", ""], ["Lee", "Jaewon", ""], ["Lee", "Hongseok", ""], ["Jung", "Kyu-Hwan", ""]]}, {"id": "2104.03531", "submitter": "Zhao Kang", "authors": "Juncheng Lv and Zhao Kang and Xiao Lu and Zenglin Xu", "title": "Pseudo-supervised Deep Subspace Clustering", "comments": null, "journal-ref": "IEEE Transactions on Image Processing 2021", "doi": "10.1109/TIP.2021.3079800", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Auto-Encoder (AE)-based deep subspace clustering (DSC) methods have achieved\nimpressive performance due to the powerful representation extracted using deep\nneural networks while prioritizing categorical separability. However,\nself-reconstruction loss of an AE ignores rich useful relation information and\nmight lead to indiscriminative representation, which inevitably degrades the\nclustering performance. It is also challenging to learn high-level similarity\nwithout feeding semantic labels. Another unsolved problem facing DSC is the\nhuge memory cost due to $n\\times n$ similarity matrix, which is incurred by the\nself-expression layer between an encoder and decoder. To tackle these problems,\nwe use pairwise similarity to weigh the reconstruction loss to capture local\nstructure information, while a similarity is learned by the self-expression\nlayer. Pseudo-graphs and pseudo-labels, which allow benefiting from uncertain\nknowledge acquired during network training, are further employed to supervise\nsimilarity learning. Joint learning and iterative training facilitate to obtain\nan overall optimal solution. Extensive experiments on benchmark datasets\ndemonstrate the superiority of our approach. By combining with the $k$-nearest\nneighbors algorithm, we further show that our method can address the\nlarge-scale and out-of-sample problems.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 06:25:47 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Lv", "Juncheng", ""], ["Kang", "Zhao", ""], ["Lu", "Xiao", ""], ["Xu", "Zenglin", ""]]}, {"id": "2104.03538", "submitter": "Szu-Wei Fu", "authors": "Szu-Wei Fu, Cheng Yu, Tsun-An Hsieh, Peter Plantinga, Mirco Ravanelli,\n  Xugang Lu, Yu Tsao", "title": "MetricGAN+: An Improved Version of MetricGAN for Speech Enhancement", "comments": "Accepted by Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discrepancy between the cost function used for training a speech\nenhancement model and human auditory perception usually makes the quality of\nenhanced speech unsatisfactory. Objective evaluation metrics which consider\nhuman perception can hence serve as a bridge to reduce the gap. Our previously\nproposed MetricGAN was designed to optimize objective metrics by connecting the\nmetric with a discriminator. Because only the scores of the target evaluation\nfunctions are needed during training, the metrics can even be\nnon-differentiable. In this study, we propose a MetricGAN+ in which three\ntraining techniques incorporating domain-knowledge of speech processing are\nproposed. With these techniques, experimental results on the VoiceBank-DEMAND\ndataset show that MetricGAN+ can increase PESQ score by 0.3 compared to the\nprevious MetricGAN and achieve state-of-the-art results (PESQ score = 3.15).\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 06:46:35 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 09:15:25 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Fu", "Szu-Wei", ""], ["Yu", "Cheng", ""], ["Hsieh", "Tsun-An", ""], ["Plantinga", "Peter", ""], ["Ravanelli", "Mirco", ""], ["Lu", "Xugang", ""], ["Tsao", "Yu", ""]]}, {"id": "2104.03543", "submitter": "Andargachew Mekonnen Gezmu", "authors": "Andargachew Mekonnen Gezmu, Andreas N\\\"urnberger and Tesfaye Bayu Bati", "title": "Extended Parallel Corpus for Amharic-English Machine Translation", "comments": "Accepted to 2nd AfricanNLP workshop at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper describes the acquisition, preprocessing, segmentation, and\nalignment of an Amharic-English parallel corpus. It will be useful for machine\ntranslation of an under-resourced language, Amharic. The corpus is larger than\npreviously compiled corpora; it is released for research purposes. We trained\nneural machine translation and phrase-based statistical machine translation\nmodels using the corpus. In the automatic evaluation, neural machine\ntranslation models outperform phrase-based statistical machine translation\nmodels.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 06:51:08 GMT"}, {"version": "v2", "created": "Fri, 25 Jun 2021 07:52:02 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Gezmu", "Andargachew Mekonnen", ""], ["N\u00fcrnberger", "Andreas", ""], ["Bati", "Tesfaye Bayu", ""]]}, {"id": "2104.03571", "submitter": "Paolo Liberatore", "authors": "Paolo Liberatore", "title": "On Mixed Iterated Revisions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several forms of iterable belief change exist, differing in the kind of\nchange and its strength: some operators introduce formulae, others remove them;\nsome add formulae unconditionally, others only as additions to the previous\nbeliefs; some only relative to the current situation, others in all possible\ncases. A sequence of changes may involve several of them: for example, the\nfirst step is a revision, the second a contraction and the third a refinement\nof the previous beliefs. The ten operators considered in this article are shown\nto be all reducible to three: lexicographic revision, refinement and severe\nwithdrawal. In turn, these three can be expressed in terms of lexicographic\nrevision at the cost of restructuring the sequence. This restructuring needs\nnot to be done explicitly: an algorithm that works on the original sequence is\nshown. The complexity of mixed sequences of belief change operators is also\nanalyzed. Most of them require only a polynomial number of calls to a\nsatisfiability checker, some are even easier.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 07:34:56 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Liberatore", "Paolo", ""]]}, {"id": "2104.03613", "submitter": "Luca Biggio", "authors": "Luca Biggio, Alexander Wieland, Manuel Arias Chao, Iason Kastanis,\n  Olga Fink", "title": "Uncertainty-aware Remaining Useful Life predictor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.AP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Remaining Useful Life (RUL) estimation is the problem of inferring how long a\ncertain industrial asset can be expected to operate within its defined\nspecifications. Deploying successful RUL prediction methods in real-life\napplications is a prerequisite for the design of intelligent maintenance\nstrategies with the potential of drastically reducing maintenance costs and\nmachine downtimes. In light of their superior performance in a wide range of\nengineering fields, Machine Learning (ML) algorithms are natural candidates to\ntackle the challenges involved in the design of intelligent maintenance\nsystems. In particular, given the potentially catastrophic consequences or\nsubstantial costs associated with maintenance decisions that are either too\nlate or too early, it is desirable that ML algorithms provide uncertainty\nestimates alongside their predictions. However, standard data-driven methods\nused for uncertainty estimation in RUL problems do not scale well to large\ndatasets or are not sufficiently expressive to model the high-dimensional\nmapping from raw sensor data to RUL estimates. In this work, we consider Deep\nGaussian Processes (DGPs) as possible solutions to the aforementioned\nlimitations. We perform a thorough evaluation and comparison of several\nvariants of DGPs applied to RUL predictions. The performance of the algorithms\nis evaluated on the N-CMAPSS (New Commercial Modular Aero-Propulsion System\nSimulation) dataset from NASA for aircraft engines. The results show that the\nproposed methods are able to provide very accurate RUL predictions along with\nsensible uncertainty estimates, providing more reliable solutions for\n(safety-critical) real-life industrial applications.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 08:50:44 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Biggio", "Luca", ""], ["Wieland", "Alexander", ""], ["Chao", "Manuel Arias", ""], ["Kastanis", "Iason", ""], ["Fink", "Olga", ""]]}, {"id": "2104.03616", "submitter": "Linh K\\\"astner", "authors": "Linh K\\\"astner, Teham Buiyan, Xinlin Zhao, Lei Jiao, Zhengcheng Shen\n  and Jens Lambrecht", "title": "Towards Deployment of Deep-Reinforcement-Learning-Based Obstacle\n  Avoidance into Conventional Autonomous Navigation Systems", "comments": "8 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recently, mobile robots have become important tools in various industries,\nespecially in logistics. Deep reinforcement learning emerged as an alternative\nplanning method to replace overly conservative approaches and promises more\nefficient and flexible navigation. However, deep reinforcement learning\napproaches are not suitable for long-range navigation due to their proneness to\nlocal minima and lack of long term memory, which hinders its widespread\nintegration into industrial applications of mobile robotics. In this paper, we\npropose a navigation system incorporating deep-reinforcement-learning-based\nlocal planners into conventional navigation stacks for long-range navigation.\nTherefore, a framework for training and testing the deep reinforcement learning\nalgorithms along with classic approaches is presented. We evaluated our\ndeep-reinforcement-learning-enhanced navigation system against various\nconventional planners and found that our system outperforms them in terms of\nsafety, efficiency and robustness.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 08:56:53 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["K\u00e4stner", "Linh", ""], ["Buiyan", "Teham", ""], ["Zhao", "Xinlin", ""], ["Jiao", "Lei", ""], ["Shen", "Zhengcheng", ""], ["Lambrecht", "Jens", ""]]}, {"id": "2104.03617", "submitter": "Jiangyan Yi", "authors": "Jiangyan Yi, Ye Bai, Jianhua Tao, Zhengkun Tian, Chenglong Wang, Tao\n  Wang, Ruibo Fu", "title": "Half-Truth: A Partially Fake Audio Detection Dataset", "comments": "submitted to Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diverse promising datasets have been designed to hold back the development of\nfake audio detection, such as ASVspoof databases. However, previous datasets\nignore an attacking situation, in which the hacker hides some small fake clips\nin real speech audio. This poses a serious threat since that it is difficult to\ndistinguish the small fake clip from the whole speech utterance. Therefore,\nthis paper develops such a dataset for half-truth audio detection (HAD).\nPartially fake audio in the HAD dataset involves only changing a few words in\nan utterance.The audio of the words is generated with the very latest\nstate-of-the-art speech synthesis technology. We can not only detect fake\nuttrances but also localize manipulated regions in a speech using this dataset.\nSome benchmark results are presented on this dataset. The results show that\npartially fake audio presents much more challenging than fully fake audio for\nfake audio detection.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 08:57:13 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Yi", "Jiangyan", ""], ["Bai", "Ye", ""], ["Tao", "Jianhua", ""], ["Tian", "Zhengkun", ""], ["Wang", "Chenglong", ""], ["Wang", "Tao", ""], ["Fu", "Ruibo", ""]]}, {"id": "2104.03624", "submitter": "Kurt Willis", "authors": "Kurt Willis, Luis Oala", "title": "Post-Hoc Domain Adaptation via Guided Data Homogenization", "comments": "Published as a conference paper at ICLR 2021; 4 pages, plus appendix,\n  5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Addressing shifts in data distributions is an important prerequisite for the\ndeployment of deep learning models to real-world settings. A general approach\nto this problem involves the adjustment of models to a new domain through\ntransfer learning. However, in many cases, this is not applicable in a post-hoc\nmanner to deployed models and further parameter adjustments jeopardize safety\ncertifications that were established beforehand. In such a context, we propose\nto deal with changes in the data distribution via guided data homogenization\nwhich shifts the burden of adaptation from the model to the data. This approach\nmakes use of information about the training data contained implicitly in the\ndeep learning model to learn a domain transfer function. This allows for a\ntargeted deployment of models to unknown scenarios without changing the model\nitself. We demonstrate the potential of data homogenization through experiments\non the CIFAR-10 and MNIST data sets.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 09:18:48 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Willis", "Kurt", ""], ["Oala", "Luis", ""]]}, {"id": "2104.03638", "submitter": "Linh K\\\"astner", "authors": "Zhengcheng Shen, Linh K\\\"astner and Jens Lambrecht", "title": "Spatial Imagination With Semantic Cognition for Mobile Robots", "comments": "7 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The imagination of the surrounding environment based on experience and\nsemantic cognition has great potential to extend the limited observations and\nprovide more information for mapping, collision avoidance, and path planning.\nThis paper provides a training-based algorithm for mobile robots to perform\nspatial imagination based on semantic cognition and evaluates the proposed\nmethod for the mapping task. We utilize a photo-realistic simulation\nenvironment, Habitat, for training and evaluation. The trained model is\ncomposed of Resent-18 as encoder and Unet as the backbone. We demonstrate that\nthe algorithm can perform imagination for unseen parts of the object\nuniversally, by recalling the images and experience and compare our approach\nwith traditional semantic mapping methods. It is found that our approach will\nimprove the efficiency and accuracy of semantic mapping.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 09:44:49 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Shen", "Zhengcheng", ""], ["K\u00e4stner", "Linh", ""], ["Lambrecht", "Jens", ""]]}, {"id": "2104.03663", "submitter": "Linh K\\\"astner", "authors": "Linh K\\\"astner, Teham Buiyan, Xinlin Zhao, Zhengcheng Shen, Cornelius\n  Marx and Jens Lambrecht", "title": "Connecting Deep-Reinforcement-Learning-based Obstacle Avoidance with\n  Conventional Global Planners using Waypoint Generators", "comments": "8 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deep Reinforcement Learning has emerged as an efficient dynamic obstacle\navoidance method in highly dynamic environments. It has the potential to\nreplace overly conservative or inefficient navigation approaches. However, the\nintegration of Deep Reinforcement Learning into existing navigation systems is\nstill an open frontier due to the myopic nature of\nDeep-Reinforcement-Learning-based navigation, which hinders its widespread\nintegration into current navigation systems. In this paper, we propose the\nconcept of an intermediate planner to interconnect novel\nDeep-Reinforcement-Learning-based obstacle avoidance with conventional global\nplanning methods using waypoint generation. Therefore, we integrate different\nwaypoint generators into existing navigation systems and compare the joint\nsystem against traditional ones. We found an increased performance in terms of\nsafety, efficiency and path smoothness especially in highly dynamic\nenvironments.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 10:23:23 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["K\u00e4stner", "Linh", ""], ["Buiyan", "Teham", ""], ["Zhao", "Xinlin", ""], ["Shen", "Zhengcheng", ""], ["Marx", "Cornelius", ""], ["Lambrecht", "Jens", ""]]}, {"id": "2104.03692", "submitter": "Esmaeil Delfaraz Pahlevanloo", "authors": "Gianlorenzo D'Angelo, Esmaeil Delfaraz and Hugo Gilbert", "title": "Computation and Bribery of Voting Power in Delegative Simple Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Weighted voting games is one of the most important classes of cooperative\ngames. Recently, Zhang and Grossi [53] proposed a variant of this class, called\ndelegative simple games, which is well suited to analyse the relative\nimportance of each voter in liquid democracy elections. Moreover, they defined\na power index, called the delagative Banzhaf index to compute the importance of\neach agent (i.e., both voters and delegators) in a delegation graph based on\ntwo key parameters: the total voting weight she has accumulated and the\nstructure of supports she receives from her delegators.\n  We obtain several results related to delegative simple games. We first\npropose a pseudo-polynomial time algorithm to compute the delegative Banzhaf\nand Shapley-Shubik values in delegative simple games. We then investigate a\nbribery problem where the goal is to maximize/minimize the voting power/weight\nof a given voter in a delegation graph by changing at most a fixed number of\ndelegations. We show that the problems of minimizing/maximizing a voter's power\nindex value are strongly NP-hard. Furthermore, we prove that having a better\napproximation guarantee than $1-1/e$ to maximize the voting weight of a voter\nis not possible, unless $P = NP$, then we provide some parameterized complexity\nresults for this problem. Finally, we show that finding a delegation graph with\na given number of gurus that maximizes the minimum power index value an agent\ncan have is a computationally hard problem.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 11:28:50 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["D'Angelo", "Gianlorenzo", ""], ["Delfaraz", "Esmaeil", ""], ["Gilbert", "Hugo", ""]]}, {"id": "2104.03722", "submitter": "Muhammad AbdurRafae", "authors": "Muhammad AbdurRafae", "title": "HindSight: A Graph-Based Vision Model Architecture For Representing\n  Part-Whole Hierarchies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a model architecture for encoding the representations of\npart-whole hierarchies in images in form of a graph. The idea is to divide the\nimage into patches of different levels and then treat all of these patches as\nnodes for a fully connected graph. A dynamic feature extraction module is used\nto extract feature representations from these patches in each graph iteration.\nThis enables us to learn a rich graph representation of the image that\nencompasses the inherent part-whole hierarchical information. Utilizing proper\nself-supervised training techniques, such a model can be trained as a general\npurpose vision encoder model which can then be used for various vision related\ndownstream tasks (e.g., Image Classification, Object Detection, Image\nCaptioning, etc.).\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 12:17:54 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["AbdurRafae", "Muhammad", ""]]}, {"id": "2104.03725", "submitter": "Joan Serr\\`a", "authors": "Joan Serr\\`a, Santiago Pascual, Jordi Pons", "title": "On tuning consistent annealed sampling for denoising score matching", "comments": "3 pages and 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Score-based generative models provide state-of-the-art quality for image and\naudio synthesis. Sampling from these models is performed iteratively, typically\nemploying a discretized series of noise levels and a predefined scheme. In this\nnote, we first overview three common sampling schemes for models trained with\ndenoising score matching. Next, we focus on one of them, consistent annealed\nsampling, and study its hyper-parameter boundaries. We then highlight a\npossible formulation of such hyper-parameter that explicitly considers those\nboundaries and facilitates tuning when using few or a variable number of steps.\nFinally, we highlight some connections of the formulation with other sampling\nschemes.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 12:19:10 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Serr\u00e0", "Joan", ""], ["Pascual", "Santiago", ""], ["Pons", "Jordi", ""]]}, {"id": "2104.03736", "submitter": "Su Lu", "authors": "Su Lu, Han-Jia Ye, Le Gan, De-Chuan Zhan", "title": "Towards Enabling Meta-Learning from Target Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Meta-learning can extract an inductive bias from previous learning experience\nand assist the training processes of new tasks. It is often realized through\noptimizing a meta-model with the evaluation loss of a series of task-specific\nsolvers. Most existing algorithms sample non-overlapping $\\mathit{support}$\nsets and $\\mathit{query}$ sets to train and evaluate the solvers respectively\ndue to simplicity ($\\mathcal{S}/\\mathcal{Q}$ protocol). However, another\nevaluation method that assesses the discrepancy between the solver and a target\nmodel is short of research ($\\mathcal{S}/\\mathcal{T}$ protocol).\n$\\mathcal{S}/\\mathcal{T}$ protocol has unique advantages such as offering more\ninformative supervision, but it is computationally expensive. This paper looks\ninto this special evaluation method and takes a step towards putting it into\npractice. We find that with a small ratio of tasks armed with target models,\nclassic meta-learning algorithms can be improved a lot without consuming many\nresources. Furthermore, we empirically verify the effectiveness of\n$\\mathcal{S}/\\mathcal{T}$ protocol in a typical application of meta-learning,\n$\\mathit{i.e.}$, few-shot learning. In detail, after constructing target models\nby fine-tuning the pre-trained network on those hard tasks, we match the\ntask-specific solvers to target models via knowledge distillation. Experiments\ndemonstrate the superiority of our proposal.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 12:41:33 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 07:29:24 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Lu", "Su", ""], ["Ye", "Han-Jia", ""], ["Gan", "Le", ""], ["Zhan", "De-Chuan", ""]]}, {"id": "2104.03737", "submitter": "Su Lu", "authors": "Su Lu, Han-Jia Ye, De-Chuan Zhan", "title": "Few-Shot Action Recognition with Compromised Metric via Optimal\n  Transport", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although vital to computer vision systems, few-shot action recognition is\nstill not mature despite the wide research of few-shot image classification.\nPopular few-shot learning algorithms extract a transferable embedding from seen\nclasses and reuse it on unseen classes by constructing a metric-based\nclassifier. One main obstacle to applying these algorithms in action\nrecognition is the complex structure of videos. Some existing solutions sample\nframes from a video and aggregate their embeddings to form a video-level\nrepresentation, neglecting important temporal relations. Others perform an\nexplicit sequence matching between two videos and define their distance as\nmatching cost, imposing too strong restrictions on sequence ordering. In this\npaper, we propose Compromised Metric via Optimal Transport (CMOT) to combine\nthe advantages of these two solutions. CMOT simultaneously considers semantic\nand temporal information in videos under Optimal Transport framework, and is\ndiscriminative for both content-sensitive and ordering-sensitive tasks. In\ndetail, given two videos, we sample segments from them and cast the calculation\nof their distance as an optimal transport problem between two segment\nsequences. To preserve the inherent temporal ordering information, we\nadditionally amend the ground cost matrix by penalizing it with the positional\ndistance between a pair of segments. Empirical results on benchmark datasets\ndemonstrate the superiority of CMOT.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 12:42:05 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Lu", "Su", ""], ["Ye", "Han-Jia", ""], ["Zhan", "De-Chuan", ""]]}, {"id": "2104.03739", "submitter": "Mostafa Mehdipour Ghazi", "authors": "Mostafa Mehdipour Ghazi, Lauge S{\\o}rensen, S\\'ebastien Ourselin, Mads\n  Nielsen", "title": "CARRNN: A Continuous Autoregressive Recurrent Neural Network for Deep\n  Representation Learning from Sporadic Temporal Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning temporal patterns from multivariate longitudinal data is challenging\nespecially in cases when data is sporadic, as often seen in, e.g., healthcare\napplications where the data can suffer from irregularity and asynchronicity as\nthe time between consecutive data points can vary across features and samples,\nhindering the application of existing deep learning models that are constructed\nfor complete, evenly spaced data with fixed sequence lengths. In this paper, a\nnovel deep learning-based model is developed for modeling multiple temporal\nfeatures in sporadic data using an integrated deep learning architecture based\non a recurrent neural network (RNN) unit and a continuous-time autoregressive\n(CAR) model. The proposed model, called CARRNN, uses a generalized\ndiscrete-time autoregressive model that is trainable end-to-end using neural\nnetworks modulated by time lags to describe the changes caused by the\nirregularity and asynchronicity. It is applied to multivariate time-series\nregression tasks using data provided for Alzheimer's disease progression\nmodeling and intensive care unit (ICU) mortality rate prediction, where the\nproposed model based on a gated recurrent unit (GRU) achieves the lowest\nprediction errors among the proposed RNN-based models and state-of-the-art\nmethods using GRUs and long short-term memory (LSTM) networks in their\narchitecture.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 12:43:44 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Ghazi", "Mostafa Mehdipour", ""], ["S\u00f8rensen", "Lauge", ""], ["Ourselin", "S\u00e9bastien", ""], ["Nielsen", "Mads", ""]]}, {"id": "2104.03741", "submitter": "The Anh Han", "authors": "The Anh Han, Tom Lenaerts, Francisco C. Santos, and Luis Moniz Pereira", "title": "Voluntary safety commitments provide an escape from over-regulation in\n  AI development", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.MA nlin.AO nlin.CD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the introduction of Artificial Intelligence (AI) and related\ntechnologies in our daily lives, fear and anxiety about their misuse as well as\nthe hidden biases in their creation have led to a demand for regulation to\naddress such issues. Yet blindly regulating an innovation process that is not\nwell understood, may stifle this process and reduce benefits that society may\ngain from the generated technology, even under the best intentions. In this\npaper, starting from a baseline model that captures the fundamental dynamics of\na race for domain supremacy using AI technology, we demonstrate how socially\nunwanted outcomes may be produced when sanctioning is applied unconditionally\nto risk-taking, i.e. potentially unsafe, behaviours. As an alternative to\nresolve the detrimental effect of over-regulation, we propose a voluntary\ncommitment approach wherein technologists have the freedom of choice between\nindependently pursuing their course of actions or establishing binding\nagreements to act safely, with sanctioning of those that do not abide to what\nthey pledged. Overall, this work reveals for the first time how voluntary\ncommitments, with sanctions either by peers or an institution, leads to\nsocially beneficial outcomes in all scenarios envisageable in a short-term race\ntowards domain supremacy through AI technology. These results are directly\nrelevant for the design of governance and regulatory policies that aim to\nensure an ethical and responsible AI technology development process.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 12:54:56 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Han", "The Anh", ""], ["Lenaerts", "Tom", ""], ["Santos", "Francisco C.", ""], ["Pereira", "Luis Moniz", ""]]}, {"id": "2104.03760", "submitter": "Pierre Tassel", "authors": "Pierre Tassel, Martin Gebser, Konstantin Schekotihin", "title": "A Reinforcement Learning Environment For Job-Shop Scheduling", "comments": "7 pages, 4 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scheduling is a fundamental task occurring in various automated systems\napplications, e.g., optimal schedules for machines on a job shop allow for a\nreduction of production costs and waste. Nevertheless, finding such schedules\nis often intractable and cannot be achieved by Combinatorial Optimization\nProblem (COP) methods within a given time limit. Recent advances of Deep\nReinforcement Learning (DRL) in learning complex behavior enable new COP\napplication possibilities. This paper presents an efficient DRL environment for\nJob-Shop Scheduling -- an important problem in the field. Furthermore, we\ndesign a meaningful and compact state representation as well as a novel, simple\ndense reward function, closely related to the sparse make-span minimization\ncriteria used by COP methods. We demonstrate that our approach significantly\noutperforms existing DRL methods on classic benchmark instances, coming close\nto state-of-the-art COP approaches.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 13:26:30 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Tassel", "Pierre", ""], ["Gebser", "Martin", ""], ["Schekotihin", "Konstantin", ""]]}, {"id": "2104.03778", "submitter": "Chuong Huynh", "authors": "Chuong Huynh, Anh Tran, Khoa Luu, Minh Hoai", "title": "Progressive Semantic Segmentation", "comments": "Accepted to CVPR'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The objective of this work is to segment high-resolution images without\noverloading GPU memory usage or losing the fine details in the output\nsegmentation map. The memory constraint means that we must either downsample\nthe big image or divide the image into local patches for separate processing.\nHowever, the former approach would lose the fine details, while the latter can\nbe ambiguous due to the lack of a global picture. In this work, we present\nMagNet, a multi-scale framework that resolves local ambiguity by looking at the\nimage at multiple magnification levels. MagNet has multiple processing stages,\nwhere each stage corresponds to a magnification level, and the output of one\nstage is fed into the next stage for coarse-to-fine information propagation.\nEach stage analyzes the image at a higher resolution than the previous stage,\nrecovering the previously lost details due to the lossy downsampling step, and\nthe segmentation output is progressively refined through the processing stages.\nExperiments on three high-resolution datasets of urban views, aerial scenes,\nand medical images show that MagNet consistently outperforms the\nstate-of-the-art methods by a significant margin.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 13:59:27 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Huynh", "Chuong", ""], ["Tran", "Anh", ""], ["Luu", "Khoa", ""], ["Hoai", "Minh", ""]]}, {"id": "2104.03807", "submitter": "Zahra Gharaee", "authors": "Zahra Gharaee and Karl Holmquist and Linbo He and Michael Felsberg", "title": "A Bayesian Approach to Reinforcement Learning of Vision-Based Vehicular\n  Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we present a state-of-the-art reinforcement learning method\nfor autonomous driving. Our approach employs temporal difference learning in a\nBayesian framework to learn vehicle control signals from sensor data. The agent\nhas access to images from a forward facing camera, which are preprocessed to\ngenerate semantic segmentation maps. We trained our system using both ground\ntruth and estimated semantic segmentation input. Based on our observations from\na large set of experiments, we conclude that training the system on ground\ntruth input data leads to better performance than training the system on\nestimated input even if estimated input is used for evaluation. The system is\ntrained and evaluated in a realistic simulated urban environment using the\nCARLA simulator. The simulator also contains a benchmark that allows for\ncomparing to other systems and methods. The required training time of the\nsystem is shown to be lower and the performance on the benchmark superior to\ncompeting approaches.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 14:34:57 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Gharaee", "Zahra", ""], ["Holmquist", "Karl", ""], ["He", "Linbo", ""], ["Felsberg", "Michael", ""]]}, {"id": "2104.03854", "submitter": "Viktor Varkarakis", "authors": "Viktor Varkarakis, Wang Yao, Peter Corcoran", "title": "Towards End-to-End Neural Face Authentication in the Wild -- Quantifying\n  and Compensating for Directional Lighting Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The recent availability of low-power neural accelerator hardware, combined\nwith improvements in end-to-end neural facial recognition algorithms provides,\nenabling technology for on-device facial authentication. The present research\nwork examines the effects of directional lighting on a State-of-Art(SoA) neural\nface recognizer. A synthetic re-lighting technique is used to augment data\nsamples due to the lack of public data-sets with sufficient directional\nlighting variations. Top lighting and its variants (top-left, top-right) are\nfound to have minimal effect on accuracy, while bottom-left or bottom-right\ndirectional lighting has the most pronounced effects. Following the fine-tuning\nof network weights, the face recognition model is shown to achieve close to the\noriginal Receiver Operating Characteristic curve (ROC)performance across all\nlighting conditions and demonstrates an ability to generalize beyond the\nlighting augmentations used in the fine-tuning data-set. This work shows that\nan SoA neural face recognition model can be tuned to compensate for directional\nlighting effects, removing the need for a pre-processing step before applying\nfacial recognition.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 15:58:09 GMT"}], "update_date": "2021-04-10", "authors_parsed": [["Varkarakis", "Viktor", ""], ["Yao", "Wang", ""], ["Corcoran", "Peter", ""]]}, {"id": "2104.03888", "submitter": "Manuel Carranza-Garc\\'ia", "authors": "Manuel Carranza-Garc\\'ia, Pedro Lara-Ben\\'itez, Jorge\n  Garc\\'ia-Guti\\'errez, Jos\\'e C. Riquelme", "title": "Enhancing Object Detection for Autonomous Driving by Optimizing Anchor\n  Generation and Addressing Class Imbalance", "comments": null, "journal-ref": "Neurocomputing, 2021", "doi": "10.1016/j.neucom.2021.04.001", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Object detection has been one of the most active topics in computer vision\nfor the past years. Recent works have mainly focused on pushing the\nstate-of-the-art in the general-purpose COCO benchmark. However, the use of\nsuch detection frameworks in specific applications such as autonomous driving\nis yet an area to be addressed. This study presents an enhanced 2D object\ndetector based on Faster R-CNN that is better suited for the context of\nautonomous vehicles. Two main aspects are improved: the anchor generation\nprocedure and the performance drop in minority classes. The default uniform\nanchor configuration is not suitable in this scenario due to the perspective\nprojection of the vehicle cameras. Therefore, we propose a perspective-aware\nmethodology that divides the image into key regions via clustering and uses\nevolutionary algorithms to optimize the base anchors for each of them.\nFurthermore, we add a module that enhances the precision of the second-stage\nheader network by including the spatial information of the candidate regions\nproposed in the first stage. We also explore different re-weighting strategies\nto address the foreground-foreground class imbalance, showing that the use of a\nreduced version of focal loss can significantly improve the detection of\ndifficult and underrepresented objects in two-stage detectors. Finally, we\ndesign an ensemble model to combine the strengths of the different learning\nstrategies. Our proposal is evaluated with the Waymo Open Dataset, which is the\nmost extensive and diverse up to date. The results demonstrate an average\naccuracy improvement of 6.13% mAP when using the best single model, and of\n9.69% mAP with the ensemble. The proposed modifications over the Faster R-CNN\ndo not increase computational cost and can easily be extended to optimize other\nanchor-based detection frameworks.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 16:58:31 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Carranza-Garc\u00eda", "Manuel", ""], ["Lara-Ben\u00edtez", "Pedro", ""], ["Garc\u00eda-Guti\u00e9rrez", "Jorge", ""], ["Riquelme", "Jos\u00e9 C.", ""]]}, {"id": "2104.03893", "submitter": "Mehrshad Zandigohar", "authors": "Mehrshad Zandigohar, Mo Han, Mohammadreza Sharif, Sezen Yagmur Gunay,\n  Mariusz P. Furmanek, Mathew Yarossi, Paolo Bonato, Cagdas Onal, Taskin Padir,\n  Deniz Erdogmus, Gunar Schirner", "title": "Multimodal Fusion of EMG and Vision for Human Grasp Intent Inference in\n  Prosthetic Hand Control", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.HC eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For lower arm amputees, robotic prosthetic hands offer the promise to regain\nthe capability to perform fine object manipulation in activities of daily\nliving. Current control methods based on physiological signals such as EEG and\nEMG are prone to poor inference outcomes due to motion artifacts, variability\nof skin electrode junction impedance over time, muscle fatigue, and other\nfactors. Visual evidence is also susceptible to its own artifacts, most often\ndue to object occlusion, lighting changes, variable shapes of objects depending\non view-angle, among other factors. Multimodal evidence fusion using\nphysiological and vision sensor measurements is a natural approach due to the\ncomplementary strengths of these modalities.\n  In this paper, we present a Bayesian evidence fusion framework for grasp\nintent inference using eye-view video, gaze, and EMG from the forearm processed\nby neural network models. We analyze individual and fused performance as a\nfunction of time as the hand approaches the object to grasp it. For this\npurpose, we have also developed novel data processing and augmentation\ntechniques to train neural network components. Our experimental data analyses\ndemonstrate that EMG and visual evidence show complementary strengths, and as a\nconsequence, fusion of multimodal evidence can outperform each individual\nevidence modality at any given time. Specifically, results indicate that, on\naverage, fusion improves the instantaneous upcoming grasp type classification\naccuracy while in the reaching phase by 13.66% and 14.8%, relative to EMG and\nvisual evidence individually. An overall fusion accuracy of 95.3% among 13\nlabels (compared to a chance level of 7.7%) is achieved, and more detailed\nanalysis indicate that the correct grasp is inferred sufficiently early and\nwith high confidence compared to the top contender, in order to allow\nsuccessful robot actuation to close the loop.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 17:01:19 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Zandigohar", "Mehrshad", ""], ["Han", "Mo", ""], ["Sharif", "Mohammadreza", ""], ["Gunay", "Sezen Yagmur", ""], ["Furmanek", "Mariusz P.", ""], ["Yarossi", "Mathew", ""], ["Bonato", "Paolo", ""], ["Onal", "Cagdas", ""], ["Padir", "Taskin", ""], ["Erdogmus", "Deniz", ""], ["Schirner", "Gunar", ""]]}, {"id": "2104.03899", "submitter": "Haoqi Li", "authors": "Haoqi Li, Brian Baucom, Shrikanth Narayanan, Panayiotis Georgiou", "title": "Unsupervised Speech Representation Learning for Behavior Modeling using\n  Triplet Enhanced Contextualized Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.AI cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speech encodes a wealth of information related to human behavior and has been\nused in a variety of automated behavior recognition tasks. However, extracting\nbehavioral information from speech remains challenging including due to\ninadequate training data resources stemming from the often low occurrence\nfrequencies of specific behavioral patterns. Moreover, supervised behavioral\nmodeling typically relies on domain-specific construct definitions and\ncorresponding manually-annotated data, rendering generalizing across domains\nchallenging. In this paper, we exploit the stationary properties of human\nbehavior within an interaction and present a representation learning method to\ncapture behavioral information from speech in an unsupervised way. We\nhypothesize that nearby segments of speech share the same behavioral context\nand hence map onto similar underlying behavioral representations. We present an\nencoder-decoder based Deep Contextualized Network (DCN) as well as a\nTriplet-Enhanced DCN (TE-DCN) framework to capture the behavioral context and\nderive a manifold representation, where speech frames with similar behaviors\nare closer while frames of different behaviors maintain larger distances. The\nmodels are trained on movie audio data and validated on diverse domains\nincluding on a couples therapy corpus and other publicly collected data (e.g.,\nstand-up comedy). With encouraging results, our proposed framework shows the\nfeasibility of unsupervised learning within cross-domain behavioral modeling.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 22:44:23 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Li", "Haoqi", ""], ["Baucom", "Brian", ""], ["Narayanan", "Shrikanth", ""], ["Georgiou", "Panayiotis", ""]]}, {"id": "2104.03902", "submitter": "William J. Cunningham", "authors": "Stephon Alexander, William J. Cunningham, Jaron Lanier, Lee Smolin,\n  Stefan Stanojevic, Michael W. Toomey, Dave Wecker", "title": "The Autodidactic Universe", "comments": "79 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "hep-th cs.AI cs.LG gr-qc physics.hist-ph quant-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an approach to cosmology in which the Universe learns its own\nphysical laws. It does so by exploring a landscape of possible laws, which we\nexpress as a certain class of matrix models. We discover maps that put each of\nthese matrix models in correspondence with both a gauge/gravity theory and a\nmathematical model of a learning machine, such as a deep recurrent, cyclic\nneural network. This establishes a correspondence between each solution of the\nphysical theory and a run of a neural network. This correspondence is not an\nequivalence, partly because gauge theories emerge from $N \\rightarrow \\infty $\nlimits of the matrix models, whereas the same limits of the neural networks\nused here are not well-defined. We discuss in detail what it means to say that\nlearning takes place in autodidactic systems, where there is no supervision. We\npropose that if the neural network model can be said to learn without\nsupervision, the same can be said for the corresponding physical theory. We\nconsider other protocols for autodidactic physical systems, such as\noptimization of graph variety, subset-replication using self-attention and\nlook-ahead, geometrogenesis guided by reinforcement learning, structural\nlearning using renormalization group techniques, and extensions. These\nprotocols together provide a number of directions in which to explore the\norigin of physical laws based on putting machine learning architectures in\ncorrespondence with physical theories.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 02:25:02 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Alexander", "Stephon", ""], ["Cunningham", "William J.", ""], ["Lanier", "Jaron", ""], ["Smolin", "Lee", ""], ["Stanojevic", "Stefan", ""], ["Toomey", "Michael W.", ""], ["Wecker", "Dave", ""]]}, {"id": "2104.03909", "submitter": "Tina Eliassi-Rad", "authors": "David Liu, Zohair Shafi, William Fleisher, Tina Eliassi-Rad, Scott\n  Alfeld", "title": "RAWLSNET: Altering Bayesian Networks to Encode Rawlsian Fair Equality of\n  Opportunity", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present RAWLSNET, a system for altering Bayesian Network (BN) models to\nsatisfy the Rawlsian principle of fair equality of opportunity (FEO).\nRAWLSNET's BN models generate aspirational data distributions: data generated\nto reflect an ideally fair, FEO-satisfying society. FEO states that everyone\nwith the same talent and willingness to use it should have the same chance of\nachieving advantageous social positions (e.g., employment), regardless of their\nbackground circumstances (e.g., socioeconomic status). Satisfying FEO requires\nalterations to social structures such as school assignments. Our paper\ndescribes RAWLSNET, a method which takes as input a BN representation of an FEO\napplication and alters the BN's parameters so as to satisfy FEO when possible,\nand minimize deviation from FEO otherwise. We also offer guidance for applying\nRAWLSNET, including on recognizing proper applications of FEO. We demonstrate\nthe use of our system with publicly available data sets. RAWLSNET's altered BNs\noffer the novel capability of generating aspirational data for FEO-relevant\ntasks. Aspirational data are free from the biases of real-world data, and thus\nare useful for recognizing and detecting sources of unfairness in machine\nlearning algorithms besides biased data.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 03:06:47 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Liu", "David", ""], ["Shafi", "Zohair", ""], ["Fleisher", "William", ""], ["Eliassi-Rad", "Tina", ""], ["Alfeld", "Scott", ""]]}, {"id": "2104.03920", "submitter": "Seamus Brady", "authors": "Simon James (Seamus) Brady", "title": "Finding Experts in Social Media Data using a Hybrid Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Several approaches to the problem of expert finding have emerged in computer\nscience research. In this work, three of these approaches - content analysis,\nsocial graph analysis and the use of Semantic Web technologies are examined. An\nintegrated set of system requirements is then developed that uses all three\napproaches in one hybrid approach.\n  To show the practicality of this hybrid approach, a usable prototype expert\nfinding system called ExpertQuest is developed using a modern functional\nprogramming language (Clojure) to query social media data and Linked Data. This\nsystem is evaluated and discussed. Finally, a discussion and conclusions are\npresented which describe the benefits and shortcomings of the hybrid approach\nand the technologies used in this work.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 09:08:41 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["James", "Simon", "", "Seamus"], ["Brady", "", ""]]}, {"id": "2104.03936", "submitter": "Achkan Salehi", "authors": "Achkan Salehi, Alexandre Coninx, Stephane Doncieux", "title": "BR-NS: an Archive-less Approach to Novelty Search", "comments": "Author version of the paper accepted at GECCO 21", "journal-ref": null, "doi": "10.1145/3449639.3459303", "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As open-ended learning based on divergent search algorithms such as Novelty\nSearch (NS) draws more and more attention from the research community, it is\nnatural to expect that its application to increasingly complex real-world\nproblems will require the exploration to operate in higher dimensional Behavior\nSpaces which will not necessarily be Euclidean. Novelty Search traditionally\nrelies on k-nearest neighbours search and an archive of previously visited\nbehavior descriptors which are assumed to live in a Euclidean space. This is\nproblematic because of a number of issues. On one hand, Euclidean distance and\nNearest-neighbour search are known to behave differently and become less\nmeaningful in high dimensional spaces. On the other hand, the archive has to be\nbounded since, memory considerations aside, the computational complexity of\nfinding nearest neighbours in that archive grows linearithmically with its\nsize. A sub-optimal bound can result in \"cycling\" in the behavior space, which\ninhibits the progress of the exploration. Furthermore, the performance of NS\ndepends on a number of algorithmic choices and hyperparameters, such as the\nstrategies to add or remove elements to the archive and the number of\nneighbours to use in k-nn search. In this paper, we discuss an alternative\napproach to novelty estimation, dubbed Behavior Recognition based Novelty\nSearch (BR-NS), which does not require an archive, makes no assumption on the\nmetrics that can be defined in the behavior space and does not rely on nearest\nneighbours search. We conduct experiments to gain insight into its feasibility\nand dynamics as well as potential advantages over archive-based NS in terms of\ntime complexity.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 17:31:34 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Salehi", "Achkan", ""], ["Coninx", "Alexandre", ""], ["Doncieux", "Stephane", ""]]}, {"id": "2104.03946", "submitter": "David Lindner", "authors": "David Lindner, Rohin Shah, Pieter Abbeel, Anca Dragan", "title": "Learning What To Do by Simulating the Past", "comments": "Presented at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since reward functions are hard to specify, recent work has focused on\nlearning policies from human feedback. However, such approaches are impeded by\nthe expense of acquiring such feedback. Recent work proposed that agents have\naccess to a source of information that is effectively free: in any environment\nthat humans have acted in, the state will already be optimized for human\npreferences, and thus an agent can extract information about what humans want\nfrom the state. Such learning is possible in principle, but requires simulating\nall possible past trajectories that could have led to the observed state. This\nis feasible in gridworlds, but how do we scale it to complex tasks? In this\nwork, we show that by combining a learned feature encoder with learned inverse\nmodels, we can enable agents to simulate human actions backwards in time to\ninfer what they must have done. The resulting algorithm is able to reproduce a\nspecific skill in MuJoCo environments given a single state sampled from the\noptimal policy for that skill.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 17:43:29 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 10:51:40 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Lindner", "David", ""], ["Shah", "Rohin", ""], ["Abbeel", "Pieter", ""], ["Dragan", "Anca", ""]]}, {"id": "2104.03956", "submitter": "Sean Segal", "authors": "Sean Segal, Nishanth Kumar, Sergio Casas, Wenyuan Zeng, Mengye Ren,\n  Jingkang Wang, Raquel Urtasun", "title": "Just Label What You Need: Fine-Grained Active Selection for Perception\n  and Prediction through Partially Labeled Scenes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-driving vehicles must perceive and predict the future positions of\nnearby actors in order to avoid collisions and drive safely. A learned deep\nlearning module is often responsible for this task, requiring large-scale,\nhigh-quality training datasets. As data collection is often significantly\ncheaper than labeling in this domain, the decision of which subset of examples\nto label can have a profound impact on model performance. Active learning\ntechniques, which leverage the state of the current model to iteratively select\nexamples for labeling, offer a promising solution to this problem. However,\ndespite the appeal of this approach, there has been little scientific analysis\nof active learning approaches for the perception and prediction (P&P) problem.\nIn this work, we study active learning techniques for P&P and find that the\ntraditional active learning formulation is ill-suited for the P&P setting. We\nthus introduce generalizations that ensure that our approach is both cost-aware\nand allows for fine-grained selection of examples through partially labeled\nscenes. Our experiments on a real-world, large-scale self-driving dataset\nsuggest that fine-grained selection can improve the performance across\nperception, prediction, and downstream planning tasks.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 17:57:41 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Segal", "Sean", ""], ["Kumar", "Nishanth", ""], ["Casas", "Sergio", ""], ["Zeng", "Wenyuan", ""], ["Ren", "Mengye", ""], ["Wang", "Jingkang", ""], ["Urtasun", "Raquel", ""]]}, {"id": "2104.03958", "submitter": "Piyawat Lertvittayakumjorn", "authors": "Piyawat Lertvittayakumjorn, Leshem Choshen, Eyal Shnarch, Francesca\n  Toni", "title": "GrASP: A Library for Extracting and Exploring Human-Interpretable\n  Textual Patterns", "comments": "4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data exploration is an important step of every data science and machine\nlearning project, including those involving textual data. We provide a Python\nlibrary for GrASP, an existing algorithm for drawing patterns from textual\ndata. The library is equipped with a web-based interface empowering human users\nto conveniently explore the data and the extracted patterns. We also\ndemonstrate the use of the library in two settings (spam detection and argument\nmining) and discuss future deployments of the library, e.g., beyond textual\ndata exploration.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 17:58:03 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Lertvittayakumjorn", "Piyawat", ""], ["Choshen", "Leshem", ""], ["Shnarch", "Eyal", ""], ["Toni", "Francesca", ""]]}, {"id": "2104.03986", "submitter": "Arjit Jain", "authors": "Arjit Jain, Sunita Sarawagi, Prithviraj Sen", "title": "Deep Indexed Active Learning for Matching Heterogeneous Entity\n  Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given two large lists of records, the task in entity resolution (ER) is to\nfind the pairs from the Cartesian product of the lists that correspond to the\nsame real world entity. Typically, passive learning methods on tasks like ER\nrequire large amounts of labeled data to yield useful models. Active Learning\nis a promising approach for ER in low resource settings. However, the search\nspace, to find informative samples for the user to label, grows quadratically\nfor instance-pair tasks making active learning hard to scale. Previous works,\nin this setting, rely on hand-crafted predicates, pre-trained language model\nembeddings, or rule learning to prune away unlikely pairs from the Cartesian\nproduct. This blocking step can miss out on important regions in the product\nspace leading to low recall. We propose DIAL, a scalable active learning\napproach that jointly learns embeddings to maximize recall for blocking and\naccuracy for matching blocked pairs. DIAL uses an Index-By-Committee framework,\nwhere each committee member learns representations based on powerful\ntransformer models. We highlight surprising differences between the matcher and\nthe blocker in the creation of the training data and the objective used to\ntrain their parameters. Experiments on five benchmark datasets and a\nmultilingual record matching dataset show the effectiveness of our approach in\nterms of precision, recall and running time. Code is available at\nhttps://github.com/ArjitJ/DIAL\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 18:00:19 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Jain", "Arjit", ""], ["Sarawagi", "Sunita", ""], ["Sen", "Prithviraj", ""]]}, {"id": "2104.04000", "submitter": "Callie Hao", "authors": "Cong Hao, Deming Chen", "title": "Software/Hardware Co-design for Multi-modal Multi-task Learning in\n  Autonomous Systems", "comments": "Invited paper at IEEE AICAS 2021, 5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimizing the quality of result (QoR) and the quality of service (QoS) of\nAI-empowered autonomous systems simultaneously is very challenging. First,\nthere are multiple input sources, e.g., multi-modal data from different\nsensors, requiring diverse data preprocessing, sensor fusion, and feature\naggregation. Second, there are multiple tasks that require various AI models to\nrun simultaneously, e.g., perception, localization, and control. Third, the\ncomputing and control system is heterogeneous, composed of hardware components\nwith varied features, such as embedded CPUs, GPUs, FPGAs, and dedicated\naccelerators. Therefore, autonomous systems essentially require multi-modal\nmulti-task (MMMT) learning which must be aware of hardware performance and\nimplementation strategies. While MMMT learning has been attracting intensive\nresearch interests, its applications in autonomous systems are still\nunderexplored. In this paper, we first discuss the opportunities of applying\nMMMT techniques in autonomous systems and then discuss the unique challenges\nthat must be solved. In addition, we discuss the necessity and opportunities of\nMMMT model and hardware co-design, which is critical for autonomous systems\nespecially with power/resource-limited or heterogeneous platforms. We formulate\nthe MMMT model and heterogeneous hardware implementation co-design as a\ndifferentiable optimization problem, with the objective of improving the\nsolution quality and reducing the overall power consumption and critical path\nlatency. We advocate for further explorations of MMMT in autonomous systems and\nsoftware/hardware co-design solutions.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 18:29:30 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Hao", "Cong", ""], ["Chen", "Deming", ""]]}, {"id": "2104.04008", "submitter": "Mark Keane", "authors": "Mohammed Temraz and Eoin Kenny and Elodie Ruelle and Laurence Shalloo\n  and Barry Smyth and Mark T Keane", "title": "Handling Climate Change Using Counterfactuals: Using Counterfactuals in\n  Data Augmentation to Predict Crop Growth in an Uncertain Climate Future", "comments": "15 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Climate change poses a major challenge to humanity, especially in its impact\non agriculture, a challenge that a responsible AI should meet. In this paper,\nwe examine a CBR system (PBI-CBR) designed to aid sustainable dairy farming by\nsupporting grassland management, through accurate crop growth prediction. As\nclimate changes, PBI-CBRs historical cases become less useful in predicting\nfuture grass growth. Hence, we extend PBI-CBR using data augmentation, to\nspecifically handle disruptive climate events, using a counterfactual method\n(from XAI). Study 1 shows that historical, extreme climate-events (climate\noutlier cases) tend to be used by PBI-CBR to predict grass growth during\nclimate disrupted periods. Study 2 shows that synthetic outliers, generated as\ncounterfactuals on a outlier-boundary, improve the predictive accuracy of\nPBICBR, during the drought of 2018. This study also shows that an\ninstance-based counterfactual method does better than a benchmark,\nconstraint-guided method.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 18:54:21 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Temraz", "Mohammed", ""], ["Kenny", "Eoin", ""], ["Ruelle", "Elodie", ""], ["Shalloo", "Laurence", ""], ["Smyth", "Barry", ""], ["Keane", "Mark T", ""]]}, {"id": "2104.04013", "submitter": "Mohamed Ibrahim", "authors": "Mohamed R. Ibrahim, James Haworth, Nicola Christie", "title": "Re-designing cities with conditional adversarial networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper introduces a conditional generative adversarial network to\nredesign a street-level image of urban scenes by generating 1) an urban\nintervention policy, 2) an attention map that localises where intervention is\nneeded, 3) a high-resolution street-level image (1024 X 1024 or 1536 X1536)\nafter implementing the intervention. We also introduce a new dataset that\ncomprises aligned street-level images of before and after urban interventions\nfrom real-life scenarios that make this research possible. The introduced\nmethod has been trained on different ranges of urban interventions applied to\nrealistic images. The trained model shows strong performance in re-modelling\ncities, outperforming existing methods that apply image-to-image translation in\nother domains that is computed in a single GPU. This research opens the door\nfor machine intelligence to play a role in re-thinking and re-designing the\ndifferent attributes of cities based on adversarial learning, going beyond the\nmainstream of facial landmarks manipulation or image synthesis from semantic\nsegmentation.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 19:03:34 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 09:43:26 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Ibrahim", "Mohamed R.", ""], ["Haworth", "James", ""], ["Christie", "Nicola", ""]]}, {"id": "2104.04039", "submitter": "Zhiyu Lin", "authors": "Zhiyu Lin, Mark Riedl", "title": "Plug-and-Blend: A Framework for Controllable Story Generation with\n  Blended Control Codes", "comments": "8 pages, To appear at AIIDE 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large pre-trained neural language models (LM) have very powerful text\ngeneration capabilities. However, in practice, they are hard to control for\ncreative purposes. We describe a Plug-and-Play controllable language generation\nframework, Plug-and-Blend, that allows a human user to input multiple control\ncodes (topics). In the context of automated story generation, this allows a\nhuman user loose or fine-grained control of the topics and transitions between\nthem that will appear in the generated story, and can even allow for\noverlapping, blended topics. Automated evaluations show our framework, working\nwith different generative LMs, controls the generation towards given\ncontinuous-weighted control codes while keeping the generated sentences fluent,\ndemonstrating strong blending capability. A human participant evaluation shows\nthat the generated stories are observably transitioning between two topics.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 03:15:14 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 14:33:47 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Lin", "Zhiyu", ""], ["Riedl", "Mark", ""]]}, {"id": "2104.04055", "submitter": "Gaurav Bharaj", "authors": "David Ferman, Gaurav Bharaj", "title": "Generative Landmarks", "comments": "2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general purpose approach to detect landmarks with improved\ntemporal consistency, and personalization. Most sparse landmark detection\nmethods rely on laborious, manually labelled landmarks, where inconsistency in\nannotations over a temporal volume leads to sub-optimal landmark learning.\nFurther, high-quality landmarks with personalization is often hard to achieve.\nWe pose landmark detection as an image translation problem. We capture two sets\nof unpaired marked (with paint) and unmarked videos. We then use a generative\nadversarial network and cyclic consistency to predict deformations of landmark\ntemplates that simulate markers on unmarked images until these images are\nindistinguishable from ground-truth marked images. Our novel method does not\nrely on manually labelled priors, is temporally consistent, and image class\nagnostic -- face, and hand landmarks detection examples are shown.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 20:59:21 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Ferman", "David", ""], ["Bharaj", "Gaurav", ""]]}, {"id": "2104.04067", "submitter": "Nicholas Dacre PhD", "authors": "Nicholas Dacre, Fredrik Kockum, PK Senyo", "title": "Transient Information Adaptation of Artificial Intelligence: Towards\n  Sustainable Data Processes in Complex Projects", "comments": "British Academy of Management", "journal-ref": null, "doi": "10.2139/ssrn.3813559", "report-no": null, "categories": "cs.CY cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large scale projects increasingly operate in complicated settings whilst\ndrawing on an array of complex data-points, which require precise analysis for\naccurate control and interventions to mitigate possible project failure.\nCoupled with a growing tendency to rely on new information systems and\nprocesses in change projects, 90% of megaprojects globally fail to achieve\ntheir planned objectives. Renewed interest in the concept of Artificial\nIntelligence (AI) against a backdrop of disruptive technological innovations,\nseeks to enhance project managers cognitive capacity through the project\nlifecycle and enhance project excellence. However, despite growing interest\nthere remains limited empirical insights on project managers ability to\nleverage AI for cognitive load enhancement in complex settings. As such this\nresearch adopts an exploratory sequential linear mixed methods approach to\naddress unresolved empirical issues on transient adaptations of AI in complex\nprojects, and the impact on cognitive load enhancement. Initial thematic\nfindings from semi-structured interviews with domain experts, suggest that in\norder to leverage AI technologies and processes for sustainable cognitive load\nenhancement with complex data over time, project managers require improved\nknowledge and access to relevant technologies that mediate data processes in\ncomplex projects, but equally reflect application across different project\nphases. These initial findings support further hypothesis testing through a\nlarger quantitative study incorporating structural equation modelling to\nexamine the relationship between artificial intelligence and project managers\ncognitive load with project data in complex contexts.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 22:28:52 GMT"}, {"version": "v2", "created": "Sun, 18 Apr 2021 18:07:20 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Dacre", "Nicholas", ""], ["Kockum", "Fredrik", ""], ["Senyo", "PK", ""]]}, {"id": "2104.04072", "submitter": "Soaad Hossain Mr", "authors": "Soaad Hossain, Syed Ishtiaque Ahmed", "title": "Towards a New Participatory Approach for Designing Artificial\n  Intelligence and Data-Driven Technologies", "comments": "5 pages, 2 figures, accepted to Artificially Intelligent Technology\n  for the Margins workshop at Conference on Human Factors in Computing Systems\n  (CHI 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With there being many technical and ethical issues with artificial\nintelligence (AI) that involve marginalized communities, there is a growing\ninterest for design methods used with marginalized people that may be\ntransferable to the design of AI technologies. Participatory design (PD) is a\ndesign method that is often used with marginalized communities for the design\nof social development, policy, IT and other matters and solutions. However,\nthere are issues with the current PD, raising concerns when it is applied to\nthe design of technologies, including AI technologies. This paper argues for\nthe use of PD for the design of AI technologies, and introduces and proposes a\nnew PD, which we call agile participatory design, that not only can could be\nused for the design of AI and data-driven technologies, but also overcomes\nissues surrounding current PD and its use in the design of such technologies.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 23:36:25 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Hossain", "Soaad", ""], ["Ahmed", "Syed Ishtiaque", ""]]}, {"id": "2104.04075", "submitter": "Rohit Saluja Mr.", "authors": "Rohit Saluja, Avleen Malhi, Samanta Knapi\\v{c}, Kary Fr\\\"amling, Cicek\n  Cavdar", "title": "Towards a Rigorous Evaluation of Explainability for Multivariate Time\n  Series", "comments": "Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Machine learning-based systems are rapidly gaining popularity and in-line\nwith that there has been a huge research surge in the field of explainability\nto ensure that machine learning models are reliable, fair, and can be held\nliable for their decision-making process. Explainable Artificial Intelligence\n(XAI) methods are typically deployed to debug black-box machine learning models\nbut in comparison to tabular, text, and image data, explainability in time\nseries is still relatively unexplored. The aim of this study was to achieve and\nevaluate model agnostic explainability in a time series forecasting problem.\nThis work focused on proving a solution for a digital consultancy company\naiming to find a data-driven approach in order to understand the effect of\ntheir sales related activities on the sales deals closed. The solution involved\nframing the problem as a time series forecasting problem to predict the sales\ndeals and the explainability was achieved using two novel model agnostic\nexplainability techniques, Local explainable model-agnostic explanations (LIME)\nand Shapley additive explanations (SHAP) which were evaluated using human\nevaluation of explainability. The results clearly indicate that the\nexplanations produced by LIME and SHAP greatly helped lay humans in\nunderstanding the predictions made by the machine learning model. The presented\nwork can easily be extended to any time\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 17:16:36 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Saluja", "Rohit", ""], ["Malhi", "Avleen", ""], ["Knapi\u010d", "Samanta", ""], ["Fr\u00e4mling", "Kary", ""], ["Cavdar", "Cicek", ""]]}, {"id": "2104.04076", "submitter": "Omer Aydin", "authors": "\\\"Omer Aydin, Cem Ali Kandemir, Umut Kira\\c{c}, Feri\\c{s}tah\n  Dalkili\\c{c}", "title": "An artificial intelligence and Internet of things based automated\n  irrigation system", "comments": null, "journal-ref": "International Conference on Computer Technologies and Applications\n  in Food and Agriculture, 11-12 July 2019, Konya, Turkey. Pages:95-106", "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  It is not hard to see that the need for clean water is growing by considering\nthe decrease of the water sources day by day in the world. Potable fresh water\nis also used for irrigation, so it should be planned to decrease freshwater\nwastage. With the development of technology and the availability of cheaper and\nmore effective solutions, the efficiency of irrigation increased and the water\nloss can be reduced. In particular, Internet of things (IoT) devices has begun\nto be used in all areas. We can easily and precisely collect temperature,\nhumidity and mineral values from the irrigation field with the IoT devices and\nsensors. Most of the operations and decisions about irrigation are carried out\nby people. For people, it is hard to have all the real-time data such as\ntemperature, moisture and mineral levels in the decision-making process and\nmake decisions by considering them. People usually make decisions with their\nexperience. In this study, a wide range of information from the irrigation\nfield was obtained by using IoT devices and sensors. Data collected from IoT\ndevices and sensors sent via communication channels and stored on MongoDB. With\nthe help of Weka software, the data was normalized and the normalized data was\nused as a learning set. As a result of the examinations, a decision tree (J48)\nalgorithm with the highest accuracy was chosen and an artificial intelligence\nmodel was created. Decisions are used to manage operations such as starting,\nmaintaining and stopping the irrigation. The accuracy of the decisions was\nevaluated and the irrigation system was tested with the results. There are\noptions to manage, view the system remotely and manually and also see the\nsystem s decisions with the created mobile application.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 21:05:26 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Aydin", "\u00d6mer", ""], ["Kandemir", "Cem Ali", ""], ["Kira\u00e7", "Umut", ""], ["Dalkili\u00e7", "Feri\u015ftah", ""]]}, {"id": "2104.04105", "submitter": "Sangjae Bae", "authors": "Sangjae Bae, David Isele, Kikuo Fujimura, Scott J. Moura", "title": "Risk-Aware Lane Selection on Highway with Dynamic Obstacles", "comments": "Submitted to 32nd IEEE Intelligent Vehicles Symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a discretionary lane selection algorithm. In particular,\nhighway driving is considered as a targeted scenario, where each lane has a\ndifferent level of traffic flow. When lane-changing is discretionary, it is\nadvised not to change lanes unless highly beneficial, e.g., reducing travel\ntime significantly or securing higher safety. Evaluating such \"benefit\" is a\nchallenge, along with multiple surrounding vehicles in dynamic speed and\nheading with uncertainty. We propose a real-time lane-selection algorithm with\ncareful cost considerations and with modularity in design. The algorithm is a\nsearch-based optimization method that evaluates uncertain dynamic positions of\nother vehicles under a continuous time and space domain. For demonstration, we\nincorporate a state-of-the-art motion planner framework (Neural Networks\nintegrated Model Predictive Control) under a CARLA simulation environment.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 22:54:27 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Bae", "Sangjae", ""], ["Isele", "David", ""], ["Fujimura", "Kikuo", ""], ["Moura", "Scott J.", ""]]}, {"id": "2104.04108", "submitter": "Eleftheria Briakou", "authors": "Eleftheria Briakou, Di Lu, Ke Zhang, Joel Tetreault", "title": "XFORMAL: A Benchmark for Multilingual Formality Style Transfer", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We take the first step towards multilingual style transfer by creating and\nreleasing XFORMAL, a benchmark of multiple formal reformulations of informal\ntext in Brazilian Portuguese, French, and Italian. Results on XFORMAL suggest\nthat state-of-the-art style transfer approaches perform close to simple\nbaselines, indicating that style transfer is even more challenging when moving\nmultilingual.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 23:01:17 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Briakou", "Eleftheria", ""], ["Lu", "Di", ""], ["Zhang", "Ke", ""], ["Tetreault", "Joel", ""]]}, {"id": "2104.04121", "submitter": "Angel Yanguas-Gil", "authors": "Angel Yanguas-Gil", "title": "Fast, Smart Neuromorphic Sensors Based on Heterogeneous Networks and\n  Mixed Encodings", "comments": "Paper accepted in GOMACTech 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuromorphic architectures are ideally suited for the implementation of smart\nsensors able to react, learn, and respond to a changing environment. Our work\nuses the insect brain as a model to understand how heterogeneous architectures,\nincorporating different types of neurons and encodings, can be leveraged to\ncreate systems integrating input processing, evaluation, and response. Here we\nshow how the combination of time and rate encodings can lead to fast sensors\nthat are able to generate a hypothesis on the input in only a few cycles and\nthen use that hypothesis as secondary input for more detailed analysis.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 00:26:37 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Yanguas-Gil", "Angel", ""]]}, {"id": "2104.04123", "submitter": "Erkan Kayacan", "authors": "Erdal Kayacan, Erkan Kayacan, Herman Ramon, Okyay Kaynak and Wouter\n  Saeys", "title": "Towards Agrobots: Trajectory Control of an Autonomous Tractor Using\n  Type-2 Fuzzy Logic Controllers", "comments": null, "journal-ref": "IEEE/ASME Transactions on Mechatronics, vol. 20, no. 1, pp.\n  287-298, Feb. 2015", "doi": "10.1109/TMECH.2013.2291874.", "report-no": null, "categories": "cs.RO cs.AI cs.LG cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Provision of some autonomous functions to an agricultural vehicle would\nlighten the job of the operator but in doing so, the accuracy should not be\nlost to still obtain an optimal yield. Autonomous navigation of an agricultural\nvehicle involves the control of different dynamic subsystems, such as the yaw\nangle dynamics and the longitudinal speed dynamics. In this study, a\nproportional-integral-derivative controller is used to control the longitudinal\nvelocity of the tractor. For the control of the yaw angle dynamics, a\nproportional-derivative controller works in parallel with a type-2 fuzzy neural\nnetwork. In such an arrangement, the former ensures the stability of the\nrelated subsystem, while the latter learns the system dynamics and becomes the\nleading controller. In this way, instead of modeling the interactions between\nthe subsystems prior to the design of model-based control, we develop a control\nalgorithm which learns the interactions online from the measured feedback\nerror. In addition to the control of the stated subsystems, a kinematic\ncontroller is needed to correct the errors in both the x- and the y- axis for\nthe trajectory tracking problem of the tractor. To demonstrate the real-time\nabilities of the proposed control scheme, an autonomous tractor is equipped\nwith the use of reasonably priced sensors and actuators. Experimental results\nshow the efficacy and efficiency of the proposed learning algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 00:46:23 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Kayacan", "Erdal", ""], ["Kayacan", "Erkan", ""], ["Ramon", "Herman", ""], ["Kaynak", "Okyay", ""], ["Saeys", "Wouter", ""]]}, {"id": "2104.04132", "submitter": "Tyler Hayes", "authors": "Tyler L. Hayes, Giri P. Krishnan, Maxim Bazhenov, Hava T. Siegelmann,\n  Terrence J. Sejnowski, Christopher Kanan", "title": "Replay in Deep Learning: Current Approaches and Missing Biological\n  Elements", "comments": "Accepted for publication in the MIT Press journal of Neural\n  Computation", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Replay is the reactivation of one or more neural patterns, which are similar\nto the activation patterns experienced during past waking experiences. Replay\nwas first observed in biological neural networks during sleep, and it is now\nthought to play a critical role in memory formation, retrieval, and\nconsolidation. Replay-like mechanisms have been incorporated into deep\nartificial neural networks that learn over time to avoid catastrophic\nforgetting of previous knowledge. Replay algorithms have been successfully used\nin a wide range of deep learning methods within supervised, unsupervised, and\nreinforcement learning paradigms. In this paper, we provide the first\ncomprehensive comparison between replay in the mammalian brain and replay in\nartificial neural networks. We identify multiple aspects of biological replay\nthat are missing in deep learning systems and hypothesize how they could be\nutilized to improve artificial neural networks.\n", "versions": [{"version": "v1", "created": "Thu, 1 Apr 2021 15:19:08 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 21:01:25 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Hayes", "Tyler L.", ""], ["Krishnan", "Giri P.", ""], ["Bazhenov", "Maxim", ""], ["Siegelmann", "Hava T.", ""], ["Sejnowski", "Terrence J.", ""], ["Kanan", "Christopher", ""]]}, {"id": "2104.04140", "submitter": "Manas Gaur", "authors": "Manas Gaur, Vamsi Aribandi, Amanuel Alambo, Ugur Kursuncu,\n  Krishnaprasad Thirunarayan, Jonanthan Beich, Jyotishman Pathak, Amit Sheth", "title": "Characterization of Time-variant and Time-invariant Assessment of\n  Suicidality on Reddit using C-SSRS", "comments": "24 Pages, 8 Tables, 6 Figures; Accepted by PLoS One ; One of the two\n  mentioned Datasets in the manuscript has Closed Access. We will make it\n  public after PLoS One produces the manuscript", "journal-ref": null, "doi": "10.1371/journal.pone.0250448", "report-no": null, "categories": "cs.SI cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Suicide is the 10th leading cause of death in the U.S (1999-2019). However,\npredicting when someone will attempt suicide has been nearly impossible. In the\nmodern world, many individuals suffering from mental illness seek emotional\nsupport and advice on well-known and easily-accessible social media platforms\nsuch as Reddit. While prior artificial intelligence research has demonstrated\nthe ability to extract valuable information from social media on suicidal\nthoughts and behaviors, these efforts have not considered both severity and\ntemporality of risk. The insights made possible by access to such data have\nenormous clinical potential - most dramatically envisioned as a trigger to\nemploy timely and targeted interventions (i.e., voluntary and involuntary\npsychiatric hospitalization) to save lives. In this work, we address this\nknowledge gap by developing deep learning algorithms to assess suicide risk in\nterms of severity and temporality from Reddit data based on the Columbia\nSuicide Severity Rating Scale (C-SSRS). In particular, we employ two deep\nlearning approaches: time-variant and time-invariant modeling, for user-level\nsuicide risk assessment, and evaluate their performance against a\nclinician-adjudicated gold standard Reddit corpus annotated based on the\nC-SSRS. Our results suggest that the time-variant approach outperforms the\ntime-invariant method in the assessment of suicide-related ideations and\nsupportive behaviors (AUC:0.78), while the time-invariant model performed\nbetter in predicting suicide-related behaviors and suicide attempt (AUC:0.64).\nThe proposed approach can be integrated with clinical diagnostic interviews for\nimproving suicide risk assessments.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 01:39:41 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Gaur", "Manas", ""], ["Aribandi", "Vamsi", ""], ["Alambo", "Amanuel", ""], ["Kursuncu", "Ugur", ""], ["Thirunarayan", "Krishnaprasad", ""], ["Beich", "Jonanthan", ""], ["Pathak", "Jyotishman", ""], ["Sheth", "Amit", ""]]}, {"id": "2104.04144", "submitter": "Alfredo Carrillo MSc", "authors": "Alfredo Carrillo, Luis F. Cant\\'u and Alejandro Noriega", "title": "Individual Explanations in Machine Learning Models: A Survey for\n  Practitioners", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, the use of sophisticated statistical models that influence\ndecisions in domains of high societal relevance is on the rise. Although these\nmodels can often bring substantial improvements in the accuracy and efficiency\nof organizations, many governments, institutions, and companies are reluctant\nto their adoption as their output is often difficult to explain in\nhuman-interpretable ways. Hence, these models are often regarded as\nblack-boxes, in the sense that their internal mechanisms can be opaque to human\naudit. In real-world applications, particularly in domains where decisions can\nhave a sensitive impact--e.g., criminal justice, estimating credit scores,\ninsurance risk, health risks, etc.--model interpretability is desired.\nRecently, the academic literature has proposed a substantial amount of methods\nfor providing interpretable explanations to machine learning models. This\nsurvey reviews the most relevant and novel methods that form the\nstate-of-the-art for addressing the particular problem of explaining individual\ninstances in machine learning. It seeks to provide a succinct review that can\nguide data science and machine learning practitioners in the search for\nappropriate methods to their problem domain.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 01:46:34 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 02:46:34 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Carrillo", "Alfredo", ""], ["Cant\u00fa", "Luis F.", ""], ["Noriega", "Alejandro", ""]]}, {"id": "2104.04147", "submitter": "David Leslie", "authors": "David Leslie, Christopher Burr, Mhairi Aitken, Josh Cowls, Michael\n  Katell and Morgan Briggs", "title": "Artificial intelligence, human rights, democracy, and the rule of law: a\n  primer", "comments": null, "journal-ref": null, "doi": "10.5281/zenodo.4639743", "report-no": null, "categories": "cs.CY cs.AI cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In September 2019, the Council of Europe's Committee of Ministers adopted the\nterms of reference for the Ad Hoc Committee on Artificial Intelligence (CAHAI).\nThe CAHAI is charged with examining the feasibility and potential elements of a\nlegal framework for the design, development, and deployment of AI systems that\naccord with Council of Europe standards across the interrelated areas of human\nrights, democracy, and the rule of law. As a first and necessary step in\ncarrying out this responsibility, the CAHAI's Feasibility Study, adopted by its\nplenary in December 2020, has explored options for an international legal\nresponse that fills existing gaps in legislation and tailors the use of binding\nand non-binding legal instruments to the specific risks and opportunities\npresented by AI systems. The Study examines how the fundamental rights and\nfreedoms that are already codified in international human rights law can be\nused as the basis for such a legal framework. The purpose of this primer is to\nintroduce the main concepts and principles presented in the CAHAI's Feasibility\nStudy for a general, non-technical audience. It also aims to provide some\nbackground information on the areas of AI innovation, human rights law,\ntechnology policy, and compliance mechanisms covered therein. In keeping with\nthe Council of Europe's commitment to broad multi-stakeholder consultations,\noutreach, and engagement, this primer has been designed to help facilitate the\nmeaningful and informed participation of an inclusive group of stakeholders as\nthe CAHAI seeks feedback and guidance regarding the essential issues raised by\nthe Feasibility Study.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 05:58:42 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Leslie", "David", ""], ["Burr", "Christopher", ""], ["Aitken", "Mhairi", ""], ["Cowls", "Josh", ""], ["Katell", "Michael", ""], ["Briggs", "Morgan", ""]]}, {"id": "2104.04148", "submitter": "Alfredo Carrillo MSc", "authors": "Alfredo Carrillo, Luis F. Cant\\'u, Luis Tejerina and Alejandro Noriega", "title": "Individual Explanations in Machine Learning Models: A Case Study on\n  Poverty Estimation", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning methods are being increasingly applied in sensitive societal\ncontexts, where decisions impact human lives. Hence it has become necessary to\nbuild capabilities for providing easily-interpretable explanations of models'\npredictions. Recently in academic literature, a vast number of explanations\nmethods have been proposed. Unfortunately, to our knowledge, little has been\ndocumented about the challenges machine learning practitioners most often face\nwhen applying them in real-world scenarios. For example, a typical procedure\nsuch as feature engineering can make some methodologies no longer applicable.\nThe present case study has two main objectives. First, to expose these\nchallenges and how they affect the use of relevant and novel explanations\nmethods. And second, to present a set of strategies that mitigate such\nchallenges, as faced when implementing explanation methods in a relevant\napplication domain -- poverty estimation and its use for prioritizing access to\nsocial policies.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 01:54:58 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 03:06:05 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Carrillo", "Alfredo", ""], ["Cant\u00fa", "Luis F.", ""], ["Tejerina", "Luis", ""], ["Noriega", "Alejandro", ""]]}, {"id": "2104.04159", "submitter": "Harel Yedidsion", "authors": "Harel Yedidsion, Shani Alkoby, Peter Stone", "title": "Sequential Online Chore Division for Autonomous Vehicle Convoy Formation", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chore division is a class of fair division problems in which some undesirable\n\"resource\" must be shared among a set of participants, with each participant\nwanting to get as little as possible. Typically the set of participants is\nfixed and known at the outset. This paper introduces a novel variant, called\nsequential online chore division (SOCD), in which participants arrive and\ndepart online, while the chore is being performed: both the total number of\nparticipants and their arrival/departure times are initially unknown. In SOCD,\nexactly one agent must be performing the chore at any give time (e.g. keeping\nlookout), and switching the performer incurs a cost. In this paper, we propose\nand analyze three mechanisms for SOCD: one centralized mechanism using side\npayments, and two distributed ones that seek to balance the participants'\nloads. Analysis and results are presented in a domain motivated by autonomous\nvehicle convoy formation, where the chore is leading the convoy so that all\nfollowers can enjoy reduced wind resistance.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 02:28:28 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Yedidsion", "Harel", ""], ["Alkoby", "Shani", ""], ["Stone", "Peter", ""]]}, {"id": "2104.04162", "submitter": "Ademola Okerinde", "authors": "Ademola Okerinde and Lior Shamir and William Hsu and Tom Theis and\n  Nasik Nafi", "title": "eGAN: Unsupervised approach to class imbalance using transfer learning", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Class imbalance is an inherent problem in many machine learning\nclassification tasks. This often leads to trained models that are unusable for\nany practical purpose. In this study we explore an unsupervised approach to\naddress these imbalances by leveraging transfer learning from pre-trained image\nclassification models to encoder-based Generative Adversarial Network (eGAN).\nTo the best of our knowledge, this is the first work to tackle this problem\nusing GAN without needing to augment with synthesized fake images.\n  In the proposed approach we use the discriminator network to output a\nnegative or positive score. We classify as minority, test samples with negative\nscores and as majority those with positive scores. Our approach eliminates\nepistemic uncertainty in model predictions, as the P(minority) + P(majority)\nneed not sum up to 1. The impact of transfer learning and combinations of\ndifferent pre-trained image classification models at the generator and\ndiscriminator is also explored. Best result of 0.69 F1-score was obtained on\nCIFAR-10 classification task with imbalance ratio of 1:2500.\n  Our approach also provides a mechanism of thresholding the specificity or\nsensitivity of our machine learning system. Keywords: Class imbalance, Transfer\nLearning, GAN, nash equilibrium\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 02:37:55 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Okerinde", "Ademola", ""], ["Shamir", "Lior", ""], ["Hsu", "William", ""], ["Theis", "Tom", ""], ["Nafi", "Nasik", ""]]}, {"id": "2104.04191", "submitter": "Jessica Yung", "authors": "Jessica Yung, Rob Romijnders, Alexander Kolesnikov, Lucas Beyer, Josip\n  Djolonga, Neil Houlsby, Sylvain Gelly, Mario Lucic, Xiaohua Zhai", "title": "SI-Score: An image dataset for fine-grained analysis of robustness to\n  object location, rotation and size", "comments": "4 pages (10 pages including references and appendix), 10 figures.\n  Accepted at the ICLR 2021 RobustML Workshop. arXiv admin note: text overlap\n  with arXiv:2007.08558", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Before deploying machine learning models it is critical to assess their\nrobustness. In the context of deep neural networks for image understanding,\nchanging the object location, rotation and size may affect the predictions in\nnon-trivial ways. In this work we perform a fine-grained analysis of robustness\nwith respect to these factors of variation using SI-Score, a synthetic dataset.\nIn particular, we investigate ResNets, Vision Transformers and CLIP, and\nidentify interesting qualitative differences between these.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 05:00:49 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Yung", "Jessica", ""], ["Romijnders", "Rob", ""], ["Kolesnikov", "Alexander", ""], ["Beyer", "Lucas", ""], ["Djolonga", "Josip", ""], ["Houlsby", "Neil", ""], ["Gelly", "Sylvain", ""], ["Lucic", "Mario", ""], ["Zhai", "Xiaohua", ""]]}, {"id": "2104.04194", "submitter": "Srividya Subramanian", "authors": "Sihem Amer-Yahia (2), Georgia Koutrika (1), Frederic Bastian (7),\n  Theofilos Belmpas (1), Martin Braschler (9), Ursin Brunner (9), Diego\n  Calvanese (8), Maximilian Fabricius (5), Orest Gkini (1), Catherine Kosten\n  (9), Davide Lanti (8), Antonis Litke (6), Hendrik L\\\"ucke-Tieke (3),\n  Francesco Alessandro Massucci (6), Tarcisio Mendes de Farias (7), Alessandro\n  Mosca (8), Francesco Multari (6), Nikolaos Papadakis (4), Dimitris\n  Papadopoulos (4), Yogendra Patil (2), Aur\\'elien Personnaz (2), Guillem Rull\n  (6), Ana Sima (7), Ellery Smith (9), Dimitrios Skoutas (1), Srividya\n  Subramanian (5), Guohui Xiao (8), Kurt Stockinger (9) ((1) Athena Research\n  Center, Greece, (2) CNRS, University Grenoble Alpes, France, (3) Fraunhofer\n  IGD, Germany, (4) Infili, Greece, (5) Max Planck Institute, Germany, (6)\n  SIRIS Academic, Spain, (7) SIB Swiss Institute of Bioinformatics,\n  Switzerland, (8) Free University of Bozen-Bolzano, Italy, (9) ZHAW Zurich\n  University of Applied Sciences, Switzerland)", "title": "INODE: Building an End-to-End Data Exploration System in Practice\n  [Extended Vision]", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A full-fledged data exploration system must combine different access\nmodalities with a powerful concept of guiding the user in the exploration\nprocess, by being reactive and anticipative both for data discovery and for\ndata linking. Such systems are a real opportunity for our community to cater to\nusers with different domain and data science expertise. We introduce INODE --\nan end-to-end data exploration system -- that leverages, on the one hand,\nMachine Learning and, on the other hand, semantics for the purpose of Data\nManagement (DM). Our vision is to develop a classic unified, comprehensive\nplatform that provides extensive access to open datasets, and we demonstrate it\nin three significant use cases in the fields of Cancer Biomarker Reearch,\nResearch and Innovation Policy Making, and Astrophysics. INODE offers\nsustainable services in (a) data modeling and linking, (b) integrated query\nprocessing using natural language, (c) guidance, and (d) data exploration\nthrough visualization, thus facilitating the user in discovering new insights.\nWe demonstrate that our system is uniquely accessible to a wide range of users\nfrom larger scientific communities to the public. Finally, we briefly\nillustrate how this work paves the way for new research opportunities in DM.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 05:04:04 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Amer-Yahia", "Sihem", ""], ["Koutrika", "Georgia", ""], ["Bastian", "Frederic", ""], ["Belmpas", "Theofilos", ""], ["Braschler", "Martin", ""], ["Brunner", "Ursin", ""], ["Calvanese", "Diego", ""], ["Fabricius", "Maximilian", ""], ["Gkini", "Orest", ""], ["Kosten", "Catherine", ""], ["Lanti", "Davide", ""], ["Litke", "Antonis", ""], ["L\u00fccke-Tieke", "Hendrik", ""], ["Massucci", "Francesco Alessandro", ""], ["de Farias", "Tarcisio Mendes", ""], ["Mosca", "Alessandro", ""], ["Multari", "Francesco", ""], ["Papadakis", "Nikolaos", ""], ["Papadopoulos", "Dimitris", ""], ["Patil", "Yogendra", ""], ["Personnaz", "Aur\u00e9lien", ""], ["Rull", "Guillem", ""], ["Sima", "Ana", ""], ["Smith", "Ellery", ""], ["Skoutas", "Dimitrios", ""], ["Subramanian", "Srividya", ""], ["Xiao", "Guohui", ""], ["Stockinger", "Kurt", ""]]}, {"id": "2104.04197", "submitter": "Long Wang", "authors": "Zhongju Wang, Long Wang, Chao Huang, Xiong Luo", "title": "BERT-based Chinese Text Classification for Emergency Domain with a Novel\n  Loss Function", "comments": "16 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes an automatic Chinese text categorization method for\nsolving the emergency event report classification problem. Since bidirectional\nencoder representations from transformers (BERT) has achieved great success in\nnatural language processing domain, it is employed to derive emergency text\nfeatures in this study. To overcome the data imbalance problem in the\ndistribution of emergency event categories, a novel loss function is proposed\nto improve the performance of the BERT-based model. Meanwhile, to avoid the\nimpact of the extreme learning rate, the Adabound optimization algorithm that\nachieves a gradual smooth transition from Adam to SGD is employed to learn\nparameters of the model. To verify the feasibility and effectiveness of the\nproposed method, a Chinese emergency text dataset collected from the Internet\nis employed. Compared with benchmarking methods, the proposed method has\nachieved the best performance in terms of accuracy, weighted-precision,\nweighted-recall, and weighted-F1 values. Therefore, it is promising to employ\nthe proposed method for real applications in smart emergency management\nsystems.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 05:25:00 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Wang", "Zhongju", ""], ["Wang", "Long", ""], ["Huang", "Chao", ""], ["Luo", "Xiong", ""]]}, {"id": "2104.04243", "submitter": "Vivek Gupta", "authors": "J. Neeraja, Vivek Gupta, Vivek Srikumar", "title": "Incorporating External Knowledge to Enhance Tabular Reasoning", "comments": "11 pages, 1 Figure, 14 tables, To appear in NAACL 2021 (Short paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reasoning about tabular information presents unique challenges to modern NLP\napproaches which largely rely on pre-trained contextualized embeddings of text.\nIn this paper, we study these challenges through the problem of tabular natural\nlanguage inference. We propose easy and effective modifications to how\ninformation is presented to a model for this task. We show via systematic\nexperiments that these strategies substantially improve tabular inference\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 08:25:01 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Neeraja", "J.", ""], ["Gupta", "Vivek", ""], ["Srikumar", "Vivek", ""]]}, {"id": "2104.04258", "submitter": "Tim Pearce", "authors": "Tim Pearce, Jun Zhu", "title": "Counter-Strike Deathmatch with Large-Scale Behavioural Cloning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes an AI agent that plays the popular first-person-shooter\n(FPS) video game `Counter-Strike; Global Offensive' (CSGO) from pixel input.\nThe agent, a deep neural network, matches the performance of the medium\ndifficulty built-in AI on the deathmatch game mode, whilst adopting a humanlike\nplay style. Unlike much prior work in games, no API is available for CSGO, so\nalgorithms must train and run in real-time. This limits the quantity of\non-policy data that can be generated, precluding many reinforcement learning\nalgorithms. Our solution uses behavioural cloning - training on a large noisy\ndataset scraped from human play on online servers (4 million frames, comparable\nin size to ImageNet), and a smaller dataset of high-quality expert\ndemonstrations. This scale is an order of magnitude larger than prior work on\nimitation learning in FPS games.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 09:12:12 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Pearce", "Tim", ""], ["Zhu", "Jun", ""]]}, {"id": "2104.04269", "submitter": "Leni Kenneth Le Goff", "authors": "L\\'eni K. Le Goff, Edgar Buchanan, Emma Hart, Agoston E. Eiben, Wei\n  Li, Matteo De Carlo, Alan F. Winfield, Matthew F. Hale, Robert Woolley, Mike\n  Angus, Jon Timmis, Andy M. Tyrrell", "title": "Morpho-evolution with learning using a controller archive as an\n  inheritance mechanism", "comments": "14 pages including 2 pages of supplementary materials, 14 figures, 1\n  table. Currently under review for the special issue of IEEE TCDS on Towards\n  autonomous evolution, (re)production and learning in robotic eco-systems.\n  https://www.york.ac.uk/robot-lab/are/ieee_special_issue_2020/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In evolutionary robotics, several approaches have been shown to be capable of\nthe joint optimisation of body-plans and controllers by either using only\nevolution or combining evolution and learning. When working in rich\nmorphological spaces, it is common for offspring to have body-plans that are\nvery different from either of their parents, which can cause difficulties with\nrespect to inheriting a suitable controller. To address this, we propose a\nframework that combines an evolutionary algorithm to generate body-plans and a\nlearning algorithm to optimise the parameters of a neural controller where the\ntopology of this controller is created once the body-plan of each offspring\nbody-plan is generated. The key novelty of the approach is to add an external\narchive for storing learned controllers that map to explicit `types' of robots\n(where this is defined with respect the features of the body-plan). By\ninheriting an appropriate controller from the archive rather than learning from\na randomly initialised one, we show that both the speed and magnitude of\nlearning increases over time when compared to an approach that starts from\nscratch, using three different test-beds. The framework also provides new\ninsights into the complex interactions between evolution and learning, and the\nrole of morphological intelligence in robot design.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 09:32:36 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Goff", "L\u00e9ni K. Le", ""], ["Buchanan", "Edgar", ""], ["Hart", "Emma", ""], ["Eiben", "Agoston E.", ""], ["Li", "Wei", ""], ["De Carlo", "Matteo", ""], ["Winfield", "Alan F.", ""], ["Hale", "Matthew F.", ""], ["Woolley", "Robert", ""], ["Angus", "Mike", ""], ["Timmis", "Jon", ""], ["Tyrrell", "Andy M.", ""]]}, {"id": "2104.04278", "submitter": "Tristan Cazenave", "authors": "Tristan Cazenave", "title": "Batch Monte Carlo Tree Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Making inferences with a deep neural network on a batch of states is much\nfaster with a GPU than making inferences on one state after another. We build\non this property to propose Monte Carlo Tree Search algorithms using batched\ninferences. Instead of using either a search tree or a transposition table we\npropose to use both in the same algorithm. The transposition table contains the\nresults of the inferences while the search tree contains the statistics of\nMonte Carlo Tree Search. We also propose to analyze multiple heuristics that\nimprove the search: the $\\mu$ FPU, the Virtual Mean, the Last Iteration and the\nSecond Move heuristics. They are evaluated for the game of Go using a MobileNet\nneural network.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 09:54:21 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Cazenave", "Tristan", ""]]}, {"id": "2104.04323", "submitter": "Johannes H\\\"ohne", "authors": "Jonas Dippel, Steffen Vogler, Johannes H\\\"ohne", "title": "Towards Fine-grained Visual Representations by Combining Contrastive\n  Learning with Image Reconstruction and Attention-weighted Pooling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents Contrastive Reconstruction, ConRec - a self-supervised\nlearning algorithm that obtains image representations by jointly optimizing a\ncontrastive and a self-reconstruction loss. We showcase that state-of-the-art\ncontrastive learning methods (e.g. SimCLR) have shortcomings to capture\nfine-grained visual features in their representations. ConRec extends the\nSimCLR framework by adding (1) a self-reconstruction task and (2) an attention\nmechanism within the contrastive learning task. This is accomplished by\napplying a simple encoder-decoder architecture with two heads. We show that\nboth extensions contribute towards an improved vector representation for images\nwith fine-grained visual features. Combining those concepts, ConRec outperforms\nSimCLR and SimCLR with Attention-Pooling on fine-grained classification\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 12:12:10 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Dippel", "Jonas", ""], ["Vogler", "Steffen", ""], ["H\u00f6hne", "Johannes", ""]]}, {"id": "2104.04353", "submitter": "Boris Ruf", "authors": "Boris Ruf, Marcin Detyniecki", "title": "Implementing Fair Regression In The Real World", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most fair regression algorithms mitigate bias towards sensitive sub\npopulations and therefore improve fairness at group level. In this paper, we\ninvestigate the impact of such implementation of fair regression on the\nindividual. More precisely, we assess the evolution of continuous predictions\nfrom an unconstrained to a fair algorithm by comparing results from baseline\nalgorithms with fair regression algorithms for the same data points. Based on\nour findings, we propose a set of post-processing algorithms to improve the\nutility of the existing fair regression approaches.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 13:31:16 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Ruf", "Boris", ""], ["Detyniecki", "Marcin", ""]]}, {"id": "2104.04375", "submitter": "Yunfeng Zhang", "authors": "Shweta Narkar, Yunfeng Zhang, Q. Vera Liao, Dakuo Wang, Justin D Weisz", "title": "Model LineUpper: Supporting Interactive Model Comparison at Multiple\n  Levels for AutoML", "comments": null, "journal-ref": null, "doi": "10.1145/3397481.3450658", "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated Machine Learning (AutoML) is a rapidly growing set of technologies\nthat automate the model development pipeline by searching model space and\ngenerating candidate models. A critical, final step of AutoML is human\nselection of a final model from dozens of candidates. In current AutoML\nsystems, selection is supported only by performance metrics. Prior work has\nshown that in practice, people evaluate ML models based on additional criteria,\nsuch as the way a model makes predictions. Comparison may happen at multiple\nlevels, from types of errors, to feature importance, to how the model makes\npredictions of specific instances. We developed \\tool{} to support interactive\nmodel comparison for AutoML by integrating multiple Explainable AI (XAI) and\nvisualization techniques. We conducted a user study in which we both evaluated\nthe system and used it as a technology probe to understand how users perform\nmodel comparison in an AutoML system. We discuss design implications for\nutilizing XAI techniques for model comparison and supporting the unique needs\nof data scientists in comparing AutoML models.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 14:06:13 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Narkar", "Shweta", ""], ["Zhang", "Yunfeng", ""], ["Liao", "Q. Vera", ""], ["Wang", "Dakuo", ""], ["Weisz", "Justin D", ""]]}, {"id": "2104.04424", "submitter": "Ammar Fayad", "authors": "Ammar Fayad and Majd Ibrahim", "title": "Behavior-Guided Actor-Critic: Improving Exploration via Learning Policy\n  Behavior Representation for Deep Reinforcement Learning", "comments": "Preprint. Under Review. 9 pages, 3 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose Behavior-Guided Actor-Critic (BAC), an off-policy\nactor-critic deep RL algorithm. BAC mathematically formulates the behavior of\nthe policy through autoencoders by providing an accurate estimation of how\nfrequently each state-action pair was visited while taking into consideration\nstate dynamics that play a crucial role in determining the trajectories\nproduced by the policy. The agent is encouraged to change its behavior\nconsistently towards less-visited state-action pairs while attaining good\nperformance by maximizing the expected discounted sum of rewards, resulting in\nan efficient exploration of the environment and good exploitation of all high\nreward regions. One prominent aspect of our approach is that it is applicable\nto both stochastic and deterministic actors in contrast to maximum entropy deep\nreinforcement learning algorithms. Results show considerably better\nperformances of BAC when compared to several cutting-edge learning algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 15:22:35 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Fayad", "Ammar", ""], ["Ibrahim", "Majd", ""]]}, {"id": "2104.04525", "submitter": "Shunji Umetani", "authors": "Shunji Umetani and Shohei Murakami", "title": "Coordinate descent heuristics for the irregular strip packing problem of\n  rasterized shapes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.AI math.OC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We consider the irregular strip packing problem of rasterized shapes, where a\ngiven set of pieces of irregular shapes represented in pixels should be placed\ninto a rectangular container without overlap. The rasterized shapes enable us\nto check overlap without any exceptional handling due to geometric issues,\nwhile they often require much memory and computational effort in\nhigh-resolution. We develop an efficient algorithm to check overlap using a\npair of scanlines that reduces the complexity of rasterized shapes by merging\nconsecutive pixels in each row and column into strips with unit width,\nrespectively. Based on this, we develop coordinate descent heuristics that\nrepeat a line search in the horizontal and vertical directions alternately.\nComputational results for test instances show that the proposed algorithm\nobtains sufficiently dense layouts of rasterized shapes in high-resolution\nwithin a reasonable computation time.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 08:55:52 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Umetani", "Shunji", ""], ["Murakami", "Shohei", ""]]}, {"id": "2104.04572", "submitter": "Le Xia", "authors": "Le Xia, Yao Sun, Rafiq Swash, Lina Mohjazi, Lei Zhang, and Muhammad\n  Ali Imran", "title": "Smart and Secure CAV Networks Empowered by AI-Enabled Blockchain: Next\n  Frontier for Intelligent Safe-Driving Assessment", "comments": "8 pages, 6 figures, this paper has been submitted to IEEE Network\n  Magazine and is still awaiting the review results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Securing safe-driving for connected and autonomous vehicles (CAVs) continues\nto be a widespread concern despite various sophisticated functions delivered by\nartificial intelligence for in-vehicle devices. Besides, diverse malicious\nnetwork attacks become ubiquitous along with the worldwide implementation of\nthe Internet of Vehicles, which exposes a range of reliability and privacy\nthreats for managing data in CAV networks. Combined with the fact that the\ncapability of existing CAVs in handling intensive computation tasks is limited,\nthis implies a need for designing an efficient assessment system to guarantee\nautonomous driving safety without compromising data security. Motivated by\nthis, in this article, we propose a novel framework, namely Blockchain-enabled\nintElligent Safe-driving assessmenT (BEST), that offers a smart and reliable\napproach for conducting safe driving supervision while protecting vehicular\ninformation. Specifically, a promising solution that exploits a long short-term\nmemory model is introduced to assess the safety level of the moving CAVs. Then,\nwe investigate how a distributed blockchain obtains adequate trustworthiness\nand robustness for CAV data by adopting a byzantine fault tolerance-based\ndelegated proof-of-stake consensus mechanism. Simulation results demonstrate\nthat our presented BEST gains better data credibility with a higher prediction\naccuracy for vehicular safety assessment when compared with existing schemes.\nFinally, we discuss several open challenges that need to be addressed in future\nCAV networks.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 19:08:34 GMT"}, {"version": "v2", "created": "Sun, 4 Jul 2021 11:03:20 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Xia", "Le", ""], ["Sun", "Yao", ""], ["Swash", "Rafiq", ""], ["Mohjazi", "Lina", ""], ["Zhang", "Lei", ""], ["Imran", "Muhammad Ali", ""]]}, {"id": "2104.04580", "submitter": "Jian Wu", "authors": "Jian Wu, Rajal Nivargi, Sree Sai Teja Lanka, Arjun Manoj Menon, Sai\n  Ajay Modukuri, Nishanth Nakshatri, Xin Wei, Zhuoer Wang, James Caverlee,\n  Sarah M. Rajtmajer, C. Lee Giles", "title": "Predicting the Reproducibility of Social and Behavioral Science Papers\n  Using Supervised Learning Models", "comments": "17 pages, 8 figures, a draft to be submitted to JCDL'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.AI cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In recent years, significant effort has been invested verifying the\nreproducibility and robustness of research claims in social and behavioral\nsciences (SBS), much of which has involved resource-intensive replication\nprojects. In this paper, we investigate prediction of the reproducibility of\nSBS papers using machine learning methods based on a set of features. We\npropose a framework that extracts five types of features from scholarly work\nthat can be used to support assessments of reproducibility of published\nresearch claims. Bibliometric features, venue features, and author features are\ncollected from public APIs or extracted using open source machine learning\nlibraries with customized parsers. Statistical features, such as p-values, are\nextracted by recognizing patterns in the body text. Semantic features, such as\nfunding information, are obtained from public APIs or are extracted using\nnatural language processing models. We analyze pairwise correlations between\nindividual features and their importance for predicting a set of human-assessed\nground truth labels. In doing so, we identify a subset of 9 top features that\nplay relatively more important roles in predicting the reproducibility of SBS\npapers in our corpus. Results are verified by comparing performances of 10\nsupervised predictive classifiers trained on different sets of features.\n", "versions": [{"version": "v1", "created": "Thu, 8 Apr 2021 00:45:20 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Wu", "Jian", ""], ["Nivargi", "Rajal", ""], ["Lanka", "Sree Sai Teja", ""], ["Menon", "Arjun Manoj", ""], ["Modukuri", "Sai Ajay", ""], ["Nakshatri", "Nishanth", ""], ["Wei", "Xin", ""], ["Wang", "Zhuoer", ""], ["Caverlee", "James", ""], ["Rajtmajer", "Sarah M.", ""], ["Giles", "C. Lee", ""]]}, {"id": "2104.04597", "submitter": "Muhao Chen", "authors": "Xuelu Chen, Michael Boratko, Muhao Chen, Shib Sankar Dasgupta, Xiang\n  Lorraine Li, Andrew McCallum", "title": "Probabilistic Box Embeddings for Uncertain Knowledge Graph Reasoning", "comments": "NAACL-HLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge bases often consist of facts which are harvested from a variety of\nsources, many of which are noisy and some of which conflict, resulting in a\nlevel of uncertainty for each triple. Knowledge bases are also often\nincomplete, prompting the use of embedding methods to generalize from known\nfacts, however, existing embedding methods only model triple-level uncertainty,\nand reasoning results lack global consistency. To address these shortcomings,\nwe propose BEUrRE, a novel uncertain knowledge graph embedding method with\ncalibrated probabilistic semantics. BEUrRE models each entity as a box (i.e.\naxis-aligned hyperrectangle) and relations between two entities as affine\ntransforms on the head and tail entity boxes. The geometry of the boxes allows\nfor efficient calculation of intersections and volumes, endowing the model with\ncalibrated probabilistic semantics and facilitating the incorporation of\nrelational constraints. Extensive experiments on two benchmark datasets show\nthat BEUrRE consistently outperforms baselines on confidence prediction and\nfact ranking due to its probabilistic calibration and ability to capture\nhigh-order dependencies among facts.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 21:01:52 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Chen", "Xuelu", ""], ["Boratko", "Michael", ""], ["Chen", "Muhao", ""], ["Dasgupta", "Shib Sankar", ""], ["Li", "Xiang Lorraine", ""], ["McCallum", "Andrew", ""]]}, {"id": "2104.04610", "submitter": "Vincent Le-Guen", "authors": "Vincent Le Guen, Nicolas Thome", "title": "Deep Time Series Forecasting with Shape and Temporal Criteria", "comments": "arXiv admin note: text overlap with arXiv:2010.07349", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper addresses the problem of multi-step time series forecasting for\nnon-stationary signals that can present sudden changes. Current\nstate-of-the-art deep learning forecasting methods, often trained with variants\nof the MSE, lack the ability to provide sharp predictions in deterministic and\nprobabilistic contexts. To handle these challenges, we propose to incorporate\nshape and temporal criteria in the training objective of deep models. We define\nshape and temporal similarities and dissimilarities, based on a smooth\nrelaxation of Dynamic Time Warping (DTW) and Temporal Distortion Index (TDI),\nthat enable to build differentiable loss functions and positive semi-definite\n(PSD) kernels. With these tools, we introduce DILATE (DIstortion Loss including\nshApe and TimE), a new objective for deterministic forecasting, that explicitly\nincorporates two terms supporting precise shape and temporal change detection.\nFor probabilistic forecasting, we introduce STRIPE++ (Shape and Time diverRsIty\nin Probabilistic forEcasting), a framework for providing a set of sharp and\ndiverse forecasts, where the structured shape and time diversity is enforced\nwith a determinantal point process (DPP) diversity loss. Extensive experiments\nand ablations studies on synthetic and real-world datasets confirm the benefits\nof leveraging shape and time features in time series forecasting.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 21:24:33 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Guen", "Vincent Le", ""], ["Thome", "Nicolas", ""]]}, {"id": "2104.04630", "submitter": "Tharindu Ranasinghe Mr", "authors": "Tharindu Ranasinghe, Diptanu Sarkar, Marcos Zampieri, Alexander\n  Ororbia", "title": "WLV-RIT at SemEval-2021 Task 5: A Neural Transformer Framework for\n  Detecting Toxic Spans", "comments": "Accepted to SemEval-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, the widespread use of social media has led to an increase in\nthe generation of toxic and offensive content on online platforms. In response,\nsocial media platforms have worked on developing automatic detection methods\nand employing human moderators to cope with this deluge of offensive content.\nWhile various state-of-the-art statistical models have been applied to detect\ntoxic posts, there are only a few studies that focus on detecting the words or\nexpressions that make a post offensive. This motivates the organization of the\nSemEval-2021 Task 5: Toxic Spans Detection competition, which has provided\nparticipants with a dataset containing toxic spans annotation in English posts.\nIn this paper, we present the WLV-RIT entry for the SemEval-2021 Task 5. Our\nbest performing neural transformer model achieves an $0.68$ F1-Score.\nFurthermore, we develop an open-source framework for multilingual detection of\noffensive spans, i.e., MUDES, based on neural transformers that detect toxic\nspans in texts.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 22:52:26 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 22:32:17 GMT"}, {"version": "v3", "created": "Thu, 27 May 2021 22:09:39 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Ranasinghe", "Tharindu", ""], ["Sarkar", "Diptanu", ""], ["Zampieri", "Marcos", ""], ["Ororbia", "Alexander", ""]]}, {"id": "2104.04632", "submitter": "Tharindu Ranasinghe Mr", "authors": "Hansi Hettiarachchi, Tharindu Ranasinghe", "title": "TransWiC at SemEval-2021 Task 2: Transformer-based Multilingual and\n  Cross-lingual Word-in-Context Disambiguation", "comments": "Accepted to SemEval-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Identifying whether a word carries the same meaning or different meaning in\ntwo contexts is an important research area in natural language processing which\nplays a significant role in many applications such as question answering,\ndocument summarisation, information retrieval and information extraction. Most\nof the previous work in this area rely on language-specific resources making it\ndifficult to generalise across languages. Considering this limitation, our\napproach to SemEval-2021 Task 2 is based only on pretrained transformer models\nand does not use any language-specific processing and resources. Despite that,\nour best model achieves 0.90 accuracy for English-English subtask which is very\ncompatible compared to the best result of the subtask; 0.93 accuracy. Our\napproach also achieves satisfactory results in other monolingual and\ncross-lingual language pairs as well.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 23:06:05 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Hettiarachchi", "Hansi", ""], ["Ranasinghe", "Tharindu", ""]]}, {"id": "2104.04650", "submitter": "Deval Mehta", "authors": "Deval Mehta, Umar Asif, Tian Hao, Erhan Bilal, Stefan Von Cavallar,\n  Stefan Harrer, Jeffrey Rogers", "title": "Towards Automated and Marker-less Parkinson Disease Assessment:\n  Predicting UPDRS Scores using Sit-stand videos", "comments": "Accepted by CVPR Workshops 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel deep learning enabled, video based analysis\nframework for assessing the Unified Parkinsons Disease Rating Scale (UPDRS)\nthat can be used in the clinic or at home. We report results from comparing the\nperformance of the framework to that of trained clinicians on a population of\n32 Parkinsons disease (PD) patients. In-person clinical assessments by trained\nneurologists are used as the ground truth for training our framework and for\ncomparing the performance. We find that the standard sit-to-stand activity can\nbe used to evaluate the UPDRS sub-scores of bradykinesia (BRADY) and posture\ninstability and gait disorders (PIGD). For BRADY we find F1-scores of 0.75\nusing our framework compared to 0.50 for the video based rater clinicians,\nwhile for PIGD we find 0.78 for the framework and 0.45 for the video based\nrater clinicians. We believe our proposed framework has potential to provide\nclinically acceptable end points of PD in greater granularity without imposing\nburdens on patients and clinicians, which empowers a variety of use cases such\nas passive tracking of PD progression in spaces such as nursing homes, in-home\nself-assessment, and enhanced tele-medicine.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 00:05:51 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Mehta", "Deval", ""], ["Asif", "Umar", ""], ["Hao", "Tian", ""], ["Bilal", "Erhan", ""], ["Von Cavallar", "Stefan", ""], ["Harrer", "Stefan", ""], ["Rogers", "Jeffrey", ""]]}, {"id": "2104.04654", "submitter": "Debvrat Varshney", "authors": "Debvrat Varshney, Maryam Rahnemoonfar, Masoud Yari, and John Paden", "title": "Regression Networks For Calculating Englacial Layer Thickness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ice thickness estimation is an important aspect of ice sheet studies. In this\nwork, we use convolutional neural networks with multiple output nodes to\nregress and learn the thickness of internal ice layers in Snow Radar images\ncollected in northwest Greenland. We experiment with some state-of-the-art\nnetworks and find that with the residual connections of ResNet50, we could\nachieve a mean absolute error of 1.251 pixels over the test set. Such\nregression-based networks can further be improved by embedding domain knowledge\nand radar information in the neural network in order to reduce the requirement\nof manual annotations.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 00:31:32 GMT"}, {"version": "v2", "created": "Sat, 12 Jun 2021 00:46:07 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Varshney", "Debvrat", ""], ["Rahnemoonfar", "Maryam", ""], ["Yari", "Masoud", ""], ["Paden", "John", ""]]}, {"id": "2104.04659", "submitter": "Yeye He", "authors": "Jie Song, Yeye He", "title": "Auto-Validate: Unsupervised Data Validation Using Data-Domain Patterns\n  Inferred from Data Lakes", "comments": "full version of a SIGMOD 2021 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Complex data pipelines are increasingly common in diverse applications such\nas BI reporting and ML modeling. These pipelines often recur regularly (e.g.,\ndaily or weekly), as BI reports need to be refreshed, and ML models need to be\nretrained. However, it is widely reported that in complex production pipelines,\nupstream data feeds can change in unexpected ways, causing downstream\napplications to break silently that are expensive to resolve.\n  Data validation has thus become an important topic, as evidenced by notable\nrecent efforts from Google and Amazon, where the objective is to catch data\nquality issues early as they arise in the pipelines. Our experience on\nproduction data suggests, however, that on string-valued data, these existing\napproaches yield high false-positive rates and frequently require human\nintervention. In this work, we develop a corpus-driven approach to\nauto-validate \\emph{machine-generated data} by inferring suitable\ndata-validation \"patterns\" that accurately describe the underlying data domain,\nwhich minimizes false positives while maximizing data quality issues caught.\nEvaluations using production data from real data lakes suggest that\nAuto-Validate is substantially more effective than existing methods. Part of\nthis technology ships as an Auto-Tag feature in Microsoft Azure Purview.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 01:15:48 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 17:29:18 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Song", "Jie", ""], ["He", "Yeye", ""]]}, {"id": "2104.04670", "submitter": "Ruiqi Zhong", "authors": "Ruiqi Zhong, Kristy Lee, Zheng Zhang, Dan Klein", "title": "Meta-tuning Language Models to Answer Prompts Better", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Large pretrained language models like GPT-3 have acquired a surprising\nability to perform zero-shot classification (ZSC). For example, to classify\nreview sentiments, we can \"prompt\" the language model with the review and the\nquestion \"Is the review positive?\" as the context, and ask it to predict\nwhether the next word is \"Yes\" or \"No\". However, these models are not\nspecialized for answering these prompts. To address this weakness, we propose\nmeta-tuning, which trains the model to specialize in answering prompts but\nstill generalize to unseen tasks. To create the training data, we aggregated 43\nexisting datasets, annotated 441 label descriptions in total, and unified them\ninto the above question answering (QA) format. After meta-tuning, our model\noutperforms a same-sized QA model for most labels on unseen tasks, and we\nforecast that the performance would improve for even larger models. Therefore,\nmeasuring ZSC performance on non-specialized language models might\nunderestimate their true capability, and community-wide efforts on aggregating\ndatasets and unifying their formats can help build models that understand\nprompts better.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 02:57:22 GMT"}, {"version": "v2", "created": "Sat, 17 Apr 2021 03:43:15 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Zhong", "Ruiqi", ""], ["Lee", "Kristy", ""], ["Zhang", "Zheng", ""], ["Klein", "Dan", ""]]}, {"id": "2104.04676", "submitter": "Xutan Peng", "authors": "Xutan Peng, Guanyi Chen, Chenghua Lin, Mark Stevenson", "title": "Highly Efficient Knowledge Graph Embedding Learning with Orthogonal\n  Procrustes Analysis", "comments": "To appear at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge Graph Embeddings (KGEs) have been intensively explored in recent\nyears due to their promise for a wide range of applications. However, existing\nstudies focus on improving the final model performance without acknowledging\nthe computational cost of the proposed approaches, in terms of execution time\nand environmental impact. This paper proposes a simple yet effective KGE\nframework which can reduce the training time and carbon footprint by orders of\nmagnitudes compared with state-of-the-art approaches, while producing\ncompetitive performance. We highlight three technical innovations: full batch\nlearning via relational matrices, closed-form Orthogonal Procrustes Analysis\nfor KGEs, and non-negative-sampling training. In addition, as the first KGE\nmethod whose entity embeddings also store full relation information, our\ntrained models encode rich semantics and are highly interpretable.\nComprehensive experiments and ablation studies involving 13 strong baselines\nand two standard datasets verify the effectiveness and efficiency of our\nalgorithm.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 03:55:45 GMT"}, {"version": "v2", "created": "Sat, 17 Apr 2021 12:17:05 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Peng", "Xutan", ""], ["Chen", "Guanyi", ""], ["Lin", "Chenghua", ""], ["Stevenson", "Mark", ""]]}, {"id": "2104.04696", "submitter": "Antony Thomas", "authors": "Antony Thomas, Fulvio Mastrogiovanni, Marco Baglietto", "title": "MPTP: Motion-Planning-aware Task Planning for Navigation in Belief Space", "comments": "Accepted for publication in Robotics and Autonomous Systems. arXiv\n  admin note: text overlap with arXiv:1910.11683, arXiv:2010.00780", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an integrated Task-Motion Planning (TMP) framework for navigation\nin large-scale environments. Of late, TMP for manipulation has attracted\nsignificant interest resulting in a proliferation of different approaches. In\ncontrast, TMP for navigation has received considerably less attention.\nAutonomous robots operating in real-world complex scenarios require planning in\nthe discrete (task) space and the continuous (motion) space. In\nknowledge-intensive domains, on the one hand, a robot has to reason at the\nhighest-level, for example, the objects to procure, the regions to navigate to\nin order to acquire them; on the other hand, the feasibility of the respective\nnavigation tasks have to be checked at the execution level. This presents a\nneed for motion-planning-aware task planners. In this paper, we discuss a\nprobabilistically complete approach that leverages this task-motion interaction\nfor navigating in large knowledge-intensive domains, returning a plan that is\noptimal at the task-level. The framework is intended for motion planning under\nmotion and sensing uncertainty, which is formally known as belief space\nplanning. The underlying methodology is validated in simulation, in an office\nenvironment and its scalability is tested in the larger Willow Garage world. A\nreasonable comparison with a work that is closest to our approach is also\nprovided. We also demonstrate the adaptability of our approach by considering a\nbuilding floor navigation domain. Finally, we also discuss the limitations of\nour approach and put forward suggestions for improvements and future work.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 06:52:16 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Thomas", "Antony", ""], ["Mastrogiovanni", "Fulvio", ""], ["Baglietto", "Marco", ""]]}, {"id": "2104.04736", "submitter": "Anna Langedijk", "authors": "Anna Langedijk, Verna Dankers, Phillip Lippe, Sander Bos, Bryan\n  Cardenas Guevara, Helen Yannakoudakis, Ekaterina Shutova", "title": "Meta-learning for fast cross-lingual adaptation in dependency parsing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-learning, or learning to learn, is a technique that can help to overcome\nresource scarcity in cross-lingual NLP problems, by enabling fast adaptation to\nnew tasks. We apply model-agnostic meta-learning (MAML) to the task of\ncross-lingual dependency parsing. We train our model on a diverse set of\nlanguages to learn a parameter initialization that can adapt quickly to new\nlanguages. We find that meta-learning with pre-training can significantly\nimprove upon the performance of language transfer and standard supervised\nlearning baselines for a variety of unseen, typologically diverse, and\nlow-resource languages, in a few-shot learning setup.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 11:10:16 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 16:10:40 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Langedijk", "Anna", ""], ["Dankers", "Verna", ""], ["Lippe", "Phillip", ""], ["Bos", "Sander", ""], ["Guevara", "Bryan Cardenas", ""], ["Yannakoudakis", "Helen", ""], ["Shutova", "Ekaterina", ""]]}, {"id": "2104.04739", "submitter": "Anna Glazkova", "authors": "Mikhail Kotyushev, Anna Glazkova, Dmitry Morozov", "title": "MIPT-NSU-UTMN at SemEval-2021 Task 5: Ensembling Learning with\n  Pre-trained Language Models for Toxic Spans Detection", "comments": "Accepted at SemEval-2021 Workshop, ACL-IJCNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes our system for SemEval-2021 Task 5 on Toxic Spans\nDetection. We developed ensemble models using BERT-based neural architectures\nand post-processing to combine tokens into spans. We evaluated several\npre-trained language models using various ensemble techniques for toxic span\nidentification and achieved sizable improvements over our baseline fine-tuned\nBERT models. Finally, our system obtained a F1-score of 67.55% on test data.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 11:27:32 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Kotyushev", "Mikhail", ""], ["Glazkova", "Anna", ""], ["Morozov", "Dmitry", ""]]}, {"id": "2104.04748", "submitter": "Zhengxu Hou", "authors": "Zhengxu Hou, Bang Liu, Ruihui Zhao, Zijing Ou, Yafei Liu, Xi Chen,\n  Yefeng Zheng", "title": "Imperfect also Deserves Reward: Multi-Level and Sequential Reward\n  Modeling for Better Dialog Management", "comments": "9 pages", "journal-ref": "NAACL 2021", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For task-oriented dialog systems, training a Reinforcement Learning (RL)\nbased Dialog Management module suffers from low sample efficiency and slow\nconvergence speed due to the sparse rewards in RL.To solve this problem, many\nstrategies have been proposed to give proper rewards when training RL, but\ntheir rewards lack interpretability and cannot accurately estimate the\ndistribution of state-action pairs in real dialogs. In this paper, we propose a\nmulti-level reward modeling approach that factorizes a reward into a\nthree-level hierarchy: domain, act, and slot. Based on inverse adversarial\nreinforcement learning, our designed reward model can provide more accurate and\nexplainable reward signals for state-action pairs.Extensive evaluations show\nthat our approach can be applied to a wide range of reinforcement\nlearning-based dialog systems and significantly improves both the performance\nand the speed of convergence.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 12:20:23 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Hou", "Zhengxu", ""], ["Liu", "Bang", ""], ["Zhao", "Ruihui", ""], ["Ou", "Zijing", ""], ["Liu", "Yafei", ""], ["Chen", "Xi", ""], ["Zheng", "Yefeng", ""]]}, {"id": "2104.04768", "submitter": "Alexandre Chenu", "authors": "Alexandre Chenu, Nicolas Perrin-Gilbert, St\\'ephane Doncieux, Olivier\n  Sigaud", "title": "Selection-Expansion: A Unifying Framework for Motion-Planning and\n  Diversity Search Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning agents need a reward signal to learn successful\npolicies. When this signal is sparse or the corresponding gradient is\ndeceptive, such agents need a dedicated mechanism to efficiently explore their\nsearch space without relying on the reward. Looking for a large diversity of\nbehaviors or using Motion Planning (MP) algorithms are two options in this\ncontext. In this paper, we build on the common roots between these two options\nto investigate the properties of two diversity search algorithms, the Novelty\nSearch and the Goal Exploration Process algorithms. These algorithms look for\ndiversity in an outcome space or behavioral space which is generally\nhand-designed to represent what matters for a given task. The relation to MP\nalgorithms reveals that the smoothness, or lack of smoothness of the mapping\nbetween the policy parameter space and the outcome space plays a key role in\nthe search efficiency. In particular, we show empirically that, if the mapping\nis smooth enough, i.e. if two close policies in the parameter space lead to\nsimilar outcomes, then diversity algorithms tend to inherit exploration\nproperties of MP algorithms. By contrast, if it is not, diversity algorithms\nlose these properties and their performance strongly depends on specific\nheuristics, notably filtering mechanisms that discard some of the explored\npolicies.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 13:52:27 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Chenu", "Alexandre", ""], ["Perrin-Gilbert", "Nicolas", ""], ["Doncieux", "St\u00e9phane", ""], ["Sigaud", "Olivier", ""]]}, {"id": "2104.04781", "submitter": "Sankeerth Rao Karingula", "authors": "Sankeerth Rao Karingula and Nandini Ramanan and Rasool Tahmasbi and\n  Mehrnaz Amjadi and Deokwoo Jung and Ricky Si and Charanraj Thimmisetty and\n  Luisa Polania Cabrera and Marjorie Sayer and Claudionor Nunes Coelho Jr", "title": "Boosted Embeddings for Time Series Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Time series forecasting is a fundamental task emerging from diverse\ndata-driven applications. Many advanced autoregressive methods such as ARIMA\nwere used to develop forecasting models. Recently, deep learning based methods\nsuch as DeepAr, NeuralProphet, Seq2Seq have been explored for time series\nforecasting problem. In this paper, we propose a novel time series forecast\nmodel, DeepGB. We formulate and implement a variant of Gradient boosting\nwherein the weak learners are DNNs whose weights are incrementally found in a\ngreedy manner over iterations. In particular, we develop a new embedding\narchitecture that improves the performance of many deep learning models on time\nseries using Gradient boosting variant. We demonstrate that our model\noutperforms existing comparable state-of-the-art models using real-world sensor\ndata and public dataset.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 14:38:11 GMT"}, {"version": "v2", "created": "Sun, 11 Jul 2021 17:45:20 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Karingula", "Sankeerth Rao", ""], ["Ramanan", "Nandini", ""], ["Tahmasbi", "Rasool", ""], ["Amjadi", "Mehrnaz", ""], ["Jung", "Deokwoo", ""], ["Si", "Ricky", ""], ["Thimmisetty", "Charanraj", ""], ["Cabrera", "Luisa Polania", ""], ["Sayer", "Marjorie", ""], ["Coelho", "Claudionor Nunes", "Jr"]]}, {"id": "2104.04829", "submitter": "Sally Ghanem", "authors": "Sally Ghanem, Siddharth Roheda, and Hamid Krim", "title": "Latent Code-Based Fusion: A Volterra Neural Network Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a deep structure encoder using the recently introduced Volterra\nNeural Networks (VNNs) to seek a latent representation of multi-modal data\nwhose features are jointly captured by a union of subspaces. The so-called\nself-representation embedding of the latent codes leads to a simplified fusion\nwhich is driven by a similarly constructed decoding. The Volterra Filter\narchitecture achieved reduction in parameter complexity is primarily due to\ncontrolled non-linearities being introduced by the higher-order convolutions in\ncontrast to generalized activation functions. Experimental results on two\ndifferent datasets have shown a significant improvement in the clustering\nperformance for VNNs auto-encoder over conventional Convolutional Neural\nNetworks (CNNs) auto-encoder. In addition, we also show that the proposed\napproach demonstrates a much-improved sample complexity over CNN-based\nauto-encoder with a superb robust classification performance.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 18:29:01 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Ghanem", "Sally", ""], ["Roheda", "Siddharth", ""], ["Krim", "Hamid", ""]]}, {"id": "2104.04830", "submitter": "Aidin Zehtab-Salmasi", "authors": "Aidin Zehtab-Salmasi, Mohammad-Reza Feizi-Derakhshi, Mohamad-Ali\n  Balafar", "title": "FRAKE: Fusional Real-time Automatic Keyword Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Keyword extraction is called identifying words or phrases that express the\nmain concepts of texts in best. There is a huge amount of texts that are\ncreated every day and at all times through electronic infrastructure. So, it is\npractically impossible for humans to study and manage this volume of documents.\nHowever, the need for efficient and effective access to these documents is\nevident in various purposes. Weblogs, News, and technical notes are almost long\ntexts, while the reader seeks to understand the concepts by topics or keywords\nto decide for reading the full text. To this aim, we use a combined approach\nthat consists of two models of graph centrality features and textural features.\nIn the following, graph centralities, such as degree, betweenness, eigenvector,\nand closeness centrality, have been used to optimally combine them to extract\nthe best keyword among the candidate keywords extracted by the proposed method.\nAlso, another approach has been introduced to distinguishing keywords among\ncandidate phrases and considering them as a separate keyword. To evaluate the\nproposed method, seven datasets named, Semeval2010, SemEval2017, Inspec, fao30,\nThesis100, pak2018 and WikiNews have been used, and results reported Precision,\nRecall, and F- measure.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 18:30:17 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Zehtab-Salmasi", "Aidin", ""], ["Feizi-Derakhshi", "Mohammad-Reza", ""], ["Balafar", "Mohamad-Ali", ""]]}, {"id": "2104.04840", "submitter": "Alexander Jones", "authors": "Alex Jones, Derry Tanti Wijaya", "title": "Sentiment-based Candidate Selection for NMT", "comments": "14 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The explosion of user-generated content (UGC)--e.g. social media posts,\ncomments, and reviews--has motivated the development of NLP applications\ntailored to these types of informal texts. Prevalent among these applications\nhave been sentiment analysis and machine translation (MT). Grounded in the\nobservation that UGC features highly idiomatic, sentiment-charged language, we\npropose a decoder-side approach that incorporates automatic sentiment scoring\ninto the MT candidate selection process. We train separate English and Spanish\nsentiment classifiers, then, using n-best candidates generated by a baseline MT\nmodel with beam search, select the candidate that minimizes the absolute\ndifference between the sentiment score of the source sentence and that of the\ntranslation, and perform a human evaluation to assess the produced\ntranslations. Unlike previous work, we select this minimally divergent\ntranslation by considering the sentiment scores of the source sentence and\ntranslation on a continuous interval, rather than using e.g. binary\nclassification, allowing for more fine-grained selection of translation\ncandidates. The results of human evaluations show that, in comparison to the\nopen-source MT baseline model on top of which our sentiment-based pipeline is\nbuilt, our pipeline produces more accurate translations of colloquial,\nsentiment-heavy source texts.\n", "versions": [{"version": "v1", "created": "Sat, 10 Apr 2021 19:01:52 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Jones", "Alex", ""], ["Wijaya", "Derry Tanti", ""]]}, {"id": "2104.04891", "submitter": "Qingyong Hu", "authors": "Qingyong Hu, Bo Yang, Guangchi Fang, Yulan Guo, Ales Leonardis, Niki\n  Trigoni, Andrew Markham", "title": "SQN: Weakly-Supervised Semantic Segmentation of Large-Scale 3D Point\n  Clouds with 1000x Fewer Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We study the problem of labelling effort for semantic segmentation of\nlarge-scale 3D point clouds. Existing works usually rely on densely annotated\npoint-level semantic labels to provide supervision for network training.\nHowever, in real-world scenarios that contain billions of points, it is\nimpractical and extremely costly to manually annotate every single point. In\nthis paper, we first investigate whether dense 3D labels are truly required for\nlearning meaningful semantic representations. Interestingly, we find that the\nsegmentation performance of existing works only drops slightly given as few as\n1% of the annotations. However, beyond this point (e.g. 1 per thousand and\nbelow) existing techniques fail catastrophically. To this end, we propose a new\nweak supervision method to implicitly augment the total amount of available\nsupervision signals, by leveraging the semantic similarity between neighboring\npoints. Extensive experiments demonstrate that the proposed Semantic Query\nNetwork (SQN) achieves state-of-the-art performance on six large-scale open\ndatasets under weak supervision schemes, while requiring only 1000x fewer\nlabeled points for training. The code is available at\nhttps://github.com/QingyongHu/SQN.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 01:29:50 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Hu", "Qingyong", ""], ["Yang", "Bo", ""], ["Fang", "Guangchi", ""], ["Guo", "Yulan", ""], ["Leonardis", "Ales", ""], ["Trigoni", "Niki", ""], ["Markham", "Andrew", ""]]}, {"id": "2104.04893", "submitter": "Brittany Davis Pierson", "authors": "Brittany Davis Pierson, Justine Ventura, Matthew E. Taylor", "title": "The Atari Data Scraper", "comments": "3 authors, nine pages, 6 figures, papers with code", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reinforcement learning has made great strides in recent years due to the\nsuccess of methods using deep neural networks. However, such neural networks\nact as a black box, obscuring the inner workings. While reinforcement learning\nhas the potential to solve unique problems, a lack of trust and understanding\nof reinforcement learning algorithms could prevent their widespread adoption.\nHere, we present a library that attaches a \"data scraper\" to deep reinforcement\nlearning agents, acting as an observer, and then show how the data collected by\nthe Atari Data Scraper can be used to understand and interpret deep\nreinforcement learning agents. The code for the Atari Data Scraper can be found\nhere: https://github.com/IRLL/Atari-Data-Scraper\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 01:39:33 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Pierson", "Brittany Davis", ""], ["Ventura", "Justine", ""], ["Taylor", "Matthew E.", ""]]}, {"id": "2104.04906", "submitter": "Xu Jiang", "authors": "Qi Wang, Xu Jiang, Mulin Chen and Xuelong Li", "title": "Auto-weighted Multi-view Feature Selection with Graph Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we focus on the unsupervised multi-view feature selection\nwhich tries to handle high dimensional data in the field of multi-view\nlearning. Although some graph-based methods have achieved satisfactory\nperformance, they ignore the underlying data structure across different views.\nBesides, their pre-defined laplacian graphs are sensitive to the noises in the\noriginal data space, and fail to get the optimal neighbor assignment. To\naddress the above problems, we propose a novel unsupervised multi-view feature\nselection model based on graph learning, and the contributions are threefold:\n(1) during the feature selection procedure, the consensus similarity graph\nshared by different views is learned. Therefore, the proposed model can reveal\nthe data relationship from the feature subset. (2) a reasonable rank constraint\nis added to optimize the similarity matrix to obtain more accurate information;\n(3) an auto-weighted framework is presented to assign view weights adaptively,\nand an effective alternative iterative algorithm is proposed to optimize the\nproblem. Experiments on various datasets demonstrate the superiority of the\nproposed method compared with the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 03:25:25 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Wang", "Qi", ""], ["Jiang", "Xu", ""], ["Chen", "Mulin", ""], ["Li", "Xuelong", ""]]}, {"id": "2104.04907", "submitter": "Ningyu Zhang", "authors": "Xiang Chen, Xin Xie, Zhen Bi, Hongbin Ye, Shumin Deng, Ningyu Zhang,\n  Huajun Chen", "title": "Disentangled Contrastive Learning for Learning Robust Textual\n  Representations", "comments": "Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although the self-supervised pre-training of transformer models has resulted\nin the revolutionizing of natural language processing (NLP) applications and\nthe achievement of state-of-the-art results with regard to various benchmarks,\nthis process is still vulnerable to small and imperceptible permutations\noriginating from legitimate inputs. Intuitively, the representations should be\nsimilar in the feature space with subtle input permutations, while large\nvariations occur with different meanings. This motivates us to investigate the\nlearning of robust textual representation in a contrastive manner. However, it\nis non-trivial to obtain opposing semantic instances for textual samples. In\nthis study, we propose a disentangled contrastive learning method that\nseparately optimizes the uniformity and alignment of representations without\nnegative sampling. Specifically, we introduce the concept of momentum\nrepresentation consistency to align features and leverage power normalization\nwhile conforming the uniformity. Our experimental results for the NLP\nbenchmarks demonstrate that our approach can obtain better results compared\nwith the baselines, as well as achieve promising improvements with invariance\ntests and adversarial attacks. The code is available in\nhttps://github.com/zjunlp/DCL.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 03:32:49 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Chen", "Xiang", ""], ["Xie", "Xin", ""], ["Bi", "Zhen", ""], ["Ye", "Hongbin", ""], ["Deng", "Shumin", ""], ["Zhang", "Ningyu", ""], ["Chen", "Huajun", ""]]}, {"id": "2104.04939", "submitter": "Abdul Wahid", "authors": "Abdul Wahid, Rajesh Sharma, and Chandra Sekhara Rao Annavarapu", "title": "A Graph Convolutional Neural Network based Framework for Estimating\n  Future Citations Count of Research Articles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.AI cs.ET cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scientific publications play a vital role in the career of a researcher.\nHowever, some articles become more popular than others among the research\ncommunity and subsequently drive future research directions. One of the\nindicative signs of popular articles is the number of citations an article\nreceives. The citation count, which is also the basis with various other\nmetrics, such as the journal impact factor score, the $h$-index, is an\nessential measure for assessing a scientific paper's quality. In this work, we\nproposed a Graph Convolutional Network (GCN) based framework for estimating\nfuture research publication citations for both the short-term (1-year) and\nlong-term (for 5-years and 10-years) duration. We have tested our proposed\napproach over the AMiner dataset, specifically on research articles from the\ncomputer science domain, consisting of more than 0.8 million articles.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 07:20:53 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Wahid", "Abdul", ""], ["Sharma", "Rajesh", ""], ["Annavarapu", "Chandra Sekhara Rao", ""]]}, {"id": "2104.04945", "submitter": "Tomasz Szandala", "authors": "Tomasz Szandala", "title": "Enhancing Deep Neural Network Saliency Visualizations with Gradual\n  Extrapolation", "comments": "Published in IEEE Access:\n  https://ieeexplore.ieee.org/document/9468713", "journal-ref": "IEEE Access, 2021", "doi": "10.1109/ACCESS.2021.3093824", "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, an enhancement technique for the class activation mapping\nmethods such as gradient-weighted class activation maps or excitation\nbackpropagation is proposed to present the visual explanations of decisions\nfrom convolutional neural network-based models. The proposed idea, called\nGradual Extrapolation, can supplement any method that generates a heatmap\npicture by sharpening the output. Instead of producing a coarse localization\nmap that highlights the important predictive regions in the image, the proposed\nmethod outputs the specific shape that most contributes to the model output.\nThus, the proposed method improves the accuracy of saliency maps. The effect\nhas been achieved by the gradual propagation of the crude map obtained in the\ndeep layer through all preceding layers with respect to their activations. In\nvalidation tests conducted on a selected set of images, the faithfulness,\ninterpretability, and applicability of the method are evaluated. The proposed\ntechnique significantly improves the localization detection of the neural\nnetworks attention at low additional computational costs. Furthermore, the\nproposed method is applicable to a variety deep neural network models. The code\nfor the method can be found at\nhttps://github.com/szandala/gradual-extrapolation\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 07:39:35 GMT"}, {"version": "v2", "created": "Sun, 27 Jun 2021 21:37:11 GMT"}, {"version": "v3", "created": "Wed, 7 Jul 2021 15:30:26 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Szandala", "Tomasz", ""]]}, {"id": "2104.04947", "submitter": "Kun Xu", "authors": "Kun Xu, Han Wu, Linfeng Song, Haisong Zhang, Linqi Song, Dong Yu", "title": "Conversational Semantic Role Labeling", "comments": "Accepted by TASLP. arXiv admin note: text overlap with\n  arXiv:2010.01417", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Semantic role labeling (SRL) aims to extract the arguments for each predicate\nin an input sentence. Traditional SRL can fail to analyze dialogues because it\nonly works on every single sentence, while ellipsis and anaphora frequently\noccur in dialogues. To address this problem, we propose the conversational SRL\ntask, where an argument can be the dialogue participants, a phrase in the\ndialogue history or the current sentence. As the existing SRL datasets are in\nthe sentence level, we manually annotate semantic roles for 3,000 chit-chat\ndialogues (27,198 sentences) to boost the research in this direction.\nExperiments show that while traditional SRL systems (even with the help of\ncoreference resolution or rewriting) perform poorly for analyzing dialogues,\nmodeling dialogue histories and participants greatly helps the performance,\nindicating that adapting SRL to conversations is very promising for universal\ndialogue understanding. Our initial study by applying CSRL to two mainstream\nconversational tasks, dialogue response generation and dialogue context\nrewriting, also confirms the usefulness of CSRL.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 07:45:04 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Xu", "Kun", ""], ["Wu", "Han", ""], ["Song", "Linfeng", ""], ["Zhang", "Haisong", ""], ["Song", "Linqi", ""], ["Yu", "Dong", ""]]}, {"id": "2104.04952", "submitter": "Junghyo Sohn", "authors": "Junghyo Sohn, Eunjin Jeon, Wonsik Jung, Eunsong Kang, Heung-Il Suk", "title": "Fine-Grained Attention for Weakly Supervised Object Localization", "comments": "16 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although recent advances in deep learning accelerated an improvement in a\nweakly supervised object localization (WSOL) task, there are still challenges\nto identify the entire body of an object, rather than only discriminative\nparts. In this paper, we propose a novel residual fine-grained attention (RFGA)\nmodule that autonomously excites the less activated regions of an object by\nutilizing information distributed over channels and locations within feature\nmaps in combination with a residual operation. To be specific, we devise a\nseries of mechanisms of triple-view attention representation, attention\nexpansion, and feature calibration. Unlike other attention-based WSOL methods\nthat learn a coarse attention map, having the same values across elements in\nfeature maps, our proposed RFGA learns fine-grained values in an attention map\nby assigning different attention values for each of the elements. We validated\nthe superiority of our proposed RFGA module by comparing it with the recent\nmethods in the literature over three datasets. Further, we analyzed the effect\nof each mechanism in our RFGA and visualized attention maps to get insights.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 08:14:05 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Sohn", "Junghyo", ""], ["Jeon", "Eunjin", ""], ["Jung", "Wonsik", ""], ["Kang", "Eunsong", ""], ["Suk", "Heung-Il", ""]]}, {"id": "2104.04958", "submitter": "Mario Di Mauro", "authors": "Mario Di Mauro, Giovanni Galatro, Giancarlo Fortino, Antonio Liotta", "title": "Supervised Feature Selection Techniques in Network Intrusion Detection:\n  a Critical Review", "comments": null, "journal-ref": "Engineering Applications of Artificial Intelligence Volume 101,\n  May 2021, 104216", "doi": "10.1016/j.engappai.2021.104216", "report-no": null, "categories": "cs.CR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning (ML) techniques are becoming an invaluable support for\nnetwork intrusion detection, especially in revealing anomalous flows, which\noften hide cyber-threats. Typically, ML algorithms are exploited to\nclassify/recognize data traffic on the basis of statistical features such as\ninter-arrival times, packets length distribution, mean number of flows, etc.\nDealing with the vast diversity and number of features that typically\ncharacterize data traffic is a hard problem. This results in the following\nissues: i) the presence of so many features leads to lengthy training processes\n(particularly when features are highly correlated), while prediction accuracy\ndoes not proportionally improve; ii) some of the features may introduce bias\nduring the classification process, particularly those that have scarce relation\nwith the data traffic to be classified. To this end, by reducing the feature\nspace and retaining only the most significant features, Feature Selection (FS)\nbecomes a crucial pre-processing step in network management and, specifically,\nfor the purposes of network intrusion detection. In this review paper, we\ncomplement other surveys in multiple ways: i) evaluating more recent datasets\n(updated w.r.t. obsolete KDD 99) by means of a designed-from-scratch\nPython-based procedure; ii) providing a synopsis of most credited FS approaches\nin the field of intrusion detection, including Multi-Objective Evolutionary\ntechniques; iii) assessing various experimental analyses such as feature\ncorrelation, time complexity, and performance. Our comparisons offer useful\nguidelines to network/security managers who are considering the incorporation\nof ML concepts into network intrusion detection, where trade-offs between\nperformance and resource consumption are crucial.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 08:42:01 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Di Mauro", "Mario", ""], ["Galatro", "Giovanni", ""], ["Fortino", "Giancarlo", ""], ["Liotta", "Antonio", ""]]}, {"id": "2104.04987", "submitter": "Chaoyu Guan", "authors": "Chaoyu Guan, Ziwei Zhang, Haoyang Li, Heng Chang, Zeyang Zhang, Yijian\n  Qin, Jiyan Jiang, Xin Wang, Wenwu Zhu", "title": "AutoGL: A Library for Automated Graph Learning", "comments": "*Equal contributions. 8 pages, 1 figure, accepted at ICLR 2021\n  Workshop on Geometrical and Topological Representation Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent years have witnessed an upsurge of research interests and applications\nof machine learning on graphs. Automated machine learning (AutoML) on graphs is\non the horizon to automatically design the optimal machine learning algorithm\nfor a given graph task. However, none of the existing libraries can fully\nsupport AutoML on graphs. To fill this gap, we present Automated Graph Learning\n(AutoGL), the first library for automated machine learning on graphs. AutoGL is\nopen-source, easy to use, and flexible to be extended. Specifically, we propose\nan automated machine learning pipeline for graph data containing four modules:\nauto feature engineering, model training, hyper-parameter optimization, and\nauto ensemble. For each module, we provide numerous state-of-the-art methods\nand flexible base classes and APIs, which allow easy customization. We further\nprovide experimental results to showcase the usage of our AutoGL library.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 10:49:23 GMT"}, {"version": "v2", "created": "Tue, 4 May 2021 10:50:50 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Guan", "Chaoyu", ""], ["Zhang", "Ziwei", ""], ["Li", "Haoyang", ""], ["Chang", "Heng", ""], ["Zhang", "Zeyang", ""], ["Qin", "Yijian", ""], ["Jiang", "Jiyan", ""], ["Wang", "Xin", ""], ["Zhu", "Wenwu", ""]]}, {"id": "2104.04999", "submitter": "Huong Ha", "authors": "Huong Ha, Sunil Gupta, Santu Rana, Svetha Venkatesh", "title": "ALT-MAS: A Data-Efficient Framework for Active Testing of Machine\n  Learning Algorithms", "comments": "Accepted to the RobustML workshop at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning models are being used extensively in many important areas,\nbut there is no guarantee a model will always perform well or as its developers\nintended. Understanding the correctness of a model is crucial to prevent\npotential failures that may have significant detrimental impact in critical\napplication areas. In this paper, we propose a novel framework to efficiently\ntest a machine learning model using only a small amount of labeled test data.\nThe idea is to estimate the metrics of interest for a model-under-test using\nBayesian neural network (BNN). We develop a novel data augmentation method\nhelping to train the BNN to achieve high accuracy. We also devise a theoretic\ninformation based sampling strategy to sample data points so as to achieve\naccurate estimations for the metrics of interest. Finally, we conduct an\nextensive set of experiments to test various machine learning models for\ndifferent types of metrics. Our experiments show that the metrics estimations\nby our method are significantly better than existing baselines.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 12:14:04 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Ha", "Huong", ""], ["Gupta", "Sunil", ""], ["Rana", "Santu", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "2104.05003", "submitter": "Chengjin Xu", "authors": "Chengjin Xu, Mojtaba Nayyeri, Sahar Vahdati, and Jens Lehmann", "title": "Multiple Run Ensemble Learning with Low-Dimensional Knowledge Graph\n  Embeddings", "comments": "Accepted by the 2021 International Joint Conference on Neural\n  Networks (IJCNN 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Among the top approaches of recent years, link prediction using knowledge\ngraph embedding (KGE) models has gained significant attention for knowledge\ngraph completion. Various embedding models have been proposed so far, among\nwhich, some recent KGE models obtain state-of-the-art performance on link\nprediction tasks by using embeddings with a high dimension (e.g. 1000) which\naccelerate the costs of training and evaluation considering the large scale of\nKGs. In this paper, we propose a simple but effective performance boosting\nstrategy for KGE models by using multiple low dimensions in different\nrepetition rounds of the same model. For example, instead of training a model\none time with a large embedding size of 1200, we repeat the training of the\nmodel 6 times in parallel with an embedding size of 200 and then combine the 6\nseparate models for testing while the overall numbers of adjustable parameters\nare same (6*200=1200) and the total memory footprint remains the same. We show\nthat our approach enables different models to better cope with their\nexpressiveness issues on modeling various graph patterns such as symmetric,\n1-n, n-1 and n-n. In order to justify our findings, we conduct experiments on\nvarious KGE models. Experimental results on standard benchmark datasets, namely\nFB15K, FB15K-237 and WN18RR, show that multiple low-dimensional models of the\nsame kind outperform the corresponding single high-dimensional models on link\nprediction in a certain range and have advantages in training efficiency by\nusing parallel training while the overall numbers of adjustable parameters are\nsame.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 12:26:50 GMT"}, {"version": "v2", "created": "Sun, 30 May 2021 08:51:14 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Xu", "Chengjin", ""], ["Nayyeri", "Mojtaba", ""], ["Vahdati", "Sahar", ""], ["Lehmann", "Jens", ""]]}, {"id": "2104.05018", "submitter": "Yu Pan", "authors": "Yu Pan, Maolin Wang, Zenglin Xu", "title": "TedNet: A Pytorch Toolkit for Tensor Decomposition Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tensor Decomposition Networks(TDNs) prevail for their inherent compact\narchitectures. For providing convenience, we present a toolkit named TedNet\nthat is based on the Pytorch framework, to give more researchers a flexible way\nto exploit TDNs. TedNet implements 5 kinds of tensor decomposition(i.e.,\nCANDECOMP/PARAFAC(CP), Block-Term Tucker(BT), Tucker-2, Tensor Train(TT) and\nTensor Ring(TR)) on traditional deep neural layers, the convolutional layer and\nthe fully-connected layer. By utilizing these basic layers, it is simple to\nconstruct a variety of TDNs like TR-ResNet, TT-LSTM, etc. TedNet is available\nat https://github.com/tnbar/tednet.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 14:04:06 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Pan", "Yu", ""], ["Wang", "Maolin", ""], ["Xu", "Zenglin", ""]]}, {"id": "2104.05046", "submitter": "Suyash Shandilya", "authors": "Suyash Shandilya", "title": "Print Error Detection using Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the need of an automated system for detecting print\nerrors and the efficacy of Convolutional Neural Networks in such an\napplication. We recognise the need of a dataset containing print error samples\nand propose a way to generate one artificially. We discuss the algorithms to\ngenerate such data along with the limitaions and advantages of such an\napporach. Our final trained network gives a remarkable accuracy of 99.83\\% in\ntesting. We further evaluate how such efficiency was achieved and what\nmodifications can be tested to further the results.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 16:30:17 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Shandilya", "Suyash", ""]]}, {"id": "2104.05049", "submitter": "Alaaeddine Chaoub", "authors": "Alaaeddine Chaoub, Alexandre Voisin, Christophe Cerisara, Beno\\^it\n  Iung", "title": "Learning representations with end-to-end models for improved remaining\n  useful life prognostics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The remaining Useful Life (RUL) of equipment is defined as the duration\nbetween the current time and its failure. An accurate and reliable prognostic\nof the remaining useful life provides decision-makers with valuable information\nto adopt an appropriate maintenance strategy to maximize equipment utilization\nand avoid costly breakdowns. In this work, we propose an end-to-end deep\nlearning model based on multi-layer perceptron and long short-term memory\nlayers (LSTM) to predict the RUL. After normalization of all data, inputs are\nfed directly to an MLP layers for feature learning, then to an LSTM layer to\ncapture temporal dependencies, and finally to other MLP layers for RUL\nprognostic. The proposed architecture is tested on the NASA commercial modular\naero-propulsion system simulation (C-MAPSS) dataset. Despite its simplicity\nwith respect to other recently proposed models, the model developed outperforms\nthem with a significant decrease in the competition score and in the root mean\nsquare error score between the predicted and the gold value of the RUL. In this\npaper, we will discuss how the proposed end-to-end model is able to achieve\nsuch good results and compare it to other deep learning and state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 16:45:18 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 15:35:54 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Chaoub", "Alaaeddine", ""], ["Voisin", "Alexandre", ""], ["Cerisara", "Christophe", ""], ["Iung", "Beno\u00eet", ""]]}, {"id": "2104.05050", "submitter": "ZhongYang Liu Bo", "authors": "Yang Liu, Shengmao Zhang, Fei Wang, Wei Fan, Guohua Zou, Jing Bo", "title": "Research on Optimization Method of Multi-scale Fish Target Fast\n  Detection Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The fish target detection algorithm lacks a good quality data set, and the\nalgorithm achieves real-time detection with lower power consumption on embedded\ndevices, and it is difficult to balance the calculation speed and\nidentification ability. To this end, this paper collected and annotated a data\nset named \"Aquarium Fish\" of 84 fishes containing 10042 images, and based on\nthis data set, proposed a multi-scale input fast fish target detection network\n(BTP-yoloV3) and its optimization method. The experiment uses Depthwise\nconvolution to redesign the backbone of the yoloV4 network, which reduces the\namount of calculation by 94.1%, and the test accuracy is 92.34%. Then, the\ntraining model is enhanced with MixUp, CutMix, and mosaic to increase the test\naccuracy by 1.27%; Finally, use the mish, swish, and ELU activation functions\nto increase the test accuracy by 0.76%. As a result, the accuracy of testing\nthe network with 2000 fish images reached 94.37%, and the computational\ncomplexity of the network BFLOPS was only 5.47. Comparing the YoloV3~4,\nMobileNetV2-yoloV3, and YoloV3-tiny networks of migration learning on this data\nset. The results show that BTP-Yolov3 has smaller model parameters, faster\ncalculation speed, and lower energy consumption during operation while ensuring\nthe calculation accuracy. It provides a certain reference value for the\npractical application of neural network.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 16:53:34 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Liu", "Yang", ""], ["Zhang", "Shengmao", ""], ["Wang", "Fei", ""], ["Fan", "Wei", ""], ["Zou", "Guohua", ""], ["Bo", "Jing", ""]]}, {"id": "2104.05097", "submitter": "Louis B\\'ethune", "authors": "Louis B\\'ethune, Alberto Gonz\\'alez-Sanz, Franck Mamalet, Mathieu\n  Serrurier", "title": "The Many Faces of 1-Lipschitz Neural Networks", "comments": "21 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lipschitz constrained models have been used to solve specifics deep learning\nproblems such as the estimation of Wasserstein distance for GAN, or the\ntraining of neural networks robust to adversarial attacks. Regardless the novel\nand effective algorithms to build such 1-Lipschitz networks, their usage\nremains marginal, and they are commonly considered as less expressive and less\nable to fit properly the data than their unconstrained counterpart.\n  The goal of the paper is to demonstrate that, despite being empirically\nharder to train, 1-Lipschitz neural networks are theoretically better grounded\nthan unconstrained ones when it comes to classification. To achieve that we\nrecall some results about 1-Lipschitz function in the scope of deep learning\nand we extend and illustrate them to derive general properties for\nclassification.\n  First, we show that 1-Lipschitz neural network can fit arbitrarily difficult\nfrontier making them as expressive as classical ones. When minimizing the log\nloss, we prove that the optimization problem under Lipschitz constraint is well\nposed and have a minimum, whereas regular neural networks can diverge even on\nremarkably simple situations. Then, we study the link between classification\nwith 1-Lipschitz network and optimal transport thanks to regularized versions\nof Kantorovich-Rubinstein duality theory. Last, we derive preliminary bounds on\ntheir VC dimension.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 20:31:32 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 10:15:02 GMT"}, {"version": "v3", "created": "Tue, 18 May 2021 20:27:55 GMT"}, {"version": "v4", "created": "Thu, 27 May 2021 10:01:49 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["B\u00e9thune", "Louis", ""], ["Gonz\u00e1lez-Sanz", "Alberto", ""], ["Mamalet", "Franck", ""], ["Serrurier", "Mathieu", ""]]}, {"id": "2104.05154", "submitter": "Wenjun Tang", "authors": "Wenjun Tang, Hao Wang, Xian-Long Lee, Hong-Tzer Yang", "title": "Uncover Residential Energy Consumption Patterns Using Socioeconomic and\n  Smart Meter Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NA math.NA", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper models residential consumers' energy-consumption behavior by load\npatterns and distributions and reveals the relationship between consumers' load\npatterns and socioeconomic features by machine learning. We analyze the\nreal-world smart meter data and extract load patterns using K-Medoids\nclustering, which is robust to outliers. We develop an analytical framework\nwith feature selection and deep learning models to estimate the relationship\nbetween load patterns and socioeconomic features. Specifically, we use an\nentropy-based feature selection method to identify the critical socioeconomic\ncharacteristics that affect load patterns and benefit our method's\ninterpretability. We further develop a customized deep neural network model to\ncharacterize the relationship between consumers' load patterns and selected\nsocioeconomic features. Numerical studies validate our proposed framework using\nPecan Street smart meter data and survey. We demonstrate that our framework can\ncapture the relationship between load patterns and socioeconomic information\nand outperform benchmarks such as regression and single DNN models.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 01:57:14 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Tang", "Wenjun", ""], ["Wang", "Hao", ""], ["Lee", "Xian-Long", ""], ["Yang", "Hong-Tzer", ""]]}, {"id": "2104.05158", "submitter": "Dheevatsa Mudigere", "authors": "Dheevatsa Mudigere, Yuchen Hao, Jianyu Huang, Andrew Tulloch, Srinivas\n  Sridharan, Xing Liu, Mustafa Ozdal, Jade Nie, Jongsoo Park, Liang Luo, Jie\n  Amy Yang, Leon Gao, Dmytro Ivchenko, Aarti Basant, Yuxi Hu, Jiyan Yang, Ehsan\n  K. Ardestani, Xiaodong Wang, Rakesh Komuravelli, Ching-Hsiang Chu, Serhat\n  Yilmaz, Huayu Li, Jiyuan Qian, Zhuobo Feng, Yinbin Ma, Junjie Yang, Ellie\n  Wen, Hong Li, Lin Yang, Chonglin Sun, Whitney Zhao, Dimitry Melts, Krishna\n  Dhulipala, KR Kishore, Tyler Graf, Assaf Eisenman, Kiran Kumar Matam, Adi\n  Gangidi, Guoqiang Jerry Chen, Manoj Krishnan, Avinash Nayak, Krishnakumar\n  Nair, Bharath Muthiah, Mahmoud khorashadi, Pallab Bhattacharya, Petr\n  Lapukhov, Maxim Naumov, Lin Qiao, Mikhail Smelyanskiy, Bill Jia, Vijay Rao", "title": "High-performance, Distributed Training of Large-scale Deep Learning\n  Recommendation Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning recommendation models (DLRMs) are used across many\nbusiness-critical services at Facebook and are the single largest AI\napplication in terms of infrastructure demand in its data-centers. In this\npaper we discuss the SW/HW co-designed solution for high-performance\ndistributed training of large-scale DLRMs. We introduce a high-performance\nscalable software stack based on PyTorch and pair it with the new evolution of\nZion platform, namely ZionEX. We demonstrate the capability to train very large\nDLRMs with up to 12 Trillion parameters and show that we can attain 40X speedup\nin terms of time to solution over previous systems. We achieve this by (i)\ndesigning the ZionEX platform with dedicated scale-out network, provisioned\nwith high bandwidth, optimal topology and efficient transport (ii) implementing\nan optimized PyTorch-based training stack supporting both model and data\nparallelism (iii) developing sharding algorithms capable of hierarchical\npartitioning of the embedding tables along row, column dimensions and load\nbalancing them across multiple workers; (iv) adding high-performance core\noperators while retaining flexibility to support optimizers with fully\ndeterministic updates (v) leveraging reduced precision communications,\nmulti-level memory hierarchy (HBM+DDR+SSD) and pipelining. Furthermore, we\ndevelop and briefly comment on distributed data ingestion and other supporting\nservices that are required for the robust and efficient end-to-end training in\nproduction environments.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 02:15:55 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 01:30:23 GMT"}, {"version": "v3", "created": "Thu, 15 Apr 2021 22:58:58 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Mudigere", "Dheevatsa", ""], ["Hao", "Yuchen", ""], ["Huang", "Jianyu", ""], ["Tulloch", "Andrew", ""], ["Sridharan", "Srinivas", ""], ["Liu", "Xing", ""], ["Ozdal", "Mustafa", ""], ["Nie", "Jade", ""], ["Park", "Jongsoo", ""], ["Luo", "Liang", ""], ["Yang", "Jie Amy", ""], ["Gao", "Leon", ""], ["Ivchenko", "Dmytro", ""], ["Basant", "Aarti", ""], ["Hu", "Yuxi", ""], ["Yang", "Jiyan", ""], ["Ardestani", "Ehsan K.", ""], ["Wang", "Xiaodong", ""], ["Komuravelli", "Rakesh", ""], ["Chu", "Ching-Hsiang", ""], ["Yilmaz", "Serhat", ""], ["Li", "Huayu", ""], ["Qian", "Jiyuan", ""], ["Feng", "Zhuobo", ""], ["Ma", "Yinbin", ""], ["Yang", "Junjie", ""], ["Wen", "Ellie", ""], ["Li", "Hong", ""], ["Yang", "Lin", ""], ["Sun", "Chonglin", ""], ["Zhao", "Whitney", ""], ["Melts", "Dimitry", ""], ["Dhulipala", "Krishna", ""], ["Kishore", "KR", ""], ["Graf", "Tyler", ""], ["Eisenman", "Assaf", ""], ["Matam", "Kiran Kumar", ""], ["Gangidi", "Adi", ""], ["Chen", "Guoqiang Jerry", ""], ["Krishnan", "Manoj", ""], ["Nayak", "Avinash", ""], ["Nair", "Krishnakumar", ""], ["Muthiah", "Bharath", ""], ["khorashadi", "Mahmoud", ""], ["Bhattacharya", "Pallab", ""], ["Lapukhov", "Petr", ""], ["Naumov", "Maxim", ""], ["Qiao", "Lin", ""], ["Smelyanskiy", "Mikhail", ""], ["Jia", "Bill", ""], ["Rao", "Vijay", ""]]}, {"id": "2104.05163", "submitter": "Yan Haoyang", "authors": "Haoyang Yan, Xiaolei Ma", "title": "Learning dynamic and hierarchical traffic spatiotemporal features with\n  Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traffic forecasting is an indispensable part of Intelligent transportation\nsystems (ITS), and long-term network-wide accurate traffic speed forecasting is\none of the most challenging tasks. Recently, deep learning methods have become\npopular in this domain. As traffic data are physically associated with road\nnetworks, most proposed models treat it as a spatiotemporal graph modeling\nproblem and use Graph Convolution Network (GCN) based methods. These GCN-based\nmodels highly depend on a predefined and fixed adjacent matrix to reflect the\nspatial dependency. However, the predefined fixed adjacent matrix is limited in\nreflecting the actual dependence of traffic flow. This paper proposes a novel\nmodel, Traffic Transformer, for spatial-temporal graph modeling and long-term\ntraffic forecasting to overcome these limitations. Transformer is the most\npopular framework in Natural Language Processing (NLP). And by adapting it to\nthe spatiotemporal problem, Traffic Transformer hierarchically extracts\nspatiotemporal features through data dynamically by multi-head attention and\nmasked multi-head attention mechanism, and fuse these features for traffic\nforecasting. Furthermore, analyzing the attention weight matrixes can find the\ninfluential part of road networks, allowing us to learn the traffic networks\nbetter. Experimental results on the public traffic network datasets and\nreal-world traffic network datasets generated by ourselves demonstrate our\nproposed model achieves better performance than the state-of-the-art ones.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 02:29:58 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Yan", "Haoyang", ""], ["Ma", "Xiaolei", ""]]}, {"id": "2104.05188", "submitter": "James Evans", "authors": "Jamshid Sourati, James Evans", "title": "Accelerating science with human versus alien artificial intelligences", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cond-mat.mtrl-sci q-bio.BM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Data-driven artificial intelligence models fed with published scientific\nfindings have been used to create powerful prediction engines for scientific\nand technological advance, such as the discovery of novel materials with\ndesired properties and the targeted invention of new therapies and vaccines.\nThese AI approaches typically ignore the distribution of human prediction\nengines -- scientists and inventor -- who continuously alter the landscape of\ndiscovery and invention. As a result, AI hypotheses are designed to substitute\nfor human experts, failing to complement them for punctuated collective\nadvance. Here we show that incorporating the distribution of human expertise\ninto self-supervised models by training on inferences cognitively available to\nexperts dramatically improves AI prediction of future human discoveries and\ninventions. Including expert-awareness into models that propose (a) valuable\nenergy-relevant materials increases the precision of materials predictions by\n~100%, (b) repurposing thousands of drugs to treat new diseases increases\nprecision by 43%, and (c) COVID-19 vaccine candidates examined in clinical\ntrials by 260%. These models succeed by predicting human predictions and the\nscientists who will make them. By tuning AI to avoid the crowd, however, it\ngenerates scientifically promising \"alien\" hypotheses unlikely to be imagined\nor pursued without intervention, not only accelerating but punctuating\nscientific advance. By identifying and correcting for collective human bias,\nthese models also suggest opportunities to improve human prediction by\nreformulating science education for discovery.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 03:50:30 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Sourati", "Jamshid", ""], ["Evans", "James", ""]]}, {"id": "2104.05190", "submitter": "Jixiang Deng", "authors": "Jixiang Deng, Yong Deng", "title": "QZNs: Quantum Z-numbers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because of the efficiency of modeling fuzziness and vagueness, Z-number plays\nan important role in real practice. However, Z-numbers, defined in the real\nnumber field, lack the ability to process the quantum information in quantum\nenvironment. It is reasonable to generalize Z-number into its quantum\ncounterpart. In this paper, we propose quantum Z-numbers (QZNs), which are the\nquantum generalization of Z-numbers. In addition, seven basic quantum fuzzy\noperations of QZNs and their corresponding quantum circuits are presented and\nillustrated by numerical examples. Moreover, based on QZNs, a novel quantum\nmulti-attributes decision making (MADM) algorithm is proposed and applied in\nmedical diagnosis. The results show that, with the help of quantum computation,\nthe proposed algorithm can make diagnoses correctly and efficiently.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 04:04:05 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Deng", "Jixiang", ""], ["Deng", "Yong", ""]]}, {"id": "2104.05196", "submitter": "Paul Pu Liang", "authors": "Yiwei Lyu, Paul Pu Liang, Hai Pham, Eduard Hovy, Barnab\\'as P\\'oczos,\n  Ruslan Salakhutdinov, Louis-Philippe Morency", "title": "StylePTB: A Compositional Benchmark for Fine-grained Controllable Text\n  Style Transfer", "comments": "NAACL 2021, code available at https://github.com/lvyiwei1/StylePTB/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Text style transfer aims to controllably generate text with targeted\nstylistic changes while maintaining core meaning from the source sentence\nconstant. Many of the existing style transfer benchmarks primarily focus on\nindividual high-level semantic changes (e.g. positive to negative), which\nenable controllability at a high level but do not offer fine-grained control\ninvolving sentence structure, emphasis, and content of the sentence. In this\npaper, we introduce a large-scale benchmark, StylePTB, with (1) paired\nsentences undergoing 21 fine-grained stylistic changes spanning atomic lexical,\nsyntactic, semantic, and thematic transfers of text, as well as (2)\ncompositions of multiple transfers which allow modeling of fine-grained\nstylistic changes as building blocks for more complex, high-level transfers. By\nbenchmarking existing methods on StylePTB, we find that they struggle to model\nfine-grained changes and have an even more difficult time composing multiple\nstyles. As a result, StylePTB brings novel challenges that we hope will\nencourage future research in controllable text style transfer, compositional\nmodels, and learning disentangled representations. Solving these challenges\nwould present important steps towards controllable text generation.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 04:25:09 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Lyu", "Yiwei", ""], ["Liang", "Paul Pu", ""], ["Pham", "Hai", ""], ["Hovy", "Eduard", ""], ["P\u00f3czos", "Barnab\u00e1s", ""], ["Salakhutdinov", "Ruslan", ""], ["Morency", "Louis-Philippe", ""]]}, {"id": "2104.05204", "submitter": "Tianxiang Zhan", "authors": "Tianxiang Zhan, Fuyuan Xiao", "title": "A Fast Evidential Approach for Stock Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.AI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within the framework of evidence theory, the confidence functions of\ndifferent information can be combined into a combined confidence function to\nsolve uncertain problems. The Dempster combination rule is a classic method of\nfusing different information. This paper proposes a similar confidence function\nfor the time point in the time series. The Dempster combination rule can be\nused to fuse the growth rate of the last time point, and finally a relatively\naccurate forecast data can be obtained. Stock price forecasting is a concern of\neconomics. The stock price data is large in volume, and more accurate forecasts\nare required at the same time. The classic methods of time series, such as\nARIMA, cannot balance forecasting efficiency and forecasting accuracy at the\nsame time. In this paper, the fusion method of evidence theory is applied to\nstock price prediction. Evidence theory deals with the uncertainty of stock\nprice prediction and improves the accuracy of prediction. At the same time, the\nfusion method of evidence theory has low time complexity and fast prediction\nprocessing speed.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 04:58:51 GMT"}, {"version": "v2", "created": "Sat, 17 Jul 2021 08:49:12 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Zhan", "Tianxiang", ""], ["Xiao", "Fuyuan", ""]]}, {"id": "2104.05207", "submitter": "Cezary Kaliszyk", "authors": "Liao Zhang, Lasse Blaauwbroek, Bartosz Piotrowski, Prokop \\v{C}ern\\'y,\n  Cezary Kaliszyk, and Josef Urban", "title": "Online Machine Learning Techniques for Coq: A Comparison", "comments": "Intelligent Computer Mathematics 14th International Conference, CICM\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a comparison of several online machine learning techniques for\ntactical learning and proving in the Coq proof assistant. This work builds on\ntop of Tactician, a plugin for Coq that learns from proofs written by the user\nto synthesize new proofs. Learning happens in an online manner, meaning that\nTactician's machine learning model is updated immediately every time the user\nperforms a step in an interactive proof. This has important advantages compared\nto the more studied offline learning systems: (1) it provides the user with a\nseamless, interactive experience with Tactician and, (2) it takes advantage of\nlocality of proof similarity, which means that proofs similar to the current\nproof are likely to be found close by. We implement two online methods, namely\napproximate k-nearest neighbors based on locality sensitive hashing forests and\nrandom decision forests. Additionally, we conduct experiments with gradient\nboosted trees in an offline setting using XGBoost. We compare the relative\nperformance of Tactician using these three learning methods on Coq's standard\nlibrary.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 05:12:35 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 08:11:35 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Zhang", "Liao", ""], ["Blaauwbroek", "Lasse", ""], ["Piotrowski", "Bartosz", ""], ["\u010cern\u00fd", "Prokop", ""], ["Kaliszyk", "Cezary", ""], ["Urban", "Josef", ""]]}, {"id": "2104.05234", "submitter": "Shi Min", "authors": "Cong Li, Min Shi, Bo Qu, Xiang Li", "title": "Deep Attributed Network Representation Learning via Attribute Enhanced\n  Neighborhood", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attributed network representation learning aims at learning node embeddings\nby integrating network structure and attribute information. It is a challenge\nto fully capture the microscopic structure and the attribute semantics\nsimultaneously, where the microscopic structure includes the one-step, two-step\nand multi-step relations, indicating the first-order, second-order and\nhigh-order proximity of nodes, respectively. In this paper, we propose a deep\nattributed network representation learning via attribute enhanced neighborhood\n(DANRL-ANE) model to improve the robustness and effectiveness of node\nrepresentations. The DANRL-ANE model adopts the idea of the autoencoder, and\nexpands the decoder component to three branches to capture different order\nproximity. We linearly combine the adjacency matrix with the attribute\nsimilarity matrix as the input of our model, where the attribute similarity\nmatrix is calculated by the cosine similarity between the attributes based on\nthe social homophily. In this way, we preserve the second-order proximity to\nenhance the robustness of DANRL-ANE model on sparse networks, and deal with the\ntopological and attribute information simultaneously. Moreover, the sigmoid\ncross-entropy loss function is extended to capture the neighborhood character,\nso that the first-order proximity is better preserved. We compare our model\nwith the state-of-the-art models on five real-world datasets and two network\nanalysis tasks, i.e., link prediction and node classification. The DANRL-ANE\nmodel performs well on various networks, even on sparse networks or networks\nwith isolated nodes given the attribute information is sufficient.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 07:03:16 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Li", "Cong", ""], ["Shi", "Min", ""], ["Qu", "Bo", ""], ["Li", "Xiang", ""]]}, {"id": "2104.05235", "submitter": "Km  Poonam", "authors": "Km Poonam, Rajlakshmi Guha, Partha P Chakrabarti", "title": "Artificial Intelligence Methods Based Hierarchical Classification of\n  Frontotemporal Dementia to Improve Diagnostic Predictability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Patients with Frontotemporal Dementia (FTD) have impaired cognitive\nabilities, executive and behavioral traits, loss of language ability, and\ndecreased memory capabilities. Based on the distinct patterns of cortical\natrophy and symptoms, the FTD spectrum primarily includes three variants:\nbehavioral variant FTD (bvFTD), non-fluent variant primary progressive aphasia\n(nfvPPA), and semantic variant primary progressive aphasia (svPPA). The purpose\nof this study is to classify MRI images of every single subject into one of the\nspectrums of the FTD in a hierarchical order by applying data-driven techniques\nof Artificial Intelligence (AI) on cortical thickness data. This data is\ncomputed by FreeSurfer software. We used the Smallest Univalue Segment\nAssimilating Nucleus (SUSAN) technique to minimize the noise in cortical\nthickness data. Specifically, we took 204 subjects from the frontotemporal\nlobar degeneration neuroimaging initiative (NIFTD) database to validate this\napproach, and each subject was diagnosed in one of the diagnostic categories\n(bvFTD, svPPA, nfvPPA and cognitively normal). Our proposed automated\nclassification model yielded classification accuracy of 86.5, 76, and 72.7 with\nsupport vector machine (SVM), linear discriminant analysis (LDA), and Naive\nBayes methods, respectively, in 10-fold cross-validation analysis, which is a\nsignificant improvement on a traditional single multi-class model with an\naccuracy of 82.7, 73.4, and 69.2.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 07:04:11 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Poonam", "Km", ""], ["Guha", "Rajlakshmi", ""], ["Chakrabarti", "Partha P", ""]]}, {"id": "2104.05243", "submitter": "Nayeon Lee", "authors": "Nayeon Lee, Belinda Z. Li, Sinong Wang, Pascale Fung, Hao Ma, Wen-tau\n  Yih, Madian Khabsa", "title": "On Unifying Misinformation Detection", "comments": "Accepted to NAACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we introduce UnifiedM2, a general-purpose misinformation model\nthat jointly models multiple domains of misinformation with a single, unified\nsetup. The model is trained to handle four tasks: detecting news bias,\nclickbait, fake news, and verifying rumors. By grouping these tasks together,\nUnifiedM2learns a richer representation of misinformation, which leads to\nstate-of-the-art or comparable performance across all tasks. Furthermore, we\ndemonstrate that UnifiedM2's learned representation is helpful for few-shot\nlearning of unseen misinformation tasks/datasets and model's generalizability\nto unseen events.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 07:25:49 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Lee", "Nayeon", ""], ["Li", "Belinda Z.", ""], ["Wang", "Sinong", ""], ["Fung", "Pascale", ""], ["Ma", "Hao", ""], ["Yih", "Wen-tau", ""], ["Khabsa", "Madian", ""]]}, {"id": "2104.05252", "submitter": "Victor Berger", "authors": "Victor Berger (TAU), Michele Sebag (TAU)", "title": "Boltzmann Tuning of Generative Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper focuses on the a posteriori tuning of a generative model in order\nto favor the generation of good instances in the sense of some external\ndifferentiable criterion. The proposed approach, called Boltzmann Tuning of\nGenerative Models (BTGM), applies to a wide range of applications. It covers\nconditional generative modelling as a particular case, and offers an affordable\nalternative to rejection sampling. The contribution of the paper is twofold.\nFirstly, the objective is formalized and tackled as a well-posed optimization\nproblem; a practical methodology is proposed to choose among the candidate\ncriteria representing the same goal, the one best suited to efficiently learn a\ntuned generative model. Secondly, the merits of the approach are demonstrated\non a real-world application, in the context of robust design for energy\npolicies, showing the ability of BTGM to sample the extreme regions of the\nconsidered criteria.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 07:35:27 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Berger", "Victor", "", "TAU"], ["Sebag", "Michele", "", "TAU"]]}, {"id": "2104.05261", "submitter": "Sebastian G\\\"undel", "authors": "Sebastian G\\\"undel, Arnaud A. A. Setio, Florin C. Ghesu, Sasa Grbic,\n  Bogdan Georgescu, Andreas Maier, Dorin Comaniciu", "title": "Robust Classification from Noisy Labels: Integrating Additional\n  Knowledge for Chest Radiography Abnormality Assessment", "comments": "Accepted in Medical Image Analysis (MedIA). arXiv admin note: text\n  overlap with arXiv:1905.06362", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Chest radiography is the most common radiographic examination performed in\ndaily clinical practice for the detection of various heart and lung\nabnormalities. The large amount of data to be read and reported, with more than\n100 studies per day for a single radiologist, poses a challenge in consistently\nmaintaining high interpretation accuracy. The introduction of large-scale\npublic datasets has led to a series of novel systems for automated abnormality\nclassification. However, the labels of these datasets were obtained using\nnatural language processed medical reports, yielding a large degree of label\nnoise that can impact the performance. In this study, we propose novel training\nstrategies that handle label noise from such suboptimal data. Prior label\nprobabilities were measured on a subset of training data re-read by 4\nboard-certified radiologists and were used during training to increase the\nrobustness of the training model to the label noise. Furthermore, we exploit\nthe high comorbidity of abnormalities observed in chest radiography and\nincorporate this information to further reduce the impact of label noise.\nAdditionally, anatomical knowledge is incorporated by training the system to\npredict lung and heart segmentation, as well as spatial knowledge labels. To\ndeal with multiple datasets and images derived from various scanners that apply\ndifferent post-processing techniques, we introduce a novel image normalization\nstrategy. Experiments were performed on an extensive collection of 297,541\nchest radiographs from 86,876 patients, leading to a state-of-the-art\nperformance level for 17 abnormalities from 2 datasets. With an average AUC\nscore of 0.880 across all abnormalities, our proposed training strategies can\nbe used to significantly improve performance scores.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 07:51:07 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 19:43:52 GMT"}, {"version": "v3", "created": "Wed, 21 Apr 2021 09:07:00 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["G\u00fcndel", "Sebastian", ""], ["Setio", "Arnaud A. A.", ""], ["Ghesu", "Florin C.", ""], ["Grbic", "Sasa", ""], ["Georgescu", "Bogdan", ""], ["Maier", "Andreas", ""], ["Comaniciu", "Dorin", ""]]}, {"id": "2104.05286", "submitter": "Georgios Mylonas", "authors": "Dimitrios Amaxilatis, Georgios Mylonas, Evangelos Theodoridis, Luis\n  Diez, Katerina Deligiannidou", "title": "LearningCity: Knowledge Generation for Smart Cities", "comments": "Preprint of chapter submitted to \"Smart Cities Performability,\n  Cognition, & Security\". EAI/Springer Innovations in Communication and\n  Computing. Springer, Cham. arXiv admin note: text overlap with\n  arXiv:2103.16998", "journal-ref": null, "doi": "10.1007/978-3-030-14718-1_2", "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although we have reached new levels in smart city installations and systems,\nefforts so far have focused on providing diverse sources of data to smart city\nservices consumers while neglecting to provide ways to simplify making good use\nof them. In this context, one first step that will bring added value to smart\ncities is knowledge creation in smart cities through anomaly detection and data\nannotation, supported in both an automated and a crowdsourced manner. We\npresent here LearningCity, our solution that has been validated over an\nexisting smart city deployment in Santander, and the OrganiCity\nexperimentation-as-a-service ecosystem. We discuss key challenges along with\ncharacteristic use cases, and report on our design and implementation, together\nwith some preliminary results derived from combining large smart city datasets\nwith machine learning.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 08:31:10 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Amaxilatis", "Dimitrios", ""], ["Mylonas", "Georgios", ""], ["Theodoridis", "Evangelos", ""], ["Diez", "Luis", ""], ["Deligiannidou", "Katerina", ""]]}, {"id": "2104.05314", "submitter": "Christian Janiesch", "authors": "Christian Janiesch, Patrick Zschech, Kai Heinrich", "title": "Machine learning and deep learning", "comments": "Published online first in Electronic Markets", "journal-ref": null, "doi": "10.1007/s12525-021-00475-2", "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Today, intelligent systems that offer artificial intelligence capabilities\noften rely on machine learning. Machine learning describes the capacity of\nsystems to learn from problem-specific training data to automate the process of\nanalytical model building and solve associated tasks. Deep learning is a\nmachine learning concept based on artificial neural networks. For many\napplications, deep learning models outperform shallow machine learning models\nand traditional data analysis approaches. In this article, we summarize the\nfundamentals of machine learning and deep learning to generate a broader\nunderstanding of the methodical underpinning of current intelligent systems. In\nparticular, we provide a conceptual distinction between relevant terms and\nconcepts, explain the process of automated analytical model building through\nmachine learning and deep learning, and discuss the challenges that arise when\nimplementing such intelligent systems in the field of electronic markets and\nnetworked business. These naturally go beyond technological aspects and\nhighlight issues in human-machine interaction and artificial intelligence\nservitization.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 09:54:12 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 10:31:01 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Janiesch", "Christian", ""], ["Zschech", "Patrick", ""], ["Heinrich", "Kai", ""]]}, {"id": "2104.05331", "submitter": "Rushil Thareja", "authors": "Rushil Thareja", "title": "MeToo Tweets Sentiment Analysis Using Multi Modal frameworks", "comments": "This is a good introductory paper , for those interested in multi\n  modal frameworks . Dont forget to cite :)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, We present our approach for IEEEBigMM 2020, Grand Challenge\n(BMGC), Identifying senti-ments from tweets related to the MeToo movement. The\nmodelis based on an ensemble of Convolutional Neural Network,Bidirectional LSTM\nand a DNN for final classification. Thispaper is aimed at providing a detailed\nanalysis of the modeland the results obtained. We have ranked 5th out of 10\nteamswith a score of 0.51491\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 10:18:33 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Thareja", "Rushil", ""]]}, {"id": "2104.05374", "submitter": "Hongbin Xu", "authors": "Hongbin Xu, Zhipeng Zhou, Yu Qiao, Wenxiong Kang, Qiuxia Wu", "title": "Self-supervised Multi-view Stereo via Effective Co-Segmentation and\n  Data-Augmentation", "comments": "This paper is accepted by AAAI-21 with a Distinguished Paper Award", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have witnessed that self-supervised methods based on view\nsynthesis obtain clear progress on multi-view stereo (MVS). However, existing\nmethods rely on the assumption that the corresponding points among different\nviews share the same color, which may not always be true in practice. This may\nlead to unreliable self-supervised signal and harm the final reconstruction\nperformance. To address the issue, we propose a framework integrated with more\nreliable supervision guided by semantic co-segmentation and data-augmentation.\nSpecially, we excavate mutual semantic from multi-view images to guide the\nsemantic consistency. And we devise effective data-augmentation mechanism which\nensures the transformation robustness by treating the prediction of regular\nsamples as pseudo ground truth to regularize the prediction of augmented\nsamples. Experimental results on DTU dataset show that our proposed methods\nachieve the state-of-the-art performance among unsupervised methods, and even\ncompete on par with supervised methods. Furthermore, extensive experiments on\nTanks&Temples dataset demonstrate the effective generalization ability of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 11:48:54 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Xu", "Hongbin", ""], ["Zhou", "Zhipeng", ""], ["Qiao", "Yu", ""], ["Kang", "Wenxiong", ""], ["Wu", "Qiuxia", ""]]}, {"id": "2104.05407", "submitter": "Vladimir Ivanov", "authors": "V. K. Ivanov, I. V. Obraztsov, B. V. Palyukh", "title": "Implementing an expert system to evaluate technical solutions\n  innovativeness", "comments": "12 pages, in Russian", "journal-ref": "Software & Systems. 2019. T. 4 (32)", "doi": "10.15827/0236-235X.128.696-707", "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The paper presents a possible solution to the problem of algorithmization for\nquantifying inno-vativeness indicators of technical products, inventions and\ntechnologies. The concepts of technological nov-elty, relevance and\nimplementability as components of product innovation criterion are introduced.\nAuthors propose a model and algorithm to calculate every of these indicators of\ninnovativeness under conditions of incompleteness and inaccuracy, and sometimes\ninconsistency of the initial information. The paper describes the developed\nspecialized software that is a promising methodological tool for using interval\nestimations in accordance with the theory of evidence. These estimations are\nused in the analysis of complex multicomponent systems, aggregations of large\nvolumes of fuzzy and incomplete data of various structures. Composition and\nstructure of a multi-agent expert system are presented. The purpose of such\nsystem is to process groups of measurement results and to estimate indicators\nvalues of objects innovativeness. The paper defines active elements of the\nsystem, their functionality, roles, interaction order, input and output\ninter-faces, as well as the general software functioning algorithm. It\ndescribes implementation of software modules and gives an example of solving a\nspecific problem to determine the level of technical products innovation.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 10:11:44 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Ivanov", "V. K.", ""], ["Obraztsov", "I. V.", ""], ["Palyukh", "B. V.", ""]]}, {"id": "2104.05416", "submitter": "Yuanpeng He", "authors": "Yuanpeng He", "title": "An approach utilizing negation of extended-dimensional vector of\n  disposing mass for ordinal evidences combination in a fuzzy environment", "comments": "28 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to measure the degree of uncertainty of a given frame of discernment has\nbeen a hot topic for years. A lot of meaningful works have provided some\neffective methods to measure the degree properly. However, a crucial factor,\nsequence of propositions, is missing in the definition of traditional frame of\ndiscernment. In this paper, a detailed definition of ordinal frame of\ndiscernment has been provided. Besides, an innovative method utilizing a\nconcept of computer vision to combine the order of propositions and the mass of\nthem is proposed to better manifest relationships between the two important\nelement of the frame of discernment. More than that, a specially designed\nmethod covering some powerful tools in indicating the degree of uncertainty of\na traditional frame of discernment is also offered to give an indicator of\nlevel of uncertainty of an ordinal frame of discernment on the level of vector.\n", "versions": [{"version": "v1", "created": "Tue, 6 Apr 2021 09:35:29 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["He", "Yuanpeng", ""]]}, {"id": "2104.05417", "submitter": "Kevin Brol{\\o}s", "authors": "Kevin Ren\\'e Brol{\\o}s, Meera Vieira Machado, Chris Cave, Jaan Kasak,\n  Valdemar Stentoft-Hansen, Victor Galindo Batanero, Tom Jelen, Casper Wilstrup", "title": "An Approach to Symbolic Regression Using Feyn", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.ET", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article we introduce the supervised machine learning tool called\nFeyn. The simulation engine that powers this tool is called the QLattice. The\nQLattice is a supervised machine learning tool inspired by Richard Feynman's\npath integral formulation, that explores many potential models that solves a\ngiven problem. It formulates these models as graphs that can be interpreted as\nmathematical equations, allowing the user to completely decide on the trade-off\nbetween interpretability, complexity and model performance.\n  We touch briefly upon the inner workings of the QLattice, and show how to\napply the python package, Feyn, to scientific problems. We show how it differs\nfrom traditional machine learning approaches, what it has in common with them,\nas well as some of its commonalities with symbolic regression. We describe the\nbenefits of this approach as opposed to black box models.\n  To illustrate this, we go through an investigative workflow using a basic\ndata set and show how the QLattice can help you reason about the relationships\nbetween your features and do data discovery.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 12:50:50 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Brol\u00f8s", "Kevin Ren\u00e9", ""], ["Machado", "Meera Vieira", ""], ["Cave", "Chris", ""], ["Kasak", "Jaan", ""], ["Stentoft-Hansen", "Valdemar", ""], ["Batanero", "Victor Galindo", ""], ["Jelen", "Tom", ""], ["Wilstrup", "Casper", ""]]}, {"id": "2104.05421", "submitter": "Mahdi Nazemi", "authors": "Mahdi Nazemi, Arash Fayyazi, Amirhossein Esmaili, Atharva Khare,\n  Soheil Nazar Shahsavani, and Massoud Pedram", "title": "NullaNet Tiny: Ultra-low-latency DNN Inference Through Fixed-function\n  Combinational Logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While there is a large body of research on efficient processing of deep\nneural networks (DNNs), ultra-low-latency realization of these models for\napplications with stringent, sub-microsecond latency requirements continues to\nbe an unresolved, challenging problem. Field-programmable gate array\n(FPGA)-based DNN accelerators are gaining traction as a serious contender to\nreplace graphics processing unit/central processing unit-based platforms\nconsidering their performance, flexibility, and energy efficiency. This paper\npresents NullaNet Tiny, an across-the-stack design and optimization framework\nfor constructing resource and energy-efficient, ultra-low-latency FPGA-based\nneural network accelerators. The key idea is to replace expensive operations\nrequired to compute various filter/neuron functions in a DNN with Boolean logic\nexpressions that are mapped to the native look-up tables (LUTs) of the FPGA\ndevice (examples of such operations are multiply-and-accumulate and batch\nnormalization). At about the same level of classification accuracy, compared to\nXilinx's LogicNets, our design achieves 2.36$\\times$ lower latency and\n24.42$\\times$ lower LUT utilization.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 00:16:39 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Nazemi", "Mahdi", ""], ["Fayyazi", "Arash", ""], ["Esmaili", "Amirhossein", ""], ["Khare", "Atharva", ""], ["Shahsavani", "Soheil Nazar", ""], ["Pedram", "Massoud", ""]]}, {"id": "2104.05422", "submitter": "Stefan Edelkamp", "authors": "Stefan Edelkamp", "title": "ELO System for Skat and Other Games of Chance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Assessing the skill level of players to predict the outcome and to rank the\nplayers in a longer series of games is of critical importance for tournament\nplay. Besides weaknesses, like an observed continuous inflation, through a\nsteadily increasing playing body, the ELO ranking system, named after its\ncreator Arpad Elo, has proven to be a reliable method for calculating the\nrelative skill levels of players in zero-sum games.\n  The evaluation of player strength in trick-taking card games like Skat or\nBridge, however, is not obvious. Firstly, these are incomplete information\npartially observable games with more than one player, where opponent strength\nshould influence the scoring as it does in existing ELO systems. Secondly, they\nare game of both skill and chance, so that besides the playing strength the\noutcome of a game also depends on the deal. Last but not least, there are\ninternationally established scoring systems, in which the players are used to\nbe evaluated, and to which ELO should align. Based on a tournament scoring\nsystem, we propose a new ELO system for Skat to overcome these weaknesses.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 08:30:01 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Edelkamp", "Stefan", ""]]}, {"id": "2104.05423", "submitter": "Stefan Edelkamp", "authors": "Stefan Edelkamp", "title": "Knowledge-Based Paranoia Search in Trick-Taking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes \\emph{knowledge-based paraonoia search} (KBPS) to find\nforced wins during trick-taking in the card game Skat; for some one of the most\ninteresting card games for three players. It combines efficient partial\ninformation game-tree search with knowledge representation and reasoning. This\nworst-case analysis, initiated after a small number of tricks, leads to a\nprioritized choice of cards. We provide variants of KBPS for the declarer and\nthe opponents, and an approximation to find a forced win against most worlds in\nthe belief space. Replaying thousands of expert games, our evaluation indicates\nthat the AIs with the new algorithms perform better than humans in their play,\nachieving an average score of over 1,000 points in the agreed standard for\nevaluating Skat tournaments, the extended Seeger system.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 09:12:45 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Edelkamp", "Stefan", ""]]}, {"id": "2104.05432", "submitter": "Nils Gumpfer", "authors": "Michael Guckert, Nils Gumpfer, Jennifer Hannig, Till Keller and Neil\n  Urquhart", "title": "A Conceptual Framework for Establishing Trust in Real World Intelligent\n  Systems", "comments": null, "journal-ref": null, "doi": "10.1016/j.cogsys.2021.04.001", "report-no": null, "categories": "cs.CY cs.AI cs.GT cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent information systems that contain emergent elements often\nencounter trust problems because results do not get sufficiently explained and\nthe procedure itself can not be fully retraced. This is caused by a control\nflow depending either on stochastic elements or on the structure and relevance\nof the input data. Trust in such algorithms can be established by letting users\ninteract with the system so that they can explore results and find patterns\nthat can be compared with their expected solution. Reflecting features and\npatterns of human understanding of a domain against algorithmic results can\ncreate awareness of such patterns and may increase the trust that a user has in\nthe solution. If expectations are not met, close inspection can be used to\ndecide whether a solution conforms to the expectations or whether it goes\nbeyond the expected. By either accepting or rejecting a solution, the user's\nset of expectations evolves and a learning process for the users is\nestablished. In this paper we present a conceptual framework that reflects and\nsupports this process. The framework is the result of an analysis of two\nexemplary case studies from two different disciplines with information systems\nthat assist experts in their complex tasks.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 12:58:47 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Guckert", "Michael", ""], ["Gumpfer", "Nils", ""], ["Hannig", "Jennifer", ""], ["Keller", "Till", ""], ["Urquhart", "Neil", ""]]}, {"id": "2104.05467", "submitter": "Thibault Laugel", "authors": "Xavier Renard, Thibault Laugel, Marcin Detyniecki", "title": "Understanding Prediction Discrepancies in Machine Learning Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A multitude of classifiers can be trained on the same data to achieve similar\nperformances during test time, while having learned significantly different\nclassification patterns. This phenomenon, which we call prediction\ndiscrepancies, is often associated with the blind selection of one model\ninstead of another with similar performances. When making a choice, the machine\nlearning practitioner has no understanding on the differences between models,\ntheir limits, where they agree and where they don't. But his/her choice will\nresult in concrete consequences for instances to be classified in the\ndiscrepancy zone, since the final decision will be based on the selected\nclassification pattern. Besides the arbitrary nature of the result, a bad\nchoice could have further negative consequences such as loss of opportunity or\nlack of fairness. This paper proposes to address this question by analyzing the\nprediction discrepancies in a pool of best-performing models trained on the\nsame data. A model-agnostic algorithm, DIG, is proposed to capture and explain\ndiscrepancies locally, to enable the practitioner to make the best educated\ndecision when selecting a model by anticipating its potential undesired\nconsequences. All the code to reproduce the experiments is available.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 13:42:50 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Renard", "Xavier", ""], ["Laugel", "Thibault", ""], ["Detyniecki", "Marcin", ""]]}, {"id": "2104.05470", "submitter": "Yuan Shen", "authors": "Yuan Shen and Niviru Wijayaratne and Katherine Driggs-Campbell", "title": "Building Mental Models through Preview of Autopilot Behaviors", "comments": "in TRAITS Workshop Proceedings (arXiv:2103.12679) held in conjunction\n  with Companion of the 2021 ACM/IEEE International Conference on Human-Robot\n  Interaction, March 2021, Pages 709-711", "journal-ref": null, "doi": null, "report-no": "TRAITS/2021/03", "categories": "cs.HC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Effective human-vehicle collaboration requires an appropriate un-derstanding\nof vehicle behavior for safety and trust. Improvingon our prior work by adding\na future prediction module, we in-troduce our framework, calledAutoPreview, to\nenable humans topreview autopilot behaviors prior to direct interaction with\nthevehicle. Previewing autopilot behavior can help to ensure\nsmoothhuman-vehicle collaboration during the initial exploration stagewith the\nvehicle. To demonstrate its practicality, we conducted acase study on\nhuman-vehicle collaboration and built a prototypeof our framework with the\nCARLA simulator. Additionally, weconducted a between-subject control experiment\n(n=10) to studywhether ourAutoPreviewframework can provide a deeper\nunder-standing of autopilot behavior compared to direct interaction. Ourresults\nsuggest that theAutoPreviewframework does, in fact, helpusers understand\nautopilot behavior and develop appropriate men-tal models\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 13:46:55 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Shen", "Yuan", ""], ["Wijayaratne", "Niviru", ""], ["Driggs-Campbell", "Katherine", ""]]}, {"id": "2104.05489", "submitter": "Yanzhe Zhang", "authors": "Yufan Huang, Yanzhe Zhang, Jiaao Chen, Xuezhi Wang and Diyi Yang", "title": "Continual Learning for Text Classification with Information\n  Disentanglement Based Regularization", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continual learning has become increasingly important as it enables NLP models\nto constantly learn and gain knowledge over time. Previous continual learning\nmethods are mainly designed to preserve knowledge from previous tasks, without\nmuch emphasis on how to well generalize models to new tasks. In this work, we\npropose an information disentanglement based regularization method for\ncontinual learning on text classification. Our proposed method first\ndisentangles text hidden spaces into representations that are generic to all\ntasks and representations specific to each individual task, and further\nregularizes these representations differently to better constrain the knowledge\nrequired to generalize. We also introduce two simple auxiliary tasks: next\nsentence prediction and task-id prediction, for learning better generic and\nspecific representation spaces. Experiments conducted on large-scale benchmarks\ndemonstrate the effectiveness of our method in continual text classification\ntasks with various sequences and lengths over state-of-the-art baselines. We\nhave publicly released our code at https://github.com/GT-SALT/IDBR.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 14:17:43 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 00:33:19 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Huang", "Yufan", ""], ["Zhang", "Yanzhe", ""], ["Chen", "Jiaao", ""], ["Wang", "Xuezhi", ""], ["Yang", "Diyi", ""]]}, {"id": "2104.05500", "submitter": "Arseny Moskvichev", "authors": "Arseny Moskvichev, James A. Liu", "title": "Updater-Extractor Architecture for Inductive World State Representations", "comments": "15 pages (12 main content, 3 references and appendix), 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Developing NLP models traditionally involves two stages - training and\napplication. Retention of information acquired after training (at application\ntime) is architecturally limited by the size of the model's context window (in\nthe case of transformers), or by the practical difficulties associated with\nlong sequences (in the case of RNNs). In this paper, we propose a novel\ntransformer-based Updater-Extractor architecture and a training procedure that\ncan work with sequences of arbitrary length and refine its knowledge about the\nworld based on linguistic inputs. We explicitly train the model to incorporate\nincoming information into its world state representation, obtaining strong\ninductive generalization and the ability to handle extremely long-range\ndependencies. We prove a lemma that provides a theoretical basis for our\napproach. The result also provides insight into success and failure modes of\nmodels trained with variants of Truncated Back-Propagation Through Time (such\nas Transformer XL). Empirically, we investigate the model performance on three\ndifferent tasks, demonstrating its promise. This preprint is still a work in\nprogress. At present, we focused on easily interpretable tasks, leaving the\napplication of the proposed ideas to practical NLP applications for the future.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 14:30:11 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Moskvichev", "Arseny", ""], ["Liu", "James A.", ""]]}, {"id": "2104.05504", "submitter": "Xiquan Cui", "authors": "Rebecca West, Khalifeh Al Jadda, Unaiza Ahsan, Huiming Qu, Xiquan Cui", "title": "Interpretable Methods for Identifying Product Variants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For e-commerce companies with large product selections, the organization and\ngrouping of products in meaningful ways is important for creating great\ncustomer shopping experiences and cultivating an authoritative brand image. One\nimportant way of grouping products is to identify a family of product variants,\nwhere the variants are mostly the same with slight and yet distinct differences\n(e.g. color or pack size). In this paper, we introduce a novel approach to\nidentifying product variants. It combines both constrained clustering and\ntailored NLP techniques (e.g. extraction of product family name from\nunstructured product title and identification of products with similar model\nnumbers) to achieve superior performance compared with an existing baseline\nusing a vanilla classification approach. In addition, we design the algorithm\nto meet certain business criteria, including meeting high accuracy requirements\non a wide range of categories (e.g. appliances, decor, tools, and building\nmaterials, etc.) as well as prioritizing the interpretability of the model to\nmake it accessible and understandable to all business partners.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 14:37:16 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["West", "Rebecca", ""], ["Jadda", "Khalifeh Al", ""], ["Ahsan", "Unaiza", ""], ["Qu", "Huiming", ""], ["Cui", "Xiquan", ""]]}, {"id": "2104.05522", "submitter": "Kin Gutierrez Olivares", "authors": "Kin G. Olivares and Cristian Challu and Grzegorz Marcjasz and Rafa{\\l}\n  Weron and Artur Dubrawski", "title": "Neural basis expansion analysis with exogenous variables: Forecasting\n  electricity prices with NBEATSx", "comments": "30 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the neural basis expansion analysis (NBEATS) to incorporate\nexogenous factors. The resulting method, called NBEATSx, improves on a well\nperforming deep learning model, extending its capabilities by including\nexogenous variables and allowing it to integrate multiple sources of useful\ninformation. To showcase the utility of the NBEATSx model, we conduct a\ncomprehensive study of its application to electricity price forecasting (EPF)\ntasks across a broad range of years and markets. We observe state-of-the-art\nperformance, significantly improving the forecast accuracy by nearly 20% over\nthe original NBEATS model, and by up to 5% over other well established\nstatistical and machine learning methods specialized for these tasks.\nAdditionally, the proposed neural network has an interpretable configuration\nthat can structurally decompose time series, visualizing the relative impact of\ntrend and seasonal components and revealing the modeled processes' interactions\nwith exogenous factors. To assist related work we made the code available in\nhttps://github.com/cchallu/nbeatsx.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 14:47:55 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 14:36:36 GMT"}, {"version": "v3", "created": "Wed, 21 Apr 2021 20:38:24 GMT"}, {"version": "v4", "created": "Fri, 23 Apr 2021 12:48:00 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Olivares", "Kin G.", ""], ["Challu", "Cristian", ""], ["Marcjasz", "Grzegorz", ""], ["Weron", "Rafa\u0142", ""], ["Dubrawski", "Artur", ""]]}, {"id": "2104.05527", "submitter": "An Zhang", "authors": "An Zhang, Xiang Wang, Chengfang Fang, Jie Shi, Tat-seng Chua, Zehua\n  Chen", "title": "A-FMI: Learning Attributions from Deep Networks via Feature Map\n  Importance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Gradient-based attribution methods can aid in the understanding of\nconvolutional neural networks (CNNs). However, the redundancy of attribution\nfeatures and the gradient saturation problem, which weaken the ability to\nidentify significant features and cause an explanation focus shift, are\nchallenges that attribution methods still face. In this work, we propose: 1) an\nessential characteristic, Strong Relevance, when selecting attribution\nfeatures; 2) a new concept, feature map importance (FMI), to refine the\ncontribution of each feature map, which is faithful to the CNN model; and 3) a\nnovel attribution method via FMI, termed A-FMI, to address the gradient\nsaturation problem, which couples the target image with a reference image, and\nassigns the FMI to the difference-from-reference at the granularity of feature\nmap. Through visual inspections and qualitative evaluations on the ImageNet\ndataset, we show the compelling advantages of A-FMI on its faithfulness,\ninsensitivity to the choice of reference, class discriminability, and superior\nexplanation performance compared with popular attribution methods across\nvarying CNN architectures.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 14:54:44 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Zhang", "An", ""], ["Wang", "Xiang", ""], ["Fang", "Chengfang", ""], ["Shi", "Jie", ""], ["Chua", "Tat-seng", ""], ["Chen", "Zehua", ""]]}, {"id": "2104.05565", "submitter": "Victor Uc-Cetina", "authors": "Victor Uc-Cetina, Nicolas Navarro-Guerrero, Anabel Martin-Gonzalez,\n  Cornelius Weber, Stefan Wermter", "title": "Survey on reinforcement learning for language processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years some researchers have explored the use of reinforcement\nlearning (RL) algorithms as key components in the solution of various natural\nlanguage processing tasks. For instance, some of these algorithms leveraging\ndeep neural learning have found their way into conversational systems. This\npaper reviews the state of the art of RL methods for their possible use for\ndifferent problems of natural language processing, focusing primarily on\nconversational systems, mainly due to their growing relevance. We provide\ndetailed descriptions of the problems as well as discussions of why RL is\nwell-suited to solve them. Also, we analyze the advantages and limitations of\nthese methods. Finally, we elaborate on promising research directions in\nnatural language processing that might benefit from reinforcement learning.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 15:33:11 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Uc-Cetina", "Victor", ""], ["Navarro-Guerrero", "Nicolas", ""], ["Martin-Gonzalez", "Anabel", ""], ["Weber", "Cornelius", ""], ["Wermter", "Stefan", ""]]}, {"id": "2104.05570", "submitter": "Bowen Li", "authors": "Bowen Li, Xinping Ren, Ke Yan, Le Lu, Guotong Xie, Jing Xiao, Dar-In\n  Tai, Adam P. Harrison", "title": "Learning from Subjective Ratings Using Auto-Decoded Deep Latent\n  Embeddings", "comments": "Main body includes 10 pages and 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Depending on the application, radiological diagnoses can be associated with\nhigh inter- and intra-rater variabilities. Most computer-aided diagnosis (CAD)\nsolutions treat such data as incontrovertible, exposing learning algorithms to\nconsiderable and possibly contradictory label noise and biases. Thus, managing\nsubjectivity in labels is a fundamental problem in medical imaging analysis. To\naddress this challenge, we introduce auto-decoded deep latent embeddings\n(ADDLE), which explicitly models the tendencies of each rater using an\nauto-decoder framework. After a simple linear transformation, the latent\nvariables can be injected into any backbone at any and multiple points,\nallowing the model to account for rater-specific effects on the diagnosis.\nImportantly, ADDLE does not expect multiple raters per image in training,\nmeaning it can readily learn from data mined from hospital archives. Moreover,\nthe complexity of training ADDLE does not increase as more raters are added.\nDuring inference each rater can be simulated and a 'mean' or 'greedy' virtual\nrating can be produced. We test ADDLE on the problem of liver steatosis\ndiagnosis from 2D ultrasound (US) by collecting 46 084 studies along with\nclinical US diagnoses originating from 65 different raters. We evaluated\ndiagnostic performance using a separate dataset with gold-standard biopsy\ndiagnoses. ADDLE can improve the partial areas under the curve (AUCs) for\ndiagnosing severe steatosis by 10.5% over standard classifiers while\noutperforming other annotator-noise approaches, including those requiring 65\ntimes the parameters.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 15:40:42 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2021 17:53:38 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Li", "Bowen", ""], ["Ren", "Xinping", ""], ["Yan", "Ke", ""], ["Lu", "Le", ""], ["Xie", "Guotong", ""], ["Xiao", "Jing", ""], ["Tai", "Dar-In", ""], ["Harrison", "Adam P.", ""]]}, {"id": "2104.05592", "submitter": "Philip Naumann", "authors": "Philip Naumann and Eirini Ntoutsi", "title": "Consequence-aware Sequential Counterfactual Generation", "comments": "16 pages, 6 figures, Accepted for publication in the research track\n  at ECML-PKDD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Counterfactuals have become a popular technique nowadays for interacting with\nblack-box machine learning models and understanding how to change a particular\ninstance to obtain a desired outcome from the model. However, most existing\napproaches assume instant materialization of these changes, ignoring that they\nmay require effort and a specific order of application. Recently, methods have\nbeen proposed that also consider the order in which actions are applied,\nleading to the so-called sequential counterfactual generation problem.\n  In this work, we propose a model-agnostic method for sequential\ncounterfactual generation. We formulate the task as a multi-objective\noptimization problem and present a genetic algorithm approach to find optimal\nsequences of actions leading to the counterfactuals. Our cost model considers\nnot only the direct effect of an action, but also its consequences.\nExperimental results show that compared to state-of-the-art, our approach\ngenerates less costly solutions, is more efficient and provides the user with a\ndiverse set of solutions to choose from.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 16:10:03 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 15:41:39 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Naumann", "Philip", ""], ["Ntoutsi", "Eirini", ""]]}, {"id": "2104.05632", "submitter": "Philip Ball", "authors": "Philip J. Ball, Cong Lu, Jack Parker-Holder, Stephen Roberts", "title": "Augmented World Models Facilitate Zero-Shot Dynamics Generalization From\n  a Single Offline Environment", "comments": "Accepted @ ICML 2021; Spotlight @ ICLR 2021 \"Self-Supervision for\n  Reinforcement Learning Workshop\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reinforcement learning from large-scale offline datasets provides us with the\nability to learn policies without potentially unsafe or impractical\nexploration. Significant progress has been made in the past few years in\ndealing with the challenge of correcting for differing behavior between the\ndata collection and learned policies. However, little attention has been paid\nto potentially changing dynamics when transferring a policy to the online\nsetting, where performance can be up to 90% reduced for existing methods. In\nthis paper we address this problem with Augmented World Models (AugWM). We\naugment a learned dynamics model with simple transformations that seek to\ncapture potential changes in physical properties of the robot, leading to more\nrobust policies. We not only train our policy in this new setting, but also\nprovide it with the sampled augmentation as a context, allowing it to adapt to\nchanges in the environment. At test time we learn the context in a\nself-supervised fashion by approximating the augmentation which corresponds to\nthe new environment. We rigorously evaluate our approach on over 100 different\nchanged dynamics settings, and show that this simple approach can significantly\nimprove the zero-shot generalization of a recent state-of-the-art baseline,\noften achieving successful policies where the baseline fails.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 16:53:55 GMT"}, {"version": "v2", "created": "Sat, 12 Jun 2021 16:51:52 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Ball", "Philip J.", ""], ["Lu", "Cong", ""], ["Parker-Holder", "Jack", ""], ["Roberts", "Stephen", ""]]}, {"id": "2104.05658", "submitter": "Jahna Otterbacher", "authors": "Fausto Giunchiglia, Jahna Otterbacher, Styliani Kleanthous,\n  Khuyagbaatar Batsuren, Veronika Bogin, Tsvi Kuflik, Avital Shulner Tal", "title": "Towards Algorithmic Transparency: A Diversity Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  As the role of algorithmic systems and processes increases in society, so\ndoes the risk of bias, which can result in discrimination against individuals\nand social groups. Research on algorithmic bias has exploded in recent years,\nhighlighting both the problems of bias, and the potential solutions, in terms\nof algorithmic transparency (AT). Transparency is important for facilitating\nfairness management as well as explainability in algorithms; however, the\nconcept of diversity, and its relationship to bias and transparency, has been\nlargely left out of the discussion. We reflect on the relationship between\ndiversity and bias, arguing that diversity drives the need for transparency.\nUsing a perspective-taking lens, which takes diversity as a given, we propose a\nconceptual framework to characterize the problem and solution spaces of AT, to\naid its application in algorithmic systems. Example cases from three research\ndomains are described using our framework.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 17:28:12 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Giunchiglia", "Fausto", ""], ["Otterbacher", "Jahna", ""], ["Kleanthous", "Styliani", ""], ["Batsuren", "Khuyagbaatar", ""], ["Bogin", "Veronika", ""], ["Kuflik", "Tsvi", ""], ["Tal", "Avital Shulner", ""]]}, {"id": "2104.05661", "submitter": "Lars Klitzke", "authors": "Lars Klitzke, Kay Gimm, Carsten Koch, Frank K\\\"oster", "title": "Unsupervised Lane-Change Identification for On-Ramp Merge Analysis in\n  Naturalistic Driving Data", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Connected and Automated Vehicles (CAVs) are envisioned to transform the\nfuture industrial and private transportation sectors. Due to the complexity of\nthe systems, functional verification and validation of safety aspects are\nessential before the technology merges into the public domain. In recent years,\na scenario-driven approach has gained acceptance for CAVs emphasizing the\nrequirement of a solid data basis of scenarios. The large-scale research\nfacility Test Bed Lower Saxony (TFNDS) enables the provision of substantial\ninformation for a database of scenarios on motorways. For that purpose,\nhowever, the scenarios of interest must be identified and categorized in the\ncollected trajectory data. This work addresses this problem and proposes a\nframework for on-ramp scenario identification that also enables for scenario\ncategorization and assessment. The efficacy of the framework is shown with a\ndataset collected on the TFNDS.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 17:32:22 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Klitzke", "Lars", ""], ["Gimm", "Kay", ""], ["Koch", "Carsten", ""], ["K\u00f6ster", "Frank", ""]]}, {"id": "2104.05694", "submitter": "Tianyi Zhang", "authors": "Tianyi Zhang and Tatsunori Hashimoto", "title": "On the Inductive Bias of Masked Language Modeling: From Statistical to\n  Syntactic Dependencies", "comments": "NAACL-HLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study how masking and predicting tokens in an unsupervised fashion can\ngive rise to linguistic structures and downstream performance gains. Recent\ntheories have suggested that pretrained language models acquire useful\ninductive biases through masks that implicitly act as cloze reductions for\ndownstream tasks. While appealing, we show that the success of the random\nmasking strategy used in practice cannot be explained by such cloze-like masks\nalone. We construct cloze-like masks using task-specific lexicons for three\ndifferent classification datasets and show that the majority of pretrained\nperformance gains come from generic masks that are not associated with the\nlexicon. To explain the empirical success of these generic masks, we\ndemonstrate a correspondence between the Masked Language Model (MLM) objective\nand existing methods for learning statistical dependencies in graphical models.\nUsing this, we derive a method for extracting these learned statistical\ndependencies in MLMs and show that these dependencies encode useful inductive\nbiases in the form of syntactic structures. In an unsupervised parsing\nevaluation, simply forming a minimum spanning tree on the implied statistical\ndependence structure outperforms a classic method for unsupervised parsing\n(58.74 vs. 55.91 UUAS).\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 17:55:27 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Zhang", "Tianyi", ""], ["Hashimoto", "Tatsunori", ""]]}, {"id": "2104.05700", "submitter": "Thamme Gowda", "authors": "Thamme Gowda, Weiqiu You, Constantine Lignos, Jonathan May", "title": "Macro-Average: Rare Types Are Important Too", "comments": null, "journal-ref": null, "doi": "10.18653/v1/2021.naacl-main.90", "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  While traditional corpus-level evaluation metrics for machine translation\n(MT) correlate well with fluency, they struggle to reflect adequacy.\nModel-based MT metrics trained on segment-level human judgments have emerged as\nan attractive replacement due to strong correlation results. These models,\nhowever, require potentially expensive re-training for new domains and\nlanguages. Furthermore, their decisions are inherently non-transparent and\nappear to reflect unwelcome biases. We explore the simple type-based classifier\nmetric, MacroF1, and study its applicability to MT evaluation. We find that\nMacroF1 is competitive on direct assessment, and outperforms others in\nindicating downstream cross-lingual information retrieval task performance.\nFurther, we show that MacroF1 can be used to effectively compare supervised and\nunsupervised neural machine translation, and reveal significant qualitative\ndifferences in the methods' outputs.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 17:57:42 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Gowda", "Thamme", ""], ["You", "Weiqiu", ""], ["Lignos", "Constantine", ""], ["May", "Jonathan", ""]]}, {"id": "2104.05703", "submitter": "Xiaoyu Xiang", "authors": "Xiaoyu Xiang, Ding Liu, Xiao Yang, Yiheng Zhu, Xiaohui Shen, Jan P.\n  Allebach", "title": "Adversarial Open Domain Adaption for Sketch-to-Photo Synthesis", "comments": "19 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we explore the open-domain sketch-to-photo translation, which\naims to synthesize a realistic photo from a freehand sketch with its class\nlabel, even if the sketches of that class are missing in the training data. It\nis challenging due to the lack of training supervision and the large geometry\ndistortion between the freehand sketch and photo domains. To synthesize the\nabsent freehand sketches from photos, we propose a framework that jointly\nlearns sketch-to-photo and photo-to-sketch generation. However, the generator\ntrained from fake sketches might lead to unsatisfying results when dealing with\nsketches of missing classes, due to the domain gap between synthesized sketches\nand real ones. To alleviate this issue, we further propose a simple yet\neffective open-domain sampling and optimization strategy to \"fool\" the\ngenerator into treating fake sketches as real ones. Our method takes advantage\nof the learned sketch-to-photo and photo-to-sketch mapping of in-domain data\nand generalizes them to the open-domain classes. We validate our method on the\nScribble and SketchyCOCO datasets. Compared with the recent competing methods,\nour approach shows impressive results in synthesizing realistic color, texture,\nand maintaining the geometric composition for various categories of open-domain\nsketches.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 17:58:46 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Xiang", "Xiaoyu", ""], ["Liu", "Ding", ""], ["Yang", "Xiao", ""], ["Zhu", "Yiheng", ""], ["Shen", "Xiaohui", ""], ["Allebach", "Jan P.", ""]]}, {"id": "2104.05710", "submitter": "Bereket Abera Yilma Mr.", "authors": "Bereket Abera Yilma, Herv\\'e Panetto, Yannick Naudet", "title": "Systemic formalisation of Cyber-Physical-Social System (CPSS): A\n  systematic literature review", "comments": null, "journal-ref": "Computers in Industry, Volume 129, 2021, 103458, ISSN 0166-3615", "doi": "10.1016/j.compind.2021.103458", "report-no": "Volume 129", "categories": "cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The notion of Cyber-Physical-Social System (CPSS) is an emerging concept\ndeveloped as a result of the need to understand the impact of Cyber-Physical\nSystems (CPS) on humans and vice versa. This paradigm shift from CPS to CPSS\nwas mainly attributed to the increasing use of sensor-enabled smart devices and\nthe tight link with the users. The concept of CPSS has been around for over a\ndecade and it has gained increasing attention over the past few years. The\nevolution to incorporate human aspects in the CPS research has unlocked a\nnumber of research challenges. Particularly human dynamics brings additional\ncomplexity that is yet to be explored. The exploration to conceptualise the\nnotion of CPSS has been partially addressed in few scientific literatures.\nAlthough its conceptualisation has always been use-case dependent. Thus, there\nis a lack of generic view as most works focus on specific domains. Furthermore,\nthe systemic core and design principles linking it with the theory of systems\nare loose. This work aims at addressing these issues by first exploring and\nanalysing scientific literature to understand the complete spectrum of CPSS\nthrough a Systematic Literature Review (SLR). Thereby identifying the\nstate-of-the-art perspectives on CPSS regarding definitions, underlining\nprinciples and application areas. Subsequently, based on the findings of the\nSLR, we propose a domain-independent definition and a meta-model for CPSS,\ngrounded in the Theory of Systems. Finally, a discussion on feasible future\nresearch directions is presented based on the systemic notion and the proposed\nmeta-models.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 22:31:57 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Yilma", "Bereket Abera", ""], ["Panetto", "Herv\u00e9", ""], ["Naudet", "Yannick", ""]]}, {"id": "2104.05742", "submitter": "Tarik A. Rashid", "authors": "Nitish Maharjan, Abeer Alsadoon, P.W.C. Prasad, Salma Abdullah, Tarik\n  A. Rashid", "title": "A Novel Visualization System of Using Augmented Reality in Knee\n  Replacement Surgery: Enhanced Bidirectional Maximum Correntropy Algorithm", "comments": "27 pages", "journal-ref": "The International Journal of Medical Robotics and Computer\n  Assisted Surgery, 2020", "doi": "10.1002/rcs.2154", "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Background and aim: Image registration and alignment are the main limitations\nof augmented reality-based knee replacement surgery. This research aims to\ndecrease the registration error, eliminate outcomes that are trapped in local\nminima to improve the alignment problems, handle the occlusion, and maximize\nthe overlapping parts. Methodology: markerless image registration method was\nused for Augmented reality-based knee replacement surgery to guide and\nvisualize the surgical operation. While weight least square algorithm was used\nto enhance stereo camera-based tracking by filling border occlusion in right to\nleft direction and non-border occlusion from left to right direction. Results:\nThis study has improved video precision to 0.57 mm~0.61 mm alignment error.\nFurthermore, with the use of bidirectional points, for example, forwards and\nbackwards directional cloud point, the iteration on image registration was\ndecreased. This has led to improve the processing time as well. The processing\ntime of video frames was improved to 7.4~11.74 fps. Conclusions: It seems clear\nthat this proposed system has focused on overcoming the misalignment difficulty\ncaused by movement of patient and enhancing the AR visualization during knee\nreplacement surgery. The proposed system was reliable and favorable which helps\nin eliminating alignment error by ascertaining the optimal rigid transformation\nbetween two cloud points and removing the outliers and non-Gaussian noise. The\nproposed augmented reality system helps in accurate visualization and\nnavigation of anatomy of knee such as femur, tibia, cartilage, blood vessels,\netc.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 19:18:16 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Maharjan", "Nitish", ""], ["Alsadoon", "Abeer", ""], ["Prasad", "P. W. C.", ""], ["Abdullah", "Salma", ""], ["Rashid", "Tarik A.", ""]]}, {"id": "2104.05753", "submitter": "Felermino Ali", "authors": "Felermino D. M. A. Ali, Andrew Caines, Jaimito L. A. Malavi", "title": "Towards a parallel corpus of Portuguese and the Bantu language Emakhuwa\n  of Mozambique", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Major advancement in the performance of machine translation models has been\nmade possible in part thanks to the availability of large-scale parallel\ncorpora. But for most languages in the world, the existence of such corpora is\nrare. Emakhuwa, a language spoken in Mozambique, is like most African languages\nlow-resource in NLP terms. It lacks both computational and linguistic resources\nand, to the best of our knowledge, few parallel corpora including Emakhuwa\nalready exist. In this paper we describe the creation of the\nEmakhuwa-Portuguese parallel corpus, which is a collection of texts from the\nJehovah's Witness website and a variety of other sources including the African\nStory Book website, the Universal Declaration of Human Rights and Mozambican\nlegal documents. The dataset contains 47,415 sentence pairs, amounting to\n699,976 word tokens of Emakhuwa and 877,595 word tokens in Portuguese. After\nnormalization processes which remain to be completed, the corpus will be made\nfreely available for research use.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 18:31:56 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Ali", "Felermino D. M. A.", ""], ["Caines", "Andrew", ""], ["Malavi", "Jaimito L. A.", ""]]}, {"id": "2104.05755", "submitter": "Evangelos Georganas", "authors": "Evangelos Georganas, Dhiraj Kalamkar, Sasikanth Avancha, Menachem\n  Adelman, Cristina Anderson, Alexander Breuer, Narendra Chaudhary, Abhisek\n  Kundu, Vasimuddin Md, Sanchit Misra, Ramanarayan Mohanty, Hans Pabst, Barukh\n  Ziv, Alexander Heinecke", "title": "Tensor Processing Primitives: A Programming Abstraction for Efficiency\n  and Portability in Deep Learning Workloads", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  During the past decade, novel Deep Learning (DL) algorithms/workloads and\nhardware have been developed to tackle a wide range of problems. Despite the\nadvances in workload/hardware ecosystems, the programming methodology of\nDL-systems is stagnant. DL-workloads leverage either highly-optimized, yet\nplatform-specific and inflexible kernels from DL-libraries, or in the case of\nnovel operators, reference implementations are built via DL-framework\nprimitives with underwhelming performance. This work introduces the Tensor\nProcessing Primitives (TPP), a programming abstraction striving for efficient,\nportable implementation of DL-workloads with high-productivity. TPPs define a\ncompact, yet versatile set of 2D-tensor operators (or a virtual Tensor ISA),\nwhich subsequently can be utilized as building-blocks to construct complex\noperators on high-dimensional tensors. The TPP specification is\nplatform-agnostic, thus code expressed via TPPs is portable, whereas the TPP\nimplementation is highly-optimized and platform-specific. We demonstrate the\nefficacy of our approach using standalone kernels and end-to-end DL-workloads\nexpressed entirely via TPPs that outperform state-of-the-art implementations on\nmultiple platforms.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 18:35:49 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 15:38:38 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Georganas", "Evangelos", ""], ["Kalamkar", "Dhiraj", ""], ["Avancha", "Sasikanth", ""], ["Adelman", "Menachem", ""], ["Anderson", "Cristina", ""], ["Breuer", "Alexander", ""], ["Chaudhary", "Narendra", ""], ["Kundu", "Abhisek", ""], ["Md", "Vasimuddin", ""], ["Misra", "Sanchit", ""], ["Mohanty", "Ramanarayan", ""], ["Pabst", "Hans", ""], ["Ziv", "Barukh", ""], ["Heinecke", "Alexander", ""]]}, {"id": "2104.05758", "submitter": "Miao Yin", "authors": "Miao Yin, Siyu Liao, Xiao-Yang Liu, Xiaodong Wang and Bo Yuan", "title": "Towards Extremely Compact RNNs for Video Recognition with Fully\n  Decomposed Hierarchical Tucker Structure", "comments": "This paper is a preprint that was accepted in CVPR'21. arXiv admin\n  note: substantial text overlap with arXiv:2005.04366", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recurrent Neural Networks (RNNs) have been widely used in sequence analysis\nand modeling. However, when processing high-dimensional data, RNNs typically\nrequire very large model sizes, thereby bringing a series of deployment\nchallenges. Although various prior works have been proposed to reduce the RNN\nmodel sizes, executing RNN models in resource-restricted environments is still\na very challenging problem. In this paper, we propose to develop extremely\ncompact RNN models with fully decomposed hierarchical Tucker (FDHT) structure.\nThe HT decomposition does not only provide much higher storage cost reduction\nthan the other tensor decomposition approaches but also brings better accuracy\nperformance improvement for the compact RNN models. Meanwhile, unlike the\nexisting tensor decomposition-based methods that can only decompose the\ninput-to-hidden layer of RNNs, our proposed fully decomposition approach\nenables the comprehensive compression for the entire RNN models with\nmaintaining very high accuracy. Our experimental results on several popular\nvideo recognition datasets show that our proposed fully decomposed hierarchical\ntucker-based LSTM (FDHT-LSTM) is extremely compact and highly efficient. To the\nbest of our knowledge, FDHT-LSTM, for the first time, consistently achieves\nvery high accuracy with only few thousand parameters (3,132 to 8,808) on\ndifferent datasets. Compared with the state-of-the-art compressed RNN models,\nsuch as TT-LSTM, TR-LSTM and BT-LSTM, our FDHT-LSTM simultaneously enjoys both\norder-of-magnitude (3,985x to 10,711x) fewer parameters and significant\naccuracy improvement (0.6% to 12.7%).\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 18:40:44 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 23:51:47 GMT"}, {"version": "v3", "created": "Tue, 20 Apr 2021 17:54:22 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Yin", "Miao", ""], ["Liao", "Siyu", ""], ["Liu", "Xiao-Yang", ""], ["Wang", "Xiaodong", ""], ["Yuan", "Bo", ""]]}, {"id": "2104.05773", "submitter": "Hrishav Bakul Barua", "authors": "Hrishav Bakul Barua", "title": "Towards a Next Generation Computing Paradigm: Approximate Computing in\n  Robotics Systems and Environment-Experimentation, Case Study and Practical\n  Implications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approximate computing is a computation domain which can be used to trade time\nand energy with quality and therefore is useful in embedded systems. Energy is\nthe prime resource in battery-driven embedded systems, like robots. Approximate\ncomputing can be used as a technique to generate approximate version of the\ncontrol functionalities of a robot, enabling it to ration energy for\ncomputation at the cost of degraded quality. Usually, the programmer of the\nfunction specifies the extent of degradation that is safe for the overall\nsafety of the system. However, in a collaborative environment, where several\nsub-systems co-exist and some of the functionality of each of them have been\napproximated, the safety of the overall system may be compromised. In this\npaper, we consider multiple identical robots operate in a warehouse, and the\npath planning function of the robot is approximated. Although the planned paths\nare safe for individual robots (i.e. they do not collide with the racks), we\nshow that this leads to a collision among the robots. So, a controlled\napproximation needs to be carried out in such situations to harness the full\npower of this new paradigm if it needs to be a mainstream paradigm in future.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 19:03:48 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Barua", "Hrishav Bakul", ""]]}, {"id": "2104.05781", "submitter": "Arun Verma", "authors": "Arun Verma, Manjesh K. Hanawal, Arun Rajkumar, Raman Sankaran", "title": "Censored Semi-Bandits for Resource Allocation", "comments": "Extended version of the NeurIPS 2019 paper (Censored Semi-Bandits: A\n  Framework for Resource Allocation with Censored Feedback)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of sequentially allocating resources in a censored\nsemi-bandits setup, where the learner allocates resources at each step to the\narms and observes loss. The loss depends on two hidden parameters, one specific\nto the arm but independent of the resource allocation, and the other depends on\nthe allocated resource. More specifically, the loss equals zero for an arm if\nthe resource allocated to it exceeds a constant (but unknown) arm dependent\nthreshold. The goal is to learn a resource allocation that minimizes the\nexpected loss. The problem is challenging because the loss distribution and\nthreshold value of each arm are unknown. We study this setting by establishing\nits `equivalence' to Multiple-Play Multi-Armed Bandits (MP-MAB) and\nCombinatorial Semi-Bandits. Exploiting these equivalences, we derive optimal\nalgorithms for our problem setting using known algorithms for MP-MAB and\nCombinatorial Semi-Bandits. The experiments on synthetically generated data\nvalidate the performance guarantees of the proposed algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 19:15:32 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Verma", "Arun", ""], ["Hanawal", "Manjesh K.", ""], ["Rajkumar", "Arun", ""], ["Sankaran", "Raman", ""]]}, {"id": "2104.05785", "submitter": "Kenji Kawaguchi", "authors": "Kenji Kawaguchi, Qingyun Sun", "title": "A Recipe for Global Convergence Guarantee in Deep Neural Networks", "comments": "Published in AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing global convergence guarantees of (stochastic) gradient descent do\nnot apply to practical deep networks in the practical regime of deep learning\nbeyond the neural tangent kernel (NTK) regime. This paper proposes an\nalgorithm, which is ensured to have global convergence guarantees in the\npractical regime beyond the NTK regime, under a verifiable condition called the\nexpressivity condition. The expressivity condition is defined to be both\ndata-dependent and architecture-dependent, which is the key property that makes\nour results applicable for practical settings beyond the NTK regime. On the one\nhand, the expressivity condition is theoretically proven to hold\ndata-independently for fully-connected deep neural networks with narrow hidden\nlayers and a single wide layer. On the other hand, the expressivity condition\nis numerically shown to hold data-dependently for deep (convolutional) ResNet\nwith batch normalization with various standard image datasets. We also show\nthat the proposed algorithm has generalization performances comparable with\nthose of the heuristic algorithm, with the same hyper-parameters and total\nnumber of iterations. Therefore, the proposed algorithm can be viewed as a step\ntowards providing theoretical guarantees for deep learning in the practical\nregime.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 19:25:30 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 20:35:03 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Kawaguchi", "Kenji", ""], ["Sun", "Qingyun", ""]]}, {"id": "2104.05807", "submitter": "Marco Valentino", "authors": "Deborah Ferreira, Julia Rozanova, Mokanarangan Thayaparan, Marco\n  Valentino, Andr\\'e Freitas", "title": "Does My Representation Capture X? Probe-Ably", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probing (or diagnostic classification) has become a popular strategy for\ninvestigating whether a given set of intermediate features is present in the\nrepresentations of neural models. Naive probing studies may have misleading\nresults, but various recent works have suggested more reliable methodologies\nthat compensate for the possible pitfalls of probing. However, these best\npractices are numerous and fast-evolving. To simplify the process of running a\nset of probing experiments in line with suggested methodologies, we introduce\nProbe-Ably: an extendable probing framework which supports and automates the\napplication of probing methods to the user's inputs\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 20:43:10 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Ferreira", "Deborah", ""], ["Rozanova", "Julia", ""], ["Thayaparan", "Mokanarangan", ""], ["Valentino", "Marco", ""], ["Freitas", "Andr\u00e9", ""]]}, {"id": "2104.05816", "submitter": "Jonad Pulaj", "authors": "Thomas R. Cameron, Sebastian Charmot, Jonad Pulaj", "title": "On the Linear Ordering Problem and the Rankability of Data", "comments": "17 pages, 10 figures, to be published in Foundations of Data Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 2019, Anderson et al. proposed the concept of rankability, which refers to\na dataset's inherent ability to be meaningfully ranked. In this article, we\ngive an expository review of the linear ordering problem (LOP) and then use it\nto analyze the rankability of data. Specifically, the degree of linearity is\nused to quantify what percentage of the data aligns with an optimal ranking. In\na sports context, this is analogous to the number of games that a ranking can\ncorrectly predict in hindsight. In fact, under the appropriate objective\nfunction, we show that the optimal rankings computed via the LOP maximize the\nhindsight accuracy of a ranking. Moreover, we develop a binary program to\ncompute the maximal Kendall tau ranking distance between two optimal rankings,\nwhich can be used to measure the diversity among optimal rankings without\nhaving to enumerate all optima. Finally, we provide several examples from the\nworld of sports and college rankings to illustrate these concepts and\ndemonstrate our results.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 21:05:17 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Cameron", "Thomas R.", ""], ["Charmot", "Sebastian", ""], ["Pulaj", "Jonad", ""]]}, {"id": "2104.05832", "submitter": "Roshanak Mirzaee", "authors": "Roshanak Mirzaee, Hossein Rajaby Faghihi, Qiang Ning, Parisa\n  Kordjmashidi", "title": "SpartQA: : A Textual Question Answering Benchmark for Spatial Reasoning", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a question-answering (QA) benchmark for spatial reasoning\non natural language text which contains more realistic spatial phenomena not\ncovered by prior work and is challenging for state-of-the-art language models\n(LM). We propose a distant supervision method to improve on this task.\nSpecifically, we design grammar and reasoning rules to automatically generate a\nspatial description of visual scenes and corresponding QA pairs. Experiments\nshow that further pretraining LMs on these automatically generated data\nsignificantly improves LMs' capability on spatial understanding, which in turn\nhelps to better solve two external datasets, bAbI, and boolQ. We hope that this\nwork can foster investigations into more sophisticated models for spatial\nreasoning over text.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 21:37:18 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Mirzaee", "Roshanak", ""], ["Faghihi", "Hossein Rajaby", ""], ["Ning", "Qiang", ""], ["Kordjmashidi", "Parisa", ""]]}, {"id": "2104.05833", "submitter": "Daiqing Li", "authors": "Daiqing Li, Junlin Yang, Karsten Kreis, Antonio Torralba, Sanja Fidler", "title": "Semantic Segmentation with Generative Models: Semi-Supervised Learning\n  and Strong Out-of-Domain Generalization", "comments": "CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Training deep networks with limited labeled data while achieving a strong\ngeneralization ability is key in the quest to reduce human annotation efforts.\nThis is the goal of semi-supervised learning, which exploits more widely\navailable unlabeled data to complement small labeled data sets. In this paper,\nwe propose a novel framework for discriminative pixel-level tasks using a\ngenerative model of both images and labels. Concretely, we learn a generative\nadversarial network that captures the joint image-label distribution and is\ntrained efficiently using a large set of unlabeled images supplemented with\nonly few labeled ones. We build our architecture on top of StyleGAN2, augmented\nwith a label synthesis branch. Image labeling at test time is achieved by first\nembedding the target image into the joint latent space via an encoder network\nand test-time optimization, and then generating the label from the inferred\nembedding. We evaluate our approach in two important domains: medical image\nsegmentation and part-based face segmentation. We demonstrate strong in-domain\nperformance compared to several baselines, and are the first to showcase\nextreme out-of-domain generalization, such as transferring from CT to MRI in\nmedical imaging, and photographs of real faces to paintings, sculptures, and\neven cartoons and animal faces. Project Page:\n\\url{https://nv-tlabs.github.io/semanticGAN/}\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 21:41:25 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Li", "Daiqing", ""], ["Yang", "Junlin", ""], ["Kreis", "Karsten", ""], ["Torralba", "Antonio", ""], ["Fidler", "Sanja", ""]]}, {"id": "2104.05837", "submitter": "Tara Safavi", "authors": "Tara Safavi, Danai Koutra", "title": "Relational world knowledge representation in contextual language models:\n  A review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Relational knowledge bases (KBs) are established tools for world knowledge\nrepresentation in machines. While they are advantageous for their precision and\ninterpretability, they usually sacrifice some data modeling flexibility for\nthese advantages because they adhere to a manually engineered schema. In this\nreview, we take a natural language processing perspective to the limitations of\nKBs, examining how they may be addressed in part by training neural contextual\nlanguage models (LMs) to internalize and express relational knowledge in\nfree-text form. We propose a novel taxonomy for relational knowledge\nrepresentation in contextual LMs based on the level of KB supervision provided,\nconsidering both works that probe LMs for implicit relational knowledge\nacquired during self-supervised pretraining on unstructured text alone, and\nworks that explicitly supervise LMs at the level of KB entities and/or\nrelations. We conclude that LMs and KBs are complementary representation tools,\nas KBs provide a high standard of factual precision which can in turn be\nflexibly and expressively modeled by LMs, and provide suggestions for future\nresearch in this direction.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 21:50:55 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Safavi", "Tara", ""], ["Koutra", "Danai", ""]]}, {"id": "2104.05845", "submitter": "Yue Yang", "authors": "Yue Yang, Artemis Panagopoulou, Qing Lyu, Li Zhang, Mark Yatskar,\n  Chris Callison-Burch", "title": "Visual Goal-Step Inference using wikiHow", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Procedural events can often be thought of as a high level goal composed of a\nsequence of steps. Inferring the sub-sequence of steps of a goal can help\nartificial intelligence systems reason about human activities. Past work in NLP\nhas examined the task of goal-step inference for text. We introduce the visual\nanalogue. We propose the Visual Goal-Step Inference (VGSI) task where a model\nis given a textual goal and must choose a plausible step towards that goal from\namong four candidate images. Our task is challenging for state-of-the-art\nmuitimodal models. We introduce a novel dataset harvested from wikiHow that\nconsists of 772,294 images representing human actions. We show that the\nknowledge learned from our data can effectively transfer to other datasets like\nHowTo100M, increasing the multiple-choice accuracy by 15% to 20%. Our task will\nfacilitate multi-modal reasoning about procedural events.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 22:20:09 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Yang", "Yue", ""], ["Panagopoulou", "Artemis", ""], ["Lyu", "Qing", ""], ["Zhang", "Li", ""], ["Yatskar", "Mark", ""], ["Callison-Burch", "Chris", ""]]}, {"id": "2104.05848", "submitter": "Zhong Zhou", "authors": "Zhong Zhou, Alex Waibel", "title": "Family of Origin and Family of Choice: Massively Parallel Lexiconized\n  Iterative Pretraining for Severely Low Resource Machine Translation", "comments": null, "journal-ref": "In Proceedings of the 3rd Workshop on Research in Computational\n  Typology and Multilingual NLP of the 20th Conference of the North American\n  Chapter of the Association for Computational Linguistics on Human Language\n  Technologies in 2021", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We translate a closed text that is known in advance into a severely low\nresource language by leveraging massive source parallelism. In other words,\ngiven a text in 124 source languages, we translate it into a severely low\nresource language using only ~1,000 lines of low resource data without any\nexternal help. Firstly, we propose a systematic method to rank and choose\nsource languages that are close to the low resource language. We call the\nlinguistic definition of language family Family of Origin (FAMO), and we call\nthe empirical definition of higher-ranked languages using our metrics Family of\nChoice (FAMC). Secondly, we build an Iteratively Pretrained Multilingual\nOrder-preserving Lexiconized Transformer (IPML) to train on ~1,000 lines\n(~3.5%) of low resource data. To translate named entities correctly, we build a\nmassive lexicon table for 2,939 Bible named entities in 124 source languages,\nand include many that occur once and covers more than 66 severely low resource\nlanguages. Moreover, we also build a novel method of combining translations\nfrom different source languages into one. Using English as a hypothetical low\nresource language, we get a +23.9 BLEU increase over a multilingual baseline,\nand a +10.3 BLEU increase over our asymmetric baseline in the Bible dataset. We\nget a 42.8 BLEU score for Portuguese-English translation on the medical EMEA\ndataset. We also have good results for a real severely low resource Mayan\nlanguage, Eastern Pokomchi.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 22:32:58 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 19:54:42 GMT"}, {"version": "v3", "created": "Wed, 28 Apr 2021 14:12:27 GMT"}, {"version": "v4", "created": "Wed, 19 May 2021 17:48:05 GMT"}, {"version": "v5", "created": "Mon, 24 May 2021 12:56:39 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Zhou", "Zhong", ""], ["Waibel", "Alex", ""]]}, {"id": "2104.05857", "submitter": "Robert Hawkins", "authors": "Robert D. Hawkins, Michael Franke, Michael C. Frank, Kenny Smith,\n  Thomas L. Griffiths, Noah D. Goodman", "title": "From partners to populations: A hierarchical Bayesian account of\n  coordination and convention", "comments": "Draft version, 4/12/2021. This paper has not been peer reviewed.\n  Please do not copy or cite without author's permission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Languages are powerful solutions to coordination problems: they provide\nstable, shared expectations about how the words we say correspond to the\nbeliefs and intentions in our heads. Yet language use in a variable and\nnon-stationary social environment requires linguistic representations to be\nflexible: old words acquire new ad hoc or partner-specific meanings on the fly.\nIn this paper, we introduce a hierarchical Bayesian theory of convention\nformation that aims to reconcile the long-standing tension between these two\nbasic observations. More specifically, we argue that the central computational\nproblem of communication is not simply transmission, as in classical\nformulations, but learning and adaptation over multiple timescales. Under our\naccount, rapid learning within dyadic interactions allows for coordination on\npartner-specific common ground, while social conventions are stable priors that\nhave been abstracted away from interactions with multiple partners. We present\nnew empirical data alongside simulations showing how our model provides a\ncognitive foundation for explaining several phenomena that have posed a\nchallenge for previous accounts: (1) the convergence to more efficient\nreferring expressions across repeated interaction with the same partner, (2)\nthe gradual transfer of partner-specific common ground to novel partners, and\n(3) the influence of communicative context on which conventions eventually\nform.\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 23:00:40 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Hawkins", "Robert D.", ""], ["Franke", "Michael", ""], ["Frank", "Michael C.", ""], ["Smith", "Kenny", ""], ["Griffiths", "Thomas L.", ""], ["Goodman", "Noah D.", ""]]}, {"id": "2104.05859", "submitter": "Dhruv Shah", "authors": "Dhruv Shah, Benjamin Eysenbach, Nicholas Rhinehart, Sergey Levine", "title": "RECON: Rapid Exploration for Open-World Navigation with Latent Goal\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We describe a robotic learning system for autonomous navigation in diverse\nenvironments. At the core of our method are two components: (i) a\nnon-parametric map that reflects the connectivity of the environment but does\nnot require geometric reconstruction or localization, and (ii) a latent\nvariable model of distances and actions that enables efficiently constructing\nand traversing this map. The model is trained on a large dataset of prior\nexperience to predict the expected amount of time and next action needed to\ntransit between the current image and a goal image. Training the model in this\nway enables it to develop a representation of goals robust to distracting\ninformation in the input images, which aids in deploying the system to quickly\nexplore new environments. We demonstrate our method on a mobile ground robot in\na range of outdoor navigation scenarios. Our method can learn to reach new\ngoals, specified as images, in a radius of up to 80 meters in just 20 minutes,\nand reliably revisit these goals in changing environments. We also demonstrate\nour method's robustness to previously-unseen obstacles and variable weather\nconditions. We encourage the reader to visit the project website for videos of\nour experiments and demonstrations https://sites.google.com/view/recon-robot\n", "versions": [{"version": "v1", "created": "Mon, 12 Apr 2021 23:14:41 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 04:30:53 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Shah", "Dhruv", ""], ["Eysenbach", "Benjamin", ""], ["Rhinehart", "Nicholas", ""], ["Levine", "Sergey", ""]]}, {"id": "2104.05874", "submitter": "Matt Calder", "authors": "Matt Calder", "title": "Gradient Kernel Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article a surprising result is demonstrated using the neural tangent\nkernel. This kernel is defined as the inner product of the vector of the\ngradient of an underlying model evaluated at training points. This kernel is\nused to perform kernel regression. The surprising thing is that the accuracy of\nthat regression is independent of the accuracy of the underlying network.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 00:32:34 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Calder", "Matt", ""]]}, {"id": "2104.05888", "submitter": "Xingjian Zhen", "authors": "Xingjian Zhen, Rudrasis Chakraborty, Vikas Singh", "title": "Simpler Certified Radius Maximization by Propagating Covariances", "comments": "This paper has been accepted by CVPR 2021 as an oral presentation. An\n  introduction video can be found: https://youtu.be/m1ya2oNf5iE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One strategy for adversarially training a robust model is to maximize its\ncertified radius -- the neighborhood around a given training sample for which\nthe model's prediction remains unchanged. The scheme typically involves\nanalyzing a \"smoothed\" classifier where one estimates the prediction\ncorresponding to Gaussian samples in the neighborhood of each sample in the\nmini-batch, accomplished in practice by Monte Carlo sampling. In this paper, we\ninvestigate the hypothesis that this sampling bottleneck can potentially be\nmitigated by identifying ways to directly propagate the covariance matrix of\nthe smoothed distribution through the network. To this end, we find that other\nthan certain adjustments to the network, propagating the covariances must also\nbe accompanied by additional accounting that keeps track of how the\ndistributional moments transform and interact at each stage in the network. We\nshow how satisfying these criteria yields an algorithm for maximizing the\ncertified radius on datasets including Cifar-10, ImageNet, and Places365 while\noffering runtime savings on networks with moderate depth, with a small\ncompromise in overall accuracy. We describe the details of the key\nmodifications that enable practical use. Via various experiments, we evaluate\nwhen our simplifications are sensible, and what the key benefits and\nlimitations are.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 01:38:36 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Zhen", "Xingjian", ""], ["Chakraborty", "Rudrasis", ""], ["Singh", "Vikas", ""]]}, {"id": "2104.05902", "submitter": "Haotian Liu", "authors": "Haotian Liu, Wenchuan Wu", "title": "Bi-level Off-policy Reinforcement Learning for Volt/VAR Control\n  Involving Continuous and Discrete Devices", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.AI cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Volt/Var control (VVC) of active distribution networks(ADNs), both slow\ntimescale discrete devices (STDDs) and fast timescale continuous devices\n(FTCDs) are involved. The STDDs such as on-load tap changers (OLTC) and FTCDs\nsuch as distributed generators should be coordinated in time sequence. Such VCC\nis formulated as a two-timescale optimization problem to jointly optimize FTCDs\nand STDDs in ADNs. Traditional optimization methods are heavily based on\naccurate models of the system, but sometimes impractical because of their\nunaffordable effort on modelling. In this paper, a novel bi-level off-policy\nreinforcement learning (RL) algorithm is proposed to solve this problem in a\nmodel-free manner. A Bi-level Markov decision process (BMDP) is defined to\ndescribe the two-timescale VVC problem and separate agents are set up for the\nslow and fast timescale sub-problems. For the fast timescale sub-problem, we\nadopt an off-policy RL method soft actor-critic with high sample efficiency.\nFor the slow one, we develop an off-policy multi-discrete soft actor-critic\n(MDSAC) algorithm to address the curse of dimensionality with various STDDs. To\nmitigate the non-stationary issue existing the two agents' learning processes,\nwe propose a multi-timescale off-policy correction (MTOPC) method by adopting\nimportance sampling technique. Comprehensive numerical studies not only\ndemonstrate that the proposed method can achieve stable and satisfactory\noptimization of both STDDs and FTCDs without any model information, but also\nsupport that the proposed method outperforms existing two-timescale VVC\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 02:22:43 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Liu", "Haotian", ""], ["Wu", "Wenchuan", ""]]}, {"id": "2104.05914", "submitter": "Yang Li", "authors": "Yang Li, Di Wang, and Jos\\'e M. F. Moura", "title": "GSA-Forecaster: Forecasting Graph-Based Time-Dependent Data with Graph\n  Sequence Attention", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecasting graph-based time-dependent data has many practical applications.\nThis task is challenging as models need not only to capture spatial dependency\nand temporal dependency within the data, but also to leverage useful auxiliary\ninformation for accurate predictions. In this paper, we analyze limitations of\nstate-of-the-art models on dealing with temporal dependency. To address this\nlimitation, we propose GSA-Forecaster, a new deep learning model for\nforecasting graph-based time-dependent data. GSA-Forecaster leverages graph\nsequence attention (GSA), a new attention mechanism proposed in this paper, for\neffectively capturing temporal dependency. GSA-Forecaster embeds the graph\nstructure of the data into its architecture to address spatial dependency.\nGSA-Forecaster also accounts for auxiliary information to further improve\npredictions. We evaluate GSA-Forecaster with large-scale real-world graph-based\ntime-dependent data and demonstrate its effectiveness over state-of-the-art\nmodels with 6.7% RMSE and 5.8% MAPE reduction.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 03:19:10 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Li", "Yang", ""], ["Wang", "Di", ""], ["Moura", "Jos\u00e9 M. F.", ""]]}, {"id": "2104.05915", "submitter": "Rohitash Chandra", "authors": "Rohitash Chandra, Mahir Jain, Manavendra Maharana, Pavel N. Krivitsky", "title": "Revisiting Bayesian Autoencoders with MCMC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Autoencoders gained popularity in the deep learning revolution given their\nability to compress data and provide dimensionality reduction. Although\nprominent deep learning methods have been used to enhance autoencoders, the\nneed to provide robust uncertainty quantification remains a challenge. This has\nbeen addressed with variational autoencoders so far. Bayesian inference via\nMCMC methods have faced limitations but recent advances with parallel computing\nand advanced proposal schemes that incorporate gradients have opened routes\nless travelled. In this paper, we present Bayesian autoencoders powered MCMC\nsampling implemented using parallel computing and Langevin gradient proposal\nscheme. Our proposed Bayesian autoencoder provides similar performance accuracy\nwhen compared to related methods from the literature, with the additional\nfeature of robust uncertainty quantification in compressed datasets. This\nmotivates further application of the Bayesian autoencoder framework for other\ndeep learning models.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 03:23:07 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Chandra", "Rohitash", ""], ["Jain", "Mahir", ""], ["Maharana", "Manavendra", ""], ["Krivitsky", "Pavel N.", ""]]}, {"id": "2104.05928", "submitter": "James Evans", "authors": "Brendan Chambers and James Evans", "title": "Semantic maps and metrics for science Semantic maps and metrics for\n  science using deep transformer encoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The growing deluge of scientific publications demands text analysis tools\nthat can help scientists and policy-makers navigate, forecast and beneficially\nguide scientific research. Recent advances in natural language understanding\ndriven by deep transformer networks offer new possibilities for mapping\nscience. Because the same surface text can take on multiple and sometimes\ncontradictory specialized senses across distinct research communities,\nsensitivity to context is critical for infometric applications. Transformer\nembedding models such as BERT capture shades of association and connotation\nthat vary across the different linguistic contexts of any particular word or\nspan of text. Here we report a procedure for encoding scientific documents with\nthese tools, measuring their improvement over static word embeddings in a\nnearest-neighbor retrieval task. We find discriminability of contextual\nrepresentations is strongly influenced by choice of pooling strategy for\nsummarizing the high-dimensional network activations. Importantly, we note that\nfundamentals such as domain-matched training data are more important than\nstate-of-the-art NLP tools. Yet state-of-the-art models did offer significant\ngains. The best approach we investigated combined domain-matched pretraining,\nsound pooling, and state-of-the-art deep transformer network encoders. Finally,\nwith the goal of leveraging contextual representations from deep encoders, we\npresent a range of measurements for understanding and forecasting research\ncommunities in science.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 04:12:20 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Chambers", "Brendan", ""], ["Evans", "James", ""]]}, {"id": "2104.05930", "submitter": "Brenden Petersen", "authors": "Joanne T. Kim, Mikel Landajuela Larma, Brenden K. Petersen", "title": "Distilling Wikipedia mathematical knowledge into neural network models", "comments": "6 pages, 4 figures", "journal-ref": "1st Mathematical Reasoning in General Artificial Intelligence\n  Workshop, ICLR 2021", "doi": null, "report-no": "LLNL-CONF-820039", "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning applications to symbolic mathematics are becoming\nincreasingly popular, yet there lacks a centralized source of real-world\nsymbolic expressions to be used as training data. In contrast, the field of\nnatural language processing leverages resources like Wikipedia that provide\nenormous amounts of real-world textual data. Adopting the philosophy of\n\"mathematics as language,\" we bridge this gap by introducing a pipeline for\ndistilling mathematical expressions embedded in Wikipedia into symbolic\nencodings to be used in downstream machine learning tasks. We demonstrate that\na $\\textit{mathematical}$ $\\textit{language}$ $\\textit{model}$ trained on this\n\"corpus\" of expressions can be used as a prior to improve the performance of\nneural-guided search for the task of symbolic regression.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 04:16:50 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Kim", "Joanne T.", ""], ["Larma", "Mikel Landajuela", ""], ["Petersen", "Brenden K.", ""]]}, {"id": "2104.05931", "submitter": "Taeyoung Kim", "authors": "Taeyoung Kim, Luiz Felipe Vecchietti, Kyujin Choi, Sanem Sariel,\n  Dongsoo Har", "title": "Two-stage training algorithm for AI robot soccer", "comments": "This work is submitted to Peer J Computer Science and is currently\n  under review. If published, we put the DOI to the paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multi-agent reinforcement learning, the cooperative learning behavior of\nagents is very important. In the field of heterogeneous multi-agent\nreinforcement learning, cooperative behavior among different types of agents in\na group is pursued. Learning a joint-action set during centralized training is\nan attractive way to obtain such cooperative behavior, however, this method\nbrings limited learning performance with heterogeneous agents. To improve the\nlearning performance of heterogeneous agents during centralized training,\ntwo-stage heterogeneous centralized training which allows the training of\nmultiple roles of heterogeneous agents is proposed. During training, two\ntraining processes are conducted in a series. One of the two stages is to\nattempt training each agent according to its role, aiming at the maximization\nof individual role rewards. The other is for training the agents as a whole to\nmake them learn cooperative behaviors while attempting to maximize shared\ncollective rewards, e.g., team rewards. Because these two training processes\nare conducted in a series in every timestep, agents can learn how to maximize\nrole rewards and team rewards simultaneously. The proposed method is applied to\n5 versus 5 AI robot soccer for validation. Simulation results show that the\nproposed method can train the robots of the robot soccer team effectively,\nachieving higher role rewards and higher team rewards as compared to other\napproaches that can be used to solve problems of training cooperative\nmulti-agent.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 04:24:13 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Kim", "Taeyoung", ""], ["Vecchietti", "Luiz Felipe", ""], ["Choi", "Kyujin", ""], ["Sariel", "Sanem", ""], ["Har", "Dongsoo", ""]]}, {"id": "2104.05932", "submitter": "Shubham Shrivastava", "authors": "Shubham Shrivastava", "title": "VR3Dense: Voxel Representation Learning for 3D Object Detection and\n  Monocular Dense Depth Reconstruction", "comments": "7 pages, 4 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D object detection and dense depth estimation are one of the most vital\ntasks in autonomous driving. Multiple sensor modalities can jointly attribute\ntowards better robot perception, and to that end, we introduce a method for\njointly training 3D object detection and monocular dense depth reconstruction\nneural networks. It takes as inputs, a LiDAR point-cloud, and a single RGB\nimage during inference and produces object pose predictions as well as a\ndensely reconstructed depth map. LiDAR point-cloud is converted into a set of\nvoxels, and its features are extracted using 3D convolution layers, from which\nwe regress object pose parameters. Corresponding RGB image features are\nextracted using another 2D convolutional neural network. We further use these\ncombined features to predict a dense depth map. While our object detection is\ntrained in a supervised manner, the depth prediction network is trained with\nboth self-supervised and supervised loss functions. We also introduce a loss\nfunction, edge-preserving smooth loss, and show that this results in better\ndepth estimation compared to the edge-aware smooth loss function, frequently\nused in depth prediction works.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 04:25:54 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Shrivastava", "Shubham", ""]]}, {"id": "2104.05940", "submitter": "Bin Wang", "authors": "Kaitai Zhang, Bin Wang, Hong-Shuo Chen, Ye Wang, Shiyu Mou, and C.-C.\n  Jay Kuo", "title": "Dynamic Texture Synthesis by Incorporating Long-range Spatial and\n  Temporal Correlations", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main challenge of dynamic texture synthesis lies in how to maintain\nspatial and temporal consistency in synthesized videos. The major drawback of\nexisting dynamic texture synthesis models comes from poor treatment of the\nlong-range texture correlation and motion information. To address this problem,\nwe incorporate a new loss term, called the Shifted Gram loss, to capture the\nstructural and long-range correlation of the reference texture video.\nFurthermore, we introduce a frame sampling strategy to exploit long-period\nmotion across multiple frames. With these two new techniques, the application\nscope of existing texture synthesis models can be extended. That is, they can\nsynthesize not only homogeneous but also structured dynamic texture patterns.\nThorough experimental results are provided to demonstrate that our proposed\ndynamic texture synthesis model offers state-of-the-art visual performance.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 05:04:51 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 04:15:31 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Zhang", "Kaitai", ""], ["Wang", "Bin", ""], ["Chen", "Hong-Shuo", ""], ["Wang", "Ye", ""], ["Mou", "Shiyu", ""], ["Kuo", "C. -C. Jay", ""]]}, {"id": "2104.05959", "submitter": "Yunsheng Tian", "authors": "Yunsheng Tian, Mina Konakovi\\'c Lukovi\\'c, Timothy Erps, Michael\n  Foshey, Wojciech Matusik", "title": "AutoOED: Automated Optimal Experiment Design Platform", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present AutoOED, an Optimal Experiment Design platform powered with\nautomated machine learning to accelerate the discovery of optimal solutions.\nThe platform solves multi-objective optimization problems in time- and\ndata-efficient manner by automatically guiding the design of experiments to be\nevaluated. To automate the optimization process, we implement several\nmulti-objective Bayesian optimization algorithms with state-of-the-art\nperformance. AutoOED is open-source and written in Python. The codebase is\nmodular, facilitating extensions and tailoring the code, serving as a testbed\nfor machine learning researchers to easily develop and evaluate their own\nmulti-objective Bayesian optimization algorithms. An intuitive graphical user\ninterface (GUI) is provided to visualize and guide the experiments for users\nwith little or no experience with coding, machine learning, or optimization.\nFurthermore, a distributed system is integrated to enable parallelized\nexperimental evaluations by independent workers in remote locations. The\nplatform is available at https://autooed.org.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 06:20:54 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Tian", "Yunsheng", ""], ["Lukovi\u0107", "Mina Konakovi\u0107", ""], ["Erps", "Timothy", ""], ["Foshey", "Michael", ""], ["Matusik", "Wojciech", ""]]}, {"id": "2104.05964", "submitter": "Kyeongpil Kang", "authors": "Kyeongpil Kang, Kyohoon Jin, Soyoung Yang, Sujin Jang, Jaegul Choo,\n  Youngbin Kim", "title": "Restoring and Mining the Records of the Joseon Dynasty via Neural\n  Language Modeling and Machine Translation", "comments": "Accepted to NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding voluminous historical records provides clues on the past in\nvarious aspects, such as social and political issues and even natural science\nfacts. However, it is generally difficult to fully utilize the historical\nrecords, since most of the documents are not written in a modern language and\npart of the contents are damaged over time. As a result, restoring the damaged\nor unrecognizable parts as well as translating the records into modern\nlanguages are crucial tasks. In response, we present a multi-task learning\napproach to restore and translate historical documents based on a\nself-attention mechanism, specifically utilizing two Korean historical records,\nones of the most voluminous historical records in the world. Experimental\nresults show that our approach significantly improves the accuracy of the\ntranslation task than baselines without multi-task learning. In addition, we\npresent an in-depth exploratory analysis on our translated results via topic\nmodeling, uncovering several significant historical events.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 06:40:25 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 06:18:25 GMT"}, {"version": "v3", "created": "Fri, 7 May 2021 03:25:29 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Kang", "Kyeongpil", ""], ["Jin", "Kyohoon", ""], ["Yang", "Soyoung", ""], ["Jang", "Sujin", ""], ["Choo", "Jaegul", ""], ["Kim", "Youngbin", ""]]}, {"id": "2104.05988", "submitter": "Marcel B\\\"uhler", "authors": "Marcel C. B\\\"uhler (1), Abhimitra Meka (2), Gengyan Li (1 and 2),\n  Thabo Beeler (2), Otmar Hilliges (1) ((1) ETH Zurich, (2) Google)", "title": "VariTex: Variational Neural Face Textures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep generative models have recently demonstrated the ability to synthesize\nphotorealistic images of human faces with novel identities. A key challenge to\nthe wide applicability of such techniques is to provide independent control\nover semantically meaningful parameters: appearance, head pose, face shape, and\nfacial expressions. In this paper, we propose VariTex - to the best of our\nknowledge the first method that learns a variational latent feature space of\nneural face textures, which allows sampling of novel identities. We combine\nthis generative model with a parametric face model and gain explicit control\nover head pose and facial expressions. To generate images of complete human\nheads, we propose an additive decoder that generates plausible additional\ndetails such as hair. A novel training scheme enforces a pose independent\nlatent space and in consequence, allows learning of a one-to-many mapping\nbetween latent codes and pose-conditioned exterior regions. The resulting\nmethod can generate geometrically consistent images of novel identities\nallowing fine-grained control over head pose, face shape, and facial\nexpressions, facilitating a broad range of downstream tasks, like sampling\nnovel identities, re-posing, expression transfer, and more.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 07:47:53 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 17:24:03 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["B\u00fchler", "Marcel C.", "", "ETH Zurich"], ["Meka", "Abhimitra", "", "Google"], ["Li", "Gengyan", "", "1 and 2"], ["Beeler", "Thabo", "", "Google"], ["Hilliges", "Otmar", "", "ETH Zurich"]]}, {"id": "2104.06015", "submitter": "Xingyu Zhao", "authors": "Xingyu Zhao, Wei Huang, Sven Schewe, Yi Dong, Xiaowei Huang", "title": "Detecting Operational Adversarial Examples for Reliable Deep Learning", "comments": "Preprint accepted by the fast abstract track of DSN'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The utilisation of Deep Learning (DL) raises new challenges regarding its\ndependability in critical applications. Sound verification and validation\nmethods are needed to assure the safe and reliable use of DL. However,\nstate-of-the-art debug testing methods on DL that aim at detecting adversarial\nexamples (AEs) ignore the operational profile, which statistically depicts the\nsoftware's future operational use. This may lead to very modest effectiveness\non improving the software's delivered reliability, as the testing budget is\nlikely to be wasted on detecting AEs that are unrealistic or encountered very\nrarely in real-life operation. In this paper, we first present the novel notion\nof \"operational AEs\" which are AEs that have relatively high chance to be seen\nin future operation. Then an initial design of a new DL testing method to\nefficiently detect \"operational AEs\" is provided, as well as some insights on\nour prospective research plan.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 08:31:42 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2021 11:01:51 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Zhao", "Xingyu", ""], ["Huang", "Wei", ""], ["Schewe", "Sven", ""], ["Dong", "Yi", ""], ["Huang", "Xiaowei", ""]]}, {"id": "2104.06039", "submitter": "Alon Talmor", "authors": "Alon Talmor, Ori Yoran, Amnon Catav, Dan Lahav, Yizhong Wang, Akari\n  Asai, Gabriel Ilharco, Hannaneh Hajishirzi, Jonathan Berant", "title": "MultiModalQA: Complex Question Answering over Text, Tables and Images", "comments": "ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When answering complex questions, people can seamlessly combine information\nfrom visual, textual and tabular sources. While interest in models that reason\nover multiple pieces of evidence has surged in recent years, there has been\nrelatively little work on question answering models that reason across multiple\nmodalities. In this paper, we present MultiModalQA(MMQA): a challenging\nquestion answering dataset that requires joint reasoning over text, tables and\nimages. We create MMQA using a new framework for generating complex multi-modal\nquestions at scale, harvesting tables from Wikipedia, and attaching images and\ntext paragraphs using entities that appear in each table. We then define a\nformal language that allows us to take questions that can be answered from a\nsingle modality, and combine them to generate cross-modal questions. Last,\ncrowdsourcing workers take these automatically-generated questions and rephrase\nthem into more fluent language. We create 29,918 questions through this\nprocedure, and empirically demonstrate the necessity of a multi-modal multi-hop\napproach to solve our task: our multi-hop model, ImplicitDecomp, achieves an\naverage F1of 51.7 over cross-modal questions, substantially outperforming a\nstrong baseline that achieves 38.2 F1, but still lags significantly behind\nhuman performance, which is at 90.1 F1\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 09:14:28 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Talmor", "Alon", ""], ["Yoran", "Ori", ""], ["Catav", "Amnon", ""], ["Lahav", "Dan", ""], ["Wang", "Yizhong", ""], ["Asai", "Akari", ""], ["Ilharco", "Gabriel", ""], ["Hajishirzi", "Hannaneh", ""], ["Berant", "Jonathan", ""]]}, {"id": "2104.06040", "submitter": "Ioannis Mollas", "authors": "Ioannis Mollas, Nick Bassiliades, Grigorios Tsoumakas", "title": "Conclusive Local Interpretation Rules for Random Forests", "comments": "32 pages, 31 figures, 4 Tables, submitted to data mining and\n  knowledge discovery journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In critical situations involving discrimination, gender inequality, economic\ndamage, and even the possibility of casualties, machine learning models must be\nable to provide clear interpretations for their decisions. Otherwise, their\nobscure decision-making processes can lead to socioethical issues as they\ninterfere with people's lives. In the aforementioned sectors, random forest\nalgorithms strive, thus their ability to explain themselves is an obvious\nrequirement. In this paper, we present LionForests, which relies on a\npreliminary work of ours. LionForests is a random forest-specific\ninterpretation technique, which provides rules as explanations. It is\napplicable from binary classification tasks to multi-class classification and\nregression tasks, and it is supported by a stable theoretical background.\nExperimentation, including sensitivity analysis and comparison with\nstate-of-the-art techniques, is also performed to demonstrate the efficacy of\nour contribution. Finally, we highlight a unique property of LionForests,\ncalled conclusiveness, that provides interpretation validity and distinguishes\nit from previous techniques.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 09:15:23 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Mollas", "Ioannis", ""], ["Bassiliades", "Nick", ""], ["Tsoumakas", "Grigorios", ""]]}, {"id": "2104.06054", "submitter": "Viet-Man Le", "authors": "Viet-Man Le", "title": "Group Recommendation Techniques for Feature Modeling and Configuration", "comments": "to appear in the ICSE-DS'21 Proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In large-scale feature models, feature modeling and configuration processes\nare highly expected to be done by a group of stakeholders. In this context,\nrecommendation techniques can increase the efficiency of feature-model design\nand find optimal configurations for groups of stakeholders. Existing studies\nshow plenty of issues concerning feature model navigation support, group\nmembers' satisfaction, and conflict resolution. This study proposes group\nrecommendation techniques for feature modeling and configuration on the basis\nof addressing the mentioned issues.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 09:34:27 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Le", "Viet-Man", ""]]}, {"id": "2104.06057", "submitter": "Ioannis Mollas", "authors": "Ioannis Mollas, Nick Bassiliades, Grigorios Tsoumakas", "title": "LioNets: A Neural-Specific Local Interpretation Technique Exploiting\n  Penultimate Layer Information", "comments": "23 pages, 22 figures, 2 tables, submitted to Information Fusion\n  Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Intelligence (AI) has a tremendous impact on the unexpected growth\nof technology in almost every aspect. AI-powered systems are monitoring and\ndeciding about sensitive economic and societal issues. The future is towards\nautomation, and it must not be prevented. However, this is a conflicting\nviewpoint for a lot of people, due to the fear of uncontrollable AI systems.\nThis concern could be reasonable if it was originating from considerations\nassociated with social issues, like gender-biased, or obscure decision-making\nsystems. Explainable AI (XAI) is recently treated as a huge step towards\nreliable systems, enhancing the trust of people to AI. Interpretable machine\nlearning (IML), a subfield of XAI, is also an urgent topic of research. This\npaper presents a small but significant contribution to the IML community,\nfocusing on a local-based, neural-specific interpretation process applied to\ntextual and time-series data. The proposed methodology introduces new\napproaches to the presentation of feature importance based interpretations, as\nwell as the production of counterfactual words on textual datasets. Eventually,\nan improved evaluation metric is introduced for the assessment of\ninterpretation techniques, which supports an extensive set of qualitative and\nquantitative experiments.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 09:39:33 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Mollas", "Ioannis", ""], ["Bassiliades", "Nick", ""], ["Tsoumakas", "Grigorios", ""]]}, {"id": "2104.06063", "submitter": "Dumitru-Clementin Cercel", "authors": "R\\u{a}zvan-Alexandru Sm\\u{a}du, Dumitru-Clementin Cercel, Mihai\n  Dascalu", "title": "UPB at SemEval-2021 Task 7: Adversarial Multi-Task Learning for\n  Detecting and Rating Humor and Offense", "comments": "7 pages, 2 figures, Accepted at SemEval-2021 Workshop, ACL-IJCNLP\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Detecting humor is a challenging task since words might share multiple\nvalences and, depending on the context, the same words can be even used in\noffensive expressions. Neural network architectures based on Transformer obtain\nstate-of-the-art results on several Natural Language Processing tasks,\nespecially text classification. Adversarial learning, combined with other\ntechniques such as multi-task learning, aids neural models learn the intrinsic\nproperties of data. In this work, we describe our adversarial multi-task\nnetwork, AMTL-Humor, used to detect and rate humor and offensive texts from\nTask 7 at SemEval-2021. Each branch from the model is focused on solving a\nrelated task, and consists of a BiLSTM layer followed by Capsule layers, on top\nof BERTweet used for generating contextualized embeddings. Our best model\nconsists of an ensemble of all tested configurations, and achieves a 95.66%\nF1-score and 94.70% accuracy for Task 1a, while obtaining RMSE scores of 0.6200\nand 0.5318 for Tasks 1b and 2, respectively.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 09:59:05 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Sm\u0103du", "R\u0103zvan-Alexandru", ""], ["Cercel", "Dumitru-Clementin", ""], ["Dascalu", "Mihai", ""]]}, {"id": "2104.06106", "submitter": "Takumi Tanabe", "authors": "Takumi Tanabe, Kazuto Fukuchi, Jun Sakuma, Youhei Akimoto", "title": "Level Generation for Angry Birds with Sequential VAE and Latent Variable\n  Evolution", "comments": "The Genetic and Evolutionary Computation Conference 2021 (GECCO '21)", "journal-ref": null, "doi": "10.1145/3449639.3459290", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video game level generation based on machine learning (ML), in particular,\ndeep generative models, has attracted attention as a technique to automate\nlevel generation. However, applications of existing ML-based level generations\nare mostly limited to tile-based level representation. When ML techniques are\napplied to game domains with non-tile-based level representation, such as Angry\nBirds, where objects in a level are specified by real-valued parameters, ML\noften fails to generate playable levels. In this study, we develop a\ndeep-generative-model-based level generation for the game domain of Angry\nBirds. To overcome these drawbacks, we propose a sequential encoding of a level\nand process it as text data, whereas existing approaches employ a tile-based\nencoding and process it as an image. Experiments show that the proposed level\ngenerator drastically improves the stability and diversity of generated levels\ncompared with existing approaches. We apply latent variable evolution with the\nproposed generator to control the feature of a generated level computed through\nan AI agent's play, while keeping the level stable and natural.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 11:23:39 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Tanabe", "Takumi", ""], ["Fukuchi", "Kazuto", ""], ["Sakuma", "Jun", ""], ["Akimoto", "Youhei", ""]]}, {"id": "2104.06159", "submitter": "Ivo Danihelka", "authors": "Matteo Hessel, Ivo Danihelka, Fabio Viola, Arthur Guez, Simon Schmitt,\n  Laurent Sifre, Theophane Weber, David Silver, Hado van Hasselt", "title": "Muesli: Combining Improvements in Policy Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel policy update that combines regularized policy\noptimization with model learning as an auxiliary loss. The update (henceforth\nMuesli) matches MuZero's state-of-the-art performance on Atari. Notably, Muesli\ndoes so without using deep search: it acts directly with a policy network and\nhas computation speed comparable to model-free baselines. The Atari results are\ncomplemented by extensive ablations, and by additional results on continuous\ncontrol and 9x9 Go.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 13:04:29 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Hessel", "Matteo", ""], ["Danihelka", "Ivo", ""], ["Viola", "Fabio", ""], ["Guez", "Arthur", ""], ["Schmitt", "Simon", ""], ["Sifre", "Laurent", ""], ["Weber", "Theophane", ""], ["Silver", "David", ""], ["van Hasselt", "Hado", ""]]}, {"id": "2104.06163", "submitter": "Takato Okudo", "authors": "Takato Okudo and Seiji Yamada", "title": "Reward Shaping with Dynamic Trajectory Aggregation", "comments": "accepted by The International Joint Conference on Neural\n  Networks(IJCNN2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reinforcement learning, which acquires a policy maximizing long-term rewards,\nhas been actively studied. Unfortunately, this learning type is too slow and\ndifficult to use in practical situations because the state-action space becomes\nhuge in real environments. The essential factor for learning efficiency is\nrewards. Potential-based reward shaping is a basic method for enriching\nrewards. This method is required to define a specific real-value function\ncalled a potential function for every domain. It is often difficult to\nrepresent the potential function directly. SARSA-RS learns the potential\nfunction and acquires it. However, SARSA-RS can only be applied to the simple\nenvironment. The bottleneck of this method is the aggregation of states to make\nabstract states since it is almost impossible for designers to build an\naggregation function for all states. We propose a trajectory aggregation that\nuses subgoal series. This method dynamically aggregates states in an episode\nduring trial and error with only the subgoal series and subgoal identification\nfunction. It makes designer effort minimal and the application to environments\nwith high-dimensional observations possible. We obtained subgoal series from\nparticipants for experiments. We conducted the experiments in three domains,\nfour-rooms(discrete states and discrete actions), pinball(continuous and\ndiscrete), and picking(both continuous). We compared our method with a baseline\nreinforcement learning algorithm and other subgoal-based methods, including\nrandom subgoal and naive subgoal-based reward shaping. As a result, our reward\nshaping outperformed all other methods in learning efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 13:07:48 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Okudo", "Takato", ""], ["Yamada", "Seiji", ""]]}, {"id": "2104.06172", "submitter": "Gilles Audemard", "authors": "Gilles Audemard, Steve Bellart, Louenas Bounia, Fr\\'ed\\'eric Koriche,\n  Jean-Marie Lagniez, Pierre Marquis", "title": "On the Computational Intelligibility of Boolean Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we investigate the computational intelligibility of Boolean\nclassifiers, characterized by their ability to answer XAI queries in polynomial\ntime. The classifiers under consideration are decision trees, DNF formulae,\ndecision lists, decision rules, tree ensembles, and Boolean neural nets. Using\n9 XAI queries, including both explanation queries and verification queries, we\nshow the existence of large intelligibility gap between the families of\nclassifiers. On the one hand, all the 9 XAI queries are tractable for decision\ntrees. On the other hand, none of them is tractable for DNF formulae, decision\nlists, random forests, boosted decision trees, Boolean multilayer perceptrons,\nand binarized neural networks.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 13:24:39 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Audemard", "Gilles", ""], ["Bellart", "Steve", ""], ["Bounia", "Louenas", ""], ["Koriche", "Fr\u00e9d\u00e9ric", ""], ["Lagniez", "Jean-Marie", ""], ["Marquis", "Pierre", ""]]}, {"id": "2104.06174", "submitter": "Lingzhi He", "authors": "Lingzhi He, Hongguang Zhu, Feng Li, Huihui Bai, Runmin Cong, Chunjie\n  Zhang, Chunyu Lin, Meiqin Liu, Yao Zhao", "title": "Towards Fast and Accurate Real-World Depth Super-Resolution: Benchmark\n  Dataset and Baseline", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Depth maps obtained by commercial depth sensors are always in low-resolution,\nmaking it difficult to be used in various computer vision tasks. Thus, depth\nmap super-resolution (SR) is a practical and valuable task, which upscales the\ndepth map into high-resolution (HR) space. However, limited by the lack of\nreal-world paired low-resolution (LR) and HR depth maps, most existing methods\nuse downsampling to obtain paired training samples. To this end, we first\nconstruct a large-scale dataset named \"RGB-D-D\", which can greatly promote the\nstudy of depth map SR and even more depth-related real-world tasks. The \"D-D\"\nin our dataset represents the paired LR and HR depth maps captured from mobile\nphone and Lucid Helios respectively ranging from indoor scenes to challenging\noutdoor scenes. Besides, we provide a fast depth map super-resolution (FDSR)\nbaseline, in which the high-frequency component adaptively decomposed from RGB\nimage to guide the depth map SR. Extensive experiments on existing public\ndatasets demonstrate the effectiveness and efficiency of our network compared\nwith the state-of-the-art methods. Moreover, for the real-world LR depth maps,\nour algorithm can produce more accurate HR depth maps with clearer boundaries\nand to some extent correct the depth value errors.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 13:27:26 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["He", "Lingzhi", ""], ["Zhu", "Hongguang", ""], ["Li", "Feng", ""], ["Bai", "Huihui", ""], ["Cong", "Runmin", ""], ["Zhang", "Chunjie", ""], ["Lin", "Chunyu", ""], ["Liu", "Meiqin", ""], ["Zhao", "Yao", ""]]}, {"id": "2104.06214", "submitter": "Zhe Zhou", "authors": "Zhe Zhou, Bizhao Shi, Zhe Zhang, Yijin Guan, Guangyu Sun, Guojie Luo", "title": "BlockGNN: Towards Efficient GNN Acceleration Using Block-Circulant\n  Weight Matrices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Graph Neural Networks (GNNs) appear to be state-of-the-art\nalgorithms for analyzing non-euclidean graph data. By applying deep-learning to\nextract high-level representations from graph structures, GNNs achieve\nextraordinary accuracy and great generalization ability in various tasks.\nHowever, with the ever-increasing graph sizes, more and more complicated GNN\nlayers, and higher feature dimensions, the computational complexity of GNNs\ngrows exponentially. How to inference GNNs in real time has become a\nchallenging problem, especially for some resource-limited edge-computing\nplatforms.\n  To tackle this challenge, we propose BlockGNN, a software-hardware co-design\napproach to realize efficient GNN acceleration. At the algorithm level, we\npropose to leverage block-circulant weight matrices to greatly reduce the\ncomplexity of various GNN models. At the hardware design level, we propose a\npipelined CirCore architecture, which supports efficient block-circulant\nmatrices computation. Basing on CirCore, we present a novel BlockGNN\naccelerator to compute various GNNs with low latency. Moreover, to determine\nthe optimal configurations for diverse deployed tasks, we also introduce a\nperformance and resource model that helps choose the optimal hardware\nparameters automatically. Comprehensive experiments on the ZC706 FPGA platform\ndemonstrate that on various GNN tasks, BlockGNN achieves up to $8.3\\times$\nspeedup compared to the baseline HyGCN architecture and $111.9\\times$ energy\nreduction compared to the Intel Xeon CPU platform.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 14:09:22 GMT"}, {"version": "v2", "created": "Fri, 25 Jun 2021 16:43:09 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Zhou", "Zhe", ""], ["Shi", "Bizhao", ""], ["Zhang", "Zhe", ""], ["Guan", "Yijin", ""], ["Sun", "Guangyu", ""], ["Luo", "Guojie", ""]]}, {"id": "2104.06220", "submitter": "Pedro M. Fernandes", "authors": "Pedro M. Fernandes, Manuel Lopes, Rui Prada", "title": "Agents for Automated User Experience Testing", "comments": null, "journal-ref": null, "doi": "10.1109/ICSTW52544.2021.00049", "report-no": null, "categories": "cs.AI cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The automation of functional testing in software has allowed developers to\ncontinuously check for negative impacts on functionality throughout the\niterative phases of development. This is not the case for User eXperience (UX),\nwhich has hitherto relied almost exclusively on testing with real users. User\ntesting is a slow endeavour that can become a bottleneck for development of\ninteractive systems. To address this problem, we here propose an agent based\napproach for automatic UX testing. We develop agents with basic problem solving\nskills and a core affect model, allowing us to model an artificial affective\nstate as they traverse different levels of a game. Although this research is\nstill at a primordial state, we believe the results here presented make a\nstrong case for the use of intelligent agents endowed with affective computing\nmodels for automating UX testing.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 14:13:28 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Fernandes", "Pedro M.", ""], ["Lopes", "Manuel", ""], ["Prada", "Rui", ""]]}, {"id": "2104.06239", "submitter": "Daniel Fern\\'andez-Gonz\\'alez", "authors": "Daniel Fern\\'andez-Gonz\\'alez and Carlos G\\'omez-Rodr\\'iguez", "title": "Reducing Discontinuous to Continuous Parsing with Pointer Network\n  Reordering", "comments": "8 pages (incl. appendices)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discontinuous constituent parsers have always lagged behind continuous\napproaches in terms of accuracy and speed, as the presence of constituents with\ndiscontinuous yield introduces extra complexity to the task. However, a\ndiscontinuous tree can be converted into a continuous variant by reordering\ntokens. Based on that, we propose to reduce discontinuous parsing to a\ncontinuous problem, which can then be directly solved by any off-the-shelf\ncontinuous parser. To that end, we develop a Pointer Network capable of\naccurately generating the continuous token arrangement for a given input\nsentence and define a bijective function to recover the original order.\nExperiments on the main benchmarks with two continuous parsers prove that our\napproach is on par in accuracy with purely discontinuous state-of-the-art\nalgorithms, but considerably faster.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 14:32:59 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Fern\u00e1ndez-Gonz\u00e1lez", "Daniel", ""], ["G\u00f3mez-Rodr\u00edguez", "Carlos", ""]]}, {"id": "2104.06255", "submitter": "Ali Siahkoohi", "authors": "Ali Siahkoohi and Felix J. Herrmann", "title": "Learning by example: fast reliability-aware seismic imaging with\n  normalizing flows", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.geo-ph cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Uncertainty quantification provides quantitative measures on the reliability\nof candidate solutions of ill-posed inverse problems. Due to their sequential\nnature, Monte Carlo sampling methods require large numbers of sampling steps\nfor accurate Bayesian inference and are often computationally infeasible for\nlarge-scale inverse problems, such as seismic imaging. Our main contribution is\na data-driven variational inference approach where we train a normalizing flow\n(NF), a type of invertible neural net, capable of cheaply sampling the\nposterior distribution given previously unseen seismic data from neighboring\nsurveys. To arrive at this result, we train the NF on pairs of low- and\nhigh-fidelity migrated images. In our numerical example, we obtain\nhigh-fidelity images from the Parihaka dataset and low-fidelity images are\nderived from these images through the process of demigration, followed by\nadding noise and migration. During inference, given shot records from a new\nneighboring seismic survey, we first compute the reverse-time migration image.\nNext, by feeding this low-fidelity migrated image to the NF we gain access to\nsamples from the posterior distribution virtually for free. We use these\nsamples to compute a high-fidelity image including a first assessment of the\nimage's reliability. To our knowledge, this is the first attempt to train a\nconditional network on what we know from neighboring images to improve the\ncurrent image and assess its reliability.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 15:13:45 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Siahkoohi", "Ali", ""], ["Herrmann", "Felix J.", ""]]}, {"id": "2104.06309", "submitter": "Hadi Sarieddeen Dr.", "authors": "Sara Helal, Hadi Sarieddeen, Hayssam Dahrouj, Tareq Y. Al-Naffouri,\n  Mohamed Slim Alouini", "title": "Signal Processing and Machine Learning Techniques for Terahertz Sensing:\n  An Overview", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the recent progress in Terahertz (THz) signal generation and\nradiation methods, joint THz communications and sensing applications are\nshaping the future of wireless systems. Towards this end, THz spectroscopy is\nexpected to be carried over user equipment devices to identify material and\ngaseous components of interest. THz-specific signal processing techniques\nshould complement this re-surged interest in THz sensing for efficient\nutilization of the THz band. In this paper, we present an overview of these\ntechniques, with an emphasis on signal pre-processing (standard normal variate\nnormalization, min-max normalization, and Savitzky-Golay filtering), feature\nextraction (principal component analysis, partial least squares, t-distributed\nstochastic neighbor embedding, and nonnegative matrix factorization), and\nclassification techniques (support vector machines, k-nearest neighbor,\ndiscriminant analysis, and naive Bayes). We also address the effectiveness of\ndeep learning techniques by exploring their promising sensing capabilities at\nthe THz band. Lastly, we investigate the performance and complexity trade-offs\nof the studied methods in the context of joint communications and sensing; we\nmotivate the corresponding use-cases, and we present few future research\ndirections in the field.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 01:38:34 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Helal", "Sara", ""], ["Sarieddeen", "Hadi", ""], ["Dahrouj", "Hayssam", ""], ["Al-Naffouri", "Tareq Y.", ""], ["Alouini", "Mohamed Slim", ""]]}, {"id": "2104.06310", "submitter": "Umberto Michelucci", "authors": "Francesca Venturini and Michela Sperti and Umberto Michelucci and Ivo\n  Herzig and Michael Baumgartner and Josep Palau Caballero and Arturo Jimenez\n  and and Marco Agostino Deriu", "title": "Exploration of Spanish Olive Oil Quality with a Miniaturized Low-Cost\n  Fluorescence Sensor and Machine Learning Techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Extra virgin olive oil (EVOO) is the highest quality of olive oil and is\ncharacterized by highly beneficial nutritional properties. The large increase\nin both consumption and fraud, for example through adulteration, creates new\nchallenges and an increasing demand for developing new quality assessment\nmethodologies that are easier and cheaper to perform. As of today, the\ndetermination of olive oil quality is performed by producers through chemical\nanalysis and organoleptic evaluation. The chemical analysis requires the\nadvanced equipment and chemical knowledge of certified laboratories, and has\ntherefore a limited accessibility. In this work a minimalist, portable and\nlow-cost sensor is presented, which can perform olive oil quality assessment\nusing fluorescence spectroscopy. The potential of the proposed technology is\nexplored by analyzing several olive oils of different quality levels, EVOO,\nvirgin olive oil (VOO), and lampante olive oil (LOO). The spectral data were\nanalyzed using a large number of machine learning methods, including artificial\nneural networks. The analysis performed in this work demonstrates the\npossibility of performing classification of olive oil in the three mentioned\nclasses with an accuracy of 100$\\%$. These results confirm that this minimalist\nlow-cost sensor has the potential of substituting expensive and complex\nchemical analysis.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 11:50:33 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Venturini", "Francesca", ""], ["Sperti", "Michela", ""], ["Michelucci", "Umberto", ""], ["Herzig", "Ivo", ""], ["Baumgartner", "Michael", ""], ["Caballero", "Josep Palau", ""], ["Jimenez", "Arturo", ""], ["Deriu", "and Marco Agostino", ""]]}, {"id": "2104.06313", "submitter": "Yang Gao", "authors": "Yang Gao, Yi-Fan Li, Yu Lin, Charu Aggarwal, Latifur Khan", "title": "SetConv: A New Approach for Learning from Imbalanced Data", "comments": "Accepted by EMNLP 2020 (11 pages, 9 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  For many real-world classification problems, e.g., sentiment classification,\nmost existing machine learning methods are biased towards the majority class\nwhen the Imbalance Ratio (IR) is high. To address this problem, we propose a\nset convolution (SetConv) operation and an episodic training strategy to\nextract a single representative for each class, so that classifiers can later\nbe trained on a balanced class distribution. We prove that our proposed\nalgorithm is permutation-invariant despite the order of inputs, and experiments\non multiple large-scale benchmark text datasets show the superiority of our\nproposed framework when compared to other SOTA methods.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 22:33:30 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Gao", "Yang", ""], ["Li", "Yi-Fan", ""], ["Lin", "Yu", ""], ["Aggarwal", "Charu", ""], ["Khan", "Latifur", ""]]}, {"id": "2104.06323", "submitter": "Dan Ley", "authors": "Dan Ley, Umang Bhatt, Adrian Weller", "title": "{\\delta}-CLUE: Diverse Sets of Explanations for Uncertainty Estimates", "comments": "Appeared as a workshop paper at ICLR 2021 (Responsible AI | Secure ML\n  | Robust ML)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To interpret uncertainty estimates from differentiable probabilistic models,\nrecent work has proposed generating Counterfactual Latent Uncertainty\nExplanations (CLUEs). However, for a single input, such approaches could output\na variety of explanations due to the lack of constraints placed on the\nexplanation. Here we augment the original CLUE approach, to provide what we\ncall $\\delta$-CLUE. CLUE indicates $\\it{one}$ way to change an input, while\nremaining on the data manifold, such that the model becomes more confident\nabout its prediction. We instead return a $\\it{set}$ of plausible CLUEs:\nmultiple, diverse inputs that are within a $\\delta$ ball of the original input\nin latent space, all yielding confident predictions.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 16:03:27 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 08:10:33 GMT"}, {"version": "v3", "created": "Sat, 24 Apr 2021 14:08:57 GMT"}, {"version": "v4", "created": "Tue, 27 Apr 2021 15:23:09 GMT"}, {"version": "v5", "created": "Sat, 8 May 2021 09:29:40 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Ley", "Dan", ""], ["Bhatt", "Umang", ""], ["Weller", "Adrian", ""]]}, {"id": "2104.06344", "submitter": "Manling Li", "authors": "Manling Li, Sha Li, Zhenhailong Wang, Lifu Huang, Kyunghyun Cho, Heng\n  Ji, Jiawei Han, Clare Voss", "title": "Future is not One-dimensional: Graph Modeling based Complex Event Schema\n  Induction for Event Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Event schemas encode knowledge of stereotypical structures of events and\ntheir connections. As events unfold, schemas are crucial to act as a\nscaffolding. Previous work on event schema induction either focuses on atomic\nevents or linear temporal event sequences, ignoring the interplay between\nevents via arguments and argument relations. We introduce the concept of\nTemporal Complex Event Schema: a graph-based schema representation that\nencompasses events, arguments, temporal connections and argument relations.\nAdditionally, we propose a Temporal Event Graph Model that models the emergence\nof event instances following the temporal complex event schema. To build and\nevaluate such schemas, we release a new schema learning corpus containing 6,399\ndocuments accompanied with event graphs, and manually constructed gold schemas.\nIntrinsic evaluation by schema matching and instance graph perplexity, prove\nthe superior quality of our probabilistic graph schema library compared to\nlinear representations. Extrinsic evaluation on schema-guided event prediction\nfurther demonstrates the predictive power of our event graph model,\nsignificantly surpassing human schemas and baselines by more than 17.8% on\nHITS@1.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 16:41:05 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 17:14:37 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Li", "Manling", ""], ["Li", "Sha", ""], ["Wang", "Zhenhailong", ""], ["Huang", "Lifu", ""], ["Cho", "Kyunghyun", ""], ["Ji", "Heng", ""], ["Han", "Jiawei", ""], ["Voss", "Clare", ""]]}, {"id": "2104.06374", "submitter": "Aly El Gamal", "authors": "Ahmed P. Mohamed, Abu Shafin Mohammad Mahdee Jameel, Aly El Gamal", "title": "Knowledge Distillation For Wireless Edge Learning", "comments": "5 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a framework for predicting frame errors in the\ncollaborative spectrally congested wireless environments of the DARPA Spectrum\nCollaboration Challenge (SC2) via a recently collected dataset. We employ\ndistributed deep edge learning that is shared among edge nodes and a central\ncloud. Using this close-to-practice dataset, we find that widely used federated\nlearning approaches, specially those that are privacy preserving, are worse\nthan local training for a wide range of settings. We hence utilize the\nsynthetic minority oversampling technique to maintain privacy via avoiding the\ntransfer of local data to the cloud, and utilize knowledge distillation with an\naim to benefit from high cloud computing and storage capabilities. The proposed\nframework achieves overall better performance than both local and federated\ntraining approaches, while being robust against catastrophic failures as well\nas challenging channel conditions that result in high frame error rates.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 22:20:08 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Mohamed", "Ahmed P.", ""], ["Jameel", "Abu Shafin Mohammad Mahdee", ""], ["Gamal", "Aly El", ""]]}, {"id": "2104.06375", "submitter": "Aly El Gamal", "authors": "Jinho Yi and Aly El Gamal", "title": "Gradient-based Adversarial Deep Modulation Classification with\n  Data-driven Subsampling", "comments": "5 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic modulation classification can be a core component for intelligent\nspectrally efficient wireless communication networks, and deep learning\ntechniques have recently been shown to deliver superior performance to\nconventional model-based strategies, particularly when distinguishing between a\nlarge number of modulation types. However, such deep learning techniques have\nalso been recently shown to be vulnerable to gradient-based adversarial attacks\nthat rely on subtle input perturbations, which would be particularly feasible\nin a wireless setting via jamming. One such potent attack is the one known as\nthe Carlini-Wagner attack, which we consider in this work. We further consider\na data-driven subsampling setting, where several recently introduced\ndeep-learning-based algorithms are employed to select a subset of samples that\nlead to reducing the final classifier's training time with minimal loss in\naccuracy. In this setting, the attacker has to make an assumption about the\nemployed subsampling strategy, in order to calculate the loss gradient. Based\non state of the art techniques available to both the attacker and defender, we\nevaluate best strategies under various assumptions on the knowledge of the\nother party's strategy. Interestingly, in presence of knowledgeable attackers,\nwe identify computational cost reduction opportunities for the defender with no\nor minimal loss in performance.\n", "versions": [{"version": "v1", "created": "Sat, 3 Apr 2021 22:28:04 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Yi", "Jinho", ""], ["Gamal", "Aly El", ""]]}, {"id": "2104.06392", "submitter": "R. Kenny Jones", "authors": "R. Kenny Jones, David Charatan, Paul Guerrero, Niloy J. Mitra, Daniel\n  Ritchie", "title": "ShapeMOD: Macro Operation Discovery for 3D Shape Programs", "comments": "SIGGRAPH 2021. Project Page: https://rkjones4.github.io/shapeMOD.html", "journal-ref": null, "doi": "10.1145/3450626.3459821", "report-no": null, "categories": "cs.GR cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A popular way to create detailed yet easily controllable 3D shapes is via\nprocedural modeling, i.e. generating geometry using programs. Such programs\nconsist of a series of instructions along with their associated parameter\nvalues. To fully realize the benefits of this representation, a shape program\nshould be compact and only expose degrees of freedom that allow for meaningful\nmanipulation of output geometry. One way to achieve this goal is to design\nhigher-level macro operators that, when executed, expand into a series of\ncommands from the base shape modeling language. However, manually authoring\nsuch macros, much like shape programs themselves, is difficult and largely\nrestricted to domain experts. In this paper, we present ShapeMOD, an algorithm\nfor automatically discovering macros that are useful across large datasets of\n3D shape programs. ShapeMOD operates on shape programs expressed in an\nimperative, statement-based language. It is designed to discover macros that\nmake programs more compact by minimizing the number of function calls and free\nparameters required to represent an input shape collection. We run ShapeMOD on\nmultiple collections of programs expressed in a domain-specific language for 3D\nshape structures. We show that it automatically discovers a concise set of\nmacros that abstract out common structural and parametric patterns that\ngeneralize over large shape collections. We also demonstrate that the macros\nfound by ShapeMOD improve performance on downstream tasks including shape\ngenerative modeling and inferring programs from point clouds. Finally, we\nconduct a user study that indicates that ShapeMOD's discovered macros make\ninteractive shape editing more efficient.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 17:54:03 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 20:56:17 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Jones", "R. Kenny", ""], ["Charatan", "David", ""], ["Guerrero", "Paul", ""], ["Mitra", "Niloy J.", ""], ["Ritchie", "Daniel", ""]]}, {"id": "2104.06393", "submitter": "Liang Ding", "authors": "Di Wu, Yiren Chen, Liang Ding, Dacheng Tao", "title": "Bridging the Gap Between Clean Data Training and Real-World Inference\n  for Spoken Language Understanding", "comments": "Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spoken language understanding (SLU) system usually consists of various\npipeline components, where each component heavily relies on the results of its\nupstream ones. For example, Intent detection (ID), and slot filling (SF)\nrequire its upstream automatic speech recognition (ASR) to transform the voice\ninto text. In this case, the upstream perturbations, e.g. ASR errors,\nenvironmental noise and careless user speaking, will propagate to the ID and SF\nmodels, thus deteriorating the system performance. Therefore, the\nwell-performing SF and ID models are expected to be noise resistant to some\nextent. However, existing models are trained on clean data, which causes a\n\\textit{gap between clean data training and real-world inference.} To bridge\nthe gap, we propose a method from the perspective of domain adaptation, by\nwhich both high- and low-quality samples are embedding into similar vector\nspace. Meanwhile, we design a denoising generation model to reduce the impact\nof the low-quality samples. Experiments on the widely-used dataset, i.e. Snips,\nand large scale in-house dataset (10 million training examples) demonstrate\nthat this method not only outperforms the baseline models on real-world (noisy)\ncorpus but also enhances the robustness, that is, it produces high-quality\nresults under a noisy environment. The source code will be released.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 17:54:33 GMT"}], "update_date": "2021-04-14", "authors_parsed": [["Wu", "Di", ""], ["Chen", "Yiren", ""], ["Ding", "Liang", ""], ["Tao", "Dacheng", ""]]}, {"id": "2104.06410", "submitter": "Takato Okudo", "authors": "Takato Okudo and Seiji Yamada", "title": "Reward Shaping with Subgoals for Social Navigation", "comments": "arXiv admin note: substantial text overlap with arXiv:2104.06163", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Social navigation has been gaining attentions with the growth in machine\nintelligence. Since reinforcement learning can select an action in the\nprediction phase at a low computational cost, it has been formulated in a\nsocial navigation tasks. However, reinforcement learning takes an enormous\nnumber of iterations until acquiring a behavior policy in the learning phase.\nThis negatively affects the learning of robot behaviors in the real world. In\nparticular, social navigation includes humans who are unpredictable moving\nobstacles in an environment. We proposed a reward shaping method with subgoals\nto accelerate learning. The main part is an aggregation method that use\nsubgoals to shape a reinforcement learning algorithm. We performed a learning\nexperiment with a social navigation task in which a robot avoided collisions\nand then reached its goal. The experimental results show that our method\nimproved the learning efficiency from a base algorithm in the task.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 13:52:58 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Okudo", "Takato", ""], ["Yamada", "Seiji", ""]]}, {"id": "2104.06411", "submitter": "Takato Okudo", "authors": "Takato Okudo and Seiji Yamada", "title": "Subgoal-based Reward Shaping to Improve Efficiency in Reinforcement\n  Learning", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible. arXiv admin note: substantial text overlap with\n  arXiv:2104.06163", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reinforcement learning, which acquires a policy maximizing long-term rewards,\nhas been actively studied. Unfortunately, this learning type is too slow and\ndifficult to use in practical situations because the state-action space becomes\nhuge in real environments. Many studies have incorporated human knowledge into\nreinforcement Learning. Though human knowledge on trajectories is often used, a\nhuman could be asked to control an AI agent, which can be difficult. Knowledge\non subgoals may lessen this requirement because humans need only to consider a\nfew representative states on an optimal trajectory in their minds. The\nessential factor for learning efficiency is rewards. Potential-based reward\nshaping is a basic method for enriching rewards. However, it is often difficult\nto incorporate subgoals for accelerating learning over potential-based reward\nshaping. This is because the appropriate potentials are not intuitive for\nhumans. We extend potential-based reward shaping and propose a subgoal-based\nreward shaping. The method makes it easier for human trainers to share their\nknowledge of subgoals. To evaluate our method, we obtained a subgoal series\nfrom participants and conducted experiments in three domains,\nfour-rooms(discrete states and discrete actions), pinball(continuous and\ndiscrete), and picking(both continuous). We compared our method with a baseline\nreinforcement learning algorithm and other subgoal-based methods, including\nrandom subgoal and naive subgoal-based reward shaping. As a result, we found\nout that our reward shaping outperformed all other methods in learning\nefficiency.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 14:28:48 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Okudo", "Takato", ""], ["Yamada", "Seiji", ""]]}, {"id": "2104.06486", "submitter": "Jay DeYoung", "authors": "Jay DeYoung, Iz Beltagy, Madeleine van Zuylen, Bailey Kuehl, Lucy Lu\n  Wang", "title": "MS2: Multi-Document Summarization of Medical Studies", "comments": "8 pages of content, 20 pages including references and appendix. See\n  https://github.com/allenai/ms2/ for code,\n  https://ai2-s2-ms2.s3-us-west-2.amazonaws.com/ms_data_2021-04-12.zip for data\n  (1.8G, zipped)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To assess the effectiveness of any medical intervention, researchers must\nconduct a time-intensive and highly manual literature review. NLP systems can\nhelp to automate or assist in parts of this expensive process. In support of\nthis goal, we release MS^2 (Multi-Document Summarization of Medical Studies), a\ndataset of over 470k documents and 20k summaries derived from the scientific\nliterature. This dataset facilitates the development of systems that can assess\nand aggregate contradictory evidence across multiple studies, and is the first\nlarge-scale, publicly available multi-document summarization dataset in the\nbiomedical domain. We experiment with a summarization system based on BART,\nwith promising early results. We formulate our summarization inputs and targets\nin both free text and structured forms and modify a recently proposed metric to\nassess the quality of our system's generated summaries. Data and models are\navailable at https://github.com/allenai/ms2\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 19:59:34 GMT"}, {"version": "v2", "created": "Thu, 15 Apr 2021 16:09:21 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["DeYoung", "Jay", ""], ["Beltagy", "Iz", ""], ["van Zuylen", "Madeleine", ""], ["Kuehl", "Bailey", ""], ["Wang", "Lucy Lu", ""]]}, {"id": "2104.06498", "submitter": "Mohammadreza Begli", "authors": "Mohammadreza Begli, Farnaz Derakhshan", "title": "A multiagent based framework secured with layered SVM-based IDS for\n  remote healthcare systems", "comments": "12 pages in two-column format, 4 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Since the number of elderly and patients who are in hospitals and healthcare\ncenters are growing, providing efficient remote healthcare services seems very\nimportant. Currently, most such systems benefit from the distribution and\nautonomy features of multiagent systems and the structure of wireless sensor\nnetworks. On the one hand, securing the data of remote healthcare systems is\none of the most significant concerns; particularly recent types of research\nabout the security of remote healthcare systems keep them secure from\neavesdropping and data modification. On the other hand, existing remote\nhealthcare systems are still vulnerable against other common attacks of\nhealthcare networks such as Denial of Service (DoS) and User to Root (U2R)\nattacks, because they are managed remotely and based on the Internet.\nTherefore, in this paper, we propose a secure framework for remote healthcare\nsystems that consists of two phases. First, we design a healthcare system base\non multiagent technology to collect data from a sensor network. Then, in the\nsecond phase, a layered architecture of intrusion detection systems that uses\nSupport Vector Machine to learn the behavior of network traffic is applied.\nBased on our framework, we implement a secure remote healthcare system and\nevaluate this system against the frequent attacks of healthcare networks such\nas Smurf, Buffer overflow, Neptune, and Pod attacks. In the end, evaluation\nparameters of the layered architecture of intrusion detection systems prove the\nefficiency and correctness of our proposed framework.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 20:34:38 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Begli", "Mohammadreza", ""], ["Derakhshan", "Farnaz", ""]]}, {"id": "2104.06510", "submitter": "Pedro Henrique Suruagy Perrusi", "authors": "Pedro Henrique Suruagy Perrusi, Anna Cazzaniga, Paul Baksic, Eleonora\n  Tagliabue, Elena de Momi, Hadrien Courtecuisse", "title": "Robotic needle steering in deformable tissues with extreme learning\n  machines", "comments": null, "journal-ref": "AUTOMED 2021, Jun 2021, Basel, Switzerland", "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Control strategies for robotic needle steering in soft tissues must account\nfor complex interactions between the needle and the tissue to achieve accurate\nneedle tip positioning. Recent findings show faster robotic command rate can\nimprove the control stability in realistic scenarios. This study proposes the\nuse of Extreme Learning Machines to provide fast commands for robotic needle\nsteering. A synthetic dataset based on the inverse finite element simulation\ncontrol framework is used to train the model. Results show the model is capable\nto infer commands 66% faster than the inverse simulation and reaches acceptable\nprecision even on previously unseen trajectories.\n", "versions": [{"version": "v1", "created": "Fri, 2 Apr 2021 07:04:29 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Perrusi", "Pedro Henrique Suruagy", ""], ["Cazzaniga", "Anna", ""], ["Baksic", "Paul", ""], ["Tagliabue", "Eleonora", ""], ["de Momi", "Elena", ""], ["Courtecuisse", "Hadrien", ""]]}, {"id": "2104.06517", "submitter": "Eunjeong Koh", "authors": "Eunjeong Koh and Shlomo Dubnov", "title": "Comparison and Analysis of Deep Audio Embeddings for Music Emotion\n  Recognition", "comments": "AAAI Workshop on Affective Content Analysis 2021 Camera Ready Version", "journal-ref": "AAAI 2021", "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.LG cs.MM eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Emotion is a complicated notion present in music that is hard to capture even\nwith fine-tuned feature engineering. In this paper, we investigate the utility\nof state-of-the-art pre-trained deep audio embedding methods to be used in the\nMusic Emotion Recognition (MER) task. Deep audio embedding methods allow us to\nefficiently capture the high dimensional features into a compact\nrepresentation. We implement several multi-class classifiers with deep audio\nembeddings to predict emotion semantics in music. We investigate the\neffectiveness of L3-Net and VGGish deep audio embedding methods for music\nemotion inference over four music datasets. The experiments with several\nclassifiers on the task show that the deep audio embedding solutions can\nimprove the performances of the previous baseline MER models. We conclude that\ndeep audio embeddings represent musical emotion semantics for the MER task\nwithout expert human engineering.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 21:09:54 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Koh", "Eunjeong", ""], ["Dubnov", "Shlomo", ""]]}, {"id": "2104.06521", "submitter": "Haonan Yu", "authors": "Haonan Yu, Wei Xu, Haichao Zhang", "title": "TAAC: Temporally Abstract Actor-Critic for Continuous Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose temporally abstract actor-critic (TAAC), an off-policy RL\nalgorithm that incorporates closed-loop temporal abstraction into the\nactor-critic framework in a simple manner. TAAC adds a second-stage binary\npolicy to choose between the previous action and a new action output by an\nactor. Crucially, its act-or-repeat decision hinges on the actually sampled\naction instead of the expected behavior of the actor. This post-acting\nswitching scheme let the overall policy make more informed decisions. TAAC has\ntwo important features: persistent exploration and a new compare-through Q\noperator for multi-step TD backup. We demonstrate TAAC's advantages over\nseveral strong baselines across 5 different categories of 14 continuous control\ntasks. Code is available at https://github.com/hnyu/taac.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 21:24:44 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 02:57:36 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Yu", "Haonan", ""], ["Xu", "Wei", ""], ["Zhang", "Haichao", ""]]}, {"id": "2104.06555", "submitter": "Dominic Widdows", "authors": "Dominic Widdows, Kristen Howell, Trevor Cohen", "title": "Should Semantic Vector Composition be Explicit? Can it be Linear?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Vector representations have become a central element in semantic language\nmodelling, leading to mathematical overlaps with many fields including quantum\ntheory. Compositionality is a core goal for such representations: given\nrepresentations for 'wet' and 'fish', how should the concept 'wet fish' be\nrepresented?\n  This position paper surveys this question from two points of view. The first\nconsiders the question of whether an explicit mathematical representation can\nbe successful using only tools from within linear algebra, or whether other\nmathematical tools are needed. The second considers whether semantic vector\ncomposition should be explicitly described mathematically, or whether it can be\na model-internal side-effect of training a neural network.\n  A third and newer question is whether a compositional model can be\nimplemented on a quantum computer. Given the fundamentally linear nature of\nquantum mechanics, we propose that these questions are related, and that this\nsurvey may help to highlight candidate operations for future quantum\nimplementation.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 23:58:26 GMT"}, {"version": "v2", "created": "Sat, 8 May 2021 19:14:03 GMT"}, {"version": "v3", "created": "Tue, 11 May 2021 02:44:04 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Widdows", "Dominic", ""], ["Howell", "Kristen", ""], ["Cohen", "Trevor", ""]]}, {"id": "2104.06557", "submitter": "Sreya Francis", "authors": "Sreya Francis, Irene Tenison, Irina Rish", "title": "Towards Causal Federated Learning For Enhanced Robustness and Privacy", "comments": null, "journal-ref": "ICLR 2021 Distributed and Private Machine Learning(DPML) Workshop", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning is an emerging privacy-preserving distributed machine\nlearning approach to building a shared model by performing distributed training\nlocally on participating devices (clients) and aggregating the local models\ninto a global one. As this approach prevents data collection and aggregation,\nit helps in reducing associated privacy risks to a great extent. However, the\ndata samples across all participating clients are usually not independent and\nidentically distributed (non-iid), and Out of Distribution(OOD) generalization\nfor the learned models can be poor. Besides this challenge, federated learning\nalso remains vulnerable to various attacks on security wherein a few malicious\nparticipating entities work towards inserting backdoors, degrading the\ngenerated aggregated model as well as inferring the data owned by participating\nentities. In this paper, we propose an approach for learning invariant (causal)\nfeatures common to all participating clients in a federated learning setup and\nanalyze empirically how it enhances the Out of Distribution (OOD) accuracy as\nwell as the privacy of the final learned model.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 00:08:45 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Francis", "Sreya", ""], ["Tenison", "Irene", ""], ["Rish", "Irina", ""]]}, {"id": "2104.06563", "submitter": "Shiqing Wu", "authors": "Weihua Li, Yuxuan Hu, Shiqing Wu, Quan Bai, Edmund Lai", "title": "ABEM: An Adaptive Agent-based Evolutionary Approach for Mining\n  Influencers in Online Social Networks", "comments": "22 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A key step in influence maximization in online social networks is the\nidentification of a small number of users, known as influencers, who are able\nto spread influence quickly and widely to other users. The evolving nature of\nthe topological structure of these networks makes it difficult to locate and\nidentify these influencers. In this paper, we propose an adaptive agent-based\nevolutionary approach to address this problem in the context of both static and\ndynamic networks. This approach is shown to be able to adapt the solution as\nthe network evolves. It is also applicable to large-scale networks due to its\ndistributed framework. Evaluation of our approach is performed by using both\nsynthetic networks and real-world datasets. Experimental results demonstrate\nthat the proposed approach outperforms state-of-the-art seeding algorithms in\nterms of maximizing influence.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 00:31:08 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Li", "Weihua", ""], ["Hu", "Yuxuan", ""], ["Wu", "Shiqing", ""], ["Bai", "Quan", ""], ["Lai", "Edmund", ""]]}, {"id": "2104.06593", "submitter": "Yanni Ren", "authors": "Yanni Ren and Hangyu Deng and Hao Jiang and Jinglu Hu", "title": "A Semi-Supervised Classification Method of Apicomplexan Parasites and\n  Host Cell Using Contrastive Learning Strategy", "comments": "6 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common shortfall of supervised learning for medical imaging is the greedy\nneed for human annotations, which is often expensive and time-consuming to\nobtain. This paper proposes a semi-supervised classification method for three\nkinds of apicomplexan parasites and non-infected host cells microscopic images,\nwhich uses a small number of labeled data and a large number of unlabeled data\nfor training. There are two challenges in microscopic image recognition. The\nfirst is that salient structures of the microscopic images are more fuzzy and\nintricate than natural images' on a real-world scale. The second is that\ninsignificant textures, like background staining, lightness, and contrast\nlevel, vary a lot in samples from different clinical scenarios. To address\nthese challenges, we aim to learn a distinguishable and appearance-invariant\nrepresentation by contrastive learning strategy. On one hand, macroscopic\nimages, which share similar shape characteristics in morphology, are introduced\nto contrast for structure enhancement. On the other hand, different appearance\ntransformations, including color distortion and flittering, are utilized to\ncontrast for texture elimination. In the case where only 1% of microscopic\nimages are labeled, the proposed method reaches an accuracy of 94.90% in a\ngeneralized testing set.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 02:34:50 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Ren", "Yanni", ""], ["Deng", "Hangyu", ""], ["Jiang", "Hao", ""], ["Hu", "Jinglu", ""]]}, {"id": "2104.06600", "submitter": "Guangliang Li", "authors": "Jie Huang, Rongshun Juan, Randy Gomez, Keisuke Nakamura, Qixin Sha, Bo\n  He, Guangliang Li", "title": "GAN-Based Interactive Reinforcement Learning from Demonstration and\n  Human Evaluative Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep reinforcement learning (DRL) has achieved great successes in many\nsimulated tasks. The sample inefficiency problem makes applying traditional DRL\nmethods to real-world robots a great challenge. Generative Adversarial\nImitation Learning (GAIL) -- a general model-free imitation learning method,\nallows robots to directly learn policies from expert trajectories in large\nenvironments. However, GAIL shares the limitation of other imitation learning\nmethods that they can seldom surpass the performance of demonstrations. In this\npaper, to address the limit of GAIL, we propose GAN-Based Interactive\nReinforcement Learning (GAIRL) from demonstration and human evaluative feedback\nby combining the advantages of GAIL and interactive reinforcement learning. We\ntested our proposed method in six physics-based control tasks, ranging from\nsimple low-dimensional control tasks -- Cart Pole and Mountain Car, to\ndifficult high-dimensional tasks -- Inverted Double Pendulum, Lunar Lander,\nHopper and HalfCheetah. Our results suggest that with both optimal and\nsuboptimal demonstrations, a GAIRL agent can always learn a more stable policy\nwith optimal or close to optimal performance, while the performance of the GAIL\nagent is upper bounded by the performance of demonstrations or even worse than\nit. In addition, our results indicate the reason that GAIRL is superior over\nGAIL is the complementary effect of demonstrations and human evaluative\nfeedback.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 02:58:51 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Huang", "Jie", ""], ["Juan", "Rongshun", ""], ["Gomez", "Randy", ""], ["Nakamura", "Keisuke", ""], ["Sha", "Qixin", ""], ["He", "Bo", ""], ["Li", "Guangliang", ""]]}, {"id": "2104.06612", "submitter": "Seonho Park", "authors": "Seonho Park, Panos M. Pardalos", "title": "Deep Data Density Estimation through Donsker-Varadhan Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.IT math.IT math.PR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Estimating the data density is one of the challenging problems in deep\nlearning. In this paper, we present a simple yet effective method for\nestimating the data density using a deep neural network and the\nDonsker-Varadhan variational lower bound on the KL divergence. We show that the\noptimal critic function associated with the Donsker-Varadhan representation on\nthe KL divergence between the data and the uniform distribution can estimate\nthe data density. We also present the deep neural network-based modeling and\nits stochastic learning. The experimental results and possible applications of\nthe proposed method demonstrate that it is competitive with the previous\nmethods and has a lot of possibilities in applied to various applications.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 03:38:32 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Park", "Seonho", ""], ["Pardalos", "Panos M.", ""]]}, {"id": "2104.06622", "submitter": "Yuxin Chen", "authors": "Chinmaya Mahesh, Kristin Dona, David W. Miller, Yuxin Chen", "title": "Towards an Interpretable Data-driven Trigger System for High-throughput\n  Physics Facilities", "comments": "Appeared in the 3rd Workshop on Machine Learning and the Physical\n  Sciences, NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG hep-ex physics.ins-det", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-intensive science is increasingly reliant on real-time processing\ncapabilities and machine learning workflows, in order to filter and analyze the\nextreme volumes of data being collected. This is especially true at the energy\nand intensity frontiers of particle physics where bandwidths of raw data can\nexceed 100 Tb/s of heterogeneous, high-dimensional data sourced from hundreds\nof millions of individual sensors. In this paper, we introduce a new\ndata-driven approach for designing and optimizing high-throughput data\nfiltering and trigger systems such as those in use at physics facilities like\nthe Large Hadron Collider (LHC). Concretely, our goal is to design a\ndata-driven filtering system with a minimal run-time cost for determining which\ndata event to keep, while preserving (and potentially improving upon) the\ndistribution of the output as generated by the hand-designed trigger system. We\nintroduce key insights from interpretable predictive modeling and\ncost-sensitive learning in order to account for non-local inefficiencies in the\ncurrent paradigm and construct a cost-effective data filtering and trigger\nmodel that does not compromise physics coverage.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 05:01:32 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Mahesh", "Chinmaya", ""], ["Dona", "Kristin", ""], ["Miller", "David W.", ""], ["Chen", "Yuxin", ""]]}, {"id": "2104.06624", "submitter": "Jiangchao Yao", "authors": "Jiangchao Yao and Feng Wang and KunYang Jia and Bo Han and Jingren\n  Zhou and Hongxia Yang", "title": "Device-Cloud Collaborative Learning for Recommendation", "comments": "KDD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  With the rapid development of storage and computing power on mobile devices,\nit becomes critical and popular to deploy models on devices to save onerous\ncommunication latencies and to capture real-time features. While quite a lot of\nworks have explored to facilitate on-device learning and inference, most of\nthem focus on dealing with response delay or privacy protection. Little has\nbeen done to model the collaboration between the device and the cloud modeling\nand benefit both sides jointly. To bridge this gap, we are among the first\nattempts to study the Device-Cloud Collaborative Learning (DCCL) framework.\nSpecifically, we propose a novel MetaPatch learning approach on the device side\nto efficiently achieve \"thousands of people with thousands of models\" given a\ncentralized cloud model. Then, with billions of updated personalized device\nmodels, we propose a \"model-over-models\" distillation algorithm, namely\nMoMoDistill, to update the centralized cloud model. Our extensive experiments\nover a range of datasets with different settings demonstrate the effectiveness\nof such collaboration on both cloud and devices, especially its superiority to\nmodel long-tailed users.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 05:06:59 GMT"}, {"version": "v2", "created": "Sat, 29 May 2021 16:07:24 GMT"}, {"version": "v3", "created": "Tue, 1 Jun 2021 01:57:03 GMT"}, {"version": "v4", "created": "Wed, 2 Jun 2021 03:43:18 GMT"}, {"version": "v5", "created": "Wed, 16 Jun 2021 02:21:31 GMT"}, {"version": "v6", "created": "Thu, 17 Jun 2021 01:33:33 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Yao", "Jiangchao", ""], ["Wang", "Feng", ""], ["Jia", "KunYang", ""], ["Han", "Bo", ""], ["Zhou", "Jingren", ""], ["Yang", "Hongxia", ""]]}, {"id": "2104.06629", "submitter": "Huiqi Deng", "authors": "Huiqi Deng, Na Zou, Weifu Chen, Guocan Feng, Mengnan Du, Xia Hu", "title": "Mutual Information Preserving Back-propagation: Learn to Invert for\n  Faithful Attribution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Back propagation based visualizations have been proposed to interpret deep\nneural networks (DNNs), some of which produce interpretations with good visual\nquality. However, there exist doubts about whether these intuitive\nvisualizations are related to the network decisions. Recent studies have\nconfirmed this suspicion by verifying that almost all these modified\nback-propagation visualizations are not faithful to the model's decision-making\nprocess. Besides, these visualizations produce vague \"relative importance\nscores\", among which low values can't guarantee to be independent of the final\nprediction. Hence, it's highly desirable to develop a novel back-propagation\nframework that guarantees theoretical faithfulness and produces a quantitative\nattribution score with a clear understanding. To achieve the goal, we resort to\nmutual information theory to generate the interpretations, studying how much\ninformation of output is encoded in each input neuron. The basic idea is to\nlearn a source signal by back-propagation such that the mutual information\nbetween input and output should be as much as possible preserved in the mutual\ninformation between input and the source signal. In addition, we propose a\nMutual Information Preserving Inverse Network, termed MIP-IN, in which the\nparameters of each layer are recursively trained to learn how to invert. During\nthe inversion, forward Relu operation is adopted to adapt the general\ninterpretations to the specific input. We then empirically demonstrate that the\ninverted source signal satisfies completeness and minimality property, which\nare crucial for a faithful interpretation. Furthermore, the empirical study\nvalidates the effectiveness of interpretations generated by MIP-IN.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 05:20:48 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Deng", "Huiqi", ""], ["Zou", "Na", ""], ["Chen", "Weifu", ""], ["Feng", "Guocan", ""], ["Du", "Mengnan", ""], ["Hu", "Xia", ""]]}, {"id": "2104.06655", "submitter": "Yuan Pu", "authors": "Yuan Pu, Shaochen Wang, Rui Yang, Xin Yao, Bin Li", "title": "Decomposed Soft Actor-Critic Method for Cooperative Multi-Agent\n  Reinforcement Learning", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep reinforcement learning methods have shown great performance on many\nchallenging cooperative multi-agent tasks. Two main promising research\ndirections are multi-agent value function decomposition and multi-agent policy\ngradients. In this paper, we propose a new decomposed multi-agent soft\nactor-critic (mSAC) method, which effectively combines the advantages of the\naforementioned two methods. The main modules include decomposed Q network\narchitecture, discrete probabilistic policy and counterfactual advantage\nfunction (optinal). Theoretically, mSAC supports efficient off-policy learning\nand addresses credit assignment problem partially in both discrete and\ncontinuous action spaces. Tested on StarCraft II micromanagement cooperative\nmultiagent benchmark, we empirically investigate the performance of mSAC\nagainst its variants and analyze the effects of the different components.\nExperimental results demonstrate that mSAC significantly outperforms\npolicy-based approach COMA, and achieves competitive results with SOTA\nvalue-based approach Qmix on most tasks in terms of asymptotic perfomance\nmetric. In addition, mSAC achieves pretty good results on large action space\ntasks, such as 2c_vs_64zg and MMM2.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 07:02:40 GMT"}, {"version": "v2", "created": "Sat, 8 May 2021 03:34:07 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Pu", "Yuan", ""], ["Wang", "Shaochen", ""], ["Yang", "Rui", ""], ["Yao", "Xin", ""], ["Li", "Bin", ""]]}, {"id": "2104.06669", "submitter": "Varun Gangal", "authors": "Varun Gangal, Steven Y. Feng, Eduard Hovy, Teruko Mitamura", "title": "NAREOR: The Narrative Reordering Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose the task of Narrative Reordering(NAREOR) which involves rewriting\na given story in a different narrative order while preserving its plot,\nsemantic, and temporal aspects. We present a dataset, NAREORC, with over 1000\nhuman rewritings of stories within ROCStories in non-linear orders, and conduct\na detailed analysis of it. Further, we propose novel initial task-specific\ntraining methods and evaluation metrics. We perform experiments on NAREORC\nusing GPT-2 and Transformer models and conduct an extensive human evaluation.\nWe demonstrate that NAREOR is a challenging task with potential for further\nexploration.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 07:33:02 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Gangal", "Varun", ""], ["Feng", "Steven Y.", ""], ["Hovy", "Eduard", ""], ["Mitamura", "Teruko", ""]]}, {"id": "2104.06670", "submitter": "Yuan Gao", "authors": "Yuan Gao, Jiawei Li, Maoguo Gong, Yu Xie and A. K. Qin", "title": "Towards Explainable Multi-Party Learning: A Contrastive Knowledge\n  Sharing Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-party learning provides solutions for training joint models with\ndecentralized data under legal and practical constraints. However, traditional\nmulti-party learning approaches are confronted with obstacles such as system\nheterogeneity, statistical heterogeneity, and incentive design. How to deal\nwith these challenges and further improve the efficiency and performance of\nmulti-party learning has become an urgent problem to be solved. In this paper,\nwe propose a novel contrastive multi-party learning framework for knowledge\nrefinement and sharing with an accountable incentive mechanism. Since the\nexisting naive model parameter averaging method is contradictory to the\nlearning paradigm of neural networks, we simulate the process of human\ncognition and communication, and analogy multi-party learning as a many-to-one\nknowledge sharing problem. The approach is capable of integrating the acquired\nexplicit knowledge of each client in a transparent manner without privacy\ndisclosure, and it reduces the dependence on data distribution and\ncommunication environments. The proposed scheme achieves significant\nimprovement in model performance in a variety of scenarios, as we demonstrated\nthrough experiments on several real-world datasets.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 07:33:48 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 14:51:20 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Gao", "Yuan", ""], ["Li", "Jiawei", ""], ["Gong", "Maoguo", ""], ["Xie", "Yu", ""], ["Qin", "A. K.", ""]]}, {"id": "2104.06681", "submitter": "Konstantin Yakovlev S", "authors": "Konstantin Yakovlev, Anton Andreychuk", "title": "Towards Time-Optimal Any-Angle Path Planning With Dynamic Obstacles", "comments": "This is a pre-print of the paper accepted to ICAPS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Path finding is a well-studied problem in AI, which is often framed as graph\nsearch. Any-angle path finding is a technique that augments the initial graph\nwith additional edges to build shorter paths to the goal. Indeed, optimal\nalgorithms for any-angle path finding in static environments exist. However,\nwhen dynamic obstacles are present and time is the objective to be minimized,\nthese algorithms can no longer guarantee optimality. In this work, we elaborate\non why this is the case and what techniques can be used to solve the problem\noptimally. We present two algorithms, grounded in the same idea, that can\nobtain provably optimal solutions to the considered problem. One of them is a\nnaive algorithm and the other one is much more involved. We conduct a thorough\nempirical evaluation showing that, in certain setups, the latter algorithm\nmight be as fast as the previously-known greedy non-optimal solver while\nproviding solutions of better quality. In some (rare) cases, the difference in\ncost is up to 76%, while on average it is lower than one percent (the same cost\ndifference is typically observed between optimal and greedy any-angle solvers\nin static environments).\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 07:59:53 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Yakovlev", "Konstantin", ""], ["Andreychuk", "Anton", ""]]}, {"id": "2104.06683", "submitter": "Vikas Raunak", "authors": "Vikas Raunak, Arul Menezes and Marcin Junczys-Dowmunt", "title": "The Curious Case of Hallucinations in Neural Machine Translation", "comments": "Accepted to NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we study hallucinations in Neural Machine Translation (NMT),\nwhich lie at an extreme end on the spectrum of NMT pathologies. Firstly, we\nconnect the phenomenon of hallucinations under source perturbation to the\nLong-Tail theory of Feldman (2020), and present an empirically validated\nhypothesis that explains hallucinations under source perturbation. Secondly, we\nconsider hallucinations under corpus-level noise (without any source\nperturbation) and demonstrate that two prominent types of natural\nhallucinations (detached and oscillatory outputs) could be generated and\nexplained through specific corpus-level noise patterns. Finally, we elucidate\nthe phenomenon of hallucination amplification in popular data-generation\nprocesses such as Backtranslation and sequence-level Knowledge Distillation.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 08:09:57 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Raunak", "Vikas", ""], ["Menezes", "Arul", ""], ["Junczys-Dowmunt", "Marcin", ""]]}, {"id": "2104.06687", "submitter": "Yawei Wang", "authors": "Yawei Wang and Xiu Li", "title": "Reward function shape exploration in adversarial imitation learning: an\n  empirical study", "comments": "Accepted by ICAICA2021, the code will be available soon", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For adversarial imitation learning algorithms (AILs), no true rewards are\nobtained from the environment for learning the strategy. However, the pseudo\nrewards based on the output of the discriminator are still required. Given the\nimplicit reward bias problem in AILs, we design several representative reward\nfunction shapes and compare their performances by large-scale experiments. To\nensure our results' reliability, we conduct the experiments on a series of\nMujoco and Box2D continuous control tasks based on four different AILs.\nBesides, we also compare the performance of various reward function shapes\nusing varying numbers of expert trajectories. The empirical results reveal that\nthe positive logarithmic reward function works well in typical continuous\ncontrol tasks. In contrast, the so-called unbiased reward function is limited\nto specific kinds of tasks. Furthermore, several designed reward functions\nperform excellently in these environments as well.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 08:21:49 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Wang", "Yawei", ""], ["Li", "Xiu", ""]]}, {"id": "2104.06709", "submitter": "Damian Pascual", "authors": "Damian Pascual, Sandro Luck, Roger Wattenhofer", "title": "Towards BERT-based Automatic ICD Coding: Limitations and Opportunities", "comments": "Accepted at BioNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic ICD coding is the task of assigning codes from the International\nClassification of Diseases (ICD) to medical notes. These codes describe the\nstate of the patient and have multiple applications, e.g., computer-assisted\ndiagnosis or epidemiological studies. ICD coding is a challenging task due to\nthe complexity and length of medical notes. Unlike the general trend in\nlanguage processing, no transformer model has been reported to reach high\nperformance on this task. Here, we investigate in detail ICD coding using\nPubMedBERT, a state-of-the-art transformer model for biomedical language\nunderstanding. We find that the difficulty of fine-tuning the model on long\npieces of text is the main limitation for BERT-based models on ICD coding. We\nrun extensive experiments and show that despite the gap with current\nstate-of-the-art, pretrained transformers can reach competitive performance\nusing relatively small portions of text. We point at better methods to\naggregate information from long texts as the main need for improving BERT-based\nICD coding.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 09:12:53 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Pascual", "Damian", ""], ["Luck", "Sandro", ""], ["Wattenhofer", "Roger", ""]]}, {"id": "2104.06735", "submitter": "Marcin Chlebus", "authors": "Przemys{\\l}aw Biecek, Marcin Chlebus, Janusz Gajda, Alicja Gosiewska,\n  Anna Kozak, Dominik Ogonowski, Jakub Sztachelski, Piotr Wojewnik", "title": "Enabling Machine Learning Algorithms for Credit Scoring -- Explainable\n  Artificial Intelligence (XAI) methods for clear understanding complex\n  predictive models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.RM cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid development of advanced modelling techniques gives an opportunity to\ndevelop tools that are more and more accurate. However as usually, everything\ncomes with a price and in this case, the price to pay is to loose\ninterpretability of a model while gaining on its accuracy and precision. For\nmanagers to control and effectively manage credit risk and for regulators to be\nconvinced with model quality the price to pay is too high. In this paper, we\nshow how to take credit scoring analytics in to the next level, namely we\npresent comparison of various predictive models (logistic regression, logistic\nregression with weight of evidence transformations and modern artificial\nintelligence algorithms) and show that advanced tree based models give best\nresults in prediction of client default. What is even more important and\nvaluable we also show how to boost advanced models using techniques which allow\nto interpret them and made them more accessible for credit risk practitioners,\nresolving the crucial obstacle in widespread deployment of more complex, 'black\nbox' models like random forests, gradient boosted or extreme gradient boosted\ntrees. All this will be shown on the large dataset obtained from the Polish\nCredit Bureau to which all the banks and most of the lending companies in the\ncountry do report the credit files. In this paper the data from lending\ncompanies were used. The paper then compares state of the art best practices in\ncredit risk modelling with new advanced modern statistical tools boosted by the\nlatest developments in the field of interpretability and explainability of\nartificial intelligence algorithms. We believe that this is a valuable\ncontribution when it comes to presentation of different modelling tools but\nwhat is even more important it is showing which methods might be used to get\ninsight and understanding of AI methods in credit risk context.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 09:44:04 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Biecek", "Przemys\u0142aw", ""], ["Chlebus", "Marcin", ""], ["Gajda", "Janusz", ""], ["Gosiewska", "Alicja", ""], ["Kozak", "Anna", ""], ["Ogonowski", "Dominik", ""], ["Sztachelski", "Jakub", ""], ["Wojewnik", "Piotr", ""]]}, {"id": "2104.06737", "submitter": "Gregor Betz", "authors": "Gregor Betz", "title": "Natural-Language Multi-Agent Simulations of Argumentative Opinion\n  Dynamics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.MA cs.SI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper develops a natural-language agent-based model of argumentation\n(ABMA). Its artificial deliberative agents (ADAs) are constructed with the help\nof so-called neural language models recently developed in AI and computational\nlinguistics. ADAs are equipped with a minimalist belief system and may generate\nand submit novel contributions to a conversation. The natural-language ABMA\nallows us to simulate collective deliberation in English, i.e. with arguments,\nreasons, and claims themselves -- rather than with their mathematical\nrepresentations (as in formal models). This paper uses the natural-language\nABMA to test the robustness of formal reason-balancing models of argumentation\n[Maes & Flache 2013, Singer et al. 2019]: First of all, as long as ADAs remain\npassive, confirmation bias and homophily updating trigger polarization, which\nis consistent with results from formal models. However, once ADAs start to\nactively generate new contributions, the evolution of a conservation is\ndominated by properties of the agents *as authors*. This suggests that the\ncreation of new arguments, reasons, and claims critically affects a\nconversation and is of pivotal importance for understanding the dynamics of\ncollective deliberation. The paper closes by pointing out further fruitful\napplications of the model and challenges for future research.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 09:45:22 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Betz", "Gregor", ""]]}, {"id": "2104.06744", "submitter": "Nicolas Michael M\\\"uller", "authors": "Nicolas M. M\\\"uller, Simon Roschmann, Konstantin B\\\"ottinger", "title": "Defending against Adversarial Denial-of-Service Attacks", "comments": "Submitted to ACSAC DYNAMICS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data poisoning is one of the most relevant security threats against machine\nlearning and data-driven technologies. Since many applications rely on\nuntrusted training data, an attacker can easily craft malicious samples and\ninject them into the training dataset to degrade the performance of machine\nlearning models. As recent work has shown, such Denial-of-Service (DoS) data\npoisoning attacks are highly effective. To mitigate this threat, we propose a\nnew approach of detecting DoS poisoned instances. In comparison to related\nwork, we deviate from clustering and anomaly detection based approaches, which\noften suffer from the curse of dimensionality and arbitrary anomaly threshold\nselection. Rather, our defence is based on extracting information from the\ntraining data in such a generalized manner that we can identify poisoned\nsamples based on the information present in the unpoisoned portion of the data.\nWe evaluate our defence against two DoS poisoning attacks and seven datasets,\nand find that it reliably identifies poisoned instances. In comparison to\nrelated work, our defence improves false positive / false negative rates by at\nleast 50%, often more.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 09:52:36 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 14:32:01 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["M\u00fcller", "Nicolas M.", ""], ["Roschmann", "Simon", ""], ["B\u00f6ttinger", "Konstantin", ""]]}, {"id": "2104.06751", "submitter": "Xin Lv", "authors": "Xin Lv, Yixin Cao, Lei Hou, Juanzi Li, Zhiyuan Liu, Yichi Zhang, Zelin\n  Dai", "title": "Is Multi-Hop Reasoning Really Explainable? Towards Benchmarking\n  Reasoning Interpretability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Multi-hop reasoning has been widely studied in recent years to obtain more\ninterpretable link prediction. However, we find in experiments that many paths\ngiven by these models are actually unreasonable, while little works have been\ndone on interpretability evaluation for them. In this paper, we propose a\nunified framework to quantitatively evaluate the interpretability of multi-hop\nreasoning models so as to advance their development. In specific, we define\nthree metrics including path recall, local interpretability, and global\ninterpretability for evaluation, and design an approximate strategy to\ncalculate them using the interpretability scores of rules. Furthermore, we\nmanually annotate all possible rules and establish a Benchmark to detect the\nInterpretability of Multi-hop Reasoning (BIMR). In experiments, we run nine\nbaselines on our benchmark. The experimental results show that the\ninterpretability of current multi-hop reasoning models is less satisfactory and\nis still far from the upper bound given by our benchmark. Moreover, the\nrule-based models outperform the multi-hop reasoning models in terms of\nperformance and interpretability, which points to a direction for future\nresearch, i.e., we should investigate how to better incorporate rule\ninformation into the multi-hop reasoning model. Our codes and datasets can be\nobtained from https://github.com/THU-KEG/BIMR.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 10:12:05 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Lv", "Xin", ""], ["Cao", "Yixin", ""], ["Hou", "Lei", ""], ["Li", "Juanzi", ""], ["Liu", "Zhiyuan", ""], ["Zhang", "Yichi", ""], ["Dai", "Zelin", ""]]}, {"id": "2104.06772", "submitter": "Anthony Ngo", "authors": "Anthony Ngo, Max Paul Bauer, Michael Resch", "title": "Deep Evaluation Metric: Learning to Evaluate Simulated Radar Point\n  Clouds for Virtual Testing of Autonomous Driving", "comments": "2021 IEEE Radar Conference (IEEE RadarConf 2021)", "journal-ref": null, "doi": "10.1109/RadarConf2147009.2021.9455235", "report-no": null, "categories": "cs.CV cs.AI eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The usage of environment sensor models for virtual testing is a promising\napproach to reduce the testing effort of autonomous driving. However, in order\nto deduce any statements regarding the performance of an autonomous driving\nfunction based on simulation, the sensor model has to be validated to determine\nthe discrepancy between the synthetic and real sensor data. Since a certain\ndegree of divergence can be assumed to exist, the sufficient level of fidelity\nmust be determined, which poses a major challenge. In particular, a method for\nquantifying the fidelity of a sensor model does not exist and the problem of\ndefining an appropriate metric remains. In this work, we train a neural network\nto distinguish real and simulated radar sensor data with the purpose of\nlearning the latent features of real radar point clouds. Furthermore, we\npropose the classifier's confidence score for the `real radar point cloud'\nclass as a metric to determine the degree of fidelity of synthetically\ngenerated radar data. The presented approach is evaluated and it can be\ndemonstrated that the proposed deep evaluation metric outperforms conventional\nmetrics in terms of its capability to identify characteristic differences\nbetween real and simulated radar data.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 11:04:50 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 08:30:09 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Ngo", "Anthony", ""], ["Bauer", "Max Paul", ""], ["Resch", "Michael", ""]]}, {"id": "2104.06831", "submitter": "Weijie Zheng", "authors": "Weijie Zheng, Qiaozhi Zhang, Huanhuan Chen, Xin Yao", "title": "When Non-Elitism Meets Time-Linkage Problems", "comments": "To appear at GECCO2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world applications have the time-linkage property, and the only\ntheoretical analysis is recently given by Zheng, et al. (TEVC 2021) on their\nproposed time-linkage OneMax problem, OneMax$_{(0,1^n)}$. However, only two\nelitist algorithms (1+1)EA and ($\\mu$+1)EA are analyzed, and it is unknown\nwhether the non-elitism mechanism could help to escape the local optima existed\nin OneMax$_{(0,1^n)}$. In general, there are few theoretical results on the\nbenefits of the non-elitism in evolutionary algorithms. In this work, we\nanalyze on the influence of the non-elitism via comparing the performance of\nthe elitist (1+$\\lambda$)EA and its non-elitist counterpart (1,$\\lambda$)EA. We\nprove that with probability $1-o(1)$ (1+$\\lambda$)EA will get stuck in the\nlocal optima and cannot find the global optimum, but with probability $1$,\n(1,$\\lambda$)EA can reach the global optimum and its expected runtime is\n$O(n^{3+c}\\log n)$ with $\\lambda=c \\log_{\\frac{e}{e-1}} n$ for the constant\n$c\\ge 1$. Noting that a smaller offspring size is helpful for escaping from the\nlocal optima, we further resort to the compact genetic algorithm where only two\nindividuals are sampled to update the probabilistic model, and prove its\nexpected runtime of $O(n^3\\log n)$. Our computational experiments also verify\nthe efficiency of the two non-elitist algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 13:03:42 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Zheng", "Weijie", ""], ["Zhang", "Qiaozhi", ""], ["Chen", "Huanhuan", ""], ["Yao", "Xin", ""]]}, {"id": "2104.06832", "submitter": "Chengbo Dong", "authors": "Xinru Chen, Chengbo Dong, Jiaqi Ji, Juan Cao, Xirong Li", "title": "Image Manipulation Detection by Multi-View Multi-Scale Supervision", "comments": "Accepted by ICCV 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The key challenge of image manipulation detection is how to learn\ngeneralizable features that are sensitive to manipulations in novel data,\nwhilst specific to prevent false alarms on authentic images. Current research\nemphasizes the sensitivity, with the specificity overlooked. In this paper we\naddress both aspects by multi-view feature learning and multi-scale\nsupervision. By exploiting noise distribution and boundary artifact surrounding\ntampered regions, the former aims to learn semantic-agnostic and thus more\ngeneralizable features. The latter allows us to learn from authentic images\nwhich are nontrivial to be taken into account by current semantic segmentation\nnetwork based methods. Our thoughts are realized by a new network which we term\nMVSS-Net. Extensive experiments on five benchmark sets justify the viability of\nMVSS-Net for both pixel-level and image-level manipulation detection.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 13:05:58 GMT"}, {"version": "v2", "created": "Sun, 25 Jul 2021 06:45:00 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Chen", "Xinru", ""], ["Dong", "Chengbo", ""], ["Ji", "Jiaqi", ""], ["Cao", "Juan", ""], ["Li", "Xirong", ""]]}, {"id": "2104.06839", "submitter": "Fabio Cuzzolin", "authors": "Fabio Cuzzolin", "title": "Uncertainty measures: The big picture", "comments": "18 pages, 1 table, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.AI math.PR stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Probability theory is far from being the most general mathematical theory of\nuncertainty. A number of arguments point at its inability to describe\nsecond-order ('Knightian') uncertainty. In response, a wide array of theories\nof uncertainty have been proposed, many of them generalisations of classical\nprobability. As we show here, such frameworks can be organised into clusters\nsharing a common rationale, exhibit complex links, and are characterised by\ndifferent levels of generality. Our goal is a critical appraisal of the current\nlandscape in uncertainty theory.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 13:11:20 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Cuzzolin", "Fabio", ""]]}, {"id": "2104.06849", "submitter": "Marko Mihajlovic", "authors": "Marko Mihajlovic, Yan Zhang, Michael J. Black, Siyu Tang", "title": "LEAP: Learning Articulated Occupancy of People", "comments": "In Proceedings of the IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition. 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Substantial progress has been made on modeling rigid 3D objects using deep\nimplicit representations. Yet, extending these methods to learn neural models\nof human shape is still in its infancy. Human bodies are complex and the key\nchallenge is to learn a representation that generalizes such that it can\nexpress body shape deformations for unseen subjects in unseen,\nhighly-articulated, poses. To address this challenge, we introduce LEAP\n(LEarning Articulated occupancy of People), a novel neural occupancy\nrepresentation of the human body. Given a set of bone transformations (i.e.\njoint locations and rotations) and a query point in space, LEAP first maps the\nquery point to a canonical space via learned linear blend skinning (LBS)\nfunctions and then efficiently queries the occupancy value via an occupancy\nnetwork that models accurate identity- and pose-dependent deformations in the\ncanonical space. Experiments show that our canonicalized occupancy estimation\nwith the learned LBS functions greatly improves the generalization capability\nof the learned occupancy representation across various human shapes and poses,\noutperforming existing solutions in all settings.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 13:41:56 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Mihajlovic", "Marko", ""], ["Zhang", "Yan", ""], ["Black", "Michael J.", ""], ["Tang", "Siyu", ""]]}, {"id": "2104.06885", "submitter": "Yutao Chen", "authors": "Yutao Chen, Yuxuan Zhang, Zhongrui Huang, Zhenyao Luo, Jinpeng Chen", "title": "CelebHair: A New Large-Scale Dataset for Hairstyle Recommendation based\n  on CelebA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we present a new large-scale dataset for hairstyle\nrecommendation, CelebHair, based on the celebrity facial attributes dataset,\nCelebA. Our dataset inherited the majority of facial images along with some\nbeauty-related facial attributes from CelebA. Additionally, we employed facial\nlandmark detection techniques to extract extra features such as nose length and\npupillary distance, and deep convolutional neural networks for face shape and\nhairstyle classification. Empirical comparison has demonstrated the superiority\nof our dataset to other existing hairstyle-related datasets regarding variety,\nveracity, and volume. Analysis and experiments have been conducted on the\ndataset in order to evaluate its robustness and usability.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 14:26:37 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Chen", "Yutao", ""], ["Zhang", "Yuxuan", ""], ["Huang", "Zhongrui", ""], ["Luo", "Zhenyao", ""], ["Chen", "Jinpeng", ""]]}, {"id": "2104.06890", "submitter": "Ruo-Ze Liu", "authors": "Ruo-Ze Liu, Wenhai Wang, Yanjie Shen, Zhiqi Li, Yang Yu, Tong Lu", "title": "An Introduction of mini-AlphaStar", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  StarCraft II (SC2) is a real-time strategy game, in which players produce and\ncontrol multiple units to win. Due to its difficulties, such as huge state\nspace, various action space, a long time horizon, and imperfect information,\nSC2 has been a research highlight in reinforcement learning research. Recently,\nan SC2 agent called AlphaStar is proposed which shows excellent performance,\nobtaining a high win-rates of 99.8% against Grandmaster level human players. We\nimplemented a mini-scaled version of it called mini-AlphaStar based on their\npaper and the pseudocode they provided. The usage and analysis of it are shown\nin this technical report. The difference between AlphaStar and mini-AlphaStar\nis that we substituted the hyper-parameters in the former version with much\nsmaller ones for mini-scale training. The codes of mini-AlphaStar are all\nopen-sourced. The objective of mini-AlphaStar is to provide a reproduction of\nthe original AlphaStar and facilitate the future research of RL on large-scale\nproblems.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 14:31:51 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Liu", "Ruo-Ze", ""], ["Wang", "Wenhai", ""], ["Shen", "Yanjie", ""], ["Li", "Zhiqi", ""], ["Yu", "Yang", ""], ["Lu", "Tong", ""]]}, {"id": "2104.06893", "submitter": "Danushka Bollegala", "authors": "James O'Neill and Polina Rozenshtein and Ryuichi Kiryo and Motoko\n  Kubota and Danushka Bollegala", "title": "I Wish I Would Have Loved This One, But I Didn't -- A Multilingual\n  Dataset for Counterfactual Detection in Product Reviews", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Counterfactual statements describe events that did not or cannot take place.\nWe consider the problem of counterfactual detection (CFD) in product reviews.\nFor this purpose, we annotate a multilingual CFD dataset from Amazon product\nreviews covering counterfactual statements written in English, German, and\nJapanese languages. The dataset is unique as it contains counterfactuals in\nmultiple languages, covers a new application area of e-commerce reviews, and\nprovides high quality professional annotations. We train CFD models using\ndifferent text representation methods and classifiers. We find that these\nmodels are robust against the selectional biases introduced due to cue\nphrase-based sentence selection. Moreover, our CFD dataset is compatible with\nprior datasets and can be merged to learn accurate CFD models. Applying machine\ntranslation on English counterfactual examples to create multilingual data\nperforms poorly, demonstrating the language-specificity of this problem, which\nhas been ignored so far.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 14:38:36 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["O'Neill", "James", ""], ["Rozenshtein", "Polina", ""], ["Kiryo", "Ryuichi", ""], ["Kubota", "Motoko", ""], ["Bollegala", "Danushka", ""]]}, {"id": "2104.06901", "submitter": "Rohan Kumar Yadav", "authors": "Rohan Kumar Yadav, Lei Jiao, Ole-Christoffer Granmo, and Morten\n  Goodwin", "title": "Distributed Word Representation in Tsetlin Machine", "comments": "9 pages, 13 figures, and 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tsetlin Machine (TM) is an interpretable pattern recognition algorithm based\non propositional logic. The algorithm has demonstrated competitive performance\nin many Natural Language Processing (NLP) tasks, including sentiment analysis,\ntext classification, and Word Sense Disambiguation (WSD). To obtain human-level\ninterpretability, legacy TM employs Boolean input features such as bag-of-words\n(BOW). However, the BOW representation makes it difficult to use any\npre-trained information, for instance, word2vec and GloVe word representations.\nThis restriction has constrained the performance of TM compared to deep neural\nnetworks (DNNs) in NLP. To reduce the performance gap, in this paper, we\npropose a novel way of using pre-trained word representations for TM. The\napproach significantly enhances the TM performance and maintains\ninterpretability at the same time. We achieve this by extracting semantically\nrelated words from pre-trained word representations as input features to the\nTM. Our experiments show that the accuracy of the proposed approach is\nsignificantly higher than the previous BOW-based TM, reaching the level of\nDNN-based models.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 14:48:41 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Yadav", "Rohan Kumar", ""], ["Jiao", "Lei", ""], ["Granmo", "Ole-Christoffer", ""], ["Goodwin", "Morten", ""]]}, {"id": "2104.06910", "submitter": "Jessica Morley", "authors": "Jessica Morley, Caroline Morton, Kassandra Karpathakis, Mariarosaria\n  Taddeo, Luciano Floridi", "title": "Towards a framework for evaluating the safety, acceptability and\n  efficacy of AI systems for health: an initial synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The potential presented by Artificial Intelligence (AI) for healthcare has\nlong been recognised by the technical community. More recently, this potential\nhas been recognised by policymakers, resulting in considerable public and\nprivate investment in the development of AI for healthcare across the globe.\nDespite this, excepting limited success stories, real-world implementation of\nAI systems into front-line healthcare has been limited. There are numerous\nreasons for this, but a main contributory factor is the lack of internationally\naccepted, or formalised, regulatory standards to assess AI safety and impact\nand effectiveness. This is a well-recognised problem with numerous ongoing\nresearch and policy projects to overcome it. Our intention here is to\ncontribute to this problem-solving effort by seeking to set out a minimally\nviable framework for evaluating the safety, acceptability and efficacy of AI\nsystems for healthcare. We do this by conducting a systematic search across\nScopus, PubMed and Google Scholar to identify all the relevant literature\npublished between January 1970 and November 2020 related to the evaluation of:\noutput performance; efficacy; and real-world use of AI systems, and\nsynthesising the key themes according to the stages of evaluation: pre-clinical\n(theoretical phase); exploratory phase; definitive phase; and post-market\nsurveillance phase (monitoring). The result is a framework to guide AI system\ndevelopers, policymakers, and regulators through a sufficient evaluation of an\nAI system designed for use in healthcare.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 15:00:39 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Morley", "Jessica", ""], ["Morton", "Caroline", ""], ["Karpathakis", "Kassandra", ""], ["Taddeo", "Mariarosaria", ""], ["Floridi", "Luciano", ""]]}, {"id": "2104.06917", "submitter": "Dmitry Kazhdan", "authors": "Dmitry Kazhdan, Botty Dimanov, Helena Andres Terre, Mateja Jamnik,\n  Pietro Li\\`o, Adrian Weller", "title": "Is Disentanglement all you need? Comparing Concept-based &\n  Disentanglement Approaches", "comments": "Presented at the RAI, WeaSul, and RobustML workshops at The Ninth\n  International Conference on Learning Representations (ICLR) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Concept-based explanations have emerged as a popular way of extracting\nhuman-interpretable representations from deep discriminative models. At the\nsame time, the disentanglement learning literature has focused on extracting\nsimilar representations in an unsupervised or weakly-supervised way, using deep\ngenerative models. Despite the overlapping goals and potential synergies, to\nour knowledge, there has not yet been a systematic comparison of the\nlimitations and trade-offs between concept-based explanations and\ndisentanglement approaches. In this paper, we give an overview of these fields,\ncomparing and contrasting their properties and behaviours on a diverse set of\ntasks, and highlighting their potential strengths and limitations. In\nparticular, we demonstrate that state-of-the-art approaches from both classes\ncan be data inefficient, sensitive to the specific nature of the\nclassification/regression task, or sensitive to the employed concept\nrepresentation.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 15:06:34 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Kazhdan", "Dmitry", ""], ["Dimanov", "Botty", ""], ["Terre", "Helena Andres", ""], ["Jamnik", "Mateja", ""], ["Li\u00f2", "Pietro", ""], ["Weller", "Adrian", ""]]}, {"id": "2104.06954", "submitter": "Ivan Skorokhodov", "authors": "Ivan Skorokhodov, Grigorii Sotnikov, Mohamed Elhoseiny", "title": "Aligning Latent and Image Spaces to Connect the Unconnectable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we develop a method to generate infinite high-resolution images\nwith diverse and complex content. It is based on a perfectly equivariant\ngenerator with synchronous interpolations in the image and latent spaces.\nLatent codes, when sampled, are positioned on the coordinate grid, and each\npixel is computed from an interpolation of the nearby style codes. We modify\nthe AdaIN mechanism to work in such a setup and train the generator in an\nadversarial setting to produce images positioned between any two latent\nvectors. At test time, this allows for generating complex and diverse infinite\nimages and connecting any two unrelated scenes into a single arbitrarily large\npanorama. Apart from that, we introduce LHQ: a new dataset of \\lhqsize\nhigh-resolution nature landscapes. We test the approach on LHQ, LSUN Tower and\nLSUN Bridge and outperform the baselines by at least 4 times in terms of\nquality and diversity of the produced infinite images. The project page is\nlocated at https://universome.github.io/alis.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 16:29:20 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Skorokhodov", "Ivan", ""], ["Sotnikov", "Grigorii", ""], ["Elhoseiny", "Mohamed", ""]]}, {"id": "2104.06957", "submitter": "Martin Ferianc", "authors": "Martin Ferianc, Divyansh Manocha, Hongxiang Fan, Miguel Rodrigues", "title": "ComBiNet: Compact Convolutional Bayesian Neural Network for Image\n  Segmentation", "comments": "Accepted for publication at ICANN 2021. Code at:\n  https://github.com/martinferianc/ComBiNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fully convolutional U-shaped neural networks have largely been the dominant\napproach for pixel-wise image segmentation. In this work, we tackle two defects\nthat hinder their deployment in real-world applications: 1) Predictions lack\nuncertainty quantification that may be crucial to many decision-making systems;\n2) Large memory storage and computational consumption demanding extensive\nhardware resources. To address these issues and improve their practicality we\ndemonstrate a few-parameter compact Bayesian convolutional architecture, that\nachieves a marginal improvement in accuracy in comparison to related work using\nsignificantly fewer parameters and compute operations. The architecture\ncombines parameter-efficient operations such as separable convolutions,\nbilinear interpolation, multi-scale feature propagation and Bayesian inference\nfor per-pixel uncertainty quantification through Monte Carlo Dropout. The best\nperforming configurations required fewer than 2.5 million parameters on diverse\nchallenging datasets with few observations.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 16:33:48 GMT"}, {"version": "v2", "created": "Sat, 26 Jun 2021 11:34:44 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Ferianc", "Martin", ""], ["Manocha", "Divyansh", ""], ["Fan", "Hongxiang", ""], ["Rodrigues", "Miguel", ""]]}, {"id": "2104.06973", "submitter": "Sugam Garg", "authors": "Haswanth Aekula, Sugam Garg, Animesh Gupta", "title": "[RE] Double-Hard Debias: Tailoring Word Embeddings for Gender Bias\n  Mitigation", "comments": "Under review at ML Reproducibility Challenge 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Despite widespread use in natural language processing (NLP) tasks, word\nembeddings have been criticized for inheriting unintended gender bias from\ntraining corpora. programmer is more closely associated with man and homemaker\nis more closely associated with woman. Such gender bias has also been shown to\npropagate in downstream tasks.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 16:56:14 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Aekula", "Haswanth", ""], ["Garg", "Sugam", ""], ["Gupta", "Animesh", ""]]}, {"id": "2104.06982", "submitter": "Kim de Bie", "authors": "Kim de Bie, Ana Lucic, Hinda Haned", "title": "To Trust or Not to Trust a Regressor: Estimating and Explaining\n  Trustworthiness of Regression Predictions", "comments": "Accepted to ICML 2021 Workshop on Human in the Loop Learning (HILL)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In hybrid human-AI systems, users need to decide whether or not to trust an\nalgorithmic prediction while the true error in the prediction is unknown. To\naccommodate such settings, we introduce RETRO-VIZ, a method for (i) estimating\nand (ii) explaining trustworthiness of regression predictions. It consists of\nRETRO, a quantitative estimate of the trustworthiness of a prediction, and VIZ,\na visual explanation that helps users identify the reasons for the (lack of)\ntrustworthiness of a prediction. We find that RETRO-scores negatively correlate\nwith prediction error across 117 experimental settings, indicating that RETRO\nprovides a useful measure to distinguish trustworthy predictions from\nuntrustworthy ones. In a user study with 41 participants, we find that\nVIZ-explanations help users identify whether a prediction is trustworthy or\nnot: on average, 95.1% of participants correctly select the more trustworthy\nprediction, given a pair of predictions. In addition, an average of 75.6% of\nparticipants can accurately describe why a prediction seems to be (not)\ntrustworthy. Finally, we find that the vast majority of users subjectively\nexperience RETRO-VIZ as a useful tool to assess the trustworthiness of\nalgorithmic predictions.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 17:04:20 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 13:29:09 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["de Bie", "Kim", ""], ["Lucic", "Ana", ""], ["Haned", "Hinda", ""]]}, {"id": "2104.07079", "submitter": "Maria Leonor Pacheco", "authors": "I-Ta Lee, Maria Leonor Pacheco and Dan Goldwasser", "title": "Modeling Human Mental States with an Entity-based Narrative Graph", "comments": "Accepted at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding narrative text requires capturing characters' motivations,\ngoals, and mental states. This paper proposes an Entity-based Narrative Graph\n(ENG) to model the internal-states of characters in a story. We explicitly\nmodel entities, their interactions and the context in which they appear, and\nlearn rich representations for them. We experiment with different task-adaptive\npre-training objectives, in-domain training, and symbolic inference to capture\ndependencies between different decisions in the output space. We evaluate our\nmodel on two narrative understanding tasks: predicting character mental states,\nand desire fulfillment, and conduct a qualitative analysis.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 19:05:19 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Lee", "I-Ta", ""], ["Pacheco", "Maria Leonor", ""], ["Goldwasser", "Dan", ""]]}, {"id": "2104.07115", "submitter": "Jiguang He", "authors": "Jiguang He and Nhan Thanh Nguyen and Rafaela Schroeder and Visa Tapio\n  and Joonas Kokkoniemi and Markku Juntti", "title": "Channel Estimation and Hybrid Architectures for RIS-Assisted\n  Communications", "comments": "6 pages, 7 figures, accepted by European Conference on Networks and\n  Communications", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reconfigurable intelligent surfaces (RISs) are considered as potential\ntechnologies for the upcoming sixth-generation (6G) wireless communication\nsystem. Various benefits brought by deploying one or multiple RISs include\nincreased spectrum and energy efficiency, enhanced connectivity, extended\ncommunication coverage, reduced complexity at transceivers, and even improved\nlocalization accuracy. However, to unleash their full potential, fundamentals\nrelated to RISs, ranging from physical-layer (PHY) modelling to RIS phase\ncontrol, need to be addressed thoroughly. In this paper, we provide an overview\nof some timely research problems related to the RIS technology, i.e., PHY\nmodelling (including also physics), channel estimation, potential RIS\narchitectures, and RIS phase control (via both model-based and data-driven\napproaches), along with recent numerical results. We envision that more efforts\nwill be devoted towards intelligent wireless environments, enabled by RISs.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 20:28:09 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["He", "Jiguang", ""], ["Nguyen", "Nhan Thanh", ""], ["Schroeder", "Rafaela", ""], ["Tapio", "Visa", ""], ["Kokkoniemi", "Joonas", ""], ["Juntti", "Markku", ""]]}, {"id": "2104.07123", "submitter": "Lukas Stappen", "authors": "Lukas Stappen, Alice Baird, Lukas Christ, Lea Schumann, Benjamin\n  Sertolli, Eva-Maria Messner, Erik Cambria, Guoying Zhao, and Bj\\\"orn W.\n  Schuller", "title": "The MuSe 2021 Multimodal Sentiment Analysis Challenge: Sentiment,\n  Emotion, Physiological-Emotion, and Stress", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Multimodal Sentiment Analysis (MuSe) 2021 is a challenge focusing on the\ntasks of sentiment and emotion, as well as physiological-emotion and\nemotion-based stress recognition through more comprehensively integrating the\naudio-visual, language, and biological signal modalities. The purpose of MuSe\n2021 is to bring together communities from different disciplines; mainly, the\naudio-visual emotion recognition community (signal-based), the sentiment\nanalysis community (symbol-based), and the health informatics community. We\npresent four distinct sub-challenges: MuSe-Wilder and MuSe-Stress which focus\non continuous emotion (valence and arousal) prediction; MuSe-Sent, in which\nparticipants recognise five classes each for valence and arousal; and\nMuSe-Physio, in which the novel aspect of `physiological-emotion' is to be\npredicted. For this years' challenge, we utilise the MuSe-CaR dataset focusing\non user-generated reviews and introduce the Ulm-TSST dataset, which displays\npeople in stressful depositions. This paper also provides detail on the\nstate-of-the-art feature sets extracted from these datasets for utilisation by\nour baseline model, a Long Short-Term Memory-Recurrent Neural Network. For each\nsub-challenge, a competitive baseline for participants is set; namely, on test,\nwe report a Concordance Correlation Coefficient (CCC) of .4616 CCC for\nMuSe-Wilder; .4717 CCC for MuSe-Stress, and .4606 CCC for MuSe-Physio. For\nMuSe-Sent an F1 score of 32.82 % is obtained.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 20:56:04 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Stappen", "Lukas", ""], ["Baird", "Alice", ""], ["Christ", "Lukas", ""], ["Schumann", "Lea", ""], ["Sertolli", "Benjamin", ""], ["Messner", "Eva-Maria", ""], ["Cambria", "Erik", ""], ["Zhao", "Guoying", ""], ["Schuller", "Bj\u00f6rn W.", ""]]}, {"id": "2104.07145", "submitter": "Chaoyang He", "authors": "Chaoyang He, Keshav Balasubramanian, Emir Ceyani, Yu Rong, Peilin\n  Zhao, Junzhou Huang, Murali Annavaram, Salman Avestimehr", "title": "FedGraphNN: A Federated Learning System and Benchmark for Graph Neural\n  Networks", "comments": "The first three authors contribute equally. Our shorter versions are\n  accepted to ICLR 2021 Workshop on Distributed and Private Machine\n  Learning(DPML) and MLSys 2021 GNNSys Workshop on Graph Neural Networks and\n  Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph Neural Network (GNN) research is rapidly growing thanks to the capacity\nof GNNs to learn representations from graph-structured data. However,\ncentralizing a massive amount of real-world graph data for GNN training is\nprohibitive due to user-side privacy concerns, regulation restrictions, and\ncommercial competition. Federated learning (FL), a trending distributed\nlearning paradigm, aims to solve this challenge while preserving privacy.\nDespite recent advances in vision and language domains, there is no suitable\nplatform for the federated training of GNNs. To this end, we introduce\nFedGraphNN, an open research federated learning system and a benchmark to\nfacilitate GNN-based FL research. FedGraphNN is built on a unified formulation\nof federated GNNs and supports commonly used datasets, GNN models, FL\nalgorithms, and flexible APIs. We also contribute a new molecular dataset,\nhERG, to promote research exploration. Our experimental results present\nsignificant challenges in federated GNN training: federated GNNs perform worse\nin most datasets with a non-I.I.D split than centralized GNNs; the GNN model\nthat attains the best result in the centralized setting may not hold its\nadvantage in the federated setting. These results imply that more research\nefforts are needed to unravel the mystery behind federated GNN training.\nMoreover, our system performance analysis demonstrates that the FedGraphNN\nsystem is computationally affordable to most research labs with limited GPUs.\nWe maintain the source code at https://github.com/FedML-AI/FedGraphNN.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 22:11:35 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["He", "Chaoyang", ""], ["Balasubramanian", "Keshav", ""], ["Ceyani", "Emir", ""], ["Rong", "Yu", ""], ["Zhao", "Peilin", ""], ["Huang", "Junzhou", ""], ["Annavaram", "Murali", ""], ["Avestimehr", "Salman", ""]]}, {"id": "2104.07149", "submitter": "Sailik Sengupta", "authors": "Jason Krone, Sailik Sengupta, Saab Mansoor", "title": "On the Robustness of Goal Oriented Dialogue Systems to Real-world Noise", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Goal oriented dialogue systems, that interact in real-word environments,\noften encounter noisy data. In this work, we investigate how robust goal\noriented dialogue systems are to noisy data. Specifically, our analysis\nconsiders intent classification (IC) and slot labeling (SL) models that form\nthe basis of most dialogue systems. We collect a test-suite for six common\nphenomena found in live human-to-bot conversations (abbreviations, casing,\nmisspellings, morphological variants, paraphrases, and synonyms) and show that\nthese phenomena can degrade the IC/SL performance of state-of-the-art BERT\nbased models. Through the use of synthetic data augmentation, we are improve\nIC/SL model's robustness to real-world noise by +11.5 for IC and +17.3 points\nfor SL on average across noise types. We make our suite of noisy test data\npublic to enable further research into the robustness of dialog systems.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 22:14:41 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Krone", "Jason", ""], ["Sengupta", "Sailik", ""], ["Mansoor", "Saab", ""]]}, {"id": "2104.07177", "submitter": "Chao Cai Chris", "authors": "Chao Cai, Ruinan Jin, Peng Wang, Liyuan Ye, Hongbo Jiang, Jun Luo", "title": "PURE: Passive mUlti-peRson idEntification via Deep Footstep Separation\n  and Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, \\textit{passive behavioral biometrics} (e.g., gesture or footstep)\nhave become promising complements to conventional user identification methods\n(e.g., face or fingerprint) under special situations, yet existing sensing\ntechnologies require lengthy measurement traces and cannot identify multiple\nusers at the same time. To this end, we propose \\systemname\\ as a passive\nmulti-person identification system leveraging deep learning enabled footstep\nseparation and recognition. \\systemname\\ passively identifies a user by\ndeciphering the unique \"footprints\" in its footstep. Different from existing\ngait-enabled recognition systems incurring a long sensing delay to acquire many\nfootsteps, \\systemname\\ can recognize a person by as few as only one step,\nsubstantially cutting the identification latency. To make \\systemname\\ adaptive\nto walking pace variations, environmental dynamics, and even unseen targets, we\napply an adversarial learning technique to improve its domain generalisability\nand identification accuracy. Finally, \\systemname\\ can defend itself against\nreplay attack, enabled by the richness of footstep and spatial awareness. We\nimplement a \\systemname\\ prototype using commodity hardware and evaluate it in\ntypical indoor settings. Evaluation results demonstrate a cross-domain\nidentification accuracy of over 90\\%.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 00:28:05 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Cai", "Chao", ""], ["Jin", "Ruinan", ""], ["Wang", "Peng", ""], ["Ye", "Liyuan", ""], ["Jiang", "Hongbo", ""], ["Luo", "Jun", ""]]}, {"id": "2104.07190", "submitter": "Weishun Song", "authors": "Liying Zheng, Yue Deng, Weishun Song, Liang Xu, Jing Xiao", "title": "An Alignment-Agnostic Model for Chinese Text Error Correction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates how to correct Chinese text errors with types of\nmistaken, missing and redundant characters, which is common for Chinese native\nspeakers. Most existing models based on detect-correct framework can correct\nmistaken characters errors, but they cannot deal with missing or redundant\ncharacters. The reason is that lengths of sentences before and after correction\nare not the same, leading to the inconsistence between model inputs and\noutputs. Although the Seq2Seq-based or sequence tagging methods provide\nsolutions to the problem and achieved relatively good results on English\ncontext, but they do not perform well in Chinese context according to our\nexperimental results. In our work, we propose a novel detect-correct framework\nwhich is alignment-agnostic, meaning that it can handle both text aligned and\nnon-aligned occasions, and it can also serve as a cold start model when there\nare no annotated data provided. Experimental results on three datasets\ndemonstrate that our method is effective and achieves the best performance\namong existing published models.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 01:17:34 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Zheng", "Liying", ""], ["Deng", "Yue", ""], ["Song", "Weishun", ""], ["Xu", "Liang", ""], ["Xiao", "Jing", ""]]}, {"id": "2104.07195", "submitter": "Wei Li", "authors": "Lei Zhang, Wei Bai, Wei Li, Shiming Xia, Qibin Zheng", "title": "Discover the Hidden Attack Path in Multi-domain Cyberspace Based on\n  Reinforcement Learning", "comments": "12 pages, 2 figures, 3 tables. arXiv admin note: substantial text\n  overlap with arXiv:2007.04614", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this work, we present a learning-based approach to analysis cyberspace\nsecurity configuration. Unlike prior methods, our approach has the ability to\nlearn from past experience and improve over time. In particular, as we train\nover a greater number of agents as attackers, our method becomes better at\ndiscovering hidden attack paths for previously methods, especially in\nmulti-domain cyberspace. To achieve these results, we pose discovering attack\npaths as a Reinforcement Learning (RL) problem and train an agent to discover\nmulti-domain cyberspace attack paths. To enable our RL policy to discover more\nhidden attack paths and shorter attack paths, we ground representation\nintroduction an multi-domain action select module in RL. Our objective is to\ndiscover more hidden attack paths and shorter attack paths by our proposed\nmethod, to analysis the weakness of cyberspace security configuration. At last,\nwe designed a simulated cyberspace experimental environment to verify our\nproposed method, the experimental results show that our method can discover\nmore hidden multi-domain attack paths and shorter attack paths than existing\nbaseline methods.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 01:38:51 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Zhang", "Lei", ""], ["Bai", "Wei", ""], ["Li", "Wei", ""], ["Xia", "Shiming", ""], ["Zheng", "Qibin", ""]]}, {"id": "2104.07213", "submitter": "Hye-jin Shim", "authors": "Hye-jin Shim, Ju-ho Kim, Jee-weon Jung, Ha-Jin Yu", "title": "Attentive Max Feature Map for Acoustic Scene Classification with Joint\n  Learning considering the Abstraction of Classes", "comments": "5 pages, 2 figure, 5 tables, submitted to INTERSPEECH 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The attention mechanism has been widely adopted in acoustic scene\nclassification. However, we find that during the process of attention\nexclusively emphasizing information, it tends to excessively discard\ninformation although improving the performance. We propose a mechanism referred\nto as the attentive max feature map which combines two effective techniques,\nattention and max feature map, to further elaborate the attention mechanism and\nmitigate the abovementioned phenomenon. Furthermore, we explore various joint\nlearning methods that utilize additional labels originally generated for\nsubtask B (3-classes) on top of existing labels for subtask A (10-classes) of\nthe DCASE2020 challenge. We expect that using two kinds of labels\nsimultaneously would be helpful because the labels of the two subtasks differ\nin their degree of abstraction. Applying two proposed techniques, our proposed\nsystem achieves state-of-the-art performance among single systems on subtask A.\nIn addition, because the model has a complexity comparable to subtask B's\nrequirement, it shows the possibility of developing a system that fulfills the\nrequirements of both subtasks; generalization on multiple devices and\nlow-complexity.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 03:14:15 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Shim", "Hye-jin", ""], ["Kim", "Ju-ho", ""], ["Jung", "Jee-weon", ""], ["Yu", "Ha-Jin", ""]]}, {"id": "2104.07219", "submitter": "Athul Paul Jacob", "authors": "Athul Paul Jacob, Mike Lewis, Jacob Andreas", "title": "Multitasking Inhibits Semantic Drift", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When intelligent agents communicate to accomplish shared goals, how do these\ngoals shape the agents' language? We study the dynamics of learning in latent\nlanguage policies (LLPs), in which instructor agents generate natural-language\nsubgoal descriptions and executor agents map these descriptions to low-level\nactions. LLPs can solve challenging long-horizon reinforcement learning\nproblems and provide a rich model for studying task-oriented language use. But\nprevious work has found that LLP training is prone to semantic drift (use of\nmessages in ways inconsistent with their original natural language meanings).\nHere, we demonstrate theoretically and empirically that multitask training is\nan effective counter to this problem: we prove that multitask training\neliminates semantic drift in a well-studied family of signaling games, and show\nthat multitask training of neural LLPs in a complex strategy game reduces drift\nand while improving sample efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 03:42:17 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Jacob", "Athul Paul", ""], ["Lewis", "Mike", ""], ["Andreas", "Jacob", ""]]}, {"id": "2104.07225", "submitter": "Krzysztof Fiok", "authors": "Krzysztof Fiok (1), Waldemar Karwowski (1), Edgar Gutierrez (1)(2),\n  Mohammad Reza Davahli (1), Maciej Wilamowski (3), Tareq Ahram (1), Awad\n  Al-Juaid (4), and Jozef Zurada (5) ((1) Department of Industrial Engineering\n  and Management Systems, University of Central Florida, USA, (2) Center for\n  Latin-American Logistics Innovation, LOGyCA, Bogota, Colombia (3) Faculty of\n  Economic Sciences, University of Warsaw, Warsaw, Poland (4) Department of\n  Industrial Engineering, College of Engineering, Taif University, Saudi Arabia\n  (5) Business School, University of Louisville, USA)", "title": "Text Guide: Improving the quality of long text classification by a text\n  selection method based on feature importance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The performance of text classification methods has improved greatly over the\nlast decade for text instances of less than 512 tokens. This limit has been\nadopted by most state-of-the-research transformer models due to the high\ncomputational cost of analyzing longer text instances. To mitigate this problem\nand to improve classification for longer texts, researchers have sought to\nresolve the underlying causes of the computational cost and have proposed\noptimizations for the attention mechanism, which is the key element of every\ntransformer model. In our study, we are not pursuing the ultimate goal of long\ntext classification, i.e., the ability to analyze entire text instances at one\ntime while preserving high performance at a reasonable computational cost.\nInstead, we propose a text truncation method called Text Guide, in which the\noriginal text length is reduced to a predefined limit in a manner that improves\nperformance over naive and semi-naive approaches while preserving low\ncomputational costs. Text Guide benefits from the concept of feature\nimportance, a notion from the explainable artificial intelligence domain. We\ndemonstrate that Text Guide can be used to improve the performance of recent\nlanguage models specifically designed for long text classification, such as\nLongformer. Moreover, we discovered that parameter optimization is the key to\nText Guide performance and must be conducted before the method is deployed.\nFuture experiments may reveal additional benefits provided by this new method.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 04:10:08 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Fiok", "Krzysztof", ""], ["Karwowski", "Waldemar", ""], ["Gutierrez", "Edgar", ""], ["Davahli", "Mohammad Reza", ""], ["Wilamowski", "Maciej", ""], ["Ahram", "Tareq", ""], ["Al-Juaid", "Awad", ""], ["Zurada", "Jozef", ""]]}, {"id": "2104.07228", "submitter": "Wenhao Yu", "authors": "Wenhao Yu, Chenguang Zhu, Tong Zhao, Zhichun Guo, Meng Jiang", "title": "Sentence-Permuted Paragraph Generation", "comments": "19 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generating paragraphs of diverse contents is important in many applications.\nExisting generation models produce similar contents from homogenized contexts\ndue to the fixed left-to-right sentence order. Our idea is permuting the\nsentence orders to improve the content diversity of multi-sentence paragraph.\nWe propose a novel framework PermGen whose objective is to maximize the\nexpected log-likelihood of output paragraph distributions with respect to all\npossible sentence orders. PermGen uses hierarchical positional embedding and\ndesigns new procedures for training, decoding, and candidate ranking in the\nsentence-permuted generation. Experiments on three paragraph generation\nbenchmarks demonstrate PermGen generates more diverse outputs with a higher\nquality than existing models.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 04:17:03 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Yu", "Wenhao", ""], ["Zhu", "Chenguang", ""], ["Zhao", "Tong", ""], ["Guo", "Zhichun", ""], ["Jiang", "Meng", ""]]}, {"id": "2104.07245", "submitter": "Shaily Kabir", "authors": "Shaily Kabir, Christian Wagner and Zack Ellerby", "title": "Towards Handling Uncertainty-at-Source in AI -- A Review and Next Steps\n  for Interval Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of statistics and AI draw insights through modelling discord or variance\nbetween sources of information (i.e., inter-source uncertainty). Increasingly,\nhowever, research is focusing upon uncertainty arising at the level of\nindividual measurements (i.e., within- or intra-source), such as for a given\nsensor output or human response. Here, adopting intervals rather than numbers\nas the fundamental data-type provides an efficient, powerful, yet challenging\nway forward -- offering systematic capture of uncertainty-at-source, increasing\ninformational capacity, and ultimately potential for insight. Following recent\nprogress in the capture of interval-valued data, including from human\nparticipants, conducting machine learning directly upon intervals is a crucial\nnext step. This paper focuses on linear regression for interval-valued data as\na recent growth area, providing an essential foundation for broader use of\nintervals in AI. We conduct an in-depth analysis of state-of-the-art methods,\nelucidating their behaviour, advantages, and pitfalls when applied to datasets\nwith different properties. Specific emphasis is given to the challenge of\npreserving mathematical coherence -- i.e., ensuring that models maintain\nfundamental mathematical properties of intervals throughout -- and the paper\nputs forward extensions to an existing approach to guarantee this. Carefully\ndesigned experiments, using both synthetic and real-world data, are conducted\n-- with findings presented alongside novel visualizations for interval-valued\nregression outputs, designed to maximise model interpretability. Finally, the\npaper makes recommendations concerning method suitability for data sets with\nspecific properties and highlights remaining challenges and important next\nsteps for developing AI with the capacity to handle uncertainty-at-source.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 05:31:10 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Kabir", "Shaily", ""], ["Wagner", "Christian", ""], ["Ellerby", "Zack", ""]]}, {"id": "2104.07249", "submitter": "Minbyul Jeong", "authors": "Minbyul Jeong and Jaewoo Kang", "title": "Regularizing Models via Pointwise Mutual Information for Named Entity\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In Named Entity Recognition (NER), pre-trained language models have been\noverestimated by focusing on dataset biases to solve current benchmark\ndatasets. However, these biases hinder generalizability which is necessary to\naddress real-world situations such as weak name regularity and plenty of unseen\nmentions. To alleviate the use of dataset biases and make the models fully\nexploit data, we propose a debiasing method that our bias-only model can be\nreplaced with a Pointwise Mutual Information (PMI) to enhance generalization\nability while outperforming an in-domain performance. Our approach enables to\ndebias highly correlated word and labels in the benchmark datasets; reflect\ninformative statistics via subword frequency; alleviates a class imbalance\nbetween positive and negative examples. For long-named and complex-structure\nentities, our method can predict these entities through debiasing on\nconjunction or special characters. Extensive experiments on both general and\nbiomedical domains demonstrate the effectiveness and generalization\ncapabilities of the PMI.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 05:47:27 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Jeong", "Minbyul", ""], ["Kang", "Jaewoo", ""]]}, {"id": "2104.07252", "submitter": "Haiqin Yang", "authors": "Haiqin Yang and Jianping Shen", "title": "Emotion Dynamics Modeling via BERT", "comments": "9 pages, 9 figures, 5 tables, in IJCNN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Emotion dynamics modeling is a significant task in emotion recognition in\nconversation. It aims to predict conversational emotions when building\nempathetic dialogue systems. Existing studies mainly develop models based on\nRecurrent Neural Networks (RNNs). They cannot benefit from the power of the\nrecently-developed pre-training strategies for better token representation\nlearning in conversations. More seriously, it is hard to distinguish the\ndependency of interlocutors and the emotional influence among interlocutors by\nsimply assembling the features on top of RNNs. In this paper, we develop a\nseries of BERT-based models to specifically capture the inter-interlocutor and\nintra-interlocutor dependencies of the conversational emotion dynamics.\nConcretely, we first substitute BERT for RNNs to enrich the token\nrepresentations. Then, a Flat-structured BERT (F-BERT) is applied to link up\nutterances in a conversation directly, and a Hierarchically-structured BERT\n(H-BERT) is employed to distinguish the interlocutors when linking up\nutterances. More importantly, a Spatial-Temporal-structured BERT, namely\nST-BERT, is proposed to further determine the emotional influence among\ninterlocutors. Finally, we conduct extensive experiments on two popular emotion\nrecognition in conversation benchmark datasets and demonstrate that our\nproposed models can attain around 5\\% and 10\\% improvement over the\nstate-of-the-art baselines, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 05:58:48 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 03:05:14 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Yang", "Haiqin", ""], ["Shen", "Jianping", ""]]}, {"id": "2104.07253", "submitter": "Donghyun Kwak", "authors": "Seunghyun Seo, Donghyun Kwak, Bowon Lee", "title": "Integration of Pre-trained Networks with Continuous Token Interface for\n  End-to-End Spoken Language Understanding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most End-to-End (E2E) SLU networks leverage the pre-trained ASR networks but\nstill lack the capability to understand the semantics of utterances, crucial\nfor the SLU task. To solve this, recently proposed studies use pre-trained NLU\nnetworks. However, it is not trivial to fully utilize both pre-trained\nnetworks; many solutions were proposed, such as Knowledge Distillation,\ncross-modal shared embedding, and network integration with Interface. We\npropose a simple and robust integration method for the E2E SLU network with\nnovel Interface, Continuous Token Interface (CTI), the junctional\nrepresentation of the ASR and NLU networks when both networks are pre-trained\nwith the same vocabulary. Because the only difference is the noise level, we\ndirectly feed the ASR network's output to the NLU network. Thus, we can train\nour SLU network in an E2E manner without additional modules, such as\nGumbel-Softmax. We evaluate our model using SLURP, a challenging SLU dataset\nand achieve state-of-the-art scores on both intent classification and slot\nfilling tasks. We also verify the NLU network, pre-trained with Masked Language\nModel, can utilize a noisy textual representation of CTI. Moreover, we show our\nmodel can be trained with multi-task learning from heterogeneous data even\nafter integration with CTI.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 05:59:28 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Seo", "Seunghyun", ""], ["Kwak", "Donghyun", ""], ["Lee", "Bowon", ""]]}, {"id": "2104.07276", "submitter": "Divya Grover", "authors": "Divya Grover, Christos Dimitrakakis", "title": "Adaptive Belief Discretization for POMDP Planning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Partially Observable Markov Decision Processes (POMDP) is a widely used model\nto represent the interaction of an environment and an agent, under state\nuncertainty. Since the agent does not observe the environment state, its\nuncertainty is typically represented through a probabilistic belief. While the\nset of possible beliefs is infinite, making exact planning intractable, the\nbelief space's complexity (and hence planning complexity) is characterized by\nits covering number. Many POMDP solvers uniformly discretize the belief space\nand give the planning error in terms of the (typically unknown) covering\nnumber. We instead propose an adaptive belief discretization scheme, and give\nits associated planning error. We furthermore characterize the covering number\nwith respect to the POMDP parameters. This allows us to specify the exact\nmemory requirements on the planner, needed to bound the value function error.\nWe then propose a novel, computationally efficient solver using this scheme. We\ndemonstrate that our algorithm is highly competitive with the state of the art\nin a variety of scenarios.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 07:04:32 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Grover", "Divya", ""], ["Dimitrakakis", "Christos", ""]]}, {"id": "2104.07282", "submitter": "Zhi Wang", "authors": "Yuanyang Zhu, Zhi Wang, Chunlin Chen, and Daoyi Dong", "title": "Rule-Based Reinforcement Learning for Efficient Robot Navigation with\n  Space Reduction", "comments": "Accepted by IEEE/ASME Transactions on Mechatronics, 2021, DOI:\n  10.1109/TMECH.2021.3072675", "journal-ref": "IEEE/ASME Transactions on Mechatronics, 2021", "doi": "10.1109/TMECH.2021.3072675", "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For real-world deployments, it is critical to allow robots to navigate in\ncomplex environments autonomously. Traditional methods usually maintain an\ninternal map of the environment, and then design several simple rules, in\nconjunction with a localization and planning approach, to navigate through the\ninternal map. These approaches often involve a variety of assumptions and prior\nknowledge. In contrast, recent reinforcement learning (RL) methods can provide\na model-free, self-learning mechanism as the robot interacts with an initially\nunknown environment, but are expensive to deploy in real-world scenarios due to\ninefficient exploration. In this paper, we focus on efficient navigation with\nthe RL technique and combine the advantages of these two kinds of methods into\na rule-based RL (RuRL) algorithm for reducing the sample complexity and cost of\ntime. First, we use the rule of wall-following to generate a closed-loop\ntrajectory. Second, we employ a reduction rule to shrink the trajectory, which\nin turn effectively reduces the redundant exploration space. Besides, we give\nthe detailed theoretical guarantee that the optimal navigation path is still in\nthe reduced space. Third, in the reduced space, we utilize the Pledge rule to\nguide the exploration strategy for accelerating the RL process at the early\nstage. Experiments conducted on real robot navigation problems in hex-grid\nenvironments demonstrate that RuRL can achieve improved navigation performance.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 07:40:27 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Zhu", "Yuanyang", ""], ["Wang", "Zhi", ""], ["Chen", "Chunlin", ""], ["Dong", "Daoyi", ""]]}, {"id": "2104.07284", "submitter": "Jungsoo Park", "authors": "Jungsoo Park, Gyuwan Kim, Jaewoo Kang", "title": "Consistency Training with Virtual Adversarial Discrete Perturbation", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose an effective consistency training framework that enforces a\ntraining model's predictions given original and perturbed inputs to be similar\nby adding a discrete noise that would incur the highest divergence between\npredictions. This virtual adversarial discrete noise obtained by replacing a\nsmall portion of tokens while keeping original semantics as much as possible\nefficiently pushes a training model's decision boundary. Moreover, we perform\nan iterative refinement process to alleviate the degraded fluency of the\nperturbed sentence due to the conditional independence assumption. Experimental\nresults show that our proposed method outperforms other consistency training\nbaselines with text editing, paraphrasing, or a continuous noise on\nsemi-supervised text classification tasks and a robustness benchmark.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 07:49:43 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Park", "Jungsoo", ""], ["Kim", "Gyuwan", ""], ["Kang", "Jaewoo", ""]]}, {"id": "2104.07294", "submitter": "Christopher Bamford", "authors": "Christopher Bamford, Alvaro Ovalle", "title": "Generalising Discrete Action Spaces with Conditional Action Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There are relatively few conventions followed in reinforcement learning (RL)\nenvironments to structure the action spaces. As a consequence the application\nof RL algorithms to tasks with large action spaces with multiple components\nrequire additional effort to adjust to different formats. In this paper we\nintroduce {\\em Conditional Action Trees} with two main objectives: (1) as a\nmethod of structuring action spaces in RL to generalise across several action\nspace specifications, and (2) to formalise a process to significantly reduce\nthe action space by decomposing it into multiple sub-spaces, favoring a\nmulti-staged decision making approach. We show several proof-of-concept\nexperiments validating our scheme, ranging from environments with basic\ndiscrete action spaces to those with large combinatorial action spaces commonly\nfound in RTS-style games.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 08:10:18 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Bamford", "Christopher", ""], ["Ovalle", "Alvaro", ""]]}, {"id": "2104.07295", "submitter": "Shuiqiao Yang", "authors": "Shuiqiao Yang, Sunny Verma, Borui Cai, Jiaojiao Jiang, Kun Yu, Fang\n  Chen, Shui Yu", "title": "Variational Co-embedding Learning for Attributed Network Clustering", "comments": "This manuscript is under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works for attributed network clustering utilize graph convolution to\nobtain node embeddings and simultaneously perform clustering assignments on the\nembedding space. It is effective since graph convolution combines the\nstructural and attributive information for node embedding learning. However, a\nmajor limitation of such works is that the graph convolution only incorporates\nthe attribute information from the local neighborhood of nodes but fails to\nexploit the mutual affinities between nodes and attributes. In this regard, we\npropose a variational co-embedding learning model for attributed network\nclustering (VCLANC). VCLANC is composed of dual variational auto-encoders to\nsimultaneously embed nodes and attributes. Relying on this, the mutual affinity\ninformation between nodes and attributes could be reconstructed from the\nembedding space and served as extra self-supervised knowledge for\nrepresentation learning. At the same time, trainable Gaussian mixture model is\nused as priors to infer the node clustering assignments. To strengthen the\nperformance of the inferred clusters, we use a mutual distance loss on the\ncenters of the Gaussian priors and a clustering assignment hardening loss on\nthe node embeddings. Experimental results on four real-world attributed network\ndatasets demonstrate the effectiveness of the proposed VCLANC for attributed\nnetwork clustering.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 08:11:47 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Yang", "Shuiqiao", ""], ["Verma", "Sunny", ""], ["Cai", "Borui", ""], ["Jiang", "Jiaojiao", ""], ["Yu", "Kun", ""], ["Chen", "Fang", ""], ["Yu", "Shui", ""]]}, {"id": "2104.07329", "submitter": "Cheng-Wei Huang", "authors": "Cheng-Wei Huang, Tim-Wei Chen, and Juinn-Dar Huang", "title": "All-You-Can-Fit 8-Bit Flexible Floating-Point Format for Accurate and\n  Memory-Efficient Inference of Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern deep neural network (DNN) models generally require a huge amount of\nweight and activation values to achieve good inference outcomes. Those data\ninevitably demand a massive off-chip memory capacity/bandwidth, and the\nsituation gets even worse if they are represented in high-precision\nfloating-point formats. Effort has been made for representing those data in\ndifferent 8-bit floating-point formats, nevertheless, a notable accuracy loss\nis still unavoidable. In this paper we introduce an extremely flexible 8-bit\nfloating-point (FFP8) format whose defining factors - the bit width of\nexponent/fraction field, the exponent bias, and even the presence of the sign\nbit - are all configurable. We also present a methodology to properly determine\nthose factors so that the accuracy of model inference can be maximized. The\nfoundation of this methodology is based on a key observation - both the maximum\nmagnitude and the value distribution are quite dissimilar between weights and\nactivations in most DNN models. Experimental results demonstrate that the\nproposed FFP8 format achieves an extremely low accuracy loss of $0.1\\%\\sim\n0.3\\%$ for several representative image classification models even without the\nneed of model retraining. Besides, it is easy to turn a classical\nfloating-point processing unit into an FFP8-compliant one, and the extra\nhardware cost is minor.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 09:37:23 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 05:32:02 GMT"}, {"version": "v3", "created": "Sat, 24 Apr 2021 07:14:41 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Huang", "Cheng-Wei", ""], ["Chen", "Tim-Wei", ""], ["Huang", "Juinn-Dar", ""]]}, {"id": "2104.07345", "submitter": "Jamal Al Qundus", "authors": "Jamal Al Qundus, Ralph Sch\\\"afermeier, Naouel Karam, Silvio Peikert,\n  Adrian Paschke", "title": "ROC: An Ontology for Country Responses towards COVID-19", "comments": "10 pages, 3 figures", "journal-ref": "Qurator2021 - Conference on Digital Curation Technologies", "doi": null, "report-no": null, "categories": "cs.DL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The ROC ontology for country responses to COVID-19 provides a model for\ncollecting, linking and sharing data on the COVID-19 pandemic. It follows\nsemantic standardization (W3C standards RDF, OWL, SPARQL) for the\nrepresentation of concepts and creation of vocabularies. ROC focuses on country\nmeasures and enables the integration of data from heterogeneous data sources.\nThe proposed ontology is intended to facilitate statistical analysis to study\nand evaluate the effectiveness and side effects of government responses to\nCOVID-19 in different countries. The ontology contains data collected by OxCGRT\nfrom publicly available information. This data has been compiled from\ninformation provided by ECDC for most countries, as well as from various\nrepositories used to collect data on COVID-19.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 10:12:19 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Qundus", "Jamal Al", ""], ["Sch\u00e4fermeier", "Ralph", ""], ["Karam", "Naouel", ""], ["Peikert", "Silvio", ""], ["Paschke", "Adrian", ""]]}, {"id": "2104.07365", "submitter": "Erick Lavoie", "authors": "Aur\\'elien Bellet, Anne-Marie Kermarrec, Erick Lavoie", "title": "D-Cliques: Compensating NonIIDness in Decentralized Federated Learning\n  with Topology", "comments": "22 pages, 11 figures. Revisions in v2 and v3: Corrected typos, minor\n  mistakes in scaling analysis that did not affect the overall argument, and\n  clarified convergence plots to read better in black&white", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The convergence speed of machine learning models trained with Federated\nLearning is significantly affected by non-independent and identically\ndistributed (non-IID) data partitions, even more so in a fully decentralized\nsetting without a central server. In this paper, we show that the impact of\nlocal class bias, an important type of data non-IIDness, can be significantly\nreduced by carefully designing the underlying communication topology. We\npresent D-Cliques, a novel topology that reduces gradient bias by grouping\nnodes in interconnected cliques such that the local joint distribution in a\nclique is representative of the global class distribution. We also show how to\nadapt the updates of decentralized SGD to obtain unbiased gradients and\nimplement an effective momentum with D-Cliques. Our empirical evaluation on\nMNIST and CIFAR10 demonstrates that our approach provides similar convergence\nspeed as a fully-connected topology with a significant reduction in the number\nof edges and messages. In a 1000-node topology, D-Cliques requires 98% less\nedges and 96% less total messages, with further possible gains using a\nsmall-world topology across cliques.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 10:47:27 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 10:05:56 GMT"}, {"version": "v3", "created": "Mon, 26 Jul 2021 13:58:09 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Bellet", "Aur\u00e9lien", ""], ["Kermarrec", "Anne-Marie", ""], ["Lavoie", "Erick", ""]]}, {"id": "2104.07378", "submitter": "Maya Ramanath", "authors": "Saransh Goyal, Pratyush Pandey, Garima Gaur, Subhalingam D, Srikanta\n  Bedathur, Maya Ramanath", "title": "Tracking entities in technical procedures -- a new dataset and baselines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce TechTrack, a new dataset for tracking entities in technical\nprocedures. The dataset, prepared by annotating open domain articles from\nWikiHow, consists of 1351 procedures, e.g., \"How to connect a printer\",\nidentifies more than 1200 unique entities with an average of 4.7 entities per\nprocedure. We evaluate the performance of state-of-the-art models on the\nentity-tracking task and find that they are well below the human annotation\nperformance. We describe how TechTrack can be used to take forward the research\non understanding procedures from temporal texts.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 11:16:41 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Goyal", "Saransh", ""], ["Pandey", "Pratyush", ""], ["Gaur", "Garima", ""], ["D", "Subhalingam", ""], ["Bedathur", "Srikanta", ""], ["Ramanath", "Maya", ""]]}, {"id": "2104.07391", "submitter": "Daniel Weber", "authors": "Daniel Weber, Clemens G\\\"uhmann, Thomas Seel", "title": "RIANN -- A Robust Neural Network Outperforms Attitude Estimation Filters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inertial-sensor-based attitude estimation is a crucial technology in various\napplications, from human motion tracking to autonomous aerial and ground\nvehicles. Application scenarios differ in characteristics of the performed\nmotion, presence of disturbances, and environmental conditions. Since\nstate-of-the-art attitude estimators do not generalize well over these\ncharacteristics, their parameters must be tuned for the individual motion\ncharacteristics and circumstances. We propose RIANN, a real-time-capable neural\nnetwork for robust IMU-based attitude estimation, which generalizes well across\ndifferent motion dynamics, environments, and sampling rates, without the need\nfor application-specific adaptations. We exploit two publicly available\ndatasets for the method development and the training, and we add four\ncompletely different datasets for evaluation of the trained neural network in\nthree different test scenarios with varying practical relevance. Results show\nthat RIANN performs at least as well as state-of-the-art attitude estimation\nfilters and outperforms them in several cases, even if the filter is tuned on\nthe very same test dataset itself while RIANN has never seen data from that\ndataset, from the specific application, the same sensor hardware, or the same\nsampling frequency before. RIANN is expected to enable plug-and-play solutions\nin numerous applications, especially when accuracy is crucial but no\nground-truth data is available for tuning or when motion and disturbance\ncharacteristics are uncertain. We made RIANN publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 11:40:25 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 12:43:55 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Weber", "Daniel", ""], ["G\u00fchmann", "Clemens", ""], ["Seel", "Thomas", ""]]}, {"id": "2104.07395", "submitter": "Mingfu Xue", "authors": "Mingfu Xue, Can He, Shichang Sun, Jian Wang, Weiqiang Liu", "title": "Robust Backdoor Attacks against Deep Neural Networks in Real Physical\n  World", "comments": null, "journal-ref": "The 20th IEEE International Conference on Trust, Security and\n  Privacy in Computing and Communications (TrustCom 2021)", "doi": null, "report-no": null, "categories": "cs.CR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNN) have been widely deployed in various applications.\nHowever, many researches indicated that DNN is vulnerable to backdoor attacks.\nThe attacker can create a hidden backdoor in target DNN model, and trigger the\nmalicious behaviors by submitting specific backdoor instance. However, almost\nall the existing backdoor works focused on the digital domain, while few\nstudies investigate the backdoor attacks in real physical world. Restricted to\na variety of physical constraints, the performance of backdoor attacks in the\nreal physical world will be severely degraded. In this paper, we propose a\nrobust physical backdoor attack method, PTB (physical transformations for\nbackdoors), to implement the backdoor attacks against deep learning models in\nthe real physical world. Specifically, in the training phase, we perform a\nseries of physical transformations on these injected backdoor instances at each\nround of model training, so as to simulate various transformations that a\nbackdoor may experience in real world, thus improves its physical robustness.\nExperimental results on the state-of-the-art face recognition model show that,\ncompared with the backdoor methods that without PTB, the proposed attack method\ncan significantly improve the performance of backdoor attacks in real physical\nworld. Under various complex physical conditions, by injecting only a very\nsmall ratio (0.5%) of backdoor instances, the attack success rate of physical\nbackdoor attacks with the PTB method on VGGFace is 82%, while the attack\nsuccess rate of backdoor attacks without the proposed PTB method is lower than\n11%. Meanwhile, the normal performance of the target DNN model has not been\naffected.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 11:51:14 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 12:50:49 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["Xue", "Mingfu", ""], ["He", "Can", ""], ["Sun", "Shichang", ""], ["Wang", "Jian", ""], ["Liu", "Weiqiang", ""]]}, {"id": "2104.07411", "submitter": "Dieter Brughmans", "authors": "Dieter Brughmans and David Martens", "title": "NICE: An Algorithm for Nearest Instance Counterfactual Explanations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we suggest NICE: a new algorithm to generate counterfactual\nexplanations for heterogeneous tabular data. The design of our algorithm\nspecifically takes into account algorithmic requirements that often emerge in\nreal-life deployments: the ability to provide an explanation for all\npredictions, being efficient in run-time, and being able to handle any\nclassification model (also non-differentiable ones). More specifically, our\napproach exploits information from a nearest instance tospeed up the search\nprocess. We propose four versions of NICE, where three of them optimize the\nexplanations for one of the following properties: sparsity, proximity or\nplausibility. An extensive empirical comparison on 10 datasets shows that our\nalgorithm performs better on all properties than the current state-of-the-art.\nThese analyses show a trade-off between on the one hand plausiblity and on the\nother hand proximity or sparsity, with our different optimization methods\noffering the choice to select the preferred trade-off. An open-source\nimplementation of NICE can be found at https://github.com/ADMAntwerp/NICE.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 12:21:01 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Brughmans", "Dieter", ""], ["Martens", "David", ""]]}, {"id": "2104.07412", "submitter": "Sebastian Ruder", "authors": "Sebastian Ruder, Noah Constant, Jan Botha, Aditya Siddhant, Orhan\n  Firat, Jinlan Fu, Pengfei Liu, Junjie Hu, Graham Neubig, Melvin Johnson", "title": "XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning has brought striking advances in multilingual natural\nlanguage processing capabilities over the past year. For example, the latest\ntechniques have improved the state-of-the-art performance on the XTREME\nmultilingual benchmark by more than 13 points. While a sizeable gap to\nhuman-level performance remains, improvements have been easier to achieve in\nsome tasks than in others. This paper analyzes the current state of\ncross-lingual transfer learning and summarizes some lessons learned. In order\nto catalyze meaningful progress, we extend XTREME to XTREME-R, which consists\nof an improved set of ten natural language understanding tasks, including\nchallenging language-agnostic retrieval tasks, and covers 50 typologically\ndiverse languages. In addition, we provide a massively multilingual diagnostic\nsuite and fine-grained multi-dataset evaluation capabilities through an\ninteractive public leaderboard to gain a better understanding of such models.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 12:26:12 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Ruder", "Sebastian", ""], ["Constant", "Noah", ""], ["Botha", "Jan", ""], ["Siddhant", "Aditya", ""], ["Firat", "Orhan", ""], ["Fu", "Jinlan", ""], ["Liu", "Pengfei", ""], ["Hu", "Junjie", ""], ["Neubig", "Graham", ""], ["Johnson", "Melvin", ""]]}, {"id": "2104.07423", "submitter": "Preslav Nakov", "authors": "Shaden Shaar, Firoj Alam, Giovanni Da San Martino, Preslav Nakov", "title": "The Role of Context in Detecting Previously Fact-Checked Claims", "comments": "detecting previously fact-checked claims, fact-checking,\n  disinformation, fake news, social media, political debates", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen the proliferation of disinformation and misinformation\nonline, thanks to the freedom of expression on the Internet and to the rise of\nsocial media. Two solutions were proposed to address the problem: (i) manual\nfact-checking, which is accurate and credible, but slow and non-scalable, and\n(ii) automatic fact-checking, which is fast and scalable, but lacks\nexplainability and credibility. With the accumulation of enough manually\nfact-checked claims, a middle-ground approach has emerged: checking whether a\ngiven claim has previously been fact-checked. This can be made automatically,\nand thus fast, while also offering credibility and explainability, thanks to\nthe human fact-checking and explanations in the associated fact-checking\narticle. This is a relatively new and understudied research direction, and here\nwe focus on claims made in a political debate, where context really matters.\nThus, we study the impact of modeling the context of the claim: both on the\nsource side, i.e., in the debate, as well as on the target side, i.e., in the\nfact-checking explanation document. We do this by modeling the local context,\nthe global context, as well as by means of co-reference resolution, and\nreasoning over the target text using Transformer-XH. The experimental results\nshow that each of these represents a valuable information source, but that\nmodeling the source-side context is more important, and can yield 10+ points of\nabsolute improvement.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 12:39:37 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Shaar", "Shaden", ""], ["Alam", "Firoj", ""], ["Martino", "Giovanni Da San", ""], ["Nakov", "Preslav", ""]]}, {"id": "2104.07454", "submitter": "Rohitash Chandra", "authors": "Animesh Renanse, Rohitash Chandra, Alok Sharma", "title": "Memory Capacity of Neural Turing Machines with Matrix Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It is well known that recurrent neural networks (RNNs) faced limitations in\nlearning long-term dependencies that have been addressed by memory structures\nin long short-term memory (LSTM) networks. Matrix neural networks feature\nmatrix representation which inherently preserves the spatial structure of data\nand has the potential to provide better memory structures when compared to\ncanonical neural networks that use vector representation. Neural Turing\nmachines (NTMs) are novel RNNs that implement notion of programmable computers\nwith neural network controllers to feature algorithms that have copying,\nsorting, and associative recall tasks. In this paper, we study the augmentation\nof memory capacity with a matrix representation of RNNs and NTMs (MatNTMs). We\ninvestigate if matrix representation has a better memory capacity than the\nvector representations in conventional neural networks. We use a probabilistic\nmodel of the memory capacity using Fisher information and investigate how the\nmemory capacity for matrix representation networks are limited under various\nconstraints, and in general, without any constraints. In the case of memory\ncapacity without any constraints, we found that the upper bound on memory\ncapacity to be $N^2$ for an $N\\times N$ state matrix. The results from our\nexperiments using synthetic algorithmic tasks show that MatNTMs have a better\nlearning capacity when compared to its counterparts.\n", "versions": [{"version": "v1", "created": "Sun, 11 Apr 2021 23:43:28 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Renanse", "Animesh", ""], ["Chandra", "Rohitash", ""], ["Sharma", "Alok", ""]]}, {"id": "2104.07456", "submitter": "Hassan Sajjad", "authors": "Hassan Sajjad and Firoj Alam and Fahim Dalvi and Nadir Durrani", "title": "Effect of Post-processing on Contextualized Word Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Post-processing of static embedding has beenshown to improve their\nperformance on both lexical and sequence-level tasks. However, post-processing\nfor contextualized embeddings is an under-studied problem. In this work, we\nquestion the usefulness of post-processing for contextualized embeddings\nobtained from different layers of pre-trained language models. More\nspecifically, we standardize individual neuron activations using z-score,\nmin-max normalization, and by removing top principle components using the\nall-but-the-top method. Additionally, we apply unit length normalization to\nword representations. On a diverse set of pre-trained models, we show that\npost-processing unwraps vital information present in the representations for\nboth lexical tasks (such as word similarity and analogy)and sequence\nclassification tasks. Our findings raise interesting points in relation to\ntheresearch studies that use contextualized representations, and suggest\nz-score normalization as an essential step to consider when using them in an\napplication.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 13:40:42 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Sajjad", "Hassan", ""], ["Alam", "Firoj", ""], ["Dalvi", "Fahim", ""], ["Durrani", "Nadir", ""]]}, {"id": "2104.07461", "submitter": "Min-Hung Chen", "authors": "Min-Hung Chen, Baopu Li, Yingze Bao, Ghassan AlRegib", "title": "Action Segmentation with Mixed Temporal Domain Adaptation", "comments": "Winter Conference on Applications of Computer Vision (WACV) 2020.\n  Website: https://minhungchen.netlify.app/publication/mtda", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main progress for action segmentation comes from densely-annotated data\nfor fully-supervised learning. Since manual annotation for frame-level actions\nis time-consuming and challenging, we propose to exploit auxiliary unlabeled\nvideos, which are much easier to obtain, by shaping this problem as a domain\nadaptation (DA) problem. Although various DA techniques have been proposed in\nrecent years, most of them have been developed only for the spatial direction.\nTherefore, we propose Mixed Temporal Domain Adaptation (MTDA) to jointly align\nframe- and video-level embedded feature spaces across domains, and further\nintegrate with the domain attention mechanism to focus on aligning the\nframe-level features with higher domain discrepancy, leading to more effective\ndomain adaptation. Finally, we evaluate our proposed methods on three\nchallenging datasets (GTEA, 50Salads, and Breakfast), and validate that MTDA\noutperforms the current state-of-the-art methods on all three datasets by large\nmargins (e.g. 6.4% gain on F1@50 and 6.8% gain on the edit score for GTEA).\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 13:48:14 GMT"}, {"version": "v2", "created": "Fri, 16 Apr 2021 00:46:15 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Chen", "Min-Hung", ""], ["Li", "Baopu", ""], ["Bao", "Yingze", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "2104.07473", "submitter": "Xiaoyu Xiang", "authors": "Xiaoyu Xiang, Yapeng Tian, Yulun Zhang, Yun Fu, Jan P. Allebach,\n  Chenliang Xu", "title": "Zooming SlowMo: An Efficient One-Stage Framework for Space-Time Video\n  Super-Resolution", "comments": "Journal version of \"Zooming Slow-Mo: Fast and Accurate One-Stage\n  Space-Time Video Super-Resolution\"(CVPR-2020). 14 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we address the space-time video super-resolution, which aims\nat generating a high-resolution (HR) slow-motion video from a low-resolution\n(LR) and low frame rate (LFR) video sequence. A na\\\"ive method is to decompose\nit into two sub-tasks: video frame interpolation (VFI) and video\nsuper-resolution (VSR). Nevertheless, temporal interpolation and spatial\nupscaling are intra-related in this problem. Two-stage approaches cannot fully\nmake use of this natural property. Besides, state-of-the-art VFI or VSR deep\nnetworks usually have a large frame reconstruction module in order to obtain\nhigh-quality photo-realistic video frames, which makes the two-stage approaches\nhave large models and thus be relatively time-consuming. To overcome the\nissues, we present a one-stage space-time video super-resolution framework,\nwhich can directly reconstruct an HR slow-motion video sequence from an input\nLR and LFR video. Instead of reconstructing missing LR intermediate frames as\nVFI models do, we temporally interpolate LR frame features of the missing LR\nframes capturing local temporal contexts by a feature temporal interpolation\nmodule. Extensive experiments on widely used benchmarks demonstrate that the\nproposed framework not only achieves better qualitative and quantitative\nperformance on both clean and noisy LR frames but also is several times faster\nthan recent state-of-the-art two-stage networks. The source code is released in\nhttps://github.com/Mukosame/Zooming-Slow-Mo-CVPR-2020 .\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 17:59:23 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Xiang", "Xiaoyu", ""], ["Tian", "Yapeng", ""], ["Zhang", "Yulun", ""], ["Fu", "Yun", ""], ["Allebach", "Jan P.", ""], ["Xu", "Chenliang", ""]]}, {"id": "2104.07474", "submitter": "Murali Karthick Baskar", "authors": "Murali Karthick Baskar, Luk\\'a\\v{s} Burget, Shinji Watanabe, Ramon\n  Fernandez Astudillo, and Jan \"Honza'' \\v{C}ernock\\'y", "title": "EAT: Enhanced ASR-TTS for Self-supervised Speech Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.AI cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised ASR-TTS models suffer in out-of-domain data conditions. Here\nwe propose an enhanced ASR-TTS (EAT) model that incorporates two main features:\n1) The ASR$\\rightarrow$TTS direction is equipped with a language model reward\nto penalize the ASR hypotheses before forwarding it to TTS. 2) In the\nTTS$\\rightarrow$ASR direction, a hyper-parameter is introduced to scale the\nattention context from synthesized speech before sending it to ASR to handle\nout-of-domain data. Training strategies and the effectiveness of the EAT model\nare explored under out-of-domain data conditions. The results show that EAT\nreduces the performance gap between supervised and self-supervised training\nsignificantly by absolute 2.6\\% and 2.7\\% on Librispeech and BABEL\nrespectively.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 23:18:25 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Baskar", "Murali Karthick", ""], ["Burget", "Luk\u00e1\u0161", ""], ["Watanabe", "Shinji", ""], ["Astudillo", "Ramon Fernandez", ""], ["\u010cernock\u00fd", "Jan \"Honza''", ""]]}, {"id": "2104.07495", "submitter": "Pietro Mazzaglia", "authors": "Pietro Mazzaglia, Ozan Catal, Tim Verbelen, Bart Dhoedt", "title": "Self-Supervised Exploration via Latent Bayesian Surprise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Training with Reinforcement Learning requires a reward function that is used\nto guide the agent towards achieving its objective. However, designing smooth\nand well-behaved rewards is in general not trivial and requires significant\nhuman engineering efforts. Generating rewards in self-supervised way, by\ninspiring the agent with an intrinsic desire to learn and explore the\nenvironment, might induce more general behaviours. In this work, we propose a\ncuriosity-based bonus as intrinsic reward for Reinforcement Learning, computed\nas the Bayesian surprise with respect to a latent state variable, learnt by\nreconstructing fixed random features. We extensively evaluate our model by\nmeasuring the agent's performance in terms of environment exploration, for\ncontinuous tasks, and looking at the game scores achieved, for video games. Our\nmodel is computationally cheap and empirically shows state-of-the-art\nperformance on several problems. Furthermore, experimenting on an environment\nwith stochastic actions, our approach emerged to be the most resilient to\nsimple stochasticity. Further visualization is available on the project\nwebpage.(https://lbsexploration.github.io/)\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 14:40:16 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Mazzaglia", "Pietro", ""], ["Catal", "Ozan", ""], ["Verbelen", "Tim", ""], ["Dhoedt", "Bart", ""]]}, {"id": "2104.07501", "submitter": "Dezhong Yao", "authors": "Dezhong Yao, Peilin Zhao, Chen Yu, Hai Jin, Bin Li", "title": "Sparse online relative similarity learning", "comments": null, "journal-ref": null, "doi": "10.1109/ICDM.2015.100", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For many data mining and machine learning tasks, the quality of a similarity\nmeasure is the key for their performance. To automatically find a good\nsimilarity measure from datasets, metric learning and similarity learning are\nproposed and studied extensively. Metric learning will learn a Mahalanobis\ndistance based on positive semi-definite (PSD) matrix, to measure the distances\nbetween objectives, while similarity learning aims to directly learn a\nsimilarity function without PSD constraint so that it is more attractive. Most\nof the existing similarity learning algorithms are online similarity learning\nmethod, since online learning is more scalable than offline learning. However,\nmost existing online similarity learning algorithms learn a full matrix with d\n2 parameters, where d is the dimension of the instances. This is clearly\ninefficient for high dimensional tasks due to its high memory and computational\ncomplexity. To solve this issue, we introduce several Sparse Online Relative\nSimilarity (SORS) learning algorithms, which learn a sparse model during the\nlearning process, so that the memory and computational cost can be\nsignificantly reduced. We theoretically analyze the proposed algorithms, and\nevaluate them on some real-world high dimensional datasets. Encouraging\nempirical results demonstrate the advantages of our approach in terms of\nefficiency and efficacy.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 14:50:01 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Yao", "Dezhong", ""], ["Zhao", "Peilin", ""], ["Yu", "Chen", ""], ["Jin", "Hai", ""], ["Li", "Bin", ""]]}, {"id": "2104.07511", "submitter": "Idan Schwartz", "authors": "Idan Schwartz", "title": "Ensemble of MRR and NDCG models for Visual Dialog", "comments": "Accepted to NAACL2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Assessing an AI agent that can converse in human language and understand\nvisual content is challenging. Generation metrics, such as BLEU scores favor\ncorrect syntax over semantics. Hence a discriminative approach is often used,\nwhere an agent ranks a set of candidate options. The mean reciprocal rank (MRR)\nmetric evaluates the model performance by taking into account the rank of a\nsingle human-derived answer. This approach, however, raises a new challenge:\nthe ambiguity and synonymy of answers, for instance, semantic equivalence\n(e.g., `yeah' and `yes'). To address this, the normalized discounted cumulative\ngain (NDCG) metric has been used to capture the relevance of all the correct\nanswers via dense annotations. However, the NDCG metric favors the usually\napplicable uncertain answers such as `I don't know. Crafting a model that\nexcels on both MRR and NDCG metrics is challenging. Ideally, an AI agent should\nanswer a human-like reply and validate the correctness of any answer. To\naddress this issue, we describe a two-step non-parametric ranking approach that\ncan merge strong MRR and NDCG models. Using our approach, we manage to keep\nmost MRR state-of-the-art performance (70.41% vs. 71.24%) and the NDCG\nstate-of-the-art performance (72.16% vs. 75.35%). Moreover, our approach won\nthe recent Visual Dialog 2020 challenge. Source code is available at\nhttps://github.com/idansc/mrr-ndcg.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 15:09:32 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 16:52:11 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Schwartz", "Idan", ""]]}, {"id": "2104.07512", "submitter": "Mehrdad Nasser", "authors": "Mehrdad Nasser, Mohamad Bagher Sajadi, Behrouz Minaei-Bidgoli", "title": "A Sample-Based Training Method for Distantly Supervised Relation\n  Extraction with Pre-Trained Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple instance learning (MIL) has become the standard learning paradigm\nfor distantly supervised relation extraction (DSRE). However, due to relation\nextraction being performed at bag level, MIL has significant hardware\nrequirements for training when coupled with large sentence encoders such as\ndeep transformer neural networks. In this paper, we propose a novel sampling\nmethod for DSRE that relaxes these hardware requirements. In the proposed\nmethod, we limit the number of sentences in a batch by randomly sampling\nsentences from the bags in the batch. However, this comes at the cost of losing\nvalid sentences from bags. To alleviate the issues caused by random sampling,\nwe use an ensemble of trained models for prediction. We demonstrate the\neffectiveness of our approach by using our proposed learning setting to\nfine-tuning BERT on the widely NYT dataset. Our approach significantly\noutperforms previous state-of-the-art methods in terms of AUC and P@N metrics.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 15:09:34 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Nasser", "Mehrdad", ""], ["Sajadi", "Mohamad Bagher", ""], ["Minaei-Bidgoli", "Behrouz", ""]]}, {"id": "2104.07519", "submitter": "Th\\'eis Bazin", "authors": "Th\\'eis Bazin and Ga\\\"etan Hadjeres and Philippe Esling and Mikhail\n  Malt", "title": "Spectrogram Inpainting for Interactive Generation of Instrument Sounds", "comments": "8 pages + references + appendices. 4 figures. Published as a\n  conference paper at the The 2020 Joint Conference on AI Music Creativity,\n  October 19-23, 2020, organized and hosted virtually by the Royal Institute of\n  Technology (KTH), Stockholm, Sweden", "journal-ref": "Proceedings of the 1st Joint Conference on AI Music Creativity,\n  2020 (p. 10). Stockholm, Sweden: AIMC", "doi": "10.30746/978-91-519-5560-5", "report-no": null, "categories": "cs.SD cs.AI cs.HC eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Modern approaches to sound synthesis using deep neural networks are hard to\ncontrol, especially when fine-grained conditioning information is not\navailable, hindering their adoption by musicians.\n  In this paper, we cast the generation of individual instrumental notes as an\ninpainting-based task, introducing novel and unique ways to iteratively shape\nsounds. To this end, we propose a two-step approach: first, we adapt the\nVQ-VAE-2 image generation architecture to spectrograms in order to convert\nreal-valued spectrograms into compact discrete codemaps, we then implement\ntoken-masked Transformers for the inpainting-based generation of these\ncodemaps.\n  We apply the proposed architecture on the NSynth dataset on masked resampling\ntasks. Most crucially, we open-source an interactive web interface to transform\nsounds by inpainting, for artists and practitioners alike, opening up to new,\ncreative uses.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 15:17:31 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Bazin", "Th\u00e9is", ""], ["Hadjeres", "Ga\u00ebtan", ""], ["Esling", "Philippe", ""], ["Malt", "Mikhail", ""]]}, {"id": "2104.07528", "submitter": "Kilian Kleeberger", "authors": "Kilian Kleeberger, Markus V\\\"olk, Richard Bormann, Marco F. Huber", "title": "Investigations on Output Parameterizations of Neural Networks for Single\n  Shot 6D Object Pose Estimation", "comments": "Accepted at 2021 IEEE International Conference on Robotics and\n  Automation (ICRA 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Single shot approaches have demonstrated tremendous success on various\ncomputer vision tasks. Finding good parameterizations for 6D object pose\nestimation remains an open challenge. In this work, we propose different novel\nparameterizations for the output of the neural network for single shot 6D\nobject pose estimation. Our learning-based approach achieves state-of-the-art\nperformance on two public benchmark datasets. Furthermore, we demonstrate that\nthe pose estimates can be used for real-world robotic grasping tasks without\nadditional ICP refinement.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 15:29:53 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Kleeberger", "Kilian", ""], ["V\u00f6lk", "Markus", ""], ["Bormann", "Richard", ""], ["Huber", "Marco F.", ""]]}, {"id": "2104.07532", "submitter": "Hesam Hamledari", "authors": "Hesam Hamledari and Martin Fischer", "title": "Measuring the Impact of Blockchain and Smart Contract on Construction\n  Supply Chain Visibility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work assesses the impact of blockchain and smart contract on the\nvisibility of construction supply chain and in the context of payments\n(intersection of cash and product flows). It uses comparative empirical\nexperiments (Charrette Test Method) to draw comparisons between the visibility\nof state-of-practice and blockchain-enabled payment systems in a commercial\nconstruction project. Comparisons were drawn across four levels of granularity.\nThe findings are twofold: 1) blockchain improved information completeness and\ninformation accuracy respectively by an average 216% and 261% compared with the\ndigital state-of-practice solution. The improvements were significantly more\npronounced for inquiries that had higher product, trade, and temporal\ngranularity; 2) blockchain-enabled solution was robust in the face of increased\ngranularity, while the conventional solution experienced 50% and 66.7% decline\nrespectively in completeness and accuracy of information. The paper concludes\nwith a discussion of mechanisms contributing to visibility and technology\nadoption based on business objectives.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 15:35:28 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Hamledari", "Hesam", ""], ["Fischer", "Martin", ""]]}, {"id": "2104.07538", "submitter": "Laura von Rueden", "authors": "Laura von Rueden, Tim Wirtz, Fabian Hueger, Jan David Schneider, Nico\n  Piatkowski, Christian Bauckhage", "title": "Street-Map Based Validation of Semantic Segmentation in Autonomous\n  Driving", "comments": "Final version accepted at the International Conference on Pattern\n  Recognition (ICPR). arXiv admin note: substantial text overlap with\n  arXiv:2011.08008", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial intelligence for autonomous driving must meet strict requirements\non safety and robustness, which motivates the thorough validation of learned\nmodels. However, current validation approaches mostly require ground truth data\nand are thus both cost-intensive and limited in their applicability. We propose\nto overcome these limitations by a model agnostic validation using a-priori\nknowledge from street maps. In particular, we show how to validate semantic\nsegmentation masks and demonstrate the potential of our approach using\nOpenStreetMap. We introduce validation metrics that indicate false positive or\nnegative road segments. Besides the validation approach, we present a method to\ncorrect the vehicle's GPS position so that a more accurate localization can be\nused for the street-map based validation. Lastly, we present quantitative\nresults on the Cityscapes dataset indicating that our validation approach can\nindeed uncover errors in semantic segmentation masks.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 15:48:11 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["von Rueden", "Laura", ""], ["Wirtz", "Tim", ""], ["Hueger", "Fabian", ""], ["Schneider", "Jan David", ""], ["Piatkowski", "Nico", ""], ["Bauckhage", "Christian", ""]]}, {"id": "2104.07567", "submitter": "Kurt Shuster", "authors": "Kurt Shuster, Spencer Poff, Moya Chen, Douwe Kiela, Jason Weston", "title": "Retrieval Augmentation Reduces Hallucination in Conversation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite showing increasingly human-like conversational abilities,\nstate-of-the-art dialogue models often suffer from factual incorrectness and\nhallucination of knowledge (Roller et al., 2020). In this work we explore the\nuse of neural-retrieval-in-the-loop architectures - recently shown to be\neffective in open-domain QA (Lewis et al., 2020b; Izacard and Grave, 2020) -\nfor knowledge-grounded dialogue, a task that is arguably more challenging as it\nrequires querying based on complex multi-turn dialogue context and generating\nconversationally coherent responses. We study various types of architectures\nwith multiple components - retrievers, rankers, and encoder-decoders - with the\ngoal of maximizing knowledgeability while retaining conversational ability. We\ndemonstrate that our best models obtain state-of-the-art performance on two\nknowledge-grounded conversational tasks. The models exhibit open-domain\nconversational capabilities, generalize effectively to scenarios not within the\ntraining data, and, as verified by human evaluations, substantially reduce the\nwell-known problem of knowledge hallucination in state-of-the-art chatbots.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 16:24:43 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Shuster", "Kurt", ""], ["Poff", "Spencer", ""], ["Chen", "Moya", ""], ["Kiela", "Douwe", ""], ["Weston", "Jason", ""]]}, {"id": "2104.07572", "submitter": "Xiquan Cui", "authors": "Mingming Guo, Nian Yan, Xiquan Cui, San He Wu, Unaiza Ahsan, Rebecca\n  West, Khalifeh Al Jadda", "title": "Deep Learning-based Online Alternative Product Recommendations at Scale", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Alternative recommender systems are critical for ecommerce companies. They\nguide customers to explore a massive product catalog and assist customers to\nfind the right products among an overwhelming number of options. However, it is\na non-trivial task to recommend alternative products that fit customer needs.\nIn this paper, we use both textual product information (e.g. product titles and\ndescriptions) and customer behavior data to recommend alternative products. Our\nresults show that the coverage of alternative products is significantly\nimproved in offline evaluations as well as recall and precision. The final A/B\ntest shows that our algorithm increases the conversion rate by 12 percent in a\nstatistically significant way. In order to better capture the semantic meaning\nof product information, we build a Siamese Network with Bidirectional LSTM to\nlearn product embeddings. In order to learn a similarity space that better\nmatches the preference of real customers, we use co-compared data from\nhistorical customer behavior as labels to train the network. In addition, we\nuse NMSLIB to accelerate the computationally expensive kNN computation for\nmillions of products so that the alternative recommendation is able to scale\nacross the entire catalog of a major ecommerce site.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 16:27:45 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Guo", "Mingming", ""], ["Yan", "Nian", ""], ["Cui", "Xiquan", ""], ["Wu", "San He", ""], ["Ahsan", "Unaiza", ""], ["West", "Rebecca", ""], ["Jadda", "Khalifeh Al", ""]]}, {"id": "2104.07587", "submitter": "Sola Shirai", "authors": "Sola Shirai, Oshani Seneviratne, and Deborah L. McGuinness", "title": "Applying Personal Knowledge Graphs to Health", "comments": "Extended abstract for the PHKG2020 workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowledge graphs that encapsulate personal health information, or personal\nhealth knowledge graphs (PHKG), can help enable personalized health care in\nknowledge-driven systems. In this paper we provide a short survey of existing\nwork surrounding the emerging paradigm of PHKGs and highlight the major\nchallenges that remain. We find that while some preliminary exploration exists\non the topic of personal knowledge graphs, development of PHKGs remains\nunder-explored. A range of challenges surrounding the collection, linkage, and\nmaintenance of personal health knowledge remains to be addressed to fully\nrealize PHKGs.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 16:44:27 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Shirai", "Sola", ""], ["Seneviratne", "Oshani", ""], ["McGuinness", "Deborah L.", ""]]}, {"id": "2104.07595", "submitter": "Hariharan Subramonyam", "authors": "Hariharan Subramonyam, Colleen Seifert, Eytan Adar", "title": "Towards A Process Model for Co-Creating AI Experiences", "comments": "ACM DIS'21 pre-print", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thinking of technology as a design material is appealing. It encourages\ndesigners to explore the material's properties to understand its capabilities\nand limitations, a prerequisite to generative design thinking. However, as a\nmaterial, AI resists this approach because its properties emerge as part of the\ndesign process itself. Therefore, designers and AI engineers must collaborate\nin new ways to create both the material and its application experience. We\ninvestigate the co-creation process through a design study with 10 pairs of\ndesigners and engineers. We find that design 'probes' with user data are a\nuseful tool in defining AI materials. Through data probes, designers construct\ndesignerly representations of the envisioned AI experience (AIX) to identify\ndesirable AI characteristics. Data probes facilitate divergent thinking,\nmaterial testing, and design validation. Based on our findings, we propose a\nprocess model for co-creating AIX and offer design considerations for\nincorporating data probes in design tools.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 16:53:34 GMT"}, {"version": "v2", "created": "Thu, 6 May 2021 14:27:03 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Subramonyam", "Hariharan", ""], ["Seifert", "Colleen", ""], ["Adar", "Eytan", ""]]}, {"id": "2104.07635", "submitter": "Hossein Rajaby Faghihi", "authors": "Hossein Rajaby Faghihi and Parisa Kordjamshidi", "title": "Time-Stamped Language Model: Teaching Language Models to Understand the\n  Flow of Events", "comments": "Accepted at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking entities throughout a procedure described in a text is challenging\ndue to the dynamic nature of the world described in the process. Firstly, we\npropose to formulate this task as a question answering problem. This enables us\nto use pre-trained transformer-based language models on other QA benchmarks by\nadapting those to the procedural text understanding. Secondly, since the\ntransformer-based language models cannot encode the flow of events by\nthemselves, we propose a Time-Stamped Language Model~(TSLM model) to encode\nevent information in LMs architecture by introducing the timestamp encoding.\nOur model evaluated on the Propara dataset shows improvements on the published\nstate-of-the-art results with a $3.1\\%$ increase in F1 score. Moreover, our\nmodel yields better results on the location prediction task on the NPN-Cooking\ndataset. This result indicates that our approach is effective for procedural\ntext understanding in general.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 17:50:41 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Faghihi", "Hossein Rajaby", ""], ["Kordjamshidi", "Parisa", ""]]}, {"id": "2104.07637", "submitter": "Yuchen Lian", "authors": "Yuchen Lian, Arianna Bisazza and Tessa Verhoef", "title": "The Effect of Efficient Messaging and Input Variability on Neural-Agent\n  Iterated Language Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural languages commonly display a trade-off among different strategies to\nconvey constituent roles. A similar trade-off, however, has not been observed\nin recent simulations of iterated language learning with neural network based\nagents (Chaabouni et al., 2019b). In this work, we re-evaluate this result in\nthe light of two important factors, namely: the lack of effort-based pressure\nin the agents and the lack of variability in the initial input language.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 17:50:42 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Lian", "Yuchen", ""], ["Bisazza", "Arianna", ""], ["Verhoef", "Tessa", ""]]}, {"id": "2104.07639", "submitter": "Xian Li", "authors": "Xian Li, Hongyu Gong", "title": "Robust Optimization for Multilingual Translation with Imbalanced Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multilingual models are parameter-efficient with the prospect improving\nlow-resource languages by leveraging crosslingual transfer. Despite recent\nadvance in massive multilingual translation with ever-growing model and data,\nhow to effectively train multilingual models has not been well understood. In\nthis paper, we show that a common situation in multilingual training, data\nimbalance among languages, poses optimization tension between high resource and\nlow resource languages where the found multilingual solution is often\nsub-optimal for low resources. We show that common training method which\nupsamples low resources can not robustly optimize population loss with risks of\neither underfitting high resource languages or overfitting low resource ones.\nDrawing on recent findings on the geometry of loss landscape and its effect on\ngeneralization, we propose a principled optimization algorithm, Curvature Aware\nTask Scaling (CATS), which adaptively rescales gradients from different tasks\nwith a meta objective of guiding multilingual training to low-curvature\nneighborhoods with uniformly low loss for all languages. We ran experiments on\ncommon benchmarks (TED, WMT and OPUS-100) with varying degrees of data\nimbalance. CATS effectively improved multilingual optimization and as a result\ndemonstrated consistent gains on low resources ( to BLEU) without hurting high\nresources. In addition, CATS is robust to overparameterization and large batch\nsize training, making it a promising training method for massive multilingual\nmodels that truly improve low resource languages.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 17:51:03 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 06:40:27 GMT"}, {"version": "v3", "created": "Sun, 13 Jun 2021 00:05:27 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Li", "Xian", ""], ["Gong", "Hongyu", ""]]}, {"id": "2104.07644", "submitter": "Swarnadeep Saha", "authors": "Swarnadeep Saha, Prateek Yadav, Lisa Bauer, Mohit Bansal", "title": "ExplaGraphs: An Explanation Graph Generation Task for Structured\n  Commonsense Reasoning", "comments": "25 pages, 10 tables, 24 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent commonsense-reasoning tasks are typically discriminative in nature,\nwhere a model answers a multiple-choice question for a certain context.\nDiscriminative tasks are limiting because they fail to adequately evaluate the\nmodel's ability to reason and explain predictions with underlying commonsense\nknowledge. They also allow such models to use reasoning shortcuts and not be\n\"right for the right reasons\". In this work, we present ExplaGraphs, a new\ngenerative and structured commonsense-reasoning task (and an associated\ndataset) of explanation graph generation for stance prediction. Specifically,\ngiven a belief and an argument, a model has to predict whether the argument\nsupports or counters the belief and also generate a commonsense-augmented graph\nthat serves as non-trivial, complete, and unambiguous explanation for the\npredicted stance. The explanation graphs for our dataset are collected via\ncrowdsourcing through a novel Collect-Judge-And-Refine graph collection\nframework that improves the graph quality via multiple rounds of verification\nand refinement. A significant 83% of our graphs contain external commonsense\nnodes with diverse structures and reasoning depths. We also propose a\nmulti-level evaluation framework that checks for the structural and semantic\ncorrectness of the generated graphs and their plausibility with human-written\ngraphs. We experiment with state-of-the-art text generation models like BART\nand T5 to generate explanation graphs and observe that there is a large gap\nwith human performance, thereby encouraging useful future work for this new\ncommonsense graph-based explanation generation task.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 17:51:36 GMT"}, {"version": "v2", "created": "Sat, 17 Apr 2021 23:34:27 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Saha", "Swarnadeep", ""], ["Yadav", "Prateek", ""], ["Bauer", "Lisa", ""], ["Bansal", "Mohit", ""]]}, {"id": "2104.07650", "submitter": "Ningyu Zhang", "authors": "Xiang Chen, Xin Xie, Ningyu Zhang, Jiahuan Yan, Shumin Deng, Chuanqi\n  Tan, Fei Huang, Luo Si, Huajun Chen", "title": "AdaPrompt: Adaptive Prompt-based Finetuning for Relation Extraction", "comments": "Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we reformulate the relation extraction task as mask language\nmodeling and propose a novel adaptive prompt-based finetuning approach. We\npropose an adaptive label words selection mechanism that scatters the relation\nlabel into variable number of label tokens to handle the complex multiple label\nspace. We further introduce an auxiliary entity discriminator object to\nencourage the model to focus on context representation learning. Extensive\nexperiments on benchmark datasets demonstrate that our approach can achieve\nbetter performance on both the few-shot and supervised setting.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 17:57:43 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Chen", "Xiang", ""], ["Xie", "Xin", ""], ["Zhang", "Ningyu", ""], ["Yan", "Jiahuan", ""], ["Deng", "Shumin", ""], ["Tan", "Chuanqi", ""], ["Huang", "Fei", ""], ["Si", "Luo", ""], ["Chen", "Huajun", ""]]}, {"id": "2104.07662", "submitter": "Deepak Pathak", "authors": "Yuqing Du, Olivia Watkins, Trevor Darrell, Pieter Abbeel, Deepak\n  Pathak", "title": "Auto-Tuned Sim-to-Real Transfer", "comments": "ICRA 2021. First two authors contributed equally. Website at\n  https://yuqingd.github.io/autotuned-sim2real/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Policies trained in simulation often fail when transferred to the real world\ndue to the `reality gap' where the simulator is unable to accurately capture\nthe dynamics and visual properties of the real world. Current approaches to\ntackle this problem, such as domain randomization, require prior knowledge and\nengineering to determine how much to randomize system parameters in order to\nlearn a policy that is robust to sim-to-real transfer while also not being too\nconservative. We propose a method for automatically tuning simulator system\nparameters to match the real world using only raw RGB images of the real world\nwithout the need to define rewards or estimate state. Our key insight is to\nreframe the auto-tuning of parameters as a search problem where we iteratively\nshift the simulation system parameters to approach the real-world system\nparameters. We propose a Search Param Model (SPM) that, given a sequence of\nobservations and actions and a set of system parameters, predicts whether the\ngiven parameters are higher or lower than the true parameters used to generate\nthe observations. We evaluate our method on multiple robotic control tasks in\nboth sim-to-sim and sim-to-real transfer, demonstrating significant improvement\nover naive domain randomization. Project videos and code at\nhttps://yuqingd.github.io/autotuned-sim2real/\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 17:59:55 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 17:58:26 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Du", "Yuqing", ""], ["Watkins", "Olivia", ""], ["Darrell", "Trevor", ""], ["Abbeel", "Pieter", ""], ["Pathak", "Deepak", ""]]}, {"id": "2104.07666", "submitter": "Antoine Rolland", "authors": "Antoine Rolland (ERIC), Jean-Baptiste Aubin (PSPM), Ir\\`ene Gannaz\n  (PSPM), Samuela Leoni", "title": "A Note on Data Simulations for Voting by Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Voting rules based on evaluation inputs rather than preference orders have\nbeen recently proposed, like majority judgement, range voting or approval\nvoting. Traditionally, probabilistic analysis of voting rules supposes the use\nof simulation models to generate preferences data, like the Impartial Culture\n(IC) or Impartial and Anonymous Culture (IAC) models. But these simulation\nmodels are not suitable for the analysis of evaluation-based voting rules as\nthey generate preference orders instead of the needed evaluations. We propose\nin this paper several simulation models for generating evaluation-based voting\ninputs. These models, inspired by classical ones, are defined, tested and\ncompared for recommendation purpose.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 07:50:32 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Rolland", "Antoine", "", "ERIC"], ["Aubin", "Jean-Baptiste", "", "PSPM"], ["Gannaz", "Ir\u00e8ne", "", "PSPM"], ["Leoni", "Samuela", ""]]}, {"id": "2104.07705", "submitter": "Peter Izsak", "authors": "Peter Izsak, Moshe Berchansky, Omer Levy", "title": "How to Train BERT with an Academic Budget", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While large language models \\`a la BERT are used ubiquitously in NLP,\npretraining them is considered a luxury that only a few well-funded industry\nlabs can afford. How can one train such models with a more modest budget? We\npresent a recipe for pretraining a masked language model in 24 hours, using\nonly 8 low-range 12GB GPUs. We demonstrate that through a combination of\nsoftware optimizations, design choices, and hyperparameter tuning, it is\npossible to produce models that are competitive with BERT-base on GLUE tasks at\na fraction of the original pretraining cost.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 18:17:12 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Izsak", "Peter", ""], ["Berchansky", "Moshe", ""], ["Levy", "Omer", ""]]}, {"id": "2104.07713", "submitter": "Xiao Wang", "authors": "Xiao Wang, Guo-Jun Qi", "title": "Contrastive Learning with Stronger Augmentations", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Representation learning has significantly been developed with the advance of\ncontrastive learning methods. Most of those methods have benefited from various\ndata augmentations that are carefully designated to maintain their identities\nso that the images transformed from the same instance can still be retrieved.\nHowever, those carefully designed transformations limited us to further explore\nthe novel patterns exposed by other transformations. Meanwhile, as found in our\nexperiments, the strong augmentations distorted the images' structures,\nresulting in difficult retrieval. Thus, we propose a general framework called\nContrastive Learning with Stronger Augmentations~(CLSA) to complement current\ncontrastive learning approaches. Here, the distribution divergence between the\nweakly and strongly augmented images over the representation bank is adopted to\nsupervise the retrieval of strongly augmented queries from a pool of instances.\nExperiments on the ImageNet dataset and downstream datasets showed the\ninformation from the strongly augmented images can significantly boost the\nperformance. For example, CLSA achieves top-1 accuracy of 76.2% on ImageNet\nwith a standard ResNet-50 architecture with a single-layer classifier\nfine-tuned, which is almost the same level as 76.5% of supervised results. The\ncode and pre-trained models are available in\nhttps://github.com/maple-research-lab/CLSA.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 18:40:04 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Wang", "Xiao", ""], ["Qi", "Guo-Jun", ""]]}, {"id": "2104.07715", "submitter": "Samuel Yen-Chi Chen", "authors": "En-Jui Kuo, Yao-Lung L. Fang, Samuel Yen-Chi Chen", "title": "Quantum Architecture Search via Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in quantum computing have drawn considerable attention to\nbuilding realistic application for and using quantum computers. However,\ndesigning a suitable quantum circuit architecture requires expert knowledge.\nFor example, it is non-trivial to design a quantum gate sequence for generating\na particular quantum state with as fewer gates as possible. We propose a\nquantum architecture search framework with the power of deep reinforcement\nlearning (DRL) to address this challenge. In the proposed framework, the DRL\nagent can only access the Pauli-$X$, $Y$, $Z$ expectation values and a\npredefined set of quantum operations for learning the target quantum state, and\nis optimized by the advantage actor-critic (A2C) and proximal policy\noptimization (PPO) algorithms. We demonstrate a successful generation of\nquantum gate sequences for multi-qubit GHZ states without encoding any\nknowledge of quantum physics in the agent. The design of our framework is\nrather general and can be employed with other DRL architectures or optimization\nmethods to study gate synthesis and compilation for many quantum states.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 18:53:26 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Kuo", "En-Jui", ""], ["Fang", "Yao-Lung L.", ""], ["Chen", "Samuel Yen-Chi", ""]]}, {"id": "2104.07719", "submitter": "Guangxing Han", "authors": "Guangxing Han, Shiyuan Huang, Jiawei Ma, Yicheng He, Shih-Fu Chang", "title": "Meta Faster R-CNN: Towards Accurate Few-Shot Object Detection with\n  Attentive Feature Alignment", "comments": "14 pages; Typos corrected and references added", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot object detection (FSOD) aims to detect objects using only few\nexamples. It's critically needed for many practical applications but so far\nremains challenging. We propose a meta-learning based few-shot object detection\nmethod by transferring meta-knowledge learned from data-abundant base classes\nto data-scarce novel classes. Our method incorporates a coarse-to-fine approach\ninto the proposal based object detection framework and integrates prototype\nbased classifiers into both the proposal generation and classification stages.\nTo improve proposal generation for few-shot novel classes, we propose to learn\na lightweight matching network to measure the similarity between each spatial\nposition in the query image feature map and spatially-pooled class features,\ninstead of the traditional object/nonobject classifier, thus generating\ncategory-specific proposals and improving proposal recall for novel classes. To\naddress the spatial misalignment between generated proposals and few-shot class\nexamples, we propose a novel attentive feature alignment method, thus improving\nthe performance of few-shot object detection. Meanwhile we jointly learn a\nFaster R-CNN detection head for base classes. Extensive experiments conducted\non multiple FSOD benchmarks show our proposed approach achieves state of the\nart results under (incremental) few-shot learning settings.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 19:01:27 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 17:30:08 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Han", "Guangxing", ""], ["Huang", "Shiyuan", ""], ["Ma", "Jiawei", ""], ["He", "Yicheng", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "2104.07720", "submitter": "Konstantinos Kotis", "authors": "Konstantinos Sikelis, George E Tsekouras, Konstantinos I Kotis", "title": "Ontology-based Feature Selection: A Survey", "comments": "25 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The SemanticWeb emerged as an extension to the traditional Web, towards\nadding meaning to a distributed Web of structured and linked data. At its core,\nthe concept of ontology provides the means to semantically describe and\nstructure information and data and expose it to software and human agents in a\nmachine and human-readable form. For software agents to be realized, it is\ncrucial to develop powerful artificial intelligence and machine learning\ntechniques, able to extract knowledge from information and data sources and\nrepresent it in the underlying ontology. This survey aims to provide insight\ninto key aspects of ontology-based knowledge extraction, from various sources\nsuch as text, images, databases and human expertise, with emphasis on the task\nof feature selection. First, some of the most common classification and feature\nselection algorithms are briefly presented. Then, selected methodologies, which\nutilize ontologies to represent features and perform feature selection and\nclassification, are described. The presented examples span diverse application\ndomains, e.g., medicine, tourism, mechanical and civil engineering, and\ndemonstrate the feasibility and applicability of such methods.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 19:03:31 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 14:17:37 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Sikelis", "Konstantinos", ""], ["Tsekouras", "George E", ""], ["Kotis", "Konstantinos I", ""]]}, {"id": "2104.07750", "submitter": "Dennis Lee", "authors": "Dennis Lee, Natasha Jaques, Chase Kew, Douglas Eck, Dale Schuurmans,\n  Aleksandra Faust", "title": "Joint Attention for Multi-Agent Coordination and Social Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Joint attention - the ability to purposefully coordinate attention with\nanother agent, and mutually attend to the same thing -- is a critical component\nof human social cognition. In this paper, we ask whether joint attention can be\nuseful as a mechanism for improving multi-agent coordination and social\nlearning. We first develop deep reinforcement learning (RL) agents with a\nrecurrent visual attention architecture. We then train agents to minimize the\ndifference between the attention weights that they apply to the environment at\neach timestep, and the attention of other agents. Our results show that this\njoint attention incentive improves agents' ability to solve difficult\ncoordination tasks, by reducing the exponential cost of exploring the joint\nmulti-agent action space. Joint attention leads to higher performance than a\ncompetitive centralized critic baseline across multiple environments. Further,\nwe show that joint attention enhances agents' ability to learn from experts\npresent in their environment, even when completing hard exploration tasks that\ndo not require coordination. Taken together, these findings suggest that joint\nattention may be a useful inductive bias for multi-agent learning.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 20:14:19 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Lee", "Dennis", ""], ["Jaques", "Natasha", ""], ["Kew", "Chase", ""], ["Eck", "Douglas", ""], ["Schuurmans", "Dale", ""], ["Faust", "Aleksandra", ""]]}, {"id": "2104.07762", "submitter": "Sarthak Jain", "authors": "Eric Lehman, Sarthak Jain, Karl Pichotta, Yoav Goldberg, Byron C.\n  Wallace", "title": "Does BERT Pretrained on Clinical Notes Reveal Sensitive Data?", "comments": "NAACL Camera Ready Submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large Transformers pretrained over clinical notes from Electronic Health\nRecords (EHR) have afforded substantial gains in performance on predictive\nclinical tasks. The cost of training such models (and the necessity of data\naccess to do so) coupled with their utility motivates parameter sharing, i.e.,\nthe release of pretrained models such as ClinicalBERT. While most efforts have\nused deidentified EHR, many researchers have access to large sets of sensitive,\nnon-deidentified EHR with which they might train a BERT model (or similar).\nWould it be safe to release the weights of such a model if they did? In this\nwork, we design a battery of approaches intended to recover Personal Health\nInformation (PHI) from a trained BERT. Specifically, we attempt to recover\npatient names and conditions with which they are associated. We find that\nsimple probing methods are not able to meaningfully extract sensitive\ninformation from BERT trained over the MIMIC-III corpus of EHR. However, more\nsophisticated \"attacks\" may succeed in doing so: To facilitate such research,\nwe make our experimental setup and baseline probing models available at\nhttps://github.com/elehman16/exposing_patient_data_release\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 20:40:05 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 22:57:03 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Lehman", "Eric", ""], ["Jain", "Sarthak", ""], ["Pichotta", "Karl", ""], ["Goldberg", "Yoav", ""], ["Wallace", "Byron C.", ""]]}, {"id": "2104.07763", "submitter": "Iulian Vlad Serban", "authors": "Francois St-Hilaire, Nathan Burns, Robert Belfer, Muhammad Shayan,\n  Ariella Smofsky, Dung Do Vu, Antoine Frau, Joseph Potochny, Farid Faraji,\n  Vincent Pavero, Neroli Ko, Ansona Onyi Ching, Sabina Elkins, Anush Stepanyan,\n  Adela Matajova, Laurent Charlin, Yoshua Bengio, Iulian Vlad Serban and\n  Ekaterina Kochmar", "title": "Comparative Study of Learning Outcomes for Online Learning Platforms", "comments": "14 pages, 3 figures, 2 tables, accepted at AIED 2021 (2021 Conference\n  on Artificial Intelligence in Education)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.CL cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalization and active learning are key aspects to successful learning.\nThese aspects are important to address in intelligent educational applications,\nas they help systems to adapt and close the gap between students with varying\nabilities, which becomes increasingly important in the context of online and\ndistance learning. We run a comparative head-to-head study of learning outcomes\nfor two popular online learning platforms: Platform A, which follows a\ntraditional model delivering content over a series of lecture videos and\nmultiple-choice quizzes, and Platform B, which creates a personalized learning\nenvironment and provides problem-solving exercises and personalized feedback.\nWe report on the results of our study using pre- and post-assessment quizzes\nwith participants taking courses on an introductory data science topic on two\nplatforms. We observe a statistically significant increase in the learning\noutcomes on Platform B, highlighting the impact of well-designed and\nwell-engineered technology supporting active learning and problem-based\nlearning in online education. Moreover, the results of the self-assessment\nquestionnaire, where participants reported on perceived learning gains, suggest\nthat participants using Platform B improve their metacognition.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 20:40:24 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["St-Hilaire", "Francois", ""], ["Burns", "Nathan", ""], ["Belfer", "Robert", ""], ["Shayan", "Muhammad", ""], ["Smofsky", "Ariella", ""], ["Vu", "Dung Do", ""], ["Frau", "Antoine", ""], ["Potochny", "Joseph", ""], ["Faraji", "Farid", ""], ["Pavero", "Vincent", ""], ["Ko", "Neroli", ""], ["Ching", "Ansona Onyi", ""], ["Elkins", "Sabina", ""], ["Stepanyan", "Anush", ""], ["Matajova", "Adela", ""], ["Charlin", "Laurent", ""], ["Bengio", "Yoshua", ""], ["Serban", "Iulian Vlad", ""], ["Kochmar", "Ekaterina", ""]]}, {"id": "2104.07782", "submitter": "Ha Thanh Nguyen", "authors": "Ha-Thanh Nguyen, Le-Minh Nguyen", "title": "Sublanguage: A Serious Issue Affects Pretrained Models in Legal Domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Legal English is a sublanguage that is important for everyone but not for\neveryone to understand. Pretrained models have become best practices among\ncurrent deep learning approaches for different problems. It would be a waste or\neven a danger if these models were applied in practice without knowledge of the\nsublanguage of the law. In this paper, we raise the issue and propose a trivial\nsolution by introducing BERTLaw a legal sublanguage pretrained model. The\npaper's experiments demonstrate the superior effectiveness of the method\ncompared to the baseline pretrained model\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 21:25:53 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Nguyen", "Ha-Thanh", ""], ["Nguyen", "Le-Minh", ""]]}, {"id": "2104.07788", "submitter": "Benedek Rozemberczki", "authors": "Benedek Rozemberczki and Paul Scherer and Yixuan He and George\n  Panagopoulos and Alexander Riedel and Maria Astefanoaei and Oliver Kiss and\n  Ferenc Beres and Guzm\\'an L\\'opez and Nicolas Collignon and Rik Sarkar", "title": "PyTorch Geometric Temporal: Spatiotemporal Signal Processing with Neural\n  Machine Learning Models", "comments": "Source code at:\n  https://github.com/benedekrozemberczki/pytorch_geometric_temporal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present PyTorch Geometric Temporal a deep learning framework combining\nstate-of-the-art machine learning algorithms for neural spatiotemporal signal\nprocessing. The main goal of the library is to make temporal geometric deep\nlearning available for researchers and machine learning practitioners in a\nunified easy-to-use framework. PyTorch Geometric Temporal was created with\nfoundations on existing libraries in the PyTorch eco-system, streamlined neural\nnetwork layer definitions, temporal snapshot generators for batching, and\nintegrated benchmark datasets. These features are illustrated with a\ntutorial-like case study. Experiments demonstrate the predictive performance of\nthe models implemented in the library on real world problems such as\nepidemiological forecasting, ridehail demand prediction and web-traffic\nmanagement. Our sensitivity analysis of runtime shows that the framework can\npotentially operate on web-scale datasets with rich temporal features and\nspatial structure.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 21:45:57 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 20:23:20 GMT"}, {"version": "v3", "created": "Thu, 10 Jun 2021 19:58:36 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Rozemberczki", "Benedek", ""], ["Scherer", "Paul", ""], ["He", "Yixuan", ""], ["Panagopoulos", "George", ""], ["Riedel", "Alexander", ""], ["Astefanoaei", "Maria", ""], ["Kiss", "Oliver", ""], ["Beres", "Ferenc", ""], ["L\u00f3pez", "Guzm\u00e1n", ""], ["Collignon", "Nicolas", ""], ["Sarkar", "Rik", ""]]}, {"id": "2104.07789", "submitter": "Danushka Bollegala", "authors": "Michael Abaho, Danushka Bollegala, Paula Williamson, Susanna Dodd", "title": "Detect and Classify -- Joint Span Detection and Classification for\n  Health Outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A health outcome is a measurement or an observation used to capture and\nassess the effect of a treatment. Automatic detection of health outcomes from\ntext would undoubtedly speed up access to evidence necessary in healthcare\ndecision making. Prior work on outcome detection has modelled this task as\neither (a) a sequence labelling task, where the goal is to detect which text\nspans describe health outcomes or (b) a classification task, where the goal is\nto classify a text into a pre-defined set of categories depending on an outcome\nthat is mentioned somewhere in that text. However, this decoupling of span\ndetection and classification is problematic from a modelling perspective and\nignores global structural correspondences between sentence-level and word-level\ninformation present in a given text. We propose a method that uses both\nword-level and sentence-level information to simultaneously perform outcome\nspan detection and outcome type classification. In addition to injecting\ncontextual information to hidden vectors, we use label attention to\nappropriately weight both word-level and sentence-level information.\nExperimental results on several benchmark datasets for health outcome detection\nshow that our model consistently outperforms decoupled methods, reporting\ncompetitive results.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 21:47:15 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Abaho", "Michael", ""], ["Bollegala", "Danushka", ""], ["Williamson", "Paula", ""], ["Dodd", "Susanna", ""]]}, {"id": "2104.07800", "submitter": "Revanth Reddy", "authors": "Revanth Gangi Reddy, Vikas Yadav, Md Arafat Sultan, Martin Franz,\n  Vittorio Castelli, Heng Ji, Avirup Sil", "title": "Towards Robust Neural Retrieval Models with Synthetic Pre-Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent work has shown that commonly available machine reading comprehension\n(MRC) datasets can be used to train high-performance neural information\nretrieval (IR) systems. However, the evaluation of neural IR has so far been\nlimited to standard supervised learning settings, where they have outperformed\ntraditional term matching baselines. We conduct in-domain and out-of-domain\nevaluations of neural IR, and seek to improve its robustness across different\nscenarios, including zero-shot settings. We show that synthetic training\nexamples generated using a sequence-to-sequence generator can be effective\ntowards this goal: in our experiments, pre-training with synthetic examples\nimproves retrieval performance in both in-domain and out-of-domain evaluation\non five different test sets.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 22:12:01 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Reddy", "Revanth Gangi", ""], ["Yadav", "Vikas", ""], ["Sultan", "Md Arafat", ""], ["Franz", "Martin", ""], ["Castelli", "Vittorio", ""], ["Ji", "Heng", ""], ["Sil", "Avirup", ""]]}, {"id": "2104.07853", "submitter": "Nariman Torkzaban", "authors": "Anousheh Gholami, Nariman Torkzaban, John S. Baras", "title": "On the Importance of Trust in Next-Generation Networked CPS Systems: An\n  AI Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.AI cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the increasing scale, complexity, and heterogeneity of the next\ngeneration networked systems, seamless control, management, and security of\nsuch systems becomes increasingly challenging. Many diverse applications have\ndriven interest in networked systems, including large-scale distributed\nlearning, multi-agent optimization, 5G service provisioning, and network\nslicing, etc. In this paper, we propose trust as a measure to evaluate the\nstatus of network agents and improve the decision-making process. We interpret\ntrust as a relation among entities that participate in various protocols. Trust\nrelations are based on evidence created by the interactions of entities within\na protocol and may be a composite of multiple metrics such as availability,\nreliability, resilience, etc. depending on application context. We first\nelaborate on the importance of trust as a metric and then present a\nmathematical framework for trust computation and aggregation within a network.\nThen we show in practice, how trust can be integrated into network\ndecision-making processes by presenting two examples. In the first example, we\nshow how utilizing the trust evidence can improve the performance and the\nsecurity of Federated Learning. Second, we show how a 5G network resource\nprovisioning framework can be improved when augmented with a trust-aware\ndecision-making scheme. We verify the validity of our trust-based approach\nthrough simulations. Finally, we explain the challenges associated with\naggregating the trust evidence and briefly explain our ideas to tackle them.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 02:12:13 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Gholami", "Anousheh", ""], ["Torkzaban", "Nariman", ""], ["Baras", "John S.", ""]]}, {"id": "2104.07857", "submitter": "Samyam Rajbhandari", "authors": "Samyam Rajbhandari, Olatunji Ruwase, Jeff Rasley, Shaden Smith,\n  Yuxiong He", "title": "ZeRO-Infinity: Breaking the GPU Memory Wall for Extreme Scale Deep\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the last three years, the largest dense deep learning models have grown\nover 1000x to reach hundreds of billions of parameters, while the GPU memory\nhas only grown by 5x (16 GB to 80 GB). Therefore, the growth in model scale has\nbeen supported primarily though system innovations that allow large models to\nfit in the aggregate GPU memory of multiple GPUs. However, we are getting close\nto the GPU memory wall. It requires 800 NVIDIA V100 GPUs just to fit a trillion\nparameter model for training, and such clusters are simply out of reach for\nmost data scientists. In addition, training models at that scale requires\ncomplex combinations of parallelism techniques that puts a big burden on the\ndata scientists to refactor their model.\n  In this paper we present ZeRO-Infinity, a novel heterogeneous system\ntechnology that leverages GPU, CPU, and NVMe memory to allow for unprecedented\nmodel scale on limited resources without requiring model code refactoring. At\nthe same time it achieves excellent training throughput and scalability,\nunencumbered by the limited CPU or NVMe bandwidth. ZeRO-Infinity can fit models\nwith tens and even hundreds of trillions of parameters for training on current\ngeneration GPU clusters. It can be used to fine-tune trillion parameter models\non a single NVIDIA DGX-2 node, making large models more accessible. In terms of\ntraining throughput and scalability, it sustains over 25 petaflops on 512\nNVIDIA V100 GPUs(40% of peak), while also demonstrating super linear\nscalability. An open source implementation of ZeRO-Infinity is available\nthrough DeepSpeed, a deep learning optimization library that makes distributed\ntraining easy, efficient, and effective.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 02:22:12 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Rajbhandari", "Samyam", ""], ["Ruwase", "Olatunji", ""], ["Rasley", "Jeff", ""], ["Smith", "Shaden", ""], ["He", "Yuxiong", ""]]}, {"id": "2104.07874", "submitter": "Denis Newman-Griffis", "authors": "Denis Newman-Griffis, Jill Fain Lehman, Carolyn Ros\\'e, Harry\n  Hochheiser", "title": "Translational NLP: A New Paradigm and General Principles for Natural\n  Language Processing Research", "comments": "Accepted to NAACL-HLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language processing (NLP) research combines the study of universal\nprinciples, through basic science, with applied science targeting specific use\ncases and settings. However, the process of exchange between basic NLP and\napplications is often assumed to emerge naturally, resulting in many\ninnovations going unapplied and many important questions left unstudied. We\ndescribe a new paradigm of Translational NLP, which aims to structure and\nfacilitate the processes by which basic and applied NLP research inform one\nanother. Translational NLP thus presents a third research paradigm, focused on\nunderstanding the challenges posed by application needs and how these\nchallenges can drive innovation in basic science and technology design. We show\nthat many significant advances in NLP research have emerged from the\nintersection of basic principles with application needs, and present a\nconceptual framework outlining the stakeholders and key questions in\ntranslational research. Our framework provides a roadmap for developing\nTranslational NLP as a dedicated research area, and identifies general\ntranslational principles to facilitate exchange between basic and applied\nresearch.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 03:46:10 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Newman-Griffis", "Denis", ""], ["Lehman", "Jill Fain", ""], ["Ros\u00e9", "Carolyn", ""], ["Hochheiser", "Harry", ""]]}, {"id": "2104.07876", "submitter": "Xingxuan Zhang", "authors": "Xingxuan Zhang, Peng Cui, Renzhe Xu, Linjun Zhou, Yue He, Zheyan Shen", "title": "Deep Stable Learning for Out-Of-Distribution Generalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Approaches based on deep neural networks have achieved striking performance\nwhen testing data and training data share similar distribution, but can\nsignificantly fail otherwise. Therefore, eliminating the impact of distribution\nshifts between training and testing data is crucial for building\nperformance-promising deep models. Conventional methods assume either the known\nheterogeneity of training data (e.g. domain labels) or the approximately equal\ncapacities of different domains. In this paper, we consider a more challenging\ncase where neither of the above assumptions holds. We propose to address this\nproblem by removing the dependencies between features via learning weights for\ntraining samples, which helps deep models get rid of spurious correlations and,\nin turn, concentrate more on the true connection between discriminative\nfeatures and labels. Extensive experiments clearly demonstrate the\neffectiveness of our method on multiple distribution generalization benchmarks\ncompared with state-of-the-art counterparts. Through extensive experiments on\ndistribution generalization benchmarks including PACS, VLCS, MNIST-M, and NICO,\nwe show the effectiveness of our method compared with state-of-the-art\ncounterparts.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 03:54:21 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Zhang", "Xingxuan", ""], ["Cui", "Peng", ""], ["Xu", "Renzhe", ""], ["Zhou", "Linjun", ""], ["He", "Yue", ""], ["Shen", "Zheyan", ""]]}, {"id": "2104.07919", "submitter": "Oliver Biggar", "authors": "Oliver Biggar, Mohammad Zamani, Iman Shames", "title": "An expressiveness hierarchy of Behavior Trees and related architectures", "comments": "8 pages, 2 figures. Accepted to IEEE Robotics and Automation Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide a formal framework for comparing the expressive\npower of Behavior Trees (BTs) to other action selection architectures. Taking\ninspiration from the analogous comparisons of structural programming\nmethodologies, we formalise the concept of `expressiveness'. This leads us to\nan expressiveness hierarchy of control architectures, which includes BTs,\nDecision Trees (DTs), Teleo-reactive Programs (TRs) and Finite State Machines\n(FSMs). By distinguishing between BTs with auxiliary variables and those\nwithout, we demonstrate the existence of a trade-off in BT design between\nreadability and expressiveness. We discuss what this means for BTs in practice.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 06:46:52 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Biggar", "Oliver", ""], ["Zamani", "Mohammad", ""], ["Shames", "Iman", ""]]}, {"id": "2104.07921", "submitter": "Hung Le", "authors": "Hung Le, Nancy F. Chen, Steven C.H. Hoi", "title": "VGNMN: Video-grounded Neural Module Network to Video-Grounded Language\n  Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Neural module networks (NMN) have achieved success in image-grounded tasks\nsuch as Visual Question Answering (VQA) on synthetic images. However, very\nlimited work on NMN has been studied in the video-grounded language tasks.\nThese tasks extend the complexity of traditional visual tasks with the\nadditional visual temporal variance. Motivated by recent NMN approaches on\nimage-grounded tasks, we introduce Video-grounded Neural Module Network (VGNMN)\nto model the information retrieval process in video-grounded language tasks as\na pipeline of neural modules. VGNMN first decomposes all language components to\nexplicitly resolve any entity references and detect corresponding action-based\ninputs from the question. The detected entities and actions are used as\nparameters to instantiate neural module networks and extract visual cues from\nthe video. Our experiments show that VGNMN can achieve promising performance on\ntwo video-grounded language tasks: video QA and video-grounded dialogues.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 06:47:41 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Le", "Hung", ""], ["Chen", "Nancy F.", ""], ["Hoi", "Steven C. H.", ""]]}, {"id": "2104.07944", "submitter": "Amir Hadifar", "authors": "Amir Hadifar, Sofie Labat, V\\'eronique Hoste, Chris Develder and\n  Thomas Demeester", "title": "A Million Tweets Are Worth a Few Points: Tuning Transformers for\n  Customer Service Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In online domain-specific customer service applications, many companies\nstruggle to deploy advanced NLP models successfully, due to the limited\navailability of and noise in their datasets. While prior research demonstrated\nthe potential of migrating large open-domain pretrained models for\ndomain-specific tasks, the appropriate (pre)training strategies have not yet\nbeen rigorously evaluated in such social media customer service settings,\nespecially under multilingual conditions. We address this gap by collecting a\nmultilingual social media corpus containing customer service conversations\n(865k tweets), comparing various pipelines of pretraining and finetuning\napproaches, applying them on 5 different end tasks. We show that pretraining a\ngeneric multilingual transformer model on our in-domain dataset, before\nfinetuning on specific end tasks, consistently boosts performance, especially\nin non-English settings.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 07:45:04 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Hadifar", "Amir", ""], ["Labat", "Sofie", ""], ["Hoste", "V\u00e9ronique", ""], ["Develder", "Chris", ""], ["Demeester", "Thomas", ""]]}, {"id": "2104.07955", "submitter": "Weiqi Shu", "authors": "Weiqi Shu, Ling Wang, Bolong Liu, and Jie Liu", "title": "LAI Estimation of Cucumber Crop Based on Improved Fully Convolutional\n  Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LAI (Leaf Area Index) is of great importance for crop yield estimation in\nagronomy. It is directly related to plant growth status, net assimilation rate,\nplant photosynthesis, and carbon dioxide in the environment. How to measure LAI\naccurately and efficiently is the key to the crop yield estimation problem.\nManual measurement consumes a lot of human resources and material resources.\nRemote sensing technology is not suitable for near-Earth LAI measurement.\nBesides, methods based on traditional digital image processing are greatly\naffected by environmental noise and image exposure. Nowadays, deep learning is\nwidely used in many fields. The improved FCN (Fully Convolutional Network) is\nproposed in our study for LAI measure task. Eighty-two cucumber images\ncollected from our greenhouse are labeled to fine-tuning the pre-trained model.\nThe result shows that the improved FCN model performs well on our dataset. Our\nmethod's mean IoU can reach 0.908, which is 11% better than conventional\nmethods and 4.7% better than the basic FCN model.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 08:12:06 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Shu", "Weiqi", ""], ["Wang", "Ling", ""], ["Liu", "Bolong", ""], ["Liu", "Jie", ""]]}, {"id": "2104.08002", "submitter": "Narendra Chaudhary", "authors": "Narendra Chaudhary, Sanchit Misra, Dhiraj Kalamkar, Alexander\n  Heinecke, Evangelos Georganas, Barukh Ziv, Menachem Adelman, Bharat Kaul", "title": "Efficient and Generic 1D Dilated Convolution Layer for Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional neural networks (CNNs) have found many applications in tasks\ninvolving two-dimensional (2D) data, such as image classification and image\nprocessing. Therefore, 2D convolution layers have been heavily optimized on\nCPUs and GPUs. However, in many applications - for example genomics and speech\nrecognition, the data can be one-dimensional (1D). Such applications can\nbenefit from optimized 1D convolution layers. In this work, we introduce our\nefficient implementation of a generic 1D convolution layer covering a wide\nrange of parameters. It is optimized for x86 CPU architectures, in particular,\nfor architectures containing Intel AVX-512 and AVX-512 BFloat16 instructions.\nWe use the LIBXSMM library's batch-reduce General Matrix Multiplication\n(BRGEMM) kernel for FP32 and BFloat16 precision. We demonstrate that our\nimplementation can achieve up to 80% efficiency on Intel Xeon Cascade Lake and\nCooper Lake CPUs. Additionally, we show the generalization capability of our\nBRGEMM based approach by achieving high efficiency across a range of\nparameters. We consistently achieve higher efficiency than the 1D convolution\nlayer with Intel oneDNN library backend for varying input tensor widths, filter\nwidths, number of channels, filters, and dilation parameters. Finally, we\ndemonstrate the performance of our optimized 1D convolution layer by utilizing\nit in the end-to-end neural network training with real genomics datasets and\nachieve up to 6.86x speedup over the oneDNN library-based implementation on\nCascade Lake CPUs. We also demonstrate the scaling with 16 sockets of\nCascade/Cooper Lake CPUs and achieve significant speedup over eight V100 GPUs\nusing a similar power envelop. In the end-to-end training, we get a speedup of\n1.41x on Cascade Lake with FP32, 1.57x on Cooper Lake with FP32, and 2.27x on\nCooper Lake with BFloat16 over eight V100 GPUs with FP32.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 09:54:30 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Chaudhary", "Narendra", ""], ["Misra", "Sanchit", ""], ["Kalamkar", "Dhiraj", ""], ["Heinecke", "Alexander", ""], ["Georganas", "Evangelos", ""], ["Ziv", "Barukh", ""], ["Adelman", "Menachem", ""], ["Kaul", "Bharat", ""]]}, {"id": "2104.08015", "submitter": "Mika\\\"el Monet", "authors": "Marcelo Arenas, Pablo Barcel\\'o, Leopoldo Bertossi, Mika\\\"el Monet", "title": "On the Complexity of SHAP-Score-Based Explanations: Tractability via\n  Knowledge Compilation and Non-Approximability Results", "comments": "52 pages, including 48 pages of main text. This is an extended\n  version of the AAAI conference paper \"The Tractability of SHAP-Score-Based\n  Explanations over Deterministic and Decomposable Boolean Circuits\"\n  (arXiv:2007.14045), with additional results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In Machine Learning, the $\\mathsf{SHAP}$-score is a version of the Shapley\nvalue that is used to explain the result of a learned model on a specific\nentity by assigning a score to every feature. While in general computing\nShapley values is an intractable problem, we prove a strong positive result\nstating that the $\\mathsf{SHAP}$-score can be computed in polynomial time over\ndeterministic and decomposable Boolean circuits. Such circuits are studied in\nthe field of Knowledge Compilation and generalize a wide range of Boolean\ncircuits and binary decision diagrams classes, including binary decision trees\nand Ordered Binary Decision Diagrams (OBDDs).\n  We also establish the computational limits of the SHAP-score by observing\nthat computing it over a class of Boolean models is always polynomially as hard\nas the model counting problem for that class. This implies that both\ndeterminism and decomposability are essential properties for the circuits that\nwe consider. It also implies that computing $\\mathsf{SHAP}$-scores is\nintractable as well over the class of propositional formulas in DNF. Based on\nthis negative result, we look for the existence of fully-polynomial randomized\napproximation schemes (FPRAS) for computing $\\mathsf{SHAP}$-scores over such\nclass. In contrast to the model counting problem for DNF formulas, which admits\nan FPRAS, we prove that no such FPRAS exists for the computation of\n$\\mathsf{SHAP}$-scores. Surprisingly, this negative result holds even for the\nclass of monotone formulas in DNF. These techniques can be further extended to\nprove another strong negative result: Under widely believed complexity\nassumptions, there is no polynomial-time algorithm that checks, given a\nmonotone DNF formula $\\varphi$ and features $x,y$, whether the\n$\\mathsf{SHAP}$-score of $x$ in $\\varphi$ is smaller than the\n$\\mathsf{SHAP}$-score of $y$ in $\\varphi$.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 10:22:32 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Arenas", "Marcelo", ""], ["Barcel\u00f3", "Pablo", ""], ["Bertossi", "Leopoldo", ""], ["Monet", "Mika\u00ebl", ""]]}, {"id": "2104.08027", "submitter": "Fangyu Liu", "authors": "Fangyu Liu, Ivan Vuli\\'c, Anna Korhonen, Nigel Collier", "title": "Fast, Effective and Self-Supervised: Transforming Masked LanguageModels\n  into Universal Lexical and Sentence Encoders", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pretrained Masked Language Models (MLMs) have revolutionised NLP in recent\nyears. However, previous work has indicated that off-the-shelf MLMs are not\neffective as universal lexical or sentence encoders without further\ntask-specific fine-tuning on NLI, sentence similarity, or paraphrasing tasks\nusing annotated task data. In this work, we demonstrate that it is possible to\nturn MLMs into effective universal lexical and sentence encoders even without\nany additional data and without any supervision. We propose an extremely\nsimple, fast and effective contrastive learning technique, termed Mirror-BERT,\nwhich converts MLMs (e.g., BERT and RoBERTa) into such encoders in less than a\nminute without any additional external knowledge. Mirror-BERT relies on fully\nidentical or slightly modified string pairs as positive (i.e., synonymous)\nfine-tuning examples, and aims to maximise their similarity during identity\nfine-tuning. We report huge gains over off-the-shelf MLMs with Mirror-BERT in\nboth lexical-level and sentence-level tasks, across different domains and\ndifferent languages. Notably, in the standard sentence semantic similarity\n(STS) tasks, our self-supervised Mirror-BERT model even matches the performance\nof the task-tuned Sentence-BERT models from prior work. Finally, we delve\ndeeper into the inner workings of MLMs, and suggest some evidence on why this\nsimple approach can yield effective univeral lexical and sentence encoders.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 10:49:56 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Liu", "Fangyu", ""], ["Vuli\u0107", "Ivan", ""], ["Korhonen", "Anna", ""], ["Collier", "Nigel", ""]]}, {"id": "2104.08028", "submitter": "Asahi Ushio", "authors": "Asahi Ushio and Federico Liberatore and Jose Camacho-Collados", "title": "Back to the Basics: A Quantitative Analysis of Statistical and\n  Graph-Based Term Weighting Schemes for Keyword Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Term weighting schemes are widely used in Natural Language Processing and\nInformation Retrieval. In particular, term weighting is the basis for keyword\nextraction. However, there are relatively few evaluation studies that shed\nlight about the strengths and shortcomings of each weighting scheme. In fact,\nin most cases researchers and practitioners resort to the well-known tf-idf as\ndefault, despite the existence of other suitable alternatives, including\ngraph-based models. In this paper, we perform an exhaustive and large-scale\nempirical comparison of both statistical and graph-based term weighting methods\nin the context of keyword extraction. Our analysis reveals some interesting\nfindings such as the advantages of the less-known lexical specificity with\nrespect to tf-idf, or the qualitative differences between statistical and\ngraph-based methods. Finally, based on our findings we discuss and devise some\nsuggestions for practitioners. We release our code at\nhttps://github.com/asahi417/kex .\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 10:49:58 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Ushio", "Asahi", ""], ["Liberatore", "Federico", ""], ["Camacho-Collados", "Jose", ""]]}, {"id": "2104.08045", "submitter": "Manoj Goyal", "authors": "Rachit S Munjal, Manoj Goyal, Rutika Moharir, Sukumar Moharana", "title": "TeLCoS: OnDevice Text Localization with Clustering of Script", "comments": "Accepted for publication in IJCNN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recent research in the field of text localization in a resource constrained\nenvironment has made extensive use of deep neural networks. Scene text\nlocalization and recognition on low-memory mobile devices have a wide range of\napplications including content extraction, image categorization and keyword\nbased image search. For text recognition of multi-lingual localized text, the\nOCR systems require prior knowledge of the script of each text instance. This\nleads to word script identification being an essential step for text\nrecognition. Most existing methods treat text localization, script\nidentification and text recognition as three separate tasks. This makes script\nidentification an overhead in the recognition pipeline. To reduce this\noverhead, we propose TeLCoS: OnDevice Text Localization with Clustering of\nScript, a multi-task dual branch lightweight CNN network that performs\nreal-time on device Text Localization and High-level Script Clustering\nsimultaneously. The network drastically reduces the number of calls to a\nseparate script identification module, by grouping and identifying some majorly\nused scripts through a single feed-forward pass over the localization network.\nWe also introduce a novel structural similarity based channel pruning mechanism\nto build an efficient network with only 1.15M parameters. Experiments on\nbenchmark datasets suggest that our method achieves state-of-the-art\nperformance, with execution latency of 60 ms for the entire pipeline on the\nExynos 990 chipset device.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 11:45:20 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 12:32:57 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Munjal", "Rachit S", ""], ["Goyal", "Manoj", ""], ["Moharir", "Rutika", ""], ["Moharana", "Sukumar", ""]]}, {"id": "2104.08048", "submitter": "Arkadiy Dushatskiy", "authors": "Arkadiy Dushatskiy, Tanja Alderliesten, Peter A. N. Bosman", "title": "A Novel Surrogate-assisted Evolutionary Algorithm Applied to\n  Partition-based Ensemble Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel surrogate-assisted Evolutionary Algorithm for solving\nexpensive combinatorial optimization problems. We integrate a surrogate model,\nwhich is used for fitness value estimation, into a state-of-the-art P3-like\nvariant of the Gene-Pool Optimal Mixing Algorithm (GOMEA) and adapt the\nresulting algorithm for solving non-binary combinatorial problems. We test the\nproposed algorithm on an ensemble learning problem. Ensembling several models\nis a common Machine Learning technique to achieve better performance. We\nconsider ensembles of several models trained on disjoint subsets of a dataset.\nFinding the best dataset partitioning is naturally a combinatorial non-binary\noptimization problem. Fitness function evaluations can be extremely expensive\nif complex models, such as Deep Neural Networks, are used as learners in an\nensemble. Therefore, the number of fitness function evaluations is typically\nlimited, necessitating expensive optimization techniques. In our experiments we\nuse five classification datasets from the OpenML-CC18 benchmark and\nSupport-vector Machines as learners in an ensemble. The proposed algorithm\ndemonstrates better performance than alternative approaches, including Bayesian\noptimization algorithms. It manages to find better solutions using just several\nthousand fitness function evaluations for an ensemble learning problem with up\nto 500 variables.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 11:51:18 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Dushatskiy", "Arkadiy", ""], ["Alderliesten", "Tanja", ""], ["Bosman", "Peter A. N.", ""]]}, {"id": "2104.08066", "submitter": "Taichi Iki", "authors": "Taichi Iki, Akiko Aizawa", "title": "Effect of Vision-and-Language Extensions on Natural Language\n  Understanding in Vision-and-Language Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extending language models with structural modifications and\nvision-and-language (V&L) pretraining are successful ways of making V&L models\nthat can ground vision and language. Potential applications of these advanced\nmodels include multi-modal machine reading comprehension models and multi-modal\ndialogue models, which require language ability upon grounding. Although\nlanguage capability is crucial for such applications, the impact of extending\ntheir visual capabilities on their language capabilities is not fully\nunderstood. This paper investigates how visual extension affects the language\ncapability of V&L models using the GLUE benchmark. We found that visual\nextension causes some decreases in language capability and that V&L pretraining\nhas a greater impact than structural modifications on the decreases. Our\nresults suggest the need for further study on pretraining that can maintain or,\nif possible, improve a model's language capability.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 12:28:50 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Iki", "Taichi", ""], ["Aizawa", "Akiko", ""]]}, {"id": "2104.08100", "submitter": "Yifan Hu", "authors": "Zhao Wang, Yifan Hu, Jun Xiao, Chao Wu", "title": "Efficient Ring-topology Decentralized Federated Learning with Deep\n  Generative Models for Industrial Artificial Intelligent", "comments": "arXiv admin note: text overlap with arXiv:2010.10996", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By leveraging deep learning based technologies, the data-driven based\napproaches have reached great success with the rapid increase of data generated\nof Industrial Indernet of Things(IIot). However, security and privacy concerns\nare obstacles for data providers in many sensitive data-driven industrial\nscenarios, such as healthcare and auto-driving. Many Federated Learning(FL)\napproaches have been proposed with DNNs for IIoT applications, these works\nstill suffer from low usability of data due to data incompleteness, low\nquality, insufficient quantity, sensitivity, etc. Therefore, we propose a\nring-topogy based decentralized federated learning(RDFL) scheme for Deep\nGenerative Models(DGMs), where DGMs is a promising solution for solving the\naforementioned data usability issues. Compare with existing IIoT FL works, our\nRDFL schemes provides communication efficiency and maintain training\nperformance to boost DGMs in target IIoT tasks. A novel ring FL topology as\nwell as a map-reduce based synchronizing method are designed in the proposed\nRDFL to improve decentralized FL performance and bandwidth utilization. In\naddition, InterPlanetary File System(IPFS) is introduced to further improve\ncommunication efficiency and FL security. Extensive experiments have been taken\nto demonstate the superiority of RDFL with either independent and identically\ndistributed(IID) datasets or non-independent and identically\ndistributed(Non-IID) datasets.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 08:09:54 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Wang", "Zhao", ""], ["Hu", "Yifan", ""], ["Xiao", "Jun", ""], ["Wu", "Chao", ""]]}, {"id": "2104.08126", "submitter": "Dac Tung Vu", "authors": "Dac Tung Vu, Juan Luis Gonzalez, Munchurl Kim", "title": "Exploiting Global and Local Attentions for Heavy Rain Removal on Single\n  Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Heavy rain removal from a single image is the task of simultaneously\neliminating rain streaks and fog, which can dramatically degrade the quality of\ncaptured images. Most existing rain removal methods do not generalize well for\nthe heavy rain case. In this work, we propose a novel network architecture\nconsisting of three sub-networks to remove heavy rain from a single image\nwithout estimating rain streaks and fog separately. The first sub-net, a\nU-net-based architecture that incorporates our Spatial Channel Attention (SCA)\nblocks, extracts global features that provide sufficient contextual information\nneeded to remove atmospheric distortions caused by rain and fog. The second\nsub-net learns the additive residues information, which is useful in removing\nrain streak artifacts via our proposed Residual Inception Modules (RIM). The\nthird sub-net, the multiplicative sub-net, adopts our Channel-attentive\nInception Modules (CIM) and learns the essential brighter local features which\nare not effectively extracted in the SCA and additive sub-nets by modulating\nthe local pixel intensities in the derained images. Our three clean image\nresults are then combined via an attentive blending block to generate the final\nclean image. Our method with SCA, RIM, and CIM significantly outperforms the\nprevious state-of-the-art single-image deraining methods on the synthetic\ndatasets, shows considerably cleaner and sharper derained estimates on the real\nimage datasets. We present extensive experiments and ablation studies\nsupporting each of our method's contributions on both synthetic and real image\ndatasets.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 14:08:27 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Vu", "Dac Tung", ""], ["Gonzalez", "Juan Luis", ""], ["Kim", "Munchurl", ""]]}, {"id": "2104.08145", "submitter": "Keyur Faldu", "authors": "Keyur Faldu, Amit Sheth, Prashant Kikani, Hemang Akabari", "title": "KI-BERT: Infusing Knowledge Context for Better Language and Domain\n  Understanding", "comments": "10 pages, 4 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextualized entity representations learned by state-of-the-art deep\nlearning models (BERT, GPT, T5, etc) leverage the attention mechanism to learn\nthe data context. However, these models are still blind to leverage the\nknowledge context present in the knowledge graph. Knowledge context can be\nunderstood as semantics about entities, and their relationship with neighboring\nentities in knowledge graphs. We propose a novel and effective technique to\ninfuse knowledge context from knowledge graphs for conceptual and ambiguous\nentities into models based on transformer architecture. Our novel technique\nproject knowledge graph embedding in the homogeneous vector-space, introduces\nnew token-types for entities, align entity position ids, and a selective\nattention mechanism. We take BERT as a baseline model and implement\n\"KnowledgeInfused BERT\" by infusing knowledge context from ConceptNet and\nWordNet, which significantly outperforms BERT over a wide range of NLP tasks\nover eight different GLUE datasets. KI-BERT-base model even outperforms\nBERT-large for domain-specific tasks like SciTail and academic subsets of QQP,\nQNLI, and MNLI.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 16:15:31 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Faldu", "Keyur", ""], ["Sheth", "Amit", ""], ["Kikani", "Prashant", ""], ["Akabari", "Hemang", ""]]}, {"id": "2104.08154", "submitter": "Yaoming Zhu", "authors": "Yaoming Zhu, Jiangtao Feng, Chengqi Zhao, Mingxuan Wang, Lei Li", "title": "Serial or Parallel? Plug-able Adapter for multilingual machine\n  translation", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Developing a unified multilingual translation model is a key topic in machine\ntranslation research. However, existing approaches suffer from performance\ndegradation: multilingual models yield inferior performance compared to the\nones trained separately on rich bilingual data. We attribute the performance\ndegradation to two issues: multilingual embedding conflation and multilingual\nfusion effects. To address the two issues, we propose PAM, a Transformer model\naugmented with defusion adaptation for multilingual machine translation.\nSpecifically, PAM consists of embedding and layer adapters to shift the word\nand intermediate representations towards language-specific ones. Extensive\nexperiment results on IWSLT, OPUS-100, and WMT benchmarks show that \\method\noutperforms several strong competitors, including series adapter and\nmultilingual knowledge distillation.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 14:58:28 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Zhu", "Yaoming", ""], ["Feng", "Jiangtao", ""], ["Zhao", "Chengqi", ""], ["Wang", "Mingxuan", ""], ["Li", "Lei", ""]]}, {"id": "2104.08156", "submitter": "Eliane Maalouf", "authors": "Eliane Maalouf, David Ginsbourger and Niklas Linde", "title": "Fast ABC with joint generative modelling and subset simulation", "comments": "13 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach for solving inverse-problems with\nhigh-dimensional inputs and an expensive forward mapping. It leverages joint\ndeep generative modelling to transfer the original problem spaces to a lower\ndimensional latent space. By jointly modelling input and output variables and\nendowing the latent with a prior distribution, the fitted probabilistic model\nindirectly gives access to the approximate conditional distributions of\ninterest. Since model error and observational noise with unknown distributions\nare common in practice, we resort to likelihood-free inference with Approximate\nBayesian Computation (ABC). Our method calls on ABC by Subset Simulation to\nexplore the regions of the latent space with dissimilarities between generated\nand observed outputs below prescribed thresholds. We diagnose the diversity of\napproximate posterior solutions by monitoring the probability content of these\nregions as a function of the threshold. We further analyze the curvature of the\nresulting diagnostic curve to propose an adequate ABC threshold. When applied\nto a cross-borehole tomography example from geophysics, our approach delivers\npromising performance without using prior knowledge of the forward nor of the\nnoise distribution.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 15:03:23 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Maalouf", "Eliane", ""], ["Ginsbourger", "David", ""], ["Linde", "Niklas", ""]]}, {"id": "2104.08164", "submitter": "Nicola De Cao", "authors": "Nicola De Cao, Wilker Aziz, Ivan Titov", "title": "Editing Factual Knowledge in Language Models", "comments": "15 pages, 6 figures, 2 tables. Code at\n  https://github.com/nicola-decao/KnowledgeEditor", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The factual knowledge acquired during pretraining and stored in the\nparameters of Language Models (LM) can be useful in downstream tasks (e.g.,\nquestion answering or textual inference). However, some facts can be\nincorrectly induced or become obsolete over time. We present KnowledgeEditor, a\nmethod that can be used to edit this knowledge and, thus, fix 'bugs' or\nunexpected predictions without the need for expensive re-training or\nfine-tuning. Besides being computationally efficient, KnowledgeEditor does not\nrequire any modifications in LM pre-training (e.g., the use of meta-learning).\nIn our approach, we train a hyper-network with constrained optimization to\nmodify a fact without affecting the rest of the knowledge; the trained\nhyper-network is then used to predict the weight update at test time. We show\nKnowledgeEditor's efficacy with two popular architectures and\nknowledge-intensive tasks: i) a BERT model fine-tuned for fact-checking, and\nii) a sequence-to-sequence BART model for question answering. With our method,\nchanging a prediction on the specific wording of a query tends to result in a\nconsistent change in predictions also for its paraphrases. We show that this\ncan be further encouraged by exploiting (e.g., automatically-generated)\nparaphrases during training. Interestingly, our hyper-network can be regarded\nas a 'probe' revealing which components of a model need to be changed to\nmanipulate factual knowledge; our analysis shows that the updates tend to be\nconcentrated on a small subset of components. Code at\nhttps://github.com/nicola-decao/KnowledgeEditor\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 15:24:42 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["De Cao", "Nicola", ""], ["Aziz", "Wilker", ""], ["Titov", "Ivan", ""]]}, {"id": "2104.08166", "submitter": "Huibin Shen", "authors": "Anastasia Makarova, Huibin Shen, Valerio Perrone, Aaron Klein, Jean\n  Baptiste Faddoul, Andreas Krause, Matthias Seeger, Cedric Archambeau", "title": "Overfitting in Bayesian Optimization: an empirical study and\n  early-stopping solution", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Tuning machine learning models with Bayesian optimization (BO) is a\nsuccessful strategy to find good hyperparameters. BO defines an iterative\nprocedure where a cross-validated metric is evaluated on promising\nhyperparameters. In practice, however, an improvement of the validation metric\nmay not translate in better predictive performance on a test set, especially\nwhen tuning models trained on small datasets. In other words, unlike\nconventional wisdom dictates, BO can overfit. In this paper, we carry out the\nfirst systematic investigation of overfitting in BO and demonstrate that this\nissue is serious, yet often overlooked in practice. We propose a novel\ncriterion to early stop BO, which aims to maintain the solution quality while\nsaving the unnecessary iterations that can lead to overfitting. Experiments on\nreal-world hyperparameter optimization problems show that our approach\neffectively meets these goals and is more adaptive comparing to baselines.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 15:26:23 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 14:25:25 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Makarova", "Anastasia", ""], ["Shen", "Huibin", ""], ["Perrone", "Valerio", ""], ["Klein", "Aaron", ""], ["Faddoul", "Jean Baptiste", ""], ["Krause", "Andreas", ""], ["Seeger", "Matthias", ""], ["Archambeau", "Cedric", ""]]}, {"id": "2104.08189", "submitter": "Boris Ginsburg", "authors": "Stanislav Beliaev, Boris Ginsburg", "title": "TalkNet 2: Non-Autoregressive Depth-Wise Separable Convolutional Model\n  for Speech Synthesis with Explicit Pitch and Duration Prediction", "comments": "arXiv admin note: substantial text overlap with arXiv:2005.05514", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose TalkNet, a non-autoregressive convolutional neural model for\nspeech synthesis with explicit pitch and duration prediction. The model\nconsists of three feed-forward convolutional networks. The first network\npredicts grapheme durations. An input text is expanded by repeating each symbol\naccording to the predicted duration. The second network predicts pitch value\nfor every mel frame. The third network generates a mel-spectrogram from the\nexpanded text conditioned on predicted pitch. All networks are based on 1D\ndepth-wise separable convolutional architecture. The explicit duration\nprediction eliminates word skipping and repeating. The quality of the generated\nspeech nearly matches the best auto-regressive models - TalkNet trained on the\nLJSpeech dataset got MOS 4.08. The model has only 13.2M parameters, almost 2x\nless than the present state-of-the-art text-to-speech models. The\nnon-autoregressive architecture allows for fast training and inference. The\nsmall model size and fast inference make the TalkNet an attractive candidate\nfor embedded speech synthesis.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 15:58:46 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 17:56:23 GMT"}, {"version": "v3", "created": "Thu, 17 Jun 2021 19:28:31 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Beliaev", "Stanislav", ""], ["Ginsburg", "Boris", ""]]}, {"id": "2104.08203", "submitter": "Tesfamariam M Abuhay", "authors": "Tesfamariam M. Abuhay, Adane Mamuye, Stewart Robinson, Sergey V.\n  Kovalchuk", "title": "Why Machine Learning Integrated Patient Flow Simulation?", "comments": "Proceedings of the Operational Research Society Simulation Workshop\n  2021 (SW21)", "journal-ref": "Proceedings of the Operational Research Society Simulation\n  Workshop 2021 (SW21)", "doi": "10.36819/SW21.041", "report-no": null, "categories": "cs.LG cs.AI cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Patient flow analysis can be studied from a clinical and or operational\nperspective using simulation. Traditional statistical methods such as\nstochastic distribution methods have been used to construct patient flow\nsimulation submodels such as patient inflow, Length of Stay (LoS), Cost of\nTreatment (CoT) and Clinical Pathway (CP) models. However, patient inflow\ndemonstrates seasonality, trend and variation over time. LoS, CoT and CP are\nsignificantly determined by attributes of patients and clinical and laboratory\ntest results. For this reason, patient flow simulation models constructed using\ntraditional statistical methods are criticized for ignoring heterogeneity and\ntheir contribution to personalized and value based healthcare. On the other\nhand, machine learning methods have proven to be efficient to study and predict\nadmission rate, LoS, CoT, and CP. This paper, hence, describes why coupling\nmachine learning with patient flow simulation is important and proposes a\nconceptual architecture that shows how to integrate machine learning with\npatient flow simulation.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 16:23:17 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Abuhay", "Tesfamariam M.", ""], ["Mamuye", "Adane", ""], ["Robinson", "Stewart", ""], ["Kovalchuk", "Sergey V.", ""]]}, {"id": "2104.08219", "submitter": "George Chrysostomou", "authors": "George Chrysostomou and Nikolaos Aletras", "title": "Variable Instance-Level Explainability for Text Classification", "comments": "NLP Interpretability", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the high accuracy of pretrained transformer networks in text\nclassification, a persisting issue is their significant complexity that makes\nthem hard to interpret. Recent research has focused on developing feature\nscoring methods for identifying which parts of the input are most important for\nthe model to make a particular prediction and use it as an explanation (i.e.\nrationale). A limitation of these approaches is that they assume that a\nparticular feature scoring method should be used across all instances in a\ndataset using a predefined fixed length, which might not be optimal across all\ninstances. To address this, we propose a method for extracting variable-length\nexplanations using a set of different feature scoring methods at\ninstance-level. Our method is inspired by word erasure approaches which assume\nthat the most faithful rationale for a prediction should be the one with the\nhighest divergence between the model's output distribution using the full text\nand the text after removing the rationale for a particular instance. Evaluation\non four standard text classification datasets shows that our method\nconsistently provides more faithful explanations compared to previous\nfixed-length and fixed-feature scoring methods for rationale extraction.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 16:53:48 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Chrysostomou", "George", ""], ["Aletras", "Nikolaos", ""]]}, {"id": "2104.08244", "submitter": "Ikram Chairi", "authors": "Abdelkrim Alahyane and Mohamed El Fakir and Saad Benjelloun and Ikram\n  Chairi", "title": "Open data for Moroccan license plates for OCR applications : data\n  collection, labeling, and model construction", "comments": null, "journal-ref": null, "doi": null, "report-no": "MSDA reports 01", "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Significant number of researches have been developed recently around\nintelligent system for traffic management, especially, OCR based license plate\nrecognition, as it is considered as a main step for any automatic traffic\nmanagement system. Good quality data sets are increasingly needed and produced\nby the research community to improve the performance of those algorithms.\nFurthermore, a special need of data is noted for countries having special\ncharacters on their licence plates, like Morocco, where Arabic Alphabet is\nused. In this work, we present a labeled open data set of circulation plates\ntaken in Morocco, for different type of vehicles, namely cars, trucks and\nmotorcycles. This data was collected manually and consists of 705 unique and\ndifferent images. Furthermore this data was labeled for plate segmentation and\nfor matriculation number OCR. Also, As we show in this paper, the data can be\nenriched using data augmentation techniques to create training sets with few\nthousands of images for different machine leaning and AI applications. We\npresent and compare a set of models built on this data. Also, we publish this\ndata as an open access data to encourage innovation and applications in the\nfield of OCR and image processing for traffic control and other applications\nfor transportation and heterogeneous vehicle management.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 17:26:46 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Alahyane", "Abdelkrim", ""], ["Fakir", "Mohamed El", ""], ["Benjelloun", "Saad", ""], ["Chairi", "Ikram", ""]]}, {"id": "2104.08273", "submitter": "Lichao Sun", "authors": "Yu Wang, Lichao Sun", "title": "Membership Inference Attacks on Knowledge Graphs", "comments": "Under submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graphs have become increasingly popular supplemental information\nbecause they represented structural relations between entities. Knowledge graph\nembedding methods (KGE) are used for various downstream tasks, e.g., knowledge\ngraph completion, including triple classification, link prediction. However,\nthe knowledge graph also includes much sensitive information in the training\nset, which is very vulnerable to privacy attacks. In this paper, we conduct\nsuch one attack, i.e., membership inference attack, on four standard KGE\nmethods to explore the privacy vulnerabilities of knowledge graphs. Our\nexperimental results on four benchmark knowledge graph datasets show that our\nprivacy attacks can reveal the membership information leakage of KGE methods.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 17:56:48 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Wang", "Yu", ""], ["Sun", "Lichao", ""]]}, {"id": "2104.08274", "submitter": "Matthias Hofer", "authors": "Matthias Hofer, Tuan Anh Le, Roger Levy, Josh Tenenbaum", "title": "Learning Evolved Combinatorial Symbols with a Neuro-symbolic Generative\n  Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans have the ability to rapidly understand rich combinatorial concepts\nfrom limited data. Here we investigate this ability in the context of auditory\nsignals, which have been evolved in a cultural transmission experiment to study\nthe emergence of combinatorial structure in language. We propose a\nneuro-symbolic generative model which combines the strengths of previous\napproaches to concept learning. Our model performs fast inference drawing on\nneural network methods, while still retaining the interpretability and\ngeneralization from limited data seen in structured generative approaches. This\nmodel outperforms a purely neural network-based approach on classification as\nevaluated against both ground truth and human experimental classification\npreferences, and produces superior reproductions of observed signals as well.\nOur results demonstrate the power of flexible combined neural-symbolic\narchitectures for human-like generalization in raw perceptual domains and\noffers a step towards developing precise computational models of inductive\nbiases in language evolution.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 17:57:51 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Hofer", "Matthias", ""], ["Le", "Tuan Anh", ""], ["Levy", "Roger", ""], ["Tenenbaum", "Josh", ""]]}, {"id": "2104.08301", "submitter": "Rifat Shahriyar", "authors": "Masum Hasan, Kazi Sajeed Mehrab, Wasi Uddin Ahmad, Rifat Shahriyar", "title": "Text2App: A Framework for Creating Android Apps from Text Descriptions", "comments": "Submitted to EMNLP 2021 System Demonstrations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Text2App -- a framework that allows users to create functional\nAndroid applications from natural language specifications. The conventional\nmethod of source code generation tries to generate source code directly, which\nis impractical for creating complex software. We overcome this limitation by\ntransforming natural language into an abstract intermediate formal language\nrepresenting an application with a substantially smaller number of tokens. The\nintermediate formal representation is then compiled into target source codes.\nThis abstraction of programming details allows seq2seq networks to learn\ncomplex application structures with less overhead. In order to train sequence\nmodels, we introduce a data synthesis method grounded in a human survey. We\ndemonstrate that Text2App generalizes well to unseen combination of app\ncomponents and it is capable of handling noisy natural language instructions.\nWe explore the possibility of creating applications from highly abstract\ninstructions by coupling our system with GPT-3 -- a large pretrained language\nmodel. We perform an extensive human evaluation and identify the capabilities\nand limitations of our system. The source code, a ready-to-run demo notebook,\nand a demo video are publicly available at\n\\url{https://github.com/text2app/Text2App}.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 18:13:10 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 16:37:15 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Hasan", "Masum", ""], ["Mehrab", "Kazi Sajeed", ""], ["Ahmad", "Wasi Uddin", ""], ["Shahriyar", "Rifat", ""]]}, {"id": "2104.08303", "submitter": "Mustafa Canim", "authors": "Michael Glass, Mustafa Canim, Alfio Gliozzo, Saneem Chemmengath,\n  Vishwajeet Kumar, Rishav Chakravarti, Avi Sil, Feifei Pan, Samarth Bharadwaj,\n  Nicolas Rodolfo Fauceglia", "title": "Capturing Row and Column Semantics in Transformer Based Question\n  Answering over Tables", "comments": "To appear at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Transformer based architectures are recently used for the task of answering\nquestions over tables. In order to improve the accuracy on this task,\nspecialized pre-training techniques have been developed and applied on millions\nof open-domain web tables. In this paper, we propose two novel approaches\ndemonstrating that one can achieve superior performance on table QA task\nwithout even using any of these specialized pre-training techniques. The first\nmodel, called RCI interaction, leverages a transformer based architecture that\nindependently classifies rows and columns to identify relevant cells. While\nthis model yields extremely high accuracy at finding cell values on recent\nbenchmarks, a second model we propose, called RCI representation, provides a\nsignificant efficiency advantage for online QA systems over tables by\nmaterializing embeddings for existing tables. Experiments on recent benchmarks\nprove that the proposed methods can effectively locate cell values on tables\n(up to ~98% Hit@1 accuracy on WikiSQL lookup questions). Also, the interaction\nmodel outperforms the state-of-the-art transformer based approaches,\npre-trained on very large table corpora (TAPAS and TaBERT), achieving ~3.4% and\n~18.86% additional precision improvement on the standard WikiSQL benchmark.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 18:22:30 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 21:52:55 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Glass", "Michael", ""], ["Canim", "Mustafa", ""], ["Gliozzo", "Alfio", ""], ["Chemmengath", "Saneem", ""], ["Kumar", "Vishwajeet", ""], ["Chakravarti", "Rishav", ""], ["Sil", "Avi", ""], ["Pan", "Feifei", ""], ["Bharadwaj", "Samarth", ""], ["Fauceglia", "Nicolas Rodolfo", ""]]}, {"id": "2104.08313", "submitter": "Benjamin Devillers", "authors": "Benjamin Devillers, Bhavin Choksi, Romain Bielawski and Rufin\n  VanRullen", "title": "Does language help generalization in vision models?", "comments": "Paper accepted for presentation at the ViGIL 2021 workshop @NAACL.\n  This version: added models to the comparison (ICMLM, TSM); added tests of\n  adversarial robustness; mistake identified and corrected in the normalization\n  of image features; results and conclusions updated accordingly", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision models trained on multimodal datasets can benefit from the wide\navailability of large image-caption datasets. A recent model (CLIP) was found\nto generalize well in zero-shot and transfer learning settings. This could\nimply that linguistic or \"semantic grounding\" confers additional generalization\nabilities to the visual feature space. Here, we systematically evaluate various\nmultimodal architectures and vision-only models in terms of unsupervised\nclustering, few-shot learning, transfer learning and adversarial robustness. In\neach setting, multimodal training produced no additional generalization\ncapability compared to standard supervised visual training. We conclude that\nwork is still required for semantic grounding to help improve vision models.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 18:54:14 GMT"}, {"version": "v2", "created": "Sat, 15 May 2021 17:23:52 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Devillers", "Benjamin", ""], ["Choksi", "Bhavin", ""], ["Bielawski", "Romain", ""], ["VanRullen", "Rufin", ""]]}, {"id": "2104.08336", "submitter": "Siyi Tang", "authors": "Siyi Tang, Jared A. Dunnmon, Khaled Saab, Xuan Zhang, Qianying Huang,\n  Florian Dubost, Daniel L. Rubin, Christopher Lee-Messer", "title": "Automated Seizure Detection and Seizure Type Classification From\n  Electroencephalography With a Graph Neural Network and Self-Supervised\n  Pre-Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated seizure detection and classification from electroencephalography\n(EEG) can greatly improve the diagnosis and treatment of seizures. While prior\nstudies mainly used convolutional neural networks (CNNs) that assume image-like\nstructure in EEG signals or spectrograms, this modeling choice does not reflect\nthe natural geometry of or connectivity between EEG electrodes. In this study,\nwe propose modeling EEGs as graphs and present a graph neural network for\nautomated seizure detection and classification. In addition, we leverage\nunlabeled EEG data using a self-supervised pre-training strategy. Our graph\nmodel with self-supervised pre-training significantly outperforms previous\nstate-of-the-art CNN and Long Short-Term Memory (LSTM) models by 6.3 points\n(7.8%) in Area Under the Receiver Operating Characteristic curve (AUROC) for\nseizure detection and 6.3 points (9.2%) in weighted F1-score for seizure type\nclassification. Ablation studies show that our graph-based modeling approach\nsignificantly outperforms existing CNN or LSTM models, and that\nself-supervision helps further improve the model performance. Moreover, we find\nthat self-supervised pre-training substantially improves model performance on\ncombined tonic seizures, a low-prevalence seizure type. Furthermore, our model\ninterpretability analysis suggests that our model is better at identifying\nseizure regions compared to an existing CNN. In summary, our graph-based\nmodeling approach integrates domain knowledge about EEG, sets a new\nstate-of-the-art for seizure detection and classification on a large public\ndataset (5,499 EEG files), and provides better ability to identify seizure\nregions.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 20:32:10 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Tang", "Siyi", ""], ["Dunnmon", "Jared A.", ""], ["Saab", "Khaled", ""], ["Zhang", "Xuan", ""], ["Huang", "Qianying", ""], ["Dubost", "Florian", ""], ["Rubin", "Daniel L.", ""], ["Lee-Messer", "Christopher", ""]]}, {"id": "2104.08337", "submitter": "Chunhua Ye", "authors": "Chunhua Ye, Zhong Yin, Chenxi Wu, Xiayidai Abulaiti, Yixing Zhang,\n  Zhenqi Sun, and Jianhua Zhang", "title": "Identification of mental fatigue in language comprehension tasks based\n  on EEG and deep learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mental fatigue increases the risk of operator error in language comprehension\ntasks. In order to prevent operator performance degradation, we used EEG\nsignals to assess the mental fatigue of operators in human-computer systems.\nThis study presents an experimental design for fatigue detection in language\ncomprehension tasks. We obtained EEG signals from a 14-channel wireless EEG\ndetector in 15 healthy participants. Each participant was given a cognitive\ntest of a language comprehension task, in the form of multiple choice\nquestions, in which pronoun references were selected between nominal and\nsurrogate sentences. In this paper, the 2400 EEG fragments collected are\ndivided into three data sets according to different utilization rates, namely\n1200s data set with 50% utilization rate, 1500s data set with 62.5% utilization\nrate, and 1800s data set with 75% utilization rate. In the aspect of feature\nextraction, different EEG features were extracted, including time domain\nfeatures, frequency domain features and entropy features, and the effects of\ndifferent features and feature combinations on classification accuracy were\nexplored. In terms of classification, we introduced the Convolutional Neural\nNetwork (CNN) method as the preferred method, It was compared with Least\nSquares Support Vector Machines(LSSVM),Support Vector Machines(SVM),Logistic\nRegression (LR), Random Forest(RF), Naive Bayes (NB), K-Nearest Neighbor (KNN)\nand Decision Tree(DT).According to the results, the classification accuracy of\nconvolutional neural network (CNN) is higher than that of other classification\nmethods. The classification results show that the classification accuracy of\n1200S dataset is higher than the other two datasets. The combination of\nFrequency and entropy feature and CNN has the highest classification accuracy,\nwhich is 85.34%.\n", "versions": [{"version": "v1", "created": "Wed, 14 Apr 2021 14:00:57 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Ye", "Chunhua", ""], ["Yin", "Zhong", ""], ["Wu", "Chenxi", ""], ["Abulaiti", "Xiayidai", ""], ["Zhang", "Yixing", ""], ["Sun", "Zhenqi", ""], ["Zhang", "Jianhua", ""]]}, {"id": "2104.08368", "submitter": "Ameni Trabelsi", "authors": "Ameni Trabelsi, Ross J. Beveridge and Nathaniel Blanchard", "title": "Drowned out by the noise: Evidence for Tracking-free Motion Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous driving consists of a multitude of interacting modules, where each\nmodule must contend with errors from the others. Typically, the motion\nprediction module depends on a robust tracking system to capture each agent's\npast movement. In this work, we systematically explore the importance of the\ntracking module for the motion prediction task and ultimately conclude that the\ntracking module is detrimental to overall motion prediction performance when\nthe module is imperfect (with as low as 1% error). We explicitly compare models\nthat use tracking information to models that do not across multiple scenarios\nand conditions. We find that the tracking information only improves performance\nin noise-free conditions. A noise-free tracker is unlikely to remain noise-free\nin real-world scenarios, and the inevitable noise will subsequently negatively\naffect performance. We thus argue future work should be mindful of noise when\ndeveloping and testing motion/tracking modules, or that they should do away\nwith the tracking component entirely.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 21:03:55 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Trabelsi", "Ameni", ""], ["Beveridge", "Ross J.", ""], ["Blanchard", "Nathaniel", ""]]}, {"id": "2104.08378", "submitter": "Jeff Pool", "authors": "Asit Mishra, Jorge Albericio Latorre, Jeff Pool, Darko Stosic, Dusan\n  Stosic, Ganesh Venkatesh, Chong Yu, Paulius Micikevicius", "title": "Accelerating Sparse Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As neural network model sizes have dramatically increased, so has the\ninterest in various techniques to reduce their parameter counts and accelerate\ntheir execution. An active area of research in this field is sparsity -\nencouraging zero values in parameters that can then be discarded from storage\nor computations. While most research focuses on high levels of sparsity, there\nare challenges in universally maintaining model accuracy as well as achieving\nsignificant speedups over modern matrix-math hardware. To make sparsity\nadoption practical, the NVIDIA Ampere GPU architecture introduces sparsity\nsupport in its matrix-math units, Tensor Cores. We present the design and\nbehavior of Sparse Tensor Cores, which exploit a 2:4 (50%) sparsity pattern\nthat leads to twice the math throughput of dense matrix units. We also describe\na simple workflow for training networks that both satisfy 2:4 sparsity pattern\nrequirements and maintain accuracy, verifying it on a wide range of common\ntasks and model architectures. This workflow makes it easy to prepare accurate\nmodels for efficient deployment on Sparse Tensor Cores.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 21:27:32 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Mishra", "Asit", ""], ["Latorre", "Jorge Albericio", ""], ["Pool", "Jeff", ""], ["Stosic", "Darko", ""], ["Stosic", "Dusan", ""], ["Venkatesh", "Ganesh", ""], ["Yu", "Chong", ""], ["Micikevicius", "Paulius", ""]]}, {"id": "2104.08382", "submitter": "Arjun Nitin Bhagoji", "authors": "Arjun Nitin Bhagoji, Daniel Cullina, Vikash Sehwag, Prateek Mittal", "title": "Lower Bounds on Cross-Entropy Loss in the Presence of Test-time\n  Adversaries", "comments": "16 pages, 12 figures; Accepted to ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding the fundamental limits of robust supervised learning has\nemerged as a problem of immense interest, from both practical and theoretical\nstandpoints. In particular, it is critical to determine classifier-agnostic\nbounds on the training loss to establish when learning is possible. In this\npaper, we determine optimal lower bounds on the cross-entropy loss in the\npresence of test-time adversaries, along with the corresponding optimal\nclassification outputs. Our formulation of the bound as a solution to an\noptimization problem is general enough to encompass any loss function depending\non soft classifier outputs. We also propose and provide a proof of correctness\nfor a bespoke algorithm to compute this lower bound efficiently, allowing us to\ndetermine lower bounds for multiple practical datasets of interest. We use our\nlower bounds as a diagnostic tool to determine the effectiveness of current\nrobust training methods and find a gap from optimality at larger budgets.\nFinally, we investigate the possibility of using of optimal classification\noutputs as soft labels to empirically improve robust training.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 21:41:28 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 20:47:26 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Bhagoji", "Arjun Nitin", ""], ["Cullina", "Daniel", ""], ["Sehwag", "Vikash", ""], ["Mittal", "Prateek", ""]]}, {"id": "2104.08401", "submitter": "Peter Clark", "authors": "Nora Kassner, Oyvind Tafjord, Hinrich Schutze, Peter Clark", "title": "Enriching a Model's Notion of Belief using a Persistent Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Although pretrained language models (PTLMs) have been shown to contain\nsignificant amounts of world knowledge, they can still produce inconsistent\nanswers to questions when probed, even after using specialized training\ntechniques to reduce inconsistency. As a result, it can be hard to identify\nwhat the model actually \"believes\" about the world. Our goal is to reduce this\nproblem, so systems are more globally consistent and accurate in their answers.\nOur approach is to add a memory component - a BeliefBank - that records a\nmodel's answers, and two mechanisms that use it to improve consistency among\nbeliefs. First, a reasoning component - a weighted SAT solver - improves\nconsistency by flipping answers that significantly clash with others. Second, a\nfeedback component re-queries the model but using known beliefs as context. We\nshow that, in a controlled experimental setting, these two mechanisms improve\nboth accuracy and consistency. This is significant as it is a first step\ntowards endowing models with an evolving memory, allowing them to construct a\nmore coherent picture of the world.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 23:09:11 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Kassner", "Nora", ""], ["Tafjord", "Oyvind", ""], ["Schutze", "Hinrich", ""], ["Clark", "Peter", ""]]}, {"id": "2104.08419", "submitter": "Jiapeng Wu", "authors": "Jiapeng Wu, Yishi Xu, Yingxue Zhang, Chen Ma, Mark Coates and Jackie\n  Chi Kit Cheung", "title": "TIE: A Framework for Embedding-based Incremental Temporal Knowledge\n  Graph Completion", "comments": "SIGIR 2021 long paper. 13 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reasoning in a temporal knowledge graph (TKG) is a critical task for\ninformation retrieval and semantic search. It is particularly challenging when\nthe TKG is updated frequently. The model has to adapt to changes in the TKG for\nefficient training and inference while preserving its performance on historical\nknowledge. Recent work approaches TKG completion (TKGC) by augmenting the\nencoder-decoder framework with a time-aware encoding function. However, naively\nfine-tuning the model at every time step using these methods does not address\nthe problems of 1) catastrophic forgetting, 2) the model's inability to\nidentify the change of facts (e.g., the change of the political affiliation and\nend of a marriage), and 3) the lack of training efficiency. To address these\nchallenges, we present the Time-aware Incremental Embedding (TIE) framework,\nwhich combines TKG representation learning, experience replay, and temporal\nregularization. We introduce a set of metrics that characterizes the\nintransigence of the model and propose a constraint that associates the deleted\nfacts with negative labels. Experimental results on Wikidata12k and YAGO11k\ndatasets demonstrate that the proposed TIE framework reduces training time by\nabout ten times and improves on the proposed metrics compared to vanilla\nfull-batch training. It comes without a significant loss in performance for any\ntraditional measures. Extensive ablation studies reveal performance trade-offs\namong different evaluation metrics, which is essential for decision-making\naround real-world TKG applications.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 01:40:46 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 00:32:29 GMT"}, {"version": "v3", "created": "Sun, 9 May 2021 03:00:52 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Wu", "Jiapeng", ""], ["Xu", "Yishi", ""], ["Zhang", "Yingxue", ""], ["Ma", "Chen", ""], ["Coates", "Mark", ""], ["Cheung", "Jackie Chi Kit", ""]]}, {"id": "2104.08436", "submitter": "Majid Mobini", "authors": "Majid Mobini, Georges Kaddoum (Senior Member, IEEE)", "title": "Deep Chaos Synchronization", "comments": null, "journal-ref": "IEEE Open Journal of the Communications Society, 2020, vol. 1, pp:\n  1571-1582", "doi": null, "report-no": null, "categories": "eess.SP cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this study, we address the problem of chaotic synchronization over a noisy\nchannel by introducing a novel Deep Chaos Synchronization (DCS) system using a\nConvolutional Neural Network (CNN). Conventional Deep Learning (DL) based\ncommunication strategies are extremely powerful but training on large data sets\nis usually a difficult and time-consuming procedure. To tackle this challenge,\nDCS does not require prior information or large data sets. In addition, we\nprovide a novel Recurrent Neural Network (RNN)-based chaotic synchronization\nsystem for comparative analysis. The results show that the proposed DCS\narchitecture is competitive with RNN-based synchronization in terms of\nrobustness against noise, convergence, and training. Hence, with these\nfeatures, the DCS scheme will open the door for a new class of modulator\nschemes and meet the robustness against noise, convergence, and training\nrequirements of the Ultra Reliable Low Latency Communications (URLLC) and\nIndustrial Internet of Things (IIoT).\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 03:57:53 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Mobini", "Majid", "", "Senior Member, IEEE"], ["Kaddoum", "Georges", "", "Senior Member, IEEE"]]}, {"id": "2104.08440", "submitter": "Ercument Ilhan", "authors": "Ercument Ilhan, Jeremy Gow and Diego Perez-Liebana", "title": "Learning on a Budget via Teacher Imitation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep Reinforcement Learning (RL) techniques can benefit greatly from\nleveraging prior experience, which can be either self-generated or acquired\nfrom other entities. Action advising is a framework that provides a flexible\nway to transfer such knowledge in the form of actions between teacher-student\npeers. However, due to the realistic concerns, the number of these interactions\nis limited with a budget; therefore, it is crucial to perform these in the most\nappropriate moments. There have been several promising studies recently that\naddress this problem setting especially from the student's perspective. Despite\ntheir success, they have some shortcomings when it comes to the practical\napplicability and integrity as an overall solution to the learning from advice\nchallenge. In this paper, we extend the idea of advice reusing via teacher\nimitation to construct a unified approach that addresses both advice collection\nand advice utilisation problems. We also propose a method to automatically tune\nthe relevant hyperparameters of these components on-the-fly to make it able to\nadapt to any task with minimal human intervention. The experiments we performed\nin 5 different Atari games verify that our algorithm either surpasses or\nperforms on-par with its top competitors while being far simpler to be\nemployed. Furthermore, its individual components are also found to be providing\nsignificant advantages alone.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 04:15:00 GMT"}, {"version": "v2", "created": "Fri, 18 Jun 2021 22:15:43 GMT"}, {"version": "v3", "created": "Wed, 30 Jun 2021 04:31:58 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Ilhan", "Ercument", ""], ["Gow", "Jeremy", ""], ["Perez-Liebana", "Diego", ""]]}, {"id": "2104.08441", "submitter": "Ercument Ilhan", "authors": "Ercument Ilhan, Jeremy Gow and Diego Perez-Liebana", "title": "Action Advising with Advice Imitation in Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Action advising is a peer-to-peer knowledge exchange technique built on the\nteacher-student paradigm to alleviate the sample inefficiency problem in deep\nreinforcement learning. Recently proposed student-initiated approaches have\nobtained promising results. However, due to being in the early stages of\ndevelopment, these also have some substantial shortcomings. One of the\nabilities that are absent in the current methods is further utilising advice by\nreusing, which is especially crucial in the practical settings considering the\nbudget and cost constraints in peer-to-peer. In this study, we present an\napproach to enable the student agent to imitate previously acquired advice to\nreuse them directly in its exploration policy, without any interventions in the\nlearning mechanism itself. In particular, we employ a behavioural cloning\nmodule to imitate the teacher policy and use dropout regularisation to have a\nnotion of epistemic uncertainty to keep track of which state-advice pairs are\nactually collected. As the results of experiments we conducted in three Atari\ngames show, advice reusing via generalisation is indeed a feasible option in\ndeep RL and our approach can successfully achieve this while significantly\nimproving the learning performance, even when paired with a simple early\nadvising heuristic.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 04:24:04 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Ilhan", "Ercument", ""], ["Gow", "Jeremy", ""], ["Perez-Liebana", "Diego", ""]]}, {"id": "2104.08445", "submitter": "Sewon Min", "authors": "Sewon Min, Kenton Lee, Ming-Wei Chang, Kristina Toutanova, Hannaneh\n  Hajishirzi", "title": "Joint Passage Ranking for Diverse Multi-Answer Retrieval", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We study multi-answer retrieval, an under-explored problem that requires\nretrieving passages to cover multiple distinct answers for a given question.\nThis task requires joint modeling of retrieved passages, as models should not\nrepeatedly retrieve passages containing the same answer at the cost of missing\na different valid answer. Prior work focusing on single-answer retrieval is\nlimited as it cannot reason about the set of passages jointly. In this paper,\nwe introduce JPR, a joint passage retrieval model focusing on reranking. To\nmodel the joint probability of the retrieved passages, JPR makes use of an\nautoregressive reranker that selects a sequence of passages, equipped with\nnovel training and decoding algorithms. Compared to prior approaches, JPR\nachieves significantly better answer coverage on three multi-answer datasets.\nWhen combined with downstream question answering, the improved retrieval\nenables larger answer generation models since they need to consider fewer\npassages, establishing a new state-of-the-art.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 04:48:36 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Min", "Sewon", ""], ["Lee", "Kenton", ""], ["Chang", "Ming-Wei", ""], ["Toutanova", "Kristina", ""], ["Hajishirzi", "Hannaneh", ""]]}, {"id": "2104.08450", "submitter": "Xiyun Li", "authors": "Xiyun Li and Yong Xu and Meng Yu and Shi-Xiong Zhang and Jiaming Xu\n  and Bo Xu and Dong Yu", "title": "MIMO Self-attentive RNN Beamformer for Multi-speaker Speech Separation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, our proposed recurrent neural network (RNN) based all deep learning\nminimum variance distortionless response (ADL-MVDR) beamformer method yielded\nsuperior performance over the conventional MVDR by replacing the matrix\ninversion and eigenvalue decomposition with two recurrent neural networks. In\nthis work, we present a self-attentive RNN beamformer to further improve our\nprevious RNN-based beamformer by leveraging on the powerful modeling capability\nof self-attention. Temporal-spatial self-attention module is proposed to better\nlearn the beamforming weights from the speech and noise spatial covariance\nmatrices. The temporal self-attention module could help RNN to learn global\nstatistics of covariance matrices. The spatial self-attention module is\ndesigned to attend on the cross-channel correlation in the covariance matrices.\nFurthermore, a multi-channel input with multi-speaker directional features and\nmulti-speaker speech separation outputs (MIMO) model is developed to improve\nthe inference efficiency. The evaluations demonstrate that our proposed MIMO\nself-attentive RNN beamformer improves both the automatic speech recognition\n(ASR) accuracy and the perceptual estimation of speech quality (PESQ) against\nprior arts.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 05:02:04 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 08:18:36 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Li", "Xiyun", ""], ["Xu", "Yong", ""], ["Yu", "Meng", ""], ["Zhang", "Shi-Xiong", ""], ["Xu", "Jiaming", ""], ["Xu", "Bo", ""], ["Yu", "Dong", ""]]}, {"id": "2104.08451", "submitter": "Zuohui Fu", "authors": "Zhe Hu, Zuohui Fu, Yu Yin, Gerard de Melo and Cheng Peng", "title": "Context-Aware Interaction Network for Question Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Impressive milestones have been achieved in text matching by adopting a\ncross-attention mechanism to capture pertinent semantic connections between two\nsentences. However, these cross-attention mechanisms focus on word-level links\nbetween the two inputs, neglecting the importance of contextual information. We\npropose a context-aware interaction network (COIN) to properly align two\nsequences and infer their semantic relationship. Specifically, each interaction\nblock includes (1) a context-aware cross-attention mechanism to effectively\nintegrate contextual information, and (2) a gate fusion layer to flexibly\ninterpolate aligned representations. We apply multiple stacked interaction\nblocks to produce alignments at different levels and gradually refine the\nattention results. Experiments on two question matching datasets and detailed\nanalyses confirm the effectiveness of our model.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 05:03:56 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Hu", "Zhe", ""], ["Fu", "Zuohui", ""], ["Yin", "Yu", ""], ["de Melo", "Gerard", ""], ["Peng", "Cheng", ""]]}, {"id": "2104.08492", "submitter": "Eltayeb Ahmed", "authors": "Eltayeb Ahmed, Luisa Zintgraf, Christian A. Schroeder de Witt and\n  Nicolas Usunier", "title": "A Self-Supervised Auxiliary Loss for Deep RL in Partially Observable\n  Settings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work we explore an auxiliary loss useful for reinforcement learning\nin environments where strong performing agents are required to be able to\nnavigate a spatial environment. The auxiliary loss proposed is to minimize the\nclassification error of a neural network classifier that predicts whether or\nnot a pair of states sampled from the agents current episode trajectory are in\norder. The classifier takes as input a pair of states as well as the agent's\nmemory. The motivation for this auxiliary loss is that there is a strong\ncorrelation with which of a pair of states is more recent in the agents episode\ntrajectory and which of the two states is spatially closer to the agent. Our\nhypothesis is that learning features to answer this question encourages the\nagent to learn and internalize in memory representations of states that\nfacilitate spatial reasoning. We tested this auxiliary loss on a navigation\ntask in a gridworld and achieved 9.6% increase in accumulative episode reward\ncompared to a strong baseline approach.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 09:28:17 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Ahmed", "Eltayeb", ""], ["Zintgraf", "Luisa", ""], ["de Witt", "Christian A. Schroeder", ""], ["Usunier", "Nicolas", ""]]}, {"id": "2104.08521", "submitter": "Minori Toyoda", "authors": "Minori Toyoda, Kanata Suzuki, Hiroki Mori, Yoshihiko Hayashi, Tetsuya\n  Ogata", "title": "Embodying Pre-Trained Word Embeddings Through Robot Actions", "comments": "To appear in IEEE Robotics and Automation Letters (RA-L) and IEEE\n  International Conference on Robotics and Automation (ICRA 2021)", "journal-ref": "IEEE Robotics and Automation Letters, vol. 6, no. 2, pp.\n  4225-4232, 2021", "doi": "10.1109/LRA.2021.3067862", "report-no": null, "categories": "cs.RO cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a promising neural network model with which to acquire a grounded\nrepresentation of robot actions and the linguistic descriptions thereof.\nProperly responding to various linguistic expressions, including polysemous\nwords, is an important ability for robots that interact with people via\nlinguistic dialogue. Previous studies have shown that robots can use words that\nare not included in the action-description paired datasets by using pre-trained\nword embeddings. However, the word embeddings trained under the distributional\nhypothesis are not grounded, as they are derived purely from a text corpus. In\nthis letter, we transform the pre-trained word embeddings to embodied ones by\nusing the robot's sensory-motor experiences. We extend a bidirectional\ntranslation model for actions and descriptions by incorporating non-linear\nlayers that retrofit the word embeddings. By training the retrofit layer and\nthe bidirectional translation model alternately, our proposed model is able to\ntransform the pre-trained word embeddings to adapt to a paired\naction-description dataset. Our results demonstrate that the embeddings of\nsynonyms form a semantic cluster by reflecting the experiences (actions and\nenvironments) of a robot. These embeddings allow the robot to properly generate\nactions from unseen words that are not paired with actions in a dataset.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 12:04:49 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Toyoda", "Minori", ""], ["Suzuki", "Kanata", ""], ["Mori", "Hiroki", ""], ["Hayashi", "Yoshihiko", ""], ["Ogata", "Tetsuya", ""]]}, {"id": "2104.08542", "submitter": "Huifeng Guo", "authors": "Huifeng Guo, Wei Guo, Yong Gao, Ruiming Tang, Xiuqiang He, Wenzhi Liu", "title": "ScaleFreeCTR: MixCache-based Distributed Training System for CTR Models\n  with Huge Embedding Table", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because of the superior feature representation ability of deep learning,\nvarious deep Click-Through Rate (CTR) models are deployed in the commercial\nsystems by industrial companies. To achieve better performance, it is necessary\nto train the deep CTR models on huge volume of training data efficiently, which\nmakes speeding up the training process an essential problem. Different from the\nmodels with dense training data, the training data for CTR models is usually\nhigh-dimensional and sparse. To transform the high-dimensional sparse input\ninto low-dimensional dense real-value vectors, almost all deep CTR models adopt\nthe embedding layer, which easily reaches hundreds of GB or even TB. Since a\nsingle GPU cannot afford to accommodate all the embedding parameters, when\nperforming distributed training, it is not reasonable to conduct the\ndata-parallelism only. Therefore, existing distributed training platforms for\nrecommendation adopt model-parallelism. Specifically, they use CPU (Host)\nmemory of servers to maintain and update the embedding parameters and utilize\nGPU worker to conduct forward and backward computations. Unfortunately, these\nplatforms suffer from two bottlenecks: (1) the latency of pull \\& push\noperations between Host and GPU; (2) parameters update and synchronization in\nthe CPU servers. To address such bottlenecks, in this paper, we propose the\nScaleFreeCTR: a MixCache-based distributed training system for CTR models.\nSpecifically, in SFCTR, we also store huge embedding table in CPU but utilize\nGPU instead of CPU to conduct embedding synchronization efficiently. To reduce\nthe latency of data transfer between both GPU-Host and GPU-GPU, the MixCache\nmechanism and Virtual Sparse Id operation are proposed. Comprehensive\nexperiments and ablation studies are conducted to demonstrate the effectiveness\nand efficiency of SFCTR.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 13:36:19 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 14:11:46 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Guo", "Huifeng", ""], ["Guo", "Wei", ""], ["Gao", "Yong", ""], ["Tang", "Ruiming", ""], ["He", "Xiuqiang", ""], ["Liu", "Wenzhi", ""]]}, {"id": "2104.08543", "submitter": "Katya Kudashkina", "authors": "Katya Kudashkina, Yi Wan, Abhishek Naik, Richard S. Sutton", "title": "Planning with Expectation Models for Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In model-based reinforcement learning (MBRL), Wan et al. (2019) showed\nconditions under which the environment model could produce the expectation of\nthe next feature vector rather than the full distribution, or a sample thereof,\nwith no loss in planning performance. Such expectation models are of interest\nwhen the environment is stochastic and non-stationary, and the model is\napproximate, such as when it is learned using function approximation. In these\ncases a full distribution model may be impractical and a sample model may be\neither more expensive computationally or of high variance. Wan et al.\nconsidered only planning for prediction to evaluate a fixed policy. In this\npaper, we treat the control case - planning to improve and find a good\napproximate policy. We prove that planning with an expectation model must\nupdate a state-value function, not an action-value function as previously\nsuggested (e.g., Sorg & Singh, 2010). This opens the question of how planning\ninfluences action selections. We consider three strategies for this and present\ngeneral MBRL algorithms for each. We identify the strengths and weaknesses of\nthese algorithms in computational experiments. Our algorithms and experiments\nare the first to treat MBRL with expectation models in a general setting.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 13:37:14 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Kudashkina", "Katya", ""], ["Wan", "Yi", ""], ["Naik", "Abhishek", ""], ["Sutton", "Richard S.", ""]]}, {"id": "2104.08551", "submitter": "Yatin Chaudhary", "authors": "Pankaj Gupta, Yatin Chaudhary, Hinrich Sch\\\"utze", "title": "Multi-source Neural Topic Modeling in Multi-view Embedding Spaces", "comments": "NAACL2021, 13 pages, 14 tables, 2 figures. arXiv admin note:\n  substantial text overlap with arXiv:1909.06563", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Though word embeddings and topics are complementary representations, several\npast works have only used pretrained word embeddings in (neural) topic modeling\nto address data sparsity in short-text or small collection of documents. This\nwork presents a novel neural topic modeling framework using multi-view\nembedding spaces: (1) pretrained topic-embeddings, and (2) pretrained\nword-embeddings (context insensitive from Glove and context-sensitive from BERT\nmodels) jointly from one or many sources to improve topic quality and better\ndeal with polysemy. In doing so, we first build respective pools of pretrained\ntopic (i.e., TopicPool) and word embeddings (i.e., WordPool). We then identify\none or more relevant source domain(s) and transfer knowledge to guide\nmeaningful learning in the sparse target domain. Within neural topic modeling,\nwe quantify the quality of topics and document representations via\ngeneralization (perplexity), interpretability (topic coherence) and information\nretrieval (IR) using short-text, long-text, small and large document\ncollections from news and medical domains. Introducing the multi-source\nmulti-view embedding spaces, we have shown state-of-the-art neural topic\nmodeling using 6 source (high-resource) and 5 target (low-resource) corpora.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 14:08:00 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Gupta", "Pankaj", ""], ["Chaudhary", "Yatin", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "2104.08555", "submitter": "Qin Liang", "authors": "Qin Liang, Minjie Zhang, Fenghui Ren, Takayuki Ito", "title": "A Robust Model for Trust Evaluation during Interactions between Agents\n  in a Sociable Environment", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": "SSMCS2019-08", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trust evaluation is an important topic in both research and applications in\nsociable environments. This paper presents a model for trust evaluation between\nagents by the combination of direct trust, indirect trust through neighbouring\nlinks and the reputation of an agent in the environment (i.e. social network)\nto provide the robust evaluation. Our approach is typology independent from\nsocial network structures and in a decentralized manner without a central\ncontroller, so it can be used in broad domains.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 14:38:02 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Liang", "Qin", ""], ["Zhang", "Minjie", ""], ["Ren", "Fenghui", ""], ["Ito", "Takayuki", ""]]}, {"id": "2104.08556", "submitter": "Alberto Garcia-Duran", "authors": "Alberto Garc\\'ia-Dur\\'an, Robert West", "title": "Recursive input and state estimation: A general framework for learning\n  from time series with missing data", "comments": "Published at ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series with missing data are signals encountered in important settings\nfor machine learning. Some of the most successful prior approaches for modeling\nsuch time series are based on recurrent neural networks that transform the\ninput and previous state to account for the missing observations, and then\ntreat the transformed signal in a standard manner.\n  In this paper, we introduce a single unifying framework, Recursive Input and\nState Estimation (RISE), for this general approach and reformulate existing\nmodels as specific instances of this framework. We then explore additional\nnovel variations within the RISE framework to improve the performance of any\ninstance. We exploit representation learning techniques to learn latent\nrepresentations of the signals used by RISE instances. We discuss and develop\nvarious encoding techniques to learn latent signal representations. We\nbenchmark instances of the framework with various encoding functions on three\ndata imputation datasets, observing that RISE instances always benefit from\nencoders that learn representations for numerical values from the digits into\nwhich they can be decomposed.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 14:43:33 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Garc\u00eda-Dur\u00e1n", "Alberto", ""], ["West", "Robert", ""]]}, {"id": "2104.08578", "submitter": "Debanjan Mahata", "authors": "Laiba Mehnaz, Debanjan Mahata, Rakesh Gosangi, Uma Sushmitha Gunturi,\n  Riya Jain, Gauri Gupta, Amardeep Kumar, Isabelle Lee, Anish Acharya, Rajiv\n  Ratn Shah", "title": "GupShup: An Annotated Corpus for Abstractive Summarization of\n  Open-Domain Code-Switched Conversations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Code-switching is the communication phenomenon where speakers switch between\ndifferent languages during a conversation. With the widespread adoption of\nconversational agents and chat platforms, code-switching has become an integral\npart of written conversations in many multi-lingual communities worldwide. This\nmakes it essential to develop techniques for summarizing and understanding\nthese conversations. Towards this objective, we introduce abstractive\nsummarization of Hindi-English code-switched conversations and develop the\nfirst code-switched conversation summarization dataset - GupShup, which\ncontains over 6,831 conversations in Hindi-English and their corresponding\nhuman-annotated summaries in English and Hindi-English. We present a detailed\naccount of the entire data collection and annotation processes. We analyze the\ndataset using various code-switching statistics. We train state-of-the-art\nabstractive summarization models and report their performances using both\nautomated metrics and human evaluation. Our results show that multi-lingual\nmBART and multi-view seq2seq models obtain the best performances on the new\ndataset\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 15:42:01 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Mehnaz", "Laiba", ""], ["Mahata", "Debanjan", ""], ["Gosangi", "Rakesh", ""], ["Gunturi", "Uma Sushmitha", ""], ["Jain", "Riya", ""], ["Gupta", "Gauri", ""], ["Kumar", "Amardeep", ""], ["Lee", "Isabelle", ""], ["Acharya", "Anish", ""], ["Shah", "Rajiv Ratn", ""]]}, {"id": "2104.08610", "submitter": "Michael Glass", "authors": "Michael Glass, Gaetano Rossiello, Alfio Gliozzo", "title": "Zero-shot Slot Filling with DPR and RAG", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to automatically extract Knowledge Graphs (KG) from a given\ncollection of documents is a long-standing problem in Artificial Intelligence.\nOne way to assess this capability is through the task of slot filling. Given an\nentity query in form of [Entity, Slot, ?], a system is asked to `fill' the slot\nby generating or extracting the missing value from a relevant passage or\npassages. This capability is crucial to create systems for automatic knowledge\nbase population, which is becoming in ever-increasing demand, especially in\nenterprise applications. Recently, there has been a promising direction in\nevaluating language models in the same way we would evaluate knowledge bases,\nand the task of slot filling is the most suitable to this intent. The recent\nadvancements in the field try to solve this task in an end-to-end fashion using\nretrieval-based language models. Models like Retrieval Augmented Generation\n(RAG) show surprisingly good performance without involving complex information\nextraction pipelines. However, the results achieved by these models on the two\nslot filling tasks in the KILT benchmark are still not at the level required by\nreal-world information extraction systems. In this paper, we describe several\nstrategies we adopted to improve the retriever and the generator of RAG in\norder to make it a better slot filler. Our KGI0 system (available at\nhttps://github.com/IBM/retrieve-write-slot-filling) reached the top-1 position\non the KILT leaderboard on both T-REx and zsRE dataset with a large margin.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 18:24:51 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Glass", "Michael", ""], ["Rossiello", "Gaetano", ""], ["Gliozzo", "Alfio", ""]]}, {"id": "2104.08614", "submitter": "Michael Bronstein", "authors": "Jacob Andreas, Ga\\v{s}per Begu\\v{s}, Michael M. Bronstein, Roee\n  Diamant, Denley Delaney, Shane Gero, Shafi Goldwasser, David F. Gruber, Sarah\n  de Haas, Peter Malkin, Roger Payne, Giovanni Petri, Daniela Rus, Pratyusha\n  Sharma, Dan Tchernov, Pernille T{\\o}nnesen, Antonio Torralba, Daniel Vogt,\n  Robert J. Wood", "title": "Cetacean Translation Initiative: a roadmap to deciphering the\n  communication of sperm whales", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.CL cs.LG cs.RO eess.AS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The past decade has witnessed a groundbreaking rise of machine learning for\nhuman language analysis, with current methods capable of automatically\naccurately recovering various aspects of syntax and semantics - including\nsentence structure and grounded word meaning - from large data collections.\nRecent research showed the promise of such tools for analyzing acoustic\ncommunication in nonhuman species. We posit that machine learning will be the\ncornerstone of future collection, processing, and analysis of multimodal\nstreams of data in animal communication studies, including bioacoustic,\nbehavioral, biological, and environmental data. Cetaceans are unique non-human\nmodel species as they possess sophisticated acoustic communications, but\nutilize a very different encoding system that evolved in an aquatic rather than\nterrestrial medium. Sperm whales, in particular, with their highly-developed\nneuroanatomical features, cognitive abilities, social structures, and discrete\nclick-based encoding make for an excellent starting point for advanced machine\nlearning tools that can be applied to other animals in the future. This paper\ndetails a roadmap toward this goal based on currently existing technology and\nmultidisciplinary scientific community effort. We outline the key elements\nrequired for the collection and processing of massive bioacoustic data of sperm\nwhales, detecting their basic communication units and language-like\nhigher-level structures, and validating these models through interactive\nplayback experiments. The technological capabilities developed by such an\nundertaking are likely to yield cross-applications and advancements in broader\ncommunities investigating non-human communication and animal behavioral\nresearch.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 18:39:22 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Andreas", "Jacob", ""], ["Begu\u0161", "Ga\u0161per", ""], ["Bronstein", "Michael M.", ""], ["Diamant", "Roee", ""], ["Delaney", "Denley", ""], ["Gero", "Shane", ""], ["Goldwasser", "Shafi", ""], ["Gruber", "David F.", ""], ["de Haas", "Sarah", ""], ["Malkin", "Peter", ""], ["Payne", "Roger", ""], ["Petri", "Giovanni", ""], ["Rus", "Daniela", ""], ["Sharma", "Pratyusha", ""], ["Tchernov", "Dan", ""], ["T\u00f8nnesen", "Pernille", ""], ["Torralba", "Antonio", ""], ["Vogt", "Daniel", ""], ["Wood", "Robert J.", ""]]}, {"id": "2104.08618", "submitter": "Kiavash Satvat", "authors": "Kiavash Satvat, Rigel Gjomemo and V.N. Venkatakrishnan", "title": "EXTRACTOR: Extracting Attack Behavior from Threat Reports", "comments": "6th IEEE European Symposium on Security and Privacy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The knowledge on attacks contained in Cyber Threat Intelligence (CTI) reports\nis very important to effectively identify and quickly respond to cyber threats.\nHowever, this knowledge is often embedded in large amounts of text, and\ntherefore difficult to use effectively. To address this challenge, we propose a\nnovel approach and tool called EXTRACTOR that allows precise automatic\nextraction of concise attack behaviors from CTI reports. EXTRACTOR makes no\nstrong assumptions about the text and is capable of extracting attack behaviors\nas provenance graphs from unstructured text. We evaluate EXTRACTOR using\nreal-world incident reports from various sources as well as reports of DARPA\nadversarial engagements that involve several attack campaigns on various OS\nplatforms of Windows, Linux, and FreeBSD. Our evaluation results show that\nEXTRACTOR can extract concise provenance graphs from CTI reports and show that\nthese graphs can successfully be used by cyber-analytics tools in\nthreat-hunting.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 18:51:00 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Satvat", "Kiavash", ""], ["Gjomemo", "Rigel", ""], ["Venkatakrishnan", "V. N.", ""]]}, {"id": "2104.08620", "submitter": "Joshua Rozner", "authors": "Josh Rozner, Christopher Potts, Kyle Mahowald", "title": "Decrypting Cryptic Crosswords: Semantically Complex Wordplay Puzzles as\n  a Target for NLP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cryptic crosswords, the dominant English-language crossword variety in the\nUnited Kingdom, can be solved by expert humans using flexible, creative\nintelligence and knowledge of language. Cryptic clues read like fluent natural\nlanguage, but they are adversarially composed of two parts: a definition and a\nwordplay cipher requiring sub-word or character-level manipulations. As such,\nthey are a promising target for evaluating and advancing NLP systems that seek\nto process language in more creative, human-like ways. We present a dataset of\ncryptic crossword clues from a major newspaper that can be used as a benchmark\nand train a sequence-to-sequence model to solve them. We also develop related\nbenchmarks that can guide development of approaches to this challenging task.\nWe show that performance can be substantially improved using a novel curriculum\nlearning approach in which the model is pre-trained on related tasks involving,\ne.g, unscrambling words, before it is trained to solve cryptics. However, even\nthis curricular approach does not generalize to novel clue types in the way\nthat humans can, and so cryptic crosswords remain a challenge for NLP systems\nand a potential source of future innovation.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 18:54:00 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 23:20:46 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Rozner", "Josh", ""], ["Potts", "Christopher", ""], ["Mahowald", "Kyle", ""]]}, {"id": "2104.08631", "submitter": "Marina Y Aoyama", "authors": "Marina Y. Aoyama, Matthew Howard", "title": "Training Humans to Train Robots Dynamic Motor Skills", "comments": "6 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning from demonstration (LfD) is commonly considered to be a natural and\nintuitive way to allow novice users to teach motor skills to robots. However,\nit is important to acknowledge that the effectiveness of LfD is heavily\ndependent on the quality of teaching, something that may not be assured with\nnovices. It remains an open question as to the most effective way of guiding\ndemonstrators to produce informative demonstrations beyond ad hoc advice for\nspecific teaching tasks. To this end, this paper investigates the use of\nmachine teaching to derive an index for determining the quality of\ndemonstrations and evaluates its use in guiding and training novices to become\nbetter teachers. Experiments with a simple learner robot suggest that guidance\nand training of teachers through the proposed approach can lead to up to 66.5%\ndecrease in error in the learnt skill.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 19:39:07 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 06:01:30 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Aoyama", "Marina Y.", ""], ["Howard", "Matthew", ""]]}, {"id": "2104.08633", "submitter": "Caroline Pacheco Do Espirito Silva", "authors": "Caroline Pacheco do Esp\\'irito Silva, Jos\\'e A. M. Felippe De Souza,\n  Antoine Vacavant, Thierry Bouwmans, Andrews Cordolino Sobral", "title": "Automated Mathematical Equation Structure Discovery for Visual Analysis", "comments": "25 pages, 8 figures, submitted to JMLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Finding the best mathematical equation to deal with the different challenges\nfound in complex scenarios requires a thorough understanding of the scenario\nand a trial and error process carried out by experts. In recent years, most\nstate-of-the-art equation discovery methods have been widely applied in\nmodeling and identification systems. However, equation discovery approaches can\nbe very useful in computer vision, particularly in the field of feature\nextraction. In this paper, we focus on recent AI advances to present a novel\nframework for automatically discovering equations from scratch with little\nhuman intervention to deal with the different challenges encountered in\nreal-world scenarios. In addition, our proposal can reduce human bias by\nproposing a search space design through generative network instead of\nhand-designed. As a proof of concept, the equations discovered by our framework\nare used to distinguish moving objects from the background in video sequences.\nExperimental results show the potential of the proposed approach and its\neffectiveness in discovering the best equation in video sequences. The code and\ndata are available at:\nhttps://github.com/carolinepacheco/equation-discovery-scene-analysis\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 19:42:06 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Silva", "Caroline Pacheco do Esp\u00edrito", ""], ["De Souza", "Jos\u00e9 A. M. Felippe", ""], ["Vacavant", "Antoine", ""], ["Bouwmans", "Thierry", ""], ["Sobral", "Andrews Cordolino", ""]]}, {"id": "2104.08637", "submitter": "Vassilis N. Ioannidis", "authors": "Konstantinos D. Polyzos, Costas Mavromatis, Vassilis N. Ioannidis, and\n  Georgios B. Giannakis", "title": "Unveiling Anomalous Edges and Nominal Connectivity of Attributed\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncovering anomalies in attributed networks has recently gained popularity\ndue to its importance in unveiling outliers and flagging adversarial behavior\nin a gamut of data and network science applications including {the Internet of\nThings (IoT)}, finance, security, to list a few. The present work deals with\nuncovering anomalous edges in attributed graphs using two distinct formulations\nwith complementary strengths, which can be easily distributed, and hence\nefficient. The first relies on decomposing the graph data matrix into low rank\nplus sparse components to markedly improve performance. The second broadens the\nscope of the first by performing robust recovery of the unperturbed graph,\nwhich enhances the anomaly identification performance. The novel methods not\nonly capture anomalous edges linking nodes of different communities, but also\nspurious connections between any two nodes with different features. Experiments\nconducted on real and synthetic data corroborate the effectiveness of both\nmethods in the anomaly identification task.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 20:00:40 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Polyzos", "Konstantinos D.", ""], ["Mavromatis", "Costas", ""], ["Ioannidis", "Vassilis N.", ""], ["Giannakis", "Georgios B.", ""]]}, {"id": "2104.08641", "submitter": "Diego Perez Liebana Dr.", "authors": "Diego Perez-Liebana, Cristina Guerrero-Romero, Alexander Dockhorn,\n  Linjie Xu, Jorge Hurtado, Dominik Jeurissen", "title": "Generating Diverse and Competitive Play-Styles for Strategy Games", "comments": "8 pages, 2 figures, published in Proc. IEEE CoG 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Designing agents that are able to achieve different play-styles while\nmaintaining a competitive level of play is a difficult task, especially for\ngames for which the research community has not found super-human performance\nyet, like strategy games. These require the AI to deal with large action\nspaces, long-term planning and partial observability, among other well-known\nfactors that make decision-making a hard problem. On top of this, achieving\ndistinct play-styles using a general algorithm without reducing playing\nstrength is not trivial. In this paper, we propose Portfolio Monte Carlo Tree\nSearch with Progressive Unpruning for playing a turn-based strategy game\n(Tribes) and show how it can be parameterized so a quality-diversity algorithm\n(MAP-Elites) is used to achieve different play-styles while keeping a\ncompetitive level of play. Our results show that this algorithm is capable of\nachieving these goals even for an extensive collection of game levels beyond\nthose used for training.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 20:33:24 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 08:59:31 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Perez-Liebana", "Diego", ""], ["Guerrero-Romero", "Cristina", ""], ["Dockhorn", "Alexander", ""], ["Xu", "Linjie", ""], ["Hurtado", "Jorge", ""], ["Jeurissen", "Dominik", ""]]}, {"id": "2104.08653", "submitter": "Baban Gain", "authors": "Baban Gain, Dibyanayan Bandyopadhyay, Tanik Saikh, Asif Ekbal", "title": "IITP@COLIEE 2019: Legal Information Retrieval using BM25 and BERT", "comments": "5 pages. IITP@ COLIEE 2019: legal information retrieval using BM25\n  and BERT. Proceedings of the 6th Competition on Legal Information\n  Extraction/Entailment. COLIEE", "journal-ref": null, "doi": "10.13140/RG.2.2.24136.44804", "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Natural Language Processing (NLP) and Information Retrieval (IR) in the\njudicial domain is an essential task. With the advent of availability\ndomain-specific data in electronic form and aid of different Artificial\nintelligence (AI) technologies, automated language processing becomes more\ncomfortable, and hence it becomes feasible for researchers and developers to\nprovide various automated tools to the legal community to reduce human burden.\nThe Competition on Legal Information Extraction/Entailment (COLIEE-2019) run in\nassociation with the International Conference on Artificial Intelligence and\nLaw (ICAIL)-2019 has come up with few challenging tasks. The shared defined\nfour sub-tasks (i.e. Task1, Task2, Task3 and Task4), which will be able to\nprovide few automated systems to the judicial system. The paper presents our\nworking note on the experiments carried out as a part of our participation in\nall the sub-tasks defined in this shared task. We make use of different\nInformation Retrieval(IR) and deep learning based approaches to tackle these\nproblems. We obtain encouraging results in all these four sub-tasks.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 22:28:15 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 19:07:25 GMT"}, {"version": "v3", "created": "Tue, 22 Jun 2021 08:39:42 GMT"}, {"version": "v4", "created": "Thu, 24 Jun 2021 14:40:18 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Gain", "Baban", ""], ["Bandyopadhyay", "Dibyanayan", ""], ["Saikh", "Tanik", ""], ["Ekbal", "Asif", ""]]}, {"id": "2104.08657", "submitter": "Jiang Yu Zheng Dr.", "authors": "Jiang Yu Zheng", "title": "IUPUI Driving Videos and Images in All Weather and Illumination\n  Conditions", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This document describes an image and video dataset of driving views captured\nin all weather and illumination conditions. The data set has been submitted to\nCDVL.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 22:59:15 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Zheng", "Jiang Yu", ""]]}, {"id": "2104.08661", "submitter": "Bhavana Dalvi Mishra", "authors": "Bhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zhengnan Xie, Hannah\n  Smith, Leighanna Pipatanangkura, Peter Clark", "title": "Explaining Answers with Entailment Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal, in the context of open-domain textual question-answering (QA), is\nto explain answers by not just listing supporting textual evidence\n(\"rationales\"), but also showing how such evidence leads to the answer in a\nsystematic way. If this could be done, new opportunities for understanding and\ndebugging the system's reasoning would become possible. Our approach is to\ngenerate explanations in the form of entailment trees, namely a tree of\nentailment steps from facts that are known, through intermediate conclusions,\nto the final answer. To train a model with this skill, we created\nENTAILMENTBANK, the first dataset to contain multistep entailment trees. At\neach node in the tree (typically) two or more facts compose together to produce\na new conclusion. Given a hypothesis (question + answer), we define three\nincreasingly difficult explanation tasks: generate a valid entailment tree\ngiven (a) all relevant sentences (the leaves of the gold entailment tree), (b)\nall relevant and some irrelevant sentences, or (c) a corpus. We show that a\nstrong language model only partially solves these tasks, and identify several\nnew directions to improve performance. This work is significant as it provides\na new type of dataset (multistep entailments) and baselines, offering a new\navenue for the community to generate richer, more systematic explanations.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 23:13:56 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Dalvi", "Bhavana", ""], ["Jansen", "Peter", ""], ["Tafjord", "Oyvind", ""], ["Xie", "Zhengnan", ""], ["Smith", "Hannah", ""], ["Pipatanangkura", "Leighanna", ""], ["Clark", "Peter", ""]]}, {"id": "2104.08663", "submitter": "Nandan Thakur", "authors": "Nandan Thakur, Nils Reimers, Andreas R\\\"uckl\\'e, Abhishek Srivastava,\n  Iryna Gurevych", "title": "BEIR: A Heterogenous Benchmark for Zero-shot Evaluation of Information\n  Retrieval Models", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Neural IR models have often been studied in homogeneous and narrow settings,\nwhich has considerably limited insights into their generalization capabilities.\nTo address this, and to allow researchers to more broadly establish the\neffectiveness of their models, we introduce BEIR (Benchmarking IR), a\nheterogeneous benchmark for information retrieval. We leverage a careful\nselection of 17 datasets for evaluation spanning diverse retrieval tasks\nincluding open-domain datasets as well as narrow expert domains. We study the\neffectiveness of nine state-of-the-art retrieval models in a zero-shot\nevaluation setup on BEIR, finding that performing well consistently across all\ndatasets is challenging. Our results show BM25 is a robust baseline and\nReranking-based models overall achieve the best zero-shot performances,\nhowever, at high computational costs. In contrast, Dense-retrieval models are\ncomputationally more efficient but often underperform other approaches,\nhighlighting the considerable room for improvement in their generalization\ncapabilities. In this work, we extensively analyze different retrieval models\nand provide several suggestions that we believe may be useful for future work.\nBEIR datasets and code are available at https://github.com/UKPLab/beir.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 23:29:55 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 13:59:17 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Thakur", "Nandan", ""], ["Reimers", "Nils", ""], ["R\u00fcckl\u00e9", "Andreas", ""], ["Srivastava", "Abhishek", ""], ["Gurevych", "Iryna", ""]]}, {"id": "2104.08667", "submitter": "Satwik Kottur", "authors": "Satwik Kottur, Seungwhan Moon, Alborz Geramifard, Babak Damavandi", "title": "SIMMC 2.0: A Task-oriented Dialog Dataset for Immersive Multimodal\n  Conversations", "comments": "10 pages, 7 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a new corpus for the Situated and Interactive Multimodal\nConversations, SIMMC 2.0, aimed at building a successful multimodal assistant\nagent. Specifically, the dataset features 11K task-oriented dialogs (117K\nutterances) between a user and a virtual assistant on the shopping domain\n(fashion and furniture), grounded in situated and photo-realistic VR scenes.\nThe dialogs are collected using a two-phase pipeline, which first generates\nsimulated dialog flows via a novel multimodal dialog simulator we propose,\nfollowed by manual paraphrasing of the generated utterances. In this paper, we\nprovide an in-depth analysis of the collected dataset, and describe in detail\nthe four main benchmark tasks we propose for SIMMC 2.0. The preliminary\nanalysis with a baseline model highlights the new challenges that the SIMMC 2.0\ndataset brings, suggesting new directions for future research. Our dataset and\ncode will be made publicly available.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 00:14:29 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Kottur", "Satwik", ""], ["Moon", "Seungwhan", ""], ["Geramifard", "Alborz", ""], ["Damavandi", "Babak", ""]]}, {"id": "2104.08675", "submitter": "Xingyi Cheng", "authors": "Xingyi Cheng", "title": "Dual-View Distilled BERT for Sentence Embedding", "comments": "Accepted at SIGIR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, BERT realized significant progress for sentence matching via\nword-level cross sentence attention. However, the performance significantly\ndrops when using siamese BERT-networks to derive two sentence embeddings, which\nfall short in capturing the global semantic since the word-level attention\nbetween two sentences is absent. In this paper, we propose a Dual-view\ndistilled BERT~(DvBERT) for sentence matching with sentence embeddings. Our\nmethod deals with a sentence pair from two distinct views, i.e., Siamese View\nand Interaction View. Siamese View is the backbone where we generate sentence\nembeddings. Interaction View integrates the cross sentence interaction as\nmultiple teachers to boost the representation ability of sentence embeddings.\nExperiments on six STS tasks show that our method outperforms the\nstate-of-the-art sentence embedding methods significantly.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 01:20:11 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Cheng", "Xingyi", ""]]}, {"id": "2104.08676", "submitter": "Xiang Zhou", "authors": "Xiang Zhou, Yixin Nie, Mohit Bansal", "title": "Distributed NLI: Learning to Predict Human Opinion Distributions for\n  Language Reasoning", "comments": "13 pages, 2 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce distributed NLI, a new NLU task with a goal to predict the\ndistribution of human judgements for natural language inference. We show that\nmodels can capture human judgement distribution by applying additional\ndistribution estimation methods, namely, Monte Carlo (MC) Dropout, Deep\nEnsemble, Re-Calibration, and Distribution Distillation. All four of these\nmethods substantially outperform the softmax baseline. We show that MC Dropout\nis able to achieve decent performance without any distribution annotations\nwhile Re-Calibration can further give substantial improvements when extra\ndistribution annotations are provided, suggesting the value of multiple\nannotations for the example in modeling the distribution of human judgements.\nMoreover, MC Dropout and Re-Calibration can achieve decent transfer performance\non out-of-domain data. Despite these improvements, the best results are still\nfar below estimated human upper-bound, indicating that the task of predicting\nthe distribution of human judgements is still an open, challenging problem with\nlarge room for future improvements. We showcase the common errors for MC\nDropout and Re-Calibration. Finally, we give guidelines on the usage of these\nmethods with different levels of data availability and encourage future work on\nmodeling the human opinion distribution for language reasoning.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 01:25:19 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Zhou", "Xiang", ""], ["Nie", "Yixin", ""], ["Bansal", "Mohit", ""]]}, {"id": "2104.08682", "submitter": "Dongkuan Xu", "authors": "Dongkuan Xu, Ian E.H. Yen, Jinxi Zhao, Zhibin Xiao", "title": "Rethinking Network Pruning -- under the Pre-train and Fine-tune Paradigm", "comments": "7 pages, 6 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Transformer-based pre-trained language models have significantly improved the\nperformance of various natural language processing (NLP) tasks in the recent\nyears. While effective and prevalent, these models are usually prohibitively\nlarge for resource-limited deployment scenarios. A thread of research has thus\nbeen working on applying network pruning techniques under the\npretrain-then-finetune paradigm widely adopted in NLP. However, the existing\npruning results on benchmark transformers, such as BERT, are not as remarkable\nas the pruning results in the literature of convolutional neural networks\n(CNNs). In particular, common wisdom in pruning CNN states that sparse pruning\ntechnique compresses a model more than that obtained by reducing number of\nchannels and layers (Elsen et al., 2020; Zhu and Gupta, 2017), while existing\nworks on sparse pruning of BERT yields inferior results than its small-dense\ncounterparts such as TinyBERT (Jiao et al., 2020). In this work, we aim to fill\nthis gap by studying how knowledge are transferred and lost during the\npre-train, fine-tune, and pruning process, and proposing a knowledge-aware\nsparse pruning process that achieves significantly superior results than\nexisting literature. We show for the first time that sparse pruning compresses\na BERT model significantly more than reducing its number of channels and\nlayers. Experiments on multiple data sets of GLUE benchmark show that our\nmethod outperforms the leading competitors with a 20-times weight/FLOPs\ncompression and neglectable loss in prediction accuracy.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 02:20:37 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Xu", "Dongkuan", ""], ["Yen", "Ian E. H.", ""], ["Zhao", "Jinxi", ""], ["Xiao", "Zhibin", ""]]}, {"id": "2104.08704", "submitter": "Tianyu Liu", "authors": "Tianyu Liu, Yizhe Zhang, Chris Brockett, Yi Mao, Zhifang Sui, Weizhu\n  Chen and Bill Dolan", "title": "A Token-level Reference-free Hallucination Detection Benchmark for\n  Free-form Text Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Large pretrained generative models like GPT-3 often suffer from hallucinating\nnon-existent or incorrect content, which undermines their potential merits in\nreal applications. Existing work usually attempts to detect these\nhallucinations based on a corresponding oracle reference at a sentence or\ndocument level. However ground-truth references may not be readily available\nfor many free-form text generation applications, and sentence- or\ndocument-level detection may fail to provide the fine-grained signals that\nwould prevent fallacious content in real time. As a first step to addressing\nthese issues, we propose a novel token-level, reference-free hallucination\ndetection task and an associated annotated dataset named HaDes (HAllucination\nDEtection dataSet). To create this dataset, we first perturb a large number of\ntext segments extracted from English language Wikipedia, and then verify these\nwith crowd-sourced annotations. To mitigate label imbalance during annotation,\nwe utilize an iterative model-in-loop strategy. We conduct comprehensive data\nanalyses and create multiple baseline models.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 04:09:48 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Liu", "Tianyu", ""], ["Zhang", "Yizhe", ""], ["Brockett", "Chris", ""], ["Mao", "Yi", ""], ["Sui", "Zhifang", ""], ["Chen", "Weizhu", ""], ["Dolan", "Bill", ""]]}, {"id": "2104.08716", "submitter": "Yu Zhang", "authors": "Huangbin Zhang, Chong Zhao, Yu Zhang, Danlei Wang, Haichao Yang", "title": "Deep Latent Emotion Network for Multi-Task Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Feed recommendation models are widely adopted by numerous feed platforms to\nencourage users to explore the contents they are interested in. However, most\nof the current research simply focus on targeting user's preference and lack\nin-depth study of avoiding objectionable contents to be frequently recommended,\nwhich is a common reason that let user detest. To address this issue, we\npropose a Deep Latent Emotion Network (DLEN) model to extract latent\nprobability of a user preferring a feed by modeling multiple targets with\nsemi-supervised learning. With this method, the conflicts of different targets\nare successfully reduced in the training phase, which improves the training\naccuracy of each target effectively. Besides, by adding this latent state of\nuser emotion to multi-target fusion, the model is capable of decreasing the\nprobability to recommend objectionable contents to improve user retention and\nstay time during online testing phase. DLEN is deployed on a real-world\nmulti-task feed recommendation scenario of Tencent QQ-Small-World with a\ndataset containing over a billion samples, and it exhibits a significant\nperformance advantage over the SOTA MTL model in offline evaluation, together\nwith a considerable increase by 3.02% in view-count and 2.63% in user stay-time\nin production. Complementary offline experiments of DLEN model on a public\ndataset also repeat improvements in various scenarios. At present, DLEN model\nhas been successfully deployed in Tencent's feed recommendation system.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 04:55:13 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Zhang", "Huangbin", ""], ["Zhao", "Chong", ""], ["Zhang", "Yu", ""], ["Wang", "Danlei", ""], ["Yang", "Haichao", ""]]}, {"id": "2104.08727", "submitter": "Daniel Khashabi Mr.", "authors": "Daniel Khashabi, Amos Ng, Tushar Khot, Ashish Sabharwal, Hannaneh\n  Hajishirzi, Chris Callison-Burch", "title": "GooAQ: Open Question Answering with Diverse Answer Types", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While day-to-day questions come with a variety of answer types, the current\nquestion-answering (QA) literature has failed to adequately address the answer\ndiversity of questions. To this end, we present GooAQ, a large-scale dataset\nwith a variety of answer types. This dataset contains over 5 million questions\nand 3 million answers collected from Google. GooAQ questions are collected\nsemi-automatically from the Google search engine using its autocomplete\nfeature. This results in naturalistic questions of practical interest that are\nnonetheless short and expressed using simple language. GooAQ answers are mined\nfrom Google's responses to our collected questions, specifically from the\nanswer boxes in the search results. This yields a rich space of answer types,\ncontaining both textual answers (short and long) as well as more structured\nones such as collections. We benchmarkT5 models on GooAQ and observe that: (a)\nin line with recent work, LM's strong performance on GooAQ's short-answer\nquestions heavily benefit from annotated data; however, (b) their quality in\ngenerating coherent and accurate responses for questions requiring long\nresponses (such as 'how' and 'why' questions) is less reliant on observing\nannotated data and mainly supported by their pre-training. We release GooAQ to\nfacilitate further research on improving QA with diverse response types.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 05:40:39 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Khashabi", "Daniel", ""], ["Ng", "Amos", ""], ["Khot", "Tushar", ""], ["Sabharwal", "Ashish", ""], ["Hajishirzi", "Hannaneh", ""], ["Callison-Burch", "Chris", ""]]}, {"id": "2104.08731", "submitter": "Jifan Chen", "authors": "Jifan Chen, Eunsol Choi, Greg Durrett", "title": "Can NLI Models Verify QA Systems' Predictions?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To build robust question answering systems, we need the ability to verify\nwhether answers to questions are truly correct, not just \"good enough\" in the\ncontext of imperfect QA datasets. We explore the use of natural language\ninference (NLI) as a way to achieve this goal, as NLI inherently requires the\npremise (document context) to contain all necessary information to support the\nhypothesis (proposed answer to the question). We leverage large pre-trained\nmodels and recent prior datasets to construct powerful question converter and\ndecontextualization modules, which can reformulate QA instances as\npremise-hypothesis pairs with very high reliability. Then, by combining\nstandard NLI datasets with NLI examples automatically derived from QA training\ndata, we can train NLI models to judge the correctness of QA models' proposed\nanswers. We show that our NLI approach can generally improve the confidence\nestimation of a QA model across different domains, evaluated in a selective QA\nsetting. Careful manual analysis over the predictions of our NLI model shows\nthat it can further identify cases where the QA model produces the right answer\nfor the wrong reason, or where the answer cannot be verified as addressing all\naspects of the question.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 06:03:07 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Chen", "Jifan", ""], ["Choi", "Eunsol", ""], ["Durrett", "Greg", ""]]}, {"id": "2104.08736", "submitter": "Qi Qi", "authors": "Qi Qi, Youzhi Luo, Zhao Xu, Shuiwang Ji, Tianbao Yang", "title": "Stochastic Optimization of Areas Under Precision-Recall Curves with\n  Provable Convergence", "comments": "24 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Areas under ROC (AUROC) and precision-recall curves (AUPRC) are common\nmetrics for evaluating classification performance for imbalanced problems.\nCompared with AUROC, AUPRC is a more appropriate metric for highly imbalanced\ndatasets. While stochastic optimization of AUROC has been studied extensively,\nprincipled stochastic optimization of AUPRC has been rarely explored. In this\nwork, we propose a principled technical method to optimize AUPRC for deep\nlearning. Our approach is based on maximizing the averaged precision (AP),\nwhich is an unbiased point estimator of AUPRC. We cast the objective into a sum\nof {\\it dependent compositional functions} with inner functions dependent on\nrandom variables of the outer level. We propose efficient adaptive and\nnon-adaptive stochastic algorithms with {\\it provable convergence guarantee\nunder mild conditions} by leveraging recent advances in stochastic\ncompositional optimization. Extensive experimental results on image and graph\ndatasets demonstrate that our proposed method outperforms prior methods on\nimbalanced problems in terms of AUPRC. To the best of our knowledge, our work\nrepresents the first attempt to optimize AUPRC with provable convergence.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 06:22:21 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 05:06:40 GMT"}, {"version": "v3", "created": "Sun, 6 Jun 2021 15:20:25 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Qi", "Qi", ""], ["Luo", "Youzhi", ""], ["Xu", "Zhao", ""], ["Ji", "Shuiwang", ""], ["Yang", "Tianbao", ""]]}, {"id": "2104.08747", "submitter": "Xue Yu", "authors": "Yu Xue, Yihang Tang, Xin Xu, Jiayu Liang, Ferrante Neri", "title": "Multi-objective Feature Selection with Missing Data in Classification", "comments": "1", "journal-ref": null, "doi": "10.1109/TETCI.2021.3074147", "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Feature selection (FS) is an important research topic in machine learning.\nUsually, FS is modelled as a+ bi-objective optimization problem whose\nobjectives are: 1) classification accuracy; 2) number of features. One of the\nmain issues in real-world applications is missing data. Databases with missing\ndata are likely to be unreliable. Thus, FS performed on a data set missing some\ndata is also unreliable. In order to directly control this issue plaguing the\nfield, we propose in this study a novel modelling of FS: we include reliability\nas the third objective of the problem. In order to address the modified\nproblem, we propose the application of the non-dominated sorting genetic\nalgorithm-III (NSGA-III). We selected six incomplete data sets from the\nUniversity of California Irvine (UCI) machine learning repository. We used the\nmean imputation method to deal with the missing data. In the experiments,\nk-nearest neighbors (K-NN) is used as the classifier to evaluate the feature\nsubsets. Experimental results show that the proposed three-objective model\ncoupled with NSGA-III efficiently addresses the FS problem for the six data\nsets included in this study.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 07:12:39 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Xue", "Yu", ""], ["Tang", "Yihang", ""], ["Xu", "Xin", ""], ["Liang", "Jiayu", ""], ["Neri", "Ferrante", ""]]}, {"id": "2104.08755", "submitter": "Zhaohao Zeng", "authors": "Zhaohao Zeng and Tetsuya Sakai", "title": "DCH-2: A Parallel Customer-Helpdesk Dialogue Corpus with Distributions\n  of Annotators' Labels", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We introduce a data set called DCH-2, which contains 4,390 real\ncustomer-helpdesk dialogues in Chinese and their English translations. DCH-2\nalso contains dialogue-level annotations and turn-level annotations obtained\nindependently from either 19 or 20 annotators. The data set was built through\nour effort as organisers of the NTCIR-14 Short Text Conversation and NTCIR-15\nDialogue Evaluation tasks, to help researchers understand what constitutes an\neffective customer-helpdesk dialogue, and thereby build efficient and helpful\nhelpdesk systems that are available to customers at all times. In addition,\nDCH-2 may be utilised for other purposes, for example, as a repository for\nretrieval-based dialogue systems, or as a parallel corpus for machine\ntranslation in the helpdesk domain.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 07:35:15 GMT"}, {"version": "v2", "created": "Sun, 30 May 2021 15:32:47 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Zeng", "Zhaohao", ""], ["Sakai", "Tetsuya", ""]]}, {"id": "2104.08758", "submitter": "Jesse Dodge", "authors": "Jesse Dodge, Maarten Sap, Ana Marasovic, William Agnew, Gabriel\n  Ilharco, Dirk Groeneveld, Matt Gardner", "title": "Documenting the English Colossal Clean Crawled Corpus", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As language models are trained on ever more text, researchers are turning to\nsome of the largest corpora available. Unlike most other types of datasets in\nNLP, large unlabeled text corpora are often presented with minimal\ndocumentation, and best practices for documenting them have not been\nestablished. In this work we provide the first documentation for the Colossal\nClean Crawled Corpus (C4; Raffel et al., 2020), a dataset created by applying a\nset of filters to a single snapshot of Common Crawl. We begin with a high-level\nsummary of the data, including distributions of where the text came from and\nwhen it was written. We then give more detailed analysis on salient parts of\nthis data, including the most frequent sources of text (e.g.,\npatents.google.com, which contains a significant percentage of machine\ntranslated and/or OCR'd text), the effect that the filters had on the data\n(they disproportionately remove text in AAE), and evidence that some other\nbenchmark NLP dataset examples are contained in the text. We release a web\ninterface to an interactive, indexed copy of this dataset, encouraging the\ncommunity to continuously explore and report additional findings.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 07:42:52 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Dodge", "Jesse", ""], ["Sap", "Maarten", ""], ["Marasovic", "Ana", ""], ["Agnew", "William", ""], ["Ilharco", "Gabriel", ""], ["Groeneveld", "Dirk", ""], ["Gardner", "Matt", ""]]}, {"id": "2104.08759", "submitter": "Ofir Gordon", "authors": "Ofir Gordon, Yuval Filmus, Oren Salzman", "title": "Revisiting the Complexity Analysis of Conflict-Based Search: New\n  Computational Techniques and Improved Bounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.CC cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The problem of Multi-Agent Path Finding (MAPF) calls for finding a set of\nconflict-free paths for a fleet of agents operating in a given environment.\nArguably, the state-of-the-art approach to computing optimal solutions is\nConflict-Based Search (CBS). In this work we revisit the complexity analysis of\nCBS to provide tighter bounds on the algorithm's run-time in the worst-case.\nOur analysis paves the way to better pinpoint the parameters that govern (in\nthe worst case) the algorithm's computational complexity.\n  Our analysis is based on two complementary approaches: In the first approach\nwe bound the run-time using the size of a Multi-valued Decision Diagram (MDD)\n-- a layered graph which compactly contains all possible single-agent paths\nbetween two given vertices for a specific path length.\n  In the second approach we express the running time by a novel recurrence\nrelation which bounds the algorithm's complexity. We use generating\nfunctions-based analysis in order to tightly bound the recurrence.\n  Using these technique we provide several new upper-bounds on CBS's\ncomplexity. The results allow us to improve the existing bound on the running\ntime of CBS for many cases. For example, on a set of common benchmarks we\nimprove the upper-bound by a factor of at least $2^{10^{7}}$.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 07:46:28 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Gordon", "Ofir", ""], ["Filmus", "Yuval", ""], ["Salzman", "Oren", ""]]}, {"id": "2104.08760", "submitter": "Guangrun Wang", "authors": "Guangrun Wang, Keze Wang, Guangcong Wang, Philip H.S. Torr, Liang Lin", "title": "Towards Solving Inefficiency of Self-supervised Representation Learning", "comments": "12 pages, 5 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised learning (especially contrastive learning) has attracted\ngreat interest due to its tremendous potentials in learning discriminative\nrepresentations in an unsupervised manner. Despite the acknowledged successes,\nexisting contrastive learning methods suffer from very low learning efficiency,\ne.g., taking about ten times more training epochs than supervised learning for\ncomparable recognition accuracy. In this paper, we discover two contradictory\nphenomena in contrastive learning that we call under-clustering and\nover-clustering problems, which are major obstacles to learning efficiency.\nUnder-clustering means that the model cannot efficiently learn to discover the\ndissimilarity between inter-class samples when the negative sample pairs for\ncontrastive learning are insufficient to differentiate all the actual object\ncategories. Over-clustering implies that the model cannot efficiently learn the\nfeature representation from excessive negative sample pairs, which enforces the\nmodel to over-cluster samples of the same actual categories into different\nclusters. To simultaneously overcome these two problems, we propose a novel\nself-supervised learning framework using a median triplet loss. Precisely, we\nemploy a triplet loss tending to maximize the relative distance between the\npositive pair and negative pairs to address the under-clustering problem; and\nwe construct the negative pair by selecting the negative sample of a median\nsimilarity score from all negative samples to avoid the over-clustering\nproblem, guaranteed by the Bernoulli Distribution model. We extensively\nevaluate our proposed framework in several large-scale benchmarks (e.g.,\nImageNet, SYSU-30k, and COCO). The results demonstrate the superior performance\n(e.g., the learning efficiency) of our model over the latest state-of-the-art\nmethods by a clear margin. Codes available at:\nhttps://github.com/wanggrun/triplet.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 07:47:10 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 10:30:49 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Wang", "Guangrun", ""], ["Wang", "Keze", ""], ["Wang", "Guangcong", ""], ["Torr", "Philip H. S.", ""], ["Lin", "Liang", ""]]}, {"id": "2104.08762", "submitter": "Rajarshi Das", "authors": "Rajarshi Das, Manzil Zaheer, Dung Thai, Ameya Godbole, Ethan Perez,\n  Jay-Yoon Lee, Lizhen Tan, Lazaros Polymenakos, Andrew McCallum", "title": "Case-based Reasoning for Natural Language Queries over Knowledge Bases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is often challenging for a system to solve a new complex problem from\nscratch, but much easier if the system can access other similar problems and\ndescription of their solutions -- a paradigm known as case-based reasoning\n(CBR). We propose a neuro-symbolic CBR approach for question answering over\nlarge knowledge bases (CBR-KBQA). While the idea of CBR is tempting, composing\na solution from cases is nontrivial, when individual cases only contain partial\nlogic to the full solution. To resolve this, CBR-KBQA consists of two modules:\na non-parametric memory that stores cases (question and logical forms) and a\nparametric model which can generate logical forms by retrieving relevant cases\nfrom memory. Through experiments, we show that CBR-KBQA can effectively derive\nnovel combination of relations not presented in case memory that is required to\nanswer compositional questions. On several KBQA datasets that test\ncompositional generalization, CBR-KBQA achieves competitive performance. For\nexample, on the challenging ComplexWebQuestions dataset, CBR-KBQA outperforms\nthe current state of the art by 11% accuracy. Furthermore, we show that\nCBR-KBQA is capable of using new cases \\emph{without} any further training.\nJust by incorporating few human-labeled examples in the non-parametric case\nmemory, CBR-KBQA is able to successfully generate queries containing unseen KB\nrelations.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 07:50:31 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Das", "Rajarshi", ""], ["Zaheer", "Manzil", ""], ["Thai", "Dung", ""], ["Godbole", "Ameya", ""], ["Perez", "Ethan", ""], ["Lee", "Jay-Yoon", ""], ["Tan", "Lizhen", ""], ["Polymenakos", "Lazaros", ""], ["McCallum", "Andrew", ""]]}, {"id": "2104.08763", "submitter": "Shunsuke Kitada", "authors": "Shunsuke Kitada, Hitoshi Iyatomi", "title": "Making Attention Mechanisms More Robust and Interpretable with Virtual\n  Adversarial Training for Semi-Supervised Text Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new general training technique for attention mechanisms based on\nvirtual adversarial training (VAT). VAT can compute adversarial perturbations\nfrom unlabeled data in a semi-supervised setting for the attention mechanisms\nthat have been reported in previous studies to be vulnerable to perturbations.\nEmpirical experiments reveal that our technique (1) provides significantly\nbetter prediction performance compared to not only conventional adversarial\ntraining-based techniques but also VAT-based techniques in a semi-supervised\nsetting, (2) demonstrates a stronger correlation with the word importance and\nbetter agreement with evidence provided by humans, and (3) gains in performance\nwith increasing amounts of unlabeled data.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 07:51:45 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Kitada", "Shunsuke", ""], ["Iyatomi", "Hitoshi", ""]]}, {"id": "2104.08769", "submitter": "Ziqian Zeng", "authors": "Ziqian Zeng, Rashidul Islam, Kamrun Naher Keya, James Foulds, Yangqiu\n  Song, Shimei Pan", "title": "Fair Representation Learning for Heterogeneous Information Networks", "comments": "Accepted at ICWSM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, much attention has been paid to the societal impact of AI,\nespecially concerns regarding its fairness. A growing body of research has\nidentified unfair AI systems and proposed methods to debias them, yet many\nchallenges remain. Representation learning for Heterogeneous Information\nNetworks (HINs), a fundamental building block used in complex network mining,\nhas socially consequential applications such as automated career counseling,\nbut there have been few attempts to ensure that it will not encode or amplify\nharmful biases, e.g. sexism in the job market. To address this gap, in this\npaper we propose a comprehensive set of de-biasing methods for fair HINs\nrepresentation learning, including sampling-based, projection-based, and graph\nneural networks (GNNs)-based techniques. We systematically study the behavior\nof these algorithms, especially their capability in balancing the trade-off\nbetween fairness and prediction accuracy. We evaluate the performance of the\nproposed methods in an automated career counseling application where we\nmitigate gender bias in career recommendation. Based on the evaluation results\non two datasets, we identify the most effective fair HINs representation\nlearning techniques under different conditions.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 08:28:18 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Zeng", "Ziqian", ""], ["Islam", "Rashidul", ""], ["Keya", "Kamrun Naher", ""], ["Foulds", "James", ""], ["Song", "Yangqiu", ""], ["Pan", "Shimei", ""]]}, {"id": "2104.08773", "submitter": "Swaroop Mishra", "authors": "Swaroop Mishra, Daniel Khashabi, Chitta Baral, Hannaneh Hajishirzi", "title": "Natural Instructions: Benchmarking Generalization to New Tasks from\n  Natural Language Instructions", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can we enable NLP models to appropriately respond to instructional prompts\nand consequently generalize to new tasks? To study this question, we leverage\nthe existing NLP datasets and the instructions that were used to crowdsource\nthem to create NATURAL INSTRUCTIONS, a dataset of instructions and\ntask-specific input/output data. This dataset consists of 61 distinct language\ninstructions and about 600k task instances, and is used to evaluate existing\nstate-of-the-art language-models (LMs) in addressing new tasks by few-shot\nprompting of GPT3 and fine-tuning BART. Our analysis indicates that: (a) the\nexisting models indeed benefit from instructions and hence, show improved\ngeneralization to new tasks; (b) while models like GPT-3 generally benefit from\ninstructions, the extent of their gains varies across different fields of\ninstructions and also depends on the task being solved; (c) generalization to\nunseen tasks in NATURAL INSTRUCTIONS remains far from perfect for the\nstate-of-the-art, indicating significant room for more progress in this\ndirection.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 08:44:56 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Mishra", "Swaroop", ""], ["Khashabi", "Daniel", ""], ["Baral", "Chitta", ""], ["Hajishirzi", "Hannaneh", ""]]}, {"id": "2104.08775", "submitter": "Nayeon Lee", "authors": "Nayeon Lee, Andrea Madotto, Yejin Bang, Pascale Fung", "title": "Dynamically Addressing Unseen Rumor via Continual Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Rumors are often associated with newly emerging events, thus, an ability to\ndeal with unseen rumors is crucial for a rumor veracity classification model.\nPrevious works address this issue by improving the model's generalizability,\nwith an assumption that the model will stay unchanged even after the new\noutbreak of an event. In this work, we propose an alternative solution to\ncontinuously update the model in accordance with the dynamics of rumor domain\ncreations. The biggest technical challenge associated with this new approach is\nthe catastrophic forgetting of previous learnings due to new learnings. We\nadopt continual learning strategies that control the new learnings to avoid\ncatastrophic forgetting and propose an additional strategy that can jointly be\nused to strengthen the forgetting alleviation.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 08:50:10 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Lee", "Nayeon", ""], ["Madotto", "Andrea", ""], ["Bang", "Yejin", ""], ["Fung", "Pascale", ""]]}, {"id": "2104.08779", "submitter": "Ziqian Zeng", "authors": "Ziqian Zeng, Yangqiu Song", "title": "Variational Weakly Supervised Sentiment Analysis with Posterior\n  Regularization", "comments": "Accepted at EACL 2021. arXiv admin note: text overlap with\n  arXiv:2008.09394", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sentiment analysis is an important task in natural language processing (NLP).\nMost of existing state-of-the-art methods are under the supervised learning\nparadigm. However, human annotations can be scarce. Thus, we should leverage\nmore weak supervision for sentiment analysis. In this paper, we propose a\nposterior regularization framework for the variational approach to the weakly\nsupervised sentiment analysis to better control the posterior distribution of\nthe label assignment. The intuition behind the posterior regularization is that\nif extracted opinion words from two documents are semantically similar, the\nposterior distributions of two documents should be similar. Our experimental\nresults show that the posterior regularization can improve the original\nvariational approach to the weakly supervised sentiment analysis and the\nperformance is more stable with smaller prediction variance.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 09:05:31 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Zeng", "Ziqian", ""], ["Song", "Yangqiu", ""]]}, {"id": "2104.08786", "submitter": "Yao Lu", "authors": "Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, Pontus\n  Stenetorp", "title": "Fantastically Ordered Prompts and Where to Find Them: Overcoming\n  Few-Shot Prompt Order Sensitivity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When primed with only a handful of training samples, very large pretrained\nlanguage models such as GPT-3, have shown competitive results when compared to\nfully-supervised fine-tuned large pretrained language models. We demonstrate\nthat the order in which the samples are provided can be the difference between\nnear state-of-the-art and random guess performance: Essentially some\npermutations are \"fantastic\" and some not. We analyse this phenomenon in\ndetail, establishing that: it is present across model sizes (even for the\nlargest current models), it is not related to a specific subset of samples, and\nthat a given good permutation for one model is not transferable to another.\nWhile one could use a development set to determine which permutations are\nperformant, this would deviate from the few-shot setting as it requires\nadditional annotated data. Instead, we use the generative nature of the\nlanguage models to construct an artificial development set and based on entropy\nstatistics of the candidate permutations from this set we identify performant\nprompts. Our method improves upon GPT-family models by on average 13% relative\nacross eleven different established text classification tasks.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 09:29:16 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Lu", "Yao", ""], ["Bartolo", "Max", ""], ["Moore", "Alastair", ""], ["Riedel", "Sebastian", ""], ["Stenetorp", "Pontus", ""]]}, {"id": "2104.08801", "submitter": "Devang Kulshreshtha", "authors": "Devang Kulshreshtha, Robert Belfer, Iulian Vlad Serban, Siva Reddy", "title": "Back-Training excels Self-Training at Unsupervised Domain Adaptation of\n  Question Generation and Passage Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a new domain adaptation method called\n$\\textit{back-training}$, a superior alternative to self-training. While\nself-training results in synthetic training data of the form quality inputs\naligned with noisy outputs, back-training results in noisy inputs aligned with\nquality outputs. Our experimental results on unsupervised domain adaptation of\nquestion generation and passage retrieval models from $\\textit{Natural\nQuestions}$ domain to the machine learning domain show that back-training\noutperforms self-training by a large margin: 9.3 BLEU-1 points on generation,\nand 7.9 accuracy points on top-1 retrieval. We release $\\textit{MLQuestions}$,\na domain-adaptation dataset for the machine learning domain containing 50K\nunaligned passages and 35K unaligned questions, and 3K aligned passage and\nquestion pairs. Our data and code are available at\nhttps://github.com/McGill-NLP/MLQuestions\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 10:20:07 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Kulshreshtha", "Devang", ""], ["Belfer", "Robert", ""], ["Serban", "Iulian Vlad", ""], ["Reddy", "Siva", ""]]}, {"id": "2104.08803", "submitter": "Tal Schuster", "authors": "Tal Schuster, Adam Fisch, Tommi Jaakkola, Regina Barzilay", "title": "Consistent Accelerated Inference via Confident Adaptive Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop a novel approach for confidently accelerating inference in the\nlarge and expensive multilayer Transformers that are now ubiquitous in natural\nlanguage processing (NLP). Amortized or approximate computational methods\nincrease efficiency, but can come with unpredictable performance costs. In this\nwork, we present CATs -- Confident Adaptive Transformers -- in which we\nsimultaneously increase computational efficiency, while guaranteeing a\nspecifiable degree of consistency with the original model with high confidence.\nOur method trains additional prediction heads on top of intermediate layers,\nand dynamically decides when to stop allocating computational effort to each\ninput using a meta consistency classifier. To calibrate our early prediction\nstopping rule, we formulate a unique extension of conformal prediction. We\ndemonstrate the effectiveness of this approach on four classification and\nregression tasks.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 10:22:28 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Schuster", "Tal", ""], ["Fisch", "Adam", ""], ["Jaakkola", "Tommi", ""], ["Barzilay", "Regina", ""]]}, {"id": "2104.08804", "submitter": "Harkanwar Singh", "authors": "Harkanwar Singh, Prachi Jain, Mausam, Soumen Chakrabarti", "title": "Multilingual Knowledge Graph Completion with Joint Relation and Entity\n  Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge Graph Completion (KGC) predicts missing facts in an incomplete\nKnowledge Graph. Almost all of existing KGC research is applicable to only one\nKG at a time, and in one language only. However, different language speakers\nmay maintain separate KGs in their language and no individual KG is expected to\nbe complete. Moreover, common entities or relations in these KGs have different\nsurface forms and IDs, leading to ID proliferation. Entity alignment (EA) and\nrelation alignment (RA) tasks resolve this by recognizing pairs of entity\n(relation) IDs in different KGs that represent the same entity (relation). This\ncan further help prediction of missing facts, since knowledge from one KG is\nlikely to benefit completion of another. High confidence predictions may also\nadd valuable information for the alignment tasks. In response, we study the\nnovel task of jointly training multilingual KGC, relation alignment and entity\nalignment models. We present ALIGNKGC, which uses some seed alignments to\njointly optimize all three of KGC, EA and RA losses. A key component of\nALIGNKGC is an embedding based soft notion of asymmetric overlap defined on the\n(subject, object) set signatures of relations this aids in better predicting\nrelations that are equivalent to or implied by other relations. Extensive\nexperiments with DBPedia in five languages establish the benefits of joint\ntraining for all tasks, achieving 10-32 MRR improvements of ALIGNKGC over a\nstrong state-of-the-art single-KGC system completion model over each\nmonolingual KG . Further, ALIGNKGC achieves reasonable gains in EA and RA tasks\nover a vanilla completion model over a KG that combines all facts without\nalignment, underscoring the value of joint training for these tasks.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 10:27:44 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Singh", "Harkanwar", ""], ["Jain", "Prachi", ""], ["Mausam", "", ""], ["Chakrabarti", "Soumen", ""]]}, {"id": "2104.08805", "submitter": "Sergio Rozada", "authors": "Sergio Rozada, Victor Tenorio, and Antonio G. Marques", "title": "Low-rank State-action Value-function Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Value functions are central to Dynamic Programming and Reinforcement Learning\nbut their exact estimation suffers from the curse of dimensionality,\nchallenging the development of practical value-function (VF) estimation\nalgorithms. Several approaches have been proposed to overcome this issue, from\nnon-parametric schemes that aggregate states or actions to parametric\napproximations of state and action VFs via, e.g., linear estimators or deep\nneural networks. Relevantly, several high-dimensional state problems can be\nwell-approximated by an intrinsic low-rank structure. Motivated by this and\nleveraging results from low-rank optimization, this paper proposes different\nstochastic algorithms to estimate a low-rank factorization of the $Q(s, a)$\nmatrix. This is a non-parametric alternative to VF approximation that\ndramatically reduces the computational and sample complexities relative to\nclassical $Q$-learning methods that estimate $Q(s,a)$ separately for each\nstate-action pair.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 10:31:39 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Rozada", "Sergio", ""], ["Tenorio", "Victor", ""], ["Marques", "Antonio G.", ""]]}, {"id": "2104.08811", "submitter": "Anton Belyy", "authors": "Noah Weber, Anton Belyy, Nils Holzenberger, Rachel Rudinger, Benjamin\n  Van Durme", "title": "Schema Curation via Causal Association Rule Mining", "comments": "9 pages, 3 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Event schemas are structured knowledge sources defining typical real-world\nscenarios (e.g., going to an airport). We present a framework for efficient\nhuman-in-the-loop construction of a schema library, based on a novel mechanism\nfor schema induction and a well-crafted interface that allows non-experts to\n\"program\" complex event structures. Associated with this work we release a\nmachine readable resource (schema library) of 232 detailed event schemas, each\nof which describe a distinct typical scenario in terms of its relevant\nsub-event structure (what happens in the scenario), participants (who plays a\nrole in the scenario), fine-grained typing of each participant, and the implied\nrelational constraints between them. Our custom annotation interface,\nSchemaBlocks, and the event schemas are available online.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 10:48:26 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Weber", "Noah", ""], ["Belyy", "Anton", ""], ["Holzenberger", "Nils", ""], ["Rudinger", "Rachel", ""], ["Van Durme", "Benjamin", ""]]}, {"id": "2104.08813", "submitter": "Abdul Karim Gizzini", "authors": "Abdul Karim Gizzini, Marwa Chafii, Ahmad Nimr, Raed M. Shubair,\n  Gerhard Fettweis", "title": "CNN aided Weighted Interpolation for Channel Estimation in Vehicular\n  Communications", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AI math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  IEEE 802.11p standard defines wireless technology protocols that enable\nvehicular transportation and manage traffic efficiency. A major challenge in\nthe development of this technology is ensuring communication reliability in\nhighly dynamic vehicular environments, where the wireless communication\nchannels are doubly selective, thus making channel estimation and tracking a\nrelevant problem to investigate. In this paper, a novel deep learning\n(DL)-based weighted interpolation estimator is proposed to accurately estimate\nvehicular channels especially in high mobility scenarios. The proposed\nestimator is based on modifying the pilot allocation of the IEEE 802.11p\nstandard so that more transmission data rates are achieved. Extensive numerical\nexperiments demonstrate that the developed estimator significantly outperforms\nthe recently proposed DL-based frame-by-frame estimators in different vehicular\nscenarios, while substantially reducing the overall computational complexity.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 10:57:52 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Gizzini", "Abdul Karim", ""], ["Chafii", "Marwa", ""], ["Nimr", "Ahmad", ""], ["Shubair", "Raed M.", ""], ["Fettweis", "Gerhard", ""]]}, {"id": "2104.08815", "submitter": "Bill Yuchen Lin", "authors": "Bill Yuchen Lin, Chaoyang He, Zihang Zeng, Hulin Wang, Yufen Huang,\n  Mahdi Soltanolkotabi, Xiang Ren, Salman Avestimehr", "title": "FedNLP: A Research Platform for Federated Learning in Natural Language\n  Processing", "comments": "Github link: https://github.com/FedML-AI/FedNLP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Increasing concerns and regulations about data privacy, necessitate the study\nof privacy-preserving methods for natural language processing (NLP)\napplications. Federated learning (FL) provides promising methods for a large\nnumber of clients (i.e., personal devices or organizations) to collaboratively\nlearn a shared global model to benefit all clients, while allowing users to\nkeep their data locally. To facilitate FL research in NLP, we present the\nFedNLP, a research platform for federated learning in NLP. FedNLP supports\nvarious popular task formulations in NLP such as text classification, sequence\ntagging, question answering, seq2seq generation, and language modeling. We also\nimplement an interface between Transformer language models (e.g., BERT) and FL\nmethods (e.g., FedAvg, FedOpt, etc.) for distributed training. The evaluation\nprotocol of this interface supports a comprehensive collection of non-IID\npartitioning strategies. Our preliminary experiments with FedNLP reveal that\nthere exists a large performance gap between learning on decentralized and\ncentralized datasets -- opening intriguing and exciting future research\ndirections aimed at developing FL methods suited to NLP tasks.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 11:04:49 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Lin", "Bill Yuchen", ""], ["He", "Chaoyang", ""], ["Zeng", "Zihang", ""], ["Wang", "Hulin", ""], ["Huang", "Yufen", ""], ["Soltanolkotabi", "Mahdi", ""], ["Ren", "Xiang", ""], ["Avestimehr", "Salman", ""]]}, {"id": "2104.08819", "submitter": "Manjushree Laddha", "authors": "Manjushree D. Laddha, Varsha T. Lokare, Arvind W. Kiwelekar and Laxman\n  D. Netak", "title": "Classifications of the Summative Assessment for Revised Blooms Taxonomy\n  by using Deep Learning", "comments": "8 pages, 7 figures, 2 tables", "journal-ref": "International Journal of Engineering Trends and Technology\n  69.3(2021):211-218", "doi": "10.14445/22315381/IJETT-V69I3P232", "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Education is the basic step of understanding the truth and the preparation of\nthe intelligence to reflect. Focused on the rational capacity of the human\nbeing the Cognitive process and knowledge dimensions of Revised Blooms Taxonomy\nhelps to differentiate the procedure of studying into six types of various\ncognitive processes and four types of knowledge dimensions. These types are\nsynchronized in the increasing level of difficulty. In this paper Software\nEngineering courses of B.Tech Computer Engineering and Information Technology\noffered by various Universities and Educational Institutes have been\ninvestigated for Revised Blooms Taxonomy RBT. Questions are a very useful\nconstituent. Knowledge intelligence and strength of the learners can be tested\nby applying questions.The fundamental goal of this paper is to create a\nrelative study of the classification of the summative assessment based on\nRevised Blooms Taxonomy using the Convolutional Neural Networks CNN Long\nShort-Term Memory LSTM of Deep Learning techniques in an endeavor to attain\nsignificant accomplishment and elevated precision levels.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 11:21:48 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Laddha", "Manjushree D.", ""], ["Lokare", "Varsha T.", ""], ["Kiwelekar", "Arvind W.", ""], ["Netak", "Laxman D.", ""]]}, {"id": "2104.08825", "submitter": "Kaj Bostrom", "authors": "Kaj Bostrom, Xinyu Zhao, Swarat Chaudhuri, Greg Durrett", "title": "Flexible Operations for Natural Language Deduction", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An interpretable system for complex, open-domain reasoning needs an\ninterpretable meaning representation. Natural language is an excellent\ncandidate -- it is both extremely expressive and easy for humans to understand.\nHowever, manipulating natural language statements in logically consistent ways\nis hard. Models have to be precise, yet robust enough to handle variation in\nhow information is expressed. In this paper, we describe ParaPattern, a method\nfor building models to generate logical transformations of diverse natural\nlanguage inputs without direct human supervision. We use a BART-based model\n(Lewis et al., 2020) to generate the result of applying a particular logical\noperation to one or more premise statements. Crucially, we have a largely\nautomated pipeline for scraping and constructing suitable training examples\nfrom Wikipedia, which are then paraphrased to give our models the ability to\nhandle lexical variation. We evaluate our models using targeted contrast sets\nas well as out-of-domain sentence compositions from the QASC dataset (Khot et\nal., 2020). Our results demonstrate that our operation models are both accurate\nand flexible.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 11:36:26 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Bostrom", "Kaj", ""], ["Zhao", "Xinyu", ""], ["Chaudhuri", "Swarat", ""], ["Durrett", "Greg", ""]]}, {"id": "2104.08826", "submitter": "Kang Min Yoo", "authors": "Kang Min Yoo, Dongju Park, Jaewook Kang, Sang-Woo Lee, Woomyeong Park", "title": "GPT3Mix: Leveraging Large-scale Language Models for Text Augmentation", "comments": "11 pages, 7 tables, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large-scale language models such as GPT-3 are excellent few-shot learners,\nallowing them to be controlled via natural text prompts. Recent studies report\nthat prompt-based direct classification eliminates the need for fine-tuning but\nlacks data and inference scalability. This paper proposes a novel data\naugmentation technique that leverages large-scale language models to generate\nrealistic text samples from a mixture of real samples. We also propose\nutilizing soft-labels predicted by the language models, effectively distilling\nknowledge from the large-scale language models and creating textual\nperturbations simultaneously. We perform data augmentation experiments on\ndiverse classification tasks and show that our method hugely outperforms\nexisting text augmentation methods. Ablation studies and a qualitative analysis\nprovide more insights into our approach.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 11:39:33 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Yoo", "Kang Min", ""], ["Park", "Dongju", ""], ["Kang", "Jaewook", ""], ["Lee", "Sang-Woo", ""], ["Park", "Woomyeong", ""]]}, {"id": "2104.08829", "submitter": "Valentin Hofmann", "authors": "Valentin Hofmann, Janet B. Pierrehumbert, Hinrich Sch\\\"utze", "title": "Modeling Ideological Agenda Setting and Framing in Polarized Online\n  Groups with Graph Neural Networks and Structured Sparsity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing polarization of online political discourse calls for\ncomputational tools that are able to automatically detect and monitor\nideological divides in social media. Here, we introduce a minimally supervised\nmethod that directly leverages the network structure of online discussion\nforums, specifically Reddit, to detect polarized concepts. We model\npolarization along the dimensions of agenda setting and framing, drawing upon\ninsights from moral psychology. The architecture we propose combines graph\nneural networks with structured sparsity and results in representations for\nconcepts and subreddits that capture phenomena such as ideological\nradicalization and subreddit hijacking. We also create a new dataset of\npolitical discourse covering 12 years and more than 600 online groups with\ndifferent ideologies.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 11:48:25 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 17:02:48 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Hofmann", "Valentin", ""], ["Pierrehumbert", "Janet B.", ""], ["Sch\u00fctze", "Hinrich", ""]]}, {"id": "2104.08842", "submitter": "Avijit Basak", "authors": "Avijit Basak", "title": "A Rank based Adaptive Mutation in Genetic Algorithm", "comments": null, "journal-ref": "August 2020 International Journal of Computer Applications 175", "doi": "10.5120/ijca2020920572", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally Genetic Algorithm has been used for optimization of unimodal\nand multimodal functions. Earlier researchers worked with constant\nprobabilities of GA control operators like crossover, mutation etc. for tuning\nthe optimization in specific domains. Recent advancements in this field\nwitnessed adaptive approach in probability determination. In Adaptive mutation\nprimarily poor individuals are utilized to explore state space, so mutation\nprobability is usually generated proportionally to the difference between\nfitness of best chromosome and itself (fMAX - f). However, this approach is\nsusceptible to nature of fitness distribution during optimization. This paper\npresents an alternate approach of mutation probability generation using\nchromosome rank to avoid any susceptibility to fitness distribution.\nExperiments are done to compare results of simple genetic algorithm (SGA) with\nconstant mutation probability and adaptive approaches within a limited resource\nconstraint for unimodal, multimodal functions and Travelling Salesman Problem\n(TSP). Measurements are done for average best fitness, number of generations\nevolved and percentage of global optimum achievements out of several trials.\nThe results demonstrate that the rank-based adaptive mutation approach is\nsuperior to fitness-based adaptive approach as well as SGA in a multimodal\nproblem space.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 12:41:33 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Basak", "Avijit", ""]]}, {"id": "2104.08845", "submitter": "Kecheng Chen", "authors": "Kecheng Chen, Kun Long, Yazhou Ren, Jiayu Sun and Xiaorong Pu", "title": "Lesion-Inspired Denoising Network: Connecting Medical Image Denoising\n  and Lesion Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has achieved notable performance in the denoising task of\nlow-quality medical images and the detection task of lesions, respectively.\nHowever, existing low-quality medical image denoising approaches are\ndisconnected from the detection task of lesions. Intuitively, the quality of\ndenoised images will influence the lesion detection accuracy that in turn can\nbe used to affect the denoising performance. To this end, we propose a\nplay-and-plug medical image denoising framework, namely Lesion-Inspired\nDenoising Network (LIDnet), to collaboratively improve both denoising\nperformance and detection accuracy of denoised medical images. Specifically, we\npropose to insert the feedback of downstream detection task into existing\ndenoising framework by jointly learning a multi-loss objective. Instead of\nusing perceptual loss calculated on the entire feature map, a novel\nregion-of-interest (ROI) perceptual loss induced by the lesion detection task\nis proposed to further connect these two tasks. To achieve better optimization\nfor overall framework, we propose a customized collaborative training strategy\nfor LIDnet. On consideration of clinical usability and imaging characteristics,\nthree low-dose CT images datasets are used to evaluate the effectiveness of the\nproposed LIDnet. Experiments show that, by equipping with LIDnet, both of the\ndenoising and lesion detection performance of baseline methods can be\nsignificantly improved.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 12:53:36 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Chen", "Kecheng", ""], ["Long", "Kun", ""], ["Ren", "Yazhou", ""], ["Sun", "Jiayu", ""], ["Pu", "Xiaorong", ""]]}, {"id": "2104.08878", "submitter": "Samuel Bell", "authors": "Samuel J. Bell and Onno P. Kampman", "title": "Perspectives on Machine Learning from Psychology's Reproducibility\n  Crisis", "comments": "Added acknowledgements; assorted minor edits", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the early 2010s, a crisis of reproducibility rocked the field of\npsychology. Following a period of reflection, the field has responded with\nradical reform of its scientific practices. More recently, similar questions\nabout the reproducibility of machine learning research have also come to the\nfore. In this short paper, we present select ideas from psychology's\nreformation, translating them into relevance for a machine learning audience.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 15:17:35 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 11:44:47 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Bell", "Samuel J.", ""], ["Kampman", "Onno P.", ""]]}, {"id": "2104.08886", "submitter": "Szymon P{\\l}otka", "authors": "Szymon P{\\l}otka, Tomasz W{\\l}odarczyk, Ryszard Szczerba,\n  Przemys{\\l}aw Rokita, Patrycja Bartkowska, Oskar Komisarek, Artur\n  Matthews-Brzozowski, Tomasz Trzci\\'nski", "title": "Convolutional Neural Networks in Orthodontics: a review", "comments": "Preprint to Elsevier", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional neural networks (CNNs) are used in many areas of computer\nvision, such as object tracking and recognition, security, military, and\nbiomedical image analysis. This review presents the application of\nconvolutional neural networks in one of the fields of dentistry - orthodontics.\nAdvances in medical imaging technologies and methods allow CNNs to be used in\northodontics to shorten the planning time of orthodontic treatment, including\nan automatic search of landmarks on cephalometric X-ray images, tooth\nsegmentation on Cone-Beam Computed Tomography (CBCT) images or digital models,\nand classification of defects on X-Ray panoramic images. In this work, we\ndescribe the current methods, the architectures of deep convolutional neural\nnetworks used, and their implementations, together with a comparison of the\nresults achieved by them. The promising results and visualizations of the\ndescribed studies show that the use of methods based on convolutional neural\nnetworks allows for the improvement of computer-based orthodontic treatment\nplanning, both by reducing the examination time and, in many cases, by\nperforming the analysis much more accurately than a manual orthodontist does.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 16:02:30 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["P\u0142otka", "Szymon", ""], ["W\u0142odarczyk", "Tomasz", ""], ["Szczerba", "Ryszard", ""], ["Rokita", "Przemys\u0142aw", ""], ["Bartkowska", "Patrycja", ""], ["Komisarek", "Oskar", ""], ["Matthews-Brzozowski", "Artur", ""], ["Trzci\u0144ski", "Tomasz", ""]]}, {"id": "2104.08890", "submitter": "Adarsh Pyarelal", "authors": "Adarsh Pyarelal, Aditya Banerjee, Kobus Barnard", "title": "Modular Procedural Generation for Voxel Maps", "comments": "8 pages, 7 figures, submitted to IEEE Conference on Games 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Task environments developed in Minecraft are becoming increasingly popular\nfor artificial intelligence (AI) research. However, most of these are currently\nconstructed manually, thus failing to take advantage of procedural content\ngeneration (PCG), a capability unique to virtual task environments. In this\npaper, we present mcg, an open-source library to facilitate implementing PCG\nalgorithms for voxel-based environments such as Minecraft. The library is\ndesigned with human-machine teaming research in mind, and thus takes a\n'top-down' approach to generation, simultaneously generating low and high level\nmachine-readable representations that are suitable for empirical research.\nThese can be consumed by downstream AI applications that consider human spatial\ncognition. The benefits of this approach include rapid, scalable, and efficient\ndevelopment of virtual environments, the ability to control the statistics of\nthe environment at a semantic level, and the ability to generate novel\nenvironments in response to player actions in real time.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 16:21:35 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Pyarelal", "Adarsh", ""], ["Banerjee", "Aditya", ""], ["Barnard", "Kobus", ""]]}, {"id": "2104.08936", "submitter": "Vivek Khetan", "authors": "Vivek Khetan, Annervaz K M, Erin Wetherley, Elena Eneva, Shubhashis\n  Sengupta, and Andrew E. Fano", "title": "Knowledge Graph Anchored Information-Extraction for Domain-Specific\n  Insights", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing quantity and complexity of data pose challenges for humans to\nconsume information and respond in a timely manner. For businesses in domains\nwith rapidly changing rules and regulations, failure to identify changes can be\ncostly. In contrast to expert analysis or the development of domain-specific\nontology and taxonomies, we use a task-based approach for fulfilling specific\ninformation needs within a new domain. Specifically, we propose to extract\ntask-based information from incoming instance data. A pipeline constructed of\nstate of the art NLP technologies, including a bi-LSTM-CRF model for entity\nextraction, attention-based deep Semantic Role Labeling, and an automated\nverb-based relationship extractor, is used to automatically extract an instance\nlevel semantic structure. Each instance is then combined with a larger,\ndomain-specific knowledge graph to produce new and timely insights. Preliminary\nresults, validated manually, show the methodology to be effective for\nextracting specific information to complete end use-cases.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 19:28:10 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 02:44:06 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Khetan", "Vivek", ""], ["M", "Annervaz K", ""], ["Wetherley", "Erin", ""], ["Eneva", "Elena", ""], ["Sengupta", "Shubhashis", ""], ["Fano", "Andrew E.", ""]]}, {"id": "2104.08940", "submitter": "Xiaopeng Zhao", "authors": "Fengpei Yuan, Amir Sadovnik, Ran Zhang, Devin Casenhiser, Eun Jin\n  Paek, Si On Yoon, and Xiaopeng Zhao", "title": "A Simulated Experiment to Explore Robotic Dialogue Strategies for People\n  with Dementia", "comments": "6 pages, 3 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People with Alzheimer's disease and related dementias (ADRD) often show the\nproblem of repetitive questioning, which brings a great burden on persons with\nADRD (PwDs) and their caregivers. Conversational robots hold promise of coping\nwith this problem and hence alleviating the burdens on caregivers. In this\npaper, we proposed a partially observable markov decision process (POMDP) model\nfor the PwD-robot interaction in the context of repetitive questioning, and\nused Q-learning to learn an adaptive conversation strategy (i.e., rate of\nfollow-up question and difficulty of follow-up question) towards PwDs with\ndifferent cognitive capabilities and different engagement levels. The results\nindicated that Q-learning was helpful for action selection for the robot. This\nmay be a useful step towards the application of conversational social robots to\ncope with repetitive questioning in PwDs.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 19:35:19 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Yuan", "Fengpei", ""], ["Sadovnik", "Amir", ""], ["Zhang", "Ran", ""], ["Casenhiser", "Devin", ""], ["Paek", "Eun Jin", ""], ["Yoon", "Si On", ""], ["Zhao", "Xiaopeng", ""]]}, {"id": "2104.08942", "submitter": "Neel Kanwal", "authors": "Neel Kanwal, Giuseppe Rizzo", "title": "Attention-based Clinical Note Summarization", "comments": "9 Pages, 6 Figure, 2 Tables, Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The trend of deploying digital systems in numerous industries has induced a\nhike in recording digital information. The health sector has observed a large\nadoption of digital devices and systems generating large volumes of personal\nmedical health records. Electronic health records contain valuable information\nfor retrospective and prospective analysis that is often not entirely exploited\nbecause of the dense information storage. The crude purpose of condensing\nhealth records is to select the information that holds most characteristics of\nthe original documents based on reported disease. These summaries may boost\ndiagnosis and extend a doctor's interaction time with the patient during a high\nworkload situation like the COVID-19 pandemic. In this paper, we propose a\nmulti-head attention-based mechanism to perform extractive summarization of\nmeaningful phrases in clinical notes. This method finds major sentences for a\nsummary by correlating tokens, segments and positional embeddings. The model\noutputs attention scores that are statistically transformed to extract key\nphrases and can be used for a projection on the heat-mapping tool for visual\nand human use.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 19:40:26 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Kanwal", "Neel", ""], ["Rizzo", "Giuseppe", ""]]}, {"id": "2104.08943", "submitter": "Thuy Vu", "authors": "Vivek Krishnamurthy, Thuy Vu, Alessandro Moschitti", "title": "Reference-based Weak Supervision for Answer Sentence Selection using Web\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answer sentence selection (AS2) modeling requires annotated data, i.e.,\nhand-labeled question-answer pairs. We present a strategy to collect weakly\nsupervised answers for a question based on its reference to improve AS2\nmodeling. Specifically, we introduce Reference-based Weak Supervision (RWS), a\nfully automatic large-scale data pipeline that harvests high-quality\nweakly-supervised answers from abundant Web data requiring only a\nquestion-reference pair as input. We study the efficacy and robustness of RWS\nin the setting of TANDA, a recent state-of-the-art fine-tuning approach\nspecialized for AS2. Our experiments indicate that the produced data\nconsistently bolsters TANDA. We achieve the state of the art in terms of P@1,\n90.1%, and MAP, 92.9%, on WikiQA.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 19:41:17 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Krishnamurthy", "Vivek", ""], ["Vu", "Thuy", ""], ["Moschitti", "Alessandro", ""]]}, {"id": "2104.08955", "submitter": "Eliya Nachmani", "authors": "Shaked Dovrat, Eliya Nachmani, Lior Wolf", "title": "Many-Speakers Single Channel Speech Separation with Optimal Permutation\n  Training", "comments": "Accepted to Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Single channel speech separation has experienced great progress in the last\nfew years. However, training neural speech separation for a large number of\nspeakers (e.g., more than 10 speakers) is out of reach for the current methods,\nwhich rely on the Permutation Invariant Loss (PIT). In this work, we present a\npermutation invariant training that employs the Hungarian algorithm in order to\ntrain with an $O(C^3)$ time complexity, where $C$ is the number of speakers, in\ncomparison to $O(C!)$ of PIT based methods. Furthermore, we present a modified\narchitecture that can handle the increased number of speakers. Our approach\nseparates up to $20$ speakers and improves the previous results for large $C$\nby a wide margin.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 20:56:12 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 18:42:04 GMT"}, {"version": "v3", "created": "Fri, 2 Jul 2021 10:57:33 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Dovrat", "Shaked", ""], ["Nachmani", "Eliya", ""], ["Wolf", "Lior", ""]]}, {"id": "2104.08959", "submitter": "TrungTin Nguyen", "authors": "TrungTin Nguyen, Faicel Chamroukhi, Hien Duy Nguyen, Florence Forbes", "title": "Non-asymptotic model selection in block-diagonal mixture of polynomial\n  experts models", "comments": "Corrected typos. Extended results from arXiv:2104.02640. arXiv admin\n  note: substantial text overlap with arXiv:2104.02640", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.AI cs.LG stat.ME stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model selection, via penalized likelihood type criteria, is a standard task\nin many statistical inference and machine learning problems. Progress has led\nto deriving criteria with asymptotic consistency results and an increasing\nemphasis on introducing non-asymptotic criteria. We focus on the problem of\nmodeling non-linear relationships in regression data with potential hidden\ngraph-structured interactions between the high-dimensional predictors, within\nthe mixture of experts modeling framework. In order to deal with such a complex\nsituation, we investigate a block-diagonal localized mixture of polynomial\nexperts (BLoMPE) regression model, which is constructed upon an inverse\nregression and block-diagonal structures of the Gaussian expert covariance\nmatrices. We introduce a penalized maximum likelihood selection criterion to\nestimate the unknown conditional density of the regression model. This model\nselection criterion allows us to handle the challenging problem of inferring\nthe number of mixture components, the degree of polynomial mean functions, and\nthe hidden block-diagonal structures of the covariance matrices, which reduces\nthe number of parameters to be estimated and leads to a trade-off between\ncomplexity and sparsity in the model. In particular, we provide a strong\ntheoretical guarantee: a finite-sample oracle inequality satisfied by the\npenalized maximum likelihood estimator with a Jensen-Kullback-Leibler type\nloss, to support the introduced non-asymptotic model selection criterion. The\npenalty shape of this criterion depends on the complexity of the considered\nrandom subcollection of BLoMPE models, including the relevant graph structures,\nthe degree of polynomial mean functions, and the number of mixture components.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 21:32:20 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 21:05:06 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Nguyen", "TrungTin", ""], ["Chamroukhi", "Faicel", ""], ["Nguyen", "Hien Duy", ""], ["Forbes", "Florence", ""]]}, {"id": "2104.08962", "submitter": "Rakesh Gosangi", "authors": "Rakesh Gosangi, Ravneet Arora, Mohsen Gheisarieha, Debanjan Mahata,\n  Haimin Zhang", "title": "On the Use of Context for Predicting Citation Worthiness of Sentences in\n  Scholarly Articles", "comments": "To be published in the proceedings of NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper, we study the importance of context in predicting the citation\nworthiness of sentences in scholarly articles. We formulate this problem as a\nsequence labeling task solved using a hierarchical BiLSTM model. We contribute\na new benchmark dataset containing over two million sentences and their\ncorresponding labels. We preserve the sentence order in this dataset and\nperform document-level train/test splits, which importantly allows\nincorporating contextual information in the modeling process. We evaluate the\nproposed approach on three benchmark datasets. Our results quantify the\nbenefits of using context and contextual embeddings for citation worthiness.\nLastly, through error analysis, we provide insights into cases where context\nplays an essential role in predicting citation worthiness.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 21:47:30 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Gosangi", "Rakesh", ""], ["Arora", "Ravneet", ""], ["Gheisarieha", "Mohsen", ""], ["Mahata", "Debanjan", ""], ["Zhang", "Haimin", ""]]}, {"id": "2104.08963", "submitter": "Ly Trieu", "authors": "Ly Ly Trieu, Tran Cao Son, Enrico Pontelli, and Marcello Balduccini", "title": "Generating explanations for answer set programming applications", "comments": "Paper presented at SPIE 11746, Artificial Intelligence and Machine\n  Learning for Multi-Domain Operations Applications III, 117461L (12 April\n  2021), 14 pages. arXiv admin note: text overlap with arXiv:2008.01253", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an explanation system for applications that leverage Answer Set\nProgramming (ASP). Given a program P, an answer set A of P, and an atom a in\nthe program P, our system generates all explanation graphs of a which help\nexplain why a is true (or false) given the program P and the answer set A. We\nillustrate the functionality of the system using some examples from the\nliterature.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 21:47:40 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Trieu", "Ly Ly", ""], ["Son", "Tran Cao", ""], ["Pontelli", "Enrico", ""], ["Balduccini", "Marcello", ""]]}, {"id": "2104.08964", "submitter": "Luciana Benotti", "authors": "Luciana Benotti and Patrick Blackburn", "title": "A recipe for annotating grounded clarifications", "comments": "Accepted for publication at the 2021 Annual Conference of the North\n  American Chapter of the Association for Computational Linguistics (NAACL\n  2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In order to interpret the communicative intents of an utterance, it needs to\nbe grounded in something that is outside of language; that is, grounded in\nworld modalities. In this paper, we argue that dialogue clarification\nmechanisms make explicit the process of interpreting the communicative intents\nof the speaker's utterances by grounding them in the various modalities in\nwhich the dialogue is situated. This paper frames dialogue clarification\nmechanisms as an understudied research problem and a key missing piece in the\ngiant jigsaw puzzle of natural language understanding. We discuss both the\ntheoretical background and practical challenges posed by this problem and\npropose a recipe for obtaining grounding annotations. We conclude by\nhighlighting ethical issues that need to be addressed in future work.\n", "versions": [{"version": "v1", "created": "Sun, 18 Apr 2021 21:47:48 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Benotti", "Luciana", ""], ["Blackburn", "Patrick", ""]]}, {"id": "2104.08994", "submitter": "Ashutosh Dutta", "authors": "Ashutosh Dutta, Ehab Al-Shaer, and Samrat Chatterjee", "title": "Constraints Satisfiability Driven Reinforcement Learning for Autonomous\n  Cyber Defense", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.GT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the increasing system complexity and attack sophistication, the\nnecessity of autonomous cyber defense becomes vivid for cyber and\ncyber-physical systems (CPSs). Many existing frameworks in the current\nstate-of-the-art either rely on static models with unrealistic assumptions, or\nfail to satisfy the system safety and security requirements. In this paper, we\npresent a new hybrid autonomous agent architecture that aims to optimize and\nverify defense policies of reinforcement learning (RL) by incorporating\nconstraints verification (using satisfiability modulo theory (SMT)) into the\nagent's decision loop. The incorporation of SMT does not only ensure the\nsatisfiability of safety and security requirements, but also provides constant\nfeedback to steer the RL decision-making toward safe and effective actions.\nThis approach is critically needed for CPSs that exhibit high risk due to\nsafety or security violations. Our evaluation of the presented approach in a\nsimulated CPS environment shows that the agent learns the optimal policy fast\nand defeats diversified attack strategies in 99\\% cases.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 01:08:30 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Dutta", "Ashutosh", ""], ["Al-Shaer", "Ehab", ""], ["Chatterjee", "Samrat", ""]]}, {"id": "2104.09014", "submitter": "Pulasthi Wickramasinghe", "authors": "Pulasthi Wickramasinghe, Geoffrey Fox", "title": "Multidimensional Scaling for Gene Sequence Data with Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multidimensional scaling of gene sequence data has long played a vital role\nin analysing gene sequence data to identify clusters and patterns. However the\ncomputation complexities and memory requirements of state-of-the-art\ndimensional scaling algorithms make it infeasible to scale to large datasets.\nIn this paper we present an autoencoder-based dimensional reduction model which\ncan easily scale to datasets containing millions of gene sequences, while\nattaining results comparable to state-of-the-art MDS algorithms with minimal\nresource requirements. The model also supports out-of-sample data points with a\n99.5%+ accuracy based on our experiments. The proposed model is evaluated\nagainst DAMDS with a real world fungi gene sequence dataset. The presented\nresults showcase the effectiveness of the autoencoder-based dimension reduction\nmodel and its advantages.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 02:14:17 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Wickramasinghe", "Pulasthi", ""], ["Fox", "Geoffrey", ""]]}, {"id": "2104.09015", "submitter": "Shiyu Duan", "authors": "Shiyu Duan and Jose C. Principe", "title": "Labels, Information, and Computation: Efficient, Privacy-Preserving\n  Learning Using Sufficient Labels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In supervised learning, obtaining a large set of fully-labeled training data\nis expensive. We show that we do not always need full label information on\nevery single training example to train a competent classifier. Specifically,\ninspired by the principle of sufficiency in statistics, we present a statistic\n(a summary) of the fully-labeled training set that captures almost all the\nrelevant information for classification but at the same time is easier to\nobtain directly. We call this statistic \"sufficiently-labeled data\" and prove\nits sufficiency and efficiency for finding the optimal hidden representations,\non which competent classifier heads can be trained using as few as a single\nrandomly-chosen fully-labeled example per class. Sufficiently-labeled data can\nbe obtained from annotators directly without collecting the fully-labeled data\nfirst. And we prove that it is easier to directly obtain sufficiently-labeled\ndata than obtaining fully-labeled data. Furthermore, sufficiently-labeled data\nnaturally preserves user privacy by storing relative, instead of absolute,\ninformation. Extensive experimental results are provided to support our theory.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 02:15:25 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Duan", "Shiyu", ""], ["Principe", "Jose C.", ""]]}, {"id": "2104.09023", "submitter": "Hao Pan", "authors": "Lei Chu, Hao Pan, Wenping Wang", "title": "Unsupervised Shape Completion via Deep Prior in the Neural Tangent\n  Kernel Perspective", "comments": "This is the author's preprint; see the final publication at ACM TOG", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach for completing and reconstructing 3D shapes from\nincomplete scanned data by using deep neural networks. Rather than being\ntrained on supervised completion tasks and applied on a testing shape, the\nnetwork is optimized from scratch on the single testing shape, to fully adapt\nto the shape and complete the missing data using contextual guidance from the\nknown regions. The ability to complete missing data by an untrained neural\nnetwork is usually referred to as the deep prior. In this paper, we interpret\nthe deep prior from a neural tangent kernel (NTK) perspective and show that the\ncompleted shape patches by the trained CNN are naturally similar to existing\npatches, as they are proximate in the kernel feature space induced by NTK. The\ninterpretation allows us to design more efficient network structures and\nlearning mechanisms for the shape completion and reconstruction task. Being\nmore aware of structural regularities than both traditional and other\nunsupervised learning-based reconstruction methods, our approach completes\nlarge missing regions with plausible shapes and complements supervised\nlearning-based methods that use database priors by requiring no extra training\ndata set and showing flexible adaptation to a particular shape instance.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 02:41:15 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Chu", "Lei", ""], ["Pan", "Hao", ""], ["Wang", "Wenping", ""]]}, {"id": "2104.09024", "submitter": "Yao Wu", "authors": "Yao Wu and Jian Cao and Guandong Xu and Yudong Tan", "title": "TFROM: A Two-sided Fairness-Aware Recommendation Model for Both\n  Customers and Providers", "comments": "The 44th International ACM SIGIR Conference on Research and\n  Development in Information Retrieval", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At present, most research on the fairness of recommender systems is conducted\neither from the perspective of customers or from the perspective of product (or\nservice) providers. However, such a practice ignores the fact that when\nfairness is guaranteed to one side, the fairness and rights of the other side\nare likely to reduce. In this paper, we consider recommendation scenarios from\nthe perspective of two sides (customers and providers). From the perspective of\nproviders, we consider the fairness of the providers' exposure in recommender\nsystem. For customers, we consider the fairness of the reduced quality of\nrecommendation results due to the introduction of fairness measures. We\ntheoretically analyzed the relationship between recommendation quality,\ncustomers fairness, and provider fairness, and design a two-sided\nfairness-aware recommendation model (TFROM) for both customers and providers.\nSpecifically, we design two versions of TFROM for offline and online\nrecommendation. The effectiveness of the model is verified on three real-world\ndata sets. The experimental results show that TFROM provides better two-sided\nfairness while still maintaining a higher level of personalization than the\nbaseline algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 02:46:54 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Wu", "Yao", ""], ["Cao", "Jian", ""], ["Xu", "Guandong", ""], ["Tan", "Yudong", ""]]}, {"id": "2104.09040", "submitter": "Soroush Vosoughi Dr", "authors": "Aadil Islam, Weicheng Ma, Soroush Vosoughi", "title": "BigGreen at SemEval-2021 Task 1: Lexical Complexity Prediction with\n  Assembly Models", "comments": "In Proceedings of the 15th International Workshop on Semantic\n  Evaluation (SemEval-2021). Colocated with ACL-IJCNLP 2021", "journal-ref": null, "doi": "10.18653/v1/2021.semeval-1.86", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper describes a system submitted by team BigGreen to LCP 2021 for\npredicting the lexical complexity of English words in a given context. We\nassemble a feature engineering-based model with a deep neural network model\nfounded on BERT. While BERT itself performs competitively, our feature\nengineering-based model helps in extreme cases, eg. separating instances of\neasy and neutral difficulty. Our handcrafted features comprise a breadth of\nlexical, semantic, syntactic, and novel phonological measures. Visualizations\nof BERT attention maps offer insight into potential features that Transformers\nmodels may learn when fine-tuned for lexical complexity prediction. Our\nensembled predictions score reasonably well for the single word subtask, and we\ndemonstrate how they can be harnessed to perform well on the multi word\nexpression subtask too.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 04:05:50 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Islam", "Aadil", ""], ["Ma", "Weicheng", ""], ["Vosoughi", "Soroush", ""]]}, {"id": "2104.09058", "submitter": "Qinyuan Wu", "authors": "Qinyuan Wu and Yong Deng", "title": "A Negation Quantum Decision Model to Predict the Interference Effect in\n  Categorization", "comments": "27 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Categorization is a significant task in decision-making, which is a key part\nof human behavior. An interference effect is caused by categorization in some\ncases, which breaks the total probability principle. A negation quantum model\n(NQ model) is developed in this article to predict the interference. Taking the\nadvantage of negation to bring more information in the distribution from a\ndifferent perspective, the proposed model is a combination of the negation of a\nprobability distribution and the quantum decision model. Information of the\nphase contained in quantum probability and the special calculation method to it\ncan easily represented the interference effect. The results of the proposed NQ\nmodel is closely to the real experiment data and has less error than the\nexisted models.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 05:30:00 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Wu", "Qinyuan", ""], ["Deng", "Yong", ""]]}, {"id": "2104.09059", "submitter": "Fei Shen", "authors": "Fei Shen, Xin He, Mengwan Wei and Yi Xie", "title": "A Competitive Method to VIPriors Object Detection Challenge", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this report, we introduce the technical details of our submission to the\nVIPriors object detection challenge. Our solution is based on mmdetction of a\nstrong baseline open-source detection toolbox. Firstly, we introduce an\neffective data augmentation method to address the lack of data problem, which\ncontains bbox-jitter, grid-mask, and mix-up. Secondly, we present a robust\nregion of interest (ROI) extraction method to learn more significant ROI\nfeatures via embedding global context features. Thirdly, we propose a\nmulti-model integration strategy to refinement the prediction box, which\nweighted boxes fusion (WBF). Experimental results demonstrate that our approach\ncan significantly improve the average precision (AP) of object detection on the\nsubset of the COCO2017 dataset.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 05:33:39 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Shen", "Fei", ""], ["He", "Xin", ""], ["Wei", "Mengwan", ""], ["Xie", "Yi", ""]]}, {"id": "2104.09062", "submitter": "Jokin Labaien Soto", "authors": "Jokin Labaien, Ekhi Zugasti, Xabier De Carlos", "title": "DA-DGCEx: Ensuring Validity of Deep Guided Counterfactual Explanations\n  With Distribution-Aware Autoencoder Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Learning has become a very valuable tool in different fields, and no one\ndoubts the learning capacity of these models. Nevertheless, since Deep Learning\nmodels are often seen as black boxes due to their lack of interpretability,\nthere is a general mistrust in their decision-making process. To find a balance\nbetween effectiveness and interpretability, Explainable Artificial Intelligence\n(XAI) is gaining popularity in recent years, and some of the methods within\nthis area are used to generate counterfactual explanations. The process of\ngenerating these explanations generally consists of solving an optimization\nproblem for each input to be explained, which is unfeasible when real-time\nfeedback is needed. To speed up this process, some methods have made use of\nautoencoders to generate instant counterfactual explanations. Recently, a\nmethod called Deep Guided Counterfactual Explanations (DGCEx) has been\nproposed, which trains an autoencoder attached to a classification model, in\norder to generate straightforward counterfactual explanations. However, this\nmethod does not ensure that the generated counterfactual instances are close to\nthe data manifold, so unrealistic counterfactual instances may be generated. To\novercome this issue, this paper presents Distribution Aware Deep Guided\nCounterfactual Explanations (DA-DGCEx), which adds a term to the DGCEx cost\nfunction that penalizes out of distribution counterfactual instances.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 05:44:18 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 05:59:49 GMT"}, {"version": "v3", "created": "Thu, 22 Apr 2021 06:41:30 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Labaien", "Jokin", ""], ["Zugasti", "Ekhi", ""], ["De Carlos", "Xabier", ""]]}, {"id": "2104.09079", "submitter": "Yifei Ding", "authors": "Yifei Ding, Minping Jia, Qiuhua Miao, Yudong Cao", "title": "A novel Time-frequency Transformer and its Application in Fault\n  Diagnosis of Rolling Bearings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The scope of data-driven fault diagnosis models is greatly improved through\ndeep learning (DL). However, the classical convolution and recurrent structure\nhave their defects in computational efficiency and feature representation,\nwhile the latest Transformer architecture based on attention mechanism has not\nbeen applied in this field. To solve these problems, we propose a novel\ntime-frequency Transformer (TFT) model inspired by the massive success of\nstandard Transformer in sequence processing. Specially, we design a fresh\ntokenizer and encoder module to extract effective abstractions from the\ntime-frequency representation (TFR) of vibration signals. On this basis, a new\nend-to-end fault diagnosis framework based on time-frequency Transformer is\npresented in this paper. Through the case studies on bearing experimental\ndatasets, we constructed the optimal Transformer structure and verified the\nperformance of the diagnostic method. The superiority of the proposed method is\ndemonstrated in comparison with the benchmark model and other state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 06:53:31 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 05:29:19 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Ding", "Yifei", ""], ["Jia", "Minping", ""], ["Miao", "Qiuhua", ""], ["Cao", "Yudong", ""]]}, {"id": "2104.09083", "submitter": "Yidan Sun", "authors": "Yidan Sun, Guiyuan Jiang, Siew-Kei Lam, Peilan He, Fangxin Ning", "title": "Multi-fold Correlation Attention Network for Predicting Traffic Speeds\n  with Heterogeneous Frequency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Substantial efforts have been devoted to the investigation of spatiotemporal\ncorrelations for improving traffic speed prediction accuracy. However, existing\nworks typically model the correlations based solely on the observed traffic\nstate (e.g. traffic speed) without due consideration that different correlation\nmeasurements of the traffic data could exhibit a diverse set of patterns under\ndifferent traffic situations. In addition, the existing works assume that all\nroad segments can employ the same sampling frequency of traffic states, which\nis impractical. In this paper, we propose new measurements to model the spatial\ncorrelations among traffic data and show that the resulting correlation\npatterns vary significantly under various traffic situations. We propose a\nHeterogeneous Spatial Correlation (HSC) model to capture the spatial\ncorrelation based on a specific measurement, where the traffic data of varying\nroad segments can be heterogeneous (i.e. obtained with different sampling\nfrequency). We propose a Multi-fold Correlation Attention Network (MCAN), which\nrelies on the HSC model to explore multi-fold spatial correlations and leverage\nLSTM networks to capture multi-fold temporal correlations to provide\ndiscriminating features in order to achieve accurate traffic prediction. The\nlearned multi-fold spatiotemporal correlations together with contextual factors\nare fused with attention mechanism to make the final predictions. Experiments\non real-world datasets demonstrate that the proposed MCAN model outperforms the\nstate-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 06:58:51 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Sun", "Yidan", ""], ["Jiang", "Guiyuan", ""], ["Lam", "Siew-Kei", ""], ["He", "Peilan", ""], ["Ning", "Fangxin", ""]]}, {"id": "2104.09119", "submitter": "Jiangli Shao", "authors": "Jiangli Shao, Yongqing Wang, Hao Gao, Huawei Shen, Xueqi Cheng", "title": "Locate Who You Are: Matching Geo-location to Text for Anchor Link\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Nowadays, users are encouraged to activate across multiple online social\nnetworks simultaneously. Anchor link prediction, which aims to reveal the\ncorrespondence among different accounts of the same user across networks, has\nbeen regarded as a fundamental problem for user profiling, marketing,\ncybersecurity, and recommendation. Existing methods mainly address the\nprediction problem by utilizing profile, content, or structural features of\nusers in symmetric ways. However, encouraged by online services, users would\nalso post asymmetric information across networks, such as geo-locations and\ntexts. It leads to an emerged challenge in aligning users with asymmetric\ninformation across networks. Instead of similarity evaluation applied in\nprevious works, we formalize correlation between geo-locations and texts and\npropose a novel anchor link prediction framework for matching users across\nnetworks. Moreover, our model can alleviate the label scarcity problem by\nintroducing external data. Experimental results on real-world datasets show\nthat our approach outperforms existing methods and achieves state-of-the-art\nresults.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 08:15:28 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Shao", "Jiangli", ""], ["Wang", "Yongqing", ""], ["Gao", "Hao", ""], ["Shen", "Huawei", ""], ["Cheng", "Xueqi", ""]]}, {"id": "2104.09122", "submitter": "Zihan Ding", "authors": "Jie Ren, Yewen Li, Zihan Ding, Wei Pan and Hao Dong", "title": "Probabilistic Mixture-of-Experts for Efficient Deep Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep reinforcement learning (DRL) has successfully solved various problems\nrecently, typically with a unimodal policy representation. However, grasping\ndistinguishable skills for some tasks with non-unique optima can be essential\nfor further improving its learning efficiency and performance, which may lead\nto a multimodal policy represented as a mixture-of-experts (MOE). To our best\nknowledge, present DRL algorithms for general utility do not deploy this method\nas policy function approximators due to the potential challenge in its\ndifferentiability for policy learning. In this work, we propose a probabilistic\nmixture-of-experts (PMOE) implemented with a Gaussian mixture model (GMM) for\nmultimodal policy, together with a novel gradient estimator for the\nindifferentiability problem, which can be applied in generic off-policy and\non-policy DRL algorithms using stochastic policies, e.g., Soft Actor-Critic\n(SAC) and Proximal Policy Optimisation (PPO). Experimental results testify the\nadvantage of our method over unimodal polices and two different MOE methods, as\nwell as a method of option frameworks, based on the above two types of DRL\nalgorithms, on six MuJoCo tasks. Different gradient estimations for GMM like\nthe reparameterisation trick (Gumbel-Softmax) and the score-ratio trick are\nalso compared with our method. We further empirically demonstrate the\ndistinguishable primitives learned with PMOE and show the benefits of our\nmethod in terms of exploration.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 08:21:56 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Ren", "Jie", ""], ["Li", "Yewen", ""], ["Ding", "Zihan", ""], ["Pan", "Wei", ""], ["Dong", "Hao", ""]]}, {"id": "2104.09123", "submitter": "Laura D\\\"orr", "authors": "Laura D\\\"orr, Felix Brandt, Alexander Naumann, Martin Pouls", "title": "TetraPackNet: Four-Corner-Based Object Detection in Logistics Use-Cases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  While common image object detection tasks focus on bounding boxes or\nsegmentation masks as object representations, we consider the problem of\nfinding objects based on four arbitrary vertices. We propose a novel model,\nnamed TetraPackNet, to tackle this problem. TetraPackNet is based on CornerNet\nand uses similar algorithms and ideas. It is designated for applications\nrequiring high-accuracy detection of regularly shaped objects, which is the\ncase in the logistics use-case of packaging structure recognition. We evaluate\nour model on our specific real-world dataset for this use-case. Baselined\nagainst a previous solution, consisting of a Mask R-CNN model and suitable\npost-processing steps, TetraPackNet achieves superior results (9% higher in\naccuracy) in the sub-task of four-corner based transport unit side detection.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 08:22:14 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 08:52:49 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["D\u00f6rr", "Laura", ""], ["Brandt", "Felix", ""], ["Naumann", "Alexander", ""], ["Pouls", "Martin", ""]]}, {"id": "2104.09124", "submitter": "Jia-Xin Zhuang", "authors": "Yuting Gao, Jia-Xin Zhuang, Ke Li, Hao Cheng, Xiaowei Guo, Feiyue\n  Huang, Rongrong Ji, Xing Sun", "title": "DisCo: Remedy Self-supervised Learning on Lightweight Models with\n  Distilled Contrastive Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While self-supervised representation learning (SSL) has received widespread\nattention from the community, recent research argue that its performance will\nsuffer a cliff fall when the model size decreases. The current method mainly\nrelies on contrastive learning to train the network and in this work, we\npropose a simple yet effective Distilled Contrastive Learning (DisCo) to ease\nthe issue by a large margin. Specifically, we find the final embedding obtained\nby the mainstream SSL methods contains the most fruitful information, and\npropose to distill the final embedding to maximally transmit a teacher's\nknowledge to a lightweight model by constraining the last embedding of the\nstudent to be consistent with that of the teacher. In addition, in the\nexperiment, we find that there exists a phenomenon termed Distilling BottleNeck\nand present to enlarge the embedding dimension to alleviate this problem. Our\nmethod does not introduce any extra parameter to lightweight models during\ndeployment. Experimental results demonstrate that our method achieves the\nstate-of-the-art on all lightweight models. Particularly, when\nResNet-101/ResNet-50 is used as teacher to teach EfficientNet-B0, the linear\nresult of EfficientNet-B0 on ImageNet is very close to ResNet-101/ResNet-50,\nbut the number of parameters of EfficientNet-B0 is only 9.4%/16.3% of\nResNet-101/ResNet-50.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 08:22:52 GMT"}, {"version": "v2", "created": "Wed, 14 Jul 2021 11:29:35 GMT"}], "update_date": "2021-07-15", "authors_parsed": [["Gao", "Yuting", ""], ["Zhuang", "Jia-Xin", ""], ["Li", "Ke", ""], ["Cheng", "Hao", ""], ["Guo", "Xiaowei", ""], ["Huang", "Feiyue", ""], ["Ji", "Rongrong", ""], ["Sun", "Xing", ""]]}, {"id": "2104.09130", "submitter": "Piotr Skowron", "authors": "Piotr Faliszewski and Piotr Skowron and Nimrod Talmon", "title": "Bribery as a Measure of Candidate Success: Complexity Results for\n  Approval-Based Multiwinner Rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of bribery in multiwinner elections, for the case where\nthe voters cast approval ballots (i.e., sets of candidates they approve) and\nthe bribery actions are limited to: adding an approval to a vote, deleting an\napproval from a vote, or moving an approval within a vote from one candidate to\nthe other. We consider a number of approval-based multiwinner rules (AV, SAV,\nGAV, RAV, approval-based Chamberlin--Courant, and PAV). We find the landscape\nof complexity results quite rich, going from polynomial-time algorithms through\nNP-hardness with constant-factor approximations, to outright inapproximability.\nMoreover, in general, our problems tend to be easier when we limit out bribery\nactions on increasing the number of approvals of the candidate that we want to\nbe in a winning committee (i.e., adding approvals only for this preferred\ncandidate, or moving approvals only to him or her). We also study parameterized\ncomplexity of our problems, with a focus on parameterizations by the numbers of\nvoters or candidates.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 08:26:40 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Faliszewski", "Piotr", ""], ["Skowron", "Piotr", ""], ["Talmon", "Nimrod", ""]]}, {"id": "2104.09163", "submitter": "Louis Annabi", "authors": "Louis Annabi, Alexandre Pitti, Mathias Quoy", "title": "Bidirectional Interaction between Visual and Motor Generative Models\n  using Predictive Coding and Active Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this work, we build upon the Active Inference (AIF) and Predictive Coding\n(PC) frameworks to propose a neural architecture comprising a generative model\nfor sensory prediction, and a distinct generative model for motor trajectories.\nWe highlight how sequences of sensory predictions can act as rails guiding\nlearning, control and online adaptation of motor trajectories. We furthermore\ninquire the effects of bidirectional interactions between the motor and the\nvisual modules. The architecture is tested on the control of a simulated\nrobotic arm learning to reproduce handwritten letters.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 09:41:31 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Annabi", "Louis", ""], ["Pitti", "Alexandre", ""], ["Quoy", "Mathias", ""]]}, {"id": "2104.09203", "submitter": "Mingfu Xue", "authors": "Shichang Sun, Mingfu Xue, Jian Wang, Weiqiang Liu", "title": "Protecting the Intellectual Properties of Deep Neural Networks with an\n  Additional Class and Steganographic Images", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, the research on protecting the intellectual properties (IP) of deep\nneural networks (DNN) has attracted serious concerns. A number of DNN copyright\nprotection methods have been proposed. However, most of the existing\nwatermarking methods focus on verifying the copyright of the model, which do\nnot support the authentication and management of users' fingerprints, thus can\nnot satisfy the requirements of commercial copyright protection. In addition,\nthe query modification attack which was proposed recently can invalidate most\nof the existing backdoor-based watermarking methods. To address these\nchallenges, in this paper, we propose a method to protect the intellectual\nproperties of DNN models by using an additional class and steganographic\nimages. Specifically, we use a set of watermark key samples to embed an\nadditional class into the DNN, so that the watermarked DNN will classify the\nwatermark key sample as the predefined additional class in the copyright\nverification stage. We adopt the least significant bit (LSB) image\nsteganography to embed users' fingerprints into watermark key images. Each user\nwill be assigned with a unique fingerprint image so that the user's identity\ncan be authenticated later. Experimental results demonstrate that, the proposed\nmethod can protect the copyright of DNN models effectively. On Fashion-MNIST\nand CIFAR-10 datasets, the proposed method can obtain 100% watermark accuracy\nand 100% fingerprint authentication success rate. In addition, the proposed\nmethod is demonstrated to be robust to the model fine-tuning attack, model\npruning attack, and the query modification attack. Compared with three existing\nwatermarking methods (the logo-based, noise-based, and adversarial frontier\nstitching watermarking methods), the proposed method has better performance on\nwatermark accuracy and robustness against the query modification attack.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 11:03:53 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Sun", "Shichang", ""], ["Xue", "Mingfu", ""], ["Wang", "Jian", ""], ["Liu", "Weiqiang", ""]]}, {"id": "2104.09216", "submitter": "Jiacheng Chen", "authors": "Jiacheng Chen, Bin-Bin Gao, Zongqing Lu, Jing-Hao Xue, Chengjie Wang,\n  Qingmin Liao", "title": "SCNet: Enhancing Few-Shot Semantic Segmentation by Self-Contrastive\n  Background Prototypes", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Few-shot semantic segmentation aims to segment novel-class objects in a query\nimage with only a few annotated examples in support images. Most of advanced\nsolutions exploit a metric learning framework that performs segmentation\nthrough matching each pixel to a learned foreground prototype. However, this\nframework suffers from biased classification due to incomplete construction of\nsample pairs with the foreground prototype only. To address this issue, in this\npaper, we introduce a complementary self-contrastive task into few-shot\nsemantic segmentation. Our new model is able to associate the pixels in a\nregion with the prototype of this region, no matter they are in the foreground\nor background. To this end, we generate self-contrastive background prototypes\ndirectly from the query image, with which we enable the construction of\ncomplete sample pairs and thus a complementary and auxiliary segmentation task\nto achieve the training of a better segmentation model. Extensive experiments\non PASCAL-5$^i$ and COCO-20$^i$ demonstrate clearly the superiority of our\nproposal. At no expense of inference efficiency, our model achieves\nstate-of-the results in both 1-shot and 5-shot settings for few-shot semantic\nsegmentation.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 11:21:47 GMT"}, {"version": "v2", "created": "Sun, 25 Apr 2021 12:15:10 GMT"}, {"version": "v3", "created": "Wed, 28 Apr 2021 04:29:59 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Chen", "Jiacheng", ""], ["Gao", "Bin-Bin", ""], ["Lu", "Zongqing", ""], ["Xue", "Jing-Hao", ""], ["Wang", "Chengjie", ""], ["Liao", "Qingmin", ""]]}, {"id": "2104.09224", "submitter": "Aditya Prakash", "authors": "Aditya Prakash, Kashyap Chitta, Andreas Geiger", "title": "Multi-Modal Fusion Transformer for End-to-End Autonomous Driving", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How should representations from complementary sensors be integrated for\nautonomous driving? Geometry-based sensor fusion has shown great promise for\nperception tasks such as object detection and motion forecasting. However, for\nthe actual driving task, the global context of the 3D scene is key, e.g. a\nchange in traffic light state can affect the behavior of a vehicle\ngeometrically distant from that traffic light. Geometry alone may therefore be\ninsufficient for effectively fusing representations in end-to-end driving\nmodels. In this work, we demonstrate that imitation learning policies based on\nexisting sensor fusion methods under-perform in the presence of a high density\nof dynamic agents and complex scenarios, which require global contextual\nreasoning, such as handling traffic oncoming from multiple directions at\nuncontrolled intersections. Therefore, we propose TransFuser, a novel\nMulti-Modal Fusion Transformer, to integrate image and LiDAR representations\nusing attention. We experimentally validate the efficacy of our approach in\nurban settings involving complex scenarios using the CARLA urban driving\nsimulator. Our approach achieves state-of-the-art driving performance while\nreducing collisions by 76% compared to geometry-based fusion.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 11:48:13 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Prakash", "Aditya", ""], ["Chitta", "Kashyap", ""], ["Geiger", "Andreas", ""]]}, {"id": "2104.09225", "submitter": "Hariharan Manikandan", "authors": "Anshul Tanwar, Hariharan Manikandan, Krishna Sundaresan, Prasanna\n  Ganesan, Sathish Kumar Chandrasekaran, Sriram Ravi", "title": "Multi-context Attention Fusion Neural Network for Software Vulnerability\n  Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Security issues in shipped code can lead to unforeseen device malfunction,\nsystem crashes or malicious exploitation by crackers, post-deployment. These\nvulnerabilities incur a cost of repair and foremost risk the credibility of the\ncompany. It is rewarding when these issues are detected and fixed well ahead of\ntime, before release. Common Weakness Estimation (CWE) is a nomenclature\ndescribing general vulnerability patterns observed in C code. In this work, we\npropose a deep learning model that learns to detect some of the common\ncategories of security vulnerabilities in source code efficiently. The AI\narchitecture is an Attention Fusion model, that combines the effectiveness of\nrecurrent, convolutional and self-attention networks towards decoding the\nvulnerability hotspots in code. Utilizing the code AST structure, our model\nbuilds an accurate understanding of code semantics with a lot less learnable\nparameters. Besides a novel way of efficiently detecting code vulnerability, an\nadditional novelty in this model is to exactly point to the code sections,\nwhich were deemed vulnerable by the model. Thus helping a developer to quickly\nfocus on the vulnerable code sections; and this becomes the \"explainable\" part\nof the vulnerability detection. The proposed AI achieves 98.40% F1-score on\nspecific CWEs from the benchmarked NIST SARD dataset and compares well with\nstate of the art.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 11:50:36 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Tanwar", "Anshul", ""], ["Manikandan", "Hariharan", ""], ["Sundaresan", "Krishna", ""], ["Ganesan", "Prasanna", ""], ["Chandrasekaran", "Sathish Kumar", ""], ["Ravi", "Sriram", ""]]}, {"id": "2104.09233", "submitter": "Reza Khani Shekarab", "authors": "Reza Khani-Shekarab, Alireza khani-shekarab", "title": "Comprehensive systematic review into combinations of artificial\n  intelligence, human factors, and automation", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Artificial intelligence (AI)-based models used to improve different fields\nincluding healthcare, and finance. One of the field that receive advantages of\nAI is automation. However, it is important to consider human factors in\napplication of AI in automation. This paper reports on a systematic review of\nthe published studies used to investigate the application of AI in PM. This\ncomprehensive systematic review used ScienceDirect to identify relevant\narticles. Of the 422 articles found, 40 met the inclusion and exclusion\ncriteria and were used in the review. Selected articles were classified based\non categories of human factors and areas of application. The results indicated\nthat application of AI in automation with respect to human factors could be\ndivided into three areas of physical ergonomics, cognitive ergonomic and\norganizational ergonomics. The main areas of application in physical and\ncognitive ergonomics are including transportation, User experience, and\nhuman-machine interactions.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 19:01:15 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Khani-Shekarab", "Reza", ""], ["khani-shekarab", "Alireza", ""]]}, {"id": "2104.09342", "submitter": "Grigory Malinovsky", "authors": "Grigory Malinovsky, Alibek Sailanbayev, Peter Richt\\'arik", "title": "Random Reshuffling with Variance Reduction: New Analysis and Better\n  Rates", "comments": "24 pages, 5 figures, 4 algorithms, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtually all state-of-the-art methods for training supervised machine\nlearning models are variants of SGD enhanced with a number of additional\ntricks, such as minibatching, momentum, and adaptive stepsizes. One of the\ntricks that works so well in practice that it is used as default in virtually\nall widely used machine learning software is {\\em random reshuffling (RR)}.\nHowever, the practical benefits of RR have until very recently been eluding\nattempts at being satisfactorily explained using theory. Motivated by recent\ndevelopment due to Mishchenko, Khaled and Richt\\'{a}rik (2020), in this work we\nprovide the first analysis of SVRG under Random Reshuffling (RR-SVRG) for\ngeneral finite-sum problems. First, we show that RR-SVRG converges linearly\nwith the rate $\\mathcal{O}(\\kappa^{3/2})$ in the strongly-convex case, and can\nbe improved further to $\\mathcal{O}(\\kappa)$ in the big data regime (when $n >\n\\mathcal{O}(\\kappa)$), where $\\kappa$ is the condition number. This improves\nupon the previous best rate $\\mathcal{O}(\\kappa^2)$ known for a variance\nreduced RR method in the strongly-convex case due to Ying, Yuan and Sayed\n(2020). Second, we obtain the first sublinear rate for general convex problems.\nThird, we establish similar fast rates for Cyclic-SVRG and Shuffle-Once-SVRG.\nFinally, we develop and analyze a more general variance reduction scheme for\nRR, which allows for less frequent updates of the control variate. We\ncorroborate our theoretical results with suitably chosen experiments on\nsynthetic and real datasets.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 14:30:10 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Malinovsky", "Grigory", ""], ["Sailanbayev", "Alibek", ""], ["Richt\u00e1rik", "Peter", ""]]}, {"id": "2104.09350", "submitter": "Alessandro Sebastianelli", "authors": "Alessandro Sebastianelli, Maria Pia Del Rosso, Silvia Liberata Ullo", "title": "A SAR speckle filter based on Residual Convolutional Neural Networks", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Machine Learning (ML) algorithms have become widespread in\nall fields of Remote Sensing (RS) and Earth Observation (EO). This has allowed\na rapid development of new procedures to solve problems affecting these\nsectors. In this context, the authors of this work aim to present a novel\nmethod for filtering the speckle noise from Sentinel-1 data by applying Deep\nLearning (DL) algorithms, based on Convolutional Neural Networks (CNNs). The\nobtained results, if compared with the state of the art, show a clear\nimprovement in terms of Peak Signal-to-Noise Ratio (PSNR) and Structural\nSimilarity Index ({SSIM}), by proving the effectiveness of the proposed\narchitecture. Moreover, the generated open-source code and dataset have been\nmade available for further developments and investigation by interested\nresearchers.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 14:43:07 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Sebastianelli", "Alessandro", ""], ["Del Rosso", "Maria Pia", ""], ["Ullo", "Silvia Liberata", ""]]}, {"id": "2104.09367", "submitter": "Haiyan Wu", "authors": "Haiyan Wu, Yanyun Qu, Shaohui Lin, Jian Zhou, Ruizhi Qiao, Zhizhong\n  Zhang, Yuan Xie, Lizhuang Ma", "title": "Contrastive Learning for Compact Single Image Dehazing", "comments": "CVPR 2021 accepted paper. Code: https://github.com/GlassyWu/AECR-Net", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Single image dehazing is a challenging ill-posed problem due to the severe\ninformation degeneration. However, existing deep learning based dehazing\nmethods only adopt clear images as positive samples to guide the training of\ndehazing network while negative information is unexploited. Moreover, most of\nthem focus on strengthening the dehazing network with an increase of depth and\nwidth, leading to a significant requirement of computation and memory. In this\npaper, we propose a novel contrastive regularization (CR) built upon\ncontrastive learning to exploit both the information of hazy images and clear\nimages as negative and positive samples, respectively. CR ensures that the\nrestored image is pulled to closer to the clear image and pushed to far away\nfrom the hazy image in the representation space. Furthermore, considering\ntrade-off between performance and memory storage, we develop a compact dehazing\nnetwork based on autoencoder-like (AE) framework. It involves an adaptive mixup\noperation and a dynamic feature enhancement module, which can benefit from\npreserving information flow adaptively and expanding the receptive field to\nimprove the network's transformation capability, respectively. We term our\ndehazing network with autoencoder and contrastive regularization as AECR-Net.\nThe extensive experiments on synthetic and real-world datasets demonstrate that\nour AECR-Net surpass the state-of-the-art approaches. The code is released in\nhttps://github.com/GlassyWu/AECR-Net.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 14:56:21 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Wu", "Haiyan", ""], ["Qu", "Yanyun", ""], ["Lin", "Shaohui", ""], ["Zhou", "Jian", ""], ["Qiao", "Ruizhi", ""], ["Zhang", "Zhizhong", ""], ["Xie", "Yuan", ""], ["Ma", "Lizhuang", ""]]}, {"id": "2104.09369", "submitter": "Lyuyi Zhu", "authors": "Lyuyi Zhu, Kairui Feng, Ziyuan Pu, Wei Ma", "title": "Adversarial Diffusion Attacks on Graph-based Traffic Prediction Models", "comments": "Our code is available at\n  https://github.com/LYZ98/Adversarial-Diffusion-Attacks-on-Graph-based-Traffic-Prediction-Models", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time traffic prediction models play a pivotal role in smart mobility\nsystems and have been widely used in route guidance, emerging mobility\nservices, and advanced traffic management systems. With the availability of\nmassive traffic data, neural network-based deep learning methods, especially\nthe graph convolutional networks (GCN) have demonstrated outstanding\nperformance in mining spatio-temporal information and achieving high prediction\naccuracy. Recent studies reveal the vulnerability of GCN under adversarial\nattacks, while there is a lack of studies to understand the vulnerability\nissues of the GCN-based traffic prediction models. Given this, this paper\nproposes a new task -- diffusion attack, to study the robustness of GCN-based\ntraffic prediction models. The diffusion attack aims to select and attack a\nsmall set of nodes to degrade the performance of the entire prediction model.\nTo conduct the diffusion attack, we propose a novel attack algorithm, which\nconsists of two major components: 1) approximating the gradient of the\nblack-box prediction model with Simultaneous Perturbation Stochastic\nApproximation (SPSA); 2) adapting the knapsack greedy algorithm to select the\nattack nodes. The proposed algorithm is examined with three GCN-based traffic\nprediction models: St-Gcn, T-Gcn, and A3t-Gcn on two cities. The proposed\nalgorithm demonstrates high efficiency in the adversarial attack tasks under\nvarious scenarios, and it can still generate adversarial samples under the drop\nregularization such as DropOut, DropNode, and DropEdge. The research outcomes\ncould help to improve the robustness of the GCN-based traffic prediction models\nand better protect the smart mobility systems. Our code is available at\nhttps://github.com/LYZ98/Adversarial-Diffusion-Attacks-on-Graph-based-Traffic-Prediction-Models\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 14:57:25 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Zhu", "Lyuyi", ""], ["Feng", "Kairui", ""], ["Pu", "Ziyuan", ""], ["Ma", "Wei", ""]]}, {"id": "2104.09376", "submitter": "Chuxiong Sun", "authors": "Chuxiong Sun, Hongming Gu, Jie Hu", "title": "Scalable and Adaptive Graph Neural Networks with Self-Label-Enhanced\n  training", "comments": "23 pages, 13 figures, fixed typos, add authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is hard to directly implement Graph Neural Networks (GNNs) on large scaled\ngraphs. Besides of existed neighbor sampling techniques, scalable methods\ndecoupling graph convolutions and other learnable transformations into\npreprocessing and post classifier allow normal minibatch training. By replacing\nredundant concatenation operation with attention mechanism in SIGN, we propose\nScalable and Adaptive Graph Neural Networks (SAGN). SAGN can adaptively gather\nneighborhood information among different hops. To further improve scalable\nmodels on semi-supervised learning tasks, we propose Self-Label-Enhance (SLE)\nframework combining self-training approach and label propagation in depth. We\nadd base model with a scalable node label module. Then we iteratively train\nmodels and enhance train set in several stages. To generate input of node label\nmodule, we directly apply label propagation based on one-hot encoded label\nvectors without inner random masking. We find out that empirically the label\nleakage has been effectively alleviated after graph convolutions. The hard\npseudo labels in enhanced train set participate in label propagation with true\nlabels. Experiments on both inductive and transductive datasets demonstrate\nthat, compared with other sampling-based and sampling-free methods, SAGN\nachieves better or comparable results and SLE can further improve performance.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 15:08:06 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 07:18:50 GMT"}, {"version": "v3", "created": "Thu, 1 Jul 2021 10:08:16 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Sun", "Chuxiong", ""], ["Gu", "Hongming", ""], ["Hu", "Jie", ""]]}, {"id": "2104.09393", "submitter": "Bhaskar Mitra", "authors": "Bhaskar Mitra, Sebastian Hofstatter, Hamed Zamani and Nick Craswell", "title": "Improving Transformer-Kernel Ranking Model Using Conformer and Query\n  Term Independence", "comments": "arXiv admin note: substantial text overlap with arXiv:2007.10434", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Transformer-Kernel (TK) model has demonstrated strong reranking\nperformance on the TREC Deep Learning benchmark -- and can be considered to be\nan efficient (but slightly less effective) alternative to other\nTransformer-based architectures that employ (i) large-scale pretraining (high\ntraining cost), (ii) joint encoding of query and document (high inference\ncost), and (iii) larger number of Transformer layers (both high training and\nhigh inference costs). Since, a variant of the TK model -- called TKL -- has\nbeen developed that incorporates local self-attention to efficiently process\nlonger input sequences in the context of document ranking. In this work, we\npropose a novel Conformer layer as an alternative approach to scale TK to\nlonger input sequences. Furthermore, we incorporate query term independence and\nexplicit term matching to extend the model to the full retrieval setting. We\nbenchmark our models under the strictly blind evaluation setting of the TREC\n2020 Deep Learning track and find that our proposed architecture changes lead\nto improved retrieval quality over TKL. Our best model also outperforms all\nnon-neural runs (\"trad\") and two-thirds of the pretrained Transformer-based\nruns (\"nnlm\") on NDCG@10.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 15:32:34 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Mitra", "Bhaskar", ""], ["Hofstatter", "Sebastian", ""], ["Zamani", "Hamed", ""], ["Craswell", "Nick", ""]]}, {"id": "2104.09395", "submitter": "Li Wang", "authors": "Marta Li Wang", "title": "Algoritmos de miner\\'ia de datos en la industria sanitaria", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In this paper, we review data mining approaches for health applications. Our\nfocus is on hardware-centric approaches. Modern computers consist of multiple\nprocessors, each equipped with multiple cores, each with a set of\narithmetic/logical units. Thus, a modern computer may be composed of several\nthousand units capable of doing arithmetic operations like addition and\nmultiplication. Graphic processors, in addition may offer some thousand such\nunits. In both cases, single instruction multiple data and multiple instruction\nmultiple data parallelism must be exploited. We review the principles of\nalgorithms which exploit this parallelism and focus also on the memory issues\nwhen multiple processing units access main memory through caches. This is\nimportant for many applications of health, such as ECG, EEG, CT, SPECT, fMRI,\nDTI, ultrasound, microscopy, dermascopy, etc.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 15:36:13 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Wang", "Marta Li", ""]]}, {"id": "2104.09399", "submitter": "Bhaskar Mitra", "authors": "Nick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, Ellen M.\n  Voorhees and Ian Soboroff", "title": "TREC Deep Learning Track: Reusable Test Collections in the Large Data\n  Regime", "comments": "arXiv admin note: text overlap with arXiv:2003.07820", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The TREC Deep Learning (DL) Track studies ad hoc search in the large data\nregime, meaning that a large set of human-labeled training data is available.\nResults so far indicate that the best models with large data may be deep neural\nnetworks. This paper supports the reuse of the TREC DL test collections in\nthree ways. First we describe the data sets in detail, documenting clearly and\nin one place some details that are otherwise scattered in track guidelines,\noverview papers and in our associated MS MARCO leaderboard pages. We intend\nthis description to make it easy for newcomers to use the TREC DL data. Second,\nbecause there is some risk of iteration and selection bias when reusing a data\nset, we describe the best practices for writing a paper using TREC DL data,\nwithout overfitting. We provide some illustrative analysis. Finally we address\na number of issues around the TREC DL data, including an analysis of\nreusability.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 15:41:28 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Craswell", "Nick", ""], ["Mitra", "Bhaskar", ""], ["Yilmaz", "Emine", ""], ["Campos", "Daniel", ""], ["Voorhees", "Ellen M.", ""], ["Soboroff", "Ian", ""]]}, {"id": "2104.09402", "submitter": "Wenling Shang", "authors": "Wenling Shang, Lasse Espeholt, Anton Raichuk, Tim Salimans", "title": "Agent-Centric Representations for Multi-Agent Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Object-centric representations have recently enabled significant progress in\ntackling relational reasoning tasks. By building a strong object-centric\ninductive bias into neural architectures, recent efforts have improved\ngeneralization and data efficiency of machine learning algorithms for these\nproblems. One problem class involving relational reasoning that still remains\nunder-explored is multi-agent reinforcement learning (MARL). Here we\ninvestigate whether object-centric representations are also beneficial in the\nfully cooperative MARL setting. Specifically, we study two ways of\nincorporating an agent-centric inductive bias into our RL algorithm: 1.\nIntroducing an agent-centric attention module with explicit connections across\nagents 2. Adding an agent-centric unsupervised predictive objective (i.e. not\nusing action labels), to be used as an auxiliary loss for MARL, or as the basis\nof a pre-training step. We evaluate these approaches on the Google Research\nFootball environment as well as DeepMind Lab 2D. Empirically, agent-centric\nrepresentation learning leads to the emergence of more complex cooperation\nstrategies between agents as well as enhanced sample efficiency and\ngeneralization.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 15:43:40 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Shang", "Wenling", ""], ["Espeholt", "Lasse", ""], ["Raichuk", "Anton", ""], ["Salimans", "Tim", ""]]}, {"id": "2104.09428", "submitter": "Jamal Al Qundus", "authors": "Jamal Al Qundus, Silvio Peikert, Adrian Paschke", "title": "AI supported Topic Modeling using KNIME-Workflows", "comments": "7 pages, 7 figures. Qurator2020 - Conference on Digital Curation\n  Technologies", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Topic modeling algorithms traditionally model topics as list of weighted\nterms. These topic models can be used effectively to classify texts or to\nsupport text mining tasks such as text summarization or fact extraction. The\ngeneral procedure relies on statistical analysis of term frequencies. The focus\nof this work is on the implementation of the knowledge-based topic modelling\nservices in a KNIME workflow. A brief description and evaluation of the\nDBPedia-based enrichment approach and the comparative evaluation of enriched\ntopic models will be outlined based on our previous work. DBpedia-Spotlight is\nused to identify entities in the input text and information from DBpedia is\nused to extend these entities. We provide a workflow developed in KNIME\nimplementing this approach and perform a result comparison of topic modeling\nsupported by knowledge base information to traditional LDA. This topic modeling\napproach allows semantic interpretation both by algorithms and by humans.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 10:19:58 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Qundus", "Jamal Al", ""], ["Peikert", "Silvio", ""], ["Paschke", "Adrian", ""]]}, {"id": "2104.09435", "submitter": "Jong Chul Ye", "authors": "Hyoungjun Park, Myeongsu Na, Bumju Kim, Soohyun Park, Ki Hean Kim,\n  Sunghoe Chang, and Jong Chul Ye", "title": "Deep learning enables reference-free isotropic super-resolution for\n  volumetric fluorescence microscopy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Volumetric imaging by fluorescence microscopy is often limited by anisotropic\nspatial resolution from inferior axial resolution compared to the lateral\nresolution. To address this problem, here we present a deep-learning-enabled\nunsupervised super-resolution technique that enhances anisotropic images in\nvolumetric fluorescence microscopy. In contrast to the existing deep learning\napproaches that require matched high-resolution target volume images, our\nmethod greatly reduces the effort to put into practice as the training of a\nnetwork requires as little as a single 3D image stack, without a priori\nknowledge of the image formation process, registration of training data, or\nseparate acquisition of target data. This is achieved based on the optimal\ntransport driven cycle-consistent generative adversarial network that learns\nfrom an unpaired matching between high-resolution 2D images in lateral image\nplane and low-resolution 2D images in the other planes. Using fluorescence\nconfocal microscopy and light-sheet microscopy, we demonstrate that the trained\nnetwork not only enhances axial resolution, but also restores suppressed visual\ndetails between the imaging planes and removes imaging artifacts.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 16:31:12 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 02:45:47 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Park", "Hyoungjun", ""], ["Na", "Myeongsu", ""], ["Kim", "Bumju", ""], ["Park", "Soohyun", ""], ["Kim", "Ki Hean", ""], ["Chang", "Sunghoe", ""], ["Ye", "Jong Chul", ""]]}, {"id": "2104.09447", "submitter": "Guy Ben-Yosef", "authors": "Guy Ben-Yosef, Gabriel Kreiman, Shimon Ullman", "title": "What can human minimal videos tell us about dynamic recognition models?", "comments": "Published as a workshop paper at Bridging AI and Cognitive Science\n  (ICLR 2020). Extended paper was published at Cognition", "journal-ref": null, "doi": "10.1016/j.cognition.2020.104263", "report-no": null, "categories": "cs.CV cs.AI q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In human vision objects and their parts can be visually recognized from\npurely spatial or purely temporal information but the mechanisms integrating\nspace and time are poorly understood. Here we show that human visual\nrecognition of objects and actions can be achieved by efficiently combining\nspatial and motion cues in configurations where each source on its own is\ninsufficient for recognition. This analysis is obtained by identifying minimal\nvideos: these are short and tiny video clips in which objects, parts, and\nactions can be reliably recognized, but any reduction in either space or time\nmakes them unrecognizable. State-of-the-art deep networks for dynamic visual\nrecognition cannot replicate human behavior in these configurations. This gap\nbetween humans and machines points to critical mechanisms in human dynamic\nvision that are lacking in current models.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 16:53:25 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Ben-Yosef", "Guy", ""], ["Kreiman", "Gabriel", ""], ["Ullman", "Shimon", ""]]}, {"id": "2104.09460", "submitter": "Willie Neiswanger", "authors": "Willie Neiswanger, Ke Alexander Wang, Stefano Ermon", "title": "Bayesian Algorithm Execution: Estimating Computable Properties of\n  Black-box Functions Using Mutual Information", "comments": "Appears in Proceedings of the 38th International Conference on\n  Machine Learning (ICML), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.IT cs.LG cs.NE math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world problems, we want to infer some property of an expensive\nblack-box function $f$, given a budget of $T$ function evaluations. One example\nis budget constrained global optimization of $f$, for which Bayesian\noptimization is a popular method. Other properties of interest include local\noptima, level sets, integrals, or graph-structured information induced by $f$.\nOften, we can find an algorithm $\\mathcal{A}$ to compute the desired property,\nbut it may require far more than $T$ queries to execute. Given such an\n$\\mathcal{A}$, and a prior distribution over $f$, we refer to the problem of\ninferring the output of $\\mathcal{A}$ using $T$ evaluations as Bayesian\nAlgorithm Execution (BAX). To tackle this problem, we present a procedure,\nInfoBAX, that sequentially chooses queries that maximize mutual information\nwith respect to the algorithm's output. Applying this to Dijkstra's algorithm,\nfor instance, we infer shortest paths in synthetic and real-world graphs with\nblack-box edge costs. Using evolution strategies, we yield variants of Bayesian\noptimization that target local, rather than global, optima. On these problems,\nInfoBAX uses up to 500 times fewer queries to $f$ than required by the original\nalgorithm. Our method is closely connected to other Bayesian optimal\nexperimental design procedures such as entropy search methods and optimal\nsensor placement using Gaussian processes.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 17:22:11 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 17:56:46 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Neiswanger", "Willie", ""], ["Wang", "Ke Alexander", ""], ["Ermon", "Stefano", ""]]}, {"id": "2104.09461", "submitter": "Xiang Xiang", "authors": "Xin Wei, Runqi Qiu, Houyu Yu, Yurun Yang, Haoyu Tian, Xiang Xiang", "title": "Entropy-based Optimization via A* Algorithm for Parking Space\n  Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper addresses the path planning problems for recommending parking\nspaces, given the difficulties of identifying the most optimal route to vacant\nparking spaces and the shortest time to leave the parking space. Our\noptimization approach is based on the entropy method and realized by the A*\nalgorithm. Experiments have shown that the combination of A* and the entropy\nvalue induces the optimal parking solution with the shortest route while being\nrobust to environmental factors.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 17:24:51 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Wei", "Xin", ""], ["Qiu", "Runqi", ""], ["Yu", "Houyu", ""], ["Yang", "Yurun", ""], ["Tian", "Haoyu", ""], ["Xiang", "Xiang", ""]]}, {"id": "2104.09469", "submitter": "Spencer Frazier", "authors": "Md Sultan Al Nahian, Spencer Frazier, Brent Harrison, Mark Riedl", "title": "Training Value-Aligned Reinforcement Learning Agents Using a Normative\n  Prior", "comments": "(Nahian and Frazier contributed equally to this work)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As more machine learning agents interact with humans, it is increasingly a\nprospect that an agent trained to perform a task optimally, using only a\nmeasure of task performance as feedback, can violate societal norms for\nacceptable behavior or cause harm. Value alignment is a property of intelligent\nagents wherein they solely pursue non-harmful behaviors or human-beneficial\ngoals. We introduce an approach to value-aligned reinforcement learning, in\nwhich we train an agent with two reward signals: a standard task performance\nreward, plus a normative behavior reward. The normative behavior reward is\nderived from a value-aligned prior model previously shown to classify text as\nnormative or non-normative. We show how variations on a policy shaping\ntechnique can balance these two sources of reward and produce policies that are\nboth effective and perceived as being more normative. We test our\nvalue-alignment technique on three interactive text-based worlds; each world is\ndesigned specifically to challenge agents with a task as well as provide\nopportunities to deviate from the task to engage in normative and/or altruistic\nbehavior.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 17:33:07 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Nahian", "Md Sultan Al", ""], ["Frazier", "Spencer", ""], ["Harrison", "Brent", ""], ["Riedl", "Mark", ""]]}, {"id": "2104.09489", "submitter": "Gasper Begus", "authors": "Ga\\v{s}per Begu\\v{s} and Alan Zhou", "title": "Interpreting intermediate convolutional layers of CNNs trained on raw\n  speech", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a technique to interpret and visualize intermediate\nlayers in CNNs trained on raw speech data in an unsupervised manner. We show\nthat averaging over feature maps after ReLU activation in each convolutional\nlayer yields interpretable time-series data. The proposed technique enables\nacoustic analysis of intermediate convolutional layers. To uncover how\nmeaningful representation in speech gets encoded in intermediate layers of\nCNNs, we manipulate individual latent variables to marginal levels outside of\nthe training range. We train and probe internal representations on two models\n-- a bare WaveGAN architecture and a ciwGAN extension which forces the\nGenerator to output informative data and results in emergence of linguistically\nmeaningful representations. Interpretation and visualization is performed for\nthree basic acoustic properties of speech: periodic vibration (corresponding to\nvowels), aperiodic noise vibration (corresponding to fricatives), and silence\n(corresponding to stops). We also argue that the proposed technique allows\nacoustic analysis of intermediate layers that parallels the acoustic analysis\nof human speech data: we can extract F0, intensity, duration, formants, and\nother acoustic properties from intermediate layers in order to test where and\nhow CNNs encode various types of information. The models are trained on two\nspeech processes with different degrees of complexity: a simple presence of [s]\nand a computationally complex presence of reduplication (copied material).\nObserving the causal effect between interpolation and the resulting changes in\nintermediate layers can reveal how individual variables get transformed into\nspikes in activation in intermediate layers. Using the proposed technique, we\ncan analyze how linguistically meaningful units in speech get encoded in\ndifferent convolutional layers.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 17:52:06 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 17:43:29 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Begu\u0161", "Ga\u0161per", ""], ["Zhou", "Alan", ""]]}, {"id": "2104.09492", "submitter": "Rodolfo Garc\\'ia Berm\\'udez", "authors": "Camilo Vel\\'azquez-Rodr\\'iguez, Rodolfo Garc\\'ia-Berm\\'udez, Fernando\n  Rojas-Ruiz, Roberto Becerra-Garc\\'ia, Luis Vel\\'azquez", "title": "Automatic glissade determination through a mathematical model in\n  electrooculographic records", "comments": null, "journal-ref": "Bioinformatics and Biomedical Engineering. Springer International\n  Publishing; 2017. p. 546-56. (Lecture Notes in Computer Science)", "doi": "10.1007/978-3-319-56148-6_49", "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The glissadic overshoot is characterized by an unwanted type of movement\nknown as glissades. The glissades are a short ocular movement that describe the\nfailure of the neural programming of saccades to move the eyes in order to\nreach a specific target. In this paper we develop a procedure to determine if a\nspecific saccade have a glissade appended to the end of it. The use of the\nthird partial sum of the Gauss series as mathematical model, a comparison\nbetween some specific parameters and the RMSE error are the steps made to reach\nthis goal. Finally a machine learning algorithm is trained, returning expected\nresponses of the presence or not of this kind of ocular movement.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 17:56:55 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Vel\u00e1zquez-Rodr\u00edguez", "Camilo", ""], ["Garc\u00eda-Berm\u00fadez", "Rodolfo", ""], ["Rojas-Ruiz", "Fernando", ""], ["Becerra-Garc\u00eda", "Roberto", ""], ["Vel\u00e1zquez", "Luis", ""]]}, {"id": "2104.09494", "submitter": "Gabriel Mittag", "authors": "Gabriel Mittag, Babak Naderi, Assmaa Chehadi, Sebastian M\\\"oller", "title": "NISQA: A Deep CNN-Self-Attention Model for Multidimensional Speech\n  Quality Prediction with Crowdsourced Datasets", "comments": "Submitted to Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.AI cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an update to the NISQA speech quality prediction\nmodel that is focused on distortions that occur in communication networks. In\ncontrast to the previous version, the model is trained end-to-end and the\ntime-dependency modelling and time-pooling is achieved through a Self-Attention\nmechanism. Besides overall speech quality, the model also predicts the four\nspeech quality dimensions Noisiness, Coloration, Discontinuity, and Loudness,\nand in this way gives more insight into the cause of a quality degradation.\nFurthermore, new datasets with over 13,000 speech files were created for\ntraining and validation of the model. The model was finally tested on a new,\nlive-talking test dataset that contains recordings of real telephone calls.\nOverall, NISQA was trained and evaluated on 81 datasets from different sources\nand showed to provide reliable predictions also for unknown speech samples. The\ncode, model weights, and datasets are open-sourced.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 17:56:59 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Mittag", "Gabriel", ""], ["Naderi", "Babak", ""], ["Chehadi", "Assmaa", ""], ["M\u00f6ller", "Sebastian", ""]]}, {"id": "2104.09500", "submitter": "Arthur Bra\\v{z}inskas", "authors": "Arthur Bra\\v{z}inskas, Mengwen Liu, Ramesh Nallapati, Sujith Ravi,\n  Markus Dreyer", "title": "Transductive Learning for Abstractive News Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pre-trained language models have recently advanced abstractive summarization.\nThese models are further fine-tuned on human-written references before summary\ngeneration in test time. In this work, we propose the first application of\ntransductive learning to summarization. In this paradigm, a model can learn\nfrom the test set's input before inference. To perform transduction, we propose\nto utilize input document summarizing sentences to construct references for\nlearning in test time. These sentences are often compressed and fused to form\nabstractive summaries and provide omitted details and additional context to the\nreader. We show that our approach yields state-of-the-art results on CNN/DM and\nNYT datasets. For instance, we achieve over 1 ROUGE-L point improvement on\nCNN/DM. Further, we show the benefits of transduction from older to more recent\nnews. Finally, through human and automatic evaluation, we show that our\nsummaries become more abstractive and coherent.\n", "versions": [{"version": "v1", "created": "Sat, 17 Apr 2021 17:33:12 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Bra\u017einskas", "Arthur", ""], ["Liu", "Mengwen", ""], ["Nallapati", "Ramesh", ""], ["Ravi", "Sujith", ""], ["Dreyer", "Markus", ""]]}, {"id": "2104.09554", "submitter": "Lior Vassertail", "authors": "Adi Haviv, Lior Vassertail and Omer Levy", "title": "Can Latent Alignments Improve Autoregressive Machine Translation?", "comments": "Accepted to NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Latent alignment objectives such as CTC and AXE significantly improve\nnon-autoregressive machine translation models. Can they improve autoregressive\nmodels as well? We explore the possibility of training autoregressive machine\ntranslation models with latent alignment objectives, and observe that, in\npractice, this approach results in degenerate models. We provide a theoretical\nexplanation for these empirical results, and prove that latent alignment\nobjectives are incompatible with teacher forcing.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 18:31:56 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Haviv", "Adi", ""], ["Vassertail", "Lior", ""], ["Levy", "Omer", ""]]}, {"id": "2104.09557", "submitter": "Dylan Cope", "authors": "Dylan Cope and Nandi Schoots", "title": "Learning to Communicate with Strangers via Channel Randomisation Methods", "comments": null, "journal-ref": "4th Workshop on Emergent Communication at NeurIPS 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce two methods for improving the performance of agents meeting for\nthe first time to accomplish a communicative task. The methods are: (1)\n`message mutation' during the generation of the communication protocol; and (2)\nrandom permutations of the communication channel. These proposals are tested\nusing a simple two-player game involving a `teacher' who generates a\ncommunication protocol and sends a message, and a `student' who interprets the\nmessage. After training multiple agents via self-play we analyse the\nperformance of these agents when they are matched with a stranger, i.e. their\nzero-shot communication performance. We find that both message mutation and\nchannel permutation positively influence performance, and we discuss their\neffects.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 18:42:48 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Cope", "Dylan", ""], ["Schoots", "Nandi", ""]]}, {"id": "2104.09570", "submitter": "Shuaicheng Zhang", "authors": "Shuaicheng Zhang, Lifu Huang, Qiang Ning", "title": "Extracting Temporal Event Relation with Syntactic-Guided Temporal Graph\n  Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Extracting temporal relations (e.g., before, after, concurrent) among events\nis crucial to natural language understanding. Previous studies mainly rely on\nneural networks to learn effective features or manual-crafted linguistic\nfeatures for temporal relation extraction, which usually fail when the context\nbetween two events is complex or wide. Inspired by the examination of available\ntemporal relation annotations and human-like cognitive procedures, we propose a\nnew Temporal Graph Transformer network to (1) explicitly find the connection\nbetween two events from a syntactic graph constructed from one or two\ncontinuous sentences, and (2) automatically locate the most indicative temporal\ncues from the path of the two event mentions as well as their surrounding\nconcepts in the syntactic graph with a new temporal-oriented attention\nmechanism. Experiments on MATRES and TB-Dense datasets show that our approach\nsignificantly outperforms previous state-of-the-art methods on both end-to-end\ntemporal relation extraction and temporal relation classification.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 19:00:45 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Zhang", "Shuaicheng", ""], ["Huang", "Lifu", ""], ["Ning", "Qiang", ""]]}, {"id": "2104.09574", "submitter": "Pei Zhou", "authors": "Pei Zhou, Pegah Jandaghi, Bill Yuchen Lin, Justin Cho, Jay Pujara,\n  Xiang Ren", "title": "Probing Causal Common Sense in Dialogue Response Generation", "comments": "This article has been withdrawn by the authors. The submitted version\n  was an early draft that has errors in the results which renders the analysis\n  invalid", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Communication is a cooperative effort that requires reaching mutual\nunderstanding among the participants. Humans use commonsense reasoning\nimplicitly to produce natural and logically-coherent responses. As a step\ntowards fluid human-AI communication, we study if response generation (RG)\nmodels can emulate human reasoning process and use common sense to help produce\nbetter-quality responses. We aim to tackle two research questions: how to\nformalize conversational common sense and how to examine RG models capability\nto use common sense? We first propose a task, CEDAR: Causal common sEnse in\nDiAlogue Response generation, that concretizes common sense as textual\nexplanations for what might lead to the response and evaluates RG models\nbehavior by comparing the modeling loss given a valid explanation with an\ninvalid one. Then we introduce a process that automatically generates such\nexplanations and ask humans to verify them. Finally, we design two probing\nsettings for RG models targeting two reasoning capabilities using verified\nexplanations. We find that RG models have a hard time determining the logical\nvalidity of explanations but can identify grammatical naturalness of the\nexplanation easily.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 19:10:05 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 20:00:17 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Zhou", "Pei", ""], ["Jandaghi", "Pegah", ""], ["Lin", "Bill Yuchen", ""], ["Cho", "Justin", ""], ["Pujara", "Jay", ""], ["Ren", "Xiang", ""]]}, {"id": "2104.09580", "submitter": "Jialu Li", "authors": "Jialu Li, Hao Tan, Mohit Bansal", "title": "Improving Cross-Modal Alignment in Vision Language Navigation via\n  Syntactic Information", "comments": "NAACL 2021 (10 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision language navigation is the task that requires an agent to navigate\nthrough a 3D environment based on natural language instructions. One key\nchallenge in this task is to ground instructions with the current visual\ninformation that the agent perceives. Most of the existing work employs soft\nattention over individual words to locate the instruction required for the next\naction. However, different words have different functions in a sentence (e.g.,\nmodifiers convey attributes, verbs convey actions). Syntax information like\ndependencies and phrase structures can aid the agent to locate important parts\nof the instruction. Hence, in this paper, we propose a navigation agent that\nutilizes syntax information derived from a dependency tree to enhance alignment\nbetween the instruction and the current visual scenes. Empirically, our agent\noutperforms the baseline model that does not use syntax information on the\nRoom-to-Room dataset, especially in the unseen environment. Besides, our agent\nachieves the new state-of-the-art on Room-Across-Room dataset, which contains\ninstructions in 3 languages (English, Hindi, and Telugu). We also show that our\nagent is better at aligning instructions with the current visual information\nvia qualitative visualizations. Code and models:\nhttps://github.com/jialuli-luka/SyntaxVLN\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 19:18:41 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Li", "Jialu", ""], ["Tan", "Hao", ""], ["Bansal", "Mohit", ""]]}, {"id": "2104.09586", "submitter": "Hamed Jelodar", "authors": "Hamed Jelodar, Richard Frank", "title": "Semantic Knowledge Discovery and Discussion Mining of Incel Online\n  Community: Topic modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online forums provide a unique opportunity for online users to share comments\nand exchange information on a particular topic. Understanding user behaviour is\nvaluable to organizations and has applications for social and security\nstrategies, for instance, identifying user opinions within a community or\npredicting future behaviour. Discovering the semantic aspects in Incel forums\nare the main goal of this research; we apply Natural language processing\ntechniques based on topic modeling to latent topic discovery and opinion mining\nof users from a popular online Incel discussion forum. To prepare the input\ndata for our study, we extracted the comments from Incels.co. The research\nexperiments show that Artificial Intelligence (AI) based on NLP models can be\neffective for semantic and emotion knowledge discovery and retrieval of useful\ninformation from the Incel community. For example, we discovered\nsemantic-related words that describe issues within a large volume of Incel\ncomments, which is difficult with manual methods.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 19:39:07 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 16:57:14 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Jelodar", "Hamed", ""], ["Frank", "Richard", ""]]}, {"id": "2104.09612", "submitter": "Ronal Singh", "authors": "Ronal Singh, Upol Ehsan, Marc Cheong, Mark O. Riedl, Tim Miller", "title": "LEx: A Framework for Operationalising Layers of Machine Learning\n  Explanations", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several social factors impact how people respond to AI explanations used to\njustify AI decisions affecting them personally. In this position paper, we\ndefine a framework called the \\textit{layers of explanation} (LEx), a lens\nthrough which we can assess the appropriateness of different types of\nexplanations. The framework uses the notions of \\textit{sensitivity} (emotional\nresponsiveness) of features and the level of \\textit{stakes} (decision's\nconsequence) in a domain to determine whether different types of explanations\nare \\textit{appropriate} in a given context. We demonstrate how to use the\nframework to assess the appropriateness of different types of explanations in\ndifferent domains.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 23:31:04 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Singh", "Ronal", ""], ["Ehsan", "Upol", ""], ["Cheong", "Marc", ""], ["Riedl", "Mark O.", ""], ["Miller", "Tim", ""]]}, {"id": "2104.09620", "submitter": "Caleb Bowyer", "authors": "Caleb Bowyer", "title": "Predictor-Corrector(PC) Temporal Difference(TD) Learning (PCTD)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Using insight from numerical approximation of ODEs and the problem\nformulation and solution methodology of TD learning through a Galerkin\nrelaxation, I propose a new class of TD learning algorithms. After applying the\nimproved numerical methods, the parameter being approximated has a guaranteed\norder of magnitude reduction in the Taylor Series error of the solution to the\nODE for the parameter $\\theta(t)$ that is used in constructing the linearly\nparameterized value function. Predictor-Corrector Temporal Difference (PCTD) is\nwhat I call the translated discrete time Reinforcement Learning(RL) algorithm\nfrom the continuous time ODE using the theory of Stochastic Approximation(SA).\nBoth causal and non-causal implementations of the algorithm are provided, and\nsimulation results are listed for an infinite horizon task to compare the\noriginal TD(0) algorithm against both versions of PCTD(0).\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 18:54:16 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Bowyer", "Caleb", ""]]}, {"id": "2104.09627", "submitter": "Mo Han", "authors": "Mo Han, Mehrshad Zandigohar, Mariusz P. Furmanek, Mathew Yarossi,\n  Gunar Schirner, Deniz Erdogmus", "title": "Segmentation and Classification of EMG Time-Series During Reach-to-Grasp\n  Motion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The electromyography (EMG) signals have been widely utilized in human robot\ninteraction for extracting user hand and arm motion instructions. A major\nchallenge of the online interaction with robots is the reliable EMG recognition\nfrom real-time data. However, previous studies mainly focused on using\nsteady-state EMG signals with a small number of grasp patterns to implement\nclassification algorithms, which is insufficient to generate robust control\nregarding the dynamic muscular activity variation in practice. Introducing more\nEMG variability during training and validation could implement a better\ndynamic-motion detection, but only limited research focused on such\ngrasp-movement identification, and all of those assessments on the non-static\nEMG classification require supervised ground-truth label of the movement\nstatus. In this study, we propose a framework for classifying EMG signals\ngenerated from continuous grasp movements with variations on dynamic arm/hand\npostures, using an unsupervised motion status segmentation method. We collected\ndata from large gesture vocabularies with multiple dynamic motion phases to\nencode the transitions from one intent to another based on common sequences of\nthe grasp movements. Two classifiers were constructed for identifying the\nmotion-phase label and grasp-type label, where the dynamic motion phases were\nsegmented and labeled in an unsupervised manner. The proposed framework was\nevaluated in real-time with the accuracy variation over time presented, which\nwas shown to be efficient due to the high degree of freedom of the EMG data.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 20:41:06 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Han", "Mo", ""], ["Zandigohar", "Mehrshad", ""], ["Furmanek", "Mariusz P.", ""], ["Yarossi", "Mathew", ""], ["Schirner", "Gunar", ""], ["Erdogmus", "Deniz", ""]]}, {"id": "2104.09630", "submitter": "Danilo Comminiello", "authors": "Eleonora Grassucci, Edoardo Cicero, Danilo Comminiello", "title": "Quaternion Generative Adversarial Networks", "comments": "Accepted as a Chapter for the SPRINGER book \"Generative Adversarial\n  Learning: Architectures and Applications\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Latest Generative Adversarial Networks (GANs) are gathering outstanding\nresults through a large-scale training, thus employing models composed of\nmillions of parameters requiring extensive computational capabilities. Building\nsuch huge models undermines their replicability and increases the training\ninstability. Moreover, multi-channel data, such as images or audio, are usually\nprocessed by realvalued convolutional networks that flatten and concatenate the\ninput, often losing intra-channel spatial relations. To address these issues\nrelated to complexity and information loss, we propose a family of\nquaternion-valued generative adversarial networks (QGANs). QGANs exploit the\nproperties of quaternion algebra, e.g., the Hamilton product, that allows to\nprocess channels as a single entity and capture internal latent relations,\nwhile reducing by a factor of 4 the overall number of parameters. We show how\nto design QGANs and to extend the proposed approach even to advanced models.We\ncompare the proposed QGANs with real-valued counterparts on several image\ngeneration benchmarks. Results show that QGANs are able to obtain better FID\nscores than real-valued GANs and to generate visually pleasing images.\nFurthermore, QGANs save up to 75% of the training parameters. We believe these\nresults may pave the way to novel, more accessible, GANs capable of improving\nperformance and saving computational resources.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 20:46:18 GMT"}, {"version": "v2", "created": "Tue, 27 Jul 2021 15:30:42 GMT"}], "update_date": "2021-07-28", "authors_parsed": [["Grassucci", "Eleonora", ""], ["Cicero", "Edoardo", ""], ["Comminiello", "Danilo", ""]]}, {"id": "2104.09644", "submitter": "Yanshan Wang", "authors": "Bhavani Singh Agnikula Kshatriya, Nicolas A Nunez, Manuel Gardea-\n  Resendez, Euijung Ryu, Brandon J Coombes, Sunyang Fu, Mark A Frye, Joanna M\n  Biernacka, Yanshan Wang", "title": "Neural Language Models with Distant Supervision to Identify Major\n  Depressive Disorder from Clinical Notes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Major depressive disorder (MDD) is a prevalent psychiatric disorder that is\nassociated with significant healthcare burden worldwide. Phenotyping of MDD can\nhelp early diagnosis and consequently may have significant advantages in\npatient management. In prior research MDD phenotypes have been extracted from\nstructured Electronic Health Records (EHR) or using Electroencephalographic\n(EEG) data with traditional machine learning models to predict MDD phenotypes.\nHowever, MDD phenotypic information is also documented in free-text EHR data,\nsuch as clinical notes. While clinical notes may provide more accurate\nphenotyping information, natural language processing (NLP) algorithms must be\ndeveloped to abstract such information. Recent advancements in NLP resulted in\nstate-of-the-art neural language models, such as Bidirectional Encoder\nRepresentations for Transformers (BERT) model, which is a transformer-based\nmodel that can be pre-trained from a corpus of unsupervised text data and then\nfine-tuned on specific tasks. However, such neural language models have been\nunderutilized in clinical NLP tasks due to the lack of large training datasets.\nIn the literature, researchers have utilized the distant supervision paradigm\nto train machine learning models on clinical text classification tasks to\nmitigate the issue of lacking annotated training data. It is still unknown\nwhether the paradigm is effective for neural language models. In this paper, we\npropose to leverage the neural language models in a distant supervision\nparadigm to identify MDD phenotypes from clinical notes. The experimental\nresults indicate that our proposed approach is effective in identifying MDD\nphenotypes and that the Bio- Clinical BERT, a specific BERT model for clinical\ndata, achieved the best performance in comparison with conventional machine\nlearning models.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 21:11:41 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Kshatriya", "Bhavani Singh Agnikula", ""], ["Nunez", "Nicolas A", ""], ["Resendez", "Manuel Gardea-", ""], ["Ryu", "Euijung", ""], ["Coombes", "Brandon J", ""], ["Fu", "Sunyang", ""], ["Frye", "Mark A", ""], ["Biernacka", "Joanna M", ""], ["Wang", "Yanshan", ""]]}, {"id": "2104.09667", "submitter": "Ilia Shumailov", "authors": "Ilia Shumailov, Zakhar Shumaylov, Dmitry Kazhdan, Yiren Zhao, Nicolas\n  Papernot, Murat A. Erdogdu, Ross Anderson", "title": "Manipulating SGD with Data Ordering Attacks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning is vulnerable to a wide variety of attacks. It is now well\nunderstood that by changing the underlying data distribution, an adversary can\npoison the model trained with it or introduce backdoors. In this paper we\npresent a novel class of training-time attacks that require no changes to the\nunderlying dataset or model architecture, but instead only change the order in\nwhich data are supplied to the model. In particular, we find that the attacker\ncan either prevent the model from learning, or poison it to learn behaviours\nspecified by the attacker. Furthermore, we find that even a single\nadversarially-ordered epoch can be enough to slow down model learning, or even\nto reset all of the learning progress. Indeed, the attacks presented here are\nnot specific to the model or dataset, but rather target the stochastic nature\nof modern learning procedures. We extensively evaluate our attacks on computer\nvision and natural language benchmarks to find that the adversary can disrupt\nmodel training and even introduce backdoors.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 22:17:27 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 10:22:15 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Shumailov", "Ilia", ""], ["Shumaylov", "Zakhar", ""], ["Kazhdan", "Dmitry", ""], ["Zhao", "Yiren", ""], ["Papernot", "Nicolas", ""], ["Erdogdu", "Murat A.", ""], ["Anderson", "Ross", ""]]}, {"id": "2104.09683", "submitter": "Pierre Lison", "authors": "Pierre Lison and Jeremy Barnes and Aliaksandr Hubin", "title": "skweak: Weak Supervision Made Easy for NLP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present skweak, a versatile, Python-based software toolkit enabling NLP\ndevelopers to apply weak supervision to a wide range of NLP tasks. Weak\nsupervision is an emerging machine learning paradigm based on a simple idea:\ninstead of labelling data points by hand, we use labelling functions derived\nfrom domain knowledge to automatically obtain annotations for a given dataset.\nThe resulting labels are then aggregated with a generative model that estimates\nthe accuracy (and possible confusions) of each labelling function. The skweak\ntoolkit makes it easy to implement a large spectrum of labelling functions\n(such as heuristics, gazetteers, neural models or linguistic constraints) on\ntext data, apply them on a corpus, and aggregate their results in a fully\nunsupervised fashion. skweak is especially designed to facilitate the use of\nweak supervision for NLP tasks such as text classification and sequence\nlabelling. We illustrate the use of skweak for NER and sentiment analysis.\nskweak is released under an open-source license and is available at:\nhttps://github.com/NorskRegnesentral/skweak\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 23:26:51 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Lison", "Pierre", ""], ["Barnes", "Jeremy", ""], ["Hubin", "Aliaksandr", ""]]}, {"id": "2104.09713", "submitter": "Jing Zhang", "authors": "Hong Wen and Jing Zhang and Fuyu Lv and Wentian Bao and Tianyi Wang\n  and Zulong Chen", "title": "Hierarchically Modeling Micro and Macro Behaviors via Multi-Task\n  Learning for Conversion Rate Prediction", "comments": "Accepted as SIGIR 2021 short paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversion Rate (\\emph{CVR}) prediction in modern industrial e-commerce\nplatforms is becoming increasingly important, which directly contributes to the\nfinal revenue. In order to address the well-known sample selection bias\n(\\emph{SSB}) and data sparsity (\\emph{DS}) issues encountered during CVR\nmodeling, the abundant labeled macro behaviors ($i.e.$, user's interactions\nwith items) are used. Nonetheless, we observe that several purchase-related\nmicro behaviors ($i.e.$, user's interactions with specific components on the\nitem detail page) can supplement fine-grained cues for \\emph{CVR} prediction.\nMotivated by this observation, we propose a novel \\emph{CVR} prediction method\nby Hierarchically Modeling both Micro and Macro behaviors ($HM^3$).\nSpecifically, we first construct a complete user sequential behavior graph to\nhierarchically represent micro behaviors and macro behaviors as one-hop and\ntwo-hop post-click nodes. Then, we embody $HM^3$ as a multi-head deep neural\nnetwork, which predicts six probability variables corresponding to explicit\nsub-paths in the graph. They are further combined into the prediction targets\nof four auxiliary tasks as well as the final $CVR$ according to the conditional\nprobability rule defined on the graph. By employing multi-task learning and\nleveraging the abundant supervisory labels from micro and macro behaviors,\n$HM^3$ can be trained end-to-end and address the \\emph{SSB} and \\emph{DS}\nissues. Extensive experiments on both offline and online settings demonstrate\nthe superiority of the proposed $HM^3$ over representative state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 01:45:06 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Wen", "Hong", ""], ["Zhang", "Jing", ""], ["Lv", "Fuyu", ""], ["Bao", "Wentian", ""], ["Wang", "Tianyi", ""], ["Chen", "Zulong", ""]]}, {"id": "2104.09753", "submitter": "Daowen Qiu", "authors": "Daowen Qiu", "title": "Supervisory Control of Quantum Discrete Event Systems", "comments": "17 pages, 8 figures, minor typos are corrected; comments are welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.AI cs.FL cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discrete event systems (DES) have been established and deeply developed in\nthe framework of probabilistic and fuzzy computing models due to the necessity\nof practical applications in fuzzy and probabilistic systems. With the\ndevelopment of quantum computing and quantum control, a natural problem is to\nsimulate DES by means of quantum computing models and to establish {\\it quantum\nDES} (QDES). The motivation is twofold: on the one hand, QDES have potential\napplications when DES are simulated and processed by quantum computers, where\nquantum systems are employed to simulate the evolution of states driven by\ndiscrete events, and on the other hand, QDES may have essential advantages over\nDES concerning state complexity for imitating some practical problems. The goal\nof this paper is to establish a basic framework of QDES by using {\\it quantum\nfinite automata} (QFA) as the modelling formalisms, and the supervisory control\ntheorems of QDES are established and proved. Then we present a polynomial-time\nalgorithm to decide whether or not the controllability condition holds. In\nparticular, we construct a number of new examples of QFA to illustrate the\nsupervisory control of QDES and to verify the essential advantages of QDES over\nDES in state complexity.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 04:17:41 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 16:48:46 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Qiu", "Daowen", ""]]}, {"id": "2104.09757", "submitter": "Kai Yi", "authors": "Mohamed Elhoseiny, Divyansh Jha, Kai Yi, Ivan Skorokhodov", "title": "Imaginative Walks: Generative Random Walk Deviation Loss for Improved\n  Unseen Learning Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel loss for generative models, dubbed as GRaWD (Generative\nRandom Walk Deviation), to improve learning representations of unexplored\nvisual spaces. Quality learning representation of unseen classes (or styles) is\ncrucial to facilitate novel image generation and better generative\nunderstanding of unseen visual classes (a.k.a. Zero-Shot Learning, ZSL). By\ngenerating representations of unseen classes from their semantic descriptions,\nsuch as attributes or text, Generative ZSL aims at identifying unseen\ncategories discriminatively from seen ones. We define GRaWD by constructing a\ndynamic graph, including the seen class/style centers and generated samples in\nthe current mini-batch. Our loss starts a random walk probability from each\ncenter through visual generations produced from hallucinated unseen classes. As\na deviation signal, we encourage the random walk to eventually land after t\nsteps in a feature representation that is hard to classify to any of the seen\nclasses. We show that our loss can improve unseen class representation quality\non four text-based ZSL benchmarks on CUB and NABirds datasets and three\nattribute-based ZSL benchmarks on AWA2, SUN, and aPY datasets. We also study\nour loss's ability to produce meaningful novel visual art generations on\nWikiArt dataset. Our experiments and human studies show that our loss can\nimprove StyleGAN1 and StyleGAN2 generation quality, creating novel art that is\nsignificantly more preferred. Code will be made available.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 04:34:28 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Elhoseiny", "Mohamed", ""], ["Jha", "Divyansh", ""], ["Yi", "Kai", ""], ["Skorokhodov", "Ivan", ""]]}, {"id": "2104.09758", "submitter": "Keval Doshi", "authors": "Keval Doshi, Yasin Yilmaz", "title": "An Efficient Approach for Anomaly Detection in Traffic Videos", "comments": "Accepted to CVPR 2021 - AI City Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Due to its relevance in intelligent transportation systems, anomaly detection\nin traffic videos has recently received much interest. It remains a difficult\nproblem due to a variety of factors influencing the video quality of a\nreal-time traffic feed, such as temperature, perspective, lighting conditions,\nand so on. Even though state-of-the-art methods perform well on the available\nbenchmark datasets, they need a large amount of external training data as well\nas substantial computational resources. In this paper, we propose an efficient\napproach for a video anomaly detection system which is capable of running at\nthe edge devices, e.g., on a roadside camera. The proposed approach comprises a\npre-processing module that detects changes in the scene and removes the\ncorrupted frames, a two-stage background modelling module and a two-stage\nobject detector. Finally, a backtracking anomaly detection algorithm computes a\nsimilarity statistic and decides on the onset time of the anomaly. We also\npropose a sequential change detection algorithm that can quickly adapt to a new\nscene and detect changes in the similarity statistic. Experimental results on\nthe Track 4 test set of the 2021 AI City Challenge show the efficacy of the\nproposed framework as we achieve an F1-score of 0.9157 along with 8.4027 root\nmean square error (RMSE) and are ranked fourth in the competition.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 04:43:18 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Doshi", "Keval", ""], ["Yilmaz", "Yasin", ""]]}, {"id": "2104.09780", "submitter": "Yu Liu", "authors": "Yu Liu, Quanming Yao, Yong Li", "title": "Role-Aware Modeling for N-ary Relational Knowledge Bases", "comments": "WWW2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  N-ary relational knowledge bases (KBs) represent knowledge with binary and\nbeyond-binary relational facts. Especially, in an n-ary relational fact, the\ninvolved entities play different roles, e.g., the ternary relation\nPlayCharacterIn consists of three roles, ACTOR, CHARACTER and MOVIE. However,\nexisting approaches are often directly extended from binary relational KBs,\ni.e., knowledge graphs, while missing the important semantic property of role.\nTherefore, we start from the role level, and propose a Role-Aware Modeling, RAM\nfor short, for facts in n-ary relational KBs. RAM explores a latent space that\ncontains basis vectors, and represents roles by linear combinations of these\nvectors. This way encourages semantically related roles to have close\nrepresentations. RAM further introduces a pattern matrix that captures the\ncompatibility between the role and all involved entities. To this end, it\npresents a multilinear scoring function to measure the plausibility of a fact\ncomposed by certain roles and entities. We show that RAM achieves both\ntheoretical full expressiveness and computation efficiency, which also provides\nan elegant generalization for approaches in binary relational KBs. Experiments\ndemonstrate that RAM outperforms representative baselines on both n-ary and\nbinary relational datasets.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 06:37:22 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Liu", "Yu", ""], ["Yao", "Quanming", ""], ["Li", "Yong", ""]]}, {"id": "2104.09785", "submitter": "Glenn Ceusters", "authors": "Glenn Ceusters, Rom\\'an Cant\\'u Rodr\\'iguez, Alberte Bouso Garc\\'ia,\n  R\\\"udiger Franke, Geert Deconinck, Lieve Helsen, Ann Now\\'e, Maarten\n  Messagie, Luis Ramirez Camargo", "title": "Model-predictive control and reinforcement learning in multi-energy\n  system case studies", "comments": "35 pages, 27 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.AI cs.LG cs.SY math.OC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Model-predictive-control (MPC) offers an optimal control technique to\nestablish and ensure that the total operation cost of multi-energy systems\nremains at a minimum while fulfilling all system constraints. However, this\nmethod presumes an adequate model of the underlying system dynamics, which is\nprone to modelling errors and is not necessarily adaptive. This has an\nassociated initial and ongoing project-specific engineering cost. In this\npaper, we present an on- and off-policy multi-objective reinforcement learning\n(RL) approach, that does not assume a model a priori, benchmarking this against\na linear MPC (LMPC - to reflect current practice, though non-linear MPC\nperforms better) - both derived from the general optimal control problem,\nhighlighting their differences and similarities. In a simple multi-energy\nsystem (MES) configuration case study, we show that a twin delayed deep\ndeterministic policy gradient (TD3) RL agent offers potential to match and\noutperform the perfect foresight LMPC benchmark (101.5%). This while the\nrealistic LMPC, i.e. imperfect predictions, only achieves 98%. While in a more\ncomplex MES system configuration, the RL agent's performance is generally lower\n(94.6%), yet still better than the realistic LMPC (88.9%). In both case\nstudies, the RL agents outperformed the realistic LMPC after a training period\nof 2 years using quarterly interactions with the environment. We conclude that\nreinforcement learning is a viable optimal control technique for multi-energy\nsystems given adequate constraint handling and pre-training, to avoid unsafe\ninteractions and long training periods, as is proposed in fundamental future\nwork.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 06:51:50 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Ceusters", "Glenn", ""], ["Rodr\u00edguez", "Rom\u00e1n Cant\u00fa", ""], ["Garc\u00eda", "Alberte Bouso", ""], ["Franke", "R\u00fcdiger", ""], ["Deconinck", "Geert", ""], ["Helsen", "Lieve", ""], ["Now\u00e9", "Ann", ""], ["Messagie", "Maarten", ""], ["Camargo", "Luis Ramirez", ""]]}, {"id": "2104.09798", "submitter": "Alireza Khadem", "authors": "Alireza Khadem, Haojie Ye, Trevor Mudge", "title": "CoDR: Computation and Data Reuse Aware CNN Accelerator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.AI cs.LG cs.NE cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computation and Data Reuse is critical for the resource-limited Convolutional\nNeural Network (CNN) accelerators. This paper presents Universal Computation\nReuse to exploit weight sparsity, repetition, and similarity simultaneously in\na convolutional layer. Moreover, CoDR decreases the cost of weight memory\naccess by proposing a customized Run-Length Encoding scheme and the number of\nmemory accesses to the intermediate results by introducing an input and output\nstationary dataflow. Compared to two recent compressed CNN accelerators with\nthe same area of 2.85 mm^2, CoDR decreases SRAM access by 5.08x and 7.99x, and\nconsumes 3.76x and 6.84x less energy.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 07:20:17 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Khadem", "Alireza", ""], ["Ye", "Haojie", ""], ["Mudge", "Trevor", ""]]}, {"id": "2104.09839", "submitter": "Marco Forgione", "authors": "Dario Piga, Marco Forgione, Manas Mejari", "title": "Deep learning with transfer functions: new applications in system\n  identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a linear dynamical operator described in terms of a\nrational transfer function, endowed with a well-defined and efficient\nback-propagation behavior for automatic derivatives computation. The operator\nenables end-to-end training of structured networks containing linear transfer\nfunctions and other differentiable units {by} exploiting standard deep learning\nsoftware.\n  Two relevant applications of the operator in system identification are\npresented. The first one consists in the integration of {prediction error\nmethods} in deep learning. The dynamical operator is included as {the} last\nlayer of a neural network in order to obtain the optimal one-step-ahead\nprediction error.\n  The second one considers identification of general block-oriented models from\nquantized data. These block-oriented models are constructed by combining linear\ndynamical operators with static nonlinearities described as standard\nfeed-forward neural networks. A custom loss function corresponding to the\nlog-likelihood of quantized output observations is defined. For gradient-based\noptimization, the derivatives of the log-likelihood are computed by applying\nthe back-propagation algorithm through the whole network. Two system\nidentification benchmarks are used to show the effectiveness of the proposed\nmethodologies.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 08:58:55 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Piga", "Dario", ""], ["Forgione", "Marco", ""], ["Mejari", "Manas", ""]]}, {"id": "2104.09841", "submitter": "Daehee Kim", "authors": "Daehee Kim, Seunghyun Park, Jinkyu Kim, and Jaekoo Lee", "title": "SelfReg: Self-supervised Contrastive Regularization for Domain\n  Generalization", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In general, an experimental environment for deep learning assumes that the\ntraining and the test dataset are sampled from the same distribution. However,\nin real-world situations, a difference in the distribution between two\ndatasets, domain shift, may occur, which becomes a major factor impeding the\ngeneralization performance of the model. The research field to solve this\nproblem is called domain generalization, and it alleviates the domain shift\nproblem by extracting domain-invariant features explicitly or implicitly. In\nrecent studies, contrastive learning-based domain generalization approaches\nhave been proposed and achieved high performance. These approaches require\nsampling of the negative data pair. However, the performance of contrastive\nlearning fundamentally depends on quality and quantity of negative data pairs.\nTo address this issue, we propose a new regularization method for domain\ngeneralization based on contrastive learning, self-supervised contrastive\nregularization (SelfReg). The proposed approach use only positive data pairs,\nthus it resolves various problems caused by negative pair sampling. Moreover,\nwe propose a class-specific domain perturbation layer (CDPL), which makes it\npossible to effectively apply mixup augmentation even when only positive data\npairs are used. The experimental results show that the techniques incorporated\nby SelfReg contributed to the performance in a compatible manner. In the recent\nbenchmark, DomainBed, the proposed method shows comparable performance to the\nconventional state-of-the-art alternatives. Codes are available at\nhttps://github.com/dnap512/SelfReg.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 09:08:29 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Kim", "Daehee", ""], ["Park", "Seunghyun", ""], ["Kim", "Jinkyu", ""], ["Lee", "Jaekoo", ""]]}, {"id": "2104.09864", "submitter": "Jianlin Su", "authors": "Jianlin Su, Yu Lu, Shengfeng Pan, Bo Wen, Yunfeng Liu", "title": "RoFormer: Enhanced Transformer with Rotary Position Embedding", "comments": "Preprint. English experiments are coming", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Position encoding in transformer architecture provides supervision for\ndependency modeling between elements at different positions in the sequence. We\ninvestigate various methods to encode positional information in\ntransformer-based language models and propose a novel implementation named\nRotary Position Embedding(RoPE). The proposed RoPE encodes absolute positional\ninformation with rotation matrix and naturally incorporates explicit relative\nposition dependency in self-attention formulation. Notably, RoPE comes with\nvaluable properties such as flexibility of being expand to any sequence\nlengths, decaying inter-token dependency with increasing relative distances,\nand capability of equipping the linear self-attention with relative position\nencoding. As a result, the enhanced transformer with rotary position embedding,\nor RoFormer, achieves superior performance in tasks with long texts. We release\nthe theoretical analysis along with some preliminary experiment results on\nChinese data. The undergoing experiment for English benchmark will soon be\nupdated.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 09:54:06 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Su", "Jianlin", ""], ["Lu", "Yu", ""], ["Pan", "Shengfeng", ""], ["Wen", "Bo", ""], ["Liu", "Yunfeng", ""]]}, {"id": "2104.09866", "submitter": "Elahe Arani", "authors": "Prashant Bhat, Elahe Arani, and Bahram Zonooz", "title": "Distill on the Go: Online knowledge distillation in self-supervised\n  learning", "comments": "Spotlight @ Learning from Limited or Imperfect Data (L2ID) Workshop -\n  CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Self-supervised learning solves pretext prediction tasks that do not require\nannotations to learn feature representations. For vision tasks, pretext tasks\nsuch as predicting rotation, solving jigsaw are solely created from the input\ndata. Yet, predicting this known information helps in learning representations\nuseful for downstream tasks. However, recent works have shown that wider and\ndeeper models benefit more from self-supervised learning than smaller models.\nTo address the issue of self-supervised pre-training of smaller models, we\npropose Distill-on-the-Go (DoGo), a self-supervised learning paradigm using\nsingle-stage online knowledge distillation to improve the representation\nquality of the smaller models. We employ deep mutual learning strategy in which\ntwo models collaboratively learn from each other to improve one another.\nSpecifically, each model is trained using self-supervised learning along with\ndistillation that aligns each model's softmax probabilities of similarity\nscores with that of the peer model. We conduct extensive experiments on\nmultiple benchmark datasets, learning objectives, and architectures to\ndemonstrate the potential of our proposed method. Our results show significant\nperformance gain in the presence of noisy and limited labels and generalization\nto out-of-distribution data.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 09:59:23 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 13:03:14 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Bhat", "Prashant", ""], ["Arani", "Elahe", ""], ["Zonooz", "Bahram", ""]]}, {"id": "2104.09884", "submitter": "Chao Qian", "authors": "Chao Qian, Dan-Xuan Liu, Chao Feng, Ke Tang", "title": "Multi-objective Evolutionary Algorithms are Generally Good: Maximizing\n  Monotone Submodular Functions over Sequences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolutionary algorithms (EAs) are general-purpose optimization algorithms,\ninspired by natural evolution. Recent theoretical studies have shown that EAs\ncan achieve good approximation guarantees for solving the problem classes of\nsubmodular optimization, which have a wide range of applications, such as\nmaximum coverage, sparse regression, influence maximization, document\nsummarization and sensor placement, just to name a few. Though they have\nprovided some theoretical explanation for the general-purpose nature of EAs,\nthe considered submodular objective functions are defined only over sets or\nmultisets. To complement this line of research, this paper studies the problem\nclass of maximizing monotone submodular functions over sequences, where the\nobjective function depends on the order of items. We prove that for each kind\nof previously studied monotone submodular objective functions over sequences,\ni.e., prefix monotone submodular functions, weakly monotone and strongly\nsubmodular functions, and DAG monotone submodular functions, a simple\nmulti-objective EA, i.e., GSEMO, can always reach or improve the best known\napproximation guarantee after running polynomial time in expectation. Note that\nthese best-known approximation guarantees can be obtained only by different\ngreedy-style algorithms before. Empirical studies on various applications,\ne.g., accomplishing tasks, maximizing information gain, search-and-tracking and\nrecommender systems, show the excellent performance of the GSEMO.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 10:36:10 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Qian", "Chao", ""], ["Liu", "Dan-Xuan", ""], ["Feng", "Chao", ""], ["Tang", "Ke", ""]]}, {"id": "2104.09903", "submitter": "David Fernandez Llorca", "authors": "Antonio Hern\\'andez Mart\\'inez, Javier Lorenzo D\\'iaz, Iv\\'an Garc\\'ia\n  Daza, David Fern\\'andez Llorca", "title": "Data-driven vehicle speed detection from synthetic driving simulator\n  images", "comments": "Submitted to the IEEE Intelligent Transportation Systems Conference\n  2021 (ITSC2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite all the challenges and limitations, vision-based vehicle speed\ndetection is gaining research interest due to its great potential benefits such\nas cost reduction, and enhanced additional functions. As stated in a recent\nsurvey [1], the use of learning-based approaches to address this problem is\nstill in its infancy. One of the main difficulties is the need for a large\namount of data, which must contain the input sequences and, more importantly,\nthe output values corresponding to the actual speed of the vehicles. Data\ncollection in this context requires a complex and costly setup to capture the\nimages from the camera synchronized with a high precision speed sensor to\ngenerate the ground truth speed values. In this paper we explore, for the first\ntime, the use of synthetic images generated from a driving simulator (e.g.,\nCARLA) to address vehicle speed detection using a learning-based approach. We\nsimulate a virtual camera placed over a stretch of road, and generate thousands\nof images with variability corresponding to multiple speeds, different vehicle\ntypes and colors, and lighting and weather conditions. Two different approaches\nto map the sequence of images to an output speed (regression) are studied,\nincluding CNN-GRU and 3D-CNN. We present preliminary results that support the\nhigh potential of this approach to address vehicle speed detection.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 11:26:13 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Mart\u00ednez", "Antonio Hern\u00e1ndez", ""], ["D\u00edaz", "Javier Lorenzo", ""], ["Daza", "Iv\u00e1n Garc\u00eda", ""], ["Llorca", "David Fern\u00e1ndez", ""]]}, {"id": "2104.09936", "submitter": "Zhenning Li", "authors": "Zhenning Li, Hao Yu, Guohui Zhang, Shangjia Dong, Cheng-Zhong Xu", "title": "Network-wide traffic signal control optimization using a multi-agent\n  deep reinforcement learning", "comments": null, "journal-ref": "Transportation Research Part C: Emerging Technologies Volume 125,\n  April 2021, 103059", "doi": "10.1016/j.trc.2021.103059", "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inefficient traffic control may cause numerous problems such as traffic\ncongestion and energy waste. This paper proposes a novel multi-agent\nreinforcement learning method, named KS-DDPG (Knowledge Sharing Deep\nDeterministic Policy Gradient) to achieve optimal control by enhancing the\ncooperation between traffic signals. By introducing the knowledge-sharing\nenabled communication protocol, each agent can access to the collective\nrepresentation of the traffic environment collected by all agents. The proposed\nmethod is evaluated through two experiments respectively using synthetic and\nreal-world datasets. The comparison with state-of-the-art reinforcement\nlearning-based and conventional transportation methods demonstrate the proposed\nKS-DDPG has significant efficiency in controlling large-scale transportation\nnetworks and coping with fluctuations in traffic flow. In addition, the\nintroduced communication mechanism has also been proven to speed up the\nconvergence of the model without significantly increasing the computational\nburden.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 12:53:08 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Li", "Zhenning", ""], ["Yu", "Hao", ""], ["Zhang", "Guohui", ""], ["Dong", "Shangjia", ""], ["Xu", "Cheng-Zhong", ""]]}, {"id": "2104.09943", "submitter": "Olga Lukyanova", "authors": "Oleg Nikitin, Olga Lukyanova, Alex Kunin", "title": "The principle of weight divergence facilitation for unsupervised pattern\n  recognition in spiking neural networks", "comments": "9 pages, 5 figures, submitted to the conference ICANN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Parallels between the signal processing tasks and biological neurons lead to\nan understanding of the principles of self-organized optimization of input\nsignal recognition. In the present paper, we discuss such similarities among\nbiological and technical systems. We propose the addition to the well-known\nSTDP synaptic plasticity rule to directs the weight modification towards the\nstate associated with the maximal difference between the background noise and\ncorrelated signals. The principle of physically constrained weight growth is\nused as a basis for such control of the modification of the weights. It is\nproposed, that biological synaptic straight modification is restricted by the\nexistence and production of bio-chemical 'substances' needed for plasticity\ndevelopment. In this paper, the information about the noise-to-signal ratio is\nused to control such a substances' production and storage and to drive the\nneuron's synaptic pressures towards the state with the best signal-to-noise\nratio. Several experiments with different input signal regimes are considered\nto understand the functioning of the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 13:11:15 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Nikitin", "Oleg", ""], ["Lukyanova", "Olga", ""], ["Kunin", "Alex", ""]]}, {"id": "2104.09962", "submitter": "Marco Pegoraro", "authors": "Marco Pegoraro and Merih Seran Uysal and David Benedikt Georgi and Wil\n  M.P. van der Aalst", "title": "Text-Aware Predictive Monitoring of Business Processes", "comments": "14 pages, 4 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The real-time prediction of business processes using historical event data is\nan important capability of modern business process monitoring systems. Existing\nprocess prediction methods are able to also exploit the data perspective of\nrecorded events, in addition to the control-flow perspective. However, while\nwell-structured numerical or categorical attributes are considered in many\nprediction techniques, almost no technique is able to utilize text documents\nwritten in natural language, which can hold information critical to the\nprediction task. In this paper, we illustrate the design, implementation, and\nevaluation of a novel text-aware process prediction model based on Long\nShort-Term Memory (LSTM) neural networks and natural language models. The\nproposed model can take categorical, numerical and textual attributes in event\ndata into account to predict the activity and timestamp of the next event, the\noutcome, and the cycle time of a running process instance. Experiments show\nthat the text-aware model is able to outperform state-of-the-art process\nprediction methods on simulated and real-world event logs containing textual\ndata.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 13:51:27 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 13:12:07 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Pegoraro", "Marco", ""], ["Uysal", "Merih Seran", ""], ["Georgi", "David Benedikt", ""], ["van der Aalst", "Wil M. P.", ""]]}, {"id": "2104.09978", "submitter": "Karan Jindal", "authors": "Rahul Singh, Karan Jindal, Yufei Yu, Hanyu Yang, Tarun Joshi, Matthew\n  A. Campbell, Wayne B. Shoumaker", "title": "Robustness Tests of NLP Machine Learning Models: Search and Semantically\n  Replace", "comments": "18 pages, 2 figures, 18 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a strategy to assess the robustness of different machine\nlearning models that involve natural language processing (NLP). The overall\napproach relies upon a Search and Semantically Replace strategy that consists\nof two steps: (1) Search, which identifies important parts in the text; (2)\nSemantically Replace, which finds replacements for the important parts, and\nconstrains the replaced tokens with semantically similar words. We introduce\ndifferent types of Search and Semantically Replace methods designed\nspecifically for particular types of machine learning models. We also\ninvestigate the effectiveness of this strategy and provide a general framework\nto assess a variety of machine learning models. Finally, an empirical\ncomparison is provided of robustness performance among three different model\ntypes, each with a different text representation.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 14:05:36 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Singh", "Rahul", ""], ["Jindal", "Karan", ""], ["Yu", "Yufei", ""], ["Yang", "Hanyu", ""], ["Joshi", "Tarun", ""], ["Campbell", "Matthew A.", ""], ["Shoumaker", "Wayne B.", ""]]}, {"id": "2104.09987", "submitter": "Alexandre Defossez", "authors": "Alexandre D\\'efossez, Yossi Adi, Gabriel Synnaeve", "title": "Differentiable Model Compression via Pseudo Quantization Noise", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose to add independent pseudo quantization noise to model parameters\nduring training to approximate the effect of a quantization operator. This\nmethod, DiffQ, is differentiable both with respect to the unquantized\nparameters, and the number of bits used. Given a single hyper-parameter\nexpressing the desired balance between the quantized model size and accuracy,\nDiffQ can optimize the number of bits used per individual weight or groups of\nweights, in a single training. We experimentally verify that our method\noutperforms state-of-the-art quantization techniques on several benchmarks and\narchitectures for image classification, language modeling, and audio source\nseparation. For instance, on the Wikitext-103 language modeling benchmark,\nDiffQ compresses a 16 layers transformer model by a factor of 8, equivalent to\n4 bits precision, while losing only 0.5 points of perplexity. Code is available\nat: https://github.com/facebookresearch/diffq\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 14:14:03 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["D\u00e9fossez", "Alexandre", ""], ["Adi", "Yossi", ""], ["Synnaeve", "Gabriel", ""]]}, {"id": "2104.10010", "submitter": "Olga Lukyanova", "authors": "Olga Lukyanova, Oleg Nikitin, Alex Kunin", "title": "BraidNet: procedural generation of neural networks for image\n  classification problems using braid theory", "comments": "9 pages, 8 figures, submitted to the conference ICANN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.IT cs.LG math.GT math.IT", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this article, we propose the approach to procedural optimization of a\nneural network, based on the combination of information theory and braid\ntheory. The network studied in the article implemented with the intersections\nbetween the braid strands, as well as simplified networks (a network with\nstrands without intersections and a simple convolutional deep neural network),\nare used to solve various problems of multiclass image classification that\nallow us to analyze the comparative effectiveness of the proposed architecture.\nThe simulation results showed BraidNet's comparative advantage in learning\nspeed and classification accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 14:40:30 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Lukyanova", "Olga", ""], ["Nikitin", "Oleg", ""], ["Kunin", "Alex", ""]]}, {"id": "2104.10011", "submitter": "Daniel Koguciuk M.Sc.Eng.", "authors": "Daniel Koguciuk, Elahe Arani, Bahram Zonooz", "title": "Perceptual Loss for Robust Unsupervised Homography Estimation", "comments": "Accepted at Image Matching: Local Features & Beyond (CVPR 2021\n  Workshop)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Homography estimation is often an indispensable step in many computer vision\ntasks. The existing approaches, however, are not robust to illumination and/or\nlarger viewpoint changes. In this paper, we propose bidirectional implicit\nHomography Estimation (biHomE) loss for unsupervised homography estimation.\nbiHomE minimizes the distance in the feature space between the warped image\nfrom the source viewpoint and the corresponding image from the target\nviewpoint. Since we use a fixed pre-trained feature extractor and the only\nlearnable component of our framework is the homography network, we effectively\ndecouple the homography estimation from representation learning. We use an\nadditional photometric distortion step in the synthetic COCO dataset generation\nto better represent the illumination variation of the real-world scenarios. We\nshow that biHomE achieves state-of-the-art performance on synthetic COCO\ndataset, which is also comparable or better compared to supervised approaches.\nFurthermore, the empirical results demonstrate the robustness of our approach\nto illumination variation compared to existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 14:41:54 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Koguciuk", "Daniel", ""], ["Arani", "Elahe", ""], ["Zonooz", "Bahram", ""]]}, {"id": "2104.10027", "submitter": "Ilche Georgievski", "authors": "Ilche Georgievski", "title": "HTN Planning Domain for Deployment of Cloud Applications", "comments": "Published in the proceedings of the 10th International Planning\n  Competition: Planner and Domain Abstracts - Hierarchical Task Network (HTN)\n  Planning Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud providers are facing a complex problem in configuring software\napplications ready for deployment on their infrastructures. Hierarchical Task\nNetwork (HTN) planning can provide effective means to solve such deployment\nproblems. We present an HTN planning domain that models deployment problems as\nfound in realistic Cloud environments.\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 10:36:42 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 09:16:17 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Georgievski", "Ilche", ""]]}, {"id": "2104.10033", "submitter": "Manh Duong Phung", "authors": "Manh Duong Phung and Quang Phuc Ha", "title": "Safety-enhanced UAV Path Planning with Spherical Vector-based Particle\n  Swarm Optimization", "comments": null, "journal-ref": "Applied Soft Computing, Volume 107, August 2021, 107376", "doi": "10.1016/j.asoc.2021.107376", "report-no": null, "categories": "cs.NE cs.AI cs.RO cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new algorithm named spherical vector-based particle\nswarm optimization (SPSO) to deal with the problem of path planning for\nunmanned aerial vehicles (UAVs) in complicated environments subjected to\nmultiple threats. A cost function is first formulated to convert the path\nplanning into an optimization problem that incorporates requirements and\nconstraints for the feasible and safe operation of the UAV. SPSO is then used\nto find the optimal path that minimizes the cost function by efficiently\nsearching the configuration space of the UAV via the correspondence between the\nparticle position and the speed, turn angle and climb/dive angle of the UAV. To\nevaluate the performance of SPSO, eight benchmarking scenarios have been\ngenerated from real digital elevation model maps. The results show that the\nproposed SPSO outperforms not only other particle swarm optimization (PSO)\nvariants including the classic PSO, phase angle-encoded PSO and quantum-behave\nPSO but also other state-of-the-art metaheuristic optimization algorithms\nincluding the genetic algorithm (GA), artificial bee colony (ABC), and\ndifferential evolution (DE) in most scenarios. In addition, experiments have\nbeen conducted to demonstrate the validity of the generated paths for real UAV\noperations. Source code of the algorithm can be found at\nhttps://github.com/duongpm/SPSO.\n", "versions": [{"version": "v1", "created": "Tue, 13 Apr 2021 06:45:11 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Phung", "Manh Duong", ""], ["Ha", "Quang Phuc", ""]]}, {"id": "2104.10036", "submitter": "Pankaj Mishra", "authors": "Pankaj Mishra, Riccardo Verk, Daniele Fornasier, Claudio Piciarelli,\n  Gian Luca Foresti", "title": "VT-ADL: A Vision Transformer Network for Image Anomaly Detection and\n  Localization", "comments": "6 Pages, 4 images, conference published paper", "journal-ref": "IEEE ISIE 2021", "doi": null, "report-no": "KD-003638", "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a transformer-based image anomaly detection and localization\nnetwork. Our proposed model is a combination of a reconstruction-based approach\nand patch embedding. The use of transformer networks helps to preserve the\nspatial information of the embedded patches, which are later processed by a\nGaussian mixture density network to localize the anomalous areas. In addition,\nwe also publish BTAD, a real-world industrial anomaly dataset. Our results are\ncompared with other state-of-the-art algorithms using publicly available\ndatasets like MNIST and MVTec.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 15:12:30 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Mishra", "Pankaj", ""], ["Verk", "Riccardo", ""], ["Fornasier", "Daniele", ""], ["Piciarelli", "Claudio", ""], ["Foresti", "Gian Luca", ""]]}, {"id": "2104.10066", "submitter": "Christian Requena-Mesa", "authors": "Christian Requena-Mesa, Vitus Benson, Markus Reichstein, Jakob Runge,\n  Joachim Denzler", "title": "EarthNet2021: A large-scale dataset and challenge for Earth surface\n  forecasting as a guided video prediction task", "comments": "8 pages, 8 figures, accepted at CVPR2021 workshop EarthVision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Satellite images are snapshots of the Earth surface. We propose to forecast\nthem. We frame Earth surface forecasting as the task of predicting satellite\nimagery conditioned on future weather. EarthNet2021 is a large dataset suitable\nfor training deep neural networks on the task. It contains Sentinel 2 satellite\nimagery at 20m resolution, matching topography and mesoscale (1.28km)\nmeteorological variables packaged into 32000 samples. Additionally we frame\nEarthNet2021 as a challenge allowing for model intercomparison. Resulting\nforecasts will greatly improve (>x50) over the spatial resolution found in\nnumerical models. This allows localized impacts from extreme weather to be\npredicted, thus supporting downstream applications such as crop yield\nprediction, forest health assessments or biodiversity monitoring. Find data,\ncode, and how to participate at www.earthnet.tech\n", "versions": [{"version": "v1", "created": "Fri, 16 Apr 2021 09:47:30 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Requena-Mesa", "Christian", ""], ["Benson", "Vitus", ""], ["Reichstein", "Markus", ""], ["Runge", "Jakob", ""], ["Denzler", "Joachim", ""]]}, {"id": "2104.10076", "submitter": "Yijun Yang", "authors": "Yang Yijun, Gao Ruiyuan, Li Yu, Lai Qiuxia, Xu Qiang", "title": "MixDefense: A Defense-in-Depth Framework for Adversarial Example\n  Detection Based on Statistical and Semantic Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Machine learning with deep neural networks (DNNs) has become one of the\nfoundation techniques in many safety-critical systems, such as autonomous\nvehicles and medical diagnosis systems. DNN-based systems, however, are known\nto be vulnerable to adversarial examples (AEs) that are maliciously perturbed\nvariants of legitimate inputs. While there has been a vast body of research to\ndefend against AE attacks in the literature, the performances of existing\ndefense techniques are still far from satisfactory, especially for adaptive\nattacks, wherein attackers are knowledgeable about the defense mechanisms and\ncraft AEs accordingly. In this work, we propose a multilayer defense-in-depth\nframework for AE detection, namely MixDefense. For the first layer, we focus on\nthose AEs with large perturbations. We propose to leverage the `noise' features\nextracted from the inputs to discover the statistical difference between\nnatural images and tampered ones for AE detection. For AEs with small\nperturbations, the inference result of such inputs would largely deviate from\ntheir semantic information. Consequently, we propose a novel learning-based\nsolution to model such contradictions for AE detection. Both layers are\nresilient to adaptive attacks because there do not exist gradient propagation\npaths for AE generation. Experimental results with various AE attack methods on\nimage classification datasets show that the proposed MixDefense solution\noutperforms the existing AE detection techniques by a considerable margin.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 15:57:07 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Yijun", "Yang", ""], ["Ruiyuan", "Gao", ""], ["Yu", "Li", ""], ["Qiuxia", "Lai", ""], ["Qiang", "Xu", ""]]}, {"id": "2104.10085", "submitter": "Kordian Gontarska", "authors": "Kordian Gontarska and Weronika Wrazen and Jossekin Beilharz and Robert\n  Schmid and Lauritz Thamsen and Andreas Polze", "title": "Predicting Medical Interventions from Vital Parameters: Towards a\n  Decision Support System for Remote Patient Monitoring", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cardiovascular diseases and heart failures in particular are the main cause\nof non-communicable disease mortality in the world. Constant patient monitoring\nenables better medical treatment as it allows practitioners to react on time\nand provide the appropriate treatment. Telemedicine can provide constant remote\nmonitoring so patients can stay in their homes, only requiring medical sensing\nequipment and network connections. A limiting factor for telemedical centers is\nthe amount of patients that can be monitored simultaneously. We aim to increase\nthis amount by implementing a decision support system. This paper investigates\na machine learning model to estimate a risk score based on patient vital\nparameters that allows sorting all cases every day to help practitioners focus\ntheir limited capacities on the most severe cases. The model we propose reaches\nan AUCROC of 0.84, whereas the baseline rule-based model reaches an AUCROC of\n0.73. Our results indicate that the usage of deep learning to improve the\nefficiency of telemedical centers is feasible. This way more patients could\nbenefit from better health-care through remote monitoring.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 16:13:37 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Gontarska", "Kordian", ""], ["Wrazen", "Weronika", ""], ["Beilharz", "Jossekin", ""], ["Schmid", "Robert", ""], ["Thamsen", "Lauritz", ""], ["Polze", "Andreas", ""]]}, {"id": "2104.10093", "submitter": "Gido van de Ven", "authors": "Gido M. van de Ven, Zhe Li, Andreas S. Tolias", "title": "Class-Incremental Learning with Generative Classifiers", "comments": "To appear in the IEEE Conference on Computer Vision and Pattern\n  Recognition Workshop (CVPR-W) on Continual Learning in Computer Vision\n  (CLVision) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Incrementally training deep neural networks to recognize new classes is a\nchallenging problem. Most existing class-incremental learning methods store\ndata or use generative replay, both of which have drawbacks, while\n'rehearsal-free' alternatives such as parameter regularization or\nbias-correction methods do not consistently achieve high performance. Here, we\nput forward a new strategy for class-incremental learning: generative\nclassification. Rather than directly learning the conditional distribution\np(y|x), our proposal is to learn the joint distribution p(x,y), factorized as\np(x|y)p(y), and to perform classification using Bayes' rule. As a\nproof-of-principle, here we implement this strategy by training a variational\nautoencoder for each class to be learned and by using importance sampling to\nestimate the likelihoods p(x|y). This simple approach performs very well on a\ndiverse set of continual learning benchmarks, outperforming generative replay\nand other existing baselines that do not store data.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 16:26:14 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 09:19:48 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["van de Ven", "Gido M.", ""], ["Li", "Zhe", ""], ["Tolias", "Andreas S.", ""]]}, {"id": "2104.10130", "submitter": "Xiang Zhou", "authors": "Xiang Zhou, Heba Elfardy, Christos Christodoulopoulos, Thomas Butler,\n  Mohit Bansal", "title": "Hidden Biases in Unreliable News Detection Datasets", "comments": "EACL 2021 (11 pages, 3 figures, 8 tables)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic unreliable news detection is a research problem with great\npotential impact. Recently, several papers have shown promising results on\nlarge-scale news datasets with models that only use the article itself without\nresorting to any fact-checking mechanism or retrieving any supporting evidence.\nIn this work, we take a closer look at these datasets. While they all provide\nvaluable resources for future research, we observe a number of problems that\nmay lead to results that do not generalize in more realistic settings.\nSpecifically, we show that selection bias during data collection leads to\nundesired artifacts in the datasets. In addition, while most systems train and\npredict at the level of individual articles, overlapping article sources in the\ntraining and evaluation data can provide a strong confounding factor that\nmodels can exploit. In the presence of this confounding factor, the models can\nachieve good performance by directly memorizing the site-label mapping instead\nof modeling the real task of unreliable news detection. We observed a\nsignificant drop (>10%) in accuracy for all models tested in a clean split with\nno train/test source overlap. Using the observations and experimental results,\nwe provide practical suggestions on how to create more reliable datasets for\nthe unreliable news detection task. We suggest future dataset creation include\na simple model as a difficulty/bias probe and future model development use a\nclean non-overlapping site and date split.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 17:16:41 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Zhou", "Xiang", ""], ["Elfardy", "Heba", ""], ["Christodoulopoulos", "Christos", ""], ["Butler", "Thomas", ""], ["Bansal", "Mohit", ""]]}, {"id": "2104.10132", "submitter": "Claudio Gallicchio", "authors": "Claudio Gallicchio, Alessio Micheli, Luca Silvestri", "title": "Phase Transition Adaptation", "comments": "Accepted at IJCNN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Artificial Recurrent Neural Networks are a powerful information processing\nabstraction, and Reservoir Computing provides an efficient strategy to build\nrobust implementations by projecting external inputs into high dimensional\ndynamical system trajectories. In this paper, we propose an extension of the\noriginal approach, a local unsupervised learning mechanism we call Phase\nTransition Adaptation, designed to drive the system dynamics towards the `edge\nof stability'. Here, the complex behavior exhibited by the system elicits an\nenhancement in its overall computational capacity. We show experimentally that\nour approach consistently achieves its purpose over several datasets.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 17:18:34 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Gallicchio", "Claudio", ""], ["Micheli", "Alessio", ""], ["Silvestri", "Luca", ""]]}, {"id": "2104.10159", "submitter": "Luis Pineda", "authors": "Luis Pineda, Brandon Amos, Amy Zhang, Nathan O. Lambert, Roberto\n  Calandra", "title": "MBRL-Lib: A Modular Library for Model-based Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-based reinforcement learning is a compelling framework for\ndata-efficient learning of agents that interact with the world. This family of\nalgorithms has many subcomponents that need to be carefully selected and tuned.\nAs a result the entry-bar for researchers to approach the field and to deploy\nit in real-world tasks can be daunting. In this paper, we present MBRL-Lib -- a\nmachine learning library for model-based reinforcement learning in continuous\nstate-action spaces based on PyTorch. MBRL-Lib is designed as a platform for\nboth researchers, to easily develop, debug and compare new algorithms, and\nnon-expert user, to lower the entry-bar of deploying state-of-the-art\nalgorithms. MBRL-Lib is open-source at\nhttps://github.com/facebookresearch/mbrl-lib.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 17:58:22 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Pineda", "Luis", ""], ["Amos", "Brandon", ""], ["Zhang", "Amy", ""], ["Lambert", "Nathan O.", ""], ["Calandra", "Roberto", ""]]}, {"id": "2104.10190", "submitter": "Tim G. J. Rudner", "authors": "Tim G. J. Rudner and Vitchyr H. Pong and Rowan McAllister and Yarin\n  Gal and Sergey Levine", "title": "Outcome-Driven Reinforcement Learning via Variational Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While reinforcement learning algorithms provide automated acquisition of\noptimal policies, practical application of such methods requires a number of\ndesign decisions, such as manually designing reward functions that not only\ndefine the task, but also provide sufficient shaping to accomplish it. In this\npaper, we discuss a new perspective on reinforcement learning, recasting it as\nthe problem of inferring actions that achieve desired outcomes, rather than a\nproblem of maximizing rewards. To solve the resulting outcome-directed\ninference problem, we establish a novel variational inference formulation that\nallows us to derive a well-shaped reward function which can be learned directly\nfrom environment interactions. From the corresponding variational objective, we\nalso derive a new probabilistic Bellman backup operator reminiscent of the\nstandard Bellman backup operator and use it to develop an off-policy algorithm\nto solve goal-directed tasks. We empirically demonstrate that this method\neliminates the need to design reward functions and leads to effective\ngoal-directed behaviors.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 18:16:21 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Rudner", "Tim G. J.", ""], ["Pong", "Vitchyr H.", ""], ["McAllister", "Rowan", ""], ["Gal", "Yarin", ""], ["Levine", "Sergey", ""]]}, {"id": "2104.10193", "submitter": "Lisa Bauer", "authors": "Lisa Bauer, Mohit Bansal", "title": "Identify, Align, and Integrate: Matching Knowledge Graphs to Commonsense\n  Reasoning Tasks", "comments": "EACL 2021 (14 pages, 2 figures, 10 tables)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrating external knowledge into commonsense reasoning tasks has shown\nprogress in resolving some, but not all, knowledge gaps in these tasks. For\nknowledge integration to yield peak performance, it is critical to select a\nknowledge graph (KG) that is well-aligned with the given task's objective. We\npresent an approach to assess how well a candidate KG can correctly identify\nand accurately fill in gaps of reasoning for a task, which we call KG-to-task\nmatch. We show this KG-to-task match in 3 phases: knowledge-task\nidentification, knowledge-task alignment, and knowledge-task integration. We\nalso analyze our transformer-based KG-to-task models via commonsense probes to\nmeasure how much knowledge is captured in these models before and after KG\nintegration. Empirically, we investigate KG matches for the SocialIQA (SIQA)\n(Sap et al., 2019b), Physical IQA (PIQA) (Bisk et al., 2020), and MCScript2.0\n(Ostermann et al., 2019) datasets with 3 diverse KGs: ATOMIC (Sap et al.,\n2019a), ConceptNet (Speer et al., 2017), and an automatically constructed\ninstructional KG based on WikiHow (Koupaee and Wang, 2018). With our methods we\nare able to demonstrate that ATOMIC, an event-inference focused KG, is the best\nmatch for SIQA and MCScript2.0, and that the taxonomic ConceptNet and\nWikiHow-based KGs are the best matches for PIQA across all 3 analysis phases.\nWe verify our methods and findings with human evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 18:23:45 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Bauer", "Lisa", ""], ["Bansal", "Mohit", ""]]}, {"id": "2104.10201", "submitter": "David Eriksson", "authors": "Ryan Turner, David Eriksson, Michael McCourt, Juha Kiili, Eero\n  Laaksonen, Zhen Xu, Isabelle Guyon", "title": "Bayesian Optimization is Superior to Random Search for Machine Learning\n  Hyperparameter Tuning: Analysis of the Black-Box Optimization Challenge 2020", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the results and insights from the black-box optimization\n(BBO) challenge at NeurIPS 2020 which ran from July-October, 2020. The\nchallenge emphasized the importance of evaluating derivative-free optimizers\nfor tuning the hyperparameters of machine learning models. This was the first\nblack-box optimization challenge with a machine learning emphasis. It was based\non tuning (validation set) performance of standard machine learning models on\nreal datasets. This competition has widespread impact as black-box optimization\n(e.g., Bayesian optimization) is relevant for hyperparameter tuning in almost\nevery machine learning project as well as many applications outside of machine\nlearning. The final leaderboard was determined using the optimization\nperformance on held-out (hidden) objective functions, where the optimizers ran\nwithout human intervention. Baselines were set using the default settings of\nseveral open-source black-box optimization packages as well as random search.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 18:44:59 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Turner", "Ryan", ""], ["Eriksson", "David", ""], ["McCourt", "Michael", ""], ["Kiili", "Juha", ""], ["Laaksonen", "Eero", ""], ["Xu", "Zhen", ""], ["Guyon", "Isabelle", ""]]}, {"id": "2104.10213", "submitter": "George Papakostas Prof.", "authors": "N.-I. Galanis, P. Vafiadis, K.-G. Mirzaev, G.A. Papakostas", "title": "Machine Learning Meets Natural Language Processing -- The story so far", "comments": "13 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Natural Language Processing (NLP) has evolved significantly over the last\ndecade. This paper highlights the most important milestones of this period\nwhile trying to pinpoint the contribution of each individual model and\nalgorithm to the overall progress. Furthermore, it focuses on issues still\nremaining to be solved, emphasizing the groundbreaking proposals of\nTransformers, BERT, and all the similar attention-based models.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 16:41:34 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Galanis", "N. -I.", ""], ["Vafiadis", "P.", ""], ["Mirzaev", "K. -G.", ""], ["Papakostas", "G. A.", ""]]}, {"id": "2104.10215", "submitter": "Sopan Khosla", "authors": "Sopan Khosla, James Fiacco, Carolyn Rose", "title": "Evaluating the Impact of a Hierarchical Discourse Representation on\n  Entity Coreference Resolution Performance", "comments": "Also contains the Appendix. Accepted to NAACL 2021 as a short paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent work on entity coreference resolution (CR) follows current trends in\nDeep Learning applied to embeddings and relatively simple task-related\nfeatures. SOTA models do not make use of hierarchical representations of\ndiscourse structure. In this work, we leverage automatically constructed\ndiscourse parse trees within a neural approach and demonstrate a significant\nimprovement on two benchmark entity coreference-resolution datasets. We explore\nhow the impact varies depending upon the type of mention.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 19:14:57 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Khosla", "Sopan", ""], ["Fiacco", "James", ""], ["Rose", "Carolyn", ""]]}, {"id": "2104.10241", "submitter": "Dapeng Zhao", "authors": "Dapeng Zhao", "title": "Predicting Human Trajectories by Learning and Matching Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Thesis document of the degree of Master of Science in Robotics of Carnegie\nMellon University School of Computer Science.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 20:36:27 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 16:07:52 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Zhao", "Dapeng", ""]]}, {"id": "2104.10262", "submitter": "Andr\\'es Molina-Markham", "authors": "Andres Molina-Markham, Ransom K. Winder, Ahmad Ridley", "title": "Network Defense is Not a Game", "comments": "AI4CS, April 2021, NY. arXiv admin note: substantial text overlap\n  with arXiv:2103.07583", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research seeks to apply Artificial Intelligence (AI) to scale and extend the\ncapabilities of human operators to defend networks. A fundamental problem that\nhinders the generalization of successful AI approaches -- i.e., beating humans\nat playing games -- is that network defense cannot be defined as a single game\nwith a fixed set of rules. Our position is that network defense is better\ncharacterized as a collection of games with uncertain and possibly drifting\nrules. Hence, we propose to define network defense tasks as distributions of\nnetwork environments, to: (i) enable research to apply modern AI techniques,\nsuch as unsupervised curriculum learning and reinforcement learning for network\ndefense; and, (ii) facilitate the design of well-defined challenges that can be\nused to compare approaches for autonomous cyberdefense.\n  To demonstrate that an approach for autonomous network defense is practical\nit is important to be able to reason about the boundaries of its applicability.\nHence, we need to be able to define network defense tasks that capture sets of\nadversarial tactics, techniques, and procedures (TTPs); quality of service\n(QoS) requirements; and TTPs available to defenders. Furthermore, the\nabstractions to define these tasks must be extensible; must be backed by\nwell-defined semantics that allow us to reason about distributions of\nenvironments; and should enable the generation of data and experiences from\nwhich an agent can learn.\n  Our approach named Network Environment Design for Autonomous Cyberdefense\ninspired the architecture of FARLAND, a Framework for Advanced Reinforcement\nLearning for Autonomous Network Defense, which we use at MITRE to develop RL\nnetwork defenders that perform blue actions from the MITRE Shield matrix\nagainst attackers with TTPs that drift from MITRE ATT&CK TTPs.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 21:52:51 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Molina-Markham", "Andres", ""], ["Winder", "Ransom K.", ""], ["Ridley", "Ahmad", ""]]}, {"id": "2104.10270", "submitter": "Andrea Bruera", "authors": "Andrea Bruera and Aur\\'elie Herbelot", "title": "Novel Aficionados and Doppelg\\\"angers: a referential task for semantic\n  representations of individual entities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In human semantic cognition, proper names (names which refer to individual\nentities) are harder to learn and retrieve than common nouns. This seems to be\nthe case for machine learning algorithms too, but the linguistic and\ndistributional reasons for this behaviour have not been investigated in depth\nso far. To tackle this issue, we show that the semantic distinction between\nproper names and common nouns is reflected in their linguistic distributions by\nemploying an original task for distributional semantics, the Doppelg\\\"anger\ntest, an extensive set of models, and a new dataset, the Novel Aficionados\ndataset. The results indicate that the distributional representations of\ndifferent individual entities are less clearly distinguishable from each other\nthan those of common nouns, an outcome which intriguingly mirrors human\ncognition.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 22:24:19 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Bruera", "Andrea", ""], ["Herbelot", "Aur\u00e9lie", ""]]}, {"id": "2104.10317", "submitter": "Zhiling Zhang", "authors": "Zhiling Zhang, Kenny Q. Zhu", "title": "Diverse and Specific Clarification Question Generation with Keywords", "comments": "11 pages, 3 figures, WWW 2021", "journal-ref": null, "doi": "10.1145/3442381.3449876", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Product descriptions on e-commerce websites often suffer from missing\nimportant aspects. Clarification question generation (CQGen) can be a promising\napproach to help alleviate the problem. Unlike traditional QGen assuming the\nexistence of answers in the context and generating questions accordingly, CQGen\nmimics user behaviors of asking for unstated information. The generated CQs can\nserve as a sanity check or proofreading to help e-commerce merchant to identify\npotential missing information before advertising their product, and improve\nconsumer experience consequently. Due to the variety of possible user\nbackgrounds and use cases, the information need can be quite diverse but also\nspecific to a detailed topic, while previous works assume generating one CQ per\ncontext and the results tend to be generic. We thus propose the task of Diverse\nCQGen and also tackle the challenge of specificity. We propose a new model\nnamed KPCNet, which generates CQs with Keyword Prediction and Conditioning, to\ndeal with the tasks. Automatic and human evaluation on 2 datasets (Home &\nKitchen, Office) showed that KPCNet can generate more specific questions and\npromote better group-level diversity than several competing baselines.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 02:29:33 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Zhang", "Zhiling", ""], ["Zhu", "Kenny Q.", ""]]}, {"id": "2104.10319", "submitter": "Frederico Araujo", "authors": "Frederico Araujo and Dhilung Kirat and Xiaokui Shu and Teryl Taylor\n  and Jiyong Jang", "title": "Evidential Cyber Threat Hunting", "comments": "5 pages, SDM AI4CS 2021", "journal-ref": "In Proceedings of the 2021 SIAM AI/ML for Cybersecurity Workshop\n  (AI4CS)", "doi": null, "report-no": null, "categories": "cs.CR cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A formal cyber reasoning framework for automating the threat hunting process\nis described. The new cyber reasoning methodology introduces an operational\nsemantics that operates over three subspaces -- knowledge, hypothesis, and\naction -- to enable human-machine co-creation of threat hypotheses and\nprotective recommendations. An implementation of this framework shows that the\napproach is practical and can be used to generalize evidence-based\nmulti-criteria threat investigations.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 02:38:29 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Araujo", "Frederico", ""], ["Kirat", "Dhilung", ""], ["Shu", "Xiaokui", ""], ["Taylor", "Teryl", ""], ["Jang", "Jiyong", ""]]}, {"id": "2104.10336", "submitter": "Haiqin Yang", "authors": "Jian Ma, Shuyi Xie, Haiqin Yang, Lianxin Jiang, Mengyuan Zhou, Xiaoyi\n  Ruan, Yang Mo", "title": "MagicPai at SemEval-2021 Task 7: Method for Detecting and Rating Humor\n  Based on Multi-Task Adversarial Training", "comments": "7 pages, 1 figure, 4 tables", "journal-ref": "A system for SemEval-2021 Task 7", "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper describes MagicPai's system for SemEval 2021 Task 7, HaHackathon:\nDetecting and Rating Humor and Offense. This task aims to detect whether the\ntext is humorous and how humorous it is. There are four subtasks in the\ncompetition. In this paper, we mainly present our solution, a multi-task\nlearning model based on adversarial examples, for task 1a and 1b. More\nspecifically, we first vectorize the cleaned dataset and add the perturbation\nto obtain more robust embedding representations. We then correct the loss via\nthe confidence level. Finally, we perform interactive joint learning on\nmultiple tasks to capture the relationship between whether the text is humorous\nand how humorous it is. The final result shows the effectiveness of our system.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 03:23:02 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 08:41:59 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Ma", "Jian", ""], ["Xie", "Shuyi", ""], ["Yang", "Haiqin", ""], ["Jiang", "Lianxin", ""], ["Zhou", "Mengyuan", ""], ["Ruan", "Xiaoyi", ""], ["Mo", "Yang", ""]]}, {"id": "2104.10340", "submitter": "Wangzhi Li", "authors": "Wangzhi Li, Yaxing Cai, Ujwal Dinesha, Yongjie Fu, Xuan Di", "title": "CVLight: Deep Reinforcement Learning for Adaptive Traffic Signal Control\n  with Connected Vehicles", "comments": "27 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper develops a reinforcement learning (RL) scheme for adaptive traffic\nsignal control (ATSC), called \"CVLight\", that leverages data collected only\nfrom connected vehicles (CV). Seven types of RL models are proposed within this\nscheme that contain various state and reward representations, including\nincorporation of CV delay and green light duration into state and the usage of\nCV delay as reward. To further incorporate information of both CV and non-CV\ninto CVLight, an algorithm based on actor-critic, A2C-Full, is proposed where\nboth CV and non-CV information is used to train the critic network, while only\nCV information is used to update the policy network and execute optimal signal\ntiming. These models are compared at an isolated intersection under various CV\nmarket penetration rates. A full model with the best performance (i.e., minimum\naverage travel delay per vehicle) is then selected and applied to compare with\nstate-of-the-art benchmarks under different levels of traffic demands, turning\nproportions, and dynamic traffic demands, respectively. Two case studies are\nperformed on an isolated intersection and a corridor with three consecutive\nintersections located in Manhattan, New York, to further demonstrate the\neffectiveness of the proposed algorithm under real-world scenarios. Compared to\nother baseline models that use all vehicle information, the trained CVLight\nagent can efficiently control multiple intersections solely based on CV data\nand can achieve a similar or even greater performance when the CV penetration\nrate is no less than 20%.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 03:38:11 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Li", "Wangzhi", ""], ["Cai", "Yaxing", ""], ["Dinesha", "Ujwal", ""], ["Fu", "Yongjie", ""], ["Di", "Xuan", ""]]}, {"id": "2104.10353", "submitter": "Zixuan Li", "authors": "Zixuan Li, Xiaolong Jin, Wei Li, Saiping Guan, Jiafeng Guo, Huawei\n  Shen, Yuanzhuo Wang and Xueqi Cheng", "title": "Temporal Knowledge Graph Reasoning Based on Evolutional Representation\n  Learning", "comments": "SIGIR 2021 Full Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge Graph (KG) reasoning that predicts missing facts for incomplete KGs\nhas been widely explored. However, reasoning over Temporal KG (TKG) that\npredicts facts in the future is still far from resolved. The key to predict\nfuture facts is to thoroughly understand the historical facts. A TKG is\nactually a sequence of KGs corresponding to different timestamps, where all\nconcurrent facts in each KG exhibit structural dependencies and temporally\nadjacent facts carry informative sequential patterns. To capture these\nproperties effectively and efficiently, we propose a novel Recurrent Evolution\nnetwork based on Graph Convolution Network (GCN), called RE-GCN, which learns\nthe evolutional representations of entities and relations at each timestamp by\nmodeling the KG sequence recurrently. Specifically, for the evolution unit, a\nrelation-aware GCN is leveraged to capture the structural dependencies within\nthe KG at each timestamp. In order to capture the sequential patterns of all\nfacts in parallel, the historical KG sequence is modeled auto-regressively by\nthe gate recurrent components. Moreover, the static properties of entities such\nas entity types, are also incorporated via a static graph constraint component\nto obtain better entity representations. Fact prediction at future timestamps\ncan then be realized based on the evolutional entity and relation\nrepresentations. Extensive experiments demonstrate that the RE-GCN model\nobtains substantial performance and efficiency improvement for the temporal\nreasoning tasks on six benchmark datasets. Especially, it achieves up to\n11.46\\% improvement in MRR for entity prediction with up to 82 times speedup\ncomparing to the state-of-the-art baseline.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 05:12:21 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Li", "Zixuan", ""], ["Jin", "Xiaolong", ""], ["Li", "Wei", ""], ["Guan", "Saiping", ""], ["Guo", "Jiafeng", ""], ["Shen", "Huawei", ""], ["Wang", "Yuanzhuo", ""], ["Cheng", "Xueqi", ""]]}, {"id": "2104.10355", "submitter": "Wei-Lun Chao", "authors": "Jihyung Kil, Wei-Lun Chao", "title": "Revisiting Document Representations for Large-Scale Zero-Shot Learning", "comments": "Accepted to NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Zero-shot learning aims to recognize unseen objects using their semantic\nrepresentations. Most existing works use visual attributes labeled by humans,\nnot suitable for large-scale applications. In this paper, we revisit the use of\ndocuments as semantic representations. We argue that documents like Wikipedia\npages contain rich visual information, which however can easily be buried by\nthe vast amount of non-visual sentences. To address this issue, we propose a\nsemi-automatic mechanism for visual sentence extraction that leverages the\ndocument section headers and the clustering structure of visual sentences. The\nextracted visual sentences, after a novel weighting scheme to distinguish\nsimilar classes, essentially form semantic representations like visual\nattributes but need much less human effort. On the ImageNet dataset with over\n10,000 unseen classes, our representations lead to a 64% relative improvement\nagainst the commonly used ones.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 05:17:55 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Kil", "Jihyung", ""], ["Chao", "Wei-Lun", ""]]}, {"id": "2104.10366", "submitter": "Haiqin Yang", "authors": "Xiaoyi Ruan, Meizhi Jin, Jian Ma, Haiqin Yang, Lianxin Jiang, Yang Mo,\n  Mengyuan Zhou", "title": "Sattiy at SemEval-2021 Task 9: An Ensemble Solution for Statement\n  Verification and Evidence Finding with Tables", "comments": "7 pages, 1 figure, 3 tables, the champion solution for SemEval-21\n  Task 9", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Question answering from semi-structured tables can be seen as a semantic\nparsing task and is significant and practical for pushing the boundary of\nnatural language understanding. Existing research mainly focuses on\nunderstanding contents from unstructured evidence, e.g., news, natural language\nsentences, and documents. The task of verification from structured evidence,\nsuch as tables, charts, and databases, is still less explored. This paper\ndescribes sattiy team's system in SemEval-2021 task 9: Statement Verification\nand Evidence Finding with Tables (SEM-TAB-FACT). This competition aims to\nverify statements and to find evidence from tables for scientific articles and\nto promote the proper interpretation of the surrounding article. In this paper,\nwe exploited ensemble models of pre-trained language models over tables, TaPas\nand TaBERT, for Task A and adjust the result based on some rules extracted for\nTask B. Finally, in the leaderboard, we attain the F1 scores of 0.8496 and\n0.7732 in Task A for the 2-way and 3-way evaluation, respectively, and the F1\nscore of 0.4856 in Task B.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 06:11:49 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 08:39:25 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Ruan", "Xiaoyi", ""], ["Jin", "Meizhi", ""], ["Ma", "Jian", ""], ["Yang", "Haiqin", ""], ["Jiang", "Lianxin", ""], ["Mo", "Yang", ""], ["Zhou", "Mengyuan", ""]]}, {"id": "2104.10375", "submitter": "Haiqin Yang", "authors": "Shuyi Xie, Jian Ma, Haiqin Yang, Lianxin Jiang, Yang Mo, Jianping Shen", "title": "PALI at SemEval-2021 Task 2: Fine-Tune XLM-RoBERTa for Word in Context\n  Disambiguation", "comments": "7 pages, 2 figures, 2 tables, winning solution on En-Ar, En-Fr,\n  En-Ru, and En-Zh cross-lingual tasks of SemEval'21 Task 2", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents the PALI team's winning system for SemEval-2021 Task 2:\nMultilingual and Cross-lingual Word-in-Context Disambiguation. We fine-tune\nXLM-RoBERTa model to solve the task of word in context disambiguation, i.e., to\ndetermine whether the target word in the two contexts contains the same meaning\nor not. In the implementation, we first specifically design an input tag to\nemphasize the target word in the contexts. Second, we construct a new vector on\nthe fine-tuned embeddings from XLM-RoBERTa and feed it to a fully-connected\nnetwork to output the probability of whether the target word in the context has\nthe same meaning or not. The new vector is attained by concatenating the\nembedding of the [CLS] token and the embeddings of the target word in the\ncontexts. In training, we explore several tricks, such as the Ranger optimizer,\ndata augmentation, and adversarial training, to improve the model prediction.\nConsequently, we attain first place in all four cross-lingual tasks.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 06:24:49 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 08:35:35 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Xie", "Shuyi", ""], ["Ma", "Jian", ""], ["Yang", "Haiqin", ""], ["Jiang", "Lianxin", ""], ["Mo", "Yang", ""], ["Shen", "Jianping", ""]]}, {"id": "2104.10398", "submitter": "Gian Maria Campedelli", "authors": "Gian Maria Campedelli, Mihovil Bartulovic, Kathleen M. Carley", "title": "Learning future terrorist targets through temporal meta-graphs", "comments": "19 pages, 18 figures", "journal-ref": "Sci Rep 11, 8533 (2021)", "doi": "10.1038/s41598-021-87709-7", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In the last 20 years, terrorism has led to hundreds of thousands of deaths\nand massive economic, political, and humanitarian crises in several regions of\nthe world. Using real-world data on attacks occurred in Afghanistan and Iraq\nfrom 2001 to 2018, we propose the use of temporal meta-graphs and deep learning\nto forecast future terrorist targets. Focusing on three event dimensions, i.e.,\nemployed weapons, deployed tactics and chosen targets, meta-graphs map the\nconnections among temporally close attacks, capturing their operational\nsimilarities and dependencies. From these temporal meta-graphs, we derive\n2-day-based time series that measure the centrality of each feature within each\ndimension over time. Formulating the problem in the context of the strategic\nbehavior of terrorist actors, these multivariate temporal sequences are then\nutilized to learn what target types are at the highest risk of being chosen.\nThe paper makes two contributions. First, it demonstrates that engineering the\nfeature space via temporal meta-graphs produces richer knowledge than shallow\ntime-series that only rely on frequency of feature occurrences. Second, the\nperformed experiments reveal that bi-directional LSTM networks achieve superior\nforecasting performance compared to other algorithms, calling for future\nresearch aiming at fully discovering the potential of artificial intelligence\nto counter terrorist violence.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 08:09:57 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Campedelli", "Gian Maria", ""], ["Bartulovic", "Mihovil", ""], ["Carley", "Kathleen M.", ""]]}, {"id": "2104.10400", "submitter": "Xu Chenxin", "authors": "Chenxin Xu, Rong Xia, Yong Xiao, Yingyu Li, Guangming Shi, Kwang-cheng\n  Chen", "title": "Federated Traffic Synthesizing and Classification Using Generative\n  Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the fast growing demand on new services and applications as well as the\nincreasing awareness of data protection, traditional centralized traffic\nclassification approaches are facing unprecedented challenges. This paper\nintroduces a novel framework, Federated Generative Adversarial Networks and\nAutomatic Classification (FGAN-AC), which integrates decentralized data\nsynthesizing with traffic classification. FGAN-AC is able to synthesize and\nclassify multiple types of service data traffic from decentralized local\ndatasets without requiring a large volume of manually labeled dataset or\ncausing any data leakage. Two types of data synthesizing approaches have been\nproposed and compared: computation-efficient FGAN\n(FGAN-\\uppercase\\expandafter{\\romannumeral1}) and communication-efficient FGAN\n(FGAN-\\uppercase\\expandafter{\\romannumeral2}). The former only implements a\nsingle CNN model for processing each local dataset and the later only requires\ncoordination of intermediate model training parameters. An automatic data\nclassification and model updating framework has been proposed to automatically\nidentify unknown traffic from the synthesized data samples and create new\npseudo-labels for model training. Numerical results show that our proposed\nframework has the ability to synthesize highly mixed service data traffic and\ncan significantly improve the traffic classification performance compared to\nexisting solutions.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 08:10:46 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Xu", "Chenxin", ""], ["Xia", "Rong", ""], ["Xiao", "Yong", ""], ["Li", "Yingyu", ""], ["Shi", "Guangming", ""], ["Chen", "Kwang-cheng", ""]]}, {"id": "2104.10401", "submitter": "Sang Hun Lee", "authors": "Sangrok Lee, Taekang Woo, Sang Hun Lee", "title": "Multi-Attention-Based Soft Partition Network for Vehicle\n  Re-Identification", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Vehicle re-identification (Re-ID) distinguishes between the same vehicle and\nother vehicles in images. It is challenging due to significant intra-instance\ndifferences between identical vehicles from different views and subtle\ninter-instance differences of similar vehicles. Researchers have tried to\naddress this problem by extracting features robust to variations of viewpoints\nand environments. More recently, they tried to improve performance by using\nadditional metadata such as key points, orientation, and temporal information.\nAlthough these attempts have been relatively successful, they all require\nexpensive annotations. Therefore, this paper proposes a novel deep neural\nnetwork called a multi-attention-based soft partition (MUSP) network to solve\nthis problem. This network does not use metadata and only uses multiple soft\nattentions to identify a specific vehicle area. This function was performed by\nmetadata in previous studies. Experiments verified that MUSP achieved\nstate-of-the-art (SOTA) performance for the VehicleID dataset without any\nadditional annotations and was comparable to VeRi-776 and VERI-Wild.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 08:13:17 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Lee", "Sangrok", ""], ["Woo", "Taekang", ""], ["Lee", "Sang Hun", ""]]}, {"id": "2104.10414", "submitter": "Yao Gao", "authors": "Zhong-Qiu Zhao, Yao Gao, Yuchen Ge and Weidong Tian", "title": "Orderly Dual-Teacher Knowledge Distillation for Lightweight Human Pose\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep convolution neural networks (DCNN) have achieved excellent\nperformance in human pose estimation, these networks often have a large number\nof parameters and computations, leading to the slow inference speed. For this\nissue, an effective solution is knowledge distillation, which transfers\nknowledge from a large pre-trained network (teacher) to a small network\n(student). However, there are some defects in the existing approaches: (I) Only\na single teacher is adopted, neglecting the potential that a student can learn\nfrom multiple teachers. (II) The human segmentation mask can be regarded as\nadditional prior information to restrict the location of keypoints, which is\nnever utilized. (III) A student with a small number of parameters cannot fully\nimitate heatmaps provided by datasets and teachers. (IV) There exists noise in\nheatmaps generated by teachers, which causes model degradation. To overcome\nthese defects, we propose an orderly dual-teacher knowledge distillation (ODKD)\nframework, which consists of two teachers with different capabilities.\nSpecifically, the weaker one (primary teacher, PT) is used to teach keypoints\ninformation, the stronger one (senior teacher, ST) is utilized to transfer\nsegmentation and keypoints information by adding the human segmentation mask.\nTaking dual-teacher together, an orderly learning strategy is proposed to\npromote knowledge absorbability. Moreover, we employ a binarization operation\nwhich further improves the learning ability of the student and reduces noise in\nheatmaps. Experimental results on COCO and OCHuman keypoints datasets show that\nour proposed ODKD can improve the performance of different lightweight models\nby a large margin, and HRNet-W16 equipped with ODKD achieves state-of-the-art\nperformance for lightweight human pose estimation.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 08:50:36 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 15:28:36 GMT"}, {"version": "v3", "created": "Thu, 17 Jun 2021 12:01:49 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Zhao", "Zhong-Qiu", ""], ["Gao", "Yao", ""], ["Ge", "Yuchen", ""], ["Tian", "Weidong", ""]]}, {"id": "2104.10424", "submitter": "Saiping Guan", "authors": "Saiping Guan, Xiaolong Jin, Jiafeng Guo, Yuanzhuo Wang, Xueqi Cheng", "title": "Link Prediction on N-ary Relational Data Based on Relatedness Evaluation", "comments": "Accepted to TKDE 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the overwhelming popularity of Knowledge Graphs (KGs), researchers have\npoured attention to link prediction to fill in missing facts for a long time.\nHowever, they mainly focus on link prediction on binary relational data, where\nfacts are usually represented as triples in the form of (head entity, relation,\ntail entity). In practice, n-ary relational facts are also ubiquitous. When\nencountering such facts, existing studies usually decompose them into triples\nby introducing a multitude of auxiliary virtual entities and additional\ntriples. These conversions result in the complexity of carrying out link\nprediction on n-ary relational data. It has even proven that they may cause\nloss of structure information. To overcome these problems, in this paper, we\nrepresent each n-ary relational fact as a set of its role and role-value pairs.\nWe then propose a method called NaLP to conduct link prediction on n-ary\nrelational data, which explicitly models the relatedness of all the role and\nrole-value pairs in an n-ary relational fact. We further extend NaLP by\nintroducing type constraints of roles and role-values without any external\ntype-specific supervision, and proposing a more reasonable negative sampling\nmechanism. Experimental results validate the effectiveness and merits of the\nproposed methods.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 09:06:54 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Guan", "Saiping", ""], ["Jin", "Xiaolong", ""], ["Guo", "Jiafeng", ""], ["Wang", "Yuanzhuo", ""], ["Cheng", "Xueqi", ""]]}, {"id": "2104.10429", "submitter": "Alexander Dockhorn", "authors": "Alexander Dockhorn, Jorge Hurtado-Grueso, Dominik Jeurissen, Linjie\n  Xu, Diego Perez-Liebana", "title": "Portfolio Search and Optimization for General Strategy Game-Playing", "comments": "8 pages, 5 figures, submitted to CEC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Portfolio methods represent a simple but efficient type of action abstraction\nwhich has shown to improve the performance of search-based agents in a range of\nstrategy games. We first review existing portfolio techniques and propose a new\nalgorithm for optimization and action-selection based on the Rolling Horizon\nEvolutionary Algorithm. Moreover, a series of variants are developed to solve\nproblems in different aspects. We further analyze the performance of discussed\nagents in a general strategy game-playing task. For this purpose, we run\nexperiments on three different game-modes of the Stratega framework. For the\noptimization of the agents' parameters and portfolio sets we study the use of\nthe N-tuple Bandit Evolutionary Algorithm. The resulting portfolio sets suggest\na high diversity in play-styles while being able to consistently beat the\nsample agents. An analysis of the agents' performance shows that the proposed\nalgorithm generalizes well to all game-modes and is able to outperform other\nportfolio methods.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 09:28:28 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Dockhorn", "Alexander", ""], ["Hurtado-Grueso", "Jorge", ""], ["Jeurissen", "Dominik", ""], ["Xu", "Linjie", ""], ["Perez-Liebana", "Diego", ""]]}, {"id": "2104.10454", "submitter": "Petr Marek", "authors": "Petr Marek, \\v{S}t\\v{e}p\\'an M\\\"uller, Jakub Konr\\'ad, Petr Lorenc,\n  Jan Pichl and Jan \\v{S}ediv\\'y", "title": "Text Summarization of Czech News Articles Using Named Entities", "comments": null, "journal-ref": "The Prague Bulletin of Mathematical Linguistics 2021 116", "doi": "10.14712/00326585.012", "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The foundation for the research of summarization in the Czech language was\nlaid by the work of Straka et al. (2018). They published the SumeCzech, a large\nCzech news-based summarization dataset, and proposed several baseline\napproaches. However, it is clear from the achieved results that there is a\nlarge space for improvement. In our work, we focus on the impact of named\nentities on the summarization of Czech news articles. First, we annotate\nSumeCzech with named entities. We propose a new metric ROUGE_NE that measures\nthe overlap of named entities between the true and generated summaries, and we\nshow that it is still challenging for summarization systems to reach a high\nscore in it. We propose an extractive summarization approach Named Entity\nDensity that selects a sentence with the highest ratio between a number of\nentities and the length of the sentence as the summary of the article. The\nexperiments show that the proposed approach reached results close to the solid\nbaseline in the domain of news articles selecting the first sentence. Moreover,\nwe demonstrate that the selected sentence reflects the style of reports\nconcisely identifying to whom, when, where, and what happened. We propose that\nsuch a summary is beneficial in combination with the first sentence of an\narticle in voice applications presenting news articles. We propose two\nabstractive summarization approaches based on Seq2Seq architecture. The first\napproach uses the tokens of the article. The second approach has access to the\nnamed entity annotations. The experiments show that both approaches exceed\nstate-of-the-art results previously reported by Straka et al. (2018), with the\nlatter achieving slightly better results on SumeCzech's out-of-domain testing\nset.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 10:48:14 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Marek", "Petr", ""], ["M\u00fcller", "\u0160t\u011bp\u00e1n", ""], ["Konr\u00e1d", "Jakub", ""], ["Lorenc", "Petr", ""], ["Pichl", "Jan", ""], ["\u0160ediv\u00fd", "Jan", ""]]}, {"id": "2104.10455", "submitter": "Alvaro Cabrejas Egea", "authors": "Alvaro Cabrejas-Egea, Raymond Zhang, Neil Walton", "title": "Reinforcement Learning for Traffic Signal Control: Comparison with\n  Commercial Systems", "comments": "8 pages, 13 figures, 3 tables, conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Recently, Intelligent Transportation Systems are leveraging the power of\nincreased sensory coverage and computing power to deliver data-intensive\nsolutions achieving higher levels of performance than traditional systems.\nWithin Traffic Signal Control (TSC), this has allowed the emergence of Machine\nLearning (ML) based systems. Among this group, Reinforcement Learning (RL)\napproaches have performed particularly well. Given the lack of industry\nstandards in ML for TSC, literature exploring RL often lacks comparison against\ncommercially available systems and straightforward formulations of how the\nagents operate. Here we attempt to bridge that gap. We propose three different\narchitectures for TSC RL agents and compare them against the currently used\ncommercial systems MOVA, SurTrac and Cyclic controllers and provide pseudo-code\nfor them. The agents use variations of Deep Q-Learning and Actor Critic, using\nstates and rewards based on queue lengths. Their performance is compared in\nacross different map scenarios with variable demand, assessing them in terms of\nthe global delay and average queue length. We find that the RL-based systems\ncan significantly and consistently achieve lower delays when compared with\nexisting commercial systems.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 10:48:48 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 11:40:14 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Cabrejas-Egea", "Alvaro", ""], ["Zhang", "Raymond", ""], ["Walton", "Neil", ""]]}, {"id": "2104.10459", "submitter": "Kenneth Co", "authors": "Kenneth T. Co, David Martinez Rego, Emil C. Lupu", "title": "Jacobian Regularization for Mitigating Universal Adversarial\n  Perturbations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Universal Adversarial Perturbations (UAPs) are input perturbations that can\nfool a neural network on large sets of data. They are a class of attacks that\nrepresents a significant threat as they facilitate realistic, practical, and\nlow-cost attacks on neural networks. In this work, we derive upper bounds for\nthe effectiveness of UAPs based on norms of data-dependent Jacobians. We\nempirically verify that Jacobian regularization greatly increases model\nrobustness to UAPs by up to four times whilst maintaining clean performance.\nOur theoretical analysis also allows us to formulate a metric for the strength\nof shared adversarial perturbations between pairs of inputs. We apply this\nmetric to benchmark datasets and show that it is highly correlated with the\nactual observed robustness. This suggests that realistic and practical\nuniversal attacks can be reliably mitigated without sacrificing clean accuracy,\nwhich shows promise for the robustness of machine learning systems.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 11:00:21 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Co", "Kenneth T.", ""], ["Rego", "David Martinez", ""], ["Lupu", "Emil C.", ""]]}, {"id": "2104.10501", "submitter": "Shouhua Zhang", "authors": "Jiehan Zhou, Shouhua Zhang, Qinghua Lu, Wenbin Dai, Min Chen, Xin Liu,\n  Susanna Pirttikangas, Yang Shi, Weishan Zhang, Enrique Herrera-Viedma", "title": "A Survey on Federated Learning and its Applications for Accelerating\n  Industrial Internet of Things", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) brings collaborative intelligence into industries\nwithout centralized training data to accelerate the process of Industry 4.0 on\nthe edge computing level. FL solves the dilemma in which enterprises wish to\nmake the use of data intelligence with security concerns. To accelerate\nindustrial Internet of things with the further leverage of FL, existing\nachievements on FL are developed from three aspects: 1) define terminologies\nand elaborate a general framework of FL for accommodating various scenarios; 2)\ndiscuss the state-of-the-art of FL on fundamental researches including data\npartitioning, privacy preservation, model optimization, local model\ntransportation, personalization, motivation mechanism, platform & tools, and\nbenchmark; 3) discuss the impacts of FL from the economic perspective. To\nattract more attention from industrial academia and practice, a FL-transformed\nmanufacturing paradigm is presented, and future research directions of FL are\ngiven and possible immediate applications in Industry 4.0 domain are also\nproposed.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 12:40:11 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Zhou", "Jiehan", ""], ["Zhang", "Shouhua", ""], ["Lu", "Qinghua", ""], ["Dai", "Wenbin", ""], ["Chen", "Min", ""], ["Liu", "Xin", ""], ["Pirttikangas", "Susanna", ""], ["Shi", "Yang", ""], ["Zhang", "Weishan", ""], ["Herrera-Viedma", "Enrique", ""]]}, {"id": "2104.10527", "submitter": "Mike Huisman", "authors": "Mike Huisman and Aske Plaat and Jan N. van Rijn", "title": "Stateless Neural Meta-Learning using Second-Order Gradients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning typically requires large data sets and much compute power for\neach new problem that is learned. Meta-learning can be used to learn a good\nprior that facilitates quick learning, thereby relaxing these requirements so\nthat new tasks can be learned quicker; two popular approaches are MAML and the\nmeta-learner LSTM. In this work, we compare the two and formally show that the\nmeta-learner LSTM subsumes MAML. Combining this insight with recent empirical\nfindings, we construct a new algorithm (dubbed TURTLE) which is simpler than\nthe meta-learner LSTM yet more expressive than MAML. TURTLE outperforms both\ntechniques at few-shot sine wave regression and image classification on\nminiImageNet and CUB without any additional hyperparameter tuning, at a\ncomputational cost that is comparable with second-order MAML. The key to\nTURTLE's success lies in the use of second-order gradients, which also\nsignificantly increases the performance of the meta-learner LSTM by 1-6%\naccuracy.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 13:34:31 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Huisman", "Mike", ""], ["Plaat", "Aske", ""], ["van Rijn", "Jan N.", ""]]}, {"id": "2104.10529", "submitter": "Li Yang", "authors": "Li Yang, Abdallah Shami", "title": "A Lightweight Concept Drift Detection and Adaptation Framework for IoT\n  Data Streams", "comments": "Accepted and to appear in IEEE Internet of Things Magazine; Code is\n  available at Github\n  link:https://github.com/Western-OC2-Lab/OASW-Concept-Drift-Detection-and-Adaptation", "journal-ref": null, "doi": "10.1109/IOTM.0001.2100012", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, with the increasing popularity of \"Smart Technology\", the\nnumber of Internet of Things (IoT) devices and systems have surged\nsignificantly. Various IoT services and functionalities are based on the\nanalytics of IoT streaming data. However, IoT data analytics faces concept\ndrift challenges due to the dynamic nature of IoT systems and the ever-changing\npatterns of IoT data streams. In this article, we propose an adaptive IoT\nstreaming data analytics framework for anomaly detection use cases based on\noptimized LightGBM and concept drift adaptation. A novel drift adaptation\nmethod named Optimized Adaptive and Sliding Windowing (OASW) is proposed to\nadapt to the pattern changes of online IoT data streams. Experiments on two\npublic datasets show the high accuracy and efficiency of our proposed adaptive\nLightGBM model compared against other state-of-the-art approaches. The proposed\nadaptive LightGBM model can perform continuous learning and drift adaptation on\nIoT data streams without human intervention.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 13:41:41 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Yang", "Li", ""], ["Shami", "Abdallah", ""]]}, {"id": "2104.10535", "submitter": "Matias Greco", "authors": "Pablo Araneda, Matias Greco, Jorge Baier", "title": "Exploiting Learned Policies in Focal Search", "comments": "Accepted in SoCS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent machine-learning approaches to deterministic search and\ndomain-independent planning employ policy learning to speed up search.\nUnfortunately, when attempting to solve a search problem by successively\napplying a policy, no guarantees can be given on solution quality. The problem\nof how to effectively use a learned policy within a bounded-suboptimal search\nalgorithm remains largely as an open question. In this paper, we propose\nvarious ways in which such policies can be integrated into Focal Search,\nassuming that the policy is a neural network classifier. Furthermore, we\nprovide mathematical foundations for some of the resulting algorithms. To\nevaluate the resulting algorithms over a number of policies with varying\naccuracy, we use synthetic policies which can be generated for a target\naccuracy for problems where the search space can be held in memory. We evaluate\nour focal search variants over three benchmark domains using our synthetic\napproach, and on the 15-puzzle using a neural network learned using 1.5 million\nexamples. We observe that \\emph{Discrepancy Focal Search}, which we show\nexpands the node which maximizes an approximation of the probability that its\ncorresponding path is a prefix of an optimal path, obtains, in general, the\nbest results in terms of runtime and solution quality.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 13:50:40 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Araneda", "Pablo", ""], ["Greco", "Matias", ""], ["Baier", "Jorge", ""]]}, {"id": "2104.10544", "submitter": "James Townsend", "authors": "James Townsend", "title": "Lossless Compression with Latent Variable Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT stat.CO stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We develop a simple and elegant method for lossless compression using latent\nvariable models, which we call 'bits back with asymmetric numeral systems'\n(BB-ANS). The method involves interleaving encode and decode steps, and\nachieves an optimal rate when compressing batches of data. We demonstrate it\nfirstly on the MNIST test set, showing that state-of-the-art lossless\ncompression is possible using a small variational autoencoder (VAE) model. We\nthen make use of a novel empirical insight, that fully convolutional generative\nmodels, trained on small images, are able to generalize to images of arbitrary\nsize, and extend BB-ANS to hierarchical latent variable models, enabling\nstate-of-the-art lossless compression of full-size colour images from the\nImageNet dataset. We describe 'Craystack', a modular software framework which\nwe have developed for rapid prototyping of compression using deep generative\nmodels.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 14:03:05 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 09:28:41 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Townsend", "James", ""]]}, {"id": "2104.10563", "submitter": "Samim Ahmadi", "authors": "Samim Ahmadi, Linh K\\\"astner, Jan Christian Hauffen, Peter Jung,\n  Mathias Ziegler", "title": "Photothermal-SR-Net: A Customized Deep Unfolding Neural Network for\n  Photothermal Super Resolution Imaging", "comments": "10 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV physics.app-ph physics.comp-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents deep unfolding neural networks to handle inverse problems\nin photothermal radiometry enabling super resolution (SR) imaging. Photothermal\nimaging is a well-known technique in active thermography for nondestructive\ninspection of defects in materials such as metals or composites. A grand\nchallenge of active thermography is to overcome the spatial resolution\nlimitation imposed by heat diffusion in order to accurately resolve each\ndefect. The photothermal SR approach enables to extract high-frequency spatial\ncomponents based on the deconvolution with the thermal point spread function.\nHowever, stable deconvolution can only be achieved by using the sparse\nstructure of defect patterns, which often requires tedious, hand-crafted tuning\nof hyperparameters and results in computationally intensive algorithms. On this\naccount, Photothermal-SR-Net is proposed in this paper, which performs\ndeconvolution by deep unfolding considering the underlying physics. This\nenables to super resolve 2D thermal images for nondestructive testing with a\nsubstantially improved convergence rate. Since defects appear sparsely in\nmaterials, Photothermal-SR-Net applies trained block-sparsity thresholding to\nthe acquired thermal images in each convolutional layer. The performance of the\nproposed approach is evaluated and discussed using various deep unfolding and\nthresholding approaches applied to 2D thermal images. Subsequently, studies are\nconducted on how to increase the reconstruction quality and the computational\nperformance of Photothermal-SR-Net is evaluated. Thereby, it was found that the\ncomputing time for creating high-resolution images could be significantly\nreduced without decreasing the reconstruction quality by using pixel binning as\na preprocessing step.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 14:41:04 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Ahmadi", "Samim", ""], ["K\u00e4stner", "Linh", ""], ["Hauffen", "Jan Christian", ""], ["Jung", "Peter", ""], ["Ziegler", "Mathias", ""]]}, {"id": "2104.10567", "submitter": "Yueming Lyu", "authors": "Yueming Lyu, Jing Dong, Bo Peng, Wei Wang, Tieniu Tan", "title": "SOGAN: 3D-Aware Shadow and Occlusion Robust GAN for Makeup Transfer", "comments": "Accepted by ACM MM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, virtual makeup applications have become more and more\npopular. However, it is still challenging to propose a robust makeup transfer\nmethod in the real-world environment. Current makeup transfer methods mostly\nwork well on good-conditioned clean makeup images, but transferring makeup that\nexhibits shadow and occlusion is not satisfying. To alleviate it, we propose a\nnovel makeup transfer method, called 3D-Aware Shadow and Occlusion Robust GAN\n(SOGAN). Given the source and the reference faces, we first fit a 3D face model\nand then disentangle the faces into shape and texture. In the texture branch,\nwe map the texture to the UV space and design a UV texture generator to\ntransfer the makeup. Since human faces are symmetrical in the UV space, we can\nconveniently remove the undesired shadow and occlusion from the reference image\nby carefully designing a Flip Attention Module (FAM). After obtaining cleaner\nmakeup features from the reference image, a Makeup Transfer Module (MTM) is\nintroduced to perform accurate makeup transfer. The qualitative and\nquantitative experiments demonstrate that our SOGAN not only achieves superior\nresults in shadow and occlusion situations but also performs well in large pose\nand expression variations.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 14:48:49 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 14:16:02 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Lyu", "Yueming", ""], ["Dong", "Jing", ""], ["Peng", "Bo", ""], ["Wang", "Wei", ""], ["Tan", "Tieniu", ""]]}, {"id": "2104.10575", "submitter": "Ron Alford", "authors": "Ron Alford (1), Andy Applebaum (1) ((1) The MITRE Corporation)", "title": "Towards Causal Models for Adversary Distractions", "comments": "To be presented in the AI/ML for Cybersecurity workshop at SDM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Automated adversary emulation is becoming an indispensable tool of network\nsecurity operators in testing and evaluating their cyber defenses. At the same\ntime, it has exposed how quickly adversaries can propagate through the network.\nWhile research has greatly progressed on quality decoy generation to fool human\nadversaries, we may need different strategies to slow computer agents. In this\npaper, we show that decoy generation can slow an automated agent's decision\nprocess, but that the degree to which it is inhibited is greatly dependent on\nthe types of objects used. This points to the need to explicitly evaluate decoy\ngeneration and placement strategies against fast moving, automated adversaries.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 15:02:00 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Alford", "Ron", "", "The MITRE Corporation"], ["Applebaum", "Andy", "", "The MITRE Corporation"]]}, {"id": "2104.10586", "submitter": "Hao Cheng", "authors": "Kaidi Xu, Chenan Wang, Hao Cheng, Bhavya Kailkhura, Xue Lin, Ryan\n  Goldhahn", "title": "Mixture of Robust Experts (MoRE):A Robust Denoising Method towards\n  multiple perturbations", "comments": "This paper is a seminar and dicussing paper, which will not be\n  published and printed anywhere. And it will be keep updating", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To tackle the susceptibility of deep neural networks to examples, the\nadversarial training has been proposed which provides a notion of robust\nthrough an inner maximization problem presenting the first-order embedded\nwithin the outer minimization of the training loss. To generalize the\nadversarial robustness over different perturbation types, the adversarial\ntraining method has been augmented with the improved inner maximization\npresenting a union of multiple perturbations e.g., various $\\ell_p$\nnorm-bounded perturbations.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 15:27:07 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 15:05:42 GMT"}, {"version": "v3", "created": "Mon, 21 Jun 2021 15:57:13 GMT"}, {"version": "v4", "created": "Tue, 20 Jul 2021 06:25:54 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Xu", "Kaidi", ""], ["Wang", "Chenan", ""], ["Cheng", "Hao", ""], ["Kailkhura", "Bhavya", ""], ["Lin", "Xue", ""], ["Goldhahn", "Ryan", ""]]}, {"id": "2104.10602", "submitter": "Yunzhong Hou", "authors": "Yunzhong Hou, Liang Zheng", "title": "Visualizing Adapted Knowledge in Domain Transfer", "comments": null, "journal-ref": "CVPR 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A source model trained on source data and a target model learned through\nunsupervised domain adaptation (UDA) usually encode different knowledge. To\nunderstand the adaptation process, we portray their knowledge difference with\nimage translation. Specifically, we feed a translated image and its original\nversion to the two models respectively, formulating two branches. Through\nupdating the translated image, we force similar outputs from the two branches.\nWhen such requirements are met, differences between the two images can\ncompensate for and hence represent the knowledge difference between models. To\nenforce similar outputs from the two branches and depict the adapted knowledge,\nwe propose a source-free image translation method that generates source-style\nimages using only target images and the two models. We visualize the adapted\nknowledge on several datasets with different UDA methods and find that\ngenerated images successfully capture the style difference between the two\ndomains. For application, we show that generated images enable further tuning\nof the target model without accessing source data. Code available at\nhttps://github.com/hou-yz/DA_visualization.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 17:59:05 GMT"}, {"version": "v2", "created": "Sat, 1 May 2021 11:11:24 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Hou", "Yunzhong", ""], ["Zheng", "Liang", ""]]}, {"id": "2104.10649", "submitter": "Ruiqing Yan", "authors": "Ruiqing Yan, Lanchang Sun, Fang Wang, Xiaoming Zhang", "title": "K-XLNet: A General Method for Combining Explicit Knowledge with Language\n  Model Pretraining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though pre-trained language models such as Bert and XLNet, have rapidly\nadvanced the state-of-the-art on many NLP tasks, they implicit semantics only\nrelying on surface information between words in corpus. Intuitively, background\nknowledge influences the efficacy of understanding. Inspired by this common\nsense, we focus on improving model pretraining by leveraging explicit\nknowledge. Different from recent research that optimize pretraining model by\nknowledge masking strategies, we propose a simple but general method to combine\nexplicit knowledge with pretraining. To be specific, we first match knowledge\nfacts from knowledge graph (KG) and then add a knowledge injunction layer to\ntransformer directly without changing its architecture. The present study seeks\nto find the direct impact of explicit knowledge on transformer per-training. We\nconduct experiments on various datasets for different downstream tasks. The\nexperimental results show that solely by adding external knowledge to\ntransformer can improve the learning performance on many NLP tasks.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 06:14:18 GMT"}, {"version": "v2", "created": "Sat, 29 May 2021 10:08:32 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Yan", "Ruiqing", ""], ["Sun", "Lanchang", ""], ["Wang", "Fang", ""], ["Zhang", "Xiaoming", ""]]}, {"id": "2104.10651", "submitter": "Fabio Cuzzolin", "authors": "Fabio Cuzzolin", "title": "A geometric approach to conditioning belief functions", "comments": "43 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI math.PR math.ST stat.TH", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Conditioning is crucial in applied science when inference involving time\nseries is involved. Belief calculus is an effective way of handling such\ninference in the presence of epistemic uncertainty -- unfortunately, different\napproaches to conditioning in the belief function framework have been proposed\nin the past, leaving the matter somewhat unsettled. Inspired by the geometric\napproach to uncertainty, in this paper we propose an approach to the\nconditioning of belief functions based on geometrically projecting them onto\nthe simplex associated with the conditioning event in the space of all belief\nfunctions. We show here that such a geometric approach to conditioning often\nproduces simple results with straightforward interpretations in terms of\ndegrees of belief. This raises the question of whether classical approaches,\nsuch as for instance Dempster's conditioning, can also be reduced to some form\nof distance minimisation in a suitable space. The study of families of\ncombination rules generated by (geometric) conditioning rules appears to be the\nnatural prosecution of the presented research.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 17:24:19 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Cuzzolin", "Fabio", ""]]}, {"id": "2104.10671", "submitter": "Yongfeng Zhang", "authors": "Yunqi Li, Hanxiong Chen, Zuohui Fu, Yingqiang Ge, Yongfeng Zhang", "title": "User-oriented Fairness in Recommendation", "comments": "Accepted to the 30th Web Conference (WWW 2021)", "journal-ref": null, "doi": "10.1145/3442381.3449866", "report-no": null, "categories": "cs.IR cs.AI cs.HC cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a highly data-driven application, recommender systems could be affected by\ndata bias, resulting in unfair results for different data groups, which could\nbe a reason that affects the system performance. Therefore, it is important to\nidentify and solve the unfairness issues in recommendation scenarios. In this\npaper, we address the unfairness problem in recommender systems from the user\nperspective. We group users into advantaged and disadvantaged groups according\nto their level of activity, and conduct experiments to show that current\nrecommender systems will behave unfairly between two groups of users.\nSpecifically, the advantaged users (active) who only account for a small\nproportion in data enjoy much higher recommendation quality than those\ndisadvantaged users (inactive). Such bias can also affect the overall\nperformance since the disadvantaged users are the majority. To solve this\nproblem, we provide a re-ranking approach to mitigate this unfairness problem\nby adding constraints over evaluation metrics. The experiments we conducted on\nseveral real-world datasets with various recommendation algorithms show that\nour approach can not only improve group fairness of users in recommender\nsystems, but also achieve better overall recommendation performance.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 17:50:31 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Li", "Yunqi", ""], ["Chen", "Hanxiong", ""], ["Fu", "Zuohui", ""], ["Ge", "Yingqiang", ""], ["Zhang", "Yongfeng", ""]]}, {"id": "2104.10680", "submitter": "Bingyang Wen", "authors": "Bingyang Wen, Luis Oliveros Colon, K.P. Subbalakshmi and R.\n  Chandramouli", "title": "Causal-TGAN: Generating Tabular Data Using Causal Generative Adversarial\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synthetic data generation becomes prevalent as a solution to privacy leakage\nand data shortage. Generative models are designed to generate a realistic\nsynthetic dataset, which can precisely express the data distribution for the\nreal dataset. The generative adversarial networks (GAN), which gain great\nsuccess in the computer vision fields, are doubtlessly used for synthetic data\ngeneration. Though there are prior works that have demonstrated great progress,\nmost of them learn the correlations in the data distributions rather than the\ntrue processes in which the datasets are naturally generated. Correlation is\nnot reliable for it is a statistical technique that only tells linear\ndependencies and is easily affected by the dataset's bias. Causality, which\nencodes all underlying factors of how the real data be naturally generated, is\nmore reliable than correlation. In this work, we propose a causal model named\nCausal Tabular Generative Neural Network (Causal-TGAN) to generate synthetic\ntabular data using the tabular data's causal information. Extensive experiments\non both simulated datasets and real datasets demonstrate the better performance\nof our method when given the true causal graph and a comparable performance\nwhen using the estimated causal graph.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 17:59:41 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Wen", "Bingyang", ""], ["Colon", "Luis Oliveros", ""], ["Subbalakshmi", "K. P.", ""], ["Chandramouli", "R.", ""]]}, {"id": "2104.10683", "submitter": "Arnd Koeppe", "authors": "Arnd Koeppe and Franz Bamer and Michael Selzer and Britta Nestler and\n  Bernd Markert", "title": "Explainable artificial intelligence for mechanics: physics-informing\n  neural networks for constitutive models", "comments": "Preprint - Rev-2 (minor grammatical changes; prevent Google from\n  archiving)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  (Artificial) neural networks have become increasingly popular in mechanics to\naccelerate computations with model order reduction techniques and as universal\nmodels for a wide variety of materials. However, the major disadvantage of\nneural networks remains: their numerous parameters are challenging to interpret\nand explain. Thus, neural networks are often labeled as black boxes, and their\nresults often elude human interpretation. In mechanics, the new and active\nfield of physics-informed neural networks attempts to mitigate this\ndisadvantage by designing deep neural networks on the basis of mechanical\nknowledge. By using this a priori knowledge, deeper and more complex neural\nnetworks became feasible, since the mechanical assumptions could be explained.\nHowever, the internal reasoning and explanation of neural network parameters\nremain mysterious.\n  Complementary to the physics-informed approach, we propose a first step\ntowards a physics-informing approach, which explains neural networks trained on\nmechanical data a posteriori. This novel explainable artificial intelligence\napproach aims at elucidating the black box of neural networks and their\nhigh-dimensional representations. Therein, the principal component analysis\ndecorrelates the distributed representations in cell states of RNNs and allows\nthe comparison to known and fundamental functions. The novel approach is\nsupported by a systematic hyperparameter search strategy that identifies the\nbest neural network architectures and training parameters. The findings of\nthree case studies on fundamental constitutive models (hyperelasticity,\nelastoplasticity, and viscoelasticity) imply that the proposed strategy can\nhelp identify numerical and analytical closed-form solutions to characterize\nnew materials.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 18:38:52 GMT"}, {"version": "v2", "created": "Thu, 20 May 2021 10:27:59 GMT"}, {"version": "v3", "created": "Mon, 14 Jun 2021 08:01:35 GMT"}, {"version": "v4", "created": "Mon, 12 Jul 2021 15:36:21 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Koeppe", "Arnd", ""], ["Bamer", "Franz", ""], ["Selzer", "Michael", ""], ["Nestler", "Britta", ""], ["Markert", "Bernd", ""]]}, {"id": "2104.10715", "submitter": "Rishab Khincha", "authors": "Utkarsh Sarawgi, Rishab Khincha, Wazeer Zulfikar, Satrajit Ghosh,\n  Pattie Maes", "title": "Uncertainty-Aware Boosted Ensembling in Multi-Modal Settings", "comments": "Accepted at IJCNN 2021, to appear in IEEE proceedings. Equal\n  contributions from US, RK and WZ", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reliability of machine learning (ML) systems is crucial in safety-critical\napplications such as healthcare, and uncertainty estimation is a widely\nresearched method to highlight the confidence of ML systems in deployment.\nSequential and parallel ensemble techniques have shown improved performance of\nML systems in multi-modal settings by leveraging the feature sets together. We\npropose an uncertainty-aware boosting technique for multi-modal ensembling in\norder to focus on the data points with higher associated uncertainty estimates,\nrather than the ones with higher loss values. We evaluate this method on\nhealthcare tasks related to Dementia and Parkinson's disease which involve\nreal-world multi-modal speech and text data, wherein our method shows an\nimproved performance. Additional analysis suggests that introducing\nuncertainty-awareness into the boosted ensembles decreases the overall entropy\nof the system, making it more robust to heteroscedasticity in the data, as well\nas better calibrating each of the modalities along with high quality prediction\nintervals. We open-source our entire codebase at\nhttps://github.com/usarawgi911/Uncertainty-aware-boosting\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 18:28:13 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Sarawgi", "Utkarsh", ""], ["Khincha", "Rishab", ""], ["Zulfikar", "Wazeer", ""], ["Ghosh", "Satrajit", ""], ["Maes", "Pattie", ""]]}, {"id": "2104.10719", "submitter": "Biswadeep Chakraborty", "authors": "Biswadeep Chakraborty, Xueyuan She, Saibal Mukhopadhyay", "title": "A Fully Spiking Hybrid Neural Network for Energy-Efficient Object\n  Detection", "comments": "10 pages, Submitted Manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a Fully Spiking Hybrid Neural Network (FSHNN) for\nenergy-efficient and robust object detection in resource-constrained platforms.\nThe network architecture is based on Convolutional SNN using\nleaky-integrate-fire neuron models. The model combines unsupervised Spike\nTime-Dependent Plasticity (STDP) learning with back-propagation (STBP) learning\nmethods and also uses Monte Carlo Dropout to get an estimate of the uncertainty\nerror. FSHNN provides better accuracy compared to DNN based object detectors\nwhile being 150X energy-efficient. It also outperforms these object detectors,\nwhen subjected to noisy input data and less labeled training data with a lower\nuncertainty error.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 18:39:32 GMT"}, {"version": "v2", "created": "Sat, 24 Jul 2021 01:30:47 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Chakraborty", "Biswadeep", ""], ["She", "Xueyuan", ""], ["Mukhopadhyay", "Saibal", ""]]}, {"id": "2104.10726", "submitter": "Xiaoyu Shen", "authors": "Jidong Ge, Yunyun huang, Xiaoyu Shen, Chuanyi Li, Wei Hu and Bin Luo", "title": "Learning Fine-grained Fact-Article Correspondence in Legal Cases", "comments": "Code and dataset are available at https://github.com/gjdnju/MLMN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automatically recommending relevant law articles to a given legal case has\nattracted much attention as it can greatly release human labor from searching\nover the large database of laws. However, current researches only support\ncoarse-grained recommendation where all relevant articles are predicted as a\nwhole without explaining which specific fact each article is relevant with.\nSince one case can be formed of many supporting facts, traversing over them to\nverify the correctness of recommendation results can be time-consuming. We\nbelieve that learning fine-grained correspondence between each single fact and\nlaw articles is crucial for an accurate and trustworthy AI system. With this\nmotivation, we perform a pioneering study and create a corpus with manually\nannotated fact-article correspondences. We treat the learning as a text\nmatching task and propose a multi-level matching network to address it. To help\nthe model better digest the content of law articles, we parse articles in form\nof premise-conclusion pairs with random forest. Experiments show that the\nparsed form yielded better performance and the resulting model surpassed other\npopular text matching baselines. Furthermore, we compare with previous\nresearches and find that establishing the fine-grained fact-article\ncorrespondences can improve the recommendation accuracy by a large margin. Our\nbest system reaches an F1 score of 96.3%, making it of great potential for\npractical use. It can also significantly boost the downstream task of legal\ndecision prediction, increasing the F1 score by up to 12.7%.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 19:06:58 GMT"}, {"version": "v2", "created": "Sat, 24 Apr 2021 11:11:34 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Ge", "Jidong", ""], ["huang", "Yunyun", ""], ["Shen", "Xiaoyu", ""], ["Li", "Chuanyi", ""], ["Hu", "Wei", ""], ["Luo", "Bin", ""]]}, {"id": "2104.10742", "submitter": "John Emanuello Ph.D.", "authors": "Vance Wong and John Emanuello", "title": "Robustness of ML-Enhanced IDS to Stealthy Adversaries", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intrusion Detection Systems (IDS) enhanced with Machine Learning (ML) have\ndemonstrated the capacity to efficiently build a prototype of \"normal\" cyber\nbehaviors in order to detect cyber threats' activity with greater accuracy than\ntraditional rule-based IDS. Because these are largely black boxes, their\nacceptance requires proof of robustness to stealthy adversaries. Since it is\nimpossible to build a baseline from activity completely clean of that of\nmalicious cyber actors (outside of controlled experiments), the training data\nfor deployed models will be poisoned with examples of activity that analysts\nwould want to be alerted about. We train an autoencoder-based anomaly detection\nsystem on network activity with various proportions of malicious activity mixed\nin and demonstrate that they are robust to this sort of poisoning.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 20:00:31 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Wong", "Vance", ""], ["Emanuello", "John", ""]]}, {"id": "2104.10743", "submitter": "Sarath Sreedharan", "authors": "Sarath Sreedharan, Anagha Kulkarni, David E. Smith, Subbarao\n  Kambhampati", "title": "A Unifying Bayesian Formulation of Measures of Interpretability in\n  Human-AI", "comments": "arXiv admin note: substantial text overlap with arXiv:2011.10920", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing approaches for generating human-aware agent behaviors have\nconsidered different measures of interpretability in isolation. Further, these\nmeasures have been studied under differing assumptions, thus precluding the\npossibility of designing a single framework that captures these measures under\nthe same assumptions. In this paper, we present a unifying Bayesian framework\nthat models a human observer's evolving beliefs about an agent and thereby\ndefine the problem of Generalized Human-Aware Planning. We will show that the\ndefinitions of interpretability measures like explicability, legibility and\npredictability from the prior literature fall out as special cases of our\ngeneral framework. Through this framework, we also bring a previously ignored\nfact to light that the human-robot interactions are in effect open-world\nproblems, particularly as a result of modeling the human's beliefs over the\nagent. Since the human may not only hold beliefs unknown to the agent but may\nalso form new hypotheses about the agent when presented with novel or\nunexpected behaviors.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 20:06:33 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Sreedharan", "Sarath", ""], ["Kulkarni", "Anagha", ""], ["Smith", "David E.", ""], ["Kambhampati", "Subbarao", ""]]}, {"id": "2104.10753", "submitter": "Anssi Kanervisto", "authors": "Keishi Ishihara, Anssi Kanervisto, Jun Miura, Ville Hautam\\\"aki", "title": "Multi-task Learning with Attention for End-to-end Autonomous Driving", "comments": "Accepted to CVPR 2021 Workshop on Autonomous Driving", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous driving systems need to handle complex scenarios such as lane\nfollowing, avoiding collisions, taking turns, and responding to traffic\nsignals. In recent years, approaches based on end-to-end behavioral cloning\nhave demonstrated remarkable performance in point-to-point navigational\nscenarios, using a realistic simulator and standard benchmarks. Offline\nimitation learning is readily available, as it does not require expensive hand\nannotation or interaction with the target environment, but it is difficult to\nobtain a reliable system. In addition, existing methods have not specifically\naddressed the learning of reaction for traffic lights, which are a rare\noccurrence in the training datasets. Inspired by the previous work on\nmulti-task learning and attention modeling, we propose a novel multi-task\nattention-aware network in the conditional imitation learning (CIL) framework.\nThis does not only improve the success rate of standard benchmarks, but also\nthe ability to react to traffic lights, which we show with standard benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 20:34:57 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Ishihara", "Keishi", ""], ["Kanervisto", "Anssi", ""], ["Miura", "Jun", ""], ["Hautam\u00e4ki", "Ville", ""]]}, {"id": "2104.10789", "submitter": "Michael Cook", "authors": "Michael Cook", "title": "The Road Less Travelled: Trying And Failing To Generate Walking\n  Simulators", "comments": "Originally written for the Foundations of Digital Games 2021\n  Reflections track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated game design is a rapidly growing area of research, yet many aspects\nof game design lie largely unexamined still, as most systems focus on\ntwo-dimensional games with clear objectives and goal-oriented gameplay. This\npaper describes several attempts to build an automated game designer for 3D\ngames more focused on space, atmosphere and experience. We describe our\nattempts to build these systems, why they failed, and what steps and future\nwork we believe would be useful for future attempts by others.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 23:05:10 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 16:29:16 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Cook", "Michael", ""]]}, {"id": "2104.10796", "submitter": "Zelong Li", "authors": "Zelong Li, Jianchao Ji, Zuohui Fu, Yingqiang Ge, Shuyuan Xu, Chong\n  Chen, Yongfeng Zhang", "title": "Efficient Non-Sampling Knowledge Graph Embedding", "comments": "10 pages, 3 figures. The first two authors contributed equally to the\n  work. Accepted to WWW 2021", "journal-ref": null, "doi": "10.1145/3442381.3449859", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge Graph (KG) is a flexible structure that is able to describe the\ncomplex relationship between data entities. Currently, most KG embedding models\nare trained based on negative sampling, i.e., the model aims to maximize some\nsimilarity of the connected entities in the KG, while minimizing the similarity\nof the sampled disconnected entities. Negative sampling helps to reduce the\ntime complexity of model learning by only considering a subset of negative\ninstances, which may fail to deliver stable model performance due to the\nuncertainty in the sampling procedure. To avoid such deficiency, we propose a\nnew framework for KG embedding -- Efficient Non-Sampling Knowledge Graph\nEmbedding (NS-KGE). The basic idea is to consider all of the negative instances\nin the KG for model learning, and thus to avoid negative sampling. The\nframework can be applied to square-loss based knowledge graph embedding models\nor models whose loss can be converted to a square loss. A natural side-effect\nof this non-sampling strategy is the increased computational complexity of\nmodel learning. To solve the problem, we leverage mathematical derivations to\nreduce the complexity of non-sampling loss function, which eventually provides\nus both better efficiency and better accuracy in KG embedding compared with\nexisting models. Experiments on benchmark datasets show that our NS-KGE\nframework can achieve a better performance on efficiency and accuracy over\ntraditional negative sampling based models, and that the framework is\napplicable to a large class of knowledge graph embedding models.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 23:36:39 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 19:47:54 GMT"}, {"version": "v3", "created": "Wed, 16 Jun 2021 15:25:34 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Li", "Zelong", ""], ["Ji", "Jianchao", ""], ["Fu", "Zuohui", ""], ["Ge", "Yingqiang", ""], ["Xu", "Shuyuan", ""], ["Chen", "Chong", ""], ["Zhang", "Yongfeng", ""]]}, {"id": "2104.10810", "submitter": "Munazza Zaib", "authors": "Munazza Zaib and Quan Z. Sheng and Wei Emma Zhang", "title": "A Short Survey of Pre-trained Language Models for Conversational AI-A\n  NewAge in NLP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building a dialogue system that can communicate naturally with humans is a\nchallenging yet interesting problem of agent-based computing. The rapid growth\nin this area is usually hindered by the long-standing problem of data scarcity\nas these systems are expected to learn syntax, grammar, decision making, and\nreasoning from insufficient amounts of task-specific dataset. The recently\nintroduced pre-trained language models have the potential to address the issue\nof data scarcity and bring considerable advantages by generating contextualized\nword embeddings. These models are considered counterpart of ImageNet in NLP and\nhave demonstrated to capture different facets of language such as hierarchical\nrelations, long-term dependency, and sentiment. In this short survey paper, we\ndiscuss the recent progress made in the field of pre-trained language models.\nWe also deliberate that how the strengths of these language models can be\nleveraged in designing more engaging and more eloquent conversational agents.\nThis paper, therefore, intends to establish whether these pre-trained models\ncan overcome the challenges pertinent to dialogue systems, and how their\narchitecture could be exploited in order to overcome these challenges. Open\nchallenges in the field of dialogue systems have also been deliberated.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 01:00:56 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Zaib", "Munazza", ""], ["Sheng", "Quan Z.", ""], ["Zhang", "Wei Emma", ""]]}, {"id": "2104.10818", "submitter": "Aaron M. Roth", "authors": "Aaron M. Roth, Jing Liang, and Dinesh Manocha", "title": "XAI-N: Sensor-based Robot Navigation using Expert Policies and Decision\n  Trees", "comments": null, "journal-ref": "2021 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS)", "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present a novel sensor-based learning navigation algorithm to compute a\ncollision-free trajectory for a robot in dense and dynamic environments with\nmoving obstacles or targets. Our approach uses deep reinforcement\nlearning-based expert policy that is trained using a sim2real paradigm. In\norder to increase the reliability and handle the failure cases of the expert\npolicy, we combine with a policy extraction technique to transform the\nresulting policy into a decision tree format. The resulting decision tree has\nproperties which we use to analyze and modify the policy and improve\nperformance on navigation metrics including smoothness, frequency of\noscillation, frequency of immobilization, and obstruction of target. We are\nable to modify the policy to address these imperfections without retraining,\ncombining the learning power of deep learning with the control of\ndomain-specific algorithms. We highlight the benefits of our algorithm in\nsimulated environments and navigating a Clearpath Jackal robot among moving\npedestrians. (Videos at this url:\nhttps://gamma.umd.edu/researchdirections/xrl/navviper)\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 01:33:10 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 03:57:42 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Roth", "Aaron M.", ""], ["Liang", "Jing", ""], ["Manocha", "Dinesh", ""]]}, {"id": "2104.10824", "submitter": "Kaidong Li", "authors": "Kaidong Li, Mohammad I. Fathan, Krushi Patel, Tianxiao Zhang, Cuncong\n  Zhong, Ajay Bansal, Amit Rastogi, Jean S. Wang, Guanghui Wang", "title": "Colonoscopy Polyp Detection and Classification: Dataset Creation and\n  Comparative Evaluations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Colorectal cancer (CRC) is one of the most common types of cancer with a high\nmortality rate. Colonoscopy is the preferred procedure for CRC screening and\nhas proven to be effective in reducing CRC mortality. Thus, a reliable\ncomputer-aided polyp detection and classification system can significantly\nincrease the effectiveness of colonoscopy. In this paper, we create an\nendoscopic dataset collected from various sources and annotate the ground truth\nof polyp location and classification results with the help of experienced\ngastroenterologists. The dataset can serve as a benchmark platform to train and\nevaluate the machine learning models for polyp classification. We have also\ncompared the performance of eight state-of-the-art deep learning-based object\ndetection models. The results demonstrate that deep CNN models are promising in\nCRC screening. This work can serve as a baseline for future research in polyp\ndetection and classification.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 01:57:35 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Li", "Kaidong", ""], ["Fathan", "Mohammad I.", ""], ["Patel", "Krushi", ""], ["Zhang", "Tianxiao", ""], ["Zhong", "Cuncong", ""], ["Bansal", "Ajay", ""], ["Rastogi", "Amit", ""], ["Wang", "Jean S.", ""], ["Wang", "Guanghui", ""]]}, {"id": "2104.10845", "submitter": "Li Zhang", "authors": "Yuxuan Chen, Li Zhang, Shijian Li, Gang Pan", "title": "Optimize Neural Fictitious Self-Play in Regret Minimization Thinking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Optimization of deep learning algorithms to approach Nash Equilibrium remains\na significant problem in imperfect information games, e.g. StarCraft and poker.\nNeural Fictitious Self-Play (NFSP) has provided an effective way to learn\napproximate Nash Equilibrium without prior domain knowledge in imperfect\ninformation games. However, optimality gap was left as an optimization problem\nof NFSP and by solving the problem, the performance of NFSP could be improved.\nIn this study, focusing on the optimality gap of NFSP, we have proposed a new\nmethod replacing NFSP's best response computation with regret matching method.\nThe new algorithm can make the optimality gap converge to zero as it iterates,\nthus converge faster than original NFSP. We have conduct experiments on three\ntypical environments of perfect-information games and imperfect information\ngames in OpenSpiel and all showed that our new algorithm performances better\nthan original NFSP.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 03:24:23 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Chen", "Yuxuan", ""], ["Zhang", "Li", ""], ["Li", "Shijian", ""], ["Pan", "Gang", ""]]}, {"id": "2104.10857", "submitter": "Yun Li", "authors": "Yun Li, Zhe Liu, Lina Yao, Xiaojun Chang", "title": "Attribute-Modulated Generative Meta Learning for Zero-Shot\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning (ZSL) aims to transfer knowledge from seen classes to\nsemantically related unseen classes, which are absent during training. The\npromising strategies for ZSL are to synthesize visual features of unseen\nclasses conditioned on semantic side information and to incorporate\nmeta-learning to eliminate the model's inherent bias towards seen classes.\nWhile existing meta generative approaches pursue a common model shared across\ntask distributions, we aim to construct a generative network adaptive to task\ncharacteristics. To this end, we propose an Attribute-Modulated generAtive\nmeta-model for Zero-shot learning (AMAZ). Our model consists of an\nattribute-aware modulation network, an attribute-augmented generative network,\nand an attribute-weighted classifier. Given unseen classes, the modulation\nnetwork adaptively modulates the generator by applying task-specific\ntransformations so that the generative network can adapt to highly diverse\ntasks. The weighted classifier utilizes the data quality to enhance the\ntraining procedure, further improving the model performance. Our empirical\nevaluations on four widely-used benchmarks show that AMAZ outperforms\nstate-of-the-art methods by 3.8% and 3.1% in ZSL and generalized ZSL settings,\nrespectively, demonstrating the superiority of our method. Our experiments on a\nzero-shot image retrieval task show AMAZ's ability to synthesize instances that\nportray real visual characteristics.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 04:16:43 GMT"}, {"version": "v2", "created": "Sat, 24 Jul 2021 12:32:31 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Li", "Yun", ""], ["Liu", "Zhe", ""], ["Yao", "Lina", ""], ["Chang", "Xiaojun", ""]]}, {"id": "2104.10907", "submitter": "Runlong Yu", "authors": "Runlong Yu, Yuyang Ye, Qi Liu, Zihan Wang, Chunfeng Yang, Yucheng Hu,\n  Enhong Chen", "title": "XCrossNet: Feature Structure-Oriented Learning for Click-Through Rate\n  Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Click-Through Rate (CTR) prediction is a core task in nowadays commercial\nrecommender systems. Feature crossing, as the mainline of research on CTR\nprediction, has shown a promising way to enhance predictive performance.\n  Even though various models are able to learn feature interactions without\nmanual feature engineering, they rarely attempt to individually learn\nrepresentations for different feature structures.\n  In particular, they mainly focus on the modeling of cross sparse features but\nneglect to specifically represent cross dense features.\n  Motivated by this, we propose a novel Extreme Cross Network, abbreviated\nXCrossNet, which aims at learning dense and sparse feature interactions in an\nexplicit manner.\n  XCrossNet as a feature structure-oriented model leads to a more expressive\nrepresentation and a more precise CTR prediction, which is not only explicit\nand interpretable, but also time-efficient and easy to implement.\n  Experimental studies on Criteo Kaggle dataset show significant improvement of\nXCrossNet over state-of-the-art models on both effectiveness and efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 07:37:36 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Yu", "Runlong", ""], ["Ye", "Yuyang", ""], ["Liu", "Qi", ""], ["Wang", "Zihan", ""], ["Yang", "Chunfeng", ""], ["Hu", "Yucheng", ""], ["Chen", "Enhong", ""]]}, {"id": "2104.10925", "submitter": "Junhan Yang", "authors": "Junhan Yang, Zheng Liu, Bowen Jin, Jianxun Lian, Defu Lian, Akshay\n  Soni, Eun Yong Kang, Yajun Wang, Guangzhong Sun, Xing Xie", "title": "Hybrid Encoder: Towards Efficient and Precise Native AdsRecommendation\n  via Hybrid Transformer Encoding Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Transformer encoding networks have been proved to be a powerful tool of\nunderstanding natural languages. They are playing a critical role in native ads\nservice, which facilitates the recommendation of appropriate ads based on\nuser's web browsing history. For the sake of efficient recommendation,\nconventional methods would generate user and advertisement embeddings\nindependently with a siamese transformer encoder, such that approximate nearest\nneighbour search (ANN) can be leveraged. Given that the underlying semantic\nabout user and ad can be complicated, such independently generated embeddings\nare prone to information loss, which leads to inferior recommendation quality.\nAlthough another encoding strategy, the cross encoder, can be much more\naccurate, it will lead to huge running cost and become infeasible for realtime\nservices, like native ads recommendation. In this work, we propose hybrid\nencoder, which makes efficient and precise native ads recommendation through\ntwo consecutive steps: retrieval and ranking. In the retrieval step, user and\nad are encoded with a siamese component, which enables relevant candidates to\nbe retrieved via ANN search. In the ranking step, it further represents each ad\nwith disentangled embeddings and each user with ad-related embeddings, which\ncontributes to the fine-grained selection of high-quality ads from the\ncandidate set. Both steps are light-weighted, thanks to the pre-computed and\ncached intermedia results. To optimize the hybrid encoder's performance in this\ntwo-stage workflow, a progressive training pipeline is developed, which builds\nup the model's capability in the retrieval and ranking task step-by-step. The\nhybrid encoder's effectiveness is experimentally verified: with very little\nadditional cost, it outperforms the siamese encoder significantly and achieves\ncomparable recommendation quality as the cross encoder.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 08:42:07 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Yang", "Junhan", ""], ["Liu", "Zheng", ""], ["Jin", "Bowen", ""], ["Lian", "Jianxun", ""], ["Lian", "Defu", ""], ["Soni", "Akshay", ""], ["Kang", "Eun Yong", ""], ["Wang", "Yajun", ""], ["Sun", "Guangzhong", ""], ["Xie", "Xing", ""]]}, {"id": "2104.10935", "submitter": "Peihua Li", "authors": "Jiangtao Xie, Ruiren Zeng, Qilong Wang, Ziqi Zhou, Peihua Li", "title": "So-ViT: Mind Visual Tokens for Vision Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recently the vision transformer (ViT) architecture, where the backbone purely\nconsists of self-attention mechanism, has achieved very promising performance\nin visual classification. However, the high performance of the original ViT\nheavily depends on pretraining using ultra large-scale datasets, and it\nsignificantly underperforms on ImageNet-1K if trained from scratch. This paper\nmakes the efforts toward addressing this problem, by carefully considering the\nrole of visual tokens. First, for classification head, existing ViT only\nexploits class token while entirely neglecting rich semantic information\ninherent in high-level visual tokens. Therefore, we propose a new\nclassification paradigm, where the second-order, cross-covariance pooling of\nvisual tokens is combined with class token for final classification. Meanwhile,\na fast singular value power normalization is proposed for improving the\nsecond-order pooling. Second, the original ViT employs the naive embedding of\nfixed-size image patches, lacking the ability to model translation equivariance\nand locality. To alleviate this problem, we develop a light-weight,\nhierarchical module based on off-the-shelf convolutions for visual token\nembedding. The proposed architecture, which we call So-ViT, is thoroughly\nevaluated on ImageNet-1K. The results show our models, when trained from\nscratch, outperform the competing ViT variants, while being on par with or\nbetter than state-of-the-art CNN models. Code is available at\nhttps://github.com/jiangtaoxie/So-ViT\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 09:05:09 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Xie", "Jiangtao", ""], ["Zeng", "Ruiren", ""], ["Wang", "Qilong", ""], ["Zhou", "Ziqi", ""], ["Li", "Peihua", ""]]}, {"id": "2104.10955", "submitter": "Yanbei Chen", "authors": "Yanbei Chen, Yongqin Xian, A. Sophia Koepke, Ying Shan, Zeynep Akata", "title": "Distilling Audio-Visual Knowledge by Compositional Contrastive Learning", "comments": "Accepted to CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Having access to multi-modal cues (e.g. vision and audio) empowers some\ncognitive tasks to be done faster compared to learning from a single modality.\nIn this work, we propose to transfer knowledge across heterogeneous modalities,\neven though these data modalities may not be semantically correlated. Rather\nthan directly aligning the representations of different modalities, we compose\naudio, image, and video representations across modalities to uncover richer\nmulti-modal knowledge. Our main idea is to learn a compositional embedding that\ncloses the cross-modal semantic gap and captures the task-relevant semantics,\nwhich facilitates pulling together representations across modalities by\ncompositional contrastive learning. We establish a new, comprehensive\nmulti-modal distillation benchmark on three video datasets: UCF101,\nActivityNet, and VGGSound. Moreover, we demonstrate that our model\nsignificantly outperforms a variety of existing knowledge distillation methods\nin transferring audio-visual knowledge to improve video representation\nlearning. Code is released here: https://github.com/yanbeic/CCL.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 09:31:20 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Chen", "Yanbei", ""], ["Xian", "Yongqin", ""], ["Koepke", "A. Sophia", ""], ["Shan", "Ying", ""], ["Akata", "Zeynep", ""]]}, {"id": "2104.10956", "submitter": "Tai Wang", "authors": "Tai Wang, Xinge Zhu, Jiangmiao Pang, Dahua Lin", "title": "FCOS3D: Fully Convolutional One-Stage Monocular 3D Object Detection", "comments": "Technical report for the best vision-only method (1st place of the\n  camera track) in the nuScenes 3D detection challenge of NeurIPS 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Monocular 3D object detection is an important task for autonomous driving\nconsidering its advantage of low cost. It is much more challenging compared to\nconventional 2D case due to its inherent ill-posed property, which is mainly\nreflected on the lack of depth information. Recent progress on 2D detection\noffers opportunities to better solving this problem. However, it is non-trivial\nto make a general adapted 2D detector work in this 3D task. In this technical\nreport, we study this problem with a practice built on fully convolutional\nsingle-stage detector and propose a general framework FCOS3D. Specifically, we\nfirst transform the commonly defined 7-DoF 3D targets to image domain and\ndecouple it as 2D and 3D attributes. Then the objects are distributed to\ndifferent feature levels with the consideration of their 2D scales and assigned\nonly according to the projected 3D-center for training procedure. Furthermore,\nthe center-ness is redefined with a 2D Guassian distribution based on the\n3D-center to fit the 3D target formulation. All of these make this framework\nsimple yet effective, getting rid of any 2D detection or 2D-3D correspondence\npriors. Our solution achieves 1st place out of all the vision-only methods in\nthe nuScenes 3D detection challenge of NeurIPS 2020. Code and models are\nreleased at https://github.com/open-mmlab/mmdetection3d.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 09:35:35 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Wang", "Tai", ""], ["Zhu", "Xinge", ""], ["Pang", "Jiangmiao", ""], ["Lin", "Dahua", ""]]}, {"id": "2104.10970", "submitter": "Antal Jakovac", "authors": "A. Jakovac", "title": "Time series analysis with dynamic law exploration", "comments": "17 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this paper we examine, how the dynamic laws governing the time evolution\nof a time series can be identified. We give a finite difference equation as\nwell as a differential equation representation for that. We also study, how the\nrequired symmetries, like time reversal can be imposed on the laws. We study\nthe compression performance of linear laws on sound data.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 10:08:02 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Jakovac", "A.", ""]]}, {"id": "2104.10984", "submitter": "C\\'edric Marco-Detchart", "authors": "Cedric Marco-Detchart, Giancarlo Lucca, Carlos Lopez-Molina, Laura De\n  Miguel, Gra\\c{c}aliz Pereira Dimuro, Humberto Bustince", "title": "Neuro-inspired edge feature fusion using Choquet integrals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that the human visual system performs a hierarchical information\nprocess in which early vision cues (or primitives) are fused in the visual\ncortex to compose complex shapes and descriptors. While different aspects of\nthe process have been extensively studied, as the lens adaptation or the\nfeature detection, some other,as the feature fusion, have been mostly left\naside. In this work we elaborate on the fusion of early vision primitives using\ngeneralizations of the Choquet integral, and novel aggregation operators that\nhave been extensively studied in recent years. We propose to use\ngeneralizations of the Choquet integral to sensibly fuse elementary edge cues,\nin an attempt to model the behaviour of neurons in the early visual cortex. Our\nproposal leads to a full-framed edge detection algorithm, whose performance is\nput to the test in state-of-the-art boundary detection datasets.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 10:45:52 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Marco-Detchart", "Cedric", ""], ["Lucca", "Giancarlo", ""], ["Lopez-Molina", "Carlos", ""], ["De Miguel", "Laura", ""], ["Dimuro", "Gra\u00e7aliz Pereira", ""], ["Bustince", "Humberto", ""]]}, {"id": "2104.10995", "submitter": "Samuel T. Wauthier", "authors": "Samuel T. Wauthier, Pietro Mazzaglia, Ozan \\c{C}atal, Cedric De Boom,\n  Tim Verbelen, Bart Dhoedt", "title": "A learning gap between neuroscience and reinforcement learning", "comments": "Accepted as a workshop paper at BRAIN2AI @ ICLR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Historically, artificial intelligence has drawn much inspiration from\nneuroscience to fuel advances in the field. However, current progress in\nreinforcement learning is largely focused on benchmark problems that fail to\ncapture many of the aspects that are of interest in neuroscience today. We\nillustrate this point by extending a T-maze task from neuroscience for use with\nreinforcement learning algorithms, and show that state-of-the-art algorithms\nare not capable of solving this problem. Finally, we point out where insights\nfrom neuroscience could help explain some of the issues encountered.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 11:25:21 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 08:38:39 GMT"}, {"version": "v3", "created": "Tue, 4 May 2021 13:01:13 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Wauthier", "Samuel T.", ""], ["Mazzaglia", "Pietro", ""], ["\u00c7atal", "Ozan", ""], ["De Boom", "Cedric", ""], ["Verbelen", "Tim", ""], ["Dhoedt", "Bart", ""]]}, {"id": "2104.11004", "submitter": "Jian Pang", "authors": "Jian Pang, Dacheng Zhang, Huafeng Li, Weifeng Liu, Zhengtao Yu", "title": "Hazy Re-ID: An Interference Suppression Model For Domain Adaptation\n  Person Re-identification Under Inclement Weather Condition", "comments": "Accepted by ICME2021 as oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In a conventional domain adaptation person Re-identification (Re-ID) task,\nboth the training and test images in target domain are collected under the\nsunny weather. However, in reality, the pedestrians to be retrieved may be\nobtained under severe weather conditions such as hazy, dusty and snowing, etc.\nThis paper proposes a novel Interference Suppression Model (ISM) to deal with\nthe interference caused by the hazy weather in domain adaptation person Re-ID.\nA teacherstudent model is used in the ISM to distill the interference\ninformation at the feature level by reducing the discrepancy between the clear\nand the hazy intrinsic similarity matrix. Furthermore, in the distribution\nlevel, the extra discriminator is introduced to assist the student model make\nthe interference feature distribution more clear. The experimental results show\nthat the proposed method achieves the superior performance on two synthetic\ndatasets than the stateof-the-art methods. The related code will be released\nonline https://github.com/pangjian123/ISM-ReID.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 11:59:27 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Pang", "Jian", ""], ["Zhang", "Dacheng", ""], ["Li", "Huafeng", ""], ["Liu", "Weifeng", ""], ["Yu", "Zhengtao", ""]]}, {"id": "2104.11014", "submitter": "Min-Hung Chen", "authors": "Min-Fong Hong, Hao-Yun Chen, Min-Hung Chen, Yu-Syuan Xu, Hsien-Kai\n  Kuo, Yi-Min Tsai, Hung-Jen Chen, Kevin Jou", "title": "Network Space Search for Pareto-Efficient Spaces", "comments": "CVPRW2021 [Oral] (Efficient Deep Learning for Computer Vision\n  Workshop). Website: https://minhungchen.netlify.app/publication/nss", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Network spaces have been known as a critical factor in both handcrafted\nnetwork designs or defining search spaces for Neural Architecture Search (NAS).\nHowever, an effective space involves tremendous prior knowledge and/or manual\neffort, and additional constraints are required to discover efficiency-aware\narchitectures. In this paper, we define a new problem, Network Space Search\n(NSS), as searching for favorable network spaces instead of a single\narchitecture. We propose an NSS method to directly search for efficient-aware\nnetwork spaces automatically, reducing the manual effort and immense cost in\ndiscovering satisfactory ones. The resultant network spaces, named Elite\nSpaces, are discovered from Expanded Search Space with minimal human expertise\nimposed. The Pareto-efficient Elite Spaces are aligned with the Pareto front\nunder various complexity constraints and can be further served as NAS search\nspaces, benefiting differentiable NAS approaches (e.g. In CIFAR-100, an\naveragely 2.3% lower error rate and 3.7% closer to target constraint than the\nbaseline with around 90% fewer samples required to find satisfactory networks).\nMoreover, our NSS approach is capable of searching for superior spaces in\nfuture unexplored spaces, revealing great potential in searching for network\nspaces automatically. Website: https://minhungchen.netlify.app/publication/nss.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 12:23:53 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 03:31:32 GMT"}, {"version": "v3", "created": "Sat, 12 Jun 2021 03:39:42 GMT"}, {"version": "v4", "created": "Tue, 15 Jun 2021 07:36:27 GMT"}, {"version": "v5", "created": "Wed, 16 Jun 2021 00:22:11 GMT"}, {"version": "v6", "created": "Sun, 20 Jun 2021 01:51:58 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Hong", "Min-Fong", ""], ["Chen", "Hao-Yun", ""], ["Chen", "Min-Hung", ""], ["Xu", "Yu-Syuan", ""], ["Kuo", "Hsien-Kai", ""], ["Tsai", "Yi-Min", ""], ["Chen", "Hung-Jen", ""], ["Jou", "Kevin", ""]]}, {"id": "2104.11026", "submitter": "Yang An", "authors": "Yang An and Liang Zhang and Mao You and Xueqing Tian and Bo Jin and\n  Xiaopeng Wei", "title": "MeSIN: Multilevel Selective and Interactive Network for Medication\n  Recommendation", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommending medications for patients using electronic health records (EHRs)\nis a crucial data mining task for an intelligent healthcare system. It can\nassist doctors in making clinical decisions more efficiently. However, the\ninherent complexity of the EHR data renders it as a challenging task: (1)\nMultilevel structures: the EHR data typically contains multilevel structures\nwhich are closely related with the decision-making pathways, e.g., laboratory\nresults lead to disease diagnoses, and then contribute to the prescribed\nmedications; (2) Multiple sequences interactions: multiple sequences in EHR\ndata are usually closely correlated with each other; (3) Abundant noise: lots\nof task-unrelated features or noise information within EHR data generally\nresult in suboptimal performance. To tackle the above challenges, we propose a\nmultilevel selective and interactive network (MeSIN) for medication\nrecommendation. Specifically, MeSIN is designed with three components. First,\nan attentional selective module (ASM) is applied to assign flexible attention\nscores to different medical codes embeddings by their relevance to the\nrecommended medications in every admission. Second, we incorporate a novel\ninteractive long-short term memory network (InLSTM) to reinforce the\ninteractions of multilevel medical sequences in EHR data with the help of the\ncalibrated memory-augmented cell and an enhanced input gate. Finally, we employ\na global selective fusion module (GSFM) to infuse the multi-sourced information\nembeddings into final patient representations for medications recommendation.\nTo validate our method, extensive experiments have been conducted on a\nreal-world clinical dataset. The results demonstrate a consistent superiority\nof our framework over several baselines and testify the effectiveness of our\nproposed approach.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 12:59:50 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["An", "Yang", ""], ["Zhang", "Liang", ""], ["You", "Mao", ""], ["Tian", "Xueqing", ""], ["Jin", "Bo", ""], ["Wei", "Xiaopeng", ""]]}, {"id": "2104.11037", "submitter": "Antonios Liapis", "authors": "Antonios Liapis", "title": "10 Years of the PCG workshop: Past and Future Trends", "comments": "10 pages", "journal-ref": "Proceedings of the FDG Workshop on Procedural Content Generation,\n  2020", "doi": "10.1145/3402942.3409598", "report-no": null, "categories": "cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As of 2020, the international workshop on Procedural Content Generation\nenters its second decade. The annual workshop, hosted by the international\nconference on the Foundations of Digital Games, has collected a corpus of 95\npapers published in its first 10 years. This paper provides an overview of the\nworkshop's activities and surveys the prevalent research topics emerging over\nthe years.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 13:02:29 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Liapis", "Antonios", ""]]}, {"id": "2104.11044", "submitter": "James Lucas", "authors": "James Lucas, Juhan Bae, Michael R. Zhang, Stanislav Fort, Richard\n  Zemel, Roger Grosse", "title": "Analyzing Monotonic Linear Interpolation in Neural Network Loss\n  Landscapes", "comments": "15 pages in main paper, 4 pages of references, 24 pages in appendix.\n  29 figures in total", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Linear interpolation between initial neural network parameters and converged\nparameters after training with stochastic gradient descent (SGD) typically\nleads to a monotonic decrease in the training objective. This Monotonic Linear\nInterpolation (MLI) property, first observed by Goodfellow et al. (2014)\npersists in spite of the non-convex objectives and highly non-linear training\ndynamics of neural networks. Extending this work, we evaluate several\nhypotheses for this property that, to our knowledge, have not yet been\nexplored. Using tools from differential geometry, we draw connections between\nthe interpolated paths in function space and the monotonicity of the network -\nproviding sufficient conditions for the MLI property under mean squared error.\nWhile the MLI property holds under various settings (e.g. network architectures\nand learning problems), we show in practice that networks violating the MLI\nproperty can be produced systematically, by encouraging the weights to move far\nfrom initialization. The MLI property raises important questions about the loss\nlandscape geometry of neural networks and highlights the need to further study\ntheir global properties.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 13:22:12 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 17:24:48 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Lucas", "James", ""], ["Bae", "Juhan", ""], ["Zhang", "Michael R.", ""], ["Fort", "Stanislav", ""], ["Zemel", "Richard", ""], ["Grosse", "Roger", ""]]}, {"id": "2104.11059", "submitter": "Shalabh Gupta", "authors": "Zongyuan Shen, James P. Wilson, Ryan Harvey and Shalabh Gupta", "title": "MRRT: Multiple Rapidly-Exploring Random Trees for Fast Online Replanning\n  in Dynamic Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a novel algorithm, called MRRT, which uses multiple\nrapidly-exploring random trees for fast online replanning of autonomous\nvehicles in dynamic environments with moving obstacles. The proposed algorithm\nis built upon the RRT algorithm with a multi-tree structure. At the beginning,\nthe RRT algorithm is applied to find the initial solution based on partial\nknowledge of the environment. Then, the robot starts to execute this path. At\neach iteration, the new obstacle configurations are collected by the robot's\nsensor and used to replan the path. This new information can come from unknown\nstatic obstacles (e.g., seafloor layout) as well as moving obstacles. Then, to\naccommodate the environmental changes, two procedures are adopted: 1) edge\npruning, and 2) tree regrowing. Specifically, the edge pruning procedure checks\nthe collision status through the tree and only removes the invalid edges while\nmaintaining the tree structure of already-explored regions. Due to removal of\ninvalid edges, the tree could be broken into multiple disjoint trees. As such,\nthe RRT algorithm is applied to regrow the trees. Specifically, a sample is\ncreated randomly and joined to all the disjoint trees in its local neighborhood\nby connecting to the nearest nodes. Finally, a new solution is found for the\nrobot. The advantages of the proposed MRRT algorithm are as follows: i) retains\nthe maximal tree structure by only pruning the edges which collide with the\nobstacles, ii) guarantees probabilistic completeness, and iii) is computational\nefficient for fast replanning since all disjoint trees are maintained for\nfuture connections and expanded simultaneously.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 13:41:48 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Shen", "Zongyuan", ""], ["Wilson", "James P.", ""], ["Harvey", "Ryan", ""], ["Gupta", "Shalabh", ""]]}, {"id": "2104.11067", "submitter": "Ulrich Furbach", "authors": "Ulrike Barthelme{\\ss}, Ulrich Furbach", "title": "K\\\"unstliche Intelligenz, quo vadis?", "comments": "in German", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper outlines the state of the art in AI. It then describes basic\nmachine learning and knowledge processing techniques. Based on this, some\npossibilities and limitations of future AI developments are discussed.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 09:30:16 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Barthelme\u00df", "Ulrike", ""], ["Furbach", "Ulrich", ""]]}, {"id": "2104.11079", "submitter": "Tamara Kolda", "authors": "Aydin Buluc, Tamara G. Kolda, Stefan M. Wild, Mihai Anitescu, Anthony\n  DeGennaro, John Jakeman, Chandrika Kamath, Ramakrishnan (Ramki) Kannan, Miles\n  E. Lopes, Per-Gunnar Martinsson, Kary Myers, Jelani Nelson, Juan M. Restrepo,\n  C. Seshadhri, Draguna Vrabie, Brendt Wohlberg, Stephen J. Wright, Chao Yang,\n  Peter Zwart", "title": "Randomized Algorithms for Scientific Computing (RASC)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Randomized algorithms have propelled advances in artificial intelligence and\nrepresent a foundational research area in advancing AI for Science. Future\nadvancements in DOE Office of Science priority areas such as climate science,\nastrophysics, fusion, advanced materials, combustion, and quantum computing all\nrequire randomized algorithms for surmounting challenges of complexity,\nrobustness, and scalability. This report summarizes the outcomes of that\nworkshop, \"Randomized Algorithms for Scientific Computing (RASC),\" held\nvirtually across four days in December 2020 and January 2021.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 18:59:26 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Buluc", "Aydin", "", "Ramki"], ["Kolda", "Tamara G.", "", "Ramki"], ["Wild", "Stefan M.", "", "Ramki"], ["Anitescu", "Mihai", "", "Ramki"], ["DeGennaro", "Anthony", "", "Ramki"], ["Jakeman", "John", "", "Ramki"], ["Kamath", "Chandrika", "", "Ramki"], ["Ramakrishnan", "", "", "Ramki"], ["Kannan", "", ""], ["Lopes", "Miles E.", ""], ["Martinsson", "Per-Gunnar", ""], ["Myers", "Kary", ""], ["Nelson", "Jelani", ""], ["Restrepo", "Juan M.", ""], ["Seshadhri", "C.", ""], ["Vrabie", "Draguna", ""], ["Wohlberg", "Brendt", ""], ["Wright", "Stephen J.", ""], ["Yang", "Chao", ""], ["Zwart", "Peter", ""]]}, {"id": "2104.11101", "submitter": "Yi Wang", "authors": "Arman Maesumi and Mingkang Zhu and Yi Wang and Tianlong Chen and\n  Zhangyang Wang and Chandrajit Bajaj", "title": "Learning Transferable 3D Adversarial Cloaks for Deep Trained Detectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a novel patch-based adversarial attack pipeline that\ntrains adversarial patches on 3D human meshes. We sample triangular faces on a\nreference human mesh, and create an adversarial texture atlas over those faces.\nThe adversarial texture is transferred to human meshes in various poses, which\nare rendered onto a collection of real-world background images. Contrary to the\ntraditional patch-based adversarial attacks, where prior work attempts to fool\ntrained object detectors using appended adversarial patches, this new form of\nattack is mapped into the 3D object world and back-propagated to the texture\natlas through differentiable rendering. As such, the adversarial patch is\ntrained under deformation consistent with real-world materials. In addition,\nand unlike existing adversarial patches, our new 3D adversarial patch is shown\nto fool state-of-the-art deep object detectors robustly under varying views,\npotentially leading to an attacking scheme that is persistently strong in the\nphysical world.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 14:36:08 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Maesumi", "Arman", ""], ["Zhu", "Mingkang", ""], ["Wang", "Yi", ""], ["Chen", "Tianlong", ""], ["Wang", "Zhangyang", ""], ["Bajaj", "Chandrajit", ""]]}, {"id": "2104.11106", "submitter": "Adrian Remonda", "authors": "Adrian Remonda, Sarah Krebs, Eduardo Veas, Granit Luzhnica, Roman Kern", "title": "Formula RL: Deep Reinforcement Learning for Autonomous Racing using\n  Telemetry Data", "comments": null, "journal-ref": "IJCAI 2019 - Workshop on Scaling-Up Reinforcement Learning:SURL -\n  Macau, China", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper explores the use of reinforcement learning (RL) models for\nautonomous racing. In contrast to passenger cars, where safety is the top\npriority, a racing car aims to minimize the lap-time. We frame the problem as a\nreinforcement learning task with a multidimensional input consisting of the\nvehicle telemetry, and a continuous action space. To find out which RL methods\nbetter solve the problem and whether the obtained models generalize to driving\non unknown tracks, we put 10 variants of deep deterministic policy gradient\n(DDPG) to race in two experiments: i)~studying how RL methods learn to drive a\nracing car and ii)~studying how the learning scenario influences the capability\nof the models to generalize. Our studies show that models trained with RL are\nnot only able to drive faster than the baseline open source handcrafted bots\nbut also generalize to unknown tracks.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 14:40:12 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Remonda", "Adrian", ""], ["Krebs", "Sarah", ""], ["Veas", "Eduardo", ""], ["Luzhnica", "Granit", ""], ["Kern", "Roman", ""]]}, {"id": "2104.11153", "submitter": "Felix Schoeller", "authors": "Felix Schoeller, Mark Miller, Roy Salomon, Karl J. Friston", "title": "Trust as Extended Control: Active Inference and User Feedback During\n  Human-Robot Collaboration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To interact seamlessly with robots, users must infer the causes of a robot's\nbehavior and be confident about that inference. Hence, trust is a necessary\ncondition for human-robot collaboration (HRC). Despite its crucial role, it is\nlargely unknown how trust emerges, develops, and supports human interactions\nwith nonhuman artefacts. Here, we review the literature on trust, human-robot\ninteraction, human-robot collaboration, and human interaction at large. Early\nmodels of trust suggest that trust entails a trade-off between benevolence and\ncompetence, while studies of human-to-human interaction emphasize the role of\nshared behavior and mutual knowledge in the gradual building of trust. We then\nintroduce a model of trust as an agent's best explanation for reliable sensory\nexchange with an extended motor plant or partner. This model is based on the\ncognitive neuroscience of active inference and suggests that, in the context of\nHRC, trust can be cast in terms of virtual control over an artificial agent. In\nthis setting, interactive feedback becomes a necessary component of the\ntrustor's perception-action cycle. The resulting model has important\nimplications for understanding human-robot interaction and collaboration, as it\nallows the traditional determinants of human trust to be defined in terms of\nactive inference, information exchange and empowerment. Furthermore, this model\nsuggests that boredom and surprise may be used as markers for under and\nover-reliance on the system. Finally, we examine the role of shared behavior in\nthe genesis of trust, especially in the context of dyadic collaboration,\nsuggesting important consequences for the acceptability and design of\nhuman-robot collaborative systems.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 16:11:22 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Schoeller", "Felix", ""], ["Miller", "Mark", ""], ["Salomon", "Roy", ""], ["Friston", "Karl J.", ""]]}, {"id": "2104.11165", "submitter": "Zahra Gharaee", "authors": "Zahra Gharaee", "title": "Hierarchical growing grid networks for skeleton based action recognition", "comments": null, "journal-ref": "Cognitive Systems Research, vol.63, pp.11-29 (2020)", "doi": "10.1016/j.cogsys.2020.05.002", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, a novel cognitive architecture for action recognition is\ndeveloped by applying layers of growing grid neural networks.Using these layers\nmakes the system capable of automatically arranging its representational\nstructure. In addition to the expansion of the neural map during the growth\nphase, the system is provided with a prior knowledge of the input space, which\nincreases the processing speed of the learning phase. Apart from two layers of\ngrowing grid networks the architecture is composed of a preprocessing layer, an\nordered vector representation layer and a one-layer supervised neural network.\nThese layers are designed to solve the action recognition problem. The\nfirst-layer growing grid receives the input data of human actions and the\nneural map generates an action pattern vector representing each action sequence\nby connecting the elicited activation of the trained map. The pattern vectors\nare then sent to the ordered vector representation layer to build the\ntime-invariant input vectors of key activations for the second-layer growing\ngrid. The second-layer growing grid categorizes the input vectors to the\ncorresponding action clusters/sub-clusters and finally the one-layer supervised\nneural network labels the shaped clusters with action labels. Three experiments\nusing different datasets of actions show that the system is capable of learning\nto categorize the actions quickly and efficiently. The performance of the\ngrowing grid architecture is com-pared with the results from a system based on\nSelf-Organizing Maps, showing that the growing grid architecture performs\nsignificantly superior on the action recognition tasks.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 16:35:32 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Gharaee", "Zahra", ""]]}, {"id": "2104.11169", "submitter": "Seongsik Park", "authors": "Seongsik Park, Dongjin Lee, Sungroh Yoon", "title": "Noise-Robust Deep Spiking Neural Networks with Temporal Information", "comments": "Accepted to DAC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Spiking neural networks (SNNs) have emerged as energy-efficient neural\nnetworks with temporal information. SNNs have shown a superior efficiency on\nneuromorphic devices, but the devices are susceptible to noise, which hinders\nthem from being applied in real-world applications. Several studies have\nincreased noise robustness, but most of them considered neither deep SNNs nor\ntemporal information. In this paper, we investigate the effect of noise on deep\nSNNs with various neural coding methods and present a noise-robust deep SNN\nwith temporal information. With the proposed methods, we have achieved a deep\nSNN that is efficient and robust to spike deletion and jitter.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 16:40:33 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Park", "Seongsik", ""], ["Lee", "Dongjin", ""], ["Yoon", "Sungroh", ""]]}, {"id": "2104.11170", "submitter": "Lucrezia Grassi", "authors": "Lucrezia Grassi, Carmine Tommaso Recchiuto, Antonio Sgorbissa", "title": "Knowledge Triggering, Extraction and Storage via Human-Robot Verbal\n  Interaction", "comments": "19 pages, 7 figures, submitted to Robotics and Autonomous Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article describes a novel approach to expand in run-time the knowledge\nbase of an Artificial Conversational Agent. A technique for automatic knowledge\nextraction from the user's sentence and four methods to insert the new acquired\nconcepts in the knowledge base have been developed and integrated into a system\nthat has already been tested for knowledge-based conversation between a social\nhumanoid robot and residents of care homes. The run-time addition of new\nknowledge allows overcoming some limitations that affect most robots and\nchatbots: the incapability of engaging the user for a long time due to the\nrestricted number of conversation topics. The insertion in the knowledge base\nof new concepts recognized in the user's sentence is expected to result in a\nwider range of topics that can be covered during an interaction, making the\nconversation less repetitive. Two experiments are presented to assess the\nperformance of the knowledge extraction technique, and the efficiency of the\ndeveloped insertion methods when adding several concepts in the Ontology.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 16:41:15 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Grassi", "Lucrezia", ""], ["Recchiuto", "Carmine Tommaso", ""], ["Sgorbissa", "Antonio", ""]]}, {"id": "2104.11178", "submitter": "Hassan Akbari", "authors": "Hassan Akbari, Linagzhe Yuan, Rui Qian, Wei-Hong Chuang, Shih-Fu\n  Chang, Yin Cui, Boqing Gong", "title": "VATT: Transformers for Multimodal Self-Supervised Learning from Raw\n  Video, Audio and Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework for learning multimodal representations from unlabeled\ndata using convolution-free Transformer architectures. Specifically, our\nVideo-Audio-Text Transformer (VATT) takes raw signals as inputs and extracts\nmultimodal representations that are rich enough to benefit a variety of\ndownstream tasks. We train VATT end-to-end from scratch using multimodal\ncontrastive losses and evaluate its performance by the downstream tasks of\nvideo action recognition, audio event classification, image classification, and\ntext-to-video retrieval. Furthermore, we study a modality-agnostic\nsingle-backbone Transformer by sharing weights among the three modalities. We\nshow that the convolution-free VATT outperforms state-of-the-art ConvNet-based\narchitectures in the downstream tasks. Especially, VATT's vision Transformer\nachieves the top-1 accuracy of 82.1% on Kinetics-400, 83.6% on Kinetics-600,and\n41.1% on Moments in Time, new records while avoiding supervised pre-training.\nTransferring to image classification leads to 78.7% top-1 accuracy on ImageNet\ncompared to 64.7% by training the same Transformer from scratch, showing the\ngeneralizability of our model despite the domain gap between videos and images.\nVATT's audio Transformer also sets a new record on waveform-based audio event\nrecognition by achieving the mAP of 39.4% on AudioSet without any supervised\npre-training.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 17:07:41 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Akbari", "Hassan", ""], ["Yuan", "Linagzhe", ""], ["Qian", "Rui", ""], ["Chuang", "Wei-Hong", ""], ["Chang", "Shih-Fu", ""], ["Cui", "Yin", ""], ["Gong", "Boqing", ""]]}, {"id": "2104.11213", "submitter": "Kiana Ehsani", "authors": "Kiana Ehsani, Winson Han, Alvaro Herrasti, Eli VanderBilt, Luca Weihs,\n  Eric Kolve, Aniruddha Kembhavi, Roozbeh Mottaghi", "title": "ManipulaTHOR: A Framework for Visual Object Manipulation", "comments": "CVPR 2021 -- (Oral presentation)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The domain of Embodied AI has recently witnessed substantial progress,\nparticularly in navigating agents within their environments. These early\nsuccesses have laid the building blocks for the community to tackle tasks that\nrequire agents to actively interact with objects in their environment. Object\nmanipulation is an established research domain within the robotics community\nand poses several challenges including manipulator motion, grasping and\nlong-horizon planning, particularly when dealing with oft-overlooked practical\nsetups involving visually rich and complex scenes, manipulation using mobile\nagents (as opposed to tabletop manipulation), and generalization to unseen\nenvironments and objects. We propose a framework for object manipulation built\nupon the physics-enabled, visually rich AI2-THOR framework and present a new\nchallenge to the Embodied AI community known as ArmPointNav. This task extends\nthe popular point navigation task to object manipulation and offers new\nchallenges including 3D obstacle avoidance, manipulating objects in the\npresence of occlusion, and multi-object manipulation that necessitates long\nterm planning. Popular learning paradigms that are successful on PointNav\nchallenges show promise, but leave a large room for improvement.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 17:49:04 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Ehsani", "Kiana", ""], ["Han", "Winson", ""], ["Herrasti", "Alvaro", ""], ["VanderBilt", "Eli", ""], ["Weihs", "Luca", ""], ["Kolve", "Eric", ""], ["Kembhavi", "Aniruddha", ""], ["Mottaghi", "Roozbeh", ""]]}, {"id": "2104.11216", "submitter": "Sumith Kulal", "authors": "Sumith Kulal, Jiayuan Mao, Alex Aiken, Jiajun Wu", "title": "Hierarchical Motion Understanding via Motion Programs", "comments": "CVPR 2021. First two authors contributed equally. Project page:\n  https://sumith1896.github.io/motion2prog/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current approaches to video analysis of human motion focus on raw pixels or\nkeypoints as the basic units of reasoning. We posit that adding higher-level\nmotion primitives, which can capture natural coarser units of motion such as\nbackswing or follow-through, can be used to improve downstream analysis tasks.\nThis higher level of abstraction can also capture key features, such as loops\nof repeated primitives, that are currently inaccessible at lower levels of\nrepresentation. We therefore introduce Motion Programs, a neuro-symbolic,\nprogram-like representation that expresses motions as a composition of\nhigh-level primitives. We also present a system for automatically inducing\nmotion programs from videos of human motion and for leveraging motion programs\nin video synthesis. Experiments show that motion programs can accurately\ndescribe a diverse set of human motions and the inferred programs contain\nsemantically meaningful motion primitives, such as arm swings and jumping\njacks. Our representation also benefits downstream tasks such as video\ninterpolation and video prediction and outperforms off-the-shelf models. We\nfurther demonstrate how these programs can detect diverse kinds of repetitive\nmotion and facilitate interactive video editing.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 17:49:59 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Kulal", "Sumith", ""], ["Mao", "Jiayuan", ""], ["Aiken", "Alex", ""], ["Wu", "Jiajun", ""]]}, {"id": "2104.11227", "submitter": "Christoph Feichtenhofer", "authors": "Haoqi Fan, Bo Xiong, Karttikeya Mangalam, Yanghao Li, Zhicheng Yan,\n  Jitendra Malik, Christoph Feichtenhofer", "title": "Multiscale Vision Transformers", "comments": "Technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Multiscale Vision Transformers (MViT) for video and image\nrecognition, by connecting the seminal idea of multiscale feature hierarchies\nwith transformer models. Multiscale Transformers have several\nchannel-resolution scale stages. Starting from the input resolution and a small\nchannel dimension, the stages hierarchically expand the channel capacity while\nreducing the spatial resolution. This creates a multiscale pyramid of features\nwith early layers operating at high spatial resolution to model simple\nlow-level visual information, and deeper layers at spatially coarse, but\ncomplex, high-dimensional features. We evaluate this fundamental architectural\nprior for modeling the dense nature of visual signals for a variety of video\nrecognition tasks where it outperforms concurrent vision transformers that rely\non large scale external pre-training and are 5-10x more costly in computation\nand parameters. We further remove the temporal dimension and apply our model\nfor image classification where it outperforms prior work on vision\ntransformers. Code is available at:\nhttps://github.com/facebookresearch/SlowFast\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 17:59:45 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Fan", "Haoqi", ""], ["Xiong", "Bo", ""], ["Mangalam", "Karttikeya", ""], ["Li", "Yanghao", ""], ["Yan", "Zhicheng", ""], ["Malik", "Jitendra", ""], ["Feichtenhofer", "Christoph", ""]]}, {"id": "2104.11230", "submitter": "Jiajie Wu", "authors": "Jiajie Wu", "title": "Literature review on vulnerability detection using NLP technology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Vulnerability detection has always been the most important task in the field\nof software security. With the development of technology, in the face of\nmassive source code, automated analysis and detection of vulnerabilities has\nbecome a current research hotspot. For special text files such as source code,\nusing some of the hottest NLP technologies to build models and realize the\nautomatic analysis and detection of source code has become one of the most\nanticipated studies in the field of vulnerability detection. This article does\na brief survey of some recent new documents and technologies, such as CodeBERT,\nand summarizes the previous technologies.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 03:16:51 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Wu", "Jiajie", ""]]}, {"id": "2104.11231", "submitter": "Tekin Evrim Ozmermer", "authors": "Tekin Evrim Ozmermer, Viktors Roze, Stanislavs Hilcuks, Alina\n  Nescerecka", "title": "VeriMedi: Pill Identification using Proxy-based Deep Metric Learning and\n  Exact Solution", "comments": "31 pages, 21 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IT cs.LG math.IT", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present the system that we have developed for the identification and\nverification of pills using images that are taken by the VeriMedi device. The\nVeriMedi device is an Internet of Things device that takes pictures of a filled\npill vial from the bottom of the vial and uses the solution that is presented\nin this research to identify the pills in the vials. The solution has two\nserially connected deep learning solutions which do segmentation and\nidentification. The segmentation solution creates the masks for each pill in\nthe vial image by using the Mask R-CNN model, then segments and crops the pills\nand blurs the background. After that, the segmented pill images are sent to the\nidentification solution where a Deep Metric Learning model that is trained with\nProxy Anchor Loss (PAL) function generates embedding vectors for each pill\nimage. The generated embedding vectors are fed into a one-layer fully connected\nnetwork that is trained with the exact solution to predict each single pill\nimage. Then, the aggregation/verification function aggregates the multiple\npredictions coming from multiple single pill images and verifies the\ncorrectness of the final prediction with respect to predefined rules. Besides,\nwe enhanced the PAL with a better proxy initialization that increased the\nperformance of the models and let the model learn the new classes of images\ncontinually without retraining the model with the whole dataset. When the model\nthat is trained with initial classes is retrained only with new classes, the\naccuracy of the model increases for both old and new classes. The\nidentification solution that we have presented in this research can also be\nreused for other problem domains which require continual learning and/or\nFine-Grained Visual Categorization.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 06:52:30 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Ozmermer", "Tekin Evrim", ""], ["Roze", "Viktors", ""], ["Hilcuks", "Stanislavs", ""], ["Nescerecka", "Alina", ""]]}, {"id": "2104.11276", "submitter": "Krenare Pireva Nuci", "authors": "Lumbardh Elshani, Krenare Pireva Nu\\c{c}i", "title": "Constructing a personalized learning path using genetic algorithms\n  approach", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A substantial disadvantage of traditional learning is that all students\nfollow the same learning sequence, but not all of them have the same background\nof knowledge, the same preferences, the same learning goals, and the same\nneeds. Traditional teaching resources, such as textbooks, in most cases pursue\nstudents to follow fixed sequences during the learning process, thus impairing\ntheir performance. Learning sequencing is an important research issue as part\nof the learning process because no fixed learning paths will be appropriate for\nall learners. For this reason, many research papers are focused on the\ndevelopment of mechanisms to offer personalization on learning paths,\nconsidering the learner needs, interests, behaviors, and abilities. In most\ncases, these researchers are totally focused on the student's preferences,\nignoring the level of difficulty and the relation degree that exists between\nvarious concepts in a course. This research paper presents the possibility of\nconstructing personalized learning paths using genetic algorithm-based model,\nencountering the level of difficulty and relation degree of the constituent\nconcepts of a course. The experimental results shows that the genetic algorithm\nis suitable to generate optimal learning paths based on learning object\ndifficulty level, duration, rating, and relation degree between each learning\nobject as elementary parts of the sequence of the learning path. From these\nresults compared to the quality of the traditional learning path, we observed\nthat even the quality of the weakest learning path generated by our GA approach\nis in a favor compared to quality of the traditional learning path, with a\ndifference of 3.59\\%, while the highest solution generated in the end resulted\n8.34\\% in favor of our proposal compared to the traditional learning paths.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 18:43:47 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Elshani", "Lumbardh", ""], ["Nu\u00e7i", "Krenare Pireva", ""]]}, {"id": "2104.11315", "submitter": "Sewoong Oh", "authors": "Jonathan Hayase, Weihao Kong, Raghav Somani, Sewoong Oh", "title": "SPECTRE: Defending Against Backdoor Attacks Using Robust Statistics", "comments": "29 pages 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Modern machine learning increasingly requires training on a large collection\nof data from multiple sources, not all of which can be trusted. A particularly\nconcerning scenario is when a small fraction of poisoned data changes the\nbehavior of the trained model when triggered by an attacker-specified\nwatermark. Such a compromised model will be deployed unnoticed as the model is\naccurate otherwise. There have been promising attempts to use the intermediate\nrepresentations of such a model to separate corrupted examples from clean ones.\nHowever, these defenses work only when a certain spectral signature of the\npoisoned examples is large enough for detection. There is a wide range of\nattacks that cannot be protected against by the existing defenses. We propose a\nnovel defense algorithm using robust covariance estimation to amplify the\nspectral signature of corrupted data. This defense provides a clean model,\ncompletely removing the backdoor, even in regimes where previous methods have\nno hope of detecting the poisoned examples. Code and pre-trained models are\navailable at https://github.com/SewoongLab/spectre-defense .\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 20:49:40 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Hayase", "Jonathan", ""], ["Kong", "Weihao", ""], ["Somani", "Raghav", ""], ["Oh", "Sewoong", ""]]}, {"id": "2104.11360", "submitter": "X. San Liang", "authors": "X. San Liang", "title": "Normalized multivariate time series causality analysis and causal graph\n  reconstruction", "comments": "17 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Causality analysis is an important problem lying at the heart of science, and\nis of particular importance in data science and machine learning. An endeavor\nduring the past 16 years viewing causality as real physical notion so as to\nformulate it from first principles, however, seems to go unnoticed. This study\nintroduces to the community this line of work, with a long-due generalization\nof the information flow-based bivariate time series causal inference to\nmultivariate series, based on the recent advance in theoretical development.\nThe resulting formula is transparent, and can be implemented as a\ncomputationally very efficient algorithm for application. It can be normalized,\nand tested for statistical significance. Different from the previous work along\nthis line where only information flows are estimated, here an algorithm is also\nimplemented to quantify the influence of a unit to itself. While this forms a\nchallenge in some causal inferences, here it comes naturally, and hence the\nidentification of self-loops in a causal graph is fulfilled automatically as\nthe causalities along edges are inferred.\n  To demonstrate the power of the approach, presented here are two applications\nin extreme situations. The first is a network of multivariate processes buried\nin heavy noises (with the noise-to-signal ratio exceeding 100), and the second\na network with nearly synchronized chaotic oscillators. In both graphs,\nconfounding processes exist. While it seems to be a huge challenge to\nreconstruct from given series these causal graphs, an easy application of the\nalgorithm immediately reveals the desideratum. Particularly, the confounding\nprocesses have been accurately differentiated. Considering the surge of\ninterest in the community, this study is very timely.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 00:46:35 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Liang", "X. San", ""]]}, {"id": "2104.11454", "submitter": "Cheng Luo", "authors": "Cheng Luo, Dayiheng Liu, Chanjuan Li, Li Lu, Jiancheng Lv", "title": "Prediction, Selection, and Generation: Exploration of Knowledge-Driven\n  Conversation System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In open-domain conversational systems, it is important but challenging to\nleverage background knowledge. We can use the incorporation of knowledge to\nmake the generation of dialogue controllable, and can generate more diverse\nsentences that contain real knowledge. In this paper, we combine the knowledge\nbases and pre-training model to propose a knowledge-driven conversation system.\nThe system includes modules such as dialogue topic prediction, knowledge\nmatching and dialogue generation. Based on this system, we study the\nperformance factors that maybe affect the generation of knowledge-driven\ndialogue: topic coarse recall algorithm, number of knowledge choices,\ngeneration model choices, etc., and finally made the system reach\nstate-of-the-art. These experimental results will provide some guiding\nsignificance for the future research of this task. As far as we know, this is\nthe first work to study and analyze the effects of the related factors.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 07:59:55 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 02:19:37 GMT"}, {"version": "v3", "created": "Wed, 5 May 2021 06:58:12 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Luo", "Cheng", ""], ["Liu", "Dayiheng", ""], ["Li", "Chanjuan", ""], ["Lu", "Li", ""], ["Lv", "Jiancheng", ""]]}, {"id": "2104.11510", "submitter": "Guangcan Liu", "authors": "Guangcan Liu", "title": "Time Series Forecasting via Learning Convolutionally Low-Rank Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently,~\\citet{liu:arxiv:2019} studied the rather challenging problem of\ntime series forecasting from the perspective of compressed sensing. They\nproposed a no-learning method, named Convolution Nuclear Norm Minimization\n(CNNM), and proved that CNNM can exactly recover the future part of a series\nfrom its observed part, provided that the series is convolutionally low-rank.\nWhile impressive, the convolutional low-rankness condition may not be satisfied\nwhenever the series is far from being seasonal, and is in fact brittle to the\npresence of trends and dynamics. This paper tries to approach the issues by\nintegrating a learnable, orthonormal transformation into CNNM, with the purpose\nfor converting the series of involute structures into regular signals of\nconvolutionally low-rank. We prove that the resulted model, termed\nLearning-Based CNNM (LbCNNM), strictly succeeds in identifying the future part\nof a series, as long as the transform of the series is convolutionally\nlow-rank. To learn proper transformations that may meet the required success\nconditions, we devise an interpretable method based on Principal Component\nPurist (PCP). Equipped with this learning method and some elaborate data\nargumentation skills, LbCNNM not only can handle well the major components of\ntime series (including trends, seasonality and dynamics), but also can make use\nof the forecasts provided by some other forecasting methods; this means LbCNNM\ncan be used as a general tool for model combination. Extensive experiments on\n100,452 real-world time series from TSDL and M4 demonstrate the superior\nperformance of LbCNNM.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 09:53:28 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Liu", "Guangcan", ""]]}, {"id": "2104.11530", "submitter": "Junaid Ahmed Ghauri", "authors": "Junaid Ahmed Ghauri, Sherzod Hakimov, Ralph Ewerth", "title": "Supervised Video Summarization via Multiple Feature Sets with Parallel\n  Attention", "comments": "Accepted in IEEE International Conference on Multimedia and Expo\n  (ICME) 2021 (They have copyright to publish camera ready version of this\n  work)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The assignment of importance scores to particular frames or (short) segments\nin a video is crucial for summarization, but also a difficult task. Previous\nwork utilizes only one source of visual features. In this paper, we suggest a\nnovel model architecture that combines three feature sets for visual content\nand motion to predict importance scores. The proposed architecture utilizes an\nattention mechanism before fusing motion features and features representing the\n(static) visual content, i.e., derived from an image classification model.\nComprehensive experimental evaluations are reported for two well-known\ndatasets, SumMe and TVSum. In this context, we identify methodological issues\non how previous work used these benchmark datasets, and present a fair\nevaluation scheme with appropriate data splits that can be used in future work.\nWhen using static and motion features with parallel attention mechanism, we\nimprove state-of-the-art results for SumMe, while being on par with the state\nof the art for the other dataset.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 10:46:35 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 16:07:41 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Ghauri", "Junaid Ahmed", ""], ["Hakimov", "Sherzod", ""], ["Ewerth", "Ralph", ""]]}, {"id": "2104.11559", "submitter": "Jochen Z\\\"ollner", "authors": "Jochen Z\\\"ollner, Konrad Sperfeld, Christoph Wick, Roger Labahn", "title": "Optimizing small BERTs trained for German NER", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Currently, the most widespread neural network architecture for training\nlanguage models is the so called BERT which led to improvements in various NLP\ntasks. In general, the larger the number of parameters in a BERT model, the\nbetter the results obtained in these NLP tasks. Unfortunately, the memory\nconsumption and the training duration drastically increases with the size of\nthese models, though. In this article, we investigate various training\ntechniques of smaller BERT models and evaluate them on five public German NER\ntasks of which two are introduced by this article. We combine different methods\nfrom other BERT variants like ALBERT, RoBERTa, and relative positional\nencoding. In addition, we propose two new fine-tuning techniques leading to\nbetter performance: CSE-tagging and a modified form of LCRF. Furthermore, we\nintroduce a new technique called WWA which reduces BERT memory usage and leads\nto a small increase in performance.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 12:36:13 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Z\u00f6llner", "Jochen", ""], ["Sperfeld", "Konrad", ""], ["Wick", "Christoph", ""], ["Labahn", "Roger", ""]]}, {"id": "2104.11573", "submitter": "Michael Bennett", "authors": "Michael Timothy Bennett, Yoshihiro Maruyama", "title": "Intensional Artificial Intelligence: From Symbol Emergence to\n  Explainable and Empathetic AI", "comments": "7 pages, submitted to IEEE ICDL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We argue that an explainable artificial intelligence must possess a rationale\nfor its decisions, be able to infer the purpose of observed behaviour, and be\nable to explain its decisions in the context of what its audience understands\nand intends. To address these issues we present four novel contributions.\nFirstly, we define an arbitrary task in terms of perceptual states, and discuss\ntwo extremes of a domain of possible solutions. Secondly, we define the\nintensional solution. Optimal by some definitions of intelligence, it describes\nthe purpose of a task. An agent possessed of it has a rationale for its\ndecisions in terms of that purpose, expressed in a perceptual symbol system\ngrounded in hardware. Thirdly, to communicate that rationale requires natural\nlanguage, a means of encoding and decoding perceptual states. We propose a\ntheory of meaning in which, to acquire language, an agent should model the\nworld a language describes rather than the language itself. If the utterances\nof humans are of predictive value to the agent's goals, then the agent will\nimbue those utterances with meaning in terms of its own goals and perceptual\nstates. In the context of Peircean semiotics, a community of agents must share\nrough approximations of signs, referents and interpretants in order to\ncommunicate. Meaning exists only in the context of intent, so to communicate\nwith humans an agent must have comparable experiences and goals. An agent that\nlearns intensional solutions, compelled by objective functions somewhat\nanalogous to human motivators such as hunger and pain, may be capable of\nexplaining its rationale not just in terms of its own intent, but in terms of\nwhat its audience understands and intends. It forms some approximation of the\nperceptual states of humans.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 13:13:46 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Bennett", "Michael Timothy", ""], ["Maruyama", "Yoshihiro", ""]]}, {"id": "2104.11576", "submitter": "Erik Hemberg", "authors": "Prakruthi Karuna and Erik Hemberg and Una-May O'Reilly and Nick Rutar", "title": "Automating Cyber Threat Hunting Using NLP, Automated Query Generation,\n  and Genetic Perturbation", "comments": "5 pages 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scaling the cyber hunt problem poses several key technical challenges.\nDetecting and characterizing cyber threats at scale in large enterprise\nnetworks is hard because of the vast quantity and complexity of the data that\nmust be analyzed as adversaries deploy varied and evolving tactics to\naccomplish their goals. There is a great need to automate all aspects, and,\nindeed, the workflow of cyber hunting. AI offers many ways to support this. We\nhave developed the WILEE system that automates cyber threat hunting by\ntranslating high-level threat descriptions into many possible concrete\nimplementations. Both the (high-level) abstract and (low-level) concrete\nimplementations are represented using a custom domain specific language (DSL).\nWILEE uses the implementations along with other logic, also written in the DSL,\nto automatically generate queries to confirm (or refute) any hypotheses tied to\nthe potential adversarial workflows represented at various layers of\nabstraction.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 13:19:12 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Karuna", "Prakruthi", ""], ["Hemberg", "Erik", ""], ["O'Reilly", "Una-May", ""], ["Rutar", "Nick", ""]]}, {"id": "2104.11589", "submitter": "Sang Hun Lee", "authors": "Sangrok Lee, Taekang Woo, Sang Hun Lee", "title": "SBNet: Segmentation-based Network for Natural Language-based Vehicle\n  Search", "comments": "7 pages, 4 figures, CVPR Workshop Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Natural language-based vehicle retrieval is a task to find a target vehicle\nwithin a given image based on a natural language description as a query. This\ntechnology can be applied to various areas including police searching for a\nsuspect vehicle. However, it is challenging due to the ambiguity of language\ndescriptions and the difficulty of processing multi-modal data. To tackle this\nproblem, we propose a deep neural network called SBNet that performs natural\nlanguage-based segmentation for vehicle retrieval. We also propose two\ntask-specific modules to improve performance: a substitution module that helps\nfeatures from different domains to be embedded in the same space and a future\nprediction module that learns temporal information. SBnet has been trained\nusing the CityFlow-NL dataset that contains 2,498 tracks of vehicles with three\nunique natural language descriptions each and tested 530 unique vehicle tracks\nand their corresponding query sets. SBNet achieved a significant improvement\nover the baseline in the natural language-based vehicle tracking track in the\nAI City Challenge 2021.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 08:06:17 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Lee", "Sangrok", ""], ["Woo", "Taekang", ""], ["Lee", "Sang Hun", ""]]}, {"id": "2104.11597", "submitter": "Zhiyuan Zhou", "authors": "Zhiyuan Zhou, Kai Xuan, Zhifu Tao, Ligang Zhou", "title": "Generalized-TODIM Method for Multi-criteria Decision Making with Basic\n  Uncertain Information and its Application", "comments": "24 pages, 2 figure, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the fact that basic uncertain information provides a simple form for\ndecision information with certainty degree, it has been developed to reflect\nthe quality of observed or subjective assessments. In order to study the\nalgebra structure and preference relation of basic uncertain information, we\ndevelop some algebra operations for basic uncertain information. The order\nrelation of such type of information has also been considered. Finally, to\napply the developed algebra operations and order relations, a generalized TODIM\nmethod for multi-attribute decision making with basic uncertain information is\ngiven. The numerical example shows that the developed decision procedure is\nvalid.\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 04:18:53 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 15:28:58 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Zhou", "Zhiyuan", ""], ["Xuan", "Kai", ""], ["Tao", "Zhifu", ""], ["Zhou", "Ligang", ""]]}, {"id": "2104.11600", "submitter": "Patrick Zschech", "authors": "Patrick Zschech, Jannis Walk, Kai Heinrich, Michael V\\\"ossing, Niklas\n  K\\\"uhl", "title": "A Picture is Worth a Collaboration: Accumulating Design Knowledge for\n  Computer-Vision-based Hybrid Intelligence Systems", "comments": "Preprint accepted for archival and presentation at the 29th European\n  Conference on Information Systems (ECIS 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Computer vision (CV) techniques try to mimic human capabilities of visual\nperception to support labor-intensive and time-consuming tasks like the\nrecognition and localization of critical objects. Nowadays, CV increasingly\nrelies on artificial intelligence (AI) to automatically extract useful\ninformation from images that can be utilized for decision support and business\nprocess automation. However, the focus of extant research is often exclusively\non technical aspects when designing AI-based CV systems while neglecting\nsocio-technical facets, such as trust, control, and autonomy. For this purpose,\nwe consider the design of such systems from a hybrid intelligence (HI)\nperspective and aim to derive prescriptive design knowledge for CV-based HI\nsystems. We apply a reflective, practice-inspired design science approach and\naccumulate design knowledge from six comprehensive CV projects. As a result, we\nidentify four design-related mechanisms (i.e., automation, signaling,\nmodification, and collaboration) that inform our derived meta-requirements and\ndesign principles. This can serve as a basis for further socio-technical\nresearch on CV-based HI systems.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 13:47:57 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Zschech", "Patrick", ""], ["Walk", "Jannis", ""], ["Heinrich", "Kai", ""], ["V\u00f6ssing", "Michael", ""], ["K\u00fchl", "Niklas", ""]]}, {"id": "2104.11604", "submitter": "Giorgia Dellaferrera", "authors": "Giorgia Dellaferrera, Stanislaw Wozniak, Giacomo Indiveri, Angeliki\n  Pantazi, Evangelos Eleftheriou", "title": "Learning in Deep Neural Networks Using a Biologically Inspired Optimizer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plasticity circuits in the brain are known to be influenced by the\ndistribution of the synaptic weights through the mechanisms of synaptic\nintegration and local regulation of synaptic strength. However, the complex\ninterplay of stimulation-dependent plasticity with local learning signals is\ndisregarded by most of the artificial neural network training algorithms\ndevised so far. Here, we propose a novel biologically inspired optimizer for\nartificial (ANNs) and spiking neural networks (SNNs) that incorporates key\nprinciples of synaptic integration observed in dendrites of cortical neurons:\nGRAPES (Group Responsibility for Adjusting the Propagation of Error Signals).\nGRAPES implements a weight-distribution dependent modulation of the error\nsignal at each node of the neural network. We show that this biologically\ninspired mechanism leads to a systematic improvement of the convergence rate of\nthe network, and substantially improves classification accuracy of ANNs and\nSNNs with both feedforward and recurrent architectures. Furthermore, we\ndemonstrate that GRAPES supports performance scalability for models of\nincreasing complexity and mitigates catastrophic forgetting by enabling\nnetworks to generalize to unseen tasks based on previously acquired knowledge.\nThe local characteristics of GRAPES minimize the required memory resources,\nmaking it optimally suited for dedicated hardware implementations. Overall, our\nwork indicates that reconciling neurophysiology insights with machine\nintelligence is key to boosting the performance of neural networks.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 13:50:30 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Dellaferrera", "Giorgia", ""], ["Wozniak", "Stanislaw", ""], ["Indiveri", "Giacomo", ""], ["Pantazi", "Angeliki", ""], ["Eleftheriou", "Evangelos", ""]]}, {"id": "2104.11610", "submitter": "Alan Blair", "authors": "Xuefeng Li and Alan Blair", "title": "Eccentric Regularization: Minimizing Hyperspherical Energy without\n  explicit projection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several regularization methods have recently been introduced which force the\nlatent activations of an autoencoder or deep neural network to conform to\neither a Gaussian or hyperspherical distribution, or to minimize the implicit\nrank of the distribution in latent space. In the present work, we introduce a\nnovel regularizing loss function which simulates a pairwise repulsive force\nbetween items and an attractive force of each item toward the origin. We show\nthat minimizing this loss function in isolation achieves a hyperspherical\ndistribution. Moreover, when used as a regularizing term, the scaling factor\ncan be adjusted to allow greater flexibility and tolerance of eccentricity,\nthus allowing the latent variables to be stratified according to their relative\nimportance, while still promoting diversity. We apply this method of Eccentric\nRegularization to an autoencoder, and demonstrate its effectiveness in image\ngeneration, representation learning and downstream classification tasks.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 13:55:17 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Li", "Xuefeng", ""], ["Blair", "Alan", ""]]}, {"id": "2104.11637", "submitter": "Zahra Gharaee", "authors": "Zahra Gharaee", "title": "Online recognition of unsegmented actions with hierarchical SOM\n  architecture", "comments": null, "journal-ref": "Cogn Process 22, 77-91 (2021)", "doi": "10.1007/s10339-020-00986-4", "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Automatic recognition of an online series of unsegmented actions requires a\nmethod for segmentation that determines when an action starts and when it ends.\nIn this paper, a novel approach for recognizing unsegmented actions in online\ntest experiments is proposed. The method uses self-organizing neural networks\nto build a three-layer cognitive architecture. The unique features of an action\nsequence are represented as a series of elicited key activations by the\nfirst-layer self-organizing map. An average length of a key activation vector\nis calculated for all action sequences in a training set and adjusted in\nlearning trials to generate input patterns to the second-layer self-organizing\nmap. The pattern vectors are clustered in the second layer, and the clusters\nare then labeled by an action identity in the third layer neural network. The\nexperiment results show that although the performance drops slightly in online\nexperiments compared to the offline tests, the ability of the proposed\narchitecture to deal with the unsegmented action sequences as well as the\nonline performance makes the system more plausible and practical in real-case\nscenarios.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 14:41:46 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Gharaee", "Zahra", ""]]}, {"id": "2104.11673", "submitter": "Gabriel Mittag", "authors": "Gabriel Mittag, Sebastian M\\\"oller", "title": "Deep Learning Based Assessment of Synthetic Speech Naturalness", "comments": "Late upload, presented at Interspeech 2020", "journal-ref": null, "doi": "10.21437/Interspeech.2020-2382", "report-no": null, "categories": "cs.SD cs.AI cs.CL cs.LG eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a new objective prediction model for synthetic\nspeech naturalness. It can be used to evaluate Text-To-Speech or Voice\nConversion systems and works language independently. The model is trained\nend-to-end and based on a CNN-LSTM network that previously showed to give good\nresults for speech quality estimation. We trained and tested the model on 16\ndifferent datasets, such as from the Blizzard Challenge and the Voice\nConversion Challenge. Further, we show that the reliability of deep\nlearning-based naturalness prediction can be improved by transfer learning from\nspeech quality prediction models that are trained on objective POLQA scores.\nThe proposed model is made publicly available and can, for example, be used to\nevaluate different TTS system configurations.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 16:05:20 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Mittag", "Gabriel", ""], ["M\u00f6ller", "Sebastian", ""]]}, {"id": "2104.11677", "submitter": "Arsalan Tahir", "authors": "Arsalan Tahir, Muhammad Adil and Arslan Ali", "title": "Rapid Detection of Aircrafts in Satellite Imagery based on Deep Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Object detection is one of the fundamental objectives in Applied Computer\nVision. In some of the applications, object detection becomes very challenging\nsuch as in the case of satellite image processing. Satellite image processing\nhas remained the focus of researchers in domains of Precision Agriculture,\nClimate Change, Disaster Management, etc. Therefore, object detection in\nsatellite imagery is one of the most researched problems in this domain. This\npaper focuses on aircraft detection. in satellite imagery using deep learning\ntechniques. In this paper, we used YOLO deep learning framework for aircraft\ndetection. This method uses satellite images collected by different sources as\nlearning for the model to perform detection. Object detection in satellite\nimages is mostly complex because objects have many variations, types, poses,\nsizes, complex and dense background. YOLO has some limitations for small size\nobjects (less than$\\sim$32 pixels per object), therefore we upsample the\nprediction grid to reduce the coarseness of the model and to accurately detect\nthe densely clustered objects. The improved model shows good accuracy and\nperformance on different unknown images having small, rotating, and dense\nobjects to meet the requirements in real-time.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 18:13:16 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Tahir", "Arsalan", ""], ["Adil", "Muhammad", ""], ["Ali", "Arslan", ""]]}, {"id": "2104.11681", "submitter": "Zhen Bi", "authors": "Zhen Bi, Ningyu Zhang, Ganqiang Ye, Haiyang Yu, Xi Chen, Huajun Chen", "title": "Interventional Aspect-Based Sentiment Analysis", "comments": "Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent neural-based aspect-based sentiment analysis approaches, though\nachieving promising improvement on benchmark datasets, have reported suffering\nfrom poor robustness when encountering confounder such as non-target aspects.\nIn this paper, we take a causal view to addressing this issue. We propose a\nsimple yet effective method, namely, Sentiment Adjustment (SENTA), by applying\na backdoor adjustment to disentangle those confounding factors. Experimental\nresults on the Aspect Robustness Test Set (ARTS) dataset demonstrate that our\napproach improves the performance while maintaining accuracy in the original\ntest set.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 07:54:29 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Bi", "Zhen", ""], ["Zhang", "Ningyu", ""], ["Ye", "Ganqiang", ""], ["Yu", "Haiyang", ""], ["Chen", "Xi", ""], ["Chen", "Huajun", ""]]}, {"id": "2104.11683", "submitter": "Wauter Bosma", "authors": "Wauter Bosma, Sander Dalm, Erwin van Eijk, Rachid el Harchaoui, Edwin\n  Rijgersberg, Hannah Tereza Tops, Alle Veenstra, Rolf Ypma", "title": "Establishing phone-pair co-usage by comparing mobility patterns", "comments": null, "journal-ref": "Science & Justice 2 (2020)", "doi": "10.1016/j.scijus.2019.10.005", "report-no": null, "categories": "stat.AP cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In forensic investigations it is often of value to establish whether two\nphones were used by the same person during a given time period. We present a\nmethod that uses time and location of cell tower registrations of mobile phones\nto assess the strength of evidence that any pair of phones were used by the\nsame person. The method is transparent as it uses logistic regression to\ndiscriminate between the hypotheses of same and different user, and a standard\nkernel density estimation to quantify the weight of evidence in terms of a\nlikelihood ratio. We further add to previous theoretical work by training and\nvalidating our method on real world data, paving the way for application in\npractice. The method shows good performance under different modeling choices\nand robustness under lower quantity or quality of data. We discuss practical\nusage in court.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 16:21:38 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 05:56:30 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Bosma", "Wauter", ""], ["Dalm", "Sander", ""], ["van Eijk", "Erwin", ""], ["Harchaoui", "Rachid el", ""], ["Rijgersberg", "Edwin", ""], ["Tops", "Hannah Tereza", ""], ["Veenstra", "Alle", ""], ["Ypma", "Rolf", ""]]}, {"id": "2104.11699", "submitter": "Keping Yu", "authors": "Keping Yu, Zhiwei Guo, Yu Shen, Wei Wang, Jerry Chun-Wei Lin, Takuro\n  Sato", "title": "Secure Artificial Intelligence of Things for Implicit Group\n  Recommendations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The emergence of Artificial Intelligence of Things (AIoT) has provided novel\ninsights for many social computing applications such as group recommender\nsystems. As distance among people has been greatly shortened, it has been a\nmore general demand to provide personalized services to groups instead of\nindividuals. In order to capture group-level preference features from\nindividuals, existing methods were mostly established via aggregation and face\ntwo aspects of challenges: secure data management workflow is absent, and\nimplicit preference feedbacks is ignored. To tackle current difficulties, this\npaper proposes secure Artificial Intelligence of Things for implicit Group\nRecommendations (SAIoT-GR). As for hardware module, a secure IoT structure is\ndeveloped as the bottom support platform. As for software module, collaborative\nBayesian network model and non-cooperative game are can be introduced as\nalgorithms. Such a secure AIoT architecture is able to maximize the advantages\nof the two modules. In addition, a large number of experiments are carried out\nto evaluate the performance of the SAIoT-GR in terms of efficiency and\nrobustness.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 16:38:26 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Yu", "Keping", ""], ["Guo", "Zhiwei", ""], ["Shen", "Yu", ""], ["Wang", "Wei", ""], ["Lin", "Jerry Chun-Wei", ""], ["Sato", "Takuro", ""]]}, {"id": "2104.11707", "submitter": "Soroush Nasiriany", "authors": "Soroush Nasiriany, Vitchyr H. Pong, Ashvin Nair, Alexander Khazatsky,\n  Glen Berseth, Sergey Levine", "title": "DisCo RL: Distribution-Conditioned Reinforcement Learning for\n  General-Purpose Policies", "comments": "ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Can we use reinforcement learning to learn general-purpose policies that can\nperform a wide range of different tasks, resulting in flexible and reusable\nskills? Contextual policies provide this capability in principle, but the\nrepresentation of the context determines the degree of generalization and\nexpressivity. Categorical contexts preclude generalization to entirely new\ntasks. Goal-conditioned policies may enable some generalization, but cannot\ncapture all tasks that might be desired. In this paper, we propose goal\ndistributions as a general and broadly applicable task representation suitable\nfor contextual policies. Goal distributions are general in the sense that they\ncan represent any state-based reward function when equipped with an appropriate\ndistribution class, while the particular choice of distribution class allows us\nto trade off expressivity and learnability. We develop an off-policy algorithm\ncalled distribution-conditioned reinforcement learning (DisCo RL) to\nefficiently learn these policies. We evaluate DisCo RL on a variety of robot\nmanipulation tasks and find that it significantly outperforms prior methods on\ntasks that require generalization to new goal distributions.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 16:51:58 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Nasiriany", "Soroush", ""], ["Pong", "Vitchyr H.", ""], ["Nair", "Ashvin", ""], ["Khazatsky", "Alexander", ""], ["Berseth", "Glen", ""], ["Levine", "Sergey", ""]]}, {"id": "2104.11776", "submitter": "Pablo Martinez-Gonzalez", "authors": "Pablo Martinez-Gonzalez, Sergiu Oprea, John Alejandro Castro-Vargas,\n  Alberto Garcia-Garcia, Sergio Orts-Escolano, Jose Garcia-Rodriguez and Markus\n  Vincze", "title": "UnrealROX+: An Improved Tool for Acquiring Synthetic Data from Virtual\n  3D Environments", "comments": "Accepted at International Joint Conference on Neural Networks (IJCNN)\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Synthetic data generation has become essential in last years for feeding\ndata-driven algorithms, which surpassed traditional techniques performance in\nalmost every computer vision problem. Gathering and labelling the amount of\ndata needed for these data-hungry models in the real world may become\nunfeasible and error-prone, while synthetic data give us the possibility of\ngenerating huge amounts of data with pixel-perfect annotations. However, most\nsynthetic datasets lack from enough realism in their rendered images. In that\ncontext UnrealROX generation tool was presented in 2019, allowing to generate\nhighly realistic data, at high resolutions and framerates, with an efficient\npipeline based on Unreal Engine, a cutting-edge videogame engine. UnrealROX\nenabled robotic vision researchers to generate realistic and visually plausible\ndata with full ground truth for a wide variety of problems such as class and\ninstance semantic segmentation, object detection, depth estimation, visual\ngrasping, and navigation. Nevertheless, its workflow was very tied to generate\nimage sequences from a robotic on-board camera, making hard to generate data\nfor other purposes. In this work, we present UnrealROX+, an improved version of\nUnrealROX where its decoupled and easy-to-use data acquisition system allows to\nquickly design and generate data in a much more flexible and customizable way.\nMoreover, it is packaged as an Unreal plug-in, which makes it more comfortable\nto use with already existing Unreal projects, and it also includes new features\nsuch as generating albedo or a Python API for interacting with the virtual\nenvironment from Deep Learning frameworks.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 18:45:42 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Martinez-Gonzalez", "Pablo", ""], ["Oprea", "Sergiu", ""], ["Castro-Vargas", "John Alejandro", ""], ["Garcia-Garcia", "Alberto", ""], ["Orts-Escolano", "Sergio", ""], ["Garcia-Rodriguez", "Jose", ""], ["Vincze", "Markus", ""]]}, {"id": "2104.11805", "submitter": "Gunduz Vehbi Demirci", "authors": "Gunduz Vehbi Demirci, Hakan Ferhatosmanoglu", "title": "Partitioning sparse deep neural networks for scalable training and\n  inference", "comments": "Gunduz Vehbi Demirci and Hakan Ferhatosmanoglu. 2021. Partitioning\n  Sparse Deep Neural Networks for Scalable Training and Inference. In 2021\n  International Conference on Supercomputing (ICS '21), June 14-17, 2021,\n  Virtual Event, USA. ACM, New York, NY, USA, 12 pages", "journal-ref": null, "doi": "10.1145/3447818.3460372", "report-no": null, "categories": "cs.LG cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The state-of-the-art deep neural networks (DNNs) have significant\ncomputational and data management requirements. The size of both training data\nand models continue to increase. Sparsification and pruning methods are shown\nto be effective in removing a large fraction of connections in DNNs. The\nresulting sparse networks present unique challenges to further improve the\ncomputational efficiency of training and inference in deep learning. Both the\nfeedforward (inference) and backpropagation steps in stochastic gradient\ndescent (SGD) algorithm for training sparse DNNs involve consecutive sparse\nmatrix-vector multiplications (SpMVs). We first introduce a distributed-memory\nparallel SpMV-based solution for the SGD algorithm to improve its scalability.\nThe parallelization approach is based on row-wise partitioning of weight\nmatrices that represent neuron connections between consecutive layers. We then\npropose a novel hypergraph model for partitioning weight matrices to reduce the\ntotal communication volume and ensure computational load-balance among\nprocessors. Experiments performed on sparse DNNs demonstrate that the proposed\nsolution is highly efficient and scalable. By utilizing the proposed matrix\npartitioning scheme, the performance of our solution is further improved\nsignificantly.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 20:05:52 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Demirci", "Gunduz Vehbi", ""], ["Ferhatosmanoglu", "Hakan", ""]]}, {"id": "2104.11809", "submitter": "Pavel Surynek", "authors": "Pavel Surynek", "title": "Compilation-based Solvers for Multi-Agent Path Finding: a Survey,\n  Discussion, and Future Opportunities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-agent path finding (MAPF) attracts considerable attention in artificial\nintelligence community as well as in robotics, and other fields such as\nwarehouse logistics. The task in the standard MAPF is to find paths through\nwhich agents can navigate from their starting positions to specified individual\ngoal positions. The combination of two additional requirements makes the\nproblem computationally challenging: (i) agents must not collide with each\nother and (ii) the paths must be optimal with respect to some objective. Two\nmajor approaches to optimal MAPF solving include (1) dedicated search-based\nmethods, which solve MAPF directly, and (2) compilation-based methods that\nreduce a MAPF instance to an instance in a different well established\nformalism, for which an efficient solver exists. The compilation-based MAPF\nsolving can benefit from advancements accumulated during the development of the\ntarget solver often decades long. We summarize and compare contemporary\ncompilation-based solvers for MAPF using formalisms like ASP, MIP, and SAT. We\nshow the lessons learned from past developments and current trends in the topic\nand discuss its wider impact.\n", "versions": [{"version": "v1", "created": "Fri, 23 Apr 2021 20:13:12 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Surynek", "Pavel", ""]]}, {"id": "2104.11880", "submitter": "SeyyedPooya HekmatiAthar", "authors": "SeyyedPooya HekmatiAthar and Mohd Anwar", "title": "Music Embedding: A Tool for Incorporating Music Theory into\n  Computational Music Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Advancements in the digital technologies have enabled researchers to develop\na variety of Computational Music applications. Such applications are required\nto capture, process, and generate data related to music. Therefore, it is\nimportant to digitally represent music in a music theoretic and concise manner.\nExisting approaches for representing music are ineffective in terms of\nutilizing music theory. In this paper, we address the disjoint of music theory\nand computational music by developing an opensource representation tool based\non music theory. Through the wide range of use cases, we run an analysis on the\nclassical music pieces to show the usefulness of the developed music embedding.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 04:32:45 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["HekmatiAthar", "SeyyedPooya", ""], ["Anwar", "Mohd", ""]]}, {"id": "2104.11902", "submitter": "Jivat Neet Kaur", "authors": "Jivat Neet Kaur, Yiding Jiang, Paul Pu Liang", "title": "Ask & Explore: Grounded Question Answering for Curiosity-Driven\n  Exploration", "comments": "Accepted at ICLR 2021 Workshop on Embodied Multimodal Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In many real-world scenarios where extrinsic rewards to the agent are\nextremely sparse, curiosity has emerged as a useful concept providing intrinsic\nrewards that enable the agent to explore its environment and acquire\ninformation to achieve its goals. Despite their strong performance on many\nsparse-reward tasks, existing curiosity approaches rely on an overly holistic\nview of state transitions, and do not allow for a structured understanding of\nspecific aspects of the environment. In this paper, we formulate curiosity\nbased on grounded question answering by encouraging the agent to ask questions\nabout the environment and be curious when the answers to these questions\nchange. We show that natural language questions encourage the agent to uncover\nspecific knowledge about their environment such as the physical properties of\nobjects as well as their spatial relationships with other objects, which serve\nas valuable curiosity rewards to solve sparse-reward tasks more efficiently.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 07:56:31 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Kaur", "Jivat Neet", ""], ["Jiang", "Yiding", ""], ["Liang", "Paul Pu", ""]]}, {"id": "2104.11914", "submitter": "Natalia D\\'iaz-Rodr\\'iguez PhD", "authors": "Natalia D\\'iaz-Rodr\\'iguez, Alberto Lamas, Jules Sanchez, Gianni\n  Franchi, Ivan Donadello, Siham Tabik, David Filliat, Policarpo Cruz, Rosana\n  Montes, Francisco Herrera", "title": "EXplainable Neural-Symbolic Learning (X-NeSyL) methodology to fuse deep\n  learning representations with expert knowledge graphs: the MonuMAI cultural\n  heritage use case", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.SC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The latest Deep Learning (DL) models for detection and classification have\nachieved an unprecedented performance over classical machine learning\nalgorithms. However, DL models are black-box methods hard to debug, interpret,\nand certify. DL alone cannot provide explanations that can be validated by a\nnon technical audience. In contrast, symbolic AI systems that convert concepts\ninto rules or symbols -- such as knowledge graphs -- are easier to explain.\nHowever, they present lower generalisation and scaling capabilities. A very\nimportant challenge is to fuse DL representations with expert knowledge. One\nway to address this challenge, as well as the performance-explainability\ntrade-off is by leveraging the best of both streams without obviating domain\nexpert knowledge. We tackle such problem by considering the symbolic knowledge\nis expressed in form of a domain expert knowledge graph. We present the\neXplainable Neural-symbolic learning (X-NeSyL) methodology, designed to learn\nboth symbolic and deep representations, together with an explainability metric\nto assess the level of alignment of machine and human expert explanations. The\nultimate objective is to fuse DL representations with expert domain knowledge\nduring the learning process to serve as a sound basis for explainability.\nX-NeSyL methodology involves the concrete use of two notions of explanation at\ninference and training time respectively: 1) EXPLANet: Expert-aligned\neXplainable Part-based cLAssifier NETwork Architecture, a compositional CNN\nthat makes use of symbolic representations, and 2) SHAP-Backprop, an\nexplainable AI-informed training procedure that guides the DL process to align\nwith such symbolic representations in form of knowledge graphs. We showcase\nX-NeSyL methodology using MonuMAI dataset for monument facade image\nclassification, and demonstrate that our approach improves explainability and\nperformance.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 09:06:08 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["D\u00edaz-Rodr\u00edguez", "Natalia", ""], ["Lamas", "Alberto", ""], ["Sanchez", "Jules", ""], ["Franchi", "Gianni", ""], ["Donadello", "Ivan", ""], ["Tabik", "Siham", ""], ["Filliat", "David", ""], ["Cruz", "Policarpo", ""], ["Montes", "Rosana", ""], ["Herrera", "Francisco", ""]]}, {"id": "2104.11918", "submitter": "Helge Spieker", "authors": "Helge Spieker", "title": "Constraint-Guided Reinforcement Learning: Augmenting the\n  Agent-Environment-Interaction", "comments": "IJCNN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reinforcement Learning (RL) agents have great successes in solving tasks with\nlarge observation and action spaces from limited feedback. Still, training the\nagents is data-intensive and there are no guarantees that the learned behavior\nis safe and does not violate rules of the environment, which has limitations\nfor the practical deployment in real-world scenarios. This paper discusses the\nengineering of reliable agents via the integration of deep RL with\nconstraint-based augmentation models to guide the RL agent towards safe\nbehavior. Within the constraints set, the RL agent is free to adapt and\nexplore, such that its effectiveness to solve the given problem is not\nhindered. However, once the RL agent leaves the space defined by the\nconstraints, the outside models can provide guidance to still work reliably. We\ndiscuss integration points for constraint guidance within the RL process and\nperform experiments on two case studies: a strictly constrained card game and a\ngrid world environment with additional combinatorial subgoals. Our results show\nthat constraint-guidance does both provide reliability improvements and safer\nbehavior, as well as accelerated training.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 10:04:14 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Spieker", "Helge", ""]]}, {"id": "2104.11927", "submitter": "Furkan Ulger Mr.", "authors": "Furkan Ulger, Seniha Esen Yuksel, Atila Yilmaz", "title": "Anomaly Detection for Solder Joints Using $\\beta$-VAE", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In the assembly process of printed circuit boards (PCB), most of the errors\nare caused by solder joints in Surface Mount Devices (SMD). In the literature,\ntraditional feature extraction based methods require designing hand-crafted\nfeatures and rely on the tiered RGB illumination to detect solder joint errors,\nwhereas the supervised Convolutional Neural Network (CNN) based approaches\nrequire a lot of labelled abnormal samples (defective solder joints) to achieve\nhigh accuracy. To solve the optical inspection problem in unrestricted\nenvironments with no special lighting and without the existence of error-free\nreference boards, we propose a new beta-Variational Autoencoders (beta-VAE)\narchitecture for anomaly detection that can work on both IC and non-IC\ncomponents. We show that the proposed model learns disentangled representation\nof data, leading to more independent features and improved latent space\nrepresentations. We compare the activation and gradient-based representations\nthat are used to characterize anomalies; and observe the effect of different\nbeta parameters on accuracy and on untwining the feature representations in\nbeta-VAE. Finally, we show that anomalies on solder joints can be detected with\nhigh accuracy via a model trained on directly normal samples without designated\nhardware or feature engineering.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 11:19:27 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Ulger", "Furkan", ""], ["Yuksel", "Seniha Esen", ""], ["Yilmaz", "Atila", ""]]}, {"id": "2104.11934", "submitter": "Jun Chen", "authors": "Jun Chen, Aniket Agarwal, Sherif Abdelkarim, Deyao Zhu, Mohamed\n  Elhoseiny", "title": "RelTransformer: Balancing the Visual Relationship Detection from Local\n  Context, Scene and Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Visual relationship recognition (VRR) is a fundamental scene understanding\ntask. The structure that VRR provides is essential to improve the AI\ninterpretability in downstream tasks such as image captioning and visual\nquestion answering. Several recent studies showed that the long-tail problem in\nVRR is even more critical than that in object recognition due to the\ncompositional complexity and structure. To overcome this limitation, we propose\na novel transformer-based framework, dubbed as RelTransformer, which performs\nrelationship prediction using rich semantic features from multiple image\nlevels. We assume that more abundantcon textual features can generate more\naccurate and discriminative relationships, which can be useful when sufficient\ntraining data are lacking. The key feature of our model is its ability to\naggregate three different-level features (local context, scene, and\ndataset-level) to compositionally predict the visual relationship. We evaluate\nour model on the visual genome and two \"long-tail\" VRR datasets, GQA-LT and\nVG8k-LT. Extensive experiments demonstrate that our RelTransformer could\nimprove over the state-of-the-art baselines on all the datasets. In addition,\nour model significantly improves the accuracy of GQA-LT by 27.4% upon the best\nbaselines on tail-relationship prediction. Our code is available in\nhttps://github.com/Vision-CAIR/RelTransformer.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 12:04:04 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Chen", "Jun", ""], ["Agarwal", "Aniket", ""], ["Abdelkarim", "Sherif", ""], ["Zhu", "Deyao", ""], ["Elhoseiny", "Mohamed", ""]]}, {"id": "2104.11951", "submitter": "Xavier Gillard", "authors": "Xavier Gillard, Vianney Copp\\'e, Pierre Schaus, Andr\\'e Augusto Cire", "title": "Improving the filtering of Branch-And-Bound MDD solver (extended)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents and evaluates two pruning techniques to reinforce the\nefficiency of constraint optimization solvers based on multi-valued\ndecision-diagrams (MDD). It adopts the branch-and-bound framework proposed by\nBergman et al. in 2016 to solve dynamic programs to optimality. In particular,\nour paper presents and evaluates the effectiveness of the local-bound (LocB)\nand rough upper-bound pruning (RUB). LocB is a new and effective rule that\nleverages the approximate MDD structure to avoid the exploration of\nnon-interesting nodes. RUB is a rule to reduce the search space during the\ndevelopment of bounded-width-MDDs. The experimental study we conducted on the\nMaximum Independent Set Problem (MISP), Maximum Cut Problem (MCP), Maximum 2\nSatisfiability (MAX2SAT) and the Traveling Salesman Problem with Time Windows\n(TSPTW) shows evidence indicating that rough-upper-bound and local-bound\npruning have a high impact on optimization solvers based on branch-and-bound\nwith MDDs. In particular, it shows that RUB delivers excellent results but\nrequires some effort when defining the model. Also, it shows that LocB provides\na significant improvement automatically; without necessitating any\nuser-supplied information. Finally, it also shows that rough-upper-bound and\nlocal-bound pruning are not mutually exclusive, and their combined benefit\nsupersedes the individual benefit of using each technique.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 13:42:42 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Gillard", "Xavier", ""], ["Copp\u00e9", "Vianney", ""], ["Schaus", "Pierre", ""], ["Cire", "Andr\u00e9 Augusto", ""]]}, {"id": "2104.11970", "submitter": "Hassan Alsawadi", "authors": "Hassan Alsawadi and Muhammad Bilal", "title": "Measuring Novelty in Autonomous Vehicles Motion Using Local Outlier\n  Factor Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Under unexpected conditions or scenarios, autonomous vehicles (AV) are more\nlikely to follow abnormal unplanned actions, due to the limited set of rules or\namount of experience they possess at that time. Enabling AV to measure the\ndegree at which their movements are novel in real-time may help to decrease any\npossible negative consequences. We propose a method based on the Local Outlier\nFactor (LOF) algorithm to quantify this novelty measure. We extracted features\nfrom the inertial measurement unit (IMU) sensor's readings, which captures the\nvehicle's motion. We followed a novelty detection approach in which the model\nis fitted only using the normal data. Using datasets obtained from real-world\nvehicle missions, we demonstrate that the suggested metric can quantify to some\nextent the degree of novelty. Finally, a performance evaluation of the model\nconfirms that our novelty metric can be practical.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 15:19:35 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Alsawadi", "Hassan", ""], ["Bilal", "Muhammad", ""]]}, {"id": "2104.12000", "submitter": "Hadi Jahanshahi", "authors": "Hadi Jahanshahi, Aysun Bozanta, Mucahit Cevik, Eray Mert Kavuk,\n  Ay\\c{s}e Tosun, Sibel B. Sonuc, Bilgin Kosucu, Ay\\c{s}e Ba\\c{s}ar", "title": "A Deep Reinforcement Learning Approach for the Meal Delivery Problem", "comments": "Keywords: meal delivery, courier assignment, reinforcement learning,\n  DQN, DDQN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a meal delivery service fulfilling dynamic customer requests\ngiven a set of couriers over the course of a day. A courier's duty is to\npick-up an order from a restaurant and deliver it to a customer. We model this\nservice as a Markov decision process and use deep reinforcement learning as the\nsolution approach. We experiment with the resulting policies on synthetic and\nreal-world datasets and compare those with the baseline policies. We also\nexamine the courier utilization for different numbers of couriers. In our\nanalysis, we specifically focus on the impact of the limited available\nresources in the meal delivery problem. Furthermore, we investigate the effect\nof intelligent order rejection and re-positioning of the couriers. Our\nnumerical experiments show that, by incorporating the geographical locations of\nthe restaurants, customers, and the depot, our model significantly improves the\noverall service quality as characterized by the expected total reward and the\ndelivery times. Our results present valuable insights on both the courier\nassignment process and the optimal number of couriers for different order\nfrequencies on a given day. The proposed model also shows a robust performance\nunder a variety of scenarios for real-world implementation.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 19:01:59 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Jahanshahi", "Hadi", ""], ["Bozanta", "Aysun", ""], ["Cevik", "Mucahit", ""], ["Kavuk", "Eray Mert", ""], ["Tosun", "Ay\u015fe", ""], ["Sonuc", "Sibel B.", ""], ["Kosucu", "Bilgin", ""], ["Ba\u015far", "Ay\u015fe", ""]]}, {"id": "2104.12005", "submitter": "Panagiotis Diamantoulakis", "authors": "Pavlos S. Bouzinis, Panagiotis D. Diamantoulakis, George K.\n  Karagiannidis", "title": "Wireless Federated Learning (WFL) for 6G Networks -- Part II: The\n  Compute-then-Transmit NOMA Paradigm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AI math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As it has been discussed in the first part of this work, the utilization of\nadvanced multiple access protocols and the joint optimization of the\ncommunication and computing resources can facilitate the reduction of delay for\nwireless federated learning (WFL), which is of paramount importance for the\nefficient integration of WFL in the sixth generation of wireless networks (6G).\nTo this end, in this second part we introduce and optimize a novel\ncommunication protocol for WFL networks, that is based on non-orthogonal\nmultiple access (NOMA). More specifically, the Compute-then-Transmit NOMA\n(CT-NOMA) protocol is introduced, where users terminate concurrently the local\nmodel training and then simultaneously transmit the trained parameters to the\ncentral server. Moreover, two different detection schemes for the mitigation of\ninter-user interference in NOMA are considered and evaluated, which correspond\nto fixed and variable decoding order during the successive interference\ncancellation process. Furthermore, the computation and communication resources\nare jointly optimized for both considered schemes, with the aim to minimize the\ntotal delay during a WFL communication round. Finally, the simulation results\nverify the effectiveness of CT-NOMA in terms of delay reduction, compared to\nthe considered benchmark that is based on time-division multiple access.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 19:14:28 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Bouzinis", "Pavlos S.", ""], ["Diamantoulakis", "Panagiotis D.", ""], ["Karagiannidis", "George K.", ""]]}, {"id": "2104.12021", "submitter": "Debaditya Chakraborty", "authors": "Debaditya Chakraborty, Cristina Ivan, Paola Amero, Maliha Khan,\n  Cristian Rodriguez-Aguayo, Hakan Ba\\c{s}a\\u{g}ao\\u{g}lu, and Gabriel\n  Lopez-Berestein", "title": "Explainable Artificial Intelligence Reveals Novel Insight into Tumor\n  Microenvironment Conditions Linked with Better Prognosis in Patients with\n  Breast Cancer", "comments": "14 pages, 5 figures, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We investigated the data-driven relationship between features in the tumor\nmicroenvironment (TME) and the overall and 5-year survival in triple-negative\nbreast cancer (TNBC) and non-TNBC (NTNBC) patients by using Explainable\nArtificial Intelligence (XAI) models. We used clinical information from\npatients with invasive breast carcinoma from The Cancer Genome Atlas and from\ntwo studies from the cbioPortal, the PanCanAtlas project and the GDAC Firehose\nstudy. In this study, we used a normalized RNA sequencing data-driven cohort\nfrom 1,015 breast cancer patients, alive or deceased, from the UCSC Xena data\nset and performed integrated deconvolution with the EPIC method to estimate the\npercentage of seven different immune and stromal cells from RNA sequencing\ndata. Novel insights derived from our XAI model showed that CD4+ T cells and B\ncells are more critical than other TME features for enhanced prognosis for both\nTNBC and NTNBC patients. Our XAI model revealed the critical inflection points\n(i.e., threshold fractions) of CD4+ T cells and B cells above or below which\n5-year survival rates improve. Subsequently, we ascertained the conditional\nprobabilities of $\\geq$ 5-year survival in both TNBC and NTNBC patients under\nspecific conditions inferred from the inflection points. In particular, the XAI\nmodels revealed that a B-cell fraction exceeding 0.018 in the TME could ensure\n100% 5-year survival for NTNBC patients. The findings from this research could\nlead to more accurate clinical predictions and enhanced immunotherapies and to\nthe design of innovative strategies to reprogram the TME of breast cancer\npatients.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 20:50:41 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Chakraborty", "Debaditya", ""], ["Ivan", "Cristina", ""], ["Amero", "Paola", ""], ["Khan", "Maliha", ""], ["Rodriguez-Aguayo", "Cristian", ""], ["Ba\u015fa\u011fao\u011flu", "Hakan", ""], ["Lopez-Berestein", "Gabriel", ""]]}, {"id": "2104.12037", "submitter": "Pegah Nokhiz", "authors": "Pegah Nokhiz, Aravinda Kanchana Ruwanpathirana, Neal Patwari, Suresh\n  Venkatasubramanian", "title": "Precarity: Modeling the Long Term Effects of Compounded Decisions on\n  Individual Instability", "comments": "To appear at AIES 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When it comes to studying the impacts of decision making, the research has\nbeen largely focused on examining the fairness of the decisions, the long-term\neffects of the decision pipelines, and utility-based perspectives considering\nboth the decision-maker and the individuals. However, there has hardly been any\nfocus on precarity which is the term that encapsulates the instability in\npeople's lives. That is, a negative outcome can overspread to other decisions\nand measures of well-being. Studying precarity necessitates a shift in focus -\nfrom the point of view of the decision-maker to the perspective of the decision\nsubject. This centering of the subject is an important direction that unlocks\nthe importance of parting with aggregate measures to examine the long-term\neffects of decision making. To address this issue, in this paper, we propose a\nmodeling framework that simulates the effects of compounded decision-making on\nprecarity over time. Through our simulations, we are able to show the\nheterogeneity of precarity by the non-uniform ruinous aftereffects of negative\ndecisions on different income classes of the underlying population and how\npolicy interventions can help mitigate such effects.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 23:38:07 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Nokhiz", "Pegah", ""], ["Ruwanpathirana", "Aravinda Kanchana", ""], ["Patwari", "Neal", ""], ["Venkatasubramanian", "Suresh", ""]]}, {"id": "2104.12076", "submitter": "Usman Sajid", "authors": "Usman Sajid, Michael Chow, Jin Zhang, Taejoon Kim, Guanghui Wang", "title": "Parallel Scale-wise Attention Network for Effective Scene Text\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper proposes a new text recognition network for scene-text images. Many\nstate-of-the-art methods employ the attention mechanism either in the text\nencoder or decoder for the text alignment. Although the encoder-based attention\nyields promising results, these schemes inherit noticeable limitations. They\nperform the feature extraction (FE) and visual attention (VA) sequentially,\nwhich bounds the attention mechanism to rely only on the FE final single-scale\noutput. Moreover, the utilization of the attention process is limited by only\napplying it directly to the single scale feature-maps. To address these issues,\nwe propose a new multi-scale and encoder-based attention network for text\nrecognition that performs the multi-scale FE and VA in parallel. The\nmulti-scale channels also undergo regular fusion with each other to develop the\ncoordinated knowledge together. Quantitative evaluation and robustness analysis\non the standard benchmarks demonstrate that the proposed network outperforms\nthe state-of-the-art in most cases.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 06:44:26 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Sajid", "Usman", ""], ["Chow", "Michael", ""], ["Zhang", "Jin", ""], ["Kim", "Taejoon", ""], ["Wang", "Guanghui", ""]]}, {"id": "2104.12086", "submitter": "Chen Zhao", "authors": "Chen Zhao, Zhipeng Gao, Qian Wang, Kaile Xiao, Zijia Mo, M. Jamal Deen", "title": "FedSup: A Communication-Efficient Federated Learning Fatigue Driving\n  Behaviors Supervision Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the proliferation of edge smart devices and the Internet of Vehicles\n(IoV) technologies, intelligent fatigue detection has become one of the\nmost-used methods in our daily driving. To improve the performance of the\ndetection model, a series of techniques have been developed. However, existing\nwork still leaves much to be desired, such as privacy disclosure and\ncommunication cost. To address these issues, we propose FedSup, a\nclient-edge-cloud framework for privacy and efficient fatigue detection.\nInspired by the federated learning technique, FedSup intelligently utilizes the\ncollaboration between client, edge, and cloud server to realizing dynamic model\noptimization while protecting edge data privacy. Moreover, to reduce the\nunnecessary system communication overhead, we further propose a Bayesian\nconvolutional neural network (BCNN) approximation strategy on the clients and\nan uncertainty weighted aggregation algorithm on the cloud to enhance the\ncentral model training efficiency. Extensive experiments demonstrate that the\nFedSup framework is suitable for IoV scenarios and outperforms other mainstream\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 07:16:49 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Zhao", "Chen", ""], ["Gao", "Zhipeng", ""], ["Wang", "Qian", ""], ["Xiao", "Kaile", ""], ["Mo", "Zijia", ""], ["Deen", "M. Jamal", ""]]}, {"id": "2104.12103", "submitter": "Louis-Serge Bouchard", "authors": "Khalid Youssef, Greg Schuette, Yubin Cai, Daisong Zhang, Yikun Huang,\n  Yahya Rahmat-Samii, Louis-S. Bouchard", "title": "Scalable End-to-End RF Classification: A Case Study on Undersized\n  Dataset Regularization by Convolutional-MST", "comments": "12 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unlike areas such as computer vision and speech recognition where\nconvolutional and recurrent neural networks-based approaches have proven\neffective to the nature of the respective areas of application, deep learning\n(DL) still lacks a general approach suitable for the unique nature and\nchallenges of RF systems such as radar, signals intelligence, electronic\nwarfare, and communications. Existing approaches face problems in robustness,\nconsistency, efficiency, repeatability and scalability. One of the main\nchallenges in RF sensing such as radar target identification is the difficulty\nand cost of obtaining data. Hundreds to thousands of samples per class are\ntypically used when training for classifying signals into 2 to 12 classes with\nreported accuracy ranging from 87% to 99%, where accuracy generally decreases\nwith more classes added. In this paper, we present a new DL approach based on\nmultistage training and demonstrate it on RF sensing signal classification. We\nconsistently achieve over 99% accuracy for up to 17 diverse classes using only\n11 samples per class for training, yielding up to 35% improvement in accuracy\nover standard DL approaches.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 08:41:52 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 08:44:15 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Youssef", "Khalid", ""], ["Schuette", "Greg", ""], ["Cai", "Yubin", ""], ["Zhang", "Daisong", ""], ["Huang", "Yikun", ""], ["Rahmat-Samii", "Yahya", ""], ["Bouchard", "Louis-S.", ""]]}, {"id": "2104.12156", "submitter": "Christian Anti\\'c", "authors": "Christian Antic", "title": "Algebraic answer set programming", "comments": "arXiv admin note: text overlap with arXiv:2009.05774", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DM cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Non-monotonic reasoning is an essential part of human intelligence\nprominently formalized in artificial intelligence research via answer set\nprogramming. Describing complex objects as the composition of elementary ones\nis a common strategy in computer science and science in general. This paper\ncontributes to the foundations of answer set programming and artificial\nintelligence by introducing and studying the sequential composition of answer\nset programs. Specifically, we show that the notion of composition gives rise\nto a family of finite monoids and seminearrings, baptized {\\em ASP monoids} and\n{\\em ASP seminearrings} in this paper. Particularly, we show that the\ncombination of composition and union yields the structure of a finite\nidempotent seminearring. We also show that the restricted class of proper\nKrom-Horn programs, which only contain rules with exactly one body atom, yields\na finite idempotent semiring. On the semantic side, we show that the van\nEmden-Kowalski immediate consequence operator of a program can be represented\nvia composition, which allows us to compute the least model semantics of Horn\nprograms without any explicit reference to operators. As a result, we\ncharacterize answer sets algebraically, which bridges the conceptual gap\nbetween the syntax and semantics of an answer set program in a mathematically\nsatisfactory way, and which provides an algebraic characterization of strong\nand uniform equivalence. Moreover, it gives rise to an algebraic meta-calculus\nfor answer set programs. In a broader sense, this paper is a further step\ntowards an algebra of rule-based logical theories and in the future we plan to\nadapt and generalize the methods of this paper to wider classes of formalisms,\nmost importantly to first-order and disjunctive answer set programs and\nextensions thereof.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 13:27:22 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Antic", "Christian", ""]]}, {"id": "2104.12174", "submitter": "Ivan Y. Tyukin", "authors": "Ivan Y. Tyukin, Alexander N. Gorban, Muhammad H. Alkhudaydi, Qinghua\n  Zhou", "title": "Demystification of Few-shot and One-shot Learning", "comments": "IEEE International Joint Conference on Neural Networks, IJCNN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.ST stat.TH", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Few-shot and one-shot learning have been the subject of active and intensive\nresearch in recent years, with mounting evidence pointing to successful\nimplementation and exploitation of few-shot learning algorithms in practice.\nClassical statistical learning theories do not fully explain why few- or\none-shot learning is at all possible since traditional generalisation bounds\nnormally require large training and testing samples to be meaningful. This\nsharply contrasts with numerous examples of successful one- and few-shot\nlearning systems and applications.\n  In this work we present mathematical foundations for a theory of one-shot and\nfew-shot learning and reveal conditions specifying when such learning schemes\nare likely to succeed. Our theory is based on intrinsic properties of\nhigh-dimensional spaces. We show that if the ambient or latent decision space\nof a learning machine is sufficiently high-dimensional than a large class of\nobjects in this space can indeed be easily learned from few examples provided\nthat certain data non-concentration conditions are met.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 14:47:05 GMT"}, {"version": "v2", "created": "Sat, 29 May 2021 12:25:59 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Tyukin", "Ivan Y.", ""], ["Gorban", "Alexander N.", ""], ["Alkhudaydi", "Muhammad H.", ""], ["Zhou", "Qinghua", ""]]}, {"id": "2104.12233", "submitter": "Zheng Tang", "authors": "Milind Naphade, Shuo Wang, David C. Anastasiu, Zheng Tang, Ming-Ching\n  Chang, Xiaodong Yang, Yue Yao, Liang Zheng, Pranamesh Chakraborty, Christian\n  E. Lopez, Anuj Sharma, Qi Feng, Vitaly Ablavsky, Stan Sclaroff", "title": "The 5th AI City Challenge", "comments": "Summary of the 5th AI City Challenge Workshop in conjunction with\n  CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The AI City Challenge was created with two goals in mind: (1) pushing the\nboundaries of research and development in intelligent video analysis for\nsmarter cities use cases, and (2) assessing tasks where the level of\nperformance is enough to cause real-world adoption. Transportation is a segment\nripe for such adoption. The fifth AI City Challenge attracted 305 participating\nteams across 38 countries, who leveraged city-scale real traffic data and\nhigh-quality synthetic data to compete in five challenge tracks. Track 1\naddressed video-based automatic vehicle counting, where the evaluation being\nconducted on both algorithmic effectiveness and computational efficiency. Track\n2 addressed city-scale vehicle re-identification with augmented synthetic data\nto substantially increase the training set for the task. Track 3 addressed\ncity-scale multi-target multi-camera vehicle tracking. Track 4 addressed\ntraffic anomaly detection. Track 5 was a new track addressing vehicle retrieval\nusing natural language descriptions. The evaluation system shows a general\nleader board of all submitted results, and a public leader board of results\nlimited to the contest participation rules, where teams are not allowed to use\nexternal data in their work. The public leader board shows results more close\nto real-world situations where annotated data is limited. Results show the\npromise of AI in Smarter Transportation. State-of-the-art performance for some\ntasks shows that these technologies are ready for adoption in real-world\nsystems.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 19:15:27 GMT"}, {"version": "v2", "created": "Mon, 24 May 2021 19:26:35 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Naphade", "Milind", ""], ["Wang", "Shuo", ""], ["Anastasiu", "David C.", ""], ["Tang", "Zheng", ""], ["Chang", "Ming-Ching", ""], ["Yang", "Xiaodong", ""], ["Yao", "Yue", ""], ["Zheng", "Liang", ""], ["Chakraborty", "Pranamesh", ""], ["Lopez", "Christian E.", ""], ["Sharma", "Anuj", ""], ["Feng", "Qi", ""], ["Ablavsky", "Vitaly", ""], ["Sclaroff", "Stan", ""]]}, {"id": "2104.12269", "submitter": "Diwanshu Shekhar", "authors": "Diwanshu Shekhar, Pooran S. Negi, Mohammad Mahoor", "title": "A Bi-Encoder LSTM Model For Learning Unstructured Dialogs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Creating a data-driven model that is trained on a large dataset of\nunstructured dialogs is a crucial step in developing Retrieval-based Chatbot\nsystems. This paper presents a Long Short Term Memory (LSTM) based architecture\nthat learns unstructured multi-turn dialogs and provides results on the task of\nselecting the best response from a collection of given responses. Ubuntu Dialog\nCorpus Version 2 was used as the corpus for training. We show that our model\nachieves 0.8%, 1.0% and 0.3% higher accuracy for Recall@1, Recall@2 and\nRecall@5 respectively than the benchmark model. We also show results on\nexperiments performed by using several similarity functions, model\nhyper-parameters and word embeddings on the proposed architecture\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 21:37:35 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Shekhar", "Diwanshu", ""], ["Negi", "Pooran S.", ""], ["Mahoor", "Mohammad", ""]]}, {"id": "2104.12278", "submitter": "Lu Cheng", "authors": "Lu Cheng, Ahmadreza Mosallanezhad, Paras Sheth, Huan Liu", "title": "Causal Learning for Socially Responsible AI", "comments": "8 pages, 3 figures, accepted at IJCAI21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There have been increasing concerns about Artificial Intelligence (AI) due to\nits unfathomable potential power. To make AI address ethical challenges and\nshun undesirable outcomes, researchers proposed to develop socially responsible\nAI (SRAI). One of these approaches is causal learning (CL). We survey\nstate-of-the-art methods of CL for SRAI. We begin by examining the seven CL\ntools to enhance the social responsibility of AI, then review how existing\nworks have succeeded using these tools to tackle issues in developing SRAI such\nas fairness. The goal of this survey is to bring forefront the potentials and\npromises of CL for SRAI.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 22:09:11 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Cheng", "Lu", ""], ["Mosallanezhad", "Ahmadreza", ""], ["Sheth", "Paras", ""], ["Liu", "Huan", ""]]}, {"id": "2104.12282", "submitter": "Yuyu Zhang", "authors": "Yuyu Zhang, Heng Chi, Binghong Chen, Tsz Ling Elaine Tang, Lucia\n  Mirabella, Le Song, Glaucio H. Paulino", "title": "Speeding up Computational Morphogenesis with Online Neural Synthetic\n  Gradients", "comments": "Accepted by IJCNN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A wide range of modern science and engineering applications are formulated as\noptimization problems with a system of partial differential equations (PDEs) as\nconstraints. These PDE-constrained optimization problems are typically solved\nin a standard discretize-then-optimize approach. In many industry applications\nthat require high-resolution solutions, the discretized constraints can easily\nhave millions or even billions of variables, making it very slow for the\nstandard iterative optimizer to solve the exact gradients. In this work, we\npropose a general framework to speed up PDE-constrained optimization using\nonline neural synthetic gradients (ONSG) with a novel two-scale optimization\nscheme. We successfully apply our ONSG framework to computational\nmorphogenesis, a representative and challenging class of PDE-constrained\noptimization problems. Extensive experiments have demonstrated that our method\ncan significantly speed up computational morphogenesis (also known as topology\noptimization), and meanwhile maintain the quality of final solution compared to\nthe standard optimizer. On a large-scale 3D optimal design problem with around\n1,400,000 design variables, our method achieves up to 7.5x speedup while\nproducing optimized designs with comparable objectives.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 22:43:51 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 01:27:57 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Zhang", "Yuyu", ""], ["Chi", "Heng", ""], ["Chen", "Binghong", ""], ["Tang", "Tsz Ling Elaine", ""], ["Mirabella", "Lucia", ""], ["Song", "Le", ""], ["Paulino", "Glaucio H.", ""]]}, {"id": "2104.12294", "submitter": "Mohammad Rahimzadeh", "authors": "Mohammad Rahimzadeh, Soroush Parvin, Elnaz Safi, Mohammad Reza\n  Mohammadi", "title": "Wise-SrNet: A Novel Architecture for Enhancing Image Classification by\n  Learning Spatial Resolution of Feature Maps", "comments": "The code is shared at\n  https://github.com/mr7495/image-classification-spatial", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  One of the main challenges since the advancement of convolutional neural\nnetworks is how to connect the extracted feature map to the final\nclassification layer. VGG models used two sets of fully connected layers for\nthe classification part of their architectures, which significantly increases\nthe number of models' weights. ResNet and next deep convolutional models used\nthe Global Average Pooling (GAP) layer to compress the feature map and feed it\nto the classification layer. Although using the GAP layer reduces the\ncomputational cost, but also causes losing spatial resolution of the feature\nmap, which results in decreasing learning efficiency. In this paper, we aim to\ntackle this problem by replacing the GAP layer with a new architecture called\nWise-SrNet. It is inspired by the depthwise convolutional idea and is designed\nfor processing spatial resolution and also not increasing computational cost.\nWe have evaluated our method using three different datasets: Intel Image\nClassification Challenge, MIT Indoors Scenes, and a part of the ImageNet\ndataset. We investigated the implementation of our architecture on several\nmodels of Inception, ResNet and DensNet families. Applying our architecture has\nrevealed a significant effect on increasing convergence speed and accuracy. Our\nExperiments on images with 224x224 resolution increased the Top-1 accuracy\nbetween 2% to 8% on different datasets and models. Running our models on\n512x512 resolution images of the MIT Indoors Scenes dataset showed a notable\nresult of improving the Top-1 accuracy within 3% to 26%. We will also\ndemonstrate the GAP layer's disadvantage when the input images are large and\nthe number of classes is not few. In this circumstance, our proposed\narchitecture can do a great help in enhancing classification results. The code\nis shared at https://github.com/mr7495/image-classification-spatial.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 00:37:11 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Rahimzadeh", "Mohammad", ""], ["Parvin", "Soroush", ""], ["Safi", "Elnaz", ""], ["Mohammadi", "Mohammad Reza", ""]]}, {"id": "2104.12300", "submitter": "Ricky Ma", "authors": "Ricky Ma (The University of British Columbia)", "title": "ODDObjects: A Framework for Multiclass Unsupervised Anomaly Detection on\n  Masked Objects", "comments": "11 pages, 15 Postscript figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper presents a novel framework for unsupervised anomaly detection on\nmasked objects called ODDObjects, which stands for Out-of-Distribution\nDetection on Objects. ODDObjects is designed to detect anomalies of various\ncategories using unsupervised autoencoders trained on COCO-style datasets. The\nmethod utilizes autoencoder-based image reconstruction, where high\nreconstruction error indicates the possibility of an anomaly. The framework\nextends previous work on anomaly detection with autoencoders, comparing\nstate-of-the-art models trained on object recognition datasets. Various model\narchitectures were compared, and experimental results show that\nmemory-augmented deep convolutional autoencoders perform the best at detecting\nout-of-distribution objects.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 01:13:28 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Ma", "Ricky", "", "The University of British Columbia"]]}, {"id": "2104.12311", "submitter": "Zexuan Yin", "authors": "Zexuan Yin, Paolo Barucca", "title": "Stochastic Recurrent Neural Network for Multistep Time Series\n  Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Time series forecasting based on deep architectures has been gaining\npopularity in recent years due to their ability to model complex non-linear\ntemporal dynamics. The recurrent neural network is one such model capable of\nhandling variable-length input and output. In this paper, we leverage recent\nadvances in deep generative models and the concept of state space models to\npropose a stochastic adaptation of the recurrent neural network for\nmultistep-ahead time series forecasting, which is trained with stochastic\ngradient variational Bayes. In our model design, the transition function of the\nrecurrent neural network, which determines the evolution of the hidden states,\nis stochastic rather than deterministic as in a regular recurrent neural\nnetwork; this is achieved by incorporating a latent random variable into the\ntransition process which captures the stochasticity of the temporal dynamics.\nOur model preserves the architectural workings of a recurrent neural network\nfor which all relevant information is encapsulated in its hidden states, and\nthis flexibility allows our model to be easily integrated into any deep\narchitecture for sequential modelling. We test our model on a wide range of\ndatasets from finance to healthcare; results show that the stochastic recurrent\nneural network consistently outperforms its deterministic counterpart.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 01:43:43 GMT"}, {"version": "v2", "created": "Sun, 2 May 2021 00:59:40 GMT"}, {"version": "v3", "created": "Fri, 25 Jun 2021 03:00:47 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Yin", "Zexuan", ""], ["Barucca", "Paolo", ""]]}, {"id": "2104.12333", "submitter": "Tao Ni", "authors": "Tao Ni, Qing Wang, Gabriela Ferraro", "title": "Explore BiLSTM-CRF-Based Models for Open Relation Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting multiple relations from text sentences is still a challenge for\ncurrent Open Relation Extraction (Open RE) tasks. In this paper, we develop\nseveral Open RE models based on the bidirectional LSTM-CRF (BiLSTM-CRF) neural\nnetwork and different contextualized word embedding methods. We also propose a\nnew tagging scheme to solve overlapping problems and enhance models'\nperformance. From the evaluation results and comparisons between models, we\nselect the best combination of tagging scheme, word embedder, and BiLSTM-CRF\nnetwork to achieve an Open RE model with a remarkable extracting ability on\nmultiple-relation sentences.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 03:37:22 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Ni", "Tao", ""], ["Wang", "Qing", ""], ["Ferraro", "Gabriela", ""]]}, {"id": "2104.12365", "submitter": "Zeshi Yang", "authors": "Zeshi Yang and Zhiqi Yin", "title": "Efficient Hyperparameter Optimization for Physics-based Character\n  Animation", "comments": "published in ACM SIGGRAPH Symposium on Interactive 3D Graphics and\n  Games 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.AI cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Physics-based character animation has seen significant advances in recent\nyears with the adoption of Deep Reinforcement Learning (DRL). However,\nDRL-based learning methods are usually computationally expensive and their\nperformance crucially depends on the choice of hyperparameters. Tuning\nhyperparameters for these methods often requires repetitive training of control\npolicies, which is even more computationally prohibitive. In this work, we\npropose a novel Curriculum-based Multi-Fidelity Bayesian Optimization framework\n(CMFBO) for efficient hyperparameter optimization of DRL-based character\ncontrol systems. Using curriculum-based task difficulty as fidelity criterion,\nour method improves searching efficiency by gradually pruning search space\nthrough evaluation on easier motor skill tasks. We evaluate our method on two\nphysics-based character control tasks: character morphology optimization and\nhyperparameter tuning of DeepMimic. Our algorithm significantly outperforms\nstate-of-the-art hyperparameter optimization methods applicable for\nphysics-based character animation. In particular, we show that hyperparameters\noptimized through our algorithm result in at least 5x efficiency gain comparing\nto author-released settings in DeepMimic.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 06:46:36 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Yang", "Zeshi", ""], ["Yin", "Zhiqi", ""]]}, {"id": "2104.12379", "submitter": "Luca Erculiani Mr", "authors": "Fausto Giunchiglia and Luca Erculiani and Andrea Passerini", "title": "Towards Visual Semantics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Visual Semantics we study how humans build mental representations, i.e.,\nconcepts , of what they visually perceive. We call such concepts, substance\nconcepts. In this paper we provide a theory and an algorithm which learns\nsubstance concepts which correspond to the concepts, that we call\nclassification concepts , that in Lexical Semantics are used to encode word\nmeanings. The theory and algorithm are based on three main contributions: (i)\nsubstance concepts are modeled as visual objects , namely sequences of similar\nframes, as perceived in multiple encounters ; (ii) substance concepts are\norganized into a visual subsumption hierarchy based on the notions of Genus and\nDifferentia that resemble the notions that, in Lexical Semantics, allow to\nconstruct hierarchies of classification concepts; (iii) the human feedback is\nexploited not to name objects, as it has been the case so far, but, rather, to\nalign the hierarchy of substance concepts with that of classification concepts.\nThe learning algorithm is implemented for the base case of a hierarchy of depth\ntwo. The experiments, though preliminary, show that the algorithm manages to\nacquire the notions of Genus and Differentia with reasonable accuracy, this\ndespite seeing a small number of examples and receiving supervision on a\nfraction of them.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 07:28:02 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Giunchiglia", "Fausto", ""], ["Erculiani", "Luca", ""], ["Passerini", "Andrea", ""]]}, {"id": "2104.12386", "submitter": "Ryuta Arisaka", "authors": "Ryuta Arisaka, Takayuki Ito", "title": "Relational Argumentation Semantics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose a fresh perspective on argumentation semantics, to\nview them as a relational database. It offers encapsulation of the underlying\nargumentation graph, and allows us to understand argumentation semantics under\na single, relational perspective, leading to the concept of relational\nargumentation semantics. This is a direction to understand argumentation\nsemantics through a common formal language. We show that many existing\nsemantics such as explanation semantics, multi-agent semantics, and more\ntypical semantics, that have been proposed for specific purposes, are\nunderstood in the relational perspective.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 07:58:17 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Arisaka", "Ryuta", ""], ["Ito", "Takayuki", ""]]}, {"id": "2104.12417", "submitter": "Augusto Luis Ballardini PhD", "authors": "Augusto Luis Ballardini and \\'Alvaro Hern\\'andez and Miguel \\'Angel\n  Sotelo", "title": "Model Guided Road Intersection Classification", "comments": "To be presented at the 2021 32nd IEEE Intelligent Vehicles Symposium\n  (IV) (IV 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Understanding complex scenarios from in-vehicle cameras is essential for\nsafely operating autonomous driving systems in densely populated areas. Among\nthese, intersection areas are one of the most critical as they concentrate a\nconsiderable number of traffic accidents and fatalities. Detecting and\nunderstanding the scene configuration of these usually crowded areas is then of\nextreme importance for both autonomous vehicles and modern ADAS aimed at\npreventing road crashes and increasing the safety of vulnerable road users.\nThis work investigates inter-section classification from RGB images using\nwell-consolidate neural network approaches along with a method to enhance the\nresults based on the teacher/student training paradigm. An extensive\nexperimental activity aimed at identifying the best input configuration and\nevaluating different network parameters on both the well-known KITTI dataset\nand the new KITTI-360 sequences shows that our method outperforms current\nstate-of-the-art approaches on a per-frame basis and prove the effectiveness of\nthe proposed learning scheme.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 09:15:28 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Ballardini", "Augusto Luis", ""], ["Hern\u00e1ndez", "\u00c1lvaro", ""], ["Sotelo", "Miguel \u00c1ngel", ""]]}, {"id": "2104.12418", "submitter": "Swarup Mohalik", "authors": "Moumita Das, Rajarshi Ray, Swarup Kumar Mohalik, Ansuman Banerjee", "title": "Fast Falsification of Neural Networks using Property Directed Testing", "comments": "10 pages, 3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are now extensively used in perception, prediction and\ncontrol of autonomous systems. Their deployment in safety-critical systems\nbrings forth the need for verification techniques for such networks. As an\nalternative to exhaustive and costly verification algorithms, lightweight\nfalsification algorithms have been heavily used to search for an input to the\nsystem that produces an unsafe output, i.e., a counterexample to the safety of\nthe system. In this work, we propose a falsification algorithm for neural\nnetworks that directs the search for a counterexample, guided by a safety\nproperty specification. Our algorithm uses a derivative-free sampling-based\noptimization method. We evaluate our algorithm on 45 trained neural network\nbenchmarks of the ACAS Xu system against 10 safety properties. We show that our\nfalsification procedure detects all the unsafe instances that other\nverification tools also report as unsafe. Moreover, in terms of performance,\nour falsification procedure identifies most of the unsafe instances faster, in\ncomparison to the state-of-the-art verification tools for feed-forward neural\nnetworks such as NNENUM and Neurify and in many instances, by orders of\nmagnitude.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 09:16:27 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Das", "Moumita", ""], ["Ray", "Rajarshi", ""], ["Mohalik", "Swarup Kumar", ""], ["Banerjee", "Ansuman", ""]]}, {"id": "2104.12419", "submitter": "Quentin Paletta", "authors": "Quentin Paletta, Anthony Hu, Guillaume Arbod, Joan Lasenby", "title": "ECLIPSE : Envisioning Cloud Induced Perturbations in Solar Energy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient integration of solar energy into the electricity mix depends on a\nreliable anticipation of its intermittency. A promising approach to forecast\nthe temporal variability of solar irradiance resulting from the cloud cover\ndynamics, is based on the analysis of sequences of ground-taken sky images.\nDespite encouraging results, a recurrent limitation of current Deep Learning\napproaches lies in the ubiquitous tendency of reacting to past observations\nrather than actively anticipating future events. This leads to a systematic\ntemporal lag and little ability to predict sudden events. To address this\nchallenge, we introduce ECLIPSE, a spatio-temporal neural network architecture\nthat models cloud motion from sky images to predict both future segmented\nimages and corresponding irradiance levels. We show that ECLIPSE anticipates\ncritical events and considerably reduces temporal delay while generating\nvisually realistic futures.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 09:19:43 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Paletta", "Quentin", ""], ["Hu", "Anthony", ""], ["Arbod", "Guillaume", ""], ["Lasenby", "Joan", ""]]}, {"id": "2104.12459", "submitter": "Pedro Saleiro", "authors": "Catarina Bel\\'em, Vladimir Balayan, Pedro Saleiro, Pedro Bizarro", "title": "Weakly Supervised Multi-task Learning for Concept-based Explainability", "comments": "Accepted at ICLR 2021 Workshop on Weakly Supervised Learning (WeaSuL)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In ML-aided decision-making tasks, such as fraud detection or medical\ndiagnosis, the human-in-the-loop, usually a domain-expert without technical ML\nknowledge, prefers high-level concept-based explanations instead of low-level\nexplanations based on model features. To obtain faithful concept-based\nexplanations, we leverage multi-task learning to train a neural network that\njointly learns to predict a decision task based on the predictions of a\nprecedent explainability task (i.e., multi-label concepts). There are two main\nchallenges to overcome: concept label scarcity and the joint learning. To\naddress both, we propose to: i) use expert rules to generate a large dataset of\nnoisy concept labels, and ii) apply two distinct multi-task learning strategies\ncombining noisy and golden labels. We compare these strategies with a fully\nsupervised approach in a real-world fraud detection application with few golden\nlabels available for the explainability task. With improvements of 9.26% and of\n417.8% at the explainability and decision tasks, respectively, our results show\nit is possible to improve performance at both tasks by combining labels of\nheterogeneous quality.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 10:42:19 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Bel\u00e9m", "Catarina", ""], ["Balayan", "Vladimir", ""], ["Saleiro", "Pedro", ""], ["Bizarro", "Pedro", ""]]}, {"id": "2104.12465", "submitter": "Jia-Hong Huang", "authors": "Jia-Hong Huang, Luka Murn, Marta Mrak, Marcel Worring", "title": "GPT2MVS: Generative Pre-trained Transformer-2 for Multi-modal Video\n  Summarization", "comments": "This paper is accepted by ACM International Conference on Multimedia\n  Retrieval (ICMR), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Traditional video summarization methods generate fixed video representations\nregardless of user interest. Therefore such methods limit users' expectations\nin content search and exploration scenarios. Multi-modal video summarization is\none of the methods utilized to address this problem. When multi-modal video\nsummarization is used to help video exploration, a text-based query is\nconsidered as one of the main drivers of video summary generation, as it is\nuser-defined. Thus, encoding the text-based query and the video effectively are\nboth important for the task of multi-modal video summarization. In this work, a\nnew method is proposed that uses a specialized attention network and\ncontextualized word representations to tackle this task. The proposed model\nconsists of a contextualized video summary controller, multi-modal attention\nmechanisms, an interactive attention network, and a video summary generator.\nBased on the evaluation of the existing multi-modal video summarization\nbenchmark, experimental results show that the proposed model is effective with\nthe increase of +5.88% in accuracy and +4.06% increase of F1-score, compared\nwith the state-of-the-art method.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 10:50:37 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Huang", "Jia-Hong", ""], ["Murn", "Luka", ""], ["Mrak", "Marta", ""], ["Worring", "Marcel", ""]]}, {"id": "2104.12469", "submitter": "Konstantin Klemmer", "authors": "Konstantin Klemmer, Sudipan Saha, Matthias Kahl, Tianlin Xu, Xiao\n  Xiang Zhu", "title": "Generative modeling of spatio-temporal weather patterns with extreme\n  event conditioning", "comments": "ICLR'21 Workshop AI: Modeling Oceans and Climate Change (AIMOCC)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG physics.ao-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep generative models are increasingly used to gain insights in the\ngeospatial data domain, e.g., for climate data. However, most existing\napproaches work with temporal snapshots or assume 1D time-series; few are able\nto capture spatio-temporal processes simultaneously. Beyond this, Earth-systems\ndata often exhibit highly irregular and complex patterns, for example caused by\nextreme weather events. Because of climate change, these phenomena are only\nincreasing in frequency. Here, we proposed a novel GAN-based approach for\ngenerating spatio-temporal weather patterns conditioned on detected extreme\nevents. Our approach augments GAN generator and discriminator with an encoded\nextreme weather event segmentation mask. These segmentation masks can be\ncreated from raw input using existing event detection frameworks. As such, our\napproach is highly modular and can be combined with custom GAN architectures.\nWe highlight the applicability of our proposed approach in experiments with\nreal-world surface radiation and zonal wind data.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 10:58:44 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Klemmer", "Konstantin", ""], ["Saha", "Sudipan", ""], ["Kahl", "Matthias", ""], ["Xu", "Tianlin", ""], ["Zhu", "Xiao Xiang", ""]]}, {"id": "2104.12471", "submitter": "Jia-Hong Huang", "authors": "Jia-Hong Huang, Ting-Wei Wu, Marcel Worring", "title": "Contextualized Keyword Representations for Multi-modal Retinal Image\n  Captioning", "comments": "This paper is accepted by ACM International Conference on Multimedia\n  Retrieval (ICMR), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.IR cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Medical image captioning automatically generates a medical description to\ndescribe the content of a given medical image. A traditional medical image\ncaptioning model creates a medical description only based on a single medical\nimage input. Hence, an abstract medical description or concept is hard to be\ngenerated based on the traditional approach. Such a method limits the\neffectiveness of medical image captioning. Multi-modal medical image captioning\nis one of the approaches utilized to address this problem. In multi-modal\nmedical image captioning, textual input, e.g., expert-defined keywords, is\nconsidered as one of the main drivers of medical description generation. Thus,\nencoding the textual input and the medical image effectively are both important\nfor the task of multi-modal medical image captioning. In this work, a new\nend-to-end deep multi-modal medical image captioning model is proposed.\nContextualized keyword representations, textual feature reinforcement, and\nmasked self-attention are used to develop the proposed approach. Based on the\nevaluation of the existing multi-modal medical image captioning dataset,\nexperimental results show that the proposed model is effective with the\nincrease of +53.2% in BLEU-avg and +18.6% in CIDEr, compared with the\nstate-of-the-art method.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 11:08:13 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Huang", "Jia-Hong", ""], ["Wu", "Ting-Wei", ""], ["Worring", "Marcel", ""]]}, {"id": "2104.12483", "submitter": "Yinjiang Cai", "authors": "Yinjiang Cai, Zeyu Cui, Shu Wu, Zhen Lei, Xibo Ma", "title": "Represent Items by Items: An Enhanced Representation of the Target Item\n  for Recommendation", "comments": "10 pages,7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Item-based collaborative filtering (ICF) has been widely used in industrial\napplications such as recommender system and online advertising. It models\nusers' preference on target items by the items they have interacted with.\nRecent models use methods such as attention mechanism and deep neural network\nto learn the user representation and scoring function more accurately. However,\ndespite their effectiveness, such models still overlook a problem that\nperformance of ICF methods heavily depends on the quality of item\nrepresentation especially the target item representation. In fact, due to the\nlong-tail distribution in the recommendation, most item embeddings can not\nrepresent the semantics of items accurately and thus degrade the performance of\ncurrent ICF methods. In this paper, we propose an enhanced representation of\nthe target item which distills relevant information from the co-occurrence\nitems. We design sampling strategies to sample fix number of co-occurrence\nitems for the sake of noise reduction and computational cost. Considering the\ndifferent importance of sampled items to the target item, we apply attention\nmechanism to selectively adopt the semantic information of the sampled items.\nOur proposed Co-occurrence based Enhanced Representation model (CER) learns the\nscoring function by a deep neural network with the attentive user\nrepresentation and fusion of raw representation and enhanced representation of\ntarget item as input. With the enhanced representation, CER has stronger\nrepresentation power for the tail items compared to the state-of-the-art ICF\nmethods. Extensive experiments on two public benchmarks demonstrate the\neffectiveness of CER.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 11:28:28 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Cai", "Yinjiang", ""], ["Cui", "Zeyu", ""], ["Wu", "Shu", ""], ["Lei", "Zhen", ""], ["Ma", "Xibo", ""]]}, {"id": "2104.12507", "submitter": "Jiaoyang Yin", "authors": "Jiaoyang Yin, Yiling Xu, Hao Chen, Yunfei Zhang, Steve Appleby, Zhan\n  Ma", "title": "ANT: Learning Accurate Network Throughput for Better Adaptive Video\n  Streaming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive Bit Rate (ABR) decision plays a crucial role for ensuring\nsatisfactory Quality of Experience (QoE) in video streaming applications, in\nwhich past network statistics are mainly leveraged for future network bandwidth\nprediction. However, most algorithms, either rules-based or learning-driven\napproaches, feed throughput traces or classified traces based on traditional\nstatistics (i.e., mean/standard deviation) to drive ABR decision, leading to\ncompromised performances in specific scenarios. Given the diverse network\nconnections (e.g., WiFi, cellular and wired link) from time to time, this paper\nthus proposes to learn the ANT (a.k.a., Accurate Network Throughput) model to\ncharacterize the full spectrum of network throughput dynamics in the past for\nderiving the proper network condition associated with a specific cluster of\nnetwork throughput segments (NTS). Each cluster of NTS is then used to generate\na dedicated ABR model, by which we wish to better capture the network dynamics\nfor diverse connections. We have integrated the ANT model with existing\nreinforcement learning (RL)-based ABR decision engine, where different ABR\nmodels are applied to respond to the accurate network sensing for better rate\ndecision. Extensive experiment results show that our approach can significantly\nimprove the user QoE by 65.5% and 31.3% respectively, compared with the\nstate-of-the-art Pensive and Oboe, across a wide range of network scenarios.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 12:15:53 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 12:28:27 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Yin", "Jiaoyang", ""], ["Xu", "Yiling", ""], ["Chen", "Hao", ""], ["Zhang", "Yunfei", ""], ["Appleby", "Steve", ""], ["Ma", "Zhan", ""]]}, {"id": "2104.12546", "submitter": "Andrea Loreggia", "authors": "Andrea Loreggia, Anna Passarelli", "title": "The Effects of Air Quality on the Spread of the COVID-19. An Artificial\n  Intelligence Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The COVID-19 pandemic considerably affects public health systems around the\nworld. The lack of knowledge about the virus, the extension of this phenomenon,\nand the speed of the evolution of the infection are all factors that highlight\nthe necessity of employing new approaches to study these events. Artificial\nintelligence techniques may be useful in analyzing data related to areas\naffected by the virus. The aim of this work is to investigate any possible\nrelationships between air quality and confirmed cases of COVID-19 in Italian\ndistricts. Specifically, we report an analysis of the correlation between daily\nCOVID-19 cases and environmental factors, such as temperature, relative\nhumidity, and atmospheric pollutants. Our analysis confirms a significant\nassociation of some environmental parameters with the spread of the virus. This\nsuggests that machine learning models trained on the environmental parameters\nto predict the number of future infected cases may be accurate. Predictive\nmodels may be useful for helping institutions in making decisions for\nprotecting the population and contrasting the pandemic.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 19:08:59 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Loreggia", "Andrea", ""], ["Passarelli", "Anna", ""]]}, {"id": "2104.12547", "submitter": "Lambert Hogenhout", "authors": "Lambert Hogenhout", "title": "A Framework for Ethical AI at the United Nations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper aims to provide an overview of the ethical concerns in artificial\nintelligence (AI) and the framework that is needed to mitigate those risks, and\nto suggest a practical path to ensure the development and use of AI at the\nUnited Nations (UN) aligns with our ethical values. The overview discusses how\nAI is an increasingly powerful tool with potential for good, albeit one with a\nhigh risk of negative side-effects that go against fundamental human rights and\nUN values. It explains the need for ethical principles for AI aligned with\nprinciples for data governance, as data and AI are tightly interwoven. It\nexplores different ethical frameworks that exist and tools such as assessment\nlists. It recommends that the UN develop a framework consisting of ethical\nprinciples, architectural standards, assessment methods, tools and\nmethodologies, and a policy to govern the implementation and adherence to this\nframework, accompanied by an education program for staff.\n", "versions": [{"version": "v1", "created": "Fri, 9 Apr 2021 23:44:37 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Hogenhout", "Lambert", ""]]}, {"id": "2104.12561", "submitter": "Kashif Ahmad", "authors": "Yasir Saleem Afridi, Kashif Ahmad, Laiq Hassan", "title": "Artificial Intelligence Based Prognostic Maintenance of Renewable Energy\n  Systems: A Review of Techniques, Challenges, and Future Research Directions", "comments": "20 pages, 4 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Since the depletion of fossil fuels, the world has started to rely heavily on\nrenewable sources of energy. With every passing year, our dependency on the\nrenewable sources of energy is increasing exponentially. As a result, complex\nand hybrid generation systems are being designed and developed to meet the\nenergy demands and ensure energy security in a country. The continual\nimprovement in the technology and an effort towards the provision of\nuninterrupted power to the end-users is strongly dependent on an effective and\nfault resilient Operation and Maintenance (O&M) system. Ingenious algorithms\nand techniques are hence been introduced aiming to minimize equipment and plant\ndowntime. Efforts are being made to develop robust Prognostic Maintenance\nsystems that can identify the faults before they occur. To this aim, complex\nData Analytics and Machine Learning (ML) techniques are being used to increase\nthe overall efficiency of these prognostic maintenance systems.\n  This paper provides an overview of the predictive/prognostic maintenance\nframeworks reported in the literature. We pay a particular focus to the\napproaches, challenges including data-related issues, such as the availability\nand quality of the data and data auditing, feature engineering,\ninterpretability, and security issues. Being a key aspect of ML-based\nsolutions, we also discuss some of the commonly used publicly available\ndatasets in the domain. The paper also identifies key future research\ndirections. We believe such detailed analysis will provide a baseline for\nfuture research in the domain.\n", "versions": [{"version": "v1", "created": "Tue, 20 Apr 2021 11:41:00 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Afridi", "Yasir Saleem", ""], ["Ahmad", "Kashif", ""], ["Hassan", "Laiq", ""]]}, {"id": "2104.12582", "submitter": "Robert Williams", "authors": "Robert M. Williams, Roman V. Yampolskiy", "title": "Understanding and Avoiding AI Failures: A Practical Guide", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As AI technologies increase in capability and ubiquity, AI accidents are\nbecoming more common. Based on normal accident theory, high reliability theory,\nand open systems theory, we create a framework for understanding the risks\nassociated with AI applications. In addition, we also use AI safety principles\nto quantify the unique risks of increased intelligence and human-like qualities\nin AI. Together, these two fields give a more complete picture of the risks of\ncontemporary AI. By focusing on system properties near accidents instead of\nseeking a root cause of accidents, we identify where attention should be paid\nto safety for current generation AI systems.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 17:05:27 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 12:31:55 GMT"}, {"version": "v3", "created": "Wed, 28 Apr 2021 21:33:11 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Williams", "Robert M.", ""], ["Yampolskiy", "Roman V.", ""]]}, {"id": "2104.12589", "submitter": "Jurian Baas", "authors": "Jurian Baas, Mehdi Dastani, Ad Feelders", "title": "Exploiting Transitivity Constraints for Entity Matching in Knowledge\n  Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DL cs.SI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The goal of entity matching in knowledge graphs is to identify entities that\nrefer to the same real-world objects using some similarity metric. The result\nof entity matching can be seen as a set of entity pairs interpreted as the\nsame-as relation. However, the identified set of pairs may fail to satisfy some\nstructural properties, in particular transitivity, that are expected from the\nsame-as relation. In this work, we show that an ad-hoc enforcement of\ntransitivity, i.e. taking the transitive closure, on the identified set of\nentity pairs may decrease precision dramatically. We therefore propose a\nmethodology that starts with a given similarity measure, generates a set of\nentity pairs that are identified as referring to the same real-world objects,\nand applies the cluster editing algorithm to enforce transitivity without\nadding many spurious links, leading to overall improved performance.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 10:57:01 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Baas", "Jurian", ""], ["Dastani", "Mehdi", ""], ["Feelders", "Ad", ""]]}, {"id": "2104.12592", "submitter": "Chenguang Lu", "authors": "Chenguang Lu", "title": "Understanding and Accelerating EM Algorithm's Convergence by Fair\n  Competition Principle and Rate-Verisimilitude Function", "comments": "31 pages,12 figures. arXiv admin note: text overlap with\n  arXiv:2007.12845", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Why can the Expectation-Maximization (EM) algorithm for mixture models\nconverge? Why can different initial parameters cause various convergence\ndifficulties? The Q-L synchronization theory explains that the observed data\nlog-likelihood L and the complete data log-likelihood Q are positively\ncorrelated; we can achieve maximum L by maximizing Q. According to this theory,\nthe Deterministic Annealing EM (DAEM) algorithm's authors make great efforts to\neliminate locally maximal Q for avoiding L's local convergence. However, this\npaper proves that in some cases, Q may and should decrease for L to increase;\nslow or local convergence exists only because of small samples and unfair\ncompetition. This paper uses marriage competition to explain different\nconvergence difficulties and proposes the Fair Competition Principle (FCP) with\nan initialization map for improving initializations. It uses the\nrate-verisimilitude function, extended from the rate-distortion function, to\nexplain the convergence of the EM and improved EM algorithms. This convergence\nproof adopts variational and iterative methods that Shannon et al. used for\nanalyzing rate-distortion functions. The initialization map can vastly save\nboth algorithms' running times for binary Gaussian mixtures. The FCP and the\ninitialization map are useful for complicated mixtures but not sufficient; we\nneed further studies for specific methods.\n", "versions": [{"version": "v1", "created": "Wed, 21 Apr 2021 20:27:25 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Lu", "Chenguang", ""]]}, {"id": "2104.12620", "submitter": "Sasanka Sekhar Chanda", "authors": "Sasanka Sekhar Chanda", "title": "An Algorithm to Effect Prompt Termination of Myopic Local Search on\n  Kauffman-s NK Landscape", "comments": "13 Pages, 10 Figures, 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI physics.atm-clus q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Kauffman-s NK model, myopic local search involves flipping one\nrandomly-chosen bit of an N-bit decision string in every time step and\naccepting the new configuration if that has higher fitness. One issue is that,\nthis algorithm consumes the full extent of computational resources allocated -\ngiven by the number of alternative configurations inspected - even though\nsearch is expected to terminate the moment there are no neighbors having higher\nfitness. Otherwise, the algorithm must compute the fitness of all N neighbors\nin every time step, consuming a high amount of resources. In order to get\naround this problem, I describe an algorithm that allows search to logically\nterminate relatively early, without having to evaluate fitness of all N\nneighbors at every time step. I further suggest that when the efficacy of two\nalgorithms need to be compared head to head, imposing a common limit on the\nnumber of alternatives evaluated - metering - provides the necessary level\nfield.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 14:42:41 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 16:37:46 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Chanda", "Sasanka Sekhar", ""]]}, {"id": "2104.12643", "submitter": "Jialin Yu", "authors": "Jialin Yu, Laila Alrajhi, Anoushka Harit, Zhongtian Sun, Alexandra I.\n  Cristea, Lei Shi", "title": "Exploring Bayesian Deep Learning for Urgent Instructor Intervention Need\n  in MOOC Forums", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-80421-3_10", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Massive Open Online Courses (MOOCs) have become a popular choice for\ne-learning thanks to their great flexibility. However, due to large numbers of\nlearners and their diverse backgrounds, it is taxing to offer real-time\nsupport. Learners may post their feelings of confusion and struggle in the\nrespective MOOC forums, but with the large volume of posts and high workloads\nfor MOOC instructors, it is unlikely that the instructors can identify all\nlearners requiring intervention. This problem has been studied as a Natural\nLanguage Processing (NLP) problem recently, and is known to be challenging, due\nto the imbalance of the data and the complex nature of the task. In this paper,\nwe explore for the first time Bayesian deep learning on learner-based text\nposts with two methods: Monte Carlo Dropout and Variational Inference, as a new\nsolution to assessing the need of instructor interventions for a learner's\npost. We compare models based on our proposed methods with probabilistic\nmodelling to its baseline non-Bayesian models under similar circumstances, for\ndifferent cases of applying prediction. The results suggest that Bayesian deep\nlearning offers a critical uncertainty measure that is not supplied by\ntraditional neural networks. This adds more explainability, trust and\nrobustness to AI, which is crucial in education-based applications.\nAdditionally, it can achieve similar or better performance compared to\nnon-probabilistic neural networks, as well as grant lower variance.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 15:12:13 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Yu", "Jialin", ""], ["Alrajhi", "Laila", ""], ["Harit", "Anoushka", ""], ["Sun", "Zhongtian", ""], ["Cristea", "Alexandra I.", ""], ["Shi", "Lei", ""]]}, {"id": "2104.12717", "submitter": "Tommaso Teofili", "authors": "Rob Geada, Tommaso Teofili, Rui Vieira, Rebecca Whitworth, Daniele\n  Zonca", "title": "TrustyAI Explainability Toolkit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial intelligence (AI) is becoming increasingly more popular and can be\nfound in workplaces and homes around the world. However, how do we ensure trust\nin these systems? Regulation changes such as the GDPR mean that users have a\nright to understand how their data has been processed as well as saved.\nTherefore if, for example, you are denied a loan you have the right to ask why.\nThis can be hard if the method for working this out uses \"black box\" machine\nlearning techniques such as neural networks. TrustyAI is a new initiative which\nlooks into explainable artificial intelligence (XAI) solutions to address\ntrustworthiness in ML as well as decision services landscapes.\n  In this paper we will look at how TrustyAI can support trust in decision\nservices and predictive models. We investigate techniques such as LIME, SHAP\nand counterfactuals, benchmarking both LIME and counterfactual techniques\nagainst existing implementations. We also look into an extended version of\nSHAP, which supports background data selection to be evaluated based on\nquantitative data and allows for error bounds.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 17:00:32 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Geada", "Rob", ""], ["Teofili", "Tommaso", ""], ["Vieira", "Rui", ""], ["Whitworth", "Rebecca", ""], ["Zonca", "Daniele", ""]]}, {"id": "2104.12759", "submitter": "Pranoy Panda", "authors": "Pranoy Panda, Sai Srinivas Kancheti, Vineeth N Balasubramanian", "title": "Instance-wise Causal Feature Selection for Model Interpretation", "comments": "6 pages, 5 figures. Accepted at the Causality in Vision workshop,\n  CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We formulate a causal extension to the recently introduced paradigm of\ninstance-wise feature selection to explain black-box visual classifiers. Our\nmethod selects a subset of input features that has the greatest causal effect\non the models output. We quantify the causal influence of a subset of features\nby the Relative Entropy Distance measure. Under certain assumptions this is\nequivalent to the conditional mutual information between the selected subset\nand the output variable. The resulting causal selections are sparser and cover\nsalient objects in the scene. We show the efficacy of our approach on multiple\nvision datasets by measuring the post-hoc accuracy and Average Causal Effect of\nselected features on the models output.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 17:51:42 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Panda", "Pranoy", ""], ["Kancheti", "Sai Srinivas", ""], ["Balasubramanian", "Vineeth N", ""]]}, {"id": "2104.12762", "submitter": "Omar Abdel Wahab", "authors": "Ahmed Saleh Bataineh, Jamal Bentahar, Rabeb Mizouni, Omar Abdel Wahab,\n  Gaith Rjoub, May El Barachi", "title": "Cloud computing as a platform for monetizing data services: A two-sided\n  game business model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the unprecedented reliance on cloud computing as the backbone for\nstoring today's big data, we argue in this paper that the role of the cloud\nshould be reshaped from being a passive virtual market to become an active\nplatform for monetizing the big data through Artificial Intelligence (AI)\nservices. The objective is to enable the cloud to be an active platform that\ncan help big data service providers reach a wider set of customers and cloud\nusers (i.e., data consumers) to be exposed to a larger and richer variety of\ndata to run their data analytic tasks. To achieve this vision, we propose a\nnovel game theoretical model, which consists of a mix of cooperative and\ncompetitive strategies. The players of the game are the big data service\nproviders, cloud computing platform, and cloud users. The strategies of the\nplayers are modeled using the two-sided market theory that takes into\nconsideration the network effects among involved parties, while integrating the\nexternalities between the cloud resources and consumer demands into the design\nof the game. Simulations conducted using Amazon and google clustered data show\nthat the proposed model improves the total surplus of all the involved parties\nin terms of cloud resources provision and monetary profits compared to the\ncurrent merchant model.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 17:54:31 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Bataineh", "Ahmed Saleh", ""], ["Bentahar", "Jamal", ""], ["Mizouni", "Rabeb", ""], ["Wahab", "Omar Abdel", ""], ["Rjoub", "Gaith", ""], ["Barachi", "May El", ""]]}, {"id": "2104.12828", "submitter": "Syed Eqbal Alam", "authors": "Syed Eqbal Alam and Fabian Wirth and Jia Yuan Yu", "title": "Multi-resource allocation for federated settings: A non-homogeneous\n  Markov chain model", "comments": "The paper was published without the co-authors' notice, and it is\n  withdrawn due to their objection and due to authorship conflicts", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.AI cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In a federated setting, agents coordinate with a central agent or a server to\nsolve an optimization problem in which agents do not share their information\nwith each other. Wirth and his co-authors, in a recent paper, describe how the\nbasic additive-increase multiplicative-decrease (AIMD) algorithm can be\nmodified in a straightforward manner to solve a class of optimization problems\nfor federated settings for a single shared resource with no inter-agent\ncommunication. The AIMD algorithm is one of the most successful distributed\nresource allocation algorithms currently deployed in practice. It is best known\nas the backbone of the Internet and is also widely explored in other\napplication areas. We extend the single-resource algorithm to multiple\nheterogeneous shared resources that emerge in smart cities, sharing economy,\nand many other applications. Our main results show the convergence of the\naverage allocations to the optimal values. We model the system as a\nnon-homogeneous Markov chain with place-dependent probabilities. Furthermore,\nsimulation results are presented to demonstrate the efficacy of the algorithms\nand to highlight the main features of our analysis.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 19:10:00 GMT"}, {"version": "v2", "created": "Mon, 24 May 2021 14:44:00 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Alam", "Syed Eqbal", ""], ["Wirth", "Fabian", ""], ["Yu", "Jia Yuan", ""]]}, {"id": "2104.12835", "submitter": "Daniel Glasner", "authors": "Srikumar Ramalingam, Daniel Glasner, Kaushal Patel, Raviteja\n  Vemulapalli, Sadeep Jayasumana, Sanjiv Kumar", "title": "Balancing Constraints and Submodularity in Data Subset Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning has yielded extraordinary results in vision and natural\nlanguage processing, but this achievement comes at a cost. Most deep learning\nmodels require enormous resources during training, both in terms of computation\nand in human labeling effort. In this paper, we show that one can achieve\nsimilar accuracy to traditional deep-learning models, while using less training\ndata. Much of the previous work in this area relies on using uncertainty or\nsome form of diversity to select subsets of a larger training set.\nSubmodularity, a discrete analogue of convexity, has been exploited to model\ndiversity in various settings including data subset selection. In contrast to\nprior methods, we propose a novel diversity driven objective function, and\nbalancing constraints on class labels and decision boundaries using matroids.\nThis allows us to use efficient greedy algorithms with approximation guarantees\nfor subset selection. We outperform baselines on standard image classification\ndatasets such as CIFAR-10, CIFAR-100, and ImageNet. In addition, we also show\nthat the proposed balancing constraints can play a key role in boosting the\nperformance in long-tailed datasets such as CIFAR-100-LT.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 19:22:27 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Ramalingam", "Srikumar", ""], ["Glasner", "Daniel", ""], ["Patel", "Kaushal", ""], ["Vemulapalli", "Raviteja", ""], ["Jayasumana", "Sadeep", ""], ["Kumar", "Sanjiv", ""]]}, {"id": "2104.12837", "submitter": "Joel G\\'ongora", "authors": "Trent J. Bradberry, Christopher H. Hase, LeAnna Kent, Joel A.\n  G\\'ongora", "title": "Unsupervised Instance Selection with Low-Label, Supervised Learning for\n  Outlier Detection", "comments": "16 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The laborious process of labeling data often bottlenecks projects that aim to\nleverage the power of supervised machine learning. Active Learning (AL) has\nbeen established as a technique to ameliorate this condition through an\niterative framework that queries a human annotator for labels of instances with\nthe most uncertain class assignment. Via this mechanism, AL produces a binary\nclassifier trained on less labeled data but with little, if any, loss in\npredictive performance. Despite its advantages, AL can have difficulty with\nclass-imbalanced datasets and results in an inefficient labeling process. To\naddress these drawbacks, we investigate our unsupervised instance selection\n(UNISEL) technique followed by a Random Forest (RF) classifier on 10 outlier\ndetection datasets under low-label conditions. These results are compared to AL\nperformed on the same datasets. Further, we investigate the combination of\nUNISEL and AL. Results indicate that UNISEL followed by an RF performs\ncomparably to AL with an RF and that the combination of UNISEL and AL\ndemonstrates superior performance. The practical implications of these findings\nin terms of time savings and generalizability afforded by UNISEL are discussed.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 19:23:58 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Bradberry", "Trent J.", ""], ["Hase", "Christopher H.", ""], ["Kent", "LeAnna", ""], ["G\u00f3ngora", "Joel A.", ""]]}, {"id": "2104.12842", "submitter": "Mohammadreza Sharif", "authors": "Mohammadreza Sharif, Deniz Erdogmus, Christopher Amato, and Taskin\n  Padir", "title": "End-to-end grasping policies for human-in-the-loop robots via deep\n  reinforcement learning", "comments": "ICRA 2021 Camera-ready version. Source code available at\n  https://github.com/sharif1093/dextron", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  State-of-the-art human-in-the-loop robot grasping is hugely suffered by\nElectromyography (EMG) inference robustness issues. As a workaround,\nresearchers have been looking into integrating EMG with other signals, often in\nan ad hoc manner. In this paper, we are presenting a method for end-to-end\ntraining of a policy for human-in-the-loop robot grasping on real reaching\ntrajectories. For this purpose we use Reinforcement Learning (RL) and Imitation\nLearning (IL) in DEXTRON (DEXTerity enviRONment), a stochastic simulation\nenvironment with real human trajectories that are augmented and selected using\na Monte Carlo (MC) simulation method. We also offer a success model which once\ntrained on the expert policy data and the RL policy roll-out transitions, can\nprovide transparency to how the deep policy works and when it is probably going\nto fail.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 19:39:23 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Sharif", "Mohammadreza", ""], ["Erdogmus", "Deniz", ""], ["Amato", "Christopher", ""], ["Padir", "Taskin", ""]]}, {"id": "2104.12868", "submitter": "Ali Akbar Sadat Asl", "authors": "Ali Akbar Sadat Asl, Mohammad Mahdi Ershadi, Shahabeddin Sotudian", "title": "Fuzzy Expert Systems for Prediction of ICU Admission in Patients with\n  COVID-19", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The pandemic COVID-19 disease has had a dramatic impact on almost all\ncountries around the world so that many hospitals have been overwhelmed with\nCovid-19 cases. As medical resources are limited, deciding on the proper\nallocation of these resources is a very crucial issue. Besides, uncertainty is\na major factor that can affect decisions, especially in medical fields. To cope\nwith this issue, we use fuzzy logic (FL) as one of the most suitable methods in\nmodeling systems with high uncertainty and complexity. We intend to make use of\nthe advantages of FL in decisions on cases that need to treat in ICU. In this\nstudy, an interval type-2 fuzzy expert system is proposed for prediction of ICU\nadmission in COVID-19 patients. For this prediction task, we also developed an\nadaptive neuro-fuzzy inference system (ANFIS). Finally, the results of these\nfuzzy systems are compared to some well-known classification methods such as\nNaive Bayes (NB), Case-Based Reasoning (CBR), Decision Tree (DT), and K Nearest\nNeighbor (KNN). The results show that the type-2 fuzzy expert system and ANFIS\nmodels perform competitively in terms of accuracy and F-measure compared to the\nother system modeling techniques.\n", "versions": [{"version": "v1", "created": "Thu, 22 Apr 2021 05:12:49 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Asl", "Ali Akbar Sadat", ""], ["Ershadi", "Mohammad Mahdi", ""], ["Sotudian", "Shahabeddin", ""]]}, {"id": "2104.12871", "submitter": "Melanie Mitchell", "authors": "Melanie Mitchell", "title": "Why AI is Harder Than We Think", "comments": "12 pages; typos corrected in newest version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since its beginning in the 1950s, the field of artificial intelligence has\ncycled several times between periods of optimistic predictions and massive\ninvestment (\"AI spring\") and periods of disappointment, loss of confidence, and\nreduced funding (\"AI winter\"). Even with today's seemingly fast pace of AI\nbreakthroughs, the development of long-promised technologies such as\nself-driving cars, housekeeping robots, and conversational companions has\nturned out to be much harder than many people expected. One reason for these\nrepeating cycles is our limited understanding of the nature and complexity of\nintelligence itself. In this paper I describe four fallacies in common\nassumptions made by AI researchers, which can lead to overconfident predictions\nabout the field. I conclude by discussing the open questions spurred by these\nfallacies, including the age-old challenge of imbuing machines with humanlike\ncommon sense.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 20:39:18 GMT"}, {"version": "v2", "created": "Wed, 28 Apr 2021 15:51:25 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Mitchell", "Melanie", ""]]}, {"id": "2104.12895", "submitter": "Claude Kl\\\"ockl", "authors": "Christoph Graf, Viktor Zobernig, Johannes Schmidt, Claude Kl\\\"ockl", "title": "Computational Performance of Deep Reinforcement Learning to find Nash\n  Equilibria", "comments": "48 pages + 9 figures, comments welcome!", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.LG cs.MA econ.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We test the performance of deep deterministic policy gradient (DDPG), a deep\nreinforcement learning algorithm, able to handle continuous state and action\nspaces, to learn Nash equilibria in a setting where firms compete in prices.\nThese algorithms are typically considered model-free because they do not\nrequire transition probability functions (as in e.g., Markov games) or\npredefined functional forms. Despite being model-free, a large set of\nparameters are utilized in various steps of the algorithm. These are e.g.,\nlearning rates, memory buffers, state-space dimensioning, normalizations, or\nnoise decay rates and the purpose of this work is to systematically test the\neffect of these parameter configurations on convergence to the analytically\nderived Bertrand equilibrium. We find parameter choices that can reach\nconvergence rates of up to 99%. The reliable convergence may make the method a\nuseful tool to study strategic behavior of firms even in more complex settings.\nKeywords: Bertrand Equilibrium, Competition in Uniform Price Auctions, Deep\nDeterministic Policy Gradient Algorithm, Parameter Sensitivity Analysis\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 22:14:17 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Graf", "Christoph", ""], ["Zobernig", "Viktor", ""], ["Schmidt", "Johannes", ""], ["Kl\u00f6ckl", "Claude", ""]]}, {"id": "2104.12920", "submitter": "Kenneth Holstein", "authors": "Kenneth Holstein and Shayan Doroudi", "title": "Equity and Artificial Intelligence in Education: Will \"AIEd\" Amplify or\n  Alleviate Inequities in Education?", "comments": "The co-first authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The development of educational AI (AIEd) systems has often been motivated by\ntheir potential to promote educational equity and reduce achievement gaps\nacross different groups of learners -- for example, by scaling up the benefits\nof one-on-one human tutoring to a broader audience, or by filling gaps in\nexisting educational services. Given these noble intentions, why might AIEd\nsystems have inequitable impacts in practice? In this chapter, we discuss four\nlenses that can be used to examine how and why AIEd systems risk amplifying\nexisting inequities. Building from these lenses, we then outline possible paths\ntowards more equitable futures for AIEd, while highlighting debates surrounding\neach proposal. In doing so, we hope to provoke new conversations around the\ndesign of equitable AIEd, and to push ongoing conversations in the field\nforward.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 00:28:38 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Holstein", "Kenneth", ""], ["Doroudi", "Shayan", ""]]}, {"id": "2104.12922", "submitter": "Joseph Turian", "authors": "Joseph Turian and Jordie Shier and George Tzanetakis and Kirk McNally\n  and Max Henry", "title": "One Billion Audio Sounds from GPU-enabled Modular Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.LG eess.AS eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We release synth1B1, a multi-modal audio corpus consisting of 1 billion\n4-second synthesized sounds, paired with the synthesis parameters used to\ngenerate them. The dataset is 100x larger than any audio dataset in the\nliterature. We also introduce torchsynth, an open source modular synthesizer\nthat generates the synth1B1 samples on-the-fly at 16200x faster than real-time\n(714MHz) on a single GPU. Finally, we release two new audio datasets: FM synth\ntimbre and subtractive synth pitch. Using these datasets, we demonstrate new\nrank-based evaluation criteria for existing audio representations. Finally, we\npropose a novel approach to synthesizer hyperparameter optimization.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 00:38:52 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 17:46:41 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Turian", "Joseph", ""], ["Shier", "Jordie", ""], ["Tzanetakis", "George", ""], ["McNally", "Kirk", ""], ["Henry", "Max", ""]]}, {"id": "2104.12950", "submitter": "Balaji Ganesan", "authors": "Abhay M Shalghar, Ayush Kumar, Balaji Ganesan, Aswin Kannan, Shobha G", "title": "Document Structure aware Relational Graph Convolutional Networks for\n  Ontology Population", "comments": "16 pages single column, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ontologies comprising of concepts, their attributes, and relationships, form\nthe quintessential backbone of many knowledge based AI systems. These systems\nmanifest in the form of question-answering or dialogue in number of business\nanalytics and master data management applications. While there have been\nefforts towards populating domain specific ontologies, we examine the role of\ndocument structure in learning ontological relationships between concepts in\nany document corpus. Inspired by ideas from hypernym discovery and\nexplainability, our method performs about 15 points more accurate than a\nstand-alone R-GCN model for this task.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 02:50:39 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Shalghar", "Abhay M", ""], ["Kumar", "Ayush", ""], ["Ganesan", "Balaji", ""], ["Kannan", "Aswin", ""], ["G", "Shobha", ""]]}, {"id": "2104.12953", "submitter": "Yuandu Lai", "authors": "Yuandu Lai, Yucheng Shi, Yahong Han, Yunfeng Shao, Meiyu Qi, Bingshuai\n  Li", "title": "Exploring Uncertainty in Deep Learning for Construction of Prediction\n  Intervals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has achieved impressive performance on many tasks in recent\nyears. However, it has been found that it is still not enough for deep neural\nnetworks to provide only point estimates. For high-risk tasks, we need to\nassess the reliability of the model predictions. This requires us to quantify\nthe uncertainty of model prediction and construct prediction intervals. In this\npaper, We explore the uncertainty in deep learning to construct the prediction\nintervals. In general, We comprehensively consider two categories of\nuncertainties: aleatory uncertainty and epistemic uncertainty. We design a\nspecial loss function, which enables us to learn uncertainty without\nuncertainty label. We only need to supervise the learning of regression task.\nWe learn the aleatory uncertainty implicitly from the loss function. And that\nepistemic uncertainty is accounted for in ensembled form. Our method correlates\nthe construction of prediction intervals with the uncertainty estimation.\nImpressive results on some publicly available datasets show that the\nperformance of our method is competitive with other state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 02:58:20 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Lai", "Yuandu", ""], ["Shi", "Yucheng", ""], ["Han", "Yahong", ""], ["Shao", "Yunfeng", ""], ["Qi", "Meiyu", ""], ["Li", "Bingshuai", ""]]}, {"id": "2104.13046", "submitter": "Shuai Wang", "authors": "Shuai Wang, Penghui Wei, Jiahao Zhao, Wenji Mao", "title": "A Knowledge Enhanced Learning and Semantic Composition Model for\n  Multi-Claim Fact Checking", "comments": "28 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To inhibit the spread of rumorous information and its severe consequences,\ntraditional fact checking aims at retrieving relevant evidence to verify the\nveracity of a given claim. Fact checking methods typically use knowledge graphs\n(KGs) as external repositories and develop reasoning mechanism to retrieve\nevidence for verifying the triple claim. However, existing methods only focus\non verifying a single claim. As real-world rumorous information is more complex\nand a textual statement is often composed of multiple clauses (i.e. represented\nas multiple claims instead of a single one), multiclaim fact checking is not\nonly necessary but more important for practical applications. Although previous\nmethods for verifying a single triple can be applied repeatedly to verify\nmultiple triples one by one, they ignore the contextual information implied in\na multi-claim statement and could not learn the rich semantic information in\nthe statement as a whole. In this paper, we propose an end-to-end knowledge\nenhanced learning and verification method for multi-claim fact checking. Our\nmethod consists of two modules, KG-based learning enhancement and multi-claim\nsemantic composition. To fully utilize the contextual information, the KG-based\nlearning enhancement module learns the dynamic context-specific representations\nvia selectively aggregating relevant attributes of entities. To capture the\ncompositional semantics of multiple triples, the multi-claim semantic\ncomposition module constructs the graph structure to model claim-level\ninteractions, and integrates global and salient local semantics with multi-head\nattention. Experimental results on a real-world dataset and two benchmark\ndatasets show the effectiveness of our method for multi-claim fact checking\nover KG.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 08:43:14 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Wang", "Shuai", ""], ["Wei", "Penghui", ""], ["Zhao", "Jiahao", ""], ["Mao", "Wenji", ""]]}, {"id": "2104.13048", "submitter": "Zelin Zang", "authors": "Zelin Zang, Siyuan Li, Di Wu, Jianzhu Guo, Yongjie Xu, Stan Z. Li", "title": "Unsupervised Deep Manifold Attributed Graph Embedding", "comments": "arXiv admin note: text overlap with arXiv:2007.01594 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unsupervised attributed graph representation learning is challenging since\nboth structural and feature information are required to be represented in the\nlatent space. Existing methods concentrate on learning latent representation\nvia reconstruction tasks, but cannot directly optimize representation and are\nprone to oversmoothing, thus limiting the applications on downstream tasks. To\nalleviate these issues, we propose a novel graph embedding framework named Deep\nManifold Attributed Graph Embedding (DMAGE). A node-to-node geodesic similarity\nis proposed to compute the inter-node similarity between the data space and the\nlatent space and then use Bergman divergence as loss function to minimize the\ndifference between them. We then design a new network structure with fewer\naggregation to alleviate the oversmoothing problem and incorporate graph\nstructure augmentation to improve the representation's stability. Our proposed\nDMAGE surpasses state-of-the-art methods by a significant margin on three\ndownstream tasks: unsupervised visualization, node clustering, and link\nprediction across four popular datasets.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 08:47:39 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Zang", "Zelin", ""], ["Li", "Siyuan", ""], ["Wu", "Di", ""], ["Guo", "Jianzhu", ""], ["Xu", "Yongjie", ""], ["Li", "Stan Z.", ""]]}, {"id": "2104.13083", "submitter": "Moussa Doumbouya", "authors": "Moussa Doumbouya, Lisa Einstein, Chris Piech", "title": "Using Radio Archives for Low-Resource Speech Recognition: Towards an\n  Intelligent Virtual Assistant for Illiterate Users", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  For many of the 700 million illiterate people around the world, speech\nrecognition technology could provide a bridge to valuable information and\nservices. Yet, those most in need of this technology are often the most\nunderserved by it. In many countries, illiterate people tend to speak only\nlow-resource languages, for which the datasets necessary for speech technology\ndevelopment are scarce. In this paper, we investigate the effectiveness of\nunsupervised speech representation learning on noisy radio broadcasting\narchives, which are abundant even in low-resource languages. We make three core\ncontributions. First, we release two datasets to the research community. The\nfirst, West African Radio Corpus, contains 142 hours of audio in more than 10\nlanguages with a labeled validation subset. The second, West African Virtual\nAssistant Speech Recognition Corpus, consists of 10K labeled audio clips in\nfour languages. Next, we share West African wav2vec, a speech encoder trained\non the noisy radio corpus, and compare it with the baseline Facebook speech\nencoder trained on six times more data of higher quality. We show that West\nAfrican wav2vec performs similarly to the baseline on a multilingual speech\nrecognition task, and significantly outperforms the baseline on a West African\nlanguage identification task. Finally, we share the first-ever speech\nrecognition models for Maninka, Pular and Susu, languages spoken by a combined\n10 million people in over seven countries, including six where the majority of\nthe adult population is illiterate. Our contributions offer a path forward for\nethical AI research to serve the needs of those most disadvantaged by the\ndigital divide.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 10:09:34 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Doumbouya", "Moussa", ""], ["Einstein", "Lisa", ""], ["Piech", "Chris", ""]]}, {"id": "2104.13092", "submitter": "Bin Cao", "authors": "Mingrui Cao, Long Zhang, Bin Cao", "title": "Towards On-Device Federated Learning: A Direct Acyclic Graph-based\n  Blockchain Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the distributed characteristics of Federated Learning (FL), the\nvulnerability of global model and coordination of devices are the main\nobstacle. As a promising solution of decentralization, scalability and\nsecurity, leveraging blockchain in FL has attracted much attention in recent\nyears. However, the traditional consensus mechanisms designed for blockchain\nlike Proof of Work (PoW) would cause extreme resource consumption, which\nreduces the efficiency of FL greatly, especially when the participating devices\nare wireless and resource-limited. In order to address device asynchrony and\nanomaly detection in FL while avoiding the extra resource consumption caused by\nblockchain, this paper introduces a framework for empowering FL using Direct\nAcyclic Graph (DAG)-based blockchain systematically (DAG-FL). Accordingly,\nDAG-FL is first introduced from a three-layer architecture in details, and then\ntwo algorithms DAG-FL Controlling and DAG-FL Updating are designed running on\ndifferent nodes to elaborate the operation of DAG-FL consensus mechanism. After\nthat, a Poisson process model is formulated to discuss that how to set\ndeployment parameters to maintain DAG-FL stably in different federated learning\ntasks. The extensive simulations and experiments show that DAG-FL can achieve\nbetter performance in terms of training efficiency and model accuracy compared\nwith the typical existing on-device federated learning systems as the\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 10:29:38 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Cao", "Mingrui", ""], ["Zhang", "Long", ""], ["Cao", "Bin", ""]]}, {"id": "2104.13095", "submitter": "Guanglin Niu", "authors": "Guanglin Niu, Yang Li, Chengguang Tang, Ruiying Geng, Jian Dai, Qiao\n  Liu, Hao Wang, Jian Sun, Fei Huang, Luo Si", "title": "Relational Learning with Gated and Attentive Neighbor Aggregator for\n  Few-Shot Knowledge Graph Completion", "comments": "The full version of a paper accepted to SIGIR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aiming at expanding few-shot relations' coverage in knowledge graphs (KGs),\nfew-shot knowledge graph completion (FKGC) has recently gained more research\ninterests. Some existing models employ a few-shot relation's multi-hop neighbor\ninformation to enhance its semantic representation. However, noise neighbor\ninformation might be amplified when the neighborhood is excessively sparse and\nno neighbor is available to represent the few-shot relation. Moreover, modeling\nand inferring complex relations of one-to-many (1-N), many-to-one (N-1), and\nmany-to-many (N-N) by previous knowledge graph completion approaches requires\nhigh model complexity and a large amount of training instances. Thus, inferring\ncomplex relations in the few-shot scenario is difficult for FKGC models due to\nlimited training instances. In this paper, we propose a few-shot relational\nlearning with global-local framework to address the above issues. At the global\nstage, a novel gated and attentive neighbor aggregator is built for accurately\nintegrating the semantics of a few-shot relation's neighborhood, which helps\nfiltering the noise neighbors even if a KG contains extremely sparse\nneighborhoods. For the local stage, a meta-learning based TransH (MTransH)\nmethod is designed to model complex relations and train our model in a few-shot\nlearning fashion. Extensive experiments show that our model outperforms the\nstate-of-the-art FKGC approaches on the frequently-used benchmark datasets\nNELL-One and Wiki-One. Compared with the strong baseline model MetaR, our model\nachieves 5-shot FKGC performance improvements of 8.0% on NELL-One and 2.8% on\nWiki-One by the metric Hits@10.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 10:38:44 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 02:56:43 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Niu", "Guanglin", ""], ["Li", "Yang", ""], ["Tang", "Chengguang", ""], ["Geng", "Ruiying", ""], ["Dai", "Jian", ""], ["Liu", "Qiao", ""], ["Wang", "Hao", ""], ["Sun", "Jian", ""], ["Huang", "Fei", ""], ["Si", "Luo", ""]]}, {"id": "2104.13096", "submitter": "Jo\\~ao Rico", "authors": "Jo\\~ao Rico, Jos\\'e Barateiro, Arlindo Oliveira", "title": "Graph Neural Networks for Traffic Forecasting", "comments": null, "journal-ref": "OMAINTEC 2019", "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The significant increase in world population and urbanisation has brought\nseveral important challenges, in particular regarding the sustainability,\nmaintenance and planning of urban mobility. At the same time, the exponential\nincrease of computing capability and of available sensor and location data have\noffered the potential for innovative solutions to these challenges. In this\nwork, we focus on the challenge of traffic forecasting and review the recent\ndevelopment and application of graph neural networks (GNN) to this problem.\nGNNs are a class of deep learning methods that directly process the input as\ngraph data. This leverages more directly the spatial dependencies of traffic\ndata and makes use of the advantages of deep learning producing\nstate-of-the-art results. We introduce and review the emerging topic of GNNs,\nincluding their most common variants, with a focus on its application to\ntraffic forecasting. We address the different ways of modelling traffic\nforecasting as a (temporal) graph, the different approaches developed so far to\ncombine the graph and temporal learning components, as well as current\nlimitations and research opportunities.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 10:39:00 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Rico", "Jo\u00e3o", ""], ["Barateiro", "Jos\u00e9", ""], ["Oliveira", "Arlindo", ""]]}, {"id": "2104.13101", "submitter": "Felix P. Kemeth", "authors": "Felix P. Kemeth, Tom Bertalan, Nikolaos Evangelou, Tianqi Cui, Saurabh\n  Malani, Ioannis G. Kevrekidis", "title": "Initializing LSTM internal states via manifold learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG nlin.PS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present an approach, based on learning an intrinsic data manifold, for the\ninitialization of the internal state values of LSTM recurrent neural networks,\nensuring consistency with the initial observed input data. Exploiting the\ngeneralized synchronization concept, we argue that the converged, \"mature\"\ninternal states constitute a function on this learned manifold. The dimension\nof this manifold then dictates the length of observed input time series data\nrequired for consistent initialization. We illustrate our approach through a\npartially observed chemical model system, where initializing the internal LSTM\nstates in this fashion yields visibly improved performance. Finally, we show\nthat learning this data manifold enables the transformation of partially\nobserved dynamics into fully observed ones, facilitating alternative\nidentification paths for nonlinear dynamical systems.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 10:54:53 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 06:59:24 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Kemeth", "Felix P.", ""], ["Bertalan", "Tom", ""], ["Evangelou", "Nikolaos", ""], ["Cui", "Tianqi", ""], ["Malani", "Saurabh", ""], ["Kevrekidis", "Ioannis G.", ""]]}, {"id": "2104.13130", "submitter": "Shuo Yuan", "authors": "Shuo Yuan, Bin Cao, Yao Sun, Mugen Peng", "title": "Secure and Efficient Federated Learning Through Layering and Sharding\n  Blockchain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) has emerged as a promising master/slave learning\nparadigm to alleviate systemic privacy risks and communication costs incurred\nby cloud-centric machine learning methods. However, it is very challenging to\nresist the single point of failure of the master aggregator and attacks from\nmalicious participants while guaranteeing model convergence speed and accuracy.\nRecently, blockchain has been brought into FL systems transforming the paradigm\nto a decentralized manner thus further improve the system security and learning\nreliability. Unfortunately, the traditional consensus mechanism and\narchitecture of blockchain systems can hardly handle the large-scale FL task\ndue to the huge resource consumption, limited transaction throughput, and high\ncommunication complexity. To address these issues, this paper proposes a\ntwo-layer blockchaindriven FL framework, called as ChainsFL, which is composed\nof multiple subchain networks (subchain layer) and a direct acyclic graph\n(DAG)-based mainchain (mainchain layer). In ChainsFL, the subchain layer limits\nthe scale of each shard for a small range of information exchange, and the\nmainchain layer allows each shard to share and validate the learning model in\nparallel and asynchronously to improve the efficiency of cross-shard\nvalidation. Furthermore, the FL procedure is customized to deeply integrate\nwith blockchain technology, and the modified DAG consensus mechanism is\nproposed to mitigate the distortion caused by abnormal models. In order to\nprovide a proof-ofconcept implementation and evaluation, multiple subchains\nbase on Hyperledger Fabric are deployed as the subchain layer, and the\nself-developed DAG-based mainchain is deployed as the mainchain layer. The\nexperimental results show that ChainsFL provides acceptable and sometimes\nbetter training efficiency and stronger robustness compared with the typical\nexisting FL systems.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 12:19:07 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Yuan", "Shuo", ""], ["Cao", "Bin", ""], ["Sun", "Yao", ""], ["Peng", "Mugen", ""]]}, {"id": "2104.13138", "submitter": "Patrick Koopmann", "authors": "Christian Alrabbaa and Franz Baader and Stefan Borgwardt and Patrick\n  Koopmann and Alisa Kovtunova", "title": "Finding Good Proofs for Description Logic Entailments Using Recursive\n  Quality Measures (Extended Technical Report)", "comments": "Extended version of a paper accepted at CADE-28", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Logic-based approaches to AI have the advantage that their behavior can in\nprinciple be explained to a user. If, for instance, a Description Logic\nreasoner derives a consequence that triggers some action of the overall system,\nthen one can explain such an entailment by presenting a proof of the\nconsequence in an appropriate calculus. How comprehensible such a proof is\ndepends not only on the employed calculus, but also on the properties of the\nparticular proof, such as its overall size, its depth, the complexity of the\nemployed sentences and proof steps, etc. For this reason, we want to determine\nthe complexity of generating proofs that are below a certain threshold w.r.t. a\ngiven measure of proof quality. Rather than investigating this problem for a\nfixed proof calculus and a fixed measure, we aim for general results that hold\nfor wide classes of calculi and measures. In previous work, we first restricted\nthe attention to a setting where proof size is used to measure the quality of a\nproof. We then extended the approach to a more general setting, but important\nmeasures such as proof depth were not covered. In the present paper, we provide\nresults for a class of measures called recursive, which yields lower\ncomplexities and also encompasses proof depth. In addition, we close some gaps\nleft open in our previous work, thus providing a comprehensive picture of the\ncomplexity landscape.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 12:34:13 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Alrabbaa", "Christian", ""], ["Baader", "Franz", ""], ["Borgwardt", "Stefan", ""], ["Koopmann", "Patrick", ""], ["Kovtunova", "Alisa", ""]]}, {"id": "2104.13155", "submitter": "Li Weigang", "authors": "Li Weigang, Liriam Enamoto, Denise Leyi Li, Geraldo Pereira Rocha\n  Filho", "title": "Watershed of Artificial Intelligence: Human Intelligence, Machine\n  Intelligence, and Biological Intelligence", "comments": "This article reviews the Once Learning mechanism and divides\n  Artificial Intelligence into three categories: Artificial Human Intelligence\n  (AHI), Artificial Machine Intelligence (AMI), and Artificial Biological\n  Intelligence (ABI). The paper is with 16 pages and 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article reviews the \"Once learning\" mechanism that was proposed 23 years\nago and the subsequent successes of \"One-shot learning\" in image classification\nand \"You Only Look Once - YOLO\" in objective detection. Analyzing the current\ndevelopment of Artificial Intelligence (AI), the proposal is that AI should be\nclearly divided into the following categories: Artificial Human Intelligence\n(AHI), Artificial Machine Intelligence (AMI), and Artificial Biological\nIntelligence (ABI), which will also be the main directions of theory and\napplication development for AI. As a watershed for the branches of AI, some\nclassification standards and methods are discussed: 1) Human-oriented,\nmachine-oriented, and biological-oriented AI R&D; 2) Information input\nprocessed by Dimensionality-up or Dimensionality-reduction; 3) The use of\none/few or large samples for knowledge learning.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 13:03:25 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 18:34:10 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Weigang", "Li", ""], ["Enamoto", "Liriam", ""], ["Li", "Denise Leyi", ""], ["Filho", "Geraldo Pereira Rocha", ""]]}, {"id": "2104.13180", "submitter": "Ioannis Stefanou", "authors": "Efthymios Papachristos and Ioannis Stefanou", "title": "Controlling earthquake-like instabilities using artificial intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.geo-ph cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Earthquakes are lethal and costly. This study aims at avoiding these\ncatastrophic events by the application of injection policies retrieved through\nreinforcement learning. With the rapid growth of artificial intelligence,\nprediction-control problems are all the more tackled by function approximation\nmodels that learn how to control a specific task, even for systems with\nunmodeled/unknown dynamics and important uncertainties. Here, we show for the\nfirst time the possibility of controlling earthquake-like instabilities using\nstate-of-the-art deep reinforcement learning techniques. The controller is\ntrained using a reduced model of the physical system, i.e, the spring-slider\nmodel, which embodies the main dynamics of the physical problem for a given\nearthquake magnitude. Its robustness to unmodeled dynamics is explored through\na parametric study. Our study is a first step towards minimizing seismicity in\nindustrial projects (geothermal energy, hydrocarbons production, CO2\nsequestration) while, in a second step for inspiring techniques for natural\nearthquakes control and prevention.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 13:39:58 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Papachristos", "Efthymios", ""], ["Stefanou", "Ioannis", ""]]}, {"id": "2104.13187", "submitter": "Tegg Sung", "authors": "Tegg Taekyong Sung, Bo Ryu", "title": "A Scalable and Reproducible System-on-Chip Simulation for Reinforcement\n  Learning", "comments": "6 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Reinforcement Learning (DRL) underlies in a simulated environment and\noptimizes objective goals. By extending the conventional interaction scheme,\nthis paper proffers gym-ds3, a scalable and reproducible open environment\ntailored for a high-fidelity Domain-Specific System-on-Chip (DSSoC)\napplication. The simulation corroborates to schedule hierarchical jobs onto\nheterogeneous System-on-Chip (SoC) processors and bridges the system to\nreinforcement learning research. We systematically analyze the representative\nSoC simulator and discuss the primary challenging aspects that the system (1)\ncontinuously generates indefinite jobs at a rapid injection rate, (2) optimizes\ncomplex objectives, and (3) operates in steady-state scheduling. We provide\nexemplary snippets and experimentally demonstrate the run-time performances on\ndifferent schedulers that successfully mimic results achieved from the standard\nDS3 framework and real-world embedded systems.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 13:46:57 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Sung", "Tegg Taekyong", ""], ["Ryu", "Bo", ""]]}, {"id": "2104.13190", "submitter": "Md Tahmid Rahman Laskar", "authors": "Md Tahmid Rahman Laskar, Jimmy Huang, Vladan Smetana, Chris Stewart,\n  Kees Pouw, Aijun An, Stephen Chan, Lei Liu", "title": "Extending Isolation Forest for Anomaly Detection in Big Data via K-Means", "comments": "The final version will be published at ACM Transactions on\n  Cyber-Physical Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Industrial Information Technology (IT) infrastructures are often vulnerable\nto cyberattacks. To ensure security to the computer systems in an industrial\nenvironment, it is required to build effective intrusion detection systems to\nmonitor the cyber-physical systems (e.g., computer networks) in the industry\nfor malicious activities. This paper aims to build such intrusion detection\nsystems to protect the computer networks from cyberattacks. More specifically,\nwe propose a novel unsupervised machine learning approach that combines the\nK-Means algorithm with the Isolation Forest for anomaly detection in industrial\nbig data scenarios. Since our objective is to build the intrusion detection\nsystem for the big data scenario in the industrial domain, we utilize the\nApache Spark framework to implement our proposed model which was trained in\nlarge network traffic data (about 123 million instances of network traffic)\nstored in Elasticsearch. Moreover, we evaluate our proposed model on the live\nstreaming data and find that our proposed system can be used for real-time\nanomaly detection in the industrial setup. In addition, we address different\nchallenges that we face while training our model on large datasets and\nexplicitly describe how these issues were resolved. Based on our empirical\nevaluation in different use-cases for anomaly detection in real-world network\ntraffic data, we observe that our proposed system is effective to detect\nanomalies in big data scenarios. Finally, we evaluate our proposed model on\nseveral academic datasets to compare with other models and find that it\nprovides comparable performance with other state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 16:21:48 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Laskar", "Md Tahmid Rahman", ""], ["Huang", "Jimmy", ""], ["Smetana", "Vladan", ""], ["Stewart", "Chris", ""], ["Pouw", "Kees", ""], ["An", "Aijun", ""], ["Chan", "Stephen", ""], ["Liu", "Lei", ""]]}, {"id": "2104.13207", "submitter": "R\\'emy Portelas", "authors": "Grgur Kova\\v{c}, R\\'emy Portelas, Katja Hofmann, Pierre-Yves Oudeyer", "title": "SocialAI 0.1: Towards a Benchmark to Stimulate Research on\n  Socio-Cognitive Abilities in Deep Reinforcement Learning Agents", "comments": "Accepted at NAACL ViGIL Workshop 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building embodied autonomous agents capable of participating in social\ninteractions with humans is one of the main challenges in AI. This problem\nmotivated many research directions on embodied language use. Current approaches\nfocus on language as a communication tool in very simplified and non diverse\nsocial situations: the \"naturalness\" of language is reduced to the concept of\nhigh vocabulary size and variability. In this paper, we argue that aiming\ntowards human-level AI requires a broader set of key social skills: 1) language\nuse in complex and variable social contexts; 2) beyond language, complex\nembodied communication in multimodal settings within constantly evolving social\nworlds. In this work we explain how concepts from cognitive sciences could help\nAI to draw a roadmap towards human-like intelligence, with a focus on its\nsocial dimensions. We then study the limits of a recent SOTA Deep RL approach\nwhen tested on a first grid-world environment from the upcoming SocialAI, a\nbenchmark to assess the social skills of Deep RL agents. Videos and code are\navailable at https://sites.google.com/view/socialai01 .\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 14:16:29 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Kova\u010d", "Grgur", ""], ["Portelas", "R\u00e9my", ""], ["Hofmann", "Katja", ""], ["Oudeyer", "Pierre-Yves", ""]]}, {"id": "2104.13215", "submitter": "Elsa Rizk", "authors": "Elsa Rizk and Ali H. Sayed", "title": "A Graph Federated Architecture with Privacy Preserving Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning involves a central processor that works with multiple\nagents to find a global model. The process consists of repeatedly exchanging\nestimates, which results in the diffusion of information pertaining to the\nlocal private data. Such a scheme can be inconvenient when dealing with\nsensitive data, and therefore, there is a need for the privatization of the\nalgorithms. Furthermore, the current architecture of a server connected to\nmultiple clients is highly sensitive to communication failures and\ncomputational overloads at the server. Thus in this work, we develop a private\nmulti-server federated learning scheme, which we call graph federated learning.\nWe use cryptographic and differential privacy concepts to privatize the\nfederated learning algorithm that we extend to the graph structure. We study\nthe effect of privatization on the performance of the learning algorithm for\ngeneral private schemes that can be modeled as additive noise. We show under\nconvexity and Lipschitz conditions, that the privatized process matches the\nperformance of the non-private algorithm, even when we increase the noise\nvariance.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 09:51:24 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Rizk", "Elsa", ""], ["Sayed", "Ali H.", ""]]}, {"id": "2104.13216", "submitter": "Cheng Wang", "authors": "Cheng Wang, Sun Kim, Taiwoo Park, Sajal Choudhary, Sunghyun Park,\n  Young-Bum Kim, Ruhi Sarikaya, Sungjin Lee", "title": "Handling Long-Tail Queries with Slice-Aware Conversational Systems", "comments": "Published at ICLR 2021 Workshop on Weakly Supervised Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We have been witnessing the usefulness of conversational AI systems such as\nSiri and Alexa, directly impacting our daily lives. These systems normally rely\non machine learning models evolving over time to provide quality user\nexperience. However, the development and improvement of the models are\nchallenging because they need to support both high (head) and low (tail) usage\nscenarios, requiring fine-grained modeling strategies for specific data subsets\nor slices. In this paper, we explore the recent concept of slice-based learning\n(SBL) (Chen et al., 2019) to improve our baseline conversational skill routing\nsystem on the tail yet critical query traffic. We first define a set of\nlabeling functions to generate weak supervision data for the tail intents. We\nthen extend the baseline model towards a slice-aware architecture, which\nmonitors and improves the model performance on the selected tail intents.\nApplied to de-identified live traffic from a commercial conversational AI\nsystem, our experiments show that the slice-aware model is beneficial in\nimproving model performance for the tail intents while maintaining the overall\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 26 Apr 2021 10:23:28 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Wang", "Cheng", ""], ["Kim", "Sun", ""], ["Park", "Taiwoo", ""], ["Choudhary", "Sajal", ""], ["Park", "Sunghyun", ""], ["Kim", "Young-Bum", ""], ["Sarikaya", "Ruhi", ""], ["Lee", "Sungjin", ""]]}, {"id": "2104.13225", "submitter": "Grzegorz Chrupa{\\l}a", "authors": "Grzegorz Chrupa{\\l}a", "title": "Visually grounded models of spoken language: A survey of datasets,\n  architectures and evaluation techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG cs.SD eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This survey provides an overview of the evolution of visually grounded models\nof spoken language over the last 20 years. Such models are inspired by the\nobservation that when children pick up a language, they rely on a wide range of\nindirect and noisy clues, crucially including signals from the visual modality\nco-occurring with spoken utterances. Several fields have made important\ncontributions to this approach to modeling or mimicking the process of learning\nlanguage: Machine Learning, Natural Language and Speech Processing, Computer\nVision and Cognitive Science. The current paper brings together these\ncontributions in order to provide a useful introduction and overview for\npractitioners in all these areas. We discuss the central research questions\naddressed, the timeline of developments, and the datasets which enabled much of\nthis work. We then summarize the main modeling architectures and offer an\nexhaustive overview of the evaluation metrics and analysis techniques.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 14:32:22 GMT"}, {"version": "v2", "created": "Sat, 1 May 2021 14:59:07 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Chrupa\u0142a", "Grzegorz", ""]]}, {"id": "2104.13227", "submitter": "Mohammad-Ali Javidian", "authors": "Mohammad Ali Javidian, Vaneet Aggarwal, Zubin Jacob", "title": "Quantum Causal Inference in the Presence of Hidden Common Causes: an\n  Entropic Approach", "comments": "arXiv admin note: text overlap with arXiv:2102.11764", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.AI cs.IT cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum causality is an emerging field of study which has the potential to\ngreatly advance our understanding of quantum systems. One of the most important\nproblems in quantum causality is linked to this prominent aphorism that states\ncorrelation does not mean causation. A direct generalization of the existing\ncausal inference techniques to the quantum domain is not possible due to\nsuperposition and entanglement. We put forth a new theoretical framework for\nmerging quantum information science and causal inference by exploiting entropic\nprinciples. For this purpose, we leverage the concept of conditional density\nmatrices to develop a scalable algorithmic approach for inferring causality in\nthe presence of latent confounders (common causes) in quantum systems. We apply\nour proposed framework to an experimentally relevant scenario of identifying\nmessage senders on quantum noisy links, where it is validated that the input\nbefore noise as a latent confounder is the cause of the noisy outputs. We also\ndemonstrate that the proposed approach outperforms the results of classical\ncausal inference even when the variables are classical by exploiting quantum\ndependence between variables through density matrices rather than joint\nprobability distributions. Thus, the proposed approach unifies classical and\nquantum causal inference in a principled way. This successful inference on a\nsynthetic quantum dataset can lay the foundations of identifying originators of\nmalicious activity on future multi-node quantum networks.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 22:45:50 GMT"}], "update_date": "2021-06-12", "authors_parsed": [["Javidian", "Mohammad Ali", ""], ["Aggarwal", "Vaneet", ""], ["Jacob", "Zubin", ""]]}, {"id": "2104.13230", "submitter": "Manish Shukla", "authors": "Sanjay Seetharaman, Shubham Malaviya, Rosni KV, Manish Shukla, Sachin\n  Lodha", "title": "Influence Based Defense Against Data Poisoning Attacks in Online\n  Learning", "comments": "18 pages, 3 Figures, 2 Tables, Adversarial Machine Learning, Data\n  Poisoning, Online Learning, Defense, Influence Function", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Data poisoning is a type of adversarial attack on training data where an\nattacker manipulates a fraction of data to degrade the performance of machine\nlearning model. Therefore, applications that rely on external data-sources for\ntraining data are at a significantly higher risk. There are several known\ndefensive mechanisms that can help in mitigating the threat from such attacks.\nFor example, data sanitization is a popular defensive mechanism wherein the\nlearner rejects those data points that are sufficiently far from the set of\ntraining instances. Prior work on data poisoning defense primarily focused on\noffline setting, wherein all the data is assumed to be available for analysis.\nDefensive measures for online learning, where data points arrive sequentially,\nhave not garnered similar interest.\n  In this work, we propose a defense mechanism to minimize the degradation\ncaused by the poisoned training data on a learner's model in an online setup.\nOur proposed method utilizes an influence function which is a classic technique\nin robust statistics. Further, we supplement it with the existing data\nsanitization methods for filtering out some of the poisoned data points. We\nstudy the effectiveness of our defense mechanism on multiple datasets and\nacross multiple attack strategies against an online learner.\n", "versions": [{"version": "v1", "created": "Sat, 24 Apr 2021 08:39:13 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Seetharaman", "Sanjay", ""], ["Malaviya", "Shubham", ""], ["KV", "Rosni", ""], ["Shukla", "Manish", ""], ["Lodha", "Sachin", ""]]}, {"id": "2104.13254", "submitter": "John Emanuello Ph.D.", "authors": "John Emanuello, Kimberly Ferguson-Walter, Erik Hemberg, Una-May O\n  Reilly, Ahmad Ridley, Dennis Ross, Diane Staheli, William Streilein", "title": "Proceedings - AI/ML for Cybersecurity: Challenges, Solutions, and Novel\n  Ideas at SIAM Data Mining 2021", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Malicious cyber activity is ubiquitous and its harmful effects have dramatic\nand often irreversible impacts on society. Given the shortage of cybersecurity\nprofessionals, the ever-evolving adversary, the massive amounts of data which\ncould contain evidence of an attack, and the speed at which defensive actions\nmust be taken, innovations which enable autonomy in cybersecurity must continue\nto expand, in order to move away from a reactive defense posture and towards a\nmore proactive one.\n  The challenges in this space are quite different from those associated with\napplying AI in other domains such as computer vision. The environment suffers\nfrom an incredibly high degree of uncertainty, stemming from the intractability\nof ingesting all the available data, as well as the possibility that malicious\nactors are manipulating the data. Another unique challenge in this space is the\ndynamism of the adversary causes the indicators of compromise to change\nfrequently and without warning.\n  In spite of these challenges, machine learning has been applied to this\ndomain and has achieved some success in the realm of detection. While this\naspect of the problem is far from solved, a growing part of the commercial\nsector is providing ML-enhanced capabilities as a service. Many of these\nentities also provide platforms which facilitate the deployment of these\nautomated solutions. Academic research in this space is growing and continues\nto influence current solutions, as well as strengthen foundational knowledge\nwhich will make autonomous agents in this space a possibility.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 17:35:31 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 15:01:28 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Emanuello", "John", ""], ["Ferguson-Walter", "Kimberly", ""], ["Hemberg", "Erik", ""], ["Reilly", "Una-May O", ""], ["Ridley", "Ahmad", ""], ["Ross", "Dennis", ""], ["Staheli", "Diane", ""], ["Streilein", "William", ""]]}, {"id": "2104.13297", "submitter": "Otto Menegasso Pires", "authors": "O. M. Pires, E. I. Duzzioni, J. Marchi, R. Santiago", "title": "Quantum circuit synthesis of Bell and GHZ states using projective\n  simulation in the NISQ era", "comments": null, "journal-ref": "Inteligencia Artificial, 24(67) (2021), 90-101", "doi": "10.4114/intartif.vol24iss67pp90-101", "report-no": null, "categories": "quant-ph cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Quantum Computing has been evolving in the last years. Although nowadays\nquantum algorithms performance has shown superior to their classical\ncounterparts, quantum decoherence and additional auxiliary qubits needed for\nerror tolerance routines have been huge barriers for quantum algorithms\nefficient use. These restrictions lead us to search for ways to minimize\nalgorithms costs, i.e the number of quantum logical gates and the depth of the\ncircuit. For this, quantum circuit synthesis and quantum circuit optimization\ntechniques are explored. We studied the viability of using Projective\nSimulation, a reinforcement learning technique, to tackle the problem of\nquantum circuit synthesis for noise quantum computers with limited number of\nqubits. The agent had the task of creating quantum circuits up to 5 qubits to\ngenerate GHZ states in the IBM Tenerife (IBM QX4) quantum processor. Our\nsimulations demonstrated that the agent had a good performance but its capacity\nfor learning new circuits decreased as the number of qubits increased.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 16:11:27 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Pires", "O. M.", ""], ["Duzzioni", "E. I.", ""], ["Marchi", "J.", ""], ["Santiago", "R.", ""]]}, {"id": "2104.13299", "submitter": "David Alvarez-Melis", "authors": "David Alvarez-Melis, Harmanpreet Kaur, Hal Daum\\'e III, Hanna Wallach,\n  Jennifer Wortman Vaughan", "title": "A Human-Centered Interpretability Framework Based on Weight of Evidence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we take a human-centered approach to interpretable machine\nlearning. First, drawing inspiration from the study of explanation in\nphilosophy, cognitive science, and the social sciences, we propose a list of\ndesign principles for machine-generated explanations that are meaningful to\nhumans. Using the concept of weight of evidence from information theory, we\ndevelop a method for producing explanations that adhere to these principles. We\nshow that this method can be adapted to handle high-dimensional, multi-class\nsettings, yielding a flexible meta-algorithm for generating explanations. We\ndemonstrate that these explanations can be estimated accurately from finite\nsamples and are robust to small perturbations of the inputs. We also evaluate\nour method through a qualitative user study with machine learning\npractitioners, where we observe that the resulting explanations are usable\ndespite some participants struggling with background concepts like prior class\nprobabilities. Finally, we conclude by surfacing design implications for\ninterpretability tools\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 16:13:35 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Alvarez-Melis", "David", ""], ["Kaur", "Harmanpreet", ""], ["Daum\u00e9", "Hal", "III"], ["Wallach", "Hanna", ""], ["Vaughan", "Jennifer Wortman", ""]]}, {"id": "2104.13302", "submitter": "Shiqi Chen", "authors": "Shiqi Chen, Zhengyu Chen, Donglin Wang", "title": "Adaptive Adversarial Training for Meta Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Meta Reinforcement Learning (MRL) enables an agent to learn from a limited\nnumber of past trajectories and extrapolate to a new task. In this paper, we\nattempt to improve the robustness of MRL. We build upon model-agnostic\nmeta-learning (MAML) and propose a novel method to generate adversarial samples\nfor MRL by using Generative Adversarial Network (GAN). That allows us to\nenhance the robustness of MRL to adversal attacks by leveraging these attacks\nduring meta training process.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 16:23:34 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Chen", "Shiqi", ""], ["Chen", "Zhengyu", ""], ["Wang", "Donglin", ""]]}, {"id": "2104.13423", "submitter": "Abigail Copiaco", "authors": "Abigail Copiaco, Christian Ritz, Stefano Fasciani, Nidhal Abdulaziz", "title": "DASEE A Synthetic Database of Domestic Acoustic Scenes and Events in\n  Dementia Patients Environment", "comments": "5 pages, 4 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.AI cs.SD eess.SP", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Access to informative databases is a crucial part of notable research\ndevelopments. In the field of domestic audio classification, there have been\nsignificant advances in recent years. Although several audio databases exist,\nthese can be limited in terms of the amount of information they provide, such\nas the exact location of the sound sources, and the associated noise levels. In\nthis work, we detail our approach on generating an unbiased synthetic domestic\naudio database, consisting of sound scenes and events, emulated in both quiet\nand noisy environments. Data is carefully curated such that it reflects issues\ncommonly faced in a dementia patients environment, and recreate scenarios that\ncould occur in real-world settings. Similarly, the room impulse response\ngenerated is based on a typical one-bedroom apartment at Hebrew SeniorLife\nFacility. As a result, we present an 11-class database containing excerpts of\nclean and noisy signals at 5-seconds duration each, uniformly sampled at 16\nkHz. Using our baseline model using Continues Wavelet Transform Scalograms and\nAlexNet, this yielded a weighted F1-score of 86.24 percent.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 18:51:44 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 10:11:12 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Copiaco", "Abigail", ""], ["Ritz", "Christian", ""], ["Fasciani", "Stefano", ""], ["Abdulaziz", "Nidhal", ""]]}, {"id": "2104.13433", "submitter": "Siyuan Xiang", "authors": "Siyuan Xiang, Anbang Yang, Yanfei Xue, Yaoqing Yang, Chen Feng", "title": "Contrastive Spatial Reasoning on Multi-View Line Drawings", "comments": "The first two authors contributed equally. Chen Feng is the\n  corresponding author", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial reasoning on multi-view line drawings by state-of-the-art supervised\ndeep networks is recently shown with puzzling low performances on the SPARE3D\ndataset. To study the reason behind the low performance and to further our\nunderstandings of these tasks, we design controlled experiments on both input\ndata and network designs. Guided by the hindsight from these experiment\nresults, we propose a simple contrastive learning approach along with other\nnetwork modifications to improve the baseline performance. Our approach uses a\nself-supervised binary classification network to compare the line drawing\ndifferences between various views of any two similar 3D objects. It enables\ndeep networks to effectively learn detail-sensitive yet view-invariant line\ndrawing representations of 3D objects. Experiments show that our method could\nsignificantly increase the baseline performance in SPARE3D, while some popular\nself-supervised learning methods cannot.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 19:05:27 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Xiang", "Siyuan", ""], ["Yang", "Anbang", ""], ["Xue", "Yanfei", ""], ["Yang", "Yaoqing", ""], ["Feng", "Chen", ""]]}, {"id": "2104.13453", "submitter": "Zeyang Liu", "authors": "Zeyang Liu, Ke Zhou and Max L. Wilson", "title": "Meta-evaluation of Conversational Search Evaluation Metrics", "comments": "43 pages", "journal-ref": null, "doi": "10.1145/3445029", "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conversational search systems, such as Google Assistant and Microsoft\nCortana, enable users to interact with search systems in multiple rounds\nthrough natural language dialogues. Evaluating such systems is very challenging\ngiven that any natural language responses could be generated, and users\ncommonly interact for multiple semantically coherent rounds to accomplish a\nsearch task. Although prior studies proposed many evaluation metrics, the\nextent of how those measures effectively capture user preference remains to be\ninvestigated. In this paper, we systematically meta-evaluate a variety of\nconversational search metrics. We specifically study three perspectives on\nthose metrics: (1) reliability: the ability to detect \"actual\" performance\ndifferences as opposed to those observed by chance; (2) fidelity: the ability\nto agree with ultimate user preference; and (3) intuitiveness: the ability to\ncapture any property deemed important: adequacy, informativeness, and fluency\nin the context of conversational search. By conducting experiments on two test\ncollections, we find that the performance of different metrics varies\nsignificantly across different scenarios whereas consistent with prior studies,\nexisting metrics only achieve a weak correlation with ultimate user preference\nand satisfaction. METEOR is, comparatively speaking, the best existing\nsingle-turn metric considering all three perspectives. We also demonstrate that\nadapted session-based evaluation metrics can be used to measure multi-turn\nconversational search, achieving moderate concordance with user satisfaction.\nTo our knowledge, our work establishes the most comprehensive meta-evaluation\nfor conversational search to date.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 20:01:03 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Liu", "Zeyang", ""], ["Zhou", "Ke", ""], ["Wilson", "Max L.", ""]]}, {"id": "2104.13468", "submitter": "Aviv Keren", "authors": "Aviv Keren", "title": "The Role of General Intelligence in Mathematical Reasoning", "comments": null, "journal-ref": "1st Mathematical Reasoning in General Artificial Intelligence\n  Workshop, ICLR 2021", "doi": null, "report-no": null, "categories": "cs.AI cs.LG math.HO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objects are a centerpiece of the mathematical realm and our interaction with\nand reasoning about it, just as they are of the physical one (if not more). And\nhumans' mathematical reasoning must ultimately be grounded in our general\nintelligence. Yet in contemporary cognitive science and A.I., the physical and\nmathematical domains are customarily explored separately, which allows for\nbaking in assumptions for what objects are for the system - and missing\npotential connections.\n  In this paper, I put the issue into its philosophical and cognitive context.\nI then describe an abstract theoretical framework for learning object\nrepresentations, that makes room for mathematical objects on par with\nnon-mathematical ones. Finally, I describe a case study that builds on that\nview to show how our general ability for integrating different aspects of\nobjects effects our conception of the natural numbers.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 20:43:25 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Keren", "Aviv", ""]]}, {"id": "2104.13473", "submitter": "Asad Anwar Butt", "authors": "George Awad, Asad A. Butt, Keith Curtis, Jonathan Fiscus, Afzal Godil,\n  Yooyoung Lee, Andrew Delgado, Jesse Zhang, Eliot Godard, Baptiste Chocot,\n  Lukas Diduch, Jeffrey Liu, Alan F. Smeaton, Yvette Graham, Gareth J. F.\n  Jones, Wessel Kraaij, Georges Quenot", "title": "TRECVID 2020: A comprehensive campaign for evaluating video retrieval\n  tasks across multiple application domains", "comments": "TRECVID 2020 Workshop Overview Paper. arXiv admin note: substantial\n  text overlap with arXiv:2009.09984", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The TREC Video Retrieval Evaluation (TRECVID) is a TREC-style video analysis\nand retrieval evaluation with the goal of promoting progress in research and\ndevelopment of content-based exploitation and retrieval of information from\ndigital video via open, metrics-based evaluation. Over the last twenty years\nthis effort has yielded a better understanding of how systems can effectively\naccomplish such processing and how one can reliably benchmark their\nperformance. TRECVID has been funded by NIST (National Institute of Standards\nand Technology) and other US government agencies. In addition, many\norganizations and individuals worldwide contribute significant time and effort.\nTRECVID 2020 represented a continuation of four tasks and the addition of two\nnew tasks. In total, 29 teams from various research organizations worldwide\ncompleted one or more of the following six tasks: 1. Ad-hoc Video Search (AVS),\n2. Instance Search (INS), 3. Disaster Scene Description and Indexing (DSDI), 4.\nVideo to Text Description (VTT), 5. Activities in Extended Video (ActEV), 6.\nVideo Summarization (VSUM). This paper is an introduction to the evaluation\nframework, tasks, data, and measures used in the evaluation campaign.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 20:59:27 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Awad", "George", ""], ["Butt", "Asad A.", ""], ["Curtis", "Keith", ""], ["Fiscus", "Jonathan", ""], ["Godil", "Afzal", ""], ["Lee", "Yooyoung", ""], ["Delgado", "Andrew", ""], ["Zhang", "Jesse", ""], ["Godard", "Eliot", ""], ["Chocot", "Baptiste", ""], ["Diduch", "Lukas", ""], ["Liu", "Jeffrey", ""], ["Smeaton", "Alan F.", ""], ["Graham", "Yvette", ""], ["Jones", "Gareth J. F.", ""], ["Kraaij", "Wessel", ""], ["Quenot", "Georges", ""]]}, {"id": "2104.13478", "submitter": "Petar Veli\\v{c}kovi\\'c", "authors": "Michael M. Bronstein, Joan Bruna, Taco Cohen, Petar Veli\\v{c}kovi\\'c", "title": "Geometric Deep Learning: Grids, Groups, Graphs, Geodesics, and Gauges", "comments": "156 pages. Work in progress -- comments welcome!", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CG cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The last decade has witnessed an experimental revolution in data science and\nmachine learning, epitomised by deep learning methods. Indeed, many\nhigh-dimensional learning tasks previously thought to be beyond reach -- such\nas computer vision, playing Go, or protein folding -- are in fact feasible with\nappropriate computational scale. Remarkably, the essence of deep learning is\nbuilt from two simple algorithmic principles: first, the notion of\nrepresentation or feature learning, whereby adapted, often hierarchical,\nfeatures capture the appropriate notion of regularity for each task, and\nsecond, learning by local gradient-descent type methods, typically implemented\nas backpropagation.\n  While learning generic functions in high dimensions is a cursed estimation\nproblem, most tasks of interest are not generic, and come with essential\npre-defined regularities arising from the underlying low-dimensionality and\nstructure of the physical world. This text is concerned with exposing these\nregularities through unified geometric principles that can be applied\nthroughout a wide spectrum of applications.\n  Such a 'geometric unification' endeavour, in the spirit of Felix Klein's\nErlangen Program, serves a dual purpose: on one hand, it provides a common\nmathematical framework to study the most successful neural network\narchitectures, such as CNNs, RNNs, GNNs, and Transformers. On the other hand,\nit gives a constructive procedure to incorporate prior physical knowledge into\nneural architectures and provide principled way to build future architectures\nyet to be invented.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 21:09:51 GMT"}, {"version": "v2", "created": "Sun, 2 May 2021 16:16:03 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Bronstein", "Michael M.", ""], ["Bruna", "Joan", ""], ["Cohen", "Taco", ""], ["Veli\u010dkovi\u0107", "Petar", ""]]}, {"id": "2104.13484", "submitter": "Mahmoud Hossam", "authors": "Mahmoud Hossam, Trung Le, He Zhao, Viet Huynh, Dinh Phung", "title": "Improved and Efficient Text Adversarial Attacks using Target Information", "comments": "Accepted in the International Conference on Learning Representations\n  (ICLR) workshop on Robust and Reliable Machine Learning in the Real World\n  (RobustML)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been recently a growing interest in studying adversarial examples\non natural language models in the black-box setting. These methods attack\nnatural language classifiers by perturbing certain important words until the\nclassifier label is changed. In order to find these important words, these\nmethods rank all words by importance by querying the target model word by word\nfor each input sentence, resulting in high query inefficiency. A new\ninteresting approach was introduced that addresses this problem through\ninterpretable learning to learn the word ranking instead of previous expensive\nsearch. The main advantage of using this approach is that it achieves\ncomparable attack rates to the state-of-the-art methods, yet faster and with\nfewer queries, where fewer queries are desirable to avoid suspicion towards the\nattacking agent. Nonetheless, this approach sacrificed the useful information\nthat could be leveraged from the target classifier for that sake of query\nefficiency. In this paper we study the effect of leveraging the target model\noutputs and data on both attack rates and average number of queries, and we\nshow that both can be improved, with a limited overhead of additional queries.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 21:25:55 GMT"}, {"version": "v2", "created": "Sun, 2 May 2021 08:49:09 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Hossam", "Mahmoud", ""], ["Le", "Trung", ""], ["Zhao", "He", ""], ["Huynh", "Viet", ""], ["Phung", "Dinh", ""]]}, {"id": "2104.13488", "submitter": "Mahmoud Hossam", "authors": "Mahmoud Hossam, Trung Le, Michael Papasimeon, Viet Huynh, Dinh Phung", "title": "Text Generation with Deep Variational GAN", "comments": "Accepted in the Third Workshop on Bayesian Deep Learning (NIPS /\n  NeurIPS 2018)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating realistic sequences is a central task in many machine learning\napplications. There has been considerable recent progress on building deep\ngenerative models for sequence generation tasks. However, the issue of\nmode-collapsing remains a main issue for the current models. In this paper we\npropose a GAN-based generic framework to address the problem of mode-collapse\nin a principled approach. We change the standard GAN objective to maximize a\nvariational lower-bound of the log-likelihood while minimizing the\nJensen-Shanon divergence between data and model distributions. We experiment\nour model with text generation task and show that it can generate realistic\ntext with high diversity.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 21:42:13 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Hossam", "Mahmoud", ""], ["Le", "Trung", ""], ["Papasimeon", "Michael", ""], ["Huynh", "Viet", ""], ["Phung", "Dinh", ""]]}, {"id": "2104.13581", "submitter": "Mohammad Mahfujur Rahman", "authors": "Mohammad Mahfujur Rahman, Clinton Fookes, Sridha Sridharan", "title": "Deep Domain Generalization with Feature-norm Network", "comments": "Submitted to Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we tackle the problem of training with multiple source domains\nwith the aim to generalize to new domains at test time without an adaptation\nstep. This is known as domain generalization (DG). Previous works on DG assume\nidentical categories or label space across the source domains. In the case of\ncategory shift among the source domains, previous methods on DG are vulnerable\nto negative transfer due to the large mismatch among label spaces, decreasing\nthe target classification accuracy. To tackle the aforementioned problem, we\nintroduce an end-to-end feature-norm network (FNN) which is robust to negative\ntransfer as it does not need to match the feature distribution among the source\ndomains. We also introduce a collaborative feature-norm network (CFNN) to\nfurther improve the generalization capability of FNN. The CFNN matches the\npredictions of the next most likely categories for each training sample which\nincreases each network's posterior entropy. We apply the proposed FNN and CFNN\nnetworks to the problem of DG for image classification tasks and demonstrate\nsignificant improvement over the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 06:13:47 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Rahman", "Mohammad Mahfujur", ""], ["Fookes", "Clinton", ""], ["Sridharan", "Sridha", ""]]}, {"id": "2104.13617", "submitter": "Alessandro Paolo Capasso", "authors": "Alessandro Paolo Capasso, Paolo Maramotti, Anthony Dell'Eva, Alberto\n  Broggi", "title": "End-to-End Intersection Handling using Multi-Agent Deep Reinforcement\n  Learning", "comments": "IEEE Intelligent Vehicle 2021 (IV 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Navigating through intersections is one of the main challenging tasks for an\nautonomous vehicle. However, for the majority of intersections regulated by\ntraffic lights, the problem could be solved by a simple rule-based method in\nwhich the autonomous vehicle behavior is closely related to the traffic light\nstates. In this work, we focus on the implementation of a system able to\nnavigate through intersections where only traffic signs are provided. We\npropose a multi-agent system using a continuous, model-free Deep Reinforcement\nLearning algorithm used to train a neural network for predicting both the\nacceleration and the steering angle at each time step. We demonstrate that\nagents learn both the basic rules needed to handle intersections by\nunderstanding the priorities of other learners inside the environment, and to\ndrive safely along their paths. Moreover, a comparison between our system and a\nrule-based method proves that our model achieves better results especially with\ndense traffic conditions. Finally, we test our system on real world scenarios\nusing real recorded traffic data, proving that our module is able to generalize\nboth to unseen environments and to different traffic conditions.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 07:54:40 GMT"}, {"version": "v2", "created": "Sat, 1 May 2021 07:17:36 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Capasso", "Alessandro Paolo", ""], ["Maramotti", "Paolo", ""], ["Dell'Eva", "Anthony", ""], ["Broggi", "Alberto", ""]]}, {"id": "2104.13620", "submitter": "Jakob Abe{\\ss}er", "authors": "Jakob Abe{\\ss}er and Saichand Gourishetti and Andr\\'as K\\'atai and\n  Tobias Clau{\\ss} and Prachi Sharma and Judith Liebetrau", "title": "IDMT-Traffic: An Open Benchmark Dataset for Acoustic Traffic Monitoring\n  Research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.AI cs.SD", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In many urban areas, traffic load and noise pollution are constantly\nincreasing. Automated systems for traffic monitoring are promising\ncountermeasures, which allow to systematically quantify and predict local\ntraffic flow in order to to support municipal traffic planning decisions. In\nthis paper, we present a novel open benchmark dataset, containing 2.5 hours of\nstereo audio recordings of 4718 vehicle passing events captured with both\nhigh-quality sE8 and medium-quality MEMS microphones. This dataset is well\nsuited to evaluate the use-case of deploying audio classification algorithms to\nembedded sensor devices with restricted microphone quality and hardware\nprocessing power. In addition, this paper provides a detailed review of recent\nacoustic traffic monitoring (ATM) algorithms as well as the results of two\nbenchmark experiments on vehicle type classification and direction of movement\nestimation using four state-of-the-art convolutional neural network\narchitectures.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 07:58:37 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Abe\u00dfer", "Jakob", ""], ["Gourishetti", "Saichand", ""], ["K\u00e1tai", "Andr\u00e1s", ""], ["Clau\u00df", "Tobias", ""], ["Sharma", "Prachi", ""], ["Liebetrau", "Judith", ""]]}, {"id": "2104.13633", "submitter": "Eunji Jun", "authors": "Eunji Jun, Seungwoo Jeong, Da-Woon Heo, Heung-Il Suk", "title": "Medical Transformer: Universal Brain Encoder for 3D MRI Analysis", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning has gained attention in medical image analysis due to\nlimited annotated 3D medical datasets for training data-driven deep learning\nmodels in the real world. Existing 3D-based methods have transferred the\npre-trained models to downstream tasks, which achieved promising results with\nonly a small number of training samples. However, they demand a massive amount\nof parameters to train the model for 3D medical imaging. In this work, we\npropose a novel transfer learning framework, called Medical Transformer, that\neffectively models 3D volumetric images in the form of a sequence of 2D image\nslices. To make a high-level representation in 3D-form empowering spatial\nrelations better, we take a multi-view approach that leverages plenty of\ninformation from the three planes of 3D volume, while providing\nparameter-efficient training. For building a source model generally applicable\nto various tasks, we pre-train the model in a self-supervised learning manner\nfor masked encoding vector prediction as a proxy task, using a large-scale\nnormal, healthy brain magnetic resonance imaging (MRI) dataset. Our pre-trained\nmodel is evaluated on three downstream tasks: (i) brain disease diagnosis, (ii)\nbrain age prediction, and (iii) brain tumor segmentation, which are actively\nstudied in brain MRI research. The experimental results show that our Medical\nTransformer outperforms the state-of-the-art transfer learning methods,\nefficiently reducing the number of parameters up to about 92% for\nclassification and\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 08:34:21 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Jun", "Eunji", ""], ["Jeong", "Seungwoo", ""], ["Heo", "Da-Woon", ""], ["Suk", "Heung-Il", ""]]}, {"id": "2104.13640", "submitter": "Navid Rekabsaz", "authors": "Navid Rekabsaz and Simone Kopeinik and Markus Schedl", "title": "Societal Biases in Retrieved Contents: Measurement Framework and\n  Adversarial Mitigation for BERT Rankers", "comments": "Accepted at SIGIR 2021", "journal-ref": null, "doi": "10.1145/3404835.3462949", "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Societal biases resonate in the retrieved contents of information retrieval\n(IR) systems, resulting in reinforcing existing stereotypes. Approaching this\nissue requires established measures of fairness in respect to the\nrepresentation of various social groups in retrieval results, as well as\nmethods to mitigate such biases, particularly in the light of the advances in\ndeep ranking models. In this work, we first provide a novel framework to\nmeasure the fairness in the retrieved text contents of ranking models.\nIntroducing a ranker-agnostic measurement, the framework also enables the\ndisentanglement of the effect on fairness of collection from that of rankers.\nTo mitigate these biases, we propose AdvBert, a ranking model achieved by\nadapting adversarial bias mitigation for IR, which jointly learns to predict\nrelevance and remove protected attributes. We conduct experiments on two\npassage retrieval collections (MSMARCO Passage Re-ranking and TREC Deep\nLearning 2019 Passage Re-ranking), which we extend by fairness annotations of a\nselected subset of queries regarding gender attributes. Our results on the\nMSMARCO benchmark show that, (1) all ranking models are less fair in comparison\nwith ranker-agnostic baselines, and (2) the fairness of Bert rankers\nsignificantly improves when using the proposed AdvBert models. Lastly, we\ninvestigate the trade-off between fairness and utility, showing that we can\nmaintain the significant improvements in fairness without any significant loss\nin utility.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 08:53:54 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 07:02:56 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Rekabsaz", "Navid", ""], ["Kopeinik", "Simone", ""], ["Schedl", "Markus", ""]]}, {"id": "2104.13645", "submitter": "Christoph Wernhard", "authors": "Christoph Wernhard and Wolfgang Bibel", "title": "Learning from {\\L}ukasiewicz and Meredith: Investigations into Proof\n  Structures (Extended Version)", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-79876-5_4", "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The material presented in this paper contributes to establishing a basis\ndeemed essential for substantial progress in Automated Deduction. It identifies\nand studies global features in selected problems and their proofs which offer\nthe potential of guiding proof search in a more direct way. The studied\nproblems are of the wide-spread form of \"axiom(s) and rule(s) imply goal(s)\".\nThe features include the well-known concept of lemmas. For their elaboration\nboth human and automated proofs of selected theorems are taken into a close\ncomparative consideration. The study at the same time accounts for a coherent\nand comprehensive formal reconstruction of historical work by {\\L}ukasiewicz,\nMeredith and others. First experiments resulting from the study indicate novel\nways of lemma generation to supplement automated first-order provers of various\nfamilies, strengthening in particular their ability to find short proofs.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 09:09:20 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Wernhard", "Christoph", ""], ["Bibel", "Wolfgang", ""]]}, {"id": "2104.13648", "submitter": "Fei Chen", "authors": "Fei Chen and Xiaodong Wang", "title": "Two stages for visual object tracking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Siamese-based trackers have achived promising performance on visual object\ntracking tasks. Most existing Siamese-based trackers contain two separate\nbranches for tracking, including classification branch and bounding box\nregression branch. In addition, image segmentation provides an alternative way\nto obetain the more accurate target region. In this paper, we propose a novel\ntracker with two-stages: detection and segmentation. The detection stage is\ncapable of locating the target by Siamese networks. Then more accurate tracking\nresults are obtained by segmentation module given the coarse state estimation\nin the first stage. We conduct experiments on four benchmarks. Our approach\nachieves state-of-the-art results, with the EAO of 52.6$\\%$ on VOT2016,\n51.3$\\%$ on VOT2018, and 39.0$\\%$ on VOT2019 datasets, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 09:11:33 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Chen", "Fei", ""], ["Wang", "Xiaodong", ""]]}, {"id": "2104.13718", "submitter": "Jie Chen", "authors": "Jie Chen, Shouzhen Chen, Mingyuan Bai, Jian Pu, Junping Zhang, Junbin\n  Gao", "title": "Graph Decoupling Attention Markov Networks for Semi-supervised Graph\n  Node Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph neural networks (GNN) have been ubiquitous in graph learning tasks such\nas node classification. Most of GNN methods update the node embedding\niteratively by aggregating its neighbors' information. However, they often\nsuffer from negative disturbance, due to edges connecting nodes with different\nlabels. One approach to alleviate this negative disturbance is to use\nattention, but current attention always considers feature similarity and\nsuffers from the lack of supervision. In this paper, we consider the label\ndependency of graph nodes and propose a decoupling attention mechanism to learn\nboth hard and soft attention. The hard attention is learned on labels for a\nrefined graph structure with fewer inter-class edges. Its purpose is to reduce\nthe aggregation's negative disturbance. The soft attention is learned on\nfeatures maximizing the information gain by message passing over better graph\nstructures. Moreover, the learned attention guides the label propagation and\nthe feature propagation. Extensive experiments are performed on five well-known\nbenchmark graph datasets to verify the effectiveness of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 11:44:13 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Chen", "Jie", ""], ["Chen", "Shouzhen", ""], ["Bai", "Mingyuan", ""], ["Pu", "Jian", ""], ["Zhang", "Junping", ""], ["Gao", "Junbin", ""]]}, {"id": "2104.13725", "submitter": "Mohammad Mahfujur Rahman", "authors": "Mohammad Mahfujur Rahman, Clinton Fookes, Sridha Sridharan", "title": "Preserving Semantic Consistency in Unsupervised Domain Adaptation Using\n  Generative Adversarial Networks", "comments": "Submitted to Pattern Recognition Letters", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Unsupervised domain adaptation seeks to mitigate the distribution discrepancy\nbetween source and target domains, given labeled samples of the source domain\nand unlabeled samples of the target domain. Generative adversarial networks\n(GANs) have demonstrated significant improvement in domain adaptation by\nproducing images which are domain specific for training. However, most of the\nexisting GAN based techniques for unsupervised domain adaptation do not\nconsider semantic information during domain matching, hence these methods\ndegrade the performance when the source and target domain data are semantically\ndifferent. In this paper, we propose an end-to-end novel semantic consistent\ngenerative adversarial network (SCGAN). This network can achieve source to\ntarget domain matching by capturing semantic information at the feature level\nand producing images for unsupervised domain adaptation from both the source\nand the target domains. We demonstrate the robustness of our proposed method\nwhich exceeds the state-of-the-art performance in unsupervised domain\nadaptation settings by performing experiments on digit and object\nclassification tasks.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 12:23:30 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Rahman", "Mohammad Mahfujur", ""], ["Fookes", "Clinton", ""], ["Sridharan", "Sridha", ""]]}, {"id": "2104.13733", "submitter": "Chuan Guo", "authors": "Chuan Guo, Alexandre Sablayrolles, Herv\\'e J\\'egou, Douwe Kiela", "title": "Gradient-based Adversarial Attacks against Text Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose the first general-purpose gradient-based attack against\ntransformer models. Instead of searching for a single adversarial example, we\nsearch for a distribution of adversarial examples parameterized by a\ncontinuous-valued matrix, hence enabling gradient-based optimization. We\nempirically demonstrate that our white-box attack attains state-of-the-art\nattack performance on a variety of natural language tasks. Furthermore, we show\nthat a powerful black-box transfer attack, enabled by sampling from the\nadversarial distribution, matches or exceeds existing methods, while only\nrequiring hard-label outputs.\n", "versions": [{"version": "v1", "created": "Thu, 15 Apr 2021 17:43:43 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Guo", "Chuan", ""], ["Sablayrolles", "Alexandre", ""], ["J\u00e9gou", "Herv\u00e9", ""], ["Kiela", "Douwe", ""]]}, {"id": "2104.13773", "submitter": "Amena Khatun", "authors": "Amena Khatun, Simon Denman, Sridha Sridharan, Clinton Fookes", "title": "Pose-driven Attention-guided Image Generation for Person\n  Re-Identification", "comments": "Submitted to Pattern Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Person re-identification (re-ID) concerns the matching of subject images\nacross different camera views in a multi camera surveillance system. One of the\nmajor challenges in person re-ID is pose variations across the camera network,\nwhich significantly affects the appearance of a person. Existing development\ndata lack adequate pose variations to carry out effective training of person\nre-ID systems. To solve this issue, in this paper we propose an end-to-end\npose-driven attention-guided generative adversarial network, to generate\nmultiple poses of a person. We propose to attentively learn and transfer the\nsubject pose through an attention mechanism. A semantic-consistency loss is\nproposed to preserve the semantic information of the person during pose\ntransfer. To ensure fine image details are realistic after pose translation, an\nappearance discriminator is used while a pose discriminator is used to ensure\nthe pose of the transferred images will exactly be the same as the target pose.\nWe show that by incorporating the proposed approach in a person\nre-identification framework, realistic pose transferred images and\nstate-of-the-art re-identification results can be achieved.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 14:02:24 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Khatun", "Amena", ""], ["Denman", "Simon", ""], ["Sridharan", "Sridha", ""], ["Fookes", "Clinton", ""]]}, {"id": "2104.13780", "submitter": "Amena Khatun", "authors": "Amena Khatun, Simon Denman, Sridha Sridharan, Clinton Fookes", "title": "Semantic Consistency and Identity Mapping Multi-Component Generative\n  Adversarial Network for Person Re-Identification", "comments": "Accepted in WACV 2020", "journal-ref": "WACV, 2020", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In a real world environment, person re-identification (Re-ID) is a\nchallenging task due to variations in lighting conditions, viewing angles, pose\nand occlusions. Despite recent performance gains, current person Re-ID\nalgorithms still suffer heavily when encountering these variations. To address\nthis problem, we propose a semantic consistency and identity mapping\nmulti-component generative adversarial network (SC-IMGAN) which provides style\nadaptation from one to many domains. To ensure that transformed images are as\nrealistic as possible, we propose novel identity mapping and semantic\nconsistency losses to maintain identity across the diverse domains. For the\nRe-ID task, we propose a joint verification-identification quartet network\nwhich is trained with generated and real images, followed by an effective\nquartet loss for verification. Our proposed method outperforms state-of-the-art\ntechniques on six challenging person Re-ID datasets: CUHK01, CUHK03, VIPeR,\nPRID2011, iLIDS and Market-1501.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 14:12:29 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Khatun", "Amena", ""], ["Denman", "Simon", ""], ["Sridharan", "Sridha", ""], ["Fookes", "Clinton", ""]]}, {"id": "2104.13791", "submitter": "Giulio Mazzi", "authors": "Giulio Mazzi, Alberto Castellini, Alessandro Farinelli", "title": "Rule-based Shielding for Partially Observable Monte-Carlo Planning", "comments": "arXiv admin note: substantial text overlap with arXiv:2012.12732", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Partially Observable Monte-Carlo Planning (POMCP) is a powerful online\nalgorithm able to generate approximate policies for large Partially Observable\nMarkov Decision Processes. The online nature of this method supports\nscalability by avoiding complete policy representation. The lack of an explicit\nrepresentation however hinders policy interpretability and makes policy\nverification very complex. In this work, we propose two contributions. The\nfirst is a method for identifying unexpected actions selected by POMCP with\nrespect to expert prior knowledge of the task. The second is a shielding\napproach that prevents POMCP from selecting unexpected actions. The first\nmethod is based on Satisfiability Modulo Theory (SMT). It inspects traces\n(i.e., sequences of belief-action-observation triplets) generated by POMCP to\ncompute the parameters of logical formulas about policy properties defined by\nthe expert. The second contribution is a module that uses online the logical\nformulas to identify anomalous actions selected by POMCP and substitutes those\nactions with actions that satisfy the logical formulas fulfilling expert\nknowledge. We evaluate our approach on Tiger, a standard benchmark for POMDPs,\nand a real-world problem related to velocity regulation in mobile robot\nnavigation. Results show that the shielded POMCP outperforms the standard POMCP\nin a case study in which a wrong parameter of POMCP makes it select wrong\nactions from time to time. Moreover, we show that the approach keeps good\nperformance also if the parameters of the logical formula are optimized using\ntrajectories containing some wrong actions.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 14:23:38 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Mazzi", "Giulio", ""], ["Castellini", "Alberto", ""], ["Farinelli", "Alessandro", ""]]}, {"id": "2104.13803", "submitter": "Kevin Bowyer", "authors": "Ying Qiu, V\\'itor Albiero, Michael C. King, Kevin W. Bowyer", "title": "Does Face Recognition Error Echo Gender Classification Error?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is the first to explore the question of whether images that are\nclassified incorrectly by a face analytics algorithm (e.g., gender\nclassification) are any more or less likely to participate in an image pair\nthat results in a face recognition error. We analyze results from three\ndifferent gender classification algorithms (one open-source and two\ncommercial), and two face recognition algorithms (one open-source and one\ncommercial), on image sets representing four demographic groups\n(African-American female and male, Caucasian female and male). For impostor\nimage pairs, our results show that pairs in which one image has a gender\nclassification error have a better impostor distribution than pairs in which\nboth images have correct gender classification, and so are less likely to\ngenerate a false match error. For genuine image pairs, our results show that\nindividuals whose images have a mix of correct and incorrect gender\nclassification have a worse genuine distribution (increased false non-match\nrate) compared to individuals whose images all have correct gender\nclassification. Thus, compared to images that generate correct gender\nclassification, images that generate gender classification errors do generate a\ndifferent pattern of recognition errors, both better (false match) and worse\n(false non-match).\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 14:43:31 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Qiu", "Ying", ""], ["Albiero", "V\u00edtor", ""], ["King", "Michael C.", ""], ["Bowyer", "Kevin W.", ""]]}, {"id": "2104.13840", "submitter": "Xiangxiang Chu", "authors": "Xiangxiang Chu and Zhi Tian and Yuqing Wang and Bo Zhang and Haibing\n  Ren and Xiaolin Wei and Huaxia Xia and Chunhua Shen", "title": "Twins: Revisiting the Design of Spatial Attention in Vision Transformers", "comments": "Two simple and effective designs of vision transformer, which is on\n  par with the Swin transformer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Very recently, a variety of vision transformer architectures for dense\nprediction tasks have been proposed and they show that the design of spatial\nattention is critical to their success in these tasks. In this work, we revisit\nthe design of the spatial attention and demonstrate that a carefully-devised\nyet simple spatial attention mechanism performs favourably against the\nstate-of-the-art schemes. As a result, we propose two vision transformer\narchitectures, namely, Twins-PCPVT and Twins-SVT. Our proposed architectures\nare highly-efficient and easy to implement, only involving matrix\nmultiplications that are highly optimized in modern deep learning frameworks.\nMore importantly, the proposed architectures achieve excellent performance on a\nwide range of visual tasks including imagelevel classification as well as dense\ndetection and segmentation. The simplicity and strong performance suggest that\nour proposed architectures may serve as stronger backbones for many vision\ntasks. Our code will be released soon at\nhttps://github.com/Meituan-AutoML/Twins .\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 15:42:31 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 15:41:36 GMT"}, {"version": "v3", "created": "Fri, 11 Jun 2021 11:54:21 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Chu", "Xiangxiang", ""], ["Tian", "Zhi", ""], ["Wang", "Yuqing", ""], ["Zhang", "Bo", ""], ["Ren", "Haibing", ""], ["Wei", "Xiaolin", ""], ["Xia", "Huaxia", ""], ["Shen", "Chunhua", ""]]}, {"id": "2104.13844", "submitter": "Andrew Patterson", "authors": "Andrew Patterson, Adam White, Sina Ghiassian, Martha White", "title": "A Generalized Projected Bellman Error for Off-policy Value Estimation in\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many reinforcement learning algorithms rely on value estimation. However, the\nmost widely used algorithms -- namely temporal difference algorithms -- can\ndiverge under both off-policy sampling and nonlinear function approximation.\nMany algorithms have been developed for off-policy value estimation which are\nsound under linear function approximation, based on the linear mean-squared\nprojected Bellman error (PBE). Extending these methods to the non-linear case\nhas been largely unsuccessful. Recently, several methods have been introduced\nthat approximate a different objective, called the mean-squared Bellman error\n(BE), which naturally facilities nonlinear approximation. In this work, we\nbuild on these insights and introduce a new generalized PBE, that extends the\nlinear PBE to the nonlinear setting. We show how this generalized objective\nunifies previous work, including previous theory, and obtain new bounds for the\nvalue error of the solutions of the generalized objective. We derive an\neasy-to-use, but sound, algorithm to minimize the generalized objective which\nis more stable across runs, is less sensitive to hyperparameters, and performs\nfavorably across four control domains with neural network function\napproximation.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 15:50:34 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Patterson", "Andrew", ""], ["White", "Adam", ""], ["Ghiassian", "Sina", ""], ["White", "Martha", ""]]}, {"id": "2104.13877", "submitter": "Michael Zhang", "authors": "Michael R. Zhang, Tom Le Paine, Ofir Nachum, Cosmin Paduraru, George\n  Tucker, Ziyu Wang, Mohammad Norouzi", "title": "Autoregressive Dynamics Models for Offline Policy Evaluation and\n  Optimization", "comments": "ICLR 2021. 17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard dynamics models for continuous control make use of feedforward\ncomputation to predict the conditional distribution of next state and reward\ngiven current state and action using a multivariate Gaussian with a diagonal\ncovariance structure. This modeling choice assumes that different dimensions of\nthe next state and reward are conditionally independent given the current state\nand action and may be driven by the fact that fully observable physics-based\nsimulation environments entail deterministic transition dynamics. In this\npaper, we challenge this conditional independence assumption and propose a\nfamily of expressive autoregressive dynamics models that generate different\ndimensions of the next state and reward sequentially conditioned on previous\ndimensions. We demonstrate that autoregressive dynamics models indeed\noutperform standard feedforward models in log-likelihood on heldout\ntransitions. Furthermore, we compare different model-based and model-free\noff-policy evaluation (OPE) methods on RL Unplugged, a suite of offline MuJoCo\ndatasets, and find that autoregressive dynamics models consistently outperform\nall baselines, achieving a new state-of-the-art. Finally, we show that\nautoregressive dynamics models are useful for offline policy optimization by\nserving as a way to enrich the replay buffer through data augmentation and\nimproving performance using model-based planning.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 16:48:44 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Zhang", "Michael R.", ""], ["Paine", "Tom Le", ""], ["Nachum", "Ofir", ""], ["Paduraru", "Cosmin", ""], ["Tucker", "George", ""], ["Wang", "Ziyu", ""], ["Norouzi", "Mohammad", ""]]}, {"id": "2104.13901", "submitter": "Alex Devonport", "authors": "Alex Devonport, Adnane Saoud, and Murat Arcak", "title": "Symbolic Abstractions From Data: A PAC Learning Approach", "comments": "8 pages, 2 figures. Submitted to IEEE CDC 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symbolic control techniques aim to satisfy complex logic specifications. A\ncritical step in these techniques is the construction of a symbolic (discrete)\nabstraction, a finite-state system whose behaviour mimics that of a given\ncontinuous-state system. The methods used to compute symbolic abstractions,\nhowever, require knowledge of an accurate closed-form model. To generalize them\nto systems with unknown dynamics, we present a new data-driven approach that\ndoes not require closed-form dynamics, instead relying only the ability to\nevaluate successors of each state under given inputs. To provide guarantees for\nthe learned abstraction, we use the Probably Approximately Correct (PAC)\nstatistical framework. We first introduce a PAC-style behavioural relationship\nand an appropriate refinement procedure. We then show how the symbolic\nabstraction can be constructed to satisfy this new behavioural relationship.\nMoreover, we provide PAC bounds that dictate the number of data required to\nguarantee a prescribed level of accuracy and confidence. Finally, we present an\nillustrative example.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 17:34:28 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Devonport", "Alex", ""], ["Saoud", "Adnane", ""], ["Arcak", "Murat", ""]]}, {"id": "2104.13921", "submitter": "Xiuye Gu", "authors": "Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, Yin Cui", "title": "Zero-Shot Detection via Vision and Language Knowledge Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot image classification has made promising progress by training the\naligned image and text encoders. The goal of this work is to advance zero-shot\nobject detection, which aims to detect novel objects without bounding box nor\nmask annotations. We propose ViLD, a training method via Vision and Language\nknowledge Distillation. We distill the knowledge from a pre-trained zero-shot\nimage classification model (e.g., CLIP) into a two-stage detector (e.g., Mask\nR-CNN). Our method aligns the region embeddings in the detector to the text and\nimage embeddings inferred by the pre-trained model. We use the text embeddings\nas the detection classifier, obtained by feeding category names into the\npre-trained text encoder. We then minimize the distance between the region\nembeddings and image embeddings, obtained by feeding region proposals into the\npre-trained image encoder. During inference, we include text embeddings of\nnovel categories into the detection classifier for zero-shot detection. We\nbenchmark the performance on LVIS dataset by holding out all rare categories as\nnovel categories. ViLD obtains 16.1 mask AP$_r$ with a Mask R-CNN (ResNet-50\nFPN) for zero-shot detection, outperforming the supervised counterpart by 3.8.\nThe model can directly transfer to other datasets, achieving 72.2 AP$_{50}$,\n36.6 AP and 11.8 AP on PASCAL VOC, COCO and Objects365, respectively.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 17:58:57 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Gu", "Xiuye", ""], ["Lin", "Tsung-Yi", ""], ["Kuo", "Weicheng", ""], ["Cui", "Yin", ""]]}, {"id": "2104.13933", "submitter": "Tianze Shi", "authors": "Tianze Shi, Ozan \\.Irsoy, Igor Malioutov, Lillian Lee", "title": "Learning Syntax from Naturally-Occurring Bracketings", "comments": "NAACL 2021", "journal-ref": "In Proceedings of NAACL 2021", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Naturally-occurring bracketings, such as answer fragments to natural language\nquestions and hyperlinks on webpages, can reflect human syntactic intuition\nregarding phrasal boundaries. Their availability and approximate correspondence\nto syntax make them appealing as distant information sources to incorporate\ninto unsupervised constituency parsing. But they are noisy and incomplete; to\naddress this challenge, we develop a partial-brackets-aware structured ramp\nloss in learning. Experiments demonstrate that our distantly-supervised models\ntrained on naturally-occurring bracketing data are more accurate in inducing\nsyntactic structures than competing unsupervised systems. On the English WSJ\ncorpus, our models achieve an unlabeled F1 score of 68.9 for constituency\nparsing.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 18:00:02 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Shi", "Tianze", ""], ["\u0130rsoy", "Ozan", ""], ["Malioutov", "Igor", ""], ["Lee", "Lillian", ""]]}, {"id": "2104.13936", "submitter": "Tianze Shi", "authors": "Tianze Shi, Adrian Benton, Igor Malioutov, Ozan \\.Irsoy", "title": "Diversity-Aware Batch Active Learning for Dependency Parsing", "comments": "NAACL 2021", "journal-ref": "In Proceedings of NAACL 2021", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the predictive performance of modern statistical dependency parsers\nrelies heavily on the availability of expensive expert-annotated treebank data,\nnot all annotations contribute equally to the training of the parsers. In this\npaper, we attempt to reduce the number of labeled examples needed to train a\nstrong dependency parser using batch active learning (AL). In particular, we\ninvestigate whether enforcing diversity in the sampled batches, using\ndeterminantal point processes (DPPs), can improve over their diversity-agnostic\ncounterparts. Simulation experiments on an English newswire corpus show that\nselecting diverse batches with DPPs is superior to strong selection strategies\nthat do not enforce batch diversity, especially during the initial stages of\nthe learning process. Additionally, our diversityaware strategy is robust under\na corpus duplication setting, where diversity-agnostic sampling strategies\nexhibit significant degradation.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 18:00:05 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Shi", "Tianze", ""], ["Benton", "Adrian", ""], ["Malioutov", "Igor", ""], ["\u0130rsoy", "Ozan", ""]]}, {"id": "2104.13948", "submitter": "Ekaterina Zolotareva", "authors": "Ekaterina Zolotareva", "title": "Applying Convolutional Neural Networks for Stock Market Trends\n  Identification", "comments": "22 pages, 8 figures, 6 tables. This paper is the full text of the\n  research, presented at the 20th International Conference on Artificial\n  Intelligence and Soft Computing Web System (ICAISC 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.AI cs.IT math.IT", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper we apply a specific type ANNs - convolutional neural networks\n(CNNs) - to the problem of finding start and endpoints of trends, which are the\noptimal points for entering and leaving the market. We aim to explore long-term\ntrends, which last several months, not days. The key distinction of our model\nis that its labels are fully based on expert opinion data. Despite the various\nmodels based solely on stock price data, some market experts still argue that\ntraders are able to see hidden opportunities. The labelling was done via the\nGUI interface, which means that the experts worked directly with images, not\nnumerical data. This fact makes CNN the natural choice of algorithm. The\nproposed framework requires the sequential interaction of three CNN submodels,\nwhich identify the presence of a changepoint in a window, locate it and finally\nrecognize the type of new tendency - upward, downward or flat. These submodels\nhave certain pitfalls, therefore the calibration of their hyperparameters is\nthe main direction of further research. The research addresses such issues as\nimbalanced datasets and contradicting labels, as well as the need for specific\nquality metrics to keep up with practical applicability. This paper is the full\ntext of the research, presented at the 20th International Conference on\nArtificial Intelligence and Soft Computing Web System (ICAISC 2021)\n", "versions": [{"version": "v1", "created": "Mon, 19 Apr 2021 14:43:29 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Zolotareva", "Ekaterina", ""]]}, {"id": "2104.13950", "submitter": "Zafiirah Hosenie", "authors": "Zafiirah Hosenie, Steven Bloemen, Paul Groot, Robert Lyon, Bart\n  Scheers, Benjamin Stappers, Fiorenzo Stoppa, Paul Vreeswijk, Simon De Wet,\n  Marc Klein Wolt, Elmar K\\\"ording, Vanessa McBride, Rudolf Le Poole, Kerry\n  Paterson, Dani\\\"elle L. A. Pieterse and Patrick Woudt", "title": "MeerCRAB: MeerLICHT Classification of Real and Bogus Transients using\n  Deep Learning", "comments": "15 pages, 13 figures, Accepted for publication in Experimental\n  Astronomy and appeared in the 3rd Workshop on Machine Learning and the\n  Physical Sciences, NeurIPS 2020", "journal-ref": "Exp Astron (2021)", "doi": "10.1007/s10686-021-09757-1", "report-no": null, "categories": "astro-ph.IM astro-ph.GA cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Astronomers require efficient automated detection and classification\npipelines when conducting large-scale surveys of the (optical) sky for variable\nand transient sources. Such pipelines are fundamentally important, as they\npermit rapid follow-up and analysis of those detections most likely to be of\nscientific value. We therefore present a deep learning pipeline based on the\nconvolutional neural network architecture called $\\texttt{MeerCRAB}$. It is\ndesigned to filter out the so called 'bogus' detections from true astrophysical\nsources in the transient detection pipeline of the MeerLICHT telescope. Optical\ncandidates are described using a variety of 2D images and numerical features\nextracted from those images. The relationship between the input images and the\ntarget classes is unclear, since the ground truth is poorly defined and often\nthe subject of debate. This makes it difficult to determine which source of\ninformation should be used to train a classification algorithm. We therefore\nused two methods for labelling our data (i) thresholding and (ii) latent class\nmodel approaches. We deployed variants of $\\texttt{MeerCRAB}$ that employed\ndifferent network architectures trained using different combinations of input\nimages and training set choices, based on classification labels provided by\nvolunteers. The deepest network worked best with an accuracy of 99.5$\\%$ and\nMatthews correlation coefficient (MCC) value of 0.989. The best model was\nintegrated to the MeerLICHT transient vetting pipeline, enabling the accurate\nand efficient classification of detected transients that allows researchers to\nselect the most promising candidates for their research goals.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 18:12:51 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Hosenie", "Zafiirah", ""], ["Bloemen", "Steven", ""], ["Groot", "Paul", ""], ["Lyon", "Robert", ""], ["Scheers", "Bart", ""], ["Stappers", "Benjamin", ""], ["Stoppa", "Fiorenzo", ""], ["Vreeswijk", "Paul", ""], ["De Wet", "Simon", ""], ["Wolt", "Marc Klein", ""], ["K\u00f6rding", "Elmar", ""], ["McBride", "Vanessa", ""], ["Poole", "Rudolf Le", ""], ["Paterson", "Kerry", ""], ["Pieterse", "Dani\u00eblle L. A.", ""], ["Woudt", "Patrick", ""]]}, {"id": "2104.13963", "submitter": "Mahmoud Assran", "authors": "Mahmoud Assran, Mathilde Caron, Ishan Misra, Piotr Bojanowski, Armand\n  Joulin, Nicolas Ballas, Michael Rabbat", "title": "Semi-Supervised Learning of Visual Features by Non-Parametrically\n  Predicting View Assignments with Support Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel method of learning by predicting view assignments\nwith support samples (PAWS). The method trains a model to minimize a\nconsistency loss, which ensures that different views of the same unlabeled\ninstance are assigned similar pseudo-labels. The pseudo-labels are generated\nnon-parametrically, by comparing the representations of the image views to\nthose of a set of randomly sampled labeled images. The distance between the\nview representations and labeled representations is used to provide a weighting\nover class labels, which we interpret as a soft pseudo-label. By\nnon-parametrically incorporating labeled samples in this way, PAWS extends the\ndistance-metric loss used in self-supervised methods such as BYOL and SwAV to\nthe semi-supervised setting. Despite the simplicity of the approach, PAWS\noutperforms other semi-supervised methods across architectures, setting a new\nstate-of-the-art for a ResNet-50 on ImageNet trained with either 10% or 1% of\nthe labels, reaching 75.5% and 66.5% top-1 respectively. PAWS requires 4x to\n12x less training than the previous best methods.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 18:44:07 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 03:02:09 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Assran", "Mahmoud", ""], ["Caron", "Mathilde", ""], ["Misra", "Ishan", ""], ["Bojanowski", "Piotr", ""], ["Joulin", "Armand", ""], ["Ballas", "Nicolas", ""], ["Rabbat", "Michael", ""]]}, {"id": "2104.13968", "submitter": "Gurpreet Singh", "authors": "Gurpreet Singh and Soumyajit Gupta", "title": "Tail-Net: Extracting Lowest Singular Triplets for Big Data Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  SVD serves as an exploratory tool in identifying the dominant features in the\nform of top rank-r singular factors corresponding to the largest singular\nvalues. For Big Data applications it is well known that Singular Value\nDecomposition (SVD) is restrictive due to main memory requirements. However, a\nnumber of applications such as community detection, clustering, or bottleneck\nidentification in large scale graph data-sets rely upon identifying the lowest\nsingular values and the singular corresponding vectors. For example, the lowest\nsingular values of a graph Laplacian reveal the number of isolated clusters\n(zero singular values) or bottlenecks (lowest non-zero singular values) for\nundirected, acyclic graphs. A naive approach here would be to perform a full\nSVD however, this quickly becomes infeasible for practical big data\napplications due to the enormous memory requirements. Furthermore, for such\napplications only a few lowest singular factors are desired making a full\ndecomposition computationally exorbitant. In this work, we trivially extend the\npreviously proposed Range-Net to \\textbf{Tail-Net} for a memory and compute\nefficient extraction of lowest singular factors of a given big dataset and a\nspecified rank-r. We present a number of numerical experiments on both\nsynthetic and practical data-sets for verification and bench-marking using\nconventional SVD as the baseline.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 19:17:34 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Singh", "Gurpreet", ""], ["Gupta", "Soumyajit", ""]]}, {"id": "2104.13997", "submitter": "Sheng-Chun Kao", "authors": "Sheng-Chun Kao, Tushar Krishna", "title": "Domain-specific Genetic Algorithm for Multi-tenant DNNAccelerator\n  Scheduling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As Deep Learning continues to drive a variety of applications in datacenters\nand HPC, there is a growing trend towards building large accelerators with\nseveral sub-accelerator cores/chiplets. This work looks at the problem of\nsupporting multi-tenancy on such accelerators. In particular, we focus on the\nproblem of mapping layers from several DNNs simultaneously on an accelerator.\nGiven the extremely large search space, we formulate the search as an\noptimization problem and develop a specialized genetic algorithm called G#\nwithcustom operators to enable structured sample-efficient exploration. We\nquantitatively compare G# with several common heuristics, state-of-the-art\noptimization methods, and reinforcement learning methods across different\naccelerator set-tings (large/small accelerators) and different sub-accelerator\nconfigurations (homogeneous/heterogeneous), and observeG# can consistently find\nbetter solutions. Further, to enable real-time scheduling, we also demonstrate\na method to generalize the learnt schedules and transfer them to the next batch\nof jobs, reducing schedule compute time to near zero.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 19:57:55 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 14:41:36 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Kao", "Sheng-Chun", ""], ["Krishna", "Tushar", ""]]}, {"id": "2104.14029", "submitter": "Krishanu Sarker", "authors": "Krishanu Sarker, Sharbani Pandit, Anupam Sarker, Saeid Belkasim and\n  Shihao Ji", "title": "Reducing Risk and Uncertainty of Deep Neural Networks on Diagnosing\n  COVID-19 Infection", "comments": "AAAI, TAIH workshop, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Effective and reliable screening of patients via Computer-Aided Diagnosis can\nplay a crucial part in the battle against COVID-19. Most of the existing works\nfocus on developing sophisticated methods yielding high detection performance,\nyet not addressing the issue of predictive uncertainty. In this work, we\nintroduce uncertainty estimation to detect confusing cases for expert referral\nto address the unreliability of state-of-the-art (SOTA) DNNs on COVID-19\ndetection. To the best of our knowledge, we are the first to address this issue\non the COVID-19 detection problem. In this work, we investigate a number of\nSOTA uncertainty estimation methods on publicly available COVID dataset and\npresent our experimental findings. In collaboration with medical professionals,\nwe further validate the results to ensure the viability of the best performing\nmethod in clinical practice.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 21:36:25 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Sarker", "Krishanu", ""], ["Pandit", "Sharbani", ""], ["Sarker", "Anupam", ""], ["Belkasim", "Saeid", ""], ["Ji", "Shihao", ""]]}, {"id": "2104.14040", "submitter": "Kuo-Hao Zeng", "authors": "Kuo-Hao Zeng, Luca Weihs, Ali Farhadi, Roozbeh Mottaghi", "title": "Pushing it out of the Way: Interactive Visual Navigation", "comments": "14 pages, 13 figures, CVPR 2021,\n  https://prior.allenai.org/projects/interactive-visual-navigation,\n  https://youtu.be/GvTI5XCMvPw", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have observed significant progress in visual navigation for embodied\nagents. A common assumption in studying visual navigation is that the\nenvironments are static; this is a limiting assumption. Intelligent navigation\nmay involve interacting with the environment beyond just moving\nforward/backward and turning left/right. Sometimes, the best way to navigate is\nto push something out of the way. In this paper, we study the problem of\ninteractive navigation where agents learn to change the environment to navigate\nmore efficiently to their goals. To this end, we introduce the Neural\nInteraction Engine (NIE) to explicitly predict the change in the environment\ncaused by the agent's actions. By modeling the changes while planning, we find\nthat agents exhibit significant improvements in their navigational\ncapabilities. More specifically, we consider two downstream tasks in the\nphysics-enabled, visually rich, AI2-THOR environment: (1) reaching a target\nwhile the path to the target is blocked (2) moving an object to a target\nlocation by pushing it. For both tasks, agents equipped with an NIE\nsignificantly outperform agents without the understanding of the effect of the\nactions indicating the benefits of our approach.\n", "versions": [{"version": "v1", "created": "Wed, 28 Apr 2021 22:46:41 GMT"}, {"version": "v2", "created": "Sun, 2 May 2021 00:41:22 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Zeng", "Kuo-Hao", ""], ["Weihs", "Luca", ""], ["Farhadi", "Ali", ""], ["Mottaghi", "Roozbeh", ""]]}, {"id": "2104.14056", "submitter": "Safa Omri", "authors": "Safa Omri and Carsten Sinz", "title": "Machine Learning Techniques for Software Quality Assurance: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Over the last years, machine learning techniques have been applied to more\nand more application domains, including software engineering and, especially,\nsoftware quality assurance. Important application domains have been, e.g.,\nsoftware defect prediction or test case selection and prioritization. The\nability to predict which components in a large software system are most likely\nto contain the largest numbers of faults in the next release helps to better\nmanage projects, including early estimation of possible release delays, and\naffordably guide corrective actions to improve the quality of the software.\nHowever, developing robust fault prediction models is a challenging task and\nmany techniques have been proposed in the literature. Closely related to\nestimating defect-prone parts of a software system is the question of how to\nselect and prioritize test cases, and indeed test case prioritization has been\nextensively researched as a means for reducing the time taken to discover\nregressions in software. In this survey, we discuss various approaches in both\nfault prediction and test case prioritization, also explaining how in recent\nstudies deep learning algorithms for fault prediction help to bridge the gap\nbetween programs' semantics and fault prediction features. We also review\nrecently proposed machine learning methods for test case prioritization (TCP),\nand their ability to reduce the cost of regression testing without negatively\naffecting fault detection capabilities.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 00:37:27 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Omri", "Safa", ""], ["Sinz", "Carsten", ""]]}, {"id": "2104.14060", "submitter": "Yunxiang Zhao", "authors": "Yunxiang Zhao and Jianzhong Qi and Qingwei Liu and Rui Zhang", "title": "WGCN: Graph Convolutional Networks with Weighted Structural Features", "comments": null, "journal-ref": null, "doi": "10.1145/3404835.3462834", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph structural information such as topologies or connectivities provides\nvaluable guidance for graph convolutional networks (GCNs) to learn nodes'\nrepresentations. Existing GCN models that capture nodes' structural information\nweight in- and out-neighbors equally or differentiate in- and out-neighbors\nglobally without considering nodes' local topologies. We observe that in- and\nout-neighbors contribute differently for nodes with different local topologies.\nTo explore the directional structural information for different nodes, we\npropose a GCN model with weighted structural features, named WGCN. WGCN first\ncaptures nodes' structural fingerprints via a direction and degree aware Random\nWalk with Restart algorithm, where the walk is guided by both edge direction\nand nodes' in- and out-degrees. Then, the interactions between nodes'\nstructural fingerprints are used as the weighted node structural features. To\nfurther capture nodes' high-order dependencies and graph geometry, WGCN embeds\ngraphs into a latent space to obtain nodes' latent neighbors and geometrical\nrelationships. Based on nodes' geometrical relationships in the latent space,\nWGCN differentiates latent, in-, and out-neighbors with an attention-based\ngeometrical aggregation. Experiments on transductive node classification tasks\nshow that WGCN outperforms the baseline models consistently by up to 17.07% in\nterms of accuracy on five benchmark datasets.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 00:50:06 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 14:17:20 GMT"}, {"version": "v3", "created": "Wed, 21 Jul 2021 05:07:21 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Zhao", "Yunxiang", ""], ["Qi", "Jianzhong", ""], ["Liu", "Qingwei", ""], ["Zhang", "Rui", ""]]}, {"id": "2104.14067", "submitter": "Mirko Marras", "authors": "Gianni Fenu, Giacomo Medda, Mirko Marras, and Giacomo Meloni", "title": "Improving Fairness in Speaker Recognition", "comments": "Accepted at the 2020 European Symposium on Software Engineering (ESSE\n  2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human voice conveys unique characteristics of an individual, making voice\nbiometrics a key technology for verifying identities in various industries.\nDespite the impressive progress of speaker recognition systems in terms of\naccuracy, a number of ethical and legal concerns has been raised, specifically\nrelating to the fairness of such systems. In this paper, we aim to explore the\ndisparity in performance achieved by state-of-the-art deep speaker recognition\nsystems, when different groups of individuals characterized by a common\nsensitive attribute (e.g., gender) are considered. In order to mitigate the\nunfairness we uncovered by means of an exploratory study, we investigate\nwhether balancing the representation of the different groups of individuals in\nthe training set can lead to a more equal treatment of these demographic\ngroups. Experiments on two state-of-the-art neural architectures and a\nlarge-scale public dataset show that models trained with\ndemographically-balanced training sets exhibit a fairer behavior on different\ngroups, while still being accurate. Our study is expected to provide a solid\nbasis for instilling beyond-accuracy objectives (e.g., fairness) in speaker\nrecognition.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 01:08:53 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 20:36:28 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Fenu", "Gianni", ""], ["Medda", "Giacomo", ""], ["Marras", "Mirko", ""], ["Meloni", "Giacomo", ""]]}, {"id": "2104.14073", "submitter": "Renjie Li", "authors": "Renjie Li, Xinyi Wang, Katherine Lawler, Saurabh Garg, Quan Bai, Jane\n  Alty", "title": "Applications of Artificial Intelligence to aid detection of dementia: a\n  narrative review on current capabilities and future directions", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With populations ageing, the number of people with dementia worldwide is\nexpected to triple to 152 million by 2050. Seventy percent of cases are due to\nAlzheimer's disease (AD) pathology and there is a 10-20 year 'pre-clinical'\nperiod before significant cognitive decline occurs. We urgently need, cost\neffective, objective methods to detect AD, and other dementias, at an early\nstage. Risk factor modification could prevent 40% of cases and drug trials\nwould have greater chances of success if participants are recruited at an\nearlier stage. Currently, detection of dementia is largely by pen and paper\ncognitive tests but these are time consuming and insensitive to pre-clinical\nphases. Specialist brain scans and body fluid biomarkers can detect the\nearliest stages of dementia but are too invasive or expensive for widespread\nuse. With the advancement of technology, Artificial Intelligence (AI) shows\npromising results in assisting with detection of early-stage dementia. Existing\nAI-aided methods and potential future research directions are reviewed and\ndiscussed.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 01:54:36 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Li", "Renjie", ""], ["Wang", "Xinyi", ""], ["Lawler", "Katherine", ""], ["Garg", "Saurabh", ""], ["Bai", "Quan", ""], ["Alty", "Jane", ""]]}, {"id": "2104.14088", "submitter": "Zixian An", "authors": "Yunkai Wei, Zixian An, Supeng Leng and Kun Yang", "title": "Connecting AI Learning and Blockchain Mining in 6G Systems", "comments": "7 pages, 6 figures, submitted to IEEE Communications Magazine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The sixth generation (6G) systems are generally recognized to be established\non ubiquitous Artificial Intelligence (AI) and distributed ledger such as\nblockchain. However, the AI training demands tremendous computing resource,\nwhich is limited in most 6G devices. Meanwhile, miners in Proof-of-Work (PoW)\nbased blockchains devote massive computing power to block mining, and are\nwidely criticized for the waste of computation. To address this dilemma, we\npropose an Evolved-Proof-of-Work (E-PoW) consensus that can integrate the\nmatrix computations, which are widely existed in AI training, into the process\nof brute-force searches in the block mining. Consequently, E-PoW can connect AI\nlearning and block mining via the multiply used common computing resource.\nExperimental results show that E-PoW can salvage by up to 80 percent computing\npower from pure block mining for parallel AI training in 6G systems.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 03:19:52 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Wei", "Yunkai", ""], ["An", "Zixian", ""], ["Leng", "Supeng", ""], ["Yang", "Kun", ""]]}, {"id": "2104.14089", "submitter": "Ronal Singh", "authors": "Ronal Singh, Tim Miller, Darryn Reid", "title": "Collaborative Human-Agent Planning for Resilience", "comments": "International Workshop on Coordination, Organizations, Institutions,\n  Norms and Ethics for Governance of Multi-Agent Systems (COINE), co-located\n  with AAMAS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent agents powered by AI planning assist people in complex scenarios,\nsuch as managing teams of semi-autonomous vehicles. However, AI planning models\nmay be incomplete, leading to plans that do not adequately meet the stated\nobjectives, especially in unpredicted situations. Humans, who are apt at\nidentifying and adapting to unusual situations, may be able to assist planning\nagents in these situations by encoding their knowledge into a planner at\nrun-time. We investigate whether people can collaborate with agents by\nproviding their knowledge to an agent using linear temporal logic (LTL) at\nrun-time without changing the agent's domain model. We presented 24\nparticipants with baseline plans for situations in which a planner had\nlimitations, and asked the participants for workarounds for these limitations.\nWe encoded these workarounds as LTL constraints. Results show that\nparticipants' constraints improved the expected return of the plans by 10% ($p\n< 0.05$) relative to baseline plans, demonstrating that human insight can be\nused in collaborative planning for resilience. However, participants used more\ndeclarative than control constraints over time, but declarative constraints\nproduced plans less similar to the expectation of the participants, which could\nlead to potential trust issues.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 03:21:31 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Singh", "Ronal", ""], ["Miller", "Tim", ""], ["Reid", "Darryn", ""]]}, {"id": "2104.14095", "submitter": "Somak Aditya", "authors": "Vishesh Agarwal, Somak Aditya, Navin Goyal", "title": "Analyzing the Nuances of Transformers' Polynomial Simplification\n  Abilities", "comments": "16 pages, 18 Tables, Accepted ICLR 2021 MathAI Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Symbolic Mathematical tasks such as integration often require multiple\nwell-defined steps and understanding of sub-tasks to reach a solution. To\nunderstand Transformers' abilities in such tasks in a fine-grained manner, we\ndeviate from traditional end-to-end settings, and explore a step-wise\npolynomial simplification task. Polynomials can be written in a simple normal\nform as a sum of monomials which are ordered in a lexicographic order. For a\npolynomial which is not necessarily in this normal form, a sequence of\nsimplification steps is applied to reach the fully simplified (i.e., in the\nnormal form) polynomial. We propose a synthetic Polynomial dataset generation\nalgorithm that generates polynomials with unique proof steps. Through varying\ncoefficient configurations, input representation, proof granularity, and\nextensive hyper-parameter tuning, we observe that Transformers consistently\nstruggle with numeric multiplication. We explore two ways to mitigate this:\nCurriculum Learning and a Symbolic Calculator approach (where the numeric\noperations are offloaded to a calculator). Both approaches provide significant\ngains over the vanilla Transformers-based baseline.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 03:52:46 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Agarwal", "Vishesh", ""], ["Aditya", "Somak", ""], ["Goyal", "Navin", ""]]}, {"id": "2104.14098", "submitter": "S. Akshay", "authors": "Preey Shah, Aman Bansal, S. Akshay and Supratik Chakraborty", "title": "A Normal Form Characterization for Efficient Boolean Skolem Function\n  Synthesis", "comments": "Full version of conference paper accepted at LICS'2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Boolean Skolem function synthesis concerns synthesizing outputs as Boolean\nfunctions of inputs such that a relational specification between inputs and\noutputs is satisfied. This problem, also known as Boolean functional synthesis,\nhas several applications, including design of safe controllers for autonomous\nsystems, certified QBF solving, cryptanalysis etc. Recently, complexity\ntheoretic hardness results have been shown for the problem, although several\nalgorithms proposed in the literature are known to work well in practice. This\ndichotomy between theoretical hardness and practical efficacy has motivated the\nresearch into normal forms or representations of input specifications that\npermit efficient synthesis, thus explaining perhaps the efficacy of these\nalgorithms.\n  In this paper we go one step beyond this and ask if there exists a normal\nform representation that can in fact precisely characterize \"efficient\"\nsynthesis. We present a normal form called SAUNF that precisely characterizes\ntractable synthesis in the following sense: a specification is polynomial time\nsynthesizable iff it can be compiled to SAUNF in polynomial time. Additionally,\na specification admits a polynomial-sized functional solution iff there exists\na semantically equivalent polynomial-sized SAUNF representation. SAUNF is\nexponentially more succinct than well-established normal forms like BDDs and\nDNNFs, used in the context of AI problems, and strictly subsumes other more\nrecently proposed forms like SynNNF. It enjoys compositional properties that\nare similar to those of DNNF. Thus, SAUNF provides the right trade-off in\nknowledge representation for Boolean functional synthesis.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 04:16:41 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 12:52:38 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Shah", "Preey", ""], ["Bansal", "Aman", ""], ["Akshay", "S.", ""], ["Chakraborty", "Supratik", ""]]}, {"id": "2104.14102", "submitter": "Shravan Murlidaran", "authors": "Shravan Murlidaran, William Yang Wang, Miguel P. Eckstein", "title": "Comparing Visual Reasoning in Humans and AI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in natural language processing and computer vision have led\nto AI models that interpret simple scenes at human levels. Yet, we do not have\na complete understanding of how humans and AI models differ in their\ninterpretation of more complex scenes. We created a dataset of complex scenes\nthat contained human behaviors and social interactions. AI and humans had to\ndescribe the scenes with a sentence. We used a quantitative metric of\nsimilarity between scene descriptions of the AI/human and ground truth of five\nother human descriptions of each scene. Results show that the machine/human\nagreement scene descriptions are much lower than human/human agreement for our\ncomplex scenes. Using an experimental manipulation that occludes different\nspatial regions of the scenes, we assessed how machines and humans vary in\nutilizing regions of images to understand the scenes. Together, our results are\na first step toward understanding how machines fall short of human visual\nreasoning with complex scenes depicting human behaviors.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 04:44:13 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Murlidaran", "Shravan", ""], ["Wang", "William Yang", ""], ["Eckstein", "Miguel P.", ""]]}, {"id": "2104.14173", "submitter": "Yunwen Lei", "authors": "Liang Wu, Antoine Ledent, Yunwen Lei, Marius Kloft", "title": "Fine-grained Generalization Analysis of Vector-valued Learning", "comments": "To appear in AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Many fundamental machine learning tasks can be formulated as a problem of\nlearning with vector-valued functions, where we learn multiple scalar-valued\nfunctions together. Although there is some generalization analysis on different\nspecific algorithms under the empirical risk minimization principle, a unifying\nanalysis of vector-valued learning under a regularization framework is still\nlacking. In this paper, we initiate the generalization analysis of regularized\nvector-valued learning algorithms by presenting bounds with a mild dependency\non the output dimension and a fast rate on the sample size. Our discussions\nrelax the existing assumptions on the restrictive constraint of hypothesis\nspaces, smoothness of loss functions and low-noise condition. To understand the\ninteraction between optimization and learning, we further use our results to\nderive the first generalization bounds for stochastic gradient descent with\nvector-valued functions. We apply our general results to multi-class\nclassification and multi-label classification, which yield the first bounds\nwith a logarithmic dependency on the output dimension for extreme multi-label\nclassification with the Frobenius regularization. As a byproduct, we derive a\nRademacher complexity bound for loss function classes defined in terms of a\ngeneral strongly convex function.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 07:57:34 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Wu", "Liang", ""], ["Ledent", "Antoine", ""], ["Lei", "Yunwen", ""], ["Kloft", "Marius", ""]]}, {"id": "2104.14200", "submitter": "Junsu Cho", "authors": "Junsu Cho, Dongmin Hyun, SeongKu Kang, Hwanjo Yu", "title": "Learning Heterogeneous Temporal Patterns of User Preference for Timely\n  Recommendation", "comments": "Accepted to The Web Conference (WWW) 2021", "journal-ref": null, "doi": "10.1145/3442381.3449947", "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommender systems have achieved great success in modeling user's\npreferences on items and predicting the next item the user would consume.\nRecently, there have been many efforts to utilize time information of users'\ninteractions with items to capture inherent temporal patterns of user behaviors\nand offer timely recommendations at a given time. Existing studies regard the\ntime information as a single type of feature and focus on how to associate it\nwith user preferences on items. However, we argue they are insufficient for\nfully learning the time information because the temporal patterns of user\npreference are usually heterogeneous. A user's preference for a particular item\nmay 1) increase periodically or 2) evolve over time under the influence of\nsignificant recent events, and each of these two kinds of temporal pattern\nappears with some unique characteristics. In this paper, we first define the\nunique characteristics of the two kinds of temporal pattern of user preference\nthat should be considered in time-aware recommender systems. Then we propose a\nnovel recommender system for timely recommendations, called TimelyRec, which\njointly learns the heterogeneous temporal patterns of user preference\nconsidering all of the defined characteristics. In TimelyRec, a cascade of two\nencoders captures the temporal patterns of user preference using a proposed\nattention module for each encoder. Moreover, we introduce an evaluation\nscenario that evaluates the performance on predicting an interesting item and\nwhen to recommend the item simultaneously in top-K recommendation (i.e.,\nitem-timing recommendation). Our extensive experiments on a scenario for item\nrecommendation and the proposed scenario for item-timing recommendation on\nreal-world datasets demonstrate the superiority of TimelyRec and the proposed\nattention modules.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 08:37:30 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Cho", "Junsu", ""], ["Hyun", "Dongmin", ""], ["Kang", "SeongKu", ""], ["Yu", "Hwanjo", ""]]}, {"id": "2104.14222", "submitter": "Sihan Ma", "authors": "Jizhizi Li, Sihan Ma, Jing Zhang, Dacheng Tao", "title": "Privacy-Preserving Portrait Matting", "comments": "Accepted to ACM Multimedia 2021, code and dataset available at\n  https://github.com/JizhiziLi/P3M", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there has been an increasing concern about the privacy issue raised\nby using personally identifiable information in machine learning. However,\nprevious portrait matting methods were all based on identifiable portrait\nimages. To fill the gap, we present P3M-10k in this paper, which is the first\nlarge-scale anonymized benchmark for Privacy-Preserving Portrait Matting.\nP3M-10k consists of 10,000 high-resolution face-blurred portrait images along\nwith high-quality alpha mattes. We systematically evaluate both trimap-free and\ntrimap-based matting methods on P3M-10k and find that existing matting methods\nshow different generalization capabilities when following the\nPrivacy-Preserving Training (PPT) setting, i.e., training on face-blurred\nimages and testing on arbitrary images. To devise a better trimap-free portrait\nmatting model, we propose P3M-Net, which leverages the power of a unified\nframework for both semantic perception and detail matting, and specifically\nemphasizes the interaction between them and the encoder to facilitate the\nmatting process. Extensive experiments on P3M-10k demonstrate that P3M-Net\noutperforms the state-of-the-art methods in terms of both objective metrics and\nsubjective visual quality. Besides, it shows good generalization capacity under\nthe PPT setting, confirming the value of P3M-10k for facilitating future\nresearch and enabling potential real-world applications. The source code and\ndataset are available at https://github.com/JizhiziLi/P3M\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 09:20:19 GMT"}, {"version": "v2", "created": "Thu, 29 Jul 2021 13:07:00 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Li", "Jizhizi", ""], ["Ma", "Sihan", ""], ["Zhang", "Jing", ""], ["Tao", "Dacheng", ""]]}, {"id": "2104.14223", "submitter": "Oren Spector", "authors": "Oren Spector and Dotan Di Castro", "title": "InsertionNet -- A Scalable Solution for Insertion", "comments": "Qualitative results can be found in our supplementary video on our\n  website: https://sites.google.com/view/insertionnet/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complicated assembly processes can be described as a sequence of two main\nactivities: grasping and insertion. While general grasping solutions are common\nin industry, insertion is still only applicable to small subsets of problems,\nmainly ones involving simple shapes in fixed locations and in which the\nvariations are not taken into consideration. Recently, RL approaches with prior\nknowledge (e.g., LfD or residual policy) have been adopted. However, these\napproaches might be problematic in contact-rich tasks since interaction might\nendanger the robot and its equipment. In this paper, we tackled this challenge\nby formulating the problem as a regression problem. By combining visual and\nforce inputs, we demonstrate that our method can scale to 16 different\ninsertion tasks in less than 10 minutes. The resulting policies are robust to\nchanges in the socket position, orientation or peg color, as well as to small\ndifferences in peg shape. Finally, we demonstrate an end-to-end solution for 2\ncomplex assembly tasks with multi-insertion objectives when the assembly board\nis randomly placed on a table.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 09:21:17 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Spector", "Oren", ""], ["Di Castro", "Dotan", ""]]}, {"id": "2104.14237", "submitter": "Umar Khan", "authors": "Umar Khan, Sohaib Zahid, Muhammad Asad Ali, Adnan ul Hassan, Faisal\n  Shafait", "title": "TabAug: Data Driven Augmentation for Enhanced Table Structure\n  Recognition", "comments": "to be published in ICDAR2021 , 15 pages , \" packages and articles for\n  this work and its extensions at http://umarky.com \" , \" official repository\n  https://github.com/sohaib023/splerge-tab-aug?fbclid=IwAR37V79vDLMqLGcC5YCyqY_CsFYQRDZ1-wUMW7GJUYTzkf9oM1bZ25HPmgo\n  \"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Table Structure Recognition is an essential part of end-to-end tabular data\nextraction in document images. The recent success of deep learning model\narchitectures in computer vision remains to be non-reflective in table\nstructure recognition, largely because extensive datasets for this domain are\nstill unavailable while labeling new data is expensive and time-consuming.\nTraditionally, in computer vision, these challenges are addressed by standard\naugmentation techniques that are based on image transformations like color\njittering and random cropping. As demonstrated by our experiments, these\ntechniques are not effective for the task of table structure recognition. In\nthis paper, we propose TabAug, a re-imagined Data Augmentation technique that\nproduces structural changes in table images through replication and deletion of\nrows and columns. It also consists of a data-driven probabilistic model that\nallows control over the augmentation process. To demonstrate the efficacy of\nour approach, we perform experimentation on ICDAR 2013 dataset where our\napproach shows consistent improvements in all aspects of the evaluation\nmetrics, with cell-level correct detections improving from 92.16% to 96.11%\nover the baseline.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 09:59:46 GMT"}, {"version": "v2", "created": "Sat, 15 May 2021 14:31:19 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Khan", "Umar", ""], ["Zahid", "Sohaib", ""], ["Ali", "Muhammad Asad", ""], ["Hassan", "Adnan ul", ""], ["Shafait", "Faisal", ""]]}, {"id": "2104.14283", "submitter": "Nikolas Koumpis", "authors": "Nikolas P. Koumpis and Dionysios S. Kalogerias", "title": "Uncertainty Principles in Risk-Aware Statistical Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AI cs.SY eess.SY math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new uncertainty principle for risk-aware statistical estimation,\neffectively quantifying the inherent trade-off between mean squared error\n($\\mse$) and risk, the latter measured by the associated average predictive\nsquared error variance ($\\sev$), for every admissible estimator of choice. Our\nuncertainty principle has a familiar form and resembles fundamental and\nclassical results arising in several other areas, such as the Heisenberg\nprinciple in statistical and quantum mechanics, and the Gabor limit (time-scale\ntrade-offs) in harmonic analysis. In particular, we prove that, provided a\njoint generative model of states and observables, the product between $\\mse$\nand $\\sev$ is bounded from below by a computable model-dependent constant,\nwhich is explicitly related to the Pareto frontier of a recently studied\n$\\sev$-constrained minimum $\\mse$ (MMSE) estimation problem. Further, we show\nthat the aforementioned constant is inherently connected to an intuitive new\nand rigorously topologically grounded statistical measure of distribution\nskewness in multiple dimensions, consistent with Pearson's moment coefficient\nof skewness for variables on the line. Our results are also illustrated via\nnumerical simulations.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 12:06:53 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Koumpis", "Nikolas P.", ""], ["Kalogerias", "Dionysios S.", ""]]}, {"id": "2104.14335", "submitter": "Oren Rippel", "authors": "Oren Rippel, Alexander G. Anderson, Kedar Tatwawadi, Sanjay Nair,\n  Craig Lytle, Lubomir Bourdev", "title": "ELF-VC: Efficient Learned Flexible-Rate Video Coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While learned video codecs have demonstrated great promise, they have yet to\nachieve sufficient efficiency for practical deployment. In this work, we\npropose several novel ideas for learned video compression which allow for\nimproved performance for the low-latency mode (I- and P-frames only) along with\na considerable increase in computational efficiency. In this setting, for\nnatural videos our approach compares favorably across the entire R-D curve\nunder metrics PSNR, MS-SSIM and VMAF against all mainstream video standards\n(H.264, H.265, AV1) and all ML codecs. At the same time, our approach runs at\nleast 5x faster and has fewer parameters than all ML codecs which report these\nfigures.\n  Our contributions include a flexible-rate framework allowing a single model\nto cover a large and dense range of bitrates, at a negligible increase in\ncomputation and parameter count; an efficient backbone optimized for ML-based\ncodecs; and a novel in-loop flow prediction scheme which leverages prior\ninformation towards more efficient compression.\n  We benchmark our method, which we call ELF-VC (Efficient, Learned and\nFlexible Video Coding) on popular video test sets UVG and MCL-JCV under metrics\nPSNR, MS-SSIM and VMAF. For example, on UVG under PSNR, it reduces the BD-rate\nby 44% against H.264, 26% against H.265, 15% against AV1, and 35% against the\ncurrent best ML codec. At the same time, on an NVIDIA Titan V GPU our approach\nencodes/decodes VGA at 49/91 FPS, HD 720 at 19/35 FPS, and HD 1080 at 10/18\nFPS.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 17:50:35 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Rippel", "Oren", ""], ["Anderson", "Alexander G.", ""], ["Tatwawadi", "Kedar", ""], ["Nair", "Sanjay", ""], ["Lytle", "Craig", ""], ["Bourdev", "Lubomir", ""]]}, {"id": "2104.14337", "submitter": "Douwe Kiela", "authors": "Douwe Kiela, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger,\n  Zhengxuan Wu, Bertie Vidgen, Grusha Prasad, Amanpreet Singh, Pratik Ringshia,\n  Zhiyi Ma, Tristan Thrush, Sebastian Riedel, Zeerak Waseem, Pontus Stenetorp,\n  Robin Jia, Mohit Bansal, Christopher Potts, Adina Williams", "title": "Dynabench: Rethinking Benchmarking in NLP", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Dynabench, an open-source platform for dynamic dataset creation\nand model benchmarking. Dynabench runs in a web browser and supports\nhuman-and-model-in-the-loop dataset creation: annotators seek to create\nexamples that a target model will misclassify, but that another person will\nnot. In this paper, we argue that Dynabench addresses a critical need in our\ncommunity: contemporary models quickly achieve outstanding performance on\nbenchmark tasks but nonetheless fail on simple challenge examples and falter in\nreal-world scenarios. With Dynabench, dataset creation, model development, and\nmodel assessment can directly inform each other, leading to more robust and\ninformative benchmarks. We report on four initial NLP tasks, illustrating these\nconcepts and highlighting the promise of the platform, and address potential\nobjections to dynamic benchmarking as a new standard for the field.\n", "versions": [{"version": "v1", "created": "Wed, 7 Apr 2021 17:49:17 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Kiela", "Douwe", ""], ["Bartolo", "Max", ""], ["Nie", "Yixin", ""], ["Kaushik", "Divyansh", ""], ["Geiger", "Atticus", ""], ["Wu", "Zhengxuan", ""], ["Vidgen", "Bertie", ""], ["Prasad", "Grusha", ""], ["Singh", "Amanpreet", ""], ["Ringshia", "Pratik", ""], ["Ma", "Zhiyi", ""], ["Thrush", "Tristan", ""], ["Riedel", "Sebastian", ""], ["Waseem", "Zeerak", ""], ["Stenetorp", "Pontus", ""], ["Jia", "Robin", ""], ["Bansal", "Mohit", ""], ["Potts", "Christopher", ""], ["Williams", "Adina", ""]]}, {"id": "2104.14362", "submitter": "Ji Liu", "authors": "Ji Liu, Jizhou Huang, Yang Zhou, Xuhong Li, Shilei Ji, Haoyi Xiong,\n  Dejing Dou", "title": "From Distributed Machine Learning to Federated Learning: A Survey", "comments": "31 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, data and computing resources are typically distributed in\nthe devices of end users, various regions or organizations. Because of laws or\nregulations, the distributed data and computing resources cannot be directly\nshared among different regions or organizations for machine learning tasks.\nFederated learning emerges as an efficient approach to exploit distributed data\nand computing resources, so as to collaboratively train machine learning\nmodels, while obeying the laws and regulations and ensuring data security and\ndata privacy. In this paper, we provide a comprehensive survey of existing\nworks for federated learning. We propose a functional architecture of federated\nlearning systems and a taxonomy of related techniques. Furthermore, we present\nthe distributed training, data communication, and security of FL systems.\nFinally, we analyze their limitations and propose future research directions.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 14:15:11 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 10:55:17 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Liu", "Ji", ""], ["Huang", "Jizhou", ""], ["Zhou", "Yang", ""], ["Li", "Xuhong", ""], ["Ji", "Shilei", ""], ["Xiong", "Haoyi", ""], ["Dou", "Dejing", ""]]}, {"id": "2104.14372", "submitter": "Apostolos Modas", "authors": "Guillermo Ortiz-Jimenez, Itamar Franco Salazar-Reque, Apostolos Modas,\n  Seyed-Mohsen Moosavi-Dezfooli, Pascal Frossard", "title": "A neural anisotropic view of underspecification in deep learning", "comments": "Presented as a RobustML workshop paper at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The underspecification of most machine learning pipelines means that we\ncannot rely solely on validation performance to assess the robustness of deep\nlearning systems to naturally occurring distribution shifts. Instead, making\nsure that a neural network can generalize across a large number of different\nsituations requires to understand the specific way in which it solves a task.\nIn this work, we propose to study this problem from a geometric perspective\nwith the aim to understand two key characteristics of neural network solutions\nin underspecified settings: how is the geometry of the learned function related\nto the data representation? And, are deep networks always biased towards\nsimpler solutions, as conjectured in recent literature? We show that the way\nneural networks handle the underspecification of these problems is highly\ndependent on the data representation, affecting both the geometry and the\ncomplexity of the learned predictors. Our results highlight that understanding\nthe architectural inductive bias in deep learning is fundamental to address the\nfairness, robustness, and generalization of these systems.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 14:31:09 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Ortiz-Jimenez", "Guillermo", ""], ["Salazar-Reque", "Itamar Franco", ""], ["Modas", "Apostolos", ""], ["Moosavi-Dezfooli", "Seyed-Mohsen", ""], ["Frossard", "Pascal", ""]]}, {"id": "2104.14386", "submitter": "Artemij Amiranashvili", "authors": "Artemij Amiranashvili, Max Argus, Lukas Hermann, Wolfram Burgard,\n  Thomas Brox", "title": "Pre-training of Deep RL Agents for Improved Learning under Domain\n  Randomization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual domain randomization in simulated environments is a widely used method\nto transfer policies trained in simulation to real robots. However, domain\nrandomization and augmentation hamper the training of a policy. As\nreinforcement learning struggles with a noisy training signal, this additional\nnuisance can drastically impede training. For difficult tasks it can even\nresult in complete failure to learn. To overcome this problem we propose to\npre-train a perception encoder that already provides an embedding invariant to\nthe randomization. We demonstrate that this yields consistently improved\nresults on a randomized version of DeepMind control suite tasks and a stacking\nenvironment on arbitrary backgrounds with zero-shot transfer to a physical\nrobot.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 14:54:11 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Amiranashvili", "Artemij", ""], ["Argus", "Max", ""], ["Hermann", "Lukas", ""], ["Burgard", "Wolfram", ""], ["Brox", "Thomas", ""]]}, {"id": "2104.14398", "submitter": "Shaun D'Souza", "authors": "Shaun D'Souza", "title": "Implementing Reinforcement Learning Algorithms in Retail Supply Chains\n  with OpenAI Gym Toolkit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From cutting costs to improving customer experience, forecasting is the crux\nof retail supply chain management (SCM) and the key to better supply chain\nperformance. Several retailers are using AI/ML models to gather datasets and\nprovide forecast guidance in applications such as Cognitive Demand Forecasting,\nProduct End-of-Life, Forecasting, and Demand Integrated Product Flow. Early\nwork in these areas looked at classical algorithms to improve on a gamut of\nchallenges such as network flow and graphs. But the recent disruptions have\nmade it critical for supply chains to have the resiliency to handle unexpected\nevents. The biggest challenge lies in matching supply with demand.\n  Reinforcement Learning (RL) with its ability to train systems to respond to\nunforeseen environments, is being increasingly adopted in SCM to improve\nforecast accuracy, solve supply chain optimization challenges, and train\nsystems to respond to unforeseen circumstances. Companies like UPS and Amazon\nhave developed RL algorithms to define winning AI strategies and keep up with\nrising consumer delivery expectations. While there are many ways to build RL\nalgorithms for supply chain use cases, the OpenAI Gym toolkit is becoming the\npreferred choice because of the robust framework for event-driven simulations.\n  This white paper explores the application of RL in supply chain forecasting\nand describes how to build suitable RL models and algorithms by using the\nOpenAI Gym toolkit.\n", "versions": [{"version": "v1", "created": "Tue, 27 Apr 2021 03:35:42 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["D'Souza", "Shaun", ""]]}, {"id": "2104.14426", "submitter": "Andrew Cropper", "authors": "Andrew Cropper and Rolf Morel", "title": "Predicate Invention by Learning From Failures", "comments": "Rejected manuscript for IJCAI21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Discovering novel high-level concepts is one of the most important steps\nneeded for human-level AI. In inductive logic programming (ILP), discovering\nnovel high-level concepts is known as predicate invention (PI). Although seen\nas crucial since the founding of ILP, PI is notoriously difficult and most ILP\nsystems do not support it. In this paper, we introduce POPPI, an ILP system\nthat formulates the PI problem as an answer set programming problem. Our\nexperiments show that (i) PI can drastically improve learning performance when\nuseful, (ii) PI is not too costly when unnecessary, and (iii) POPPI can\nsubstantially outperform existing ILP systems.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 15:44:35 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Cropper", "Andrew", ""], ["Morel", "Rolf", ""]]}, {"id": "2104.14435", "submitter": "Changshun Wu", "authors": "Changshun Wu, Yli\\`es Falcone, Saddek Bensalem", "title": "Customizable Reference Runtime Monitoring of Neural Networks using\n  Resolution Boxes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classification neural networks fail to detect inputs that do not fall inside\nthe classes they have been trained for. Runtime monitoring techniques on the\nneuron activation pattern can be used to detect such inputs. We present an\napproach for monitoring classification systems via data abstraction. Data\nabstraction relies on the notion of box with a resolution. Box-based\nabstraction consists in representing a set of values by its minimal and maximal\nvalues in each dimension. We augment boxes with a notion of resolution and\ndefine their clustering coverage, which is intuitively a quantitative metric\nthat indicates the abstraction quality. This allows studying the effect of\ndifferent clustering parameters on the constructed boxes and estimating an\ninterval of sub-optimal parameters. Moreover, we automatically construct\nmonitors that leverage both the correct and incorrect behaviors of a system.\nThis allows checking the size of the monitor abstractions and analyzing the\nseparability of the network. Monitors are obtained by combining the\nsub-monitors of each class of the system placed at some selected layers. Our\nexperiments demonstrate the effectiveness of our clustering coverage estimation\nand show how to assess the effectiveness and precision of monitors according to\nthe selected clustering parameter and monitored layers.\n", "versions": [{"version": "v1", "created": "Sun, 25 Apr 2021 21:58:02 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 12:21:53 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Wu", "Changshun", ""], ["Falcone", "Yli\u00e8s", ""], ["Bensalem", "Saddek", ""]]}, {"id": "2104.14461", "submitter": "Mark Keane", "authors": "Mark T Keane and Eoin M Kenny and Mohammed Temraz and Derek Greene and\n  Barry Smyth", "title": "Twin Systems for DeepCBR: A Menagerie of Deep Learning and Case-Based\n  Reasoning Pairings for Explanation and Data Augmentation", "comments": "7 pages,4 figures, 2 tables", "journal-ref": "IJCAI-21 Workshop on DL-CBR-AML, July 2021", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, it has been proposed that fruitful synergies may exist between Deep\nLearning (DL) and Case Based Reasoning (CBR); that there are insights to be\ngained by applying CBR ideas to problems in DL (what could be called DeepCBR).\nIn this paper, we report on a program of research that applies CBR solutions to\nthe problem of Explainable AI (XAI) in the DL. We describe a series of\ntwin-systems pairings of opaque DL models with transparent CBR models that\nallow the latter to explain the former using factual, counterfactual and\nsemi-factual explanation strategies. This twinning shows that functional\nabstractions of DL (e.g., feature weights, feature importance and decision\nboundaries) can be used to drive these explanatory solutions. We also raise the\nprospect that this research also applies to the problem of Data Augmentation in\nDL, underscoring the fecundity of these DeepCBR ideas.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 16:26:06 GMT"}, {"version": "v2", "created": "Sun, 13 Jun 2021 16:00:01 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Keane", "Mark T", ""], ["Kenny", "Eoin M", ""], ["Temraz", "Mohammed", ""], ["Greene", "Derek", ""], ["Smyth", "Barry", ""]]}, {"id": "2104.14478", "submitter": "Markus Freitag", "authors": "Markus Freitag, George Foster, David Grangier, Viresh Ratnakar, Qijun\n  Tan, Wolfgang Macherey", "title": "Experts, Errors, and Context: A Large-Scale Study of Human Evaluation\n  for Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Human evaluation of modern high-quality machine translation systems is a\ndifficult problem, and there is increasing evidence that inadequate evaluation\nprocedures can lead to erroneous conclusions. While there has been considerable\nresearch on human evaluation, the field still lacks a commonly-accepted\nstandard procedure. As a step toward this goal, we propose an evaluation\nmethodology grounded in explicit error analysis, based on the Multidimensional\nQuality Metrics (MQM) framework. We carry out the largest MQM research study to\ndate, scoring the outputs of top systems from the WMT 2020 shared task in two\nlanguage pairs using annotations provided by professional translators with\naccess to full document context. We analyze the resulting data extensively,\nfinding among other results a substantially different ranking of evaluated\nsystems from the one established by the WMT crowd workers, exhibiting a clear\npreference for human over machine output. Surprisingly, we also find that\nautomatic metrics based on pre-trained embeddings can outperform human crowd\nworkers. We make our corpus publicly available for further research.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 16:42:09 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Freitag", "Markus", ""], ["Foster", "George", ""], ["Grangier", "David", ""], ["Ratnakar", "Viresh", ""], ["Tan", "Qijun", ""], ["Macherey", "Wolfgang", ""]]}, {"id": "2104.14512", "submitter": "Kai Sauerwald", "authors": "Faiq Miftakhul Falakh and Sebastian Rudolph and Kai Sauerwald", "title": "A General Katsuno-Mendelzon-Style Characterization of AGM Belief Base\n  Revision for Arbitrary Monotonic Logics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The AGM postulates by Alchourr\\'{o}n, G\\\"{a}rdenfors, and Makinson continue\nto represent a cornerstone in research related to belief change. We generalize\nthe approach of Katsuno and Mendelzon (KM) for characterizing AGM base revision\nfrom propositional logic to the setting of (multiple) base revision in\narbitrary monotonic logics. Our core result is a representation theorem using\nthe assignment of total - yet not transitive - \"preference\" relations to belief\nbases. We also provide a characterization of all logics for which our result\ncan be strengthened to preorder assignments (as in KM's original work).\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 17:24:21 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Falakh", "Faiq Miftakhul", ""], ["Rudolph", "Sebastian", ""], ["Sauerwald", "Kai", ""]]}, {"id": "2104.14517", "submitter": "Anthony Kenyon", "authors": "Adnan Mehonic and Anthony J Kenyon", "title": "Brain-inspired computing: We need a master plan", "comments": "9 oages 4 figures. Perspectives commentary", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AI eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  New computing technologies inspired by the brain promise fundamentally\ndifferent ways to process information with extreme energy efficiency and the\nability to handle the avalanche of unstructured and noisy data that we are\ngenerating at an ever-increasing rate. To realise this promise requires a brave\nand coordinated plan to bring together disparate research communities and to\nprovide them with the funding, focus and support needed. We have done this in\nthe past with digital technologies; we are in the process of doing it with\nquantum technologies; can we now do it for brain-inspired computing?\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 17:34:16 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Mehonic", "Adnan", ""], ["Kenyon", "Anthony J", ""]]}, {"id": "2104.14527", "submitter": "Virginie Do", "authors": "Virginie Do, Sam Corbett-Davies, Jamal Atif, Nicolas Usunier", "title": "Online certification of preference-based fairness for personalized\n  recommender systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose to assess the fairness of personalized recommender systems in the\nsense of envy-freeness: every (group of) user(s) should prefer their\nrecommendations to the recommendations of other (groups of) users. Auditing for\nenvy-freeness requires probing user preferences to detect potential blind\nspots, which may deteriorate recommendation performance. To control the cost of\nexploration, we propose an auditing algorithm based on pure exploration and\nconservative constraints in multi-armed bandits. We study, both theoretically\nand empirically, the trade-offs achieved by this algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 17:45:27 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Do", "Virginie", ""], ["Corbett-Davies", "Sam", ""], ["Atif", "Jamal", ""], ["Usunier", "Nicolas", ""]]}, {"id": "2104.14558", "submitter": "Christoph Feichtenhofer", "authors": "Christoph Feichtenhofer, Haoqi Fan, Bo Xiong, Ross Girshick, Kaiming\n  He", "title": "A Large-Scale Study on Unsupervised Spatiotemporal Representation\n  Learning", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a large-scale study on unsupervised spatiotemporal representation\nlearning from videos. With a unified perspective on four recent image-based\nframeworks, we study a simple objective that can easily generalize all these\nmethods to space-time. Our objective encourages temporally-persistent features\nin the same video, and in spite of its simplicity, it works surprisingly well\nacross: (i) different unsupervised frameworks, (ii) pre-training datasets,\n(iii) downstream datasets, and (iv) backbone architectures. We draw a series of\nintriguing observations from this study, e.g., we discover that encouraging\nlong-spanned persistency can be effective even if the timespan is 60 seconds.\nIn addition to state-of-the-art results in multiple benchmarks, we report a few\npromising cases in which unsupervised pre-training can outperform its\nsupervised counterpart. Code is made available at\nhttps://github.com/facebookresearch/SlowFast\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 17:59:53 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Feichtenhofer", "Christoph", ""], ["Fan", "Haoqi", ""], ["Xiong", "Bo", ""], ["Girshick", "Ross", ""], ["He", "Kaiming", ""]]}, {"id": "2104.14602", "submitter": "Anas Shrinah", "authors": "Anas Shrinah, Derek Long and Kerstin Eder", "title": "D-VAL: An automatic functional equivalence validation tool for planning\n  domain models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we introduce an approach to validate the functional\nequivalence of planning domain models. Validating the functional equivalence of\nplanning domain models is the problem of formally confirming that two planning\ndomain models can be used to solve the same set of problems. The need for\ntechniques to validate the functional equivalence of planning domain models has\nbeen highlighted in previous research and has applications in model learning,\ndevelopment and extension. We prove the soundness and completeness of our\nmethod. We also develop D-VAL, an automatic functional equivalence validation\ntool for planning domain models. Empirical evaluation shows that D-VAL\nvalidates the functional equivalence of most examined domains in less than five\nminutes. Additionally, we provide a benchmark to evaluate the feasibility and\nscalability of this and future related work.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 18:40:23 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Shrinah", "Anas", ""], ["Long", "Derek", ""], ["Eder", "Kerstin", ""]]}, {"id": "2104.14616", "submitter": "Adrian Moldovan", "authors": "Adrian Moldovan and Angel Ca\\c{t}aron and R\\u{a}zvan Andonie", "title": "Learning in Feedforward Neural Networks Accelerated by Transfer Entropy", "comments": "18 pages, 13 figures", "journal-ref": null, "doi": "10.3390/e22010102", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Current neural networks architectures are many times harder to train because\nof the increasing size and complexity of the used datasets. Our objective is to\ndesign more efficient training algorithms utilizing causal relationships\ninferred from neural networks. The transfer entropy (TE) was initially\nintroduced as an information transfer measure used to quantify the statistical\ncoherence between events (time series). Later, it was related to causality,\neven if they are not the same. There are only few papers reporting applications\nof causality or TE in neural networks. Our contribution is an\ninformation-theoretical method for analyzing information transfer between the\nnodes of feedforward neural networks. The information transfer is measured by\nthe TE of feedback neural connections. Intuitively, TE measures the relevance\nof a connection in the network and the feedback amplifies this connection. We\nintroduce a backpropagation type training algorithm that uses TE feedback\nconnections to improve its performance.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 19:07:07 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Moldovan", "Adrian", ""], ["Ca\u0163aron", "Angel", ""], ["Andonie", "R\u0103zvan", ""]]}, {"id": "2104.14624", "submitter": "Martin Grohe", "authors": "Martin Grohe", "title": "The Logic of Graph Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph neural networks (GNNs) are deep learning architectures for machine\nlearning problems on graphs. It has recently been shown that the expressiveness\nof GNNs can be characterised precisely by the combinatorial Weisfeiler-Leman\nalgorithms and by finite variable counting logics. The correspondence has even\nled to new, higher-order GNNs corresponding to the WL algorithm in higher\ndimensions.\n  The purpose of this paper is to explain these descriptive characterisations\nof GNNs.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 19:23:26 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Grohe", "Martin", ""]]}, {"id": "2104.14629", "submitter": "Xiao-Yun Zhou", "authors": "Xiao-Yun Zhou, Bolin Lai, Weijian Li, Yirui Wang, Kang Zheng, Fakai\n  Wang, Chihung Lin, Le Lu, Lingyun Huang, Mei Han, Guotong Xie, Jing Xiao, Kuo\n  Chang-Fu, Adam Harrison, Shun Miao", "title": "Scalable Semi-supervised Landmark Localization for X-ray Images using\n  Few-shot Deep Adaptive Graph", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Landmark localization plays an important role in medical image analysis.\nLearning based methods, including CNN and GCN, have demonstrated the\nstate-of-the-art performance. However, most of these methods are\nfully-supervised and heavily rely on manual labeling of a large training\ndataset. In this paper, based on a fully-supervised graph-based method, DAG, we\nproposed a semi-supervised extension of it, termed few-shot DAG, \\ie five-shot\nDAG. It first trains a DAG model on the labeled data and then fine-tunes the\npre-trained model on the unlabeled data with a teacher-student SSL mechanism.\nIn addition to the semi-supervised loss, we propose another loss using JS\ndivergence to regulate the consistency of the intermediate feature maps. We\nextensively evaluated our method on pelvis, hand and chest landmark detection\ntasks. Our experiment results demonstrate consistent and significant\nimprovements over previous methods.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 19:46:18 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Zhou", "Xiao-Yun", ""], ["Lai", "Bolin", ""], ["Li", "Weijian", ""], ["Wang", "Yirui", ""], ["Zheng", "Kang", ""], ["Wang", "Fakai", ""], ["Lin", "Chihung", ""], ["Lu", "Le", ""], ["Huang", "Lingyun", ""], ["Han", "Mei", ""], ["Xie", "Guotong", ""], ["Xiao", "Jing", ""], ["Chang-Fu", "Kuo", ""], ["Harrison", "Adam", ""], ["Miao", "Shun", ""]]}, {"id": "2104.14633", "submitter": "Jordan Cambe", "authors": "Krittika D'Silva, Jordan Cambe, Anastasios Noulas, Cecilia Mascolo,\n  Adam Waksman", "title": "Modelling Urban Dynamics with Multi-Modal Graph Convolutional Networks", "comments": "10 pages, 4 figures. arXiv admin note: substantial text overlap with\n  arXiv:2104.13981", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.soc-ph cs.AI cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modelling the dynamics of urban venues is a challenging task as it is\nmultifaceted in nature. Demand is a function of many complex and nonlinear\nfeatures such as neighborhood composition, real-time events, and seasonality.\nRecent advances in Graph Convolutional Networks (GCNs) have had promising\nresults as they build a graphical representation of a system and harness the\npotential of deep learning architectures. However, there has been limited work\nusing GCNs in a temporal setting to model dynamic dependencies of the network.\nFurther, within the context of urban environments, there has been no prior work\nusing dynamic GCNs to support venue demand analysis and prediction. In this\npaper, we propose a novel deep learning framework which aims to better model\nthe popularity and growth of urban venues. Using a longitudinal dataset from\nlocation technology platform Foursquare, we model individual venues and venue\ntypes across London and Paris. First, representing cities as connected networks\nof venues, we quantify their structure and note a strong community structure in\nthese retail networks, an observation that highlights the interplay of\ncooperative and competitive forces that emerge in local ecosystems of retail\nbusinesses. Next, we present our deep learning architecture which integrates\nboth spatial and topological features into a temporal model which predicts the\ndemand of a venue at the subsequent time-step. Our experiments demonstrate that\nour model can learn spatio-temporal trends of venue demand and consistently\noutperform baseline models. Relative to state-of-the-art deep learning models,\nour model reduces the RSME by ~ 28% in London and ~ 13% in Paris. Our approach\nhighlights the power of complex network measures and GCNs in building\nprediction models for urban environments. The model could have numerous\napplications within the retail sector to better model venue demand and growth.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 20:00:47 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["D'Silva", "Krittika", ""], ["Cambe", "Jordan", ""], ["Noulas", "Anastasios", ""], ["Mascolo", "Cecilia", ""], ["Waksman", "Adam", ""]]}, {"id": "2104.14644", "submitter": "Safa Alver", "authors": "Safa Alver, Doina Precup", "title": "What is Going on Inside Recurrent Meta Reinforcement Learning Agents?", "comments": "Accepted to the Never-Ending Reinforcement Learning Workshop at ICLR\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recurrent meta reinforcement learning (meta-RL) agents are agents that employ\na recurrent neural network (RNN) for the purpose of \"learning a learning\nalgorithm\". After being trained on a pre-specified task distribution, the\nlearned weights of the agent's RNN are said to implement an efficient learning\nalgorithm through their activity dynamics, which allows the agent to quickly\nsolve new tasks sampled from the same distribution. However, due to the\nblack-box nature of these agents, the way in which they work is not yet fully\nunderstood. In this study, we shed light on the internal working mechanisms of\nthese agents by reformulating the meta-RL problem using the Partially\nObservable Markov Decision Process (POMDP) framework. We hypothesize that the\nlearned activity dynamics is acting as belief states for such agents. Several\nillustrative experiments suggest that this hypothesis is true, and that\nrecurrent meta-RL agents can be viewed as agents that learn to act optimally in\npartially observable environments consisting of multiple related tasks. This\nview helps in understanding their failure cases and some interesting\nmodel-based results reported in the literature.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 20:34:39 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Alver", "Safa", ""], ["Precup", "Doina", ""]]}, {"id": "2104.14647", "submitter": "Adrian de Wynter", "authors": "Adrian de Wynter", "title": "Turing Completeness and Sid Meier's Civilization", "comments": "Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We prove that three strategy video games from the Sid Meier's Civilization\nseries: Sid Meier's Civilization: Beyond Earth, Sid Meier's Civilization V, and\nSid Meier's Civilization VI, are Turing complete. We achieve this by building\nthree universal Turing machines-one for each game-using only the elements\npresent in the games, and using their internal rules and mechanics as the\ntransition function. The existence of such machines imply that under the\nassumptions made, the games are undecidable. We show constructions of these\nmachines within a running game session, and we provide a sample execution of an\nalgorithm-the three-state Busy Beaver-with one of our machines.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 20:48:58 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["de Wynter", "Adrian", ""]]}, {"id": "2104.14690", "submitter": "Sinong Wang", "authors": "Sinong Wang, Han Fang, Madian Khabsa, Hanzi Mao, Hao Ma", "title": "Entailment as Few-Shot Learner", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large pre-trained language models (LMs) have demonstrated remarkable ability\nas few-shot learners. However, their success hinges largely on scaling model\nparameters to a degree that makes it challenging to train and serve. In this\npaper, we propose a new approach, named as EFL, that can turn small LMs into\nbetter few-shot learners. The key idea of this approach is to reformulate\npotential NLP task into an entailment one, and then fine-tune the model with as\nlittle as 8 examples. We further demonstrate our proposed method can be: (i)\nnaturally combined with an unsupervised contrastive learning-based data\naugmentation method; (ii) easily extended to multilingual few-shot learning. A\nsystematic evaluation on 18 standard NLP tasks demonstrates that this approach\nimproves the various existing SOTA few-shot learning methods by 12\\%, and\nyields competitive few-shot performance with 500 times larger models, such as\nGPT-3.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 22:52:26 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Wang", "Sinong", ""], ["Fang", "Han", ""], ["Khabsa", "Madian", ""], ["Mao", "Hanzi", ""], ["Ma", "Hao", ""]]}, {"id": "2104.14700", "submitter": "Ewan Dunbar", "authors": "Ewan Dunbar, Mathieu Bernard, Nicolas Hamilakis, Tu Anh Nguyen,\n  Maureen de Seyssel, Patricia Roz\\'e, Morgane Rivi\\`ere, Eugene Kharitonov,\n  Emmanuel Dupoux", "title": "The Interspeech Zero Resource Speech Challenge 2021: Spoken language\n  modelling", "comments": "Submitted to Interspeech 2021. arXiv admin note: text overlap with\n  arXiv:2011.11588", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present the Zero Resource Speech Challenge 2021, which asks participants\nto learn a language model directly from audio, without any text or labels. The\nchallenge is based on the Libri-light dataset, which provides up to 60k hours\nof audio from English audio books without any associated text. We provide a\npipeline baseline system consisting on an encoder based on contrastive\npredictive coding (CPC), a quantizer ($k$-means) and a standard language model\n(BERT or LSTM). The metrics evaluate the learned representations at the\nacoustic (ABX discrimination), lexical (spot-the-word), syntactic\n(acceptability judgment) and semantic levels (similarity judgment). We present\nan overview of the eight submitted systems from four groups and discuss the\nmain results.\n", "versions": [{"version": "v1", "created": "Thu, 29 Apr 2021 23:53:37 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Dunbar", "Ewan", ""], ["Bernard", "Mathieu", ""], ["Hamilakis", "Nicolas", ""], ["Nguyen", "Tu Anh", ""], ["de Seyssel", "Maureen", ""], ["Roz\u00e9", "Patricia", ""], ["Rivi\u00e8re", "Morgane", ""], ["Kharitonov", "Eugene", ""], ["Dupoux", "Emmanuel", ""]]}, {"id": "2104.14715", "submitter": "Salar Safarkhani", "authors": "Pola Lydia Lagari, Lefteri H. Tsoukalas, Salar Safarkhani, Isaac E.\n  Lagaris", "title": "Eliminating Multicollinearity Issues in Neural Network Ensembles:\n  Incremental, Negatively Correlated, Optimal Convex Blending", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a {features, target} dataset, we introduce an incremental algorithm\nthat constructs an aggregate regressor, using an ensemble of neural networks.\nIt is well known that ensemble methods suffer from the multicollinearity issue,\nwhich is the manifestation of redundancy arising mainly due to the common\ntraining-dataset. In the present incremental approach, at each stage we\noptimally blend the aggregate regressor with a newly trained neural network\nunder a convexity constraint which, if necessary, induces negative\ncorrelations. Under this framework, collinearity issues do not arise at all,\nrendering so the method both accurate and robust.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 01:32:08 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Lagari", "Pola Lydia", ""], ["Tsoukalas", "Lefteri H.", ""], ["Safarkhani", "Salar", ""], ["Lagaris", "Isaac E.", ""]]}, {"id": "2104.14721", "submitter": "Shubham Gupta", "authors": "Carola Sundaramoorthy, Lin Ziwen Kelvin, Mahak Sarin, Shubham Gupta", "title": "End-to-End Attention-based Image Captioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we address the problem of image captioning specifically for\nmolecular translation where the result would be a predicted chemical notation\nin InChI format for a given molecular structure. Current approaches mainly\nfollow rule-based or CNN+RNN based methodology. However, they seem to\nunderperform on noisy images and images with small number of distinguishable\nfeatures. To overcome this, we propose an end-to-end transformer model. When\ncompared to attention-based techniques, our proposed model outperforms on\nmolecular datasets.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 01:54:38 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Sundaramoorthy", "Carola", ""], ["Kelvin", "Lin Ziwen", ""], ["Sarin", "Mahak", ""], ["Gupta", "Shubham", ""]]}, {"id": "2104.14728", "submitter": "Ayme Arango", "authors": "Aym\\'e Arango, Jorge P\\'erez and Barbara Poblete", "title": "Cross-lingual hate speech detection based on multilingual\n  domain-specific word embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic hate speech detection in online social networks is an important\nopen problem in Natural Language Processing (NLP). Hate speech is a\nmultidimensional issue, strongly dependant on language and cultural factors.\nDespite its relevance, research on this topic has been almost exclusively\ndevoted to English. Most supervised learning resources, such as labeled\ndatasets and NLP tools, have been created for this same language. Considering\nthat a large portion of users worldwide speak in languages other than English,\nthere is an important need for creating efficient approaches for multilingual\nhate speech detection. In this work we propose to address the problem of\nmultilingual hate speech detection from the perspective of transfer learning.\nOur goal is to determine if knowledge from one particular language can be used\nto classify other language, and to determine effective ways to achieve this. We\npropose a hate specific data representation and evaluate its effectiveness\nagainst general-purpose universal representations most of which, unlike our\nproposed model, have been trained on massive amounts of data. We focus on a\ncross-lingual setting, in which one needs to classify hate speech in one\nlanguage without having access to any labeled data for that language. We show\nthat the use of our simple yet specific multilingual hate representations\nimproves classification results. We explain this with a qualitative analysis\nshowing that our specific representation is able to capture some common\npatterns in how hate speech presents itself in different languages.\n  Our proposal constitutes, to the best of our knowledge, the first attempt for\nconstructing multilingual specific-task representations. Despite its\nsimplicity, our model outperformed the previous approaches for most of the\nexperimental setups. Our findings can orient future solutions toward the use of\ndomain-specific representations.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 02:24:50 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Arango", "Aym\u00e9", ""], ["P\u00e9rez", "Jorge", ""], ["Poblete", "Barbara", ""]]}, {"id": "2104.14734", "submitter": "Dan Shiebler", "authors": "Dan Shiebler", "title": "Flattening Multiparameter Hierarchical Clustering Functors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We bring together topological data analysis, applied category theory, and\nmachine learning to study multiparameter hierarchical clustering. We begin by\nintroducing a procedure for flattening multiparameter hierarchical clusterings.\nWe demonstrate that this procedure is a functor from a category of\nmultiparameter hierarchical partitions to a category of binary integer\nprograms. We also include empirical results demonstrating its effectiveness.\nNext, we introduce a Bayesian update algorithm for learning clustering\nparameters from data. We demonstrate that the composition of this algorithm\nwith our flattening procedure satisfies a consistency property.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 03:10:20 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Shiebler", "Dan", ""]]}, {"id": "2104.14744", "submitter": "Sam Ganzfried", "authors": "Sam Ganzfried", "title": "Human strategic decision making in parametrized games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.LG econ.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world games contain parameters which can affect payoffs, action\nspaces, and information states. For fixed values of the parameters, the game\ncan be solved using standard algorithms. However, in many settings agents must\nact without knowing the values of the parameters that will be encountered in\nadvance. Often the decisions must be made by a human under time and resource\nconstraints, and it is unrealistic to assume that a human can solve the game in\nreal time. We present a new framework that enables human decision makers to\nmake fast decisions without the aid of real-time solvers. We demonstrate\napplicability to a variety of situations including settings with multiple\nplayers and imperfect information.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 03:40:27 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Ganzfried", "Sam", ""]]}, {"id": "2104.14757", "submitter": "Huijuan Wang", "authors": "Huijuan Wang, Shuangyin Li, Rong Pan", "title": "An Adversarial Transfer Network for Knowledge Representation Learning", "comments": "Accepted by TheWebConf 2021", "journal-ref": null, "doi": "10.1145/3442381.3450064", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge representation learning has received a lot of attention in the past\nfew years. The success of existing methods heavily relies on the quality of\nknowledge graphs. The entities with few triplets tend to be learned with less\nexpressive power. Fortunately, there are many knowledge graphs constructed from\nvarious sources, the representations of which could contain much information.\nWe propose an adversarial embedding transfer network ATransN, which transfers\nknowledge from one or more teacher knowledge graphs to a target one through an\naligned entity set without explicit data leakage. Specifically, we add soft\nconstraints on aligned entity pairs and neighbours to the existing knowledge\nrepresentation learning methods. To handle the problem of possible distribution\ndifferences between teacher and target knowledge graphs, we introduce an\nadversarial adaption module. The discriminator of this module evaluates the\ndegree of consistency between the embeddings of an aligned entity pair. The\nconsistency score is then used as the weights of soft constraints. It is not\nnecessary to acquire the relations and triplets in teacher knowledge graphs\nbecause we only utilize the entity representations. Knowledge graph completion\nresults show that ATransN achieves better performance against baselines without\ntransfer on three datasets, CN3l, WK3l, and DWY100k. The ablation study\ndemonstrates that ATransN can bring steady and consistent improvement in\ndifferent settings. The extension of combining other knowledge graph embedding\nalgorithms and the extension with three teacher graphs display the promising\ngeneralization of the adversarial transfer network.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 05:07:25 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Wang", "Huijuan", ""], ["Li", "Shuangyin", ""], ["Pan", "Rong", ""]]}, {"id": "2104.14762", "submitter": "Yanan Wu", "authors": "Yanan Wu, He Liu, Songhe Feng, Yi Jin, Gengyu Lyu, Zizhang Wu", "title": "GM-MLIC: Graph Matching based Multi-Label Image Classification", "comments": "Accepted by International Joint Conferences on Artificial\n  Intelligence (IJCAI-2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-Label Image Classification (MLIC) aims to predict a set of labels that\npresent in an image. The key to deal with such problem is to mine the\nassociations between image contents and labels, and further obtain the correct\nassignments between images and their labels. In this paper, we treat each image\nas a bag of instances, and reformulate the task of MLIC as an instance-label\nmatching selection problem. To model such problem, we propose a novel deep\nlearning framework named Graph Matching based Multi-Label Image Classification\n(GM-MLIC), where Graph Matching (GM) scheme is introduced owing to its\nexcellent capability of excavating the instance and label relationship.\nSpecifically, we first construct an instance spatial graph and a label semantic\ngraph respectively, and then incorporate them into a constructed assignment\ngraph by connecting each instance to all labels. Subsequently, the graph\nnetwork block is adopted to aggregate and update all nodes and edges state on\nthe assignment graph to form structured representations for each instance and\nlabel. Our network finally derives a prediction score for each instance-label\ncorrespondence and optimizes such correspondence with a weighted cross-entropy\nloss. Extensive experiments conducted on various image datasets demonstrate the\nsuperiority of our proposed method.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 05:36:25 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 09:20:47 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Wu", "Yanan", ""], ["Liu", "He", ""], ["Feng", "Songhe", ""], ["Jin", "Yi", ""], ["Lyu", "Gengyu", ""], ["Wu", "Zizhang", ""]]}, {"id": "2104.14789", "submitter": "Linde Vanbesien", "authors": "Linde Vanbesien, Maurice Bruynooghe and Marc Denecker", "title": "Analyzing Semantics of Aggregate Answer Set Programming Using\n  Approximation Fixpoint Theory", "comments": "10pages, submitted to KR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aggregates provide a concise way to express complex knowledge. While they are\neasily understood by humans, formalizing aggregates for answer set programming\n(ASP) has proven to be challenging . The literature offers many approaches that\nare not always compatible. One of these approaches, based on Approximation\nFixpoint Theory (AFT), has been developed in a logic programming context and\nhas not found much resonance in the ASP-community. In this paper we revisit\nthis work. We introduce the abstract notion of a ternary satisfaction relation\nand define stable semantics in terms of it. We show that ternary satisfaction\nrelations bridge the gap between the standard Gelfond-Lifschitz reduct, and\nstable semantics as defined in the framework of AFT. We analyse the properties\nof ternary satisfaction relations for handling aggregates in ASP programs.\nFinally, we show how different methods for handling aggregates taken from the\nliterature can be described in the framework and we study the corresponding\nternary satisfaction relations.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 07:06:27 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Vanbesien", "Linde", ""], ["Bruynooghe", "Maurice", ""], ["Denecker", "Marc", ""]]}, {"id": "2104.14795", "submitter": "Soroush Vosoughi Dr", "authors": "Ruibo Liu, Chenyan Jia, Jason Wei, Guangxuan Xu, Lili Wang, Soroush\n  Vosoughi", "title": "Mitigating Political Bias in Language Models Through Reinforced\n  Calibration", "comments": "In proceedings of the 35th AAAI Conference on Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Current large-scale language models can be politically biased as a result of\nthe data they are trained on, potentially causing serious problems when they\nare deployed in real-world settings. In this paper, we describe metrics for\nmeasuring political bias in GPT-2 generation and propose a reinforcement\nlearning (RL) framework for mitigating political biases in generated text. By\nusing rewards from word embeddings or a classifier, our RL framework guides\ndebiased generation without having access to the training data or requiring the\nmodel to be retrained. In empirical experiments on three attributes sensitive\nto political bias (gender, location, and topic), our methods reduced bias\naccording to both our metrics and human evaluation, while maintaining\nreadability and semantic coherence.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 07:21:30 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Liu", "Ruibo", ""], ["Jia", "Chenyan", ""], ["Wei", "Jason", ""], ["Xu", "Guangxuan", ""], ["Wang", "Lili", ""], ["Vosoughi", "Soroush", ""]]}, {"id": "2104.14810", "submitter": "Guy Marshall", "authors": "Guy Clarke Marshall and Caroline Jay and Andre Freitas", "title": "Structuralist analysis for neural network system diagrams", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This short paper examines diagrams describing neural network systems in\nacademic conference proceedings. Many aspects of scholarly communication are\ncontrolled, particularly with relation to text and formatting, but often\ndiagrams are not centrally curated beyond a peer review. Using a corpus-based\napproach, we argue that the heterogeneous diagrammatic notations used for\nneural network systems has implications for signification in this domain. We\ndivide this into (i) what content is being represented and (ii) how relations\nare encoded. Using a novel structuralist framework, we use a corpus analysis to\nquantitatively cluster diagrams according to the author's representational\nchoices. This quantitative diagram classification in a heterogeneous domain may\nprovide a foundation for further analysis.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 07:50:19 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Marshall", "Guy Clarke", ""], ["Jay", "Caroline", ""], ["Freitas", "Andre", ""]]}, {"id": "2104.14811", "submitter": "Guy Marshall", "authors": "Guy Clarke Marshall and Caroline Jay and Andre Freitas", "title": "Scholarly AI system diagrams as an access point to mental models", "comments": "16 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Complex systems, such as Artificial Intelligence (AI) systems, are comprised\nof many interrelated components. In order to represent these systems,\ndemonstrating the relations between components is essential. Perhaps because of\nthis, diagrams, as \"icons of relation\", are a prevalent medium for signifying\ncomplex systems. Diagrams used to communicate AI system architectures are\ncurrently extremely varied. The diversity in diagrammatic conceptual modelling\nchoices provides an opportunity to gain insight into the aspects which are\nbeing prioritised for communication. In this philosophical exploration of AI\nsystems diagrams, we integrate theories of conceptual models, communication\ntheory, and semiotics. We discuss consequences of standardised diagrammatic\nlanguages for AI systems, concluding that while we expect engineers\nimplementing systems to benefit from standards, researchers would have a larger\nbenefit from guidelines.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 07:55:18 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Marshall", "Guy Clarke", ""], ["Jay", "Caroline", ""], ["Freitas", "Andre", ""]]}, {"id": "2104.14815", "submitter": "Guy Marshall", "authors": "Guy Clarke Marshall and Caroline Jay and Andre Freitas", "title": "Number and quality of diagrams in scholarly publications is associated\n  with number of citations", "comments": "15 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Diagrams are often used in scholarly communication. We analyse a corpus of\ndiagrams found in scholarly computational linguistics conference proceedings\n(ACL 2017), and find inclusion of a system diagram to be correlated with higher\nnumbers of citations after 3 years. Inclusion of over three diagrams in this\n8-page limit conference was found to correlate with a lower citation count.\nFocusing on neural network system diagrams, we find a correlation between\nhighly cited papers and \"good diagramming practice\" quantified by level of\ncompliance with a set of diagramming guidelines. Two diagram classification\ntypes (one visually based, one mental model based) were not found to correlate\nwith number of citations, but enabled quantification of heterogeneity in those\ndimensions. Exploring scholarly paper-writing guides, we find diagrams to be a\nneglected media. This study suggests that diagrams may be a useful source of\nquality data for predicting citations, and that \"graphicacy\" is a key skill for\nscholars with insufficient support at present.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 08:01:30 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Marshall", "Guy Clarke", ""], ["Jay", "Caroline", ""], ["Freitas", "Andre", ""]]}, {"id": "2104.14870", "submitter": "Zahra Gharaee", "authors": "Zahra Gharaee", "title": "Action in Mind: A Neural Network Approach to Action Recognition and\n  Segmentation", "comments": "Lund University Cognitive Science 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Recognizing and categorizing human actions is an important task with\napplications in various fields such as human-robot interaction, video analysis,\nsurveillance, video retrieval, health care system and entertainment industry.\nThis thesis presents a novel computational approach for human action\nrecognition through different implementations of multi-layer architectures\nbased on artificial neural networks. Each system level development is designed\nto solve different aspects of the action recognition problem including online\nreal-time processing, action segmentation and the involvement of objects. The\nanalysis of the experimental results are illustrated and described in six\narticles. The proposed action recognition architecture of this thesis is\ncomposed of several processing layers including a preprocessing layer, an\nordered vector representation layer and three layers of neural networks. It\nutilizes self-organizing neural networks such as Kohonen feature maps and\ngrowing grids as the main neural network layers. Thus the architecture presents\na biological plausible approach with certain features such as topographic\norganization of the neurons, lateral interactions, semi-supervised learning and\nthe ability to represent high dimensional input space in lower dimensional\nmaps. For each level of development the system is trained with the input data\nconsisting of consecutive 3D body postures and tested with generalized input\ndata that the system has never met before. The experimental results of\ndifferent system level developments show that the system performs well with\nquite high accuracy for recognizing human actions.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 09:53:28 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Gharaee", "Zahra", ""]]}, {"id": "2104.14882", "submitter": "Nick Chen", "authors": "Junru Chen, Shiqing Geng, Yongluan Yan, Danyang Huang, Hao Liu, Yadong\n  Li", "title": "Vehicle Re-identification Method Based on Vehicle Attribute and Mutual\n  Exclusion Between Cameras", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle Re-identification aims to identify a specific vehicle across time and\ncamera view. With the rapid growth of intelligent transportation systems and\nsmart cities, vehicle Re-identification technology gets more and more\nattention. However, due to the difference of shooting angle and the high\nsimilarity of vehicles belonging to the same brand, vehicle re-identification\nbecomes a great challenge for existing method. In this paper, we propose a\nvehicle attribute-guided method to re-rank vehicle Re-ID result. The attributes\nused include vehicle orientation and vehicle brand . We also focus on the\ncamera information and introduce camera mutual exclusion theory to further\nfine-tune the search results. In terms of feature extraction, we combine the\ndata augmentations of multi-resolutions with the large model ensemble to get a\nmore robust vehicle features. Our method achieves mAP of 63.73% and rank-1\naccuracy 76.61% in the CVPR 2021 AI City Challenge.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 10:11:46 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Chen", "Junru", ""], ["Geng", "Shiqing", ""], ["Yan", "Yongluan", ""], ["Huang", "Danyang", ""], ["Liu", "Hao", ""], ["Li", "Yadong", ""]]}, {"id": "2104.14899", "submitter": "Wen Zhang", "authors": "Chi-Man Wong, Fan Feng, Wen Zhang, Chi-Man Vong, Hui Chen, Yichi\n  Zhang, Peng He, Huan Chen, Kun Zhao, Huajun Chen", "title": "Improving Conversational Recommendation System by Pretraining on\n  Billions Scale of Knowledge Graph", "comments": "Paper is accepted by ICDE2021 industry track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Conversational Recommender Systems (CRSs) in E-commerce platforms aim to\nrecommend items to users via multiple conversational interactions.\nClick-through rate (CTR) prediction models are commonly used for ranking\ncandidate items. However, most CRSs are suffer from the problem of data\nscarcity and sparseness. To address this issue, we propose a novel\nknowledge-enhanced deep cross network (K-DCN), a two-step (pretrain and\nfine-tune) CTR prediction model to recommend items. We first construct a\nbillion-scale conversation knowledge graph (CKG) from information about users,\nitems and conversations, and then pretrain CKG by introducing knowledge graph\nembedding method and graph convolution network to encode semantic and\nstructural information respectively.To make the CTR prediction model sensible\nof current state of users and the relationship between dialogues and items, we\nintroduce user-state and dialogue-interaction representations based on\npre-trained CKG and propose K-DCN.In K-DCN, we fuse the user-state\nrepresentation, dialogue-interaction representation and other normal feature\nrepresentations via deep cross network, which will give the rank of candidate\nitems to be recommended.We experimentally prove that our proposal significantly\noutperforms baselines and show it's real application in Alime.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 10:56:41 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Wong", "Chi-Man", ""], ["Feng", "Fan", ""], ["Zhang", "Wen", ""], ["Vong", "Chi-Man", ""], ["Chen", "Hui", ""], ["Zhang", "Yichi", ""], ["He", "Peng", ""], ["Chen", "Huan", ""], ["Zhao", "Kun", ""], ["Chen", "Huajun", ""]]}, {"id": "2104.14907", "submitter": "Dingming Yang D.Y.", "authors": "Dingming Yang, Yanrong Cui, Zeyu Yu and Hongqiang Yuan", "title": "Deep Learning Based Steel Pipe Weld Defect Detection", "comments": "17 pages,8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Steel pipes are widely used in high-risk and high-pressure scenarios such as\noil, chemical, natural gas, shale gas, etc. If there is some defect in steel\npipes, it will lead to serious adverse consequences. Applying object detection\nin the field of deep learning to pipe weld defect detection and identification\ncan effectively improve inspection efficiency and promote the development of\nindustrial automation. Most predecessors used traditional computer vision\nmethods applied to detect defects of steel pipe weld seams. However,\ntraditional computer vision methods rely on prior knowledge and can only detect\ndefects with a single feature, so it is difficult to complete the task of\nmulti-defect classification, while deep learning is end-to-end. In this paper,\nthe state-of-the-art single-stage object detection algorithm YOLOv5 is proposed\nto be applied to the field of steel pipe weld defect detection, and compared\nwith the two-stage representative object detection algorithm Faster R-CNN. The\nexperimental results show that applying YOLOv5 to steel pipe weld defect\ndetection can greatly improve the accuracy, complete the multi-classification\ntask, and meet the criteria of real-time detection.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 11:15:13 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Yang", "Dingming", ""], ["Cui", "Yanrong", ""], ["Yu", "Zeyu", ""], ["Yuan", "Hongqiang", ""]]}, {"id": "2104.14912", "submitter": "Kai Cui", "authors": "Ramzi Ourari, Kai Cui, Heinz Koeppl", "title": "Decentralized Swarm Collision Avoidance for Quadrotors via End-to-End\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collision avoidance algorithms are of central interest to many drone\napplications. In particular, decentralized approaches may be the key to\nenabling robust drone swarm solutions in cases where centralized communication\nbecomes computationally prohibitive. In this work, we draw biological\ninspiration from flocks of starlings (Sturnus vulgaris) and apply the insight\nto end-to-end learned decentralized collision avoidance. More specifically, we\npropose a new, scalable observation model following a biomimetic topological\ninteraction rule that leads to stable learning and robust avoidance behavior.\nAdditionally, prior work primarily focuses on invoking a separation principle,\ni.e. designing collision avoidance independent of specific tasks. By applying a\ngeneral reinforcement learning approach, we propose a holistic learning-based\napproach to integrating collision avoidance with various tasks and dynamics. To\nvalidate the generality of this approach, we successfully apply our methodology\nto a number of configurations. Our learned policies are tested in simulation\nand subsequently transferred to real-world drones to validate their real-world\napplicability.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 11:19:03 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Ourari", "Ramzi", ""], ["Cui", "Kai", ""], ["Koeppl", "Heinz", ""]]}, {"id": "2104.14917", "submitter": "Fuxian Li", "authors": "Fuxian Li, Jie Feng, Huan Yan, Guangyin Jin, Depeng Jin, and Yong Li", "title": "Dynamic Graph Convolutional Recurrent Network for Traffic Prediction:\n  Benchmark and Solution", "comments": "13 pages, journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traffic prediction is the cornerstone of an intelligent transportation\nsystem. Accurate traffic forecasting is essential for the applications of smart\ncities, i.e., intelligent traffic management and urban planning. Although\nvarious methods are proposed for spatio-temporal modeling, they ignore the\ndynamic characteristics of correlations among locations on road networks.\nMeanwhile, most Recurrent Neural Network (RNN) based works are not efficient\nenough due to their recurrent operations. Additionally, there is a severe lack\nof fair comparison among different methods on the same datasets. To address the\nabove challenges, in this paper, we propose a novel traffic prediction\nframework, named Dynamic Graph Convolutional Recurrent Network (DGCRN). In\nDGCRN, hyper-networks are designed to leverage and extract dynamic\ncharacteristics from node attributes, while the parameters of dynamic filters\nare generated at each time step. We filter the node embeddings and then use\nthem to generate a dynamic graph, which is integrated with a pre-defined static\ngraph. As far as we know, we are the first to employ a generation method to\nmodel fine topology of dynamic graph at each time step. Further, to enhance\nefficiency and performance, we employ a training strategy for DGCRN by\nrestricting the iteration number of decoder during forward and backward\npropagation. Finally, a reproducible standardized benchmark and a brand new\nrepresentative traffic dataset are opened for fair comparison and further\nresearch. Extensive experiments on three datasets demonstrate that our model\noutperforms 15 baselines consistently.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 11:25:43 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 14:11:25 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Li", "Fuxian", ""], ["Feng", "Jie", ""], ["Yan", "Huan", ""], ["Jin", "Guangyin", ""], ["Jin", "Depeng", ""], ["Li", "Yong", ""]]}, {"id": "2104.14961", "submitter": "Janet Rafner", "authors": "Janet Rafner, Miroslav Gajdacz, Gitte Kragh, Arthur Hjorth, Anna\n  Gander, Blanka Palfi, Aleks Berditchevskaia, Fran\\c{c}ois Grey, Kobi Gal, Avi\n  Segal, Mike Walmsley, Josh Aaron Miller, Dominik Dellerman, Muki Haklay,\n  Pietro Michelucci, Jacob Sherson", "title": "Revisiting Citizen Science Through the Lens of Hybrid Intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Intelligence (AI) can augment and sometimes even replace human\ncognition. Inspired by efforts to value human agency alongside productivity, we\ndiscuss the benefits of solving Citizen Science (CS) tasks with Hybrid\nIntelligence (HI), a synergetic mixture of human and artificial intelligence.\nCurrently there is no clear framework or methodology on how to create such an\neffective mixture. Due to the unique participant-centered set of values and the\nabundance of tasks drawing upon both human common sense and complex 21st\ncentury skills, we believe that the field of CS offers an invaluable testbed\nfor the development of HI and human-centered AI of the 21st century, while\nbenefiting CS as well. In order to investigate this potential, we first relate\nCS to adjacent computational disciplines. Then, we demonstrate that CS projects\ncan be grouped according to their potential for HI-enhancement by examining two\nkey dimensions: the level of digitization and the amount of knowledge or\nexperience required for participation. Finally, we propose a framework for\ntypes of human-AI interaction in CS based on established criteria of HI. This\n\"HI lens\" provides the CS community with an overview of several ways to utilize\nthe combination of AI and human intelligence in their projects. It also allows\nthe AI community to gain ideas on how developing AI in CS projects can further\ntheir own field.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 12:55:44 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Rafner", "Janet", ""], ["Gajdacz", "Miroslav", ""], ["Kragh", "Gitte", ""], ["Hjorth", "Arthur", ""], ["Gander", "Anna", ""], ["Palfi", "Blanka", ""], ["Berditchevskaia", "Aleks", ""], ["Grey", "Fran\u00e7ois", ""], ["Gal", "Kobi", ""], ["Segal", "Avi", ""], ["Walmsley", "Mike", ""], ["Miller", "Josh Aaron", ""], ["Dellerman", "Dominik", ""], ["Haklay", "Muki", ""], ["Michelucci", "Pietro", ""], ["Sherson", "Jacob", ""]]}, {"id": "2104.14962", "submitter": "Yuncong Yu", "authors": "Yuncong Yu, Dylan Kruyff, Tim Becker, Michael Behrisch", "title": "PSEUDo: Interactive Pattern Search in Multivariate Time Series with\n  Locality-Sensitive Hashing and Relevance Feedback", "comments": "11 pages including 2 pages for references, 10 figures including 1\n  teaser figure, sumbitted to IEEE VIS 2021, gitlab repository\n  https://git.science.uu.nl/vig/sublinear-algorithms-for-va/locality-sensitive-hashing-visual-analytics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present PSEUDo, an adaptive feature learning technique for exploring\nvisual patterns in multi-track sequential data. Our approach is designed with\nthe primary focus to overcome the uneconomic retraining requirements and\ninflexible representation learning in current deep learning-based systems.\nMulti-track time series data are generated on an unprecedented scale due to\nincreased sensors and data storage. These datasets hold valuable patterns, like\nin neuromarketing, where researchers try to link patterns in multivariate\nsequential data from physiological sensors to the purchase behavior of products\nand services. But a lack of ground truth and high variance make automatic\npattern detection unreliable. Our advancements are based on a novel query-aware\nlocality-sensitive hashing technique to create a feature-based representation\nof multivariate time series windows. Most importantly, our algorithm features\nsub-linear training and inference time. We can even accomplish both the\nmodeling and comparison of 10,000 different 64-track time series, each with 100\ntime steps (a typical EEG dataset) under 0.8 seconds. This performance gain\nallows for a rapid relevance feedback-driven adaption of the underlying pattern\nsimilarity model and enables the user to modify the speed-vs-accuracy trade-off\ngradually. We demonstrate superiority of PSEUDo in terms of efficiency,\naccuracy, and steerability through a quantitative performance comparison and a\nqualitative visual quality comparison to the state-of-the-art algorithms in the\nfield. Moreover, we showcase the usability of PSEUDo through a case study\ndemonstrating our visual pattern retrieval concepts in a large meteorological\ndataset. We find that our adaptive models can accurately capture the user's\nnotion of similarity and allow for an understandable exploratory visual pattern\nretrieval in large multivariate time series datasets.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 13:00:44 GMT"}, {"version": "v2", "created": "Mon, 10 May 2021 15:04:52 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Yu", "Yuncong", ""], ["Kruyff", "Dylan", ""], ["Becker", "Tim", ""], ["Behrisch", "Michael", ""]]}, {"id": "2104.14964", "submitter": "Albert Clap\\'es", "authors": "Penny Tarling, Mauricio Cantor, Albert Clap\\'es and Sergio Escalera", "title": "Deep learning with self-supervision and uncertainty regularization to\n  count fish in underwater images", "comments": "22 pages, 6 figures, submitted to indexed journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective conservation actions require effective population monitoring.\nHowever, accurately counting animals in the wild to inform conservation\ndecision-making is difficult. Monitoring populations through image sampling has\nmade data collection cheaper, wide-reaching and less intrusive but created a\nneed to process and analyse this data efficiently. Counting animals from such\ndata is challenging, particularly when densely packed in noisy images.\nAttempting this manually is slow and expensive, while traditional computer\nvision methods are limited in their generalisability. Deep learning is the\nstate-of-the-art method for many computer vision tasks, but it has yet to be\nproperly explored to count animals. To this end, we employ deep learning, with\na density-based regression approach, to count fish in low-resolution sonar\nimages. We introduce a large dataset of sonar videos, deployed to record wild\nmullet schools (Mugil liza), with a subset of 500 labelled images. We utilise\nabundant unlabelled data in a self-supervised task to improve the supervised\ncounting task. For the first time in this context, by introducing uncertainty\nquantification, we improve model training and provide an accompanying measure\nof prediction uncertainty for more informed biological decision-making.\nFinally, we demonstrate the generalisability of our proposed counting framework\nthrough testing it on a recent benchmark dataset of high-resolution annotated\nunderwater images from varying habitats (DeepFish). From experiments on both\ncontrasting datasets, we demonstrate our network outperforms the few other deep\nlearning models implemented for solving this task. By providing an open-source\nframework along with training data, our study puts forth an efficient deep\nlearning template for crowd counting aquatic animals thereby contributing\neffective methods to assess natural populations from the ever-increasing visual\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 13:02:19 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Tarling", "Penny", ""], ["Cantor", "Mauricio", ""], ["Clap\u00e9s", "Albert", ""], ["Escalera", "Sergio", ""]]}, {"id": "2104.15023", "submitter": "Ivan Lazarevich", "authors": "Ivan Lazarevich and Alexander Kozlov and Nikita Malinin", "title": "Post-training deep neural network pruning via layer-wise calibration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a post-training weight pruning method for deep neural networks\nthat achieves accuracy levels tolerable for the production setting and that is\nsufficiently fast to be run on commodity hardware such as desktop CPUs or edge\ndevices. We propose a data-free extension of the approach for computer vision\nmodels based on automatically-generated synthetic fractal images. We obtain\nstate-of-the-art results for data-free neural network pruning, with ~1.5% top@1\naccuracy drop for a ResNet50 on ImageNet at 50% sparsity rate. When using real\ndata, we are able to get a ResNet50 model on ImageNet with 65% sparsity rate in\n8-bit precision in a post-training setting with a ~1% top@1 accuracy drop. We\nrelease the code as a part of the OpenVINO(TM) Post-Training Optimization tool.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 14:20:51 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Lazarevich", "Ivan", ""], ["Kozlov", "Alexander", ""], ["Malinin", "Nikita", ""]]}, {"id": "2104.15034", "submitter": "Sz-Ting Tzeng", "authors": "Sz-Ting Tzeng (1), Nirav Ajmeri (2) and Munindar P. Singh (1) ((1)\n  North Carolina State University, (2) University of Bristol)", "title": "Noe: Norms Emergence and Robustness Based on Emotions in Multiagent\n  Systems", "comments": "15 pages with 8 figures. Accepted at COINE 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social norms characterize collective and acceptable group conducts in human\nsociety. Furthermore, some social norms emerge from interactions of agents or\nhumans. To achieve agent autonomy and make norm satisfaction explainable, we\ninclude emotions into the normative reasoning process, which evaluate whether\nto comply or violate a norm. Specifically, before selecting an action to\nexecute, an agent observes the environment and infer the state and consequences\nwith its internal states after norm satisfaction or violation of a social norm.\nBoth norm satisfaction and violation provoke further emotions, and the\nsubsequent emotions affect norm enforcement. This paper investigates how\nmodeling emotions affect the emergence and robustness of social norms via\nsocial simulation experiments. We find that an ability in agents to consider\nemotional responses to the outcomes of norm satisfaction and violation (1)\npromote norm compliance; and (2) improve societal welfare.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 14:42:22 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Tzeng", "Sz-Ting", ""], ["Ajmeri", "Nirav", ""], ["Singh", "Munindar P.", ""]]}, {"id": "2104.15040", "submitter": "Ruth Hoffmann", "authors": "Joan Espasa, Ian P. Gent, Ruth Hoffmann, Christopher Jefferson, Alice\n  M. Lynch", "title": "Using Small MUSes to Explain How to Solve Pen and Paper Puzzles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pen and paper puzzles like Sudoku, Futoshiki and Skyscrapers are hugely\npopular. Solving such puzzles can be a trivial task for modern AI systems.\nHowever, most AI systems solve problems using a form of backtracking, while\npeople try to avoid backtracking as much as possible. This means that existing\nAI systems do not output explanations about their reasoning that are meaningful\nto people. We present Demystify, a tool which allows puzzles to be expressed in\na high-level constraint programming language and uses MUSes to allow us to\nproduce descriptions of steps in the puzzle solving. We give several\nimprovements to the existing techniques for solving puzzles with MUSes, which\nallow us to solve a range of significantly more complex puzzles and give higher\nquality explanations. We demonstrate the effectiveness and generality of\nDemystify by comparing its results to documented strategies for solving a range\nof pen and paper puzzles by hand, showing that our technique can find many of\nthe same explanations.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 15:07:51 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Espasa", "Joan", ""], ["Gent", "Ian P.", ""], ["Hoffmann", "Ruth", ""], ["Jefferson", "Christopher", ""], ["Lynch", "Alice M.", ""]]}, {"id": "2104.15049", "submitter": "Xinglong Sun", "authors": "Xinglong Sun, Guangliang Han, Lihong Guo, Tingfa Xu, Jianan Li, Peixun\n  Liu", "title": "Updatable Siamese Tracker with Two-stage One-shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Offline Siamese networks have achieved very promising tracking performance,\nespecially in accuracy and efficiency. However, they often fail to track an\nobject in complex scenes due to the incapacity in online update. Traditional\nupdaters are difficult to process the irregular variations and sampling noises\nof objects, so it is quite risky to adopt them to update Siamese networks. In\nthis paper, we first present a two-stage one-shot learner, which can predict\nthe local parameters of primary classifier with object samples from diverse\nstages. Then, an updatable Siamese network is proposed based on the learner\n(SiamTOL), which is able to complement online update by itself. Concretely, we\nintroduce an extra inputting branch to sequentially capture the latest object\nfeatures, and design a residual module to update the initial exemplar using\nthese features. Besides, an effective multi-aspect training loss is designed\nfor our network to avoid overfit. Extensive experimental results on several\npopular benchmarks including OTB100, VOT2018, VOT2019, LaSOT, UAV123 and GOT10k\nmanifest that the proposed tracker achieves the leading performance and\noutperforms other state-of-the-art methods\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 15:18:41 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Sun", "Xinglong", ""], ["Han", "Guangliang", ""], ["Guo", "Lihong", ""], ["Xu", "Tingfa", ""], ["Li", "Jianan", ""], ["Liu", "Peixun", ""]]}, {"id": "2104.15061", "submitter": "Haoxi Zhan", "authors": "Haoxi Zhan, Xiaobing Pei", "title": "Black-box Gradient Attack on Graph Neural Networks: Deeper Insights in\n  Graph-based Attack and Defense", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph Neural Networks (GNNs) have received significant attention due to their\nstate-of-the-art performance on various graph representation learning tasks.\nHowever, recent studies reveal that GNNs are vulnerable to adversarial attacks,\ni.e. an attacker is able to fool the GNNs by perturbing the graph structure or\nnode features deliberately. While being able to successfully decrease the\nperformance of GNNs, most existing attacking algorithms require access to\neither the model parameters or the training data, which is not practical in the\nreal world.\n  In this paper, we develop deeper insights into the Mettack algorithm, which\nis a representative grey-box attacking method, and then we propose a\ngradient-based black-box attacking algorithm. Firstly, we show that the Mettack\nalgorithm will perturb the edges unevenly, thus the attack will be highly\ndependent on a specific training set. As a result, a simple yet useful strategy\nto defense against Mettack is to train the GNN with the validation set.\nSecondly, to overcome the drawbacks, we propose the Black-Box Gradient Attack\n(BBGA) algorithm. Extensive experiments demonstrate that out proposed method is\nable to achieve stable attack performance without accessing the training sets\nof the GNNs. Further results shows that our proposed method is also applicable\nwhen attacking against various defense methods.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 15:30:47 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Zhan", "Haoxi", ""], ["Pei", "Xiaobing", ""]]}, {"id": "2104.15075", "submitter": "Jiehua Chen", "authors": "Jiehua Chen, Martin Lackner, Jan Maly", "title": "Participatory Budgeting with Donations and Diversity Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.CC cs.DS cs.GT", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Participatory budgeting (PB) is a democratic process where citizens jointly\ndecide on how to allocate public funds to indivisible projects. This paper\nfocuses on PB processes where citizens may give additional money to projects\nthey want to see funded. We introduce a formal framework for this kind of PB\nwith donations. Our framework also allows for diversity constraints, meaning\nthat each project belongs to one or more types, and there are lower and upper\nbounds on the number of projects of the same type that can be funded. We\npropose three general classes of methods for aggregating the citizens'\npreferences in the presence of donations and analyze their axiomatic\nproperties. Furthermore, we investigate the computational complexity of\ndetermining the outcome of a PB process with donations and of finding a\ncitizen's optimal donation strategy.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 15:48:25 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Chen", "Jiehua", ""], ["Lackner", "Martin", ""], ["Maly", "Jan", ""]]}, {"id": "2104.15083", "submitter": "Rajarshi Roy", "authors": "Jean-Rapha\\\"el Gaglione, Daniel Neider, Rajarshi Roy, Ufuk Topcu and\n  Zhe Xu", "title": "Learning Linear Temporal Properties from Noisy Data: A MaxSAT Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of inferring descriptions of system behavior using\nLinear Temporal Logic (LTL) from a finite set of positive and negative\nexamples. Most of the existing approaches for solving such a task rely on\npredefined templates for guiding the structure of the inferred formula. The\napproaches that can infer arbitrary LTL formulas, on the other hand, are not\nrobust to noise in the data. To alleviate such limitations, we devise two\nalgorithms for inferring concise LTL formulas even in the presence of noise.\nOur first algorithm infers minimal LTL formulas by reducing the inference\nproblem to a problem in maximum satisfiability and then using off-the-shelf\nMaxSAT solvers to find a solution. To the best of our knowledge, we are the\nfirst to incorporate the usage of MaxSAT solvers for inferring formulas in LTL.\nOur second learning algorithm relies on the first algorithm to derive a\ndecision tree over LTL formulas based on a decision tree learning algorithm. We\nhave implemented both our algorithms and verified that our algorithms are\nefficient in extracting concise LTL descriptions even in the presence of noise.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 16:06:03 GMT"}, {"version": "v2", "created": "Thu, 24 Jun 2021 19:56:55 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Gaglione", "Jean-Rapha\u00ebl", ""], ["Neider", "Daniel", ""], ["Roy", "Rajarshi", ""], ["Topcu", "Ufuk", ""], ["Xu", "Zhe", ""]]}, {"id": "2104.15129", "submitter": "Yulong Tian", "authors": "Yulong Tian, Fnu Suya, Fengyuan Xu, David Evans", "title": "Stealthy Backdoors as Compression Artifacts", "comments": "20 pages, 9 figures, 14 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a backdoor attack on a machine learning model, an adversary produces a\nmodel that performs well on normal inputs but outputs targeted\nmisclassifications on inputs containing a small trigger pattern. Model\ncompression is a widely-used approach for reducing the size of deep learning\nmodels without much accuracy loss, enabling resource-hungry models to be\ncompressed for use on resource-constrained devices. In this paper, we study the\nrisk that model compression could provide an opportunity for adversaries to\ninject stealthy backdoors. We design stealthy backdoor attacks such that the\nfull-sized model released by adversaries appears to be free from backdoors\n(even when tested using state-of-the-art techniques), but when the model is\ncompressed it exhibits highly effective backdoors. We show this can be done for\ntwo common model compression techniques -- model pruning and model\nquantization. Our findings demonstrate how an adversary may be able to hide a\nbackdoor as a compression artifact, and show the importance of performing\nsecurity tests on the models that will actually be deployed not their\nprecompressed version.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 17:35:18 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Tian", "Yulong", ""], ["Suya", "Fnu", ""], ["Xu", "Fengyuan", ""], ["Evans", "David", ""]]}, {"id": "2104.15135", "submitter": "Piyawat Lertvittayakumjorn", "authors": "Piyawat Lertvittayakumjorn, Francesca Toni", "title": "Explanation-Based Human Debugging of NLP Models: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To fix a bug in a program, we need to locate where the bug is, understand why\nit causes the problem, and patch the code accordingly. This process becomes\nharder when the program is a trained machine learning model and even harder for\nopaque deep learning models. In this survey, we review papers that exploit\nexplanations to enable humans to debug NLP models. We call this problem\nexplanation-based human debugging (EBHD). In particular, we categorize and\ndiscuss existing works along three main dimensions of EBHD (the bug context,\nthe workflow, and the experimental setting), compile findings on how EBHD\ncomponents affect human debuggers, and highlight open problems that could be\nfuture research directions.\n", "versions": [{"version": "v1", "created": "Fri, 30 Apr 2021 17:53:07 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Lertvittayakumjorn", "Piyawat", ""], ["Toni", "Francesca", ""]]}]