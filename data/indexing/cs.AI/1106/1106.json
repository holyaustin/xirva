[{"id": "1106.0171", "submitter": "Gilberto de Paiva", "authors": "Gilberto de Paiva", "title": "Proposal of Pattern Recognition as a necessary and sufficient Principle\n  to Cognitive Science", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the prevalence of the Computational Theory of Mind and the\nConnectionist Model, the establishing of the key principles of the Cognitive\nScience are still controversy and inconclusive. This paper proposes the concept\nof Pattern Recognition as Necessary and Sufficient Principle for a general\ncognitive science modeling, in a very ambitious scientific proposal. A formal\nphysical definition of the pattern recognition concept is also proposed to\nsolve many key conceptual gaps on the field.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2011 06:40:52 GMT"}], "update_date": "2011-06-02", "authors_parsed": [["de Paiva", "Gilberto", ""]]}, {"id": "1106.0218", "submitter": "E. Birnbaum", "authors": "E. Birnbaum, E. L. Lozinskii", "title": "The Good Old Davis-Putnam Procedure Helps Counting Models", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 10, pages\n  457-477, 1999", "doi": "10.1613/jair.601", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As was shown recently, many important AI problems require counting the number\nof models of propositional formulas. The problem of counting models of such\nformulas is, according to present knowledge, computationally intractable in a\nworst case. Based on the Davis-Putnam procedure, we present an algorithm, CDP,\nthat computes the exact number of models of a propositional CNF or DNF formula\nF. Let m and n be the number of clauses and variables of F, respectively, and\nlet p denote the probability that a literal l of F occurs in a clause C of F,\nthen the average running time of CDP is shown to be O(nm^d), where\nd=-1/log(1-p). The practical performance of CDP has been estimated in a series\nof experiments on a wide variety of CNF formulas.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2011 16:14:46 GMT"}], "update_date": "2011-06-02", "authors_parsed": [["Birnbaum", "E.", ""], ["Lozinskii", "E. L.", ""]]}, {"id": "1106.0219", "submitter": "C. E. Brodley", "authors": "C. E. Brodley, M. A. Friedl", "title": "Identifying Mislabeled Training Data", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 11, pages\n  131-167, 1999", "doi": "10.1613/jair.606", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new approach to identifying and eliminating mislabeled\ntraining instances for supervised learning. The goal of this approach is to\nimprove classification accuracies produced by learning algorithms by improving\nthe quality of the training data. Our approach uses a set of learning\nalgorithms to create classifiers that serve as noise filters for the training\ndata. We evaluate single algorithm, majority vote and consensus filters on five\ndatasets that are prone to labeling errors. Our experiments illustrate that\nfiltering significantly improves classification accuracy for noise levels up to\n30 percent. An analytical and empirical evaluation of the precision of our\napproach shows that consensus filters are conservative at throwing away good\ndata at the expense of retaining bad data and that majority filters are better\nat detecting bad data at the expense of throwing away good data. This suggests\nthat for situations in which there is a paucity of data, consensus filters are\npreferable, whereas majority vote filters are preferable for situations with an\nabundance of data.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2011 16:15:28 GMT"}], "update_date": "2011-06-02", "authors_parsed": [["Brodley", "C. E.", ""], ["Friedl", "M. A.", ""]]}, {"id": "1106.0220", "submitter": "S. Argamon-Engelson", "authors": "S. Argamon-Engelson, I. Dagan", "title": "Committee-Based Sample Selection for Probabilistic Classifiers", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 11, pages\n  335-360, 1999", "doi": "10.1613/jair.612", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many real-world learning tasks, it is expensive to acquire a sufficient\nnumber of labeled examples for training. This paper investigates methods for\nreducing annotation cost by `sample selection'. In this approach, during\ntraining the learning program examines many unlabeled examples and selects for\nlabeling only those that are most informative at each stage. This avoids\nredundantly labeling examples that contribute little new information. Our work\nfollows on previous research on Query By Committee, extending the\ncommittee-based paradigm to the context of probabilistic classification. We\ndescribe a family of empirical methods for committee-based sample selection in\nprobabilistic classification models, which evaluate the informativeness of an\nexample by measuring the degree of disagreement between several model variants.\nThese variants (the committee) are drawn randomly from a probability\ndistribution conditioned by the training set labeled so far. The method was\napplied to the real-world natural language processing task of stochastic\npart-of-speech tagging. We find that all variants of the method achieve a\nsignificant reduction in annotation cost, although their computational\nefficiency differs. In particular, the simplest variant, a two member committee\nwith no parameters to tune, gives excellent results. We also show that sample\nselection yields a significant reduction in the size of the model used by the\ntagger.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2011 16:15:56 GMT"}], "update_date": "2011-06-02", "authors_parsed": [["Argamon-Engelson", "S.", ""], ["Dagan", "I.", ""]]}, {"id": "1106.0221", "submitter": "J. J. Grefenstette", "authors": "J. J. Grefenstette, D. E. Moriarty, A. C. Schultz", "title": "Evolutionary Algorithms for Reinforcement Learning", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 11, pages\n  241-276, 1999", "doi": "10.1613/jair.613", "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are two distinct approaches to solving reinforcement learning problems,\nnamely, searching in value function space and searching in policy space.\nTemporal difference methods and evolutionary algorithms are well-known examples\nof these approaches. Kaelbling, Littman and Moore recently provided an\ninformative survey of temporal difference methods. This article focuses on the\napplication of evolutionary algorithms to the reinforcement learning problem,\nemphasizing alternative policy representations, credit assignment methods, and\nproblem-specific genetic operators. Strengths and weaknesses of the\nevolutionary approach to reinforcement learning are presented, along with a\nsurvey of representative applications.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2011 16:16:14 GMT"}], "update_date": "2011-06-02", "authors_parsed": [["Grefenstette", "J. J.", ""], ["Moriarty", "D. E.", ""], ["Schultz", "A. C.", ""]]}, {"id": "1106.0222", "submitter": "W. Burgard", "authors": "W. Burgard, D. Fox, S. Thrun", "title": "Markov Localization for Mobile Robots in Dynamic Environments", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 11, pages\n  391-427, 1999", "doi": "10.1613/jair.616", "report-no": null, "categories": "cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Localization, that is the estimation of a robot's location from sensor data,\nis a fundamental problem in mobile robotics. This papers presents a version of\nMarkov localization which provides accurate position estimates and which is\ntailored towards dynamic environments. The key idea of Markov localization is\nto maintain a probability density over the space of all locations of a robot in\nits environment. Our approach represents this space metrically, using a\nfine-grained grid to approximate densities. It is able to globally localize the\nrobot from scratch and to recover from localization failures. It is robust to\napproximate models of the environment (such as occupancy grid maps) and noisy\nsensors (such as ultrasound sensors). Our approach also includes a filtering\ntechnique which allows a mobile robot to reliably estimate its position even in\ndensely populated environments in which crowds of people block the robot's\nsensors for extended periods of time. The method described here has been\nimplemented and tested in several real-world applications of mobile robots,\nincluding the deployments of two mobile robots as interactive museum\ntour-guides.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2011 16:16:48 GMT"}], "update_date": "2011-06-02", "authors_parsed": [["Burgard", "W.", ""], ["Fox", "D.", ""], ["Thrun", "S.", ""]]}, {"id": "1106.0223", "submitter": "H. Akkermans", "authors": "H. Akkermans, F. Ygge", "title": "Decentralized Markets versus Central Control: A Comparative Study", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 11, pages\n  301-333, 1999", "doi": "10.1613/jair.627", "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-Agent Systems (MAS) promise to offer solutions to problems where\nestablished, older paradigms fall short. In order to validate such claims that\nare repeatedly made in software agent publications, empirical in-depth studies\nof advantages and weaknesses of multi-agent solutions versus conventional ones\nin practical applications are needed. Climate control in large buildings is one\napplication area where multi-agent systems, and market-oriented programming in\nparticular, have been reported to be very successful, although central control\nsolutions are still the standard practice. We have therefore constructed and\nimplemented a variety of market designs for this problem, as well as different\nstandard control engineering solutions. This article gives a detailed analysis\nand comparison, so as to learn about differences between standard versus agent\napproaches, and yielding new insights about benefits and limitations of\ncomputational markets. An important outcome is that \"local information plus\nmarket communication produces global control\".\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2011 16:17:03 GMT"}], "update_date": "2011-06-02", "authors_parsed": [["Akkermans", "H.", ""], ["Ygge", "F.", ""]]}, {"id": "1106.0224", "submitter": "R. Rosati", "authors": "R. Rosati", "title": "Reasoning about Minimal Belief and Negation as Failure", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 11, pages\n  277-300, 1999", "doi": "10.1613/jair.637", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of reasoning in the propositional fragment of\nMBNF, the logic of minimal belief and negation as failure introduced by\nLifschitz, which can be considered as a unifying framework for several\nnonmonotonic formalisms, including default logic, autoepistemic logic,\ncircumscription, epistemic queries, and logic programming. We characterize the\ncomplexity and provide algorithms for reasoning in propositional MBNF. In\nparticular, we show that entailment in propositional MBNF lies at the third\nlevel of the polynomial hierarchy, hence it is harder than reasoning in all the\nabove mentioned propositional formalisms for nonmonotonic reasoning. We also\nprove the exact correspondence between negation as failure in MBNF and negative\nintrospection in Moore's autoepistemic logic.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2011 16:17:18 GMT"}], "update_date": "2011-06-02", "authors_parsed": [["Rosati", "R.", ""]]}, {"id": "1106.0225", "submitter": "R. Bar-Yehuda", "authors": "R. Bar-Yehuda, A. Becker, D. Geiger", "title": "Randomized Algorithms for the Loop Cutset Problem", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 12, pages\n  219-234, 2000", "doi": "10.1613/jair.638", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how to find a minimum weight loop cutset in a Bayesian network with\nhigh probability. Finding such a loop cutset is the first step in the method of\nconditioning for inference. Our randomized algorithm for finding a loop cutset\noutputs a minimum loop cutset after O(c 6^k kn) steps with probability at least\n1 - (1 - 1/(6^k))^c6^k, where c > 1 is a constant specified by the user, k is\nthe minimal size of a minimum weight loop cutset, and n is the number of\nvertices. We also show empirically that a variant of this algorithm often finds\na loop cutset that is closer to the minimum weight loop cutset than the ones\nfound by the best deterministic algorithms known.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2011 16:17:38 GMT"}], "update_date": "2011-06-02", "authors_parsed": [["Bar-Yehuda", "R.", ""], ["Becker", "A.", ""], ["Geiger", "D.", ""]]}, {"id": "1106.0229", "submitter": "R. M. Jensen", "authors": "R. M. Jensen, M. M. Veloso", "title": "OBDD-based Universal Planning for Synchronized Agents in\n  Non-Deterministic Domains", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 13, pages\n  189-226, 2000", "doi": "10.1613/jair.649", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently model checking representation and search techniques were shown to be\nefficiently applicable to planning, in particular to non-deterministic\nplanning. Such planning approaches use Ordered Binary Decision Diagrams (OBDDs)\nto encode a planning domain as a non-deterministic finite automaton and then\napply fast algorithms from model checking to search for a solution. OBDDs can\neffectively scale and can provide universal plans for complex planning domains.\nWe are particularly interested in addressing the complexities arising in\nnon-deterministic, multi-agent domains. In this article, we present UMOP, a new\nuniversal OBDD-based planning framework for non-deterministic, multi-agent\ndomains. We introduce a new planning domain description language, NADL, to\nspecify non-deterministic, multi-agent domains. The language contributes the\nexplicit definition of controllable agents and uncontrollable environment\nagents. We describe the syntax and semantics of NADL and show how to build an\nefficient OBDD-based representation of an NADL description. The UMOP planning\nsystem uses NADL and different OBDD-based universal planning algorithms. It\nincludes the previously developed strong and strong cyclic planning algorithms.\nIn addition, we introduce our new optimistic planning algorithm that relaxes\noptimality guarantees and generates plausible universal plans in some domains\nwhere no strong nor strong cyclic solution exists. We present empirical results\napplying UMOP to domains ranging from deterministic and single-agent with no\nenvironment actions to non-deterministic and multi-agent with complex\nenvironment actions. UMOP is shown to be a rich and efficient planning system.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2011 16:22:36 GMT"}], "update_date": "2011-06-02", "authors_parsed": [["Jensen", "R. M.", ""], ["Veloso", "M. M.", ""]]}, {"id": "1106.0230", "submitter": "S. Kambhampati", "authors": "S. Kambhampati", "title": "Planning Graph as a (Dynamic) CSP: Exploiting EBL, DDB and other CSP\n  Search Techniques in Graphplan", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 12, pages\n  1-34, 2000", "doi": "10.1613/jair.655", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews the connections between Graphplan's planning-graph and the\ndynamic constraint satisfaction problem and motivates the need for adapting CSP\nsearch techniques to the Graphplan algorithm. It then describes how explanation\nbased learning, dependency directed backtracking, dynamic variable ordering,\nforward checking, sticky values and random-restart search strategies can be\nadapted to Graphplan. Empirical results are provided to demonstrate that these\naugmentations improve Graphplan's performance significantly (up to 1000x\nspeedups) on several benchmark problems. Special attention is paid to the\nexplanation-based learning and dependency directed backtracking techniques as\nthey are empirically found to be most useful in improving the performance of\nGraphplan.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2011 16:22:50 GMT"}], "update_date": "2011-06-02", "authors_parsed": [["Kambhampati", "S.", ""]]}, {"id": "1106.0233", "submitter": "M. Cadoli", "authors": "M. Cadoli, F. M. Donini, P. Liberatore, M. Schaerf", "title": "Space Efficiency of Propositional Knowledge Representation Formalisms", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 13, pages\n  1-31, 2000", "doi": "10.1613/jair.664", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the space efficiency of a Propositional Knowledge\nRepresentation (PKR) formalism. Intuitively, the space efficiency of a\nformalism F in representing a certain piece of knowledge A, is the size of the\nshortest formula of F that represents A. In this paper we assume that knowledge\nis either a set of propositional interpretations (models) or a set of\npropositional formulae (theorems). We provide a formal way of talking about the\nrelative ability of PKR formalisms to compactly represent a set of models or a\nset of theorems. We introduce two new compactness measures, the corresponding\nclasses, and show that the relative space efficiency of a PKR formalism in\nrepresenting models/theorems is directly related to such classes. In\nparticular, we consider formalisms for nonmonotonic reasoning, such as\ncircumscription and default logic, as well as belief revision operators and the\nstable model semantics for logic programs with negation. One interesting result\nis that formalisms with the same time complexity do not necessarily belong to\nthe same space efficiency class.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2011 16:24:29 GMT"}], "update_date": "2011-06-02", "authors_parsed": [["Cadoli", "M.", ""], ["Donini", "F. M.", ""], ["Liberatore", "P.", ""], ["Schaerf", "M.", ""]]}, {"id": "1106.0234", "submitter": "M. Hauskrecht", "authors": "M. Hauskrecht", "title": "Value-Function Approximations for Partially Observable Markov Decision\n  Processes", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 13, pages\n  33-94, 2000", "doi": "10.1613/jair.678", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partially observable Markov decision processes (POMDPs) provide an elegant\nmathematical framework for modeling complex decision and planning problems in\nstochastic domains in which states of the system are observable only\nindirectly, via a set of imperfect or noisy observations. The modeling\nadvantage of POMDPs, however, comes at a price -- exact methods for solving\nthem are computationally very expensive and thus applicable in practice only to\nvery simple problems. We focus on efficient approximation (heuristic) methods\nthat attempt to alleviate the computational problem and trade off accuracy for\nspeed. We have two objectives here. First, we survey various approximation\nmethods, analyze their properties and relations and provide some new insights\ninto their differences. Second, we present a number of new approximation\nmethods and novel refinements of existing techniques. The theoretical results\nare supported by experiments on a problem from the agent navigation domain.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2011 16:24:43 GMT"}], "update_date": "2011-06-02", "authors_parsed": [["Hauskrecht", "M.", ""]]}, {"id": "1106.0235", "submitter": "G. A. Kaminka", "authors": "G. A. Kaminka, M. Tambe", "title": "Robust Agent Teams via Socially-Attentive Monitoring", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 12, pages\n  105-147, 2000", "doi": "10.1613/jair.682", "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Agents in dynamic multi-agent environments must monitor their peers to\nexecute individual and group plans. A key open question is how much monitoring\nof other agents' states is required to be effective: The Monitoring Selectivity\nProblem. We investigate this question in the context of detecting failures in\nteams of cooperating agents, via Socially-Attentive Monitoring, which focuses\non monitoring for failures in the social relationships between the agents. We\nempirically and analytically explore a family of socially-attentive teamwork\nmonitoring algorithms in two dynamic, complex, multi-agent domains, under\nvarying conditions of task distribution and uncertainty. We show that a\ncentralized scheme using a complex algorithm trades correctness for\ncompleteness and requires monitoring all teammates. In contrast, a simple\ndistributed teamwork monitoring algorithm results in correct and complete\ndetection of teamwork failures, despite relying on limited, uncertain\nknowledge, and monitoring only key agents in a team. In addition, we report on\nthe design of a socially-attentive monitoring system and demonstrate its\ngenerality in monitoring several coordination relationships, diagnosing\ndetected failures, and both on-line and off-line applications.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2011 16:25:00 GMT"}], "update_date": "2011-06-02", "authors_parsed": [["Kaminka", "G. A.", ""], ["Tambe", "M.", ""]]}, {"id": "1106.0237", "submitter": "R. M. Neal", "authors": "R. M. Neal", "title": "On Deducing Conditional Independence from d-Separation in Causal Graphs\n  with Feedback (Research Note)", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 12, pages\n  87-91, 2000", "doi": "10.1613/jair.689", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pearl and Dechter (1996) claimed that the d-separation criterion for\nconditional independence in acyclic causal networks also applies to networks of\ndiscrete variables that have feedback cycles, provided that the variables of\nthe system are uniquely determined by the random disturbances. I show by\nexample that this is not true in general. Some condition stronger than\nuniqueness is needed, such as the existence of a causal dynamics guaranteed to\nlead to the unique solution.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2011 16:36:47 GMT"}], "update_date": "2011-06-02", "authors_parsed": [["Neal", "R. M.", ""]]}, {"id": "1106.0238", "submitter": "A. Borgida", "authors": "A. Borgida, R. Kusters", "title": "What's in an Attribute? Consequences for the Least Common Subsumer", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 14, pages\n  167-203, 2001", "doi": "10.1613/jair.702", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional relationships between objects, called `attributes', are of\nconsiderable importance in knowledge representation languages, including\nDescription Logics (DLs). A study of the literature indicates that papers have\nmade, often implicitly, different assumptions about the nature of attributes:\nwhether they are always required to have a value, or whether they can be\npartial functions. The work presented here is the first explicit study of this\ndifference for subclasses of the CLASSIC DL, involving the same-as concept\nconstructor. It is shown that although determining subsumption between concept\ndescriptions has the same complexity (though requiring different algorithms),\nthe story is different in the case of determining the least common subsumer\n(lcs). For attributes interpreted as partial functions, the lcs exists and can\nbe computed relatively easily; even in this case our results correct and extend\nthree previous papers about the lcs of DLs. In the case where attributes must\nhave a value, the lcs may not exist, and even if it exists it may be of\nexponential size. Interestingly, it is possible to decide in polynomial time if\nthe lcs exists.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2011 16:36:59 GMT"}], "update_date": "2011-06-02", "authors_parsed": [["Borgida", "A.", ""], ["Kusters", "R.", ""]]}, {"id": "1106.0239", "submitter": "S. Tobies", "authors": "S. Tobies", "title": "The Complexity of Reasoning with Cardinality Restrictions and Nominals\n  in Expressive Description Logics", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 12, pages\n  199-217, 2000", "doi": "10.1613/jair.705", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of the combination of the Description Logics ALCQ and\nALCQI with a terminological formalism based on cardinality restrictions on\nconcepts. These combinations can naturally be embedded into C^2, the two\nvariable fragment of predicate logic with counting quantifiers, which yields\ndecidability in NExpTime. We show that this approach leads to an optimal\nsolution for ALCQI, as ALCQI with cardinality restrictions has the same\ncomplexity as C^2 (NExpTime-complete). In contrast, we show that for ALCQ, the\nproblem can be solved in ExpTime. This result is obtained by a reduction of\nreasoning with cardinality restrictions to reasoning with the (in general\nweaker) terminological formalism of general axioms for ALCQ extended with\nnominals. Using the same reduction, we show that, for the extension of ALCQI\nwith nominals, reasoning with general axioms is a NExpTime-complete problem.\nFinally, we sharpen this result and show that pure concept satisfiability for\nALCQI with nominals is NExpTime-complete. Without nominals, this problem is\nknown to be PSpace-complete.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2011 16:37:11 GMT"}], "update_date": "2011-06-02", "authors_parsed": [["Tobies", "S.", ""]]}, {"id": "1106.0240", "submitter": "I. P. Gent", "authors": "I. P. Gent, J. Singer, A. Smaill", "title": "Backbone Fragility and the Local Search Cost Peak", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 12, pages\n  235-270, 2000", "doi": "10.1613/jair.711", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The local search algorithm WSat is one of the most successful algorithms for\nsolving the satisfiability (SAT) problem. It is notably effective at solving\nhard Random 3-SAT instances near the so-called `satisfiability threshold', but\nstill shows a peak in search cost near the threshold and large variations in\ncost over different instances. We make a number of significant contributions to\nthe analysis of WSat on high-cost random instances, using the\nrecently-introduced concept of the backbone of a SAT instance. The backbone is\nthe set of literals which are entailed by an instance. We find that the number\nof solutions predicts the cost well for small-backbone instances but is much\nless relevant for the large-backbone instances which appear near the threshold\nand dominate in the overconstrained region. We show a very strong correlation\nbetween search cost and the Hamming distance to the nearest solution early in\nWSat's search. This pattern leads us to introduce a measure of the backbone\nfragility of an instance, which indicates how persistent the backbone is as\nclauses are removed. We propose that high-cost random instances for local\nsearch are those with very large backbones which are also backbone-fragile. We\nsuggest that the decay in cost beyond the satisfiability threshold is due to\nincreasing backbone robustness (the opposite of backbone fragility). Our\nhypothesis makes three correct predictions. First, that the backbone robustness\nof an instance is negatively correlated with the local search cost when other\nfactors are controlled for. Second, that backbone-minimal instances (which are\n3-SAT instances altered so as to be more backbone-fragile) are unusually hard\nfor WSat. Third, that the clauses most often unsatisfied during search are\nthose whose deletion has the most effect on the backbone. In understanding the\npathologies of local search methods, we hope to contribute to the development\nof new and better techniques.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2011 16:37:25 GMT"}], "update_date": "2011-06-02", "authors_parsed": [["Gent", "I. P.", ""], ["Singer", "J.", ""], ["Smaill", "A.", ""]]}, {"id": "1106.0241", "submitter": "M. A. Walker", "authors": "M. A. Walker", "title": "An Application of Reinforcement Learning to Dialogue Strategy Selection\n  in a Spoken Dialogue System for Email", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 12, pages\n  387-416, 2000", "doi": "10.1613/jair.713", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a novel method by which a spoken dialogue system can\nlearn to choose an optimal dialogue strategy from its experience interacting\nwith human users. The method is based on a combination of reinforcement\nlearning and performance modeling of spoken dialogue systems. The reinforcement\nlearning component applies Q-learning (Watkins, 1989), while the performance\nmodeling component applies the PARADISE evaluation framework (Walker et al.,\n1997) to learn the performance function (reward) used in reinforcement\nlearning. We illustrate the method with a spoken dialogue system named ELVIS\n(EmaiL Voice Interactive System), that supports access to email over the phone.\nWe conduct a set of experiments for training an optimal dialogue strategy on a\ncorpus of 219 dialogues in which human users interact with ELVIS over the\nphone. We then test that strategy on a corpus of 18 dialogues. We show that\nELVIS can learn to optimize its strategy selection for agent initiative, for\nreading messages, and for summarizing email folders.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2011 16:37:37 GMT"}], "update_date": "2011-06-02", "authors_parsed": [["Walker", "M. A.", ""]]}, {"id": "1106.0242", "submitter": "J. Goldsmith", "authors": "J. Goldsmith, C. Lusena, M. Mundhenk", "title": "Nonapproximability Results for Partially Observable Markov Decision\n  Processes", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 14, pages\n  83-103, 2001", "doi": "10.1613/jair.714", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that for several variations of partially observable Markov decision\nprocesses, polynomial-time algorithms for finding control policies are unlikely\nto or simply don't have guarantees of finding policies within a constant factor\nor a constant summand of optimal. Here \"unlikely\" means \"unless some complexity\nclasses collapse,\" where the collapses considered are P=NP, P=PSPACE, or P=EXP.\nUntil or unless these collapses are shown to hold, any control-policy designer\nmust choose between such performance guarantees and efficient computation.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2011 16:37:53 GMT"}], "update_date": "2011-06-02", "authors_parsed": [["Goldsmith", "J.", ""], ["Lusena", "C.", ""], ["Mundhenk", "M.", ""]]}, {"id": "1106.0243", "submitter": "J. Hoffmann", "authors": "J. Hoffmann, J. Koehler", "title": "On Reasonable and Forced Goal Orderings and their Use in an\n  Agenda-Driven Planning Algorithm", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 12, pages\n  338-386, 2000", "doi": "10.1613/jair.715", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper addresses the problem of computing goal orderings, which is one of\nthe longstanding issues in AI planning. It makes two new contributions. First,\nit formally defines and discusses two different goal orderings, which are\ncalled the reasonable and the forced ordering. Both orderings are defined for\nsimple STRIPS operators as well as for more complex ADL operators supporting\nnegation and conditional effects. The complexity of these orderings is\ninvestigated and their practical relevance is discussed. Secondly, two\ndifferent methods to compute reasonable goal orderings are developed. One of\nthem is based on planning graphs, while the other investigates the set of\nactions directly. Finally, it is shown how the ordering relations, which have\nbeen derived for a given set of goals G, can be used to compute a so-called\ngoal agenda that divides G into an ordered set of subgoals. Any planner can\nthen, in principle, use the goal agenda to plan for increasing sets of\nsubgoals. This can lead to an exponential complexity reduction, as the solution\nto a complex planning problem is found by solving easier subproblems. Since\nonly a polynomial overhead is caused by the goal agenda computation, a\npotential exists to dramatically speed up planning algorithms as we demonstrate\nin the empirical evaluation, where we use this method in the IPP planner.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2011 16:38:06 GMT"}], "update_date": "2011-06-02", "authors_parsed": [["Hoffmann", "J.", ""], ["Koehler", "J.", ""]]}, {"id": "1106.0244", "submitter": "D. F. Gordon", "authors": "D. F. Gordon", "title": "Asimovian Adaptive Agents", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 13, pages\n  95-153, 2000", "doi": "10.1613/jair.720", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this research is to develop agents that are adaptive and\npredictable and timely. At first blush, these three requirements seem\ncontradictory. For example, adaptation risks introducing undesirable side\neffects, thereby making agents' behavior less predictable. Furthermore,\nalthough formal verification can assist in ensuring behavioral predictability,\nit is known to be time-consuming. Our solution to the challenge of satisfying\nall three requirements is the following. Agents have finite-state automaton\nplans, which are adapted online via evolutionary learning (perturbation)\noperators. To ensure that critical behavioral constraints are always satisfied,\nagents' plans are first formally verified. They are then reverified after every\nadaptation. If reverification concludes that constraints are violated, the\nplans are repaired. The main objective of this paper is to improve the\nefficiency of reverification after learning, so that agents have a sufficiently\nrapid response time. We present two solutions: positive results that certain\nlearning operators are a priori guaranteed to preserve useful classes of\nbehavioral assurance constraints (which implies that no reverification is\nneeded for these operators), and efficient incremental reverification\nalgorithms for those learning operators that have negative a priori results.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2011 16:38:25 GMT"}], "update_date": "2011-06-02", "authors_parsed": [["Gordon", "D. F.", ""]]}, {"id": "1106.0245", "submitter": "J. Baxter", "authors": "J. Baxter", "title": "A Model of Inductive Bias Learning", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 12, pages\n  149-198, 2000", "doi": "10.1613/jair.731", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major problem in machine learning is that of inductive bias: how to choose\na learner's hypothesis space so that it is large enough to contain a solution\nto the problem being learnt, yet small enough to ensure reliable generalization\nfrom reasonably-sized training sets. Typically such bias is supplied by hand\nthrough the skill and insights of experts. In this paper a model for\nautomatically learning bias is investigated. The central assumption of the\nmodel is that the learner is embedded within an environment of related learning\ntasks. Within such an environment the learner can sample from multiple tasks,\nand hence it can search for a hypothesis space that contains good solutions to\nmany of the problems in the environment. Under certain restrictions on the set\nof all hypothesis spaces available to the learner, we show that a hypothesis\nspace that performs well on a sufficiently large number of training tasks will\nalso perform well when learning novel tasks in the same environment. Explicit\nbounds are also derived demonstrating that learning multiple tasks within an\nenvironment of related tasks can potentially give much better generalization\nthan learning a single task.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2011 16:38:38 GMT"}], "update_date": "2011-06-02", "authors_parsed": [["Baxter", "J.", ""]]}, {"id": "1106.0246", "submitter": "C. Bhattacharyya", "authors": "C. Bhattacharyya, S. S. Keerthi", "title": "Mean Field Methods for a Special Class of Belief Networks", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 15, pages\n  91-114, 2001", "doi": "10.1613/jair.734", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The chief aim of this paper is to propose mean-field approximations for a\nbroad class of Belief networks, of which sigmoid and noisy-or networks can be\nseen as special cases. The approximations are based on a powerful mean-field\ntheory suggested by Plefka. We show that Saul, Jaakkola and Jordan' s approach\nis the first order approximation in Plefka's approach, via a variational\nderivation. The application of Plefka's theory to belief networks is not\ncomputationally tractable. To tackle this problem we propose new approximations\nbased on Taylor series. Small scale experiments show that the proposed schemes\nare attractive.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2011 16:38:54 GMT"}], "update_date": "2011-06-02", "authors_parsed": [["Bhattacharyya", "C.", ""], ["Keerthi", "S. S.", ""]]}, {"id": "1106.0247", "submitter": "B. Nebel", "authors": "B. Nebel", "title": "On the Compilability and Expressive Power of Propositional Planning\n  Formalisms", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 12, pages\n  271-315, 2000", "doi": "10.1613/jair.735", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent approaches of extending the GRAPHPLAN algorithm to handle more\nexpressive planning formalisms raise the question of what the formal meaning of\n\"expressive power\" is. We formalize the intuition that expressive power is a\nmeasure of how concisely planning domains and plans can be expressed in a\nparticular formalism by introducing the notion of \"compilation schemes\" between\nplanning formalisms. Using this notion, we analyze the expressiveness of a\nlarge family of propositional planning formalisms, ranging from basic STRIPS to\na formalism with conditional effects, partial state specifications, and\npropositional formulae in the preconditions. One of the results is that\nconditional effects cannot be compiled away if plan size should grow only\nlinearly but can be compiled away if we allow for polynomial growth of the\nresulting plans. This result confirms that the recently proposed extensions to\nthe GRAPHPLAN algorithm concerning conditional effects are optimal with respect\nto the \"compilability\" framework. Another result is that general propositional\nformulae cannot be compiled into conditional effects if the plan size should be\npreserved linearly. This implies that allowing general propositional formulae\nin preconditions and effect conditions adds another level of difficulty in\ngenerating a plan.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2011 16:39:07 GMT"}], "update_date": "2011-06-02", "authors_parsed": [["Nebel", "B.", ""]]}, {"id": "1106.0249", "submitter": "C. Boutilier", "authors": "C. Boutilier, R. I. Brafman", "title": "Partial-Order Planning with Concurrent Interacting Actions", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 14, pages\n  105-136, 2001", "doi": "10.1613/jair.740", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to generate plans for agents with multiple actuators, agent teams,\nor distributed controllers, we must be able to represent and plan using\nconcurrent actions with interacting effects. This has historically been\nconsidered a challenging task requiring a temporal planner with the ability to\nreason explicitly about time. We show that with simple modifications, the\nSTRIPS action representation language can be used to represent interacting\nactions. Moreover, algorithms for partial-order planning require only small\nmodifications in order to be applied in such multiagent domains. We demonstrate\nthis fact by developing a sound and complete partial-order planner for planning\nwith concurrent interacting actions, POMP, that extends existing partial-order\nplanners in a straightforward way. These results open the way to the use of\npartial-order planners for the centralized control of cooperative multiagent\nsystems.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2011 16:39:53 GMT"}], "update_date": "2011-06-02", "authors_parsed": [["Boutilier", "C.", ""], ["Brafman", "R. I.", ""]]}, {"id": "1106.0250", "submitter": "J. L. Ambite", "authors": "J. L. Ambite, C. A. Knoblock", "title": "Planning by Rewriting", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 15, pages\n  207-261, 2001", "doi": "10.1613/jair.754", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain-independent planning is a hard combinatorial problem. Taking into\naccount plan quality makes the task even more difficult. This article\nintroduces Planning by Rewriting (PbR), a new paradigm for efficient\nhigh-quality domain-independent planning. PbR exploits declarative\nplan-rewriting rules and efficient local search techniques to transform an\neasy-to-generate, but possibly suboptimal, initial plan into a high-quality\nplan. In addition to addressing the issues of planning efficiency and plan\nquality, this framework offers a new anytime planning algorithm. We have\nimplemented this planner and applied it to several existing domains. The\nexperimental results show that the PbR approach provides significant savings in\nplanning effort while generating high-quality plans.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2011 16:40:10 GMT"}], "update_date": "2011-06-02", "authors_parsed": [["Ambite", "J. L.", ""], ["Knoblock", "C. A.", ""]]}, {"id": "1106.0251", "submitter": "N. L. Zhang", "authors": "N. L. Zhang, W. Zhang", "title": "Speeding Up the Convergence of Value Iteration in Partially Observable\n  Markov Decision Processes", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 14, pages\n  29-51, 2001", "doi": "10.1613/jair.761", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partially observable Markov decision processes (POMDPs) have recently become\npopular among many AI researchers because they serve as a natural model for\nplanning under uncertainty. Value iteration is a well-known algorithm for\nfinding optimal policies for POMDPs. It typically takes a large number of\niterations to converge. This paper proposes a method for accelerating the\nconvergence of value iteration. The method has been evaluated on an array of\nbenchmark problems and was found to be very effective: It enabled value\niteration to converge after only a few iterations on all the test problems.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2011 16:40:25 GMT"}], "update_date": "2011-06-02", "authors_parsed": [["Zhang", "N. L.", ""], ["Zhang", "W.", ""]]}, {"id": "1106.0252", "submitter": "A. Cimatti", "authors": "A. Cimatti, M. Roveri", "title": "Conformant Planning via Symbolic Model Checking", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 13, pages\n  305-338, 2000", "doi": "10.1613/jair.774", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of planning in nondeterministic domains, by presenting\na new approach to conformant planning. Conformant planning is the problem of\nfinding a sequence of actions that is guaranteed to achieve the goal despite\nthe nondeterminism of the domain. Our approach is based on the representation\nof the planning domain as a finite state automaton. We use Symbolic Model\nChecking techniques, in particular Binary Decision Diagrams, to compactly\nrepresent and efficiently search the automaton. In this paper we make the\nfollowing contributions. First, we present a general planning algorithm for\nconformant planning, which applies to fully nondeterministic domains, with\nuncertainty in the initial condition and in action effects. The algorithm is\nbased on a breadth-first, backward search, and returns conformant plans of\nminimal length, if a solution to the planning problem exists, otherwise it\nterminates concluding that the problem admits no conformant solution. Second,\nwe provide a symbolic representation of the search space based on Binary\nDecision Diagrams (BDDs), which is the basis for search techniques derived from\nsymbolic model checking. The symbolic representation makes it possible to\nanalyze potentially large sets of states and transitions in a single\ncomputation step, thus providing for an efficient implementation. Third, we\npresent CMBP (Conformant Model Based Planner), an efficient implementation of\nthe data structures and algorithm described above, directly based on BDD\nmanipulations, which allows for a compact representation of the search layers\nand an efficient implementation of the search steps. Finally, we present an\nexperimental comparison of our approach with the state-of-the-art conformant\nplanners CGP, QBFPLAN and GPT. Our analysis includes all the planning problems\nfrom the distribution packages of these systems, plus other problems defined to\nstress a number of specific factors. Our approach appears to be the most\neffective: CMBP is strictly more expressive than QBFPLAN and CGP and, in all\nthe problems where a comparison is possible, CMBP outperforms its competitors,\nsometimes by orders of magnitude.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2011 16:40:44 GMT"}], "update_date": "2011-06-02", "authors_parsed": [["Cimatti", "A.", ""], ["Roveri", "M.", ""]]}, {"id": "1106.0253", "submitter": "J. Cheng", "authors": "J. Cheng, M. J. Druzdzel", "title": "AIS-BN: An Adaptive Importance Sampling Algorithm for Evidential\n  Reasoning in Large Bayesian Networks", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 13, pages\n  155-188, 2000", "doi": "10.1613/jair.764", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic sampling algorithms, while an attractive alternative to exact\nalgorithms in very large Bayesian network models, have been observed to perform\npoorly in evidential reasoning with extremely unlikely evidence. To address\nthis problem, we propose an adaptive importance sampling algorithm, AIS-BN,\nthat shows promising convergence rates even under extreme conditions and seems\nto outperform the existing sampling algorithms consistently. Three sources of\nthis performance improvement are (1) two heuristics for initialization of the\nimportance function that are based on the theoretical properties of importance\nsampling in finite-dimensional integrals and the structural advantages of\nBayesian networks, (2) a smooth learning method for the importance function,\nand (3) a dynamic weighting function for combining samples from different\nstages of the algorithm. We tested the performance of the AIS-BN algorithm\nalong with two state of the art general purpose sampling algorithms, likelihood\nweighting (Fung and Chang, 1989; Shachter and Peot, 1989) and self-importance\nsampling (Shachter and Peot, 1989). We used in our tests three large real\nBayesian network models available to the scientific community: the CPCS network\n(Pradhan et al., 1994), the PathFinder network (Heckerman, Horvitz, and\nNathwani, 1990), and the ANDES network (Conati, Gertner, VanLehn, and Druzdzel,\n1997), with evidence as unlikely as 10^-41. While the AIS-BN algorithm always\nperformed better than the other two algorithms, in the majority of the test\ncases it achieved orders of magnitude improvement in precision of the results.\nImprovement in speed given a desired precision is even more dramatic, although\nwe are unable to report numerical results here, as the other algorithms almost\nnever achieved the precision reached even by the first few iterations of the\nAIS-BN algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2011 16:40:57 GMT"}], "update_date": "2011-06-02", "authors_parsed": [["Cheng", "J.", ""], ["Druzdzel", "M. J.", ""]]}, {"id": "1106.0254", "submitter": "X. Chen", "authors": "X. Chen, P. van Beek", "title": "Conflict-Directed Backjumping Revisited", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 14, pages\n  53-81, 2001", "doi": "10.1613/jair.788", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, many improvements to backtracking algorithms for solving\nconstraint satisfaction problems have been proposed. The techniques for\nimproving backtracking algorithms can be conveniently classified as look-ahead\nschemes and look-back schemes. Unfortunately, look-ahead and look-back schemes\nare not entirely orthogonal as it has been observed empirically that the\nenhancement of look-ahead techniques is sometimes counterproductive to the\neffects of look-back techniques. In this paper, we focus on the relationship\nbetween the two most important look-ahead techniques---using a variable\nordering heuristic and maintaining a level of local consistency during the\nbacktracking search---and the look-back technique of conflict-directed\nbackjumping (CBJ). We show that there exists a \"perfect\" dynamic variable\nordering such that CBJ becomes redundant. We also show theoretically that as\nthe level of local consistency that is maintained in the backtracking search is\nincreased, the less that backjumping will be an improvement. Our theoretical\nresults partially explain why a backtracking algorithm doing more in the\nlook-ahead phase cannot benefit more from the backjumping look-back scheme.\nFinally, we show empirically that adding CBJ to a backtracking algorithm that\nmaintains generalized arc consistency (GAC), an algorithm that we refer to as\nGAC-CBJ, can still provide orders of magnitude speedups. Our empirical results\ncontrast with Bessiere and Regin's conclusion (1996) that CBJ is useless to an\nalgorithm that maintains arc consistency.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2011 16:41:13 GMT"}], "update_date": "2011-06-02", "authors_parsed": [["Chen", "X.", ""], ["van Beek", "P.", ""]]}, {"id": "1106.0256", "submitter": "J. M. Siskind", "authors": "J. M. Siskind", "title": "Grounding the Lexical Semantics of Verbs in Visual Perception using\n  Force Dynamics and Event Logic", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 15, pages\n  31-90, 2001", "doi": "10.1613/jair.790", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an implemented system for recognizing the occurrence of\nevents described by simple spatial-motion verbs in short image sequences. The\nsemantics of these verbs is specified with event-logic expressions that\ndescribe changes in the state of force-dynamic relations between the\nparticipants of the event. An efficient finite representation is introduced for\nthe infinite sets of intervals that occur when describing liquid and\nsemi-liquid events. Additionally, an efficient procedure using this\nrepresentation is presented for inferring occurrences of compound events,\ndescribed with event-logic expressions, from occurrences of primitive events.\nUsing force dynamics and event logic to specify the lexical semantics of events\nallows the system to be more robust than prior systems based on motion profile.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2011 16:41:31 GMT"}], "update_date": "2011-06-02", "authors_parsed": [["Siskind", "J. M.", ""]]}, {"id": "1106.0257", "submitter": "R. Maclin", "authors": "R. Maclin, D. Opitz", "title": "Popular Ensemble Methods: An Empirical Study", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 11, pages\n  169-198, 1999", "doi": "10.1613/jair.614", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An ensemble consists of a set of individually trained classifiers (such as\nneural networks or decision trees) whose predictions are combined when\nclassifying novel instances. Previous research has shown that an ensemble is\noften more accurate than any of the single classifiers in the ensemble. Bagging\n(Breiman, 1996c) and Boosting (Freund and Shapire, 1996; Shapire, 1990) are two\nrelatively new but popular methods for producing ensembles. In this paper we\nevaluate these methods on 23 data sets using both neural networks and decision\ntrees as our classification algorithm. Our results clearly indicate a number of\nconclusions. First, while Bagging is almost always more accurate than a single\nclassifier, it is sometimes much less accurate than Boosting. On the other\nhand, Boosting can create ensembles that are less accurate than a single\nclassifier -- especially when using neural networks. Analysis indicates that\nthe performance of the Boosting methods is dependent on the characteristics of\nthe data set being examined. In fact, further results show that Boosting\nensembles may overfit noisy data sets, thus decreasing its performance.\nFinally, consistent with previous studies, our work suggests that most of the\ngain in an ensemble's performance comes in the first few classifiers combined;\nhowever, relatively large gains can be seen up to 25 classifiers when Boosting\ndecision trees.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2011 16:41:44 GMT"}], "update_date": "2011-06-02", "authors_parsed": [["Maclin", "R.", ""], ["Opitz", "D.", ""]]}, {"id": "1106.0284", "submitter": "E. F. Khor", "authors": "E. F. Khor, T. H. Lee, R. Sathikannan, K. C. Tan", "title": "An Evolutionary Algorithm with Advanced Goal and Priority Specification\n  for Multi-objective Optimization", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 18, pages\n  183-215, 2003", "doi": "10.1613/jair.842", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an evolutionary algorithm with a new goal-sequence\ndomination scheme for better decision support in multi-objective optimization.\nThe approach allows the inclusion of advanced hard/soft priority and constraint\ninformation on each objective component, and is capable of incorporating\nmultiple specifications with overlapping or non-overlapping objective functions\nvia logical 'OR' and 'AND' connectives to drive the search towards multiple\nregions of trade-off. In addition, we propose a dynamic sharing scheme that is\nsimple and adaptively estimated according to the on-line population\ndistribution without needing any a priori parameter setting. Each feature in\nthe proposed algorithm is examined to show its respective contribution, and the\nperformance of the algorithm is compared with other evolutionary optimization\nmethods. It is shown that the proposed algorithm has performed well in the\ndiversity of evolutionary search and uniform distribution of non-dominated\nindividuals along the final trade-offs, without significant computational\neffort. The algorithm is also applied to the design optimization of a practical\nservo control system for hard disk drives with a single voice-coil-motor\nactuator. Results of the evolutionary designed servo control system show a\nsuperior closed-loop performance compared to classical PID or RPT approaches.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2011 19:15:16 GMT"}], "update_date": "2011-06-02", "authors_parsed": [["Khor", "E. F.", ""], ["Lee", "T. H.", ""], ["Sathikannan", "R.", ""], ["Tan", "K. C.", ""]]}, {"id": "1106.0285", "submitter": "I. Refanidis", "authors": "I. Refanidis, I. Vlahavas", "title": "The GRT Planning System: Backward Heuristic Construction in Forward\n  State-Space Planning", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 15, pages\n  115-161, 2001", "doi": "10.1613/jair.893", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents GRT, a domain-independent heuristic planning system for\nSTRIPS worlds. GRT solves problems in two phases. In the pre-processing phase,\nit estimates the distance between each fact and the goals of the problem, in a\nbackward direction. Then, in the search phase, these estimates are used in\norder to further estimate the distance between each intermediate state and the\ngoals, guiding so the search process in a forward direction and on a best-first\nbasis. The paper presents the benefits from the adoption of opposite directions\nbetween the preprocessing and the search phases, discusses some difficulties\nthat arise in the pre-processing phase and introduces techniques to cope with\nthem. Moreover, it presents several methods of improving the efficiency of the\nheuristic, by enriching the representation and by reducing the size of the\nproblem. Finally, a method of overcoming local optimal states, based on domain\naxioms, is proposed. According to it, difficult problems are decomposed into\neasier sub-problems that have to be solved sequentially. The performance\nresults from various domains, including those of the recent planning\ncompetitions, show that GRT is among the fastest planners.\n", "versions": [{"version": "v1", "created": "Wed, 1 Jun 2011 19:17:11 GMT"}], "update_date": "2011-06-02", "authors_parsed": [["Refanidis", "I.", ""], ["Vlahavas", "I.", ""]]}, {"id": "1106.0357", "submitter": "Mohamad Tarifi", "authors": "Mohamad Tarifi, Meera Sitharam, Jeffery Ho", "title": "Learning Hierarchical Sparse Representations using Iterative Dictionary\n  Learning and Dimension Reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces an elemental building block which combines Dictionary\nLearning and Dimension Reduction (DRDL). We show how this foundational element\ncan be used to iteratively construct a Hierarchical Sparse Representation (HSR)\nof a sensory stream. We compare our approach to existing models showing the\ngenerality of our simple prescription. We then perform preliminary experiments\nusing this framework, illustrating with the example of an object recognition\ntask using standard datasets. This work introduces the very first steps towards\nan integrated framework for designing and analyzing various computational tasks\nfrom learning to attention to action. The ultimate goal is building a\nmathematically rigorous, integrated theory of intelligence.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2011 02:31:04 GMT"}], "update_date": "2011-06-03", "authors_parsed": [["Tarifi", "Mohamad", ""], ["Sitharam", "Meera", ""], ["Ho", "Jeffery", ""]]}, {"id": "1106.0483", "submitter": "Xaq Pitkow", "authors": "Xaq Pitkow, Yashar Ahmadian, Ken D. Miller", "title": "Learning unbelievable marginal probabilities", "comments": "10 pages, 3 figures, submitted to NIPS*2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Loopy belief propagation performs approximate inference on graphical models\nwith loops. One might hope to compensate for the approximation by adjusting\nmodel parameters. Learning algorithms for this purpose have been explored\npreviously, and the claim has been made that every set of locally consistent\nmarginals can arise from belief propagation run on a graphical model. On the\ncontrary, here we show that many probability distributions have marginals that\ncannot be reached by belief propagation using any set of model parameters or\nany learning algorithm. We call such marginals `unbelievable.' This problem\noccurs whenever the Hessian of the Bethe free energy is not positive-definite\nat the target marginals. All learning algorithms for belief propagation\nnecessarily fail in these cases, producing beliefs or sets of beliefs that may\neven be worse than the pre-learning approximation. We then show that averaging\ninaccurate beliefs, each obtained from belief propagation using model\nparameters perturbed about some learned mean values, can achieve the\nunbelievable marginals.\n", "versions": [{"version": "v1", "created": "Thu, 2 Jun 2011 18:48:59 GMT"}], "update_date": "2011-06-03", "authors_parsed": [["Pitkow", "Xaq", ""], ["Ahmadian", "Yashar", ""], ["Miller", "Ken D.", ""]]}, {"id": "1106.0566", "submitter": "Tianshi Chen", "authors": "Tianshi Chen, Yunji Chen, Ke Tang, Guoliang Chen, and Xin Yao", "title": "The Impact of Mutation Rate on the Computation Time of Evolutionary\n  Dynamic Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mutation has traditionally been regarded as an important operator in\nevolutionary algorithms. In particular, there have been many experimental\nstudies which showed the effectiveness of adapting mutation rates for various\nstatic optimization problems. Given the perceived effectiveness of adaptive and\nself-adaptive mutation for static optimization problems, there have been\nspeculations that adaptive and self-adaptive mutation can benefit dynamic\noptimization problems even more since adaptation and self-adaptation are\ncapable of following a dynamic environment. However, few theoretical results\nare available in analyzing rigorously evolutionary algorithms for dynamic\noptimization problems. It is unclear when adaptive and self-adaptive mutation\nrates are likely to be useful for evolutionary algorithms in solving dynamic\noptimization problems. This paper provides the first rigorous analysis of\nadaptive mutation and its impact on the computation times of evolutionary\nalgorithms in solving certain dynamic optimization problems. More specifically,\nfor both individual-based and population-based EAs, we have shown that any\ntime-variable mutation rate scheme will not significantly outperform a fixed\nmutation rate on some dynamic optimization problem instances. The proofs also\noffer some insights into conditions under which any time-variable mutation\nscheme is unlikely to be useful and into the relationships between the problem\ncharacteristics and algorithmic features (e.g., different mutation schemes).\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2011 05:31:34 GMT"}], "update_date": "2011-06-06", "authors_parsed": [["Chen", "Tianshi", ""], ["Chen", "Yunji", ""], ["Tang", "Ke", ""], ["Chen", "Guoliang", ""], ["Yao", "Xin", ""]]}, {"id": "1106.0664", "submitter": "M. Cristani", "authors": "M. Cristani", "title": "The Complexity of Reasoning about Spatial Congruence", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 11, pages\n  361-390, 1999", "doi": "10.1613/jair.641", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the recent literature of Artificial Intelligence, an intensive research\neffort has been spent, for various algebras of qualitative relations used in\nthe representation of temporal and spatial knowledge, on the problem of\nclassifying the computational complexity of reasoning problems for subsets of\nalgebras. The main purpose of these researches is to describe a restricted set\nof maximal tractable subalgebras, ideally in an exhaustive fashion with respect\nto the hosting algebras. In this paper we introduce a novel algebra for\nreasoning about Spatial Congruence, show that the satisfiability problem in the\nspatial algebra MC-4 is NP-complete, and present a complete classification of\ntractability in the algebra, based on the individuation of three maximal\ntractable subclasses, one containing the basic relations. The three algebras\nare formed by 14, 10 and 9 relations out of 16 which form the full algebra.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2011 14:51:33 GMT"}], "update_date": "2011-06-06", "authors_parsed": [["Cristani", "M.", ""]]}, {"id": "1106.0665", "submitter": "Jonathan Baxter", "authors": "Jonathan Baxter and Peter L. Bartlett", "title": "Infinite-Horizon Policy-Gradient Estimation", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 15, pages\n  319-350, 2001", "doi": "10.1613/jair.806", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gradient-based approaches to direct policy search in reinforcement learning\nhave received much recent attention as a means to solve problems of partial\nobservability and to avoid some of the problems associated with policy\ndegradation in value-function methods. In this paper we introduce GPOMDP, a\nsimulation-based algorithm for generating a {\\em biased} estimate of the\ngradient of the {\\em average reward} in Partially Observable Markov Decision\nProcesses (POMDPs) controlled by parameterized stochastic policies. A similar\nalgorithm was proposed by Kimura, Yamamura, and Kobayashi (1995). The\nalgorithm's chief advantages are that it requires storage of only twice the\nnumber of policy parameters, uses one free parameter $\\beta\\in [0,1)$ (which\nhas a natural interpretation in terms of bias-variance trade-off), and requires\nno knowledge of the underlying state. We prove convergence of GPOMDP, and show\nhow the correct choice of the parameter $\\beta$ is related to the {\\em mixing\ntime} of the controlled POMDP. We briefly describe extensions of GPOMDP to\ncontrolled Markov chains, continuous state, observation and control spaces,\nmultiple-agents, higher-order derivatives, and a version for training\nstochastic policies with internal states. In a companion paper (Baxter,\nBartlett, & Weaver, 2001) we show how the gradient estimates generated by\nGPOMDP can be used in both a traditional stochastic gradient algorithm and a\nconjugate-gradient procedure to find local optima of the average reward\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2011 14:52:01 GMT"}, {"version": "v2", "created": "Fri, 15 Nov 2019 16:18:16 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Baxter", "Jonathan", ""], ["Bartlett", "Peter L.", ""]]}, {"id": "1106.0666", "submitter": "Jonathan Baxter", "authors": "J. Baxter, P. L. Bartlett, L. Weaver", "title": "Experiments with Infinite-Horizon, Policy-Gradient Estimation", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 15, pages\n  351-381, 2001", "doi": "10.1613/jair.807", "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present algorithms that perform gradient ascent of the\naverage reward in a partially observable Markov decision process (POMDP). These\nalgorithms are based on GPOMDP, an algorithm introduced in a companion paper\n(Baxter and Bartlett, this volume), which computes biased estimates of the\nperformance gradient in POMDPs. The algorithm's chief advantages are that it\nuses only one free parameter beta, which has a natural interpretation in terms\nof bias-variance trade-off, it requires no knowledge of the underlying state,\nand it can be applied to infinite state, control and observation spaces. We\nshow how the gradient estimates produced by GPOMDP can be used to perform\ngradient ascent, both with a traditional stochastic-gradient algorithm, and\nwith an algorithm based on conjugate-gradients that utilizes gradient\ninformation to bracket maxima in line searches. Experimental results are\npresented illustrating both the theoretical results of (Baxter and Bartlett,\nthis volume) on a toy problem, and practical aspects of the algorithms on a\nnumber of more realistic problems.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2011 14:52:26 GMT"}, {"version": "v2", "created": "Thu, 14 Nov 2019 04:58:31 GMT"}], "update_date": "2019-11-18", "authors_parsed": [["Baxter", "J.", ""], ["Bartlett", "P. L.", ""], ["Weaver", "L.", ""]]}, {"id": "1106.0667", "submitter": "U. Straccia", "authors": "U. Straccia", "title": "Reasoning within Fuzzy Description Logics", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 14, pages\n  137-166, 2001", "doi": "10.1613/jair.813", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Description Logics (DLs) are suitable, well-known, logics for managing\nstructured knowledge. They allow reasoning about individuals and well defined\nconcepts, i.e., set of individuals with common properties. The experience in\nusing DLs in applications has shown that in many cases we would like to extend\ntheir capabilities. In particular, their use in the context of Multimedia\nInformation Retrieval (MIR) leads to the convincement that such DLs should\nallow the treatment of the inherent imprecision in multimedia object content\nrepresentation and retrieval. In this paper we will present a fuzzy extension\nof ALC, combining Zadeh's fuzzy logic with a classical DL. In particular,\nconcepts becomes fuzzy and, thus, reasoning about imprecise concepts is\nsupported. We will define its syntax, its semantics, describe its properties\nand present a constraint propagation calculus for reasoning in it.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2011 14:52:49 GMT"}], "update_date": "2011-06-06", "authors_parsed": [["Straccia", "U.", ""]]}, {"id": "1106.0668", "submitter": "T. Elomaa", "authors": "T. Elomaa, M. Kaariainen", "title": "An Analysis of Reduced Error Pruning", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 15, pages\n  163-187, 2001", "doi": "10.1613/jair.816", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Top-down induction of decision trees has been observed to suffer from the\ninadequate functioning of the pruning phase. In particular, it is known that\nthe size of the resulting tree grows linearly with the sample size, even though\nthe accuracy of the tree does not improve. Reduced Error Pruning is an\nalgorithm that has been used as a representative technique in attempts to\nexplain the problems of decision tree learning. In this paper we present\nanalyses of Reduced Error Pruning in three different settings. First we study\nthe basic algorithmic properties of the method, properties that hold\nindependent of the input decision tree and pruning examples. Then we examine a\nsituation that intuitively should lead to the subtree under consideration to be\nreplaced by a leaf node, one in which the class label and attribute values of\nthe pruning examples are independent of each other. This analysis is conducted\nunder two different assumptions. The general analysis shows that the pruning\nprobability of a node fitting pure noise is bounded by a function that\ndecreases exponentially as the size of the tree grows. In a specific analysis\nwe assume that the examples are distributed uniformly to the tree. This\nassumption lets us approximate the number of subtrees that are pruned because\nthey do not receive any pruning examples. This paper clarifies the different\nvariants of the Reduced Error Pruning algorithm, brings new insight to its\nalgorithmic properties, analyses the algorithm with less imposed assumptions\nthan before, and includes the previously overlooked empty subtrees to the\nanalysis.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2011 14:53:10 GMT"}], "update_date": "2011-06-06", "authors_parsed": [["Elomaa", "T.", ""], ["Kaariainen", "M.", ""]]}, {"id": "1106.0669", "submitter": "M. L. Ginsberg", "authors": "M. L. Ginsberg", "title": "GIB: Imperfect Information in a Computationally Challenging Game", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 14, pages\n  303-358, 2001", "doi": "10.1613/jair.820", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the problems arising in the construction of a program\nto play the game of contract bridge. These problems include both the difficulty\nof solving the game's perfect information variant, and techniques needed to\naddress the fact that bridge is not, in fact, a perfect information game. GIB,\nthe program being described, involves five separate technical advances:\npartition search, the practical application of Monte Carlo techniques to\nrealistic problems, a focus on achievable sets to solve problems inherent in\nthe Monte Carlo approach, an extension of alpha-beta pruning from total orders\nto arbitrary distributive lattices, and the use of squeaky wheel optimization\nto find approximately optimal solutions to cardplay problems. GIB is currently\nbelieved to be of approximately expert caliber, and is currently the strongest\ncomputer bridge program in the world.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2011 14:53:55 GMT"}], "update_date": "2011-06-06", "authors_parsed": [["Ginsberg", "M. L.", ""]]}, {"id": "1106.0671", "submitter": "C. Bessiere", "authors": "C. Bessiere, R. Debruyne", "title": "Domain Filtering Consistencies", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 14, pages\n  205-230, 2001", "doi": "10.1613/jair.834", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enforcing local consistencies is one of the main features of constraint\nreasoning. Which level of local consistency should be used when searching for\nsolutions in a constraint network is a basic question. Arc consistency and\npartial forms of arc consistency have been widely studied, and have been known\nfor sometime through the forward checking or the MAC search algorithms. Until\nrecently, stronger forms of local consistency remained limited to those that\nchange the structure of the constraint graph, and thus, could not be used in\npractice, especially on large networks. This paper focuses on the local\nconsistencies that are stronger than arc consistency, without changing the\nstructure of the network, i.e., only removing inconsistent values from the\ndomains. In the last five years, several such local consistencies have been\nproposed by us or by others. We make an overview of all of them, and highlight\nsome relations between them. We compare them both theoretically and\nexperimentally, considering their pruning efficiency and the time required to\nenforce them.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2011 14:54:17 GMT"}], "update_date": "2011-06-06", "authors_parsed": [["Bessiere", "C.", ""], ["Debruyne", "R.", ""]]}, {"id": "1106.0672", "submitter": "H. H. Bui", "authors": "H. H. Bui, S. Venkatesh, G. West", "title": "Policy Recognition in the Abstract Hidden Markov Model", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 17, pages\n  451-499, 2002", "doi": "10.1613/jair.839", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a method for recognising an agent's behaviour in\ndynamic, noisy, uncertain domains, and across multiple levels of abstraction.\nWe term this problem on-line plan recognition under uncertainty and view it\ngenerally as probabilistic inference on the stochastic process representing the\nexecution of the agent's plan. Our contributions in this paper are twofold. In\nterms of probabilistic inference, we introduce the Abstract Hidden Markov Model\n(AHMM), a novel type of stochastic processes, provide its dynamic Bayesian\nnetwork (DBN) structure and analyse the properties of this network. We then\ndescribe an application of the Rao-Blackwellised Particle Filter to the AHMM\nwhich allows us to construct an efficient, hybrid inference method for this\nmodel. In terms of plan recognition, we propose a novel plan recognition\nframework based on the AHMM as the plan execution model. The Rao-Blackwellised\nhybrid inference for AHMM can take advantage of the independence properties\ninherent in a model of plan execution, leading to an algorithm for online\nprobabilistic plan recognition that scales well with the number of levels in\nthe plan hierarchy. This illustrates that while stochastic models for plan\nexecution can be complex, they exhibit special structures which, if exploited,\ncan lead to efficient plan recognition algorithms. We demonstrate the\nusefulness of the AHMM framework via a behaviour recognition system in a\ncomplex spatial environment using distributed video surveillance data.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2011 14:54:32 GMT"}], "update_date": "2011-06-06", "authors_parsed": [["Bui", "H. H.", ""], ["Venkatesh", "S.", ""], ["West", "G.", ""]]}, {"id": "1106.0675", "submitter": "J. Hoffmann", "authors": "J. Hoffmann, B. Nebel", "title": "The FF Planning System: Fast Plan Generation Through Heuristic Search", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 14, pages\n  253-302, 2001", "doi": "10.1613/jair.855", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe and evaluate the algorithmic techniques that are used in the FF\nplanning system. Like the HSP system, FF relies on forward state space search,\nusing a heuristic that estimates goal distances by ignoring delete lists.\nUnlike HSP's heuristic, our method does not assume facts to be independent. We\nintroduce a novel search strategy that combines hill-climbing with systematic\nsearch, and we show how other powerful heuristic information can be extracted\nand used to prune the search space. FF was the most successful automatic\nplanner at the recent AIPS-2000 planning competition. We review the results of\nthe competition, give data for other benchmark domains, and investigate the\nreasons for the runtime performance of FF compared to HSP.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2011 14:55:02 GMT"}], "update_date": "2011-06-06", "authors_parsed": [["Hoffmann", "J.", ""], ["Nebel", "B.", ""]]}, {"id": "1106.0676", "submitter": "M. Kearns", "authors": "M. Kearns, D. Litman, S. Singh, M. Walker", "title": "Optimizing Dialogue Management with Reinforcement Learning: Experiments\n  with the NJFun System", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 16, pages\n  105-133, 2002", "doi": "10.1613/jair.859", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing the dialogue policy of a spoken dialogue system involves many\nnontrivial choices. This paper presents a reinforcement learning approach for\nautomatically optimizing a dialogue policy, which addresses the technical\nchallenges in applying reinforcement learning to a working dialogue system with\nhuman users. We report on the design, construction and empirical evaluation of\nNJFun, an experimental spoken dialogue system that provides users with access\nto information about fun things to do in New Jersey. Our results show that by\noptimizing its performance via reinforcement learning, NJFun measurably\nimproves system performance.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2011 14:55:23 GMT"}], "update_date": "2011-06-06", "authors_parsed": [["Kearns", "M.", ""], ["Litman", "D.", ""], ["Singh", "S.", ""], ["Walker", "M.", ""]]}, {"id": "1106.0678", "submitter": "M. Kearns", "authors": "M. Kearns, M. L. Littman, S. Singh, P. Stone", "title": "ATTac-2000: An Adaptive Autonomous Bidding Agent", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 15, pages\n  189-206, 2001", "doi": "10.1613/jair.865", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The First Trading Agent Competition (TAC) was held from June 22nd to July\n8th, 2000. TAC was designed to create a benchmark problem in the complex domain\nof e-marketplaces and to motivate researchers to apply unique approaches to a\ncommon task. This article describes ATTac-2000, the first-place finisher in\nTAC. ATTac-2000 uses a principled bidding strategy that includes several\nelements of adaptivity. In addition to the success at the competition, isolated\nempirical results are presented indicating the robustness and effectiveness of\nATTac-2000's adaptive strategy.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2011 14:55:42 GMT"}], "update_date": "2011-06-06", "authors_parsed": [["Kearns", "M.", ""], ["Littman", "M. L.", ""], ["Singh", "S.", ""], ["Stone", "P.", ""]]}, {"id": "1106.0679", "submitter": "B. Nebel", "authors": "B. Nebel, J. Renz", "title": "Efficient Methods for Qualitative Spatial Reasoning", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 15, pages\n  289-318, 2001", "doi": "10.1613/jair.872", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The theoretical properties of qualitative spatial reasoning in the RCC8\nframework have been analyzed extensively. However, no empirical investigation\nhas been made yet. Our experiments show that the adaption of the algorithms\nused for qualitative temporal reasoning can solve large RCC8 instances, even if\nthey are in the phase transition region -- provided that one uses the maximal\ntractable subsets of RCC8 that have been identified by us. In particular, we\ndemonstrate that the orthogonal combination of heuristic methods is successful\nin solving almost all apparently hard instances in the phase transition region\nup to a certain size in reasonable time.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2011 14:56:05 GMT"}], "update_date": "2011-06-06", "authors_parsed": [["Nebel", "B.", ""], ["Renz", "J.", ""]]}, {"id": "1106.0680", "submitter": "L. P. Kaelbling", "authors": "L. P. Kaelbling, H. Shatkay", "title": "Learning Geometrically-Constrained Hidden Markov Models for Robot\n  Navigation: Bridging the Topological-Geometrical Gap", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 16, pages\n  167-207, 2002", "doi": "10.1613/jair.874", "report-no": null, "categories": "cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hidden Markov models (HMMs) and partially observable Markov decision\nprocesses (POMDPs) provide useful tools for modeling dynamical systems. They\nare particularly useful for representing the topology of environments such as\nroad networks and office buildings, which are typical for robot navigation and\nplanning. The work presented here describes a formal framework for\nincorporating readily available odometric information and geometrical\nconstraints into both the models and the algorithm that learns them. By taking\nadvantage of such information, learning HMMs/POMDPs can be made to generate\nbetter solutions and require fewer iterations, while being robust in the face\nof data reduction. Experimental results, obtained from both simulated and real\nrobot data, demonstrate the effectiveness of the approach.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2011 14:56:46 GMT"}], "update_date": "2011-06-06", "authors_parsed": [["Kaelbling", "L. P.", ""], ["Shatkay", "H.", ""]]}, {"id": "1106.0681", "submitter": "C. Boutilier", "authors": "C. Boutilier, B. Price", "title": "Accelerating Reinforcement Learning through Implicit Imitation", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 19, pages\n  569-629, 2003", "doi": "10.1613/jair.898", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imitation can be viewed as a means of enhancing learning in multiagent\nenvironments. It augments an agent's ability to learn useful behaviors by\nmaking intelligent use of the knowledge implicit in behaviors demonstrated by\ncooperative teachers or other more experienced agents. We propose and study a\nformal model of implicit imitation that can accelerate reinforcement learning\ndramatically in certain cases. Roughly, by observing a mentor, a\nreinforcement-learning agent can extract information about its own capabilities\nin, and the relative value of, unvisited parts of the state space. We study two\nspecific instantiations of this model, one in which the learning agent and the\nmentor have identical abilities, and one designed to deal with agents and\nmentors with different action sets. We illustrate the benefits of implicit\nimitation by integrating it with prioritized sweeping, and demonstrating\nimproved performance and convergence through observation of single and multiple\nmentors. Though we make some stringent assumptions regarding observability and\npossible interactions, we briefly comment on extensions of the model that relax\nthese restricitions.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2011 14:57:02 GMT"}], "update_date": "2011-06-06", "authors_parsed": [["Boutilier", "C.", ""], ["Price", "B.", ""]]}, {"id": "1106.0707", "submitter": "H. He", "authors": "H. He, D. Hu, X. Xu", "title": "Efficient Reinforcement Learning Using Recursive Least-Squares Methods", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 16, pages\n  259-292, 2002", "doi": "10.1613/jair.946", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recursive least-squares (RLS) algorithm is one of the most well-known\nalgorithms used in adaptive filtering, system identification and adaptive\ncontrol. Its popularity is mainly due to its fast convergence speed, which is\nconsidered to be optimal in practice. In this paper, RLS methods are used to\nsolve reinforcement learning problems, where two new reinforcement learning\nalgorithms using linear value function approximators are proposed and analyzed.\nThe two algorithms are called RLS-TD(lambda) and Fast-AHC (Fast Adaptive\nHeuristic Critic), respectively. RLS-TD(lambda) can be viewed as the extension\nof RLS-TD(0) from lambda=0 to general lambda within interval [0,1], so it is a\nmulti-step temporal-difference (TD) learning algorithm using RLS methods. The\nconvergence with probability one and the limit of convergence of RLS-TD(lambda)\nare proved for ergodic Markov chains. Compared to the existing LS-TD(lambda)\nalgorithm, RLS-TD(lambda) has advantages in computation and is more suitable\nfor online learning. The effectiveness of RLS-TD(lambda) is analyzed and\nverified by learning prediction experiments of Markov chains with a wide range\nof parameter settings. The Fast-AHC algorithm is derived by applying the\nproposed RLS-TD(lambda) algorithm in the critic network of the adaptive\nheuristic critic method. Unlike conventional AHC algorithm, Fast-AHC makes use\nof RLS methods to improve the learning-prediction efficiency in the critic.\nLearning control experiments of the cart-pole balancing and the acrobot\nswing-up problems are conducted to compare the data efficiency of Fast-AHC with\nconventional AHC. From the experimental results, it is shown that the data\nefficiency of learning control can also be improved by using RLS methods in the\nlearning-prediction process of the critic. The performance of Fast-AHC is also\ncompared with that of the AHC method using LS-TD(lambda). Furthermore, it is\ndemonstrated in the experiments that different initial values of the variance\nmatrix in RLS-TD(lambda) are required to get better performance not only in\nlearning prediction but also in learning control. The experimental results are\nanalyzed based on the existing theoretical work on the transient phase of\nforgetting factor RLS methods.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2011 16:44:06 GMT"}], "update_date": "2011-06-06", "authors_parsed": [["He", "H.", ""], ["Hu", "D.", ""], ["Xu", "X.", ""]]}, {"id": "1106.0776", "submitter": "Juan Carlos Nieves", "authors": "Juan Carlos Nieves, Mauricio Osorio and Ulises Cort\\'es", "title": "Semantics for Possibilistic Disjunctive Programs", "comments": "37 pages, 5 figures. To appear in Theory and Practice of Logic\n  Programming (TPLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO cs.PL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  In this paper, a possibilistic disjunctive logic programming approach for\nmodeling uncertain, incomplete and inconsistent information is defined. This\napproach introduces the use of possibilistic disjunctive clauses which are able\nto capture incomplete information and incomplete states of a knowledge base at\nthe same time.\n  By considering a possibilistic logic program as a possibilistic logic theory,\na construction of a possibilistic logic programming semantic based on answer\nsets and the proof theory of possibilistic logic is defined. It shows that this\npossibilistic semantics for disjunctive logic programs can be characterized by\na fixed-point operator. It is also shown that the suggested possibilistic\nsemantics can be computed by a resolution algorithm and the consideration of\noptimal refutations from a possibilistic logic theory.\n  In order to manage inconsistent possibilistic logic programs, a preference\ncriterion between inconsistent possibilistic models is defined; in addition,\nthe approach of cuts for restoring consistency of an inconsistent possibilistic\nknowledge base is adopted. The approach is illustrated in a medical scenario.\n", "versions": [{"version": "v1", "created": "Fri, 3 Jun 2011 22:57:06 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Nieves", "Juan Carlos", ""], ["Osorio", "Mauricio", ""], ["Cort\u00e9s", "Ulises", ""]]}, {"id": "1106.0823", "submitter": "Oleg Kupervasser", "authors": "Oleg Kupervasser", "title": "Recovering Epipolar Geometry from Images of Smooth Surfaces", "comments": "accepted to \"Pattern Recognition and Image Analysis\" for publishing\n  in 2013, 33 pages, 19 figures", "journal-ref": "Pattern Recognition and Image Analysis, April 2013, Volume 23,\n  Issue 2, pp 236-257", "doi": "10.1134/S1054661813020107", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present four methods for recovering the epipolar geometry from images of\nsmooth surfaces. In the existing methods for recovering epipolar geometry\ncorresponding feature points are used that cannot be found in such images. The\nfirst method is based on finding corresponding characteristic points created by\nillumination (ICPM - illumination characteristic points' method (PM)). The\nsecond method is based on correspondent tangency points created by tangents\nfrom epipoles to outline of smooth bodies (OTPM - outline tangent PM). These\ntwo methods are exact and give correct results for real images, because\npositions of the corresponding illumination characteristic points and\ncorresponding outline are known with small errors. But the second method is\nlimited either to special type of scenes or to restricted camera motion. We\nalso consider two more methods which are termed CCPM (curve characteristic PM)\nand CTPM (curve tangent PM), for searching epipolar geometry for images of\nsmooth bodies based on a set of level curves with constant illumination\nintensity. The CCPM method is based on searching correspondent points on\nisophoto curves with the help of correlation of curvatures between these lines.\nThe CTPM method is based on property of the tangential to isophoto curve\nepipolar line to map into the tangential to correspondent isophoto curves\nepipolar line. The standard method (SM) based on knowledge of pairs of the\nalmost exact correspondent points. The methods have been implemented and tested\nby SM on pairs of real images. Unfortunately, the last two methods give us only\na finite subset of solutions including \"good\" solution. Exception is \"epipoles\nin infinity\". The main reason is inaccuracy of assumption of constant\nbrightness for smooth bodies. But outline and illumination characteristic\npoints are not influenced by this inaccuracy. So, the first pair of methods\ngives exact results.\n", "versions": [{"version": "v1", "created": "Sat, 4 Jun 2011 13:48:21 GMT"}, {"version": "v2", "created": "Tue, 18 Oct 2011 12:33:21 GMT"}, {"version": "v3", "created": "Wed, 19 Oct 2011 09:27:20 GMT"}, {"version": "v4", "created": "Thu, 19 Jul 2012 12:38:37 GMT"}], "update_date": "2013-06-24", "authors_parsed": [["Kupervasser", "Oleg", ""]]}, {"id": "1106.0987", "submitter": "Junping Zhang", "authors": "Junping Zhang and Ziyu Xie and Stan Z. Li", "title": "Nearest Prime Simplicial Complex for Object Recognition", "comments": "16pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CG cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The structure representation of data distribution plays an important role in\nunderstanding the underlying mechanism of generating data. In this paper, we\npropose nearest prime simplicial complex approaches (NSC) by utilizing\npersistent homology to capture such structures. Assuming that each class is\nrepresented with a prime simplicial complex, we classify unlabeled samples\nbased on the nearest projection distances from the samples to the simplicial\ncomplexes. We also extend the extrapolation ability of these complexes with a\nprojection constraint term. Experiments in simulated and practical datasets\nindicate that compared with several published algorithms, the proposed NSC\napproaches achieve promising performance without losing the structure\nrepresentation.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2011 08:32:16 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Zhang", "Junping", ""], ["Xie", "Ziyu", ""], ["Li", "Stan Z.", ""]]}, {"id": "1106.1157", "submitter": "Shakir Mohamed", "authors": "Shakir Mohamed, Katherine Heller and Zoubin Ghahramani", "title": "Bayesian and L1 Approaches to Sparse Unsupervised Learning", "comments": "In Proceedings of the 29th International Conference on Machine\n  Learning (ICML), Edinburgh, Scotland, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of L1 regularisation for sparse learning has generated immense\nresearch interest, with successful application in such diverse areas as signal\nacquisition, image coding, genomics and collaborative filtering. While existing\nwork highlights the many advantages of L1 methods, in this paper we find that\nL1 regularisation often dramatically underperforms in terms of predictive\nperformance when compared with other methods for inferring sparsity. We focus\non unsupervised latent variable models, and develop L1 minimising factor\nmodels, Bayesian variants of \"L1\", and Bayesian models with a stronger L0-like\nsparsity induced through spike-and-slab distributions. These spike-and-slab\nBayesian factor models encourage sparsity while accounting for uncertainty in a\nprincipled manner and avoiding unnecessary shrinkage of non-zero values. We\ndemonstrate on a number of data sets that in practice spike-and-slab Bayesian\nmethods outperform L1 minimisation, even on a computational budget. We thus\nhighlight the need to re-assess the wide use of L1 methods in sparsity-reliant\napplications, particularly when we care about generalising to previously unseen\ndata, and provide an alternative that, over many varying conditions, provides\nimproved generalisation performance.\n", "versions": [{"version": "v1", "created": "Mon, 6 Jun 2011 19:24:44 GMT"}, {"version": "v2", "created": "Tue, 7 Jun 2011 00:37:47 GMT"}, {"version": "v3", "created": "Fri, 17 Aug 2012 04:15:40 GMT"}], "update_date": "2012-08-20", "authors_parsed": [["Mohamed", "Shakir", ""], ["Heller", "Katherine", ""], ["Ghahramani", "Zoubin", ""]]}, {"id": "1106.1510", "submitter": "Alexander Shkotin", "authors": "Alex Shkotin, Vladimir Ryakhovsky, Dmitry Kudryavtsev", "title": "Towards OWL-based Knowledge Representation in Petrology", "comments": "10 pages. The paper has been accepted by OWLED2011 as a long\n  presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents our work on development of OWL-driven systems for formal\nrepresentation and reasoning about terminological knowledge and facts in\npetrology. The long-term aim of our project is to provide solid foundations for\na large-scale integration of various kinds of knowledge, including basic terms,\nrock classification algorithms, findings and reports. We describe three steps\nwe have taken towards that goal here. First, we develop a semi-automated\nprocedure for transforming a database of igneous rock samples to texts in a\ncontrolled natural language (CNL), and then a collection of OWL ontologies.\nSecond, we create an OWL ontology of important petrology terms currently\ndescribed in natural language thesauri. We describe a prototype of a tool for\ncollecting definitions from domain experts. Third, we present an approach to\nformalization of current industrial standards for classification of rock\nsamples, which requires linear equations in OWL 2. In conclusion, we discuss a\nrange of opportunities arising from the use of semantic technologies in\npetrology and outline the future work in this area.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2011 07:01:59 GMT"}], "update_date": "2011-06-09", "authors_parsed": [["Shkotin", "Alex", ""], ["Ryakhovsky", "Vladimir", ""], ["Kudryavtsev", "Dmitry", ""]]}, {"id": "1106.1636", "submitter": "Greg Ver Steeg", "authors": "Greg Ver Steeg and Aram Galstyan", "title": "A Sequence of Relaxations Constraining Hidden Variable Models", "comments": "UAI 2011 Best Paper Runner-Up; Proceedings of the 27th Conference on\n  Uncertainty in Artificial Intelligence (UAI 2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SI physics.soc-ph quant-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many widely studied graphical models with latent variables lead to nontrivial\nconstraints on the distribution of the observed variables. Inspired by the Bell\ninequalities in quantum mechanics, we refer to any linear inequality whose\nviolation rules out some latent variable model as a \"hidden variable test\" for\nthat model. Our main contribution is to introduce a sequence of relaxations\nwhich provides progressively tighter hidden variable tests. We demonstrate\napplicability to mixtures of sequences of i.i.d. variables, Bell inequalities,\nand homophily models in social networks. For the last, we demonstrate that our\nmethod provides a test that is able to rule out latent homophily as the sole\nexplanation for correlations on a real social network that are known to be due\nto influence.\n", "versions": [{"version": "v1", "created": "Wed, 8 Jun 2011 19:53:37 GMT"}, {"version": "v2", "created": "Wed, 20 Jul 2011 10:38:52 GMT"}], "update_date": "2011-07-21", "authors_parsed": [["Steeg", "Greg Ver", ""], ["Galstyan", "Aram", ""]]}, {"id": "1106.1716", "submitter": "Yoshiharu Maeno", "authors": "Yoshiharu Maeno", "title": "Predicting growth fluctuation in network economy", "comments": "presented at the 5th International Workshop on Data Mining and\n  Statistical Science, Osaka, March 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study presents a method to predict the growth fluctuation of firms\ninterdependent in a network economy. The risk of downward growth fluctuation of\nfirms is calculated from the statistics on Japanese industry.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2011 07:17:24 GMT"}], "update_date": "2011-06-10", "authors_parsed": [["Maeno", "Yoshiharu", ""]]}, {"id": "1106.1796", "submitter": "C. Drummond", "authors": "C. Drummond", "title": "Accelerating Reinforcement Learning by Composing Solutions of\n  Automatically Identified Subtasks", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 16, pages\n  59-104, 2002", "doi": "10.1613/jair.904", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses a system that accelerates reinforcement learning by\nusing transfer from related tasks. Without such transfer, even if two tasks are\nvery similar at some abstract level, an extensive re-learning effort is\nrequired. The system achieves much of its power by transferring parts of\npreviously learned solutions rather than a single complete solution. The system\nexploits strong features in the multi-dimensional function produced by\nreinforcement learning in solving a particular task. These features are stable\nand easy to recognize early in the learning process. They generate a\npartitioning of the state space and thus the function. The partition is\nrepresented as a graph. This is used to index and compose functions stored in a\ncase base to form a close approximation to the solution of the new task.\nExperiments demonstrate that function composition often produces more than an\norder of magnitude increase in learning rate compared to a basic reinforcement\nlearning algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2011 13:11:20 GMT"}], "update_date": "2011-06-10", "authors_parsed": [["Drummond", "C.", ""]]}, {"id": "1106.1797", "submitter": "Y. Kameya", "authors": "T. Sato, Y. Kameya", "title": "Parameter Learning of Logic Programs for Symbolic-Statistical Modeling", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 15, pages\n  391-454, 2001", "doi": "10.1613/jair.912", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a logical/mathematical framework for statistical parameter\nlearning of parameterized logic programs, i.e. definite clause programs\ncontaining probabilistic facts with a parameterized distribution. It extends\nthe traditional least Herbrand model semantics in logic programming to\ndistribution semantics, possible world semantics with a probability\ndistribution which is unconditionally applicable to arbitrary logic programs\nincluding ones for HMMs, PCFGs and Bayesian networks. We also propose a new EM\nalgorithm, the graphical EM algorithm, that runs for a class of parameterized\nlogic programs representing sequential decision processes where each decision\nis exclusive and independent. It runs on a new data structure called support\ngraphs describing the logical relationship between observations and their\nexplanations, and learns parameters by computing inside and outside probability\ngeneralized for logic programs. The complexity analysis shows that when\ncombined with OLDT search for all explanations for observations, the graphical\nEM algorithm, despite its generality, has the same time complexity as existing\nEM algorithms, i.e. the Baum-Welch algorithm for HMMs, the Inside-Outside\nalgorithm for PCFGs, and the one for singly connected Bayesian networks that\nhave been developed independently in each research field. Learning experiments\nwith PCFGs using two corpora of moderate size indicate that the graphical EM\nalgorithm can significantly outperform the Inside-Outside algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2011 13:13:03 GMT"}], "update_date": "2011-08-26", "authors_parsed": [["Sato", "T.", ""], ["Kameya", "Y.", ""]]}, {"id": "1106.1799", "submitter": "C. Meek", "authors": "C. Meek", "title": "Finding a Path is Harder than Finding a Tree", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 15, pages\n  383-389, 2001", "doi": "10.1613/jair.914", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I consider the problem of learning an optimal path graphical model from data\nand show the problem to be NP-hard for the maximum likelihood and minimum\ndescription length approaches and a Bayesian approach. This hardness result\nholds despite the fact that the problem is a restriction of the polynomially\nsolvable problem of finding the optimal tree graphical model.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2011 13:13:51 GMT"}], "update_date": "2011-06-10", "authors_parsed": [["Meek", "C.", ""]]}, {"id": "1106.1800", "submitter": "J. F. Baget", "authors": "J. F. Baget, M. L. Mugnier", "title": "Extensions of Simple Conceptual Graphs: the Complexity of Rules and\n  Constraints", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 16, pages\n  425-465, 2002", "doi": "10.1613/jair.918", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simple conceptual graphs are considered as the kernel of most knowledge\nrepresentation formalisms built upon Sowa's model. Reasoning in this model can\nbe expressed by a graph homomorphism called projection, whose semantics is\nusually given in terms of positive, conjunctive, existential FOL. We present\nhere a family of extensions of this model, based on rules and constraints,\nkeeping graph homomorphism as the basic operation. We focus on the formal\ndefinitions of the different models obtained, including their operational\nsemantics and relationships with FOL, and we analyze the decidability and\ncomplexity of the associated problems (consistency and deduction). As soon as\nrules are involved in reasonings, these problems are not decidable, but we\nexhibit a condition under which they fall in the polynomial hierarchy. These\nresults extend and complete the ones already published by the authors. Moreover\nwe systematically study the complexity of some particular cases obtained by\nrestricting the form of constraints and/or rules.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2011 13:17:53 GMT"}], "update_date": "2011-06-10", "authors_parsed": [["Baget", "J. F.", ""], ["Mugnier", "M. L.", ""]]}, {"id": "1106.1802", "submitter": "F. Baader", "authors": "F. Baader, C. Lutz, H. Sturm, F. Wolter", "title": "Fusions of Description Logics and Abstract Description Systems", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 16, pages\n  1-58, 2002", "doi": "10.1613/jair.919", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fusions are a simple way of combining logics. For normal modal logics,\nfusions have been investigated in detail. In particular, it is known that,\nunder certain conditions, decidability transfers from the component logics to\ntheir fusion. Though description logics are closely related to modal logics,\nthey are not necessarily normal. In addition, ABox reasoning in description\nlogics is not covered by the results from modal logics. In this paper, we\nextend the decidability transfer results from normal modal logics to a large\nclass of description logics. To cover different description logics in a uniform\nway, we introduce abstract description systems, which can be seen as a common\ngeneralization of description and modal logics, and show the transfer results\nin this general setting.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2011 13:18:57 GMT"}], "update_date": "2011-06-10", "authors_parsed": [["Baader", "F.", ""], ["Lutz", "C.", ""], ["Sturm", "H.", ""], ["Wolter", "F.", ""]]}, {"id": "1106.1803", "submitter": "H. Blockeel", "authors": "H. Blockeel, L. Dehaspe, B. Demoen, G. Janssens, J. Ramon, H.\n  Vandecasteele", "title": "Improving the Efficiency of Inductive Logic Programming Through the Use\n  of Query Packs", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 16, pages\n  135-166, 2002", "doi": "10.1613/jair.924", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inductive logic programming, or relational learning, is a powerful paradigm\nfor machine learning or data mining. However, in order for ILP to become\npractically useful, the efficiency of ILP systems must improve substantially.\nTo this end, the notion of a query pack is introduced: it structures sets of\nsimilar queries. Furthermore, a mechanism is described for executing such query\npacks. A complexity analysis shows that considerable efficiency improvements\ncan be achieved through the use of this query pack execution mechanism. This\nclaim is supported by empirical results obtained by incorporating support for\nquery pack execution in two existing learning systems.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2011 13:19:53 GMT"}], "update_date": "2011-06-10", "authors_parsed": [["Blockeel", "H.", ""], ["Dehaspe", "L.", ""], ["Demoen", "B.", ""], ["Janssens", "G.", ""], ["Ramon", "J.", ""], ["Vandecasteele", "H.", ""]]}, {"id": "1106.1804", "submitter": "E. Dahlman", "authors": "E. Dahlman, A. E. Howe", "title": "A Critical Assessment of Benchmark Comparison in Planning", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 17, pages\n  1-33, 2002", "doi": "10.1613/jair.935", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent trends in planning research have led to empirical comparison becoming\ncommonplace. The field has started to settle into a methodology for such\ncomparisons, which for obvious practical reasons requires running a subset of\nplanners on a subset of problems. In this paper, we characterize the\nmethodology and examine eight implicit assumptions about the problems, planners\nand metrics used in many of these comparisons. The problem assumptions are:\nPR1) the performance of a general purpose planner should not be\npenalized/biased if executed on a sampling of problems and domains, PR2) minor\nsyntactic differences in representation do not affect performance, and PR3)\nproblems should be solvable by STRIPS capable planners unless they require ADL.\nThe planner assumptions are: PL1) the latest version of a planner is the best\none to use, PL2) default parameter settings approximate good performance, and\nPL3) time cut-offs do not unduly bias outcome. The metrics assumptions are: M1)\nperformance degrades similarly for each planner when run on degraded runtime\nenvironments (e.g., machine platform) and M2) the number of plan steps\ndistinguishes performance. We find that most of these assumptions are not\nsupported empirically; in particular, that planners are affected differently by\nthese assumptions. We conclude with a call to the community to devote research\nresources to improving the state of the practice and especially to enhancing\nthe available benchmark problems.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2011 13:20:39 GMT"}], "update_date": "2011-06-10", "authors_parsed": [["Dahlman", "E.", ""], ["Howe", "A. E.", ""]]}, {"id": "1106.1813", "submitter": "K. W. Bowyer", "authors": "N. V. Chawla, K. W. Bowyer, L. O. Hall, W. P. Kegelmeyer", "title": "SMOTE: Synthetic Minority Over-sampling Technique", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 16, pages\n  321-357, 2002", "doi": "10.1613/jair.953", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An approach to the construction of classifiers from imbalanced datasets is\ndescribed. A dataset is imbalanced if the classification categories are not\napproximately equally represented. Often real-world data sets are predominately\ncomposed of \"normal\" examples with only a small percentage of \"abnormal\" or\n\"interesting\" examples. It is also the case that the cost of misclassifying an\nabnormal (interesting) example as a normal example is often much higher than\nthe cost of the reverse error. Under-sampling of the majority (normal) class\nhas been proposed as a good means of increasing the sensitivity of a classifier\nto the minority class. This paper shows that a combination of our method of\nover-sampling the minority (abnormal) class and under-sampling the majority\n(normal) class can achieve better classifier performance (in ROC space) than\nonly under-sampling the majority class. This paper also shows that a\ncombination of our method of over-sampling the minority class and\nunder-sampling the majority class can achieve better classifier performance (in\nROC space) than varying the loss ratios in Ripper or class priors in Naive\nBayes. Our method of over-sampling the minority class involves creating\nsynthetic minority class examples. Experiments are performed using C4.5, Ripper\nand a Naive Bayes classifier. The method is evaluated using the area under the\nReceiver Operating Characteristic curve (AUC) and the ROC convex hull strategy.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2011 13:53:42 GMT"}], "update_date": "2011-11-25", "authors_parsed": [["Chawla", "N. V.", ""], ["Bowyer", "K. W.", ""], ["Hall", "L. O.", ""], ["Kegelmeyer", "W. P.", ""]]}, {"id": "1106.1814", "submitter": "H. Chan", "authors": "H. Chan, A. Darwiche", "title": "When do Numbers Really Matter?", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 17, pages\n  265-287, 2002", "doi": "10.1613/jair.967", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common wisdom has it that small distinctions in the probabilities\n(parameters) quantifying a belief network do not matter much for the results of\nprobabilistic queries. Yet, one can develop realistic scenarios under which\nsmall variations in network parameters can lead to significant changes in\ncomputed queries. A pending theoretical question is then to analytically\ncharacterize parameter changes that do or do not matter. In this paper, we\nstudy the sensitivity of probabilistic queries to changes in network parameters\nand prove some tight bounds on the impact that such parameters can have on\nqueries. Our analytic results pinpoint some interesting situations under which\nparameter changes do or do not matter. These results are important for\nknowledge engineers as they help them identify influential network parameters.\nThey also help explain some of the previous experimental results and\nobservations with regards to network robustness against parameter changes.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2011 13:54:07 GMT"}], "update_date": "2011-06-10", "authors_parsed": [["Chan", "H.", ""], ["Darwiche", "A.", ""]]}, {"id": "1106.1816", "submitter": "G. A. Kaminka", "authors": "G. A. Kaminka, D. V. Pynadath, M. Tambe", "title": "Monitoring Teams by Overhearing: A Multi-Agent Plan-Recognition Approach", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 17, pages\n  83-135, 2002", "doi": "10.1613/jair.970", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years are seeing an increasing need for on-line monitoring of teams of\ncooperating agents, e.g., for visualization, or performance tracking. However,\nin monitoring deployed teams, we often cannot rely on the agents to always\ncommunicate their state to the monitoring system. This paper presents a\nnon-intrusive approach to monitoring by 'overhearing', where the monitored\nteam's state is inferred (via plan-recognition) from team-members' routine\ncommunications, exchanged as part of their coordinated task execution, and\nobserved (overheard) by the monitoring system. Key challenges in this approach\ninclude the demanding run-time requirements of monitoring, the scarceness of\nobservations (increasing monitoring uncertainty), and the need to scale-up\nmonitoring to address potentially large teams. To address these, we present a\nset of complementary novel techniques, exploiting knowledge of the social\nstructures and procedures in the monitored team: (i) an efficient probabilistic\nplan-recognition algorithm, well-suited for processing communications as\nobservations; (ii) an approach to exploiting knowledge of the team's social\nbehavior to predict future observations during execution (reducing monitoring\nuncertainty); and (iii) monitoring algorithms that trade expressivity for\nscalability, representing only certain useful monitoring hypotheses, but\nallowing for any number of agents and their different activities to be\nrepresented in a single coherent entity. We present an empirical evaluation of\nthese techniques, in combination and apart, in monitoring a deployed team of\nagents, running on machines physically distributed across the country, and\nengaged in complex, dynamic task execution. We also compare the performance of\nthese techniques to human expert and novice monitors, and show that the\ntechniques presented are capable of monitoring at human-expert levels, despite\nthe difficulty of the task.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2011 13:54:54 GMT"}], "update_date": "2011-06-10", "authors_parsed": [["Kaminka", "G. A.", ""], ["Pynadath", "D. V.", ""], ["Tambe", "M.", ""]]}, {"id": "1106.1817", "submitter": "A. Gorin", "authors": "A. Gorin, I. Langkilde-Geary, M. A. Walker, J. Wright, H. Wright\n  Hastie", "title": "Automatically Training a Problematic Dialogue Predictor for a Spoken\n  Dialogue System", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 16, pages\n  293-319, 2002", "doi": "10.1613/jair.971", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spoken dialogue systems promise efficient and natural access to a large\nvariety of information sources and services from any phone. However, current\nspoken dialogue systems are deficient in their strategies for preventing,\nidentifying and repairing problems that arise in the conversation. This paper\nreports results on automatically training a Problematic Dialogue Predictor to\npredict problematic human-computer dialogues using a corpus of 4692 dialogues\ncollected with the 'How May I Help You' (SM) spoken dialogue system. The\nProblematic Dialogue Predictor can be immediately applied to the system's\ndecision of whether to transfer the call to a human customer care agent, or be\nused as a cue to the system's dialogue manager to modify its behavior to repair\nproblems, and even perhaps, to prevent them. We show that a Problematic\nDialogue Predictor using automatically-obtainable features from the first two\nexchanges in the dialogue can predict problematic dialogues 13.2% more\naccurately than the baseline.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2011 13:55:26 GMT"}], "update_date": "2011-06-10", "authors_parsed": [["Gorin", "A.", ""], ["Langkilde-Geary", "I.", ""], ["Walker", "M. A.", ""], ["Wright", "J.", ""], ["Hastie", "H. Wright", ""]]}, {"id": "1106.1818", "submitter": "R. Nock", "authors": "R. Nock", "title": "Inducing Interpretable Voting Classifiers without Trading Accuracy for\n  Simplicity: Theoretical Results, Approximation Algorithms", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 17, pages\n  137-170, 2002", "doi": "10.1613/jair.986", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in the study of voting classification algorithms have brought\nempirical and theoretical results clearly showing the discrimination power of\nensemble classifiers. It has been previously argued that the search of this\nclassification power in the design of the algorithms has marginalized the need\nto obtain interpretable classifiers. Therefore, the question of whether one\nmight have to dispense with interpretability in order to keep classification\nstrength is being raised in a growing number of machine learning or data mining\npapers. The purpose of this paper is to study both theoretically and\nempirically the problem. First, we provide numerous results giving insight into\nthe hardness of the simplicity-accuracy tradeoff for voting classifiers. Then\nwe provide an efficient \"top-down and prune\" induction heuristic, WIDC, mainly\nderived from recent results on the weak learning and boosting frameworks. It is\nto our knowledge the first attempt to build a voting classifier as a base\nformula using the weak learning framework (the one which was previously highly\nsuccessful for decision tree induction), and not the strong learning framework\n(as usual for such classifiers with boosting-like approaches). While it uses a\nwell-known induction scheme previously successful in other classes of concept\nrepresentations, thus making it easy to implement and compare, WIDC also relies\non recent or new results we give about particular cases of boosting known as\npartition boosting and ranking loss boosting. Experimental results on\nthirty-one domains, most of which readily available, tend to display the\nability of WIDC to produce small, accurate, and interpretable decision\ncommittees.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2011 13:56:01 GMT"}], "update_date": "2011-06-10", "authors_parsed": [["Nock", "R.", ""]]}, {"id": "1106.1819", "submitter": "A. Darwiche", "authors": "A. Darwiche, P. Marquis", "title": "A Knowledge Compilation Map", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 17, pages\n  229-264, 2002", "doi": "10.1613/jair.989", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a perspective on knowledge compilation which calls for analyzing\ndifferent compilation approaches according to two key dimensions: the\nsuccinctness of the target compilation language, and the class of queries and\ntransformations that the language supports in polytime. We then provide a\nknowledge compilation map, which analyzes a large number of existing target\ncompilation languages according to their succinctness and their polytime\ntransformations and queries. We argue that such analysis is necessary for\nplacing new compilation approaches within the context of existing ones. We also\ngo beyond classical, flat target compilation languages based on CNF and DNF,\nand consider a richer, nested class based on directed acyclic graphs (such as\nOBDDs), which we show to include a relatively large number of target\ncompilation languages.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2011 13:56:25 GMT"}], "update_date": "2011-06-10", "authors_parsed": [["Darwiche", "A.", ""], ["Marquis", "P.", ""]]}, {"id": "1106.1820", "submitter": "R. Barzilay", "authors": "R. Barzilay, N. Elhadad", "title": "Inferring Strategies for Sentence Ordering in Multidocument News\n  Summarization", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 17, pages\n  35-55, 2002", "doi": "10.1613/jair.991", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of organizing information for multidocument summarization so that\nthe generated summary is coherent has received relatively little attention.\nWhile sentence ordering for single document summarization can be determined\nfrom the ordering of sentences in the input article, this is not the case for\nmultidocument summarization where summary sentences may be drawn from different\ninput articles. In this paper, we propose a methodology for studying the\nproperties of ordering information in the news genre and describe experiments\ndone on a corpus of multiple acceptable orderings we developed for the task.\nBased on these experiments, we implemented a strategy for ordering information\nthat combines constraints from chronological order of events and topical\nrelatedness. Evaluation of our augmented algorithm shows a significant\nimprovement of the ordering over two baseline strategies.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2011 13:57:02 GMT"}], "update_date": "2011-06-10", "authors_parsed": [["Barzilay", "R.", ""], ["Elhadad", "N.", ""]]}, {"id": "1106.1821", "submitter": "K. Tumer", "authors": "K. Tumer, D. H. Wolpert", "title": "Collective Intelligence, Data Routing and Braess' Paradox", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 16, pages\n  359-387, 2002", "doi": "10.1613/jair.995", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of designing the the utility functions of the\nutility-maximizing agents in a multi-agent system so that they work\nsynergistically to maximize a global utility. The particular problem domain we\nexplore is the control of network routing by placing agents on all the routers\nin the network. Conventional approaches to this task have the agents all use\nthe Ideal Shortest Path routing Algorithm (ISPA). We demonstrate that in many\ncases, due to the side-effects of one agent's actions on another agent's\nperformance, having agents use ISPA's is suboptimal as far as global aggregate\ncost is concerned, even when they are only used to route infinitesimally small\namounts of traffic. The utility functions of the individual agents are not\n\"aligned\" with the global utility, intuitively speaking. As a particular\nexample of this we present an instance of Braess' paradox in which adding new\nlinks to a network whose agents all use the ISPA results in a decrease in\noverall throughput. We also demonstrate that load-balancing, in which the\nagents' decisions are collectively made to optimize the global cost incurred by\nall traffic currently being routed, is suboptimal as far as global cost\naveraged across time is concerned. This is also due to 'side-effects', in this\ncase of current routing decision on future traffic. The mathematics of\nCollective Intelligence (COIN) is concerned precisely with the issue of\navoiding such deleterious side-effects in multi-agent systems, both over time\nand space. We present key concepts from that mathematics and use them to derive\nan algorithm whose ideal version should have better performance than that of\nhaving all agents use the ISPA, even in the infinitesimal limit. We present\nexperiments verifying this, and also showing that a machine-learning-based\nversion of this COIN algorithm in which costs are only imprecisely estimated\nvia empirical means (a version potentially applicable in the real world) also\noutperforms the ISPA, despite having access to less information than does the\nISPA. In particular, this COIN algorithm almost always avoids Braess' paradox.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2011 13:57:43 GMT"}], "update_date": "2011-06-10", "authors_parsed": [["Tumer", "K.", ""], ["Wolpert", "D. H.", ""]]}, {"id": "1106.1822", "submitter": "C. Guestrin", "authors": "C. Guestrin, D. Koller, R. Parr, S. Venkataraman", "title": "Efficient Solution Algorithms for Factored MDPs", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 19, pages\n  399-468, 2003", "doi": "10.1613/jair.1000", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of planning under uncertainty in large\nMarkov Decision Processes (MDPs). Factored MDPs represent a complex state space\nusing state variables and the transition model using a dynamic Bayesian\nnetwork. This representation often allows an exponential reduction in the\nrepresentation size of structured MDPs, but the complexity of exact solution\nalgorithms for such MDPs can grow exponentially in the representation size. In\nthis paper, we present two approximate solution algorithms that exploit\nstructure in factored MDPs. Both use an approximate value function represented\nas a linear combination of basis functions, where each basis function involves\nonly a small subset of the domain variables. A key contribution of this paper\nis that it shows how the basic operations of both algorithms can be performed\nefficiently in closed form, by exploiting both additive and context-specific\nstructure in a factored MDP. A central element of our algorithms is a novel\nlinear program decomposition technique, analogous to variable elimination in\nBayesian networks, which reduces an exponentially large LP to a provably\nequivalent, polynomial-sized one. One algorithm uses approximate linear\nprogramming, and the second approximate dynamic programming. Our dynamic\nprogramming algorithm is novel in that it uses an approximation based on\nmax-norm, a technique that more directly minimizes the terms that appear in\nerror bounds for approximate MDP algorithms. We provide experimental results on\nproblems with over 10^40 states, demonstrating a promising indication of the\nscalability of our approach, and compare our algorithm to an existing\nstate-of-the-art approach, showing, in some problems, exponential gains in\ncomputation time.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2011 13:58:37 GMT"}], "update_date": "2011-06-10", "authors_parsed": [["Guestrin", "C.", ""], ["Koller", "D.", ""], ["Parr", "R.", ""], ["Venkataraman", "S.", ""]]}, {"id": "1106.1853", "submitter": "Ching-an Hsiao", "authors": "Ching-an Hsiao and Xinchun Tian", "title": "Intelligent decision: towards interpreting the Pe Algorithm", "comments": "23pages, 12 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human intelligence lies in the algorithm, the nature of algorithm lies in\nthe classification, and the classification is equal to outlier detection. A lot\nof algorithms have been proposed to detect outliers, meanwhile a lot of\ndefinitions. Unsatisfying point is that definitions seem vague, which makes the\nsolution an ad hoc one. We analyzed the nature of outliers, and give two clear\ndefinitions. We then develop an efficient RDD algorithm, which converts outlier\nproblem to pattern and degree problem. Furthermore, a collapse mechanism was\nintroduced by IIR algorithm, which can be united seamlessly with the RDD\nalgorithm and serve for the final decision. Both algorithms are originated from\nthe study on general AI. The combined edition is named as Pe algorithm, which\nis the basis of the intelligent decision. Here we introduce longest k-turn\nsubsequence problem and corresponding solution as an example to interpret the\nfunction of Pe algorithm in detecting curve-type outliers. We also give a\ncomparison between IIR algorithm and Pe algorithm, where we can get a better\nunderstanding at both algorithms. A short discussion about intelligence is\nadded to demonstrate the function of the Pe algorithm. Related experimental\nresults indicate its robustness.\n", "versions": [{"version": "v1", "created": "Thu, 9 Jun 2011 16:45:49 GMT"}, {"version": "v2", "created": "Fri, 10 Jun 2011 13:53:05 GMT"}, {"version": "v3", "created": "Tue, 23 Aug 2011 03:25:58 GMT"}], "update_date": "2011-08-24", "authors_parsed": [["Hsiao", "Ching-an", ""], ["Tian", "Xinchun", ""]]}, {"id": "1106.1957", "submitter": "Frederick Maier", "authors": "Frederick Maier", "title": "Interdefinability of defeasible logic and logic programming under the\n  well-founded semantics", "comments": "36 pages; To appear in Theory and Practice of Logic Programming\n  (TPLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a method of translating theories of Nute's defeasible logic into\nlogic programs, and a corresponding translation in the opposite direction.\nUnder certain natural restrictions, the conclusions of defeasible theories\nunder the ambiguity propagating defeasible logic ADL correspond to those of the\nwell-founded semantics for normal logic programs, and so it turns out that the\ntwo formalisms are closely related. Using the same translation of logic\nprograms into defeasible theories, the semantics for the ambiguity blocking\ndefeasible logic NDL can be seen as indirectly providing an ambiguity blocking\nsemantics for logic programs. We also provide antimonotone operators for both\nADL and NDL, each based on the Gelfond-Lifschitz (GL) operator for logic\nprograms. For defeasible theories without defeaters or priorities on rules, the\noperator for ADL corresponds to the GL operator and so can be seen as partially\ncapturing the consequences according to ADL. Similarly, the operator for NDL\ncaptures the consequences according to NDL, though in this case no restrictions\non theories apply. Both operators can be used to define stable model semantics\nfor defeasible theories.\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2011 05:20:17 GMT"}], "update_date": "2011-06-13", "authors_parsed": [["Maier", "Frederick", ""]]}, {"id": "1106.1998", "submitter": "Yi Sun", "authors": "Yi Sun and Faustino Gomez and Tom Schaul and Juergen Schmidhuber", "title": "A Linear Time Natural Evolution Strategy for Non-Separable Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel Natural Evolution Strategy (NES) variant, the Rank-One NES\n(R1-NES), which uses a low rank approximation of the search distribution\ncovariance matrix. The algorithm allows computation of the natural gradient\nwith cost linear in the dimensionality of the parameter space, and excels in\nsolving high-dimensional non-separable problems, including the best result to\ndate on the Rosenbrock function (512 dimensions).\n", "versions": [{"version": "v1", "created": "Fri, 10 Jun 2011 09:56:00 GMT"}, {"version": "v2", "created": "Mon, 13 Jun 2011 09:57:57 GMT"}], "update_date": "2011-06-14", "authors_parsed": [["Sun", "Yi", ""], ["Gomez", "Faustino", ""], ["Schaul", "Tom", ""], ["Schmidhuber", "Juergen", ""]]}, {"id": "1106.2363", "submitter": "Daniel Hsu", "authors": "Daniel Hsu, Sham M. Kakade, Tong Zhang", "title": "Random design analysis of ridge regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.ST cs.AI cs.LG stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work gives a simultaneous analysis of both the ordinary least squares\nestimator and the ridge regression estimator in the random design setting under\nmild assumptions on the covariate/response distributions. In particular, the\nanalysis provides sharp results on the ``out-of-sample'' prediction error, as\nopposed to the ``in-sample'' (fixed design) error. The analysis also reveals\nthe effect of errors in the estimated covariance structure, as well as the\neffect of modeling errors, neither of which effects are present in the fixed\ndesign setting. The proofs of the main results are based on a simple\ndecomposition lemma combined with concentration inequalities for random vectors\nand matrices.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2011 01:08:48 GMT"}, {"version": "v2", "created": "Tue, 25 Mar 2014 02:16:11 GMT"}], "update_date": "2014-03-26", "authors_parsed": [["Hsu", "Daniel", ""], ["Kakade", "Sham M.", ""], ["Zhang", "Tong", ""]]}, {"id": "1106.2369", "submitter": "Daniel Hsu", "authors": "Miroslav Dudik, Daniel Hsu, Satyen Kale, Nikos Karampatziakis, John\n  Langford, Lev Reyzin, Tong Zhang", "title": "Efficient Optimal Learning for Contextual Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of learning in an online setting where the learner\nrepeatedly observes features, selects among a set of actions, and receives\nreward for the action taken. We provide the first efficient algorithm with an\noptimal regret. Our algorithm uses a cost sensitive classification learner as\nan oracle and has a running time $\\mathrm{polylog}(N)$, where $N$ is the number\nof classification rules among which the oracle might choose. This is\nexponentially faster than all previous algorithms that achieve optimal regret\nin this setting. Our formulation also enables us to create an algorithm with\nregret that is additive rather than multiplicative in feedback delay as in all\nprevious work.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2011 01:57:52 GMT"}], "update_date": "2011-06-17", "authors_parsed": [["Dudik", "Miroslav", ""], ["Hsu", "Daniel", ""], ["Kale", "Satyen", ""], ["Karampatziakis", "Nikos", ""], ["Langford", "John", ""], ["Reyzin", "Lev", ""], ["Zhang", "Tong", ""]]}, {"id": "1106.2489", "submitter": "Craig Boutilier", "authors": "Craig Boutilier (Department of Computer Science, University of\n  Toronto, Canada)", "title": "Eliciting Forecasts from Self-interested Experts: Scoring Rules for\n  Decision Makers", "comments": "11 pages 4 figures pdflatex See\n  http://www.cs.toronto.edu/~cebly/papers.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.MA cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Scoring rules for eliciting expert predictions of random variables are\nusually developed assuming that experts derive utility only from the quality of\ntheir predictions (e.g., score awarded by the rule, or payoff in a prediction\nmarket). We study a more realistic setting in which (a) the principal is a\ndecision maker and will take a decision based on the expert's prediction; and\n(b) the expert has an inherent interest in the decision. For example, in a\ncorporate decision market, the expert may derive different levels of utility\nfrom the actions taken by her manager. As a consequence the expert will usually\nhave an incentive to misreport her forecast to influence the choice of the\ndecision maker if typical scoring rules are used. We develop a general model\nfor this setting and introduce the concept of a compensation rule. When\ncombined with the expert's inherent utility for decisions, a compensation rule\ninduces a net scoring rule that behaves like a normal scoring rule. Assuming\nfull knowledge of expert utility, we provide a complete characterization of all\n(strictly) proper compensation rules. We then analyze the situation where the\nexpert's utility function is not fully known to the decision maker. We show\nbounds on: (a) expert incentive to misreport; (b) the degree to which an expert\nwill misreport; and (c) decision maker loss in utility due to such uncertainty.\nThese bounds depend in natural ways on the degree of uncertainty, the local\ndegree of convexity of net scoring function, and natural properties of the\ndecision maker's utility function. They also suggest optimization procedures\nfor the design of compensation rules. Finally, we briefly discuss the use of\ncompensation rules as market scoring rules for self-interested experts in a\nprediction market.\n", "versions": [{"version": "v1", "created": "Mon, 13 Jun 2011 17:04:03 GMT"}], "update_date": "2011-06-14", "authors_parsed": [["Boutilier", "Craig", "", "Department of Computer Science, University of\n  Toronto, Canada"]]}, {"id": "1106.2647", "submitter": "Joseph Y. Halpern", "authors": "Joseph Y. Halpern", "title": "From Causal Models To Counterfactual Structures", "comments": "A preliminary version of this paper appears in the Proceedings of the\n  Twelfth International Conference on Principles of Knowledge Representation\n  and Reasoning (KR 2010), 2010.}", "journal-ref": "Review of Symbolic Logic 6:2, 2013, pp. 305--322", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Galles and Pearl claimed that \"for recursive models, the causal model\nframework does not add any restrictions to counterfactuals, beyond those\nimposed by Lewis's [possible-worlds] framework.\" This claim is examined\ncarefully, with the goal of clarifying the exact relationship between causal\nmodels and Lewis's framework. Recursive models are shown to correspond\nprecisely to a subclass of (possible-world) counterfactual structures. On the\nother hand, a slight generalization of recursive models, models where all\nequations have unique solutions, is shown to be incomparable in expressive\npower to counterfactual structures, despite the fact that the Galles and Pearl\narguments should apply to them as well. The problem with the Galles and Pearl\nargument is identified: an axiom that they viewed as irrelevant, because it\ninvolved disjunction (which was not in their language), is not irrelevant at\nall.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2011 09:34:05 GMT"}, {"version": "v2", "created": "Sat, 17 Aug 2013 13:36:57 GMT"}], "update_date": "2013-08-20", "authors_parsed": [["Halpern", "Joseph Y.", ""]]}, {"id": "1106.2652", "submitter": "Joseph Y. Halpern", "authors": "Joseph Y. Halpern and Christopher Hitchcock", "title": "Actual causation and the art of modeling", "comments": "In <em>Heuristics, Probability and Causality: A Tribute to Judea\n  Pearl</em> (editors, R. Dechter, H. Geffner, and J. Y. Halpern), College\n  Publications, 2010, pp. 383-406", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We look more carefully at the modeling of causality using structural\nequations. It is clear that the structural equations can have a major impact on\nthe conclusions we draw about causality. In particular, the choice of variables\nand their values can also have a significant impact on causality. These choices\nare, to some extent, subjective. We consider what counts as an appropriate\nchoice. More generally, we consider what makes a model an appropriate model,\nespecially if we want to take defaults into account, as was argued is necessary\nin recent work.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2011 09:40:55 GMT"}], "update_date": "2011-06-15", "authors_parsed": [["Halpern", "Joseph Y.", ""], ["Hitchcock", "Christopher", ""]]}, {"id": "1106.2662", "submitter": "Luca Rose", "authors": "Luca Rose, Samir M. Perlaza, Samson Lasaulce, M\\'erouane Debbah", "title": "Learning Equilibria with Partial Information in Decentralized Wireless\n  Networks", "comments": "16 pages, 5 figures, 1 table. To appear in IEEE Communication\n  Magazine, special Issue on Game Theory", "journal-ref": null, "doi": "10.1109/MCOM.2011.5978427", "report-no": null, "categories": "cs.LG cs.AI cs.GT cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, a survey of several important equilibrium concepts for\ndecentralized networks is presented. The term decentralized is used here to\nrefer to scenarios where decisions (e.g., choosing a power allocation policy)\nare taken autonomously by devices interacting with each other (e.g., through\nmutual interference). The iterative long-term interaction is characterized by\nstable points of the wireless network called equilibria. The interest in these\nequilibria stems from the relevance of network stability and the fact that they\ncan be achieved by letting radio devices to repeatedly interact over time. To\nachieve these equilibria, several learning techniques, namely, the best\nresponse dynamics, fictitious play, smoothed fictitious play, reinforcement\nlearning algorithms, and regret matching, are discussed in terms of information\nrequirements and convergence properties. Most of the notions introduced here,\nfor both equilibria and learning schemes, are illustrated by a simple case\nstudy, namely, an interference channel with two transmitter-receiver pairs.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2011 09:58:36 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Rose", "Luca", ""], ["Perlaza", "Samir M.", ""], ["Lasaulce", "Samson", ""], ["Debbah", "M\u00e9rouane", ""]]}, {"id": "1106.2692", "submitter": "Nicolas Peltier", "authors": "Vincent Aravantinos and Nicolas Peltier", "title": "Generating Schemata of Resolution Proofs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two distinct algorithms are presented to extract (schemata of) resolution\nproofs from closed tableaux for propositional schemata. The first one handles\nthe most efficient version of the tableau calculus but generates very complex\nderivations (denoted by rather elaborate rewrite systems). The second one has\nthe advantage that much simpler systems can be obtained, however the considered\nproof procedure is less efficient.\n", "versions": [{"version": "v1", "created": "Tue, 14 Jun 2011 12:40:07 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Aravantinos", "Vincent", ""], ["Peltier", "Nicolas", ""]]}, {"id": "1106.3361", "submitter": "Miron Kursa", "authors": "Miron B. Kursa and {\\L}ukasz Komsta and Witold R. Rudnicki", "title": "Random forest models of the retention constants in the thin layer\n  chromatography", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the current study we examine an application of the machine learning\nmethods to model the retention constants in the thin layer chromatography\n(TLC). This problem can be described with hundreds or even thousands of\ndescriptors relevant to various molecular properties, most of them redundant\nand not relevant for the retention constant prediction. Hence we employed\nfeature selection to significantly reduce the number of attributes.\nAdditionally we have tested application of the bagging procedure to the feature\nselection. The random forest regression models were built using selected\nvariables. The resulting models have better correlation with the experimental\ndata than the reference models obtained with linear regression. The\ncross-validation confirms robustness of the models.\n", "versions": [{"version": "v1", "created": "Thu, 16 Jun 2011 22:05:21 GMT"}], "update_date": "2011-06-20", "authors_parsed": [["Kursa", "Miron B.", ""], ["Komsta", "\u0141ukasz", ""], ["Rudnicki", "Witold R.", ""]]}, {"id": "1106.3457", "submitter": "Panos Rondogiannis", "authors": "A. Charalambidis, K. Handjopoulos, P. Rondogiannis, W. W. Wadge", "title": "Extensional Higher-Order Logic Programming", "comments": "45 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a purely extensional semantics for higher-order logic programming.\nIn this semantics program predicates denote sets of ordered tuples, and two\npredicates are equal iff they are equal as sets. Moreover, every program has a\nunique minimum Herbrand model which is the greatest lower bound of all Herbrand\nmodels of the program and the least fixed-point of an immediate consequence\noperator. We also propose an SLD-resolution proof procedure which is proven\nsound and complete with respect to the minimum model semantics. In other words,\nwe provide a purely extensional theoretical framework for higher-order logic\nprogramming which generalizes the familiar theory of classical (first-order)\nlogic programming.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2011 12:01:01 GMT"}], "update_date": "2011-06-20", "authors_parsed": [["Charalambidis", "A.", ""], ["Handjopoulos", "K.", ""], ["Rondogiannis", "P.", ""], ["Wadge", "W. W.", ""]]}, {"id": "1106.3498", "submitter": "Olivier Bailleux", "authors": "Olivier Bailleux", "title": "On the expressive power of unit resolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This preliminary report addresses the expressive power of unit resolution\nregarding input data encoded with partial truth assignments of propositional\nvariables. A characterization of the functions that are computable in this way,\nwhich we propose to call propagatable functions, is given. By establishing that\npropagatable functions can also be computed using monotone circuits, we show\nthat there exist polynomial time complexity propagable functions requiring an\nexponential amount of clauses to be computed using unit resolution. These\nresults shed new light on studying CNF encodings of NP-complete problems in\norder to solve them using propositional satisfiability algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 17 Jun 2011 14:35:28 GMT"}], "update_date": "2011-06-20", "authors_parsed": [["Bailleux", "Olivier", ""]]}, {"id": "1106.3600", "submitter": "Liane Gabora", "authors": "Liane Gabora and Apara Ranjan", "title": "How Insight Emerges in a Distributed, Content-addressable Memory", "comments": "17 pages; 2 figures", "journal-ref": "In A. Bristol, O. Vartanian, & J. Kaufman (Eds.), The neuroscience\n  of creativity (pp. 19-43). Cambridge, MA: MIT Press (2013)", "doi": "10.7551/mitpress/9780262019583.003.0002", "report-no": null, "categories": "q-bio.NC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We begin this chapter with the bold claim that it provides a neuroscientific\nexplanation of the magic of creativity. Creativity presents a formidable\nchallenge for neuroscience. Neuroscience generally involves studying what\nhappens in the brain when someone engages in a task that involves responding to\na stimulus, or retrieving information from memory and using it the right way,\nor at the right time. If the relevant information is not already encoded in\nmemory, the task generally requires that the individual make systematic use of\ninformation that is encoded in memory. But creativity is different. It\nparadoxically involves studying how someone pulls out of their brain something\nthat was never put into it! Moreover, it must be something both new and useful,\nor appropriate to the task at hand. The ability to pull out of memory something\nnew and appropriate that was never stored there in the first place is what we\nrefer to as the magic of creativity. Even if we are so fortunate as to\ndetermine which areas of the brain are active and how these areas interact\nduring creative thought, we will not have an answer to the question of how the\nbrain comes up with solutions and artworks that are new and appropriate. On the\nother hand, since the representational capacity of neurons emerges at a level\nthat is higher than that of the individual neurons themselves, the inner\nworkings of neurons is too low a level to explain the magic of creativity. Thus\nwe look to a level that is midway between gross brain regions and neurons.\nSince creativity generally involves combining concepts from different domains,\nor seeing old ideas from new perspectives, we focus our efforts on the neural\nmechanisms underlying the representation of concepts and ideas. Thus we ask\nquestions about the brain at the level that accounts for its representational\ncapacity, i.e. at the level of distributed aggregates of neurons.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jun 2011 00:26:40 GMT"}, {"version": "v2", "created": "Sun, 30 Jun 2019 01:41:58 GMT"}, {"version": "v3", "created": "Fri, 5 Jul 2019 22:03:09 GMT"}], "update_date": "2019-07-09", "authors_parsed": [["Gabora", "Liane", ""], ["Ranjan", "Apara", ""]]}, {"id": "1106.3655", "submitter": "Christos Dimitrakakis", "authors": "Christos Dimitrakakis, Constantin Rothkopf", "title": "Bayesian multitask inverse reinforcement learning", "comments": "Corrected version. 13 pages, 8 figures", "journal-ref": "Recent Advances in Reinforcement Learning LNCS 7188, pp. 273-284,\n  2012", "doi": "10.1007/978-3-642-29946-9_27", "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generalise the problem of inverse reinforcement learning to multiple\ntasks, from multiple demonstrations. Each one may represent one expert trying\nto solve a different task, or as different experts trying to solve the same\ntask. Our main contribution is to formalise the problem as statistical\npreference elicitation, via a number of structured priors, whose form captures\nour biases about the relatedness of different tasks or expert policies. In\ndoing so, we introduce a prior on policy optimality, which is more natural to\nspecify. We show that our framework allows us not only to learn to efficiently\nfrom multiple experts but to also effectively differentiate between the goals\nof each. Possible applications include analysing the intrinsic motivations of\nsubjects in behavioural experiments and learning from multiple teachers.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jun 2011 15:00:45 GMT"}, {"version": "v2", "created": "Thu, 17 Nov 2011 15:16:11 GMT"}], "update_date": "2012-09-04", "authors_parsed": [["Dimitrakakis", "Christos", ""], ["Rothkopf", "Constantin", ""]]}, {"id": "1106.3685", "submitter": "Christoph Benzmueller", "authors": "Christoph Benzmueller and Dov Gabbay and Valerio Genovese and Daniele\n  Rispoli", "title": "Embedding and Automating Conditional Logics in Classical Higher-Order\n  Logic", "comments": "15 pages, 1 Figure, 1 Table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A sound and complete embedding of conditional logics into classical\nhigher-order logic is presented. This embedding enables the application of\noff-the-shelf higher-order automated theorem provers and model finders for\nreasoning within and about conditional logics.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jun 2011 20:20:37 GMT"}, {"version": "v2", "created": "Fri, 24 Jun 2011 04:15:11 GMT"}, {"version": "v3", "created": "Wed, 17 Aug 2011 08:03:50 GMT"}], "update_date": "2011-08-18", "authors_parsed": [["Benzmueller", "Christoph", ""], ["Gabbay", "Dov", ""], ["Genovese", "Valerio", ""], ["Rispoli", "Daniele", ""]]}, {"id": "1106.3703", "submitter": "Artemy Kolchinsky", "authors": "Artemy Kolchinsky, Luis M. Rocha", "title": "Prediction and Modularity in Dynamical Systems", "comments": "v1 published in ECAL 2011 (European Conference on Artificial Life).\n  v2 fixes error in causal risk (number of parameters should be based on\n  training distribution)", "journal-ref": null, "doi": null, "report-no": null, "categories": "nlin.AO cs.AI cs.IT cs.LG cs.SY math.IT q-bio.QM stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identifying and understanding modular organizations is centrally important in\nthe study of complex systems. Several approaches to this problem have been\nadvanced, many framed in information-theoretic terms. Our treatment starts from\nthe complementary point of view of statistical modeling and prediction of\ndynamical systems. It is known that for finite amounts of training data,\nsimpler models can have greater predictive power than more complex ones. We use\nthe trade-off between model simplicity and predictive accuracy to generate\noptimal multiscale decompositions of dynamical networks into weakly-coupled,\nsimple modules. State-dependent and causal versions of our method are also\nproposed.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jun 2011 04:20:16 GMT"}, {"version": "v2", "created": "Fri, 16 Jan 2015 06:53:24 GMT"}], "update_date": "2015-01-19", "authors_parsed": [["Kolchinsky", "Artemy", ""], ["Rocha", "Luis M.", ""]]}, {"id": "1106.3767", "submitter": "Georg Gottlob", "authors": "Georg Gottlob and Thomas Schwentick", "title": "Rewriting Ontological Queries into Small Nonrecursive Datalog Programs", "comments": "A shorter version is presented at the 24th International Workshop on\n  Description Logics, DL 2011, Barcelona, Spain, July 13-16, 2011. The present\n  version mainly extends the proof of Theorem 1 in Section 3. We plan to post\n  further extended versions of this paper in the near future", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the setting of ontological database access, where an Abox is\ngiven in form of a relational database D and where a Boolean conjunctive query\nq has to be evaluated against D modulo a Tbox T formulated in DL-Lite or Linear\nDatalog+/-. It is well-known that (T,q) can be rewritten into an equivalent\nnonrecursive Datalog program P that can be directly evaluated over D. However,\nfor Linear Datalog? or for DL-Lite versions that allow for role inclusion, the\nrewriting methods described so far result in a nonrecursive Datalog program P\nof size exponential in the joint size of T and q. This gives rise to the\ninteresting question of whether such a rewriting necessarily needs to be of\nexponential size. In this paper we show that it is actually possible to\ntranslate (T,q) into a polynomially sized equivalent nonrecursive Datalog\nprogram P.\n", "versions": [{"version": "v1", "created": "Sun, 19 Jun 2011 18:20:07 GMT"}, {"version": "v2", "created": "Sun, 10 Jul 2011 18:26:56 GMT"}, {"version": "v3", "created": "Sat, 23 Jul 2011 10:19:21 GMT"}], "update_date": "2011-07-26", "authors_parsed": [["Gottlob", "Georg", ""], ["Schwentick", "Thomas", ""]]}, {"id": "1106.3862", "submitter": "Jeremy Nicholas Butterfield", "authors": "Adam Caulton, Jeremy Butterfield", "title": "On Kinds of Indiscernibility in Logic and Metaphysics", "comments": "55 pages, 21 figures. Forthcoming, after an Appendectomy, in the\n  British Journal for the Philosophy of Science", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.hist-ph cs.AI quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using the Hilbert-Bernays account as a spring-board, we first define four\nways in which two objects can be discerned from one another, using the\nnon-logical vocabulary of the language concerned. (These definitions are based\non definitions made by Quine and Saunders.) Because of our use of the\nHilbert-Bernays account, these definitions are in terms of the syntax of the\nlanguage. But we also relate our definitions to the idea of permutations on the\ndomain of quantification, and their being symmetries. These relations turn out\nto be subtle---some natural conjectures about them are false. We will see in\nparticular that the idea of symmetry meshes with a species of indiscernibility\nthat we will call `absolute indiscernibility'. We then report all the logical\nimplications between our four kinds of discernibility. We use these four kinds\nas a resource for stating four metaphysical theses about identity. Three of\nthese theses articulate two traditional philosophical themes: viz. the\nprinciple of the identity of indiscernibles (which will come in two versions),\nand haecceitism. The fourth is recent. Its most notable feature is that it\nmakes diversity (i.e. non-identity) weaker than what we will call individuality\n(being an individual): two objects can be distinct but not individuals. For\nthis reason, it has been advocated both for quantum particles and for spacetime\npoints. Finally, we locate this fourth metaphysical thesis in a broader\nposition, which we call structuralism. We conclude with a discussion of the\nsemantics suitable for a structuralist, with particular reference to physical\ntheories as well as elementary model theory.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2011 10:56:24 GMT"}], "update_date": "2011-06-21", "authors_parsed": [["Caulton", "Adam", ""], ["Butterfield", "Jeremy", ""]]}, {"id": "1106.3876", "submitter": "Amandine Bellenger", "authors": "Amandine Bellenger (LITIS), Sylvain Gatepaille", "title": "Uncertainty in Ontologies: Dempster-Shafer Theory for Data Fusion\n  Applications", "comments": "Workshop on Theory of Belief Functions, Brest: France (2010)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays ontologies present a growing interest in Data Fusion applications.\nAs a matter of fact, the ontologies are seen as a semantic tool for describing\nand reasoning about sensor data, objects, relations and general domain\ntheories. In addition, uncertainty is perhaps one of the most important\ncharacteristics of the data and information handled by Data Fusion. However,\nthe fundamental nature of ontologies implies that ontologies describe only\nasserted and veracious facts of the world. Different probabilistic, fuzzy and\nevidential approaches already exist to fill this gap; this paper recaps the\nmost popular tools. However none of the tools meets exactly our purposes.\nTherefore, we constructed a Dempster-Shafer ontology that can be imported into\nany specific domain ontology and that enables us to instantiate it in an\nuncertain manner. We also developed a Java application that enables reasoning\nabout these uncertain ontological instances.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2011 12:05:20 GMT"}], "update_date": "2011-06-21", "authors_parsed": [["Bellenger", "Amandine", "", "LITIS"], ["Gatepaille", "Sylvain", ""]]}, {"id": "1106.3932", "submitter": "Jean-Louis Dessalles", "authors": "Jean-Louis J.-L. Dessalles (IC2)", "title": "Coincidences and the encounter problem: A formal account", "comments": "30th Annual Conference of the Cognitive Science Society, Washington :\n  United States (2008)", "journal-ref": null, "doi": null, "report-no": "jld-08020201", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Individuals have an intuitive perception of what makes a good coincidence.\nThough the sensitivity to coincidences has often been presented as resulting\nfrom an erroneous assessment of probability, it appears to be a genuine\ncompetence, based on non-trivial computations. The model presented here\nsuggests that coincidences occur when subjects perceive complexity drops.\nCo-occurring events are, together, simpler than if considered separately. This\nmodel leads to a possible redefinition of subjective probability.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2011 15:05:53 GMT"}], "update_date": "2011-06-21", "authors_parsed": [["Dessalles", "Jean-Louis J. -L.", "", "IC2"]]}, {"id": "1106.3967", "submitter": "Emilio Ferrara", "authors": "Emilio Ferrara and Robert Baumgartner", "title": "Intelligent Self-Repairable Web Wrappers", "comments": "12 pages, 4 figures; Proceedings of the 12th International Conference\n  of the Italian Association for Artificial Intelligence, 2011", "journal-ref": "Lecture Notes in Computer Science, 6934:274-285, 2011", "doi": "10.1007/978-3-642-23954-0_26", "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The amount of information available on the Web grows at an incredible high\nrate. Systems and procedures devised to extract these data from Web sources\nalready exist, and different approaches and techniques have been investigated\nduring the last years. On the one hand, reliable solutions should provide\nrobust algorithms of Web data mining which could automatically face possible\nmalfunctioning or failures. On the other, in literature there is a lack of\nsolutions about the maintenance of these systems. Procedures that extract Web\ndata may be strictly interconnected with the structure of the data source\nitself; thus, malfunctioning or acquisition of corrupted data could be caused,\nfor example, by structural modifications of data sources brought by their\nowners. Nowadays, verification of data integrity and maintenance are mostly\nmanually managed, in order to ensure that these systems work correctly and\nreliably. In this paper we propose a novel approach to create procedures able\nto extract data from Web sources -- the so called Web wrappers -- which can\nface possible malfunctioning caused by modifications of the structure of the\ndata source, and can automatically repair themselves.\n", "versions": [{"version": "v1", "created": "Mon, 20 Jun 2011 17:02:40 GMT"}], "update_date": "2012-02-13", "authors_parsed": [["Ferrara", "Emilio", ""], ["Baumgartner", "Robert", ""]]}, {"id": "1106.4083", "submitter": "Daniel Harabor D", "authors": "Daniel Harabor and Adi Botea and Philip Kilby", "title": "Symmetry-Based Search Space Reduction For Grid Maps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we explore a symmetry-based search space reduction technique\nwhich can speed up optimal pathfinding on undirected uniform-cost grid maps by\nup to 38 times. Our technique decomposes grid maps into a set of empty\nrectangles, removing from each rectangle all interior nodes and possibly some\nfrom along the perimeter. We then add a series of macro-edges between selected\npairs of remaining perimeter nodes to facilitate provably optimal traversal\nthrough each rectangle. We also develop a novel online pruning technique to\nfurther speed up search. Our algorithm is fast, memory efficient and retains\nthe same optimality and completeness guarantees as searching on an unmodified\ngrid map.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2011 04:02:30 GMT"}], "update_date": "2011-06-22", "authors_parsed": [["Harabor", "Daniel", ""], ["Botea", "Adi", ""], ["Kilby", "Philip", ""]]}, {"id": "1106.4090", "submitter": "EPTCS", "authors": "Maria Teresa Llano (Heriot-Watt University), Andrew Ireland\n  (Heriot-Watt University), Alison Pease (University of Edinburgh)", "title": "Discovery of Invariants through Automated Theory Formation", "comments": "In Proceedings Refine 2011, arXiv:1106.3488", "journal-ref": "EPTCS 55, 2011, pp. 1-19", "doi": "10.4204/EPTCS.55.1", "report-no": null, "categories": "cs.LO cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Refinement is a powerful mechanism for mastering the complexities that arise\nwhen formally modelling systems. Refinement also brings with it additional\nproof obligations -- requiring a developer to discover properties relating to\ntheir design decisions. With the goal of reducing this burden, we have\ninvestigated how a general purpose theory formation tool, HR, can be used to\nautomate the discovery of such properties within the context of Event-B. Here\nwe develop a heuristic approach to the automatic discovery of invariants and\nreport upon a series of experiments that we undertook in order to evaluate our\napproach. The set of heuristics developed provides systematic guidance in\ntailoring HR for a given Event-B development. These heuristics are based upon\nproof-failure analysis, and have given rise to some promising results.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2011 05:24:01 GMT"}], "update_date": "2011-06-22", "authors_parsed": [["Llano", "Maria Teresa", "", "Heriot-Watt University"], ["Ireland", "Andrew", "", "Heriot-Watt University"], ["Pease", "Alison", "", "University of Edinburgh"]]}, {"id": "1106.4218", "submitter": "Walter Quattrociocchi", "authors": "Francesca Giardini, Walter Quattrociocchi, Rosaria Conte", "title": "Rooting opinions in the minds: a cognitive model and a formal account of\n  opinions and their dynamics", "comments": null, "journal-ref": "SNAMAS 2011 : THIRD SOCIAL NETWORKS AND MULTIAGENT SYSTEMS\n  SYMPOSIUM SNAMAS@AISB 2011", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The study of opinions, their formation and change, is one of the defining\ntopics addressed by social psychology, but in recent years other disciplines,\nlike computer science and complexity, have tried to deal with this issue.\nDespite the flourishing of different models and theories in both fields,\nseveral key questions still remain unanswered. The understanding of how\nopinions change and the way they are affected by social influence are\nchallenging issues requiring a thorough analysis of opinion per se but also of\nthe way in which they travel between agents' minds and are modulated by these\nexchanges. To account for the two-faceted nature of opinions, which are mental\nentities undergoing complex social processes, we outline a preliminary model in\nwhich a cognitive theory of opinions is put forward and it is paired with a\nformal description of them and of their spreading among minds. Furthermore,\ninvestigating social influence also implies the necessity to account for the\nway in which people change their minds, as a consequence of interacting with\nother people, and the need to explain the higher or lower persistence of such\nchanges.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2011 14:52:09 GMT"}], "update_date": "2011-06-22", "authors_parsed": [["Giardini", "Francesca", ""], ["Quattrociocchi", "Walter", ""], ["Conte", "Rosaria", ""]]}, {"id": "1106.4221", "submitter": "Walter Quattrociocchi", "authors": "Francesca Giardini, Walter Quattrociocchi, Rosaria Conte", "title": "Understanding opinions. A cognitive and formal account", "comments": null, "journal-ref": "Cultural and opinion dynamics: Modeling, Experiments and\n  Challenges for the future @ ECCS 2011", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  The study of opinions, their formation and change, is one of the defining\ntopics addressed by social psychology, but in recent years other disciplines,\nas computer science and complexity, have addressed this challenge. Despite the\nflourishing of different models and theories in both fields, several key\nquestions still remain unanswered. The aim of this paper is to challenge the\ncurrent theories on opinion by putting forward a cognitively grounded model\nwhere opinions are described as specific mental representations whose main\nproperties are put forward. A comparison with reputation will be also\npresented.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2011 15:00:33 GMT"}], "update_date": "2011-06-22", "authors_parsed": [["Giardini", "Francesca", ""], ["Quattrociocchi", "Walter", ""], ["Conte", "Rosaria", ""]]}, {"id": "1106.4333", "submitter": "Alfredo A. Kalaitzis Mr", "authors": "Alfredo A. Kalaitzis and Neil D. Lawrence", "title": "Residual Component Analysis", "comments": "9 pages, 8 figures, submitted to NIPS2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI math.ST stat.CO stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic principal component analysis (PPCA) seeks a low dimensional\nrepresentation of a data set in the presence of independent spherical Gaussian\nnoise, Sigma = (sigma^2)*I. The maximum likelihood solution for the model is an\neigenvalue problem on the sample covariance matrix. In this paper we consider\nthe situation where the data variance is already partially explained by other\nfactors, e.g. covariates of interest, or temporal correlations leaving some\nresidual variance. We decompose the residual variance into its components\nthrough a generalized eigenvalue problem, which we call residual component\nanalysis (RCA). We show that canonical covariates analysis (CCA) is a special\ncase of our algorithm and explore a range of new algorithms that arise from the\nframework. We illustrate the ideas on a gene expression time series data set\nand the recovery of human pose from silhouette.\n", "versions": [{"version": "v1", "created": "Tue, 21 Jun 2011 21:17:51 GMT"}], "update_date": "2011-06-23", "authors_parsed": [["Kalaitzis", "Alfredo A.", ""], ["Lawrence", "Neil D.", ""]]}, {"id": "1106.4509", "submitter": "Amos Storkey", "authors": "Amos Storkey", "title": "Machine Learning Markets", "comments": "Proceedings of the Fourteenth International Conference on Artificial\n  Intelligence and Statistics 2011", "journal-ref": "Journal of Machine Learning Research W&CP 15(AISTATS):716-724,\n  2011", "doi": null, "report-no": null, "categories": "cs.AI cs.MA cs.NE q-fin.TR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction markets show considerable promise for developing flexible\nmechanisms for machine learning. Here, machine learning markets for\nmultivariate systems are defined, and a utility-based framework is established\nfor their analysis. This differs from the usual approach of defining static\nbetting functions. It is shown that such markets can implement model\ncombination methods used in machine learning, such as product of expert and\nmixture of expert approaches as equilibrium pricing models, by varying agent\nutility functions. They can also implement models composed of local potentials,\nand message passing methods. Prediction markets also allow for more flexible\ncombinations, by combining multiple different utility functions. Conversely,\nthe market mechanisms implement inference in the relevant probabilistic models.\nThis means that market mechanism can be utilized for implementing parallelized\nmodel building and inference for probabilistic modelling.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2011 17:12:42 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Storkey", "Amos", ""]]}, {"id": "1106.4557", "submitter": "F. Provost", "authors": "F. Provost, G. M. Weiss", "title": "Learning When Training Data are Costly: The Effect of Class Distribution\n  on Tree Induction", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 19, pages\n  315-354, 2003", "doi": "10.1613/jair.1199", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For large, real-world inductive learning problems, the number of training\nexamples often must be limited due to the costs associated with procuring,\npreparing, and storing the training examples and/or the computational costs\nassociated with learning from them. In such circumstances, one question of\npractical importance is: if only n training examples can be selected, in what\nproportion should the classes be represented? In this article we help to answer\nthis question by analyzing, for a fixed training-set size, the relationship\nbetween the class distribution of the training data and the performance of\nclassification trees induced from these data. We study twenty-six data sets\nand, for each, determine the best class distribution for learning. The\nnaturally occurring class distribution is shown to generally perform well when\nclassifier performance is evaluated using undifferentiated error rate (0/1\nloss). However, when the area under the ROC curve is used to evaluate\nclassifier performance, a balanced distribution is shown to perform well. Since\nneither of these choices for class distribution always generates the\nbest-performing classifier, we introduce a budget-sensitive progressive\nsampling algorithm for selecting training examples based on the class\nassociated with each example. An empirical analysis of this algorithm shows\nthat the class distribution of the resulting training set yields classifiers\nwith good (nearly-optimal) classification performance.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2011 20:11:46 GMT"}], "update_date": "2011-06-24", "authors_parsed": [["Provost", "F.", ""], ["Weiss", "G. M.", ""]]}, {"id": "1106.4561", "submitter": "M. Fox", "authors": "M. Fox, D. Long", "title": "PDDL2.1: An Extension to PDDL for Expressing Temporal Planning Domains", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 20, pages\n  61-124, 2003", "doi": "10.1613/jair.1129", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years research in the planning community has moved increasingly\ntoward s application of planners to realistic problems involving both time and\nmany typ es of resources. For example, interest in planning demonstrated by the\nspace res earch community has inspired work in observation scheduling,\nplanetary rover ex ploration and spacecraft control domains. Other temporal and\nresource-intensive domains including logistics planning, plant control and\nmanufacturing have also helped to focus the community on the modelling and\nreasoning issues that must be confronted to make planning technology meet the\nchallenges of application. The International Planning Competitions have acted\nas an important motivating fo rce behind the progress that has been made in\nplanning since 1998. The third com petition (held in 2002) set the planning\ncommunity the challenge of handling tim e and numeric resources. This\nnecessitated the development of a modelling langua ge capable of expressing\ntemporal and numeric properties of planning domains. In this paper we describe\nthe language, PDDL2.1, that was used in the competition. We describe the syntax\nof the language, its formal semantics and the validation of concurrent plans.\nWe observe that PDDL2.1 has considerable modelling power --- exceeding the\ncapabilities of current planning technology --- and presents a number of\nimportant challenges to the research community.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2011 20:20:10 GMT"}], "update_date": "2011-06-24", "authors_parsed": [["Fox", "M.", ""], ["Long", "D.", ""]]}, {"id": "1106.4569", "submitter": "D. V. Pynadath", "authors": "D. V. Pynadath, M. Tambe", "title": "The Communicative Multiagent Team Decision Problem: Analyzing Teamwork\n  Theories and Models", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 16, pages\n  389-423, 2002", "doi": "10.1613/jair.1024", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the significant progress in multiagent teamwork, existing research\ndoes not address the optimality of its prescriptions nor the complexity of the\nteamwork problem. Without a characterization of the optimality-complexity\ntradeoffs, it is impossible to determine whether the assumptions and\napproximations made by a particular theory gain enough efficiency to justify\nthe losses in overall performance. To provide a tool for use by multiagent\nresearchers in evaluating this tradeoff, we present a unified framework, the\nCOMmunicative Multiagent Team Decision Problem (COM-MTDP). The COM-MTDP model\ncombines and extends existing multiagent theories, such as decentralized\npartially observable Markov decision processes and economic team theory. In\naddition to their generality of representation, COM-MTDPs also support the\nanalysis of both the optimality of team performance and the computational\ncomplexity of the agents' decision problem. In analyzing complexity, we present\na breakdown of the computational complexity of constructing optimal teams under\nvarious classes of problem domains, along the dimensions of observability and\ncommunication cost. In analyzing optimality, we exploit the COM-MTDP's ability\nto encode existing teamwork theories and models to encode two instantiations of\njoint intentions theory taken from the literature. Furthermore, the COM-MTDP\nmodel provides a basis for the development of novel team coordination\nalgorithms. We derive a domain-independent criterion for optimal communication\nand provide a comparative analysis of the two joint intentions instantiations\nwith respect to this optimal policy. We have implemented a reusable,\ndomain-independent software package based on COM-MTDPs to analyze teamwork\ncoordination strategies, and we demonstrate its use by encoding and evaluating\nthe two joint intentions strategies within an example domain.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2011 20:55:38 GMT"}], "update_date": "2011-06-24", "authors_parsed": [["Pynadath", "D. V.", ""], ["Tambe", "M.", ""]]}, {"id": "1106.4570", "submitter": "M. Tennenholtz", "authors": "M. Tennenholtz", "title": "Competitive Safety Analysis: Robust Decision-Making in Multi-Agent\n  Systems", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 17, pages\n  363-378, 2002", "doi": "10.1613/jair.1065", "report-no": null, "categories": "cs.GT cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much work in AI deals with the selection of proper actions in a given (known\nor unknown) environment. However, the way to select a proper action when facing\nother agents is quite unclear. Most work in AI adopts classical game-theoretic\nequilibrium analysis to predict agent behavior in such settings. This approach\nhowever does not provide us with any guarantee for the agent. In this paper we\nintroduce competitive safety analysis. This approach bridges the gap between\nthe desired normative AI approach, where a strategy should be selected in order\nto guarantee a desired payoff, and equilibrium analysis. We show that a safety\nlevel strategy is able to guarantee the value obtained in a Nash equilibrium,\nin several classical computer science settings. Then, we discuss the concept of\ncompetitive safety strategies, and illustrate its use in a decentralized load\nbalancing setting, typical to network problems. In particular, we show that\nwhen we have many agents, it is possible to guarantee an expected payoff which\nis a factor of 8/9 of the payoff obtained in a Nash equilibrium. Our discussion\nof competitive safety analysis for decentralized load balancing is further\ndeveloped to deal with many communication links and arbitrary speeds. Finally,\nwe discuss the extension of the above concepts to Bayesian games, and\nillustrate their use in a basic auctions setup.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2011 20:56:47 GMT"}], "update_date": "2011-06-24", "authors_parsed": [["Tennenholtz", "M.", ""]]}, {"id": "1106.4571", "submitter": "C. Thompson", "authors": "C. Thompson", "title": "Acquiring Word-Meaning Mappings for Natural Language Interfaces", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 18, pages\n  1-44, 2003", "doi": "10.1613/jair.1063", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on a system, WOLFIE (WOrd Learning From Interpreted\nExamples), that acquires a semantic lexicon from a corpus of sentences paired\nwith semantic representations. The lexicon learned consists of phrases paired\nwith meaning representations. WOLFIE is part of an integrated system that\nlearns to transform sentences into representations such as logical database\nqueries. Experimental results are presented demonstrating WOLFIE's ability to\nlearn useful lexicons for a database interface in four different natural\nlanguages. The usefulness of the lexicons learned by WOLFIE are compared to\nthose acquired by a similar system, with results favorable to WOLFIE. A second\nset of experiments demonstrates WOLFIE's ability to scale to larger and more\ndifficult, albeit artificially generated, corpora. In natural language\nacquisition, it is difficult to gather the annotated data needed for supervised\nlearning; however, unannotated data is fairly plentiful. Active learning\nmethods attempt to select for annotation and training only the most informative\nexamples, and therefore are potentially very useful in natural language\napplications. However, most results to date for active learning have only\nconsidered standard classification tasks. To reduce annotation effort while\nmaintaining accuracy, we apply active learning to semantic lexicons. We show\nthat active learning can significantly reduce the number of annotated examples\nrequired to achieve a given level of performance.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2011 20:57:18 GMT"}], "update_date": "2011-06-24", "authors_parsed": [["Thompson", "C.", ""]]}, {"id": "1106.4572", "submitter": "A. Fern", "authors": "A. Fern, R. Givan, J. M. Siskind", "title": "Specific-to-General Learning for Temporal Events with Application to\n  Learning Event Definitions from Video", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 17, pages\n  379-449, 2002", "doi": "10.1613/jair.1050", "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop, analyze, and evaluate a novel, supervised, specific-to-general\nlearner for a simple temporal logic and use the resulting algorithm to learn\nvisual event definitions from video sequences. First, we introduce a simple,\npropositional, temporal, event-description language called AMA that is\nsufficiently expressive to represent many events yet sufficiently restrictive\nto support learning. We then give algorithms, along with lower and upper\ncomplexity bounds, for the subsumption and generalization problems for AMA\nformulas. We present a positive-examples--only specific-to-general learning\nmethod based on these algorithms. We also present a polynomial-time--computable\n``syntactic'' subsumption test that implies semantic subsumption without being\nequivalent to it. A generalization algorithm based on syntactic subsumption can\nbe used in place of semantic generalization to improve the asymptotic\ncomplexity of the resulting learning algorithm. Finally, we apply this\nalgorithm to the task of learning relational event definitions from video and\nshow that it yields definitions that are competitive with hand-coded ones.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2011 20:58:18 GMT"}], "update_date": "2011-06-24", "authors_parsed": [["Fern", "A.", ""], ["Givan", "R.", ""], ["Siskind", "J. M.", ""]]}, {"id": "1106.4573", "submitter": "D. V. Pynadath", "authors": "D. V. Pynadath, P. Scerri, M. Tambe", "title": "Towards Adjustable Autonomy for the Real World", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 17, pages\n  171-228, 2002", "doi": "10.1613/jair.1037", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adjustable autonomy refers to entities dynamically varying their own\nautonomy, transferring decision-making control to other entities (typically\nagents transferring control to human users) in key situations. Determining\nwhether and when such transfers-of-control should occur is arguably the\nfundamental research problem in adjustable autonomy. Previous work has\ninvestigated various approaches to addressing this problem but has often\nfocused on individual agent-human interactions. Unfortunately, domains\nrequiring collaboration between teams of agents and humans reveal two key\nshortcomings of these previous approaches. First, these approaches use rigid\none-shot transfers of control that can result in unacceptable coordination\nfailures in multiagent settings. Second, they ignore costs (e.g., in terms of\ntime delays or effects on actions) to an agent's team due to such\ntransfers-of-control. To remedy these problems, this article presents a novel\napproach to adjustable autonomy, based on the notion of a transfer-of-control\nstrategy. A transfer-of-control strategy consists of a conditional sequence of\ntwo types of actions: (i) actions to transfer decision-making control (e.g.,\nfrom an agent to a user or vice versa) and (ii) actions to change an agent's\npre-specified coordination constraints with team members, aimed at minimizing\nmiscoordination costs. The goal is for high-quality individual decisions to be\nmade with minimal disruption to the coordination of the team. We present a\nmathematical model of transfer-of-control strategies. The model guides and\ninforms the operationalization of the strategies using Markov Decision\nProcesses, which select an optimal strategy, given an uncertain environment and\ncosts to the individuals and teams. The approach has been carefully evaluated,\nincluding via its use in a real-world, deployed multi-agent system that assists\na research group in its daily activities.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2011 20:58:48 GMT"}], "update_date": "2011-06-24", "authors_parsed": [["Pynadath", "D. V.", ""], ["Scerri", "P.", ""], ["Tambe", "M.", ""]]}, {"id": "1106.4575", "submitter": "J. Culberson", "authors": "J. Culberson, Y. Gao", "title": "An Analysis of Phase Transition in NK Landscapes", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 17, pages\n  309-332, 2002", "doi": "10.1613/jair.1081", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we analyze the decision version of the NK landscape model from\nthe perspective of threshold phenomena and phase transitions under two random\ndistributions, the uniform probability model and the fixed ratio model. For the\nuniform probability model, we prove that the phase transition is easy in the\nsense that there is a polynomial algorithm that can solve a random instance of\nthe problem with the probability asymptotic to 1 as the problem size tends to\ninfinity. For the fixed ratio model, we establish several upper bounds for the\nsolubility threshold, and prove that random instances with parameters above\nthese upper bounds can be solved polynomially. This, together with our\nempirical study for random instances generated below and in the phase\ntransition region, suggests that the phase transition of the fixed ratio model\nis also easy.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2011 20:59:27 GMT"}], "update_date": "2011-06-24", "authors_parsed": [["Culberson", "J.", ""], ["Gao", "Y.", ""]]}, {"id": "1106.4576", "submitter": "D. Gamberger", "authors": "D. Gamberger, N. Lavrac", "title": "Expert-Guided Subgroup Discovery: Methodology and Application", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 17, pages\n  501-527, 2002", "doi": "10.1613/jair.1089", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an approach to expert-guided subgroup discovery. The main\nstep of the subgroup discovery process, the induction of subgroup descriptions,\nis performed by a heuristic beam search algorithm, using a novel parametrized\ndefinition of rule quality which is analyzed in detail. The other important\nsteps of the proposed subgroup discovery process are the detection of\nstatistically significant properties of selected subgroups and subgroup\nvisualization: statistically significant properties are used to enrich the\ndescriptions of induced subgroups, while the visualization shows subgroup\nproperties in the form of distributions of the numbers of examples in the\nsubgroups. The approach is illustrated by the results obtained for a medical\nproblem of early detection of patient risk groups.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2011 20:59:50 GMT"}], "update_date": "2011-06-24", "authors_parsed": [["Gamberger", "D.", ""], ["Lavrac", "N.", ""]]}, {"id": "1106.4577", "submitter": "P. Berry", "authors": "P. Berry, T. J. Lee, D. E. Wilkins", "title": "Interactive Execution Monitoring of Agent Teams", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 18, pages\n  217-261, 2003", "doi": "10.1613/jair.1112", "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an increasing need for automated support for humans monitoring the\nactivity of distributed teams of cooperating agents, both human and machine. We\ncharacterize the domain-independent challenges posed by this problem, and\ndescribe how properties of domains influence the challenges and their\nsolutions. We will concentrate on dynamic, data-rich domains where humans are\nultimately responsible for team behavior. Thus, the automated aid should\ninteractively support effective and timely decision making by the human. We\npresent a domain-independent categorization of the types of alerts a plan-based\nmonitoring system might issue to a user, where each type generally requires\ndifferent monitoring techniques. We describe a monitoring framework for\nintegrating many domain-specific and task-specific monitoring techniques and\nthen using the concept of value of an alert to avoid operator overload. We use\nthis framework to describe an execution monitoring approach we have used to\nimplement Execution Assistants (EAs) in two different dynamic, data-rich,\nreal-world domains to assist a human in monitoring team behavior. One domain\n(Army small unit operations) has hundreds of mobile, geographically distributed\nagents, a combination of humans, robots, and vehicles. The other domain (teams\nof unmanned ground and air vehicles) has a handful of cooperating robots. Both\ndomains involve unpredictable adversaries in the vicinity. Our approach\ncustomizes monitoring behavior for each specific task, plan, and situation, as\nwell as for user preferences. Our EAs alert the human controller when reported\nevents threaten plan execution or physically threaten team members. Alerts were\ngenerated in a timely manner without inundating the user with too many alerts\n(less than 10 percent of alerts are unwanted, as judged by domain experts).\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2011 21:00:28 GMT"}], "update_date": "2011-06-24", "authors_parsed": [["Berry", "P.", ""], ["Lee", "T. J.", ""], ["Wilkins", "D. E.", ""]]}, {"id": "1106.4578", "submitter": "J. Lang", "authors": "J. Lang, P. Liberatore, P. Marquis", "title": "Propositional Independence - Formula-Variable Independence and\n  Forgetting", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 18, pages\n  391-443, 2003", "doi": "10.1613/jair.1113", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Independence -- the study of what is relevant to a given problem of reasoning\n-- has received an increasing attention from the AI community. In this paper,\nwe consider two basic forms of independence, namely, a syntactic one and a\nsemantic one. We show features and drawbacks of them. In particular, while the\nsyntactic form of independence is computationally easy to check, there are\ncases in which things that intuitively are not relevant are not recognized as\nsuch. We also consider the problem of forgetting, i.e., distilling from a\nknowledge base only the part that is relevant to the set of queries constructed\nfrom a subset of the alphabet. While such process is computationally hard, it\nallows for a simplification of subsequent reasoning, and can thus be viewed as\na form of compilation: once the relevant part of a knowledge base has been\nextracted, all reasoning tasks to be performed can be simplified.\n", "versions": [{"version": "v1", "created": "Wed, 22 Jun 2011 21:01:16 GMT"}], "update_date": "2011-06-24", "authors_parsed": [["Lang", "J.", ""], ["Liberatore", "P.", ""], ["Marquis", "P.", ""]]}, {"id": "1106.4632", "submitter": "Heran Yang", "authors": "Heran Yang, Tiffany Low, Matthew Cong, Ashutosh Saxena", "title": "Inferring 3D Articulated Models for Box Packaging Robot", "comments": "For: RSS 2011 Workshop on Mobile Manipulation: Learning to Manipulate", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a point cloud, we consider inferring kinematic models of 3D articulated\nobjects such as boxes for the purpose of manipulating them. While previous work\nhas shown how to extract a planar kinematic model (often represented as a\nlinear chain), such planar models do not apply to 3D objects that are composed\nof segments often linked to the other segments in cyclic configurations. We\npresent an approach for building a model that captures the relation between the\ninput point cloud features and the object segment as well as the relation\nbetween the neighboring object segments. We use a conditional random field that\nallows us to model the dependencies between different segments of the object.\nWe test our approach on inferring the kinematic structure from partial and\nnoisy point cloud data for a wide variety of boxes including cake boxes, pizza\nboxes, and cardboard cartons of several sizes. The inferred structure enables\nour robot to successfully close these boxes by manipulating the flaps.\n", "versions": [{"version": "v1", "created": "Thu, 23 Jun 2011 05:32:39 GMT"}], "update_date": "2011-06-24", "authors_parsed": [["Yang", "Heran", ""], ["Low", "Tiffany", ""], ["Cong", "Matthew", ""], ["Saxena", "Ashutosh", ""]]}, {"id": "1106.4862", "submitter": "A. Ferrandez", "authors": "A. Ferrandez, J. Peral", "title": "Translation of Pronominal Anaphora between English and Spanish:\n  Discrepancies and Evaluation", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 18, pages\n  117-147, 2003", "doi": "10.1613/jair.1115", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper evaluates the different tasks carried out in the translation of\npronominal anaphora in a machine translation (MT) system. The MT interlingua\napproach named AGIR (Anaphora Generation with an Interlingua Representation)\nimproves upon other proposals presented to date because it is able to translate\nintersentential anaphors, detect co-reference chains, and translate Spanish\nzero pronouns into English---issues hardly considered by other systems. The\npaper presents the resolution and evaluation of these anaphora problems in AGIR\nwith the use of different kinds of knowledge (lexical, morphological,\nsyntactic, and semantic). The translation of English and Spanish anaphoric\nthird-person personal pronouns (including Spanish zero pronouns) into the\ntarget language has been evaluated on unrestricted corpora. We have obtained a\nprecision of 80.4% and 84.8% in the translation of Spanish and English\npronouns, respectively. Although we have only studied the Spanish and English\nlanguages, our approach can be easily extended to other languages such as\nPortuguese, Italian, or Japanese.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2011 00:55:35 GMT"}], "update_date": "2011-06-27", "authors_parsed": [["Ferrandez", "A.", ""], ["Peral", "J.", ""]]}, {"id": "1106.4863", "submitter": "A. T. Cemgil", "authors": "A. T. Cemgil, B. Kappen", "title": "Monte Carlo Methods for Tempo Tracking and Rhythm Quantization", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 18, pages\n  45-81, 2003", "doi": "10.1613/jair.1121", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a probabilistic generative model for timing deviations in\nexpressive music performance. The structure of the proposed model is equivalent\nto a switching state space model. The switch variables correspond to discrete\nnote locations as in a musical score. The continuous hidden variables denote\nthe tempo. We formulate two well known music recognition problems, namely tempo\ntracking and automatic transcription (rhythm quantization) as filtering and\nmaximum a posteriori (MAP) state estimation tasks. Exact computation of\nposterior features such as the MAP state is intractable in this model class, so\nwe introduce Monte Carlo methods for integration and optimization. We compare\nMarkov Chain Monte Carlo (MCMC) methods (such as Gibbs sampling, simulated\nannealing and iterative improvement) and sequential Monte Carlo methods\n(particle filters). Our simulation results suggest better results with\nsequential methods. The methods can be applied in both online and batch\nscenarios such as tempo tracking and transcription and are thus potentially\nuseful in a number of music applications such as adaptive automatic\naccompaniment, score typesetting and music information retrieval.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2011 00:56:05 GMT"}], "update_date": "2011-06-27", "authors_parsed": [["Cemgil", "A. T.", ""], ["Kappen", "B.", ""]]}, {"id": "1106.4864", "submitter": "D. Poole", "authors": "D. Poole, N. L. Zhang", "title": "Exploiting Contextual Independence In Probabilistic Inference", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 18, pages\n  263-313, 2003", "doi": "10.1613/jair.1122", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian belief networks have grown to prominence because they provide\ncompact representations for many problems for which probabilistic inference is\nappropriate, and there are algorithms to exploit this compactness. The next\nstep is to allow compact representations of the conditional probabilities of a\nvariable given its parents. In this paper we present such a representation that\nexploits contextual independence in terms of parent contexts; which variables\nact as parents may depend on the value of other variables. The internal\nrepresentation is in terms of contextual factors (confactors) that is simply a\npair of a context and a table. The algorithm, contextual variable elimination,\nis based on the standard variable elimination algorithm that eliminates the\nnon-query variables in turn, but when eliminating a variable, the tables that\nneed to be multiplied can depend on the context. This algorithm reduces to\nstandard variable elimination when there is no contextual independence\nstructure to exploit. We show how this can be much more efficient than variable\nelimination when there is structure to exploit. We explain why this new method\ncan exploit more structure than previous methods for structured belief network\ninference and an analogous algorithm that uses trees.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2011 00:56:26 GMT"}], "update_date": "2011-06-27", "authors_parsed": [["Poole", "D.", ""], ["Zhang", "N. L.", ""]]}, {"id": "1106.4865", "submitter": "B. Kappen", "authors": "B. Kappen, M. Leisink", "title": "Bound Propagation", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 19, pages\n  139-154, 2003", "doi": "10.1613/jair.1130", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we present an algorithm to compute bounds on the marginals of\na graphical model. For several small clusters of nodes upper and lower bounds\non the marginal values are computed independently of the rest of the network.\nThe range of allowed probability distributions over the surrounding nodes is\nrestricted using earlier computed bounds. As we will show, this can be\nconsidered as a set of constraints in a linear programming problem of which the\nobjective function is the marginal probability of the center nodes. In this way\nknowledge about the maginals of neighbouring clusters is passed to other\nclusters thereby tightening the bounds on their marginals. We show that sharp\nbounds can be obtained for undirected and directed graphs that are used for\npractical applications, but for which exact computations are infeasible.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2011 00:56:48 GMT"}], "update_date": "2011-06-27", "authors_parsed": [["Kappen", "B.", ""], ["Leisink", "M.", ""]]}, {"id": "1106.4866", "submitter": "P. Liberatore", "authors": "P. Liberatore", "title": "On Polynomial Sized MDP Succinct Policies", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 21, pages\n  551-577, 2004", "doi": "10.1613/jair.1134", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Policies of Markov Decision Processes (MDPs) determine the next action to\nexecute from the current state and, possibly, the history (the past states).\nWhen the number of states is large, succinct representations are often used to\ncompactly represent both the MDPs and the policies in a reduced amount of\nspace. In this paper, some problems related to the size of succinctly\nrepresented policies are analyzed. Namely, it is shown that some MDPs have\npolicies that can only be represented in space super-polynomial in the size of\nthe MDP, unless the polynomial hierarchy collapses. This fact motivates the\nstudy of the problem of deciding whether a given MDP has a policy of a given\nsize and reward. Since some algorithms for MDPs work by finding a succinct\nrepresentation of the value function, the problem of deciding the existence of\na succinct representation of a value function of a given size and reward is\nalso considered.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2011 00:57:19 GMT"}], "update_date": "2011-06-27", "authors_parsed": [["Liberatore", "P.", ""]]}, {"id": "1106.4867", "submitter": "F. Lin", "authors": "F. Lin", "title": "Compiling Causal Theories to Successor State Axioms and STRIPS-Like\n  Systems", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 19, pages\n  279-314, 2003", "doi": "10.1613/jair.1135", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a system for specifying the effects of actions. Unlike those\ncommonly used in AI planning, our system uses an action description language\nthat allows one to specify the effects of actions using domain rules, which are\nstate constraints that can entail new action effects from old ones.\nDeclaratively, an action domain in our language corresponds to a nonmonotonic\ncausal theory in the situation calculus. Procedurally, such an action domain is\ncompiled into a set of logical theories, one for each action in the domain,\nfrom which fully instantiated successor state-like axioms and STRIPS-like\nsystems are then generated. We expect the system to be a useful tool for\nknowledge engineers writing action specifications for classical AI planning\nsystems, GOLOG systems, and other systems where formal specifications of\nactions are needed.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2011 00:57:41 GMT"}], "update_date": "2011-06-27", "authors_parsed": [["Lin", "F.", ""]]}, {"id": "1106.4868", "submitter": "R. G. Simmons", "authors": "R. G. Simmons, H. L.S. Younes", "title": "VHPOP: Versatile Heuristic Partial Order Planner", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 20, pages\n  405-430, 2003", "doi": "10.1613/jair.1136", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  VHPOP is a partial order causal link (POCL) planner loosely based on UCPOP.\nIt draws from the experience gained in the early to mid 1990's on flaw\nselection strategies for POCL planning, and combines this with more recent\ndevelopments in the field of domain independent planning such as distance based\nheuristics and reachability analysis. We present an adaptation of the additive\nheuristic for plan space planning, and modify it to account for possible reuse\nof existing actions in a plan. We also propose a large set of novel flaw\nselection strategies, and show how these can help us solve more problems than\npreviously possible by POCL planners. VHPOP also supports planning with\ndurative actions by incorporating standard techniques for temporal constraint\nreasoning. We demonstrate that the same heuristic techniques used to boost the\nperformance of classical POCL planning can be effective in domains with\ndurative actions as well. The result is a versatile heuristic POCL planner\ncompetitive with established CSP-based and heuristic state space planners.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2011 00:58:05 GMT"}], "update_date": "2011-06-27", "authors_parsed": [["Simmons", "R. G.", ""], ["Younes", "H. L. S.", ""]]}, {"id": "1106.4869", "submitter": "T. C. Au", "authors": "T. C. Au, O. Ilghami, U. Kuter, J. W. Murdock, D. S. Nau, D. Wu, F.\n  Yaman", "title": "SHOP2: An HTN Planning System", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 20, pages\n  379-404, 2003", "doi": "10.1613/jair.1141", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The SHOP2 planning system received one of the awards for distinguished\nperformance in the 2002 International Planning Competition. This paper\ndescribes the features of SHOP2 which enabled it to excel in the competition,\nespecially those aspects of SHOP2 that deal with temporal and metric planning\ndomains.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2011 00:58:42 GMT"}], "update_date": "2011-06-27", "authors_parsed": [["Au", "T. C.", ""], ["Ilghami", "O.", ""], ["Kuter", "U.", ""], ["Murdock", "J. W.", ""], ["Nau", "D. S.", ""], ["Wu", "D.", ""], ["Yaman", "F.", ""]]}, {"id": "1106.4871", "submitter": "J. E. Laird", "authors": "J. E. Laird, R. E. Wray", "title": "An Architectural Approach to Ensuring Consistency in Hierarchical\n  Execution", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 19, pages\n  355-398, 2003", "doi": "10.1613/jair.1142", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical task decomposition is a method used in many agent systems to\norganize agent knowledge. This work shows how the combination of a hierarchy\nand persistent assertions of knowledge can lead to difficulty in maintaining\nlogical consistency in asserted knowledge. We explore the problematic\nconsequences of persistent assumptions in the reasoning process and introduce\nnovel potential solutions. Having implemented one of the possible solutions,\nDynamic Hierarchical Justification, its effectiveness is demonstrated with an\nempirical analysis.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2011 00:59:16 GMT"}], "update_date": "2011-06-27", "authors_parsed": [["Laird", "J. E.", ""], ["Wray", "R. E.", ""]]}, {"id": "1106.4872", "submitter": "C. A. Knoblock", "authors": "C. A. Knoblock, K. Lerman, S. N. Minton", "title": "Wrapper Maintenance: A Machine Learning Approach", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 18, pages\n  149-181, 2003", "doi": "10.1613/jair.1145", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proliferation of online information sources has led to an increased use\nof wrappers for extracting data from Web sources. While most of the previous\nresearch has focused on quick and efficient generation of wrappers, the\ndevelopment of tools for wrapper maintenance has received less attention. This\nis an important research problem because Web sources often change in ways that\nprevent the wrappers from extracting data correctly. We present an efficient\nalgorithm that learns structural information about data from positive examples\nalone. We describe how this information can be used for two wrapper maintenance\napplications: wrapper verification and reinduction. The wrapper verification\nsystem detects when a wrapper is not extracting correct data, usually because\nthe Web source has changed its format. The reinduction algorithm automatically\nrecovers from changes in the Web source by identifying data on Web pages so\nthat a new wrapper may be generated for this source. To validate our approach,\nwe monitored 27 wrappers over a period of a year. The verification algorithm\ncorrectly discovered 35 of the 37 wrapper changes, and made 16 mistakes,\nresulting in precision of 0.73 and recall of 0.95. We validated the reinduction\nalgorithm on ten Web sources. We were able to successfully reinduce the\nwrappers, obtaining precision and recall values of 0.90 and 0.80 on the data\nextraction task.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2011 00:59:47 GMT"}], "update_date": "2011-06-27", "authors_parsed": [["Knoblock", "C. A.", ""], ["Lerman", "K.", ""], ["Minton", "S. N.", ""]]}, {"id": "1106.4925", "submitter": "Alexander Goltsev", "authors": "S. Yoon, A. V. Goltsev, S. N. Dorogovtsev, and J. F. F. Mendes", "title": "Belief-propagation algorithm and the Ising model on networks with\n  arbitrary distributions of motifs", "comments": "9 pages, 4 figures", "journal-ref": "Physical Review E 84, 041144 (2011)", "doi": "10.1103/PhysRevE.84.041144", "report-no": null, "categories": "cond-mat.dis-nn cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We generalize the belief-propagation algorithm to sparse random networks with\narbitrary distributions of motifs (triangles, loops, etc.). Each vertex in\nthese networks belongs to a given set of motifs (generalization of the\nconfiguration model). These networks can be treated as sparse uncorrelated\nhypergraphs in which hyperedges represent motifs. Here a hypergraph is a\ngeneralization of a graph, where a hyperedge can connect any number of\nvertices. These uncorrelated hypergraphs are tree-like (hypertrees), which\ncrucially simplify the problem and allow us to apply the belief-propagation\nalgorithm to these loopy networks with arbitrary motifs. As natural examples,\nwe consider motifs in the form of finite loops and cliques. We apply the\nbelief-propagation algorithm to the ferromagnetic Ising model on the resulting\nrandom networks. We obtain an exact solution of this model on networks with\nfinite loops or cliques as motifs. We find an exact critical temperature of the\nferromagnetic phase transition and demonstrate that with increasing the\nclustering coefficient and the loop size, the critical temperature increases\ncompared to ordinary tree-like complex networks. Our solution also gives the\nbirth point of the giant connected component in these loopy networks.\n", "versions": [{"version": "v1", "created": "Fri, 24 Jun 2011 09:37:11 GMT"}, {"version": "v2", "created": "Mon, 6 Aug 2012 20:21:01 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["Yoon", "S.", ""], ["Goltsev", "A. V.", ""], ["Dorogovtsev", "S. N.", ""], ["Mendes", "J. F. F.", ""]]}, {"id": "1106.5111", "submitter": "Walter Quattrociocchi", "authors": "Walter Quattrociocchi and Rosaria Conte", "title": "Exploiting Reputation in Distributed Virtual Environments", "comments": null, "journal-ref": "Essa 2011 - The 7th European Social Simulation Association\n  Conference", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cognitive research on reputation has shown several interesting properties\nthat can improve both the quality of services and the security in distributed\nelectronic environments. In this paper, the impact of reputation on\ndecision-making under scarcity of information will be shown. First, a cognitive\ntheory of reputation will be presented, then a selection of simulation\nexperimental results from different studies will be discussed. Such results\nconcern the benefits of reputation when agents need to find out good sellers in\na virtual market-place under uncertainty and informational cheating.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jun 2011 08:40:48 GMT"}], "update_date": "2011-06-28", "authors_parsed": [["Quattrociocchi", "Walter", ""], ["Conte", "Rosaria", ""]]}, {"id": "1106.5112", "submitter": "Miron Kursa", "authors": "Miron B. Kursa and Witold R. Rudnicki", "title": "The All Relevant Feature Selection using Random Forest", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we examine the application of the random forest classifier for\nthe all relevant feature selection problem. To this end we first examine two\nrecently proposed all relevant feature selection algorithms, both being a\nrandom forest wrappers, on a series of synthetic data sets with varying size.\nWe show that reasonable accuracy of predictions can be achieved and that\nheuristic algorithms that were designed to handle the all relevant problem,\nhave performance that is close to that of the reference ideal algorithm. Then,\nwe apply one of the algorithms to four families of semi-synthetic data sets to\nassess how the properties of particular data set influence results of feature\nselection. Finally we test the procedure using a well-known gene expression\ndata set. The relevance of nearly all previously established important genes\nwas confirmed, moreover the relevance of several new ones is discovered.\n", "versions": [{"version": "v1", "created": "Sat, 25 Jun 2011 08:47:23 GMT"}], "update_date": "2011-06-28", "authors_parsed": [["Kursa", "Miron B.", ""], ["Rudnicki", "Witold R.", ""]]}, {"id": "1106.5256", "submitter": "R. I. Brafman", "authors": "R. I. Brafman, C. Domshlak", "title": "Structure and Complexity in Planning with Unary Operators", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 18, pages\n  315-349, 2003", "doi": "10.1613/jair.1146", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unary operator domains -- i.e., domains in which operators have a single\neffect -- arise naturally in many control problems. In its most general form,\nthe problem of STRIPS planning in unary operator domains is known to be as hard\nas the general STRIPS planning problem -- both are PSPACE-complete. However,\nunary operator domains induce a natural structure, called the domain's causal\ngraph. This graph relates between the preconditions and effect of each domain\noperator. Causal graphs were exploited by Williams and Nayak in order to\nanalyze plan generation for one of the controllers in NASA's Deep-Space One\nspacecraft. There, they utilized the fact that when this graph is acyclic, a\nserialization ordering over any subgoal can be obtained quickly. In this paper\nwe conduct a comprehensive study of the relationship between the structure of a\ndomain's causal graph and the complexity of planning in this domain. On the\npositive side, we show that a non-trivial polynomial time plan generation\nalgorithm exists for domains whose causal graph induces a polytree with a\nconstant bound on its node indegree. On the negative side, we show that even\nplan existence is hard when the graph is a directed-path singly connected DAG.\nMore generally, we show that the number of paths in the causal graph is closely\nrelated to the complexity of planning in the associated domain. Finally we\nrelate our results to the question of complexity of planning with serializable\nsubgoals.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jun 2011 21:01:50 GMT"}], "update_date": "2011-06-28", "authors_parsed": [["Brafman", "R. I.", ""], ["Domshlak", "C.", ""]]}, {"id": "1106.5257", "submitter": "T. Eiter", "authors": "T. Eiter, W. Faber, N. Leone, G. Pfeifer, A. Polleres", "title": "Answer Set Planning Under Action Costs", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 19, pages\n  25-71, 2003", "doi": "10.1613/jair.1148", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, planning based on answer set programming has been proposed as an\napproach towards realizing declarative planning systems. In this paper, we\npresent the language Kc, which extends the declarative planning language K by\naction costs. Kc provides the notion of admissible and optimal plans, which are\nplans whose overall action costs are within a given limit resp. minimum over\nall plans (i.e., cheapest plans). As we demonstrate, this novel language allows\nfor expressing some nontrivial planning tasks in a declarative way.\nFurthermore, it can be utilized for representing planning problems under other\noptimality criteria, such as computing ``shortest'' plans (with the least\nnumber of steps), and refinement combinations of cheapest and fastest plans. We\nstudy complexity aspects of the language Kc and provide a transformation to\nlogic programs, such that planning problems are solved via answer set\nprogramming. Furthermore, we report experimental results on selected problems.\nOur experience is encouraging that answer set planning may be a valuable\napproach to expressive planning systems in which intricate planning problems\ncan be naturally specified and solved.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jun 2011 21:02:44 GMT"}], "update_date": "2011-06-28", "authors_parsed": [["Eiter", "T.", ""], ["Faber", "W.", ""], ["Leone", "N.", ""], ["Pfeifer", "G.", ""], ["Polleres", "A.", ""]]}, {"id": "1106.5258", "submitter": "R. I. Brafman", "authors": "R. I. Brafman, M. Tennenholtz", "title": "Learning to Coordinate Efficiently: A Model-based Approach", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 19, pages\n  11-23, 2003", "doi": "10.1613/jair.1154", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In common-interest stochastic games all players receive an identical payoff.\nPlayers participating in such games must learn to coordinate with each other in\norder to receive the highest-possible value. A number of reinforcement learning\nalgorithms have been proposed for this problem, and some have been shown to\nconverge to good solutions in the limit. In this paper we show that using very\nsimple model-based algorithms, much better (i.e., polynomial) convergence rates\ncan be attained. Moreover, our model-based algorithms are guaranteed to\nconverge to the optimal value, unlike many of the existing algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jun 2011 21:03:18 GMT"}], "update_date": "2011-06-28", "authors_parsed": [["Brafman", "R. I.", ""], ["Tennenholtz", "M.", ""]]}, {"id": "1106.5260", "submitter": "M. Do", "authors": "M. Do, S. Kambhampati", "title": "SAPA: A Multi-objective Metric Temporal Planner", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 20, pages\n  155-194, 2003", "doi": "10.1613/jair.1156", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SAPA is a domain-independent heuristic forward chaining planner that can\nhandle durative actions, metric resource constraints, and deadline goals. It is\ndesigned to be capable of handling the multi-objective nature of metric\ntemporal planning. Our technical contributions include (i) planning-graph based\nmethods for deriving heuristics that are sensitive to both cost and makespan\n(ii) techniques for adjusting the heuristic estimates to take action\ninteractions and metric resource limitations into account and (iii) a linear\ntime greedy post-processing technique to improve execution flexibility of the\nsolution plans. An implementation of SAPA using many of the techniques\npresented in this paper was one of the best domain independent planners for\ndomains with metric and temporal constraints in the third International\nPlanning Competition, held at AIPS-02. We describe the technical details of\nextracting the heuristics and present an empirical evaluation of the current\nimplementation of SAPA.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jun 2011 21:03:40 GMT"}], "update_date": "2011-06-28", "authors_parsed": [["Do", "M.", ""], ["Kambhampati", "S.", ""]]}, {"id": "1106.5261", "submitter": "P. F. Patel-Schneider", "authors": "P. F. Patel-Schneider, R. Sebastiani", "title": "A New General Method to Generate Random Modal Formulae for Testing\n  Decision Procedures", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 18, pages\n  351-389, 2003", "doi": "10.1613/jair.1166", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent emergence of heavily-optimized modal decision procedures has\nhighlighted the key role of empirical testing in this domain. Unfortunately,\nthe introduction of extensive empirical tests for modal logics is recent, and\nso far none of the proposed test generators is very satisfactory. To cope with\nthis fact, we present a new random generation method that provides benefits\nover previous methods for generating empirical tests. It fixes and much\ngeneralizes one of the best-known methods, the random CNF_[]m test, allowing\nfor generating a much wider variety of problems, covering in principle the\nwhole input space. Our new method produces much more suitable test sets for the\ncurrent generation of modal decision procedures. We analyze the features of the\nnew method by means of an extensive collection of empirical tests.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jun 2011 21:04:07 GMT"}], "update_date": "2011-06-28", "authors_parsed": [["Patel-Schneider", "P. F.", ""], ["Sebastiani", "R.", ""]]}, {"id": "1106.5262", "submitter": "S. Kambhampati", "authors": "S. Kambhampati, R. Sanchez", "title": "AltAltp: Online Parallelization of Plans with Heuristic State Search", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 19, pages\n  631-657, 2003", "doi": "10.1613/jair.1168", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite their near dominance, heuristic state search planners still lag\nbehind disjunctive planners in the generation of parallel plans in classical\nplanning. The reason is that directly searching for parallel solutions in state\nspace planners would require the planners to branch on all possible subsets of\nparallel actions, thus increasing the branching factor exponentially. We\npresent a variant of our heuristic state search planner AltAlt, called AltAltp\nwhich generates parallel plans by using greedy online parallelization of\npartial plans. The greedy approach is significantly informed by the use of\nnovel distance heuristics that AltAltp derives from a graphplan-style planning\ngraph for the problem. While this approach is not guaranteed to provide optimal\nparallel plans, empirical results show that AltAltp is capable of generating\ngood quality parallel plans at a fraction of the cost incurred by the\ndisjunctive planners.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jun 2011 21:04:32 GMT"}], "update_date": "2011-06-28", "authors_parsed": [["Kambhampati", "S.", ""], ["Sanchez", "R.", ""]]}, {"id": "1106.5263", "submitter": "B. Zanuttini", "authors": "B. Zanuttini", "title": "New Polynomial Classes for Logic-Based Abduction", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 19, pages\n  1-10, 2003", "doi": "10.1613/jair.1170", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of propositional logic-based abduction, i.e., the\nproblem of searching for a best explanation for a given propositional\nobservation according to a given propositional knowledge base. We give a\ngeneral algorithm, based on the notion of projection; then we study\nrestrictions over the representations of the knowledge base and of the query,\nand find new polynomial classes of abduction problems.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jun 2011 21:04:51 GMT"}], "update_date": "2011-06-28", "authors_parsed": [["Zanuttini", "B.", ""]]}, {"id": "1106.5265", "submitter": "A. Gerevini", "authors": "A. Gerevini, A. Saetti, I. Serina", "title": "Planning Through Stochastic Local Search and Temporal Action Graphs in\n  LPG", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 20, pages\n  239-290, 2003", "doi": "10.1613/jair.1183", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present some techniques for planning in domains specified with the recent\nstandard language PDDL2.1, supporting 'durative actions' and numerical\nquantities. These techniques are implemented in LPG, a domain-independent\nplanner that took part in the 3rd International Planning Competition (IPC). LPG\nis an incremental, any time system producing multi-criteria quality plans. The\ncore of the system is based on a stochastic local search method and on a\ngraph-based representation called 'Temporal Action Graphs' (TA-graphs). This\npaper focuses on temporal planning, introducing TA-graphs and proposing some\ntechniques to guide the search in LPG using this representation. The\nexperimental results of the 3rd IPC, as well as further results presented in\nthis paper, show that our techniques can be very effective. Often LPG\noutperforms all other fully-automated planners of the 3rd IPC in terms of speed\nto derive a solution, or quality of the solutions that can be produced.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jun 2011 21:05:34 GMT"}], "update_date": "2011-06-28", "authors_parsed": [["Gerevini", "A.", ""], ["Saetti", "A.", ""], ["Serina", "I.", ""]]}, {"id": "1106.5266", "submitter": "J. Kvarnstr\\\"om", "authors": "J. Kvarnstr\\\"om, M. Magnusson", "title": "TALplanner in IPC-2002: Extensions and Control Rules", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 20, pages\n  343-377, 2003", "doi": "10.1613/jair.1189", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  TALplanner is a forward-chaining planner that relies on domain knowledge in\nthe shape of temporal logic formulas in order to prune irrelevant parts of the\nsearch space. TALplanner recently participated in the third International\nPlanning Competition, which had a clear emphasis on increasing the complexity\nof the problem domains being used as benchmark tests and the expressivity\nrequired to represent these domains in a planning system. Like many other\nplanners, TALplanner had support for some but not all aspects of this increase\nin expressivity, and a number of changes to the planner were required. After a\nshort introduction to TALplanner, this article describes some of the changes\nthat were made before and during the competition. We also describe the process\nof introducing suitable domain knowledge for several of the competition\ndomains.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jun 2011 21:06:29 GMT"}], "update_date": "2011-06-28", "authors_parsed": [["Kvarnstr\u00f6m", "J.", ""], ["Magnusson", "M.", ""]]}, {"id": "1106.5268", "submitter": "L. Console", "authors": "L. Console, C. Picardi, D. Theseider Dupr\\`e", "title": "Temporal Decision Trees: Model-based Diagnosis of Dynamic Systems\n  On-Board", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 19, pages\n  469-512, 2003", "doi": "10.1613/jair.1194", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The automatic generation of decision trees based on off-line reasoning on\nmodels of a domain is a reasonable compromise between the advantages of using a\nmodel-based approach in technical domains and the constraints imposed by\nembedded applications. In this paper we extend the approach to deal with\ntemporal information. We introduce a notion of temporal decision tree, which is\ndesigned to make use of relevant information as long as it is acquired, and we\npresent an algorithm for compiling such trees from a model-based reasoning\nsystem.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jun 2011 21:07:43 GMT"}], "update_date": "2011-06-28", "authors_parsed": [["Console", "L.", ""], ["Picardi", "C.", ""], ["Dupr\u00e8", "D. Theseider", ""]]}, {"id": "1106.5269", "submitter": "L. Finkelstein", "authors": "L. Finkelstein, S. Markovitch, E. Rivlin", "title": "Optimal Schedules for Parallelizing Anytime Algorithms: The Case of\n  Shared Resources", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 19, pages\n  73-138, 2003", "doi": "10.1613/jair.1195", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of anytime algorithms can be improved by simultaneously\nsolving several instances of algorithm-problem pairs. These pairs may include\ndifferent instances of a problem (such as starting from a different initial\nstate), different algorithms (if several alternatives exist), or several runs\nof the same algorithm (for non-deterministic algorithms). In this paper we\npresent a methodology for designing an optimal scheduling policy based on the\nstatistical characteristics of the algorithms involved. We formally analyze the\ncase where the processes share resources (a single-processor model), and\nprovide an algorithm for optimal scheduling. We analyze, theoretically and\nempirically, the behavior of our scheduling algorithm for various distribution\ntypes. Finally, we present empirical results of applying our scheduling\nalgorithm to the Latin Square problem.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jun 2011 21:08:20 GMT"}], "update_date": "2011-06-28", "authors_parsed": [["Finkelstein", "L.", ""], ["Markovitch", "S.", ""], ["Rivlin", "E.", ""]]}, {"id": "1106.5270", "submitter": "J. A. Csirik", "authors": "J. A. Csirik, M. L. Littman, D. McAllester, R. E. Schapire, P. Stone", "title": "Decision-Theoretic Bidding Based on Learned Density Models in\n  Simultaneous, Interacting Auctions", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 19, pages\n  209-242, 2003", "doi": "10.1613/jair.1200", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Auctions are becoming an increasingly popular method for transacting\nbusiness, especially over the Internet. This article presents a general\napproach to building autonomous bidding agents to bid in multiple simultaneous\nauctions for interacting goods. A core component of our approach learns a model\nof the empirical price dynamics based on past data and uses the model to\nanalytically calculate, to the greatest extent possible, optimal bids. We\nintroduce a new and general boosting-based algorithm for conditional density\nestimation problems of this kind, i.e., supervised learning problems in which\nthe goal is to estimate the entire conditional distribution of the real-valued\nlabel. This approach is fully implemented as ATTac-2001, a top-scoring agent in\nthe second Trading Agent Competition (TAC-01). We present experiments\ndemonstrating the effectiveness of our boosting-based price predictor relative\nto several reasonable alternatives.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jun 2011 21:08:54 GMT"}], "update_date": "2011-06-28", "authors_parsed": [["Csirik", "J. A.", ""], ["Littman", "M. L.", ""], ["McAllester", "D.", ""], ["Schapire", "R. E.", ""], ["Stone", "P.", ""]]}, {"id": "1106.5271", "submitter": "J. Hoffmann", "authors": "J. Hoffmann", "title": "The Metric-FF Planning System: Translating \"Ignoring Delete Lists\" to\n  Numeric State Variables", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 20, pages\n  291-341, 2003", "doi": "10.1613/jair.1144", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Planning with numeric state variables has been a challenge for many years,\nand was a part of the 3rd International Planning Competition (IPC-3). Currently\none of the most popular and successful algorithmic techniques in STRIPS\nplanning is to guide search by a heuristic function, where the heuristic is\nbased on relaxing the planning task by ignoring the delete lists of the\navailable actions. We present a natural extension of ``ignoring delete lists''\nto numeric state variables, preserving the relevant theoretical properties of\nthe STRIPS relaxation under the condition that the numeric task at hand is\n``monotonic''. We then identify a subset of the numeric IPC-3 competition\nlanguage, ``linear tasks'', where monotonicity can be achieved by\npre-processing. Based on that, we extend the algorithms used in the heuristic\nplanning system FF to linear tasks. The resulting system Metric-FF is,\naccording to the IPC-3 results which we discuss, one of the two currently most\nefficient numeric planners.\n", "versions": [{"version": "v1", "created": "Sun, 26 Jun 2011 21:09:14 GMT"}], "update_date": "2011-06-28", "authors_parsed": [["Hoffmann", "J.", ""]]}, {"id": "1106.5312", "submitter": "Nina Narodytska", "authors": "Nina Narodytska, Toby Walsh, Lirong Xia", "title": "Manipulation of Nanson's and Baldwin's Rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nanson's and Baldwin's voting rules select a winner by successively\neliminating candidates with low Borda scores. We show that these rules have a\nnumber of desirable computational properties. In particular, with unweighted\nvotes, it is NP-hard to manipulate either rule with one manipulator, whilst\nwith weighted votes, it is NP-hard to manipulate either rule with a small\nnumber of candidates and a coalition of manipulators. As only a couple of other\nvoting rules are known to be NP-hard to manipulate with a single manipulator,\nNanson's and Baldwin's rules appear to be particularly resistant to\nmanipulation from a theoretical perspective. We also propose a number of\napproximation methods for manipulating these two rules. Experiments demonstrate\nthat both rules are often difficult to manipulate in practice. These results\nsuggest that elimination style voting rules deserve further study.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2011 06:42:04 GMT"}], "update_date": "2011-06-28", "authors_parsed": [["Narodytska", "Nina", ""], ["Walsh", "Toby", ""], ["Xia", "Lirong", ""]]}, {"id": "1106.5316", "submitter": "Toby Walsh", "authors": "Toby Walsh", "title": "Online Cake Cutting (published version)", "comments": "To appear in the Proceedings of the Second International Conference\n  on Algorithmic Decision Theory (ADT 2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an online form of the cake cutting problem. This models situations\nwhere agents arrive and depart during the process of dividing a resource. We\nshow that well known fair division procedures like cut-and-choose and the\nDubins-Spanier moving knife procedure can be adapted to apply to such online\nproblems. We propose some fairness properties that online cake cutting\nprocedures can possess like online forms of proportionality and envy-freeness.\nWe also consider the impact of collusion between agents. Finally, we study\ntheoretically and empirically the competitive ratio of these online cake\ncutting procedures. Based on its resistance to collusion, and its good\nperformance in practice, our results favour the online version of the\ncut-and-choose procedure over the online version of the moving knife procedure.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2011 07:13:37 GMT"}], "update_date": "2012-04-18", "authors_parsed": [["Walsh", "Toby", ""]]}, {"id": "1106.5341", "submitter": "Daniel Ly", "authors": "Daniel L. Ly and Ashutosh Saxena and Hod Lipson", "title": "Pose Estimation from a Single Depth Image for Arbitrary Kinematic\n  Skeletons", "comments": "2 pages, 2 figures, RGB-D workshop in Robotics: Science and Systems\n  (RSS 2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for estimating pose information from a single depth image\ngiven an arbitrary kinematic structure without prior training. For an arbitrary\nskeleton and depth image, an evolutionary algorithm is used to find the optimal\nkinematic configuration to explain the observed image. Results show that our\napproach can correctly estimate poses of 39 and 78 degree-of-freedom models\nfrom a single depth image, even in cases of significant self-occlusion.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2011 09:47:28 GMT"}], "update_date": "2011-06-28", "authors_parsed": [["Ly", "Daniel L.", ""], ["Saxena", "Ashutosh", ""], ["Lipson", "Hod", ""]]}, {"id": "1106.5427", "submitter": "You Xu", "authors": "You Xu, Yixin Chen, Qiang Lu, Ruoyun Huang", "title": "Theory and Algorithms for Partial Order Based Reduction in Planning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Search is a major technique for planning. It amounts to exploring a state\nspace of planning domains typically modeled as a directed graph. However,\nprohibitively large sizes of the search space make search expensive. Developing\nbetter heuristic functions has been the main technique for improving search\nefficiency. Nevertheless, recent studies have shown that improving heuristics\nalone has certain fundamental limits on improving search efficiency. Recently,\na new direction of research called partial order based reduction (POR) has been\nproposed as an alternative to improving heuristics. POR has shown promise in\nspeeding up searches.\n  POR has been extensively studied in model checking research and is a key\nenabling technique for scalability of model checking systems. Although the POR\ntheory has been extensively studied in model checking, it has never been\ndeveloped systematically for planning before. In addition, the conditions for\nPOR in the model checking theory are abstract and not directly applicable in\nplanning. Previous works on POR algorithms for planning did not establish the\nconnection between these algorithms and existing theory in model checking.\n  In this paper, we develop a theory for POR in planning. The new theory we\ndevelop connects the stubborn set theory in model checking and POR methods in\nplanning. We show that previous POR algorithms in planning can be explained by\nthe new theory. Based on the new theory, we propose a new, stronger POR\nalgorithm. Experimental results on various planning domains show further search\ncost reduction using the new algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2011 16:06:27 GMT"}], "update_date": "2011-06-28", "authors_parsed": [["Xu", "You", ""], ["Chen", "Yixin", ""], ["Lu", "Qiang", ""], ["Huang", "Ruoyun", ""]]}, {"id": "1106.5448", "submitter": "Lirong Xia", "authors": "Vincent Conitzer and Toby Walsh and Lirong Xia", "title": "Dominating Manipulations in Voting with Partial Information", "comments": "7 pages by arxiv pdflatex, 1 figure. The 6-page version has the same\n  content and will be published in Proceedings of the Twenty-Fifth AAAI\n  Conference on Artificial Intelligence (AAAI-11)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC cs.GT cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider manipulation problems when the manipulator only has partial\ninformation about the votes of the nonmanipulators. Such partial information is\ndescribed by an information set, which is the set of profiles of the\nnonmanipulators that are indistinguishable to the manipulator. Given such an\ninformation set, a dominating manipulation is a non-truthful vote that the\nmanipulator can cast which makes the winner at least as preferable (and\nsometimes more preferable) as the winner when the manipulator votes truthfully.\nWhen the manipulator has full information, computing whether or not there\nexists a dominating manipulation is in P for many common voting rules (by known\nresults). We show that when the manipulator has no information, there is no\ndominating manipulation for many common voting rules. When the manipulator's\ninformation is represented by partial orders and only a small portion of the\npreferences are unknown, computing a dominating manipulation is NP-hard for\nmany common voting rules. Our results thus throw light on whether we can\nprevent strategic behavior by limiting information about the votes of other\nvoters.\n", "versions": [{"version": "v1", "created": "Mon, 27 Jun 2011 17:10:55 GMT"}], "update_date": "2011-06-28", "authors_parsed": [["Conitzer", "Vincent", ""], ["Walsh", "Toby", ""], ["Xia", "Lirong", ""]]}, {"id": "1106.5601", "submitter": "Jun-Yi Chai", "authors": "Junyi Chai, James N.K. Liu", "title": "Class-based Rough Approximation with Dominance Principle", "comments": "Submitted to IEEE-GrC2011", "journal-ref": null, "doi": "10.1109/GRC.2011.6122571", "report-no": null, "categories": "cs.CC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dominance-based Rough Set Approach (DRSA), as the extension of Pawlak's Rough\nSet theory, is effective and fundamentally important in Multiple Criteria\nDecision Analysis (MCDA). In previous DRSA models, the definitions of the upper\nand lower approximations are preserving the class unions rather than the\nsingleton class. In this paper, we propose a new Class-based Rough\nApproximation with respect to a series of previous DRSA models, including\nClassical DRSA model, VC-DRSA model and VP-DRSA model. In addition, the new\nclass-based reducts are investigated.\n", "versions": [{"version": "v1", "created": "Tue, 28 Jun 2011 09:12:31 GMT"}, {"version": "v2", "created": "Tue, 5 Jul 2011 02:50:26 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Chai", "Junyi", ""], ["Liu", "James N. K.", ""]]}, {"id": "1106.5829", "submitter": "Geoffrey Hollinger", "authors": "Geoffrey A. Hollinger, Urbashi Mitra, Gaurav S. Sukhatme", "title": "Active Classification: Theory and Application to Underwater Inspection", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss the problem in which an autonomous vehicle must classify an object\nbased on multiple views. We focus on the active classification setting, where\nthe vehicle controls which views to select to best perform the classification.\nThe problem is formulated as an extension to Bayesian active learning, and we\nshow connections to recent theoretical guarantees in this area. We formally\nanalyze the benefit of acting adaptively as new information becomes available.\nThe analysis leads to a probabilistic algorithm for determining the best views\nto observe based on information theoretic costs. We validate our approach in\ntwo ways, both related to underwater inspection: 3D polyhedra recognition in\nsynthetic depth maps and ship hull inspection with imaging sonar. These tasks\nencompass both the planning and recognition aspects of the active\nclassification problem. The results demonstrate that actively planning for\ninformative views can reduce the number of necessary views by up to 80% when\ncompared to passive methods.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2011 01:39:29 GMT"}], "update_date": "2011-06-30", "authors_parsed": [["Hollinger", "Geoffrey A.", ""], ["Mitra", "Urbashi", ""], ["Sukhatme", "Gaurav S.", ""]]}, {"id": "1106.5890", "submitter": "Hiu Chun Woo", "authors": "Yat-Chiu Law and Jimmy Ho-Man Lee and May Hiu-Chun Woo and Toby Walsh", "title": "A Comparison of Lex Bounds for Multiset Variables in Constraint\n  Programming", "comments": "7 pages, Proceedings of the Twenty-Fifth AAAI Conference on\n  Artificial Intelligence (AAAI-11)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Set and multiset variables in constraint programming have typically been\nrepresented using subset bounds. However, this is a weak representation that\nneglects potentially useful information about a set such as its cardinality.\nFor set variables, the length-lex (LL) representation successfully provides\ninformation about the length (cardinality) and position in the lexicographic\nordering. For multiset variables, where elements can be repeated, we consider\nricher representations that take into account additional information. We study\neight different representations in which we maintain bounds according to one of\nthe eight different orderings: length-(co)lex (LL/LC), variety-(co)lex (VL/VC),\nlength-variety-(co)lex (LVL/LVC), and variety-length-(co)lex (VLL/VLC)\norderings. These representations integrate together information about the\ncardinality, variety (number of distinct elements in the multiset), and\nposition in some total ordering. Theoretical and empirical comparisons of\nexpressiveness and compactness of the eight representations suggest that\nlength-variety-(co)lex (LVL/LVC) and variety-length-(co)lex (VLL/VLC) usually\ngive tighter bounds after constraint propagation. We implement the eight\nrepresentations and evaluate them against the subset bounds representation with\ncardinality and variety reasoning. Results demonstrate that they offer\nsignificantly better pruning and runtime.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2011 09:57:43 GMT"}], "update_date": "2011-06-30", "authors_parsed": [["Law", "Yat-Chiu", ""], ["Lee", "Jimmy Ho-Man", ""], ["Woo", "May Hiu-Chun", ""], ["Walsh", "Toby", ""]]}, {"id": "1106.5917", "submitter": "Jitesh Dundas", "authors": "Jitesh Dundas and David Chik", "title": "Implementing Human-like Intuition Mechanism in Artificial Intelligence", "comments": "14 pages with 1 figure + 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human intuition has been simulated by several research projects using\nartificial intelligence techniques. Most of these algorithms or models lack the\nability to handle complications or diversions. Moreover, they also do not\nexplain the factors influencing intuition and the accuracy of the results from\nthis process. In this paper, we present a simple series based model for\nimplementation of human-like intuition using the principles of connectivity and\nunknown entities. By using Poker hand datasets and Car evaluation datasets, we\ncompare the performance of some well-known models with our intuition model. The\naim of the experiment was to predict the maximum accurate answers using\nintuition based models. We found that the presence of unknown entities,\ndiversion from the current problem scenario, and identifying weakness without\nthe normal logic based execution, greatly affects the reliability of the\nanswers. Generally, the intuition based models cannot be a substitute for the\nlogic based mechanisms in handling such problems. The intuition can only act as\na support for an ongoing logic based model that processes all the steps in a\nsequential manner. However, when time and computational cost are very strict\nconstraints, this intuition based model becomes extremely important and useful,\nbecause it can give a reasonably good performance. Factors affecting intuition\nare analyzed and interpreted through our model.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2011 12:03:33 GMT"}], "update_date": "2011-06-30", "authors_parsed": [["Dundas", "Jitesh", ""], ["Chik", "David", ""]]}, {"id": "1106.5995", "submitter": "Nicolaie Popescu-Bodorin", "authors": "Nicolaie Popescu-Bodorin, Valentina E. Balas", "title": "From Cognitive Binary Logic to Cognitive Intelligent Agents", "comments": "3 figures, 4 pages, latest version: http://fmi.spiruharet.ro/bodorin/", "journal-ref": "Proc. 14th Int. Conf. on Intelligent Engineering Systems, pp.\n  337-340, Conference Publishing Services - IEEE Computer Society, ISBN\n  978-1-4244-7651-0, May 2010", "doi": "10.1109/INES.2010.5483820", "report-no": null, "categories": "cs.AI cs.LO math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The relation between self awareness and intelligence is an open problem these\ndays. Despite the fact that self awarness is usually related to Emotional\nIntelligence, this is not the case here. The problem described in this paper is\nhow to model an agent which knows (Cognitive) Binary Logic and which is also\nable to pass (without any mistake) a certain family of Turing Tests designed to\nverify its knowledge and its discourse about the modal states of truth\ncorresponding to well-formed formulae within the language of Propositional\nBinary Logic.\n", "versions": [{"version": "v1", "created": "Sat, 18 Jun 2011 09:09:07 GMT"}], "update_date": "2011-06-30", "authors_parsed": [["Popescu-Bodorin", "Nicolaie", ""], ["Balas", "Valentina E.", ""]]}, {"id": "1106.5998", "submitter": "M. Fox", "authors": "M. Fox, D. Long", "title": "The 3rd International Planning Competition: Results and Analysis", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 20, pages\n  1-59, 2003", "doi": "10.1613/jair.1240", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reports the outcome of the third in the series of biennial\ninternational planning competitions, held in association with the International\nConference on AI Planning and Scheduling (AIPS) in 2002. In addition to\ndescribing the domains, the planners and the objectives of the competition, the\npaper includes analysis of the results. The results are analysed from several\nperspectives, in order to address the questions of comparative performance\nbetween planners, comparative difficulty of domains, the degree of agreement\nbetween planners about the relative difficulty of individual problem instances\nand the question of how well planners scale relative to one another over\nincreasingly difficult problems. The paper addresses these questions through\nstatistical analysis of the raw results of the competition, in order to\ndetermine which results can be considered to be adequately supported by the\ndata. The paper concludes with a discussion of some challenges for the future\nof the competition series.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2011 16:42:59 GMT"}], "update_date": "2011-06-30", "authors_parsed": [["Fox", "M.", ""], ["Long", "D.", ""]]}, {"id": "1106.6022", "submitter": "W. P. Birmingham", "authors": "W. P. Birmingham, E. H. Durfee, S. Park", "title": "Use of Markov Chains to Design an Agent Bidding Strategy for Continuous\n  Double Auctions", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 22, pages\n  175-214, 2004", "doi": "10.1613/jair.1466", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As computational agents are developed for increasingly complicated e-commerce\napplications, the complexity of the decisions they face demands advances in\nartificial intelligence techniques. For example, an agent representing a seller\nin an auction should try to maximize the seller's profit by reasoning about a\nvariety of possibly uncertain pieces of information, such as the maximum prices\nvarious buyers might be willing to pay, the possible prices being offered by\ncompeting sellers, the rules by which the auction operates, the dynamic arrival\nand matching of offers to buy and sell, and so on. A naive application of\nmultiagent reasoning techniques would require the seller's agent to explicitly\nmodel all of the other agents through an extended time horizon, rendering the\nproblem intractable for many realistically-sized problems. We have instead\ndevised a new strategy that an agent can use to determine its bid price based\non a more tractable Markov chain model of the auction process. We have\nexperimentally identified the conditions under which our new strategy works\nwell, as well as how well it works in comparison to the optimal performance the\nagent could have achieved had it known the future. Our results show that our\nnew strategy in general performs well, outperforming other tractable heuristic\nstrategies in a majority of experiments, and is particularly effective in a\n'seller?s market', where many buy offers are available.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2011 18:38:48 GMT"}], "update_date": "2017-01-08", "authors_parsed": [["Birmingham", "W. P.", ""], ["Durfee", "E. H.", ""], ["Park", "S.", ""]]}, {"id": "1106.6024", "submitter": "Indraneel Mukherjee", "authors": "Indraneel Mukherjee and Cynthia Rudin and Robert E. Schapire", "title": "The Rate of Convergence of AdaBoost", "comments": "A preliminary version will appear in COLT 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The AdaBoost algorithm was designed to combine many \"weak\" hypotheses that\nperform slightly better than random guessing into a \"strong\" hypothesis that\nhas very low error. We study the rate at which AdaBoost iteratively converges\nto the minimum of the \"exponential loss.\" Unlike previous work, our proofs do\nnot require a weak-learning assumption, nor do they require that minimizers of\nthe exponential loss are finite. Our first result shows that at iteration $t$,\nthe exponential loss of AdaBoost's computed parameter vector will be at most\n$\\epsilon$ more than that of any parameter vector of $\\ell_1$-norm bounded by\n$B$ in a number of rounds that is at most a polynomial in $B$ and $1/\\epsilon$.\nWe also provide lower bounds showing that a polynomial dependence on these\nparameters is necessary. Our second result is that within $C/\\epsilon$\niterations, AdaBoost achieves a value of the exponential loss that is at most\n$\\epsilon$ more than the best possible value, where $C$ depends on the dataset.\nWe show that this dependence of the rate on $\\epsilon$ is optimal up to\nconstant factors, i.e., at least $\\Omega(1/\\epsilon)$ rounds are necessary to\nachieve within $\\epsilon$ of the optimal exponential loss.\n", "versions": [{"version": "v1", "created": "Wed, 29 Jun 2011 18:53:46 GMT"}], "update_date": "2011-06-30", "authors_parsed": [["Mukherjee", "Indraneel", ""], ["Rudin", "Cynthia", ""], ["Schapire", "Robert E.", ""]]}, {"id": "1106.6251", "submitter": "Lorenzo Rosasco", "authors": "Mauricio A. Alvarez, Lorenzo Rosasco, Neil D. Lawrence", "title": "Kernels for Vector-Valued Functions: a Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kernel methods are among the most popular techniques in machine learning.\nFrom a frequentist/discriminative perspective they play a central role in\nregularization theory as they provide a natural choice for the hypotheses space\nand the regularization functional through the notion of reproducing kernel\nHilbert spaces. From a Bayesian/generative perspective they are the key in the\ncontext of Gaussian processes, where the kernel function is also known as the\ncovariance function. Traditionally, kernel methods have been used in supervised\nlearning problem with scalar outputs and indeed there has been a considerable\namount of work devoted to designing and learning kernels. More recently there\nhas been an increasing interest in methods that deal with multiple outputs,\nmotivated partly by frameworks like multitask learning. In this paper, we\nreview different methods to design or learn valid kernel functions for multiple\noutputs, paying particular attention to the connection between probabilistic\nand functional methods.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2011 14:48:54 GMT"}, {"version": "v2", "created": "Mon, 16 Apr 2012 17:40:40 GMT"}], "update_date": "2012-04-17", "authors_parsed": [["Alvarez", "Mauricio A.", ""], ["Rosasco", "Lorenzo", ""], ["Lawrence", "Neil D.", ""]]}, {"id": "1106.6341", "submitter": "Oleg Kupervasser", "authors": "Ronen Lerner, Oleg Kupervasser and Ehud Rivlin", "title": "Vision-Based Navigation III: Pose and Motion from Omnidirectional\n  Optical Flow and a Digital Terrain Map", "comments": "6 pages, 9 figures", "journal-ref": "Proceedings of the 2006 IEEE/RSJ International Conference on\n  Intelligent Robots and Systems October 9 - 15, 2006, Beijing, China", "doi": "10.1109/IROS.2006.282569", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An algorithm for pose and motion estimation using corresponding features in\nomnidirectional images and a digital terrain map is proposed. In previous\npaper, such algorithm for regular camera was considered. Using a Digital\nTerrain (or Digital Elevation) Map (DTM/DEM) as a global reference enables\nrecovering the absolute position and orientation of the camera. In order to do\nthis, the DTM is used to formulate a constraint between corresponding features\nin two consecutive frames. In this paper, these constraints are extended to\nhandle non-central projection, as is the case with many omnidirectional\nsystems. The utilization of omnidirectional data is shown to improve the\nrobustness and accuracy of the navigation algorithm. The feasibility of this\nalgorithm is established through lab experimentation with two kinds of\nomnidirectional acquisition systems. The first one is polydioptric cameras\nwhile the second is catadioptric camera.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2011 18:59:53 GMT"}, {"version": "v2", "created": "Tue, 16 Aug 2011 09:22:33 GMT"}], "update_date": "2011-08-17", "authors_parsed": [["Lerner", "Ronen", ""], ["Kupervasser", "Oleg", ""], ["Rivlin", "Ehud", ""]]}]