[{"id": "1111.0039", "submitter": "I. Horrocks", "authors": "I. Horrocks, J. Z. Pan, G. Stamou, G. Stoilos, V. Tzouvaras", "title": "Reasoning with Very Expressive Fuzzy Description Logics", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 30, pages\n  273-320, 2007", "doi": "10.1613/jair.2279", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is widely recognized today that the management of imprecision and\nvagueness will yield more intelligent and realistic knowledge-based\napplications. Description Logics (DLs) are a family of knowledge representation\nlanguages that have gained considerable attention the last decade, mainly due\nto their decidability and the existence of empirically high performance of\nreasoning algorithms. In this paper, we extend the well known fuzzy ALC DL to\nthe fuzzy SHIN DL, which extends the fuzzy ALC DL with transitive role axioms\n(S), inverse roles (I), role hierarchies (H) and number restrictions (N). We\nillustrate why transitive role axioms are difficult to handle in the presence\nof fuzzy interpretations and how to handle them properly. Then we extend these\nresults by adding role hierarchies and finally number restrictions. The main\ncontributions of the paper are the decidability proof of the fuzzy DL languages\nfuzzy-SI and fuzzy-SHIN, as well as decision procedures for the knowledge base\nsatisfiability problem of the fuzzy-SI and fuzzy-SHIN.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2011 21:37:41 GMT"}], "update_date": "2011-11-02", "authors_parsed": [["Horrocks", "I.", ""], ["Pan", "J. Z.", ""], ["Stamou", "G.", ""], ["Stoilos", "G.", ""], ["Tzouvaras", "V.", ""]]}, {"id": "1111.0040", "submitter": "C. M. Li", "authors": "C. M. Li, F. Manya, J. Planes", "title": "New Inference Rules for Max-SAT", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 30, pages\n  321-359, 2007", "doi": "10.1613/jair.2215", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exact Max-SAT solvers, compared with SAT solvers, apply little inference at\neach node of the proof tree. Commonly used SAT inference rules like unit\npropagation produce a simplified formula that preserves satisfiability but,\nunfortunately, solving the Max-SAT problem for the simplified formula is not\nequivalent to solving it for the original formula. In this paper, we define a\nnumber of original inference rules that, besides being applied efficiently,\ntransform Max-SAT instances into equivalent Max-SAT instances which are easier\nto solve. The soundness of the rules, that can be seen as refinements of unit\nresolution adapted to Max-SAT, are proved in a novel and simple way via an\ninteger programming transformation. With the aim of finding out how powerful\nthe inference rules are in practice, we have developed a new Max-SAT solver,\ncalled MaxSatz, which incorporates those rules, and performed an experimental\ninvestigation. The results provide empirical evidence that MaxSatz is very\ncompetitive, at least, on random Max-2SAT, random Max-3SAT, Max-Cut, and Graph\n3-coloring instances, as well as on the benchmarks from the Max-SAT Evaluation\n2006.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2011 21:39:39 GMT"}], "update_date": "2011-11-02", "authors_parsed": [["Li", "C. M.", ""], ["Manya", "F.", ""], ["Planes", "J.", ""]]}, {"id": "1111.0041", "submitter": "R. H. Bordini", "authors": "R. H. Bordini, A. F. Moreira, R. Vieira, M. Wooldridge", "title": "On the Formal Semantics of Speech-Act Based Communication in an\n  Agent-Oriented Programming Language", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 29, pages\n  221-267, 2007", "doi": "10.1613/jair.2221", "report-no": null, "categories": "cs.AI cs.MA cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on agent communication languages has typically taken the speech acts\nparadigm as its starting point. Despite their manifest attractions, speech-act\nmodels of communication have several serious disadvantages as a foundation for\ncommunication in artificial agent systems. In particular, it has proved to be\nextremely difficult to give a satisfactory semantics to speech-act based agent\ncommunication languages. In part, the problem is that speech-act semantics\ntypically make reference to the \"mental states\" of agents (their beliefs,\ndesires, and intentions), and there is in general no way to attribute such\nattitudes to arbitrary computational agents. In addition, agent programming\nlanguages have only had their semantics formalised for abstract, stand-alone\nversions, neglecting aspects such as communication primitives. With respect to\ncommunication, implemented agent programming languages have tended to be rather\nad hoc. This paper addresses both of these problems, by giving semantics to\nspeech-act based messages received by an AgentSpeak agent. AgentSpeak is a\nlogic-based agent programming language which incorporates the main features of\nthe PRS model of reactive planning systems. The paper builds upon a structural\noperational semantics to AgentSpeak that we developed in previous work. The\nmain contributions of this paper are as follows: an extension of our earlier\nwork on the theoretical foundations of AgentSpeak interpreters; a\ncomputationally grounded semantics for (the core) performatives used in\nspeech-act based agent communication languages; and a well-defined extension of\nAgentSpeak that supports agent communication.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2011 21:40:21 GMT"}], "update_date": "2011-11-02", "authors_parsed": [["Bordini", "R. H.", ""], ["Moreira", "A. F.", ""], ["Vieira", "R.", ""], ["Wooldridge", "M.", ""]]}, {"id": "1111.0043", "submitter": "B. Faltings", "authors": "B. Faltings, R. Jurca", "title": "Obtaining Reliable Feedback for Sanctioning Reputation Mechanisms", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 29, pages\n  391-419, 2007", "doi": "10.1613/jair.2243", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reputation mechanisms offer an effective alternative to verification\nauthorities for building trust in electronic markets with moral hazard. Future\nclients guide their business decisions by considering the feedback from past\ntransactions; if truthfully exposed, cheating behavior is sanctioned and thus\nbecomes irrational.\n  It therefore becomes important to ensure that rational clients have the right\nincentives to report honestly. As an alternative to side-payment schemes that\nexplicitly reward truthful reports, we show that honesty can emerge as a\nrational behavior when clients have a repeated presence in the market. To this\nend we describe a mechanism that supports an equilibrium where truthful\nfeedback is obtained. Then we characterize the set of pareto-optimal equilibria\nof the mechanism, and derive an upper bound on the percentage of false reports\nthat can be recorded by the mechanism. An important role in the existence of\nthis bound is played by the fact that rational clients can establish a\nreputation for reporting honestly.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2011 21:43:18 GMT"}], "update_date": "2011-11-02", "authors_parsed": [["Faltings", "B.", ""], ["Jurca", "R.", ""]]}, {"id": "1111.0044", "submitter": "C. Domshlak", "authors": "C. Domshlak, J. Hoffmann", "title": "Probabilistic Planning via Heuristic Forward Search and Weighted Model\n  Counting", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 30, pages\n  565-620, 2007", "doi": "10.1613/jair.2289", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm for probabilistic planning with no observability.\nOur algorithm, called Probabilistic-FF, extends the heuristic forward-search\nmachinery of Conformant-FF to problems with probabilistic uncertainty about\nboth the initial state and action effects. Specifically, Probabilistic-FF\ncombines Conformant-FFs techniques with a powerful machinery for weighted model\ncounting in (weighted) CNFs, serving to elegantly define both the search space\nand the heuristic function. Our evaluation of Probabilistic-FF shows its fine\nscalability in a range of probabilistic domains, constituting a several orders\nof magnitude improvement over previous results in this area. We use a\nproblematic case to point out the main open issue to be addressed by further\nresearch.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2011 21:47:05 GMT"}], "update_date": "2011-11-02", "authors_parsed": [["Domshlak", "C.", ""], ["Hoffmann", "J.", ""]]}, {"id": "1111.0045", "submitter": "I. Bhattacharya", "authors": "I. Bhattacharya, L. Getoor", "title": "Query-time Entity Resolution", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 30, pages\n  621-657, 2007", "doi": "10.1613/jair.2290", "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity resolution is the problem of reconciling database references\ncorresponding to the same real-world entities. Given the abundance of publicly\navailable databases that have unresolved entities, we motivate the problem of\nquery-time entity resolution quick and accurate resolution for answering\nqueries over such unclean databases at query-time. Since collective entity\nresolution approaches --- where related references are resolved jointly ---\nhave been shown to be more accurate than independent attribute-based resolution\nfor off-line entity resolution, we focus on developing new algorithms for\ncollective resolution for answering entity resolution queries at query-time.\nFor this purpose, we first formally show that, for collective resolution,\nprecision and recall for individual entities follow a geometric progression as\nneighbors at increasing distances are considered. Unfolding this progression\nleads naturally to a two stage expand and resolve query processing strategy. In\nthis strategy, we first extract the related records for a query using two novel\nexpansion operators, and then resolve the extracted records collectively. We\nthen show how the same strategy can be adapted for query-time entity resolution\nby identifying and resolving only those database references that are the most\nhelpful for processing the query. We validate our approach on two large\nreal-world publication databases where we show the usefulness of collective\nresolution and at the same time demonstrate the need for adaptive strategies\nfor query processing. We then show how the same queries can be answered in\nreal-time using our adaptive approach while preserving the gains of collective\nresolution. In addition to experiments on real datasets, we use synthetically\ngenerated data to empirically demonstrate the validity of the performance\ntrends predicted by our analysis of collective entity resolution over a wide\nrange of structural characteristics in the data.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2011 21:48:16 GMT"}], "update_date": "2011-11-02", "authors_parsed": [["Bhattacharya", "I.", ""], ["Getoor", "L.", ""]]}, {"id": "1111.0049", "submitter": "B. Glimm", "authors": "Birte Glimm, Ian Horrocks, Carsten Lutz, Ulrike Sattler", "title": "Conjunctive Query Answering for the Description Logic SHIQ", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 31, pages\n  157-204, 2008", "doi": "10.1613/jair.2372", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conjunctive queries play an important role as an expressive query language\nfor Description Logics (DLs). Although modern DLs usually provide for\ntransitive roles, conjunctive query answering over DL knowledge bases is only\npoorly understood if transitive roles are admitted in the query. In this paper,\nwe consider unions of conjunctive queries over knowledge bases formulated in\nthe prominent DL SHIQ and allow transitive roles in both the query and the\nknowledge base. We show decidability of query answering in this setting and\nestablish two tight complexity bounds: regarding combined complexity, we prove\nthat there is a deterministic algorithm for query answering that needs time\nsingle exponential in the size of the KB and double exponential in the size of\nthe query, which is optimal. Regarding data complexity, we prove containment in\nco-NP.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2011 22:01:42 GMT"}], "update_date": "2011-11-02", "authors_parsed": [["Glimm", "Birte", ""], ["Horrocks", "Ian", ""], ["Lutz", "Carsten", ""], ["Sattler", "Ulrike", ""]]}, {"id": "1111.0051", "submitter": "George M. Coghill", "authors": "George M. Coghill, Ross D. King, Ashwin Srinivasan", "title": "Qualitative System Identification from Imperfect Data", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 32, pages\n  825-877, 2008", "doi": "10.1613/jair.2374", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Experience in the physical sciences suggests that the only realistic means of\nunderstanding complex systems is through the use of mathematical models.\nTypically, this has come to mean the identification of quantitative models\nexpressed as differential equations. Quantitative modelling works best when the\nstructure of the model (i.e., the form of the equations) is known; and the\nprimary concern is one of estimating the values of the parameters in the model.\nFor complex biological systems, the model-structure is rarely known and the\nmodeler has to deal with both model-identification and parameter-estimation. In\nthis paper we are concerned with providing automated assistance to the first of\nthese problems. Specifically, we examine the identification by machine of the\nstructural relationships between experimentally observed variables. These\nrelationship will be expressed in the form of qualitative abstractions of a\nquantitative model. Such qualitative models may not only provide clues to the\nprecise quantitative model, but also assist in understanding the essence of\nthat model. Our position in this paper is that background knowledge\nincorporating system modelling principles can be used to constrain effectively\nthe set of good qualitative models. Utilising the model-identification\nframework provided by Inductive Logic Programming (ILP) we present empirical\nsupport for this position using a series of increasingly complex artificial\ndatasets. The results are obtained with qualitative and quantitative data\nsubject to varying amounts of noise and different degrees of sparsity. The\nresults also point to the presence of a set of qualitative states, which we\nterm kernel subsets, that may be necessary for a qualitative model-learner to\nlearn correct models. We demonstrate scalability of the method to biological\nsystem modelling by identification of the glycolysis metabolic pathway from\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2011 22:02:30 GMT"}], "update_date": "2011-11-02", "authors_parsed": [["Coghill", "George M.", ""], ["King", "Ross D.", ""], ["Srinivasan", "Ashwin", ""]]}, {"id": "1111.0053", "submitter": "Malcolm Ross Kinsella Ryan", "authors": "Malcolm Ross Kinsella Ryan", "title": "Exploiting Subgraph Structure in Multi-Robot Path Planning", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 31, pages\n  497-542, 2008", "doi": "10.1613/jair.2408", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-robot path planning is difficult due to the combinatorial explosion of\nthe search space with every new robot added. Complete search of the combined\nstate-space soon becomes intractable. In this paper we present a novel form of\nabstraction that allows us to plan much more efficiently. The key to this\nabstraction is the partitioning of the map into subgraphs of known structure\nwith entry and exit restrictions which we can represent compactly. Planning\nthen becomes a search in the much smaller space of subgraph configurations.\nOnce an abstract plan is found, it can be quickly resolved into a correct (but\npossibly sub-optimal) concrete plan without the need for further search. We\nprove that this technique is sound and complete and demonstrate its practical\neffectiveness on a real map.\n  A contending solution, prioritised planning, is also evaluated and shown to\nhave similar performance albeit at the cost of completeness. The two approaches\nare not necessarily conflicting; we demonstrate how they can be combined into a\nsingle algorithm which outperforms either approach alone.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2011 22:09:48 GMT"}], "update_date": "2011-11-02", "authors_parsed": [["Ryan", "Malcolm Ross Kinsella", ""]]}, {"id": "1111.0054", "submitter": "Yulin Ding", "authors": "Yulin Ding, Y. Ding, Yan Zhang, Y. Zhang", "title": "CTL Model Update for System Modifications", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 31, pages\n  113-155, 2008", "doi": "10.1613/jair.2420", "report-no": null, "categories": "cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model checking is a promising technology, which has been applied for\nverification of many hardware and software systems. In this paper, we introduce\nthe concept of model update towards the development of an automatic system\nmodification tool that extends model checking functions. We define primitive\nupdate operations on the models of Computation Tree Logic (CTL) and formalize\nthe principle of minimal change for CTL model update. These primitive update\noperations, together with the underlying minimal change principle, serve as the\nfoundation for CTL model update. Essential semantic and computational\ncharacterizations are provided for our CTL model update approach. We then\ndescribe a formal algorithm that implements this approach. We also illustrate\ntwo case studies of CTL model updates for the well-known microwave oven example\nand the Andrew File System 1, from which we further propose a method to\noptimize the update results in complex system modifications.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2011 22:10:52 GMT"}], "update_date": "2011-11-02", "authors_parsed": [["Ding", "Yulin", ""], ["Ding", "Y.", ""], ["Zhang", "Yan", ""], ["Zhang", "Y.", ""]]}, {"id": "1111.0055", "submitter": "Anastasia Analyti", "authors": "Anastasia Analyti, Grigoris Antoniou, Carlos Viegas Dam\\'asio, Gerd\n  Wagner", "title": "Extended RDF as a Semantic Foundation of Rule Markup Languages", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 32, pages\n  37-94, 2008", "doi": "10.1613/jair.2425", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ontologies and automated reasoning are the building blocks of the Semantic\nWeb initiative. Derivation rules can be included in an ontology to define\nderived concepts, based on base concepts. For example, rules allow to define\nthe extension of a class or property, based on a complex relation between the\nextensions of the same or other classes and properties. On the other hand, the\ninclusion of negative information both in the form of negation-as-failure and\nexplicit negative information is also needed to enable various forms of\nreasoning. In this paper, we extend RDF graphs with weak and strong negation,\nas well as derivation rules. The ERDF stable model semantics of the extended\nframework (Extended RDF) is defined, extending RDF(S) semantics. A distinctive\nfeature of our theory, which is based on Partial Logic, is that both truth and\nfalsity extensions of properties and classes are considered, allowing for truth\nvalue gaps. Our framework supports both closed-world and open-world reasoning\nthrough the explicit representation of the particular closed-world assumptions\nand the ERDF ontological categories of total properties and total classes.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2011 22:11:46 GMT"}], "update_date": "2011-11-02", "authors_parsed": [["Analyti", "Anastasia", ""], ["Antoniou", "Grigoris", ""], ["Dam\u00e1sio", "Carlos Viegas", ""], ["Wagner", "Gerd", ""]]}, {"id": "1111.0056", "submitter": "Omer Gim\\'enez", "authors": "Omer Gim\\'enez, Anders Jonsson", "title": "The Complexity of Planning Problems With Simple Causal Graphs", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 31, pages\n  319-351, 2008", "doi": "10.1613/jair.2432", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present three new complexity results for classes of planning problems with\nsimple causal graphs. First, we describe a polynomial-time algorithm that uses\nmacros to generate plans for the class 3S of planning problems with binary\nstate variables and acyclic causal graphs. This implies that plan generation\nmay be tractable even when a planning problem has an exponentially long minimal\nsolution. We also prove that the problem of plan existence for planning\nproblems with multi-valued variables and chain causal graphs is NP-hard.\nFinally, we show that plan existence for planning problems with binary state\nvariables and polytree causal graphs is NP-complete.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2011 22:12:22 GMT"}], "update_date": "2011-11-02", "authors_parsed": [["Gim\u00e9nez", "Omer", ""], ["Jonsson", "Anders", ""]]}, {"id": "1111.0059", "submitter": "Subbarao  Kambhampati", "authors": "Menkes Hector Louis van den Briel, Thomas Vossen, Subbarao Kambhampati", "title": "Loosely Coupled Formulations for Automated Planning: An Integer\n  Programming Perspective", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 31, pages\n  217-257, 2008", "doi": "10.1613/jair.2443", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We represent planning as a set of loosely coupled network flow problems,\nwhere each network corresponds to one of the state variables in the planning\ndomain. The network nodes correspond to the state variable values and the\nnetwork arcs correspond to the value transitions. The planning problem is to\nfind a path (a sequence of actions) in each network such that, when merged,\nthey constitute a feasible plan. In this paper we present a number of integer\nprogramming formulations that model these loosely coupled networks with varying\ndegrees of flexibility. Since merging may introduce exponentially many ordering\nconstraints we implement a so-called branch-and-cut algorithm, in which these\nconstraints are dynamically generated and added to the formulation when needed.\nOur results are very promising, they improve upon previous planning as integer\nprogramming approaches and lay the foundation for integer programming\napproaches for cost optimal planning.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2011 22:16:02 GMT"}], "update_date": "2011-11-02", "authors_parsed": [["Briel", "Menkes Hector Louis van den", ""], ["Vossen", "Thomas", ""], ["Kambhampati", "Subbarao", ""]]}, {"id": "1111.0060", "submitter": "J. Christopher Beck", "authors": "Daria Terekhov, J. Christopher Beck", "title": "A Constraint Programming Approach for Solving a Queueing Control Problem", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 32, pages\n  123-167, 2008", "doi": "10.1613/jair.2446", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a facility with front room and back room operations, it is useful to\nswitch workers between the rooms in order to cope with changing customer\ndemand. Assuming stochastic customer arrival and service times, we seek a\npolicy for switching workers such that the expected customer waiting time is\nminimized while the expected back room staffing is sufficient to perform all\nwork. Three novel constraint programming models and several shaving procedures\nfor these models are presented. Experimental results show that a model based on\nclosed-form expressions together with a combination of shaving procedures is\nthe most efficient. This model is able to find and prove optimal solutions for\nmany problem instances within a reasonable run-time. Previously, the only\navailable approach was a heuristic algorithm. Furthermore, a hybrid method\ncombining the heuristic and the best constraint programming method is shown to\nperform as well as the heuristic in terms of solution quality over time, while\nachieving the same performance in terms of proving optimality as the pure\nconstraint programming model. This is the first work of which we are aware that\nsolves such queueing-based problems with constraint programming.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2011 22:16:41 GMT"}], "update_date": "2011-11-02", "authors_parsed": [["Terekhov", "Daria", ""], ["Beck", "J. Christopher", ""]]}, {"id": "1111.0062", "submitter": "Frans A. Oliehoek", "authors": "Frans A. Oliehoek, Matthijs T. J. Spaan, Nikos Vlassis", "title": "Optimal and Approximate Q-value Functions for Decentralized POMDPs", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 32, pages\n  289-353, 2008", "doi": "10.1613/jair.2447", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision-theoretic planning is a popular approach to sequential decision\nmaking problems, because it treats uncertainty in sensing and acting in a\nprincipled way. In single-agent frameworks like MDPs and POMDPs, planning can\nbe carried out by resorting to Q-value functions: an optimal Q-value function\nQ* is computed in a recursive manner by dynamic programming, and then an\noptimal policy is extracted from Q*. In this paper we study whether similar\nQ-value functions can be defined for decentralized POMDP models (Dec-POMDPs),\nand how policies can be extracted from such value functions. We define two\nforms of the optimal Q-value function for Dec-POMDPs: one that gives a\nnormative description as the Q-value function of an optimal pure joint policy\nand another one that is sequentially rational and thus gives a recipe for\ncomputation. This computation, however, is infeasible for all but the smallest\nproblems. Therefore, we analyze various approximate Q-value functions that\nallow for efficient computation. We describe how they relate, and we prove that\nthey all provide an upper bound to the optimal Q-value function Q*. Finally,\nunifying some previous approaches for solving Dec-POMDPs, we describe a family\nof algorithms for extracting policies from such Q-value functions, and perform\nan experimental evaluation on existing test problems, including a new\nfirefighting benchmark problem.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2011 22:17:44 GMT"}], "update_date": "2011-11-02", "authors_parsed": [["Oliehoek", "Frans A.", ""], ["Spaan", "Matthijs T. J.", ""], ["Vlassis", "Nikos", ""]]}, {"id": "1111.0065", "submitter": "Claudia V. Goldman", "authors": "Claudia V. Goldman, Shlomo Zilberstein", "title": "Communication-Based Decomposition Mechanisms for Decentralized MDPs", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 32, pages\n  169-202, 2008", "doi": "10.1613/jair.2466", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-agent planning in stochastic environments can be framed formally as a\ndecentralized Markov decision problem. Many real-life distributed problems that\narise in manufacturing, multi-robot coordination and information gathering\nscenarios can be formalized using this framework. However, finding the optimal\nsolution in the general case is hard, limiting the applicability of recently\ndeveloped algorithms. This paper provides a practical approach for solving\ndecentralized control problems when communication among the decision makers is\npossible, but costly. We develop the notion of communication-based mechanism\nthat allows us to decompose a decentralized MDP into multiple single-agent\nproblems. In this framework, referred to as decentralized semi-Markov decision\nprocess with direct communication (Dec-SMDP-Com), agents operate separately\nbetween communications. We show that finding an optimal mechanism is equivalent\nto solving optimally a Dec-SMDP-Com. We also provide a heuristic search\nalgorithm that converges on the optimal decomposition. Restricting the\ndecomposition to some specific types of local behaviors reduces significantly\nthe complexity of planning. In particular, we present a polynomial-time\nalgorithm for the case in which individual agents perform goal-oriented\nbehaviors between communications. The paper concludes with an additional\ntractable algorithm that enables the introduction of human knowledge, thereby\nreducing the overall problem to finding the best time to communicate. Empirical\nresults show that these approaches provide good approximate solutions.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2011 22:22:03 GMT"}], "update_date": "2011-11-02", "authors_parsed": [["Goldman", "Claudia V.", ""], ["Zilberstein", "Shlomo", ""]]}, {"id": "1111.0067", "submitter": "Joseph Culberson", "authors": "Fan Yang, Joseph Culberson, Robert Holte, Uzi Zahavi, Ariel Felner", "title": "A General Theory of Additive State Space Abstractions", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 32, pages\n  631-662, 2008", "doi": "10.1613/jair.2486", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Informally, a set of abstractions of a state space S is additive if the\ndistance between any two states in S is always greater than or equal to the sum\nof the corresponding distances in the abstract spaces. The first known additive\nabstractions, called disjoint pattern databases, were experimentally\ndemonstrated to produce state of the art performance on certain state spaces.\nHowever, previous applications were restricted to state spaces with special\nproperties, which precludes disjoint pattern databases from being defined for\nseveral commonly used testbeds, such as Rubiks Cube, TopSpin and the Pancake\npuzzle. In this paper we give a general definition of additive abstractions\nthat can be applied to any state space and prove that heuristics based on\nadditive abstractions are consistent as well as admissible. We use this new\ndefinition to create additive abstractions for these testbeds and show\nexperimentally that well chosen additive abstractions can reduce search time\nsubstantially for the (18,4)-TopSpin puzzle and by three orders of magnitude\nover state of the art methods for the 17-Pancake puzzle. We also derive a way\nof testing if the heuristic value returned by additive abstractions is provably\ntoo low and show that the use of this test can reduce search time for the\n15-puzzle and TopSpin by roughly a factor of two.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2011 22:26:44 GMT"}], "update_date": "2011-11-02", "authors_parsed": [["Yang", "Fan", ""], ["Culberson", "Joseph", ""], ["Holte", "Robert", ""], ["Zahavi", "Uzi", ""], ["Felner", "Ariel", ""]]}, {"id": "1111.0068", "submitter": "Saket Joshi", "authors": "Chenggang Wang, Saket Joshi, Roni Khardon", "title": "First Order Decision Diagrams for Relational MDPs", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 31, pages\n  431-472, 2008", "doi": "10.1613/jair.2489", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov decision processes capture sequential decision making under\nuncertainty, where an agent must choose actions so as to optimize long term\nreward. The paper studies efficient reasoning mechanisms for Relational Markov\nDecision Processes (RMDP) where world states have an internal relational\nstructure that can be naturally described in terms of objects and relations\namong them. Two contributions are presented. First, the paper develops First\nOrder Decision Diagrams (FODD), a new compact representation for functions over\nrelational structures, together with a set of operators to combine FODDs, and\nnovel reduction techniques to keep the representation small. Second, the paper\nshows how FODDs can be used to develop solutions for RMDPs, where reasoning is\nperformed at the abstract level and the resulting optimal policy is independent\nof domain size (number of objects) or instantiation. In particular, a variant\nof the value iteration algorithm is developed by using special operations over\nFODDs, and the algorithm is shown to converge to the optimal policy.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2011 22:27:57 GMT"}], "update_date": "2011-11-02", "authors_parsed": [["Wang", "Chenggang", ""], ["Joshi", "Saket", ""], ["Khardon", "Roni", ""]]}, {"id": "1111.0158", "submitter": "Sanaa Elyassami", "authors": "Sanaa Elyassami and Ali Idri", "title": "Applying Fuzzy ID3 Decision Tree for Software Effort Estimation", "comments": null, "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 8,\n  Issue 4, No 1, 131-138 (2011)", "doi": null, "report-no": null, "categories": "cs.SE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web Effort Estimation is a process of predicting the efforts and cost in\nterms of money, schedule and staff for any software project system. Many\nestimation models have been proposed over the last three decades and it is\nbelieved that it is a must for the purpose of: Budgeting, risk analysis,\nproject planning and control, and project improvement investment analysis. In\nthis paper, we investigate the use of Fuzzy ID3 decision tree for software cost\nestimation; it is designed by integrating the principles of ID3 decision tree\nand the fuzzy set-theoretic concepts, enabling the model to handle uncertain\nand imprecise data when describing the software projects, which can improve\ngreatly the accuracy of obtained estimates. MMRE and Pred are used as measures\nof prediction accuracy for this study. A series of experiments is reported\nusing two different software projects datasets namely, Tukutuku and COCOMO'81\ndatasets. The results are compared with those produced by the crisp version of\nthe ID3 decision tree.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2011 09:58:08 GMT"}], "update_date": "2011-11-02", "authors_parsed": [["Elyassami", "Sanaa", ""], ["Idri", "Ali", ""]]}, {"id": "1111.0432", "submitter": "Sangkyun Lee", "authors": "Sangkyun Lee and Stephen J. Wright", "title": "Approximate Stochastic Subgradient Estimation Training for Support\n  Vector Machines", "comments": "An extended version of the ICPRAM 2012 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subgradient algorithms for training support vector machines have been quite\nsuccessful for solving large-scale and online learning problems. However, they\nhave been restricted to linear kernels and strongly convex formulations. This\npaper describes efficient subgradient approaches without such limitations. Our\napproaches make use of randomized low-dimensional approximations to nonlinear\nkernels, and minimization of a reduced primal formulation using an algorithm\nbased on robust stochastic approximation, which do not require strong\nconvexity. Experiments illustrate that our approaches produce solutions of\ncomparable prediction accuracy with the solutions acquired from existing SVM\nsolvers, but often in much shorter time. We also suggest efficient prediction\nschemes that depend only on the dimension of kernel approximation, not on the\nnumber of support vectors.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2011 09:24:26 GMT"}, {"version": "v2", "created": "Thu, 3 Nov 2011 13:33:27 GMT"}], "update_date": "2011-11-04", "authors_parsed": [["Lee", "Sangkyun", ""], ["Wright", "Stephen J.", ""]]}, {"id": "1111.0466", "submitter": "Michael Bronstein", "authors": "Michael M Bronstein", "title": "Kernel diff-hash", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a kernel formulation of the recently introduced diff-hash\nalgorithm for the construction of similarity-sensitive hash functions. Our\nkernel diff-hash algorithm that shows superior performance on the problem of\nimage feature descriptor matching.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2011 11:42:37 GMT"}], "update_date": "2011-11-03", "authors_parsed": [["Bronstein", "Michael M", ""]]}, {"id": "1111.0708", "submitter": "Pedro Alejandro Ortega", "authors": "Pedro A. Ortega", "title": "Bayesian Causal Induction", "comments": "4 pages, 4 figures; 2011 NIPS Workshop on Philosophy and Machine\n  Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering causal relationships is a hard task, often hindered by the need\nfor intervention, and often requiring large amounts of data to resolve\nstatistical uncertainty. However, humans quickly arrive at useful causal\nrelationships. One possible reason is that humans extrapolate from past\nexperience to new, unseen situations: that is, they encode beliefs over causal\ninvariances, allowing for sound generalization from the observations they\nobtain from directly acting in the world.\n  Here we outline a Bayesian model of causal induction where beliefs over\ncompeting causal hypotheses are modeled using probability trees. Based on this\nmodel, we illustrate why, in the general case, we need interventions plus\nconstraints on our causal hypotheses in order to extract causal information\nfrom our experience.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2011 01:32:44 GMT"}, {"version": "v2", "created": "Wed, 30 Nov 2011 01:12:16 GMT"}], "update_date": "2011-12-01", "authors_parsed": [["Ortega", "Pedro A.", ""]]}, {"id": "1111.0712", "submitter": "Pannagadatta Shivaswamy", "authors": "Pannagadatta K. Shivaswamy and Thorsten Joachims", "title": "Online Learning with Preference Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new online learning model for learning with preference feedback.\nThe model is especially suited for applications like web search and recommender\nsystems, where preference data is readily available from implicit user feedback\n(e.g. clicks). In particular, at each time step a potentially structured object\n(e.g. a ranking) is presented to the user in response to a context (e.g.\nquery), providing him or her with some unobserved amount of utility. As\nfeedback the algorithm receives an improved object that would have provided\nhigher utility. We propose a learning algorithm with provable regret bounds for\nthis online learning setting and demonstrate its effectiveness on a web-search\napplication. The new learning model also applies to many other interactive\nlearning problems and admits several interesting extensions.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2011 01:58:45 GMT"}], "update_date": "2011-11-04", "authors_parsed": [["Shivaswamy", "Pannagadatta K.", ""], ["Joachims", "Thorsten", ""]]}, {"id": "1111.0860", "submitter": "E. Giunchiglia", "authors": "E. Giunchiglia, M. Narizzano, A. Tacchella", "title": "Clause/Term Resolution and Learning in the Evaluation of Quantified\n  Boolean Formulas", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 26, pages\n  371-416, 2006", "doi": "10.1613/jair.1959", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Resolution is the rule of inference at the basis of most procedures for\nautomated reasoning. In these procedures, the input formula is first translated\ninto an equisatisfiable formula in conjunctive normal form (CNF) and then\nrepresented as a set of clauses. Deduction starts by inferring new clauses by\nresolution, and goes on until the empty clause is generated or satisfiability\nof the set of clauses is proven, e.g., because no new clauses can be generated.\n  In this paper, we restrict our attention to the problem of evaluating\nQuantified Boolean Formulas (QBFs). In this setting, the above outlined\ndeduction process is known to be sound and complete if given a formula in CNF\nand if a form of resolution, called Q-resolution, is used. We introduce\nQ-resolution on terms, to be used for formulas in disjunctive normal form. We\nshow that the computation performed by most of the available procedures for\nQBFs --based on the Davis-Logemann-Loveland procedure (DLL) for propositional\nsatisfiability-- corresponds to a tree in which Q-resolution on terms and\nclauses alternate. This poses the theoretical bases for the introduction of\nlearning, corresponding to recording Q-resolution formulas associated with the\nnodes of the tree. We discuss the problems related to the introduction of\nlearning in DLL based procedures, and present solutions extending\nstate-of-the-art proposals coming from the literature on propositional\nsatisfiability. Finally, we show that our DLL based solver extended with\nlearning, performs significantly better on benchmarks used in the 2003 QBF\nsolvers comparative evaluation.\n", "versions": [{"version": "v1", "created": "Mon, 26 Sep 2011 18:43:49 GMT"}], "update_date": "2011-11-04", "authors_parsed": [["Giunchiglia", "E.", ""], ["Narizzano", "M.", ""], ["Tacchella", "A.", ""]]}, {"id": "1111.1321", "submitter": "Oleg Varlamov Oleg", "authors": "Oleg O. Varlamov", "title": "MIVAR: Transition from Productions to Bipartite Graphs MIVAR Nets and\n  Practical Realization of Automated Constructor of Algorithms Handling More\n  than Three Million Production Rules", "comments": "23 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The theoretical transition from the graphs of production systems to the\nbipartite graphs of the MIVAR nets is shown. Examples of the implementation of\nthe MIVAR nets in the formalisms of matrixes and graphs are given. The linear\ncomputational complexity of algorithms for automated building of objects and\nrules of the MIVAR nets is theoretically proved. On the basis of the MIVAR nets\nthe UDAV software complex is developed, handling more than 1.17 million objects\nand more than 3.5 million rules on ordinary computers. The results of\nexperiments that confirm a linear computational complexity of the MIVAR method\nof information processing are given.\n  Keywords: MIVAR, MIVAR net, logical inference, computational complexity,\nartificial intelligence, intelligent systems, expert systems, General Problem\nSolver.\n", "versions": [{"version": "v1", "created": "Sat, 5 Nov 2011 15:26:15 GMT"}], "update_date": "2011-11-08", "authors_parsed": [["Varlamov", "Oleg O.", ""]]}, {"id": "1111.1486", "submitter": "Yisong Wang", "authors": "Yisong Wang and Jia-Huai You and Li Yan Yuan and Yi-Dong Shen and\n  Thomas Eiter", "title": "Embedding Description Logic Programs into Default Logic", "comments": "53 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Description logic programs (dl-programs) under the answer set semantics\nformulated by Eiter {\\em et al.} have been considered as a prominent formalism\nfor integrating rules and ontology knowledge bases. A question of interest has\nbeen whether dl-programs can be captured in a general formalism of nonmonotonic\nlogic. In this paper, we study the possibility of embedding dl-programs into\ndefault logic. We show that dl-programs under the strong and weak answer set\nsemantics can be embedded in default logic by combining two translations, one\nof which eliminates the constraint operator from nonmonotonic dl-atoms and the\nother translates a dl-program into a default theory. For dl-programs without\nnonmonotonic dl-atoms but with the negation-as-failure operator, our embedding\nis polynomial, faithful, and modular. In addition, our default logic encoding\ncan be extended in a simple way to capture recently proposed weakly\nwell-supported answer set semantics, for arbitrary dl-programs. These results\nreinforce the argument that default logic can serve as a fruitful foundation\nfor query-based approaches to integrating ontology and rules. With its simple\nsyntax and intuitive semantics, plus available computational results, default\nlogic can be considered an attractive approach to integration of ontology and\nrules.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2011 04:39:56 GMT"}], "update_date": "2011-11-08", "authors_parsed": [["Wang", "Yisong", ""], ["You", "Jia-Huai", ""], ["Yuan", "Li Yan", ""], ["Shen", "Yi-Dong", ""], ["Eiter", "Thomas", ""]]}, {"id": "1111.1784", "submitter": "Ravi Ganti", "authors": "Ravi Ganti and Alexander Gray", "title": "UPAL: Unbiased Pool Based Active Learning", "comments": "20 pages, 4 figures, 2 tables, a few minor typos were corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of pool based active learning, and\nprovide an algorithm, called UPAL, that works by minimizing the unbiased\nestimator of the risk of a hypothesis in a given hypothesis space. For the\nspace of linear classifiers and the squared loss we show that UPAL is\nequivalent to an exponentially weighted average forecaster. Exploiting some\nrecent results regarding the spectra of random matrices allows us to establish\nconsistency of UPAL when the true hypothesis is a linear hypothesis. Empirical\ncomparison with an active learner implementation in Vowpal Wabbit, and a\npreviously proposed pool based active learner implementation show good\nempirical performance and better scalability.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2011 02:41:48 GMT"}, {"version": "v2", "created": "Sun, 13 Nov 2011 17:28:34 GMT"}], "update_date": "2011-11-15", "authors_parsed": [["Ganti", "Ravi", ""], ["Gray", "Alexander", ""]]}, {"id": "1111.1941", "submitter": "Jean Vincent Fonou Dombeu", "authors": "Jean Vincent Fonou-Dombeu and Magda Huisman", "title": "Semantic-Driven e-Government: Application of Uschold and King Ontology\n  Building Methodology for Semantic Ontology Models Development", "comments": "20 pages, 6 figures", "journal-ref": "International Journal of Web & Semantic Technology (IJWesT) Vol.\n  2, No. 4, October 2011, 1-20", "doi": "10.5121/ijwest.2011.2401", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electronic government (e-government) has been one of the most active areas of\nontology development during the past six years. In e-government, ontologies are\nbeing used to describe and specify e-government services (e-services) because\nthey enable easy composition, matching, mapping and merging of various\ne-government services. More importantly, they also facilitate the semantic\nintegration and interoperability of e-government services. However, it is still\nunclear in the current literature how an existing ontology building methodology\ncan be applied to develop semantic ontology models in a government service\ndomain. In this paper the Uschold and King ontology building methodology is\napplied to develop semantic ontology models in a government service domain.\nFirstly, the Uschold and King methodology is presented, discussed and applied\nto build a government domain ontology. Secondly, the domain ontology is\nevaluated for semantic consistency using its semi-formal representation in\nDescription Logic. Thirdly, an alignment of the domain ontology with the\nDescriptive Ontology for Linguistic and Cognitive Engineering (DOLCE) upper\nlevel ontology is drawn to allow its wider visibility and facilitate its\nintegration with existing metadata standard. Finally, the domain ontology is\nformally written in Web Ontology Language (OWL) to enable its automatic\nprocessing by computers. The study aims to provide direction for the\napplication of existing ontology building methodologies in the Semantic Web\ndevelopment processes of e-government domain specific ontology models; which\nwould enable their repeatability in other e-government projects and strengthen\nthe adoption of semantic technologies in e-government.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2011 15:40:10 GMT"}], "update_date": "2011-11-09", "authors_parsed": [["Fonou-Dombeu", "Jean Vincent", ""], ["Huisman", "Magda", ""]]}, {"id": "1111.2221", "submitter": "Weishan Dong", "authors": "Weishan Dong, Tianshi Chen, Peter Tino, and Xin Yao", "title": "Scaling Up Estimation of Distribution Algorithms For Continuous\n  Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since Estimation of Distribution Algorithms (EDA) were proposed, many\nattempts have been made to improve EDAs' performance in the context of global\noptimization. So far, the studies or applications of multivariate probabilistic\nmodel based continuous EDAs are still restricted to rather low dimensional\nproblems (smaller than 100D). Traditional EDAs have difficulties in solving\nhigher dimensional problems because of the curse of dimensionality and their\nrapidly increasing computational cost. However, scaling up continuous EDAs for\nhigher dimensional optimization is still necessary, which is supported by the\ndistinctive feature of EDAs: Because a probabilistic model is explicitly\nestimated, from the learnt model one can discover useful properties or features\nof the problem. Besides obtaining a good solution, understanding of the problem\nstructure can be of great benefit, especially for black box optimization. We\npropose a novel EDA framework with Model Complexity Control (EDA-MCC) to scale\nup EDAs. By using Weakly dependent variable Identification (WI) and Subspace\nModeling (SM), EDA-MCC shows significantly better performance than traditional\nEDAs on high dimensional problems. Moreover, the computational cost and the\nrequirement of large population sizes can be reduced in EDA-MCC. In addition to\nbeing able to find a good solution, EDA-MCC can also produce a useful problem\nstructure characterization. EDA-MCC is the first successful instance of\nmultivariate model based EDAs that can be effectively applied a general class\nof up to 500D problems. It also outperforms some newly developed algorithms\ndesigned specifically for large scale optimization. In order to understand the\nstrength and weakness of EDA-MCC, we have carried out extensive computational\nstudies of EDA-MCC. Our results have revealed when EDA-MCC is likely to\noutperform others on what kind of benchmark functions.\n", "versions": [{"version": "v1", "created": "Wed, 9 Nov 2011 14:44:58 GMT"}], "update_date": "2011-11-10", "authors_parsed": [["Dong", "Weishan", ""], ["Chen", "Tianshi", ""], ["Tino", "Peter", ""], ["Yao", "Xin", ""]]}, {"id": "1111.2249", "submitter": "H. H. Hoos", "authors": "Lin Xu, Frank Hutter, Holger H. Hoos, Kevin Leyton-Brown", "title": "SATzilla: Portfolio-based Algorithm Selection for SAT", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 32, pages\n  565-606, 2008", "doi": "10.1613/jair.2490", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been widely observed that there is no single \"dominant\" SAT solver;\ninstead, different solvers perform best on different instances. Rather than\nfollowing the traditional approach of choosing the best solver for a given\nclass of instances, we advocate making this decision online on a per-instance\nbasis. Building on previous work, we describe SATzilla, an automated approach\nfor constructing per-instance algorithm portfolios for SAT that use so-called\nempirical hardness models to choose among their constituent solvers. This\napproach takes as input a distribution of problem instances and a set of\ncomponent solvers, and constructs a portfolio optimizing a given objective\nfunction (such as mean runtime, percent of instances solved, or score in a\ncompetition). The excellent performance of SATzilla was independently verified\nin the 2007 SAT Competition, where our SATzilla07 solvers won three gold, one\nsilver and one bronze medal. In this article, we go well beyond SATzilla07 by\nmaking the portfolio construction scalable and completely automated, and\nimproving it by integrating local search solvers as candidate solvers, by\npredicting performance score instead of runtime, and by using hierarchical\nhardness models that take into account different types of SAT instances. We\ndemonstrate the effectiveness of these new techniques in extensive experimental\nresults on data sets including instances from the most recent SAT competition.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2011 22:28:59 GMT"}], "update_date": "2011-11-10", "authors_parsed": [["Xu", "Lin", ""], ["Hutter", "Frank", ""], ["Hoos", "Holger H.", ""], ["Leyton-Brown", "Kevin", ""]]}, {"id": "1111.2763", "submitter": "Nicolaie Popescu-Bodorin", "authors": "N. Popescu-Bodorin, V.E. Balas, I.M. Motoc", "title": "8-Valent Fuzzy Logic for Iris Recognition and Biometry", "comments": "6 pages, 2 figures, 5th IEEE Int. Symp. on Computational Intelligence\n  and Intelligent Informatics (Floriana, Malta, September 15-17), ISBN:\n  978-1-4577-1861-8 (electronic), 978-1-4577-1860-1 (print), 2011", "journal-ref": "Proc. 5th IEEE Int. Symp. on Computational Intelligence and\n  Intelligent Informatics, pp. 149-154, ISBN: 978-1-4577-1861-8 (electronic),\n  978-1-4577-1860-1 (print), IEEE Press, 2011", "doi": "10.1109/ISCIII.2011.6069761", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper shows that maintaining logical consistency of an iris recognition\nsystem is a matter of finding a suitable partitioning of the input space in\nenrollable and unenrollable pairs by negotiating the user comfort and the\nsafety of the biometric system. In other words, consistent enrollment is\nmandatory in order to preserve system consistency. A fuzzy 3-valued\ndisambiguated model of iris recognition is proposed and analyzed in terms of\ncompleteness, consistency, user comfort and biometric safety. It is also shown\nhere that the fuzzy 3-valued model of iris recognition is hosted by an 8-valued\nBoolean algebra of modulo 8 integers that represents the computational\nformalization in which a biometric system (a software agent) can achieve the\nartificial understanding of iris recognition in a logically consistent manner.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2011 21:38:25 GMT"}], "update_date": "2011-11-14", "authors_parsed": [["Popescu-Bodorin", "N.", ""], ["Balas", "V. E.", ""], ["Motoc", "I. M.", ""]]}, {"id": "1111.2988", "submitter": "Anant  Baijal", "authors": "Anant Baijal, Vikram Singh Chauhan and T. Jayabarathi", "title": "Application of PSO, Artificial Bee Colony and Bacterial Foraging\n  Optimization algorithms to economic load dispatch: An analysis", "comments": null, "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 8,\n  Issue 4, No 1, 2011, 467-470", "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper illustrates successful implementation of three evolutionary\nalgorithms, namely- Particle Swarm Optimization(PSO), Artificial Bee Colony\n(ABC) and Bacterial Foraging Optimization (BFO) algorithms to economic load\ndispatch problem (ELD). Power output of each generating unit and optimum fuel\ncost obtained using all three algorithms have been compared. The results\nobtained show that ABC and BFO algorithms converge to optimal fuel cost with\nreduced computational time when compared to PSO for the two example problems\nconsidered.\n", "versions": [{"version": "v1", "created": "Sun, 13 Nov 2011 04:46:28 GMT"}], "update_date": "2015-09-23", "authors_parsed": [["Baijal", "Anant", ""], ["Chauhan", "Vikram Singh", ""], ["Jayabarathi", "T.", ""]]}, {"id": "1111.3270", "submitter": "Mehdi Kaytoue", "authors": "Mehdi Kaytoue (DCC - UFMG), Sergei O. Kuznetsov, Juraj Macko, Wagner\n  Meira (DCC - UFMG), Amedeo Napoli (INRIA Lorraine - LORIA)", "title": "Mining Biclusters of Similar Values with Triadic Concept Analysis", "comments": "Concept Lattices and their Applications (CLA) (2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biclustering numerical data became a popular data-mining task in the\nbeginning of 2000's, especially for analysing gene expression data. A bicluster\nreflects a strong association between a subset of objects and a subset of\nattributes in a numerical object/attribute data-table. So called biclusters of\nsimilar values can be thought as maximal sub-tables with close values. Only few\nmethods address a complete, correct and non redundant enumeration of such\npatterns, which is a well-known intractable problem, while no formal framework\nexists. In this paper, we introduce important links between biclustering and\nformal concept analysis. More specifically, we originally show that Triadic\nConcept Analysis (TCA), provides a nice mathematical framework for\nbiclustering. Interestingly, existing algorithms of TCA, that usually apply on\nbinary data, can be used (directly or with slight modifications) after a\npreprocessing step for extracting maximal biclusters of similar values.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2011 16:22:33 GMT"}], "update_date": "2011-11-15", "authors_parsed": [["Kaytoue", "Mehdi", "", "DCC - UFMG"], ["Kuznetsov", "Sergei O.", "", "DCC - UFMG"], ["Macko", "Juraj", "", "DCC - UFMG"], ["Meira", "Wagner", "", "DCC - UFMG"], ["Napoli", "Amedeo", "", "INRIA Lorraine - LORIA"]]}, {"id": "1111.3690", "submitter": "Nicolas Maudet", "authors": "Yann Chevaleyre, J\\'er\\^ome Lang, Nicolas Maudet, J\\'er\\^ome Monnot,\n  Lirong Xia", "title": "New Candidates Welcome! Possible Winners with respect to the Addition of\n  New Candidates", "comments": "34 pages", "journal-ref": "Mathematical Social Sciences 64(1), 2012", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In voting contexts, some new candidates may show up in the course of the\nprocess. In this case, we may want to determine which of the initial candidates\nare possible winners, given that a fixed number $k$ of new candidates will be\nadded. We give a computational study of this problem, focusing on scoring\nrules, and we provide a formal comparison with related problems such as control\nvia adding candidates or cloning.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2011 23:41:11 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Chevaleyre", "Yann", ""], ["Lang", "J\u00e9r\u00f4me", ""], ["Maudet", "Nicolas", ""], ["Monnot", "J\u00e9r\u00f4me", ""], ["Xia", "Lirong", ""]]}, {"id": "1111.3735", "submitter": "Gabriel Synnaeve", "authors": "Gabriel Synnaeve (LIG, LPPA), Pierre Bessi\\`ere (LIG, LPPA)", "title": "A Bayesian Model for Plan Recognition in RTS Games applied to StarCraft", "comments": "7 pages; Artificial Intelligence and Interactive Digital\n  Entertainment Conference (AIIDE 2011), Palo Alto : \\'Etats-Unis (2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of keyhole (unobtrusive) plan recognition is central to adaptive\ngame AI. \"Tech trees\" or \"build trees\" are the core of real-time strategy (RTS)\ngame strategic (long term) planning. This paper presents a generic and simple\nBayesian model for RTS build tree prediction from noisy observations, which\nparameters are learned from replays (game logs). This unsupervised machine\nlearning approach involves minimal work for the game developers as it leverage\nplayers' data (com- mon in RTS). We applied it to StarCraft1 and showed that it\nyields high quality and robust predictions, that can feed an adaptive AI.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2011 09:26:14 GMT"}], "update_date": "2011-11-17", "authors_parsed": [["Synnaeve", "Gabriel", "", "LIG, LPPA"], ["Bessi\u00e8re", "Pierre", "", "LIG, LPPA"]]}, {"id": "1111.3934", "submitter": "Bill Hibbard", "authors": "Bill Hibbard", "title": "Model-based Utility Functions", "comments": "24 pages, extensive revisions", "journal-ref": "Journal of Artificial General Intelligence 3(1) 1-24, 2012", "doi": "10.2478/v10229-011-0013-5", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Orseau and Ring, as well as Dewey, have recently described problems,\nincluding self-delusion, with the behavior of agents using various definitions\nof utility functions. An agent's utility function is defined in terms of the\nagent's history of interactions with its environment. This paper argues, via\ntwo examples, that the behavior problems can be avoided by formulating the\nutility function in two steps: 1) inferring a model of the environment from\ninteractions, and 2) computing utility as a function of the environment model.\nBasing a utility function on a model that the agent must learn implies that the\nutility function must initially be expressed in terms of specifications to be\nmatched to structures in the learned model. These specifications constitute\nprior assumptions about the environment so this approach will not work with\narbitrary environments. But the approach should work for agents designed by\nhumans to act in the physical world. The paper also addresses the issue of\nself-modifying agents and shows that if provided with the possibility to modify\ntheir utility functions agents will not choose to do so, under some usual\nassumptions.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2011 20:13:54 GMT"}, {"version": "v2", "created": "Sat, 12 May 2012 16:05:46 GMT"}], "update_date": "2012-05-15", "authors_parsed": [["Hibbard", "Bill", ""]]}, {"id": "1111.4083", "submitter": "Denis Berthier", "authors": "Denis Berthier (DSI)", "title": "Unbiased Statistics of a CSP - A Controlled-Bias Generator", "comments": null, "journal-ref": "Innovations in Computing Sciences and Software Engineering, (2010)\n  165-170", "doi": "10.1007/978-90-481-3660-5_28", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that estimating the complexity (mean and distribution) of the\ninstances of a fixed size Constraint Satisfaction Problem (CSP) can be very\nhard. We deal with the main two aspects of the problem: defining a measure of\ncomplexity and generating random unbiased instances. For the first problem, we\nrely on a general framework and a measure of complexity we presented at\nCISSE08. For the generation problem, we restrict our analysis to the Sudoku\nexample and we provide a solution that also explains why it is so difficult.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2011 13:15:24 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Berthier", "Denis", "", "DSI"]]}, {"id": "1111.4232", "submitter": "Kirill Sorudeykin Mr", "authors": "Kirill A. Sorudeykin", "title": "A Model of Spatial Thinking for Computational Intelligence", "comments": "8 pages, 5 figures; IEEE East-West Design & Test Symposium, 2011", "journal-ref": null, "doi": "10.1109/EWDTS.2011.6116427", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Trying to be effective (no matter who exactly and in what field) a person\nface the problem which inevitably destroys all our attempts to easily get to a\ndesired goal. The problem is the existence of some insuperable barriers for our\nmind, anotherwords barriers for principles of thinking. They are our clue and\nmain reason for research. Here we investigate these barriers and their features\nexposing the nature of mental process. We start from special structures which\nreflect the ways to define relations between objects. Then we came to realizing\nabout what is the material our mind uses to build thoughts, to make\nconclusions, to understand, to form reasoning, etc. This can be called a mental\ndynamics. After this the nature of mental barriers on the required level of\nabstraction as well as the ways to pass through them became clear. We begin to\nunderstand why thinking flows in such a way, with such specifics and with such\nlimitations we can observe in reality. This can help us to be more optimal. At\nthe final step we start to understand, what ma-thematical models can be applied\nto such a picture. We start to express our thoughts in a language of\nmathematics, developing an apparatus for our Spatial Theory of Mind, suitable\nto represent processes and infrastructure of thinking. We use abstract algebra\nand stay invariant in relation to the nature of objects.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2011 22:22:21 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Sorudeykin", "Kirill A.", ""]]}, {"id": "1111.4267", "submitter": "Victor A. Rodriguez-Toro", "authors": "Victor A. Rodriguez-Toro, Jaime E. Garzon, Jesus A. Lopez", "title": "Control Neuronal por Modelo Inverso de un Servosistema Usando Algoritmos\n  de Aprendizaje Levenberg-Marquardt y Bayesiano", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present the experimental results of the neural network\ncontrol of a servo-system in order to control its speed. The control strategy\nis implemented by using an inverse-model control based on Artificial Neural\nNetworks (ANNs). The network training was performed using two learning\nalgorithms: Levenberg-Marquardt and Bayesian regularization. We evaluate the\ngeneralization capability for each method according to both the correct\noperation of the controller to follow the reference signal, and the control\nefforts developed by the ANN-based controller.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2011 03:26:47 GMT"}], "update_date": "2011-11-21", "authors_parsed": [["Rodriguez-Toro", "Victor A.", ""], ["Garzon", "Jaime E.", ""], ["Lopez", "Jesus A.", ""]]}, {"id": "1111.4930", "submitter": "Arka Ghosh", "authors": "Arka Ghosh", "title": "Comparative study of Financial Time Series Prediction by Artificial\n  Neural Network with Gradient Descent Learning", "comments": null, "journal-ref": "International Journal Of Scientific & Engineering Research\n  ISSN-2229-5518 Volume 3 Issue 1 January2012", "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Financial forecasting is an example of a signal processing problem which is\nchallenging due to Small sample sizes, high noise, non-stationarity, and\nnon-linearity,but fast forecasting of stock market price is very important for\nstrategic business planning.Present study is aimed to develop a comparative\npredictive model with Feedforward Multilayer Artificial Neural Network &\nRecurrent Time Delay Neural Network for the Financial Timeseries\nPrediction.This study is developed with the help of historical stockprice\ndataset made available by GoogleFinance.To develop this prediction model\nBackpropagation method with Gradient Descent learning has been\nimplemented.Finally the Neural Net, learned with said algorithm is found to be\nskillful predictor for non-stationary noisy Financial Timeseries.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2011 16:58:58 GMT"}, {"version": "v2", "created": "Tue, 31 Jan 2012 08:09:57 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Ghosh", "Arka", ""]]}, {"id": "1111.5296", "submitter": "Hossein Shokri Ghadikolaei", "authors": "Hossein Shokri-Ghadikolaei, Younes Abdi, Masoumeh Nasiri-Kenari", "title": "Analytical and Learning-Based Spectrum Sensing Time Optimization in\n  Cognitive Radio Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Powerful spectrum sensing schemes enable cognitive radios (CRs) to find\ntransmission opportunities in spectral resources allocated exclusively to the\nprimary users. In this paper, maximizing the average throughput of a secondary\nuser by optimizing its spectrum sensing time is formulated assuming that a\nprior knowledge of the presence and absence probabilities of the primary users\nis available. The energy consumed for finding a transmission opportunity is\nevaluated and a discussion on the impact of the number of the primary users on\nthe secondary user throughput and consumed energy is presented. In order to\navoid the challenges associated with the analytical method, as a second\nsolution, a systematic neural network-based sensing time optimization approach\nis also proposed in this paper. The proposed adaptive scheme is able to find\nthe optimum value of the channel sensing time without any prior knowledge or\nassumption about the wireless environment. The structure, performance, and\ncooperation of the artificial neural networks used in the proposed method are\ndisclosed in detail and a set of illustrative simulation results is presented\nto validate the analytical results as well as the performance of the proposed\nlearning-based optimization scheme.\n", "versions": [{"version": "v1", "created": "Sun, 20 Nov 2011 10:08:06 GMT"}, {"version": "v2", "created": "Thu, 29 Dec 2011 07:45:05 GMT"}], "update_date": "2011-12-30", "authors_parsed": [["Shokri-Ghadikolaei", "Hossein", ""], ["Abdi", "Younes", ""], ["Nasiri-Kenari", "Masoumeh", ""]]}, {"id": "1111.5312", "submitter": "Ryan Rossi", "authors": "Ryan A. Rossi and Jennifer Neville", "title": "Representations and Ensemble Methods for Dynamic Relational\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SI physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal networks are ubiquitous and evolve over time by the addition,\ndeletion, and changing of links, nodes, and attributes. Although many\nrelational datasets contain temporal information, the majority of existing\ntechniques in relational learning focus on static snapshots and ignore the\ntemporal dynamics. We propose a framework for discovering temporal\nrepresentations of relational data to increase the accuracy of statistical\nrelational learning algorithms. The temporal relational representations serve\nas a basis for classification, ensembles, and pattern mining in evolving\ndomains. The framework includes (1) selecting the time-varying relational\ncomponents (links, attributes, nodes), (2) selecting the temporal granularity,\n(3) predicting the temporal influence of each time-varying relational\ncomponent, and (4) choosing the weighted relational classifier. Additionally,\nwe propose temporal ensemble methods that exploit the temporal-dimension of\nrelational data. These ensembles outperform traditional and more sophisticated\nrelational ensembles while avoiding the issue of learning the most optimal\nrepresentation. Finally, the space of temporal-relational models are evaluated\nusing a sample of classifiers. In all cases, the proposed temporal-relational\nclassifiers outperform competing models that ignore the temporal information.\nThe results demonstrate the capability and necessity of the temporal-relational\nrepresentations for classification, ensembles, and for mining temporal\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2011 20:21:19 GMT"}], "update_date": "2011-11-23", "authors_parsed": [["Rossi", "Ryan A.", ""], ["Neville", "Jennifer", ""]]}, {"id": "1111.5358", "submitter": "Hema Swetha Koppula", "authors": "Abhishek Anand, Hema Swetha Koppula, Thorsten Joachims, Ashutosh\n  Saxena", "title": "Contextually Guided Semantic Labeling and Search for 3D Point Clouds", "comments": "arXiv admin note: substantial text overlap with arXiv:1106.5551", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RGB-D cameras, which give an RGB image to- gether with depths, are becoming\nincreasingly popular for robotic perception. In this paper, we address the task\nof detecting commonly found objects in the 3D point cloud of indoor scenes\nobtained from such cameras. Our method uses a graphical model that captures\nvarious features and contextual relations, including the local visual\nappearance and shape cues, object co-occurence relationships and geometric\nrelationships. With a large number of object classes and relations, the model's\nparsimony becomes important and we address that by using multiple types of edge\npotentials. We train the model using a maximum-margin learning approach. In our\nexperiments over a total of 52 3D scenes of homes and offices (composed from\nabout 550 views), we get a performance of 84.06% and 73.38% in labeling office\nand home scenes respectively for 17 object classes each. We also present a\nmethod for a robot to search for an object using the learned model and the\ncontextual information available from the current labelings of the scene. We\napplied this algorithm successfully on a mobile robot for the task of finding\n12 object classes in 10 different offices and achieved a precision of 97.56%\nwith 78.43% recall.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2011 22:39:31 GMT"}, {"version": "v2", "created": "Tue, 12 Jun 2012 02:50:50 GMT"}, {"version": "v3", "created": "Wed, 5 Sep 2012 15:16:01 GMT"}], "update_date": "2012-09-06", "authors_parsed": [["Anand", "Abhishek", ""], ["Koppula", "Hema Swetha", ""], ["Joachims", "Thorsten", ""], ["Saxena", "Ashutosh", ""]]}, {"id": "1111.5689", "submitter": "Mehdi Kaytoue", "authors": "Mehdi Kaytoue (INRIA Lorraine - LORIA), Sergei O. Kuznetsov, Amedeo\n  Napoli (INRIA Lorraine - LORIA)", "title": "Revisiting Numerical Pattern Mining with Formal Concept Analysis", "comments": null, "journal-ref": "International Joint Conference on Artificial Intelligence (IJCAI)\n  (2011)", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the problem of mining numerical data in the\nframework of Formal Concept Analysis. The usual way is to use a scaling\nprocedure --transforming numerical attributes into binary ones-- leading either\nto a loss of information or of efficiency, in particular w.r.t. the volume of\nextracted patterns. By contrast, we propose to directly work on numerical data\nin a more precise and efficient way, and we prove it. For that, the notions of\nclosed patterns, generators and equivalent classes are revisited in the\nnumerical context. Moreover, two original algorithms are proposed and used in\nan evaluation involving real-world data, showing the predominance of the\npresent approach.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2011 07:55:16 GMT"}], "update_date": "2011-11-28", "authors_parsed": [["Kaytoue", "Mehdi", "", "INRIA Lorraine - LORIA"], ["Kuznetsov", "Sergei O.", "", "INRIA Lorraine - LORIA"], ["Napoli", "Amedeo", "", "INRIA Lorraine - LORIA"]]}, {"id": "1111.5720", "submitter": "Andreas Konstantinidis", "authors": "Andreas Konstantinidis, Haris Haralambous, Alexandros Agapitos and\n  Harris Papadopoulos", "title": "A GP-MOEA/D Approach for Modelling Total Electron Content over Cyprus", "comments": null, "journal-ref": "A. Konstantinidis, H. Haralambous, A. Agapitos and H.\n  Papadopoulos. A GP-MOEA/D Approach for Modelling Total Electron Content over\n  Cyprus. Engineering Intelligent Systems 18(3-4): 193-203. CRL Publishing,\n  2010", "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vertical Total Electron Content (vTEC) is an ionospheric characteristic used\nto derive the signal delay imposed by the ionosphere on near-vertical\ntrans-ionospheric links. The major aim of this paper is to design a prediction\nmodel based on the main factors that influence the variability of this\nparameter on a diurnal, seasonal and long-term time-scale. The model should be\naccurate and general (comprehensive) enough for efficiently approximating the\nhigh variations of vTEC. However, good approximation and generalization are\nconflicting objectives. For this reason a Genetic Programming (GP) with\nMulti-objective Evolutionary Algorithm based on Decomposition characteristics\n(GP-MOEA/D) is designed and proposed for modeling vTEC over Cyprus.\nExperimental results show that the Multi-Objective GP-model, considering real\nvTEC measurements obtained over a period of 11 years, has produced a good\napproximation of the modeled parameter and can be implemented as a local model\nto account for the ionospheric imposed error in positioning. Particulary, the\nGP-MOEA/D approach performs better than a Single Objective Optimization GP, a\nGP with Non-dominated Sorting Genetic Algorithm-II (NSGA-II) characteristics\nand the previously proposed Neural Network-based approach in most cases.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2011 11:04:09 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Konstantinidis", "Andreas", ""], ["Haralambous", "Haris", ""], ["Agapitos", "Alexandros", ""], ["Papadopoulos", "Harris", ""]]}, {"id": "1111.6117", "submitter": "Marcus Hutter", "authors": "Peter Sunehag and Marcus Hutter", "title": "Principles of Solomonoff Induction and AIXI", "comments": "14 LaTeX pages", "journal-ref": "Proc. Solomonoff 85th Memorial Conference (SOL 2011) pages 386-398", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We identify principles characterizing Solomonoff Induction by demands on an\nagent's external behaviour. Key concepts are rationality, computability,\nindifference and time consistency. Furthermore, we discuss extensions to the\nfull AI case to derive AIXI.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2011 21:35:29 GMT"}], "update_date": "2014-07-15", "authors_parsed": [["Sunehag", "Peter", ""], ["Hutter", "Marcus", ""]]}, {"id": "1111.6191", "submitter": "Albrecht Zimmermann", "authors": "Bj\\\"orn Bringmann and Siegfried Nijssen and Albrecht Zimmermann", "title": "Pattern-Based Classification: A Unifying Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of patterns in predictive models is a topic that has received a lot\nof attention in recent years. Pattern mining can help to obtain models for\nstructured domains, such as graphs and sequences, and has been proposed as a\nmeans to obtain more accurate and more interpretable models. Despite the large\namount of publications devoted to this topic, we believe however that an\noverview of what has been accomplished in this area is missing. This paper\npresents our perspective on this evolving area. We identify the principles of\npattern mining that are important when mining patterns for models and provide\nan overview of pattern-based classification methods. We categorize these\nmethods along the following dimensions: (1) whether they post-process a\npre-computed set of patterns or iteratively execute pattern mining algorithms;\n(2) whether they select patterns model-independently or whether the pattern\nselection is guided by a model. We summarize the results that have been\nobtained for each of these methods.\n", "versions": [{"version": "v1", "created": "Sat, 26 Nov 2011 20:11:56 GMT"}], "update_date": "2011-11-29", "authors_parsed": [["Bringmann", "Bj\u00f6rn", ""], ["Nijssen", "Siegfried", ""], ["Zimmermann", "Albrecht", ""]]}, {"id": "1111.6387", "submitter": "Kassimi My Abdellah", "authors": "My Abdellah Kassimi and Omar El beqqali", "title": "3D Model Retrieval Based on Semantic and Shape Indexes", "comments": "IJCSI International Journal of Computer Science Issues, Vol. 8, Issue\n  3, May 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CV", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  The size of 3D models used on the web or stored in databases is becoming\nincreasingly high. Then, an efficient method that allows users to find similar\n3D objects for a given 3D model query has become necessary. Keywords and the\ngeometry of a 3D model cannot meet the needs of users' retrieval because they\ndo not include the semantic information. In this paper, a new method has been\nproposed to 3D models retrieval using semantic concepts combined with shape\nindexes. To obtain these concepts, we use the machine learning methods to label\n3D models by k-means algorithm in measures and shape indexes space. Moreover,\nsemantic concepts have been organized and represented by ontology language OWL\nand spatial relationships are used to disambiguate among models of similar\nappearance. The SPARQL query language has been used to question the information\ndisplayed in this language and to compute the similarity between two 3D models.\nWe interpret our results using the Princeton Shape Benchmark Database and the\nresults show the performance of the proposed new approach to retrieval 3D\nmodels. Keywords: 3D Model, 3D retrieval, measures, shape indexes, semantic,\nontology\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2011 10:07:41 GMT"}], "update_date": "2011-11-29", "authors_parsed": [["Kassimi", "My Abdellah", ""], ["beqqali", "Omar El", ""]]}, {"id": "1111.6401", "submitter": "Hajar Elmaghraoui", "authors": "Hajar Elmaghraoui, Imane Zaoui, Dalila Chiadmi, Laila Benhlima", "title": "Graph based E-Government web service composition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nowadays, e-government has emerged as a government policy to improve the\nquality and efficiency of public administrations. By exploiting the potential\nof new information and communication technologies, government agencies are\nproviding a wide spectrum of online services. These services are composed of\nseveral web services that comply with well defined processes. One of the big\nchallenges is the need to optimize the composition of the elementary web\nservices. In this paper, we present a solution for optimizing the computation\neffort in web service composition. Our method is based on Graph Theory. We\nmodel the semantic relationship between the involved web services through a\ndirected graph. Then, we compute all shortest paths using for the first time,\nan extended version of the Floyd-Warshall algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2011 10:52:28 GMT"}], "update_date": "2011-11-29", "authors_parsed": [["Elmaghraoui", "Hajar", ""], ["Zaoui", "Imane", ""], ["Chiadmi", "Dalila", ""], ["Benhlima", "Laila", ""]]}, {"id": "1111.6713", "submitter": "Mohammed Elmogy Dr.", "authors": "Ahmed Tolba and Nabila Eladawi and Mohammed Elmogy", "title": "An Enhanced Indexing And Ranking Technique On The Semantic Web", "comments": "8 pages, 7 figures", "journal-ref": "IJCSI International Journal of Computer Science Issues, Vol. 8,\n  Issue 5, No 3, 2011, 118-125", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  With the fast growth of the Internet, more and more information is available\non the Web. The Semantic Web has many features which cannot be handled by using\nthe traditional search engines. It extracts metadata for each discovered Web\ndocuments in RDF or OWL formats, and computes relations between documents. We\nproposed a hybrid indexing and ranking technique for the Semantic Web which\nfinds relevant documents and computes the similarity among a set of documents.\nFirst, it returns with the most related document from the repository of\nSemantic Web Documents (SWDs) by using a modified version of the ObjectRank\ntechnique. Then, it creates a sub-graph for the most related SWDs. Finally, It\nreturns the hubs and authorities of these document by using the HITS algorithm.\nOur technique increases the quality of the results and decreases the execution\ntime of processing the user's query.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2011 07:24:27 GMT"}], "update_date": "2011-11-30", "authors_parsed": [["Tolba", "Ahmed", ""], ["Eladawi", "Nabila", ""], ["Elmogy", "Mohammed", ""]]}, {"id": "1111.6790", "submitter": "Sao Mai Nguyen", "authors": "Sao Mai Nguyen (INRIA Bordeaux - Sud-Ouest), Adrien Baranes (INRIA\n  Bordeaux - Sud-Ouest), Pierre-Yves Oudeyer (INRIA Bordeaux - Sud-Ouest)", "title": "Constraining the Size Growth of the Task Space with Socially Guided\n  Intrinsic Motivation using Demonstrations", "comments": "JCAI Workshop on Agents Learning Interactively from Human Teachers\n  (ALIHT), Barcelona : Spain (2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an algorithm for learning a highly redundant inverse\nmodel in continuous and non-preset environments. Our Socially Guided Intrinsic\nMotivation by Demonstrations (SGIM-D) algorithm combines the advantages of both\nsocial learning and intrinsic motivation, to specialise in a wide range of\nskills, while lessening its dependence on the teacher. SGIM-D is evaluated on a\nfishing skill learning experiment.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2011 12:29:27 GMT"}], "update_date": "2011-11-30", "authors_parsed": [["Nguyen", "Sao Mai", "", "INRIA Bordeaux - Sud-Ouest"], ["Baranes", "Adrien", "", "INRIA\n  Bordeaux - Sud-Ouest"], ["Oudeyer", "Pierre-Yves", "", "INRIA Bordeaux - Sud-Ouest"]]}, {"id": "1111.6825", "submitter": "Mohammad Assarian", "authors": "Alireza Amirshahi, Mahmood Fathi, Morteza Romoozi and Mohammad\n  Assarian", "title": "A Fuzzy Realistic Mobility Model For Ad hoc Networks", "comments": null, "journal-ref": "International journal of computer science Issues,Vol. 8,Issue 5,No\n  3, 2011, 42-50", "doi": null, "report-no": null, "categories": "cs.AI cs.NI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Realistic mobility models can demonstrate more precise evaluation results\nbecause their parameters are closer to the reality. In this paper a realistic\nFuzzy Mobility Model has been proposed. This model has rules which is\nchangeable depending on nodes and environment conditions. This model is more\ncomplete and precise than the other mobility models and this is the advantage\nof this model. After simulation, it was found out that not only considering\nnodes movement as being imprecise (fuzzy) has a positive effects on most of ad\nhoc network parameters, but also, more importantly as they are closer to the\nreal world condition, they can have a more positive effect on the\nimplementation of ad hoc network protocols.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2011 14:36:40 GMT"}], "update_date": "2011-11-30", "authors_parsed": [["Amirshahi", "Alireza", ""], ["Fathi", "Mahmood", ""], ["Romoozi", "Morteza", ""], ["Assarian", "Mohammad", ""]]}, {"id": "1111.6843", "submitter": "Micha{\\l} B. Paradowski", "authors": "Micha{\\l} B. Paradowski, {\\L}ukasz Jonak", "title": "Understanding the Social Cascading of Geekspeak and the Upshots for\n  Social Cognitive Systems", "comments": "M.B. Paradowski, {\\L}. Jonak (2012) Understanding the social\n  cascading of geekspeak and the upshots for social cognitive systems. In: A.\n  Galton, Z. Wood (Eds) Understanding and Modelling Collective Phenomena.\n  UBham, 27-32", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Barring swarm robotics, a substantial share of current machine-human and\nmachine-machine learning and interaction mechanisms are being developed and fed\nby results of agent-based computer simulations, game-theoretic models, or\nrobotic experiments based on a dyadic communication pattern. Yet, in real life,\nhumans no less frequently communicate in groups, and gain knowledge and take\ndecisions basing on information cumulatively gleaned from more than one single\nsource. These properties should be taken into consideration in the design of\nautonomous artificial cognitive systems construed to interact with learn from\nmore than one contact or 'neighbour'. To this end, significant practical import\ncan be gleaned from research applying strict science methodology to human and\nsocial phenomena, e.g. to discovery of realistic creativity potential spans, or\nthe 'exposure thresholds' after which new information could be accepted by a\ncognitive agent. The results will be presented of a project analysing the\nsocial propagation of neologisms in a microblogging service. From local,\nlow-level interactions and information flows between agents inventing and\nimitating discrete lexemes we aim to describe the processes of the emergence of\nmore global systemic order and dynamics, using the latest methods of complexity\nscience. Whether in order to mimic them, or to 'enhance' them, parameters\ngleaned from complexity science approaches to humans' social and humanistic\nbehaviour should subsequently be incorporated as points of reference in the\nfield of robotics and human-machine interaction.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2011 15:23:42 GMT"}, {"version": "v2", "created": "Thu, 8 Dec 2011 12:54:01 GMT"}, {"version": "v3", "created": "Fri, 20 Jul 2012 16:50:34 GMT"}, {"version": "v4", "created": "Thu, 23 Aug 2012 14:25:44 GMT"}], "update_date": "2012-08-24", "authors_parsed": [["Paradowski", "Micha\u0142 B.", ""], ["Jonak", "\u0141ukasz", ""]]}, {"id": "1111.6883", "submitter": "Martin Moguillansky", "authors": "Mart\\'in O. Moguillansky, Nicol\\'as D. Rotstein, Marcelo A. Falappa,\n  Alejandro J. Garc\\'ia and Guillermo R. Simari", "title": "Dynamics of Knowledge in DeLP through Argument Theory Change", "comments": "61 pages To appear in Theory and Practice of Logic Programming (TPLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article is devoted to the study of methods to change defeasible logic\nprograms (de.l.p.s) which are the knowledge bases used by the Defeasible Logic\nProgramming (DeLP) interpreter. DeLP is an argumentation formalism that allows\nto reason over potentially inconsistent de.l.p.s. Argument Theory Change (ATC)\nstudies certain aspects of belief revision in order to make them suitable for\nabstract argumentation systems. In this article, abstract arguments are\nrendered concrete by using the particular rule-based defeasible logic adopted\nby DeLP. The objective of our proposal is to define prioritized argument\nrevision operators \\`a la ATC for de.l.p.s, in such a way that the newly\ninserted argument ends up undefeated after the revision, thus warranting its\nconclusion. In order to ensure this warrant, the de.l.p. has to be changed in\nconcordance with a minimal change principle. To this end, we discuss different\nminimal change criteria that could be adopted. Finally, an algorithm is\npresented, implementing the argument revision operations.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2011 16:34:22 GMT"}], "update_date": "2011-11-30", "authors_parsed": [["Moguillansky", "Mart\u00edn O.", ""], ["Rotstein", "Nicol\u00e1s D.", ""], ["Falappa", "Marcelo A.", ""], ["Garc\u00eda", "Alejandro J.", ""], ["Simari", "Guillermo R.", ""]]}, {"id": "1111.6983", "submitter": "Mark Levin", "authors": "Mark Sh. Levin", "title": "Aggregation of Composite Solutions: strategies, models, examples", "comments": "72 pages, 116 figures, 35 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper addresses aggregation issues for composite (modular) solutions. A\nsystemic view point is suggested for various aggregation problems. Several\nsolution structures are considered: sets, set morphologies, trees, etc. Mainly,\nthe aggregation approach is targeted to set morphologies. The aggregation\nproblems are based on basic structures as substructure, superstructure,\nmedian/consensus, and extended median/consensus. In the last case, preliminary\nstructure is built (e.g., substructure, median/consensus) and addition of\nsolution elements is considered while taking into account profit of the\nadditional elements and total resource constraint. Four aggregation strategies\nare examined: (i) extension strategy (designing a substructure of initial\nsolutions as \"system kernel\" and extension of the substructure by additional\nelements); (ii) compression strategy (designing a superstructure of initial\nsolutions and deletion of some its elements); (iii) combined strategy; and (iv)\nnew design strategy to build a new solution over an extended domain of solution\nelements. Numerical real-world examples (e.g., telemetry system, communication\nprotocol, student plan, security system, Web-based information system,\ninvestment, educational courses) illustrate the suggested aggregation approach.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2011 21:08:06 GMT"}], "update_date": "2011-12-01", "authors_parsed": [["Levin", "Mark Sh.", ""]]}, {"id": "1111.7025", "submitter": "Ilche Georgievski", "authors": "Il\\v{c}e Georgievski, Alexander Lazovik and Marco Aiello", "title": "Task Interaction in an HTN Planner", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical Task Network (HTN) planning uses task decomposition to plan for\nan executable sequence of actions as a solution to a problem. In order to\nreason effectively, an HTN planner needs expressive domain knowledge. For\ninstance, a simplified HTN planning system such as JSHOP2 uses such\nexpressivity and avoids some task interactions due to the increased complexity\nof the planning process. We address the possibility of simplifying the domain\nrepresentation needed for an HTN planner to find good solutions, especially in\nreal-world domains describing home and building automation environments. We\nextend the JSHOP2 planner to reason about task interaction that happens when\ntask's effects are already achieved by other tasks. The planner then prunes\nsome of the redundant searches that can occur due to the planning process's\ninterleaving nature. We evaluate the original and our improved planner on two\nbenchmark domains. We show that our planner behaves better by using simplified\ndomain knowledge and outperforms JSHOP2 in a number of relevant cases.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2011 00:31:47 GMT"}], "update_date": "2011-12-01", "authors_parsed": [["Georgievski", "Il\u010de", ""], ["Lazovik", "Alexander", ""], ["Aiello", "Marco", ""]]}, {"id": "1111.7190", "submitter": "Micha{\\l} B. Paradowski", "authors": "Micha{\\l} B. Paradowski", "title": "Developing Embodied Multisensory Dialogue Agents", "comments": "(2012) Developing embodied multisensory dialogue agents. In: R.\n  Rzepka, M. Ptaszy\\'nski, P. Dybala (Eds) Linguistic and Cognitive Approaches\n  To Dialogue Agents. AISB, 6-14", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  A few decades of work in the AI field have focused efforts on developing a\nnew generation of systems which can acquire knowledge via interaction with the\nworld. Yet, until very recently, most such attempts were underpinned by\nresearch which predominantly regarded linguistic phenomena as separated from\nthe brain and body. This could lead one into believing that to emulate\nlinguistic behaviour, it suffices to develop 'software' operating on abstract\nrepresentations that will work on any computational machine. This picture is\ninaccurate for several reasons, which are elucidated in this paper and extend\nbeyond sensorimotor and semantic resonance. Beginning with a review of\nresearch, I list several heterogeneous arguments against disembodied language,\nin an attempt to draw conclusions for developing embodied multisensory agents\nwhich communicate verbally and non-verbally with their environment. Without\ntaking into account both the architecture of the human brain, and embodiment,\nit is unrealistic to replicate accurately the processes which take place during\nlanguage acquisition, comprehension, production, or during non-linguistic\nactions. While robots are far from isomorphic with humans, they could benefit\nfrom strengthened associative connections in the optimization of their\nprocesses and their reactivity and sensitivity to environmental stimuli, and in\nsituated human-machine interaction. The concept of multisensory integration\nshould be extended to cover linguistic input and the complementary information\ncombined from temporally coincident sensory impressions.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2011 15:30:24 GMT"}, {"version": "v2", "created": "Thu, 8 Dec 2011 12:51:59 GMT"}, {"version": "v3", "created": "Fri, 20 Jul 2012 16:44:04 GMT"}], "update_date": "2012-07-23", "authors_parsed": [["Paradowski", "Micha\u0142 B.", ""]]}]