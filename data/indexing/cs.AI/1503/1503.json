[{"id": "1503.00022", "submitter": "Soham De", "authors": "Soham De, Indradyumna Roy, Tarunima Prabhakar, Kriti Suneja, Sourish\n  Chaudhuri, Rita Singh, Bhiksha Raj", "title": "Plagiarism Detection in Polyphonic Music using Monaural Signal\n  Separation", "comments": "Preprint version", "journal-ref": "INTERSPEECH-2012, 1744-1747 (2012)", "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given the large number of new musical tracks released each year, automated\napproaches to plagiarism detection are essential to help us track potential\nviolations of copyright. Most current approaches to plagiarism detection are\nbased on musical similarity measures, which typically ignore the issue of\npolyphony in music. We present a novel feature space for audio derived from\ncompositional modelling techniques, commonly used in signal separation, that\nprovides a mechanism to account for polyphony without incurring an inordinate\namount of computational overhead. We employ this feature representation in\nconjunction with traditional audio feature representations in a classification\nframework which uses an ensemble of distance features to characterize pairs of\nsongs as being plagiarized or not. Our experiments on a database of about 3000\nmusical track pairs show that the new feature space characterization produces\nsignificant improvements over standard baselines.\n", "versions": [{"version": "v1", "created": "Fri, 27 Feb 2015 21:57:16 GMT"}], "update_date": "2016-06-08", "authors_parsed": [["De", "Soham", ""], ["Roy", "Indradyumna", ""], ["Prabhakar", "Tarunima", ""], ["Suneja", "Kriti", ""], ["Chaudhuri", "Sourish", ""], ["Singh", "Rita", ""], ["Raj", "Bhiksha", ""]]}, {"id": "1503.00036", "submitter": "Behnam Neyshabur", "authors": "Behnam Neyshabur, Ryota Tomioka, Nathan Srebro", "title": "Norm-Based Capacity Control in Neural Networks", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the capacity, convexity and characterization of a general\nfamily of norm-constrained feed-forward networks.\n", "versions": [{"version": "v1", "created": "Fri, 27 Feb 2015 23:50:22 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2015 22:55:08 GMT"}], "update_date": "2015-04-16", "authors_parsed": [["Neyshabur", "Behnam", ""], ["Tomioka", "Ryota", ""], ["Srebro", "Nathan", ""]]}, {"id": "1503.00038", "submitter": "Md Amran Siddiqui", "authors": "Md Amran Siddiqui, Alan Fern, Thomas G. Dietterich and Weng-Keen Wong", "title": "Sequential Feature Explanations for Anomaly Detection", "comments": "9 pages, 4 figures and submitted to KDD 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications, an anomaly detection system presents the most anomalous\ndata instance to a human analyst, who then must determine whether the instance\nis truly of interest (e.g. a threat in a security setting). Unfortunately, most\nanomaly detectors provide no explanation about why an instance was considered\nanomalous, leaving the analyst with no guidance about where to begin the\ninvestigation. To address this issue, we study the problems of computing and\nevaluating sequential feature explanations (SFEs) for anomaly detectors. An SFE\nof an anomaly is a sequence of features, which are presented to the analyst one\nat a time (in order) until the information contained in the highlighted\nfeatures is enough for the analyst to make a confident judgement about the\nanomaly. Since analyst effort is related to the amount of information that they\nconsider in an investigation, an explanation's quality is related to the number\nof features that must be revealed to attain confidence. One of our main\ncontributions is to present a novel framework for large scale quantitative\nevaluations of SFEs, where the quality measure is based on analyst effort. To\ndo this we construct anomaly detection benchmarks from real data sets along\nwith artificial experts that can be simulated for evaluation. Our second\ncontribution is to evaluate several novel explanation approaches within the\nframework and on traditional anomaly detection benchmarks, offering several\ninsights into the approaches.\n", "versions": [{"version": "v1", "created": "Sat, 28 Feb 2015 00:04:11 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Siddiqui", "Md Amran", ""], ["Fern", "Alan", ""], ["Dietterich", "Thomas G.", ""], ["Wong", "Weng-Keen", ""]]}, {"id": "1503.00075", "submitter": "Kai Sheng Tai", "authors": "Kai Sheng Tai, Richard Socher, Christopher D. Manning", "title": "Improved Semantic Representations From Tree-Structured Long Short-Term\n  Memory Networks", "comments": "Accepted for publication at ACL 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because of their superior ability to preserve sequence information over time,\nLong Short-Term Memory (LSTM) networks, a type of recurrent neural network with\na more complex computational unit, have obtained strong results on a variety of\nsequence modeling tasks. The only underlying LSTM structure that has been\nexplored so far is a linear chain. However, natural language exhibits syntactic\nproperties that would naturally combine words to phrases. We introduce the\nTree-LSTM, a generalization of LSTMs to tree-structured network topologies.\nTree-LSTMs outperform all existing systems and strong LSTM baselines on two\ntasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task\n1) and sentiment classification (Stanford Sentiment Treebank).\n", "versions": [{"version": "v1", "created": "Sat, 28 Feb 2015 06:31:50 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2015 20:13:25 GMT"}, {"version": "v3", "created": "Sat, 30 May 2015 06:51:20 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Tai", "Kai Sheng", ""], ["Socher", "Richard", ""], ["Manning", "Christopher D.", ""]]}, {"id": "1503.00082", "submitter": "Weiyao Lin", "authors": "Weiyao Lin, Ming-Ting Sun, Radha Poovendran, Zhengyou Zhang", "title": "Group Event Detection with a Varying Number of Group Members for Video\n  Surveillance", "comments": "This manuscript is the accepted version for TCSVT (IEEE Transactions\n  on Circuits and Systems for Video Technology)", "journal-ref": "IEEE Trans. Circuits and Systems for Video Technology, vol. 20,\n  no. 8, pp. 1057-1067, 2010", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel approach for automatic recognition of group\nactivities for video surveillance applications. We propose to use a group\nrepresentative to handle the recognition with a varying number of group\nmembers, and use an Asynchronous Hidden Markov Model (AHMM) to model the\nrelationship between people. Furthermore, we propose a group activity detection\nalgorithm which can handle both symmetric and asymmetric group activities, and\ndemonstrate that this approach enables the detection of hierarchical\ninteractions between people. Experimental results show the effectiveness of our\napproach.\n", "versions": [{"version": "v1", "created": "Sat, 28 Feb 2015 06:51:39 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Lin", "Weiyao", ""], ["Sun", "Ming-Ting", ""], ["Poovendran", "Radha", ""], ["Zhang", "Zhengyou", ""]]}, {"id": "1503.00185", "submitter": "Jiwei Li", "authors": "Jiwei Li, Minh-Thang Luong, Dan Jurafsky and Eudard Hovy", "title": "When Are Tree Structures Necessary for Deep Learning of Representations?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recursive neural models, which use syntactic parse trees to recursively\ngenerate representations bottom-up, are a popular architecture. But there have\nnot been rigorous evaluations showing for exactly which tasks this syntax-based\nmethod is appropriate. In this paper we benchmark {\\bf recursive} neural models\nagainst sequential {\\bf recurrent} neural models (simple recurrent and LSTM\nmodels), enforcing apples-to-apples comparison as much as possible. We\ninvestigate 4 tasks: (1) sentiment classification at the sentence level and\nphrase level; (2) matching questions to answer-phrases; (3) discourse parsing;\n(4) semantic relation extraction (e.g., {\\em component-whole} between nouns).\n  Our goal is to understand better when, and why, recursive models can\noutperform simpler models. We find that recursive models help mainly on tasks\n(like semantic relation extraction) that require associating headwords across a\nlong distance, particularly on very long sequences. We then introduce a method\nfor allowing recurrent models to achieve similar performance: breaking long\nsentences into clause-like units at punctuation and processing them separately\nbefore combining. Our results thus help understand the limitations of both\nclasses of models, and suggest directions for improving recurrent models.\n", "versions": [{"version": "v1", "created": "Sat, 28 Feb 2015 21:39:31 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2015 18:16:50 GMT"}, {"version": "v3", "created": "Fri, 24 Apr 2015 17:14:49 GMT"}, {"version": "v4", "created": "Thu, 18 Jun 2015 22:07:45 GMT"}, {"version": "v5", "created": "Tue, 18 Aug 2015 05:59:18 GMT"}], "update_date": "2015-08-19", "authors_parsed": [["Li", "Jiwei", ""], ["Luong", "Minh-Thang", ""], ["Jurafsky", "Dan", ""], ["Hovy", "Eudard", ""]]}, {"id": "1503.00237", "submitter": "Aryo Jamshidpey", "authors": "Aryo Jamshidpey and Mohsen Afsharchi", "title": "Task Allocation in Robotic Swarms: Explicit Communication Based\n  Approaches", "comments": "A short version of this paper is accepted by AI2015(conference). It\n  has 13 pages and 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study multi robot cooperative task allocation issue in a\nsituation where a swarm of robots is deployed in a confined unknown environment\nwhere the number of colored spots which represent tasks and the ratios of them\nare unknown. The robots should cover this spots as far as possible to do\ncleaning and sampling actions desirably. It means that they should discover the\nspots cooperatively and spread proportional to the spots area and avoid from\nremaining idle. We proposed 4 self-organized distributed methods which are\ncalled hybrid methods for coping with this scenario. In two different\nexperiments the performance of the methods is analyzed. We compared them with\neach other and investigated their scalability and robustness in term of single\npoint of failure.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2015 08:33:28 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Jamshidpey", "Aryo", ""], ["Afsharchi", "Mohsen", ""]]}, {"id": "1503.00244", "submitter": "Kamran Kowsari", "authors": "Nima Bari, Roman Vichr, Kamran Kowsari, Simon Y. Berkovich", "title": "23-bit Metaknowledge Template Towards Big Data Knowledge Discovery and\n  Management", "comments": "IEEE Data Science and Advanced Analytics (DSAA'2014)", "journal-ref": null, "doi": "10.1109/DSAA.2014.7058121", "report-no": null, "categories": "cs.DB cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The global influence of Big Data is not only growing but seemingly endless.\nThe trend is leaning towards knowledge that is attained easily and quickly from\nmassive pools of Big Data. Today we are living in the technological world that\nDr. Usama Fayyad and his distinguished research fellows discussed in the\nintroductory explanations of Knowledge Discovery in Databases (KDD) predicted\nnearly two decades ago. Indeed, they were precise in their outlook on Big Data\nanalytics. In fact, the continued improvement of the interoperability of\nmachine learning, statistics, database building and querying fused to create\nthis increasingly popular science- Data Mining and Knowledge Discovery. The\nnext generation computational theories are geared towards helping to extract\ninsightful knowledge from even larger volumes of data at higher rates of speed.\nAs the trend increases in popularity, the need for a highly adaptive solution\nfor knowledge discovery will be necessary. In this research paper, we are\nintroducing the investigation and development of 23 bit-questions for a\nMetaknowledge template for Big Data Processing and clustering purposes. This\nresearch aims to demonstrate the construction of this methodology and proves\nthe validity and the beneficial utilization that brings Knowledge Discovery\nfrom Big Data.\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2015 09:41:11 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Bari", "Nima", ""], ["Vichr", "Roman", ""], ["Kowsari", "Kamran", ""], ["Berkovich", "Simon Y.", ""]]}, {"id": "1503.00245", "submitter": "Kamran Kowsari", "authors": "Nima Bari, Roman Vichr, Kamran Kowsari, Simon Y. Berkovich", "title": "Novel Metaknowledge-based Processing Technique for Multimedia Big Data\n  clustering challenges", "comments": "IEEE Multimedia Big Data (BigMM 2015)", "journal-ref": null, "doi": "10.1109/BigMM.2015.78", "report-no": null, "categories": "cs.DB cs.AI cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Past research has challenged us with the task of showing relational patterns\nbetween text-based data and then clustering for predictive analysis using Golay\nCode technique. We focus on a novel approach to extract metaknowledge in\nmultimedia datasets. Our collaboration has been an on-going task of studying\nthe relational patterns between datapoints based on metafeatures extracted from\nmetaknowledge in multimedia datasets. Those selected are significant to suit\nthe mining technique we applied, Golay Code algorithm. In this research paper\nwe summarize findings in optimization of metaknowledge representation for\n23-bit representation of structured and unstructured multimedia data in order\nto\n", "versions": [{"version": "v1", "created": "Sun, 1 Mar 2015 09:53:15 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Bari", "Nima", ""], ["Vichr", "Roman", ""], ["Kowsari", "Kamran", ""], ["Berkovich", "Simon Y.", ""]]}, {"id": "1503.00806", "submitter": "Joseph Y. Halpern", "authors": "Hans van Ditmarsch and Joseph Y. Halpern and Wiebe van der Hoek and\n  Barteld Kooi", "title": "An Introduction to Logics of Knowledge and Belief", "comments": "FIrst chapter of \"Handbook of Epistemic Logic\", by Hans van\n  Ditmarsch, Joseph Y. Halpern, Wiebe van der Hoek, and Barteld Kooi", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This chapter provides an introduction to some basic concepts of epistemic\nlogic, basic formal languages, their semantics, and proof systems. It also\ncontains an overview of the handbook, and a brief history of epistemic logic\nand pointers to the literature.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 02:20:43 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["van Ditmarsch", "Hans", ""], ["Halpern", "Joseph Y.", ""], ["van der Hoek", "Wiebe", ""], ["Kooi", "Barteld", ""]]}, {"id": "1503.00841", "submitter": "Biao Liu", "authors": "Biao Liu, Minlie Huang", "title": "Robustly Leveraging Prior Knowledge in Text Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Prior knowledge has been shown very useful to address many natural language\nprocessing tasks. Many approaches have been proposed to formalise a variety of\nknowledge, however, whether the proposed approach is robust or sensitive to the\nknowledge supplied to the model has rarely been discussed. In this paper, we\npropose three regularization terms on top of generalized expectation criteria,\nand conduct extensive experiments to justify the robustness of the proposed\nmethods. Experimental results demonstrate that our proposed methods obtain\nremarkable improvements and are much more robust than baselines.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 06:59:28 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["Liu", "Biao", ""], ["Huang", "Minlie", ""]]}, {"id": "1503.00899", "submitter": "Raka Jovanovic", "authors": "Raka Jovanovic, Milan Tuba, Stefan Voss", "title": "An Ant Colony Optimization Algorithm for Partitioning Graphs with Supply\n  and Demand", "comments": null, "journal-ref": null, "doi": "10.1016/j.asoc.2016.01.013", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we focus on finding high quality solutions for the problem of\nmaximum partitioning of graphs with supply and demand (MPGSD). There is a\ngrowing interest for the MPGSD due to its close connection to problems\nappearing in the field of electrical distribution systems, especially for the\noptimization of self-adequacy of interconnected microgrids. We propose an ant\ncolony optimization algorithm for the problem. With the goal of further\nimproving the algorithm we combine it with a previously developed correction\nprocedure. In our computational experiments we evaluate the performance of the\nproposed algorithm on both trees and general graphs. The tests show that the\nmethod manages to find optimal solutions in more than 50% of the problem\ninstances, and has an average relative error of less than 0.5% when compared to\nknown optimal solutions.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 11:26:02 GMT"}], "update_date": "2016-02-01", "authors_parsed": [["Jovanovic", "Raka", ""], ["Tuba", "Milan", ""], ["Voss", "Stefan", ""]]}, {"id": "1503.00980", "submitter": "Jin-Kao Hao", "authors": "Xiangjing Lai and Jin-Kao Hao", "title": "On memetic search for the max-mean dispersion problem", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a set $V$ of $n$ elements and a distance matrix $[d_{ij}]_{n\\times n}$\namong elements, the max-mean dispersion problem (MaxMeanDP) consists in\nselecting a subset $M$ from $V$ such that the mean dispersion (or distance)\namong the selected elements is maximized. Being a useful model to formulate\nseveral relevant applications, MaxMeanDP is known to be NP-hard and thus\ncomputationally difficult. In this paper, we present a highly effective memetic\nalgorithm for MaxMeanDP which relies on solution recombination and local\noptimization to find high quality solutions. Computational experiments on the\nset of 160 benchmark instances with up to 1000 elements commonly used in the\nliterature show that the proposed algorithm improves or matches the published\nbest known results for all instances in a short computing time, with only one\nexception, while achieving a high success rate of 100\\%. In particular, we\nimprove 59 previous best results out of the 60 most challenging instances.\nResults on a set of 40 new large instances with 3000 and 5000 elements are also\npresented. The key ingredients of the proposed algorithm are investigated to\nshed light on how they affect the performance of the algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 15:43:36 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["Lai", "Xiangjing", ""], ["Hao", "Jin-Kao", ""]]}, {"id": "1503.01051", "submitter": "Sander Beckers", "authors": "Sander Beckers and Joost Vennekens", "title": "Combining Probabilistic, Causal, and Normative Reasoning in CP-logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years the search for a proper formal definition of actual causation\n-- i.e., the relation of cause-effect as it is instantiated in specific\nobservations, rather than general causal relations -- has taken on impressive\nproportions. In part this is due to the insight that this concept plays a\nfundamental role in many different fields, such as legal theory, engineering,\nmedicine, ethics, etc. Because of this diversity in applications, some\nresearchers have shifted focus from a single idealized definition towards a\nmore pragmatic, context-based account. For instance, recent work by Halpern and\nHitchcock draws on empirical research regarding people's causal judgments, to\nsuggest a graded and context-sensitive notion of causation. Although we\nsympathize with many of their observations, their restriction to a merely\nqualitative ordering runs into trouble for more complex examples. Therefore we\naim to improve on their approach, by using the formal language of CP-logic\n(Causal Probabilistic logic), and the framework for defining actual causation\nthat was developed by the current authors using it. First we rephrase their\nideas into our quantitative, probabilistic setting, after which we modify it to\naccommodate a greater class of examples. Further, we introduce a formal\ndistinction between statistical and normative considerations.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 18:50:40 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["Beckers", "Sander", ""], ["Vennekens", "Joost", ""]]}, {"id": "1503.01070", "submitter": "Atousa Torabi", "authors": "Atousa Torabi, Christopher Pal, Hugo Larochelle, Aaron Courville", "title": "Using Descriptive Video Services to Create a Large Data Source for Video\n  Annotation Research", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we introduce a dataset of video annotated with high quality\nnatural language phrases describing the visual content in a given segment of\ntime. Our dataset is based on the Descriptive Video Service (DVS) that is now\nencoded on many digital media products such as DVDs. DVS is an audio narration\ndescribing the visual elements and actions in a movie for the visually\nimpaired. It is temporally aligned with the movie and mixed with the original\nmovie soundtrack. We describe an automatic DVS segmentation and alignment\nmethod for movies, that enables us to scale up the collection of a DVS-derived\ndataset with minimal human intervention. Using this method, we have collected\nthe largest DVS-derived dataset for video description of which we are aware.\nOur dataset currently includes over 84.6 hours of paired video/sentences from\n92 DVDs and is growing.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 19:22:01 GMT"}], "update_date": "2015-03-04", "authors_parsed": [["Torabi", "Atousa", ""], ["Pal", "Christopher", ""], ["Larochelle", "Hugo", ""], ["Courville", "Aaron", ""]]}, {"id": "1503.01158", "submitter": "Andrew Emmott", "authors": "Andrew Emmott, Shubhomoy Das, Thomas Dietterich, Alan Fern and\n  Weng-Keen Wong", "title": "A Meta-Analysis of the Anomaly Detection Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This article provides a thorough meta-analysis of the anomaly detection\nproblem. To accomplish this we first identify approaches to benchmarking\nanomaly detection algorithms across the literature and produce a large corpus\nof anomaly detection benchmarks that vary in their construction across several\ndimensions we deem important to real-world applications: (a) point difficulty,\n(b) relative frequency of anomalies, (c) clusteredness of anomalies, and (d)\nrelevance of features. We apply a representative set of anomaly detection\nalgorithms to this corpus, yielding a very large collection of experimental\nresults. We analyze these results to understand many phenomena observed in\nprevious work. First we observe the effects of experimental design on\nexperimental results. Second, results are evaluated with two metrics, ROC Area\nUnder the Curve and Average Precision. We employ statistical hypothesis testing\nto demonstrate the value (or lack thereof) of our benchmarks. We then offer\nseveral approaches to summarizing our experimental results, drawing several\nconclusions about the impact of our methodology as well as the strengths and\nweaknesses of some algorithms. Last, we compare results against a trivial\nsolution as an alternate means of normalizing the reported performance of\nalgorithms. The intended contributions of this article are many; in addition to\nproviding a large publicly-available corpus of anomaly detection benchmarks, we\nprovide an ontology for describing anomaly detection contexts, a methodology\nfor controlling various aspects of benchmark creation, guidelines for future\nexperimental design and a discussion of the many potential pitfalls of trying\nto measure success in this field.\n", "versions": [{"version": "v1", "created": "Tue, 3 Mar 2015 23:07:37 GMT"}, {"version": "v2", "created": "Fri, 26 Aug 2016 06:26:36 GMT"}], "update_date": "2016-08-29", "authors_parsed": [["Emmott", "Andrew", ""], ["Das", "Shubhomoy", ""], ["Dietterich", "Thomas", ""], ["Fern", "Alan", ""], ["Wong", "Weng-Keen", ""]]}, {"id": "1503.01288", "submitter": "Jaume Jord\\'an", "authors": "Jaume Jord\\'an, Eva Onaindia", "title": "Game-theoretic Approach for Non-Cooperative Planning", "comments": "Twenty-Ninth AAAI Conference on Artificial Intelligence (AAAI-15)\n  2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When two or more self-interested agents put their plans to execution in the\nsame environment, conflicts may arise as a consequence, for instance, of a\ncommon utilization of resources. In this case, an agent can postpone the\nexecution of a particular action, if this punctually solves the conflict, or it\ncan resort to execute a different plan if the agent's payoff significantly\ndiminishes due to the action deferral. In this paper, we present a\ngame-theoretic approach to non-cooperative planning that helps predict before\nexecution what plan schedules agents will adopt so that the set of strategies\nof all agents constitute a Nash equilibrium. We perform some experiments and\ndiscuss the solutions obtained with our game-theoretical approach, analyzing\nhow the conflicts between the plans determine the strategic behavior of the\nagents.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 11:46:33 GMT"}], "update_date": "2015-03-05", "authors_parsed": [["Jord\u00e1n", "Jaume", ""], ["Onaindia", "Eva", ""]]}, {"id": "1503.01299", "submitter": "Naji Shajarisales", "authors": "Naji Shajarisales, Dominik Janzing, Bernhard Shoelkopf, Michel\n  Besserve", "title": "Telling cause from effect in deterministic linear dynamical systems", "comments": "This article is under review for a peer-reviewed conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inferring a cause from its effect using observed time series data is a major\nchallenge in natural and social sciences. Assuming the effect is generated by\nthe cause trough a linear system, we propose a new approach based on the\nhypothesis that nature chooses the \"cause\" and the \"mechanism that generates\nthe effect from the cause\" independent of each other. We therefore postulate\nthat the power spectrum of the time series being the cause is uncorrelated with\nthe square of the transfer function of the linear filter generating the effect.\nWhile most causal discovery methods for time series mainly rely on the noise,\nour method relies on asymmetries of the power spectral density properties that\ncan be exploited even in the context of deterministic systems. We describe\nmathematical assumptions in a deterministic model under which the causal\ndirection is identifiable with this approach. We also discuss the method's\nperformance under the additive noise model and its relationship to Granger\ncausality. Experiments show encouraging results on synthetic as well as\nreal-world data. Overall, this suggests that the postulate of Independence of\nCause and Mechanism is a promising principle for causal inference on empirical\ntime series.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 12:48:44 GMT"}], "update_date": "2015-03-05", "authors_parsed": [["Shajarisales", "Naji", ""], ["Janzing", "Dominik", ""], ["Shoelkopf", "Bernhard", ""], ["Besserve", "Michel", ""]]}, {"id": "1503.01327", "submitter": "Liat Cohen", "authors": "Liat Cohen, Solomon Eyal Shimony, Gera Weiss", "title": "Estimating the Probability of Meeting a Deadline in Hierarchical Plans", "comments": "A jornal version of an IJCAI-2015 paper: \"Estimating the Probability\n  of Meeting a Deadline in Hierarchical Plans\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given a hierarchical plan (or schedule) with uncertain task times, we propose\na deterministic polynomial (time and memory) algorithm for estimating the\nprobability that its meets a deadline, or, alternately, that its {\\em makespan}\nis less than a given duration. Approximation is needed as it is known that this\nproblem is NP-hard even for sequential plans (just, a sum of random variables).\nIn addition, we show two new complexity results: (1) Counting the number of\nevents that do not cross deadline is \\#P-hard; (2)~Computing the expected\nmakespan of a hierarchical plan is NP-hard. For the proposed approximation\nalgorithm, we establish formal approximation bounds and show that the time and\nmemory complexities grow polynomially with the required accuracy, the number of\nnodes in the plan, and with the size of the support of the random variables\nthat represent the durations of the primitive tasks. We examine these\napproximation bounds empirically and demonstrate, using task networks taken\nfrom the literature, how our scheme outperforms sampling techniques and exact\ncomputation in terms of accuracy and run-time. As the empirical data shows much\nbetter error bounds than guaranteed, we also suggest a method for tightening\nthe bounds in some cases.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 14:56:55 GMT"}, {"version": "v2", "created": "Sun, 24 Dec 2017 19:47:45 GMT"}], "update_date": "2017-12-27", "authors_parsed": [["Cohen", "Liat", ""], ["Shimony", "Solomon Eyal", ""], ["Weiss", "Gera", ""]]}, {"id": "1503.01334", "submitter": "Davide Orsucci", "authors": "Davide Orsucci, Hans J. Briegel and Vedran Dunjko", "title": "Faster quantum mixing for slowly evolving sequences of Markov chains", "comments": "20 pages, 2 figures", "journal-ref": "Quantum 2, 105 (2018)", "doi": "10.22331/q-2018-11-09-105", "report-no": "LA-UR-17-20510", "categories": "quant-ph cs.AI cs.DS", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Markov chain methods are remarkably successful in computational physics,\nmachine learning, and combinatorial optimization. The cost of such methods\noften reduces to the mixing time, i.e., the time required to reach the steady\nstate of the Markov chain, which scales as $\\delta^{-1}$, the inverse of the\nspectral gap. It has long been conjectured that quantum computers offer nearly\ngeneric quadratic improvements for mixing problems. However, except in special\ncases, quantum algorithms achieve a run-time of $\\mathcal{O}(\\sqrt{\\delta^{-1}}\n\\sqrt{N})$, which introduces a costly dependence on the Markov chain size $N,$\nnot present in the classical case. Here, we re-address the problem of mixing of\nMarkov chains when these form a slowly evolving sequence. This setting is akin\nto the simulated annealing setting and is commonly encountered in physics,\nmaterial sciences and machine learning. We provide a quantum memory-efficient\nalgorithm with a run-time of $\\mathcal{O}(\\sqrt{\\delta^{-1}} \\sqrt[4]{N})$,\nneglecting logarithmic terms, which is an important improvement for large state\nspaces. Moreover, our algorithms output quantum encodings of distributions,\nwhich has advantages over classical outputs. Finally, we discuss the run-time\nbounds of mixing algorithms and show that, under certain assumptions, our\nalgorithms are optimal.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 15:07:07 GMT"}, {"version": "v2", "created": "Tue, 24 Apr 2018 11:37:31 GMT"}, {"version": "v3", "created": "Tue, 17 Jul 2018 15:20:37 GMT"}, {"version": "v4", "created": "Mon, 29 Oct 2018 23:33:30 GMT"}], "update_date": "2018-11-15", "authors_parsed": [["Orsucci", "Davide", ""], ["Briegel", "Hans J.", ""], ["Dunjko", "Vedran", ""]]}, {"id": "1503.01444", "submitter": "Tae-Hyun Oh", "authors": "Tae-Hyun Oh, Yu-Wing Tai, Jean-Charles Bazin, Hyeongwoo Kim, In So\n  Kweon", "title": "Partial Sum Minimization of Singular Values in Robust PCA: Algorithm and\n  Applications", "comments": "Accepted in Transactions on Pattern Analysis and Machine Intelligence\n  (TPAMI). To appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust Principal Component Analysis (RPCA) via rank minimization is a\npowerful tool for recovering underlying low-rank structure of clean data\ncorrupted with sparse noise/outliers. In many low-level vision problems, not\nonly it is known that the underlying structure of clean data is low-rank, but\nthe exact rank of clean data is also known. Yet, when applying conventional\nrank minimization for those problems, the objective function is formulated in a\nway that does not fully utilize a priori target rank information about the\nproblems. This observation motivates us to investigate whether there is a\nbetter alternative solution when using rank minimization. In this paper,\ninstead of minimizing the nuclear norm, we propose to minimize the partial sum\nof singular values, which implicitly encourages the target rank constraint. Our\nexperimental analyses show that, when the number of samples is deficient, our\napproach leads to a higher success rate than conventional rank minimization,\nwhile the solutions obtained by the two approaches are almost identical when\nthe number of samples is more than sufficient. We apply our approach to various\nlow-level vision problems, e.g. high dynamic range imaging, motion edge\ndetection, photometric stereo, image alignment and recovery, and show that our\nresults outperform those obtained by the conventional nuclear norm rank\nminimization method.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 20:14:35 GMT"}, {"version": "v2", "created": "Thu, 13 Aug 2015 12:51:08 GMT"}], "update_date": "2019-02-18", "authors_parsed": [["Oh", "Tae-Hyun", ""], ["Tai", "Yu-Wing", ""], ["Bazin", "Jean-Charles", ""], ["Kim", "Hyeongwoo", ""], ["Kweon", "In So", ""]]}, {"id": "1503.01446", "submitter": "Selene Baez Santamaria", "authors": "Selene Baez", "title": "Predicting opponent team activity in a RoboCup environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of this project is to predict the opponent's configuration in a\nRoboCup SSL environment. For simplicity, a Markov model assumption is made such\nthat the predicted formation of the opponent team only depends on its current\nformation. The field is divided into a grid and a robot state per player is\ncreated with information about its position and its velocity. To gather a more\ngeneral sense of what the opposing team is doing, the state also incorporates\nthe team's average position (centroid). All possible state transitions are\nstored in a hash table that requires minimum storage space. The table is\npopulated with transition probabilities that are learned by reading vision\npackages and counting the state transitions regardless of the specific robot\nplayer. Therefore, the computation during the game is reduced to interpreting a\ngiven vision package to assign each player to a state, and looking for the most\nlikely state it will transition to. The confidence of the predicted team's\nformation is the product of each individual player's probability. The project\nis noteworthy in that it minimizes the time and space complexity requirements\nfor opponent's moves prediction.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 20:23:21 GMT"}], "update_date": "2015-03-05", "authors_parsed": [["Baez", "Selene", ""]]}, {"id": "1503.01488", "submitter": "Hadi Hosseini", "authors": "Hadi Hosseini, Kate Larson, Robin Cohen", "title": "Random Serial Dictatorship versus Probabilistic Serial Rule: A Tale of\n  Two Random Mechanisms", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For assignment problems where agents, specifying ordinal preferences, are\nallocated indivisible objects, two widely studied randomized mechanisms are the\nRandom Serial Dictatorship (RSD) and Probabilistic Serial Rule (PS). These two\nmechanisms both have desirable economic and computational properties, but the\noutcomes they induce can be incomparable in many instances, thus creating\nchallenges in deciding which mechanism to adopt in practice. In this paper we\nfirst look at the space of lexicographic preferences and show that, as opposed\nto the general preference domain, RSD satisfies envyfreeness. Moreover, we show\nthat although under lexicographic preferences PS is strategyproof when the\nnumber of objects is less than or equal agents, it is strictly manipulable when\nthere are more objects than agents. In the space of general preferences, we\nprovide empirical results on the (in)comparability of RSD and PS, analyze\neconomic properties, and provide further insights on the applicability of each\nmechanism in different application domains.\n", "versions": [{"version": "v1", "created": "Wed, 4 Mar 2015 22:35:27 GMT"}], "update_date": "2015-03-06", "authors_parsed": [["Hosseini", "Hadi", ""], ["Larson", "Kate", ""], ["Cohen", "Robin", ""]]}, {"id": "1503.01521", "submitter": "Liwen Zhang", "authors": "Liwen Zhang, Subhransu Maji, Ryota Tomioka", "title": "Jointly Learning Multiple Measures of Similarities from Triplet\n  Comparisons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity between objects is multi-faceted and it can be easier for human\nannotators to measure it when the focus is on a specific aspect. We consider\nthe problem of mapping objects into view-specific embeddings where the distance\nbetween them is consistent with the similarity comparisons of the form \"from\nthe t-th view, object A is more similar to B than to C\". Our framework jointly\nlearns view-specific embeddings exploiting correlations between views.\nExperiments on a number of datasets, including one of multi-view crowdsourced\ncomparison on bird images, show the proposed method achieves lower triplet\ngeneralization error when compared to both learning embeddings independently\nfor each view and all views pooled into one view. Our method can also be used\nto learn multiple measures of similarity over input features taking class\nlabels into account and compares favorably to existing approaches for\nmulti-task metric learning on the ISOLET dataset.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 02:57:19 GMT"}, {"version": "v2", "created": "Fri, 6 Mar 2015 20:09:09 GMT"}, {"version": "v3", "created": "Tue, 6 Oct 2015 21:42:55 GMT"}], "update_date": "2015-10-08", "authors_parsed": [["Zhang", "Liwen", ""], ["Maji", "Subhransu", ""], ["Tomioka", "Ryota", ""]]}, {"id": "1503.01603", "submitter": "Judea Pearl", "authors": "Judea Pearl, Elias Bareinboim", "title": "External Validity: From Do-Calculus to Transportability Across\n  Populations", "comments": "Published in at http://dx.doi.org/10.1214/14-STS486 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org). arXiv admin note: text overlap with\n  arXiv:1312.7485", "journal-ref": "Statistical Science 2014, Vol. 29, No. 4, 579-595", "doi": "10.1214/14-STS486", "report-no": "IMS-STS-STS486", "categories": "stat.ME cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The generalizability of empirical findings to new environments, settings or\npopulations, often called \"external validity,\" is essential in most scientific\nexplorations. This paper treats a particular problem of generalizability,\ncalled \"transportability,\" defined as a license to transfer causal effects\nlearned in experimental studies to a new population, in which only\nobservational studies can be conducted. We introduce a formal representation\ncalled \"selection diagrams\" for expressing knowledge about differences and\ncommonalities between populations of interest and, using this representation,\nwe reduce questions of transportability to symbolic derivations in the\ndo-calculus. This reduction yields graph-based procedures for deciding, prior\nto observing any data, whether causal effects in the target population can be\ninferred from experimental findings in the study population. When the answer is\naffirmative, the procedures identify what experimental and observational\nfindings need be obtained from the two populations, and how they can be\ncombined to ensure bias-free transport.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 10:58:30 GMT"}], "update_date": "2015-03-06", "authors_parsed": [["Pearl", "Judea", ""], ["Bareinboim", "Elias", ""]]}, {"id": "1503.01707", "submitter": "Jan Van den Bussche", "authors": "Angela Bonifati, Werner Nutt, Riccardo Torlone, Jan Van den Bussche", "title": "Mapping-equivalence and oid-equivalence of single-function\n  object-creating conjunctive queries", "comments": "This revised version has been accepted on 11 January 2016 for\n  publication in The VLDB Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conjunctive database queries have been extended with a mechanism for object\ncreation to capture important applications such as data exchange, data\nintegration, and ontology-based data access. Object creation generates new\nobject identifiers in the result, that do not belong to the set of constants in\nthe source database. The new object identifiers can be also seen as Skolem\nterms. Hence, object-creating conjunctive queries can also be regarded as\nrestricted second-order tuple-generating dependencies (SO tgds), considered in\nthe data exchange literature.\n  In this paper, we focus on the class of single-function object-creating\nconjunctive queries, or sifo CQs for short. We give a new characterization for\noid-equivalence of sifo CQs that is simpler than the one given by Hull and\nYoshikawa and places the problem in the complexity class NP. Our\ncharacterization is based on Cohen's equivalence notions for conjunctive\nqueries with multiplicities. We also solve the logical entailment problem for\nsifo CQs, showing that also this problem belongs to NP. Results by Pichler et\nal. have shown that logical equivalence for more general classes of SO tgds is\neither undecidable or decidable with as yet unknown complexity upper bounds.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 17:47:04 GMT"}, {"version": "v2", "created": "Tue, 12 Jan 2016 14:59:40 GMT"}], "update_date": "2016-01-13", "authors_parsed": [["Bonifati", "Angela", ""], ["Nutt", "Werner", ""], ["Torlone", "Riccardo", ""], ["Bussche", "Jan Van den", ""]]}, {"id": "1503.01820", "submitter": "Ninghang Hu", "authors": "Ninghang Hu, Gwenn Englebienne, Zhongyu Lou, and Ben Kr\\\"ose", "title": "Latent Hierarchical Model for Activity Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel hierarchical model for human activity recognition. In\ncontrast to approaches that successively recognize actions and activities, our\napproach jointly models actions and activities in a unified framework, and\ntheir labels are simultaneously predicted. The model is embedded with a latent\nlayer that is able to capture a richer class of contextual information in both\nstate-state and observation-state pairs. Although loops are present in the\nmodel, the model has an overall linear-chain structure, where the exact\ninference is tractable. Therefore, the model is very efficient in both\ninference and learning. The parameters of the graphical model are learned with\na Structured Support Vector Machine (Structured-SVM). A data-driven approach is\nused to initialize the latent variables; therefore, no manual labeling for the\nlatent states is required. The experimental results from using two benchmark\ndatasets show that our model outperforms the state-of-the-art approach, and our\nmodel is computationally more efficient.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2015 00:05:12 GMT"}], "update_date": "2015-03-09", "authors_parsed": [["Hu", "Ninghang", ""], ["Englebienne", "Gwenn", ""], ["Lou", "Zhongyu", ""], ["Kr\u00f6se", "Ben", ""]]}, {"id": "1503.01910", "submitter": "Vijay Kamble", "authors": "Vijay Kamble, Nadia Fawaz, Fernando Silveira", "title": "Sequential Relevance Maximization with Binary Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by online settings where users can provide explicit feedback about\nthe relevance of products that are sequentially presented to them, we look at\nthe recommendation process as a problem of dynamically optimizing this\nrelevance feedback. Such an algorithm optimizes the fine tradeoff between\npresenting the products that are most likely to be relevant, and learning the\npreferences of the user so that more relevant recommendations can be made in\nthe future.\n  We assume a standard predictive model inspired by collaborative filtering, in\nwhich a user is sampled from a distribution over a set of possible types. For\nevery product category, each type has an associated relevance feedback that is\nassumed to be binary: the category is either relevant or irrelevant. Assuming\nthat the user stays for each additional recommendation opportunity with\nprobability $\\beta$ independent of the past, the problem is to find a policy\nthat maximizes the expected number of recommendations that are deemed relevant\nin a session.\n  We analyze this problem and prove key structural properties of the optimal\npolicy. Based on these properties, we first present an algorithm that strikes a\nbalance between recursion and dynamic programming to compute this policy. We\nfurther propose and analyze two heuristic policies: a `farsighted' greedy\npolicy that attains at least $1-\\beta$ factor of the optimal payoff, and a\nnaive greedy policy that attains at least $\\frac{1-\\beta}{1+\\beta}$ factor of\nthe optimal payoff in the worst case. Extensive simulations show that these\nheuristics are very close to optimal in practice.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2015 11:02:41 GMT"}], "update_date": "2015-03-09", "authors_parsed": [["Kamble", "Vijay", ""], ["Fawaz", "Nadia", ""], ["Silveira", "Fernando", ""]]}, {"id": "1503.01967", "submitter": "Panteleimon Rodis", "authors": "Panteleimon Rodis", "title": "Information entropy as an anthropomorphic concept", "comments": "Improvements in mathematical definitions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  According to E.T. Jaynes and E.P. Wigner, entropy is an anthropomorphic\nconcept in the sense that in a physical system correspond many thermodynamic\nsystems. The physical system can be examined from many points of view each time\nexamining different variables and calculating entropy differently. In this\npaper we discuss how this concept may be applied in information entropy; how\nShannon's definition of entropy can fit in Jayne's and Wigner's statement. This\nis achieved by generalizing Shannon's notion of information entropy and this is\nthe main contribution of the paper. Then we discuss how entropy under these\nconsiderations may be used for the comparison of password complexity and as a\nmeasure of diversity useful in the analysis of the behavior of genetic\nalgorithms.\n", "versions": [{"version": "v1", "created": "Fri, 6 Mar 2015 14:27:35 GMT"}, {"version": "v2", "created": "Tue, 1 Dec 2015 22:14:30 GMT"}], "update_date": "2015-12-03", "authors_parsed": [["Rodis", "Panteleimon", ""]]}, {"id": "1503.02009", "submitter": "Sergio Consoli", "authors": "Sergio Consoli, Jos\\`e Andr\\`es Moreno P\\`erez, and Nenad Mladenovic", "title": "Towards an intelligent VNS heuristic for the k-labelled spanning forest\n  problem", "comments": "2 pages, Fifteenth International Conference on Computer Aided Systems\n  Theory (EUROCAST 2015), Las Palmas de Gran Canaria, Spain", "journal-ref": "Computer Aided Systems Theory, pages 79-80 (2015)", "doi": null, "report-no": null, "categories": "cs.OH cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a currently ongoing project, we investigate a new possibility for solving\nthe k-labelled spanning forest (kLSF) problem by an intelligent Variable\nNeighbourhood Search (Int-VNS) metaheuristic. In the kLSF problem we are given\nan undirected input graph G and an integer positive value k, and the aim is to\nfind a spanning forest of G having the minimum number of connected components\nand the upper bound k on the number of labels to use. The problem is related to\nthe minimum labelling spanning tree (MLST) problem, whose goal is to get the\nspanning tree of the input graph with the minimum number of labels, and has\nseveral applications in the real world, where one aims to ensure connectivity\nby means of homogeneous connections. The Int-VNS metaheuristic that we propose\nfor the kLSF problem is derived from the promising intelligent VNS strategy\nrecently proposed for the MLST problem, and integrates the basic VNS for the\nkLSF problem with other complementary approaches from machine learning,\nstatistics and experimental algorithmics, in order to produce high-quality\nperformance and to completely automate the resulting strategy.\n", "versions": [{"version": "v1", "created": "Thu, 5 Mar 2015 14:10:19 GMT"}], "update_date": "2015-03-09", "authors_parsed": [["Consoli", "Sergio", ""], ["P\u00e8rez", "Jos\u00e8 Andr\u00e8s Moreno", ""], ["Mladenovic", "Nenad", ""]]}, {"id": "1503.02128", "submitter": "Qingming Tang", "authors": "Qingming Tang, Chao Yang, Jian Peng and Jinbo Xu", "title": "Exact Hybrid Covariance Thresholding for Joint Graphical Lasso", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of estimating multiple related Gaussian\ngraphical models from a $p$-dimensional dataset consisting of different\nclasses. Our work is based upon the formulation of this problem as group\ngraphical lasso. This paper proposes a novel hybrid covariance thresholding\nalgorithm that can effectively identify zero entries in the precision matrices\nand split a large joint graphical lasso problem into small subproblems. Our\nhybrid covariance thresholding method is superior to existing uniform\nthresholding methods in that our method can split the precision matrix of each\nindividual class using different partition schemes and thus split group\ngraphical lasso into much smaller subproblems, each of which can be solved very\nfast. In addition, this paper establishes necessary and sufficient conditions\nfor our hybrid covariance thresholding algorithm. The superior performance of\nour thresholding method is thoroughly analyzed and illustrated by a few\nexperiments on simulated data and real gene expression data.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2015 03:34:48 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2015 02:52:51 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Tang", "Qingming", ""], ["Yang", "Chao", ""], ["Peng", "Jian", ""], ["Xu", "Jinbo", ""]]}, {"id": "1503.02129", "submitter": "Qingming Tang", "authors": "Qingming Tang, Siqi Sun, and Jinbo Xu", "title": "Learning Scale-Free Networks by Dynamic Node-Specific Degree Prior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning the network structure underlying data is an important problem in\nmachine learning. This paper introduces a novel prior to study the inference of\nscale-free networks, which are widely used to model social and biological\nnetworks. The prior not only favors a desirable global node degree\ndistribution, but also takes into consideration the relative strength of all\nthe possible edges adjacent to the same node and the estimated degree of each\nindividual node.\n  To fulfill this, ranking is incorporated into the prior, which makes the\nproblem challenging to solve. We employ an ADMM (alternating direction method\nof multipliers) framework to solve the Gaussian Graphical model regularized by\nthis prior. Our experiments on both synthetic and real data show that our prior\nnot only yields a scale-free network, but also produces many more correctly\npredicted edges than the others such as the scale-free inducing prior, the\nhub-inducing prior and the $l_1$ norm.\n", "versions": [{"version": "v1", "created": "Sat, 7 Mar 2015 03:35:26 GMT"}, {"version": "v2", "created": "Thu, 19 Mar 2015 04:13:31 GMT"}, {"version": "v3", "created": "Thu, 18 Jun 2015 03:37:44 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Tang", "Qingming", ""], ["Sun", "Siqi", ""], ["Xu", "Jinbo", ""]]}, {"id": "1503.02364", "submitter": "Lifeng Shang", "authors": "Lifeng Shang, Zhengdong Lu, Hang Li", "title": "Neural Responding Machine for Short-Text Conversation", "comments": "accepted as a full paper at ACL 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Neural Responding Machine (NRM), a neural network-based response\ngenerator for Short-Text Conversation. NRM takes the general encoder-decoder\nframework: it formalizes the generation of response as a decoding process based\non the latent representation of the input text, while both encoding and\ndecoding are realized with recurrent neural networks (RNN). The NRM is trained\nwith a large amount of one-round conversation data collected from a\nmicroblogging service. Empirical study shows that NRM can generate\ngrammatically correct and content-wise appropriate responses to over 75% of the\ninput text, outperforming state-of-the-arts in the same setting, including\nretrieval-based and SMT-based models.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 02:54:29 GMT"}, {"version": "v2", "created": "Mon, 27 Apr 2015 02:28:58 GMT"}], "update_date": "2015-04-28", "authors_parsed": [["Shang", "Lifeng", ""], ["Lu", "Zhengdong", ""], ["Li", "Hang", ""]]}, {"id": "1503.02510", "submitter": "Phong Le", "authors": "Phong Le and Willem Zuidema", "title": "Compositional Distributional Semantics with Long Short Term Memory", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are proposing an extension of the recursive neural network that makes use\nof a variant of the long short-term memory architecture. The extension allows\ninformation low in parse trees to be stored in a memory register (the `memory\ncell') and used much later higher up in the parse tree. This provides a\nsolution to the vanishing gradient problem and allows the network to capture\nlong range dependencies. Experimental results show that our composition\noutperformed the traditional neural-network composition on the Stanford\nSentiment Treebank.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 15:13:38 GMT"}, {"version": "v2", "created": "Fri, 17 Apr 2015 23:54:37 GMT"}], "update_date": "2015-04-21", "authors_parsed": [["Le", "Phong", ""], ["Zuidema", "Willem", ""]]}, {"id": "1503.02521", "submitter": "Kieran Greer Dr", "authors": "Kieran Greer", "title": "A Single-Pass Classifier for Categorical Data", "comments": null, "journal-ref": "Special Issue on: IJCSysE Recent Advances in Evolutionary and\n  Natural Computing Practice and Applications, Int. J. Computational Systems\n  Engineering, Inderscience, Vol. 3, Nos. 1/2, pp. 27 - 34, 2017", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a new method for classifying a dataset that partitions\nelements into their categories. It has relations with neural networks but a\nslightly different structure, requiring only a single pass through the\nclassifier to generate the weight sets. A grid-like structure is required as\npart of a novel idea of converting a 1-D row of real values into a 2-D\nstructure of value bands. Each cell in any band then stores a distinct set of\nweights, to represent its own importance and its relation to each output\ncategory. During classification, all of the output weight lists can be\nretrieved and summed to produce a probability for what the correct output\ncategory is. The bands possibly work like hidden layers of neurons, but they\nare variable specific, making the process orthogonal. The construction process\ncan be a single update process without iterations, making it potentially much\nfaster. It can also be compared with k-NN and may be practical for partial or\ncompetitive updating.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 15:28:32 GMT"}, {"version": "v2", "created": "Tue, 14 Apr 2015 16:32:44 GMT"}, {"version": "v3", "created": "Wed, 14 Oct 2015 18:06:44 GMT"}, {"version": "v4", "created": "Wed, 29 Jun 2016 10:40:50 GMT"}], "update_date": "2019-01-04", "authors_parsed": [["Greer", "Kieran", ""]]}, {"id": "1503.02578", "submitter": "Mahdi Khademian", "authors": "Mahdi Khademian, Mohammad Mehdi Homayounpour", "title": "Modeling State-Conditional Observation Distribution using Weighted\n  Stereo Samples for Factorial Speech Processing Models", "comments": "Updated version of the first submission. Several clarifications are\n  added to previous version. One experiment is added to the experiments,\n  Circuits Syst Signal Process, Apr. 2016", "journal-ref": null, "doi": "10.1007/s00034-016-0310-y", "report-no": null, "categories": "cs.LG cs.AI cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the effectiveness of factorial speech processing\nmodels in noise-robust automatic speech recognition tasks. For this purpose,\nthe paper proposes an idealistic approach for modeling state-conditional\nobservation distribution of factorial models based on weighted stereo samples.\nThis approach is an extension to previous single pass retraining for ideal\nmodel compensation which is extended here to support multiple audio sources.\nNon-stationary noises can be considered as one of these audio sources with\nmultiple states. Experiments of this paper over the set A of the Aurora 2\ndataset show that recognition performance can be improved by this\nconsideration. The improvement is significant in low signal to noise energy\nconditions, up to 4% absolute word recognition accuracy. In addition to the\npower of the proposed method in accurate representation of state-conditional\nobservation distribution, it has an important advantage over previous methods\nby providing the opportunity to independently select feature spaces for both\nsource and corrupted features. This opens a new window for seeking better\nfeature spaces appropriate for noisy speech, independent from clean speech\nfeatures.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 17:40:08 GMT"}, {"version": "v2", "created": "Wed, 5 Oct 2016 12:05:10 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Khademian", "Mahdi", ""], ["Homayounpour", "Mohammad Mehdi", ""]]}, {"id": "1503.02626", "submitter": "David Windridge", "authors": "David Windridge", "title": "On the Intrinsic Limits to Representationally-Adaptive Machine-Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online learning is a familiar problem setting within Machine-Learning in\nwhich data is presented serially in time to a learning agent, requiring it to\nprogressively adapt within the constraints of the learning algorithm. More\nsophisticated variants may involve concepts such as transfer-learning which\nincrease this adaptive capability, enhancing the learner's cognitive capacities\nin a manner that can begin to imitate the open-ended learning capabilities of\nhuman beings.\n  We shall argue in this paper, however, that a full realization of this notion\nrequires that, in addition to the capacity to adapt to novel data, autonomous\nonline learning must ultimately incorporate the capacity to update its own\nrepresentational capabilities in relation to the data. We therefore enquire\nabout the philosophical limits of this process, and argue that only fully\nembodied learners exhibiting an a priori perception-action link in order to\nground representational adaptations are capable of exhibiting the full range of\nhuman cognitive capability.\n", "versions": [{"version": "v1", "created": "Mon, 9 Mar 2015 19:17:49 GMT"}], "update_date": "2015-03-10", "authors_parsed": [["Windridge", "David", ""]]}, {"id": "1503.02834", "submitter": "Miroslav Dud\\'{i}k", "authors": "Miroslav Dud\\'ik, Dumitru Erhan, John Langford, Lihong Li", "title": "Doubly Robust Policy Evaluation and Optimization", "comments": "Published in at http://dx.doi.org/10.1214/14-STS500 the Statistical\n  Science (http://www.imstat.org/sts/) by the Institute of Mathematical\n  Statistics (http://www.imstat.org)", "journal-ref": "Statistical Science 2014, Vol. 29, No. 4, 485-511", "doi": "10.1214/14-STS500", "report-no": "IMS-STS-STS500", "categories": "stat.ME cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study sequential decision making in environments where rewards are only\npartially observed, but can be modeled as a function of observed contexts and\nthe chosen action by the decision maker. This setting, known as contextual\nbandits, encompasses a wide variety of applications such as health care,\ncontent recommendation and Internet advertising. A central task is evaluation\nof a new policy given historic data consisting of contexts, actions and\nreceived rewards. The key challenge is that the past data typically does not\nfaithfully represent proportions of actions taken by a new policy. Previous\napproaches rely either on models of rewards or models of the past policy. The\nformer are plagued by a large bias whereas the latter have a large variance. In\nthis work, we leverage the strengths and overcome the weaknesses of the two\napproaches by applying the doubly robust estimation technique to the problems\nof policy evaluation and optimization. We prove that this approach yields\naccurate value estimates when we have either a good (but not necessarily\nconsistent) model of rewards or a good (but not necessarily consistent) model\nof past policy. Extensive empirical comparison demonstrates that the doubly\nrobust estimation uniformly improves over existing techniques, achieving both\nlower variance in value estimation and better policies. As such, we expect the\ndoubly robust approach to become common practice in policy evaluation and\noptimization.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 09:51:50 GMT"}], "update_date": "2015-03-11", "authors_parsed": [["Dud\u00edk", "Miroslav", ""], ["Erhan", "Dumitru", ""], ["Langford", "John", ""], ["Li", "Lihong", ""]]}, {"id": "1503.02917", "submitter": "Karl-Heinz Weis", "authors": "Karl-Heinz Weis", "title": "A Case Based Reasoning Approach for Answer Reranking in Question\n  Answering", "comments": "in Proceedings Informatik 2013, Koblenz, Germany, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this document I present an approach to answer validation and reranking for\nquestion answering (QA) systems. A cased-based reasoning (CBR) system judges\nanswer candidates for questions from annotated answer candidates for earlier\nquestions. The promise of this approach is that user feedback will result in\nimproved answers of the QA system, due to the growing case base. In the paper,\nI present the adequate structuring of the case base and the appropriate\nselection of relevant similarity measures, in order to solve the answer\nvalidation problem. The structural case base is built from annotated MultiNet\ngraphs, which provide representations for natural language expressions, and\ncorresponding graph similarity measures. I cover a priori relations to\nexperienced answer candidates for former questions. I compare the CBR System\nresults to current approaches in an experiment integrating CBR into an existing\nframework for answer validation and reranking. This integration is achieved by\nadding CBR-related features to the input of a learned ranking model that\ndetermines the final answer ranking. In the experiments based on QA@CLEF\nquestions, the best learned models make heavy use of CBR features. Observing\nthe results with a continually growing case base, I present a positive effect\nof the size of the case base on the accuracy of the CBR subsystem.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 14:10:47 GMT"}], "update_date": "2015-03-11", "authors_parsed": [["Weis", "Karl-Heinz", ""]]}, {"id": "1503.02994", "submitter": "Diederik Aerts", "authors": "Diederik Aerts and Sandro Sozzo", "title": "Quantum Structure in Cognition, Origins, Developments, Successes and\n  Expectations", "comments": "25 pages. arXiv admin note: text overlap with arXiv:1412.8704", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide an overview of the results we have attained in the last decade on\nthe identification of quantum structures in cognition and, more specifically,\nin the formalization and representation of natural concepts. We firstly discuss\nthe quantum foundational reasons that led us to investigate the mechanisms of\nformation and combination of concepts in human reasoning, starting from the\nempirically observed deviations from classical logical and probabilistic\nstructures. We then develop our quantum-theoretic perspective in Fock space\nwhich allows successful modeling of various sets of cognitive experiments\ncollected by different scientists, including ourselves. In addition, we\nformulate a unified explanatory hypothesis for the presence of quantum\nstructures in cognitive processes, and discuss our recent discovery of further\nquantum aspects in concept combinations, namely, 'entanglement' and\n'indistinguishability'. We finally illustrate perspectives for future research.\n", "versions": [{"version": "v1", "created": "Tue, 10 Mar 2015 17:18:10 GMT"}], "update_date": "2015-03-11", "authors_parsed": [["Aerts", "Diederik", ""], ["Sozzo", "Sandro", ""]]}, {"id": "1503.03211", "submitter": "Emmanuel Osegi", "authors": "J.O. Orove, N.E. Osegi, and B.O. Eke", "title": "A Multi-Gene Genetic Programming Application for Predicting Students\n  Failure at School", "comments": "14 pages, 9 figures, Journal paper. arXiv admin note: text overlap\n  with arXiv:1403.0623 by other authors", "journal-ref": "African Journal of Computing & ICT, 7(3), 21-34 (2015)", "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Several efforts to predict student failure rate (SFR) at school accurately\nstill remains a core problem area faced by many in the educational sector. The\nprocedure for forecasting SFR are rigid and most often times require data\nscaling or conversion into binary form such as is the case of the logistic\nmodel which may lead to lose of information and effect size attenuation. Also,\nthe high number of factors, incomplete and unbalanced dataset, and black boxing\nissues as in Artificial Neural Networks and Fuzzy logic systems exposes the\nneed for more efficient tools. Currently the application of Genetic Programming\n(GP) holds great promises and has produced tremendous positive results in\ndifferent sectors. In this regard, this study developed GPSFARPS, a software\napplication to provide a robust solution to the prediction of SFR using an\nevolutionary algorithm known as multi-gene genetic programming. The approach is\nvalidated by feeding a testing data set to the evolved GP models. Result\nobtained from GPSFARPS simulations show its unique ability to evolve a suitable\nfailure rate expression with a fast convergence at 30 generations from a\nmaximum specified generation of 500. The multi-gene system was also able to\nminimize the evolved model expression and accurately predict student failure\nrate using a subset of the original expression\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 08:17:11 GMT"}], "update_date": "2017-07-06", "authors_parsed": [["Orove", "J. O.", ""], ["Osegi", "N. E.", ""], ["Eke", "B. O.", ""]]}, {"id": "1503.03467", "submitter": "Houman Owhadi", "authors": "Houman Owhadi", "title": "Multigrid with rough coefficients and Multiresolution operator\n  decomposition from Hierarchical Information Games", "comments": "Presented at SIAM CSE 15. Final (published) version.\n  http://epubs.siam.org/doi/abs/10.1137/15M1013894", "journal-ref": "SIAM Rev. 59-1, pp. 99-149 (2017)", "doi": null, "report-no": null, "categories": "math.NA cs.AI math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a near-linear complexity (geometric and meshless/algebraic)\nmultigrid/multiresolution method for PDEs with rough ($L^\\infty$) coefficients\nwith rigorous a-priori accuracy and performance estimates. The method is\ndiscovered through a decision/game theory formulation of the problems of (1)\nidentifying restriction and interpolation operators (2) recovering a signal\nfrom incomplete measurements based on norm constraints on its image under a\nlinear operator (3) gambling on the value of the solution of the PDE based on a\nhierarchy of nested measurements of its solution or source term. The resulting\nelementary gambles form a hierarchy of (deterministic) basis functions of\n$H^1_0(\\Omega)$ (gamblets) that (1) are orthogonal across subscales/subbands\nwith respect to the scalar product induced by the energy norm of the PDE (2)\nenable sparse compression of the solution space in $H^1_0(\\Omega)$ (3) induce\nan orthogonal multiresolution operator decomposition. The operating diagram of\nthe multigrid method is that of an inverted pyramid in which gamblets are\ncomputed locally (by virtue of their exponential decay), hierarchically (from\nfine to coarse scales) and the PDE is decomposed into a hierarchy of\nindependent linear systems with uniformly bounded condition numbers. The\nresulting algorithm is parallelizable both in space (via localization) and in\nbandwith/subscale (subscales can be computed independently from each other).\nAlthough the method is deterministic it has a natural Bayesian interpretation\nunder the measure of probability emerging (as a mixed strategy) from the\ninformation game formulation and multiresolution approximations form a\nmartingale with respect to the filtration induced by the hierarchy of nested\nmeasurements.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 19:52:13 GMT"}, {"version": "v2", "created": "Thu, 26 Mar 2015 19:27:55 GMT"}, {"version": "v3", "created": "Fri, 4 Dec 2015 01:09:11 GMT"}, {"version": "v4", "created": "Tue, 8 Mar 2016 05:57:10 GMT"}, {"version": "v5", "created": "Fri, 10 Feb 2017 07:02:43 GMT"}], "update_date": "2017-02-13", "authors_parsed": [["Owhadi", "Houman", ""]]}, {"id": "1503.03506", "submitter": "Christian Wachinger", "authors": "Christian Wachinger and Polina Golland", "title": "Diverse Landmark Sampling from Determinantal Point Processes for\n  Scalable Manifold Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High computational costs of manifold learning prohibit its application for\nlarge point sets. A common strategy to overcome this problem is to perform\ndimensionality reduction on selected landmarks and to successively embed the\nentire dataset with the Nystr\\\"om method. The two main challenges that arise\nare: (i) the landmarks selected in non-Euclidean geometries must result in a\nlow reconstruction error, (ii) the graph constructed from sparsely sampled\nlandmarks must approximate the manifold well. We propose the sampling of\nlandmarks from determinantal distributions on non-Euclidean spaces. Since\ncurrent determinantal sampling algorithms have the same complexity as those for\nmanifold learning, we present an efficient approximation running in linear\ntime. Further, we recover the local geometry after the sparsification by\nassigning each landmark a local covariance matrix, estimated from the original\npoint set. The resulting neighborhood selection based on the Bhattacharyya\ndistance improves the embedding of sparsely sampled manifolds. Our experiments\nshow a significant performance improvement compared to state-of-the-art\nlandmark selection techniques.\n", "versions": [{"version": "v1", "created": "Wed, 11 Mar 2015 21:09:28 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Wachinger", "Christian", ""], ["Golland", "Polina", ""]]}, {"id": "1503.03787", "submitter": "Norbert B\\'atfai Ph.D.", "authors": "Norbert B\\'atfai", "title": "Are there intelligent Turing machines?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new computing model based on the cooperation among\nTuring machines called orchestrated machines. Like universal Turing machines,\norchestrated machines are also designed to simulate Turing machines but they\ncan also modify the original operation of the included Turing machines to\ncreate a new layer of some kind of collective behavior. Using this new model we\ncan define some interested notions related to cooperation ability of Turing\nmachines such as the intelligence quotient or the emotional intelligence\nquotient for Turing machines.\n", "versions": [{"version": "v1", "created": "Thu, 12 Mar 2015 16:00:32 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["B\u00e1tfai", "Norbert", ""]]}, {"id": "1503.03964", "submitter": "Shintaro Mori", "authors": "Shunsuke Yoshida, Masato Hisakado and Shintaro Mori", "title": "Interactive Restless Multi-armed Bandit Game and Swarm Intelligence\n  Effect", "comments": "18 pages, 4 figures", "journal-ref": "New generation computing, vol.34, No. 3, 291-306, 2016", "doi": "10.1007/s00354-016-0306-y", "report-no": null, "categories": "cs.AI cs.LG physics.data-an stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We obtain the conditions for the emergence of the swarm intelligence effect\nin an interactive game of restless multi-armed bandit (rMAB). A player competes\nwith multiple agents. Each bandit has a payoff that changes with a probability\n$p_{c}$ per round. The agents and player choose one of three options: (1)\nExploit (a good bandit), (2) Innovate (asocial learning for a good bandit among\n$n_{I}$ randomly chosen bandits), and (3) Observe (social learning for a good\nbandit). Each agent has two parameters $(c,p_{obs})$ to specify the decision:\n(i) $c$, the threshold value for Exploit, and (ii) $p_{obs}$, the probability\nfor Observe in learning. The parameters $(c,p_{obs})$ are uniformly\ndistributed. We determine the optimal strategies for the player using complete\nknowledge about the rMAB. We show whether or not social or asocial learning is\nmore optimal in the $(p_{c},n_{I})$ space and define the swarm intelligence\neffect. We conduct a laboratory experiment (67 subjects) and observe the swarm\nintelligence effect only if $(p_{c},n_{I})$ are chosen so that social learning\nis far more optimal than asocial learning.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2015 06:53:01 GMT"}], "update_date": "2016-08-22", "authors_parsed": [["Yoshida", "Shunsuke", ""], ["Hisakado", "Masato", ""], ["Mori", "Shintaro", ""]]}, {"id": "1503.03974", "submitter": "Carlo Comin", "authors": "Carlo Comin, Roberto Posenato, Romeo Rizzi", "title": "Hyper Temporal Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simple Temporal Networks (STNs) provide a powerful and general tool for\nrepresenting conjunctions of maximum delay constraints over ordered pairs of\ntemporal variables. In this paper we introduce Hyper Temporal Networks (HyTNs),\na strict generalization of STNs, to overcome the limitation of considering only\nconjunctions of constraints but maintaining a practical efficiency in the\nconsistency check of the instances. In a Hyper Temporal Network a single\ntemporal hyperarc constraint may be defined as a set of two or more maximum\ndelay constraints which is satisfied when at least one of these delay\nconstraints is satisfied. HyTNs are meant as a light generalization of STNs\noffering an interesting compromise. On one side, there exist practical\npseudo-polynomial time algorithms for checking consistency and computing\nfeasible schedules for HyTNs. On the other side, HyTNs offer a more powerful\nmodel accommodating natural constraints that cannot be expressed by STNs like\nTrigger off exactly delta min before (after) the occurrence of the first (last)\nevent in a set., which are used to represent synchronization events in some\nprocess aware information systems/workflow models proposed in the literature.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2015 07:44:19 GMT"}, {"version": "v2", "created": "Sun, 8 May 2016 17:00:34 GMT"}, {"version": "v3", "created": "Wed, 22 Mar 2017 15:49:44 GMT"}], "update_date": "2017-03-23", "authors_parsed": [["Comin", "Carlo", ""], ["Posenato", "Roberto", ""], ["Rizzi", "Romeo", ""]]}, {"id": "1503.04135", "submitter": "Giuseppe Sanfilippo", "authors": "Angelo Gilio, Niki Pfeifer and Giuseppe Sanfilippo", "title": "Transitive reasoning with imprecise probabilities", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.PR cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study probabilistically informative (weak) versions of transitivity, by\nusing suitable definitions of defaults and negated defaults, in the setting of\ncoherence and imprecise probabilities. We represent p-consistent sequences of\ndefaults and/or negated defaults by g-coherent imprecise probability\nassessments on the respective sequences of conditional events. Finally, we\nprove the coherent probability propagation rules for Weak Transitivity and the\nvalidity of selected inference patterns by proving the p-entailment for the\nassociated knowledge bases.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2015 16:34:04 GMT"}], "update_date": "2015-03-16", "authors_parsed": [["Gilio", "Angelo", ""], ["Pfeifer", "Niki", ""], ["Sanfilippo", "Giuseppe", ""]]}, {"id": "1503.04187", "submitter": "Manuel Baltieri Mr", "authors": "Simon McGregor, Manuel Baltieri and Christopher L. Buckley", "title": "A Minimal Active Inference Agent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on the so-called \"free-energy principle'' (FEP) in cognitive\nneuroscience is becoming increasingly high-profile. To date, introductions to\nthis theory have proved difficult for many readers to follow, but it depends\nmainly upon two relatively simple ideas: firstly that normative or teleological\nvalues can be expressed as probability distributions (active inference), and\nsecondly that approximate Bayesian reasoning can be effectively performed by\ngradient descent on model parameters (the free-energy principle). The notion of\nactive inference is of great interest for a number of disciplines including\ncognitive science and artificial intelligence, as well as cognitive\nneuroscience, and deserves to be more widely known.\n  This paper attempts to provide an accessible introduction to active inference\nand informational free-energy, for readers from a range of scientific\nbackgrounds. In this work introduce an agent-based model with an agent trying\nto make predictions about its position in a one-dimensional discretized world\nusing methods from the FEP.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2015 18:58:25 GMT"}], "update_date": "2015-03-16", "authors_parsed": [["McGregor", "Simon", ""], ["Baltieri", "Manuel", ""], ["Buckley", "Christopher L.", ""]]}, {"id": "1503.04193", "submitter": "Nicolas Troquard", "authors": "Daniele Porello, Nicolas Troquard", "title": "Non-normal modalities in variants of Linear Logic", "comments": null, "journal-ref": null, "doi": "10.1080/11663081.2015.1080422", "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents modal versions of resource-conscious logics. We\nconcentrate on extensions of variants of Linear Logic with one minimal\nnon-normal modality. In earlier work, where we investigated agency in\nmulti-agent systems, we have shown that the results scale up to logics with\nmultiple non-minimal modalities. Here, we start with the language of\npropositional intuitionistic Linear Logic without the additive disjunction, to\nwhich we add a modality. We provide an interpretation of this language on a\nclass of Kripke resource models extended with a neighbourhood function: modal\nKripke resource models. We propose a Hilbert-style axiomatization and a\nGentzen-style sequent calculus. We show that the proof theories are sound and\ncomplete with respect to the class of modal Kripke resource models. We show\nthat the sequent calculus admits cut elimination and that proof-search is in\nPSPACE. We then show how to extend the results when non-commutative connectives\nare added to the language. Finally, we put the logical framework to use by\ninstantiating it as logics of agency. In particular, we propose a logic to\nreason about the resource-sensitive use of artefacts and illustrate it with a\nvariety of examples.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2015 19:54:15 GMT"}, {"version": "v2", "created": "Fri, 4 Sep 2015 15:16:44 GMT"}], "update_date": "2015-09-07", "authors_parsed": [["Porello", "Daniele", ""], ["Troquard", "Nicolas", ""]]}, {"id": "1503.04220", "submitter": "Arindam Chaudhuri AC", "authors": "Arindam Chaudhuri, Dipak Chatterjee", "title": "Fuzzy Mixed Integer Optimization Model for Regression Approach", "comments": "Conference Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixed Integer Optimization has been a topic of active research in past\ndecades. It has been used to solve Statistical problems of classification and\nregression involving massive data. However, there is an inherent degree of\nvagueness present in huge real life data. This impreciseness is handled by\nFuzzy Sets. In this Paper, Fuzzy Mixed Integer Optimization Method (FMIOM) is\nused to find solution to Regression problem. The methodology exploits discrete\ncharacter of problem. In this way large scale problems are solved within\npractical limits. The data points are separated into different polyhedral\nregions and each region has its own distinct regression coefficients. In this\nattempt, an attention is drawn to Statistics and Data Mining community that\nInteger Optimization can be significantly used to revisit different Statistical\nproblems. Computational experimentations with generated and real data sets show\nthat FMIOM is comparable to and often outperforms current leading methods. The\nresults illustrate potential for significant impact of Fuzzy Integer\nOptimization methods on Computational Statistics and Data Mining.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2015 21:10:38 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Chaudhuri", "Arindam", ""], ["Chatterjee", "Dipak", ""]]}, {"id": "1503.04222", "submitter": "Arindam Chaudhuri AC", "authors": "Arindam Chaudhuri, Dipak Chatterjee, Ritesh Rajput", "title": "Fuzzy Mixed Integer Linear Programming for Air Vehicles Operations\n  Optimization", "comments": "Conference Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple Air Vehicles (AVs) to prosecute geographically dispersed targets is\nan important optimization problem. Associated multiple tasks viz., target\nclassification, attack and verification are successively performed on each\ntarget. The optimal minimum time performance of these tasks requires\ncooperation among vehicles such that critical time constraints are satisfied\ni.e. target must be classified before it can be attacked and AV is sent to\ntarget area to verify its destruction after target has been attacked. Here,\noptimal task scheduling problem from Indian Air Force is formulated as Fuzzy\nMixed Integer Linear Programming (FMILP) problem. The solution assigns all\ntasks to vehicles and performs scheduling in an optimal manner including\nscheduled staged departure times. Coupled tasks involving time and task order\nconstraints are addressed. When AVs have sufficient endurance, existence of\noptimal solution is guaranteed. The solution developed can serve as an\neffective heuristic for different categories of AV optimization problems.\n", "versions": [{"version": "v1", "created": "Fri, 13 Mar 2015 21:14:49 GMT"}], "update_date": "2015-03-17", "authors_parsed": [["Chaudhuri", "Arindam", ""], ["Chatterjee", "Dipak", ""], ["Rajput", "Ritesh", ""]]}, {"id": "1503.04260", "submitter": "Diederik Aerts", "authors": "Diederik Aerts, Sandro Sozzo and Tomas Veloz", "title": "Quantum Structure of Negation and Conjunction in Human Thought", "comments": "44 pages. arXiv admin note: text overlap with arXiv:1406.2358", "journal-ref": "Frontiers in Psychology 6, 1447, (2015)", "doi": "10.3389/fpsyg.2015.01447", "report-no": null, "categories": "cs.AI quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyse in this paper the data collected in a set of experiments performed\non human subjects on the combination of natural concepts. We investigate the\nmutual influence of conceptual conjunction and negation by measuring the\nmembership weights of a list of exemplars with respect to two concepts, e.g.,\n'Fruits' and 'Vegetables', and their conjunction 'Fruits And Vegetables', but\nalso their conjunction when one or both concepts are negated, namely, 'Fruits\nAnd Not Vegetables', 'Not Fruits And Vegetables' and 'Not Fruits And Not\nVegetables'. Our findings sharpen existing analysis on conceptual combinations,\nrevealing systematic and remarkable deviations from classical (fuzzy set) logic\nand probability theory. And, more important, our results give further\nconsiderable evidence to the validity of our quantum-theoretic framework for\nthe combination of two concepts. Indeed, the representation of conceptual\nnegation naturally arises from the general assumptions of our two-sector Fock\nspace model, and this representation faithfully agrees with the collected data.\nIn addition, we find a further significant deviation and a priori unexpected\nfrom classicality, which can exactly be explained by assuming that human\nreasoning is the superposition of an 'emergent reasoning' and a 'logical\nreasoning', and that these two processes can be successfully represented in a\nFock space algebraic structure.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2015 02:43:14 GMT"}], "update_date": "2016-09-09", "authors_parsed": [["Aerts", "Diederik", ""], ["Sozzo", "Sandro", ""], ["Veloz", "Tomas", ""]]}, {"id": "1503.04333", "submitter": "Kieran Greer Dr", "authors": "Kieran Greer", "title": "A More Human Way to Play Computer Chess", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper suggests a forward-pruning technique for computer chess that uses\n'Move Tables', which are like Transposition Tables, but for moves not\npositions. They use an efficient memory structure and has put the design into\nthe context of long and short-term memories. The long-term memory updates a\nplay path with weight reinforcement, while the short-term memory can be\nimmediately added or removed. With this, 'long branches' can play a short path,\nbefore returning to a full search at the resulting leaf nodes. Re-using an\nearlier search path allows the tree to be forward-pruned, which is known to be\ndangerous, because it removes part of the search process. Additional checks are\ntherefore made and moves can even be re-added when the search result is\nunsatisfactory. Automatic feature analysis is now central to the algorithm,\nwhere key squares and related squares can be generated automatically and used\nto guide the search process. Using this analysis, if a search result is\ninferior, it can re-insert un-played moves that cover these key squares only.\nOn the tactical side, a type of move that the forward-pruning will fail on is\nrecognised and a pattern-based solution to that problem is suggested. This has\ncompleted the theory of an earlier paper and resulted in a more human-like\napproach to searching for a chess move. Tests demonstrate that the obvious\nblunders associated with forward pruning are no longer present and that it can\ncompete at the top level with regard to playing strength.\n", "versions": [{"version": "v1", "created": "Sat, 14 Mar 2015 18:47:07 GMT"}, {"version": "v2", "created": "Sun, 28 Jun 2015 14:23:20 GMT"}, {"version": "v3", "created": "Tue, 16 May 2017 12:11:10 GMT"}, {"version": "v4", "created": "Mon, 11 Jun 2018 08:37:36 GMT"}, {"version": "v5", "created": "Thu, 17 Jan 2019 12:31:20 GMT"}], "update_date": "2019-01-18", "authors_parsed": [["Greer", "Kieran", ""]]}, {"id": "1503.04864", "submitter": "Fay\\c{c}al Hamdi", "authors": "Fay\\c{c}al Hamdi, Nathalie Abadie, B\\'en\\'edicte Bucher and\n  Abdelfettah Feliachi", "title": "GeomRDF: A Geodata Converter with a Fine-Grained Structured\n  Representation of Geometry in the Web", "comments": "12 pages, 2 figures, the 1st International Workshop on Geospatial\n  Linked Data (GeoLD 2014) - SEMANTiCS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, with the advent of the web of data, a growing number of\nnational mapping agencies tend to publish their geospatial data as Linked Data.\nHowever, differences between traditional GIS data models and Linked Data model\ncan make the publication process more complicated. Besides, it may require, to\nbe done, the setting of several parameters and some expertise in the semantic\nweb technologies. In addition, the use of standards like GeoSPARQL (or ad hoc\npredicates) is mandatory to perform spatial queries on published geospatial\ndata. In this paper, we present GeomRDF, a tool that helps users to convert\nspatial data from traditional GIS formats to RDF model easily. It generates\ngeometries represented as GeoSPARQL WKT literal but also as structured\ngeometries that can be exploited by using only the RDF query language, SPARQL.\nGeomRDF was implemented as a module in the RDF publication platform Datalift. A\nvalidation of GeomRDF has been realized against the French administrative units\ndataset (provided by IGN France).\n", "versions": [{"version": "v1", "created": "Mon, 16 Mar 2015 21:35:18 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Hamdi", "Fay\u00e7al", ""], ["Abadie", "Nathalie", ""], ["Bucher", "B\u00e9n\u00e9dicte", ""], ["Feliachi", "Abdelfettah", ""]]}, {"id": "1503.04941", "submitter": "J.H. van Hateren", "authors": "J.H. van Hateren", "title": "How the symbol grounding of living organisms can be realized in\n  artificial agents", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A system with artificial intelligence usually relies on symbol manipulation,\nat least partly and implicitly. However, the interpretation of the symbols -\nwhat they represent and what they are about - is ultimately left to humans, as\ndesigners and users of the system. How symbols can acquire meaning for the\nsystem itself, independent of external interpretation, is an unsolved problem.\nSome grounding of symbols can be obtained by embodiment, that is, by causally\nconnecting symbols (or sub-symbolic variables) to the physical environment,\nsuch as in a robot with sensors and effectors. However, a causal connection as\nsuch does not produce representation and aboutness of the kind that symbols\nhave for humans. Here I present a theory that explains how humans and other\nliving organisms have acquired the capability to have symbols and sub-symbolic\nvariables that represent, refer to, and are about something else. The theory\nshows how reference can be to physical objects, but also to abstract objects,\nand even how it can be misguided (errors in reference) or be about non-existing\nobjects. I subsequently abstract the primary components of the theory from\ntheir biological context, and discuss how and under what conditions the theory\ncould be implemented in artificial agents. A major component of the theory is\nthe strong nonlinearity associated with (potentially unlimited)\nself-reproduction. The latter is likely not acceptable in artificial systems.\nIt remains unclear if goals other than those inherently serving\nself-reproduction can have aboutness and if such goals could be stabilized.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2015 08:00:49 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["van Hateren", "J. H.", ""]]}, {"id": "1503.05055", "submitter": "Arnaud Martin", "authors": "Mouna Chebbah (IRISA), Arnaud Martin (IRISA), Boutheina Ben Yaghlane", "title": "Combining partially independent belief functions", "comments": "Decision Support Systems, Elsevier, 2015", "journal-ref": null, "doi": "10.1016/j.dss.2015.02.017", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The theory of belief functions manages uncertainty and also proposes a set of\ncombination rules to aggregate opinions of several sources. Some combination\nrules mix evidential information where sources are independent; other rules are\nsuited to combine evidential information held by dependent sources. In this\npaper we have two main contributions: First we suggest a method to quantify\nsources' degree of independence that may guide the choice of the more\nappropriate set of combination rules. Second, we propose a new combination rule\nthat takes consideration of sources' degree of independence. The proposed\nmethod is illustrated on generated mass functions.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2015 14:04:38 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Chebbah", "Mouna", "", "IRISA"], ["Martin", "Arnaud", "", "IRISA"], ["Yaghlane", "Boutheina Ben", ""]]}, {"id": "1503.05113", "submitter": "Keyan Zahedi", "authors": "Keyan Ghazi-Zahedi, Johannes Rauh", "title": "Quantifying Morphological Computation based on an Information\n  Decomposition of the Sensorimotor Loop", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The question how an agent is affected by its embodiment has attracted growing\nattention in recent years. A new field of artificial intelligence has emerged,\nwhich is based on the idea that intelligence cannot be understood without\ntaking into account embodiment. We believe that a formal approach to\nquantifying the embodiment's effect on the agent's behaviour is beneficial to\nthe fields of artificial life and artificial intelligence. The contribution of\nan agent's body and environment to its behaviour is also known as morphological\ncomputation. Therefore, in this work, we propose a quantification of\nmorphological computation, which is based on an information decomposition of\nthe sensorimotor loop into shared, unique and synergistic information. In\nnumerical simulation based on a formal representation of the sensorimotor loop,\nwe show that the unique information of the body and environment is a good\nmeasure for morphological computation. The results are compared to our\npreviously derived quantification of morphological computation.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2015 16:21:07 GMT"}], "update_date": "2015-03-18", "authors_parsed": [["Ghazi-Zahedi", "Keyan", ""], ["Rauh", "Johannes", ""]]}, {"id": "1503.05140", "submitter": "Ehsaneddin Asgari", "authors": "Ehsaneddin Asgari and Mohammad R.K. Mofrad", "title": "ProtVec: A Continuous Distributed Representation of Biological Sequences", "comments": null, "journal-ref": "PLoS ONE 10(11): e0141287, 2015", "doi": "10.1371/journal.pone.0141287", "report-no": null, "categories": "q-bio.QM cs.AI cs.LG q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new representation and feature extraction method for\nbiological sequences. Named bio-vectors (BioVec) to refer to biological\nsequences in general with protein-vectors (ProtVec) for proteins (amino-acid\nsequences) and gene-vectors (GeneVec) for gene sequences, this representation\ncan be widely used in applications of deep learning in proteomics and genomics.\nIn the present paper, we focus on protein-vectors that can be utilized in a\nwide array of bioinformatics investigations such as family classification,\nprotein visualization, structure prediction, disordered protein identification,\nand protein-protein interaction prediction. In this method, we adopt artificial\nneural network approaches and represent a protein sequence with a single dense\nn-dimensional vector. To evaluate this method, we apply it in classification of\n324,018 protein sequences obtained from Swiss-Prot belonging to 7,027 protein\nfamilies, where an average family classification accuracy of 93%+-0.06% is\nobtained, outperforming existing family classification methods. In addition, we\nuse ProtVec representation to predict disordered proteins from structured\nproteins. Two databases of disordered sequences are used: the DisProt database\nas well as a database featuring the disordered regions of nucleoporins rich\nwith phenylalanine-glycine repeats (FG-Nups). Using support vector machine\nclassifiers, FG-Nup sequences are distinguished from structured protein\nsequences found in Protein Data Bank (PDB) with a 99.8% accuracy, and\nunstructured DisProt sequences are differentiated from structured DisProt\nsequences with 100.0% accuracy. These results indicate that by only providing\nsequence data for various proteins into this model, accurate information about\nprotein structure can be determined.\n", "versions": [{"version": "v1", "created": "Tue, 17 Mar 2015 17:55:22 GMT"}, {"version": "v2", "created": "Thu, 26 May 2016 20:17:51 GMT"}], "update_date": "2016-05-30", "authors_parsed": [["Asgari", "Ehsaneddin", ""], ["Mofrad", "Mohammad R. K.", ""]]}, {"id": "1503.05296", "submitter": "Omar Al-Jarrah", "authors": "O. Y. Al-Jarrah, P. D. Yoo, S Muhaidat, G. K. Karagiannidis, and K.\n  Taha", "title": "Efficient Machine Learning for Big Data: A Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the emerging technologies and all associated devices, it is predicted\nthat massive amount of data will be created in the next few years, in fact, as\nmuch as 90% of current data were created in the last couple of years,a trend\nthat will continue for the foreseeable future. Sustainable computing studies\nthe process by which computer engineer/scientist designs computers and\nassociated subsystems efficiently and effectively with minimal impact on the\nenvironment. However, current intelligent machine-learning systems are\nperformance driven, the focus is on the predictive/classification accuracy,\nbased on known properties learned from the training samples. For instance, most\nmachine-learning-based nonparametric models are known to require high\ncomputational cost in order to find the global optima. With the learning task\nin a large dataset, the number of hidden nodes within the network will\ntherefore increase significantly, which eventually leads to an exponential rise\nin computational complexity. This paper thus reviews the theoretical and\nexperimental data-modeling literature, in large-scale data-intensive fields,\nrelating to: (1) model efficiency, including computational requirements in\nlearning, and data-intensive areas structure and design, and introduces (2) new\nalgorithmic approaches with the least memory requirements and processing to\nminimize computational cost, while maintaining/improving its\npredictive/classification accuracy and stability.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2015 07:56:12 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Al-Jarrah", "O. Y.", ""], ["Yoo", "P. D.", ""], ["Muhaidat", "S", ""], ["Karagiannidis", "G. K.", ""], ["Taha", "K.", ""]]}, {"id": "1503.05479", "submitter": "Qinqing Zheng", "authors": "Qinqing Zheng, Ryota Tomioka", "title": "Interpolating Convex and Non-Convex Tensor Decompositions via the\n  Subspace Norm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of recovering a low-rank tensor from its noisy\nobservation. Previous work has shown a recovery guarantee with signal to noise\nratio $O(n^{\\lceil K/2 \\rceil /2})$ for recovering a $K$th order rank one\ntensor of size $n\\times \\cdots \\times n$ by recursive unfolding. In this paper,\nwe first improve this bound to $O(n^{K/4})$ by a much simpler approach, but\nwith a more careful analysis. Then we propose a new norm called the subspace\nnorm, which is based on the Kronecker products of factors obtained by the\nproposed simple estimator. The imposed Kronecker structure allows us to show a\nnearly ideal $O(\\sqrt{n}+\\sqrt{H^{K-1}})$ bound, in which the parameter $H$\ncontrols the blend from the non-convex estimator to mode-wise nuclear norm\nminimization. Furthermore, we empirically demonstrate that the subspace norm\nachieves the nearly ideal denoising performance even with $H=O(1)$.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2015 16:45:04 GMT"}, {"version": "v2", "created": "Tue, 27 Oct 2015 01:44:23 GMT"}], "update_date": "2015-10-28", "authors_parsed": [["Zheng", "Qinqing", ""], ["Tomioka", "Ryota", ""]]}, {"id": "1503.05501", "submitter": "Odinaldo Rodrigues", "authors": "D. M. Gabbay and O. Rodrigues", "title": "Probabilistic Argumentation. An Equational Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a generic way to add any new feature to a system. It involves 1)\nidentifying the basic units which build up the system and 2) introducing the\nnew feature to each of these basic units.\n  In the case where the system is argumentation and the feature is\nprobabilistic we have the following. The basic units are: a. the nature of the\narguments involved; b. the membership relation in the set S of arguments; c.\nthe attack relation; and d. the choice of extensions.\n  Generically to add a new aspect (probabilistic, or fuzzy, or temporal, etc)\nto an argumentation network <S,R> can be done by adding this feature to each\ncomponent a-d. This is a brute-force method and may yield a non-intuitive or\nmeaningful result.\n  A better way is to meaningfully translate the object system into another\ntarget system which does have the aspect required and then let the target\nsystem endow the aspect on the initial system. In our case we translate\nargumentation into classical propositional logic and get probabilistic\nargumentation from the translation.\n  Of course what we get depends on how we translate.\n  In fact, in this paper we introduce probabilistic semantics to abstract\nargumentation theory based on the equational approach to argumentation\nnetworks. We then compare our semantics with existing proposals in the\nliterature including the approaches by M. Thimm and by A. Hunter. Our\nmethodology in general is discussed in the conclusion.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2015 17:29:24 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Gabbay", "D. M.", ""], ["Rodrigues", "O.", ""]]}, {"id": "1503.05508", "submitter": "Mohammed Bekkouche", "authors": "Mohammed Bekkouche", "title": "Exploration of the scalability of LocFaults approach for error\n  localization with While-loops programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A model checker can produce a trace of counterexample, for an erroneous\nprogram, which is often long and difficult to understand. In general, the part\nabout the loops is the largest among the instructions in this trace. This makes\nthe location of errors in loops critical, to analyze errors in the overall\nprogram. In this paper, we explore the scala-bility capabilities of LocFaults,\nour error localization approach exploiting paths of CFG(Control Flow Graph)\nfrom a counterexample to calculate the MCDs (Minimal Correction Deviations),\nand MCSs (Minimal Correction Subsets) from each found MCD. We present the times\nof our approach on programs with While-loops unfolded b times, and a number of\ndeviated conditions ranging from 0 to n. Our preliminary results show that the\ntimes of our approach, constraint-based and flow-driven, are better compared to\nBugAssist which is based on SAT and transforms the entire program to a Boolean\nformula, and further the information provided by LocFaults is more expressive\nfor the user.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2015 17:45:20 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Bekkouche", "Mohammed", ""]]}, {"id": "1503.05530", "submitter": "Mohammed Bekkouche", "authors": "Mohammed Bekkouche", "title": "Exploration of the scalability of LocFaults", "comments": "in French", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A model checker can produce a trace of counterexample, for an erroneous\nprogram, which is often long and difficult to understand. In general, the part\nabout the loops is the largest among the instructions in this trace. This makes\nthe location of errors in loops critical, to analyze errors in the overall\nprogram. In this paper, we explore the scalability capabilities of LocFaults,\nour error localization approach exploiting paths of CFG(Control Flow Graph)\nfrom a counterexample to calculate the MCDs (Minimal Correction Deviations),\nand MCSs (Minimal Correction Subsets) from each found MCD. We present the times\nof our approach on programs with While-loops unfolded b times, and a number of\ndeviated conditions ranging from 0 to n. Our preliminary results show that the\ntimes of our approach, constraint-based and flow-driven, are better compared to\nBugAssist which is based on SAT and transforms the entire program to a Boolean\nformula, and further the information provided by LocFaults is more expressive\nfor the user.\n", "versions": [{"version": "v1", "created": "Wed, 18 Mar 2015 18:40:55 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Bekkouche", "Mohammed", ""]]}, {"id": "1503.05667", "submitter": "Sourish Dasgupta", "authors": "Sourish Dasgupta, Gaurav Maheshwari, Priyansh Trivedi", "title": "BitSim: An Algebraic Similarity Measure for Description Logics Concepts", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this paper, we propose an algebraic similarity measure {\\sigma}BS (BS\nstands for BitSim) for assigning semantic similarity score to concept\ndefinitions in ALCH+ an expressive fragment of Description Logics (DL). We\ndefine an algebraic interpretation function, I_B, that maps a concept\ndefinition to a unique string ({\\omega}_B) called bit-code) over an alphabet\n{\\Sigma}_B of 11 symbols belonging to L_B - the language over P B. IB has\nsemantic correspondence with conventional model-theoretic interpretation of DL.\nWe then define {\\sigma}_BS on L_B. A detailed analysis of I_B and {\\sigma}_BS\nhas been given.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 08:05:03 GMT"}], "update_date": "2015-03-20", "authors_parsed": [["Dasgupta", "Sourish", ""], ["Maheshwari", "Gaurav", ""], ["Trivedi", "Priyansh", ""]]}, {"id": "1503.05947", "submitter": "Yanlai Chen", "authors": "Yanlai Chen", "title": "Reduced Basis Decomposition: a Certified and Fast Lossy Data Compression\n  Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.NA cs.AI cs.CV cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dimension reduction is often needed in the area of data mining. The goal of\nthese methods is to map the given high-dimensional data into a low-dimensional\nspace preserving certain properties of the initial data. There are two kinds of\ntechniques for this purpose. The first, projective methods, builds an explicit\nlinear projection from the high-dimensional space to the low-dimensional one.\nOn the other hand, the nonlinear methods utilizes nonlinear and implicit\nmapping between the two spaces. In both cases, the methods considered in\nliterature have usually relied on computationally very intensive matrix\nfactorizations, frequently the Singular Value Decomposition (SVD). The\ncomputational burden of SVD quickly renders these dimension reduction methods\ninfeasible thanks to the ever-increasing sizes of the practical datasets.\n  In this paper, we present a new decomposition strategy, Reduced Basis\nDecomposition (RBD), which is inspired by the Reduced Basis Method (RBM). Given\n$X$ the high-dimensional data, the method approximates it by $Y \\, T (\\approx\nX)$ with $Y$ being the low-dimensional surrogate and $T$ the transformation\nmatrix. $Y$ is obtained through a greedy algorithm thus extremely efficient. In\nfact, it is significantly faster than SVD with comparable accuracy. $T$ can be\ncomputed on the fly. Moreover, unlike many compression algorithms, it easily\nfinds the mapping for an arbitrary ``out-of-sample'' vector and it comes with\nan ``error indicator'' certifying the accuracy of the compression. Numerical\nresults are shown validating these claims.\n", "versions": [{"version": "v1", "created": "Thu, 19 Mar 2015 21:10:57 GMT"}], "update_date": "2015-03-23", "authors_parsed": [["Chen", "Yanlai", ""]]}, {"id": "1503.06087", "submitter": "Frieder Stolzenburg", "authors": "Ulrich Furbach, Claudia Schon, Frieder Stolzenburg, Karl-Heinz Weis,\n  Claus-Peter Wirth", "title": "The RatioLog Project: Rational Extensions of Logical Reasoning", "comments": "7 pages, 3 figures", "journal-ref": "KI, 29(3):271-277, 2015", "doi": "10.1007/s13218-015-0377-9", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Higher-level cognition includes logical reasoning and the ability of question\nanswering with common sense. The RatioLog project addresses the problem of\nrational reasoning in deep question answering by methods from automated\ndeduction and cognitive computing. In a first phase, we combine techniques from\ninformation retrieval and machine learning to find appropriate answer\ncandidates from the huge amount of text in the German version of the free\nencyclopedia \"Wikipedia\". In a second phase, an automated theorem prover tries\nto verify the answer candidates on the basis of their logical representations.\nIn a third phase - because the knowledge may be incomplete and inconsistent -,\nwe consider extensions of logical reasoning to improve the results. In this\ncontext, we work toward the application of techniques from human reasoning: We\nemploy defeasible reasoning to compare the answers w.r.t. specificity, deontic\nlogic, normative reasoning, and model construction. Moreover, we use integrated\ncase-based reasoning and machine learning techniques on the basis of the\nsemantic structure of the questions and answer candidates to learn giving the\nright answers.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2015 14:33:48 GMT"}, {"version": "v2", "created": "Thu, 30 Jul 2015 08:21:03 GMT"}], "update_date": "2015-07-31", "authors_parsed": [["Furbach", "Ulrich", ""], ["Schon", "Claudia", ""], ["Stolzenburg", "Frieder", ""], ["Weis", "Karl-Heinz", ""], ["Wirth", "Claus-Peter", ""]]}, {"id": "1503.06201", "submitter": "Akin Osman Kazakci", "authors": "Akin Osman (CGS), Kazak\\c{c}i Mines (CGS)", "title": "Data Science as a New Frontier for Design", "comments": "International Conference on Engineering Design, Jul 2015, Milan,\n  Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this paper is to contribute to the challenge of transferring\nknow-how, theories and methods from design research to the design processes in\ninformation science and technologies. More specifically, we shall consider a\ndomain, namely data-science, that is becoming rapidly a globally invested\nresearch and development axis with strong imperatives for innovation given the\ndata deluge we are currently facing. We argue that, in order to rise to the\ndata-related challenges that the society is facing, data-science initiatives\nshould ensure a renewal of traditional research methodologies that are still\nlargely based on trial-error processes depending on the talent and insights of\na single (or a restricted group of) researchers. It is our claim that design\ntheories and methods can provide, at least to some extent, the much-needed\nframework. We will use a worldwide data-science challenge organized to study a\ntechnical problem in physics, namely the detection of Higgs boson, as a use\ncase to demonstrate some of the ways in which design theory and methods can\nhelp in analyzing and shaping the innovation dynamics in such projects.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2015 19:31:09 GMT"}], "update_date": "2015-03-23", "authors_parsed": [["Osman", "Akin", "", "CGS"], ["Mines", "Kazak\u00e7i", "", "CGS"]]}, {"id": "1503.06239", "submitter": "Jinye Zhang", "authors": "Jinye Zhang, Zhijian Ou", "title": "Block-Wise MAP Inference for Determinantal Point Processes with\n  Application to Change-Point Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ME stat.ML", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Existing MAP inference algorithms for determinantal point processes (DPPs)\nneed to calculate determinants or conduct eigenvalue decomposition generally at\nthe scale of the full kernel, which presents a great challenge for real-world\napplications. In this paper, we introduce a class of DPPs, called BwDPPs, that\nare characterized by an almost block diagonal kernel matrix and thus can allow\nefficient block-wise MAP inference. Furthermore, BwDPPs are successfully\napplied to address the difficulty of selecting change-points in the problem of\nchange-point detection (CPD), which results in a new BwDPP-based CPD method,\nnamed BwDppCpd. In BwDppCpd, a preliminary set of change-point candidates is\nfirst created based on existing well-studied metrics. Then, these change-point\ncandidates are treated as DPP items, and DPP-based subset selection is\nconducted to give the final estimate of the change-points that favours both\nquality and diversity. The effectiveness of BwDppCpd is demonstrated through\nextensive experiments on five real-world datasets.\n", "versions": [{"version": "v1", "created": "Fri, 20 Mar 2015 22:01:45 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Zhang", "Jinye", ""], ["Ou", "Zhijian", ""]]}, {"id": "1503.06316", "submitter": "Santosh Tirunagari", "authors": "Santosh Tirunagari, Norman Poh, Guosheng Hu and David Windridge", "title": "Identifying Similar Patients Using Self-Organising Maps: A Case Study on\n  Type-1 Diabetes Self-care Survey Responses", "comments": "01-05 pages", "journal-ref": null, "doi": null, "report-no": "TR-DoC-02", "categories": "cs.CE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diabetes is considered a lifestyle disease and a well managed self-care plays\nan important role in the treatment. Clinicians often conduct surveys to\nunderstand the self-care behaviors in their patients. In this context, we\npropose to use Self-Organising Maps (SOM) to explore the survey data for\nassessing the self-care behaviors in Type-1 diabetic patients. Specifically,\nSOM is used to visualize high dimensional similar patient profiles, which is\nrarely discussed. Experiments demonstrate that our findings through SOM\nanalysis corresponds well to the expectations of the clinicians. In addition,\nour findings inspire the experts to improve their understanding of the\nself-care behaviors for their patients. The principle findings in our study\nshow: 1) patients who take correct dose of insulin, inject insulin at the right\ntime, 2) patients who take correct food portions undertake regular physical\nactivity and 3) patients who eat on time take correct food portions.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2015 15:47:35 GMT"}], "update_date": "2015-03-29", "authors_parsed": [["Tirunagari", "Santosh", ""], ["Poh", "Norman", ""], ["Hu", "Guosheng", ""], ["Windridge", "David", ""]]}, {"id": "1503.06350", "submitter": "Nikolaos Karianakis", "authors": "Nikolaos Karianakis, Thomas J. Fuchs and Stefano Soatto", "title": "Boosting Convolutional Features for Robust Object Proposals", "comments": "9 pages, 4 figures, 2 tables, 42 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Convolutional Neural Networks (CNNs) have demonstrated excellent\nperformance in image classification, but still show room for improvement in\nobject-detection tasks with many categories, in particular for cluttered scenes\nand occlusion. Modern detection algorithms like Regions with CNNs (Girshick et\nal., 2014) rely on Selective Search (Uijlings et al., 2013) to propose regions\nwhich with high probability represent objects, where in turn CNNs are deployed\nfor classification. Selective Search represents a family of sophisticated\nalgorithms that are engineered with multiple segmentation, appearance and\nsaliency cues, typically coming with a significant run-time overhead.\nFurthermore, (Hosang et al., 2014) have shown that most methods suffer from low\nreproducibility due to unstable superpixels, even for slight image\nperturbations. Although CNNs are subsequently used for classification in\ntop-performing object-detection pipelines, current proposal methods are\nagnostic to how these models parse objects and their rich learned\nrepresentations. As a result they may propose regions which may not resemble\nhigh-level objects or totally miss some of them. To overcome these drawbacks we\npropose a boosting approach which directly takes advantage of hierarchical CNN\nfeatures for detecting regions of interest fast. We demonstrate its performance\non ImageNet 2013 detection benchmark and compare it with state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Sat, 21 Mar 2015 20:54:39 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Karianakis", "Nikolaos", ""], ["Fuchs", "Thomas J.", ""], ["Soatto", "Stefano", ""]]}, {"id": "1503.06483", "submitter": "Kamran Kowsari", "authors": "Kamran Kowsari, Maryam Yammahi, Nima Bari, Roman Vichr, Faisal Alsaby,\n  Simon Y. Berkovich", "title": "Construction of FuzzyFind Dictionary using Golay Coding Transformation\n  for Searching Applications", "comments": null, "journal-ref": null, "doi": "10.14569/IJACSA.2015.060313", "report-no": null, "categories": "cs.DB cs.AI cs.DS cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Searching through a large volume of data is very critical for companies,\nscientists, and searching engines applications due to time complexity and\nmemory complexity. In this paper, a new technique of generating FuzzyFind\nDictionary for text mining was introduced. We simply mapped the 23 bits of the\nEnglish alphabet into a FuzzyFind Dictionary or more than 23 bits by using more\nFuzzyFind Dictionary, and reflecting the presence or absence of particular\nletters. This representation preserves closeness of word distortions in terms\nof closeness of the created binary vectors within Hamming distance of 2\ndeviations. This paper talks about the Golay Coding Transformation Hash Table\nand how it can be used on a FuzzyFind Dictionary as a new technology for using\nin searching through big data. This method is introduced by linear time\ncomplexity for generating the dictionary and constant time complexity to access\nthe data and update by new data sets, also updating for new data sets is linear\ntime depends on new data points. This technique is based on searching only for\nletters of English that each segment has 23 bits, and also we have more than\n23-bit and also it could work with more segments as reference table.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2015 21:46:12 GMT"}], "update_date": "2018-10-24", "authors_parsed": [["Kowsari", "Kamran", ""], ["Yammahi", "Maryam", ""], ["Bari", "Nima", ""], ["Vichr", "Roman", ""], ["Alsaby", "Faisal", ""], ["Berkovich", "Simon Y.", ""]]}, {"id": "1503.06485", "submitter": "Radhakrishna Srikanth", "authors": "Hanaan Hashim and R. Srikanth", "title": "The concept of free will as an infinite metatheoretic recursion", "comments": "Accepted in INDECS (close to the accepted version)", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.hist-ph cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is argued that the concept of free will, like the concept of truth in\nformal languages, requires a separation between an object level and a\nmeta-level for being consistently defined. The Jamesian two-stage model, which\ndeconstructs free will into the causally open \"free\" stage with its closure in\nthe \"will\" stage, is implicitly a move in this direction. However, to avoid the\ndilemma of determinism, free will additionally requires an infinite regress of\ncausal meta-stages, making free choice a hypertask. We use this model to define\nfree will of the rationalist-compatibilist type. This is shown to provide a\nnatural three-way distinction between quantum indeterminism, freedom and free\nwill, applicable respectively to artificial intelligence (AI), animal agents\nand human agents. We propose that the causal hierarchy in our model corresponds\nto a hierarchy of Turing uncomputability. Possible neurobiological and\nbehavioral tests to demonstrate free will experimentally are suggested.\nRamifications of the model for physics, evolutionary biology, neuroscience,\nneuropathological medicine and moral philosophy are briefly outlined.\n", "versions": [{"version": "v1", "created": "Sun, 22 Mar 2015 22:00:53 GMT"}, {"version": "v2", "created": "Sun, 21 Jun 2015 17:55:02 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Hashim", "Hanaan", ""], ["Srikanth", "R.", ""]]}, {"id": "1503.06572", "submitter": "Georgiana Ifrim", "authors": "Bichen Shi, Michel Schellekens, Georgiana Ifrim", "title": "A Machine Learning Approach to Predicting the Smoothed Complexity of\n  Sorting Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Smoothed analysis is a framework for analyzing the complexity of an\nalgorithm, acting as a bridge between average and worst-case behaviour. For\nexample, Quicksort and the Simplex algorithm are widely used in practical\napplications, despite their heavy worst-case complexity. Smoothed complexity\naims to better characterize such algorithms. Existing theoretical bounds for\nthe smoothed complexity of sorting algorithms are still quite weak.\nFurthermore, empirically computing the smoothed complexity via its original\ndefinition is computationally infeasible, even for modest input sizes. In this\npaper, we focus on accurately predicting the smoothed complexity of sorting\nalgorithms, using machine learning techniques. We propose two regression models\nthat take into account various properties of sorting algorithms and some of the\nknown theoretical results in smoothed analysis to improve prediction quality.\nWe show experimental results for predicting the smoothed complexity of\nQuicksort, Mergesort, and optimized Bubblesort for large input sizes, therefore\nfilling the gap between known theoretical and empirical results.\n", "versions": [{"version": "v1", "created": "Mon, 23 Mar 2015 09:37:33 GMT"}], "update_date": "2015-03-29", "authors_parsed": [["Shi", "Bichen", ""], ["Schellekens", "Michel", ""], ["Ifrim", "Georgiana", ""]]}, {"id": "1503.06902", "submitter": "Li Zhou", "authors": "Li Zhou", "title": "A Note on Information-Directed Sampling and Thompson Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note introduce three Bayesian style Multi-armed bandit algorithms:\nInformation-directed sampling, Thompson Sampling and Generalized Thompson\nSampling. The goal is to give an intuitive explanation for these three\nalgorithms and their regret bounds, and provide some derivations that are\nomitted in the original papers.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2015 03:26:28 GMT"}], "update_date": "2015-03-25", "authors_parsed": [["Zhou", "Li", ""]]}, {"id": "1503.07159", "submitter": "Preeti Bhargava", "authors": "Preeti Bhargava, Shivsubramani Krishnamoorthy, Ashok Agrawala", "title": "Modeling context and situations in pervasive computing environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In pervasive computing environments, various entities often have to cooperate\nand integrate seamlessly in a \\emph{situation} which can, thus, be considered\nas an amalgamation of the context of several entities interacting and\ncoordinating with each other, and often performing one or more activities.\nHowever, none of the existing context models and ontologies address situation\nmodeling. In this paper, we describe the design, structure and implementation\nof a generic, flexible and extensible context ontology called Rover Context\nModel Ontology (RoCoMO) for context and situation modeling in pervasive\ncomputing systems and environments. We highlight several limitations of the\nexisting context models and ontologies, such as lack of provision for\nprovenance, traceability, quality of context, multiple representation of\ncontextual information, as well as support for security, privacy and\ninteroperability, and explain how we are addressing these limitations in our\napproach. We also illustrate the applicability and utility of RoCoMO using a\npractical and extensive case study.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2015 17:14:02 GMT"}], "update_date": "2015-03-26", "authors_parsed": [["Bhargava", "Preeti", ""], ["Krishnamoorthy", "Shivsubramani", ""], ["Agrawala", "Ashok", ""]]}, {"id": "1503.07206", "submitter": "Guido F.  Montufar", "authors": "Guido Montufar, Keyan Ghazi-Zahedi, Nihat Ay", "title": "Geometry and Determinism of Optimal Stationary Control in Partially\n  Observable Markov Decision Processes", "comments": "25 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that for any finite state Markov decision process (MDP)\nthere is a memoryless deterministic policy that maximizes the expected reward.\nFor partially observable Markov decision processes (POMDPs), optimal memoryless\npolicies are generally stochastic. We study the expected reward optimization\nproblem over the set of memoryless stochastic policies. We formulate this as a\nconstrained linear optimization problem and develop a corresponding geometric\nframework. We show that any POMDP has an optimal memoryless policy of limited\nstochasticity, which allows us to reduce the dimensionality of the search\nspace. Experiments demonstrate that this approach enables better and faster\nconvergence of the policy gradient on the evaluated systems.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2015 21:07:11 GMT"}, {"version": "v2", "created": "Sat, 13 Feb 2016 11:38:37 GMT"}], "update_date": "2016-02-16", "authors_parsed": [["Montufar", "Guido", ""], ["Ghazi-Zahedi", "Keyan", ""], ["Ay", "Nihat", ""]]}, {"id": "1503.07220", "submitter": "Ekhlas Sonu", "authors": "Ekhlas Sonu, Yingke Chen, Prashant Doshi", "title": "Individual Planning in Agent Populations: Exploiting Anonymity and\n  Frame-Action Hypergraphs", "comments": "8 page article plus two page appendix containing proofs in\n  Proceedings of 25th International Conference on Autonomous Planning and\n  Scheduling, 2015", "journal-ref": "In Proceedings of 25th International Conference on Automated\n  Planning and Scheduling, 2015", "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interactive partially observable Markov decision processes (I-POMDP) provide\na formal framework for planning for a self-interested agent in multiagent\nsettings. An agent operating in a multiagent environment must deliberate about\nthe actions that other agents may take and the effect these actions have on the\nenvironment and the rewards it receives. Traditional I-POMDPs model this\ndependence on the actions of other agents using joint action and model spaces.\nTherefore, the solution complexity grows exponentially with the number of\nagents thereby complicating scalability. In this paper, we model and extend\nanonymity and context-specific independence -- problem structures often present\nin agent populations -- for computational gain. We empirically demonstrate the\nefficiency from exploiting these problem structures by solving a new multiagent\nproblem involving more than 1,000 agents.\n", "versions": [{"version": "v1", "created": "Tue, 24 Mar 2015 22:26:50 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2015 22:58:57 GMT"}], "update_date": "2015-04-06", "authors_parsed": [["Sonu", "Ekhlas", ""], ["Chen", "Yingke", ""], ["Doshi", "Prashant", ""]]}, {"id": "1503.07284", "submitter": "Sunil Kumar Kopparapu Dr", "authors": "Arijit De and Sunil Kumar Kopparapu", "title": "A Rule-Based Short Query Intent Identification System", "comments": "5 pages, 2010 International Conference on Signal and Image Processing\n  (ICSIP)", "journal-ref": null, "doi": "10.1109/ICSIP.2010.5697471", "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using SMS (Short Message System), cell phones can be used to query for\ninformation about various topics. In an SMS based search system, one of the key\nproblems is to identify a domain (broad topic) associated with the user query;\nso that a more comprehensive search can be carried out by the domain specific\nsearch engine. In this paper we use a rule based approach, to identify the\ndomain, called Short Query Intent Identification System (SQIIS). We construct\ntwo different rule-bases using different strategies to suit query intent\nidentification. We evaluate the two rule-bases experimentally.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2015 05:35:05 GMT"}], "update_date": "2015-03-26", "authors_parsed": [["De", "Arijit", ""], ["Kopparapu", "Sunil Kumar", ""]]}, {"id": "1503.07294", "submitter": "Issa Atoum", "authors": "Wendy Tan Wei Syn, Bong Chih How, Issa Atoum", "title": "Using Latent Semantic Analysis to Identify Quality in Use (QU)\n  Indicators from User Reviews", "comments": "4 Figures in The International Conference on Artificial Intelligence\n  and Pattern Recognition (AIPR2014),2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper describes a novel approach to categorize users' reviews according\nto the three Quality in Use (QU) indicators defined in ISO: effectiveness,\nefficiency and freedom from risk. With the tremendous amount of reviews\npublished each day, there is a need to automatically summarize user reviews to\ninform us if any of the software able to meet requirement of a company\naccording to the quality requirements. We implemented the method of Latent\nSemantic Analysis (LSA) and its subspace to predict QU indicators. We build a\nreduced dimensionality universal semantic space from Information System\njournals and Amazon reviews. Next, we projected set of indicators' measurement\nscales into the universal semantic space and represent them as subspace. In the\nsubspace, we can map similar measurement scales to the unseen reviews and\npredict the QU indicators. Our preliminary study able to obtain the average of\nF-measure, 0.3627.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2015 06:42:05 GMT"}], "update_date": "2015-03-26", "authors_parsed": [["Syn", "Wendy Tan Wei", ""], ["How", "Bong Chih", ""], ["Atoum", "Issa", ""]]}, {"id": "1503.07341", "submitter": "Catarina Moreira", "authors": "Catarina Moreira", "title": "An Experiment on Using Bayesian Networks for Process Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Process mining is a technique that performs an automatic analysis of business\nprocesses from a log of events with the promise of understanding how processes\nare executed in an organisation.\n  Several models have been proposed to address this problem, however, here we\npropose a different approach to deal with uncertainty. By uncertainty, we mean\nestimating the probability of some sequence of tasks occurring in a business\nprocess, given that only a subset of tasks may be observable.\n  In this sense, this work proposes a new approach to perform process mining\nusing Bayesian Networks. These structures can take into account the probability\nof a task being present or absent in the business process. Moreover, Bayesian\nNetworks are able to automatically learn these probabilities through mechanisms\nsuch as the maximum likelihood estimate and EM clustering.\n  Experiments made over a Loan Application Case study suggest that Bayesian\nNetworks are adequate structures for process mining and enable a deep analysis\nof the business process model that can be used to answer queries about that\nprocess.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2015 11:34:31 GMT"}], "update_date": "2015-03-26", "authors_parsed": [["Moreira", "Catarina", ""]]}, {"id": "1503.07469", "submitter": "Subutai Ahmad", "authors": "Subutai Ahmad and Jeff Hawkins", "title": "Properties of Sparse Distributed Representations and their Application\n  to Hierarchical Temporal Memory", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical evidence demonstrates that every region of the neocortex represents\ninformation using sparse activity patterns. This paper examines Sparse\nDistributed Representations (SDRs), the primary information representation\nstrategy in Hierarchical Temporal Memory (HTM) systems and the neocortex. We\nderive a number of properties that are core to scaling, robustness, and\ngeneralization. We use the theory to provide practical guidelines and\nillustrate the power of SDRs as the basis of HTM. Our goal is to help create a\nunified mathematical and practical framework for SDRs as it relates to cortical\nfunction.\n", "versions": [{"version": "v1", "created": "Wed, 25 Mar 2015 17:36:05 GMT"}], "update_date": "2015-03-26", "authors_parsed": [["Ahmad", "Subutai", ""], ["Hawkins", "Jeff", ""]]}, {"id": "1503.07587", "submitter": "Jose Hernandez-Orallo", "authors": "Jose Hernandez-Orallo", "title": "Universal Psychometrics Tasks: difficulty, composition and decomposition", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note revisits the concepts of task and difficulty. The notion of\ncognitive task and its use for the evaluation of intelligent systems is still\nreplete with issues. The view of tasks as MDP in the context of reinforcement\nlearning has been especially useful for the formalisation of learning tasks.\nHowever, this alternate interaction does not accommodate well for some other\ntasks that are usual in artificial intelligence and, most especially, in animal\nand human evaluation. In particular, we want to have a more general account of\nepisodes, rewards and responses, and, most especially, the computational\ncomplexity of the algorithm behind an agent solving a task. This is crucial for\nthe determination of the difficulty of a task as the (logarithm of the) number\nof computational steps required to acquire an acceptable policy for the task,\nwhich includes the exploration of policies and their verification. We introduce\na notion of asynchronous-time stochastic tasks. Based on this interpretation,\nwe can see what task difficulty is, what instance difficulty is (relative to a\ntask) and also what task compositions and decompositions are.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2015 00:34:34 GMT"}], "update_date": "2015-03-27", "authors_parsed": [["Hernandez-Orallo", "Jose", ""]]}, {"id": "1503.07609", "submitter": "Yanping Liu", "authors": "Yanping Liu, Erik D. Reichle", "title": "An Evolutionary Algorithm for Error-Driven Learning via Reinforcement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although different learning systems are coordinated to afford complex\nbehavior, little is known about how this occurs. This article describes a\ntheoretical framework that specifies how complex behaviors that might be\nthought to require error-driven learning might instead be acquired through\nsimple reinforcement. This framework includes specific assumptions about the\nmechanisms that contribute to the evolution of (artificial) neural networks to\ngenerate topologies that allow the networks to learn large-scale complex\nproblems using only information about the quality of their performance. The\npractical and theoretical implications of the framework are discussed, as are\npossible biological analogs of the approach.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2015 03:33:47 GMT"}], "update_date": "2015-03-27", "authors_parsed": [["Liu", "Yanping", ""], ["Reichle", "Erik D.", ""]]}, {"id": "1503.07715", "submitter": "Daniel Kovach Jr.", "authors": "Daniel Kovach", "title": "The Computational Theory of Intelligence: Data Aggregation", "comments": "Published in IJMNTA", "journal-ref": null, "doi": "10.4236/ijmnta.2014.34016", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we will expound upon the concepts proffered in [1], where we\nproposed an information theoretic approach to intelligence in the computational\nsense. We will examine data and meme aggregation, and study the effect of\nlimited resources on the resulting meme amplitudes.\n", "versions": [{"version": "v1", "created": "Wed, 24 Dec 2014 07:47:46 GMT"}], "update_date": "2015-03-27", "authors_parsed": [["Kovach", "Daniel", ""]]}, {"id": "1503.07717", "submitter": "Claire Lef\\`evre", "authors": "Claire Lef\\`evre, Christopher B\\'eatrix, Igor St\\'ephan, Laurent\n  Garcia", "title": "ASPeRiX, a First Order Forward Chaining Approach for Answer Set\n  Computing", "comments": "50 pages. To appear in Theory and Practice of Logic Programming\n  (TPLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The natural way to use Answer Set Programming (ASP) to represent knowledge in\nArtificial Intelligence or to solve a combinatorial problem is to elaborate a\nfirst order logic program with default negation. In a preliminary step this\nprogram with variables is translated in an equivalent propositional one by a\nfirst tool: the grounder. Then, the propositional program is given to a second\ntool: the solver. This last one computes (if they exist) one or many answer\nsets (stable models) of the program, each answer set encoding one solution of\nthe initial problem. Until today, almost all ASP systems apply this two steps\ncomputation. In this article, the project ASPeRiX is presented as a first order\nforward chaining approach for Answer Set Computing. This project was amongst\nthe first to introduce an approach of answer set computing that escapes the\npreliminary phase of rule instantiation by integrating it in the search\nprocess. The methodology applies a forward chaining of first order rules that\nare grounded on the fly by means of previously produced atoms. Theoretical\nfoundations of the approach are presented, the main algorithms of the ASP\nsolver ASPeRiX are detailed and some experiments and comparisons with existing\nsystems are provided.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2015 12:57:22 GMT"}, {"version": "v2", "created": "Wed, 16 Nov 2016 15:08:22 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Lef\u00e8vre", "Claire", ""], ["B\u00e9atrix", "Christopher", ""], ["St\u00e9phan", "Igor", ""], ["Garcia", "Laurent", ""]]}, {"id": "1503.07845", "submitter": "Heike Trautmann", "authors": "Luis Marti, Christian Grimme, Pascal Kerschke, Heike Trautmann,\n  G\\\"unter Rudolph", "title": "Averaged Hausdorff Approximations of Pareto Fronts based on\n  Multiobjective Estimation of Distribution Algorithms", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the a posteriori approach of multiobjective optimization the Pareto front\nis approximated by a finite set of solutions in the objective space. The\nquality of the approximation can be measured by different indicators that take\ninto account the approximation's closeness to the Pareto front and its\ndistribution along the Pareto front. In particular, the averaged Hausdorff\nindicator prefers an almost uniform distribution. An observed drawback of\nmultiobjective estimation of distribution algorithms (MEDAs) is that - as\ncommon for randomized metaheuristics - the final population usually is not\nuniformly distributed along the Pareto front. Therefore, we propose a\npostprocessing strategy which consists of applying the averaged Hausdorff\nindicator to the complete archive of generated solutions after optimization in\norder to select a uniformly distributed subset of nondominated solutions from\nthe archive. In this paper, we put forward a strategy for extracting the above\ndescribed subset. The effectiveness of the proposal is contrasted in a series\nof experiments that involve different MEDAs and filtering techniques.\n", "versions": [{"version": "v1", "created": "Thu, 26 Mar 2015 19:44:48 GMT"}], "update_date": "2015-03-27", "authors_parsed": [["Marti", "Luis", ""], ["Grimme", "Christian", ""], ["Kerschke", "Pascal", ""], ["Trautmann", "Heike", ""], ["Rudolph", "G\u00fcnter", ""]]}, {"id": "1503.08141", "submitter": "Bryan Renne", "authors": "Alexandru Baltag, Bryan Renne, Sonja Smets", "title": "Revisable Justified Belief: Preliminary Report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The theory $\\mathsf{CDL}$ of Conditional Doxastic Logic is the single-agent\nversion of Board's multi-agent theory $\\mathsf{BRSIC}$ of conditional belief.\n$\\mathsf{CDL}$ may be viewed as a version of AGM belief revision theory in\nwhich Boolean combinations of revisions are expressible in the language. We\nintroduce a theory $\\mathsf{JCDL}$ of Justified Conditional Doxastic Logic that\nreplaces conditional belief formulas $B^\\psi\\varphi$ by expressions\n$t{\\,:^{\\psi}}\\varphi$ made up of a term $t$ whose syntactic structure suggests\na derivation of the belief $\\varphi$ after revision by $\\psi$. This allows us\nto think of terms $t$ as reasons justifying a belief in various formulas after\na revision takes place. We show that $\\mathsf{JCDL}$-theorems are the exact\nanalogs of $\\mathsf{CDL}$-theorems, and that this result holds the other way\naround as well. This allows us to think of $\\mathsf{JCDL}$ as a theory of\nrevisable justified belief.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2015 16:46:04 GMT"}], "update_date": "2015-03-30", "authors_parsed": [["Baltag", "Alexandru", ""], ["Renne", "Bryan", ""], ["Smets", "Sonja", ""]]}, {"id": "1503.08155", "submitter": "Miao Fan", "authors": "Miao Fan, Qiang Zhou and Thomas Fang Zheng", "title": "Learning Embedding Representations for Knowledge Inference on Imperfect\n  and Incomplete Repositories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of knowledge inference on large-scale\nimperfect repositories with incomplete coverage by means of embedding entities\nand relations at the first attempt. We propose IIKE (Imperfect and Incomplete\nKnowledge Embedding), a probabilistic model which measures the probability of\neach belief, i.e. $\\langle h,r,t\\rangle$, in large-scale knowledge bases such\nas NELL and Freebase, and our objective is to learn a better low-dimensional\nvector representation for each entity ($h$ and $t$) and relation ($r$) in the\nprocess of minimizing the loss of fitting the corresponding confidence given by\nmachine learning (NELL) or crowdsouring (Freebase), so that we can use $||{\\bf\nh} + {\\bf r} - {\\bf t}||$ to assess the plausibility of a belief when\nconducting inference. We use subsets of those inexact knowledge bases to train\nour model and test the performances of link prediction and triplet\nclassification on ground truth beliefs, respectively. The results of extensive\nexperiments show that IIKE achieves significant improvement compared with the\nbaseline and state-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Fri, 27 Mar 2015 17:13:03 GMT"}], "update_date": "2015-03-30", "authors_parsed": [["Fan", "Miao", ""], ["Zhou", "Qiang", ""], ["Zheng", "Thomas Fang", ""]]}, {"id": "1503.08275", "submitter": "Rosemarie Velik", "authors": "Rosemarie Velik, Pascal Nicolay", "title": "Energy Management in Storage-Augmented, Grid-Connected Prosumer\n  Buildings and Neighbourhoods Using a Modified Simulated Annealing\n  Optimization", "comments": "Computers & Operations Research, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article introduces a modified simulated annealing optimization approach\nfor automatically determining optimal energy management strategies in\ngrid-connected, storage-augmented, photovoltaics-supplied prosumer buildings\nand neighbourhoods based on user-specific goals. For evaluating the modified\nsimulated annealing optimizer, a number of test scenarios in the field of\nenergy self-consumption maximization are defined and results are compared to a\ngradient descent and a total state space search approach. The benchmarking\nagainst these two reference methods demonstrates that the modified simulated\nannealing approach is able to find significantly better solutions than the\ngradient descent algorithm - being equal or very close to the global optimum -\nwith significantly less computational effort and processing time than the total\nstate space search approach.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2015 07:16:22 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Velik", "Rosemarie", ""], ["Nicolay", "Pascal", ""]]}, {"id": "1503.08289", "submitter": "Matteo Brunelli", "authors": "Matteo Brunelli", "title": "Recent advances on inconsistency indices for pairwise comparisons - a\n  commentary", "comments": "13 pages, 2 figures", "journal-ref": "Fundamenta Informaticae, 144(3-4), 321-332, 2016", "doi": "10.3233/FI-2016-1338", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper recalls the definition of consistency for pairwise comparison\nmatrices and briefly presents the concept of inconsistency index in connection\nto other aspects of the theory of pairwise comparisons. By commenting on a\nrecent contribution by Koczkodaj and Szwarc, it will be shown that the\ndiscussion on inconsistency indices is far from being over, and the ground is\nstill fertile for debates.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2015 10:24:43 GMT"}, {"version": "v2", "created": "Tue, 28 Jul 2015 08:56:35 GMT"}, {"version": "v3", "created": "Thu, 10 Mar 2016 14:47:12 GMT"}], "update_date": "2016-03-11", "authors_parsed": [["Brunelli", "Matteo", ""]]}, {"id": "1503.08345", "submitter": "Pravendra Singh", "authors": "Pravendra Singh", "title": "Implementing an intelligent version of the classical sliding-puzzle game\n  for unix terminals using Golang's concurrency primitives", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An intelligent version of the sliding-puzzle game is developed using the new\nGo programming language, which uses a concurrent version of the A* Informed\nSearch Algorithm to power solver-bot that runs in the background. The game runs\nin computer system's terminals. Mainly, it was developed for UNIX-type systems\nbut it works pretty well in nearly all the operating systems because of\ncross-platform compatibility of the programming language used. The game uses\nlanguage's concurrency primitives to simplify most of the hefty parts of the\ngame. A real-time notification delivery architecture is developed using\nlanguage's built-in concurrency support, which performs similar to event based\ncontext aware invocations like we see on the web platform.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2015 20:35:02 GMT"}, {"version": "v2", "created": "Sat, 22 Aug 2015 17:07:32 GMT"}], "update_date": "2015-08-25", "authors_parsed": [["Singh", "Pravendra", ""]]}, {"id": "1503.08363", "submitter": "Ravi Ganti", "authors": "Ravi Ganti", "title": "Active Model Aggregation via Stochastic Mirror Descent", "comments": "12 pages, 20 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning convex aggregation of models, that is as\ngood as the best convex aggregation, for the binary classification problem.\nWorking in the stream based active learning setting, where the active learner\nhas to make a decision on-the-fly, if it wants to query for the label of the\npoint currently seen in the stream, we propose a stochastic-mirror descent\nalgorithm, called SMD-AMA, with entropy regularization. We establish an excess\nrisk bounds for the loss of the convex aggregate returned by SMD-AMA to be of\nthe order of $O\\left(\\sqrt{\\frac{\\log(M)}{{T^{1-\\mu}}}}\\right)$, where $\\mu\\in\n[0,1)$ is an algorithm dependent parameter, that trades-off the number of\nlabels queried, and excess risk.\n", "versions": [{"version": "v1", "created": "Sat, 28 Mar 2015 22:54:12 GMT"}], "update_date": "2015-03-31", "authors_parsed": [["Ganti", "Ravi", ""]]}, {"id": "1503.08381", "submitter": "Xu Sun", "authors": "Xu Sun, Shuming Ma, Yi Zhang, Xuancheng Ren", "title": "Towards Easier and Faster Sequence Labeling for Natural Language\n  Processing: A Search-based Probabilistic Online Learning Framework (SAPO)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are two major approaches for sequence labeling. One is the\nprobabilistic gradient-based methods such as conditional random fields (CRF)\nand neural networks (e.g., RNN), which have high accuracy but drawbacks: slow\ntraining, and no support of search-based optimization (which is important in\nmany cases). The other is the search-based learning methods such as structured\nperceptron and margin infused relaxed algorithm (MIRA), which have fast\ntraining but also drawbacks: low accuracy, no probabilistic information, and\nnon-convergence in real-world tasks. We propose a novel and \"easy\" solution, a\nsearch-based probabilistic online learning method, to address most of those\nissues. The method is \"easy\", because the optimization algorithm at the\ntraining stage is as simple as the decoding algorithm at the test stage. This\nmethod searches the output candidates, derives probabilities, and conducts\nefficient online learning. We show that this method with fast training and\ntheoretical guarantee of convergence, which is easy to implement, can support\nsearch-based optimization and obtain top accuracy. Experiments on well-known\ntasks show that our method has better accuracy than CRF and BiLSTM\\footnote{The\nSAPO code is released at \\url{https://github.com/lancopku/SAPO}.}.\n", "versions": [{"version": "v1", "created": "Sun, 29 Mar 2015 03:41:03 GMT"}, {"version": "v2", "created": "Tue, 6 Feb 2018 02:20:57 GMT"}, {"version": "v3", "created": "Wed, 18 Apr 2018 01:30:13 GMT"}, {"version": "v4", "created": "Mon, 19 Nov 2018 11:11:36 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Sun", "Xu", ""], ["Ma", "Shuming", ""], ["Zhang", "Yi", ""], ["Ren", "Xuancheng", ""]]}, {"id": "1503.09105", "submitter": "Prasenjit Karmakar", "authors": "Prasenjit Karmakar and Shalabh Bhatnagar", "title": "Two Timescale Stochastic Approximation with Controlled Markov noise and\n  Off-policy temporal difference learning", "comments": "23 pages (relaxed some important assumptions from the previous\n  version), accepted in Mathematics of Operations Research in Feb, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.DS cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present for the first time an asymptotic convergence analysis of two\ntime-scale stochastic approximation driven by `controlled' Markov noise. In\nparticular, both the faster and slower recursions have non-additive controlled\nMarkov noise components in addition to martingale difference noise. We analyze\nthe asymptotic behavior of our framework by relating it to limiting\ndifferential inclusions in both time-scales that are defined in terms of the\nergodic occupation measures associated with the controlled Markov processes.\nFinally, we present a solution to the off-policy convergence problem for\ntemporal difference learning with linear function approximation, using our\nresults.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2015 16:10:55 GMT"}, {"version": "v10", "created": "Sat, 26 Mar 2016 04:53:48 GMT"}, {"version": "v11", "created": "Sun, 17 Apr 2016 13:11:17 GMT"}, {"version": "v12", "created": "Thu, 16 Feb 2017 09:37:38 GMT"}, {"version": "v13", "created": "Wed, 22 Feb 2017 17:06:39 GMT"}, {"version": "v14", "created": "Sat, 25 Feb 2017 18:46:13 GMT"}, {"version": "v2", "created": "Thu, 2 Apr 2015 17:18:37 GMT"}, {"version": "v3", "created": "Thu, 30 Apr 2015 04:11:39 GMT"}, {"version": "v4", "created": "Tue, 4 Aug 2015 12:49:32 GMT"}, {"version": "v5", "created": "Wed, 5 Aug 2015 14:02:19 GMT"}, {"version": "v6", "created": "Thu, 6 Aug 2015 12:53:51 GMT"}, {"version": "v7", "created": "Fri, 1 Jan 2016 12:10:22 GMT"}, {"version": "v8", "created": "Mon, 18 Jan 2016 15:29:21 GMT"}, {"version": "v9", "created": "Mon, 21 Mar 2016 19:25:28 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Karmakar", "Prasenjit", ""], ["Bhatnagar", "Shalabh", ""]]}, {"id": "1503.09137", "submitter": "Vit Novacek", "authors": "Vit Novacek", "title": "Formalising Hypothesis Virtues in Knowledge Graphs: A General\n  Theoretical Framework and its Validation in Literature-Based Discovery\n  Experiments", "comments": "Pre-print of an article submitted to Artificial Intelligence Journal\n  (after the manuscript has been refused by the editors of Journal of Web\n  Semantics before the peer review process due to being out of scope for that\n  journal)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an approach to discovery informatics that uses so called\nknowledge graphs as the essential representation structure. Knowledge graph is\nan umbrella term that subsumes various approaches to tractable representation\nof large volumes of loosely structured knowledge in a graph form. It has been\nused primarily in the Web and Linked Open Data contexts, but is applicable to\nany other area dealing with knowledge representation. In the perspective of our\napproach motivated by the challenges of discovery informatics, knowledge graphs\ncorrespond to hypotheses. We present a framework for formalising so called\nhypothesis virtues within knowledge graphs. The framework is based on a classic\nwork in philosophy of science, and naturally progresses from mostly informative\nfoundational notions to actionable specifications of measures corresponding to\nparticular virtues. These measures can consequently be used to determine\nrefined sub-sets of knowledge graphs that have large relative potential for\nmaking discoveries. We validate the proposed framework by experiments in\nliterature-based discovery. The experiments have demonstrated the utility of\nour work and its superiority w.r.t. related approaches.\n", "versions": [{"version": "v1", "created": "Tue, 31 Mar 2015 17:29:58 GMT"}, {"version": "v2", "created": "Tue, 28 Apr 2015 11:51:12 GMT"}], "update_date": "2015-04-29", "authors_parsed": [["Novacek", "Vit", ""]]}]