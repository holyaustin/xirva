[{"id": "0809.0271", "submitter": "Martin Josef Geiger", "authors": "Martin Josef Geiger", "title": "Randomised Variable Neighbourhood Search for Multi Objective\n  Optimisation", "comments": null, "journal-ref": "Proceedings of the 4th EU/ME Workshop: Design and Evaluation of\n  Advanced Hybrid Meta-Heuristics, November 4--5, Nottingham, United Kingdom,\n  pp. 34-42", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various local search approaches have recently been applied to machine\nscheduling problems under multiple objectives. Their foremost consideration is\nthe identification of the set of Pareto optimal alternatives. An important\naspect of successfully solving these problems lies in the definition of an\nappropriate neighbourhood structure. Unclear in this context remains, how\ninterdependencies within the fitness landscape affect the resolution of the\nproblem.\n  The paper presents a study of neighbourhood search operators for multiple\nobjective flow shop scheduling. Experiments have been carried out with twelve\ndifferent combinations of criteria. To derive exact conclusions, small problem\ninstances, for which the optimal solutions are known, have been chosen.\nStatistical tests show that no single neighbourhood operator is able to equally\nidentify all Pareto optimal alternatives. Significant improvements however have\nbeen obtained by hybridising the solution algorithm using a randomised variable\nneighbourhood search technique.\n", "versions": [{"version": "v1", "created": "Mon, 1 Sep 2008 15:32:49 GMT"}], "update_date": "2008-09-02", "authors_parsed": [["Geiger", "Martin Josef", ""]]}, {"id": "0809.0406", "submitter": "Martin Josef Geiger", "authors": "Martin Josef Geiger", "title": "Foundations of the Pareto Iterated Local Search Metaheuristic", "comments": "Proceedings of the 18th International Conference on Multiple Criteria\n  Decision Making, Chania, Greece, June 19-23, 2006", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper describes the proposition and application of a local search\nmetaheuristic for multi-objective optimization problems. It is based on two\nmain principles of heuristic search, intensification through variable\nneighborhoods, and diversification through perturbations and successive\niterations in favorable regions of the search space. The concept is\nsuccessfully tested on permutation flow shop scheduling problems under multiple\nobjectives. While the obtained results are encouraging in terms of their\nquality, another positive attribute of the approach is its' simplicity as it\ndoes require the setting of only very few parameters. The implementation of the\nPareto Iterated Local Search metaheuristic is based on the MOOPPS computer\nsystem of local search heuristics for multi-objective scheduling which has been\nawarded the European Academic Software Award 2002 in Ronneby, Sweden\n(http://www.easa-award.net/, http://www.bth.se/llab/easa_2002.nsf)\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2008 11:29:45 GMT"}], "update_date": "2008-09-03", "authors_parsed": [["Geiger", "Martin Josef", ""]]}, {"id": "0809.0410", "submitter": "Martin Josef Geiger", "authors": "Martin Josef Geiger", "title": "A Computational Study of Genetic Crossover Operators for Multi-Objective\n  Vehicle Routing Problem with Soft Time Windows", "comments": null, "journal-ref": "Habenicht, W. et al. (eds.): Multi-Criteria- und Fuzzy Systeme in\n  Theorie und Praxis-Loesungsansaetze fuer Entscheidungsprobleme mit komplexen\n  Zielsystemen, 2003, ISBN 3-8244-7864-1, pp. 191-207", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article describes an investigation of the effectiveness of genetic\nalgorithms for multi-objective combinatorial optimization (MOCO) by presenting\nan application for the vehicle routing problem with soft time windows. The work\nis motivated by the question, if and how the problem structure influences the\neffectiveness of different configurations of the genetic algorithm.\nComputational results are presented for different classes of vehicle routing\nproblems, varying in their coverage with time windows, time window size,\ndistribution and number of customers. The results are compared with a simple,\nbut effective local search approach for multi-objective combinatorial\noptimization problems.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2008 11:39:52 GMT"}], "update_date": "2008-09-03", "authors_parsed": [["Geiger", "Martin Josef", ""]]}, {"id": "0809.0416", "submitter": "Martin Josef Geiger", "authors": "Martin Josef Geiger", "title": "Genetic Algorithms for multiple objective vehicle routing", "comments": null, "journal-ref": "Proceedings of the Metaheuristics International Conference\n  MIC'2001, Porto, Portugal, pp. 349-353", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The talk describes a general approach of a genetic algorithm for multiple\nobjective optimization problems. A particular dominance relation between the\nindividuals of the population is used to define a fitness operator, enabling\nthe genetic algorithm to adress even problems with efficient, but\nconvex-dominated alternatives. The algorithm is implemented in a multilingual\ncomputer program, solving vehicle routing problems with time windows under\nmultiple objectives. The graphical user interface of the program shows the\nprogress of the genetic algorithm and the main parameters of the approach can\nbe easily modified. In addition to that, the program provides powerful decision\nsupport to the decision maker. The software has proved it's excellence at the\nfinals of the European Academic Software Award EASA, held at the Keble college/\nUniversity of Oxford/ Great Britain.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2008 12:08:56 GMT"}], "update_date": "2008-09-03", "authors_parsed": [["Geiger", "Martin Josef", ""]]}, {"id": "0809.0448", "submitter": "Eric Engle", "authors": "Eric Engle", "title": "The Stock Market as a Game: An Agent Based Approach to Trading in Stocks", "comments": "21 pages and accompanying program", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.TR cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Just as war is sometimes fallaciously represented as a zero sum game -- when\nin fact war is a negative sum game - stock market trading, a positive sum game\nover time, is often erroneously represented as a zero sum game. This is called\nthe \"zero sum fallacy\" -- the erroneous belief that one trader in a stock\nmarket exchange can only improve their position provided some other trader's\nposition deteriorates. However, a positive sum game in absolute terms can be\nrecast as a zero sum game in relative terms. Similarly it appears that negative\nsum games in absolute terms have been recast as zero sum games in relative\nterms: otherwise, why would zero sum games be used to represent situations of\nwar? Such recasting may have heuristic or pedagogic interest but recasting must\nbe clearly explicited or risks generating confusion.\n  Keywords: Game theory, stock trading and agent based AI.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2008 14:58:00 GMT"}], "update_date": "2008-12-02", "authors_parsed": [["Engle", "Eric", ""]]}, {"id": "0809.0458", "submitter": "Eric Engle", "authors": "Eric Engle", "title": "Agent Models of Political Interactions", "comments": "22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Looks at state interactions from an agent based AI perspective to see state\ninteractions as an example of emergent intelligent behavior. Exposes basic\nprinciples of game theory.\n", "versions": [{"version": "v1", "created": "Tue, 2 Sep 2008 16:06:40 GMT"}], "update_date": "2008-09-03", "authors_parsed": [["Engle", "Eric", ""]]}, {"id": "0809.0610", "submitter": "Martin Josef Geiger", "authors": "Martin Josef Geiger, Wolf Wenger", "title": "A framework for the interactive resolution of multi-objective vehicle\n  routing problems", "comments": "Proceedings of the 7th EU/ME Workshop: Adaptive, Self-Adaptive, and\n  Multi-Level Metaheuristics, Malaga, Spain, November 16-17, 2006", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article presents a framework for the resolution of rich vehicle routing\nproblems which are difficult to address with standard optimization techniques.\nWe use local search on the basis on variable neighborhood search for the\nconstruction of the solutions, but embed the techniques in a flexible framework\nthat allows the consideration of complex side constraints of the problem such\nas time windows, multiple depots, heterogeneous fleets, and, in particular,\nmultiple optimization criteria. In order to identify a compromise alternative\nthat meets the requirements of the decision maker, an interactive procedure is\nintegrated in the resolution of the problem, allowing the modification of the\npreference information articulated by the decision maker. The framework is\nprototypically implemented in a computer system. First results of test runs on\nmultiple depot vehicle routing problems with time windows are reported.\n", "versions": [{"version": "v1", "created": "Wed, 3 Sep 2008 12:22:08 GMT"}], "update_date": "2008-09-04", "authors_parsed": [["Geiger", "Martin Josef", ""], ["Wenger", "Wolf", ""]]}, {"id": "0809.0662", "submitter": "Martin Josef Geiger", "authors": "Martin Josef Geiger, Sanja Petrovic", "title": "Improving Local Search for Fuzzy Scheduling Problems", "comments": null, "journal-ref": "Proceedings of the Post Graduate Research Conference in\n  Electronics, Photonics, Communications & Networks and Computing Science PREP\n  2004, University of Hertfordshire, Great Britain, pp. 146-147", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The integration of fuzzy set theory and fuzzy logic into scheduling is a\nrather new aspect with growing importance for manufacturing applications,\nresulting in various unsolved aspects. In the current paper, we investigate an\nimproved local search technique for fuzzy scheduling problems with fitness\nplateaus, using a multi criteria formulation of the problem. We especially\naddress the problem of changing job priorities over time as studied at the\nSherwood Press Ltd, a Nottingham based printing company, who is a collaborator\non the project.\n", "versions": [{"version": "v1", "created": "Wed, 3 Sep 2008 16:16:43 GMT"}], "update_date": "2008-09-04", "authors_parsed": [["Geiger", "Martin Josef", ""], ["Petrovic", "Sanja", ""]]}, {"id": "0809.0753", "submitter": "Martin Josef Geiger", "authors": "Martin Josef Geiger", "title": "Proposition of the Interactive Pareto Iterated Local Search Procedure -\n  Elements and Initial Experiments", "comments": null, "journal-ref": "The Fourth International Conference on Evolutionary\n  Multi-Criterion Optimization: Late Breaking Papers, Matsushima, Japan, March\n  2007, pp. 19-23", "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article presents an approach to interactively solve multi-objective\noptimization problems. While the identification of efficient solutions is\nsupported by computational intelligence techniques on the basis of local\nsearch, the search is directed by partial preference information obtained from\nthe decision maker.\n  An application of the approach to biobjective portfolio optimization, modeled\nas the well-known knapsack problem, is reported, and experimental results are\nreported for benchmark instances taken from the literature. In brief, we obtain\nencouraging results that show the applicability of the approach to the\ndescribed problem.\n", "versions": [{"version": "v1", "created": "Thu, 4 Sep 2008 06:52:16 GMT"}], "update_date": "2008-09-05", "authors_parsed": [["Geiger", "Martin Josef", ""]]}, {"id": "0809.0755", "submitter": "Martin Josef Geiger", "authors": "Martin Josef Geiger", "title": "Bin Packing Under Multiple Objectives - a Heuristic Approximation\n  Approach", "comments": null, "journal-ref": "The Fourth International Conference on Evolutionary\n  Multi-Criterion Optimization: Late Breaking Papers, Matsushima, Japan, March\n  2007, pp. 53-56", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article proposes a heuristic approximation approach to the bin packing\nproblem under multiple objectives. In addition to the traditional objective of\nminimizing the number of bins, the heterogeneousness of the elements in each\nbin is minimized, leading to a biobjective formulation of the problem with a\ntradeoff between the number of bins and their heterogeneousness. An extension\nof the Best-Fit approximation algorithm is presented to solve the problem.\nExperimental investigations have been carried out on benchmark instances of\ndifferent size, ranging from 100 to 1000 items. Encouraging results have been\nobtained, showing the applicability of the heuristic approach to the described\nproblem.\n", "versions": [{"version": "v1", "created": "Thu, 4 Sep 2008 07:02:29 GMT"}], "update_date": "2008-09-05", "authors_parsed": [["Geiger", "Martin Josef", ""]]}, {"id": "0809.0757", "submitter": "Martin Josef Geiger", "authors": "Martin Josef Geiger", "title": "An application of the Threshold Accepting metaheuristic for curriculum\n  based course timetabling", "comments": null, "journal-ref": "Proceedings of the 7th International Conference on the Practice\n  and Theory of Automated Timetabling PATAT 2008, August 19-22, Montreal,\n  Canada", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article presents a local search approach for the solution of timetabling\nproblems in general, with a particular implementation for competition track 3\nof the International Timetabling Competition 2007 (ITC 2007). The heuristic\nsearch procedure is based on Threshold Accepting to overcome local optima. A\nstochastic neighborhood is proposed and implemented, randomly removing and\nreassigning events from the current solution.\n  The overall concept has been incrementally obtained from a series of\nexperiments, which we describe in each (sub)section of the paper. In result, we\nsuccessfully derived a potential candidate solution approach for the finals of\ntrack 3 of the ITC 2007.\n", "versions": [{"version": "v1", "created": "Thu, 4 Sep 2008 07:12:02 GMT"}], "update_date": "2008-10-02", "authors_parsed": [["Geiger", "Martin Josef", ""]]}, {"id": "0809.0788", "submitter": "Hubie Chen", "authors": "Manuel Bodirsky and Hubie Chen", "title": "Peek Arc Consistency", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies peek arc consistency, a reasoning technique that extends\nthe well-known arc consistency technique for constraint satisfaction. In\ncontrast to other more costly extensions of arc consistency that have been\nstudied in the literature, peek arc consistency requires only linear space and\nquadratic time and can be parallelized in a straightforward way such that it\nruns in linear time with a linear number of processors. We demonstrate that for\nvarious constraint languages, peek arc consistency gives a polynomial-time\ndecision procedure for the constraint satisfaction problem. We also present an\nalgebraic characterization of those constraint languages that can be solved by\npeek arc consistency, and study the robustness of the algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 4 Sep 2008 11:15:50 GMT"}, {"version": "v2", "created": "Fri, 3 Feb 2012 18:53:00 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Bodirsky", "Manuel", ""], ["Chen", "Hubie", ""]]}, {"id": "0809.0922", "submitter": "Matthias Horbach", "authors": "Matthias Horbach, Christoph Weidenbach", "title": "Superposition for Fixed Domains", "comments": "34 pages; to appear in ACM Transactions on Computational Logic", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Superposition is an established decision procedure for a variety of\nfirst-order logic theories represented by sets of clauses. A satisfiable\ntheory, saturated by superposition, implicitly defines a minimal term-generated\nmodel for the theory. Proving universal properties with respect to a saturated\ntheory directly leads to a modification of the minimal model's term-generated\ndomain, as new Skolem functions are introduced. For many applications, this is\nnot desired.\n  Therefore, we propose the first superposition calculus that can explicitly\nrepresent existentially quantified variables and can thus compute with respect\nto a given domain. This calculus is sound and refutationally complete in the\nlimit for a first-order fixed domain semantics. For saturated Horn theories and\nclasses of positive formulas, we can even employ the calculus to prove\nproperties of the minimal model itself, going beyond the scope of known\nsuperposition-based approaches.\n", "versions": [{"version": "v1", "created": "Thu, 4 Sep 2008 21:58:58 GMT"}, {"version": "v2", "created": "Thu, 27 Aug 2009 11:40:16 GMT"}, {"version": "v3", "created": "Mon, 30 Nov 2009 12:43:51 GMT"}], "update_date": "2009-11-30", "authors_parsed": [["Horbach", "Matthias", ""], ["Weidenbach", "Christoph", ""]]}, {"id": "0809.0961", "submitter": "Martin Josef Geiger", "authors": "Martin Josef Geiger", "title": "MOOPPS: An Optimization System for Multi Objective Scheduling", "comments": null, "journal-ref": "Proceedings of the Metaheuristics International Conference MIC\n  2005, Vienna, Austria, pp. 403-408", "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the current paper, we present an optimization system solving multi\nobjective production scheduling problems (MOOPPS). The identification of Pareto\noptimal alternatives or at least a close approximation of them is possible by a\nset of implemented metaheuristics. Necessary control parameters can easily be\nadjusted by the decision maker as the whole software is fully menu driven. This\nallows the comparison of different metaheuristic algorithms for the considered\nproblem instances. Results are visualized by a graphical user interface showing\nthe distribution of solutions in outcome space as well as their corresponding\nGantt chart representation.\n  The identification of a most preferred solution from the set of efficient\nsolutions is supported by a module based on the aspiration interactive method\n(AIM). The decision maker successively defines aspiration levels until a single\nsolution is chosen.\n  After successfully competing in the finals in Ronneby, Sweden, the MOOPPS\nsoftware has been awarded the European Academic Software Award 2002\n(http://www.bth.se/llab/easa_2002.nsf)\n", "versions": [{"version": "v1", "created": "Fri, 5 Sep 2008 06:22:36 GMT"}], "update_date": "2008-09-08", "authors_parsed": [["Geiger", "Martin Josef", ""]]}, {"id": "0809.1077", "submitter": "Martin Josef Geiger", "authors": "Martin Josef Geiger, Wolf Wenger", "title": "Variable Neighborhood Search for the University Lecturer-Student\n  Assignment Problem", "comments": "Proceedings of the 18th Mini Euro Conference on Variable Neighborhood\n  Search, November 23-25, 2005, Puerto de La Cruz, Tenerife, Spain, ISBN\n  84-689-5679-1", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper presents a study of local search heuristics in general and variable\nneighborhood search in particular for the resolution of an assignment problem\nstudied in the practical work of universities. Here, students have to be\nassigned to scientific topics which are proposed and supported by members of\nstaff. The problem involves the optimization under given preferences of\nstudents which may be expressed when applying for certain topics.\n  It is possible to observe that variable neighborhood search leads to superior\nresults for the tested problem instances. One instance is taken from an actual\ncase, while others have been generated based on the real world data to support\nthe analysis with a deeper analysis.\n  An extension of the problem has been formulated by integrating a second\nobjective function that simultaneously balances the workload of the members of\nstaff while maximizing utility of the students. The algorithmic approach has\nbeen prototypically implemented in a computer system. One important aspect in\nthis context is the application of the research work to problems of other\nscientific institutions, and therefore the provision of decision support\nfunctionalities.\n", "versions": [{"version": "v1", "created": "Fri, 5 Sep 2008 17:02:55 GMT"}], "update_date": "2008-09-08", "authors_parsed": [["Geiger", "Martin Josef", ""], ["Wenger", "Wolf", ""]]}, {"id": "0809.1226", "submitter": "Boris Ryabko", "authors": "Boris Ryabko", "title": "Applications of Universal Source Coding to Statistical Analysis of Time\n  Series", "comments": "accepted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AI math.IT math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how universal codes can be used for solving some of the most\nimportant statistical problems for time series. By definition, a universal code\n(or a universal lossless data compressor) can compress any sequence generated\nby a stationary and ergodic source asymptotically to the Shannon entropy,\nwhich, in turn, is the best achievable ratio for lossless data compressors.\n  We consider finite-alphabet and real-valued time series and the following\nproblems: estimation of the limiting probabilities for finite-alphabet time\nseries and estimation of the density for real-valued time series, the on-line\nprediction, regression, classification (or problems with side information) for\nboth types of the time series and the following problems of hypothesis testing:\ngoodness-of-fit testing, or identity testing, and testing of serial\nindependence. It is important to note that all problems are considered in the\nframework of classical mathematical statistics and, on the other hand, everyday\nmethods of data compression (or archivers) can be used as a tool for the\nestimation and testing. It turns out, that quite often the suggested methods\nand tests are more powerful than known ones when they are applied in practice.\n", "versions": [{"version": "v1", "created": "Sun, 7 Sep 2008 15:37:48 GMT"}], "update_date": "2008-09-09", "authors_parsed": [["Ryabko", "Boris", ""]]}, {"id": "0809.1618", "submitter": "Antonio Pereira", "authors": "Antonio Pereira", "title": "ECOLANG - Communications Language for Ecological Simulations Network", "comments": "16 pages, language specification description", "journal-ref": null, "doi": null, "report-no": "TR-LIACC-FEUP-AMCP 01.1", "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This document describes the communication language used in one multiagent\nsystem environment for ecological simulations, based on EcoDynamo simulator\napplication linked with several intelligent agents and visualisation\napplications, and extends the initial definition of the language. The agents\nactions and perceptions are translated into messages exchanged with the\nsimulator application and other agents. The concepts and definitions used\nfollow the BNF notation (Backus et al. 1960) and is inspired in the Coach\nUnilang language (Reis and Lau 2002).\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2008 17:46:17 GMT"}], "update_date": "2008-09-10", "authors_parsed": [["Pereira", "Antonio", ""]]}, {"id": "0809.1686", "submitter": "Antonio Pereira", "authors": "Antonio Pereira (1 and 2), Pedro Duarte (1), Luis Paulo Reis (2) ((1)\n  UFP, Porto, Portugal (2) FEUP, Porto, Portugal)", "title": "Agent-based Ecological Model Calibration - on the Edge of a New Approach", "comments": "7 pages, 6 figures, Proceedings of the International Conference on\n  Knowledge Engineering and Decision Support, pp. 107-113, ISEP, Porto,\n  Portugal, July 2004", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The purpose of this paper is to present a new approach to ecological model\ncalibration -- an agent-based software. This agent works on three stages: 1- It\nbuilds a matrix that synthesizes the inter-variable relationships; 2- It\nanalyses the steady-state sensitivity of different variables to different\nparameters; 3- It runs the model iteratively and measures model lack of fit,\nadequacy and reliability. Stage 3 continues until some convergence criteria are\nattained. At each iteration, the agent knows from stages 1 and 2, which\nparameters are most likely to produce the desired shift on predicted results.\n", "versions": [{"version": "v1", "created": "Tue, 9 Sep 2008 22:12:37 GMT"}], "update_date": "2008-09-11", "authors_parsed": [["Pereira", "Antonio", "", "1 and 2"], ["Duarte", "Pedro", ""], ["Reis", "Luis Paulo", ""]]}, {"id": "0809.1916", "submitter": "Sung-eok Jeon", "authors": "Sung-eok Jeon, and Chunayi Ji", "title": "Randomized Distributed Configuration Management of Wireless Networks:\n  Multi-layer Markov Random Fields and Near-Optimality", "comments": "15 pages, revised and submitted to IEEE Trans. on Networking", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed configuration management is imperative for wireless\ninfrastructureless networks where each node adjusts locally its physical and\nlogical configuration through information exchange with neighbors. Two issues\nremain open. The first is the optimality. The second is the complexity. We\nstudy these issues through modeling, analysis, and randomized distributed\nalgorithms. Modeling defines the optimality. We first derive a global\nprobabilistic model for a network configuration which characterizes jointly the\nstatistical spatial dependence of a physical- and a logical-configuration. We\nthen show that a local model which approximates the global model is a two-layer\nMarkov Random Field or a random bond model. The complexity of the local model\nis the communication range among nodes. The local model is near-optimal when\nthe approximation error to the global model is within a given error bound. We\nanalyze the trade-off between an approximation error and complexity, and derive\nsufficient conditions on the near-optimality of the local model. We validate\nthe model, the analysis and the randomized distributed algorithms also through\nsimulation.\n", "versions": [{"version": "v1", "created": "Thu, 11 Sep 2008 05:00:51 GMT"}], "update_date": "2008-09-12", "authors_parsed": [["Jeon", "Sung-eok", ""], ["Ji", "Chunayi", ""]]}, {"id": "0809.2421", "submitter": "Juan Manuel Ojeda Sarmiento", "authors": "Juan Ojeda Sarmiento", "title": "Electricity Demand and Energy Consumption Management System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This project describes the electricity demand and energy consumption\nmanagement system and its application to Southern Peru smelter. It is composed\nof an hourly demand-forecasting module and of a simulation component for a\nplant electrical system. The first module was done using dynamic neural\nnetworks with backpropagation training algorithm; it is used to predict the\nelectric power demanded every hour, with an error percentage below of 1%. This\ninformation allows efficient management of energy peak demands before this\nhappen, distributing the raise of electric load to other hours or improving\nthose equipments that increase the demand. The simulation module is based in\nadvanced estimation techniques, such as: parametric estimation, neural network\nmodeling, statistic regression and previously developed models, which simulates\nthe electric behavior of the smelter plant. These modules facilitate\nelectricity demand and consumption proper planning, because they allow knowing\nthe behavior of the hourly demand and the consumption patterns of the plant,\nincluding the bill components, but also energy deficiencies and opportunities\nfor improvement, based on analysis of information about equipments, processes\nand production plans, as well as maintenance programs. Finally the results of\nits application in Southern Peru smelter are presented.\n", "versions": [{"version": "v1", "created": "Sun, 14 Sep 2008 22:26:49 GMT"}, {"version": "v2", "created": "Fri, 24 Oct 2008 20:28:45 GMT"}, {"version": "v3", "created": "Wed, 24 Jun 2009 17:20:27 GMT"}, {"version": "v4", "created": "Sun, 30 Aug 2009 05:22:10 GMT"}, {"version": "v5", "created": "Mon, 18 Apr 2011 23:08:41 GMT"}], "update_date": "2011-04-20", "authors_parsed": [["Sarmiento", "Juan Ojeda", ""]]}, {"id": "0809.2553", "submitter": "Paul Vitanyi", "authors": "Paul M.B. Vitanyi (CWI and Univ. Amsterdam), Frank J. Balbach (Univ.\n  Waterloo), Rudi L. Cilibrasi (CWI), and Ming Li (Univ. Waterloo)", "title": "Normalized Information Distance", "comments": "33 pages, 12 figures, pdf, in: Normalized information distance, in:\n  Information Theory and Statistical Learning, Eds. M. Dehmer, F.\n  Emmert-Streib, Springer-Verlag, New-York, To appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The normalized information distance is a universal distance measure for\nobjects of all kinds. It is based on Kolmogorov complexity and thus\nuncomputable, but there are ways to utilize it. First, compression algorithms\ncan be used to approximate the Kolmogorov complexity if the objects have a\nstring representation. Second, for names and abstract concepts, page count\nstatistics from the World Wide Web can be used. These practical realizations of\nthe normalized information distance can then be applied to machine learning\ntasks, expecially clustering, to perform feature-free and parameter-free data\nmining. This chapter discusses the theoretical foundations of the normalized\ninformation distance and both practical realizations. It presents numerous\nexamples of successful real-world applications based on these distance\nmeasures, ranging from bioinformatics to music clustering to machine\ntranslation.\n", "versions": [{"version": "v1", "created": "Mon, 15 Sep 2008 15:33:11 GMT"}], "update_date": "2008-09-16", "authors_parsed": [["Vitanyi", "Paul M. B.", "", "CWI and Univ. Amsterdam"], ["Balbach", "Frank J.", "", "Univ.\n  Waterloo"], ["Cilibrasi", "Rudi L.", "", "CWI"], ["Li", "Ming", "", "Univ. Waterloo"]]}, {"id": "0809.2792", "submitter": "Ronny Luss", "authors": "Ronny Luss, Alexandre d'Aspremont", "title": "Predicting Abnormal Returns From News Using Text Classification", "comments": "Larger data sets, results on time of day effect, and use of delta\n  hedged covered call options to trade on daily predictions", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how text from news articles can be used to predict intraday price\nmovements of financial assets using support vector machines. Multiple kernel\nlearning is used to combine equity returns with text as predictive features to\nincrease classification performance and we develop an analytic center cutting\nplane method to solve the kernel learning problem efficiently. We observe that\nwhile the direction of returns is not predictable using either text or returns,\ntheir size is, with text features producing significantly better performance\nthan historical returns alone.\n", "versions": [{"version": "v1", "created": "Tue, 16 Sep 2008 20:05:00 GMT"}, {"version": "v2", "created": "Sat, 15 Nov 2008 23:31:38 GMT"}, {"version": "v3", "created": "Wed, 24 Jun 2009 17:45:11 GMT"}], "update_date": "2009-06-24", "authors_parsed": [["Luss", "Ronny", ""], ["d'Aspremont", "Alexandre", ""]]}, {"id": "0809.3027", "submitter": "Evimaria Terzi", "authors": "Heikki Mannila and Evimaria Terzi", "title": "Finding links and initiators: a graph reconstruction problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB physics.soc-ph", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Consider a 0-1 observation matrix M, where rows correspond to entities and\ncolumns correspond to signals; a value of 1 (or 0) in cell (i,j) of M indicates\nthat signal j has been observed (or not observed) in entity i. Given such a\nmatrix we study the problem of inferring the underlying directed links between\nentities (rows) and finding which entries in the matrix are initiators.\n  We formally define this problem and propose an MCMC framework for estimating\nthe links and the initiators given the matrix of observations M. We also show\nhow this framework can be extended to incorporate a temporal aspect; instead of\nconsidering a single observation matrix M we consider a sequence of observation\nmatrices M1,..., Mt over time.\n  We show the connection between our problem and several problems studied in\nthe field of social-network analysis. We apply our method to paleontological\nand ecological data and show that our algorithms work well in practice and give\nreasonable results.\n", "versions": [{"version": "v1", "created": "Wed, 17 Sep 2008 22:28:29 GMT"}], "update_date": "2009-09-29", "authors_parsed": [["Mannila", "Heikki", ""], ["Terzi", "Evimaria", ""]]}, {"id": "0809.3204", "submitter": "Matti J\\\"arvisalo", "authors": "Matti J\\\"arvisalo and Emilia Oikarinen", "title": "Extended ASP tableaux and rule redundancy in normal logic programs", "comments": "27 pages, 5 figures, 1 table", "journal-ref": "Theory and Practice of Logic Programming, 8(5-6):691-716, 2008", "doi": "10.1017/S1471068408003578", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an extended tableau calculus for answer set programming (ASP).\nThe proof system is based on the ASP tableaux defined in [Gebser&Schaub, ICLP\n2006], with an added extension rule. We investigate the power of Extended ASP\nTableaux both theoretically and empirically. We study the relationship of\nExtended ASP Tableaux with the Extended Resolution proof system defined by\nTseitin for sets of clauses, and separate Extended ASP Tableaux from ASP\nTableaux by giving a polynomial-length proof for a family of normal logic\nprograms P_n for which ASP Tableaux has exponential-length minimal proofs with\nrespect to n. Additionally, Extended ASP Tableaux imply interesting insight\ninto the effect of program simplification on the lengths of proofs in ASP.\nClosely related to Extended ASP Tableaux, we empirically investigate the effect\nof redundant rules on the efficiency of ASP solving.\n  To appear in Theory and Practice of Logic Programming (TPLP).\n", "versions": [{"version": "v1", "created": "Thu, 18 Sep 2008 16:35:20 GMT"}], "update_date": "2010-07-20", "authors_parsed": [["J\u00e4rvisalo", "Matti", ""], ["Oikarinen", "Emilia", ""]]}, {"id": "0809.3352", "submitter": "Steffen Kuehn", "authors": "Steffen Kuehn", "title": "Generalized Prediction Intervals for Arbitrary Distributed\n  High-Dimensional Data", "comments": "13 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper generalizes the traditional statistical concept of prediction\nintervals for arbitrary probability density functions in high-dimensional\nfeature spaces by introducing significance level distributions, which provides\ninterval-independent probabilities for continuous random variables. The\nadvantage of the transformation of a probability density function into a\nsignificance level distribution is that it enables one-class classification or\noutlier detection in a direct manner.\n", "versions": [{"version": "v1", "created": "Fri, 19 Sep 2008 11:02:39 GMT"}], "update_date": "2008-09-22", "authors_parsed": [["Kuehn", "Steffen", ""]]}, {"id": "0809.4086", "submitter": "Valentino Crespi", "authors": "George Cybenko and Valentino Crespi", "title": "Learning Hidden Markov Models using Non-Negative Matrix Factorization", "comments": "Submitted to IEEE Transactions on Information Theory in September\n  2008", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Baum-Welsh algorithm together with its derivatives and variations has\nbeen the main technique for learning Hidden Markov Models (HMM) from\nobservational data. We present an HMM learning algorithm based on the\nnon-negative matrix factorization (NMF) of higher order Markovian statistics\nthat is structurally different from the Baum-Welsh and its associated\napproaches. The described algorithm supports estimation of the number of\nrecurrent states of an HMM and iterates the non-negative matrix factorization\n(NMF) algorithm to improve the learned HMM parameters. Numerical examples are\nprovided as well.\n", "versions": [{"version": "v1", "created": "Wed, 24 Sep 2008 05:34:56 GMT"}, {"version": "v2", "created": "Sat, 8 Jan 2011 03:16:39 GMT"}], "update_date": "2011-01-11", "authors_parsed": [["Cybenko", "George", ""], ["Crespi", "Valentino", ""]]}, {"id": "0809.4530", "submitter": "Olena Medelyan", "authors": "Olena Medelyan, David Milne, Catherine Legg and Ian H. Witten", "title": "Mining Meaning from Wikipedia", "comments": "An extensive survey of re-using information in Wikipedia in natural\n  language processing, information retrieval and extraction and ontology\n  building. Accepted for publication in International Journal of Human-Computer\n  Studies", "journal-ref": null, "doi": null, "report-no": "ISSN 1177-777X", "categories": "cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wikipedia is a goldmine of information; not just for its many readers, but\nalso for the growing community of researchers who recognize it as a resource of\nexceptional scale and utility. It represents a vast investment of manual effort\nand judgment: a huge, constantly evolving tapestry of concepts and relations\nthat is being applied to a host of tasks.\n  This article provides a comprehensive description of this work. It focuses on\nresearch that extracts and makes use of the concepts, relations, facts and\ndescriptions found in Wikipedia, and organizes the work into four broad\ncategories: applying Wikipedia to natural language processing; using it to\nfacilitate information retrieval and information extraction; and as a resource\nfor ontology building. The article addresses how Wikipedia is being used as is,\nhow it is being improved and adapted, and how it is being combined with other\nstructures to create entirely new resources. We identify the research groups\nand individuals involved, and how their work has developed in the last few\nyears. We provide a comprehensive list of the open-source software they have\nproduced.\n", "versions": [{"version": "v1", "created": "Fri, 26 Sep 2008 04:47:19 GMT"}, {"version": "v2", "created": "Sun, 10 May 2009 01:51:15 GMT"}], "update_date": "2009-05-10", "authors_parsed": [["Medelyan", "Olena", ""], ["Milne", "David", ""], ["Legg", "Catherine", ""], ["Witten", "Ian H.", ""]]}, {"id": "0809.4582", "submitter": "Emilia Oikarinen", "authors": "Emilia Oikarinen, Tomi Janhunen", "title": "Achieving compositionality of the stable model semantics for Smodels\n  programs", "comments": "44 pages, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a Gaifman-Shapiro-style module architecture is tailored to the\ncase of Smodels programs under the stable model semantics. The composition of\nSmodels program modules is suitably limited by module conditions which ensure\nthe compatibility of the module system with stable models. Hence the semantics\nof an entire Smodels program depends directly on stable models assigned to its\nmodules. This result is formalized as a module theorem which truly strengthens\nLifschitz and Turner's splitting-set theorem for the class of Smodels programs.\nTo streamline generalizations in the future, the module theorem is first proved\nfor normal programs and then extended to cover Smodels programs using a\ntranslation from the latter class of programs to the former class. Moreover,\nthe respective notion of module-level equivalence, namely modular equivalence,\nis shown to be a proper congruence relation: it is preserved under\nsubstitutions of modules that are modularly equivalent. Principles for program\ndecomposition are also addressed. The strongly connected components of the\nrespective dependency graph can be exploited in order to extract a module\nstructure when there is no explicit a priori knowledge about the modules of a\nprogram. The paper includes a practical demonstration of tools that have been\ndeveloped for automated (de)composition of Smodels programs.\n  To appear in Theory and Practice of Logic Programming.\n", "versions": [{"version": "v1", "created": "Fri, 26 Sep 2008 10:32:32 GMT"}], "update_date": "2008-09-29", "authors_parsed": [["Oikarinen", "Emilia", ""], ["Janhunen", "Tomi", ""]]}, {"id": "0809.4784", "submitter": "Luis Paulo Reis", "authors": "Luis Paulo Reis, Daria Barteneva, Nuno Lau", "title": "A Computational Study on Emotions and Temperament in Multi-Agent Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in neurosciences and psychology have provided evidence that\naffective phenomena pervade intelligence at many levels, being inseparable from\nthe cognitionaction loop. Perception, attention, memory, learning,\ndecisionmaking, adaptation, communication and social interaction are some of\nthe aspects influenced by them. This work draws its inspirations from\nneurobiology, psychophysics and sociology to approach the problem of building\nautonomous robots capable of interacting with each other and building\nstrategies based on temperamental decision mechanism. Modelling emotions is a\nrelatively recent focus in artificial intelligence and cognitive modelling.\nSuch models can ideally inform our understanding of human behavior. We may see\nthe development of computational models of emotion as a core research focus\nthat will facilitate advances in the large array of computational systems that\nmodel, interpret or influence human behavior. We propose a model based on a\nscalable, flexible and modular approach to emotion which allows runtime\nevaluation between emotional quality and performance. The results achieved\nshowed that the strategies based on temperamental decision mechanism strongly\ninfluence the system performance and there are evident dependency between\nemotional state of the agents and their temperamental type, as well as the\ndependency between the team performance and the temperamental configuration of\nthe team members, and this enable us to conclude that the modular approach to\nemotional programming based on temperamental theory is the good choice to\ndevelop computational mind models for emotional behavioral Multi-Agent systems.\n", "versions": [{"version": "v1", "created": "Sat, 27 Sep 2008 16:33:34 GMT"}], "update_date": "2008-09-30", "authors_parsed": [["Reis", "Luis Paulo", ""], ["Barteneva", "Daria", ""], ["Lau", "Nuno", ""]]}, {"id": "0809.5005", "submitter": "Martyn Amos", "authors": "Yi-Chun Xu, Ren-Bin Xiao and Martyn Amos", "title": "Simulated annealing for weighted polygon packing", "comments": "Submitted to Engineering Optimization. 13 pages, 7 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a new algorithm for a layout optimization problem:\nthis concerns the placement of weighted polygons inside a circular container,\nthe two objectives being to minimize imbalance of mass and to minimize the\nradius of the container. This problem carries real practical significance in\nindustrial applications (such as the design of satellites), as well as being of\nsignificant theoretical interest. Previous work has dealt with circular or\nrectangular objects, but here we deal with the more realistic case where\nobjects may be represented as polygons and the polygons are allowed to rotate.\nWe present a solution based on simulated annealing and first test it on\ninstances with known optima. Our results show that the algorithm obtains\ncontainer radii that are close to optimal. We also compare our method with\nexisting algorithms for the (special) rectangular case. Experimental results\nshow that our approach out-performs these methods in terms of solution quality.\n", "versions": [{"version": "v1", "created": "Mon, 29 Sep 2008 16:22:28 GMT"}], "update_date": "2008-09-30", "authors_parsed": [["Xu", "Yi-Chun", ""], ["Xiao", "Ren-Bin", ""], ["Amos", "Martyn", ""]]}]