[{"id": "1006.0274", "submitter": "Nan Li", "authors": "Nan Li, William Cushing, Subbarao Kambhampati, Sungwook Yoon", "title": "Learning Probabilistic Hierarchical Task Networks to Capture User\n  Preferences", "comments": "30 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose automatically learning probabilistic Hierarchical Task Networks\n(pHTNs) in order to capture a user's preferences on plans, by observing only\nthe user's behavior. HTNs are a common choice of representation for a variety\nof purposes in planning, including work on learning in planning. Our\ncontributions are (a) learning structure and (b) representing preferences. In\ncontrast, prior work employing HTNs considers learning method preconditions\n(instead of structure) and representing domain physics or search control\nknowledge (rather than preferences). Initially we will assume that the observed\ndistribution of plans is an accurate representation of user preference, and\nthen generalize to the situation where feasibility constraints frequently\nprevent the execution of preferred plans. In order to learn a distribution on\nplans we adapt an Expectation-Maximization (EM) technique from the discipline\nof (probabilistic) grammar induction, taking the perspective of task reductions\nas productions in a context-free grammar over primitive actions. To account for\nthe difference between the distributions of possible and preferred plans we\nsubsequently modify this core EM technique, in short, by rescaling its input.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2010 01:33:11 GMT"}], "update_date": "2010-06-03", "authors_parsed": [["Li", "Nan", ""], ["Cushing", "William", ""], ["Kambhampati", "Subbarao", ""], ["Yoon", "Sungwook", ""]]}, {"id": "1006.0289", "submitter": "Carlos Lorenzetti", "authors": "Carlos M. Lorenzetti and Roc\\'io L. Cecchini and Ana G. Maguitman and\n  Andr\\'as A. Bencz\\'ur", "title": "M\\'{e}todos para la Selecci\\'{o}n y el Ajuste de Caracter\\'{i}sticas en\n  el Problema de la Detecci\\'{o}n de Spam", "comments": "5 pages, 1 figure, Workshop de Investigadores en Ciencias de la\n  Computaci\\'{o}n, WICC 2010, pp 48-52", "journal-ref": "Workshop de Investigadores en Ciencias de la Computacion, WICC\n  2010, El Calafate, Santa Cruz, Argentina", "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The email is used daily by millions of people to communicate around the globe\nand it is a mission-critical application for many businesses. Over the last\ndecade, unsolicited bulk email has become a major problem for email users. An\noverwhelming amount of spam is flowing into users' mailboxes daily. In 2004, an\nestimated 62% of all email was attributed to spam. Spam is not only frustrating\nfor most email users, it strains the IT infrastructure of organizations and\ncosts businesses billions of dollars in lost productivity. In recent years,\nspam has evolved from an annoyance into a serious security threat, and is now a\nprime medium for phishing of sensitive information, as well the spread of\nmalicious software. This work presents a first approach to attack the spam\nproblem. We propose an algorithm that will improve a classifier's results by\nadjusting its training set data. It improves the document's vocabulary\nrepresentation by detecting good topic descriptors and discriminators.\n", "versions": [{"version": "v1", "created": "Wed, 2 Jun 2010 03:48:49 GMT"}, {"version": "v2", "created": "Thu, 14 Oct 2010 15:43:13 GMT"}], "update_date": "2010-10-15", "authors_parsed": [["Lorenzetti", "Carlos M.", ""], ["Cecchini", "Roc\u00edo L.", ""], ["Maguitman", "Ana G.", ""], ["Bencz\u00far", "Andr\u00e1s A.", ""]]}, {"id": "1006.0385", "submitter": "Dr. Paul J. Werbos", "authors": "Paul J. Werbos", "title": "Brain-Like Stochastic Search: A Research Challenge and Funding\n  Opportunity", "comments": "Plenary talk at IEEE Conference on Evolutionary Computing 1999,\n  extended in 2010 with new appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  Brain-Like Stochastic Search (BLiSS) refers to this task: given a family of\nutility functions U(u,A), where u is a vector of parameters or task\ndescriptors, maximize or minimize U with respect to u, using networks (Option\nNets) which input A and learn to generate good options u stochastically. This\npaper discusses why this is crucial to brain-like intelligence (an area funded\nby NSF) and to many applications, and discusses various possibilities for\nnetwork design and training. The appendix discusses recent research, relations\nto work on stochastic optimization in operations research, and relations to\nengineering-based approaches to understanding neocortex.\n", "versions": [{"version": "v1", "created": "Tue, 1 Jun 2010 18:16:10 GMT"}], "update_date": "2010-06-03", "authors_parsed": [["Werbos", "Paul J.", ""]]}, {"id": "1006.0991", "submitter": "Noam Shazeer", "authors": "Georges Harik and Noam Shazeer", "title": "Variational Program Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": "HSL-000001", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a framework for representing a variety of interesting problems\nas inference over the execution of probabilistic model programs. We represent a\n\"solution\" to such a problem as a guide program which runs alongside the model\nprogram and influences the model program's random choices, leading the model\nprogram to sample from a different distribution than from its priors. Ideally\nthe guide program influences the model program to sample from the posteriors\ngiven the evidence. We show how the KL- divergence between the true posterior\ndistribution and the distribution induced by the guided model program can be\nefficiently estimated (up to an additive constant) by sampling multiple\nexecutions of the guided model program. In addition, we show how to use the\nguide program as a proposal distribution in importance sampling to\nstatistically prove lower bounds on the probability of the evidence and on the\nprobability of a hypothesis and the evidence. We can use the quotient of these\ntwo bounds as an estimate of the conditional probability of the hypothesis\ngiven the evidence. We thus turn the inference problem into a heuristic search\nfor better guide programs.\n", "versions": [{"version": "v1", "created": "Fri, 4 Jun 2010 20:55:04 GMT"}], "update_date": "2010-06-08", "authors_parsed": [["Harik", "Georges", ""], ["Shazeer", "Noam", ""]]}, {"id": "1006.1030", "submitter": "Andrej Kastrin", "authors": "Andrej Kastrin, Borut Peterlin", "title": "Rasch-based high-dimensionality data reduction and class prediction with\n  applications to microarray gene expression data", "comments": null, "journal-ref": "Expert Systems with Applications, 2010;37(7):5178-5185", "doi": "10.1016/j.eswa.2009.12.074", "report-no": null, "categories": "cs.AI stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Class prediction is an important application of microarray gene expression\ndata analysis. The high-dimensionality of microarray data, where number of\ngenes (variables) is very large compared to the number of samples (obser-\nvations), makes the application of many prediction techniques (e.g., logistic\nregression, discriminant analysis) difficult. An efficient way to solve this\nprob- lem is by using dimension reduction statistical techniques. Increasingly\nused in psychology-related applications, Rasch model (RM) provides an appealing\nframework for handling high-dimensional microarray data. In this paper, we\nstudy the potential of RM-based modeling in dimensionality reduction with\nbinarized microarray gene expression data and investigate its prediction ac-\ncuracy in the context of class prediction using linear discriminant analysis.\nTwo different publicly available microarray data sets are used to illustrate a\ngeneral framework of the approach. Performance of the proposed method is\nassessed by re-randomization scheme using principal component analysis (PCA) as\na benchmark method. Our results show that RM-based dimension reduction is as\neffective as PCA-based dimension reduction. The method is general and can be\napplied to the other high-dimensional data problems.\n", "versions": [{"version": "v1", "created": "Sat, 5 Jun 2010 08:27:29 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Kastrin", "Andrej", ""], ["Peterlin", "Borut", ""]]}, {"id": "1006.1080", "submitter": "Marko A. Rodriguez", "authors": "Marko A. Rodriguez, Alberto Pepe, Joshua Shinavier", "title": "The Dilated Triple", "comments": null, "journal-ref": "Emergent Web Intelligence: Advanced Semantic Technologies,\n  Advanced Information and Knowledge Processing series, pages 3-16,\n  ISBN:978-1-84996-076-2, Springer-Verlag, June 2010", "doi": null, "report-no": "LA-UR-08-03927", "categories": "cs.AI", "license": "http://creativecommons.org/licenses/publicdomain/", "abstract": "  The basic unit of meaning on the Semantic Web is the RDF statement, or\ntriple, which combines a distinct subject, predicate and object to make a\ndefinite assertion about the world. A set of triples constitutes a graph, to\nwhich they give a collective meaning. It is upon this simple foundation that\nthe rich, complex knowledge structures of the Semantic Web are built. Yet the\nvery expressiveness of RDF, by inviting comparison with real-world knowledge,\nhighlights a fundamental shortcoming, in that RDF is limited to statements of\nabsolute fact, independent of the context in which a statement is asserted.\nThis is in stark contrast with the thoroughly context-sensitive nature of human\nthought. The model presented here provides a particularly simple means of\ncontextualizing an RDF triple by associating it with related statements in the\nsame graph. This approach, in combination with a notion of graph similarity, is\nsufficient to select only those statements from an RDF graph which are\nsubjectively most relevant to the context of the requesting process.\n", "versions": [{"version": "v1", "created": "Sun, 6 Jun 2010 05:16:55 GMT"}], "update_date": "2010-11-01", "authors_parsed": [["Rodriguez", "Marko A.", ""], ["Pepe", "Alberto", ""], ["Shinavier", "Joshua", ""]]}, {"id": "1006.1190", "submitter": "Harco Leslie Hendric Spits Warnars", "authors": "Spits Warnars H.L.H", "title": "Game Information System", "comments": "14 pages", "journal-ref": "International Journal of Computer Science and Information\n  Technology 2.3 (2010) 135-148", "doi": "10.5121/ijcsit.2010.2310", "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this Information system age many organizations consider information system\nas their weapon to compete or gain competitive advantage or give the best\nservices for non profit organizations. Game Information System as combining\nInformation System and game is breakthrough to achieve organizations'\nperformance. The Game Information System will run the Information System with\ngame and how game can be implemented to run the Information System. Game is not\nonly for fun and entertainment, but will be a challenge to combine fun and\nentertainment with Information System. The Challenge to run the information\nsystem with entertainment, deliver the entertainment with information system\nall at once. Game information system can be implemented in many sectors as like\nthe information system itself but in difference's view. A view of game which\npeople can joy and happy and do their transaction as a fun things.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2010 07:22:30 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2010 02:44:03 GMT"}], "update_date": "2010-07-15", "authors_parsed": [["H", "Spits Warnars H. L.", ""]]}, {"id": "1006.1328", "submitter": "Jonathan Huang", "authors": "Jonathan Huang and Carlos Guestrin", "title": "Uncovering the Riffled Independence Structure of Rankings", "comments": "65 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representing distributions over permutations can be a daunting task due to\nthe fact that the number of permutations of $n$ objects scales factorially in\n$n$. One recent way that has been used to reduce storage complexity has been to\nexploit probabilistic independence, but as we argue, full independence\nassumptions impose strong sparsity constraints on distributions and are\nunsuitable for modeling rankings. We identify a novel class of independence\nstructures, called \\emph{riffled independence}, encompassing a more expressive\nfamily of distributions while retaining many of the properties necessary for\nperforming efficient inference and reducing sample complexity. In riffled\nindependence, one draws two permutations independently, then performs the\n\\emph{riffle shuffle}, common in card games, to combine the two permutations to\nform a single permutation. Within the context of ranking, riffled independence\ncorresponds to ranking disjoint sets of objects independently, then\ninterleaving those rankings. In this paper, we provide a formal introduction to\nriffled independence and present algorithms for using riffled independence\nwithin Fourier-theoretic frameworks which have been explored by a number of\nrecent papers. Additionally, we propose an automated method for discovering\nsets of items which are riffle independent from a training set of rankings. We\nshow that our clustering-like algorithms can be used to discover meaningful\nlatent coalitions from real preference ranking datasets and to learn the\nstructure of hierarchically decomposable models based on riffled independence.\n", "versions": [{"version": "v1", "created": "Mon, 7 Jun 2010 18:45:46 GMT"}], "update_date": "2010-06-08", "authors_parsed": [["Huang", "Jonathan", ""], ["Guestrin", "Carlos", ""]]}, {"id": "1006.1407", "submitter": "EPTCS", "authors": "Davide Bresolin (University of Verona, Verona, Italy), Pietro Sala\n  (University of Verona, Verona, Italy), Guido Sciavicco (University of Murcia,\n  Murcia, Spain)", "title": "Begin, After, and Later: a Maximal Decidable Interval Temporal Logic", "comments": null, "journal-ref": "EPTCS 25, 2010, pp. 72-88", "doi": "10.4204/EPTCS.25.10", "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interval temporal logics (ITLs) are logics for reasoning about temporal\nstatements expressed over intervals, i.e., periods of time. The most famous ITL\nstudied so far is Halpern and Shoham's HS, which is the logic of the thirteen\nAllen's interval relations. Unfortunately, HS and most of its fragments have an\nundecidable satisfiability problem. This discouraged the research in this area\nuntil recently, when a number non-trivial decidable ITLs have been discovered.\n  This paper is a contribution towards the complete classification of all\ndifferent fragments of HS. We consider different combinations of the interval\nrelations Begins, After, Later and their inverses Abar, Bbar, and Lbar. We know\nfrom previous works that the combination ABBbarAbar is decidable only when\nfinite domains are considered (and undecidable elsewhere), and that ABBbar is\ndecidable over the natural numbers. We extend these results by showing that\ndecidability of ABBar can be further extended to capture the language\nABBbarLbar, which lays in between ABBar and ABBbarAbar, and that turns out to\nbe maximal w.r.t decidability over strongly discrete linear orders (e.g. finite\norders, the naturals, the integers). We also prove that the proposed decision\nprocedure is optimal with respect to the complexity class.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2010 00:42:51 GMT"}], "update_date": "2010-06-09", "authors_parsed": [["Bresolin", "Davide", "", "University of Verona, Verona, Italy"], ["Sala", "Pietro", "", "University of Verona, Verona, Italy"], ["Sciavicco", "Guido", "", "University of Murcia,\n  Murcia, Spain"]]}, {"id": "1006.1434", "submitter": "EPTCS", "authors": "A. Steven Younger (Missouri State University), Emmett Redd (Missouri\n  State University)", "title": "Computing by Means of Physics-Based Optical Neural Networks", "comments": null, "journal-ref": "EPTCS 26, 2010, pp. 159-167", "doi": "10.4204/EPTCS.26.15", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report recent research on computing with biology-based neural network\nmodels by means of physics-based opto-electronic hardware. New technology\nprovides opportunities for very-high-speed computation and uncovers problems\nobstructing the wide-spread use of this new capability. The Computation\nModeling community may be able to offer solutions to these cross-boundary\nresearch problems.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2010 01:17:00 GMT"}], "update_date": "2010-06-09", "authors_parsed": [["Younger", "A. Steven", "", "Missouri State University"], ["Redd", "Emmett", "", "Missouri\n  State University"]]}, {"id": "1006.1512", "submitter": "Uwe Aickelin", "authors": "Julie Greensmith, Uwe Aickelin", "title": "The Deterministic Dendritic Cell Algorithm", "comments": "12 pages, 1 algorithm, 1 figure, 2 tables, 7th International\n  Conference on Artificial Immune Systems (ICARIS 2008)", "journal-ref": "Proceedings of the 7th International Conference on Artificial\n  Immune Systems (ICARIS 2008), Phuket, Thailand, p 291-303", "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Dendritic Cell Algorithm is an immune-inspired algorithm orig- inally\nbased on the function of natural dendritic cells. The original instantiation of\nthe algorithm is a highly stochastic algorithm. While the performance of the\nalgorithm is good when applied to large real-time datasets, it is difficult to\nanal- yse due to the number of random-based elements. In this paper a\ndeterministic version of the algorithm is proposed, implemented and tested\nusing a port scan dataset to provide a controllable system. This version\nconsists of a controllable amount of parameters, which are experimented with in\nthis paper. In addition the effects are examined of the use of time windows and\nvariation on the number of cells, both which are shown to influence the\nalgorithm. Finally a novel metric for the assessment of the algorithms output\nis introduced and proves to be a more sensitive metric than the metric used\nwith the original Dendritic Cell Algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2010 10:07:34 GMT"}], "update_date": "2010-07-05", "authors_parsed": [["Greensmith", "Julie", ""], ["Aickelin", "Uwe", ""]]}, {"id": "1006.1518", "submitter": "Uwe Aickelin", "authors": "Julie Greensmith, Jan Feyereisl, Uwe Aickelin", "title": "The DCA:SOMe Comparison A comparative study between two\n  biologically-inspired algorithms", "comments": "38 pages, 29 figures, 10 tables, Evolutionary Intelligence", "journal-ref": "Evolutionary Intelligence, 1 (2), p 85-112, 2008", "doi": null, "report-no": null, "categories": "cs.AI cs.CR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Dendritic Cell Algorithm (DCA) is an immune-inspired algorithm, developed\nfor the purpose of anomaly detection. The algorithm performs multi-sensor data\nfusion and correlation which results in a 'context aware' detection system.\nPrevious applications of the DCA have included the detection of potentially\nmalicious port scanning activity, where it has produced high rates of true\npositives and low rates of false positives. In this work we aim to compare the\nperformance of the DCA and of a Self-Organizing Map (SOM) when applied to the\ndetection of SYN port scans, through experimental analysis. A SOM is an ideal\ncandidate for comparison as it shares similarities with the DCA in terms of the\ndata fusion method employed. It is shown that the results of the two systems\nare comparable, and both produce false positives for the same processes. This\nshows that the DCA can produce anomaly detection results to the same standard\nas an established technique.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2010 10:41:56 GMT"}], "update_date": "2010-07-05", "authors_parsed": [["Greensmith", "Julie", ""], ["Feyereisl", "Jan", ""], ["Aickelin", "Uwe", ""]]}, {"id": "1006.1526", "submitter": "Uwe Aickelin", "authors": "William Wilson, Philip Birkin, Uwe Aickelin", "title": "The Motif Tracking Algorithm", "comments": "13 pages, 10 figures, International Journal of Automation and\n  Computing", "journal-ref": "International Journal of Automation and Computing, 5 (1), p32-44,\n  2008", "doi": "10.1007/s11633.008.0032.0", "report-no": null, "categories": "cs.AI cs.CE cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The search for patterns or motifs in data represents a problem area of key\ninterest to finance and economic researchers. In this paper we introduce the\nMotif Tracking Algorithm, a novel immune inspired pattern identification tool\nthat is able to identify unknown motifs of a non specified length which repeat\nwithin time series data. The power of the algorithm comes from the fact that it\nuses a small number of parameters with minimal assumptions regarding the data\nbeing examined or the underlying motifs. Our interest lies in applying the\nalgorithm to financial time series data to identify unknown patterns that\nexist. The algorithm is tested using three separate data sets. Particular\nsuitability to financial data is shown by applying it to oil price data. In all\ncases the algorithm identifies the presence of a motif population in a fast and\nefficient manner due to the utilisation of an intuitive symbolic\nrepresentation. The resulting population of motifs is shown to have\nconsiderable potential value for other applications such as forecasting and\nalgorithm seeding.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2010 11:08:25 GMT"}], "update_date": "2010-07-05", "authors_parsed": [["Wilson", "William", ""], ["Birkin", "Philip", ""], ["Aickelin", "Uwe", ""]]}, {"id": "1006.1537", "submitter": "Minghao Yin", "authors": "Junping Zhou and Minghao Yin and Chunguang Zhou", "title": "New worst upper bound for #SAT", "comments": "6 pages; proceedings of AAAI 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The rigorous theoretical analyses of algorithms for #SAT have been proposed\nin the literature. As we know, previous algorithms for solving #SAT have been\nanalyzed only regarding the number of variables as the parameter. However, the\ntime complexity for solving #SAT instances depends not only on the number of\nvariables, but also on the number of clauses. Therefore, it is significant to\nexploit the time complexity from the other point of view, i.e. the number of\nclauses. In this paper, we present algorithms for solving #2-SAT and #3-SAT\nwith rigorous complexity analyses using the number of clauses as the parameter.\nBy analyzing the algorithms, we obtain the new worst-case upper bounds\nO(1.1892m) for #2-SAT and O(1.4142m) for #3-SAT, where m is the number of\nclauses.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2010 11:48:12 GMT"}], "update_date": "2010-06-09", "authors_parsed": [["Zhou", "Junping", ""], ["Yin", "Minghao", ""], ["Zhou", "Chunguang", ""]]}, {"id": "1006.1563", "submitter": "Uwe Aickelin", "authors": "Jan Feyereisl, Uwe Aickelin", "title": "ToLeRating UR-STD", "comments": "7 pages, 4 figures, 1 table, 2nd International Conference on Emerging\n  Security Information, Systems and Technologies,", "journal-ref": "Proceedings of the 2nd International Conference on Emerging\n  Security Information, Systems and Technologies, Cap Esterel, France, p\n  287-293, 2008", "doi": null, "report-no": null, "categories": "cs.AI cs.CR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new emerging paradigm of Uncertain Risk of Suspicion, Threat and Danger,\nobserved across the field of information security, is described. Based on this\nparadigm a novel approach to anomaly detection is presented. Our approach is\nbased on a simple yet powerful analogy from the innate part of the human immune\nsystem, the Toll-Like Receptors. We argue that such receptors incorporated as\npart of an anomaly detector enhance the detector's ability to distinguish\nnormal and anomalous behaviour. In addition we propose that Toll-Like Receptors\nenable the classification of detected anomalies based on the types of attacks\nthat perpetrate the anomalous behaviour. Classification of such type is either\nmissing in existing literature or is not fit for the purpose of reducing the\nburden of an administrator of an intrusion detection system. For our model to\nwork, we propose the creation of a taxonomy of the digital Acytota, based on\nwhich our receptors are created.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2010 14:12:08 GMT"}], "update_date": "2010-07-05", "authors_parsed": [["Feyereisl", "Jan", ""], ["Aickelin", "Uwe", ""]]}, {"id": "1006.1568", "submitter": "Uwe Aickelin", "authors": "Jamie Twycross, Uwe Aickelin", "title": "Towards a Conceptual Framework for Innate Immunity", "comments": "14 pages, 5 figures, 2 tables, 4th International Conference on\n  Artificial Immune Systems (ICARIS 2005)", "journal-ref": "Proceedings of the 4th International Conference on Artificial\n  Immune Systems (ICARIS 2005), Lecture Notes in Computer Science 3627, Banff,\n  Canada, p 112-125", "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Innate immunity now occupies a central role in immunology. However,\nartificial immune system models have largely been inspired by adaptive not\ninnate immunity. This paper reviews the biological principles and properties of\ninnate immunity and, adopting a conceptual framework, asks how these can be\nincorporated into artificial models. The aim is to outline a meta-framework for\nmodels of innate immunity.\n", "versions": [{"version": "v1", "created": "Tue, 8 Jun 2010 14:25:23 GMT"}], "update_date": "2010-07-05", "authors_parsed": [["Twycross", "Jamie", ""], ["Aickelin", "Uwe", ""]]}, {"id": "1006.1681", "submitter": "EPTCS", "authors": "German Terrazas (University of Nottingham), Dario Landa-Silva\n  (University of Nottingham), Natalio Krasnogor (University of Nottingham)", "title": "Towards the Design of Heuristics by Means of Self-Assembly", "comments": null, "journal-ref": "EPTCS 26, 2010, pp. 135-146", "doi": "10.4204/EPTCS.26.13", "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current investigations on hyper-heuristics design have sprung up in two\ndifferent flavours: heuristics that choose heuristics and heuristics that\ngenerate heuristics. In the latter, the goal is to develop a problem-domain\nindependent strategy to automatically generate a good performing heuristic for\nthe problem at hand. This can be done, for example, by automatically selecting\nand combining different low-level heuristics into a problem specific and\neffective strategy. Hyper-heuristics raise the level of generality on automated\nproblem solving by attempting to select and/or generate tailored heuristics for\nthe problem at hand. Some approaches like genetic programming have been\nproposed for this. In this paper, we explore an elegant nature-inspired\nalternative based on self-assembly construction processes, in which structures\nemerge out of local interactions between autonomous components. This idea\narises from previous works in which computational models of self-assembly were\nsubject to evolutionary design in order to perform the automatic construction\nof user-defined structures. Then, the aim of this paper is to present a novel\nmethodology for the automated design of heuristics by means of self-assembly.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2010 02:03:41 GMT"}], "update_date": "2010-06-10", "authors_parsed": [["Terrazas", "German", "", "University of Nottingham"], ["Landa-Silva", "Dario", "", "University of Nottingham"], ["Krasnogor", "Natalio", "", "University of Nottingham"]]}, {"id": "1006.1692", "submitter": "Harco Leslie Hendric Spits Warnars", "authors": "Spits Warnars", "title": "Measuring interesting rules in Characteristic rule", "comments": "5 pages, 5 figures, 12 tables", "journal-ref": "2nd International Conference on Soft Computing, Intelligent System\n  and Information Technology (ICSIIT), Bali, Indonesia, 1-2 July 2010", "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding interesting rule in the sixth strategy step about threshold control\non generalized relations in attribute oriented induction, there is possibility\nto select candidate attribute for further generalization and merging of\nidentical tuples until the number of tuples is no greater than the threshold\nvalue, as implemented in basic attribute oriented induction algorithm. At this\nstrategy step there is possibility the number of tuples in final generalization\nresult still greater than threshold value. In order to get the final\ngeneralization result which only small number of tuples and can be easy to\ntransfer into simple logical formula, the seventh strategy step about rule\ntransformation is evolved where there will be simplification by unioning or\ngrouping the identical attribute. Our approach to measure interesting rule is\nopposite with heuristic measurement approach by Fudger and Hamilton where the\nmore complex concept hierarchies, more interesting results are likely to be\nfound, but our approach the simpler concept hierarchies, more interesting\nresults are likely to be found and the more complex concept hierarchies, more\ncomplex process generalization in concept tree. The decision to find\ninteresting rule is influenced with wide or length and depth or level of\nconcept tree.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2010 02:59:26 GMT"}], "update_date": "2010-06-10", "authors_parsed": [["Warnars", "Spits", ""]]}, {"id": "1006.1701", "submitter": "Harco Leslie Hendric Spits Warnars", "authors": "Spits Warnars", "title": "Virtual information system on working area", "comments": "6 pages, 3 figures", "journal-ref": "Indonesian Students' International Scientific Meeting, (Temu\n  Ilmiah Internasional Mahasiswa Indonesia, TIIMI), London, United Kingdom, 5-7\n  December 2008", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to get strategic positioning for competition in business\norganization, the information system must be ahead in this information age\nwhere the information as one of the weapons to win the competition and in the\nright hand the information will become a right bullet. The information system\nwith the information technology support isn't enough if just only on internet\nor implemented with internet technology. The growth of information technology\nas tools for helping and making people easy to use must be accompanied by\nwanting to make fun and happy when they make contact with the information\ntechnology itself. Basically human like to play, since childhood human have\nbeen playing, free and happy and when human grow up they can't play as much as\nwhen human was in their childhood. We have to develop the information system\nwhich is not perform information system itself but can help human to explore\ntheir natural instinct for playing, making fun and happiness when they interact\nwith the information system. Virtual information system is the way to present\nplaying and having fun atmosphere on working area.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2010 04:08:07 GMT"}], "update_date": "2010-06-10", "authors_parsed": [["Warnars", "Spits", ""]]}, {"id": "1006.1703", "submitter": "Harco Leslie Hendric Spits Warnars", "authors": "Spits Warnars", "title": "Indonesian Earthquake Decision Support System", "comments": "8 pages, 7 figures", "journal-ref": "The 5th International Conference on Information & Communication\n  Technology and Systems (ICTS) 2009, Informatics Department, Institute of\n  Technology Sepuluh Nopember (ITS), Surabaya, Indonesia, 3-4 August 2009", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Earthquake DSS is an information technology environment which can be used by\ngovernment to sharpen, make faster and better the earthquake mitigation\ndecision. Earthquake DSS can be delivered as E-government which is not only for\ngovernment itself but in order to guarantee each citizen's rights for\neducation, training and information about earthquake and how to overcome the\nearthquake. Knowledge can be managed for future use and would become mining by\nsaving and maintain all the data and information about earthquake and\nearthquake mitigation in Indonesia. Using Web technology will enhance global\naccess and easy to use. Datawarehouse as unNormalized database for\nmultidimensional analysis will speed the query process and increase reports\nvariation. Link with other Disaster DSS in one national disaster DSS, link with\nother government information system and international will enhance the\nknowledge and sharpen the reports.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2010 04:36:14 GMT"}], "update_date": "2010-06-10", "authors_parsed": [["Warnars", "Spits", ""]]}, {"id": "1006.1786", "submitter": "Diederik Aerts", "authors": "Diederik Aerts", "title": "Measuring Meaning on the World-Wide Web", "comments": "5 pages", "journal-ref": "In D. Aerts, J. Broekaert, B. D'Hooghe and N. Note (Eds.),\n  Worldviews, Science and Us: Bridging Knowledge and Its Implications for Our\n  Perspectives of the World. Singapore: World Scientific (2011)", "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the notion of the 'meaning bound' of a word with respect to\nanother word by making use of the World-Wide Web as a conceptual environment\nfor meaning. The meaning of a word with respect to another word is established\nby multiplying the product of the number of webpages containing both words by\nthe total number of webpages of the World-Wide Web, and dividing the result by\nthe product of the number of webpages for each of the single words. We\ncalculate the meaning bounds for several words and analyze different aspects of\nthese by looking at specific examples.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2010 12:24:54 GMT"}], "update_date": "2012-03-28", "authors_parsed": [["Aerts", "Diederik", ""]]}, {"id": "1006.1828", "submitter": "Dariusz Plewczynski", "authors": "Dariusz Plewczynski", "title": "Landau Theory of Adaptive Integration in Computational Intelligence", "comments": "19 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI nlin.AO q-bio.NC q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational Intelligence (CI) is a sub-branch of Artificial Intelligence\nparadigm focusing on the study of adaptive mechanisms to enable or facilitate\nintelligent behavior in complex and changing environments. There are several\nparadigms of CI [like artificial neural networks, evolutionary computations,\nswarm intelligence, artificial immune systems, fuzzy systems and many others],\neach of these has its origins in biological systems [biological neural systems,\nnatural Darwinian evolution, social behavior, immune system, interactions of\norganisms with their environment]. Most of those paradigms evolved into\nseparate machine learning (ML) techniques, where probabilistic methods are used\ncomplementary with CI techniques in order to effectively combine elements of\nlearning, adaptation, evolution and Fuzzy logic to create heuristic algorithms\nthat are, in some sense, intelligent. The current trend is to develop consensus\ntechniques, since no single machine learning algorithms is superior to others\nin all possible situations. In order to overcome this problem several\nmeta-approaches were proposed in ML focusing on the integration of results from\ndifferent methods into single prediction. We discuss here the Landau theory for\nthe nonlinear equation that can describe the adaptive integration of\ninformation acquired from an ensemble of independent learning agents. The\ninfluence of each individual agent on other learners is described similarly to\nthe social impact theory. The final decision outcome for the consensus system\nis calculated using majority rule in the stationary limit, yet the minority\nsolutions can survive inside the majority population as the complex\nintermittent clusters of opposite opinion.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2010 15:11:14 GMT"}], "update_date": "2010-06-10", "authors_parsed": [["Plewczynski", "Dariusz", ""]]}, {"id": "1006.1916", "submitter": "Carlos Sarraute", "authors": "Ariel Futoransky (1), Luciano Notarfrancesco (1), Gerardo Richarte\n  (1), Carlos Sarraute (1) ((1) CoreLabs, Core Security Technologies)", "title": "Building Computer Network Attacks", "comments": "CoreLabs Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  In this work we start walking the path to a new perspective for viewing\ncyberwarfare scenarios, by introducing conceptual tools (a formal model) to\nevaluate the costs of an attack, to describe the theater of operations,\ntargets, missions, actions, plans and assets involved in cyberwarfare attacks.\nWe also describe two applications of this model: autonomous planning leading to\nautomated penetration tests, and attack simulations, allowing a system\nadministrator to evaluate the vulnerabilities of his network.\n", "versions": [{"version": "v1", "created": "Wed, 9 Jun 2010 22:21:00 GMT"}], "update_date": "2010-06-11", "authors_parsed": [["Futoransky", "Ariel", "", "CoreLabs, Core Security Technologies"], ["Notarfrancesco", "Luciano", "", "CoreLabs, Core Security Technologies"], ["Richarte", "Gerardo", "", "CoreLabs, Core Security Technologies"], ["Sarraute", "Carlos", "", "CoreLabs, Core Security Technologies"]]}, {"id": "1006.1930", "submitter": "Diederik Aerts", "authors": "Diederik Aerts, Marek Czachor, Bart D'Hooghe and Sandro Sozzo", "title": "The Pet-Fish problem on the World-Wide Web", "comments": "8 pages", "journal-ref": "Proceedings of the AAAI Fall Symposium (FS-10-08), Quantum\n  Informatics for Cognitive, Social, and Semantic Processes, pp. 17-21, (2010)", "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We identify the presence of Pet-Fish problem situations and the corresponding\nGuppy effect of concept theory on the World-Wide Web. For this purpose, we\nintroduce absolute weights for words expressing concepts and relative weights\nbetween words expressing concepts, and the notion of 'meaning bound' between\ntwo words expressing concepts, making explicit use of the conceptual structure\nof the World-Wide Web. The Pet-Fish problem occurs whenever there are exemplars\n- in the case of Pet and Fish these can be Guppy or Goldfish - for which the\nmeaning bound with respect to the conjunction is stronger than the meaning\nbounds with respect to the individual concepts.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2010 00:39:33 GMT"}], "update_date": "2011-05-13", "authors_parsed": [["Aerts", "Diederik", ""], ["Czachor", "Marek", ""], ["D'Hooghe", "Bart", ""], ["Sozzo", "Sandro", ""]]}, {"id": "1006.2165", "submitter": "Marc Deisenroth", "authors": "Marc Peter Deisenroth and Henrik Ohlsson", "title": "A Probabilistic Perspective on Gaussian Filtering and Smoothing", "comments": "14 pages. Extended version of conference paper (ACC 2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI cs.RO cs.SY math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a general probabilistic perspective on Gaussian filtering and\nsmoothing. This allows us to show that common approaches to Gaussian\nfiltering/smoothing can be distinguished solely by their methods of\ncomputing/approximating the means and covariances of joint probabilities. This\nimplies that novel filters and smoothers can be derived straightforwardly by\nproviding methods for computing these moments. Based on this insight, we derive\nthe cubature Kalman smoother and propose a novel robust filtering and smoothing\nalgorithm based on Gibbs sampling.\n", "versions": [{"version": "v1", "created": "Thu, 10 Jun 2010 22:23:23 GMT"}, {"version": "v2", "created": "Tue, 29 Jun 2010 00:14:05 GMT"}, {"version": "v3", "created": "Mon, 9 Aug 2010 01:36:08 GMT"}, {"version": "v4", "created": "Mon, 4 Apr 2011 20:54:13 GMT"}, {"version": "v5", "created": "Wed, 8 Jun 2011 06:15:34 GMT"}], "update_date": "2011-06-09", "authors_parsed": [["Deisenroth", "Marc Peter", ""], ["Ohlsson", "Henrik", ""]]}, {"id": "1006.2204", "submitter": "Nan Rong", "authors": "Joseph Y. Halpern, Nan Rong, Ashutosh Saxena", "title": "MDPs with Unawareness", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov decision processes (MDPs) are widely used for modeling decision-making\nproblems in robotics, automated control, and economics. Traditional MDPs assume\nthat the decision maker (DM) knows all states and actions. However, this may\nnot be true in many situations of interest. We define a new framework, MDPs\nwith unawareness (MDPUs) to deal with the possibilities that a DM may not be\naware of all possible actions. We provide a complete characterization of when a\nDM can learn to play near-optimally in an MDPU, and give an algorithm that\nlearns to play near-optimally when it is possible to do so, as efficiently as\npossible. In particular, we characterize when a near-optimal solution can be\nfound in polynomial time.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2010 06:18:27 GMT"}], "update_date": "2010-06-14", "authors_parsed": [["Halpern", "Joseph Y.", ""], ["Rong", "Nan", ""], ["Saxena", "Ashutosh", ""]]}, {"id": "1006.2289", "submitter": "Franz Baader", "authors": "Franz Baader (Institute for Theoretical Computer Science, TU Dresden),\n  Barbara Morawska (Institute for Theoretical Computer Science, TU Dresden)", "title": "Unification in the Description Logic EL", "comments": "31pages", "journal-ref": "Logical Methods in Computer Science, Volume 6, Issue 3 (September\n  4, 2010) lmcs:1106", "doi": "10.2168/LMCS-6(3:17)2010", "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Description Logic EL has recently drawn considerable attention since, on\nthe one hand, important inference problems such as the subsumption problem are\npolynomial. On the other hand, EL is used to define large biomedical\nontologies. Unification in Description Logics has been proposed as a novel\ninference service that can, for example, be used to detect redundancies in\nontologies. The main result of this paper is that unification in EL is\ndecidable. More precisely, EL-unification is NP-complete, and thus has the same\ncomplexity as EL-matching. We also show that, w.r.t. the unification type, EL\nis less well-behaved: it is of type zero, which in particular implies that\nthere are unification problems that have no finite complete set of unifiers.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2010 12:56:52 GMT"}, {"version": "v2", "created": "Sat, 4 Sep 2010 10:48:21 GMT"}], "update_date": "2015-07-01", "authors_parsed": [["Baader", "Franz", "", "Institute for Theoretical Computer Science, TU Dresden"], ["Morawska", "Barbara", "", "Institute for Theoretical Computer Science, TU Dresden"]]}, {"id": "1006.2322", "submitter": "Yoshiharu Maeno", "authors": "Yoshiharu Maeno", "title": "Discovery of a missing disease spreader", "comments": "in press", "journal-ref": "Physica A vol.390, pp.3412-3426 (2011)", "doi": "10.1016/j.physa.2011.05.005", "report-no": null, "categories": "cs.AI cs.SI physics.bio-ph physics.soc-ph q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study presents a method to discover an outbreak of an infectious disease\nin a region for which data are missing, but which is at work as a disease\nspreader. Node discovery for the spread of an infectious disease is defined as\ndiscriminating between the nodes which are neighboring to a missing disease\nspreader node, and the rest, given a dataset on the number of cases. The spread\nis described by stochastic differential equations. A perturbation theory\nquantifies the impact of the missing spreader on the moments of the number of\ncases. Statistical discriminators examine the mid-body or tail-ends of the\nprobability density function, and search for the disturbance from the missing\nspreader. They are tested with computationally synthesized datasets, and\napplied to the SARS outbreak and flu pandemic.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2010 14:33:18 GMT"}, {"version": "v2", "created": "Mon, 27 Sep 2010 13:49:21 GMT"}, {"version": "v3", "created": "Mon, 20 Dec 2010 13:39:48 GMT"}, {"version": "v4", "created": "Thu, 9 Jun 2011 07:09:13 GMT"}], "update_date": "2011-10-18", "authors_parsed": [["Maeno", "Yoshiharu", ""]]}, {"id": "1006.2495", "submitter": "Xiao Wen Han", "authors": "Han Xiao Wen", "title": "Mirrored Language Structure and Innate Logic of the Human Brain as a\n  Computable Model of the Oracle Turing Machine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We wish to present a mirrored language structure (MLS) and four logic rules\ndetermined by this structure for the model of a computable Oracle Turing\nmachine. MLS has novel features that are of considerable biological and\ncomputational significance. It suggests an algorithm of relation learning and\nrecognition (RLR) that enables the deterministic computers to simulate the\nmechanism of the Oracle Turing machine, or P = NP in a mathematical term.\n", "versions": [{"version": "v1", "created": "Sat, 12 Jun 2010 21:45:26 GMT"}], "update_date": "2010-06-15", "authors_parsed": [["Wen", "Han Xiao", ""]]}, {"id": "1006.2718", "submitter": "Erik Wilde", "authors": "Rosa Alarcon and Erik Wilde", "title": "From RESTful Services to RDF: Connecting the Web and the Semantic Web", "comments": null, "journal-ref": null, "doi": null, "report-no": "2010-041", "categories": "cs.AI cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  RESTful services on the Web expose information through retrievable resource\nrepresentations that represent self-describing descriptions of resources, and\nthrough the way how these resources are interlinked through the hyperlinks that\ncan be found in those representations. This basic design of RESTful services\nmeans that for extracting the most useful information from a service, it is\nnecessary to understand a service's representations, which means both the\nsemantics in terms of describing a resource, and also its semantics in terms of\ndescribing its linkage with other resources. Based on the Resource Linking\nLanguage (ReLL), this paper describes a framework for how RESTful services can\nbe described, and how these descriptions can then be used to harvest\ninformation from these services. Building on this framework, a layered model of\nRESTful service semantics allows to represent a service's information in\nRDF/OWL. Because REST is based on the linkage between resources, the same model\ncan be used for aggregating and interlinking multiple services for extracting\nRDF data from sets of RESTful services.\n", "versions": [{"version": "v1", "created": "Fri, 11 Jun 2010 18:39:20 GMT"}], "update_date": "2010-06-15", "authors_parsed": [["Alarcon", "Rosa", ""], ["Wilde", "Erik", ""]]}, {"id": "1006.2743", "submitter": "Marek Petrik", "authors": "Marek Petrik and Shlomo Zilberstein", "title": "Global Optimization for Value Function Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing value function approximation methods have been successfully used in\nmany applications, but they often lack useful a priori error bounds. We propose\na new approximate bilinear programming formulation of value function\napproximation, which employs global optimization. The formulation provides\nstrong a priori guarantees on both robust and expected policy loss by\nminimizing specific norms of the Bellman residual. Solving a bilinear program\noptimally is NP-hard, but this is unavoidable because the Bellman-residual\nminimization itself is NP-hard. We describe and analyze both optimal and\napproximate algorithms for solving bilinear programs. The analysis shows that\nthis algorithm offers a convergent generalization of approximate policy\niteration. We also briefly analyze the behavior of bilinear programming\nalgorithms under incomplete samples. Finally, we demonstrate that the proposed\napproach can consistently minimize the Bellman residual on simple benchmark\nproblems.\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2010 15:38:41 GMT"}], "update_date": "2010-06-15", "authors_parsed": [["Petrik", "Marek", ""], ["Zilberstein", "Shlomo", ""]]}, {"id": "1006.2844", "submitter": "Carlos Sarraute", "authors": "Javier Burroni, Carlos Sarraute (CoreLabs, Core Security Technologies)", "title": "Outrepasser les limites des techniques classiques de Prise d'Empreintes\n  grace aux Reseaux de Neurones", "comments": "16 pages, 3 figures. Symposium sur la S\\'ecurit\\'e des Technologies\n  de l'Information et des Communications (SSTIC), Rennes, France, May 31-June\n  2, 2006", "journal-ref": "Actes du symposium SSTIC (2006)", "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We present an application of Artificial Intelligence techniques to the field\nof Information Security. The problem of remote Operating System (OS) Detection,\nalso called OS Fingerprinting, is a crucial step of the penetration testing\nprocess, since the attacker (hacker or security professional) needs to know the\nOS of the target host in order to choose the exploits that he will use. OS\nDetection is accomplished by passively sniffing network packets and actively\nsending test packets to the target host, to study specific variations in the\nhost responses revealing information about its operating system.\n  The first fingerprinting implementations were based on the analysis of\ndifferences between TCP/IP stack implementations. The next generation focused\nthe analysis on application layer data such as the DCE RPC endpoint\ninformation. Even though more information was analyzed, some variation of the\n\"best fit\" algorithm was still used to interpret this new information. Our new\napproach involves an analysis of the composition of the information collected\nduring the OS identification process to identify key elements and their\nrelations. To implement this approach, we have developed tools using Neural\nNetworks and techniques from the field of Statistics. These tools have been\nsuccessfully integrated in a commercial software (Core Impact).\n", "versions": [{"version": "v1", "created": "Mon, 14 Jun 2010 20:52:44 GMT"}], "update_date": "2010-06-16", "authors_parsed": [["Burroni", "Javier", "", "CoreLabs, Core Security Technologies"], ["Sarraute", "Carlos", "", "CoreLabs, Core Security Technologies"]]}, {"id": "1006.2899", "submitter": "Tamir Hazan", "authors": "Tamir Hazan, Raquel Urtasun", "title": "Approximated Structured Prediction for Learning Large Scale Graphical\n  Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This manuscripts contains the proofs for \"A Primal-Dual Message-Passing\nAlgorithm for Approximated Large Scale Structured Prediction\".\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2010 06:55:03 GMT"}, {"version": "v2", "created": "Mon, 9 Jul 2012 18:22:27 GMT"}], "update_date": "2012-07-10", "authors_parsed": [["Hazan", "Tamir", ""], ["Urtasun", "Raquel", ""]]}, {"id": "1006.2945", "submitter": "Uwe Aickelin", "authors": "Amanda Whitbrook, Uwe Aickelin, Jonathan M. Garibaldi", "title": "Two-Timescale Learning Using Idiotypic Behaviour Mediation For A\n  Navigating Mobile Robot", "comments": "40 pages, 12 tables, Journal of Applied Soft Computing", "journal-ref": "Journal of Applied Soft Computing, 10, p 876-887, 2010", "doi": null, "report-no": null, "categories": "cs.AI cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A combined Short-Term Learning (STL) and Long-Term Learning (LTL) approach to\nsolving mobile-robot navigation problems is presented and tested in both the\nreal and virtual domains. The LTL phase consists of rapid simulations that use\na Genetic Algorithm to derive diverse sets of behaviours, encoded as variable\nsets of attributes, and the STL phase is an idiotypic Artificial Immune System.\nResults from the LTL phase show that sets of behaviours develop very rapidly,\nand significantly greater diversity is obtained when multiple autonomous\npopulations are used, rather than a single one. The architecture is assessed\nunder various scenarios, including removal of the LTL phase and switching off\nthe idiotypic mechanism in the STL phase. The comparisons provide substantial\nevidence that the best option is the inclusion of both the LTL phase and the\nidiotypic system. In addition, this paper shows that structurally different\nenvironments can be used for the two phases without compromising\ntransferability.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2010 10:17:21 GMT"}], "update_date": "2010-07-05", "authors_parsed": [["Whitbrook", "Amanda", ""], ["Aickelin", "Uwe", ""], ["Garibaldi", "Jonathan M.", ""]]}, {"id": "1006.3021", "submitter": "Michael Fink", "authors": "Michael Fink", "title": "A General Framework for Equivalences in Answer-Set Programming by\n  Countermodels in the Logic of Here-and-There", "comments": "32 pages; to appear in Theory and Practice of Logic Programming\n  (TPLP)", "journal-ref": null, "doi": null, "report-no": "INFSYS RR-1843-09-05", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different notions of equivalence, such as the prominent notions of strong and\nuniform equivalence, have been studied in Answer-Set Programming, mainly for\nthe purpose of identifying programs that can serve as substitutes without\naltering the semantics, for instance in program optimization. Such semantic\ncomparisons are usually characterized by various selections of models in the\nlogic of Here-and-There (HT). For uniform equivalence however, correct\ncharacterizations in terms of HT-models can only be obtained for finite\ntheories, respectively programs. In this article, we show that a selection of\ncountermodels in HT captures uniform equivalence also for infinite theories.\nThis result is turned into coherent characterizations of the different notions\nof equivalence by countermodels, as well as by a mixture of HT-models and\ncountermodels (so-called equivalence interpretations). Moreover, we generalize\nthe so-called notion of relativized hyperequivalence for programs to\npropositional theories, and apply the same methodology in order to obtain a\nsemantic characterization which is amenable to infinite settings. This allows\nfor a lifting of the results to first-order theories under a very general\nsemantics given in terms of a quantified version of HT. We thus obtain a\ngeneral framework for the study of various notions of equivalence for theories\nunder answer-set semantics. Moreover, we prove an expedient property that\nallows for a simplified treatment of extended signatures, and provide further\nresults for non-ground logic programs. In particular, uniform equivalence\ncoincides under open and ordinary answer-set semantics, and for finite\nnon-ground programs under these semantics, also the usual characterization of\nuniform equivalence in terms of maximal and total HT-models of the grounding is\ncorrect, even for infinite domains, when corresponding ground programs are\ninfinite.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2010 16:04:54 GMT"}], "update_date": "2010-06-16", "authors_parsed": [["Fink", "Michael", ""]]}, {"id": "1006.3035", "submitter": "Shay Cohen", "authors": "Shay B. Cohen, Robert J. Simmons, Noah A. Smith", "title": "Products of Weighted Logic Programs", "comments": null, "journal-ref": "TLP 11 (2-3): 263-296, 2011", "doi": "10.1017/S1471068410000529", "report-no": null, "categories": "cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weighted logic programming, a generalization of bottom-up logic programming,\nis a well-suited framework for specifying dynamic programming algorithms. In\nthis setting, proofs correspond to the algorithm's output space, such as a path\nthrough a graph or a grammatical derivation, and are given a real-valued score\n(often interpreted as a probability) that depends on the real weights of the\nbase axioms used in the proof. The desired output is a function over all\npossible proofs, such as a sum of scores or an optimal score. We describe the\nPRODUCT transformation, which can merge two weighted logic programs into a new\none. The resulting program optimizes a product of proof scores from the\noriginal programs, constituting a scoring function known in machine learning as\na ``product of experts.'' Through the addition of intuitive constraining side\nconditions, we show that several important dynamic programming algorithms can\nbe derived by applying PRODUCT to weighted logic programs corresponding to\nsimpler weighted logic programs. In addition, we show how the computation of\nKullback-Leibler divergence, an information-theoretic measure, can be\ninterpreted using PRODUCT.\n", "versions": [{"version": "v1", "created": "Tue, 15 Jun 2010 17:22:55 GMT"}], "update_date": "2012-08-15", "authors_parsed": [["Cohen", "Shay B.", ""], ["Simmons", "Robert J.", ""], ["Smith", "Noah A.", ""]]}, {"id": "1006.3215", "submitter": "Roland Yap", "authors": "Yuanlin Zhang and Roland H.C. Yap", "title": "Solving Functional Constraints by Variable Substitution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Functional constraints and bi-functional constraints are an important\nconstraint class in Constraint Programming (CP) systems, in particular for\nConstraint Logic Programming (CLP) systems. CP systems with finite domain\nconstraints usually employ CSP-based solvers which use local consistency, for\nexample, arc consistency. We introduce a new approach which is based instead on\nvariable substitution. We obtain efficient algorithms for reducing systems\ninvolving functional and bi-functional constraints together with other\nnon-functional constraints. It also solves globally any CSP where there exists\na variable such that any other variable is reachable from it through a sequence\nof functional constraints. Our experiments on random problems show that\nvariable elimination can significantly improve the efficiency of solving\nproblems with functional constraints.\n", "versions": [{"version": "v1", "created": "Wed, 16 Jun 2010 13:49:16 GMT"}], "update_date": "2010-06-17", "authors_parsed": [["Zhang", "Yuanlin", ""], ["Yap", "Roland H. C.", ""]]}, {"id": "1006.3650", "submitter": "Uwe Aickelin", "authors": "Amanda Whitbrook, Uwe Aickelin, Jonathan M. Garibaldi", "title": "The Use of Probabilistic Systems to Mimic the Behaviour of Idiotypic AIS\n  Robot Controllers", "comments": "8 pages, 6 tables, 2 figures, Journal of Systemics, Cybernetics and\n  Informatics", "journal-ref": "Journal of Systemics, Cybernetics and Informatics, 2009, 7(6),\n  p72-79", "doi": null, "report-no": null, "categories": "cs.AI cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous work has shown that robot navigation systems that employ an\narchitecture based upon the idiotypic network theory of the immune system have\nan advantage over control techniques that rely on reinforcement learning only.\nThis is thought to be a result of intelligent behaviour selection on the part\nof the idiotypic robot. In this paper an attempt is made to imitate idiotypic\ndynamics by creating controllers that use reinforcement with a number of\ndifferent probabilistic schemes to select robot behaviour. The aims are to show\nthat the idiotypic system is not merely performing some kind of periodic random\nbehaviour selection, and to try to gain further insight into the processes that\ngovern the idiotypic mechanism. Trials are carried out using simulated Pioneer\nrobots that undertake navigation exercises. Results show that a scheme that\nboosts the probability of selecting highly-ranked alternative behaviours to 50%\nduring stall conditions comes closest to achieving the properties of the\nidiotypic system, but remains unable to match it in terms of all round\nperformance.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2010 09:38:51 GMT"}], "update_date": "2013-05-30", "authors_parsed": [["Whitbrook", "Amanda", ""], ["Aickelin", "Uwe", ""], ["Garibaldi", "Jonathan M.", ""]]}, {"id": "1006.3652", "submitter": "Uwe Aickelin", "authors": "Mazlina Abdul Majid, Peer-Olaf Siebers, Uwe Aickelin", "title": "Modelling Reactive and Proactive Behaviour in Simulation", "comments": "9 pages, 7 figures, Operational Research Society 5th Simulation\n  Workshop (SW10)", "journal-ref": "Proceedings of Operational Research Society 5th Simulation\n  Workshop (SW10), Worcestershire, England, 2010, p23-31", "doi": null, "report-no": null, "categories": "cs.AI cs.CE cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This research investigated the simulation model behaviour of a traditional\nand combined discrete event as well as agent based simulation models when\nmodelling human reactive and proactive behaviour in human centric complex\nsystems. A departmental store was chosen as human centric complex case study\nwhere the operation system of a fitting room in WomensWear department was\ninvestigated. We have looked at ways to determine the efficiency of new\nmanagement policies for the fitting room operation through simulating the\nreactive and proactive behaviour of staff towards customers. Once development\nof the simulation models and their verification had been done, we carried out a\nvalidation experiment in the form of a sensitivity analysis. Subsequently, we\nexecuted a statistical analysis where the mixed reactive and proactive\nbehaviour experimental results were compared with some reactive experimental\nresults from previously published works. Generally, this case study discovered\nthat simple proactive individual behaviour could be modelled in both simulation\nmodels. In addition, we found the traditional discrete event model performed\nsimilar in the simulation model output compared to the combined discrete event\nand agent based simulation when modelling similar human behaviour.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2010 09:44:38 GMT"}], "update_date": "2010-07-05", "authors_parsed": [["Majid", "Mazlina Abdul", ""], ["Siebers", "Peer-Olaf", ""], ["Aickelin", "Uwe", ""]]}, {"id": "1006.3654", "submitter": "Uwe Aickelin", "authors": "Jamie Twycross, Uwe Aickelin, Amanda Whitbrook", "title": "Detecting Anomalous Process Behaviour using Second Generation Artificial\n  Immune Systems", "comments": "26 pages, 4 tables, 2 figures, International Journal of\n  Unconventional Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Immune Systems have been successfully applied to a number of\nproblem domains including fault tolerance and data mining, but have been shown\nto scale poorly when applied to computer intrusion detec- tion despite the fact\nthat the biological immune system is a very effective anomaly detector. This\nmay be because AIS algorithms have previously been based on the adaptive immune\nsystem and biologically-naive mod- els. This paper focuses on describing and\ntesting a more complex and biologically-authentic AIS model, inspired by the\ninteractions between the innate and adaptive immune systems. Its performance on\na realistic process anomaly detection problem is shown to be better than\nstandard AIS methods (negative-selection), policy-based anomaly detection\nmethods (systrace), and an alternative innate AIS approach (the DCA). In\naddition, it is shown that runtime information can be used in combination with\nsystem call information to enhance detection capability.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2010 09:55:04 GMT"}], "update_date": "2010-07-05", "authors_parsed": [["Twycross", "Jamie", ""], ["Aickelin", "Uwe", ""], ["Whitbrook", "Amanda", ""]]}, {"id": "1006.3678", "submitter": "Pedro Cabalar", "authors": "Pedro Cabalar", "title": "Functional Answer Set Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose an extension of Answer Set Programming (ASP), and in\nparticular, of its most general logical counterpart, Quantified Equilibrium\nLogic (QEL), to deal with partial functions. Although the treatment of equality\nin QEL can be established in different ways, we first analyse the choice of\ndecidable equality with complete functions and Herbrand models, recently\nproposed in the literature. We argue that this choice yields some\ncounterintuitive effects from a logic programming and knowledge representation\npoint of view. We then propose a variant called QELF where the set of functions\nis partitioned into partial and Herbrand functions (we also call constructors).\nIn the rest of the paper, we show a direct connection to Scott's Logic of\nExistence and present a practical application, proposing an extension of normal\nlogic programs to deal with partial functions and equality, so that they can be\ntranslated into function-free normal programs, being possible in this way to\ncompute their answer sets with any standard ASP solver.\n", "versions": [{"version": "v1", "created": "Fri, 18 Jun 2010 12:37:08 GMT"}], "update_date": "2010-06-21", "authors_parsed": [["Cabalar", "Pedro", ""]]}, {"id": "1006.4035", "submitter": "Uwe Aickelin", "authors": "Peer-Olaf Siebers, Uwe Aickelin, Helen Celia, Chris Clegg", "title": "Towards the Development of a Simulator for Investigating the Impact of\n  People Management Practices on Retail Performance", "comments": "24 pages, 7 figures, 6 tables, Journal of Simulation 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CE cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Often models for understanding the impact of management practices on retail\nperformance are developed under the assumption of stability, equilibrium and\nlinearity, whereas retail operations are considered in reality to be dynamic,\nnon-linear and complex. Alternatively, discrete event and agent-based modelling\nare approaches that allow the development of simulation models of heterogeneous\nnon-equilibrium systems for testing out different scenarios. When developing\nsimulation models one has to abstract and simplify from the real world, which\nmeans that one has to try and capture the 'essence' of the system required for\ndeveloping a representation of the mechanisms that drive the progression in the\nreal system. Simulation models can be developed at different levels of\nabstraction. To know the appropriate level of abstraction for a specific\napplication is often more of an art than a science. We have developed a retail\nbranch simulation model to investigate which level of model accuracy is\nrequired for such a model to obtain meaningful results for practitioners.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2010 11:23:23 GMT"}], "update_date": "2010-07-05", "authors_parsed": [["Siebers", "Peer-Olaf", ""], ["Aickelin", "Uwe", ""], ["Celia", "Helen", ""], ["Clegg", "Chris", ""]]}, {"id": "1006.4039", "submitter": "Feng Yan", "authors": "Feng Yan, Shreyas Sundaram, S. V. N. Vishwanathan, Yuan Qi", "title": "Distributed Autonomous Online Learning: Regrets and Intrinsic\n  Privacy-Preserving Properties", "comments": "25 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online learning has become increasingly popular on handling massive data. The\nsequential nature of online learning, however, requires a centralized learner\nto store data and update parameters. In this paper, we consider online learning\nwith {\\em distributed} data sources. The autonomous learners update local\nparameters based on local data sources and periodically exchange information\nwith a small subset of neighbors in a communication network. We derive the\nregret bound for strongly convex functions that generalizes the work by Ram et\nal. (2010) for convex functions. Most importantly, we show that our algorithm\nhas \\emph{intrinsic} privacy-preserving properties, and we prove the sufficient\nand necessary conditions for privacy preservation in the network. These\nconditions imply that for networks with greater-than-one connectivity, a\nmalicious learner cannot reconstruct the subgradients (and sensitive raw data)\nof other learners, which makes our algorithm appealing in privacy sensitive\napplications.\n", "versions": [{"version": "v1", "created": "Mon, 21 Jun 2010 11:30:06 GMT"}, {"version": "v2", "created": "Tue, 1 Feb 2011 03:16:12 GMT"}, {"version": "v3", "created": "Fri, 4 Feb 2011 16:06:35 GMT"}], "update_date": "2011-02-07", "authors_parsed": [["Yan", "Feng", ""], ["Sundaram", "Shreyas", ""], ["Vishwanathan", "S. V. N.", ""], ["Qi", "Yuan", ""]]}, {"id": "1006.4474", "submitter": "Christoph Lange", "authors": "Andrea Kohlhase, Michael Kohlhase, Christoph Lange", "title": "sTeX+ - a System for Flexible Formalization of Linked Data", "comments": "I-SEMANTICS 2010, September 1-3, 2010, Graz, Austria", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  We present the sTeX+ system, a user-driven advancement of sTeX - a semantic\nextension of LaTeX that allows for producing high-quality PDF documents for\n(proof)reading and printing, as well as semantic XML/OMDoc documents for the\nWeb or further processing. Originally sTeX had been created as an invasive,\nsemantic frontend for authoring XML documents. Here, we used sTeX in a Software\nEngineering case study as a formalization tool. In order to deal with modular\npre-semantic vocabularies and relations, we upgraded it to sTeX+ in a\nparticipatory design process. We present a tool chain that starts with an sTeX+\neditor and ultimately serves the generated documents as XHTML+RDFa Linked Data\nvia an OMDoc-enabled, versioned XML database. In the final output, all\nstructural annotations are preserved in order to enable semantic information\nretrieval services.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2010 11:27:19 GMT"}], "update_date": "2010-06-24", "authors_parsed": [["Kohlhase", "Andrea", ""], ["Kohlhase", "Michael", ""], ["Lange", "Christoph", ""]]}, {"id": "1006.4540", "submitter": "William Jackson", "authors": "N. Suguna and K. Thanushkodi", "title": "A Novel Rough Set Reduct Algorithm for Medical Domain Based on Bee\n  Colony Optimization", "comments": "IEEE Publication Format,\n  https://sites.google.com/site/journalofcomputing/", "journal-ref": "Journal of Computing, Vol. 2, No. 6, June 2010, NY, USA, ISSN\n  2151-9617", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection refers to the problem of selecting relevant features which\nproduce the most predictive outcome. In particular, feature selection task is\ninvolved in datasets containing huge number of features. Rough set theory has\nbeen one of the most successful methods used for feature selection. However,\nthis method is still not able to find optimal subsets. This paper proposes a\nnew feature selection method based on Rough set theory hybrid with Bee Colony\nOptimization (BCO) in an attempt to combat this. This proposed work is applied\nin the medical domain to find the minimal reducts and experimentally compared\nwith the Quick Reduct, Entropy Based Reduct, and other hybrid Rough Set methods\nsuch as Genetic Algorithm (GA), Ant Colony Optimization (ACO) and Particle\nSwarm Optimization (PSO).\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2010 14:53:33 GMT"}], "update_date": "2010-06-24", "authors_parsed": [["Suguna", "N.", ""], ["Thanushkodi", "K.", ""]]}, {"id": "1006.4544", "submitter": "William Jackson", "authors": "Mir Anamul Hasan, Khaja Md. Sher-E-Alam and Ahsan Raja Chowdhury", "title": "Human Disease Diagnosis Using a Fuzzy Expert System", "comments": "IEEE Publication Format,\n  https://sites.google.com/site/journalofcomputing/", "journal-ref": "Journal of Computing, Vol. 2, No. 6, June 2010, NY, USA, ISSN\n  2151-9617", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human disease diagnosis is a complicated process and requires high level of\nexpertise. Any attempt of developing a web-based expert system dealing with\nhuman disease diagnosis has to overcome various difficulties. This paper\ndescribes a project work aiming to develop a web-based fuzzy expert system for\ndiagnosing human diseases. Now a days fuzzy systems are being used successfully\nin an increasing number of application areas; they use linguistic rules to\ndescribe systems. This research project focuses on the research and development\nof a web-based clinical tool designed to improve the quality of the exchange of\nhealth information between health care professionals and patients.\nPractitioners can also use this web-based tool to corroborate diagnosis. The\nproposed system is experimented on various scenarios in order to evaluate it's\nperformance. In all the cases, proposed system exhibits satisfactory results.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2010 15:02:19 GMT"}], "update_date": "2010-06-24", "authors_parsed": [["Hasan", "Mir Anamul", ""], ["Sher-E-Alam", "Khaja Md.", ""], ["Chowdhury", "Ahsan Raja", ""]]}, {"id": "1006.4551", "submitter": "William Jackson", "authors": "Supriya Raheja and Smita Rajpal", "title": "Vagueness of Linguistic variable", "comments": "IEEE Publication Format,\n  https://sites.google.com/site/journalofcomputing/", "journal-ref": "Journal of Computing, Vol. 2, No. 6, June 2010, NY, USA, ISSN\n  2151-9617", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the area of computer science focusing on creating machines that can engage\non behaviors that humans consider intelligent. The ability to create\nintelligent machines has intrigued humans since ancient times and today with\nthe advent of the computer and 50 years of research into various programming\ntechniques, the dream of smart machines is becoming a reality. Researchers are\ncreating systems which can mimic human thought, understand speech, beat the\nbest human chessplayer, and countless other feats never before possible.\nAbility of the human to estimate the information is most brightly shown in\nusing of natural languages. Using words of a natural language for valuation\nqualitative attributes, for example, the person pawns uncertainty in form of\nvagueness in itself estimations. Vague sets, vague judgments, vague conclusions\ntakes place there and then, where and when the reasonable subject exists and\nalso is interested in something. The vague sets theory has arisen as the answer\nto an illegibility of language the reasonable subject speaks. Language of a\nreasonable subject is generated by vague events which are created by the reason\nand which are operated by the mind. The theory of vague sets represents an\nattempt to find such approximation of vague grouping which would be more\nconvenient, than the classical theory of sets in situations where the natural\nlanguage plays a significant role. Such theory has been offered by known\nAmerican mathematician Gau and Buehrer .In our paper we are describing how\nvagueness of linguistic variables can be solved by using the vague set\ntheory.This paper is mainly designed for one of directions of the eventology\n(the theory of the random vague events), which has arisen within the limits of\nthe probability theory and which pursue the unique purpose to describe\neventologically a movement of reason.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2010 15:09:04 GMT"}], "update_date": "2010-06-24", "authors_parsed": [["Raheja", "Supriya", ""], ["Rajpal", "Smita", ""]]}, {"id": "1006.4561", "submitter": "William Jackson", "authors": "Amjad Farooq, Syed Ahsan and Abad Shah", "title": "An Efficient Technique for Similarity Identification between Ontologies", "comments": "IEEE Publication Format,\n  https://sites.google.com/site/journalofcomputing/", "journal-ref": "Journal of Computing, Vol. 2, No. 6, June 2010, NY, USA, ISSN\n  2151-9617", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ontologies usually suffer from the semantic heterogeneity when simultaneously\nused in information sharing, merging, integrating and querying processes.\nTherefore, the similarity identification between ontologies being used becomes\na mandatory task for all these processes to handle the problem of semantic\nheterogeneity. In this paper, we propose an efficient technique for similarity\nmeasurement between two ontologies. The proposed technique identifies all\ncandidate pairs of similar concepts without omitting any similar pair. The\nproposed technique can be used in different types of operations on ontologies\nsuch as merging, mapping and aligning. By analyzing its results a reasonable\nimprovement in terms of completeness, correctness and overall quality of the\nresults has been found.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2010 15:17:01 GMT"}], "update_date": "2010-06-24", "authors_parsed": [["Farooq", "Amjad", ""], ["Ahsan", "Syed", ""], ["Shah", "Abad", ""]]}, {"id": "1006.4563", "submitter": "William Jackson", "authors": "Mohammad Mustafa Taye", "title": "The State of the Art: Ontology Web-Based Languages: XML Based", "comments": "IEEE Publication Format,\n  https://sites.google.com/site/journalofcomputing/", "journal-ref": "Journal of Computing, Vol. 2, No. 6, June 2010, NY, USA, ISSN\n  2151-9617", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many formal languages have been proposed to express or represent Ontologies,\nincluding RDF, RDFS, DAML+OIL and OWL. Most of these languages are based on XML\nsyntax, but with various terminologies and expressiveness. Therefore, choosing\na language for building an Ontology is the main step. The main point of\nchoosing language to represent Ontology is based mainly on what the Ontology\nwill represent or be used for. That language should have a range of quality\nsupport features such as ease of use, expressive power, compatibility, sharing\nand versioning, internationalisation. This is because different kinds of\nknowledge-based applications need different language features. The main\nobjective of these languages is to add semantics to the existing information on\nthe web. The aims of this paper is to provide a good knowledge of existing\nlanguage and understanding of these languages and how could be used.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2010 15:18:33 GMT"}], "update_date": "2010-06-24", "authors_parsed": [["Taye", "Mohammad Mustafa", ""]]}, {"id": "1006.4567", "submitter": "William Jackson", "authors": "Mohammad Mustafa Taye", "title": "Understanding Semantic Web and Ontologies: Theory and Applications", "comments": "IEEE Publication Format,\n  https://sites.google.com/site/journalofcomputing/", "journal-ref": "Journal of Computing, Vol. 2, No. 6, June 2010, NY, USA, ISSN\n  2151-9617", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic Web is actually an extension of the current one in that it\nrepresents information more meaningfully for humans and computers alike. It\nenables the description of contents and services in machine-readable form, and\nenables annotating, discovering, publishing, advertising and composing services\nto be automated. It was developed based on Ontology, which is considered as the\nbackbone of the Semantic Web. In other words, the current Web is transformed\nfrom being machine-readable to machine-understandable. In fact, Ontology is a\nkey technique with which to annotate semantics and provide a common,\ncomprehensible foundation for resources on the Semantic Web. Moreover, Ontology\ncan provide a common vocabulary, a grammar for publishing data, and can supply\na semantic description of data which can be used to preserve the Ontologies and\nkeep them ready for inference. This paper provides basic concepts of web\nservices and the Semantic Web, defines the structure and the main applications\nof ontology, and provides many relevant terms are explained in order to provide\na basic understanding of ontologies.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2010 15:20:02 GMT"}], "update_date": "2010-06-24", "authors_parsed": [["Taye", "Mohammad Mustafa", ""]]}, {"id": "1006.4645", "submitter": "Thomas Bartz-Beielstein", "authors": "Thomas Bartz-Beielstein", "title": "SPOT: An R Package For Automatic and Interactive Tuning of Optimization\n  Algorithms by Sequential Parameter Optimization", "comments": "Related software can be downloaded from\n  http://cran.r-project.org/web/packages/SPOT/index.html", "journal-ref": null, "doi": null, "report-no": "CIOP Technical Report 05/10. Cologne University of Applied Sciences", "categories": "cs.NE cs.AI math.OC stat.AP", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The sequential parameter optimization (SPOT) package for R is a toolbox for\ntuning and understanding simulation and optimization algorithms. Model-based\ninvestigations are common approaches in simulation and optimization. Sequential\nparameter optimization has been developed, because there is a strong need for\nsound statistical analysis of simulation and optimization algorithms. SPOT\nincludes methods for tuning based on classical regression and analysis of\nvariance techniques; tree-based models such as CART and random forest; Gaussian\nprocess models (Kriging), and combinations of different meta-modeling\napproaches. This article exemplifies how SPOT can be used for automatic and\ninteractive tuning.\n", "versions": [{"version": "v1", "created": "Wed, 23 Jun 2010 21:18:36 GMT"}], "update_date": "2010-06-25", "authors_parsed": [["Bartz-Beielstein", "Thomas", ""]]}, {"id": "1006.4948", "submitter": "Marina De Vos", "authors": "Georg Boenn, Martin Brain, Marina De Vos and John ffitch", "title": "Automatic Music Composition using Answer Set Programming", "comments": "31 pages, 10 figures. Extended version of our ICLP2008 paper.\n  Formatted following TPLP guidelines", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Music composition used to be a pen and paper activity. These these days music\nis often composed with the aid of computer software, even to the point where\nthe computer compose parts of the score autonomously. The composition of most\nstyles of music is governed by rules. We show that by approaching the\nautomation, analysis and verification of composition as a knowledge\nrepresentation task and formalising these rules in a suitable logical language,\npowerful and expressive intelligent composition tools can be easily built. This\napplication paper describes the use of answer set programming to construct an\nautomated system, named ANTON, that can compose melodic, harmonic and rhythmic\nmusic, diagnose errors in human compositions and serve as a computer-aided\ncomposition tool. The combination of harmonic, rhythmic and melodic composition\nin a single framework makes ANTON unique in the growing area of algorithmic\ncomposition. With near real-time composition, ANTON reaches the point where it\ncan not only be used as a component in an interactive composition tool but also\nhas the potential for live performances and concerts or automatically generated\nbackground music in a variety of applications. With the use of a fully\ndeclarative language and an \"off-the-shelf\" reasoning engine, ANTON provides\nthe human composer a tool which is significantly simpler, more compact and more\nversatile than other existing systems. This paper has been accepted for\npublication in Theory and Practice of Logic Programming (TPLP).\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2010 09:55:20 GMT"}], "update_date": "2010-06-28", "authors_parsed": [["Boenn", "Georg", ""], ["Brain", "Martin", ""], ["De Vos", "Marina", ""], ["ffitch", "John", ""]]}, {"id": "1006.4949", "submitter": "Uwe Aickelin", "authors": "Julie Greensmith, Amanda Whitbrook, Uwe Aickelin", "title": "Artificial Immune Systems (2010)", "comments": "29 pages, 1 algorithm, 3 figures, Handbook of Metaheuristics, 2nd\n  Edition, Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The human immune system has numerous properties that make it ripe for\nexploitation in the computational domain, such as robustness and fault\ntolerance, and many different algorithms, collectively termed Artificial Immune\nSystems (AIS), have been inspired by it. Two generations of AIS are currently\nin use, with the first generation relying on simplified immune models and the\nsecond generation utilising interdisciplinary collaboration to develop a deeper\nunderstanding of the immune system and hence produce more complex models. Both\ngenerations of algorithms have been successfully applied to a variety of\nproblems, including anomaly detection, pattern recognition, optimisation and\nrobotics. In this chapter an overview of AIS is presented, its evolution is\ndiscussed, and it is shown that the diversification of the field is linked to\nthe diversity of the immune system itself, leading to a number of algorithms as\nopposed to one archetypal system. Two case studies are also presented to help\nprovide insight into the mechanisms of AIS; these are the idiotypic network\napproach and the Dendritic Cell Algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2010 09:55:30 GMT"}], "update_date": "2010-07-05", "authors_parsed": [["Greensmith", "Julie", ""], ["Whitbrook", "Amanda", ""], ["Aickelin", "Uwe", ""]]}, {"id": "1006.5008", "submitter": "Uwe Aickelin", "authors": "Julie Greensmith, Uwe Aickelin, Steve Cayzer", "title": "Detecting Danger: The Dendritic Cell Algorithm", "comments": "27 pages, 8 figures, Robust Intelligent Systems", "journal-ref": "Robust Intelligent Systems, 12, p 89-112, 2008", "doi": null, "report-no": null, "categories": "cs.AI cs.CR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Dendritic Cell Algorithm (DCA) is inspired by the function of the\ndendritic cells of the human immune system. In nature, dendritic cells are the\nintrusion detection agents of the human body, policing the tissue and organs\nfor potential invaders in the form of pathogens. In this research, and abstract\nmodel of DC behaviour is developed and subsequently used to form an algorithm,\nthe DCA. The abstraction process was facilitated through close collaboration\nwith laboratory- based immunologists, who performed bespoke experiments, the\nresults of which are used as an integral part of this algorithm. The DCA is a\npopulation based algorithm, with each agent in the system represented as an\n'artificial DC'. Each DC has the ability to combine multiple data streams and\ncan add context to data suspected as anomalous. In this chapter the abstraction\nprocess and details of the resultant algorithm are given. The algorithm is\napplied to numerous intrusion detection problems in computer security including\nthe detection of port scans and botnets, where it has produced impressive\nresults with relatively low rates of false positives.\n", "versions": [{"version": "v1", "created": "Fri, 25 Jun 2010 15:30:45 GMT"}], "update_date": "2010-07-05", "authors_parsed": [["Greensmith", "Julie", ""], ["Aickelin", "Uwe", ""], ["Cayzer", "Steve", ""]]}, {"id": "1006.5041", "submitter": "Yoshinobu Kawahara", "authors": "Yoshinobu Kawahara, Kenneth Bollen, Shohei Shimizu and Takashi Washio", "title": "GroupLiNGAM: Linear non-Gaussian acyclic models for sets of variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding the structure of a graphical model has been received much attention\nin many fields. Recently, it is reported that the non-Gaussianity of data\nenables us to identify the structure of a directed acyclic graph without any\nprior knowledge on the structure. In this paper, we propose a novel\nnon-Gaussianity based algorithm for more general type of models; chain graphs.\nThe algorithm finds an ordering of the disjoint subsets of variables by\niteratively evaluating the independence between the variable subset and the\nresiduals when the remaining variables are regressed on those. However, its\ncomputational cost grows exponentially according to the number of variables.\nTherefore, we further discuss an efficient approximate approach for applying\nthe algorithm to large sized graphs. We illustrate the algorithm with\nartificial and real-world datasets.\n", "versions": [{"version": "v1", "created": "Thu, 24 Jun 2010 13:09:36 GMT"}], "update_date": "2010-06-28", "authors_parsed": [["Kawahara", "Yoshinobu", ""], ["Bollen", "Kenneth", ""], ["Shimizu", "Shohei", ""], ["Washio", "Takashi", ""]]}, {"id": "1006.5188", "submitter": "Nicola Di Mauro", "authors": "Nicola Di Mauro and Teresa M.A. Basile and Stefano Ferilli and\n  Floriana Esposito", "title": "Feature Construction for Relational Sequence Learning", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We tackle the problem of multi-class relational sequence learning using\nrelevant patterns discovered from a set of labelled sequences. To deal with\nthis problem, firstly each relational sequence is mapped into a feature vector\nusing the result of a feature construction method. Since, the efficacy of\nsequence learning algorithms strongly depends on the features used to represent\nthe sequences, the second step is to find an optimal subset of the constructed\nfeatures leading to high classification accuracy. This feature selection task\nhas been solved adopting a wrapper approach that uses a stochastic local search\nalgorithm embedding a naive Bayes classifier. The performance of the proposed\nmethod applied to a real-world dataset shows an improvement when compared to\nother established methods, such as hidden Markov models, Fisher kernels and\nconditional random fields for relational sequences.\n", "versions": [{"version": "v1", "created": "Sun, 27 Jun 2010 08:56:11 GMT"}], "update_date": "2010-06-29", "authors_parsed": [["Di Mauro", "Nicola", ""], ["Basile", "Teresa M. A.", ""], ["Ferilli", "Stefano", ""], ["Esposito", "Floriana", ""]]}, {"id": "1006.5511", "submitter": "Athar Kharal", "authors": "Athar Kharal", "title": "Soft Approximations and uni-int Decision Making", "comments": "This paper has been withdrawn by the author due to further expansion\n  of this work. Work is also submitted to a peer reviewed journal and is\n  expected to be published very soon", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Notions of core, support and inversion of a soft set have been defined and\nstudied. Soft approximations are soft sets developed through core and support,\nand are used for granulating the soft space. Membership structure of a soft set\nhas been probed in and many interesting properties presented. The mathematical\napparatus developed so far in this paper yields a detailed analysis of two\nworks viz. [N. Cagman, S. Enginoglu, Soft set theory and uni-int decision\nmaking, European Jr. of Operational Research (article in press, available\nonline 12 May 2010)] and [N. Cagman, S. Enginoglu, Soft matrix theory and its\ndecision making, Computers and Mathematics with Applications 59 (2010) 3308 -\n3314.]. We prove (Theorem 8.1) that uni-int method of Cagman is equivalent to a\ncore-support expression which is computationally far less expansive than\nuni-int. This also highlights some shortcomings in Cagman's uni-int method and\nthus motivates us to improve the method. We first suggest an improvement in\nuni-int method and then present a new conjecture to solve the optimum choice\nproblem given by Cagman and Enginoglu. Our Example 8.6 presents a case where\nthe optimum choice is intuitively clear yet both uni-int methods (Cagman's and\nour improved one) give wrong answer but the new conjecture solves the problem\ncorrectly.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2010 06:58:35 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2010 02:01:34 GMT"}], "update_date": "2016-11-25", "authors_parsed": [["Kharal", "Athar", ""]]}, {"id": "1006.5657", "submitter": "Alessandra Mileo", "authors": "A. Mileo, D. Merico, R. Bisiani", "title": "Reasoning Support for Risk Prediction and Prevention in Independent\n  Living", "comments": "36 pages, 5 figures, 10 tables. To appear in Theory and Practice of\n  Logic Programming (TPLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years there has been growing interest in solutions for the delivery\nof clinical care for the elderly, due to the large increase in aging\npopulation. Monitoring a patient in his home environment is necessary to ensure\ncontinuity of care in home settings, but, to be useful, this activity must not\nbe too invasive for patients and a burden for caregivers. We prototyped a\nsystem called SINDI (Secure and INDependent lIving), focused on i) collecting a\nlimited amount of data about the person and the environment through Wireless\nSensor Networks (WSN), and ii) inferring from these data enough information to\nsupport caregivers in understanding patients' well being and in predicting\npossible evolutions of their health. Our hierarchical logic-based model of\nhealth combines data from different sources, sensor data, tests results,\ncommon-sense knowledge and patient's clinical profile at the lower level, and\ncorrelation rules between health conditions across upper levels. The logical\nformalization and the reasoning process are based on Answer Set Programming.\nThe expressive power of this logic programming paradigm makes it possible to\nreason about health evolution even when the available information is incomplete\nand potentially incoherent, while declarativity simplifies rules specification\nby caregivers and allows automatic encoding of knowledge. This paper describes\nhow these issues have been targeted in the application scenario of the SINDI\nsystem.\n", "versions": [{"version": "v1", "created": "Tue, 29 Jun 2010 15:49:54 GMT"}], "update_date": "2010-06-30", "authors_parsed": [["Mileo", "A.", ""], ["Merico", "D.", ""], ["Bisiani", "R.", ""]]}, {"id": "1006.5896", "submitter": "Mikolas Janota", "authors": "Mikol\\'a\\v{s} Janota and Joao Marques-Silva and Radu Grigore", "title": "Counterexample Guided Abstraction Refinement Algorithm for Propositional\n  Circumscription", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Circumscription is a representative example of a nonmonotonic reasoning\ninference technique. Circumscription has often been studied for first order\ntheories, but its propositional version has also been the subject of extensive\nresearch, having been shown equivalent to extended closed world assumption\n(ECWA). Moreover, entailment in propositional circumscription is a well-known\nexample of a decision problem in the second level of the polynomial hierarchy.\nThis paper proposes a new Boolean Satisfiability (SAT)-based algorithm for\nentailment in propositional circumscription that explores the relationship of\npropositional circumscription to minimal models. The new algorithm is inspired\nby ideas commonly used in SAT-based model checking, namely counterexample\nguided abstraction refinement. In addition, the new algorithm is refined to\ncompute the theory closure for generalized close world assumption (GCWA).\nExperimental results show that the new algorithm can solve problem instances\nthat other solutions are unable to solve.\n", "versions": [{"version": "v1", "created": "Wed, 30 Jun 2010 16:00:08 GMT"}], "update_date": "2010-07-01", "authors_parsed": [["Janota", "Mikol\u00e1\u0161", ""], ["Marques-Silva", "Joao", ""], ["Grigore", "Radu", ""]]}]