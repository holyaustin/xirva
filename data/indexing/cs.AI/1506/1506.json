[{"id": "1506.00091", "submitter": "Leon Abdillah", "authors": "Tri Murti, Leon Andretti Abdillah, Muhammad Sobri", "title": "Sistem penunjang keputusan kelayakan pemberian pinjaman dengna metode\n  fuzzy tsukamoto", "comments": "5 pages, in Indonesian, in Seminar Nasional Inovasi dan Tren 2015\n  (SNIT2015), Bekasi, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision support systems (DSS) can be used to help settlement issues or\ndecisions that are semi-structured or structured. The method used is Fuzzy\nTsukamoto. PT Triprima Finance is a company engaged in the service sector\nlending with collateral in the form of Motor Vehicle Owner Book or car (reg).\nPT. Triprima Finance should consider borrowing from its customers with the\nconsent of the head manager. Such approval requires a long time because they\nhave to pass through many stages of the reporting procedure. Decision-making\nactivities at PT Triprima Finance carried out by the analysis process manually.\nTo help overcome these problems, the need for completion method in accuracy and\nspeed of decision making feasibility of lending. To overcome this need to\ndevelop a new system that is a decision support system Tsukamoto fuzzy method.\nis expected to facilitate kaposko to determine the decisions to be taken.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2015 08:06:20 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Murti", "Tri", ""], ["Abdillah", "Leon Andretti", ""], ["Sobri", "Muhammad", ""]]}, {"id": "1506.00099", "submitter": "Reza Azizi", "authors": "Reza Azizi, Hasan Sedghi, Hamid Shoja, Alireza Sepas-Moghaddam", "title": "A Novel Energy Aware Node Clustering Algorithm for Wireless Sensor\n  Networks Using a Modified Artificial Fish Swarm Algorithm", "comments": "13 pages, 5 figures, 2 tables, International Journal of Computer\n  Networks & Communications(IJCNC) Vol.7, No.3, May 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clustering problems are considered amongst the prominent challenges in\nstatistics and computational science. Clustering of nodes in wireless sensor\nnetworks which is used to prolong the life-time of networks is one of the\ndifficult tasks of clustering procedure. In order to perform nodes clustering,\na number of nodes are determined as cluster heads and other ones are joined to\none of these heads, based on different criteria e.g. Euclidean distance. So\nfar, different approaches have been proposed for this process, where swarm and\nevolutionary algorithms contribute in this regard. In this study, a novel\nalgorithm is proposed based on Artificial Fish Swarm Algorithm (AFSA) for\nclustering procedure. In the proposed method, the performance of the standard\nAFSA is improved by increasing balance between local and global searches.\nFurthermore, a new mechanism has been added to the base algorithm for improving\nconvergence speed in clustering problems. Performance of the proposed technique\nis compared to a number of state-of-the-art techniques in this field and the\noutcomes indicate the supremacy of the proposed technique.\n", "versions": [{"version": "v1", "created": "Sat, 30 May 2015 10:08:20 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Azizi", "Reza", ""], ["Sedghi", "Hasan", ""], ["Shoja", "Hamid", ""], ["Sepas-Moghaddam", "Alireza", ""]]}, {"id": "1506.00195", "submitter": "Kaisheng Yao", "authors": "Baolin Peng and Kaisheng Yao", "title": "Recurrent Neural Networks with External Memory for Language\n  Understanding", "comments": "submitted to Interspeech 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNNs) have become increasingly popular for the\ntask of language understanding. In this task, a semantic tagger is deployed to\nassociate a semantic label to each word in an input sequence. The success of\nRNN may be attributed to its ability to memorize long-term dependence that\nrelates the current-time semantic label prediction to the observations many\ntime instances away. However, the memory capacity of simple RNNs is limited\nbecause of the gradient vanishing and exploding problem. We propose to use an\nexternal memory to improve memorization capability of RNNs. We conducted\nexperiments on the ATIS dataset, and observed that the proposed model was able\nto achieve the state-of-the-art results. We compare our proposed model with\nalternative models and report analysis results that may provide insights for\nfuture research.\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2015 05:10:03 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Peng", "Baolin", ""], ["Yao", "Kaisheng", ""]]}, {"id": "1506.00301", "submitter": "Travis Wolfe", "authors": "Travis Wolfe, Mark Dredze, James Mayfield, Paul McNamee, Craig Harman,\n  Tim Finin, Benjamin Van Durme", "title": "Interactive Knowledge Base Population", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most work on building knowledge bases has focused on collecting entities and\nfacts from as large a collection of documents as possible. We argue for and\ndescribe a new paradigm where the focus is on a high-recall extraction over a\nsmall collection of documents under the supervision of a human expert, that we\ncall Interactive Knowledge Base Population (IKBP).\n", "versions": [{"version": "v1", "created": "Sun, 31 May 2015 22:56:43 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Wolfe", "Travis", ""], ["Dredze", "Mark", ""], ["Mayfield", "James", ""], ["McNamee", "Paul", ""], ["Harman", "Craig", ""], ["Finin", "Tim", ""], ["Van Durme", "Benjamin", ""]]}, {"id": "1506.00337", "submitter": "Zhiguo Long", "authors": "Zhiguo Long, Sanjiang Li", "title": "On Distributive Subalgebras of Qualitative Spatial and Temporal Calculi", "comments": "Adding proof of Theorem 2 to appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Qualitative calculi play a central role in representing and reasoning about\nqualitative spatial and temporal knowledge. This paper studies distributive\nsubalgebras of qualitative calculi, which are subalgebras in which (weak)\ncomposition distributives over nonempty intersections. It has been proven for\nRCC5 and RCC8 that path consistent constraint network over a distributive\nsubalgebra is always minimal and globally consistent (in the sense of strong\n$n$-consistency) in a qualitative sense. The well-known subclass of convex\ninterval relations provides one such an example of distributive subalgebras.\nThis paper first gives a characterisation of distributive subalgebras, which\nstates that the intersection of a set of $n\\geq 3$ relations in the subalgebra\nis nonempty if and only if the intersection of every two of these relations is\nnonempty. We further compute and generate all maximal distributive subalgebras\nfor Point Algebra, Interval Algebra, RCC5 and RCC8, Cardinal Relation Algebra,\nand Rectangle Algebra. Lastly, we establish two nice properties which will play\nan important role in efficient reasoning with constraint networks involving a\nlarge number of variables.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2015 03:24:18 GMT"}], "update_date": "2015-06-02", "authors_parsed": [["Long", "Zhiguo", ""], ["Li", "Sanjiang", ""]]}, {"id": "1506.00366", "submitter": "Khalid Raza", "authors": "Khalid Raza", "title": "Formal Concept Analysis for Knowledge Discovery from Biological Data", "comments": "14 pages, 2 figures", "journal-ref": "International Journal of Data Mining and Bioinformatics,\n  Inderscience, 18(4): 281-300 (2017)", "doi": "10.1504/IJDMB.2017.10009312", "report-no": null, "categories": "cs.AI cs.CE q-bio.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to rapid advancement in high-throughput techniques, such as microarrays\nand next generation sequencing technologies, biological data are increasing\nexponentially. The current challenge in computational biology and\nbioinformatics research is how to analyze these huge raw biological data to\nextract biologically meaningful knowledge. This review paper presents the\napplications of formal concept analysis for the analysis and knowledge\ndiscovery from biological data, including gene expression discretization, gene\nco-expression mining, gene expression clustering, finding genes in gene\nregulatory networks, enzyme/protein classifications, binding site\nclassifications, and so on. It also presents a list of FCA-based software tools\napplied in biological domain and covers the challenges faced so far.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2015 07:18:09 GMT"}], "update_date": "2020-05-05", "authors_parsed": [["Raza", "Khalid", ""]]}, {"id": "1506.00482", "submitter": "Oded Maler", "authors": "Irini-Eleftheria Mens (CNRS-VERIMAG), Oded Maler (CNRS-VERIMAG)", "title": "Learning Regular Languages over Large Ordered Alphabets", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 11, Issue 3 (September\n  17, 2015) lmcs:1589", "doi": "10.2168/LMCS-11(3:13)2015", "report-no": null, "categories": "cs.LO cs.AI cs.FL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work is concerned with regular languages defined over large alphabets,\neither infinite or just too large to be expressed enumeratively. We define a\ngeneric model where transitions are labeled by elements of a finite partition\nof the alphabet. We then extend Angluin's L* algorithm for learning regular\nlanguages from examples for such automata. We have implemented this algorithm\nand we demonstrate its behavior where the alphabet is a subset of the natural\nor real numbers. We sketch the extension of the algorithm to a class of\nlanguages over partially ordered alphabets.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2015 13:12:54 GMT"}, {"version": "v2", "created": "Wed, 16 Sep 2015 19:04:30 GMT"}], "update_date": "2017-01-11", "authors_parsed": [["Mens", "Irini-Eleftheria", "", "CNRS-VERIMAG"], ["Maler", "Oded", "", "CNRS-VERIMAG"]]}, {"id": "1506.00529", "submitter": "Marco Zaffalon", "authors": "Marco Zaffalon and Enrique Miranda", "title": "Desirability and the birth of incomplete preferences", "comments": null, "journal-ref": "Journal of Artificial Intelligence Research 60, pp. 1057-1126,\n  2017", "doi": "10.1613/jair.5230", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish an equivalence between two seemingly different theories: one is\nthe traditional axiomatisation of incomplete preferences on horse lotteries\nbased on the mixture independence axiom; the other is the theory of desirable\ngambles developed in the context of imprecise probability. The equivalence\nallows us to revisit incomplete preferences from the viewpoint of desirability\nand through the derived notion of coherent lower previsions. On this basis, we\nobtain new results and insights: in particular, we show that the theory of\nincomplete preferences can be developed assuming only the existence of a worst\nact---no best act is needed---, and that a weakened Archimedean axiom suffices\ntoo; this axiom allows us also to address some controversy about the regularity\nassumption (that probabilities should be positive---they need not), which\nenables us also to deal with uncountable possibility spaces; we show that it is\nalways possible to extend in a minimal way a preference relation to one with a\nworst act, and yet the resulting relation is never Archimedean, except in a\ntrivial case; we show that the traditional notion of state independence\ncoincides with the notion called strong independence in imprecise\nprobability---this leads us to give much a weaker definition of state\nindependence than the traditional one; we rework and uniform the notions of\ncomplete preferences, beliefs, values; we argue that Archimedeanity does not\ncapture all the problems that can be modelled with sets of expected utilities\nand we provide a new notion that does precisely that. Perhaps most importantly,\nwe argue throughout that desirability is a powerful and natural setting to\nmodel, and work with, incomplete preferences, even in case of non-Archimedean\nproblems. This leads us to suggest that desirability, rather than preference,\nshould be the primitive notion at the basis of decision-theoretic\naxiomatisations.\n", "versions": [{"version": "v1", "created": "Mon, 1 Jun 2015 15:22:34 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Zaffalon", "Marco", ""], ["Miranda", "Enrique", ""]]}, {"id": "1506.00711", "submitter": "Babak Saleh", "authors": "Ahmed Elgammal and Babak Saleh", "title": "Quantifying Creativity in Art Networks", "comments": "This paper will be published in the sixth International Conference on\n  Computational Creativity (ICCC) June 29-July 2nd 2015, Park City, Utah, USA.\n  This arXiv version is an extended version of the conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.CY cs.MM cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can we develop a computer algorithm that assesses the creativity of a\npainting given its context within art history? This paper proposes a novel\ncomputational framework for assessing the creativity of creative products, such\nas paintings, sculptures, poetry, etc. We use the most common definition of\ncreativity, which emphasizes the originality of the product and its influential\nvalue. The proposed computational framework is based on constructing a network\nbetween creative products and using this network to infer about the originality\nand influence of its nodes. Through a series of transformations, we construct a\nCreativity Implication Network. We show that inference about creativity in this\nnetwork reduces to a variant of network centrality problems which can be solved\nefficiently. We apply the proposed framework to the task of quantifying\ncreativity of paintings (and sculptures). We experimented on two datasets with\nover 62K paintings to illustrate the behavior of the proposed framework. We\nalso propose a methodology for quantitatively validating the results of the\nproposed algorithm, which we call the \"time machine experiment\".\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2015 00:20:54 GMT"}], "update_date": "2015-07-31", "authors_parsed": [["Elgammal", "Ahmed", ""], ["Saleh", "Babak", ""]]}, {"id": "1506.00858", "submitter": "Kewei Tu", "authors": "Kewei Tu", "title": "Stochastic And-Or Grammars: A Unified Framework and Logic Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic And-Or grammars (AOG) extend traditional stochastic grammars of\nlanguage to model other types of data such as images and events. In this paper\nwe propose a representation framework of stochastic AOGs that is agnostic to\nthe type of the data being modeled and thus unifies various domain-specific\nAOGs. Many existing grammar formalisms and probabilistic models in natural\nlanguage processing, computer vision, and machine learning can be seen as\nspecial cases of this framework. We also propose a domain-independent inference\nalgorithm of stochastic context-free AOGs and show its tractability under a\nreasonable assumption. Furthermore, we provide two interpretations of\nstochastic context-free AOGs as a subset of probabilistic logic, which connects\nstochastic AOGs to the field of statistical relational learning and clarifies\ntheir relation with a few existing statistical relational models.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2015 12:30:35 GMT"}, {"version": "v2", "created": "Tue, 8 Dec 2015 09:16:27 GMT"}, {"version": "v3", "created": "Mon, 11 Apr 2016 22:52:39 GMT"}], "update_date": "2016-04-13", "authors_parsed": [["Tu", "Kewei", ""]]}, {"id": "1506.00893", "submitter": "Joana C\\^orte-Real", "authors": "Joana C\\^orte-Real and Theofrastos Mantadelis and In\\^es Dutra and\n  Ricardo Rocha", "title": "SkILL - a Stochastic Inductive Logic Learner", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic Inductive Logic Programming (PILP) is a rel- atively unexplored\narea of Statistical Relational Learning which extends classic Inductive Logic\nProgramming (ILP). This work introduces SkILL, a Stochastic Inductive Logic\nLearner, which takes probabilistic annotated data and produces First Order\nLogic theories. Data in several domains such as medicine and bioinformatics\nhave an inherent degree of uncer- tainty, that can be used to produce models\ncloser to reality. SkILL can not only use this type of probabilistic data to\nextract non-trivial knowl- edge from databases, but it also addresses\nefficiency issues by introducing a novel, efficient and effective search\nstrategy to guide the search in PILP environments. The capabilities of SkILL\nare demonstrated in three dif- ferent datasets: (i) a synthetic toy example\nused to validate the system, (ii) a probabilistic adaptation of a well-known\nbiological metabolism ap- plication, and (iii) a real world medical dataset in\nthe breast cancer domain. Results show that SkILL can perform as well as a\ndeterministic ILP learner, while also being able to incorporate probabilistic\nknowledge that would otherwise not be considered.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2015 14:10:02 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["C\u00f4rte-Real", "Joana", ""], ["Mantadelis", "Theofrastos", ""], ["Dutra", "In\u00eas", ""], ["Rocha", "Ricardo", ""]]}, {"id": "1506.00935", "submitter": "Hastagiri Prakash Vanchinathan", "authors": "Hastagiri P. Vanchinathan, Andreas Marfurt, Charles-Antoine Robelin,\n  Donald Kossmann, Andreas Krause", "title": "Discovering Valuable Items from Massive Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Suppose there is a large collection of items, each with an associated cost\nand an inherent utility that is revealed only once we commit to selecting it.\nGiven a budget on the cumulative cost of the selected items, how can we pick a\nsubset of maximal value? This task generalizes several important problems such\nas multi-arm bandits, active search and the knapsack problem. We present an\nalgorithm, GP-Select, which utilizes prior knowledge about similarity be- tween\nitems, expressed as a kernel function. GP-Select uses Gaussian process\nprediction to balance exploration (estimating the unknown value of items) and\nexploitation (selecting items of high value). We extend GP-Select to be able to\ndiscover sets that simultaneously have high utility and are diverse. Our\npreference for diversity can be specified as an arbitrary monotone submodular\nfunction that quantifies the diminishing returns obtained when selecting\nsimilar items. Furthermore, we exploit the structure of the model updates to\nachieve an order of magnitude (up to 40X) speedup in our experiments without\nresorting to approximations. We provide strong guarantees on the performance of\nGP-Select and apply it to three real-world case studies of industrial\nrelevance: (1) Refreshing a repository of prices in a Global Distribution\nSystem for the travel industry, (2) Identifying diverse, binding-affine\npeptides in a vaccine de- sign task and (3) Maximizing clicks in a web-scale\nrecommender system by recommending items to users.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2015 16:01:46 GMT"}], "update_date": "2015-06-08", "authors_parsed": [["Vanchinathan", "Hastagiri P.", ""], ["Marfurt", "Andreas", ""], ["Robelin", "Charles-Antoine", ""], ["Kossmann", "Donald", ""], ["Krause", "Andreas", ""]]}, {"id": "1506.00999", "submitter": "Antoine Bordes", "authors": "Alberto Garcia-Duran, Antoine Bordes, Nicolas Usunier, Yves Grandvalet", "title": "Combining Two And Three-Way Embeddings Models for Link Prediction in\n  Knowledge Bases", "comments": "26 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper tackles the problem of endogenous link prediction for Knowledge\nBase completion. Knowledge Bases can be represented as directed graphs whose\nnodes correspond to entities and edges to relationships. Previous attempts\neither consist of powerful systems with high capacity to model complex\nconnectivity patterns, which unfortunately usually end up overfitting on rare\nrelationships, or in approaches that trade capacity for simplicity in order to\nfairly model all relationships, frequent or not. In this paper, we propose\nTatec a happy medium obtained by complementing a high-capacity model with a\nsimpler one, both pre-trained separately and then combined. We present several\nvariants of this model with different kinds of regularization and combination\nstrategies and show that this approach outperforms existing methods on\ndifferent types of relationships by achieving state-of-the-art results on four\nbenchmarks of the literature.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2015 19:34:19 GMT"}], "update_date": "2015-06-03", "authors_parsed": [["Garcia-Duran", "Alberto", ""], ["Bordes", "Antoine", ""], ["Usunier", "Nicolas", ""], ["Grandvalet", "Yves", ""]]}, {"id": "1506.01056", "submitter": "Peng Lin", "authors": "Peng Lin", "title": "Performing Bayesian Risk Aggregation using Discrete Approximation\n  Algorithms with Graph Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Risk aggregation is a popular method used to estimate the sum of a collection\nof financial assets or events, where each asset or event is modelled as a\nrandom variable. Applications, in the financial services industry, include\ninsurance, operational risk, stress testing, and sensitivity analysis, but the\nproblem is widely encountered in many other application domains. This thesis\nhas contributed two algorithms to perform Bayesian risk aggregation when model\nexhibit hybrid dependency and high dimensional inter-dependency. The first\nalgorithm operates on a subset of the general problem, with an emphasis on\nconvolution problems, in the presence of continuous and discrete variables (so\ncalled hybrid models) and the second algorithm offer a universal method for\ngeneral purpose inference over much wider classes of Bayesian Network models.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2015 20:53:26 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Lin", "Peng", ""]]}, {"id": "1506.01062", "submitter": "Panos Ipeirotis", "authors": "Panagiotis G. Ipeirotis and Evgeniy Gabrilovich", "title": "Quizz: Targeted crowdsourcing with a billion (potential) users", "comments": "WWW '14 Proceedings of the 23rd international conference on World\n  Wide Web. 11 pages", "journal-ref": null, "doi": "10.1145/2566486.2567988", "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe Quizz, a gamified crowdsourcing system that simultaneously\nassesses the knowledge of users and acquires new knowledge from them. Quizz\noperates by asking users to complete short quizzes on specific topics; as a\nuser answers the quiz questions, Quizz estimates the user's competence. To\nacquire new knowledge, Quizz also incorporates questions for which we do not\nhave a known answer; the answers given by competent users provide useful\nsignals for selecting the correct answers for these questions. Quizz actively\ntries to identify knowledgeable users on the Internet by running advertising\ncampaigns, effectively leveraging the targeting capabilities of existing,\npublicly available, ad placement services. Quizz quantifies the contributions\nof the users using information theory and sends feedback to the\nadvertisingsystem about each user. The feedback allows the ad targeting\nmechanism to further optimize ad placement.\n  Our experiments, which involve over ten thousand users, confirm that we can\ncrowdsource knowledge curation for niche and specialized topics, as the\nadvertising network can automatically identify users with the desired expertise\nand interest in the given topic. We present controlled experiments that examine\nthe effect of various incentive mechanisms, highlighting the need for having\nshort-term rewards as goals, which incentivize the users to contribute.\nFinally, our cost-quality analysis indicates that the cost of our approach is\nbelow that of hiring workers through paid-crowdsourcing platforms, while\noffering the additional advantage of giving access to billions of potential\nusers all over the planet, and being able to reach users with specialized\nexpertise that is not typically available through existing labor marketplaces.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2015 21:12:22 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Ipeirotis", "Panagiotis G.", ""], ["Gabrilovich", "Evgeniy", ""]]}, {"id": "1506.01071", "submitter": "Aleksey Buzmakov", "authors": "Aleksey Buzmakov and Sergei O. Kuznetsov and Amedeo Napoli", "title": "Fast Generation of Best Interval Patterns for Nonmonotonic Constraints", "comments": "18 pages; 2 figures; 2 tables; 1 algorithm; PKDD 2015 Conference\n  Scientific Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In pattern mining, the main challenge is the exponential explosion of the set\nof patterns. Typically, to solve this problem, a constraint for pattern\nselection is introduced. One of the first constraints proposed in pattern\nmining is support (frequency) of a pattern in a dataset. Frequency is an\nanti-monotonic function, i.e., given an infrequent pattern, all its\nsuperpatterns are not frequent. However, many other constraints for pattern\nselection are not (anti-)monotonic, which makes it difficult to generate\npatterns satisfying these constraints. In this paper we introduce the notion of\nprojection-antimonotonicity and $\\theta$-$\\Sigma\\o\\phi\\iota\\alpha$ algorithm\nthat allows efficient generation of the best patterns for some nonmonotonic\nconstraints. In this paper we consider stability and $\\Delta$-measure, which\nare nonmonotonic constraints, and apply them to interval tuple datasets. In the\nexperiments, we compute best interval tuple patterns w.r.t. these measures and\nshow the advantage of our approach over postfiltering approaches.\n  KEYWORDS: Pattern mining, nonmonotonic constraints, interval tuple data\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2015 21:32:14 GMT"}, {"version": "v2", "created": "Tue, 16 Jun 2015 15:31:19 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Buzmakov", "Aleksey", ""], ["Kuznetsov", "Sergei O.", ""], ["Napoli", "Amedeo", ""]]}, {"id": "1506.01072", "submitter": "Xinyu Wu", "authors": "Xinyu Wu, Vishal Saxena, Kehan Zhu", "title": "Homogeneous Spiking Neuromorphic System for Real-World Pattern\n  Recognition", "comments": "This is a preprint of an article accepted for publication in IEEE\n  Journal on Emerging and Selected Topics in Circuits and Systems, vol 5, no.\n  2, June 2015", "journal-ref": "IEEE Journal on Emerging and Selected Topics in Circuits and\n  Systems, vol 5, no. 2, June 2015", "doi": "10.1109/JETCAS.2015.2433552", "report-no": null, "categories": "cs.NE cs.AI cs.CV cs.ET", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A neuromorphic chip that combines CMOS analog spiking neurons and memristive\nsynapses offers a promising solution to brain-inspired computing, as it can\nprovide massive neural network parallelism and density. Previous hybrid analog\nCMOS-memristor approaches required extensive CMOS circuitry for training, and\nthus eliminated most of the density advantages gained by the adoption of\nmemristor synapses. Further, they used different waveforms for pre and\npost-synaptic spikes that added undesirable circuit overhead. Here we describe\na hardware architecture that can feature a large number of memristor synapses\nto learn real-world patterns. We present a versatile CMOS neuron that combines\nintegrate-and-fire behavior, drives passive memristors and implements\ncompetitive learning in a compact circuit module, and enables in-situ\nplasticity in the memristor synapses. We demonstrate handwritten-digits\nrecognition using the proposed architecture using transistor-level circuit\nsimulations. As the described neuromorphic architecture is homogeneous, it\nrealizes a fundamental building block for large-scale energy-efficient\nbrain-inspired silicon chips that could lead to next-generation cognitive\ncomputing.\n", "versions": [{"version": "v1", "created": "Tue, 2 Jun 2015 21:35:51 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2015 20:32:49 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Wu", "Xinyu", ""], ["Saxena", "Vishal", ""], ["Zhu", "Kehan", ""]]}, {"id": "1506.01094", "submitter": "Kelvin Guu", "authors": "Kelvin Guu, John Miller, Percy Liang", "title": "Traversing Knowledge Graphs in Vector Space", "comments": "2015 Conference on Empirical Methods on Natural Language Processing\n  (EMNLP)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DB stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Path queries on a knowledge graph can be used to answer compositional\nquestions such as \"What languages are spoken by people living in Lisbon?\".\nHowever, knowledge graphs often have missing facts (edges) which disrupts path\nqueries. Recent models for knowledge base completion impute missing facts by\nembedding knowledge graphs in vector spaces. We show that these models can be\nrecursively applied to answer path queries, but that they suffer from cascading\nerrors. This motivates a new \"compositional\" training objective, which\ndramatically improves all models' ability to answer path queries, in some cases\nmore than doubling accuracy. On a standard knowledge base completion task, we\nalso demonstrate that compositional training acts as a novel form of structural\nregularization, reliably improving performance across all base models (reducing\nerrors by up to 43%) and achieving new state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2015 00:38:25 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2015 05:16:24 GMT"}], "update_date": "2015-08-20", "authors_parsed": [["Guu", "Kelvin", ""], ["Miller", "John", ""], ["Liang", "Percy", ""]]}, {"id": "1506.01170", "submitter": "Stefano Albrecht", "authors": "Stefano V. Albrecht, Subramanian Ramamoorthy", "title": "A Game-Theoretic Model and Best-Response Learning Method for Ad Hoc\n  Coordination in Multiagent Systems", "comments": "Technical Report, The University of Edinburgh, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ad hoc coordination problem is to design an autonomous agent which is\nable to achieve optimal flexibility and efficiency in a multiagent system with\nno mechanisms for prior coordination. We conceptualise this problem formally\nusing a game-theoretic model, called the stochastic Bayesian game, in which the\nbehaviour of a player is determined by its private information, or type. Based\non this model, we derive a solution, called Harsanyi-Bellman Ad Hoc\nCoordination (HBA), which utilises the concept of Bayesian Nash equilibrium in\na planning procedure to find optimal actions in the sense of Bellman optimal\ncontrol. We evaluate HBA in a multiagent logistics domain called level-based\nforaging, showing that it achieves higher flexibility and efficiency than\nseveral alternative algorithms. We also report on a human-machine experiment at\na public science exhibition in which the human participants played repeated\nPrisoner's Dilemma and Rock-Paper-Scissors against HBA and alternative\nalgorithms, showing that HBA achieves equal efficiency and a significantly\nhigher welfare and winning rate.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2015 09:12:58 GMT"}], "update_date": "2015-06-04", "authors_parsed": [["Albrecht", "Stefano V.", ""], ["Ramamoorthy", "Subramanian", ""]]}, {"id": "1506.01245", "submitter": "Xinhua Zhu", "authors": "Xinhua Zhu, Fei Li, Hongchao Chen, Qi Peng", "title": "A density compensation-based path computing model for measuring semantic\n  similarity", "comments": "17 pages,11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The shortest path between two concepts in a taxonomic ontology is commonly\nused to represent the semantic distance between concepts in the edge-based\nsemantic similarity measures. In the past, the edge counting is considered to\nbe the default method for the path computation, which is simple, intuitive and\nhas low computational complexity. However, a large lexical taxonomy of such as\nWordNet has the irregular densities of links between concepts due to its broad\ndomain but. The edge counting-based path computation is powerless for this\nnon-uniformity problem. In this paper, we advocate that the path computation is\nable to be separated from the edge-based similarity measures and form various\ngeneral computing models. Therefore, in order to solve the problem of\nnon-uniformity of concept density in a large taxonomic ontology, we propose a\nnew path computing model based on the compensation of local area density of\nconcepts, which is equal to the number of direct hyponyms of the subsumers of\nconcepts in their shortest path. This path model considers the local area\ndensity of concepts as an extension of the edge-based path and converts the\nlocal area density divided by their depth into the compensation for edge-based\npath with an adjustable parameter, which idea has been proven to be consistent\nwith the information theory. This model is a general path computing model and\ncan be applied in various edge-based similarity algorithms. The experiment\nresults show that the proposed path model improves the average correlation\nbetween edge-based measures with human judgments on Miller and Charles\nbenchmark from less than 0.8 to more than 0.85, and has a big advantage in\nefficiency than information content (IC) computation in a dynamic ontology,\nthereby successfully solving the non-uniformity problem of taxonomic ontology.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2015 13:53:05 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Zhu", "Xinhua", ""], ["Li", "Fei", ""], ["Chen", "Hongchao", ""], ["Peng", "Qi", ""]]}, {"id": "1506.01273", "submitter": "David Martins de Matos", "authors": "Marta Apar\\'icio, Paulo Figueiredo, Francisco Raposo, David Martins de\n  Matos, Ricardo Ribeiro, Lu\\'is Marujo", "title": "Summarization of Films and Documentaries Based on Subtitles and Scripts", "comments": "7 pages, 9 tables, 4 figures, submitted to Pattern Recognition\n  Letters (Elsevier)", "journal-ref": "Pattern Recognition Letters, Volume 73, 1 April 2016, Pages 7-12", "doi": "10.1016/j.patrec.2015.12.016", "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We assess the performance of generic text summarization algorithms applied to\nfilms and documentaries, using the well-known behavior of summarization of news\narticles as reference. We use three datasets: (i) news articles, (ii) film\nscripts and subtitles, and (iii) documentary subtitles. Standard ROUGE metrics\nare used for comparing generated summaries against news abstracts, plot\nsummaries, and synopses. We show that the best performing algorithms are LSA,\nfor news articles and documentaries, and LexRank and Support Sets, for films.\nDespite the different nature of films and documentaries, their relative\nbehavior is in accordance with that obtained for news articles.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2015 15:07:14 GMT"}, {"version": "v2", "created": "Thu, 4 Jun 2015 12:41:55 GMT"}, {"version": "v3", "created": "Wed, 9 Mar 2016 16:50:43 GMT"}], "update_date": "2016-03-10", "authors_parsed": [["Apar\u00edcio", "Marta", ""], ["Figueiredo", "Paulo", ""], ["Raposo", "Francisco", ""], ["de Matos", "David Martins", ""], ["Ribeiro", "Ricardo", ""], ["Marujo", "Lu\u00eds", ""]]}, {"id": "1506.01326", "submitter": "Philipp Hennig PhD", "authors": "Philipp Hennig and Michael A Osborne and Mark Girolami", "title": "Probabilistic Numerics and Uncertainty in Computations", "comments": "Author Generated Postprint. 17 pages, 4 Figures, 1 Table", "journal-ref": null, "doi": "10.1098/rspa.2015.0142", "report-no": null, "categories": "math.NA cs.AI cs.LG stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We deliver a call to arms for probabilistic numerical methods: algorithms for\nnumerical tasks, including linear algebra, integration, optimization and\nsolving differential equations, that return uncertainties in their\ncalculations. Such uncertainties, arising from the loss of precision induced by\nnumerical calculation with limited time or hardware, are important for much\ncontemporary science and industry. Within applications such as climate science\nand astrophysics, the need to make decisions on the basis of computations with\nlarge and complex data has led to a renewed focus on the management of\nnumerical uncertainty. We describe how several seminal classic numerical\nmethods can be interpreted naturally as probabilistic inference. We then show\nthat the probabilistic view suggests new algorithms that can flexibly be\nadapted to suit application specifics, while delivering improved empirical\nperformance. We provide concrete illustrations of the benefits of probabilistic\nnumeric algorithms on real scientific problems from astrometry and astronomical\nimaging, while highlighting open problems with these new algorithms. Finally,\nwe describe how probabilistic numerical methods provide a coherent framework\nfor identifying the uncertainty in calculations performed with a combination of\nnumerical algorithms (e.g. both numerical optimisers and differential equation\nsolvers), potentially allowing the diagnosis (and control) of error sources in\ncomputations.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2015 17:45:01 GMT"}], "update_date": "2016-02-17", "authors_parsed": [["Hennig", "Philipp", ""], ["Osborne", "Michael A", ""], ["Girolami", "Mark", ""]]}, {"id": "1506.01432", "submitter": "Ondrej Kuzelka", "authors": "Ondrej Kuzelka and Jesse Davis and Steven Schockaert", "title": "Encoding Markov Logic Networks in Possibilistic Logic", "comments": "Extended version of a paper appearing in UAI 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov logic uses weighted formulas to compactly encode a probability\ndistribution over possible worlds. Despite the use of logical formulas, Markov\nlogic networks (MLNs) can be difficult to interpret, due to the often\ncounter-intuitive meaning of their weights. To address this issue, we propose a\nmethod to construct a possibilistic logic theory that exactly captures what can\nbe derived from a given MLN using maximum a posteriori (MAP) inference.\nUnfortunately, the size of this theory is exponential in general. We therefore\nalso propose two methods which can derive compact theories that still capture\nMAP inference, but only for specific types of evidence. These theories can be\nused, among others, to make explicit the hidden assumptions underlying an MLN\nor to explain the predictions it makes.\n", "versions": [{"version": "v1", "created": "Wed, 3 Jun 2015 23:20:28 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2015 19:58:03 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Kuzelka", "Ondrej", ""], ["Davis", "Jesse", ""], ["Schockaert", "Steven", ""]]}, {"id": "1506.01597", "submitter": "Piji Li", "authors": "Lidong Bing, Piji Li, Yi Liao, Wai Lam, Weiwei Guo, Rebecca J.\n  Passonneau", "title": "Abstractive Multi-Document Summarization via Phrase Selection and\n  Merging", "comments": "11 pages, 1 figure, accepted as a full paper at ACL 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an abstraction-based multi-document summarization framework that\ncan construct new sentences by exploring more fine-grained syntactic units than\nsentences, namely, noun/verb phrases. Different from existing abstraction-based\napproaches, our method first constructs a pool of concepts and facts\nrepresented by phrases from the input documents. Then new sentences are\ngenerated by selecting and merging informative phrases to maximize the salience\nof phrases and meanwhile satisfy the sentence construction constraints. We\nemploy integer linear optimization for conducting phrase selection and merging\nsimultaneously in order to achieve the global optimal solution for a summary.\nExperimental results on the benchmark data set TAC 2011 show that our framework\noutperforms the state-of-the-art models under automated pyramid evaluation\nmetric, and achieves reasonably well results on manual linguistic quality\nevaluation.\n", "versions": [{"version": "v1", "created": "Thu, 4 Jun 2015 14:04:10 GMT"}, {"version": "v2", "created": "Fri, 5 Jun 2015 15:02:46 GMT"}], "update_date": "2015-06-08", "authors_parsed": [["Bing", "Lidong", ""], ["Li", "Piji", ""], ["Liao", "Yi", ""], ["Lam", "Wai", ""], ["Guo", "Weiwei", ""], ["Passonneau", "Rebecca J.", ""]]}, {"id": "1506.01864", "submitter": "Konstantin Yakovlev S", "authors": "Konstantin Yakovlev, Egor Baskin, Ivan Hramoin", "title": "Grid-based angle-constrained path planning", "comments": "13 pages (12 pages: main text, 1 page: references), 7 figures, 20\n  references, submitted 2015-June-22 to \"The 38 German Conference on Artificial\n  Intelligence\" (KI-2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Square grids are commonly used in robotics and game development as spatial\nmodels and well known in AI community heuristic search algorithms (such as A*,\nJPS, Theta* etc.) are widely used for path planning on grids. A lot of research\nis concentrated on finding the shortest (in geometrical sense) paths while in\nmany applications finding smooth paths (rather than the shortest ones but\ncontaining sharp turns) is preferable. In this paper we study the problem of\ngenerating smooth paths and concentrate on angle constrained path planning. We\nput angle-constrained path planning problem formally and present a new\nalgorithm tailored to solve it - LIAN. We examine LIAN both theoretically and\nempirically. We show that it is sound and complete (under some restrictions).\nWe also show that LIAN outperforms the analogues when solving numerous path\nplanning tasks within urban outdoor navigation scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2015 11:09:23 GMT"}, {"version": "v2", "created": "Tue, 25 Aug 2015 15:59:28 GMT"}], "update_date": "2015-08-26", "authors_parsed": [["Yakovlev", "Konstantin", ""], ["Baskin", "Egor", ""], ["Hramoin", "Ivan", ""]]}, {"id": "1506.01911", "submitter": "Lionel Pigou", "authors": "Lionel Pigou, A\\\"aron van den Oord, Sander Dieleman, Mieke Van\n  Herreweghe, Joni Dambre", "title": "Beyond Temporal Pooling: Recurrence and Temporal Convolutions for\n  Gesture Recognition in Video", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have demonstrated the power of recurrent neural networks for\nmachine translation, image captioning and speech recognition. For the task of\ncapturing temporal structure in video, however, there still remain numerous\nopen research questions. Current research suggests using a simple temporal\nfeature pooling strategy to take into account the temporal aspect of video. We\ndemonstrate that this method is not sufficient for gesture recognition, where\ntemporal information is more discriminative compared to general video\nclassification tasks. We explore deep architectures for gesture recognition in\nvideo and propose a new end-to-end trainable neural network architecture\nincorporating temporal convolutions and bidirectional recurrence. Our main\ncontributions are twofold; first, we show that recurrence is crucial for this\ntask; second, we show that adding temporal convolutions leads to significant\nimprovements. We evaluate the different approaches on the Montalbano gesture\nrecognition dataset, where we achieve state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2015 13:43:01 GMT"}, {"version": "v2", "created": "Mon, 9 Nov 2015 16:20:26 GMT"}, {"version": "v3", "created": "Wed, 10 Feb 2016 16:50:29 GMT"}], "update_date": "2016-02-11", "authors_parsed": [["Pigou", "Lionel", ""], ["Oord", "A\u00e4ron van den", ""], ["Dieleman", "Sander", ""], ["Van Herreweghe", "Mieke", ""], ["Dambre", "Joni", ""]]}, {"id": "1506.02060", "submitter": "Vasile Patrascu", "authors": "Vasile Patrascu", "title": "Similarity, Cardinality and Entropy for Bipolar Fuzzy Set in the\n  Framework of Penta-valued Representation", "comments": "6 pages. Submitted to journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper one presents new similarity, cardinality and entropy measures\nfor bipolar fuzzy set and for its particular forms like intuitionistic,\nparaconsistent and fuzzy set. All these are constructed in the framework of\nmulti-valued representations and are based on a penta-valued logic that uses\nthe following logical values: true, false, unknown, contradictory and\nambiguous. Also a new distance for bounded real interval was defined.\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2015 08:56:02 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Patrascu", "Vasile", ""]]}, {"id": "1506.02061", "submitter": "Vasile Patrascu", "authors": "Vasile Patrascu", "title": "Entropy and Syntropy in the Context of Five-Valued Logics", "comments": "9 pages. Submitted to journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This paper presents a five-valued representation of bifuzzy sets. This\nrepresentation is related to a five-valued logic that uses the following\nvalues: true, false, inconsistent, incomplete and ambiguous. In the framework\nof five-valued representation, formulae for similarity, entropy and syntropy of\nbifuzzy sets are constructed.\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2015 09:36:49 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Patrascu", "Vasile", ""]]}, {"id": "1506.02082", "submitter": "Philip Baback Alipour", "authors": "Philip B. Alipour, Matteus Magnusson, Martin W. Olsson, Nooshin H.\n  Ghasemi, Lawrence Henesey", "title": "A Real-time Cargo Damage Management System via a Sorting Array\n  Triangulation Technique", "comments": "This article is a report on a developed IDSS system/prototype aimed\n  to be published for a journal conference proceedings and/or a full paper\n  under Computer Science and Software Engineering categories. 28 pages; 10\n  Figures including graphs; 5 tables; presentation file is available at\n  http://web.uvic.ca/~phibal12/Presentations/IDSS_proj.pptx Ask authors for\n  full code and/or other files", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report covers an intelligent decision support system (IDSS), which\nhandles an efficient and effective way to rapidly inspect containerized cargos\nfor defection. Defection is either cargo exposure to radiation, physical\ndamages such as holes, punctured surfaces, iron surface oxidation, etc. The\nsystem uses a sorting array triangulation technique (SAT) and surface damage\ndetection (SDD) to conduct the inspection. This new technique saves time and\nmoney on finding damaged goods during transportation such that, instead of\nrunning $n$ inspections on $n$ containers, only 3 inspections per triangulation\nor a ratio of $3:n$ is required, assuming $n > 3$ containers. The damaged stack\nin the array is virtually detected contiguous to an actually-damaged cargo by\ncalculating nearby distances of such cargos, delivering reliable estimates for\nthe whole local stack population. The estimated values on damaged, somewhat\ndamaged and undamaged cargo stacks, are listed and profiled after being sorted\nby the program, thereby submitted to the manager for a final decision. The\nreport describes the problem domain and the implementation of the simulator\nprototype, showing how the system operates via software, hardware with/without\nhuman agents, conducting real-time inspections and management per se.\n", "versions": [{"version": "v1", "created": "Fri, 5 Jun 2015 22:56:18 GMT"}, {"version": "v2", "created": "Sun, 14 Jun 2015 20:49:46 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Alipour", "Philip B.", ""], ["Magnusson", "Matteus", ""], ["Olsson", "Martin W.", ""], ["Ghasemi", "Nooshin H.", ""], ["Henesey", "Lawrence", ""]]}, {"id": "1506.02113", "submitter": "David Chickering", "authors": "David Maxwell Chickering and Christopher Meek", "title": "Selective Greedy Equivalence Search: Finding Optimal Bayesian Networks\n  Using a Polynomial Number of Score Evaluations", "comments": "Full version of UAI paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Selective Greedy Equivalence Search (SGES), a restricted version\nof Greedy Equivalence Search (GES). SGES retains the asymptotic correctness of\nGES but, unlike GES, has polynomial performance guarantees. In particular, we\nshow that when data are sampled independently from a distribution that is\nperfect with respect to a DAG ${\\cal G}$ defined over the observable variables\nthen, in the limit of large data, SGES will identify ${\\cal G}$'s equivalence\nclass after a number of score evaluations that is (1) polynomial in the number\nof nodes and (2) exponential in various complexity measures including\nmaximum-number-of-parents, maximum-clique-size, and a new measure called {\\em\nv-width} that is at least as small as---and potentially much smaller than---the\nother two. More generally, we show that for any hereditary and\nequivalence-invariant property $\\Pi$ known to hold in ${\\cal G}$, we retain the\nlarge-sample optimality guarantees of GES even if we ignore any GES deletion\noperator during the backward phase that results in a state for which $\\Pi$ does\nnot hold in the common-descendants subgraph.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2015 03:56:44 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Chickering", "David Maxwell", ""], ["Meek", "Christopher", ""]]}, {"id": "1506.02188", "submitter": "Yinlam Chow", "authors": "Yinlam Chow and Aviv Tamar and Shie Mannor and Marco Pavone", "title": "Risk-Sensitive and Robust Decision-Making: a CVaR Optimization Approach", "comments": "Submitted to NIPS 15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we address the problem of decision making within a Markov\ndecision process (MDP) framework where risk and modeling errors are taken into\naccount. Our approach is to minimize a risk-sensitive conditional-value-at-risk\n(CVaR) objective, as opposed to a standard risk-neutral expectation. We refer\nto such problem as CVaR MDP. Our first contribution is to show that a CVaR\nobjective, besides capturing risk sensitivity, has an alternative\ninterpretation as expected cost under worst-case modeling errors, for a given\nerror budget. This result, which is of independent interest, motivates CVaR\nMDPs as a unifying framework for risk-sensitive and robust decision making. Our\nsecond contribution is to present an approximate value-iteration algorithm for\nCVaR MDPs and analyze its convergence rate. To our knowledge, this is the first\nsolution algorithm for CVaR MDPs that enjoys error guarantees. Finally, we\npresent results from numerical experiments that corroborate our theoretical\nfindings and show the practicality of our approach.\n", "versions": [{"version": "v1", "created": "Sat, 6 Jun 2015 19:52:23 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Chow", "Yinlam", ""], ["Tamar", "Aviv", ""], ["Mannor", "Shie", ""], ["Pavone", "Marco", ""]]}, {"id": "1506.02264", "submitter": "Shmuel Peleg", "authors": "Yedid Hoshen, Shmuel Peleg", "title": "Visual Learning of Arithmetic Operations", "comments": "To appear in AAAI 2016", "journal-ref": "Proc. AAAI'16, Phoenix, Feb. 2016, pp. 3733-3739", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A simple Neural Network model is presented for end-to-end visual learning of\narithmetic operations from pictures of numbers. The input consists of two\npictures, each showing a 7-digit number. The output, also a picture, displays\nthe number showing the result of an arithmetic operation (e.g., addition or\nsubtraction) on the two input numbers. The concepts of a number, or of an\noperator, are not explicitly introduced. This indicates that addition is a\nsimple cognitive task, which can be learned visually using a very small number\nof neurons.\n  Other operations, e.g., multiplication, were not learnable using this\narchitecture. Some tasks were not learnable end-to-end (e.g., addition with\nRoman numerals), but were easily learnable once broken into two separate\nsub-tasks: a perceptual \\textit{Character Recognition} and cognitive\n\\textit{Arithmetic} sub-tasks. This indicates that while some tasks may be\neasily learnable end-to-end, other may need to be broken into sub-tasks.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2015 13:44:15 GMT"}, {"version": "v2", "created": "Fri, 27 Nov 2015 12:18:48 GMT"}], "update_date": "2017-01-06", "authors_parsed": [["Hoshen", "Yedid", ""], ["Peleg", "Shmuel", ""]]}, {"id": "1506.02312", "submitter": "Renato Pereira", "authors": "Renato de Pontes Pereira and Paulo Martins Engel", "title": "A Framework for Constrained and Adaptive Behavior-Based Agents", "comments": "2015; 15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.RO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Behavior Trees are commonly used to model agents for robotics and games,\nwhere constrained behaviors must be designed by human experts in order to\nguarantee that these agents will execute a specific chain of actions given a\nspecific set of perceptions. In such application areas, learning is a desirable\nfeature to provide agents with the ability to adapt and improve interactions\nwith humans and environment, but often discarded due to its unreliability. In\nthis paper, we propose a framework that uses Reinforcement Learning nodes as\npart of Behavior Trees to address the problem of adding learning capabilities\nin constrained agents. We show how this framework relates to Options in\nHierarchical Reinforcement Learning, ensuring convergence of nested learning\nnodes, and we empirically show that the learning nodes do not affect the\nexecution of other nodes in the tree.\n", "versions": [{"version": "v1", "created": "Sun, 7 Jun 2015 20:52:31 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Pereira", "Renato de Pontes", ""], ["Engel", "Paulo Martins", ""]]}, {"id": "1506.02442", "submitter": "Irena Rusu Ph.D.", "authors": "Irena Rusu", "title": "NP-hardness of sortedness constraints", "comments": "15 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.AI cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Constraint Programming, global constraints allow to model and solve many\ncombinatorial problems. Among these constraints, several sortedness constraints\nhave been defined, for which propagation algorithms are available, but for\nwhich the tractability is not settled. We show that the sort(U,V) constraint\n(Older et. al, 1995) is intractable for integer variables whose domains are not\nlimited to intervals. As a consequence, the similar result holds for the\nsort(U,V, P) constraint (Zhou, 1996). Moreover, the intractability holds even\nunder the stability condition present in the recently introduced\nkeysorting(U,V,Keys,P) constraint (Carlsson et al., 2014), and requiring that\nthe order of the variables with the same value in the list U be preserved in\nthe list V. Therefore, keysorting(U,V,Keys,P) is intractable as well.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 11:24:06 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Rusu", "Irena", ""]]}, {"id": "1506.02465", "submitter": "Marius Lindauer", "authors": "Bernd Bischl, Pascal Kerschke, Lars Kotthoff, Marius Lindauer, Yuri\n  Malitsky, Alexandre Frechette, Holger Hoos, Frank Hutter, Kevin Leyton-Brown,\n  Kevin Tierney, Joaquin Vanschoren", "title": "ASlib: A Benchmark Library for Algorithm Selection", "comments": "Accepted to be published in Artificial Intelligence Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of algorithm selection involves choosing an algorithm from a set of\nalgorithms on a per-instance basis in order to exploit the varying performance\nof algorithms over a set of instances. The algorithm selection problem is\nattracting increasing attention from researchers and practitioners in AI. Years\nof fruitful applications in a number of domains have resulted in a large amount\nof data, but the community lacks a standard format or repository for this data.\nThis situation makes it difficult to share and compare different approaches\neffectively, as is done in other, more established fields. It also\nunnecessarily hinders new researchers who want to work in this area. To address\nthis problem, we introduce a standardized format for representing algorithm\nselection scenarios and a repository that contains a growing number of data\nsets from the literature. Our format has been designed to be able to express a\nwide variety of different scenarios. Demonstrating the breadth and power of our\nplatform, we describe a set of example experiments that build and evaluate\nalgorithm selection models through a common interface. The results display the\npotential of algorithm selection to achieve significant performance\nimprovements across a broad range of problems and algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 12:35:04 GMT"}, {"version": "v2", "created": "Wed, 24 Feb 2016 14:38:52 GMT"}, {"version": "v3", "created": "Wed, 6 Apr 2016 13:20:23 GMT"}], "update_date": "2016-04-07", "authors_parsed": [["Bischl", "Bernd", ""], ["Kerschke", "Pascal", ""], ["Kotthoff", "Lars", ""], ["Lindauer", "Marius", ""], ["Malitsky", "Yuri", ""], ["Frechette", "Alexandre", ""], ["Hoos", "Holger", ""], ["Hutter", "Frank", ""], ["Leyton-Brown", "Kevin", ""], ["Tierney", "Kevin", ""], ["Vanschoren", "Joaquin", ""]]}, {"id": "1506.02561", "submitter": "Lakhdar Sais", "authors": "Said Jabbour and Lakhdar Sais and Yakoub Salhi", "title": "On SAT Models Enumeration in Itemset Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Frequent itemset mining is an essential part of data analysis and data\nmining. Recent works propose interesting SAT-based encodings for the problem of\ndiscovering frequent itemsets. Our aim in this work is to define strategies for\nadapting SAT solvers to such encodings in order to improve models enumeration.\nIn this context, we deeply study the effects of restart, branching heuristics\nand clauses learning. We then conduct an experimental evaluation on SAT-Based\nitemset mining instances to show how SAT solvers can be adapted to obtain an\nefficient SAT model enumerator.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 15:50:57 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Jabbour", "Said", ""], ["Sais", "Lakhdar", ""], ["Salhi", "Yakoub", ""]]}, {"id": "1506.02639", "submitter": "Paul Beame", "authors": "Paul Beame and Vincent Liew", "title": "New Limits for Knowledge Compilation and Applications to Exact Model\n  Counting", "comments": "Full version of paper appearing UAI 2015 updated to include new\n  references to related work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show new limits on the efficiency of using current techniques to make\nexact probabilistic inference for large classes of natural problems. In\nparticular we show new lower bounds on knowledge compilation to SDD and DNNF\nforms. We give strong lower bounds on the complexity of SDD representations by\nrelating SDD size to best-partition communication complexity. We use this\nrelationship to prove exponential lower bounds on the SDD size for representing\na large class of problems that occur naturally as queries over probabilistic\ndatabases. A consequence is that for representing unions of conjunctive\nqueries, SDDs are not qualitatively more concise than OBDDs. We also derive\nsimple examples for which SDDs must be exponentially less concise than FBDDs.\nFinally, we derive exponential lower bounds on the sizes of DNNF\nrepresentations using a new quasipolynomial simulation of DNNFs by\nnondeterministic FBDDs.\n", "versions": [{"version": "v1", "created": "Mon, 8 Jun 2015 19:52:43 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2015 19:13:38 GMT"}], "update_date": "2015-08-20", "authors_parsed": [["Beame", "Paul", ""], ["Liew", "Vincent", ""]]}, {"id": "1506.02750", "submitter": "C\\'esar A. Astudillo", "authors": "C\\'esar A. Astudillo and B. John Oommen", "title": "Self Organizing Maps Whose Topologies Can Be Learned With Adaptive\n  Binary Search Trees Using Conditional Rotations", "comments": null, "journal-ref": "C\\'esar A. Astudillo and B. John Oommen. Self Organizing Maps\n  Whose Topologies Can Be Learned With Adaptive Binary Search Trees Using\n  Conditional Rotations. Pattern Recognition, 47(1), 2014", "doi": "10.1016/j.patcog.2013.04.012", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous variants of Self-Organizing Maps (SOMs) have been proposed in the\nliterature, including those which also possess an underlying structure, and in\nsome cases, this structure itself can be defined by the user Although the\nconcepts of growing the SOM and updating it have been studied, the whole issue\nof using a self-organizing Adaptive Data Structure (ADS) to further enhance the\nproperties of the underlying SOM, has been unexplored. In an earlier work, we\nimpose an arbitrary, user-defined, tree-like topology onto the codebooks, which\nconsequently enforced a neighborhood phenomenon and the so-called tree-based\nBubble of Activity. In this paper, we consider how the underlying tree itself\ncan be rendered dynamic and adaptively transformed. To do this, we present\nmethods by which a SOM with an underlying Binary Search Tree (BST) structure\ncan be adaptively re-structured using Conditional Rotations (CONROT). These\nrotations on the nodes of the tree are local, can be done in constant time, and\nperformed so as to decrease the Weighted Path Length (WPL) of the entire tree.\nIn doing this, we introduce the pioneering concept referred to as Neural\nPromotion, where neurons gain prominence in the Neural Network (NN) as their\nsignificance increases. We are not aware of any research which deals with the\nissue of Neural Promotion. The advantages of such a scheme is that the user\nneed not be aware of any of the topological peculiarities of the stochastic\ndata distribution. Rather, the algorithm, referred to as the TTOSOM with\nConditional Rotations (TTOCONROT), converges in such a manner that the neurons\nare ultimately placed in the input space so as to represent its stochastic\ndistribution, and additionally, the neighborhood properties of the neurons suit\nthe best BST that represents the data. These properties have been confirmed by\nour experimental results on a variety of data sets.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 02:29:57 GMT"}], "update_date": "2015-06-10", "authors_parsed": [["Astudillo", "C\u00e9sar A.", ""], ["Oommen", "B. John", ""]]}, {"id": "1506.02850", "submitter": "Giuseppe De Nittis", "authors": "Nicola Basilico, Giuseppe De Nittis, Nicola Gatti", "title": "Adversarial patrolling with spatially uncertain alarm signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When securing complex infrastructures or large environments, constant\nsurveillance of every area is not affordable. To cope with this issue, a common\ncountermeasure is the usage of cheap but wide-ranged sensors, able to detect\nsuspicious events that occur in large areas, supporting patrollers to improve\nthe effectiveness of their strategies. However, such sensors are commonly\naffected by uncertainty. In the present paper, we focus on spatially uncertain\nalarm signals. That is, the alarm system is able to detect an attack but it is\nuncertain on the exact position where the attack is taking place. This is\ncommon when the area to be secured is wide such as in border patrolling and\nfair site surveillance. We propose, to the best of our knowledge, the first\nPatrolling Security Game model where a Defender is supported by a spatially\nuncertain alarm system which non-deterministically generates signals once a\ntarget is under attack. We show that finding the optimal strategy in arbitrary\ngraphs is APX-hard even in zero-sum games and we provide two (exponential time)\nexact algorithms and two (polynomial time) approximation algorithms.\nFurthermore, we analyse what happens in environments with special topologies,\nshowing that in linear and cycle graphs the optimal patrolling strategy can be\nfound in polynomial time, de facto allowing our algorithms to be used in\nreal-life scenarios, while in trees the problem is NP-hard. Finally, we show\nthat without false positives and missed detections, the best patrolling\nstrategy reduces to stay in a place, wait for a signal, and respond to it at\nbest. This strategy is optimal even with non-negligible missed detection rates,\nwhich, unfortunately, affect every commercial alarm system. We evaluate our\nmethods in simulation, assessing both quantitative and qualitative aspects.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 10:22:43 GMT"}], "update_date": "2015-06-10", "authors_parsed": [["Basilico", "Nicola", ""], ["De Nittis", "Giuseppe", ""], ["Gatti", "Nicola", ""]]}, {"id": "1506.02922", "submitter": "Dimitra Gkatzia", "authors": "Dimitra Gkatzia and Helen Hastie", "title": "An Ensemble method for Content Selection for Data-to-text Systems", "comments": "3 pages, 2 figures, 1st International Workshop on Data-to-text\n  Generation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel approach for automatic report generation from time-series\ndata, in the context of student feedback generation. Our proposed methodology\ntreats content selection as a multi-label classification (MLC) problem, which\ntakes as input time-series data (students' learning data) and outputs a summary\nof these data (feedback). Unlike previous work, this method considers all data\nsimultaneously using ensembles of classifiers, and therefore, it achieves\nhigher accuracy and F- score compared to meaningful baselines.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 14:17:06 GMT"}], "update_date": "2015-06-10", "authors_parsed": [["Gkatzia", "Dimitra", ""], ["Hastie", "Helen", ""]]}, {"id": "1506.02930", "submitter": "Frantisek Duris", "authors": "Frantisek Duris", "title": "Arguments for the Effectiveness of Human Problem Solving", "comments": null, "journal-ref": null, "doi": "10.1016/j.bica.2018.04.007", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The question of how humans solve problem has been addressed extensively.\nHowever, the direct study of the effectiveness of this process seems to be\noverlooked. In this paper, we address the issue of the effectiveness of human\nproblem solving: we analyze where this effectiveness comes from and what\ncognitive mechanisms or heuristics are involved. Our results are based on the\noptimal probabilistic problem solving strategy that appeared in Solomonoff\npaper on general problem solving system. We provide arguments that a certain\nset of cognitive mechanisms or heuristics drive human problem solving in the\nsimilar manner as the optimal Solomonoff strategy. The results presented in\nthis paper can serve both cognitive psychology in better understanding of human\nproblem solving processes as well as artificial intelligence in designing more\nhuman-like agents.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 14:28:12 GMT"}, {"version": "v2", "created": "Tue, 19 Sep 2017 13:31:27 GMT"}], "update_date": "2018-04-26", "authors_parsed": [["Duris", "Frantisek", ""]]}, {"id": "1506.03041", "submitter": "Diana Borsa", "authors": "Diana Borsa, Thore Graepel, Andrew Gordon", "title": "The Wreath Process: A totally generative model of geometric shape based\n  on nested symmetries", "comments": "10 pages(double-column), 60+ figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of modelling noisy but highly symmetric shapes that\ncan be viewed as hierarchies of whole-part relationships in which higher level\nobjects are composed of transformed collections of lower level objects. To this\nend, we propose the stochastic wreath process, a fully generative probabilistic\nmodel of drawings. Following Leyton's \"Generative Theory of Shape\", we\nrepresent shapes as sequences of transformation groups composed through a\nwreath product.\n  This representation emphasizes the maximization of transfer --- the idea that\nthe most compact and meaningful representation of a given shape is achieved by\nmaximizing the re-use of existing building blocks or parts.\n  The proposed stochastic wreath process extends Leyton's theory by defining a\nprobability distribution over geometric shapes in terms of noise processes that\nare aligned with the generative group structure of the shape. We propose an\ninference scheme for recovering the generative history of given images in terms\nof the wreath process using reversible jump Markov chain Monte Carlo methods\nand Approximate Bayesian Computation. In the context of sketching we\ndemonstrate the feasibility and limitations of this approach on model-generated\nand real data.\n", "versions": [{"version": "v1", "created": "Tue, 9 Jun 2015 18:56:43 GMT"}], "update_date": "2015-06-10", "authors_parsed": [["Borsa", "Diana", ""], ["Graepel", "Thore", ""], ["Gordon", "Andrew", ""]]}, {"id": "1506.03140", "submitter": "Keenon Werling", "authors": "Keenon Werling, Arun Chaganty, Percy Liang, Chris Manning", "title": "On-the-Job Learning with Bayesian Decision Theory", "comments": "As appearing in NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal is to deploy a high-accuracy system starting with zero training\nexamples. We consider an \"on-the-job\" setting, where as inputs arrive, we use\nreal-time crowdsourcing to resolve uncertainty where needed and output our\nprediction when confident. As the model improves over time, the reliance on\ncrowdsourcing queries decreases. We cast our setting as a stochastic game based\non Bayesian decision theory, which allows us to balance latency, cost, and\naccuracy objectives in a principled way. Computing the optimal policy is\nintractable, so we develop an approximation based on Monte Carlo Tree Search.\nWe tested our approach on three datasets---named-entity recognition, sentiment\nclassification, and image classification. On the NER task we obtained more than\nan order of magnitude reduction in cost compared to full human annotation,\nwhile boosting performance relative to the expert provided labels. We also\nachieve a 8% F1 improvement over having a single human label the whole set, and\na 28% F1 improvement over online learning.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 00:40:34 GMT"}, {"version": "v2", "created": "Mon, 7 Dec 2015 21:44:07 GMT"}], "update_date": "2015-12-09", "authors_parsed": [["Werling", "Keenon", ""], ["Chaganty", "Arun", ""], ["Liang", "Percy", ""], ["Manning", "Chris", ""]]}, {"id": "1506.03340", "submitter": "Karl Moritz Hermann", "authors": "Karl Moritz Hermann, Tom\\'a\\v{s} Ko\\v{c}isk\\'y, Edward Grefenstette,\n  Lasse Espeholt, Will Kay, Mustafa Suleyman and Phil Blunsom", "title": "Teaching Machines to Read and Comprehend", "comments": "Appears in: Advances in Neural Information Processing Systems 28\n  (NIPS 2015). 14 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Teaching machines to read natural language documents remains an elusive\nchallenge. Machine reading systems can be tested on their ability to answer\nquestions posed on the contents of documents that they have seen, but until now\nlarge scale training and test datasets have been missing for this type of\nevaluation. In this work we define a new methodology that resolves this\nbottleneck and provides large scale supervised reading comprehension data. This\nallows us to develop a class of attention based deep neural networks that learn\nto read real documents and answer complex questions with minimal prior\nknowledge of language structure.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 14:54:39 GMT"}, {"version": "v2", "created": "Thu, 1 Oct 2015 15:04:49 GMT"}, {"version": "v3", "created": "Thu, 19 Nov 2015 15:43:23 GMT"}], "update_date": "2015-11-20", "authors_parsed": [["Hermann", "Karl Moritz", ""], ["Ko\u010disk\u00fd", "Tom\u00e1\u0161", ""], ["Grefenstette", "Edward", ""], ["Espeholt", "Lasse", ""], ["Kay", "Will", ""], ["Suleyman", "Mustafa", ""], ["Blunsom", "Phil", ""]]}, {"id": "1506.03374", "submitter": "Shipra Agrawal", "authors": "Shipra Agrawal and Nikhil R. Devanur and Lihong Li", "title": "An efficient algorithm for contextual bandits with knapsacks, and an\n  extension to concave objectives", "comments": "Extended abstract appeared in COLT 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a contextual version of multi-armed bandit problem with global\nknapsack constraints. In each round, the outcome of pulling an arm is a scalar\nreward and a resource consumption vector, both dependent on the context, and\nthe global knapsack constraints require the total consumption for each resource\nto be below some pre-fixed budget. The learning agent competes with an\narbitrary set of context-dependent policies. This problem was introduced by\nBadanidiyuru et al. (2014), who gave a computationally inefficient algorithm\nwith near-optimal regret bounds for it. We give a computationally efficient\nalgorithm for this problem with slightly better regret bounds, by generalizing\nthe approach of Agarwal et al. (2014) for the non-constrained version of the\nproblem. The computational time of our algorithm scales logarithmically in the\nsize of the policy space. This answers the main open question of Badanidiyuru\net al. (2014). We also extend our results to a variant where there are no\nknapsack constraints but the objective is an arbitrary Lipschitz concave\nfunction of the sum of outcome vectors.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 16:14:19 GMT"}, {"version": "v2", "created": "Sat, 9 Jul 2016 05:46:06 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Agrawal", "Shipra", ""], ["Devanur", "Nikhil R.", ""], ["Li", "Lihong", ""]]}, {"id": "1506.03378", "submitter": "Lihong Li", "authors": "Che-Yu Liu and Lihong Li", "title": "On the Prior Sensitivity of Thompson Sampling", "comments": "Appears in the 27th International Conference on Algorithmic Learning\n  Theory (ALT), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The empirically successful Thompson Sampling algorithm for stochastic bandits\nhas drawn much interest in understanding its theoretical properties. One\nimportant benefit of the algorithm is that it allows domain knowledge to be\nconveniently encoded as a prior distribution to balance exploration and\nexploitation more effectively. While it is generally believed that the\nalgorithm's regret is low (high) when the prior is good (bad), little is known\nabout the exact dependence. In this paper, we fully characterize the\nalgorithm's worst-case dependence of regret on the choice of prior, focusing on\na special yet representative case. These results also provide insights into the\ngeneral sensitivity of the algorithm to the choice of priors. In particular,\nwith $p$ being the prior probability mass of the true reward-generating model,\nwe prove $O(\\sqrt{T/p})$ and $O(\\sqrt{(1-p)T})$ regret upper bounds for the\nbad- and good-prior cases, respectively, as well as \\emph{matching} lower\nbounds. Our proofs rely on the discovery of a fundamental property of Thompson\nSampling and make heavy use of martingale theory, both of which appear novel in\nthe literature, to the best of our knowledge.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 16:22:26 GMT"}, {"version": "v2", "created": "Thu, 21 Jul 2016 01:43:09 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Liu", "Che-Yu", ""], ["Li", "Lihong", ""]]}, {"id": "1506.03379", "submitter": "Lihong Li", "authors": "Emma Brunskill and Lihong Li", "title": "The Online Coupon-Collector Problem and Its Application to Lifelong\n  Reinforcement Learning", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transferring knowledge across a sequence of related tasks is an important\nchallenge in reinforcement learning (RL). Despite much encouraging empirical\nevidence, there has been little theoretical analysis. In this paper, we study a\nclass of lifelong RL problems: the agent solves a sequence of tasks modeled as\nfinite Markov decision processes (MDPs), each of which is from a finite set of\nMDPs with the same state/action sets and different transition/reward functions.\nMotivated by the need for cross-task exploration in lifelong learning, we\nformulate a novel online coupon-collector problem and give an optimal\nalgorithm. This allows us to develop a new lifelong RL algorithm, whose overall\nsample complexity in a sequence of tasks is much smaller than single-task\nlearning, even if the sequence of tasks is generated by an adversary. Benefits\nof the algorithm are demonstrated in simulated problems, including a recently\nintroduced human-robot interaction problem.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 16:23:29 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2015 22:55:59 GMT"}], "update_date": "2015-09-23", "authors_parsed": [["Brunskill", "Emma", ""], ["Li", "Lihong", ""]]}, {"id": "1506.03425", "submitter": "Krzysztof Choromanski", "authors": "Krzysztof Choromanski and Sanjiv Kumar and Xiaofeng Liu", "title": "Fast Online Clustering with Randomized Skeleton Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new fast online clustering algorithm that reliably recovers\narbitrary-shaped data clusters in high throughout data streams. Unlike the\nexisting state-of-the-art online clustering methods based on k-means or\nk-medoid, it does not make any restrictive generative assumptions. In addition,\nin contrast to existing nonparametric clustering techniques such as DBScan or\nDenStream, it gives provable theoretical guarantees. To achieve fast\nclustering, we propose to represent each cluster by a skeleton set which is\nupdated continuously as new data is seen. A skeleton set consists of weighted\nsamples from the data where weights encode local densities. The size of each\nskeleton set is adapted according to the cluster geometry. The proposed\ntechnique automatically detects the number of clusters and is robust to\noutliers. The algorithm works for the infinite data stream where more than one\npass over the data is not feasible. We provide theoretical guarantees on the\nquality of the clustering and also demonstrate its advantage over the existing\nstate-of-the-art on several datasets.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 18:41:55 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Choromanski", "Krzysztof", ""], ["Kumar", "Sanjiv", ""], ["Liu", "Xiaofeng", ""]]}, {"id": "1506.03493", "submitter": "Aaron Schein", "authors": "Aaron Schein, John Paisley, David M. Blei, Hanna Wallach", "title": "Bayesian Poisson Tensor Factorization for Inferring Multilateral\n  Relations from Sparse Dyadic Event Counts", "comments": "To appear in Proceedings of the 21st ACM SIGKDD Conference of\n  Knowledge Discovery and Data Mining (KDD 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.SI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a Bayesian tensor factorization model for inferring latent group\nstructures from dynamic pairwise interaction patterns. For decades, political\nscientists have collected and analyzed records of the form \"country $i$ took\naction $a$ toward country $j$ at time $t$\"---known as dyadic events---in order\nto form and test theories of international relations. We represent these event\ndata as a tensor of counts and develop Bayesian Poisson tensor factorization to\ninfer a low-dimensional, interpretable representation of their salient\npatterns. We demonstrate that our model's predictive performance is better than\nthat of standard non-negative tensor factorization methods. We also provide a\ncomparison of our variational updates to their maximum likelihood counterparts.\nIn doing so, we identify a better way to form point estimates of the latent\nfactors than that typically used in Bayesian Poisson matrix factorization.\nFinally, we showcase our model as an exploratory analysis tool for political\nscientists. We show that the inferred latent factor matrices capture\ninterpretable multilateral relations that both conform to and inform our\nknowledge of international affairs.\n", "versions": [{"version": "v1", "created": "Wed, 10 Jun 2015 21:49:31 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Schein", "Aaron", ""], ["Paisley", "John", ""], ["Blei", "David M.", ""], ["Wallach", "Hanna", ""]]}, {"id": "1506.03624", "submitter": "Daniel J Mankowitz", "authors": "Daniel J. Mankowitz, Timothy A. Mann, Shie Mannor", "title": "Bootstrapping Skills", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The monolithic approach to policy representation in Markov Decision Processes\n(MDPs) looks for a single policy that can be represented as a function from\nstates to actions. For the monolithic approach to succeed (and this is not\nalways possible), a complex feature representation is often necessary since the\npolicy is a complex object that has to prescribe what actions to take all over\nthe state space. This is especially true in large domains with complicated\ndynamics. It is also computationally inefficient to both learn and plan in MDPs\nusing a complex monolithic approach. We present a different approach where we\nrestrict the policy space to policies that can be represented as combinations\nof simpler, parameterized skills---a type of temporally extended action, with a\nsimple policy representation. We introduce Learning Skills via Bootstrapping\n(LSB) that can use a broad family of Reinforcement Learning (RL) algorithms as\na \"black box\" to iteratively learn parametrized skills. Initially, the learned\nskills are short-sighted but each iteration of the algorithm allows the skills\nto bootstrap off one another, improving each skill in the process. We prove\nthat this bootstrapping process returns a near-optimal policy. Furthermore, our\nexperiments demonstrate that LSB can solve MDPs that, given the same\nrepresentational power, could not be solved by a monolithic approach. Thus,\nplanning with learned skills results in better policies without requiring\ncomplex policy representations.\n", "versions": [{"version": "v1", "created": "Thu, 11 Jun 2015 11:06:40 GMT"}], "update_date": "2015-06-12", "authors_parsed": [["Mankowitz", "Daniel J.", ""], ["Mann", "Timothy A.", ""], ["Mannor", "Shie", ""]]}, {"id": "1506.03879", "submitter": "Ji Xu", "authors": "Ji Xu, Guoyin Wang", "title": "Leading Tree in DPCLUS and Its Impact on Building Hierarchies", "comments": "11 Pages, 5 figures. It is a very fundamental topic with respect to\n  the research (clustering by fast search and find of density peaks)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reveals the tree structure as an intermediate result of clustering\nby fast search and find of density peaks (DPCLUS), and explores the power of\nusing this tree to perform hierarchical clustering. The array used to hold the\nindex of the nearest higher-densitied object for each object can be transformed\ninto a Leading Tree (LT), in which each parent node P leads its child nodes to\njoin the same cluster as P itself, and the child nodes are sorted by their\ngamma values in descendant order to accelerate the disconnecting of root in\neach subtree. There are two major advantages with the LT: One is dramatically\nreducing the running time of assigning noncenter data points to their cluster\nID, because the assigning process is turned into just disconnecting the links\nfrom each center to its parent. The other is that the tree model for\nrepresenting clusters is more informative. Because we can check which objects\nare more likely to be selected as centers in finer grained clustering, or which\nobjects reach to its center via less jumps. Experiment results and analysis\nshow the effectiveness and efficiency of the assigning process with an LT.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 00:37:54 GMT"}, {"version": "v2", "created": "Mon, 15 Jun 2015 00:38:53 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Xu", "Ji", ""], ["Wang", "Guoyin", ""]]}, {"id": "1506.03949", "submitter": "Jos Uiterwijk", "authors": "Jos Uiterwijk and Michael Barton", "title": "New Results for Domineering from Combinatorial Game Theory Endgame\n  Databases", "comments": null, "journal-ref": null, "doi": "10.1016/j.tcs.2015.05.017", "report-no": null, "categories": "cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have constructed endgame databases for all single-component positions up\nto 15 squares for Domineering, filled with exact Combinatorial Game Theory\n(CGT) values in canonical form. The most important findings are as follows.\n  First, as an extension of Conway's [8] famous Bridge Splitting Theorem for\nDomineering, we state and prove another theorem, dubbed the Bridge Destroying\nTheorem for Domineering. Together these two theorems prove very powerful in\ndetermining the CGT values of large positions as the sum of the values of\nsmaller fragments, but also to compose larger positions with specified values\nfrom smaller fragments. Using the theorems, we then prove that for any dyadic\nrational number there exist Domineering positions with that value.\n  Second, we investigate Domineering positions with infinitesimal CGT values,\nin particular ups and downs, tinies and minies, and nimbers. In the databases\nwe find many positions with single or double up and down values, but no ups and\ndowns with higher multitudes. However, we prove that such single-component ups\nand downs easily can be constructed. Further, we find Domineering positions\nwith 11 different tinies and minies values. For each we give an example. Next,\nfor nimbers we find many Domineering positions with values up to *3. This is\nsurprising, since Drummond-Cole [10] suspected that no *2 and *3 positions in\nstandard Domineering would exist. We show and characterize many *2 and *3\npositions. Finally, we give some Domineering positions with values interesting\nfor other reasons.\n  Third, we have investigated the temperature of all positions in our\ndatabases. There appears to be exactly one position with temperature 2 (as\nalready found before) and no positions with temperature larger than 2. This\nsupports Berlekamp's conjecture that 2 is the highest possible temperature in\nDomineering.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 09:20:42 GMT"}], "update_date": "2015-06-15", "authors_parsed": [["Uiterwijk", "Jos", ""], ["Barton", "Michael", ""]]}, {"id": "1506.04089", "submitter": "Hongyuan Mei", "authors": "Hongyuan Mei, Mohit Bansal, Matthew R. Walter", "title": "Listen, Attend, and Walk: Neural Mapping of Navigational Instructions to\n  Action Sequences", "comments": "To appear at AAAI 2016 (and an extended version of a NIPS 2015\n  Multimodal Machine Learning workshop paper)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a neural sequence-to-sequence model for direction following, a\ntask that is essential to realizing effective autonomous agents. Our\nalignment-based encoder-decoder model with long short-term memory recurrent\nneural networks (LSTM-RNN) translates natural language instructions to action\nsequences based upon a representation of the observable world state. We\nintroduce a multi-level aligner that empowers our model to focus on sentence\n\"regions\" salient to the current world state by using multiple abstractions of\nthe input sentence. In contrast to existing methods, our model uses no\nspecialized linguistic resources (e.g., parsers) or task-specific annotations\n(e.g., seed lexicons). It is therefore generalizable, yet still achieves the\nbest results reported to-date on a benchmark single-sentence dataset and\ncompetitive results for the limited-training multi-sentence setting. We analyze\nour model through a series of ablations that elucidate the contributions of the\nprimary components of our model.\n", "versions": [{"version": "v1", "created": "Fri, 12 Jun 2015 18:05:00 GMT"}, {"version": "v2", "created": "Thu, 2 Jul 2015 19:22:33 GMT"}, {"version": "v3", "created": "Wed, 2 Dec 2015 20:46:09 GMT"}, {"version": "v4", "created": "Thu, 17 Dec 2015 17:57:42 GMT"}], "update_date": "2015-12-18", "authors_parsed": [["Mei", "Hongyuan", ""], ["Bansal", "Mohit", ""], ["Walter", "Matthew R.", ""]]}, {"id": "1506.04272", "submitter": "Fuan Pu", "authors": "Fuan Pu, Jian Luo, Yulai Zhang, and Guiming Luo", "title": "Attacker and Defender Counting Approach for Abstract Argumentation", "comments": "7 pages, 2 figures;conference CogSci 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Dung's abstract argumentation, arguments are either acceptable or\nunacceptable, given a chosen notion of acceptability. This gives a coarse way\nto compare arguments. In this paper, we propose a counting approach for a more\nfine-gained assessment to arguments by counting the number of their respective\nattackers and defenders based on argument graph and argument game. An argument\nis more acceptable if the proponent puts forward more number of defenders for\nit and the opponent puts forward less number of attackers against it. We show\nthat our counting model has two well-behaved properties: normalization and\nconvergence. Then, we define a counting semantics based on this model, and\ninvestigate some general properties of the semantics.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2015 14:24:51 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Pu", "Fuan", ""], ["Luo", "Jian", ""], ["Zhang", "Yulai", ""], ["Luo", "Guiming", ""]]}, {"id": "1506.04299", "submitter": "Leopoldo Bertossi", "authors": "Babak Salimi and Leopoldo Bertossi", "title": "Query-Answer Causality in Databases: Abductive Diagnosis and\n  View-Updates", "comments": "To appear in Proc. UAI Causal Inference Workshop, 2015. One example\n  was fixed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causality has been recently introduced in databases, to model, characterize\nand possibly compute causes for query results (answers). Connections between\nquery causality and consistency-based diagnosis and database repairs (wrt.\nintegrity constrain violations) have been established in the literature. In\nthis work we establish connections between query causality and abductive\ndiagnosis and the view-update problem. The unveiled relationships allow us to\nobtain new complexity results for query causality -the main focus of our work-\nand also for the two other areas.\n", "versions": [{"version": "v1", "created": "Sat, 13 Jun 2015 17:33:47 GMT"}, {"version": "v2", "created": "Tue, 7 Jul 2015 18:55:13 GMT"}, {"version": "v3", "created": "Mon, 13 Jul 2015 09:15:22 GMT"}, {"version": "v4", "created": "Tue, 18 Aug 2015 23:05:44 GMT"}, {"version": "v5", "created": "Sun, 20 Sep 2015 03:41:29 GMT"}], "update_date": "2015-09-22", "authors_parsed": [["Salimi", "Babak", ""], ["Bertossi", "Leopoldo", ""]]}, {"id": "1506.04349", "submitter": "Santiago Hern\\'andez Orozco", "authors": "Santiago Hern\\'andez-Orozco, Francisco Hern\\'andez-Quiroz, Hector\n  Zenil and Wilfried Sieg", "title": "Rare Speed-up in Automatic Theorem Proving Reveals Tradeoff Between\n  Computational Time and Information Value", "comments": "14 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that strategies implemented in automatic theorem proving involve an\ninteresting tradeoff between execution speed, proving speedup/computational\ntime and usefulness of information. We advance formal definitions for these\nconcepts by way of a notion of normality related to an expected (optimal)\ntheoretical speedup when adding useful information (other theorems as axioms),\nas compared with actual strategies that can be effectively and efficiently\nimplemented. We propose the existence of an ineluctable tradeoff between this\nnormality and computational time complexity. The argument quantifies the\nusefulness of information in terms of (positive) speed-up. The results disclose\na kind of no-free-lunch scenario and a tradeoff of a fundamental nature. The\nmain theorem in this paper together with the numerical experiment---undertaken\nusing two different automatic theorem provers AProS and Prover9 on random\ntheorems of propositional logic---provide strong theoretical and empirical\narguments for the fact that finding new useful information for solving a\nspecific problem (theorem) is, in general, as hard as the problem (theorem)\nitself.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2015 04:17:45 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Hern\u00e1ndez-Orozco", "Santiago", ""], ["Hern\u00e1ndez-Quiroz", "Francisco", ""], ["Zenil", "Hector", ""], ["Sieg", "Wilfried", ""]]}, {"id": "1506.04365", "submitter": "Kuan-Yu Chen", "authors": "Kuan-Yu Chen, Shih-Hung Liu, Hsin-Min Wang, Berlin Chen, Hsin-Hsi Chen", "title": "Leveraging Word Embeddings for Spoken Document Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Owing to the rapidly growing multimedia content available on the Internet,\nextractive spoken document summarization, with the purpose of automatically\nselecting a set of representative sentences from a spoken document to concisely\nexpress the most important theme of the document, has been an active area of\nresearch and experimentation. On the other hand, word embedding has emerged as\na newly favorite research subject because of its excellent performance in many\nnatural language processing (NLP)-related tasks. However, as far as we are\naware, there are relatively few studies investigating its use in extractive\ntext or speech summarization. A common thread of leveraging word embeddings in\nthe summarization process is to represent the document (or sentence) by\naveraging the word embeddings of the words occurring in the document (or\nsentence). Then, intuitively, the cosine similarity measure can be employed to\ndetermine the relevance degree between a pair of representations. Beyond the\ncontinued efforts made to improve the representation of words, this paper\nfocuses on building novel and efficient ranking models based on the general\nword embedding methods for extractive speech summarization. Experimental\nresults demonstrate the effectiveness of our proposed methods, compared to\nexisting state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2015 09:18:36 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Chen", "Kuan-Yu", ""], ["Liu", "Shih-Hung", ""], ["Wang", "Hsin-Min", ""], ["Chen", "Berlin", ""], ["Chen", "Hsin-Hsi", ""]]}, {"id": "1506.04366", "submitter": "Arthur Franz", "authors": "Arthur Franz", "title": "Artificial general intelligence through recursive data compression and\n  grounded reasoning: a position paper", "comments": "27 pages, 3 figures, position paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a tentative outline for the construction of an\nartificial, generally intelligent system (AGI). It is argued that building a\ngeneral data compression algorithm solving all problems up to a complexity\nthreshold should be the main thrust of research. A measure for partial progress\nin AGI is suggested. Although the details are far from being clear, some\ngeneral properties for a general compression algorithm are fleshed out. Its\ninductive bias should be flexible and adapt to the input data while constantly\nsearching for a simple, orthogonal and complete set of hypotheses explaining\nthe data. It should recursively reduce the size of its representations thereby\ncompressing the data increasingly at every iteration.\n  Abstract Based on that fundamental ability, a grounded reasoning system is\nproposed. It is argued how grounding and flexible feature bases made of\nhypotheses allow for resourceful thinking. While the simulation of\nrepresentation contents on the mental stage accounts for much of the power of\npropositional logic, compression leads to simple sets of hypotheses that allow\nthe detection and verification of universally quantified statements.\n  Abstract Together, it is highlighted how general compression and grounded\nreasoning could account for the birth and growth of first concepts about the\nworld and the commonsense reasoning about them.\n", "versions": [{"version": "v1", "created": "Sun, 14 Jun 2015 09:29:11 GMT"}], "update_date": "2015-06-16", "authors_parsed": [["Franz", "Arthur", ""]]}, {"id": "1506.04744", "submitter": "Cristian Danescu-Niculescu-Mizil", "authors": "Vlad Niculae, Srijan Kumar, Jordan Boyd-Graber, Cristian\n  Danescu-Niculescu-Mizil", "title": "Linguistic Harbingers of Betrayal: A Case Study on an Online Strategy\n  Game", "comments": "To appear at ACL 2015. 10pp, 4 fig. Data and other info available at\n  http://vene.ro/betrayal/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.SI physics.soc-ph stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpersonal relations are fickle, with close friendships often dissolving\ninto enmity. In this work, we explore linguistic cues that presage such\ntransitions by studying dyadic interactions in an online strategy game where\nplayers form alliances and break those alliances through betrayal. We\ncharacterize friendships that are unlikely to last and examine temporal\npatterns that foretell betrayal.\n  We reveal that subtle signs of imminent betrayal are encoded in the\nconversational patterns of the dyad, even if the victim is not aware of the\nrelationship's fate. In particular, we find that lasting friendships exhibit a\nform of balance that manifests itself through language. In contrast, sudden\nchanges in the balance of certain conversational attributes---such as positive\nsentiment, politeness, or focus on future planning---signal impending betrayal.\n", "versions": [{"version": "v1", "created": "Mon, 15 Jun 2015 20:00:29 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Niculae", "Vlad", ""], ["Kumar", "Srijan", ""], ["Boyd-Graber", "Jordan", ""], ["Danescu-Niculescu-Mizil", "Cristian", ""]]}, {"id": "1506.04929", "submitter": "Mehul Bhatt", "authors": "Przemys{\\l}aw Andrzej Wa{\\l}\\k{e}ga and Mehul Bhatt and Carl Schultz", "title": "ASPMT(QS): Non-Monotonic Spatial Reasoning with Answer Set Programming\n  Modulo Theories", "comments": "pages 13. accepted for publication at: LPNMR 2015 - Logic Programming\n  and Nonmonotonic Reasoning, 13th International Conference, LPNMR 2015, LNAI\n  Vol. 9345., Lexington, September 27-30, 2015. Proceedings., (editors:\n  Francesco Calimeri, Giovambattista Ianni, Miroslaw Truszczynski)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The systematic modelling of \\emph{dynamic spatial systems} [9] is a key\nrequirement in a wide range of application areas such as comonsense cognitive\nrobotics, computer-aided architecture design, dynamic geographic information\nsystems. We present ASPMT(QS), a novel approach and fully-implemented prototype\nfor non-monotonic spatial reasoning ---a crucial requirement within dynamic\nspatial systems-- based on Answer Set Programming Modulo Theories (ASPMT).\nASPMT(QS) consists of a (qualitative) spatial representation module (QS) and a\nmethod for turning tight ASPMT instances into Sat Modulo Theories (SMT)\ninstances in order to compute stable models by means of SMT solvers. We\nformalise and implement concepts of default spatial reasoning and spatial frame\naxioms using choice formulas. Spatial reasoning is performed by encoding\nspatial relations as systems of polynomial constraints, and solving via SMT\nwith the theory of real nonlinear arithmetic. We empirically evaluate ASPMT(QS)\nin comparison with other prominent contemporary spatial reasoning systems. Our\nresults show that ASPMT(QS) is the only existing system that is capable of\nreasoning about indirect spatial effects (i.e. addressing the ramification\nproblem), and integrating geometric and qualitative spatial information within\na non-monotonic spatial reasoning context.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2015 11:52:50 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Wa\u0142\u0119ga", "Przemys\u0142aw Andrzej", ""], ["Bhatt", "Mehul", ""], ["Schultz", "Carl", ""]]}, {"id": "1506.04945", "submitter": "Mehul Bhatt", "authors": "Carl Schultz and Mehul Bhatt", "title": "Spatial Symmetry Driven Pruning Strategies for Efficient Declarative\n  Spatial Reasoning", "comments": "22 pages. Accepted for publication at: COSIT 2015 - Conference on\n  Spatial Information Theory XII (COSIT), Santa Fe, New Mexico, USA ,October\n  2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Declarative spatial reasoning denotes the ability to (declaratively) specify\nand solve real-world problems related to geometric and qualitative spatial\nrepresentation and reasoning within standard knowledge representation and\nreasoning (KR) based methods (e.g., logic programming and derivatives). One\napproach for encoding the semantics of spatial relations within a declarative\nprogramming framework is by systems of polynomial constraints. However, solving\nsuch constraints is computationally intractable in general (i.e. the theory of\nreal-closed fields).\n  We present a new algorithm, implemented within the declarative spatial\nreasoning system CLP(QS), that drastically improves the performance of deciding\nthe consistency of spatial constraint graphs over conventional polynomial\nencodings. We develop pruning strategies founded on spatial symmetries that\nform equivalence classes (based on affine transformations) at the qualitative\nspatial level. Moreover, pruning strategies are themselves formalised as\nknowledge about the properties of space and spatial symmetries. We evaluate our\nalgorithm using a range of benchmarks in the class of contact problems, and\nproofs in mereology and geometry. The empirical results show that CLP(QS) with\nknowledge-based spatial pruning outperforms conventional polynomial encodings\nby orders of magnitude, and can thus be applied to problems that are otherwise\nunsolvable in practice.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2015 12:40:30 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Schultz", "Carl", ""], ["Bhatt", "Mehul", ""]]}, {"id": "1506.04956", "submitter": "Ernest Davis", "authors": "Ernest Davis and Gary Marcus", "title": "The Scope and Limits of Simulation in Cognitive Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been proposed that human physical reasoning consists largely of\nrunning \"physics engines in the head\" in which the future trajectory of the\nphysical system under consideration is computed precisely using accurate\nscientific theories. In such models, uncertainty and incomplete knowledge is\ndealt with by sampling probabilistically over the space of possible\ntrajectories (\"Monte Carlo simulation\"). We argue that such simulation-based\nmodels are too weak, in that there are many important aspects of human physical\nreasoning that cannot be carried out this way, or can only be carried out very\ninefficiently; and too strong, in that humans make large systematic errors that\nthe models cannot account for. We conclude that simulation-based reasoning\nmakes up at most a small part of a larger system that encompasses a wide range\nof additional cognitive processes.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2015 13:14:26 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Davis", "Ernest", ""], ["Marcus", "Gary", ""]]}, {"id": "1506.05001", "submitter": "Liliana Lo Presti", "authors": "Liliana Lo Presti and Marco La Cascia", "title": "Using Hankel Matrices for Dynamics-based Facial Emotion Recognition and\n  Pain Detection", "comments": "in IEEE Proceedings of Workshop on Analysis and Modeling of Face and\n  Gestures (CVPRW 2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new approach to model the temporal dynamics of a\nsequence of facial expressions. To this purpose, a sequence of Face Image\nDescriptors (FID) is regarded as the output of a Linear Time Invariant (LTI)\nsystem. The temporal dynamics of such sequence of descriptors are represented\nby means of a Hankel matrix. The paper presents different strategies to compute\ndynamics-based representation of a sequence of FID, and reports classification\naccuracy values of the proposed representations within different standard\nclassification frameworks. The representations have been validated in two very\nchallenging application domains: emotion recognition and pain detection.\nExperiments on two publicly available benchmarks and comparison with\nstate-of-the-art approaches demonstrate that the dynamics-based FID\nrepresentation attains competitive performance when off-the-shelf\nclassification tools are adopted.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2015 15:22:46 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Presti", "Liliana Lo", ""], ["La Cascia", "Marco", ""]]}, {"id": "1506.05012", "submitter": "Adit Jamdar", "authors": "Adit Jamdar, Jessica Abraham, Karishma Khanna and Rahul Dubey", "title": "Emotion Analysis of Songs Based on Lyrical and Audio Features", "comments": "16 pages, 2 figures, 6 tables, 5 equations in International journal\n  of Artificial Intelligence & Applications (IJAIA)", "journal-ref": null, "doi": "10.5121/ijaia.2015.6304", "report-no": null, "categories": "cs.CL cs.AI cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a method is proposed to detect the emotion of a song based on\nits lyrical and audio features. Lyrical features are generated by segmentation\nof lyrics during the process of data extraction. ANEW and WordNet knowledge is\nthen incorporated to compute Valence and Arousal values. In addition to this,\nlinguistic association rules are applied to ensure that the issue of ambiguity\nis properly addressed. Audio features are used to supplement the lyrical ones\nand include attributes like energy, tempo, and danceability. These features are\nextracted from The Echo Nest, a widely used music intelligence platform.\nConstruction of training and test sets is done on the basis of social tags\nextracted from the last.fm website. The classification is done by applying\nfeature weighting and stepwise threshold reduction on the k-Nearest Neighbors\nalgorithm to provide fuzziness in the classification.\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2015 16:04:08 GMT"}], "update_date": "2015-06-17", "authors_parsed": [["Jamdar", "Adit", ""], ["Abraham", "Jessica", ""], ["Khanna", "Karishma", ""], ["Dubey", "Rahul", ""]]}, {"id": "1506.05154", "submitter": "Andre Filipe De Moraes Batista", "authors": "Andre Filipe de Moraes Batista and Maria das Gra\\c{c}as Bruno Marietto", "title": "SNA-based reasoning for multiagent team composition", "comments": "10 pages", "journal-ref": "International Journal of Artificial Intelligence & Applications\n  (IJAIA) Vol. 6, No. 3, May 2015", "doi": "10.5121/ijaia.2015.6305", "report-no": null, "categories": "cs.MA cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The social network analysis (SNA), branch of complex systems can be used in\nthe construction of multiagent systems. This paper proposes a study of how\nsocial network analysis can assist in modeling multiagent systems, while\naddressing similarities and differences between the two theories. We built a\nprototype of multi-agent systems for resolution of tasks through the formation\nof teams of agents that are formed on the basis of the social network\nestablished between agents. Agents make use of performance indicators to assess\nwhen should change their social network to maximize the participation in teams\n", "versions": [{"version": "v1", "created": "Tue, 16 Jun 2015 21:17:05 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Batista", "Andre Filipe de Moraes", ""], ["Marietto", "Maria das Gra\u00e7as Bruno", ""]]}, {"id": "1506.05198", "submitter": "Jia Liang Mr", "authors": "Jia Hui Liang, Vijay Ganesh, Venkatesh Raman, Krzysztof Czarnecki", "title": "SAT-based Analysis of Large Real-world Feature Models is Easy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern conflict-driven clause-learning (CDCL) Boolean SAT solvers provide\nefficient automatic analysis of real-world feature models (FM) of systems\nranging from cars to operating systems. It is well-known that solver-based\nanalysis of real-world FMs scale very well even though SAT instances obtained\nfrom such FMs are large, and the corresponding analysis problems are known to\nbe NP-complete. To better understand why SAT solvers are so effective, we\nsystematically studied many syntactic and semantic characteristics of a\nrepresentative set of large real-world FMs. We discovered that a key reason why\nlarge real-world FMs are easy-to-analyze is that the vast majority of the\nvariables in these models are unrestricted, i.e., the models are satisfiable\nfor both true and false assignments to such variables under the current partial\nassignment. Given this discovery and our understanding of CDCL SAT solvers, we\nshow that solvers can easily find satisfying assignments for such models\nwithout too many backtracks relative to the model size, explaining why solvers\nscale so well. Further analysis showed that the presence of unrestricted\nvariables in these real-world models can be attributed to their high-degree of\nvariability. Additionally, we experimented with a series of well-known\nnon-backtracking simplifications that are particularly effective in solving\nFMs. The remaining variables/clauses after simplifications, called the core,\nare so few that they are easily solved even with backtracking, further\nstrengthening our conclusions.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2015 04:25:39 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2015 02:57:03 GMT"}, {"version": "v3", "created": "Wed, 29 Jul 2015 02:54:29 GMT"}], "update_date": "2015-07-30", "authors_parsed": [["Liang", "Jia Hui", ""], ["Ganesh", "Vijay", ""], ["Raman", "Venkatesh", ""], ["Czarnecki", "Krzysztof", ""]]}, {"id": "1506.05282", "submitter": "Joseph Y. Halpern", "authors": "Joseph Y. Halpern", "title": "Why Bother With Syntax?", "comments": "To appear in \"Rohit Parikh on Logic, Language and Society\" (C.\n  Baskent, L. Moss, and R. Ramanjum, editors)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This short note discusses the role of syntax vs. semantics and the interplay\nbetween logic, philosophy, and language in computer science and game theory.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2015 11:27:12 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Halpern", "Joseph Y.", ""]]}, {"id": "1506.05382", "submitter": "Michael Lash", "authors": "Michael T. Lash and Kang Zhao", "title": "Early Predictions of Movie Success: the Who, What, and When of\n  Profitability", "comments": null, "journal-ref": null, "doi": "10.1080/07421222.2016.1243969", "report-no": null, "categories": "cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a decision support system to aid movie investment\ndecisions at the early stage of movie productions. The system predicts the\nsuccess of a movie based on its profitability by leveraging historical data\nfrom various sources. Using social network analysis and text mining techniques,\nthe system automatically extracts several groups of features, including \"who\"\nare on the cast, \"what\" a movie is about, \"when\" a movie will be released, as\nwell as \"hybrid\" features that match \"who\" with \"what\", and \"when\" with \"what\".\nExperiment results with movies during an 11-year period showed that the system\noutperforms benchmark methods by a large margin in predicting movie\nprofitability. Novel features we proposed also made great contributions to the\nprediction. In addition to designing a decision support system with practical\nutilities, our analysis of key factors for movie profitability may also have\nimplications for theoretical research on team performance and the success of\ncreative work.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2015 16:40:48 GMT"}, {"version": "v2", "created": "Fri, 29 Jan 2016 20:10:52 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Lash", "Michael T.", ""], ["Zhao", "Kang", ""]]}, {"id": "1506.05424", "submitter": "Conrado Miranda", "authors": "Conrado Silva Miranda, Fernando Jos\\'e Von Zuben", "title": "Hybrid Algorithm for Multi-Objective Optimization by Greedy Hypervolume\n  Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a high-performance hybrid algorithm, called Hybrid\nHypervolume Maximization Algorithm (H2MA), for multi-objective optimization\nthat alternates between exploring the decision space and exploiting the already\nobtained non-dominated solutions. The proposal is centered on maximizing the\nhypervolume indicator, thus converting the multi-objective problem into a\nsingle-objective one. The exploitation employs gradient-based methods, but\nconsidering a single candidate efficient solution at a time, to overcome\nlimitations associated with population-based approaches and also to allow an\neasy control of the number of solutions provided. There is an interchange\nbetween two steps. The first step is a deterministic local exploration, endowed\nwith an automatic procedure to detect stagnation. When stagnation is detected,\nthe search is switched to a second step characterized by a stochastic global\nexploration using an evolutionary algorithm. Using five ZDT benchmarks with 30\nvariables, the performance of the new algorithm is compared to state-of-the-art\nalgorithms for multi-objective optimization, more specifically NSGA-II, SPEA2,\nand SMS-EMOA. The solutions found by the H2MA guide to higher hypervolume and\nsmaller distance to the true Pareto frontier with significantly less function\nevaluations, even when the gradient is estimated numerically. Furthermore,\nalthough only continuous decision spaces have been considered here, discrete\ndecision spaces could also have been treated, replacing gradient-based search\nby hill-climbing. Finally, a thorough explanation is provided to support the\nexpressive gain in performance that was achieved.\n", "versions": [{"version": "v1", "created": "Wed, 17 Jun 2015 18:52:18 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Miranda", "Conrado Silva", ""], ["Von Zuben", "Fernando Jos\u00e9", ""]]}, {"id": "1506.05573", "submitter": "Sanlaville Kevin", "authors": "Kevin Sanlaville, G\\'erard Assayag, Fr\\'ed\\'eric Bevilacqua, Catherine\n  Pelachaud (LTCI)", "title": "Emergence of synchrony in an Adaptive Interaction Model", "comments": "Intelligent Virtual Agents 2015 Doctoral Consortium, Aug 2015, Delft,\n  Netherlands. IVA Doctoral Consortium, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a Human-Computer Interaction context, we aim to elaborate an adaptive and\ngeneric interaction model in two different use cases: Embodied Conversational\nAgents and Creative Musical Agents for musical improvisation. To reach this\ngoal, we'll try to use the concepts of adaptation and synchronization to\nenhance the interactive abilities of our agents and guide the development of\nour interaction model, and will try to make synchrony emerge from non-verbal\ndimensions of interaction.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2015 07:47:49 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Sanlaville", "Kevin", "", "LTCI"], ["Assayag", "G\u00e9rard", "", "LTCI"], ["Bevilacqua", "Fr\u00e9d\u00e9ric", "", "LTCI"], ["Pelachaud", "Catherine", "", "LTCI"]]}, {"id": "1506.05692", "submitter": "Maxime Gasse", "authors": "Maxime Gasse (DM2L), Alex Aussem (DM2L), Haytham Elghazel (DM2L)", "title": "A hybrid algorithm for Bayesian network structure learning with\n  application to multi-label learning", "comments": "arXiv admin note: text overlap with arXiv:1101.5184 by other authors", "journal-ref": "Expert Systems with Applications, Elsevier, 2014, 41 (15),\n  pp.6755-6772", "doi": "10.1016/j.eswa.2014.04.032", "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel hybrid algorithm for Bayesian network structure learning,\ncalled H2PC. It first reconstructs the skeleton of a Bayesian network and then\nperforms a Bayesian-scoring greedy hill-climbing search to orient the edges.\nThe algorithm is based on divide-and-conquer constraint-based subroutines to\nlearn the local structure around a target variable. We conduct two series of\nexperimental comparisons of H2PC against Max-Min Hill-Climbing (MMHC), which is\ncurrently the most powerful state-of-the-art algorithm for Bayesian network\nstructure learning. First, we use eight well-known Bayesian network benchmarks\nwith various data sizes to assess the quality of the learned structure returned\nby the algorithms. Our extensive experiments show that H2PC outperforms MMHC in\nterms of goodness of fit to new data and quality of the network structure with\nrespect to the true dependence structure of the data. Second, we investigate\nH2PC's ability to solve the multi-label learning problem. We provide\ntheoretical results to characterize and identify graphically the so-called\nminimal label powersets that appear as irreducible factors in the joint\ndistribution under the faithfulness condition. The multi-label learning problem\nis then decomposed into a series of multi-class classification problems, where\neach multi-class variable encodes a label powerset. H2PC is shown to compare\nfavorably to MMHC in terms of global classification accuracy over ten\nmulti-label data sets covering different application domains. Overall, our\nexperiments support the conclusions that local structural learning with H2PC in\nthe form of local neighborhood induction is a theoretically well-motivated and\nempirically effective learning framework that is well suited to multi-label\nlearning. The source code (in R) of H2PC as well as all data sets used for the\nempirical tests are publicly available.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2015 14:24:19 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Gasse", "Maxime", "", "DM2L"], ["Aussem", "Alex", "", "DM2L"], ["Elghazel", "Haytham", "", "DM2L"]]}, {"id": "1506.05846", "submitter": "John Emmons", "authors": "John Emmons, Steven Johnson, Timothy Urness, and Adina Kilpatrick", "title": "Automated Assignment of Backbone NMR Data using Artificial Intelligence", "comments": "Midwest Instruction and Computing Symposium (MICS 2013); 5 pages\n  including figures. John Emmons, Steven Johnson, Timothy Urness, and Adina\n  Kilpatrick. \"Automated Assignment Of Backbone NMR Data using Artificial\n  Intelligence\". La Crosse, Wisconsin, April 2013. Midwest Instruction and\n  Computing Symposium (MICS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI q-bio.BM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Nuclear magnetic resonance (NMR) spectroscopy is a powerful method for the\ninvestigation of three-dimensional structures of biological molecules such as\nproteins. Determining a protein structure is essential for understanding its\nfunction and alterations in function which lead to disease. One of the major\nchallenges of the post-genomic era is to obtain structural and functional\ninformation on the many unknown proteins encoded by thousands of newly\nidentified genes. The goal of this research is to design an algorithm capable\nof automating the analysis of backbone protein NMR data by implementing AI\nstrategies such as greedy and A* search.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2015 22:50:02 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Emmons", "John", ""], ["Johnson", "Steven", ""], ["Urness", "Timothy", ""], ["Kilpatrick", "Adina", ""]]}, {"id": "1506.05851", "submitter": "Jian Xu", "authors": "Jian Xu, Kuang-chih Lee, Wentong Li, Hang Qi, Quan Lu", "title": "Smart Pacing for Effective Online Ad Campaign Optimization", "comments": "KDD'15, August 10-13, 2015, Sydney, NSW, Australia", "journal-ref": null, "doi": "10.1145/2783258.2788615", "report-no": null, "categories": "cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In targeted online advertising, advertisers look for maximizing campaign\nperformance under delivery constraint within budget schedule. Most of the\nadvertisers typically prefer to impose the delivery constraint to spend budget\nsmoothly over the time in order to reach a wider range of audiences and have a\nsustainable impact. Since lots of impressions are traded through public\nauctions for online advertising today, the liquidity makes price elasticity and\nbid landscape between demand and supply change quite dynamically. Therefore, it\nis challenging to perform smooth pacing control and maximize campaign\nperformance simultaneously. In this paper, we propose a smart pacing approach\nin which the delivery pace of each campaign is learned from both offline and\nonline data to achieve smooth delivery and optimal performance goals. The\nimplementation of the proposed approach in a real DSP system is also presented.\nExperimental evaluations on both real online ad campaigns and offline\nsimulations show that our approach can effectively improve campaign performance\nand achieve delivery goals.\n", "versions": [{"version": "v1", "created": "Thu, 18 Jun 2015 23:27:31 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Xu", "Jian", ""], ["Lee", "Kuang-chih", ""], ["Li", "Wentong", ""], ["Qi", "Hang", ""], ["Lu", "Quan", ""]]}, {"id": "1506.05903", "submitter": "Vincent Labatut", "authors": "Jean-Val\\`ere Cossu (LIA), Nicolas Dugu\\'e (LIFO), Vincent Labatut\n  (LIA)", "title": "Detecting Real-World Influence Through Twitter", "comments": null, "journal-ref": "2nd European Network Intelligence Conference (ENIC), Sep 2015,\n  Karlskrona, Sweden. pp.83-90", "doi": "10.1109/ENIC.2015.20", "report-no": null, "categories": "cs.SI cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the issue of detecting the real-life influence\nof people based on their Twitter account. We propose an overview of common\nTwitter features used to characterize such accounts and their activity, and\nshow that these are inefficient in this context. In particular, retweets and\nfollowers numbers, and Klout score are not relevant to our analysis. We thus\npropose several Machine Learning approaches based on Natural Language\nProcessing and Social Network Analysis to label Twitter users as Influencers or\nnot. We also rank them according to a predicted influence level. Our proposals\nare evaluated over the CLEF RepLab 2014 dataset, and outmatch state-of-the-art\nranking methods.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 08:22:25 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2015 05:25:49 GMT"}, {"version": "v3", "created": "Thu, 22 Jul 2021 12:07:24 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Cossu", "Jean-Val\u00e8re", "", "LIA"], ["Dugu\u00e9", "Nicolas", "", "LIFO"], ["Labatut", "Vincent", "", "LIA"]]}, {"id": "1506.05908", "submitter": "Chris Piech", "authors": "Chris Piech, Jonathan Spencer, Jonathan Huang, Surya Ganguli, Mehran\n  Sahami, Leonidas Guibas, Jascha Sohl-Dickstein", "title": "Deep Knowledge Tracing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Knowledge tracing---where a machine models the knowledge of a student as they\ninteract with coursework---is a well established problem in computer supported\neducation. Though effectively modeling student knowledge would have high\neducational impact, the task has many inherent challenges. In this paper we\nexplore the utility of using Recurrent Neural Networks (RNNs) to model student\nlearning. The RNN family of models have important advantages over previous\nmethods in that they do not require the explicit encoding of human domain\nknowledge, and can capture more complex representations of student knowledge.\nUsing neural networks results in substantial improvements in prediction\nperformance on a range of knowledge tracing datasets. Moreover the learned\nmodel can be used for intelligent curriculum design and allows straightforward\ninterpretation and discovery of structure in student tasks. These results\nsuggest a promising new line of research for knowledge tracing and an exemplary\napplication task for RNNs.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 08:29:00 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Piech", "Chris", ""], ["Spencer", "Jonathan", ""], ["Huang", "Jonathan", ""], ["Ganguli", "Surya", ""], ["Sahami", "Mehran", ""], ["Guibas", "Leonidas", ""], ["Sohl-Dickstein", "Jascha", ""]]}, {"id": "1506.05934", "submitter": "Thibaut Lienart", "authors": "Thibaut Lienart, Yee Whye Teh and Arnaud Doucet", "title": "Expectation Particle Belief Propagation", "comments": "submitted to NIPS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.CO cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an original particle-based implementation of the Loopy Belief\nPropagation (LPB) algorithm for pairwise Markov Random Fields (MRF) on a\ncontinuous state space. The algorithm constructs adaptively efficient proposal\ndistributions approximating the local beliefs at each note of the MRF. This is\nachieved by considering proposal distributions in the exponential family whose\nparameters are updated iterately in an Expectation Propagation (EP) framework.\nThe proposed particle scheme provides consistent estimation of the LBP\nmarginals as the number of particles increases. We demonstrate that it provides\nmore accurate results than the Particle Belief Propagation (PBP) algorithm of\nIhler and McAllester (2009) at a fraction of the computational cost and is\nadditionally more robust empirically. The computational complexity of our\nalgorithm at each iteration is quadratic in the number of particles. We also\npropose an accelerated implementation with sub-quadratic computational\ncomplexity which still provides consistent estimates of the loopy BP marginal\ndistributions and performs almost as well as the original procedure.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 09:34:21 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Lienart", "Thibaut", ""], ["Teh", "Yee Whye", ""], ["Doucet", "Arnaud", ""]]}, {"id": "1506.05969", "submitter": "Fary Diallo", "authors": "Papa Fary Diallo (WIMMICS), Olivier Corby (WIMMICS), Isabelle Mirbel\n  (WIMMICS), Moussa Lo, Seydina M. Ndiaye", "title": "HuTO: an Human Time Ontology for Semantic Web Applications", "comments": "in French. Ing{\\'e}nierie des Connaissances 2015, Jul 2015, Rennes,\n  France. Association Fran\\c{c}aise pour Intelligence Artificielle (AFIA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The temporal phenomena have many facets that are studied by different\ncommunities. In Semantic Web, large heterogeneous data are handled and\nproduced. These data often have informal, semi-formal or formal temporal\ninformation which must be interpreted by software agents. In this paper we\npresent Human Time Ontology (HuTO) an RDFS ontology to annotate and represent\ntemporal data. A major contribution of HuTO is the modeling of non-convex\nintervals giving the ability to write queries for this kind of interval. HuTO\nalso incorporates normalization and reasoning rules to explicit certain\ninformation. HuTO also proposes an approach which associates a temporal\ndimension to the knowledge base content. This facilitates information retrieval\nby considering or not the temporal aspect.\n", "versions": [{"version": "v1", "created": "Fri, 19 Jun 2015 12:08:39 GMT"}], "update_date": "2015-06-22", "authors_parsed": [["Diallo", "Papa Fary", "", "WIMMICS"], ["Corby", "Olivier", "", "WIMMICS"], ["Mirbel", "Isabelle", "", "WIMMICS"], ["Lo", "Moussa", ""], ["Ndiaye", "Seydina M.", ""]]}, {"id": "1506.06366", "submitter": "He-Wen Chen", "authors": "He-Wen Chen and Zih-Ci Wang and Shu-Yu Kuo and Yao-Hsin Chou", "title": "A Novel Method for Stock Forecasting based on Fuzzy Time Series Combined\n  with the Longest Common/Repeated Sub-sequence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stock price forecasting is an important issue for investors since extreme\naccuracy in forecasting can bring about high profits. Fuzzy Time Series (FTS)\nand Longest Common/Repeated Sub-sequence (LCS/LRS) are two important issues for\nforecasting prices. However, to the best of our knowledge, there are no\nsignificant studies using LCS/LRS to predict stock prices. It is impossible\nthat prices stay exactly the same as historic prices. Therefore, this paper\nproposes a state-of-the-art method which combines FTS and LCS/LRS to predict\nstock prices. This method is based on the principle that history will repeat\nitself. It uses different interval lengths in FTS to fuzzify the prices, and\nLCS/LRS to look for the same pattern in the historical prices to predict future\nstock prices. In the experiment, we examine various intervals of fuzzy time\nsets in order to achieve high prediction accuracy. The proposed method\noutperforms traditional methods in terms of prediction accuracy and,\nfurthermore, it is easy to implement.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2015 14:03:42 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Chen", "He-Wen", ""], ["Wang", "Zih-Ci", ""], ["Kuo", "Shu-Yu", ""], ["Chou", "Yao-Hsin", ""]]}, {"id": "1506.06418", "submitter": "Raphael Hoffmann", "authors": "Raphael Hoffmann, Luke Zettlemoyer, Daniel S. Weld", "title": "Extreme Extraction: Only One Hour per Relation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information Extraction (IE) aims to automatically generate a large knowledge\nbase from natural language text, but progress remains slow. Supervised learning\nrequires copious human annotation, while unsupervised and weakly supervised\napproaches do not deliver competitive accuracy. As a result, most fielded\napplications of IE, as well as the leading TAC-KBP systems, rely on significant\namounts of manual engineering. Even \"Extreme\" methods, such as those reported\nin Freedman et al. 2011, require about 10 hours of expert labor per relation.\n  This paper shows how to reduce that effort by an order of magnitude. We\npresent a novel system, InstaRead, that streamlines authoring with an ensemble\nof methods: 1) encoding extraction rules in an expressive and compositional\nrepresentation, 2) guiding the user to promising rules based on corpus\nstatistics and mined resources, and 3) introducing a new interactive\ndevelopment cycle that provides immediate feedback --- even on large datasets.\nExperiments show that experts can create quality extractors in under an hour\nand even NLP novices can author good extractors. These extractors equal or\noutperform ones obtained by comparably supervised and state-of-the-art\ndistantly supervised approaches.\n", "versions": [{"version": "v1", "created": "Sun, 21 Jun 2015 22:04:39 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Hoffmann", "Raphael", ""], ["Zettlemoyer", "Luke", ""], ["Weld", "Daniel S.", ""]]}, {"id": "1506.06646", "submitter": "Tadahiro Taniguchi", "authors": "Tadahiro Taniguchi, Ryo Nakashima, and Shogo Nagasaka", "title": "Nonparametric Bayesian Double Articulation Analyzer for Direct Language\n  Acquisition from Continuous Speech Signals", "comments": "15 pages, 7 figures, Draft submitted to IEEE Transactions on\n  Autonomous Mental Development (TAMD)", "journal-ref": null, "doi": "10.1109/TCDS.2016.2550591", "report-no": null, "categories": "cs.AI cs.CL cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human infants can discover words directly from unsegmented speech signals\nwithout any explicitly labeled data. In this paper, we develop a novel machine\nlearning method called nonparametric Bayesian double articulation analyzer\n(NPB-DAA) that can directly acquire language and acoustic models from observed\ncontinuous speech signals. For this purpose, we propose an integrative\ngenerative model that combines a language model and an acoustic model into a\nsingle generative model called the \"hierarchical Dirichlet process hidden\nlanguage model\" (HDP-HLM). The HDP-HLM is obtained by extending the\nhierarchical Dirichlet process hidden semi-Markov model (HDP-HSMM) proposed by\nJohnson et al. An inference procedure for the HDP-HLM is derived using the\nblocked Gibbs sampler originally proposed for the HDP-HSMM. This procedure\nenables the simultaneous and direct inference of language and acoustic models\nfrom continuous speech signals. Based on the HDP-HLM and its inference\nprocedure, we developed a novel double articulation analyzer. By assuming\nHDP-HLM as a generative model of observed time series data, and by inferring\nlatent variables of the model, the method can analyze latent double\narticulation structure, i.e., hierarchically organized latent words and\nphonemes, of the data in an unsupervised manner. The novel unsupervised double\narticulation analyzer is called NPB-DAA.\n  The NPB-DAA can automatically estimate double articulation structure embedded\nin speech signals. We also carried out two evaluation experiments using\nsynthetic data and actual human continuous speech signals representing Japanese\nvowel sequences. In the word acquisition and phoneme categorization tasks, the\nNPB-DAA outperformed a conventional double articulation analyzer (DAA) and\nbaseline automatic speech recognition system whose acoustic model was trained\nin a supervised manner.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2015 15:21:57 GMT"}, {"version": "v2", "created": "Wed, 9 Mar 2016 15:59:07 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Taniguchi", "Tadahiro", ""], ["Nakashima", "Ryo", ""], ["Nagasaka", "Shogo", ""]]}, {"id": "1506.06714", "submitter": "Michel Galley", "authors": "Alessandro Sordoni, Michel Galley, Michael Auli, Chris Brockett,\n  Yangfeng Ji, Margaret Mitchell, Jian-Yun Nie, Jianfeng Gao, Bill Dolan", "title": "A Neural Network Approach to Context-Sensitive Generation of\n  Conversational Responses", "comments": "A. Sordoni, M. Galley, M. Auli, C. Brockett, Y. Ji, M. Mitchell,\n  J.-Y. Nie, J. Gao, B. Dolan. 2015. A Neural Network Approach to\n  Context-Sensitive Generation of Conversational Responses. In Proc. of\n  NAACL-HLT. Pages 196-205", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel response generation system that can be trained end to end\non large quantities of unstructured Twitter conversations. A neural network\narchitecture is used to address sparsity issues that arise when integrating\ncontextual information into classic statistical models, allowing the system to\ntake into account previous dialog utterances. Our dynamic-context generative\nmodels show consistent gains over both context-sensitive and\nnon-context-sensitive Machine Translation and Information Retrieval baselines.\n", "versions": [{"version": "v1", "created": "Mon, 22 Jun 2015 18:29:03 GMT"}], "update_date": "2015-06-23", "authors_parsed": [["Sordoni", "Alessandro", ""], ["Galley", "Michel", ""], ["Auli", "Michael", ""], ["Brockett", "Chris", ""], ["Ji", "Yangfeng", ""], ["Mitchell", "Margaret", ""], ["Nie", "Jian-Yun", ""], ["Gao", "Jianfeng", ""], ["Dolan", "Bill", ""]]}, {"id": "1506.06833", "submitter": "Francis Ferraro", "authors": "Francis Ferraro, Nasrin Mostafazadeh, Ting-Hao (Kenneth) Huang, Lucy\n  Vanderwende, Jacob Devlin, Michel Galley, Margaret Mitchell", "title": "A Survey of Current Datasets for Vision and Language Research", "comments": "To appear in EMNLP 2015, short proceedings. Dataset analysis and\n  discussion expanded, including an initial examination into reporting bias for\n  one of them. F.F. and N.M. contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.GL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrating vision and language has long been a dream in work on artificial\nintelligence (AI). In the past two years, we have witnessed an explosion of\nwork that brings together vision and language from images to videos and beyond.\nThe available corpora have played a crucial role in advancing this area of\nresearch. In this paper, we propose a set of quality metrics for evaluating and\nanalyzing the vision & language datasets and categorize them accordingly. Our\nanalyses show that the most recent datasets have been using more complex\nlanguage and more abstract concepts, however, there are different strengths and\nweaknesses in each.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2015 00:59:27 GMT"}, {"version": "v2", "created": "Wed, 19 Aug 2015 04:33:37 GMT"}], "update_date": "2015-08-20", "authors_parsed": [["Ferraro", "Francis", "", "Kenneth"], ["Mostafazadeh", "Nasrin", "", "Kenneth"], ["Ting-Hao", "", "", "Kenneth"], ["Huang", "", ""], ["Vanderwende", "Lucy", ""], ["Devlin", "Jacob", ""], ["Galley", "Michel", ""], ["Mitchell", "Margaret", ""]]}, {"id": "1506.07116", "submitter": "Carlo A. Trugenberger", "authors": "Carlo A. Trugenberger", "title": "Scientific Discovery by Machine Intelligence: A New Avenue for Drug\n  Research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The majority of big data is unstructured and of this majority the largest\nchunk is text. While data mining techniques are well developed and standardized\nfor structured, numerical data, the realm of unstructured data is still largely\nunexplored. The general focus lies on information extraction, which attempts to\nretrieve known information from text. The Holy Grail, however is knowledge\ndiscovery, where machines are expected to unearth entirely new facts and\nrelations that were not previously known by any human expert. Indeed,\nunderstanding the meaning of text is often considered as one of the main\ncharacteristics of human intelligence. The ultimate goal of semantic artificial\nintelligence is to devise software that can understand the meaning of free\ntext, at least in the practical sense of providing new, actionable information\ncondensed out of a body of documents. As a stepping stone on the road to this\nvision I will introduce a totally new approach to drug research, namely that of\nidentifying relevant information by employing a self-organizing semantic engine\nto text mine large repositories of biomedical research papers, a technique\npioneered by Merck with the InfoCodex software. I will describe the methodology\nand a first successful experiment for the discovery of new biomarkers and\nphenotypes for diabetes and obesity on the basis of PubMed abstracts, public\nclinical trials and Merck internal documents. The reported approach shows much\npromise and has potential to impact fundamentally pharmaceutical research as a\nway to shorten time-to-market of novel drugs, and for early recognition of dead\nends.\n", "versions": [{"version": "v1", "created": "Tue, 23 Jun 2015 18:04:14 GMT"}], "update_date": "2015-06-24", "authors_parsed": [["Trugenberger", "Carlo A.", ""]]}, {"id": "1506.07220", "submitter": "Hui Jiang", "authors": "Yangtuo Peng and Hui Jiang", "title": "Leverage Financial News to Predict Stock Price Movements Using Word\n  Embeddings and Deep Neural Networks", "comments": "5 pages, 2 figures, technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Financial news contains useful information on public companies and the\nmarket. In this paper we apply the popular word embedding methods and deep\nneural networks to leverage financial news to predict stock price movements in\nthe market. Experimental results have shown that our proposed methods are\nsimple but very effective, which can significantly improve the stock prediction\naccuracy on a standard financial database over the baseline system using only\nthe historical price information.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 01:43:11 GMT"}], "update_date": "2015-06-25", "authors_parsed": [["Peng", "Yangtuo", ""], ["Jiang", "Hui", ""]]}, {"id": "1506.07359", "submitter": "Jan Leike", "authors": "Tom Everitt and Jan Leike and Marcus Hutter", "title": "Sequential Extensions of Causal and Evidential Decision Theory", "comments": "ADT 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Moving beyond the dualistic view in AI where agent and environment are\nseparated incurs new challenges for decision making, as calculation of expected\nutility is no longer straightforward. The non-dualistic decision theory\nliterature is split between causal decision theory and evidential decision\ntheory. We extend these decision algorithms to the sequential setting where the\nagent alternates between taking actions and observing their consequences. We\nfind that evidential decision theory has two natural extensions while causal\ndecision theory only has one.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 13:16:16 GMT"}], "update_date": "2015-06-25", "authors_parsed": [["Everitt", "Tom", ""], ["Leike", "Jan", ""], ["Hutter", "Marcus", ""]]}, {"id": "1506.07504", "submitter": "Maja Rudolph", "authors": "Maja R. Rudolph, Joseph G. Ellis, and David M. Blei", "title": "Objective Variables for Probabilistic Revenue Maximization in\n  Second-Price Auctions with Reserve", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.GT cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many online companies sell advertisement space in second-price auctions with\nreserve. In this paper, we develop a probabilistic method to learn a profitable\nstrategy to set the reserve price. We use historical auction data with features\nto fit a predictor of the best reserve price. This problem is delicate - the\nstructure of the auction is such that a reserve price set too high is much\nworse than a reserve price set too low. To address this we develop objective\nvariables, a new framework for combining probabilistic modeling with optimal\ndecision-making. Objective variables are \"hallucinated observations\" that\ntransform the revenue maximization task into a regularized maximum likelihood\nestimation problem, which we solve with an EM algorithm. This framework enables\na variety of prediction mechanisms to set the reserve price. As examples, we\nstudy objective variable methods with regression, kernelized regression, and\nneural networks on simulated and real data. Our methods outperform previous\napproaches both in terms of scalability and profit.\n", "versions": [{"version": "v1", "created": "Wed, 24 Jun 2015 19:20:18 GMT"}], "update_date": "2015-06-25", "authors_parsed": [["Rudolph", "Maja R.", ""], ["Ellis", "Joseph G.", ""], ["Blei", "David M.", ""]]}, {"id": "1506.07990", "submitter": "Thomas Bolander", "authors": "Mikkel Birkegaard Andersen, Thomas Bolander, Hans van Ditmarsch,\n  Martin Holm Jensen", "title": "Bisimulation and expressivity for conditional belief, degrees of belief,\n  and safe belief", "comments": null, "journal-ref": "Synthese 194(7): 2447-2487 (2017)", "doi": "10.1007/s11229-016-1060-x", "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Plausibility models are Kripke models that agents use to reason about\nknowledge and belief, both of themselves and of each other. Such models are\nused to interpret the notions of conditional belief, degrees of belief, and\nsafe belief. The logic of conditional belief contains that modality and also\nthe knowledge modality, and similarly for the logic of degrees of belief and\nthe logic of safe belief. With respect to these logics, plausibility models may\ncontain too much information. A proper notion of bisimulation is required that\ncharacterises them. We define that notion of bisimulation and prove the\nrequired characterisations: on the class of image-finite and preimage-finite\nmodels (with respect to the plausibility relation), two pointed Kripke models\nare modally equivalent in either of the three logics, if and only if they are\nbisimilar. As a result, the information content of such a model can be\nsimilarly expressed in the logic of conditional belief, or the logic of degrees\nof belief, or that of safe belief. This, we found a surprising result. Still,\nthat does not mean that the logics are equally expressive: the logics of\nconditional and degrees of belief are incomparable, the logics of degrees of\nbelief and safe belief are incomparable, while the logic of safe belief is more\nexpressive than the logic of conditional belief. In view of the result on\nbisimulation characterisation, this is an equally surprising result. We hope\nour insights may contribute to the growing community of formal epistemology and\non the relation between qualitative and quantitative modelling.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2015 08:17:32 GMT"}, {"version": "v2", "created": "Thu, 25 Feb 2016 21:43:40 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Andersen", "Mikkel Birkegaard", ""], ["Bolander", "Thomas", ""], ["van Ditmarsch", "Hans", ""], ["Jensen", "Martin Holm", ""]]}, {"id": "1506.08009", "submitter": "Francois Petitjean Ph.D.", "authors": "Francois Petitjean, Tao Li, Nikolaj Tatti, Geoffrey I. Webb", "title": "Skopus: Mining top-k sequential patterns under leverage", "comments": null, "journal-ref": "Data Mining and Knowledge Discovery, September 2016, Volume 30,\n  Issue 5, pp 1086-1111", "doi": "10.1007/s10618-016-0467-9", "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a framework for exact discovery of the top-k sequential\npatterns under Leverage. It combines (1) a novel definition of the expected\nsupport for a sequential pattern - a concept on which most interestingness\nmeasures directly rely - with (2) SkOPUS: a new branch-and-bound algorithm for\nthe exact discovery of top-k sequential patterns under a given measure of\ninterest. Our interestingness measure employs the partition approach. A pattern\nis interesting to the extent that it is more frequent than can be explained by\nassuming independence between any of the pairs of patterns from which it can be\ncomposed. The larger the support compared to the expectation under\nindependence, the more interesting is the pattern. We build on these two\nelements to exactly extract the k sequential patterns with highest leverage,\nconsistent with our definition of expected support. We conduct experiments on\nboth synthetic data with known patterns and real-world datasets; both\nexperiments confirm the consistency and relevance of our approach with regard\nto the state of the art. This article was published in Data Mining and\nKnowledge Discovery and is accessible at\nhttp://dx.doi.org/10.1007/s10618-016-0467-9.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2015 09:36:10 GMT"}, {"version": "v2", "created": "Wed, 6 Jan 2016 04:48:08 GMT"}, {"version": "v3", "created": "Mon, 8 May 2017 23:59:16 GMT"}, {"version": "v4", "created": "Mon, 5 Feb 2018 01:26:34 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Petitjean", "Francois", ""], ["Li", "Tao", ""], ["Tatti", "Nikolaj", ""], ["Webb", "Geoffrey I.", ""]]}, {"id": "1506.08030", "submitter": "Ismail Ilkan Ceylan", "authors": "\\.Ismail \\.Ilkan Ceylan and Rafael Pe\\~naloza", "title": "Dynamic Bayesian Ontology Languages", "comments": "Fifth International Workshop on Statistical Relational AI\n  (StarAI'2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many formalisms combining ontology languages with uncertainty, usually in the\nform of probabilities, have been studied over the years. Most of these\nformalisms, however, assume that the probabilistic structure of the knowledge\nremains static over time. We present a general approach for extending ontology\nlanguages to handle time-evolving uncertainty represented by a dynamic Bayesian\nnetwork. We show how reasoning in the original language and dynamic Bayesian\ninferences can be exploited for effective reasoning in our framework.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2015 11:32:46 GMT"}], "update_date": "2015-06-29", "authors_parsed": [["Ceylan", "\u0130smail \u0130lkan", ""], ["Pe\u00f1aloza", "Rafael", ""]]}, {"id": "1506.08126", "submitter": "Dragomir", "authors": "Dragomir Radev and Amanda Stent and Joel Tetreault and Aasish Pappu\n  and Aikaterini Iliakopoulou and Agustin Chanfreau and Paloma de Juan and\n  Jordi Vallmitjana and Alejandro Jaimes and Rahul Jha and Bob Mankoff", "title": "Humor in Collective Discourse: Unsupervised Funniness Detection in the\n  New Yorker Cartoon Caption Contest", "comments": "10 pages, in submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.MM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The New Yorker publishes a weekly captionless cartoon. More than 5,000\nreaders submit captions for it. The editors select three of them and ask the\nreaders to pick the funniest one. We describe an experiment that compares a\ndozen automatic methods for selecting the funniest caption. We show that\nnegative sentiment, human-centeredness, and lexical centrality most strongly\nmatch the funniest captions, followed by positive sentiment. These results are\nuseful for understanding humor and also in the design of more engaging\nconversational agents in text and multimodal (vision+text) systems. As part of\nthis work, a large set of cartoons and captions is being made available to the\ncommunity.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2015 15:48:10 GMT"}], "update_date": "2015-06-29", "authors_parsed": [["Radev", "Dragomir", ""], ["Stent", "Amanda", ""], ["Tetreault", "Joel", ""], ["Pappu", "Aasish", ""], ["Iliakopoulou", "Aikaterini", ""], ["Chanfreau", "Agustin", ""], ["de Juan", "Paloma", ""], ["Vallmitjana", "Jordi", ""], ["Jaimes", "Alejandro", ""], ["Jha", "Rahul", ""], ["Mankoff", "Bob", ""]]}, {"id": "1506.08425", "submitter": "Chee Seng Chan", "authors": "Sue Han Lee, Chee Seng Chan, Paul Wilkin and Paolo Remagnino", "title": "Deep-Plant: Plant Identification with convolutional neural networks", "comments": "6 pages, 8 figures, accepted as oral presentation in ICIP2015,\n  Qu\\'ebec City, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper studies convolutional neural networks (CNN) to learn unsupervised\nfeature representations for 44 different plant species, collected at the Royal\nBotanic Gardens, Kew, England. To gain intuition on the chosen features from\nthe CNN model (opposed to a 'black box' solution), a visualisation technique\nbased on the deconvolutional networks (DN) is utilized. It is found that\nvenations of different order have been chosen to uniquely represent each of the\nplant species. Experimental results using these CNN features with different\nclassifiers show consistency and superiority compared to the state-of-the art\nsolutions which rely on hand-crafted features.\n", "versions": [{"version": "v1", "created": "Sun, 28 Jun 2015 16:58:47 GMT"}], "update_date": "2015-06-30", "authors_parsed": [["Lee", "Sue Han", ""], ["Chan", "Chee Seng", ""], ["Wilkin", "Paul", ""], ["Remagnino", "Paolo", ""]]}, {"id": "1506.08544", "submitter": "Stephane Robin", "authors": "Nathalie Peyrard and Marie-Jos\\'ee Cros and Simon de Givry and Alain\n  Franc and St\\'ephane Robin and R\\'egis Sabbadin and Thomas Schiex and\n  Matthieu Vignes", "title": "Exact and approximate inference in graphical models: variable\n  elimination and beyond", "comments": "47 pages, 3 tables, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Probabilistic graphical models offer a powerful framework to account for the\ndependence structure between variables, which is represented as a graph.\nHowever, the dependence between variables may render inference tasks\nintractable. In this paper we review techniques exploiting the graph structure\nfor exact inference, borrowed from optimisation and computer science. They are\nbuilt on the principle of variable elimination whose complexity is dictated in\nan intricate way by the order in which variables are eliminated. The so-called\ntreewidth of the graph characterises this algorithmic complexity: low-treewidth\ngraphs can be processed efficiently. The first message that we illustrate is\ntherefore the idea that for inference in graphical model, the number of\nvariables is not the limiting factor, and it is worth checking for the\ntreewidth before turning to approximate methods. We show how algorithms\nproviding an upper bound of the treewidth can be exploited to derive a 'good'\nelimination order enabling to perform exact inference. The second message is\nthat when the treewidth is too large, algorithms for approximate inference\nlinked to the principle of variable elimination, such as loopy belief\npropagation and variational approaches, can lead to accurate results while\nbeing much less time consuming than Monte-Carlo approaches. We illustrate the\ntechniques reviewed in this article on benchmarks of inference problems in\ngenetic linkage analysis and computer vision, as well as on hidden variables\nrestoration in coupled Hidden Markov Models.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2015 08:45:11 GMT"}, {"version": "v2", "created": "Mon, 12 Mar 2018 07:45:08 GMT"}], "update_date": "2018-03-13", "authors_parsed": [["Peyrard", "Nathalie", ""], ["Cros", "Marie-Jos\u00e9e", ""], ["de Givry", "Simon", ""], ["Franc", "Alain", ""], ["Robin", "St\u00e9phane", ""], ["Sabbadin", "R\u00e9gis", ""], ["Schiex", "Thomas", ""], ["Vignes", "Matthieu", ""]]}, {"id": "1506.08781", "submitter": "Richard Preen", "authors": "Richard J. Preen and Larry Bull", "title": "On Design Mining: Coevolution and Surrogate Models", "comments": null, "journal-ref": "Artificial Life (2017), 23(2):186-205", "doi": "10.1162/ARTL_a_00225", "report-no": null, "categories": "cs.NE cs.AI cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Design mining is the use of computational intelligence techniques to\niteratively search and model the attribute space of physical objects evaluated\ndirectly through rapid prototyping to meet given objectives. It enables the\nexploitation of novel materials and processes without formal models or complex\nsimulation. In this paper, we focus upon the coevolutionary nature of the\ndesign process when it is decomposed into concurrent sub-design threads due to\nthe overall complexity of the task. Using an abstract, tuneable model of\ncoevolution we consider strategies to sample sub-thread designs for whole\nsystem testing and how best to construct and use surrogate models within the\ncoevolutionary scenario. Drawing on our findings, the paper then describes the\neffective design of an array of six heterogeneous vertical-axis wind turbines.\n", "versions": [{"version": "v1", "created": "Mon, 29 Jun 2015 18:57:34 GMT"}, {"version": "v2", "created": "Sat, 25 Jul 2015 14:05:42 GMT"}, {"version": "v3", "created": "Fri, 31 Jul 2015 17:29:20 GMT"}, {"version": "v4", "created": "Mon, 15 Feb 2016 14:43:39 GMT"}, {"version": "v5", "created": "Wed, 26 Oct 2016 22:09:08 GMT"}, {"version": "v6", "created": "Wed, 23 Nov 2016 20:24:17 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Preen", "Richard J.", ""], ["Bull", "Larry", ""]]}, {"id": "1506.08813", "submitter": "Anthony Young", "authors": "Anthony P. Young, Sanjay Modgil, Odinaldo Rodrigues", "title": "Argumentation Semantics for Prioritised Default Logic", "comments": "46 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We endow prioritised default logic (PDL) with argumentation semantics using\nthe ASPIC+ framework for structured argumentation, and prove that the\nconclusions of the justified arguments are exactly the prioritised default\nextensions. Argumentation semantics for PDL will allow for the application of\nargument game proof theories to the process of inference in PDL, making the\nreasons for accepting a conclusion transparent and the inference process more\nintuitive. This also opens up the possibility for argumentation-based\ndistributed reasoning and communication amongst agents with PDL representations\nof mental attitudes.\n", "versions": [{"version": "v1", "created": "Fri, 26 Jun 2015 21:53:54 GMT"}, {"version": "v2", "created": "Wed, 1 Jul 2015 11:01:17 GMT"}], "update_date": "2015-07-02", "authors_parsed": [["Young", "Anthony P.", ""], ["Modgil", "Sanjay", ""], ["Rodrigues", "Odinaldo", ""]]}, {"id": "1506.08909", "submitter": "Ryan Lowe T.", "authors": "Ryan Lowe, Nissan Pow, Iulian Serban, Joelle Pineau", "title": "The Ubuntu Dialogue Corpus: A Large Dataset for Research in Unstructured\n  Multi-Turn Dialogue Systems", "comments": "SIGDIAL 2015. 10 pages, 5 figures. Update includes link to new\n  version of the dataset, with some added features and bug fixes. See:\n  https://github.com/rkadlec/ubuntu-ranking-dataset-creator", "journal-ref": null, "doi": null, "report-no": "Proc. SIGDIAL 16 (2015) pp. 285-294", "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces the Ubuntu Dialogue Corpus, a dataset containing almost\n1 million multi-turn dialogues, with a total of over 7 million utterances and\n100 million words. This provides a unique resource for research into building\ndialogue managers based on neural language models that can make use of large\namounts of unlabeled data. The dataset has both the multi-turn property of\nconversations in the Dialog State Tracking Challenge datasets, and the\nunstructured nature of interactions from microblog services such as Twitter. We\nalso describe two neural learning architectures suitable for analyzing this\ndataset, and provide benchmark performance on the task of selecting the best\nnext response.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 00:37:09 GMT"}, {"version": "v2", "created": "Tue, 21 Jul 2015 16:11:29 GMT"}, {"version": "v3", "created": "Thu, 4 Feb 2016 01:21:35 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Lowe", "Ryan", ""], ["Pow", "Nissan", ""], ["Serban", "Iulian", ""], ["Pineau", "Joelle", ""]]}, {"id": "1506.08919", "submitter": "Nicolas Schwind", "authors": "Nicolas Schwind, Katsumi Inoue", "title": "Characterization of Logic Program Revision as an Extension of\n  Propositional Revision", "comments": "42 pages, 5 figures, to appear in Theory and Practice of Logic\n  Programming (accepted in June 2015)", "journal-ref": "Theory and Practice of Logic Programming 16 (2016) 111-138", "doi": "10.1017/S1471068415000101", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of belief revision of logic programs, i.e., how to\nincorporate to a logic program P a new logic program Q. Based on the structure\nof SE interpretations, Delgrande et al. adapted the well-known AGM framework to\nlogic program (LP) revision. They identified the rational behavior of LP\nrevision and introduced some specific operators. In this paper, a constructive\ncharacterization of all rational LP revision operators is given in terms of\norderings over propositional interpretations with some further conditions\nspecific to SE interpretations. It provides an intuitive, complete procedure\nfor the construction of all rational LP revision operators and makes easier the\ncomprehension of their semantic and computational properties. We give a\nparticular consideration to logic programs of very general form, i.e., the\ngeneralized logic programs (GLPs). We show that every rational GLP revision\noperator is derived from a propositional revision operator satisfying the\noriginal AGM postulates. Interestingly, the further conditions specific to GLP\nrevision are independent from the propositional revision operator on which a\nGLP revision operator is based. Taking advantage of our characterization\nresult, we embed the GLP revision operators into structures of Boolean\nlattices, that allow us to bring to light some potential weaknesses in the\nadapted AGM postulates. To illustrate our claim, we introduce and characterize\naxiomatically two specific classes of (rational) GLP revision operators which\narguably have a drastic behavior. We additionally consider two more restricted\nforms of logic programs, i.e., the disjunctive logic programs (DLPs) and the\nnormal logic programs (NLPs) and adapt our characterization result to DLP and\nNLP revision operators.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 02:09:02 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Schwind", "Nicolas", ""], ["Inoue", "Katsumi", ""]]}, {"id": "1506.08941", "submitter": "Karthik Narasimhan", "authors": "Karthik Narasimhan, Tejas Kulkarni and Regina Barzilay", "title": "Language Understanding for Text-based Games Using Deep Reinforcement\n  Learning", "comments": "11 pages, Appearing at EMNLP, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider the task of learning control policies for\ntext-based games. In these games, all interactions in the virtual world are\nthrough text and the underlying state is not observed. The resulting language\nbarrier makes such environments challenging for automatic game players. We\nemploy a deep reinforcement learning framework to jointly learn state\nrepresentations and action policies using game rewards as feedback. This\nframework enables us to map text descriptions into vector representations that\ncapture the semantics of the game states. We evaluate our approach on two game\nworlds, comparing against baselines using bag-of-words and bag-of-bigrams for\nstate representations. Our algorithm outperforms the baselines on both worlds\ndemonstrating the importance of learning expressive representations.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 05:51:11 GMT"}, {"version": "v2", "created": "Fri, 11 Sep 2015 23:16:13 GMT"}], "update_date": "2015-09-15", "authors_parsed": [["Narasimhan", "Karthik", ""], ["Kulkarni", "Tejas", ""], ["Barzilay", "Regina", ""]]}, {"id": "1506.08959", "submitter": "Linjie Yang", "authors": "Linjie Yang, Ping Luo, Chen Change Loy, Xiaoou Tang", "title": "A Large-Scale Car Dataset for Fine-Grained Categorization and\n  Verification", "comments": "An extension to our conference paper in CVPR 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Updated on 24/09/2015: This update provides preliminary experiment results\nfor fine-grained classification on the surveillance data of CompCars. The\ntrain/test splits are provided in the updated dataset. See details in Section\n6.\n", "versions": [{"version": "v1", "created": "Tue, 30 Jun 2015 06:47:50 GMT"}, {"version": "v2", "created": "Thu, 24 Sep 2015 09:04:24 GMT"}], "update_date": "2015-09-25", "authors_parsed": [["Yang", "Linjie", ""], ["Luo", "Ping", ""], ["Loy", "Chen Change", ""], ["Tang", "Xiaoou", ""]]}]