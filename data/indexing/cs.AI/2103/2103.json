[{"id": "2103.00058", "submitter": "Harel Yedidsion", "authors": "Jiaxun Cui, William Macke, Harel Yedidsion, Aastha Goyal, Daniel\n  Urielli, Peter Stone", "title": "Scalable Multiagent Driving Policies For Reducing Traffic Congestion", "comments": "Accepted as a full paper to the International Conference on\n  Autonomous Agents and Multi Agent Systems (AAMAS). 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traffic congestion is a major challenge in modern urban settings. The\nindustry-wide development of autonomous and automated vehicles (AVs) motivates\nthe question of how can AVs contribute to congestion reduction. Past research\nhas shown that in small scale mixed traffic scenarios with both AVs and\nhuman-driven vehicles, a small fraction of AVs executing a controlled\nmultiagent driving policy can mitigate congestion. In this paper, we scale up\nexisting approaches and develop new multiagent driving policies for AVs in\nscenarios with greater complexity. We start by showing that a congestion metric\nused by past research is manipulable in open road network scenarios where\nvehicles dynamically join and leave the road. We then propose using a different\nmetric that is robust to manipulation and reflects open network traffic\nefficiency. Next, we propose a modular transfer reinforcement learning\napproach, and use it to scale up a multiagent driving policy to outperform\nhuman-like traffic and existing approaches in a simulated realistic scenario,\nwhich is an order of magnitude larger than past scenarios (hundreds instead of\ntens of vehicles). Additionally, our modular transfer learning approach saves\nup to 80% of the training time in our experiments, by focusing its data\ncollection on key locations in the network. Finally, we show for the first time\na distributed multiagent policy that improves congestion over human-driven\ntraffic. The distributed approach is more realistic and practical, as it relies\nsolely on existing sensing and actuation capabilities, and does not require\nadding new communication infrastructure.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 21:29:55 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Cui", "Jiaxun", ""], ["Macke", "William", ""], ["Yedidsion", "Harel", ""], ["Goyal", "Aastha", ""], ["Urielli", "Daniel", ""], ["Stone", "Peter", ""]]}, {"id": "2103.00067", "submitter": "Jakob Meldgaard Kj{\\ae}r", "authors": "Jakob Meldgaard Kj{\\ae}r, Lasse Kristensen, Mads Alberg Christensen", "title": "Partitioned Graph Convolution Using Adversarial and Regression Networks\n  for Road Travel Speed Prediction", "comments": "This thesis was completed 2020-06-12 and defended 2020-06-26", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Access to quality travel time information for roads in a road network has\nbecome increasingly important with the rising demand for real-time travel time\nestimation for paths within road networks. In the context of the Danish road\nnetwork (DRN) dataset used in this paper, the data coverage is sparse and\nskewed towards arterial roads, with a coverage of 23.88% across 850,980 road\nsegments, which makes travel time estimation difficult. Existing solutions for\ngraph-based data processing often neglect the size of the graph, which is an\napparent problem for road networks with a large amount of connected road\nsegments. To this end, we propose a framework for predicting road segment\ntravel speed histograms for dataless edges, based on a latent representation\ngenerated by an adversarially regularized convolutional network. We apply a\npartitioning algorithm to divide the graph into dense subgraphs, and then train\na model for each subgraph to predict speed histograms for the nodes. The\nframework achieves an accuracy of 71.5% intersection and 78.5% correlation on\npredicting travel speed histograms using the DRN dataset. Furthermore,\nexperiments show that partitioning the dataset into clusters increases the\nperformance of the framework. Specifically, partitioning the road network\ndataset into 100 clusters, with approximately 500 road segments in each\ncluster, achieves a better performance than when using 10 and 20 clusters.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 22:16:48 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Kj\u00e6r", "Jakob Meldgaard", ""], ["Kristensen", "Lasse", ""], ["Christensen", "Mads Alberg", ""]]}, {"id": "2103.00070", "submitter": "Jiaoyan Chen", "authors": "Jiaoyan Chen and Yuxia Geng and Zhuo Chen and Ian Horrocks and Jeff Z.\n  Pan and Huajun Chen", "title": "Knowledge-aware Zero-Shot Learning: Survey and Perspective", "comments": "Accepted by IJCAI'21 Survey Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Zero-shot learning (ZSL) which aims at predicting classes that have never\nappeared during the training using external knowledge (a.k.a. side information)\nhas been widely investigated. In this paper we present a literature review\ntowards ZSL in the perspective of external knowledge, where we categorize the\nexternal knowledge, review their methods and compare different external\nknowledge. With the literature review, we further discuss and outlook the role\nof symbolic knowledge in addressing ZSL and other machine learning sample\nshortage issues.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 22:18:09 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 17:45:26 GMT"}, {"version": "v3", "created": "Mon, 10 May 2021 08:35:41 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Chen", "Jiaoyan", ""], ["Geng", "Yuxia", ""], ["Chen", "Zhuo", ""], ["Horrocks", "Ian", ""], ["Pan", "Jeff Z.", ""], ["Chen", "Huajun", ""]]}, {"id": "2103.00073", "submitter": "Nan Jiang", "authors": "Nan Jiang, Thibaud Lutellier, Lin Tan", "title": "CURE: Code-Aware Neural Machine Translation for Automatic Program Repair", "comments": "This paper is accepted by 2021 IEEE/ACM 43rd International Conference\n  on Software Engineering (ICSE)", "journal-ref": null, "doi": "10.1109/ICSE43902.2021.00107", "report-no": null, "categories": "cs.SE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic program repair (APR) is crucial to improve software reliability.\nRecently, neural machine translation (NMT) techniques have been used to fix\nsoftware bugs automatically. While promising, these approaches have two major\nlimitations. Their search space often does not contain the correct fix, and\ntheir search strategy ignores software knowledge such as strict code syntax.\nDue to these limitations, existing NMT-based techniques underperform the best\ntemplate-based approaches.\n  We propose CURE, a new NMT-based APR technique with three major novelties.\nFirst, CURE pre-trains a programming language (PL) model on a large software\ncodebase to learn developer-like source code before the APR task. Second, CURE\ndesigns a new code-aware search strategy that finds more correct fixes by\nfocusing on compilable patches and patches that are close in length to the\nbuggy code. Finally, CURE uses a subword tokenization technique to generate a\nsmaller search space that contains more correct fixes.\n  Our evaluation on two widely-used benchmarks shows that CURE correctly fixes\n57 Defects4J bugs and 26 QuixBugs bugs, outperforming all existing APR\ntechniques on both benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 22:30:28 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 02:31:23 GMT"}, {"version": "v3", "created": "Thu, 20 May 2021 04:41:33 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Jiang", "Nan", ""], ["Lutellier", "Thibaud", ""], ["Tan", "Lin", ""]]}, {"id": "2103.00082", "submitter": "Michael Cochez", "authors": "Leandro Eichenberger, Michael Cochez, Benjamin Heitmann, Stefan Decker", "title": "Secure Evaluation of Knowledge Graph Merging Gain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding out the differences and commonalities between the knowledge of two\nparties is an important task. Such a comparison becomes necessary, when one\nparty wants to determine how much it is worth to acquire the knowledge of the\nsecond party, or similarly when two parties try to determine, whether a\ncollaboration could be beneficial. When these two parties cannot trust each\nother (for example, due to them being competitors) performing such a comparison\nis challenging as neither of them would be willing to share any of their\nassets. This paper addresses this problem for knowledge graphs, without a need\nfor non-disclosure agreements nor a third party during the protocol.\n  During the protocol, the intersection between the two knowledge graphs is\ndetermined in a privacy preserving fashion. This is followed by the computation\nof various metrics, which give an indication of the potential gain from\nobtaining the other parties knowledge graph, while still keeping the actual\nknowledge graph contents secret. The protocol makes use of blind signatures and\n(counting) Bloom filters to reduce the amount of leaked information. Finally,\nthe party who wants to obtain the other's knowledge graph can get a part of\nsuch in a way that neither party is able to know beforehand which parts of the\ngraph are obtained (i.e., they cannot choose to only get or share the good\nparts). After inspection of the quality of this part, the Buyer can decide to\nproceed with the transaction.\n  The analysis of the protocol indicates that the developed protocol is secure\nagainst malicious participants. Further experimental analysis shows that the\nresource consumption scales linear with the number of statements in the\nknowledge graph.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 23:19:53 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Eichenberger", "Leandro", ""], ["Cochez", "Michael", ""], ["Heitmann", "Benjamin", ""], ["Decker", "Stefan", ""]]}, {"id": "2103.00107", "submitter": "Tadashi Kozuno", "authors": "Tadashi Kozuno, Yunhao Tang, Mark Rowland, R\\'emi Munos, Steven\n  Kapturowski, Will Dabney, Michal Valko, David Abel", "title": "Revisiting Peng's Q($\\lambda$) for Modern Reinforcement Learning", "comments": "26 pages, 7 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Off-policy multi-step reinforcement learning algorithms consist of\nconservative and non-conservative algorithms: the former actively cut traces,\nwhereas the latter do not. Recently, Munos et al. (2016) proved the convergence\nof conservative algorithms to an optimal Q-function. In contrast,\nnon-conservative algorithms are thought to be unsafe and have a limited or no\ntheoretical guarantee. Nonetheless, recent studies have shown that\nnon-conservative algorithms empirically outperform conservative ones. Motivated\nby the empirical results and the lack of theory, we carry out theoretical\nanalyses of Peng's Q($\\lambda$), a representative example of non-conservative\nalgorithms. We prove that it also converges to an optimal policy provided that\nthe behavior policy slowly tracks a greedy policy in a way similar to\nconservative policy iteration. Such a result has been conjectured to be true\nbut has not been proven. We also experiment with Peng's Q($\\lambda$) in complex\ncontinuous control tasks, confirming that Peng's Q($\\lambda$) often outperforms\nconservative algorithms despite its simplicity. These results indicate that\nPeng's Q($\\lambda$), which was thought to be unsafe, is a theoretically-sound\nand practically effective algorithm.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 02:29:01 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Kozuno", "Tadashi", ""], ["Tang", "Yunhao", ""], ["Rowland", "Mark", ""], ["Munos", "R\u00e9mi", ""], ["Kapturowski", "Steven", ""], ["Dabney", "Will", ""], ["Valko", "Michal", ""], ["Abel", "David", ""]]}, {"id": "2103.00109", "submitter": "Ye Zhang", "authors": "Ye Zhang, Yuan Cao, Mahdis Mahdieh, Jeffrey Zhao, Yonghui Wu", "title": "Improving Longer-range Dialogue State Tracking", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dialogue state tracking (DST) is a pivotal component in task-oriented\ndialogue systems. While it is relatively easy for a DST model to capture belief\nstates in short conversations, the task of DST becomes more challenging as the\nlength of a dialogue increases due to the injection of more distracting\ncontexts. In this paper, we aim to improve the overall performance of DST with\na special focus on handling longer dialogues. We tackle this problem from three\nperspectives: 1) A model designed to enable hierarchical slot status\nprediction; 2) Balanced training procedure for generic and task-specific\nlanguage understanding; 3) Data perturbation which enhances the model's ability\nin handling longer conversations. We conduct experiments on the MultiWOZ\nbenchmark, and demonstrate the effectiveness of each component via a set of\nablation tests, especially on longer conversations.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 02:44:28 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 21:19:59 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Zhang", "Ye", ""], ["Cao", "Yuan", ""], ["Mahdieh", "Mahdis", ""], ["Zhao", "Jeffrey", ""], ["Wu", "Yonghui", ""]]}, {"id": "2103.00112", "submitter": "Kai Han", "authors": "Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, Yunhe Wang", "title": "Transformer in Transformer", "comments": "PyTorch code is available at\n  https://github.com/huawei-noah/CV-Backbones/tree/master/tnt_pytorch", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer is a new kind of neural architecture which encodes the input data\nas powerful features via the attention mechanism. Basically, the visual\ntransformers first divide the input images into several local patches and then\ncalculate both representations and their relationship. Since natural images are\nof high complexity with abundant detail and color information, the granularity\nof the patch dividing is not fine enough for excavating features of objects in\ndifferent scales and locations. In this paper, we point out that the attention\ninside these local patches are also essential for building visual transformers\nwith high performance and we explore a new architecture, namely, Transformer iN\nTransformer (TNT). Specifically, we regard the local patches (e.g.,\n16$\\times$16) as \"visual sentences\" and present to further divide them into\nsmaller patches (e.g., 4$\\times$4) as \"visual words\". The attention of each\nword will be calculated with other words in the given visual sentence with\nnegligible computational costs. Features of both words and sentences will be\naggregated to enhance the representation ability. Experiments on several\nbenchmarks demonstrate the effectiveness of the proposed TNT architecture,\ne.g., we achieve an $81.5%$ top-1 accuracy on the ImageNet, which is about\n$1.7%$ higher than that of the state-of-the-art visual transformer with similar\ncomputational cost. The PyTorch code is available at\nhttps://github.com/huawei-noah/CV-Backbones/tree/master/tnt_pytorch, and the\nMindSpore code is at\nhttps://gitee.com/mindspore/mindspore/tree/master/model_zoo/research/cv/TNT.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 03:12:16 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 03:31:05 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Han", "Kai", ""], ["Xiao", "An", ""], ["Wu", "Enhua", ""], ["Guo", "Jianyuan", ""], ["Xu", "Chunjing", ""], ["Wang", "Yunhe", ""]]}, {"id": "2103.00124", "submitter": "Muhammad Usman", "authors": "Muhammad Usman, Yannic Noller, Corina Pasareanu, Youcheng Sun, Divya\n  Gopinath", "title": "NEUROSPF: A tool for the Symbolic Analysis of Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents NEUROSPF, a tool for the symbolic analysis of neural\nnetworks. Given a trained neural network model, the tool extracts the\narchitecture and model parameters and translates them into a Java\nrepresentation that is amenable for analysis using the Symbolic PathFinder\nsymbolic execution tool. Notably, NEUROSPF encodes specialized peer classes for\nparsing the model's parameters, thereby enabling efficient analysis. With\nNEUROSPF the user has the flexibility to specify either the inputs or the\nnetwork internal parameters as symbolic, promoting the application of program\nanalysis and testing approaches from software engineering to the field of\nmachine learning. For instance, NEUROSPF can be used for coverage-based testing\nand test generation, finding adversarial examples and also constraint-based\nrepair of neural networks, thus improving the reliability of neural networks\nand of the applications that use them. Video URL: https://youtu.be/seal8fG78LI\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 04:28:11 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Usman", "Muhammad", ""], ["Noller", "Yannic", ""], ["Pasareanu", "Corina", ""], ["Sun", "Youcheng", ""], ["Gopinath", "Divya", ""]]}, {"id": "2103.00125", "submitter": "Nitin Jonathan Myers", "authors": "Yuyang Wang, Nitin Jonathan Myers, Nuria Gonz\\'alez-Prelcic, Robert W.\n  Heath Jr", "title": "Deep Learning-based Compressive Beam Alignment in mmWave Vehicular\n  Systems", "comments": "Submitted to the IEEE Transactions on Wireless Communications.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AI cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Millimeter wave vehicular channels exhibit structure that can be exploited\nfor beam alignment with fewer channel measurements compared to exhaustive beam\nsearch. With fixed layouts of roadside buildings and regular vehicular moving\ntrajectory, the dominant path directions of channels will likely be among a\nsubset of beam directions instead of distributing randomly over the whole\nbeamspace. In this paper, we propose a deep learning-based technique to design\na structured compressed sensing (CS) matrix that is well suited to the\nunderlying channel distribution for mmWave vehicular beam alignment. The\nproposed approach leverages both sparsity and the particular spatial structure\nthat appears in vehicular channels. We model the compressive channel\nacquisition by a two-dimensional (2D) convolutional layer followed by dropout.\nWe design fully-connected layers to optimize channel acquisition and beam\nalignment. We incorporate the low-resolution phase shifter constraint during\nneural network training by using projected gradient descent for weight updates.\nFurthermore, we exploit channel spectral structure to optimize the power\nallocated for different subcarriers. Simulations indicate that our deep\nlearning-based approach achieves better beam alignment than standard CS\ntechniques which use random phase shift-based design. Numerical experiments\nalso show that one single subcarrier is sufficient to provide necessary\ninformation for beam alignment.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 04:38:12 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Wang", "Yuyang", ""], ["Myers", "Nitin Jonathan", ""], ["Gonz\u00e1lez-Prelcic", "Nuria", ""], ["Heath", "Robert W.", "Jr"]]}, {"id": "2103.00137", "submitter": "Debmalya Mandal", "authors": "Debmalya Mandal, Sourav Medya, Brian Uzzi, and Charu Aggarwal", "title": "Meta-Learning with Graph Neural Networks: Methods and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph Neural Networks (GNNs), a generalization of deep neural networks on\ngraph data have been widely used in various domains, ranging from drug\ndiscovery to recommender systems. However, GNNs on such applications are\nlimited when there are few available samples. Meta-learning has been an\nimportant framework to address the lack of samples in machine learning, and in\nrecent years, researchers have started to apply meta-learning to GNNs. In this\nwork, we provide a comprehensive survey of different meta-learning approaches\ninvolving GNNs on various graph problems showing the power of using these two\napproaches together. We categorize the literature based on proposed\narchitectures, shared representations, and applications. Finally, we discuss\nseveral exciting future research directions and open problems.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 06:19:11 GMT"}, {"version": "v2", "created": "Sat, 10 Jul 2021 08:11:44 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Mandal", "Debmalya", ""], ["Medya", "Sourav", ""], ["Uzzi", "Brian", ""], ["Aggarwal", "Charu", ""]]}, {"id": "2103.00139", "submitter": "Mohammad-Ali Javidian", "authors": "Mohammad Ali Javidian, Om Pandey, Pooyan Jamshidi", "title": "Scalable Causal Transfer Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most important problems in transfer learning is the task of domain\nadaptation, where the goal is to apply an algorithm trained in one or more\nsource domains to a different (but related) target domain. This paper deals\nwith domain adaptation in the presence of covariate shift while there exist\ninvariances across domains. A main limitation of existing causal inference\nmethods for solving this problem is scalability. To overcome this difficulty,\nwe propose SCTL, an algorithm that avoids an exhaustive search and identifies\ninvariant causal features across the source and target domains based on Markov\nblanket discovery. SCTL does not require to have prior knowledge of the causal\nstructure, the type of interventions, or the intervention targets. There is an\nintrinsic locality associated with SCTL that makes SCTL practically scalable\nand robust because local causal discovery increases the power of computational\nindependence tests and makes the task of domain adaptation computationally\ntractable. We show the scalability and robustness of SCTL for domain adaptation\nusing synthetic and real data sets in low-dimensional and high-dimensional\nsettings.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 06:25:06 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Javidian", "Mohammad Ali", ""], ["Pandey", "Om", ""], ["Jamshidi", "Pooyan", ""]]}, {"id": "2103.00165", "submitter": "Zifeng Wang", "authors": "Zifeng Wang, Yifan Yang, Rui Wen, Xi Chen, Shao-Lun Huang, and Yefeng\n  Zheng", "title": "Lifelong Learning based Disease Diagnosis on Clinical Notes", "comments": "Pacific-Asia Conference on Knowledge Discovery and Data Mining\n  (PAKDD'21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current deep learning based disease diagnosis systems usually fall short in\ncatastrophic forgetting, i.e., directly fine-tuning the disease diagnosis model\non new tasks usually leads to abrupt decay of performance on previous tasks.\nWhat is worse, the trained diagnosis system would be fixed once deployed but\ncollecting training data that covers enough diseases is infeasible, which\ninspires us to develop a lifelong learning diagnosis system. In this work, we\npropose to adopt attention to combine medical entities and context, embedding\nepisodic memory and consolidation to retain knowledge, such that the learned\nmodel is capable of adapting to sequential disease-diagnosis tasks. Moreover,\nwe establish a new benchmark, named Jarvis-40, which contains clinical notes\ncollected from various hospitals. Our experiments show that the proposed method\ncan achieve state-of-the-art performance on the proposed benchmark.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 09:23:57 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 03:13:24 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Wang", "Zifeng", ""], ["Yang", "Yifan", ""], ["Wen", "Rui", ""], ["Chen", "Xi", ""], ["Huang", "Shao-Lun", ""], ["Zheng", "Yefeng", ""]]}, {"id": "2103.00167", "submitter": "Dirk Fahland", "authors": "Dirk Fahland, Vadim Denisov, Wil. M.P. van der Aalst", "title": "Inferring Unobserved Events in Systems With Shared Resources and Queues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.FL cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To identify the causes of performance problems or to predict process\nbehavior, it is essential to have correct and complete event data. This is\nparticularly important for distributed systems with shared resources, e.g., one\ncase can block another case competing for the same machine, leading to\ninter-case dependencies in performance. However, due to a variety of reasons,\nreal-life systems often record only a subset of all events taking place. For\nexample, to reduce costs, the number of sensors is minimized or parts of the\nsystem are not connected. To understand and analyze the behavior of processes\nwith shared resources, we aim to reconstruct bounds for timestamps of events\nthat must have happened but were not recorded. We present a novel approach that\ndecomposes system runs into entity traces of cases and resources that may need\nto synchronize in the presence of many-to-many relationships. Such\nrelationships occur, for example, in warehouses where packages for N incoming\norders are not handled in a single delivery but in M different deliveries. We\nuse linear programming over entity traces to derive the timestamps of\nunobserved events in an efficient manner. This helps to complete the event logs\nand facilitates analysis. We focus on material handling systems like baggage\nhandling systems in airports to illustrate our approach. However, the approach\ncan be applied to other settings where recording is incomplete. The ideas have\nbeen implemented in ProM and were evaluated using both synthetic and real-life\nevent logs.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 09:34:01 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Fahland", "Dirk", ""], ["Denisov", "Vadim", ""], ["van der Aalst", "Wil. M. P.", ""]]}, {"id": "2103.00172", "submitter": "Abubakr Awad", "authors": "Abubakr Awad, Wei Pang, David Lusseau, George M. Coghill", "title": "A Survey on Physarum Polycephalum Intelligent Foraging Behaviour and\n  Bio-Inspired Applications", "comments": "arXiv admin note: text overlap with arXiv:1712.02910 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In recent years, research on Physarum polycephalum has become more popular\nafter Nakagaki et al. (2000) performed their famous experiment showing that\nPhysarum was able to find the shortest route through a maze. Subsequent\nresearches have confirmed the ability of Physarum-inspired algorithms to solve\na wide range of NP-hard problems. In contrast to previous reviews that either\nfocus on biological aspects or bio-inspired applications, here we present a\ncomprehensive review that highlights recent Physarum polycephalum biological\naspects, mathematical models, and Physarum bio-inspired algorithms and their\napplications. The novelty of this review stems from our exploration of Physarum\nintelligent behaviour in competition settings. Further, we have presented our\nnew model to simulate Physarum in competition, where multiple Physarum interact\nwith each other and with their environments. The bio-inspired Physarum in\ncompetition algorithms proved to have great potentials for future research.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 10:19:41 GMT"}, {"version": "v2", "created": "Sun, 7 Mar 2021 10:49:13 GMT"}, {"version": "v3", "created": "Sat, 8 May 2021 10:22:14 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Awad", "Abubakr", ""], ["Pang", "Wei", ""], ["Lusseau", "David", ""], ["Coghill", "George M.", ""]]}, {"id": "2103.00180", "submitter": "Tirtharaj Dash", "authors": "Tirtharaj Dash, Sharad Chitlangia, Aditya Ahuja, Ashwin Srinivasan", "title": "Incorporating Domain Knowledge into Deep Neural Networks", "comments": "Submitted to IJCAI-2021 Survey Track (6+2 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a survey of ways in which domain-knowledge has been included when\nconstructing models with neural networks. The inclusion of domain-knowledge is\nof special interest not just to constructing scientific assistants, but also,\nmany other areas that involve understanding data using human-machine\ncollaboration. In many such instances, machine-based model construction may\nbenefit significantly from being provided with human-knowledge of the domain\nencoded in a sufficiently precise form. This paper examines two broad\napproaches to encode such knowledge--as logical and numerical constraints--and\ndescribes techniques and results obtained in several sub-categories under each\nof these approaches.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 10:39:43 GMT"}, {"version": "v2", "created": "Mon, 15 Mar 2021 17:47:22 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Dash", "Tirtharaj", ""], ["Chitlangia", "Sharad", ""], ["Ahuja", "Aditya", ""], ["Srinivasan", "Ashwin", ""]]}, {"id": "2103.00187", "submitter": "Michael Walton", "authors": "Michael Walton, Viliam Lisy", "title": "Multi-agent Reinforcement Learning in OpenSpiel: A Reproduction Report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this report, we present results reproductions for several core algorithms\nimplemented in the OpenSpiel framework for learning in games. The primary\ncontribution of this work is a validation of OpenSpiel's re-implemented search\nand Reinforcement Learning algorithms against the results reported in their\nrespective originating works. Additionally, we provide complete documentation\nof hyperparameters and source code required to reproduce these experiments\neasily and exactly.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 11:16:09 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 03:41:22 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Walton", "Michael", ""], ["Lisy", "Viliam", ""]]}, {"id": "2103.00200", "submitter": "Wenrui Gan", "authors": "Wenrui Gan, Zhulin Liu, C. L. Philip Chen, Tong Zhang", "title": "Siamese Labels Auxiliary Network(SiLaNet)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Auxiliary information attracts more and more attention in the area of machine\nlearning. Attempts so far to include such auxiliary information in\nstate-of-the-art learning process have often been based on simply appending\nthese auxiliary features to the data level or feature level. In this paper, we\nintend to propose a novel training method with new options and architectures.\nSiamese labels, which were used in the training phase as auxiliary modules.\nWhile in the testing phase, the auxiliary module should be removed. Siamese\nlabel module makes it easier to train and improves the performance in testing\nprocess. In general, the main contributions can be summarized as, 1) Siamese\nLabels are firstly proposed as auxiliary information to improve the learning\nefficiency; 2) We establish a new architecture, Siamese Labels Auxiliary\nNetwork (SilaNet), which is to assist the training of the model; 3) Siamese\nLabels Auxiliary Network is applied to compress the model parameters by 50% and\nensure the high accuracy at the same time. For the purpose of comparison, we\ntested the network on CIFAR-10 and CIFAR100 using some common models. The\nproposed SilaNet performs excellent efficiency both on the accuracy and\nrobustness.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 12:07:30 GMT"}, {"version": "v2", "created": "Sat, 6 Mar 2021 13:26:46 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Gan", "Wenrui", ""], ["Liu", "Zhulin", ""], ["Chen", "C. L. Philip", ""], ["Zhang", "Tong", ""]]}, {"id": "2103.00252", "submitter": "Harsh Agarwal", "authors": "Harsh Agarwal, Navyata Sanghvi, Vivek Roy, Kris Kitani", "title": "DeepBLE: Generalizing RSSI-based Localization Across Different Devices", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Accurate smartphone localization (< 1-meter error) for indoor navigation\nusing only RSSI received from a set of BLE beacons remains a challenging\nproblem, due to the inherent noise of RSSI measurements. To overcome the large\nvariance in RSSI measurements, we propose a data-driven approach that uses a\ndeep recurrent network, DeepBLE, to localize the smartphone using RSSI measured\nfrom multiple beacons in an environment. In particular, we focus on the ability\nof our approach to generalize across many smartphone brands (e.g., Apple,\nSamsung) and models (e.g., iPhone 8, S10). Towards this end, we collect a\nlarge-scale dataset of 15 hours of smartphone data, which consists of over\n50,000 BLE beacon RSSI measurements collected from 47 beacons in a single\nbuilding using 15 different popular smartphone models, along with precise 2D\nlocation annotations. Our experiments show that there is a very high\nvariability of RSSI measurements across smartphone models (especially across\nbrand), making it very difficult to apply supervised learning using only a\nsubset of smartphone models. To address this challenge, we propose a novel\nstatistic similarity loss (SSL) which enables our model to generalize to unseen\nphones using a semi-supervised learning approach. For known phones, the iPhone\nXR achieves the best mean distance error of 0.84 meters. For unknown phones,\nthe Huawei Mate20 Pro shows the greatest improvement, cutting error by over\n38\\% from 2.62 meters to 1.63 meters error using our semi-supervised adaptation\nmethod.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 15:44:41 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Agarwal", "Harsh", ""], ["Sanghvi", "Navyata", ""], ["Roy", "Vivek", ""], ["Kitani", "Kris", ""]]}, {"id": "2103.00255", "submitter": "Armin Goudarzi", "authors": "Armin Goudarzi, Carsten SPehr, Steffen Herbold", "title": "Expert decision support system for aeroacoustic classification", "comments": "Preprint for JASA special issue on machine learning in acoustics", "journal-ref": null, "doi": "10.13140/RG.2.2.33496.42249", "report-no": null, "categories": "cs.SD cs.AI cs.LG eess.AS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper presents an expert decision support system for time-invariant\naeroacoustic source classification. The system comprises two steps: first, the\ncalculation of acoustic properties based on spectral and spatial information;\nand second, the clustering of the sources based on these properties. Example\ndata of two scaled airframe half-model wind tunnel measurements is evaluated\nbased on deconvolved beamforming maps. A variety of aeroacoustic features are\nproposed that capture the characteristics and properties of the spectra. These\nfeatures represent aeroacoustic properties that can be interpreted by both the\nmachine and experts. The features are independent of absolute flow parameters\nsuch as the observed Mach numbers. This enables the proposed method to analyze\ndata which is measured at different flow configurations. The aeroacoustic\nsources are clustered based on these features to determine similar or atypical\nbehavior. For the given example data, the method results in source type\nclusters that correspond to human expert classification of the source types.\nCombined with a classification confidence and the mean feature values for each\ncluster, these clusters help aeroacoustic experts in classifying the identified\nsources and support them in analyzing their typical behavior and identifying\nspurious sources in-situ during measurement campaigns.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 15:47:59 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Goudarzi", "Armin", ""], ["SPehr", "Carsten", ""], ["Herbold", "Steffen", ""]]}, {"id": "2103.00262", "submitter": "Claudio Mura", "authors": "Claudio Mura, Renato Pajarola, Konrad Schindler, Niloy Mitra", "title": "Walk2Map: Extracting Floor Plans from Indoor Walk Trajectories", "comments": "To be published in Computer Graphics Forum (Proc. Eurographics 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen a proliferation of new digital products for the\nefficient management of indoor spaces, with important applications like\nemergency management, virtual property showcasing and interior design. These\nproducts rely on accurate 3D models of the environments considered, including\ninformation on both architectural and non-permanent elements. These models must\nbe created from measured data such as RGB-D images or 3D point clouds, whose\ncapture and consolidation involves lengthy data workflows. This strongly limits\nthe rate at which 3D models can be produced, preventing the adoption of many\ndigital services for indoor space management. We provide an alternative to such\ndata-intensive procedures by presenting Walk2Map, a data-driven approach to\ngenerate floor plans only from trajectories of a person walking inside the\nrooms. Thanks to recent advances in data-driven inertial odometry, such\nminimalistic input data can be acquired from the IMU readings of consumer-level\nsmartphones, which allows for an effortless and scalable mapping of real-world\nindoor spaces. Our work is based on learning the latent relation between an\nindoor walk trajectory and the information represented in a floor plan:\ninterior space footprint, portals, and furniture. We distinguish between\nrecovering area-related (interior footprint, furniture) and wall-related\n(doors) information and use two different neural architectures for the two\ntasks: an image-based Encoder-Decoder and a Graph Convolutional Network,\nrespectively. We train our networks using scanned 3D indoor models and apply\nthem in a cascaded fashion on an indoor walk trajectory at inference time. We\nperform a qualitative and quantitative evaluation using both simulated and\nmeasured, real-world trajectories, and compare against a baseline method for\nimage-to-image translation. The experiments confirm the feasibility of our\napproach.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 16:29:09 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Mura", "Claudio", ""], ["Pajarola", "Renato", ""], ["Schindler", "Konrad", ""], ["Mitra", "Niloy", ""]]}, {"id": "2103.00331", "submitter": "Daniela Kuinchtner", "authors": "Daniela Kuinchtner, Afonso Sales, Felipe Meneguzzi", "title": "CP-MDP: A CANDECOMP-PARAFAC Decomposition Approach to Solve a Markov\n  Decision Process Multidimensional Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Markov Decision Process (MDP) is the underlying model for optimal planning\nfor decision-theoretic agents in stochastic environments. Although much\nresearch focuses on solving MDP problems both in tabular form or using factored\nrepresentations, none focused on tensor decomposition methods. Solving MDPs\nusing tensor algebra offers the prospect of leveraging advances in tensor-based\ncomputations to further increase solver efficiency. In this paper, we develop\nan MDP solver for a multidimensional problem using a tensor decomposition\nmethod to compress the transition models and optimize the value iteration and\npolicy iteration algorithms. We empirically evaluate our approach against\ntabular methods and show our approach can compute much larger problems using\nsubstantially less memory, opening up new possibilities for tensor-based\napproaches in stochastic planning\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 21:33:19 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Kuinchtner", "Daniela", ""], ["Sales", "Afonso", ""], ["Meneguzzi", "Felipe", ""]]}, {"id": "2103.00334", "submitter": "Ziyun Yang", "authors": "Ziyun Yang, Somayyeh Soltanian-Zadeh, Sina Farsiu", "title": "BiconNet: An Edge-preserved Connectivity-based Approach for Salient\n  Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Salient object detection (SOD) is viewed as a pixel-wise saliency modeling\ntask by traditional deep learning-based methods. A limitation of current SOD\nmodels is insufficient utilization of inter-pixel information, which usually\nresults in imperfect segmentation near edge regions and low spatial coherence.\nAs we demonstrate, using a saliency mask as the only label is suboptimal. To\naddress this limitation, we propose a connectivity-based approach called\nbilateral connectivity network (BiconNet), which uses connectivity masks\ntogether with saliency masks as labels for effective modeling of inter-pixel\nrelationships and object saliency. Moreover, we propose a bilateral voting\nmodule to enhance the output connectivity map, and a novel edge feature\nenhancement method that efficiently utilizes edge-specific features. Through\ncomprehensive experiments on five benchmark datasets, we demonstrate that our\nproposed method can be plugged into any existing state-of-the-art\nsaliency-based SOD framework to improve its performance with negligible\nparameter increase.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 21:39:04 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 01:38:56 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Yang", "Ziyun", ""], ["Soltanian-Zadeh", "Somayyeh", ""], ["Farsiu", "Sina", ""]]}, {"id": "2103.00336", "submitter": "Alex Lamb", "authors": "Alex Lamb, Di He, Anirudh Goyal, Guolin Ke, Chien-Feng Liao, Mirco\n  Ravanelli, Yoshua Bengio", "title": "Transformers with Competitive Ensembles of Independent Mechanisms", "comments": "Under Review, ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An important development in deep learning from the earliest MLPs has been a\nmove towards architectures with structural inductive biases which enable the\nmodel to keep distinct sources of information and routes of processing\nwell-separated. This structure is linked to the notion of independent\nmechanisms from the causality literature, in which a mechanism is able to\nretain the same processing as irrelevant aspects of the world are changed. For\nexample, convnets enable separation over positions, while attention-based\narchitectures (especially Transformers) learn which combination of positions to\nprocess dynamically. In this work we explore a way in which the Transformer\narchitecture is deficient: it represents each position with a large monolithic\nhidden representation and a single set of parameters which are applied over the\nentire hidden representation. This potentially throws unrelated sources of\ninformation together, and limits the Transformer's ability to capture\nindependent mechanisms. To address this, we propose Transformers with\nIndependent Mechanisms (TIM), a new Transformer layer which divides the hidden\nrepresentation and parameters into multiple mechanisms, which only exchange\ninformation through attention. Additionally, we propose a competition mechanism\nwhich encourages these mechanisms to specialize over time steps, and thus be\nmore independent. We study TIM on a large-scale BERT model, on the Image\nTransformer, and on speech enhancement and find evidence for semantically\nmeaningful specialization as well as improved performance.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 21:48:46 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Lamb", "Alex", ""], ["He", "Di", ""], ["Goyal", "Anirudh", ""], ["Ke", "Guolin", ""], ["Liao", "Chien-Feng", ""], ["Ravanelli", "Mirco", ""], ["Bengio", "Yoshua", ""]]}, {"id": "2103.00342", "submitter": "Raouf Kerkouche", "authors": "Raouf Kerkouche and Gergely \\'Acs and Claude Castelluccia and Pierre\n  Genev\\`es", "title": "Constrained Differentially Private Federated Learning for Low-bandwidth\n  Devices", "comments": "arXiv admin note: text overlap with arXiv:2011.05578", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning becomes a prominent approach when different entities want\nto learn collaboratively a common model without sharing their training data.\nHowever, Federated learning has two main drawbacks. First, it is quite\nbandwidth inefficient as it involves a lot of message exchanges between the\naggregating server and the participating entities. This bandwidth and\ncorresponding processing costs could be prohibitive if the participating\nentities are, for example, mobile devices. Furthermore, although federated\nlearning improves privacy by not sharing data, recent attacks have shown that\nit still leaks information about the training data. This paper presents a novel\nprivacy-preserving federated learning scheme. The proposed scheme provides\ntheoretical privacy guarantees, as it is based on Differential Privacy.\nFurthermore, it optimizes the model accuracy by constraining the model learning\nphase on few selected weights. Finally, as shown experimentally, it reduces the\nupstream and downstream bandwidth by up to 99.9% compared to standard federated\nlearning, making it practical for mobile systems.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 22:25:06 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Kerkouche", "Raouf", ""], ["\u00c1cs", "Gergely", ""], ["Castelluccia", "Claude", ""], ["Genev\u00e8s", "Pierre", ""]]}, {"id": "2103.00355", "submitter": "Weixiao Gao", "authors": "Weixiao Gao, Liangliang Nan, Bas Boom, Hugo Ledoux", "title": "SUM: A Benchmark Dataset of Semantic Urban Meshes", "comments": "27 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent developments in data acquisition technology allow us to collect 3D\ntexture meshes quickly. Those can help us understand and analyse the urban\nenvironment, and as a consequence are useful for several applications like\nspatial analysis and urban planning. Semantic segmentation of texture meshes\nthrough deep learning methods can enhance this understanding, but it requires a\nlot of labelled data. The contributions of this work are threefold: (1) a new\nbenchmark dataset of semantic urban meshes, (2) a novel semi-automatic\nannotation framework, and (3) an annotation tool for 3D meshes. In particular,\nour dataset covers about 4 km2 in Helsinki (Finland), with six classes, and we\nestimate that we save about 600 hours of labelling work using our annotation\nframework, which includes initial segmentation and interactive refinement. We\nalso compare the performance of several state-of-theart 3D semantic\nsegmentation methods on the new benchmark dataset. Other researchers can use\nour results to train their networks: the dataset is publicly available, and the\nannotation tool is released as open-source.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 23:26:21 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 14:25:37 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Gao", "Weixiao", ""], ["Nan", "Liangliang", ""], ["Boom", "Bas", ""], ["Ledoux", "Hugo", ""]]}, {"id": "2103.00363", "submitter": "Guoyang Xie", "authors": "Guoyang Xie, Jinbao Wang, Guo Yu, Feng Zheng, Yaochu Jin", "title": "Tiny Adversarial Mulit-Objective Oneshot Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to limited computational cost and energy consumption, most neural network\nmodels deployed in mobile devices are tiny. However, tiny neural networks are\ncommonly very vulnerable to attacks. Current research has proved that larger\nmodel size can improve robustness, but little research focuses on how to\nenhance the robustness of tiny neural networks. Our work focuses on how to\nimprove the robustness of tiny neural networks without seriously deteriorating\nof clean accuracy under mobile-level resources. To this end, we propose a\nmulti-objective oneshot network architecture search (NAS) algorithm to obtain\nthe best trade-off networks in terms of the adversarial accuracy, the clean\naccuracy and the model size. Specifically, we design a novel search space based\non new tiny blocks and channels to balance model size and adversarial\nperformance. Moreover, since the supernet significantly affects the performance\nof subnets in our NAS algorithm, we reveal the insights into how the supernet\nhelps to obtain the best subnet under white-box adversarial attacks.\nConcretely, we explore a new adversarial training paradigm by analyzing the\nadversarial transferability, the width of the supernet and the difference\nbetween training the subnets from scratch and fine-tuning. Finally, we make a\nstatistical analysis for the layer-wise combination of certain blocks and\nchannels on the first non-dominated front, which can serve as a guideline to\ndesign tiny neural network architectures for the resilience of adversarial\nperturbations.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 00:54:09 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Xie", "Guoyang", ""], ["Wang", "Jinbao", ""], ["Yu", "Guo", ""], ["Zheng", "Feng", ""], ["Jin", "Yaochu", ""]]}, {"id": "2103.00364", "submitter": "Rohan Shad", "authors": "Rohan Shad, Nicolas Quach, Robyn Fong, Patpilai Kasinpila, Cayley\n  Bowles, Miguel Castro, Ashrith Guha, Eddie Suarez, Stefan Jovinge, Sangjin\n  Lee, Theodore Boeve, Myriam Amsallem, Xiu Tang, Francois Haddad, Yasuhiro\n  Shudo, Y. Joseph Woo, Jeffrey Teuteberg, John P. Cunningham, Curt P.\n  Langlotz, William Hiesinger", "title": "Predicting post-operative right ventricular failure using video-based\n  deep learning", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-invasive and cost effective in nature, the echocardiogram allows for a\ncomprehensive assessment of the cardiac musculature and valves. Despite\nprogressive improvements over the decades, the rich temporally resolved data in\nechocardiography videos remain underutilized. Human reads of echocardiograms\nreduce the complex patterns of cardiac wall motion, to a small list of\nmeasurements of heart function. Furthermore, all modern echocardiography\nartificial intelligence (AI) systems are similarly limited by design -\nautomating measurements of the same reductionist metrics rather than utilizing\nthe wealth of data embedded within each echo study. This underutilization is\nmost evident in situations where clinical decision making is guided by\nsubjective assessments of disease acuity, and tools that predict disease onset\nwithin clinically actionable timeframes are unavailable. Predicting the\nlikelihood of developing post-operative right ventricular failure (RV failure)\nin the setting of mechanical circulatory support is one such clinical example.\nTo address this, we developed a novel video AI system trained to predict\npost-operative right ventricular failure (RV failure), using the full\nspatiotemporal density of information from pre-operative echocardiography\nscans. We achieve an AUC of 0.729, specificity of 52% at 80% sensitivity and\n46% sensitivity at 80% specificity. Furthermore, we show that our ML system\nsignificantly outperforms a team of human experts tasked with predicting RV\nfailure on independent clinical evaluation. Finally, the methods we describe\nare generalizable to any cardiac clinical decision support application where\ntreatment or patient selection is guided by qualitative echocardiography\nassessments.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 00:58:53 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Shad", "Rohan", ""], ["Quach", "Nicolas", ""], ["Fong", "Robyn", ""], ["Kasinpila", "Patpilai", ""], ["Bowles", "Cayley", ""], ["Castro", "Miguel", ""], ["Guha", "Ashrith", ""], ["Suarez", "Eddie", ""], ["Jovinge", "Stefan", ""], ["Lee", "Sangjin", ""], ["Boeve", "Theodore", ""], ["Amsallem", "Myriam", ""], ["Tang", "Xiu", ""], ["Haddad", "Francois", ""], ["Shudo", "Yasuhiro", ""], ["Woo", "Y. Joseph", ""], ["Teuteberg", "Jeffrey", ""], ["Cunningham", "John P.", ""], ["Langlotz", "Curt P.", ""], ["Hiesinger", "William", ""]]}, {"id": "2103.00369", "submitter": "Muhammad Umar Karim Khan", "authors": "Muhammad Umar Karim Khan", "title": "Towards Continual, Online, Unsupervised Depth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although depth extraction with passive sensors has seen remarkable\nimprovement with deep learning, these approaches may fail to obtain correct\ndepth if they are exposed to environments not observed during training. Online\nadaptation, where the neural network trains while deployed, with unsupervised\nlearning provides a convenient solution. However, online adaptation causes a\nneural network to forget the past. Thus, past training is wasted and the\nnetwork is not able to provide good results if it observes past scenes. This\nwork deals with practical online-adaptation where the input is online and\ntemporally-correlated, and training is completely unsupervised. Regularization\nand replay-based methods without task boundaries are proposed to avoid\ncatastrophic forgetting while adapting to online data. Experiments are\nperformed on different datasets with both structure-from-motion and stereo.\nResults of forgetting as well as adaptation are provided, which are superior to\nrecent methods. The proposed approach is more inline with the artificial\ngeneral intelligence paradigm as the neural network learns the scene where it\nis deployed without any supervision (target labels and tasks) and without\nforgetting about the past. Code is available at github.com/umarKarim/cou_stereo\nand github.com/umarKarim/cou_sfm.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 01:18:49 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 02:36:44 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Khan", "Muhammad Umar Karim", ""]]}, {"id": "2103.00375", "submitter": "Chen Wang", "authors": "Chen Wang, Rui Wang, Danfei Xu, Ajay Mandlekar, Li Fei-Fei, Silvio\n  Savarese", "title": "Generalization Through Hand-Eye Coordination: An Action Space for\n  Learning Spatially-Invariant Visuomotor Control", "comments": "First two authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imitation Learning (IL) is an effective framework to learn visuomotor skills\nfrom offline demonstration data. However, IL methods often fail to generalize\nto new scene configurations not covered by training data. On the other hand,\nhumans can manipulate objects in varying conditions. Key to such capability is\nhand-eye coordination, a cognitive ability that enables humans to adaptively\ndirect their movements at task-relevant objects and be invariant to the\nobjects' absolute spatial location. In this work, we present a learnable action\nspace, Hand-eye Action Networks (HAN), that can approximate human's hand-eye\ncoordination behaviors by learning from human teleoperated demonstrations.\nThrough a set of challenging multi-stage manipulation tasks, we show that a\nvisuomotor policy equipped with HAN is able to inherit the key spatial\ninvariance property of hand-eye coordination and achieve zero-shot\ngeneralization to new scene configurations. Additional materials available at\nhttps://sites.google.com/stanford.edu/han\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 01:49:13 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Wang", "Chen", ""], ["Wang", "Rui", ""], ["Xu", "Danfei", ""], ["Mandlekar", "Ajay", ""], ["Fei-Fei", "Li", ""], ["Savarese", "Silvio", ""]]}, {"id": "2103.00378", "submitter": "Shuai Yang", "authors": "Shuai Yang, Hao Wang, Kui Yu, Fuyuan Cao, and Xindong Wu", "title": "Towards Efficient Local Causal Structure Learning", "comments": "Published on IEEE Transactions on Big Data (TBD). arXiv admin note:\n  text overlap with arXiv:1910.01288", "journal-ref": null, "doi": "10.1109/TBDATA.2021.3062937", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Local causal structure learning aims to discover and distinguish direct\ncauses (parents) and direct effects (children) of a variable of interest from\ndata. While emerging successes have been made, existing methods need to search\na large space to distinguish direct causes from direct effects of a target\nvariable T. To tackle this issue, we propose a novel Efficient Local Causal\nStructure learning algorithm, named ELCS. Specifically, we first propose the\nconcept of N-structures, then design an efficient Markov Blanket (MB) discovery\nsubroutine to integrate MB learning with N-structures to learn the MB of T and\nsimultaneously distinguish direct causes from direct effects of T. With the\nproposed MB subroutine, ELCS starts from the target variable, sequentially\nfinds MBs of variables connected to the target variable and simultaneously\nconstructs local causal structures over MBs until the direct causes and direct\neffects of the target variable have been distinguished. Using eight Bayesian\nnetworks the extensive experiments have validated that ELCS achieves better\naccuracy and efficiency than the state-of-the-art algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 02:50:48 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Yang", "Shuai", ""], ["Wang", "Hao", ""], ["Yu", "Kui", ""], ["Cao", "Fuyuan", ""], ["Wu", "Xindong", ""]]}, {"id": "2103.00384", "submitter": "Shaojie Tang", "authors": "Shaojie Tang, Jing Yuan", "title": "Adaptive Regularized Submodular Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the problem of maximizing the difference between an\nadaptive submodular (revenue) function and an non-negative modular (cost)\nfunction under the adaptive setting. The input of our problem is a set of $n$\nitems, where each item has a particular state drawn from some known prior\ndistribution $p$. The revenue function $g$ is defined over items and states,\nand the cost function $c$ is defined over items, i.e., each item has a fixed\ncost. The state of each item is unknown initially, one must select an item in\norder to observe its realized state. A policy $\\pi$ specifies which item to\npick next based on the observations made so far. Denote by $g_{avg}(\\pi)$ the\nexpected revenue of $\\pi$ and let $c_{avg}(\\pi)$ denote the expected cost of\n$\\pi$. Our objective is to identify the best policy $\\pi^o\\in\n\\arg\\max_{\\pi}g_{avg}(\\pi)-c_{avg}(\\pi)$ under a $k$-cardinality constraint.\nSince our objective function can take on both negative and positive values, the\nexisting results of submodular maximization may not be applicable. To overcome\nthis challenge, we develop a series of effective solutions with performance\ngrantees. Let $\\pi^o$ denote the optimal policy. For the case when $g$ is\nadaptive monotone and adaptive submodular, we develop an effective policy\n$\\pi^l$ such that $g_{avg}(\\pi^l) - c_{avg}(\\pi^l) \\geq\n(1-\\frac{1}{e}-\\epsilon)g_{avg}(\\pi^o) - c_{avg}(\\pi^o)$, using only\n$O(n\\epsilon^{-2}\\log \\epsilon^{-1})$ value oracle queries. For the case when\n$g$ is adaptive submodular, we present a randomized policy $\\pi^r$ such that\n$g_{avg}(\\pi^r) - c_{avg}(\\pi^r) \\geq \\frac{1}{e}g_{avg}(\\pi^o) -\nc_{avg}(\\pi^o)$.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 03:31:12 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Tang", "Shaojie", ""], ["Yuan", "Jing", ""]]}, {"id": "2103.00397", "submitter": "Tianlong Chen", "authors": "Tianlong Chen, Yu Cheng, Zhe Gan, Jingjing Liu, Zhangyang Wang", "title": "Data-Efficient GAN Training Beyond (Just) Augmentations: A Lottery\n  Ticket Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training generative adversarial networks (GANs) with limited real image data\ngenerally results in deteriorated performance and collapsed models. To conquer\nthis challenge, we are inspired by the latest observations, that one can\ndiscover independently trainable and highly sparse subnetworks (a.k.a., lottery\ntickets) from GANs. Treating this as an inductive prior, we suggest a brand-new\nangle towards data-efficient GAN training: by first identifying the lottery\nticket from the original GAN using the small training set of real images; and\nthen focusing on training that sparse subnetwork by re-using the same set. Both\nsteps have lower complexity and are more data-efficient to train. We find our\ncoordinated framework to offer orthogonal gains to existing real image data\naugmentation methods, and we additionally offer a new feature-level\naugmentation that can be applied together with them. Comprehensive experiments\nendorse the effectiveness of our proposed framework, across various GAN\narchitectures (SNGAN, BigGAN, and StyleGAN-V2) and diverse datasets (CIFAR-10,\nCIFAR-100, Tiny-ImageNet, and ImageNet). Our training framework also displays\npowerful few-shot generalization ability, i.e., generating high-fidelity images\nby training from scratch with just 100 real images, without any pre-training.\nCodes are available at:\nhttps://github.com/VITA-Group/Ultra-Data-Efficient-GAN-Training.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 05:20:29 GMT"}, {"version": "v2", "created": "Sat, 29 May 2021 04:57:31 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Chen", "Tianlong", ""], ["Cheng", "Yu", ""], ["Gan", "Zhe", ""], ["Liu", "Jingjing", ""], ["Wang", "Zhangyang", ""]]}, {"id": "2103.00418", "submitter": "Francois Luus", "authors": "Francois Luus, Prithviraj Sen, Pavan Kapanipathi, Ryan Riegel,\n  Ndivhuwo Makondo, Thabang Lebese, Alexander Gray", "title": "Logic Embeddings for Complex Query Answering", "comments": "IBM Research", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answering logical queries over incomplete knowledge bases is challenging\nbecause: 1) it calls for implicit link prediction, and 2) brute force answering\nof existential first-order logic queries is exponential in the number of\nexistential variables. Recent work of query embeddings provides fast querying,\nbut most approaches model set logic with closed regions, so lack negation.\nQuery embeddings that do support negation use densities that suffer drawbacks:\n1) only improvise logic, 2) use expensive distributions, and 3) poorly model\nanswer uncertainty. In this paper, we propose Logic Embeddings, a new approach\nto embedding complex queries that uses Skolemisation to eliminate existential\nvariables for efficient querying. It supports negation, but improves on density\napproaches: 1) integrates well-studied t-norm logic and directly evaluates\nsatisfiability, 2) simplifies modeling with truth values, and 3) models\nuncertainty with truth bounds. Logic Embeddings are competitively fast and\naccurate in query answering over large, incomplete knowledge graphs, outperform\non negation queries, and in particular, provide improved modeling of answer\nuncertainty as evidenced by a superior correlation between answer set size and\nembedding entropy.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 07:52:37 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Luus", "Francois", ""], ["Sen", "Prithviraj", ""], ["Kapanipathi", "Pavan", ""], ["Riegel", "Ryan", ""], ["Makondo", "Ndivhuwo", ""], ["Lebese", "Thabang", ""], ["Gray", "Alexander", ""]]}, {"id": "2103.00424", "submitter": "Rachmad Vidya Wicaksana Putra", "authors": "Rachmad Vidya Wicaksana Putra, Muhammad Shafique", "title": "SpikeDyn: A Framework for Energy-Efficient Spiking Neural Networks with\n  Continual and Unsupervised Learning Capabilities in Dynamic Environments", "comments": "To appear at the 58th IEEE/ACM Design Automation Conference (DAC),\n  December 2021, San Francisco, CA, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spiking Neural Networks (SNNs) bear the potential of efficient unsupervised\nand continual learning capabilities because of their biological plausibility,\nbut their complexity still poses a serious research challenge to enable their\nenergy-efficient design for resource-constrained scenarios (like embedded\nsystems, IoT-Edge, etc.). We propose SpikeDyn, a comprehensive framework for\nenergy-efficient SNNs with continual and unsupervised learning capabilities in\ndynamic environments, for both the training and inference phases. It is\nachieved through the following multiple diverse mechanisms: 1) reduction of\nneuronal operations, by replacing the inhibitory neurons with direct lateral\ninhibitions; 2) a memory- and energy-constrained SNN model search algorithm\nthat employs analytical models to estimate the memory footprint and energy\nconsumption of different candidate SNN models and selects a Pareto-optimal SNN\nmodel; and 3) a lightweight continual and unsupervised learning algorithm that\nemploys adaptive learning rates, adaptive membrane threshold potential, weight\ndecay, and reduction of spurious updates. Our experimental results show that,\nfor a network with 400 excitatory neurons, our SpikeDyn reduces the energy\nconsumption on average by 51% for training and by 37% for inference, as\ncompared to the state-of-the-art. Due to the improved learning algorithm,\nSpikeDyn provides on avg. 21% accuracy improvement over the state-of-the-art,\nfor classifying the most recently learned task, and by 8% on average for the\npreviously learned tasks.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 08:26:23 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Putra", "Rachmad Vidya Wicaksana", ""], ["Shafique", "Muhammad", ""]]}, {"id": "2103.00442", "submitter": "Fei Sun", "authors": "Xu Xie, Fei Sun, Xiaoyong Yang, Zhao Yang, Jinyang Gao, Wenwu Ou, and\n  Bin Cui", "title": "Explore User Neighborhood for Real-time E-commerce Recommendation", "comments": "To appear in ICDE 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Recommender systems play a vital role in modern online services, such as\nAmazon and Taobao. Traditional personalized methods, which focus on user-item\n(UI) relations, have been widely applied in industrial settings, owing to their\nefficiency and effectiveness. Despite their success, we argue that these\napproaches ignore local information hidden in similar users. To tackle this\nproblem, user-based methods exploit similar user relations to make\nrecommendations in a local perspective. Nevertheless, traditional user-based\nmethods, like userKNN and matrix factorization, are intractable to be deployed\nin the real-time applications since such transductive models have to be\nrecomputed or retrained with any new interaction. To overcome this challenge,\nwe propose a framework called self-complementary collaborative filtering~(SCCF)\nwhich can make recommendations with both global and local information in real\ntime. On the one hand, it utilizes UI relations and user neighborhood to\ncapture both global and local information. On the other hand, it can identify\nsimilar users for each user in real time by inferring user representations on\nthe fly with an inductive model. The proposed framework can be seamlessly\nincorporated into existing inductive UI approach and benefit from user\nneighborhood with little additional computation. It is also the first attempt\nto apply user-based methods in real-time settings. The effectiveness and\nefficiency of SCCF are demonstrated through extensive offline experiments on\nfour public datasets, as well as a large scale online A/B test in Taobao.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 09:56:09 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Xie", "Xu", ""], ["Sun", "Fei", ""], ["Yang", "Xiaoyong", ""], ["Yang", "Zhao", ""], ["Gao", "Jinyang", ""], ["Ou", "Wenwu", ""], ["Cui", "Bin", ""]]}, {"id": "2103.00445", "submitter": "Oren Peer", "authors": "Oren Peer, Chen Tessler, Nadav Merlis, Ron Meir", "title": "Ensemble Bootstrapping for Q-Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Q-learning (QL), a common reinforcement learning algorithm, suffers from\nover-estimation bias due to the maximization term in the optimal Bellman\noperator. This bias may lead to sub-optimal behavior. Double-Q-learning tackles\nthis issue by utilizing two estimators, yet results in an under-estimation\nbias. Similar to over-estimation in Q-learning, in certain scenarios, the\nunder-estimation bias may degrade performance. In this work, we introduce a new\nbias-reduced algorithm called Ensemble Bootstrapped Q-Learning (EBQL), a\nnatural extension of Double-Q-learning to ensembles. We analyze our method both\ntheoretically and empirically. Theoretically, we prove that EBQL-like updates\nyield lower MSE when estimating the maximal mean of a set of independent random\nvariables. Empirically, we show that there exist domains where both over and\nunder-estimation result in sub-optimal performance. Finally, We demonstrate the\nsuperior performance of a deep RL variant of EBQL over other deep QL algorithms\nfor a suite of ATARI games.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 10:19:47 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 11:01:36 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Peer", "Oren", ""], ["Tessler", "Chen", ""], ["Merlis", "Nadav", ""], ["Meir", "Ron", ""]]}, {"id": "2103.00479", "submitter": "Kishlay Jha", "authors": "Kishlay Jha", "title": "Knowledge-Base Enriched Word Embeddings for Biomedical Domain", "comments": "Work in progress", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Word embeddings have been shown adept at capturing the semantic and syntactic\nregularities of the natural language text, as a result of which these\nrepresentations have found their utility in a wide variety of downstream\ncontent analysis tasks. Commonly, these word embedding techniques derive the\ndistributed representation of words based on the local context information.\nHowever, such approaches ignore the rich amount of explicit information present\nin knowledge-bases. This is problematic, as it might lead to poor\nrepresentation for words with insufficient local context such as domain\nspecific words. Furthermore, the problem becomes pronounced in domain such as\nbio-medicine where the presence of these domain specific words are relatively\nhigh. Towards this end, in this project, we propose a new word embedding based\nmodel for biomedical domain that jointly leverages the information from\navailable corpora and domain knowledge in order to generate knowledge-base\npowered embeddings. Unlike existing approaches, the proposed methodology is\nsimple but adept at capturing the precise knowledge available in domain\nresources in an accurate way. Experimental results on biomedical concept\nsimilarity and relatedness task validates the effectiveness of the proposed\napproach.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 18:18:51 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Jha", "Kishlay", ""]]}, {"id": "2103.00483", "submitter": "Chenyu Tian", "authors": "Chenyu Tian, Yuchun Zhang, Zefeng Weng, Xiusen Gu, Wai Kin Victor Chan", "title": "Learning Large-scale Location Embedding From Human Mobility Trajectories\n  with Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  An increasing amount of location-based service (LBS) data is being\naccumulated and helps to study urban dynamics and human mobility. GPS\ncoordinates and other location indicators are normally low dimensional and only\nrepresenting spatial proximity, thus difficult to be effectively utilized by\nmachine learning models in Geo-aware applications. Existing location embedding\nmethods are mostly tailored for specific problems that are taken place within\nareas of interest. When it comes to the scale of a city or even a country,\nexisting approaches always suffer from extensive computational cost and\nsignificant data sparsity. Different from existing studies, we propose to learn\nrepresentations through a GCN-aided skip-gram model named GCN-L2V by\nconsidering both spatial connection and human mobility. With a flow graph and a\nspatial graph, it embeds context information into vector representations.\nGCN-L2V is able to capture relationships among locations and provide a better\nnotion of similarity in a spatial environment. Across quantitative experiments\nand case studies, we empirically demonstrate that representations learned by\nGCN-L2V are effective. As far as we know, this is the first study that provides\na fine-grained location embedding at the city level using only LBS records.\nGCN-L2V is a general-purpose embedding model with high flexibility and can be\napplied in down-streaming Geo-aware applications.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2021 09:11:33 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 10:42:38 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Tian", "Chenyu", ""], ["Zhang", "Yuchun", ""], ["Weng", "Zefeng", ""], ["Gu", "Xiusen", ""], ["Chan", "Wai Kin Victor", ""]]}, {"id": "2103.00497", "submitter": "Aryan Asadian", "authors": "Aryan Asadian, Amirali Salehi-Abari", "title": "Distilling Knowledge via Intermediate Classifiers", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The crux of knowledge distillation is to effectively train a resource-limited\nstudent model with the guide of a pre-trained larger teacher model. However,\nwhen there is a large difference between the model complexities of teacher and\nstudent (i.e., capacity gap), knowledge distillation loses its strength in\ntransferring knowledge from the teacher to the student, thus training a weaker\nstudent. To mitigate the impact of the capacity gap, we introduce knowledge\ndistillation via intermediate heads. By extending the intermediate layers of\nthe teacher (at various depths) with classifier heads, we cheaply acquire a\ncohort of heterogeneous pre-trained teachers. The intermediate classifier heads\ncan all together be efficiently learned while freezing the backbone of the\npre-trained teacher. The cohort of teachers (including the original teacher)\nco-teach the student simultaneously. Our experiments on various teacher-student\npairs and datasets have demonstrated that the proposed approach outperforms the\ncanonical knowledge distillation approach and its extensions.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 12:52:52 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 13:20:57 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Asadian", "Aryan", ""], ["Salehi-Abari", "Amirali", ""]]}, {"id": "2103.00507", "submitter": "Florentin Hildebrandt", "authors": "Florentin D Hildebrandt, Barrett Thomas, Marlin W Ulmer", "title": "Where the Action is: Let's make Reinforcement Learning for Stochastic\n  Dynamic Vehicle Routing Problems work!", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There has been a paradigm-shift in urban logistic services in the last years;\ndemand for real-time, instant mobility and delivery services grows. This poses\nnew challenges to logistic service providers as the underlying stochastic\ndynamic vehicle routing problems (SDVRPs) require anticipatory real-time\nrouting actions. Searching the combinatorial action space for efficient routing\nactions is by itself a complex task of mixed-integer programming (MIP)\nwell-known by the operations research community. This complexity is now\nmultiplied by the challenge of evaluating such actions with respect to their\neffectiveness given future dynamism and uncertainty, a potentially ideal case\nfor reinforcement learning (RL) well-known by the computer science community.\nFor solving SDVRPs, joint work of both communities is needed, but as we show,\nessentially non-existing. Both communities focus on their individual strengths\nleaving potential for improvement. Our survey paper highlights this potential\nin research originating from both communities. We point out current obstacles\nin SDVRPs and guide towards joint approaches to overcome them.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 13:26:35 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Hildebrandt", "Florentin D", ""], ["Thomas", "Barrett", ""], ["Ulmer", "Marlin W", ""]]}, {"id": "2103.00519", "submitter": "Andreas Holzinger", "authors": "Andreas Holzinger, Anna Saranti, Heimo Mueller", "title": "KANDINSKYPatterns -- An experimental exploration environment for Pattern\n  Analysis and Machine Intelligence", "comments": "12 pages, submitted to IEEE Transactions on Pattern Analysis and\n  Machine Intelligence (TPAMI), currently under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Machine intelligence is very successful at standard recognition tasks when\nhaving high-quality training data. There is still a significant gap between\nmachine-level pattern recognition and human-level concept learning. Humans can\nlearn under uncertainty from only a few examples and generalize these concepts\nto solve new problems. The growing interest in explainable machine\nintelligence, requires experimental environments and diagnostic tests to\nanalyze weaknesses in existing approaches to drive progress in the field. In\nthis paper, we discuss existing diagnostic tests and test data sets such as\nCLEVR, CLEVERER, CLOSURE, CURI, Bongard-LOGO, V-PROM, and present our own\nexperimental environment: The KANDINSKYPatterns, named after the Russian artist\nWassily Kandinksy, who made theoretical contributions to compositivity, i.e.\nthat all perceptions consist of geometrically elementary individual components.\nThis was experimentally proven by Hubel &Wiesel in the 1960s and became the\nbasis for machine learning approaches such as the Neocognitron and the even\nlater Deep Learning. While KANDINSKYPatterns have computationally controllable\nproperties on the one hand, bringing ground truth, they are also easily\ndistinguishable by human observers, i.e., controlled patterns can be described\nby both humans and algorithms, making them another important contribution to\ninternational research in machine intelligence.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 14:09:59 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Holzinger", "Andreas", ""], ["Saranti", "Anna", ""], ["Mueller", "Heimo", ""]]}, {"id": "2103.00586", "submitter": "Luca Sestini", "authors": "Luca Sestini, Benoit Rosa, Elena De Momi, Giancarlo Ferrigno and\n  Nicolas Padoy", "title": "A Kinematic Bottleneck Approach For Pose Regression of Flexible Surgical\n  Instruments directly from Images", "comments": null, "journal-ref": null, "doi": "10.1109/LRA.2021.3062308", "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  3-D pose estimation of instruments is a crucial step towards automatic scene\nunderstanding in robotic minimally invasive surgery. Although robotic systems\ncan potentially directly provide joint values, this information is not commonly\nexploited inside the operating room, due to its possible unreliability, limited\naccess and the time-consuming calibration required, especially for continuum\nrobots. For this reason, standard approaches for 3-D pose estimation involve\nthe use of external tracking systems. Recently, image-based methods have\nemerged as promising, non-invasive alternatives. While many image-based\napproaches in the literature have shown accurate results, they generally\nrequire either a complex iterative optimization for each processed image,\nmaking them unsuitable for real-time applications, or a large number of\nmanually-annotated images for efficient learning. In this paper we propose a\nself-supervised image-based method, exploiting, at training time only, the\nimprecise kinematic information provided by the robot. In order to avoid\nintroducing time-consuming manual annotations, the problem is formulated as an\nauto-encoder, smartly bottlenecked by the presence of a physical model of the\nrobotic instruments and surgical camera, forcing a separation between image\nbackground and kinematic content. Validation of the method was performed on\nsemi-synthetic, phantom and in-vivo datasets, obtained using a flexible\nrobotized endoscope, showing promising results for real-time image-based 3-D\npose estimation of surgical instruments.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 18:41:18 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Sestini", "Luca", ""], ["Rosa", "Benoit", ""], ["De Momi", "Elena", ""], ["Ferrigno", "Giancarlo", ""], ["Padoy", "Nicolas", ""]]}, {"id": "2103.00589", "submitter": "Tom Silver", "authors": "Tom Silver, Rohan Chitnis, Joshua Tenenbaum, Leslie Pack Kaelbling,\n  Tomas Lozano-Perez", "title": "Learning Symbolic Operators for Task and Motion Planning", "comments": "IROS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Robotic planning problems in hybrid state and action spaces can be solved by\nintegrated task and motion planners (TAMP) that handle the complex interaction\nbetween motion-level decisions and task-level plan feasibility. TAMP approaches\nrely on domain-specific symbolic operators to guide the task-level search,\nmaking planning efficient. In this work, we formalize and study the problem of\noperator learning for TAMP. Central to this study is the view that operators\ndefine a lossy abstraction of the transition model of a domain. We then propose\na bottom-up relational learning method for operator learning and show how the\nlearned operators can be used for planning in a TAMP system. Experimentally, we\nprovide results in three domains, including long-horizon robotic planning\ntasks. We find our approach to substantially outperform several baselines,\nincluding three graph neural network-based model-free approaches from the\nrecent literature. Video: https://youtu.be/iVfpX9BpBRo Code:\nhttps://git.io/JCT0g\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 19:08:56 GMT"}, {"version": "v2", "created": "Thu, 15 Jul 2021 20:40:35 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Silver", "Tom", ""], ["Chitnis", "Rohan", ""], ["Tenenbaum", "Joshua", ""], ["Kaelbling", "Leslie Pack", ""], ["Lozano-Perez", "Tomas", ""]]}, {"id": "2103.00623", "submitter": "Julien Perolat", "authors": "Julien Perolat, Sarah Perrin, Romuald Elie, Mathieu Lauri\\`ere,\n  Georgios Piliouras, Matthieu Geist, Karl Tuyls, Olivier Pietquin", "title": "Scaling up Mean Field Games with Online Mirror Descent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address scaling up equilibrium computation in Mean Field Games (MFGs)\nusing Online Mirror Descent (OMD). We show that continuous-time OMD provably\nconverges to a Nash equilibrium under a natural and well-motivated set of\nmonotonicity assumptions. This theoretical result nicely extends to\nmulti-population games and to settings involving common noise. A thorough\nexperimental investigation on various single and multi-population MFGs shows\nthat OMD outperforms traditional algorithms such as Fictitious Play (FP). We\nempirically show that OMD scales up and converges significantly faster than FP\nby solving, for the first time to our knowledge, examples of MFGs with hundreds\nof billions states. This study establishes the state-of-the-art for learning in\nlarge-scale multi-agent and multi-population games.\n", "versions": [{"version": "v1", "created": "Sun, 28 Feb 2021 21:28:36 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Perolat", "Julien", ""], ["Perrin", "Sarah", ""], ["Elie", "Romuald", ""], ["Lauri\u00e8re", "Mathieu", ""], ["Piliouras", "Georgios", ""], ["Geist", "Matthieu", ""], ["Tuyls", "Karl", ""], ["Pietquin", "Olivier", ""]]}, {"id": "2103.00673", "submitter": "Qing Qu", "authors": "Sheng Liu, Xiao Li, Yuexiang Zhai, Chong You, Zhihui Zhu, Carlos\n  Fernandez-Granda, and Qing Qu", "title": "Convolutional Normalization: Improving Deep Convolutional Network\n  Robustness and Training", "comments": "SL and XL contributed equally to this work; 23 pages, 6 figures, 6\n  tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Normalization techniques have become a basic component in modern\nconvolutional neural networks (ConvNets). In particular, many recent works\ndemonstrate that promoting the orthogonality of the weights helps train deep\nmodels and improve robustness. For ConvNets, most existing methods are based on\npenalizing or normalizing weight matrices derived from concatenating or\nflattening the convolutional kernels. These methods often destroy or ignore the\nbenign convolutional structure of the kernels; therefore, they are often\nexpensive or impractical for deep ConvNets. In contrast, we introduce a simple\nand efficient ``convolutional normalization'' method that can fully exploit the\nconvolutional structure in the Fourier domain and serve as a simple\nplug-and-play module to be conveniently incorporated into any ConvNets. Our\nmethod is inspired by recent work on preconditioning methods for convolutional\nsparse coding and can effectively promote each layer's channel-wise isometry.\nFurthermore, we show that convolutional normalization can reduce the layerwise\nspectral norm of the weight matrices and hence improve the Lipschitzness of the\nnetwork, leading to easier training and improved robustness for deep ConvNets.\nApplied to classification under noise corruptions and generative adversarial\nnetwork (GAN), we show that convolutional normalization improves the robustness\nof common ConvNets such as ResNet and the performance of GAN. We verify our\nfindings via extensive numerical experiments on CIFAR-10, CIFAR-100, and\nImageNet.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 00:33:04 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Liu", "Sheng", ""], ["Li", "Xiao", ""], ["Zhai", "Yuexiang", ""], ["You", "Chong", ""], ["Zhu", "Zhihui", ""], ["Fernandez-Granda", "Carlos", ""], ["Qu", "Qing", ""]]}, {"id": "2103.00683", "submitter": "Marina Haliem", "authors": "Marina Haliem, Trevor Bonjour, Aala Alsalem, Shilpa Thomas, Hongyu Li,\n  Vaneet Aggarwal, Bharat Bhargava, and Mayank Kejriwal", "title": "Learning Monopoly Gameplay: A Hybrid Model-Free Deep Reinforcement\n  Learning and Imitation Learning Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning how to adapt and make real-time informed decisions in dynamic and\ncomplex environments is a challenging problem. To learn this task,\nReinforcement Learning (RL) relies on an agent interacting with an environment\nand learning through trial and error to maximize the cumulative sum of rewards\nreceived by it. In multi-player Monopoly game, players have to make several\ndecisions every turn which involves complex actions, such as making trades.\nThis makes the decision-making harder and thus, introduces a highly complicated\ntask for an RL agent to play and learn its winning strategies. In this paper,\nwe introduce a Hybrid Model-Free Deep RL (DRL) approach that is capable of\nplaying and learning winning strategies of the popular board game, Monopoly. To\nachieve this, our DRL agent (1) starts its learning process by imitating a\nrule-based agent (that resembles the human logic) to initialize its policy, (2)\nlearns the successful actions, and improves its policy using DRL. Experimental\nresults demonstrate an intelligent behavior of our proposed agent as it shows\nhigh win rates against different types of agent-players.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 01:40:02 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Haliem", "Marina", ""], ["Bonjour", "Trevor", ""], ["Alsalem", "Aala", ""], ["Thomas", "Shilpa", ""], ["Li", "Hongyu", ""], ["Aggarwal", "Vaneet", ""], ["Bhargava", "Bharat", ""], ["Kejriwal", "Mayank", ""]]}, {"id": "2103.00686", "submitter": "Divya Mahajan", "authors": "Muhammad Adnan, Yassaman Ebrahimzadeh Maboud, Divya Mahajan, Prashant\n  J. Nair", "title": "High-Performance Training by Exploiting Hot-Embeddings in Recommendation\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.AR cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recommendation models are commonly used learning models that suggest relevant\nitems to a user for e-commerce and online advertisement-based applications.\nCurrent recommendation models include deep-learning-based (DLRM) and time-based\nsequence (TBSM) models. These models use massive embedding tables to store a\nnumerical representation of item's and user's categorical variables\n(memory-bound) while also using neural networks to generate outputs\n(compute-bound). Due to these conflicting compute and memory requirements, the\ntraining process for recommendation models is divided across CPU and GPU for\nembedding and neural network executions, respectively. Such a training process\nnaively assigns the same level of importance to each embedding entry. This\npaper observes that some training inputs and their accesses into the embedding\ntables are heavily skewed with certain entries being accessed up to 10000x\nmore. This paper tries to leverage skewed embedded table accesses to\nefficiently use the GPU resources during training. To this end, this paper\nproposes a Frequently Accessed Embeddings (FAE) framework that exposes a\ndynamic knob to the software based on the GPU memory capacity and the input\npopularity index. This framework efficiently estimates and varies the size of\nthe hot portions of the embedding tables within GPUs and reallocates the rest\nof the embeddings on the CPU. Overall, our framework speeds-up the training of\nthe recommendation models on Kaggle, Terabyte, and Alibaba datasets by 2.34x as\ncompared to a baseline that uses Intel-Xeon CPUs and Nvidia Tesla-V100 GPUs,\nwhile maintaining accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 01:43:26 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 19:16:36 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Adnan", "Muhammad", ""], ["Maboud", "Yassaman Ebrahimzadeh", ""], ["Mahajan", "Divya", ""], ["Nair", "Prashant J.", ""]]}, {"id": "2103.00710", "submitter": "Alysa Ziying Tan", "authors": "Alysa Ziying Tan, Han Yu, Lizhen Cui, Qiang Yang", "title": "Towards Personalized Federated Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As artificial intelligence (AI)-empowered applications become widespread,\nthere is growing awareness and concern for user privacy and data\nconfidentiality. This has contributed to the popularity of federated learning\n(FL). FL applications often face data distribution and device capability\nheterogeneity across data owners. This has stimulated the rapid development of\nPersonalized FL (PFL). In this paper, we complement existing surveys, which\nlargely focus on the methods and applications of FL, with a review of recent\nadvances in PFL. We discuss hurdles to PFL under the current FL settings, and\npresent a unique taxonomy dividing PFL techniques into data-based and\nmodel-based approaches. We highlight their key ideas, and envision promising\nfuture trajectories of research towards new PFL architectural design, realistic\nPFL benchmarking, and trustworthy PFL approaches.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 02:45:19 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Tan", "Alysa Ziying", ""], ["Yu", "Han", ""], ["Cui", "Lizhen", ""], ["Yang", "Qiang", ""]]}, {"id": "2103.00718", "submitter": "Keyu Li Miss", "authors": "Keyu Li, Jian Wang, Yangxin Xu, Hao Qin, Dongsheng Liu, Li Liu, Max\n  Q.-H. Meng", "title": "Autonomous Navigation of an Ultrasound Probe Towards Standard Scan\n  Planes with Deep Reinforcement Learning", "comments": "Accepted at ICRA 2021. Copyright may be transferred without notice,\n  after which this version may no longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous ultrasound (US) acquisition is an important yet challenging task,\nas it involves interpretation of the highly complex and variable images and\ntheir spatial relationships. In this work, we propose a deep reinforcement\nlearning framework to autonomously control the 6-D pose of a virtual US probe\nbased on real-time image feedback to navigate towards the standard scan planes\nunder the restrictions in real-world US scans. Furthermore, we propose a\nconfidence-based approach to encode the optimization of image quality in the\nlearning process. We validate our method in a simulation environment built with\nreal-world data collected in the US imaging of the spine. Experimental results\ndemonstrate that our method can perform reproducible US probe navigation\ntowards the standard scan plane with an accuracy of $4.91mm/4.65^\\circ$ in the\nintra-patient setting, and accomplish the task in the intra- and inter-patient\nsettings with a success rate of $92\\%$ and $46\\%$, respectively. The results\nalso show that the introduction of image quality optimization in our method can\neffectively improve the navigation performance.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 03:09:17 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Li", "Keyu", ""], ["Wang", "Jian", ""], ["Xu", "Yangxin", ""], ["Qin", "Hao", ""], ["Liu", "Dongsheng", ""], ["Liu", "Li", ""], ["Meng", "Max Q. -H.", ""]]}, {"id": "2103.00719", "submitter": "Ziqing Lu", "authors": "Ziqing Lu, Chang Xu, Bo Du, Takashi Ishida, Lefei Zhang, and Masashi\n  Sugiyama", "title": "LocalDrop: A Hybrid Regularization for Deep Neural Networks", "comments": null, "journal-ref": null, "doi": "10.1109/TPAMI.2021.3061463", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In neural networks, developing regularization algorithms to settle\noverfitting is one of the major study areas. We propose a new approach for the\nregularization of neural networks by the local Rademacher complexity called\nLocalDrop. A new regularization function for both fully-connected networks\n(FCNs) and convolutional neural networks (CNNs), including drop rates and\nweight matrices, has been developed based on the proposed upper bound of the\nlocal Rademacher complexity by the strict mathematical deduction. The analyses\nof dropout in FCNs and DropBlock in CNNs with keep rate matrices in different\nlayers are also included in the complexity analyses. With the new\nregularization function, we establish a two-stage procedure to obtain the\noptimal keep rate matrix and weight matrix to realize the whole training model.\nExtensive experiments have been conducted to demonstrate the effectiveness of\nLocalDrop in different models by comparing it with several algorithms and the\neffects of different hyperparameters on the final performances.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 03:10:11 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Lu", "Ziqing", ""], ["Xu", "Chang", ""], ["Du", "Bo", ""], ["Ishida", "Takashi", ""], ["Zhang", "Lefei", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "2103.00738", "submitter": "Aoran Xiao", "authors": "Aoran Xiao, Xiaofei Yang, Shijian Lu, Dayan Guan and Jiaxing Huang", "title": "FPS-Net: A Convolutional Fusion Network for Large-Scale LiDAR Point\n  Cloud Segmentation", "comments": "20 pages, 7 figures", "journal-ref": null, "doi": "10.1016/j.isprsjprs.2021.04.011", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Scene understanding based on LiDAR point cloud is an essential task for\nautonomous cars to drive safely, which often employs spherical projection to\nmap 3D point cloud into multi-channel 2D images for semantic segmentation. Most\nexisting methods simply stack different point attributes/modalities (e.g.\ncoordinates, intensity, depth, etc.) as image channels to increase information\ncapacity, but ignore distinct characteristics of point attributes in different\nimage channels. We design FPS-Net, a convolutional fusion network that exploits\nthe uniqueness and discrepancy among the projected image channels for optimal\npoint cloud segmentation. FPS-Net adopts an encoder-decoder structure. Instead\nof simply stacking multiple channel images as a single input, we group them\ninto different modalities to first learn modality-specific features separately\nand then map the learned features into a common high-dimensional feature space\nfor pixel-level fusion and learning. Specifically, we design a residual dense\nblock with multiple receptive fields as a building block in the encoder which\npreserves detailed information in each modality and learns hierarchical\nmodality-specific and fused features effectively. In the FPS-Net decoder, we\nuse a recurrent convolution block likewise to hierarchically decode fused\nfeatures into output space for pixel-level classification. Extensive\nexperiments conducted on two widely adopted point cloud datasets show that\nFPS-Net achieves superior semantic segmentation as compared with\nstate-of-the-art projection-based methods. In addition, the proposed modality\nfusion idea is compatible with typical projection-based methods and can be\nincorporated into them with consistent performance improvements.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 04:08:28 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Xiao", "Aoran", ""], ["Yang", "Xiaofei", ""], ["Lu", "Shijian", ""], ["Guan", "Dayan", ""], ["Huang", "Jiaxing", ""]]}, {"id": "2103.00752", "submitter": "Atoosa Kasirzadeh", "authors": "Atoosa Kasirzadeh", "title": "Reasons, Values, Stakeholders: A Philosophical Framework for Explainable\n  Artificial Intelligence", "comments": "This paper is accepted for non-archival publication at the ACM\n  conference on Fairness, Accountability, and Transparency (FAccT) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The societal and ethical implications of the use of opaque artificial\nintelligence systems for consequential decisions, such as welfare allocation\nand criminal justice, have generated a lively debate among multiple stakeholder\ngroups, including computer scientists, ethicists, social scientists, policy\nmakers, and end users. However, the lack of a common language or a\nmulti-dimensional framework to appropriately bridge the technical, epistemic,\nand normative aspects of this debate prevents the discussion from being as\nproductive as it could be. Drawing on the philosophical literature on the\nnature and value of explanations, this paper offers a multi-faceted framework\nthat brings more conceptual precision to the present debate by (1) identifying\nthe types of explanations that are most pertinent to artificial intelligence\npredictions, (2) recognizing the relevance and importance of social and ethical\nvalues for the evaluation of these explanations, and (3) demonstrating the\nimportance of these explanations for incorporating a diversified approach to\nimproving the design of truthful algorithmic ecosystems. The proposed\nphilosophical framework thus lays the groundwork for establishing a pertinent\nconnection between the technical and ethical aspects of artificial intelligence\nsystems.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 04:50:31 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Kasirzadeh", "Atoosa", ""]]}, {"id": "2103.00778", "submitter": "Mahsa Paknezhad", "authors": "Mahsa Paknezhad, Cuong Phuc Ngo, Amadeus Aristo Winarto, Alistair\n  Cheong, Beh Chuen Yang, Wu Jiayang, Lee Hwee Kuan", "title": "Explaining Adversarial Vulnerability with a Data Sparsity Hypothesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite many proposed algorithms to provide robustness to deep learning (DL)\nmodels, DL models remain susceptible to adversarial attacks. We hypothesize\nthat the adversarial vulnerability of DL models stems from two factors. The\nfirst factor is data sparsity which is that in the high dimensional data space,\nthere are large regions outside the support of the data distribution. The\nsecond factor is the existence of many redundant parameters in the DL models.\nOwing to these factors, different models are able to come up with different\ndecision boundaries with comparably high prediction accuracy. The appearance of\nthe decision boundaries in the space outside the support of the data\ndistribution does not affect the prediction accuracy of the model. However,\nthey make an important difference in the adversarial robustness of the model.\nWe propose that the ideal decision boundary should be as far as possible from\nthe support of the data distribution.\\par In this paper, we develop a training\nframework for DL models to learn such decision boundaries spanning the space\naround the class distributions further from the data points themselves.\nSemi-supervised learning was deployed to achieve this objective by leveraging\nunlabeled data generated in the space outside the support of the data\ndistribution. We measure adversarial robustness of the models trained using\nthis training framework against well-known adversarial attacks We found that\nour results, other regularization methods and adversarial training also support\nour hypothesis of data sparcity. We show that the unlabeled data generated by\nnoise using our framework is almost as effective as unlabeled data, sourced\nfrom existing data sets or generated by synthesis algorithms, on adversarial\nrobustness. Our code is available at\nhttps://github.com/MahsaPaknezhad/AdversariallyRobustTraining.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 06:04:31 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Paknezhad", "Mahsa", ""], ["Ngo", "Cuong Phuc", ""], ["Winarto", "Amadeus Aristo", ""], ["Cheong", "Alistair", ""], ["Yang", "Beh Chuen", ""], ["Jiayang", "Wu", ""], ["Kuan", "Lee Hwee", ""]]}, {"id": "2103.00791", "submitter": "Renbo Zhu", "authors": "Renbo Zhu, Meng Ma, Ping Wang", "title": "RAGA: Relation-aware Graph Attention Networks for Global Entity\n  Alignment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity alignment (EA) is the task to discover entities referring to the same\nreal-world object from different knowledge graphs (KGs), which is the most\ncrucial step in integrating multi-source KGs. The majority of the existing\nembeddings-based entity alignment methods embed entities and relations into a\nvector space based on relation triples of KGs for local alignment. As these\nmethods insufficiently consider the multiple relations between entities, the\nstructure information of KGs has not been fully leveraged. In this paper, we\npropose a novel framework based on Relation-aware Graph Attention Networks to\ncapture the interactions between entities and relations. Our framework adopts\nthe self-attention mechanism to spread entity information to the relations and\nthen aggregate relation information back to entities. Furthermore, we propose a\nglobal alignment algorithm to make one-to-one entity alignments with a\nfine-grained similarity matrix. Experiments on three real-world cross-lingual\ndatasets show that our framework outperforms the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 06:30:51 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Zhu", "Renbo", ""], ["Ma", "Meng", ""], ["Wang", "Ping", ""]]}, {"id": "2103.00792", "submitter": "Namyong Park", "authors": "Namyong Park, MinHyeok Kim, Nguyen Xuan Hoai, R.I. (Bob) McKay,\n  Dong-Kyun Kim", "title": "Knowledge-Guided Dynamic Systems Modeling: A Case Study on Modeling\n  River Water Quality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling real-world phenomena is a focus of many science and engineering\nefforts, such as ecological modeling and financial forecasting, to name a few.\nBuilding an accurate model for complex and dynamic systems improves\nunderstanding of underlying processes and leads to resource efficiency. Towards\nthis goal, knowledge-driven modeling builds a model based on human expertise,\nyet is often suboptimal. At the opposite extreme, data-driven modeling learns a\nmodel directly from data, requiring extensive data and potentially generating\noverfitting. We focus on an intermediate approach, model revision, in which\nprior knowledge and data are combined to achieve the best of both worlds. In\nthis paper, we propose a genetic model revision framework based on\ntree-adjoining grammar (TAG) guided genetic programming (GP), using the TAG\nformalism and GP operators in an effective mechanism to incorporate prior\nknowledge and make data-driven revisions in a way that complies with prior\nknowledge. Our framework is designed to address the high computational cost of\nevolutionary modeling of complex systems. Via a case study on the challenging\nproblem of river water quality modeling, we show that the framework efficiently\nlearns an interpretable model, with higher modeling accuracy than existing\nmethods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 06:31:38 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Park", "Namyong", "", "Bob"], ["Kim", "MinHyeok", "", "Bob"], ["Hoai", "Nguyen Xuan", "", "Bob"], ["I.", "R.", "", "Bob"], ["McKay", "", ""], ["Kim", "Dong-Kyun", ""]]}, {"id": "2103.00816", "submitter": "Max W. Y. Lam", "authors": "Jun Wang, Max W. Y. Lam, Dan Su, Dong Yu", "title": "Contrastive Separative Coding for Self-supervised Representation\n  Learning", "comments": "Accepted in ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.AI cs.LG cs.SD eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To extract robust deep representations from long sequential modeling of\nspeech data, we propose a self-supervised learning approach, namely Contrastive\nSeparative Coding (CSC). Our key finding is to learn such representations by\nseparating the target signal from contrastive interfering signals. First, a\nmulti-task separative encoder is built to extract shared separable and\ndiscriminative embedding; secondly, we propose a powerful cross-attention\nmechanism performed over speaker representations across various interfering\nconditions, allowing the model to focus on and globally aggregate the most\ncritical information to answer the \"query\" (current bottom-up embedding) while\npaying less attention to interfering, noisy, or irrelevant parts; lastly, we\nform a new probabilistic contrastive loss which estimates and maximizes the\nmutual information between the representations and the global speaker vector.\nWhile most prior unsupervised methods have focused on predicting the future,\nneighboring, or missing samples, we take a different perspective of predicting\nthe interfered samples. Moreover, our contrastive separative loss is free from\nnegative sampling. The experiment demonstrates that our approach can learn\nuseful representations achieving a strong speaker verification performance in\nadverse conditions.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 07:32:00 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Wang", "Jun", ""], ["Lam", "Max W. Y.", ""], ["Su", "Dan", ""], ["Yu", "Dong", ""]]}, {"id": "2103.00819", "submitter": "Max W. Y. Lam", "authors": "Max W. Y. Lam, Jun Wang, Dan Su, Dong Yu", "title": "Sandglasset: A Light Multi-Granularity Self-attentive Network For\n  Time-Domain Speech Separation", "comments": "Accepted in ICASSP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.AI cs.LG cs.SD eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the leading single-channel speech separation (SS) models is based on a\nTasNet with a dual-path segmentation technique, where the size of each segment\nremains unchanged throughout all layers. In contrast, our key finding is that\nmulti-granularity features are essential for enhancing contextual modeling and\ncomputational efficiency. We introduce a self-attentive network with a novel\nsandglass-shape, namely Sandglasset, which advances the state-of-the-art (SOTA)\nSS performance at significantly smaller model size and computational cost.\nForward along each block inside Sandglasset, the temporal granularity of the\nfeatures gradually becomes coarser until reaching half of the network blocks,\nand then successively turns finer towards the raw signal level. We also unfold\nthat residual connections between features with the same granularity are\ncritical for preserving information after passing through the bottleneck layer.\nExperiments show our Sandglasset with only 2.3M parameters has achieved the\nbest results on two benchmark SS datasets -- WSJ0-2mix and WSJ0-3mix, where the\nSI-SNRi scores have been improved by absolute 0.8 dB and 2.4 dB, respectively,\ncomparing to the prior SOTA results.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 07:36:09 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 07:37:53 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Lam", "Max W. Y.", ""], ["Wang", "Jun", ""], ["Su", "Dan", ""], ["Yu", "Dong", ""]]}, {"id": "2103.00820", "submitter": "Hung Le", "authors": "Hung Le, Nancy F. Chen, Steven C.H. Hoi", "title": "Learning Reasoning Paths over Semantic Graphs for Video-grounded\n  Dialogues", "comments": "Accepted at ICLR (International Conference on Learning\n  Representations) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Compared to traditional visual question answering, video-grounded dialogues\nrequire additional reasoning over dialogue context to answer questions in a\nmulti-turn setting. Previous approaches to video-grounded dialogues mostly use\ndialogue context as a simple text input without modelling the inherent\ninformation flows at the turn level. In this paper, we propose a novel\nframework of Reasoning Paths in Dialogue Context (PDC). PDC model discovers\ninformation flows among dialogue turns through a semantic graph constructed\nbased on lexical components in each question and answer. PDC model then learns\nto predict reasoning paths over this semantic graph. Our path prediction model\npredicts a path from the current turn through past dialogue turns that contain\nadditional visual cues to answer the current question. Our reasoning model\nsequentially processes both visual and textual information through this\nreasoning path and the propagated features are used to generate the answer. Our\nexperimental results demonstrate the effectiveness of our method and provide\nadditional insights on how models use semantic dependencies in a dialogue\ncontext to retrieve visual cues.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 07:39:26 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Le", "Hung", ""], ["Chen", "Nancy F.", ""], ["Hoi", "Steven C. H.", ""]]}, {"id": "2103.00833", "submitter": "Thomas Pellegrini", "authors": "Thomas Pellegrini (IRIT-SAMoVA), Timoth\\'ee Masquelier (CERCO)", "title": "Fast threshold optimization for multi-label audio tagging using\n  Surrogate gradient learning", "comments": null, "journal-ref": "IEEE International Conference on Acoustics, Speech and Signal\n  Processing, Jun 2021, Toronto, Canada", "doi": null, "report-no": null, "categories": "cs.AI cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-label audio tagging consists of assigning sets of tags to audio\nrecordings. At inference time, thresholds are applied on the confidence scores\noutputted by a probabilistic classifier, in order to decide which classes are\ndetected active. In this work, we consider having at disposal a trained\nclassifier and we seek to automatically optimize the decision thresholds\naccording to a performance metric of interest, in our case F-measure\n(micro-F1). We propose a new method, called SGL-Thresh for Surrogate Gradient\nLearning of Thresholds, that makes use of gradient descent. Since F1 is not\ndifferentiable, we propose to approximate the thresholding operation gradients\nwith the gradients of a sigmoid function. We report experiments on three\ndatasets, using state-of-the-art pre-trained deep neural networks. In all\ncases, SGL-Thresh outperformed three other approaches: a default threshold\nvalue (defThresh), an heuristic search algorithm and a method estimating F1\ngradients numerically. It reached 54.9\\% F1 on AudioSet eval, compared to 50.7%\nwith defThresh. SGL-Thresh is very fast and scalable to a large number of tags.\nTo facilitate reproducibility, data and source code in Pytorch are available\nonline: https://github.com/topel/SGL-Thresh\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 08:05:07 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Pellegrini", "Thomas", "", "IRIT-SAMoVA"], ["Masquelier", "Timoth\u00e9e", "", "CERCO"]]}, {"id": "2103.00845", "submitter": "Huimin Peng", "authors": "Huimin Peng", "title": "A Brief Summary of Interactions Between Meta-Learning and\n  Self-Supervised Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper briefly reviews the connections between meta-learning and\nself-supervised learning. Meta-learning can be applied to improve model\ngeneralization capability and to construct general AI algorithms.\nSelf-supervised learning utilizes self-supervision from original data and\nextracts higher-level generalizable features through unsupervised pre-training\nor optimization of contrastive loss objectives. In self-supervised learning,\ndata augmentation techniques are widely applied and data labels are not\nrequired since pseudo labels can be estimated from trained models on similar\ntasks. Meta-learning aims to adapt trained deep models to solve diverse tasks\nand to develop general AI algorithms. We review the associations of\nmeta-learning with both generative and contrastive self-supervised learning\nmodels. Unlabeled data from multiple sources can be jointly considered even\nwhen data sources are vastly different. We show that an integration of\nmeta-learning and self-supervised learning models can best contribute to the\nimprovement of model generalization capability. Self-supervised learning guided\nby meta-learner and general meta-learning algorithms under self-supervision are\nboth examples of possible combinations.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 08:31:28 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Peng", "Huimin", ""]]}, {"id": "2103.00847", "submitter": "Shahroz Tariq", "authors": "Shahroz Tariq, Sowon Jeon, Simon S. Woo", "title": "Am I a Real or Fake Celebrity? Measuring Commercial Face Recognition Web\n  APIs under Deepfake Impersonation Attack", "comments": "27 pages, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CR cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, significant advancements have been made in face recognition\ntechnologies using Deep Neural Networks. As a result, companies such as\nMicrosoft, Amazon, and Naver offer highly accurate commercial face recognition\nweb services for diverse applications to meet the end-user needs. Naturally,\nhowever, such technologies are threatened persistently, as virtually any\nindividual can quickly implement impersonation attacks. In particular, these\nattacks can be a significant threat for authentication and identification\nservices, which heavily rely on their underlying face recognition technologies'\naccuracy and robustness. Despite its gravity, the issue regarding deepfake\nabuse using commercial web APIs and their robustness has not yet been\nthoroughly investigated. This work provides a measurement study on the\nrobustness of black-box commercial face recognition APIs against Deepfake\nImpersonation (DI) attacks using celebrity recognition APIs as an example case\nstudy. We use five deepfake datasets, two of which are created by us and\nplanned to be released. More specifically, we measure attack performance based\non two scenarios (targeted and non-targeted) and further analyze the differing\nsystem behaviors using fidelity, confidence, and similarity metrics.\nAccordingly, we demonstrate how vulnerable face recognition technologies from\npopular companies are to DI attack, achieving maximum success rates of 78.0%\nand 99.9% for targeted (i.e., precise match) and non-targeted (i.e., match with\nany celebrity) attacks, respectively. Moreover, we propose practical defense\nstrategies to mitigate DI attacks, reducing the attack success rates to as low\nas 0% and 0.02% for targeted and non-targeted attacks, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 08:40:10 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 07:56:46 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Tariq", "Shahroz", ""], ["Jeon", "Sowon", ""], ["Woo", "Simon S.", ""]]}, {"id": "2103.00848", "submitter": "Xiao Huang", "authors": "Xiao Huang, Hong Qiao, Hui Li and Zhihong Jiang", "title": "A Bioinspired Retinal Neural Network for Accurately Extracting\n  Small-Target Motion Information in Cluttered Backgrounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust and accurate detection of small moving targets in cluttered moving\nbackgrounds is a significant and challenging problem for robotic visual systems\nto perform search and tracking tasks. Inspired by the neural circuitry of\nelementary motion vision in the mammalian retina, this paper proposes a\nbioinspired retinal neural network based on a new neurodynamics-based temporal\nfiltering and multiform 2-D spatial Gabor filtering. This model can estimate\nmotion direction accurately via only two perpendicular spatiotemporal filtering\nsignals, and respond to small targets of different sizes and velocities by\nadjusting the dendrite field size of the spatial filter. Meanwhile, an\nalgorithm of directionally selective inhibition is proposed to suppress the\ntarget-like features in the moving background, which can reduce the influence\nof background motion effectively. Extensive synthetic and real-data experiments\nshow that the proposed model works stably for small targets of a wider size and\nvelocity range, and has better detection performance than other bioinspired\nmodels. Additionally, it can also extract the information of motion direction\nand motion energy accurately and rapidly.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 08:44:27 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Huang", "Xiao", ""], ["Qiao", "Hong", ""], ["Li", "Hui", ""], ["Jiang", "Zhihong", ""]]}, {"id": "2103.00857", "submitter": "Xiao Huang", "authors": "Xiao Huang, Hong Qiao, Hui Li and Zhihong Jiang", "title": "A Bioinspired Approach-Sensitive Neural Network for Collision Detection\n  in Cluttered and Dynamic Backgrounds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Rapid, accurate and robust detection of looming objects in cluttered moving\nbackgrounds is a significant and challenging problem for robotic visual systems\nto perform collision detection and avoidance tasks. Inspired by the neural\ncircuit of elementary motion vision in the mammalian retina, this paper\nproposes a bioinspired approach-sensitive neural network (ASNN) that contains\nthree main contributions. Firstly, a direction-selective visual processing\nmodule is built based on the spatiotemporal energy framework, which can\nestimate motion direction accurately via only two mutually perpendicular\nspatiotemporal filtering channels. Secondly, a novel approach-sensitive neural\nnetwork is modeled as a push-pull structure formed by ON and OFF pathways,\nwhich responds strongly to approaching motion while insensitivity to lateral\nmotion. Finally, a method of directionally selective inhibition is introduced,\nwhich is able to suppress the translational backgrounds effectively. Extensive\nsynthetic and real robotic experiments show that the proposed model is able to\nnot only detect collision accurately and robustly in cluttered and dynamic\nbackgrounds but also extract more collision information like position and\ndirection, for guiding rapid decision making.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 09:16:18 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Huang", "Xiao", ""], ["Qiao", "Hong", ""], ["Li", "Hui", ""], ["Jiang", "Zhihong", ""]]}, {"id": "2103.00887", "submitter": "Zhongqi Yue", "authors": "Zhongqi Yue, Tan Wang, Hanwang Zhang, Qianru Sun, Xian-Sheng Hua", "title": "Counterfactual Zero-Shot and Open-Set Visual Recognition", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel counterfactual framework for both Zero-Shot Learning (ZSL)\nand Open-Set Recognition (OSR), whose common challenge is generalizing to the\nunseen-classes by only training on the seen-classes. Our idea stems from the\nobservation that the generated samples for unseen-classes are often out of the\ntrue distribution, which causes severe recognition rate imbalance between the\nseen-class (high) and unseen-class (low). We show that the key reason is that\nthe generation is not Counterfactual Faithful, and thus we propose a faithful\none, whose generation is from the sample-specific counterfactual question: What\nwould the sample look like, if we set its class attribute to a certain class,\nwhile keeping its sample attribute unchanged? Thanks to the faithfulness, we\ncan apply the Consistency Rule to perform unseen/seen binary classification, by\nasking: Would its counterfactual still look like itself? If ``yes'', the sample\nis from a certain class, and ``no'' otherwise. Through extensive experiments on\nZSL and OSR, we demonstrate that our framework effectively mitigates the\nseen/unseen imbalance and hence significantly improves the overall performance.\nNote that this framework is orthogonal to existing methods, thus, it can serve\nas a new baseline to evaluate how ZSL/OSR models generalize. Codes are\navailable at https://github.com/yue-zhongqi/gcm-cf.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 10:20:04 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Yue", "Zhongqi", ""], ["Wang", "Tan", ""], ["Zhang", "Hanwang", ""], ["Sun", "Qianru", ""], ["Hua", "Xian-Sheng", ""]]}, {"id": "2103.00891", "submitter": "Yiwen Liu", "authors": "Yanzhen Ren, Yiwen Liu, Lina Wang", "title": "Using contrastive learning to improve the performance of steganalysis\n  schemes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To improve the detection accuracy and generalization of steganalysis, this\npaper proposes the Steganalysis Contrastive Framework (SCF) based on\ncontrastive learning. The SCF improves the feature representation of\nsteganalysis by maximizing the distance between features of samples of\ndifferent categories and minimizing the distance between features of samples of\nthe same category. To decrease the computing complexity of the contrastive loss\nin supervised learning, we design a novel Steganalysis Contrastive Loss\n(StegCL) based on the equivalence and transitivity of similarity. The StegCL\neliminates the redundant computing in the existing contrastive loss. The\nexperimental results show that the SCF improves the generalization and\ndetection accuracy of existing steganalysis DNNs, and the maximum promotion is\n2% and 3% respectively. Without decreasing the detection accuracy, the training\ntime of using the StegCL is 10% of that of using the contrastive loss in\nsupervised learning.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 10:32:02 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Ren", "Yanzhen", ""], ["Liu", "Yiwen", ""], ["Wang", "Lina", ""]]}, {"id": "2103.00923", "submitter": "Sarah Janboecke", "authors": "Sarah Janboecke and Susanne Zajitschek", "title": "Anticipation Next -- System-sensitive technology development and\n  integration in work contexts", "comments": null, "journal-ref": "Information 2021, 12, 269", "doi": "10.3390/info12070269", "report-no": null, "categories": "cs.HC cs.AI cs.CY cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  When discussing future concerns within socio-technical systems in work\ncontexts, we often find descriptions of missed technology development and\nintegration. The experience of technology that fails whilst being integrated is\noften rooted in dysfunctional epistemological approaches within the research\nand development process. Thus, ultimately leading to sustainable\ntechnology-distrust in work contexts. This is true for organizations that\nintegrate new technologies and for organizations that invent them.\nOrganizations in which we find failed technology development and integrations\nare, in their very nature, social systems. Nowadays, those complex social\nsystems act within an even more complex environment. This urges the development\nof new anticipation methods for technology development and integration.\nGathering of and dealing with complex information in the described context is\nwhat we call Anticipation Next. This explorative work uses existing literature\nfrom the adjoining research fields of system theory, organizational theory, and\nsocio-technical research to combine various concepts. We deliberately aim at a\nnetworked way of thinking in scientific contexts and thus combine\nmultidisciplinary subject areas in one paper to present an innovative way to\ndeal with multi-faceted problems in a human-centred way. We end with suggesting\na conceptual framework that should be used in the very early stages of\ntechnology development and integration in work contexts.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 11:27:19 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 07:38:29 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Janboecke", "Sarah", ""], ["Zajitschek", "Susanne", ""]]}, {"id": "2103.00983", "submitter": "Stefano Fiorini", "authors": "Stefano Fiorini, Michele Ciavotta, Andrea Maurino", "title": "Listening to the city, attentively: A Spatio-Temporal Attention Boosted\n  Autoencoder for the Short-Term Flow Prediction Problem", "comments": "submitted to IEEE Transactions on Intelligent Transportation Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the importance of studying traffic flows and making\npredictions on alternative mobility (sharing services) has become increasingly\nimportant, as accurate and timely information on the travel flow is important\nfor the successful implementation of systems that increase the quality of\nsharing services. This need has been accentuated by the current health crisis\nthat requires alternative transport mobility such as electric bike and electric\nscooter sharing. Considering the new approaches in the world of deep learning\nand the difficulty due to the strong spatial and temporal dependence of this\nproblem, we propose a framework, called STREED-Net, with multi-attention\n(Spatial and Temporal) able to better mining the high-level spatial and\ntemporal features. We conduct experiments on three real datasets to predict the\nInflow and Outflow of the different regions into which the city has been\ndivided. The results indicate that the proposed STREED-Net model improves the\nstate-of-the-art for this problem.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 13:17:33 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 15:33:27 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Fiorini", "Stefano", ""], ["Ciavotta", "Michele", ""], ["Maurino", "Andrea", ""]]}, {"id": "2103.00993", "submitter": "Xu Tan", "authors": "Mingjian Chen, Xu Tan, Bohan Li, Yanqing Liu, Tao Qin, Sheng Zhao,\n  Tie-Yan Liu", "title": "AdaSpeech: Adaptive Text to Speech for Custom Voice", "comments": "Accepted by ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.AI cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Custom voice, a specific text to speech (TTS) service in commercial speech\nplatforms, aims to adapt a source TTS model to synthesize personal voice for a\ntarget speaker using few speech data. Custom voice presents two unique\nchallenges for TTS adaptation: 1) to support diverse customers, the adaptation\nmodel needs to handle diverse acoustic conditions that could be very different\nfrom source speech data, and 2) to support a large number of customers, the\nadaptation parameters need to be small enough for each target speaker to reduce\nmemory usage while maintaining high voice quality. In this work, we propose\nAdaSpeech, an adaptive TTS system for high-quality and efficient customization\nof new voices. We design several techniques in AdaSpeech to address the two\nchallenges in custom voice: 1) To handle different acoustic conditions, we use\ntwo acoustic encoders to extract an utterance-level vector and a sequence of\nphoneme-level vectors from the target speech during training; in inference, we\nextract the utterance-level vector from a reference speech and use an acoustic\npredictor to predict the phoneme-level vectors. 2) To better trade off the\nadaptation parameters and voice quality, we introduce conditional layer\nnormalization in the mel-spectrogram decoder of AdaSpeech, and fine-tune this\npart in addition to speaker embedding for adaptation. We pre-train the source\nTTS model on LibriTTS datasets and fine-tune it on VCTK and LJSpeech datasets\n(with different acoustic conditions from LibriTTS) with few adaptation data,\ne.g., 20 sentences, about 1 minute speech. Experiment results show that\nAdaSpeech achieves much better adaptation quality than baseline methods, with\nonly about 5K specific parameters for each speaker, which demonstrates its\neffectiveness for custom voice. Audio samples are available at\nhttps://speechresearch.github.io/adaspeech/.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 13:28:59 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Chen", "Mingjian", ""], ["Tan", "Xu", ""], ["Li", "Bohan", ""], ["Liu", "Yanqing", ""], ["Qin", "Tao", ""], ["Zhao", "Sheng", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "2103.01002", "submitter": "Peter Kokol PhD", "authors": "Peter Kokol, Marko Kokol, Sa\\v{s}o Zagoranski", "title": "Machine learning on small size samples: A synthetic knowledge synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  One of the increasingly important technologies dealing with the growing\ncomplexity of the digitalization of almost all human activities is Artificial\nintelligence, more precisely machine learning Despite the fact, that we live in\na Big data world where almost everything is digitally stored, there are many\nreal-world situations, where researchers are faced with small data samples. The\npresent study aim is to answer the following research question namely What is\nthe small data problem in machine learning and how it is solved?. Our\nbibliometric study showed a positive trend in the number of research\npublications concerning the use of small datasets and substantial growth of the\nresearch community dealing with the small dataset problem, indicating that the\nresearch field is moving toward higher maturity levels. Despite notable\ninternational cooperation, the regional concentration of research literature\nproduction in economically more developed countries was observed.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 13:49:25 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Kokol", "Peter", ""], ["Kokol", "Marko", ""], ["Zagoranski", "Sa\u0161o", ""]]}, {"id": "2103.01022", "submitter": "Jun Yuan", "authors": "Jun Yuan, Oded Nov, Enrico Bertini", "title": "Visualizing Rule Sets: Exploration and Validation of a Design Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rule sets are often used in Machine Learning (ML) as a way to communicate the\nmodel logic in settings where transparency and intelligibility are necessary.\nRule sets are typically presented as a text-based list of logical statements\n(rules). Surprisingly, to date there has been limited work on exploring visual\nalternatives for presenting rules. In this paper, we explore the idea of\ndesigning alternative representations of rules, focusing on a number of visual\nfactors we believe have a positive impact on rule readability and\nunderstanding. The paper presents an initial design space for visualizing rule\nsets and a user study exploring their impact. The results show that some design\nfactors have a strong impact on how efficiently readers can process the rules\nwhile having minimal impact on accuracy. This work can help practitioners\nemploy more effective solutions when using rules as a communication strategy to\nunderstand ML models.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 14:19:22 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 14:57:10 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Yuan", "Jun", ""], ["Nov", "Oded", ""], ["Bertini", "Enrico", ""]]}, {"id": "2103.01025", "submitter": "Piyush Shrivastava", "authors": "Piyush Shrivastava", "title": "Neural Code Summarization", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Code summarization is the task of generating readable summaries that are\nsemantically meaningful and can accurately describe the presumed task of a\nsoftware. Program comprehension has become one of the most tedious tasks for\nknowledge transfer. As the codebase evolves over time, the description needs to\nbe manually updated each time with the changes made. An automatic approach is\nproposed to infer such captions based on benchmarked and custom datasets with\ncomparison between the original and generated results.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 16:04:33 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 19:55:08 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Shrivastava", "Piyush", ""]]}, {"id": "2103.01035", "submitter": "Mark Keane", "authors": "Mark T Keane, Eoin M Kenny, Eoin Delaney, Barry Smyth", "title": "If Only We Had Better Counterfactual Explanations: Five Key Deficits to\n  Rectify in the Evaluation of Counterfactual XAI Techniques", "comments": "13 pages, 2 figures", "journal-ref": "Proceedings of the 30th International Joint Conference on\n  Artificial Intelligence (IJCAI-21), August, 2021", "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, there has been an explosion of AI research on counterfactual\nexplanations as a solution to the problem of eXplainable AI (XAI). These\nexplanations seem to offer technical, psychological and legal benefits over\nother explanation techniques. We survey 100 distinct counterfactual explanation\nmethods reported in the literature. This survey addresses the extent to which\nthese methods have been adequately evaluated, both psychologically and\ncomputationally, and quantifies the shortfalls occurring. For instance, only\n21% of these methods have been user tested. Five key deficits in the evaluation\nof these methods are detailed and a roadmap, with standardised benchmark\nevaluations, is proposed to resolve the issues arising; issues, that currently\neffectively block scientific progress in this field.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 09:57:33 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Keane", "Mark T", ""], ["Kenny", "Eoin M", ""], ["Delaney", "Eoin", ""], ["Smyth", "Barry", ""]]}, {"id": "2103.01043", "submitter": "Heiko Strathmann", "authors": "Heiko Strathmann, Mohammadamin Barekatain, Charles Blundell, Petar\n  Veli\\v{c}kovi\\'c", "title": "Persistent Message Passing", "comments": "7 pages, 2 figures. Published as a workshop paper at ICLR 2021 SimDL\n  Workshop. Accepted at the ICLR 2021 Workshop on Geometrical and Topological\n  Representation Learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph neural networks (GNNs) are a powerful inductive bias for modelling\nalgorithmic reasoning procedures and data structures. Their prowess was mainly\ndemonstrated on tasks featuring Markovian dynamics, where querying any\nassociated data structure depends only on its latest state. For many tasks of\ninterest, however, it may be highly beneficial to support efficient data\nstructure queries dependent on previous states. This requires tracking the data\nstructure's evolution through time, placing significant pressure on the GNN's\nlatent representations. We introduce Persistent Message Passing (PMP), a\nmechanism which endows GNNs with capability of querying past state by\nexplicitly persisting it: rather than overwriting node representations, it\ncreates new nodes whenever required. PMP generalises out-of-distribution to\nmore than 2x larger test inputs on dynamic temporal range queries,\nsignificantly outperforming GNNs which overwrite states.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 14:35:36 GMT"}, {"version": "v2", "created": "Tue, 27 Apr 2021 10:39:33 GMT"}], "update_date": "2021-04-28", "authors_parsed": [["Strathmann", "Heiko", ""], ["Barekatain", "Mohammadamin", ""], ["Blundell", "Charles", ""], ["Veli\u010dkovi\u0107", "Petar", ""]]}, {"id": "2103.01075", "submitter": "Yi Tay", "authors": "Yi Tay, Mostafa Dehghani, Vamsi Aribandi, Jai Gupta, Philip Pham, Zhen\n  Qin, Dara Bahri, Da-Cheng Juan, Donald Metzler", "title": "OmniNet: Omnidirectional Representations from Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes Omnidirectional Representations from Transformers\n(OmniNet). In OmniNet, instead of maintaining a strictly horizontal receptive\nfield, each token is allowed to attend to all tokens in the entire network.\nThis process can also be interpreted as a form of extreme or intensive\nattention mechanism that has the receptive field of the entire width and depth\nof the network. To this end, the omnidirectional attention is learned via a\nmeta-learner, which is essentially another self-attention based model. In order\nto mitigate the computationally expensive costs of full receptive field\nattention, we leverage efficient self-attention models such as kernel-based\n(Choromanski et al.), low-rank attention (Wang et al.) and/or Big Bird (Zaheer\net al.) as the meta-learner. Extensive experiments are conducted on\nautoregressive language modeling (LM1B, C4), Machine Translation, Long Range\nArena (LRA), and Image Recognition. The experiments show that OmniNet achieves\nconsiderable improvements across these tasks, including achieving\nstate-of-the-art performance on LM1B, WMT'14 En-De/En-Fr, and Long Range Arena.\nMoreover, using omnidirectional representation in Vision Transformers leads to\nsignificant improvements on image recognition tasks on both few-shot learning\nand fine-tuning setups.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 15:31:54 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Tay", "Yi", ""], ["Dehghani", "Mostafa", ""], ["Aribandi", "Vamsi", ""], ["Gupta", "Jai", ""], ["Pham", "Philip", ""], ["Qin", "Zhen", ""], ["Bahri", "Dara", ""], ["Juan", "Da-Cheng", ""], ["Metzler", "Donald", ""]]}, {"id": "2103.01108", "submitter": "Carl Corea", "authors": "Carl Corea, Matthias Thimm, Patrick Delfmann", "title": "Measuring Inconsistency over Sequences of Business Rule Cases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this report, we investigate (element-based) inconsistency measures for\nmultisets of business rule bases. Currently, related works allow to assess\nindividual rule bases, however, as companies might encounter thousands of such\ninstances daily, studying not only individual rule bases separately, but rather\nalso their interrelations becomes necessary, especially in regard to\ndetermining suitable re-modelling strategies. We therefore present an approach\nto induce multiset-measures from arbitrary (traditional) inconsistency\nmeasures, propose new rationality postulates for a multiset use-case, and\ninvestigate the complexity of various aspects regarding multi-rule base\ninconsistency measurement.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 16:18:26 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Corea", "Carl", ""], ["Thimm", "Matthias", ""], ["Delfmann", "Patrick", ""]]}, {"id": "2103.01109", "submitter": "Georgi Nalbantov", "authors": "Georgi Nalbantov, Svetoslav Ivanov", "title": "Optimal Linear Combination of Classifiers", "comments": "14 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The question of whether to use one classifier or a combination of classifiers\nis a central topic in Machine Learning. We propose here a method for finding an\noptimal linear combination of classifiers derived from a bias-variance\nframework for the classification task.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 16:21:40 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Nalbantov", "Georgi", ""], ["Ivanov", "Svetoslav", ""]]}, {"id": "2103.01116", "submitter": "Heju Li", "authors": "Rui Wang, Heju Li, Erwu Liu", "title": "Blockchain-Based Federated Learning in Mobile Edge Networks with\n  Application in Internet of Vehicles", "comments": "14 pages,7 figures, journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid increase of the data scale in Internet of Vehicles (IoV) system\nparadigm, hews out new possibilities in boosting the service quality for the\nemerging applications through data sharing. Nevertheless, privacy concerns are\nmajor bottlenecks for data providers to share private data in traditional IoV\nnetworks. To this end, federated learning (FL) as an emerging learning\nparadigm, where data providers only send local model updates trained on their\nlocal raw data rather than upload any raw data, has been recently proposed to\nbuild a privacy-preserving data sharing models. Unfortunately, by analyzing on\nthe differences of uploaded local model updates from data providers, private\ninformation can still be divulged, and performance of the system cannot be\nguaranteed when partial federated nodes executes malicious behavior.\nAdditionally, traditional cloud-based FL poses challenges to the communication\noverhead with the rapid increase of terminal equipment in IoV system. All these\nissues inspire us to propose an autonomous blockchain empowered\nprivacy-preserving FL framework in this paper, where the mobile edge computing\n(MEC) technology was naturally integrated in IoV system.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 16:38:40 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Wang", "Rui", ""], ["Li", "Heju", ""], ["Liu", "Erwu", ""]]}, {"id": "2103.01118", "submitter": "Richard Preen", "authors": "Richard J. Preen and Larry Bull", "title": "Deep Learning with a Classifier System: Initial Results", "comments": "arXiv admin note: text overlap with arXiv:1910.10579", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents the first results from using a learning classifier\nsystem capable of performing adaptive computation with deep neural networks.\nIndividual classifiers within the population are composed of two neural\nnetworks. The first acts as a gating or guarding component, which enables the\nconditional computation of an associated deep neural network on a per instance\nbasis. Self-adaptive mutation is applied upon reproduction and prediction\nnetworks are refined with stochastic gradient descent during lifetime learning.\nThe use of fully-connected and convolutional layers are evaluated on\nhandwritten digit recognition tasks where evolution adapts (i) the gradient\ndescent learning rate applied to each layer (ii) the number of units within\neach layer, i.e., the number of fully-connected neurons and the number of\nconvolutional kernel filters (iii) the connectivity of each layer, i.e.,\nwhether each weight is active (iv) the weight magnitudes, enabling escape from\nlocal optima. The system automatically reduces the number of weights and units\nwhile maintaining performance after achieving a maximum prediction error.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 16:40:12 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Preen", "Richard J.", ""], ["Bull", "Larry", ""]]}, {"id": "2103.01133", "submitter": "Christian Henning", "authors": "Christian Henning, Maria R. Cervera, Francesco D'Angelo, Johannes von\n  Oswald, Regina Traber, Benjamin Ehret, Seijin Kobayashi, Jo\\~ao Sacramento,\n  Benjamin F. Grewe", "title": "Posterior Meta-Replay for Continual Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning a sequence of tasks without access to i.i.d. observations is a\nwidely studied form of continual learning (CL) that remains challenging. In\nprinciple, Bayesian learning directly applies to this setting, since recursive\nand one-off Bayesian updates yield the same result. In practice, however,\nrecursive updating often leads to poor trade-off solutions across tasks because\napproximate inference is necessary for most models of interest. Here, we\ndescribe an alternative Bayesian approach where task-conditioned parameter\ndistributions are continually inferred from data. We offer a practical deep\nlearning implementation of our framework based on probabilistic\ntask-conditioned hypernetworks, an approach we term \"posterior meta-replay\".\nExperiments on standard benchmarks show that our probabilistic hypernetworks\ncompress sequences of posterior parameter distributions with virtually no\nforgetting. We obtain considerable performance gains compared to existing\nBayesian CL methods, and identify task inference as our major limiting factor.\nThis limitation has several causes that are independent of the considered\nsequential setting, opening up new avenues for progress in CL.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 17:08:35 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 15:53:31 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Henning", "Christian", ""], ["Cervera", "Maria R.", ""], ["D'Angelo", "Francesco", ""], ["von Oswald", "Johannes", ""], ["Traber", "Regina", ""], ["Ehret", "Benjamin", ""], ["Kobayashi", "Seijin", ""], ["Sacramento", "Jo\u00e3o", ""], ["Grewe", "Benjamin F.", ""]]}, {"id": "2103.01169", "submitter": "Luca Maria Aiello", "authors": "Sanja Scepanovic, Luca Maria Aiello, Ke Zhou, Sagar Joglekar, Daniele\n  Quercia", "title": "The Healthy States of America: Creating a Health Taxonomy with Social\n  Media", "comments": "In proceedings of the International Conference on Web and Social\n  Media (ICWSM'21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since the uptake of social media, researchers have mined online discussions\nto track the outbreak and evolution of specific diseases or chronic conditions\nsuch as influenza or depression. To broaden the set of diseases under study, we\ndeveloped a Deep Learning tool for Natural Language Processing that extracts\nmentions of virtually any medical condition or disease from unstructured social\nmedia text. With that tool at hand, we processed Reddit and Twitter posts,\nanalyzed the clusters of the two resulting co-occurrence networks of\nconditions, and discovered that they correspond to well-defined categories of\nmedical conditions. This resulted in the creation of the first comprehensive\ntaxonomy of medical conditions automatically derived from online discussions.\nWe validated the structure of our taxonomy against the official International\nStatistical Classification of Diseases and Related Health Problems (ICD-11),\nfinding matches of our clusters with 20 official categories, out of 22. Based\non the mentions of our taxonomy's sub-categories on Reddit posts geo-referenced\nin the U.S., we were then able to compute disease-specific health scores. As\nopposed to counts of disease mentions or counts with no knowledge of our\ntaxonomy's structure, we found that our disease-specific health scores are\ncausally linked with the officially reported prevalence of 18 conditions.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 18:07:47 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Scepanovic", "Sanja", ""], ["Aiello", "Luca Maria", ""], ["Zhou", "Ke", ""], ["Joglekar", "Sagar", ""], ["Quercia", "Daniele", ""]]}, {"id": "2103.01171", "submitter": "William Macke", "authors": "William Macke, Reuth Mirsky and Peter Stone", "title": "Expected Value of Communication for Planning in Ad Hoc Teamwork", "comments": "10 pages, 6 figure, Published at AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A desirable goal for autonomous agents is to be able to coordinate on the fly\nwith previously unknown teammates. Known as \"ad hoc teamwork\", enabling such a\ncapability has been receiving increasing attention in the research community.\nOne of the central challenges in ad hoc teamwork is quickly recognizing the\ncurrent plans of other agents and planning accordingly. In this paper, we focus\non the scenario in which teammates can communicate with one another, but only\nat a cost. Thus, they must carefully balance plan recognition based on\nobservations vs. that based on communication. This paper proposes a new metric\nfor evaluating how similar are two policies that a teammate may be following -\nthe Expected Divergence Point (EDP). We then present a novel planning algorithm\nfor ad hoc teamwork, determining which query to ask and planning accordingly.\nWe demonstrate the effectiveness of this algorithm in a range of increasingly\ngeneral communication in ad hoc teamwork problems.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 18:09:36 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 18:05:47 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Macke", "William", ""], ["Mirsky", "Reuth", ""], ["Stone", "Peter", ""]]}, {"id": "2103.01197", "submitter": "Anirudh Goyal", "authors": "Anirudh Goyal, Aniket Didolkar, Alex Lamb, Kartikeya Badola, Nan\n  Rosemary Ke, Nasim Rahaman, Jonathan Binas, Charles Blundell, Michael Mozer,\n  Yoshua Bengio", "title": "Coordination Among Neural Modules Through a Shared Global Workspace", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has seen a movement away from representing examples with a\nmonolithic hidden state towards a richly structured state. For example,\nTransformers segment by position, and object-centric architectures decompose\nimages into entities. In all these architectures, interactions between\ndifferent elements are modeled via pairwise interactions: Transformers make use\nof self-attention to incorporate information from other positions;\nobject-centric architectures make use of graph neural networks to model\ninteractions among entities. However, pairwise interactions may not achieve\nglobal coordination or a coherent, integrated representation that can be used\nfor downstream tasks. In cognitive science, a global workspace architecture has\nbeen proposed in which functionally specialized components share information\nthrough a common, bandwidth-limited communication channel. We explore the use\nof such a communication channel in the context of deep learning for modeling\nthe structure of complex environments. The proposed method includes a shared\nworkspace through which communication among different specialist modules takes\nplace but due to limits on the communication bandwidth, specialist modules must\ncompete for access. We show that capacity limitations have a rational basis in\nthat (1) they encourage specialization and compositionality and (2) they\nfacilitate the synchronization of otherwise independent specialists.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 18:43:48 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Goyal", "Anirudh", ""], ["Didolkar", "Aniket", ""], ["Lamb", "Alex", ""], ["Badola", "Kartikeya", ""], ["Ke", "Nan Rosemary", ""], ["Rahaman", "Nasim", ""], ["Binas", "Jonathan", ""], ["Blundell", "Charles", ""], ["Mozer", "Michael", ""], ["Bengio", "Yoshua", ""]]}, {"id": "2103.01203", "submitter": "Sydney Katz", "authors": "Sydney M. Katz, Kyle D. Julian, Christopher A. Strong, Mykel J.\n  Kochenderfer", "title": "Generating Probabilistic Safety Guarantees for Neural Network\n  Controllers", "comments": "24 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks serve as effective controllers in a variety of complex\nsettings due to their ability to represent expressive policies. The complex\nnature of neural networks, however, makes their output difficult to verify and\npredict, which limits their use in safety-critical applications. While\nsimulations provide insight into the performance of neural network controllers,\nthey are not enough to guarantee that the controller will perform safely in all\nscenarios. To address this problem, recent work has focused on formal methods\nto verify properties of neural network outputs. For neural network controllers,\nwe can use a dynamics model to determine the output properties that must hold\nfor the controller to operate safely. In this work, we develop a method to use\nthe results from neural network verification tools to provide probabilistic\nsafety guarantees on a neural network controller. We develop an adaptive\nverification approach to efficiently generate an overapproximation of the\nneural network policy. Next, we modify the traditional formulation of Markov\ndecision process (MDP) model checking to provide guarantees on the\noverapproximated policy given a stochastic dynamics model. Finally, we\nincorporate techniques in state abstraction to reduce overapproximation error\nduring the model checking process. We show that our method is able to generate\nmeaningful probabilistic safety guarantees for aircraft collision avoidance\nneural networks that are loosely inspired by Airborne Collision Avoidance\nSystem X (ACAS X), a family of collision avoidance systems that formulates the\nproblem as a partially observable Markov decision process (POMDP).\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 18:48:21 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Katz", "Sydney M.", ""], ["Julian", "Kyle D.", ""], ["Strong", "Christopher A.", ""], ["Kochenderfer", "Mykel J.", ""]]}, {"id": "2103.01205", "submitter": "Justin Terry", "authors": "J. K. Terry, Mario Jayakumar, Kusal De Alwis", "title": "Statistically Significant Stopping of Neural Network Training", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The general approach taken when training deep learning classifiers is to save\nthe parameters after every few iterations, train until either a human observer\nor a simple metric-based heuristic decides the network isn't learning anymore,\nand then backtrack and pick the saved parameters with the best validation\naccuracy. Simple methods are used to determine if a neural network isn't\nlearning anymore because, as long as it's well after the optimal values are\nfound, the condition doesn't impact the final accuracy of the model. However\nfrom a runtime perspective, this is of great significance to the many cases\nwhere numerous neural networks are trained simultaneously (e.g. hyper-parameter\ntuning). Motivated by this, we introduce a statistical significance test to\ndetermine if a neural network has stopped learning. This stopping criterion\nappears to represent a happy medium compared to other popular stopping\ncriterions, achieving comparable accuracy to the criterions that achieve the\nhighest final accuracies in 77% or fewer epochs, while the criterions which\nstop sooner do so with an appreciable loss to final accuracy. Additionally, we\nuse this as the basis of a new learning rate scheduler, removing the need to\nmanually choose learning rate schedules and acting as a quasi-line search,\nachieving superior or comparable empirical performance to existing methods.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 18:51:16 GMT"}, {"version": "v2", "created": "Sun, 21 Mar 2021 03:37:36 GMT"}, {"version": "v3", "created": "Wed, 28 Jul 2021 02:42:19 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Terry", "J. K.", ""], ["Jayakumar", "Mario", ""], ["De Alwis", "Kusal", ""]]}, {"id": "2103.01209", "submitter": "Drew A. Hudson", "authors": "Drew A. Hudson and C. Lawrence Zitnick", "title": "Generative Adversarial Transformers", "comments": "Published as a conference paper at ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce the GANformer, a novel and efficient type of transformer, and\nexplore it for the task of visual generative modeling. The network employs a\nbipartite structure that enables long-range interactions across the image,\nwhile maintaining computation of linear efficiency, that can readily scale to\nhigh-resolution synthesis. It iteratively propagates information from a set of\nlatent variables to the evolving visual features and vice versa, to support the\nrefinement of each in light of the other and encourage the emergence of\ncompositional representations of objects and scenes. In contrast to the classic\ntransformer architecture, it utilizes multiplicative integration that allows\nflexible region-based modulation, and can thus be seen as a generalization of\nthe successful StyleGAN network. We demonstrate the model's strength and\nrobustness through a careful evaluation over a range of datasets, from\nsimulated multi-object environments to rich real-world indoor and outdoor\nscenes, showing it achieves state-of-the-art results in terms of image quality\nand diversity, while enjoying fast learning and better data-efficiency. Further\nqualitative and quantitative experiments offer us an insight into the model's\ninner workings, revealing improved interpretability and stronger\ndisentanglement, and illustrating the benefits and efficacy of our approach. An\nimplementation of the model is available at\nhttps://github.com/dorarad/gansformer.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 18:54:04 GMT"}, {"version": "v2", "created": "Tue, 2 Mar 2021 18:39:04 GMT"}, {"version": "v3", "created": "Thu, 1 Jul 2021 03:13:31 GMT"}], "update_date": "2021-07-02", "authors_parsed": [["Hudson", "Drew A.", ""], ["Zitnick", "C. Lawrence", ""]]}, {"id": "2103.01217", "submitter": "Burak Pak", "authors": "Gorsev Argin, Burak Pak, Handan Turkoglu", "title": "Between Post-Flaneur and Smartphone Zombie Smartphone Users Altering\n  Visual Attention and Walking Behavior in Public Space", "comments": null, "journal-ref": "2020 ISPRS International Journal of Geo-Information 9, 12, 700", "doi": "10.3390/ijgi9120700", "report-no": null, "categories": "cs.HC cs.AI cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The extensive use of smartphones in our everyday lives has created new modes\nof appropriation and behavior in public spaces. Recognition of these are\nessential for urban design and planning practices which help us to improve the\nrelationship between humans, technologies, and urban environment. This study\naims to research smartphone users in public space by observing their altering\nvisual attention and walking behavior, and, in this way, to reveal the emergent\nnew figures. For this purpose, Korenmarkt square in Ghent, Belgium, was\nobserved for seven days in 10-min time intervals. The gaze and walking behavior\nof smartphone users were encoded as geo-located and temporal data, analyzed and\nmapped using statistical and spatial analysis methods. Developing and\nimplementing new methods for identifying the characteristics of smartphone\nusers, this study resulted in a nuanced characterization of novel spatial\nappropriations. The findings led to a better understanding and knowledge of the\ndifferent behavior patterns of emergent figures such as post-flaneurs and\nsmartphone zombies while uncovering their altering visual interactions with and\nmovements in the public space. The results evoked questions on how researchers\nand designers can make use of spatial analysis methods and rethink the public\nspace of the future as a hybrid construct integrating the virtual and the\nphysical.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 14:53:45 GMT"}], "update_date": "2021-03-02", "authors_parsed": [["Argin", "Gorsev", ""], ["Pak", "Burak", ""], ["Turkoglu", "Handan", ""]]}, {"id": "2103.01242", "submitter": "Avia Efrat", "authors": "Avia Efrat, Uri Shaham, Dan Kilman, Omer Levy", "title": "Cryptonite: A Cryptic Crossword Benchmark for Extreme Ambiguity in\n  Language", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Current NLP datasets targeting ambiguity can be solved by a native speaker\nwith relative ease. We present Cryptonite, a large-scale dataset based on\ncryptic crosswords, which is both linguistically complex and naturally sourced.\nEach example in Cryptonite is a cryptic clue, a short phrase or sentence with a\nmisleading surface reading, whose solving requires disambiguating semantic,\nsyntactic, and phonetic wordplays, as well as world knowledge. Cryptic clues\npose a challenge even for experienced solvers, though top-tier experts can\nsolve them with almost 100% accuracy. Cryptonite is a challenging task for\ncurrent models; fine-tuning T5-Large on 470k cryptic clues achieves only 7.6%\naccuracy, on par with the accuracy of a rule-based clue solver (8.6%).\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 19:01:01 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Efrat", "Avia", ""], ["Shaham", "Uri", ""], ["Kilman", "Dan", ""], ["Levy", "Omer", ""]]}, {"id": "2103.01301", "submitter": "Iana Polonskaia", "authors": "Iana S. Polonskaia, Nikolay O. Nikitin, Ilia Revin, Pavel Vychuzhanin,\n  Anna V. Kalyuzhnaya", "title": "Multi-Objective Evolutionary Design of Composite Data-Driven Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a multi-objective approach for the design of composite\ndata-driven mathematical models is proposed. It allows automating the\nidentification of graph-based heterogeneous pipelines that consist of different\nblocks: machine learning models, data preprocessing blocks, etc. The\nimplemented approach is based on a parameter-free genetic algorithm (GA) for\nmodel design called GPComp@Free. It is developed to be part of automated\nmachine learning solutions and to increase the efficiency of the modeling\npipeline automation. A set of experiments was conducted to verify the\ncorrectness and efficiency of the proposed approach and substantiate the\nselected solutions. The experimental results confirm that a multi-objective\napproach to the model design allows achieving better diversity and quality of\nobtained models. The implemented approach is available as a part of the\nopen-source AutoML framework FEDOT.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 20:45:24 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 21:49:55 GMT"}], "update_date": "2021-05-19", "authors_parsed": [["Polonskaia", "Iana S.", ""], ["Nikitin", "Nikolay O.", ""], ["Revin", "Ilia", ""], ["Vychuzhanin", "Pavel", ""], ["Kalyuzhnaya", "Anna V.", ""]]}, {"id": "2103.01319", "submitter": "Devansh Shah", "authors": "Devansh Shah, Parijat Dube, Supriyo Chakraborty, Ashish Verma", "title": "Adversarial training in communication constrained federated learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning enables model training over a distributed corpus of agent\ndata. However, the trained model is vulnerable to adversarial examples,\ndesigned to elicit misclassification. We study the feasibility of using\nadversarial training (AT) in the federated learning setting. Furthermore, we do\nso assuming a fixed communication budget and non-iid data distribution between\nparticipating agents. We observe a significant drop in both natural and\nadversarial accuracies when AT is used in the federated setting as opposed to\ncentralized training. We attribute this to the number of epochs of AT performed\nlocally at the agents, which in turn effects (i) drift between local models;\nand (ii) convergence time (measured in number of communication rounds). Towards\nthis end, we propose FedDynAT, a novel algorithm for performing AT in federated\nsetting. Through extensive experimentation we show that FedDynAT significantly\nimproves both natural and adversarial accuracy, as well as model convergence\ntime by reducing the model drift.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 21:37:54 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Shah", "Devansh", ""], ["Dube", "Parijat", ""], ["Chakraborty", "Supriyo", ""], ["Verma", "Ashish", ""]]}, {"id": "2103.01373", "submitter": "Aleksandra \\'Ciprijanovi\\'c", "authors": "A. \\'Ciprijanovi\\'c, D. Kafkes, K. Downey, S. Jenkins, G. N. Perdue,\n  S. Madireddy, T. Johnston, G. F. Snyder, B. Nord", "title": "DeepMerge II: Building Robust Deep Learning Algorithms for Merging\n  Galaxy Identification Across Domains", "comments": "Submitted to MNRAS; 21 pages, 9 figures, 9 tables", "journal-ref": "MNRAS, Volume 506, Issue 1, September 2021, Page 677", "doi": "10.1093/mnras/stab1677", "report-no": "FERMILAB-PUB-21-072-SCD", "categories": "astro-ph.IM astro-ph.GA cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In astronomy, neural networks are often trained on simulation data with the\nprospect of being used on telescope observations. Unfortunately, training a\nmodel on simulation data and then applying it to instrument data leads to a\nsubstantial and potentially even detrimental decrease in model accuracy on the\nnew target dataset. Simulated and instrument data represent different data\ndomains, and for an algorithm to work in both, domain-invariant learning is\nnecessary. Here we employ domain adaptation techniques$-$ Maximum Mean\nDiscrepancy (MMD) as an additional transfer loss and Domain Adversarial Neural\nNetworks (DANNs)$-$ and demonstrate their viability to extract domain-invariant\nfeatures within the astronomical context of classifying merging and non-merging\ngalaxies. Additionally, we explore the use of Fisher loss and entropy\nminimization to enforce better in-domain class discriminability. We show that\nthe addition of each domain adaptation technique improves the performance of a\nclassifier when compared to conventional deep learning algorithms. We\ndemonstrate this on two examples: between two Illustris-1 simulated datasets of\ndistant merging galaxies, and between Illustris-1 simulated data of nearby\nmerging galaxies and observed data from the Sloan Digital Sky Survey. The use\nof domain adaptation techniques in our experiments leads to an increase of\ntarget domain classification accuracy of up to ${\\sim}20\\%$. With further\ndevelopment, these techniques will allow astronomers to successfully implement\nneural network models trained on simulation data to efficiently detect and\nstudy astrophysical objects in current and future large-scale astronomical\nsurveys.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 00:24:10 GMT"}], "update_date": "2021-07-16", "authors_parsed": [["\u0106iprijanovi\u0107", "A.", ""], ["Kafkes", "D.", ""], ["Downey", "K.", ""], ["Jenkins", "S.", ""], ["Perdue", "G. N.", ""], ["Madireddy", "S.", ""], ["Johnston", "T.", ""], ["Snyder", "G. F.", ""], ["Nord", "B.", ""]]}, {"id": "2103.01378", "submitter": "Alon Jacovi", "authors": "Alon Jacovi, Swabha Swayamdipta, Shauli Ravfogel, Yanai Elazar, Yejin\n  Choi, Yoav Goldberg", "title": "Contrastive Explanations for Model Interpretability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Contrastive explanations clarify why an event occurred in contrast to\nanother. They are more inherently intuitive to humans to both produce and\ncomprehend. We propose a methodology to produce contrastive explanations for\nclassification models by modifying the representation to disregard\nnon-contrastive information, and modifying model behavior to only be based on\ncontrastive reasoning. Our method is based on projecting model representation\nto a latent space that captures only the features that are useful (to the\nmodel) to differentiate two potential decisions. We demonstrate the value of\ncontrastive explanations by analyzing two different scenarios, using both\nhigh-level abstract concept attribution and low-level input token/span\nattribution, on two widely used text classification tasks. Specifically, we\nproduce explanations for answering: for which label, and against which\nalternative label, is some aspect of the input useful? And which aspects of the\ninput are useful for and against particular decisions? Overall, our findings\nshed light on the ability of label-contrastive explanations to provide a more\naccurate and finer-grained interpretability of a model's decision.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 00:36:45 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Jacovi", "Alon", ""], ["Swayamdipta", "Swabha", ""], ["Ravfogel", "Shauli", ""], ["Elazar", "Yanai", ""], ["Choi", "Yejin", ""], ["Goldberg", "Yoav", ""]]}, {"id": "2103.01391", "submitter": "Semih Cayci", "authors": "Semih Cayci, Siddhartha Satpathi, Niao He, R. Srikant", "title": "Sample Complexity and Overparameterization Bounds for Temporal\n  Difference Learning with Neural Network Approximation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study the dynamics of temporal difference learning with\nneural network-based value function approximation over a general state space,\nnamely, \\emph{Neural TD learning}. We consider two practically used algorithms,\nprojection-free and max-norm regularized Neural TD learning, and establish the\nfirst convergence bounds for these algorithms. An interesting observation from\nour results is that max-norm regularization can dramatically improve the\nperformance of TD learning algorithms, both in terms of sample complexity and\noverparameterization. In particular, we prove that max-norm regularization\nappears to be more effective than $\\ell_2$-regularization, again both in terms\nof sample complexity and overparameterization. The results in this work rely on\na novel Lyapunov drift analysis of the network parameters as a stopped and\ncontrolled random process.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 01:05:19 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 04:51:56 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Cayci", "Semih", ""], ["Satpathi", "Siddhartha", ""], ["He", "Niao", ""], ["Srikant", "R.", ""]]}, {"id": "2103.01400", "submitter": "Sekitoshi Kanai", "authors": "Sekitoshi Kanai, Masanori Yamada, Hiroshi Takahashi, Yuki Yamanaka,\n  Yasutoshi Ida", "title": "Smoothness Analysis of Adversarial Training", "comments": "22 pages, 7 figures. In V3, we add the results of EntropySGD for\n  adversarial training", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are vulnerable to adversarial attacks. Recent studies\nabout adversarial robustness focus on the loss landscape in the parameter space\nsince it is related to optimization and generalization performance. These\nstudies conclude that the difficulty of adversarial training is caused by the\nnon-smoothness of the loss function: i.e., its gradient is not Lipschitz\ncontinuous. However, this analysis ignores the dependence of adversarial\nattacks on model parameters. Since adversarial attacks are optimized for\nmodels, they should depend on the parameters. Considering this dependence, we\nanalyze the smoothness of the loss function of adversarial training using the\noptimal attacks for the model parameter in more detail. We reveal that the\nconstraint of adversarial attacks is one cause of the non-smoothness and that\nthe smoothness depends on the types of the constraints. Specifically, the\n$L_\\infty$ constraint can cause non-smoothness more than the $L_2$ constraint.\nMoreover, our analysis implies that if we flatten the loss function with\nrespect to input data, the Lipschitz constant of the gradient of adversarial\nloss tends to increase. To address the non-smoothness, we show that EntropySGD\nsmoothens the non-smooth loss and improves the performance of adversarial\ntraining.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 01:27:16 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 06:20:23 GMT"}, {"version": "v3", "created": "Tue, 15 Jun 2021 06:30:30 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Kanai", "Sekitoshi", ""], ["Yamada", "Masanori", ""], ["Takahashi", "Hiroshi", ""], ["Yamanaka", "Yuki", ""], ["Ida", "Yasutoshi", ""]]}, {"id": "2103.01403", "submitter": "Qing Li", "authors": "Qing Li, Siyuan Huang, Yining Hong, Yixin Zhu, Ying Nian Wu, Song-Chun\n  Zhu", "title": "A HINT from Arithmetic: On Systematic Generalization of Perception,\n  Syntax, and Semantics", "comments": "Preliminary work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by humans' remarkable ability to master arithmetic and generalize to\nunseen problems, we present a new dataset, HINT, to study machines' capability\nof learning generalizable concepts at three different levels: perception,\nsyntax, and semantics. In particular, concepts in HINT, including both digits\nand operators, are required to learn in a weakly-supervised fashion: Only the\nfinal results of handwriting expressions are provided as supervision. Learning\nagents need to reckon how concepts are perceived from raw signals such as\nimages (i.e., perception), how multiple concepts are structurally combined to\nform a valid expression (i.e., syntax), and how concepts are realized to afford\nvarious reasoning tasks (i.e., semantics). With a focus on systematic\ngeneralization, we carefully design a five-fold test set to evaluate both the\ninterpolation and the extrapolation of learned concepts. To tackle this\nchallenging problem, we propose a neural-symbolic system by integrating neural\nnetworks with grammar parsing and program synthesis, learned by a novel\ndeduction--abduction strategy. In experiments, the proposed neural-symbolic\nsystem demonstrates strong generalization capability and significantly\noutperforms end-to-end neural methods like RNN and Transformer. The results\nalso indicate the significance of recursive priors for extrapolation on syntax\nand semantics.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 01:32:54 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Li", "Qing", ""], ["Huang", "Siyuan", ""], ["Hong", "Yining", ""], ["Zhu", "Yixin", ""], ["Wu", "Ying Nian", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "2103.01426", "submitter": "Ademola Okerinde", "authors": "Ademola Okerinde and Lior Shamir and William Hsu and Tom Theis", "title": "AdeNet: Deep learning architecture that identifies damaged electrical\n  insulators in power lines", "comments": "4th Black in AI Workshop of the International Confererence on Neural\n  Information Processing Systems (NeurIPS BAI 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ceramic insulators are important to electronic systems, designed and\ninstalled to protect humans from the danger of high voltage electric current.\nHowever, insulators are not immortal, and natural deterioration can gradually\ndamage them. Therefore, the condition of insulators must be continually\nmonitored, which is normally done using UAVs. UAVs collect many images of\ninsulators, and these images are then analyzed to identify those that are\ndamaged. Here we describe AdeNet as a deep neural network designed to identify\ndamaged insulators, and test multiple approaches to automatic analysis of the\ncondition of insulators. Several deep neural networks were tested, as were\nshallow learning methods. The best results (88.8\\%) were achieved using AdeNet\nwithout transfer learning. AdeNet also reduced the false negative rate to\n$\\sim$7\\%. While the method cannot fully replace human inspection, its high\nthroughput can reduce the amount of labor required to monitor lines for damaged\ninsulators and provide early warning to replace damaged insulators.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 02:40:40 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Okerinde", "Ademola", ""], ["Shamir", "Lior", ""], ["Hsu", "William", ""], ["Theis", "Tom", ""]]}, {"id": "2103.01432", "submitter": "Yong-Bin Kang", "authors": "Yong-Bin Kang and Timos Sellis", "title": "TopicTracker: A Platform for Topic Trajectory Identification and\n  Visualisation", "comments": "Submitted to SoftwareX", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic trajectory information provides crucial insight into the dynamics of\ntopics and their evolutionary relationships over a given time. Also, this\ninformation can help to improve our understanding on how new topics have\nemerged or formed through a sequential or interrelated events of emergence,\nmodification and integration of prior topics. Nevertheless, the implementation\nof the existing methods for topic trajectory identification is rarely available\nas usable software. In this paper, we present TopicTracker, a platform for\ntopic trajectory identification and visualisation. The key of Topic Tracker is\nthat it can represent the three facets of information together, given two kinds\nof input: a time-stamped topic profile consisting of the set of the underlying\ntopics over time, and the evolution strength matrix among them: evolutionary\npathways of dynamic topics, evolution states of the topics, and topic\nimportance. TopicTracker is a publicly available software implemented using the\nR software.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 02:58:48 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Kang", "Yong-Bin", ""], ["Sellis", "Timos", ""]]}, {"id": "2103.01449", "submitter": "Danfeng Hong", "authors": "Danfeng Hong and Wei He and Naoto Yokoya and Jing Yao and Lianru Gao\n  and Liangpei Zhang and Jocelyn Chanussot and Xiao Xiang Zhu", "title": "Interpretable Hyperspectral AI: When Non-Convex Modeling meets\n  Hyperspectral Remote Sensing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hyperspectral imaging, also known as image spectrometry, is a landmark\ntechnique in geoscience and remote sensing (RS). In the past decade, enormous\nefforts have been made to process and analyze these hyperspectral (HS) products\nmainly by means of seasoned experts. However, with the ever-growing volume of\ndata, the bulk of costs in manpower and material resources poses new challenges\non reducing the burden of manual labor and improving efficiency. For this\nreason, it is, therefore, urgent to develop more intelligent and automatic\napproaches for various HS RS applications. Machine learning (ML) tools with\nconvex optimization have successfully undertaken the tasks of numerous\nartificial intelligence (AI)-related applications. However, their ability in\nhandling complex practical problems remains limited, particularly for HS data,\ndue to the effects of various spectral variabilities in the process of HS\nimaging and the complexity and redundancy of higher dimensional HS signals.\nCompared to the convex models, non-convex modeling, which is capable of\ncharacterizing more complex real scenes and providing the model\ninterpretability technically and theoretically, has been proven to be a\nfeasible solution to reduce the gap between challenging HS vision tasks and\ncurrently advanced intelligent data processing models.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 03:32:10 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Hong", "Danfeng", ""], ["He", "Wei", ""], ["Yokoya", "Naoto", ""], ["Yao", "Jing", ""], ["Gao", "Lianru", ""], ["Zhang", "Liangpei", ""], ["Chanussot", "Jocelyn", ""], ["Zhu", "Xiao Xiang", ""]]}, {"id": "2103.01461", "submitter": "Max W. Y. Lam", "authors": "Jun Wang, Max W. Y. Lam, Dan Su, Dong Yu", "title": "Tune-In: Training Under Negative Environments with Interference for\n  Attention Networks Simulating Cocktail Party Effect", "comments": "Accepted in AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.AI cs.LG cs.SD eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the cocktail party problem and propose a novel attention network\ncalled Tune-In, abbreviated for training under negative environments with\ninterference. It firstly learns two separate spaces of speaker-knowledge and\nspeech-stimuli based on a shared feature space, where a new block structure is\ndesigned as the building block for all spaces, and then cooperatively solves\ndifferent tasks. Between the two spaces, information is cast towards each other\nvia a novel cross- and dual-attention mechanism, mimicking the bottom-up and\ntop-down processes of a human's cocktail party effect. It turns out that\nsubstantially discriminative and generalizable speaker representations can be\nlearnt in severely interfered conditions via our self-supervised training. The\nexperimental results verify this seeming paradox. The learnt speaker embedding\nhas superior discriminative power than a standard speaker verification method;\nmeanwhile, Tune-In achieves remarkably better speech separation performances in\nterms of SI-SNRi and SDRi consistently in all test modes, and especially at\nlower memory and computational consumption, than state-of-the-art benchmark\nsystems.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 04:03:37 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Wang", "Jun", ""], ["Lam", "Max W. Y.", ""], ["Su", "Dan", ""], ["Yu", "Dong", ""]]}, {"id": "2103.01474", "submitter": "Dong Li", "authors": "Ruoming Jin and Dong Li and Benjamin Mudrak and Jing Gao and Zhi Liu", "title": "On Estimating Recommendation Evaluation Metrics under Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Since the recent study (Krichene and Rendle 2020) done by Krichene and Rendle\non the sampling-based top-k evaluation metric for recommendation, there has\nbeen a lot of debates on the validity of using sampling to evaluate\nrecommendation algorithms. Though their work and the recent work (Li et\nal.2020) have proposed some basic approaches for mapping the sampling-based\nmetrics to their global counterparts which rank the entire set of items, there\nis still a lack of understanding and consensus on how sampling should be used\nfor recommendation evaluation. The proposed approaches either are rather\nuninformative (linking sampling to metric evaluation) or can only work on\nsimple metrics, such as Recall/Precision (Krichene and Rendle 2020; Li et al.\n2020). In this paper, we introduce a new research problem on learning the\nempirical rank distribution, and a new approach based on the estimated rank\ndistribution, to estimate the top-k metrics. Since this question is closely\nrelated to the underlying mechanism of sampling for recommendation, tackling it\ncan help better understand the power of sampling and can help resolve the\nquestions of if and how should we use sampling for evaluating recommendation.\nWe introduce two approaches based on MLE (MaximalLikelihood Estimation) and its\nweighted variants, and ME(Maximal Entropy) principals to recover the empirical\nrank distribution, and then utilize them for metrics estimation. The\nexperimental results show the advantages of using the new approaches for\nevaluating recommendation algorithms based on top-k metrics.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 05:08:21 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 06:04:29 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Jin", "Ruoming", ""], ["Li", "Dong", ""], ["Mudrak", "Benjamin", ""], ["Gao", "Jing", ""], ["Liu", "Zhi", ""]]}, {"id": "2103.01534", "submitter": "Yu Cao", "authors": "Yu Cao, Liang Ding, Zhiliang Tian, Meng Fang", "title": "Towards Efficiently Diversifying Dialogue Generation via Embedding\n  Augmentation", "comments": "5 pages, 2 figures, ICASSP2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Dialogue generation models face the challenge of producing generic and\nrepetitive responses. Unlike previous augmentation methods that mostly focus on\ntoken manipulation and ignore the essential variety within a single sample\nusing hard labels, we propose to promote the generation diversity of the neural\ndialogue models via soft embedding augmentation along with soft labels in this\npaper. Particularly, we select some key input tokens and fuse their embeddings\ntogether with embeddings from their semantic-neighbor tokens. The new\nembeddings serve as the input of the model to replace the original one.\nBesides, soft labels are used in loss calculation, resulting in multi-target\nsupervision for a given input. Our experimental results on two datasets\nillustrate that our proposed method is capable of generating more diverse\nresponses than raw models while remains a similar n-gram accuracy that ensures\nthe quality of generated responses.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 07:28:56 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Cao", "Yu", ""], ["Ding", "Liang", ""], ["Tian", "Zhiliang", ""], ["Fang", "Meng", ""]]}, {"id": "2103.01544", "submitter": "Ashutosh Modi", "authors": "Aaditya Singh and Shreeshail Hingane and Saim Wani and Ashutosh Modi", "title": "An End-to-End Network for Emotion-Cause Pair Extraction", "comments": "Accepted at WASSA-2021, 5 Pages + 2 Pages (references) + 2 Pages\n  (Appendix)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The task of Emotion-Cause Pair Extraction (ECPE) aims to extract all\npotential clause-pairs of emotions and their corresponding causes in a\ndocument. Unlike the more well-studied task of Emotion Cause Extraction (ECE),\nECPE does not require the emotion clauses to be provided as annotations.\nPrevious works on ECPE have either followed a multi-stage approach where\nemotion extraction, cause extraction, and pairing are done independently or use\ncomplex architectures to resolve its limitations. In this paper, we propose an\nend-to-end model for the ECPE task. Due to the unavailability of an English\nlanguage ECPE corpus, we adapt the NTCIR-13 ECE corpus and establish a baseline\nfor the ECPE task on this dataset. On this dataset, the proposed method\nproduces significant performance improvements (~6.5 increase in F1 score) over\nthe multi-stage approach and achieves comparable performance to the\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 08:03:03 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 06:57:09 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Singh", "Aaditya", ""], ["Hingane", "Shreeshail", ""], ["Wani", "Saim", ""], ["Modi", "Ashutosh", ""]]}, {"id": "2103.01548", "submitter": "Bingyan Liu", "authors": "Bingyan Liu, Yao Guo, Xiangqun Chen", "title": "PFA: Privacy-preserving Federated Adaptation for Effective Model\n  Personalization", "comments": "This paper has been accepted by WWW2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning (FL) has become a prevalent distributed machine learning\nparadigm with improved privacy. After learning, the resulting federated model\nshould be further personalized to each different client. While several methods\nhave been proposed to achieve personalization, they are typically limited to a\nsingle local device, which may incur bias or overfitting since data in a single\ndevice is extremely limited. In this paper, we attempt to realize\npersonalization beyond a single client. The motivation is that during FL, there\nmay exist many clients with similar data distribution, and thus the\npersonalization performance could be significantly boosted if these similar\nclients can cooperate with each other. Inspired by this, this paper introduces\na new concept called federated adaptation, targeting at adapting the trained\nmodel in a federated manner to achieve better personalization results. However,\nthe key challenge for federated adaptation is that we could not outsource any\nraw data from the client during adaptation, due to privacy concerns. In this\npaper, we propose PFA, a framework to accomplish Privacy-preserving Federated\nAdaptation. PFA leverages the sparsity property of neural networks to generate\nprivacy-preserving representations and uses them to efficiently identify\nclients with similar data distributions. Based on the grouping results, PFA\nconducts an FL process in a group-wise way on the federated model to accomplish\nthe adaptation. For evaluation, we manually construct several practical FL\ndatasets based on public datasets in order to simulate both the class-imbalance\nand background-difference conditions. Extensive experiments on these datasets\nand popular model architectures demonstrate the effectiveness of PFA,\noutperforming other state-of-the-art methods by a large margin while ensuring\nuser privacy. We will release our code at: https://github.com/lebyni/PFA.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 08:07:34 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 16:11:55 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Liu", "Bingyan", ""], ["Guo", "Yao", ""], ["Chen", "Xiangqun", ""]]}, {"id": "2103.01600", "submitter": "Parikshit Bansal", "authors": "Parikshit Bansal, Prathamesh Deshpande, Sunita Sarawagi", "title": "Missing Value Imputation on Multidimensional Time Series", "comments": "13 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  We present DeepMVI, a deep learning method for missing value imputation in\nmultidimensional time-series datasets. Missing values are commonplace in\ndecision support platforms that aggregate data over long time stretches from\ndisparate sources, and reliable data analytics calls for careful handling of\nmissing data. One strategy is imputing the missing values, and a wide variety\nof algorithms exist spanning simple interpolation, matrix factorization methods\nlike SVD, statistical models like Kalman filters, and recent deep learning\nmethods. We show that often these provide worse results on aggregate analytics\ncompared to just excluding the missing data. DeepMVI uses a neural network to\ncombine fine-grained and coarse-grained patterns along a time series, and\ntrends from related series across categorical dimensions. After failing with\noff-the-shelf neural architectures, we design our own network that includes a\ntemporal transformer with a novel convolutional window feature, and kernel\nregression with learned embeddings. The parameters and their training are\ndesigned carefully to generalize across different placements of missing blocks\nand data characteristics. Experiments across nine real datasets, four different\nmissing scenarios, comparing seven existing methods show that DeepMVI is\nsignificantly more accurate, reducing error by more than 50% in more than half\nthe cases, compared to the best existing method. Although slower than simpler\nmatrix factorization methods, we justify the increased time overheads by\nshowing that DeepMVI is the only option that provided overall more accurate\nanalytics than dropping missing values.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 09:55:05 GMT"}, {"version": "v2", "created": "Fri, 16 Jul 2021 04:56:38 GMT"}], "update_date": "2021-07-19", "authors_parsed": [["Bansal", "Parikshit", ""], ["Deshpande", "Prathamesh", ""], ["Sarawagi", "Sunita", ""]]}, {"id": "2103.01636", "submitter": "Decebal Constantin Mocanu", "authors": "Decebal Constantin Mocanu, Elena Mocanu, Tiago Pinto, Selima Curci,\n  Phuong H. Nguyen, Madeleine Gibescu, Damien Ernst, Zita A. Vale", "title": "Sparse Training Theory for Scalable and Efficient Agents", "comments": null, "journal-ref": "20th International Conference on Autonomous Agents and Multiagent\n  Systems (AAMAS 2021)", "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.MA cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental task for artificial intelligence is learning. Deep Neural\nNetworks have proven to cope perfectly with all learning paradigms, i.e.\nsupervised, unsupervised, and reinforcement learning. Nevertheless, traditional\ndeep learning approaches make use of cloud computing facilities and do not\nscale well to autonomous agents with low computational resources. Even in the\ncloud, they suffer from computational and memory limitations, and they cannot\nbe used to model adequately large physical worlds for agents which assume\nnetworks with billions of neurons. These issues are addressed in the last few\nyears by the emerging topic of sparse training, which trains sparse networks\nfrom scratch. This paper discusses sparse training state-of-the-art, its\nchallenges and limitations while introducing a couple of new theoretical\nresearch directions which has the potential of alleviating sparse training\nlimitations to push deep learning scalability well beyond its current\nboundaries. Nevertheless, the theoretical advancements impact in complex\nmulti-agents settings is discussed from a real-world perspective, using the\nsmart grid case study.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 10:48:29 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Mocanu", "Decebal Constantin", ""], ["Mocanu", "Elena", ""], ["Pinto", "Tiago", ""], ["Curci", "Selima", ""], ["Nguyen", "Phuong H.", ""], ["Gibescu", "Madeleine", ""], ["Ernst", "Damien", ""], ["Vale", "Zita A.", ""]]}, {"id": "2103.01644", "submitter": "Albert Dulian", "authors": "Albert Dulian and John C. Murray", "title": "Exploiting latent representation of sparse semantic layers for improved\n  short-term motion prediction with Capsule Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As urban environments manifest high levels of complexity it is of vital\nimportance that safety systems embedded within autonomous vehicles (AVs) are\nable to accurately anticipate short-term future motion of nearby agents. This\nproblem can be further understood as generating a sequence of coordinates\ndescribing the future motion of the tracked agent. Various proposed approaches\ndemonstrate significant benefits of using a rasterised top-down image of the\nroad, with a combination of Convolutional Neural Networks (CNNs), for\nextraction of relevant features that define the road structure (eg. driveable\nareas, lanes, walkways). In contrast, this paper explores use of Capsule\nNetworks (CapsNets) in the context of learning a hierarchical representation of\nsparse semantic layers corresponding to small regions of the High-Definition\n(HD) map. Each region of the map is dismantled into separate geometrical layers\nthat are extracted with respect to the agent's current position. By using an\narchitecture based on CapsNets the model is able to retain hierarchical\nrelationships between detected features within images whilst also preventing\nloss of spatial data often caused by the pooling operation. We train and\nevaluate our model on publicly available dataset nuTonomy scenes and compare it\nto recently published methods. We show that our model achieves significant\nimprovement over recently published works on deterministic prediction, whilst\ndrastically reducing the overall size of the network.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 11:13:43 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 19:41:08 GMT"}, {"version": "v3", "created": "Thu, 25 Mar 2021 20:40:24 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Dulian", "Albert", ""], ["Murray", "John C.", ""]]}, {"id": "2103.01696", "submitter": "Feng Zhu", "authors": "Feng Zhu, Yan Wang, Chaochao Chen, Jun Zhou, Longfei Li, Guanfeng Liu", "title": "Cross-Domain Recommendation: Challenges, Progress, and Prospects", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To address the long-standing data sparsity problem in recommender systems\n(RSs), cross-domain recommendation (CDR) has been proposed to leverage the\nrelatively richer information from a richer domain to improve the\nrecommendation performance in a sparser domain. Although CDR has been\nextensively studied in recent years, there is a lack of a systematic review of\nthe existing CDR approaches. To fill this gap, in this paper, we provide a\ncomprehensive review of existing CDR approaches, including challenges, research\nprogress, and future directions. Specifically, we first summarize existing CDR\napproaches into four types, including single-target CDR, multi-domain\nrecommendation, dual-target CDR, and multi-target CDR. We then present the\ndefinitions and challenges of these CDR approaches. Next, we propose a\nfull-view categorization and new taxonomies on these approaches and report\ntheir research progress in detail. In the end, we share several promising\nresearch directions in CDR.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 12:58:08 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Zhu", "Feng", ""], ["Wang", "Yan", ""], ["Chen", "Chaochao", ""], ["Zhou", "Jun", ""], ["Li", "Longfei", ""], ["Liu", "Guanfeng", ""]]}, {"id": "2103.01713", "submitter": "Noortje Venhuizen", "authors": "Noortje J. Venhuizen and Petra Hendriks and Matthew W. Crocker and\n  Harm Brouwer", "title": "Distributional Formal Semantics", "comments": "To appear in: Information and Computation (WoLLIC 2019 Special Issue)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IT math.IT", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Natural language semantics has recently sought to combine the complementary\nstrengths of formal and distributional approaches to meaning. More\nspecifically, proposals have been put forward to augment formal semantic\nmachinery with distributional meaning representations, thereby introducing the\nnotion of semantic similarity into formal semantics, or to define\ndistributional systems that aim to incorporate formal notions such as\nentailment and compositionality. However, given the fundamentally different\n'representational currency' underlying formal and distributional approaches -\nmodels of the world versus linguistic co-occurrence - their unification has\nproven extremely difficult. Here, we define a Distributional Formal Semantics\nthat integrates distributionality into a formal semantic system on the level of\nformal models. This approach offers probabilistic, distributed meaning\nrepresentations that are also inherently compositional, and that naturally\ncapture fundamental semantic notions such as quantification and entailment.\nFurthermore, we show how the probabilistic nature of these representations\nallows for probabilistic inference, and how the information-theoretic notion of\n\"information\" (measured in terms of Entropy and Surprisal) naturally follows\nfrom it. Finally, we illustrate how meaning representations can be derived\nincrementally from linguistic input using a recurrent neural network model, and\nhow the resultant incremental semantic construction procedure intuitively\ncaptures key semantic phenomena, including negation, presupposition, and\nanaphoricity.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 13:38:00 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Venhuizen", "Noortje J.", ""], ["Hendriks", "Petra", ""], ["Crocker", "Matthew W.", ""], ["Brouwer", "Harm", ""]]}, {"id": "2103.01719", "submitter": "Hikaru Shindo", "authors": "Hikaru Shindo, Masaaki Nishino, Akihiro Yamamoto", "title": "Differentiable Inductive Logic Programming for Structured Examples", "comments": "Accepted by AAAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The differentiable implementation of logic yields a seamless combination of\nsymbolic reasoning and deep neural networks. Recent research, which has\ndeveloped a differentiable framework to learn logic programs from examples, can\neven acquire reasonable solutions from noisy datasets. However, this framework\nseverely limits expressions for solutions, e.g., no function symbols are\nallowed, and the shapes of clauses are fixed. As a result, the framework cannot\ndeal with structured examples. Therefore we propose a new framework to learn\nlogic programs from noisy and structured examples, including the following\ncontributions. First, we propose an adaptive clause search method by looking\nthrough structured space, which is defined by the generality of the clauses, to\nyield an efficient search space for differentiable solvers. Second, we propose\nfor ground atoms an enumeration algorithm, which determines a necessary and\nsufficient set of ground atoms to perform differentiable inference functions.\nFinally, we propose a new method to compose logic programs softly, enabling the\nsystem to deal with complex programs consisting of several clauses. Our\nexperiments show that our new framework can learn logic programs from noisy and\nstructured examples, such as sequences or trees. Our framework can be scaled to\ndeal with complex programs that consist of several clauses with function\nsymbols.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 13:47:33 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Shindo", "Hikaru", ""], ["Nishino", "Masaaki", ""], ["Yamamoto", "Akihiro", ""]]}, {"id": "2103.01730", "submitter": "Elvin Isufi", "authors": "Elvin Isufi and Gabriele Mazzola", "title": "Graph-Time Convolutional Neural Networks", "comments": "IEEE Data Science and Learning Workshop, Toronto, Canada, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE eess.SP", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Spatiotemporal data can be represented as a process over a graph, which\ncaptures their spatial relationships either explicitly or implicitly. How to\nleverage such a structure for learning representations is one of the key\nchallenges when working with graphs. In this paper, we represent the\nspatiotemporal relationships through product graphs and develop a first\nprinciple graph-time convolutional neural network (GTCNN). The GTCNN is a\ncompositional architecture with each layer comprising a graph-time\nconvolutional module, a graph-time pooling module, and a nonlinearity. We\ndevelop a graph-time convolutional filter by following the shift-and-sum\nprinciples of the convolutional operator to learn higher-level features over\nthe product graph. The product graph itself is parametric so that we can learn\nalso the spatiotemporal coupling from data. We develop a zero-pad pooling that\npreserves the spatial graph (the prior about the data) while reducing the\nnumber of active nodes and the parameters. Experimental results with synthetic\nand real data corroborate the different components and compare with baseline\nand state-of-the-art solutions.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 14:03:44 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Isufi", "Elvin", ""], ["Mazzola", "Gabriele", ""]]}, {"id": "2103.01737", "submitter": "Xinting Hu", "authors": "Xinting Hu, Kaihua Tang, Chunyan Miao, Xian-Sheng Hua, Hanwang Zhang", "title": "Distilling Causal Effect of Data in Class-Incremental Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a causal framework to explain the catastrophic forgetting in\nClass-Incremental Learning (CIL) and then derive a novel distillation method\nthat is orthogonal to the existing anti-forgetting techniques, such as data\nreplay and feature/label distillation. We first 1) place CIL into the\nframework, 2) answer why the forgetting happens: the causal effect of the old\ndata is lost in new training, and then 3) explain how the existing techniques\nmitigate it: they bring the causal effect back. Based on the framework, we find\nthat although the feature/label distillation is storage-efficient, its causal\neffect is not coherent with the end-to-end feature learning merit, which is\nhowever preserved by data replay. To this end, we propose to distill the\nColliding Effect between the old and the new data, which is fundamentally\nequivalent to the causal effect of data replay, but without any cost of replay\nstorage. Thanks to the causal effect analysis, we can further capture the\nIncremental Momentum Effect of the data stream, removing which can help to\nretain the old effect overwhelmed by the new data effect, and thus alleviate\nthe forgetting of the old class in testing. Extensive experiments on three CIL\nbenchmarks: CIFAR-100, ImageNet-Sub&Full, show that the proposed causal effect\ndistillation can improve various state-of-the-art CIL methods by a large margin\n(0.72%--9.06%).\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 14:14:10 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2021 08:37:50 GMT"}, {"version": "v3", "created": "Mon, 8 Mar 2021 03:16:37 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Hu", "Xinting", ""], ["Tang", "Kaihua", ""], ["Miao", "Chunyan", ""], ["Hua", "Xian-Sheng", ""], ["Zhang", "Hanwang", ""]]}, {"id": "2103.01760", "submitter": "Hilmi Enes Egilmez", "authors": "Hilmi E. Egilmez, Ankitesh K. Singh, Muhammed Coban, Marta Karczewicz,\n  Yinhao Zhu, Yang Yang, Amir Said, Taco S. Cohen", "title": "Transform Network Architectures for Deep Learning based End-to-End\n  Image/Video Coding in Subsampled Color Spaces", "comments": "10 pages, submitted to an IEEE journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.AI cs.CV cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the existing deep learning based end-to-end image/video coding (DLEC)\narchitectures are designed for non-subsampled RGB color format. However, in\norder to achieve a superior coding performance, many state-of-the-art\nblock-based compression standards such as High Efficiency Video Coding\n(HEVC/H.265) and Versatile Video Coding (VVC/H.266) are designed primarily for\nYUV 4:2:0 format, where U and V components are subsampled by considering the\nhuman visual system. This paper investigates various DLEC designs to support\nYUV 4:2:0 format by comparing their performance against the main profiles of\nHEVC and VVC standards under a common evaluation framework. Moreover, a new\ntransform network architecture is proposed to improve the efficiency of coding\nYUV 4:2:0 data. The experimental results on YUV 4:2:0 datasets show that the\nproposed architecture significantly outperforms naive extensions of existing\narchitectures designed for RGB format and achieves about 10% average BD-rate\nimprovement over the intra-frame coding in HEVC.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 06:47:27 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Egilmez", "Hilmi E.", ""], ["Singh", "Ankitesh K.", ""], ["Coban", "Muhammed", ""], ["Karczewicz", "Marta", ""], ["Zhu", "Yinhao", ""], ["Yang", "Yang", ""], ["Said", "Amir", ""], ["Cohen", "Taco S.", ""]]}, {"id": "2103.01776", "submitter": "Huansheng Ning Prof", "authors": "Sahraoui Dhelim, Huansheng Ning, Fadi Farha, Liming Chen, Luigi Atzori\n  and Mahmoud Daneshmand", "title": "IoT-Enabled Social Relationships Meet Artificial Social Intelligence", "comments": "IEEE Internet of Things Journal (2021)", "journal-ref": null, "doi": "10.1109/JIOT.2021.3081556", "report-no": null, "categories": "cs.CY cs.AI cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the recent advances of the Internet of Things, and the increasing\naccessibility of ubiquitous computing resources and mobile devices, the\nprevalence of rich media contents, and the ensuing social, economic, and\ncultural changes, computing technology and applications have evolved quickly\nover the past decade. They now go beyond personal computing, facilitating\ncollaboration and social interactions in general, causing a quick proliferation\nof social relationships among IoT entities. The increasing number of these\nrelationships and their heterogeneous social features have led to computing and\ncommunication bottlenecks that prevent the IoT network from taking advantage of\nthese relationships to improve the offered services and customize the delivered\ncontent, known as relationship explosion. On the other hand, the quick advances\nin artificial intelligence applications in social computing have led to the\nemerging of a promising research field known as Artificial Social Intelligence\n(ASI) that has the potential to tackle the social relationship explosion\nproblem. This paper discusses the role of IoT in social relationships detection\nand management, the problem of social relationships explosion in IoT and\nreviews the proposed solutions using ASI, including social-oriented\nmachine-learning and deep-learning techniques.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 09:07:32 GMT"}, {"version": "v2", "created": "Tue, 25 May 2021 19:43:27 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Dhelim", "Sahraoui", ""], ["Ning", "Huansheng", ""], ["Farha", "Fadi", ""], ["Chen", "Liming", ""], ["Atzori", "Luigi", ""], ["Daneshmand", "Mahmoud", ""]]}, {"id": "2103.01785", "submitter": "Felix Mohr", "authors": "Felix Mohr, Gonzalo Mej\\'ia, Francisco Yuraszeck", "title": "Single and Parallel Machine Scheduling with Variable Release Dates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper we study a simple extension of the total weighted flowtime\nminimization problem for single and identical parallel machines. While the\nstandard problem simply defines a set of jobs with their processing times and\nweights and assumes that all jobs have release date 0 and have no deadline, we\nassume that the release date of each job is a decision variable that is only\nconstrained by a single global latest arrival deadline. To our knowledge, this\nsimple yet practically highly relevant extension has never been studied. Our\nmain contribution is that we show the NP- completeness of the problem even for\nthe single machine case and provide an exhaustive empirical study of different\ntypical approaches including genetic algorithms, tree search, and constraint\nprogramming.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 14:52:28 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Mohr", "Felix", ""], ["Mej\u00eda", "Gonzalo", ""], ["Yuraszeck", "Francisco", ""]]}, {"id": "2103.01834", "submitter": "Zhengzhong Liu", "authors": "Zhengzhong Liu, Guanxiong Ding, Avinash Bukkittu, Mansi Gupta, Pengzhi\n  Gao, Atif Ahmed, Shikun Zhang, Xin Gao, Swapnil Singhavi, Linwei Li, Wei Wei,\n  Zecong Hu, Haoran Shi, Xiaodan Liang, Teruko Mitamura, Eric P. Xing, and\n  Zhiting Hu", "title": "A Data-Centric Framework for Composable NLP Workflows", "comments": "8 pages, 4 figures, EMNLP 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Empirical natural language processing (NLP) systems in application domains\n(e.g., healthcare, finance, education) involve interoperation among multiple\ncomponents, ranging from data ingestion, human annotation, to text retrieval,\nanalysis, generation, and visualization. We establish a unified open-source\nframework to support fast development of such sophisticated NLP workflows in a\ncomposable manner. The framework introduces a uniform data representation to\nencode heterogeneous results by a wide range of NLP tasks. It offers a large\nrepository of processors for NLP tasks, visualization, and annotation, which\ncan be easily assembled with full interoperability under the unified\nrepresentation. The highly extensible framework allows plugging in custom\nprocessors from external off-the-shelf NLP and deep learning libraries. The\nwhole framework is delivered through two modularized yet integratable\nopen-source projects, namely Forte1 (for workflow infrastructure and NLP\nfunction processors) and Stave2 (for user interaction, visualization, and\nannotation).\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 16:19:44 GMT"}, {"version": "v2", "created": "Wed, 3 Mar 2021 02:57:35 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Liu", "Zhengzhong", ""], ["Ding", "Guanxiong", ""], ["Bukkittu", "Avinash", ""], ["Gupta", "Mansi", ""], ["Gao", "Pengzhi", ""], ["Ahmed", "Atif", ""], ["Zhang", "Shikun", ""], ["Gao", "Xin", ""], ["Singhavi", "Swapnil", ""], ["Li", "Linwei", ""], ["Wei", "Wei", ""], ["Hu", "Zecong", ""], ["Shi", "Haoran", ""], ["Liang", "Xiaodan", ""], ["Mitamura", "Teruko", ""], ["Xing", "Eric P.", ""], ["Hu", "Zhiting", ""]]}, {"id": "2103.01863", "submitter": "Ramakanth Pasunuru", "authors": "Ramakanth Pasunuru, Asli Celikyilmaz, Michel Galley, Chenyan Xiong,\n  Yizhe Zhang, Mohit Bansal, Jianfeng Gao", "title": "Data Augmentation for Abstractive Query-Focused Multi-Document\n  Summarization", "comments": "AAAI 2021 (13 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The progress in Query-focused Multi-Document Summarization (QMDS) has been\nlimited by the lack of sufficient largescale high-quality training datasets. We\npresent two QMDS training datasets, which we construct using two data\naugmentation methods: (1) transferring the commonly used single-document\nCNN/Daily Mail summarization dataset to create the QMDSCNN dataset, and (2)\nmining search-query logs to create the QMDSIR dataset. These two datasets have\ncomplementary properties, i.e., QMDSCNN has real summaries but queries are\nsimulated, while QMDSIR has real queries but simulated summaries. To cover both\nthese real summary and query aspects, we build abstractive end-to-end neural\nnetwork models on the combined datasets that yield new state-of-the-art\ntransfer results on DUC datasets. We also introduce new hierarchical encoders\nthat enable a more efficient encoding of the query together with multiple\ndocuments. Empirical results demonstrate that our data augmentation and\nencoding methods outperform baseline models on automatic metrics, as well as on\nhuman evaluations along multiple attributes.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 16:57:01 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Pasunuru", "Ramakanth", ""], ["Celikyilmaz", "Asli", ""], ["Galley", "Michel", ""], ["Xiong", "Chenyan", ""], ["Zhang", "Yizhe", ""], ["Bansal", "Mohit", ""], ["Gao", "Jianfeng", ""]]}, {"id": "2103.01867", "submitter": "Ramakanth Pasunuru", "authors": "Ramakanth Pasunuru, David Rosenberg, Gideon Mann, Mohit Bansal", "title": "Dual Reinforcement-Based Specification Generation for Image De-Rendering", "comments": "AAAI 2021 Scientific Document Understanding Workshop (9 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Advances in deep learning have led to promising progress in inferring\ngraphics programs by de-rendering computer-generated images. However, current\nmethods do not explore which decoding methods lead to better inductive bias for\ninferring graphics programs. In our work, we first explore the effectiveness of\nLSTM-RNN versus Transformer networks as decoders for order-independent graphics\nprograms. Since these are sequence models, we must choose an ordering of the\nobjects in the graphics programs for likelihood training. We found that the\nLSTM performance was highly sensitive to the sequence ordering (random order\nvs. pattern-based order), while Transformer performance was roughly independent\nof the sequence ordering. Further, we present a policy gradient based\nreinforcement learning approach for better inductive bias in the decoder via\nmultiple diverse rewards based both on the graphics program specification and\nthe rendered image. We also explore the combination of these complementary\nrewards. We achieve state-of-the-art results on two graphics program generation\ndatasets.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 17:04:56 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Pasunuru", "Ramakanth", ""], ["Rosenberg", "David", ""], ["Mann", "Gideon", ""], ["Bansal", "Mohit", ""]]}, {"id": "2103.01890", "submitter": "Neil Jethani", "authors": "Neil Jethani, Mukund Sudarshan, Yindalon Aphinyanaphongs, Rajesh\n  Ranganath", "title": "Have We Learned to Explain?: How Interpretability Methods Can Learn to\n  Encode Predictions in their Interpretations", "comments": "15 pages, 3 figures, Proceedings of the 24th International Conference\n  on Artificial Intelligence and Statistics (AISTATS) 2021", "journal-ref": "Proceedings of the 24th International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2021", "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  While the need for interpretable machine learning has been established, many\ncommon approaches are slow, lack fidelity, or hard to evaluate. Amortized\nexplanation methods reduce the cost of providing interpretations by learning a\nglobal selector model that returns feature importances for a single instance of\ndata. The selector model is trained to optimize the fidelity of the\ninterpretations, as evaluated by a predictor model for the target. Popular\nmethods learn the selector and predictor model in concert, which we show allows\npredictions to be encoded within interpretations. We introduce EVAL-X as a\nmethod to quantitatively evaluate interpretations and REAL-X as an amortized\nexplanation method, which learn a predictor model that approximates the true\ndata generating distribution given any subset of the input. We show EVAL-X can\ndetect when predictions are encoded in interpretations and show the advantages\nof REAL-X through quantitative and radiologist evaluation.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 17:42:33 GMT"}], "update_date": "2021-03-03", "authors_parsed": [["Jethani", "Neil", ""], ["Sudarshan", "Mukund", ""], ["Aphinyanaphongs", "Yindalon", ""], ["Ranganath", "Rajesh", ""]]}, {"id": "2103.01933", "submitter": "Tianmin Shu", "authors": "Aviv Netanyahu, Tianmin Shu, Boris Katz, Andrei Barbu, Joshua B.\n  Tenenbaum", "title": "PHASE: PHysically-grounded Abstract Social Events for Machine Social\n  Perception", "comments": "The first two authors contributed equally; AAAI 2021; 13 pages, 7\n  figures; Project page: https://www.tshu.io/PHASE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to perceive and reason about social interactions in the context\nof physical environments is core to human social intelligence and human-machine\ncooperation. However, no prior dataset or benchmark has systematically\nevaluated physically grounded perception of complex social interactions that go\nbeyond short actions, such as high-fiving, or simple group activities, such as\ngathering. In this work, we create a dataset of physically-grounded abstract\nsocial events, PHASE, that resemble a wide range of real-life social\ninteractions by including social concepts such as helping another agent. PHASE\nconsists of 2D animations of pairs of agents moving in a continuous space\ngenerated procedurally using a physics engine and a hierarchical planner.\nAgents have a limited field of view, and can interact with multiple objects, in\nan environment that has multiple landmarks and obstacles. Using PHASE, we\ndesign a social recognition task and a social prediction task. PHASE is\nvalidated with human experiments demonstrating that humans perceive rich\ninteractions in the social events, and that the simulated agents behave\nsimilarly to humans. As a baseline model, we introduce a Bayesian inverse\nplanning approach, SIMPLE (SIMulation, Planning and Local Estimation), which\noutperforms state-of-the-art feed-forward neural networks. We hope that PHASE\ncan serve as a difficult new challenge for developing new models that can\nrecognize complex social interactions.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 18:44:57 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 20:13:29 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Netanyahu", "Aviv", ""], ["Shu", "Tianmin", ""], ["Katz", "Boris", ""], ["Barbu", "Andrei", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "2103.01937", "submitter": "Aniket Didolkar", "authors": "Anirudh Goyal, Aniket Didolkar, Nan Rosemary Ke, Charles Blundell,\n  Philippe Beaudoin, Nicolas Heess, Michael Mozer, Yoshua Bengio", "title": "Neural Production Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Visual environments are structured, consisting of distinct objects or\nentities. These entities have properties -- both visible and latent -- that\ndetermine the manner in which they interact with one another. To partition\nimages into entities, deep-learning researchers have proposed structural\ninductive biases such as slot-based architectures. To model interactions among\nentities, equivariant graph neural nets (GNNs) are used, but these are not\nparticularly well suited to the task for two reasons. First, GNNs do not\npredispose interactions to be sparse, as relationships among independent\nentities are likely to be. Second, GNNs do not factorize knowledge about\ninteractions in an entity-conditional manner. As an alternative, we take\ninspiration from cognitive science and resurrect a classic approach, production\nsystems, which consist of a set of rule templates that are applied by binding\nplaceholder variables in the rules to specific entities. Rules are scored on\ntheir match to entities, and the best fitting rules are applied to update\nentity properties. In a series of experiments, we demonstrate that this\narchitecture achieves a flexible, dynamic flow of control and serves to\nfactorize entity-specific and rule-based information. This disentangling of\nknowledge achieves robust future-state prediction in rich visual environments,\noutperforming state-of-the-art methods using GNNs, and allows for the\nextrapolation from simple (few object) environments to more complex\nenvironments.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 18:53:20 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 18:00:29 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Goyal", "Anirudh", ""], ["Didolkar", "Aniket", ""], ["Ke", "Nan Rosemary", ""], ["Blundell", "Charles", ""], ["Beaudoin", "Philippe", ""], ["Heess", "Nicolas", ""], ["Mozer", "Michael", ""], ["Bengio", "Yoshua", ""]]}, {"id": "2103.01955", "submitter": "Akash Velu", "authors": "Chao Yu, Akash Velu, Eugene Vinitsky, Yu Wang, Alexandre Bayen, Yi Wu", "title": "The Surprising Effectiveness of PPO in Cooperative, Multi-Agent Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proximal Policy Optimization (PPO) is a popular on-policy reinforcement\nlearning algorithm but is significantly less utilized than off-policy learning\nalgorithms in multi-agent settings. This is often due the belief that on-policy\nmethods are significantly less sample efficient than their off-policy\ncounterparts in multi-agent problems. In this work, we investigate Multi-Agent\nPPO (MAPPO), a variant of PPO which is specialized for multi-agent settings.\nUsing a 1-GPU desktop, we show that MAPPO achieves surprisingly strong\nperformance in three popular multi-agent testbeds: the particle-world\nenvironments, the Starcraft multi-agent challenge, and the Hanabi challenge,\nwith minimal hyperparameter tuning and without any domain-specific algorithmic\nmodifications or architectures. In the majority of environments, we find that\ncompared to off-policy baselines, MAPPO achieves strong results while\nexhibiting comparable sample efficiency. Finally, through ablation studies, we\npresent the implementation and algorithmic factors which are most influential\nto MAPPO's practical performance.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 18:59:56 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 23:45:06 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Yu", "Chao", ""], ["Velu", "Akash", ""], ["Vinitsky", "Eugene", ""], ["Wang", "Yu", ""], ["Bayen", "Alexandre", ""], ["Wu", "Yi", ""]]}, {"id": "2103.01988", "submitter": "Priya Goyal", "authors": "Priya Goyal, Mathilde Caron, Benjamin Lefaudeux, Min Xu, Pengchao\n  Wang, Vivek Pai, Mannat Singh, Vitaliy Liptchinsky, Ishan Misra, Armand\n  Joulin, Piotr Bojanowski", "title": "Self-supervised Pretraining of Visual Features in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, self-supervised learning methods like MoCo, SimCLR, BYOL and SwAV\nhave reduced the gap with supervised methods. These results have been achieved\nin a control environment, that is the highly curated ImageNet dataset. However,\nthe premise of self-supervised learning is that it can learn from any random\nimage and from any unbounded dataset. In this work, we explore if\nself-supervision lives to its expectation by training large models on random,\nuncurated images with no supervision. Our final SElf-supERvised (SEER) model, a\nRegNetY with 1.3B parameters trained on 1B random images with 512 GPUs achieves\n84.2% top-1 accuracy, surpassing the best self-supervised pretrained model by\n1% and confirming that self-supervised learning works in a real world setting.\nInterestingly, we also observe that self-supervised models are good few-shot\nlearners achieving 77.9% top-1 with access to only 10% of ImageNet. Code:\nhttps://github.com/facebookresearch/vissl\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 19:12:29 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 13:53:36 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Goyal", "Priya", ""], ["Caron", "Mathilde", ""], ["Lefaudeux", "Benjamin", ""], ["Xu", "Min", ""], ["Wang", "Pengchao", ""], ["Pai", "Vivek", ""], ["Singh", "Mannat", ""], ["Liptchinsky", "Vitaliy", ""], ["Misra", "Ishan", ""], ["Joulin", "Armand", ""], ["Bojanowski", "Piotr", ""]]}, {"id": "2103.01991", "submitter": "Izzeddin Gur", "authors": "Izzeddin Gur, Natasha Jaques, Kevin Malta, Manoj Tiwari, Honglak Lee,\n  Aleksandra Faust", "title": "Adversarial Environment Generation for Learning to Navigate the Web", "comments": "Presented at Deep RL Workshop, NeurIPS, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to autonomously navigate the web is a difficult sequential decision\nmaking task. The state and action spaces are large and combinatorial in nature,\nand websites are dynamic environments consisting of several pages. One of the\nbottlenecks of training web navigation agents is providing a learnable\ncurriculum of training environments that can cover the large variety of\nreal-world websites. Therefore, we propose using Adversarial Environment\nGeneration (AEG) to generate challenging web environments in which to train\nreinforcement learning (RL) agents. We provide a new benchmarking environment,\ngMiniWoB, which enables an RL adversary to use compositional primitives to\nlearn to generate arbitrarily complex websites. To train the adversary, we\npropose a new technique for maximizing regret using the difference in the\nscores obtained by a pair of navigator agents. Our results show that our\napproach significantly outperforms prior methods for minimax regret AEG. The\nregret objective trains the adversary to design a curriculum of environments\nthat are \"just-the-right-challenge\" for the navigator agents; our results show\nthat over time, the adversary learns to generate increasingly complex web\nnavigation tasks. The navigator agents trained with our technique learn to\ncomplete challenging, high-dimensional web navigation tasks, such as form\nfilling, booking a flight etc. We show that the navigator agent trained with\nour proposed Flexible b-PAIRED technique significantly outperforms competitive\nautomatic curriculum generation baselines -- including a state-of-the-art RL\nweb navigation approach -- on a set of challenging unseen test environments,\nand achieves more than 80% success rate on some tasks.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 19:19:30 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Gur", "Izzeddin", ""], ["Jaques", "Natasha", ""], ["Malta", "Kevin", ""], ["Tiwari", "Manoj", ""], ["Lee", "Honglak", ""], ["Faust", "Aleksandra", ""]]}, {"id": "2103.02018", "submitter": "Siwei Lyu", "authors": "Yuezun Li, Cong Zhang, Pu Sun, Honggang Qi, and Siwei Lyu", "title": "DeepFake-o-meter: An Open Platform for DeepFake Detection", "comments": "submitted to SAPDE 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, the advent of deep learning-based techniques and the\nsignificant reduction in the cost of computation resulted in the feasibility of\ncreating realistic videos of human faces, commonly known as DeepFakes. The\navailability of open-source tools to create DeepFakes poses as a threat to the\ntrustworthiness of the online media. In this work, we develop an open-source\nonline platform, known as DeepFake-o-meter, that integrates state-of-the-art\nDeepFake detection methods and provide a convenient interface for the users. We\ndescribe the design and function of DeepFake-o-meter in this work.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 20:45:33 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Li", "Yuezun", ""], ["Zhang", "Cong", ""], ["Sun", "Pu", ""], ["Qi", "Honggang", ""], ["Lyu", "Siwei", ""]]}, {"id": "2103.02023", "submitter": "Enzo Tartaglione", "authors": "Enzo Tartaglione, Carlo Alberto Barbano, Marco Grangetto", "title": "EnD: Entangling and Disentangling deep representations for bias\n  correction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Artificial neural networks perform state-of-the-art in an ever-growing number\nof tasks, and nowadays they are used to solve an incredibly large variety of\ntasks. There are problems, like the presence of biases in the training data,\nwhich question the generalization capability of these models. In this work we\npropose EnD, a regularization strategy whose aim is to prevent deep models from\nlearning unwanted biases. In particular, we insert an \"information bottleneck\"\nat a certain point of the deep neural network, where we disentangle the\ninformation about the bias, still letting the useful information for the\ntraining task forward-propagating in the rest of the model. One big advantage\nof EnD is that we do not require additional training complexity (like decoders\nor extra layers in the model), since it is a regularizer directly applied on\nthe trained model. Our experiments show that EnD effectively improves the\ngeneralization on unbiased test sets, and it can be effectively applied on\nreal-case scenarios, like removing hidden biases in the COVID-19 detection from\nradiographic images.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 20:55:42 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Tartaglione", "Enzo", ""], ["Barbano", "Carlo Alberto", ""], ["Grangetto", "Marco", ""]]}, {"id": "2103.02074", "submitter": "Marvin Chanc\\'an", "authors": "Marvin Chanc\\'an, Michael Milford", "title": "Sequential Place Learning: Heuristic-Free High-Performance Long-Term\n  Place Recognition", "comments": "Submitted to RSS 2021. 14 pages, 5 tables, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sequential matching using hand-crafted heuristics has been standard practice\nin route-based place recognition for enhancing pairwise similarity results for\nnearly a decade. However, precision-recall performance of these algorithms\ndramatically degrades when searching on short temporal window (TW) lengths,\nwhile demanding high compute and storage costs on large robotic datasets for\nautonomous navigation research. Here, influenced by biological systems that\nrobustly navigate spacetime scales even without vision, we develop a joint\nvisual and positional representation learning technique, via a sequential\nprocess, and design a learning-based CNN+LSTM architecture, trainable via\nbackpropagation through time, for viewpoint- and appearance-invariant place\nrecognition. Our approach, Sequential Place Learning (SPL), is based on a CNN\nfunction that visually encodes an environment from a single traversal, thus\nreducing storage capacity, while an LSTM temporally fuses each visual embedding\nwith corresponding positional data -- obtained from any source of motion\nestimation -- for direct sequential inference. Contrary to classical two-stage\npipelines, e.g., match-then-temporally-filter, our network directly eliminates\nfalse-positive rates while jointly learning sequence matching from a single\nmonocular image sequence, even using short TWs. Hence, we demonstrate that our\nmodel outperforms 15 classical methods while setting new state-of-the-art\nperformance standards on 4 challenging benchmark datasets, where one of them\ncan be considered solved with recall rates of 100% at 100% precision, correctly\nmatching all places under extreme sunlight-darkness changes. In addition, we\nshow that SPL can be up to 70x faster to deploy than classical methods on a 729\nkm route comprising 35,768 consecutive frames. Extensive experiments\ndemonstrate the... Baseline code available at\nhttps://github.com/mchancan/deepseqslam\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 22:57:43 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Chanc\u00e1n", "Marvin", ""], ["Milford", "Michael", ""]]}, {"id": "2103.02084", "submitter": "Cameron Voloshin", "authors": "Cameron Voloshin, Nan Jiang, Yisong Yue", "title": "Minimax Model Learning", "comments": null, "journal-ref": "PMLR, Volume 130, 2021", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a novel off-policy loss function for learning a transition model\nin model-based reinforcement learning. Notably, our loss is derived from the\noff-policy policy evaluation objective with an emphasis on correcting\ndistribution shift. Compared to previous model-based techniques, our approach\nallows for greater robustness under model misspecification or distribution\nshift induced by learning/evaluating policies that are distinct from the\ndata-generating policy. We provide a theoretical analysis and show empirical\nimprovements over existing model-based off-policy evaluation methods. We\nprovide further analysis showing our loss can be used for off-policy\noptimization (OPO) and demonstrate its integration with more recent\nimprovements in OPO.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 23:16:36 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Voloshin", "Cameron", ""], ["Jiang", "Nan", ""], ["Yue", "Yisong", ""]]}, {"id": "2103.02099", "submitter": "Alishba Imran", "authors": "Alishba Imran, William Escobar, Freidoon Barez", "title": "Design of an Affordable Prosthetic Arm Equipped with Deep Learning\n  Vision-Based Manipulation", "comments": "Pre-print paper, 7 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many amputees throughout the world are left with limited options to\npersonally own a prosthetic arm due to the expensive cost, mechanical system\ncomplexity, and lack of availability. The three main control methods of\nprosthetic hands are: (1) body-powered control, (2) extrinsic mechanical\ncontrol, and (3) myoelectric control. These methods can perform well under a\ncontrolled situation but will often break down in clinical and everyday use due\nto poor robustness, weak adaptability, long-term training, and heavy mental\nburden during use. This paper lays the complete outline of the design process\nof an affordable and easily accessible novel prosthetic arm that reduces the\ncost of prosthetics from $10,000 to $700 on average. The 3D printed prosthetic\narm is equipped with a depth camera and closed-loop off-policy deep learning\nalgorithm to help form grasps to the object in view. Current work in\nreinforcement learning masters only individual skills and is heavily focused on\nparallel jaw grippers for in-hand manipulation. In order to create\ngeneralization, which better performs real-world manipulation, the focus is\nspecifically on using the general framework of Markov Decision Process (MDP)\nthrough scalable learning with off-policy algorithms such as deep deterministic\npolicy gradient (DDPG) and to study this question in the context of grasping a\nprosthetic arm. We were able to achieve a 78% grasp success rate on previously\nunseen objects and generalize across multiple objects for manipulation tasks.\nThis work will make prosthetics cheaper, easier to use and accessible globally\nfor amputees. Future work includes applying similar approaches to other medical\nassistive devices where a human is interacting with a machine to complete a\ntask.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 00:35:06 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Imran", "Alishba", ""], ["Escobar", "William", ""], ["Barez", "Freidoon", ""]]}, {"id": "2103.02137", "submitter": "Nadine Wirkuttis", "authors": "Nadine Wirkuttis and Jun Tani", "title": "Controlling the Sense of Agency in Dyadic Robot Interaction: An Active\n  Inference Approach", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study investigated how social interaction among robotic agents changes\ndynamically depending on individual sense of agency. In a set of simulation\nstudies, we examine dyadic imitative interactions of robots using a variational\nrecurrent neural network model. The model is based on the free energy principle\nsuch that interacting robots find themselves in a loop, attempting to predict\nand infer each other's actions using active inference. We examined how\nregulating the complexity term to minimize free energy during training\ndetermines the dynamic characteristics of networks and affects dyadic imitative\ninteractions. Our simulation results show that through softer regulation of the\ncomplexity term, a robot with stronger agency develops and dominates its\ncounterpart developed with weaker agency through tighter regulation. When two\nrobots are trained with equally soft regulation, both generate individual\nintended behavior patterns, ignoring each other. We argue that primary\nintersubjectivity does develop in dyadic robotic interactions.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 02:38:09 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Wirkuttis", "Nadine", ""], ["Tani", "Jun", ""]]}, {"id": "2103.02141", "submitter": "Chenhao Wang", "authors": "Chenhao Wang, Yubo Chen, Zhipeng Xue, Yang Zhou, Jun Zhao", "title": "CogNet: Bridging Linguistic Knowledge, World Knowledge and Commonsense\n  Knowledge", "comments": "AAAI 2021 Demonstrations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present CogNet, a knowledge base (KB) dedicated to\nintegrating three types of knowledge: (1) linguistic knowledge from FrameNet,\nwhich schematically describes situations, objects and events. (2) world\nknowledge from YAGO, Freebase, DBpedia and Wikidata, which provides explicit\nknowledge about specific instances. (3) commonsense knowledge from ConceptNet,\nwhich describes implicit general facts. To model these different types of\nknowledge consistently, we introduce a three-level unified frame-styled\nrepresentation architecture. To integrate free-form commonsense knowledge with\nother structured knowledge, we propose a strategy that combines automated\nlabeling and crowdsourced annotation. At present, CogNet integrates 1,000+\nsemantic frames from linguistic KBs, 20,000,000+ frame instances from world\nKBs, as well as 90,000+ commonsense assertions from commonsense KBs. All these\ndata can be easily queried and explored on our online platform, and free to\ndownload in RDF format for utilization under a CC-BY-SA 4.0 license. The demo\nand data are available at http://cognet.top/.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 02:47:18 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Wang", "Chenhao", ""], ["Chen", "Yubo", ""], ["Xue", "Zhipeng", ""], ["Zhou", "Yang", ""], ["Zhao", "Jun", ""]]}, {"id": "2103.02144", "submitter": "Qingyang Xu", "authors": "Qingyang Xu, Qingsong Wen, Liang Sun", "title": "Two-Stage Framework for Seasonal Time Series Forecasting", "comments": "5 pages, 2 figures, 3 tables, ICASSP 2021", "journal-ref": "IEEE ICASSP 2021", "doi": "10.1109/ICASSP39728.2021.9414118.", "report-no": null, "categories": "cs.LG cs.AI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Seasonal time series Forecasting remains a challenging problem due to the\nlong-term dependency from seasonality. In this paper, we propose a two-stage\nframework to forecast univariate seasonal time series. The first stage\nexplicitly learns the long-range time series structure in a time window beyond\nthe forecast horizon. By incorporating the learned long-range structure, the\nsecond stage can enhance the prediction accuracy in the forecast horizon. In\nboth stages, we integrate the auto-regressive model with neural networks to\ncapture both linear and non-linear characteristics in time series. Our\nframework achieves state-of-the-art performance on M4 Competition Hourly\ndatasets. In particular, we show that incorporating the intermediate results\ngenerated in the first stage to existing forecast models can effectively\nenhance their prediction performance.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 02:53:39 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Xu", "Qingyang", ""], ["Wen", "Qingsong", ""], ["Sun", "Liang", ""]]}, {"id": "2103.02150", "submitter": "Varun Bhatt", "authors": "Varun Bhatt, Michael Buro", "title": "Inference-Based Deterministic Messaging For Multi-Agent Communication", "comments": "13 pages, 10 figures. Accepted at accepted at the 35th AAAI\n  Conference on Artificial Intelligence, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communication is essential for coordination among humans and animals.\nTherefore, with the introduction of intelligent agents into the world,\nagent-to-agent and agent-to-human communication becomes necessary. In this\npaper, we first study learning in matrix-based signaling games to empirically\nshow that decentralized methods can converge to a suboptimal policy. We then\npropose a modification to the messaging policy, in which the sender\ndeterministically chooses the best message that helps the receiver to infer the\nsender's observation. Using this modification, we see, empirically, that the\nagents converge to the optimal policy in nearly all the runs. We then apply\nthis method to a partially observable gridworld environment which requires\ncooperation between two agents and show that, with appropriate approximation\nmethods, the proposed sender modification can enhance existing decentralized\ntraining methods for more complex domains as well.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 03:09:22 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Bhatt", "Varun", ""], ["Buro", "Michael", ""]]}, {"id": "2103.02152", "submitter": "Feng Liu", "authors": "Haozhe Liu, Haoqian Wu, Weicheng Xie, Feng Liu and Linlin Shen", "title": "Group-wise Inhibition based Feature Regularization for Robust\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The convolutional neural network (CNN) is vulnerable to degraded images with\neven very small variations (e.g. corrupted and adversarial samples). One of the\npossible reasons is that CNN pays more attention to the most discriminative\nregions, but ignores the auxiliary features when learning, leading to the lack\nof feature diversity for final judgment. In our method, we propose to\ndynamically suppress significant activation values of CNN by group-wise\ninhibition, but not fixedly or randomly handle them when training. The feature\nmaps with different activation distribution are then processed separately to\ntake the feature independence into account. CNN is finally guided to learn\nricher discriminative features hierarchically for robust classification\naccording to the proposed regularization. Our method is comprehensively\nevaluated under multiple settings, including classification against\ncorruptions, adversarial attacks and low data regime. Extensive experimental\nresults show that the proposed method can achieve significant improvements in\nterms of both robustness and generalization performances, when compared with\nthe state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 03:19:32 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 07:21:11 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Liu", "Haozhe", ""], ["Wu", "Haoqian", ""], ["Xie", "Weicheng", ""], ["Liu", "Feng", ""], ["Shen", "Linlin", ""]]}, {"id": "2103.02164", "submitter": "Yinjun Wu", "authors": "Yinjun Wu, Jingchao Ni, Wei Cheng, Bo Zong, Dongjin Song, Zhengzhang\n  Chen, Yanchi Liu, Xuchao Zhang, Haifeng Chen, Susan Davidson", "title": "Dynamic Gaussian Mixture based Deep Generative Model For Robust\n  Forecasting on Sparse Multivariate Time Series", "comments": "This paper is accepted by AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Forecasting on sparse multivariate time series (MTS) aims to model the\npredictors of future values of time series given their incomplete past, which\nis important for many emerging applications. However, most existing methods\nprocess MTS's individually, and do not leverage the dynamic distributions\nunderlying the MTS's, leading to sub-optimal results when the sparsity is high.\nTo address this challenge, we propose a novel generative model, which tracks\nthe transition of latent clusters, instead of isolated feature representations,\nto achieve robust modeling. It is characterized by a newly designed dynamic\nGaussian mixture distribution, which captures the dynamics of clustering\nstructures, and is used for emitting timeseries. The generative model is\nparameterized by neural networks. A structured inference network is also\ndesigned for enabling inductive analysis. A gating mechanism is further\nintroduced to dynamically tune the Gaussian mixture distributions. Extensive\nexperimental results on a variety of real-life datasets demonstrate the\neffectiveness of our method.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 04:10:07 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Wu", "Yinjun", ""], ["Ni", "Jingchao", ""], ["Cheng", "Wei", ""], ["Zong", "Bo", ""], ["Song", "Dongjin", ""], ["Chen", "Zhengzhang", ""], ["Liu", "Yanchi", ""], ["Zhang", "Xuchao", ""], ["Chen", "Haifeng", ""], ["Davidson", "Susan", ""]]}, {"id": "2103.02167", "submitter": "Zhaoqun Li", "authors": "Zhaoqun Li, Xu Liang, Dandan Fan, Jinxing Li, Wei Jia, David Zhang", "title": "Touchless Palmprint Recognition based on 3D Gabor Template and Block\n  Feature Refinement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the growing demand for hand hygiene and convenience of use, palmprint\nrecognition with touchless manner made a great development recently, providing\nan effective solution for person identification. Despite many efforts that have\nbeen devoted to this area, it is still uncertain about the discriminative\nability of the contactless palmprint, especially for large-scale datasets. To\ntackle the problem, in this paper, we build a large-scale touchless palmprint\ndataset containing 2334 palms from 1167 individuals. To our best knowledge, it\nis the largest contactless palmprint image benchmark ever collected with regard\nto the number of individuals and palms. Besides, we propose a novel deep\nlearning framework for touchless palmprint recognition named 3DCPN (3D\nConvolution Palmprint recognition Network) which leverages 3D convolution to\ndynamically integrate multiple Gabor features. In 3DCPN, a novel variant of\nGabor filter is embedded into the first layer for enhancement of curve feature\nextraction. With a well-designed ensemble scheme,low-level 3D features are then\nconvolved to extract high-level features. Finally on the top, we set a\nregion-based loss function to strengthen the discriminative ability of both\nglobal and local descriptors. To demonstrate the superiority of our method,\nextensive experiments are conducted on our dataset and other popular databases\nTongJi and IITD, where the results show the proposed 3DCPN achieves\nstate-of-the-art or comparable performances.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 04:22:24 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Li", "Zhaoqun", ""], ["Liang", "Xu", ""], ["Fan", "Dandan", ""], ["Li", "Jinxing", ""], ["Jia", "Wei", ""], ["Zhang", "David", ""]]}, {"id": "2103.02183", "submitter": "Zhen Fu", "authors": "Zhen Fu, Bo Wang, Xihong Wu, Jing Chen", "title": "Auditory Attention Decoding from EEG using Convolutional Recurrent\n  Neural Network", "comments": "5 pages, 4 figures, submitted to EUSIPCO 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AI cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The auditory attention decoding (AAD) approach was proposed to determine the\nidentity of the attended talker in a multi-talker scenario by analyzing\nelectroencephalography (EEG) data. Although the linear model-based method has\nbeen widely used in AAD, the linear assumption was considered oversimplified\nand the decoding accuracy remained lower for shorter decoding windows.\nRecently, nonlinear models based on deep neural networks (DNN) have been\nproposed to solve this problem. However, these models did not fully utilize\nboth the spatial and temporal features of EEG, and the interpretability of DNN\nmodels was rarely investigated. In this paper, we proposed novel convolutional\nrecurrent neural network (CRNN) based regression model and classification\nmodel, and compared them with both the linear model and the state-of-the-art\nDNN models. Results showed that, our proposed CRNN-based classification model\noutperformed others for shorter decoding windows (around 90% for 2 s and 5 s).\nAlthough worse than classification models, the decoding accuracy of the\nproposed CRNN-based regression model was about 5% greater than other regression\nmodels. The interpretability of DNN models was also investigated by visualizing\nlayers' weight.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 05:09:40 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Fu", "Zhen", ""], ["Wang", "Bo", ""], ["Wu", "Xihong", ""], ["Chen", "Jing", ""]]}, {"id": "2103.02197", "submitter": "Young-Eun Lee", "authors": "Young-Eun Lee, Seong-Whan Lee", "title": "Decoding Event-related Potential from Ear-EEG Signals based on Ensemble\n  Convolutional Neural Networks in Ambulatory Environment", "comments": "Submitted IEEE the 9th International Winter Conference on\n  Brain-Computer Interface. arXiv admin note: text overlap with\n  arXiv:2002.01085", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, practical brain-computer interface is actively carried out,\nespecially, in an ambulatory environment. However, the electroencephalography\n(EEG) signals are distorted by movement artifacts and electromyography signals\nwhen users are moving, which make hard to recognize human intention. In\naddition, as hardware issues are also challenging, ear-EEG has been developed\nfor practical brain-computer interface and has been widely used. In this paper,\nwe proposed ensemble-based convolutional neural networks in ambulatory\nenvironment and analyzed the visual event-related potential responses in scalp-\nand ear-EEG in terms of statistical analysis and brain-computer interface\nperformance. The brain-computer interface performance deteriorated as 3-14%\nwhen walking fast at 1.6 m/s. The proposed methods showed 0.728 in average of\nthe area under the curve. The proposed method shows robust to the ambulatory\nenvironment and imbalanced data as well.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 06:04:59 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Lee", "Young-Eun", ""], ["Lee", "Seong-Whan", ""]]}, {"id": "2103.02198", "submitter": "Kasumi Obi", "authors": "Kasumi Obi, Quan Huu Cap, Noriko Umegaki-Arao, Masaru Tanaka, Hitoshi\n  Iyatomi", "title": "Bulk Production Augmentation Towards Explainable Melanoma Diagnosis", "comments": "IEEE EMBS Conference on Biomedical Engineering and Sciences\n  (IECBES2020), Best Paper Award Student Category in Biomedical Imaging and\n  Image Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although highly accurate automated diagnostic techniques for melanoma have\nbeen reported, the realization of a system capable of providing diagnostic\nevidence based on medical indices remains an open issue because of difficulties\nin obtaining reliable training data. In this paper, we propose bulk production\naugmentation (BPA) to generate high-quality, diverse pseudo-skin tumor images\nwith the desired structural malignant features for additional training images\nfrom a limited number of labeled images. The proposed BPA acts as an effective\ndata augmentation in constructing the feature detector for the atypical pigment\nnetwork (APN), which is a key structure in melanoma diagnosis. Experiments show\nthat training with images generated by our BPA largely boosts the APN detection\nperformance by 20.0 percentage points in the area under the receiver operating\ncharacteristic curve, which is 11.5 to 13.7 points higher than that of\nconventional CycleGAN-based augmentations in AUC.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 06:06:31 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Obi", "Kasumi", ""], ["Cap", "Quan Huu", ""], ["Umegaki-Arao", "Noriko", ""], ["Tanaka", "Masaru", ""], ["Iyatomi", "Hitoshi", ""]]}, {"id": "2103.02249", "submitter": "Pawan Goyal", "authors": "Pawan Goyal and Peter Benner", "title": "LQResNet: A Deep Neural Network Architecture for Learning Dynamic\n  Processes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.DS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mathematical modeling is an essential step, for example, to analyze the\ntransient behavior of a dynamical process and to perform engineering studies\nsuch as optimization and control. With the help of first-principles and expert\nknowledge, a dynamic model can be built, but for complex dynamic processes,\nappearing, e.g., in biology, chemical plants, neuroscience, financial markets,\nthis often remains an onerous task. Hence, data-driven modeling of the dynamics\nprocess becomes an attractive choice and is supported by the rapid advancement\nin sensor and measurement technology. A data-driven approach, namely operator\ninference framework, models a dynamic process, where a particular structure of\nthe nonlinear term is assumed. In this work, we suggest combining the operator\ninference with certain deep neural network approaches to infer the unknown\nnonlinear dynamics of the system. The approach uses recent advancements in deep\nlearning and possible prior knowledge of the process if possible. We also\nbriefly discuss several extensions and advantages of the proposed methodology.\nWe demonstrate that the proposed methodology accomplishes the desired tasks for\ndynamics processes encountered in neural dynamics and the glycolytic\noscillator.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 08:19:43 GMT"}, {"version": "v2", "created": "Sat, 27 Mar 2021 09:29:32 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Goyal", "Pawan", ""], ["Benner", "Peter", ""]]}, {"id": "2103.02250", "submitter": "Jongmin Yu", "authors": "Jongmin Yu, Hyeontaek Oh", "title": "Unsupervised Vehicle Re-Identification via Self-supervised Metric\n  Learning using Feature Dictionary", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The key challenge of unsupervised vehicle re-identification (Re-ID) is\nlearning discriminative features from unlabelled vehicle images. Numerous\nmethods using domain adaptation have achieved outstanding performance, but\nthose methods still need a labelled dataset as a source domain. This paper\naddresses an unsupervised vehicle Re-ID method, which no need any types of a\nlabelled dataset, through a Self-supervised Metric Learning (SSML) based on a\nfeature dictionary. Our method initially extracts features from vehicle images\nand stores them in a dictionary. Thereafter, based on the dictionary, the\nproposed method conducts dictionary-based positive label mining (DPLM) to\nsearch for positive labels. Pair-wise similarity, relative-rank consistency,\nand adjacent feature distribution similarity are jointly considered to find\nimages that may belong to the same vehicle of a given probe image. The results\nof DPLM are applied to dictionary-based triplet loss (DTL) to improve the\ndiscriminativeness of learnt features and to refine the quality of the results\nof DPLM progressively. The iterative process with DPLM and DTL boosts the\nperformance of unsupervised vehicle Re-ID. Experimental results demonstrate the\neffectiveness of the proposed method by producing promising vehicle Re-ID\nperformance without a pre-labelled dataset. The source code for this paper is\npublicly available on `https://github.com/andreYoo/VeRI_SSML_FD.git'.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 08:29:03 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Yu", "Jongmin", ""], ["Oh", "Hyeontaek", ""]]}, {"id": "2103.02252", "submitter": "Aizaz Hussain", "authors": "Aizaz Hussain, Muhammad Umair Arshad", "title": "An Attention Based Neural Network for Code Switching Detection: English\n  & Roman Urdu", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Code-switching is a common phenomenon among people with diverse lingual\nbackground and is widely used on the internet for communication purposes. In\nthis paper, we present a Recurrent Neural Network combined with the Attention\nModel for Language Identification in Code-Switched Data in English and low\nresource Roman Urdu. The attention model enables the architecture to learn the\nimportant features of the languages hence classifying the code switched data.\nWe demonstrated our approach by comparing the results with state of the art\nmodels i.e. Hidden Markov Models, Conditional Random Field and Bidirectional\nLSTM. The models evaluation, using confusion matrix metrics, showed that the\nattention mechanism provides improved the precision and accuracy as compared to\nthe other models.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 08:36:01 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Hussain", "Aizaz", ""], ["Arshad", "Muhammad Umair", ""]]}, {"id": "2103.02255", "submitter": "Weize Guo", "authors": "Weize Guo, Li Zhang, Xiaoli Lian", "title": "Automatically detecting the conflicts between software requirements\n  based on finer semantic analysis", "comments": "17 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Context: Conflicts between software requirements bring uncertainties to\nproduct development. Some great approaches have been proposed to identify these\nconflicts. However, they usually require the software requirements represented\nwith specific templates and/or depend on other external source which is often\nuneasy to build for lots of projects in practice. Objective: We aim to propose\nan approach Finer Semantic Analysis-based Requirements Conflict Detector\n(FSARC) to automatically detecting the conflicts between the given natural\nlanguage functional requirements by analyzing their finer semantic\ncompositions. Method: We build a harmonized semantic meta-model of functional\nrequirements with the form of eight-tuple. Then we propose algorithms to\nautomatically analyze the linguistic features of requirements and to annotate\nthe semantic elements for their semantic model construction. And we define\nseven types of conflicts as long as their heuristic detecting rules on the\nground of their text pattern and semantical dependency. Finally, we design and\nimplement the algorithm for conflicts detection. Results: The experiment with\nfour requirement datasets illustrates that the recall of FSARC is nearly 100%\nand the average precision is 83.88% on conflicts detection. Conclusion: We\nprovide a useful tool for detecting the conflicts between natural language\nfunctional requirements to improve the quality of the final requirements set.\nBesides, our approach is capable of transforming the natural language\nfunctional requirements into eight semantic tuples, which is useful not only\nthe detection of the conflicts between requirements but also some other tasks\nsuch as constructing the association between requirements and so on.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 08:39:53 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Guo", "Weize", ""], ["Zhang", "Li", ""], ["Lian", "Xiaoli", ""]]}, {"id": "2103.02325", "submitter": "Maksym Andriushchenko", "authors": "Klim Kireev, Maksym Andriushchenko, Nicolas Flammarion", "title": "On the effectiveness of adversarial training against common corruptions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The literature on robustness towards common corruptions shows no consensus on\nwhether adversarial training can improve the performance in this setting.\nFirst, we show that, when used with an appropriately selected perturbation\nradius, $\\ell_p$ adversarial training can serve as a strong baseline against\ncommon corruptions. Then we explain why adversarial training performs better\nthan data augmentation with simple Gaussian noise which has been observed to be\na meaningful baseline on common corruptions. Related to this, we identify the\n$\\sigma$-overfitting phenomenon when Gaussian augmentation overfits to a\nparticular standard deviation used for training which has a significant\ndetrimental effect on common corruption accuracy. We discuss how to alleviate\nthis problem and then how to further enhance $\\ell_p$ adversarial training by\nintroducing an efficient relaxation of adversarial training with learned\nperceptual image patch similarity as the distance metric. Through experiments\non CIFAR-10 and ImageNet-100, we show that our approach does not only improve\nthe $\\ell_p$ adversarial training baseline but also has cumulative gains with\ndata augmentation methods such as AugMix, ANT, and SIN leading to\nstate-of-the-art performance on common corruptions. The code of our experiments\nis publicly available at https://github.com/tml-epfl/adv-training-corruptions.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 11:04:09 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Kireev", "Klim", ""], ["Andriushchenko", "Maksym", ""], ["Flammarion", "Nicolas", ""]]}, {"id": "2103.02354", "submitter": "Andr\\'e Artelt", "authors": "Andr\\'e Artelt and Valerie Vaquet and Riza Velioglu and Fabian Hinder\n  and Johannes Brinkrolf and Malte Schilling and Barbara Hammer", "title": "Evaluating Robustness of Counterfactual Explanations", "comments": "Rewrite paper to make things more clear; Remove one theorem &\n  corollary due to buggy proof", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transparency is a fundamental requirement for decision making systems when\nthese should be deployed in the real world. It is usually achieved by providing\nexplanations of the system's behavior. A prominent and intuitive type of\nexplanations are counterfactual explanations. Counterfactual explanations\nexplain a behavior to the user by proposing actions -- as changes to the input\n-- that would cause a different (specified) behavior of the system. However,\nsuch explanation methods can be unstable with respect to small changes to the\ninput -- i.e. even a small change in the input can lead to huge or arbitrary\nchanges in the output and of the explanation. This could be problematic for\ncounterfactual explanations, as two similar individuals might get very\ndifferent explanations. Even worse, if the recommended actions differ\nconsiderably in their complexity, one would consider such unstable\n(counterfactual) explanations as individually unfair.\n  In this work, we formally and empirically study the robustness of\ncounterfactual explanations in general, as well as under different models and\ndifferent kinds of perturbations. Furthermore, we propose that plausible\ncounterfactual explanations can be used instead of closest counterfactual\nexplanations to improve the robustness and consequently the individual fairness\nof counterfactual explanations.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 12:16:06 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 13:11:47 GMT"}, {"version": "v3", "created": "Thu, 22 Jul 2021 06:52:32 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Artelt", "Andr\u00e9", ""], ["Vaquet", "Valerie", ""], ["Velioglu", "Riza", ""], ["Hinder", "Fabian", ""], ["Brinkrolf", "Johannes", ""], ["Schilling", "Malte", ""], ["Hammer", "Barbara", ""]]}, {"id": "2103.02355", "submitter": "Mohammad Abdulaziz", "authors": "Mohammad Abdulaziz", "title": "Cost Optimal Planning as Satisfiability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate upper bounds on the length of cost optimal plans that are\nvalid for problems with 0-cost actions. We employ these upper bounds as\nhorizons for a SAT-based encoding of planning with costs. Given an initial\nupper bound on the cost of the optimal plan, we experimentally show that this\nSAT-based approach is able to compute plans with better costs, and in many\ncases it can match the optimal cost. Also, in multiple instances, the approach\nis successful in proving that a certain cost is the optimal plan cost.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 12:18:18 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Abdulaziz", "Mohammad", ""]]}, {"id": "2103.02362", "submitter": "Ting Wu", "authors": "Ting Wu, Junjie Peng, Wenqiang Zhang, Huiran Zhang, Chuanshuai Ma,\n  Yansong Huang", "title": "Video Sentiment Analysis with Bimodal Information-augmented Multi-Head\n  Attention", "comments": "25 pages, 4 figures, author name and format corrected", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment analysis is the basis of intelligent human-computer interaction. As\none of the frontier research directions of artificial intelligence, it can help\ncomputers better identify human intentions and emotional states so that provide\nmore personalized services. However, as human present sentiments by spoken\nwords, gestures, facial expressions and others which involve variable forms of\ndata including text, audio, video, etc., it poses many challenges to this\nstudy. Due to the limitations of unimodal sentiment analysis, recent research\nhas focused on the sentiment analysis of videos containing time series data of\nmultiple modalities. When analyzing videos with multimodal data, the key\nproblem is how to fuse these heterogeneous data. In consideration that the\ncontribution of each modality is different, current fusion methods tend to\nextract the important information of single modality prior to fusion, which\nignores the consistency and complementarity of bimodal interaction and has\ninfluences on the final decision. To solve this problem, a video sentiment\nanalysis method using multi-head attention with bimodal information augmented\nis proposed. Based on bimodal interaction, more important bimodal features are\nassigned larger weights. In this way, different feature representations are\nadaptively assigned corresponding attention for effective multimodal fusion.\nExtensive experiments were conducted on both Chinese and English public\ndatasets. The results show that our approach outperforms the existing methods\nand can give an insight into the contributions of bimodal interaction among\nthree modalities.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 12:30:11 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 02:54:35 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Wu", "Ting", ""], ["Peng", "Junjie", ""], ["Zhang", "Wenqiang", ""], ["Zhang", "Huiran", ""], ["Ma", "Chuanshuai", ""], ["Huang", "Yansong", ""]]}, {"id": "2103.02363", "submitter": "Daiki Kimura", "authors": "Daiki Kimura, Subhajit Chaudhury, Akifumi Wachi, Ryosuke Kohita, Asim\n  Munawar, Michiaki Tatsubori, Alexander Gray", "title": "Reinforcement Learning with External Knowledge by using Logical Neural\n  Networks", "comments": "KBRL Workshop at IJCAI-PRICAI 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional deep reinforcement learning methods are sample-inefficient and\nusually require a large number of training trials before convergence. Since\nsuch methods operate on an unconstrained action set, they can lead to useless\nactions. A recent neuro-symbolic framework called the Logical Neural Networks\n(LNNs) can simultaneously provide key-properties of both neural networks and\nsymbolic logic. The LNNs functions as an end-to-end differentiable network that\nminimizes a novel contradiction loss to learn interpretable rules. In this\npaper, we utilize LNNs to define an inference graph using basic logical\noperations, such as AND and NOT, for faster convergence in reinforcement\nlearning. Specifically, we propose an integrated method that enables model-free\nreinforcement learning from external knowledge sources in an LNNs-based logical\nconstrained framework such as action shielding and guide. Our results\nempirically demonstrate that our method converges faster compared to a\nmodel-free reinforcement learning method that doesn't have such logical\nconstraints.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 12:34:59 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Kimura", "Daiki", ""], ["Chaudhury", "Subhajit", ""], ["Wachi", "Akifumi", ""], ["Kohita", "Ryosuke", ""], ["Munawar", "Asim", ""], ["Tatsubori", "Michiaki", ""], ["Gray", "Alexander", ""]]}, {"id": "2103.02372", "submitter": "Thomas Hirsch", "authors": "Thomas Hirsch, Birgit Hofer", "title": "Root cause prediction based on bug reports", "comments": "6 pages", "journal-ref": "Proceedings of the 2020 IEEE International Symposium on Software\n  Reliability Engineering Workshops (ISSREW), Coimbra, Portugal, 2020, pp.\n  171-176", "doi": "10.1109/ISSREW51248.2020.00067", "report-no": null, "categories": "cs.SE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a supervised machine learning approach for predicting the\nroot cause of a given bug report. Knowing the root cause of a bug can help\ndevelopers in the debugging process - either directly or indirectly by choosing\nproper tool support for the debugging task. We mined 54755 closed bug reports\nfrom the issue trackers of 103 GitHub projects and applied a set of heuristics\nto create a benchmark consisting of 10459 reports. A subset was manually\nclassified into three groups (semantic, memory, and concurrency) based on the\nbugs' root causes. Since the types of root cause are not equally distributed, a\ncombination of keyword search and random selection was applied. Our data set\nfor the machine learning approach consists of 369 bug reports (122 concurrency,\n121 memory, and 126 semantic bugs). The bug reports are used as input to a\nnatural language processing algorithm. We evaluated the performance of several\nclassifiers for predicting the root causes for the given bug reports. Linear\nSupport Vector machines achieved the highest mean precision (0.74) and recall\n(0.72) scores. The created bug data set and classification are publicly\navailable.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 12:47:15 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Hirsch", "Thomas", ""], ["Hofer", "Birgit", ""]]}, {"id": "2103.02378", "submitter": "Dongmei Wang", "authors": "Dongmei Wang, Takuya Yoshioka, Zhuo Chen, Xiaofei Wang, Tianyan Zhou,\n  Zhong Meng", "title": "Continuous Speech Separation with Ad Hoc Microphone Arrays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.LG eess.AS eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Speech separation has been shown effective for multi-talker speech\nrecognition. Under the ad hoc microphone array setup where the array consists\nof spatially distributed asynchronous microphones, additional challenges must\nbe overcome as the geometry and number of microphones are unknown beforehand.\nPrior studies show, with a spatial-temporalinterleaving structure, neural\nnetworks can efficiently utilize the multi-channel signals of the ad hoc array.\nIn this paper, we further extend this approach to continuous speech separation.\nSeveral techniques are introduced to enable speech separation for real\ncontinuous recordings. First, we apply a transformer-based network for\nspatio-temporal modeling of the ad hoc array signals. In addition, two methods\nare proposed to mitigate a speech duplication problem during single talker\nsegments, which seems more severe in the ad hoc array scenarios. One method is\ndevice distortion simulation for reducing the acoustic mismatch between\nsimulated training data and real recordings. The other is speaker counting to\ndetect the single speaker segments and merge the output signal channels.\nExperimental results for AdHoc-LibiCSS, a new dataset consisting of continuous\nrecordings of concatenated LibriSpeech utterances obtained by multiple\ndifferent devices, show the proposed separation method can significantly\nimprove the ASR accuracy for overlapped speech with little performance\ndegradation for single talker segments.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 13:01:08 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Wang", "Dongmei", ""], ["Yoshioka", "Takuya", ""], ["Chen", "Zhuo", ""], ["Wang", "Xiaofei", ""], ["Zhou", "Tianyan", ""], ["Meng", "Zhong", ""]]}, {"id": "2103.02381", "submitter": "Saar Alon-Barkat", "authors": "Saar Alon-Barkat and Madalina Busuioc", "title": "Decision-makers Processing of AI Algorithmic Advice: Automation Bias\n  versus Selective Adherence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Artificial intelligence algorithms are increasingly adopted as decisional\naides by public organisations, with the promise of overcoming biases of human\ndecision-makers. At the same time, the use of algorithms may introduce new\nbiases in the human-algorithm interaction. A key concern emerging from\npsychology studies regards human overreliance on algorithmic advice even in the\nface of warning signals and contradictory information from other sources\n(automation bias). A second concern regards decision-makers inclination to\nselectively adopt algorithmic advice when it matches their pre-existing beliefs\nand stereotypes (selective adherence). To date, we lack rigorous empirical\nevidence about the prevalence of these biases in a public sector context. We\nassess these via two pre-registered experimental studies (N=1,509), simulating\nthe use of algorithmic advice in decisions pertaining to the employment of\nschool teachers in the Netherlands. In study 1, we test automation bias by\nexploring participants adherence to a prediction of teachers performance, which\ncontradicts additional evidence, while comparing between two types of\npredictions: algorithmic v. human-expert. We do not find evidence for\nautomation bias. In study 2, we replicate these findings, and we also test\nselective adherence by manipulating the teachers ethnic background. We find a\npropensity for adherence when the advice predicts low performance for a teacher\nof a negatively stereotyped ethnic minority, with no significant differences\nbetween algorithmic and human advice. Overall, our findings of selective,\nbiased adherence belie the promise of neutrality that has propelled algorithm\nuse in the public sector.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 13:10:50 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Alon-Barkat", "Saar", ""], ["Busuioc", "Madalina", ""]]}, {"id": "2103.02386", "submitter": "Thomas Hirsch", "authors": "Thomas Hirsch", "title": "A Fault Localization and Debugging Support Framework driven by Bug\n  Tracking Data", "comments": "4 pages", "journal-ref": "Proceedings of the 2020 IEEE International Symposium on Software\n  Reliability Engineering Workshops (ISSREW), Coimbra, Portugal, 2020, pp.\n  139-142", "doi": "10.1109/ISSREW51248.2020.00053", "report-no": null, "categories": "cs.SE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fault localization has been determined as a major resource factor in the\nsoftware development life cycle. Academic fault localization techniques are\nmostly unknown and unused in professional environments. Although manual\ndebugging approaches can vary significantly depending on bug type (e.g. memory\nbugs or semantic bugs), these differences are not reflected in most existing\nfault localization tools. Little research has gone into automated\nidentification of bug types to optimize the fault localization process.\nFurther, existing fault localization techniques leverage on historical data\nonly for augmentation of suspiciousness rankings. This thesis aims to provide a\nfault localization framework by combining data from various sources to help\ndevelopers in the fault localization process. To achieve this, a bug\nclassification schema is introduced, benchmarks are created, and a novel fault\nlocalization method based on historical data is proposed.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 13:23:13 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Hirsch", "Thomas", ""]]}, {"id": "2103.02398", "submitter": "Thom Badings", "authors": "Thom S. Badings, Nils Jansen, Hasan A. Poonawala, Marielle Stoelinga", "title": "Filter-Based Abstractions with Correctness Guarantees for Planning under\n  Uncertainty", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SE cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study planning problems for continuous control systems with uncertainty\ncaused by measurement and process noise. The goal is to find an optimal plan\nthat guarantees that the system reaches a desired goal state within finite\ntime. Measurement noise causes limited observability of system states, and\nprocess noise causes uncertainty in the outcome of a given plan. These factors\nrender the problem undecidable in general. Our key contribution is a novel\nabstraction scheme that employs Kalman filtering as a state estimator to obtain\na finite-state model, which we formalize as a Markov decision process (MDP).\nFor this MDP, we employ state-of-the-art model checking techniques to\nefficiently compute plans that maximize the probability of reaching goal\nstates. Moreover, we account for numerical imprecision in computing the\nabstraction by extending the MDP with intervals of probabilities as a more\nrobust model. We show the correctness of the abstraction and provide several\noptimizations that aim to balance the quality of the plan and the scalability\nof the approach. We demonstrate that our method can handle systems that result\nin MDPs with thousands of states and millions of transitions.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 13:46:52 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 16:47:30 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Badings", "Thom S.", ""], ["Jansen", "Nils", ""], ["Poonawala", "Hasan A.", ""], ["Stoelinga", "Marielle", ""]]}, {"id": "2103.02405", "submitter": "Arshdeep Sekhon", "authors": "Arshdeep Sekhon, Zhe Wang, Yanjun Qi", "title": "Relate and Predict: Structure-Aware Prediction with Jointly Optimized\n  Neural DAG", "comments": "8 pages, 6 figures, version appeared in ICML Workshop 2020 Graph\n  Representation Learning and Beyond (GRL+)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding relationships between feature variables is one important way\nhumans use to make decisions. However, state-of-the-art deep learning studies\neither focus on task-agnostic statistical dependency learning or do not model\nexplicit feature dependencies during prediction. We propose a deep neural\nnetwork framework, dGAP, to learn neural dependency Graph and optimize\nstructure-Aware target Prediction simultaneously. dGAP trains towards a\nstructure self-supervision loss and a target prediction loss jointly. Our\nmethod leads to an interpretable model that can disentangle sparse feature\nrelationships, informing the user how relevant dependencies impact the target\ntask. We empirically evaluate dGAP on multiple simulated and real datasets.\ndGAP is not only more accurate, but can also recover correct dependency\nstructure.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 13:55:12 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Sekhon", "Arshdeep", ""], ["Wang", "Zhe", ""], ["Qi", "Yanjun", ""]]}, {"id": "2103.02438", "submitter": "Adam Foster", "authors": "Adam Foster, Desi R. Ivanova, Ilyas Malik, Tom Rainforth", "title": "Deep Adaptive Design: Amortizing Sequential Bayesian Experimental Design", "comments": "Published as a conference paper at ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Deep Adaptive Design (DAD), a method for amortizing the cost of\nadaptive Bayesian experimental design that allows experiments to be run in\nreal-time. Traditional sequential Bayesian optimal experimental design\napproaches require substantial computation at each stage of the experiment.\nThis makes them unsuitable for most real-world applications, where decisions\nmust typically be made quickly. DAD addresses this restriction by learning an\namortized design network upfront and then using this to rapidly run (multiple)\nadaptive experiments at deployment time. This network represents a design\npolicy which takes as input the data from previous steps, and outputs the next\ndesign using a single forward pass; these design decisions can be made in\nmilliseconds during the live experiment. To train the network, we introduce\ncontrastive information bounds that are suitable objectives for the sequential\nsetting, and propose a customized network architecture that exploits key\nsymmetries. We demonstrate that DAD successfully amortizes the process of\nexperimental design, outperforming alternative strategies on a number of\nproblems.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 14:43:48 GMT"}, {"version": "v2", "created": "Fri, 11 Jun 2021 12:18:18 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Foster", "Adam", ""], ["Ivanova", "Desi R.", ""], ["Malik", "Ilyas", ""], ["Rainforth", "Tom", ""]]}, {"id": "2103.02464", "submitter": "Kwan Hui Lim Dr", "authors": "Ngai Lam Ho, Kwan Hui Lim", "title": "User Preferential Tour Recommendation Based on POI-Embedding Methods", "comments": "Accepted to the 26th International Conference on Intelligent User\n  Interfaces (IUI'21), Poster Track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Tour itinerary planning and recommendation are challenging tasks for tourists\nin unfamiliar countries. Many tour recommenders only consider broad POI\ncategories and do not align well with users' preferences and other locational\nconstraints. We propose an algorithm to recommend personalized tours using\nPOI-embedding methods, which provides a finer representation of POI types. Our\nrecommendation algorithm will generate a sequence of POIs that optimizes time\nand locational constraints, as well as user's preferences based on past\ntrajectories from similar tourists. Our tour recommendation algorithm is\nmodelled as a word embedding model in natural language processing, coupled with\nan iterative algorithm for generating itineraries that satisfies time\nconstraints. Using a Flickr dataset of 4 cities, preliminary experimental\nresults show that our algorithm is able to recommend a relevant and accurate\nitinerary, based on measures of recall, precision and F1-scores.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 15:18:23 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Ho", "Ngai Lam", ""], ["Lim", "Kwan Hui", ""]]}, {"id": "2103.02484", "submitter": "Javier Hernandez", "authors": "Javier Hernandez, Daniel McDuff, Ognjen (Oggi) Rudovic, Alberto Fung,\n  Mary Czerwinski", "title": "DeepFN: Towards Generalizable Facial Action Unit Recognition with Deep\n  Face Normalization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facial action unit recognition has many applications from market research to\npsychotherapy and from image captioning to entertainment. Despite its recent\nprogress, deployment of these models has been impeded due to their limited\ngeneralization to unseen people and demographics. This work conducts an\nin-depth analysis of performance across several dimensions: individuals(40\nsubjects), genders (male and female), skin types (darker and lighter), and\ndatabases (BP4D and DISFA). To help suppress the variance in data, we use the\nnotion of self-supervised denoising autoencoders to design a method for deep\nface normalization(DeepFN) that transfers facial expressions of different\npeople onto a common facial template which is then used to train and evaluate\nfacial action recognition models. We show that person-independent models yield\nsignificantly lower performance (55% average F1 and accuracy across 40\nsubjects) than person-dependent models (60.3%), leading to a generalization gap\nof 5.3%. However, normalizing the data with the newly introduced DeepFN\nsignificantly increased the performance of person-independent models (59.6%),\neffectively reducing the gap. Similarly, we observed generalization gaps when\nconsidering gender (2.4%), skin type (5.3%), and dataset (9.4%), which were\nsignificantly reduced with the use of DeepFN. These findings represent an\nimportant step towards the creation of more generalizable facial action unit\nrecognition systems.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 15:50:51 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Hernandez", "Javier", "", "Oggi"], ["McDuff", "Daniel", "", "Oggi"], ["Ognjen", "", "", "Oggi"], ["Rudovic", "", ""], ["Fung", "Alberto", ""], ["Czerwinski", "Mary", ""]]}, {"id": "2103.02503", "submitter": "Kaiyang Zhou", "authors": "Kaiyang Zhou, Ziwei Liu, Yu Qiao, Tao Xiang, Chen Change Loy", "title": "Domain Generalization in Vision: A Survey", "comments": "v4: includes the word \"vision\" in the title; improves the\n  organization and clarity in Section 2-3; adds future directions; and more", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalization to out-of-distribution (OOD) data is a capability natural to\nhumans yet challenging for machines to reproduce. This is because most learning\nalgorithms strongly rely on the i.i.d.~assumption on source/target data, which\nis often violated in practice due to domain shift. Domain generalization (DG)\naims to achieve OOD generalization by using only source data for model\nlearning. Since first introduced in 2011, research in DG has made great\nprogresses. In particular, intensive research in this topic has led to a broad\nspectrum of methodologies, e.g., those based on domain alignment,\nmeta-learning, data augmentation, or ensemble learning, just to name a few; and\nhas covered various vision applications such as object recognition,\nsegmentation, action recognition, and person re-identification. In this paper,\nfor the first time a comprehensive literature review is provided to summarize\nthe developments in DG for computer vision over the past decade. Specifically,\nwe first cover the background by formally defining DG and relating it to other\nresearch fields like domain adaptation and transfer learning. Second, we\nconduct a thorough review into existing methods and present a categorization\nbased on their methodologies and motivations. Finally, we conclude this survey\nwith insights and discussions on future research directions.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 16:12:22 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 11:23:32 GMT"}, {"version": "v3", "created": "Wed, 31 Mar 2021 04:48:14 GMT"}, {"version": "v4", "created": "Sun, 18 Jul 2021 04:28:15 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Zhou", "Kaiyang", ""], ["Liu", "Ziwei", ""], ["Qiao", "Yu", ""], ["Xiang", "Tao", ""], ["Loy", "Chen Change", ""]]}, {"id": "2103.02524", "submitter": "Longqi Yang", "authors": "Jenna Butler, Mary Czerwinski, Shamsi Iqbal, Sonia Jaffe, Kate Nowak,\n  Emily Peloquin, Longqi Yang", "title": "Personal Productivity and Well-being -- Chapter 2 of the 2021 New Future\n  of Work Report", "comments": "In The New Future of Work: Research from Microsoft on the Impact of\n  the Pandemic on Work Practices, edited by Jaime Teevan, Brent Hecht, and\n  Sonia Jaffe, 1st ed. Microsoft, 2021. https://aka.ms/newfutureofwork", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.HC cs.SE cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We now turn to understanding the impact that COVID-19 had on the personal\nproductivity and well-being of information workers as their work practices were\nimpacted by remote work. This chapter overviews people's productivity,\nsatisfaction, and work patterns, and shows that the challenges and benefits of\nremote work are closely linked. Looking forward, the infrastructure surrounding\nwork will need to evolve to help people adapt to the challenges of remote and\nhybrid work.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 16:57:45 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Butler", "Jenna", ""], ["Czerwinski", "Mary", ""], ["Iqbal", "Shamsi", ""], ["Jaffe", "Sonia", ""], ["Nowak", "Kate", ""], ["Peloquin", "Emily", ""], ["Yang", "Longqi", ""]]}, {"id": "2103.02546", "submitter": "Fan Zhou", "authors": "Fan Zhou, Brahim Chaib-draa, Boyu Wang", "title": "Multi-task Learning by Leveraging the Semantic Information", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One crucial objective of multi-task learning is to align distributions across\ntasks so that the information between them can be transferred and shared.\nHowever, existing approaches only focused on matching the marginal feature\ndistribution while ignoring the semantic information, which may hinder the\nlearning performance. To address this issue, we propose to leverage the label\ninformation in multi-task learning by exploring the semantic conditional\nrelations among tasks. We first theoretically analyze the generalization bound\nof multi-task learning based on the notion of Jensen-Shannon divergence, which\nprovides new insights into the value of label information in multi-task\nlearning. Our analysis also leads to a concrete algorithm that jointly matches\nthe semantic distribution and controls label distribution divergence. To\nconfirm the effectiveness of the proposed method, we first compare the\nalgorithm with several baselines on some benchmarks and then test the\nalgorithms under label space shift conditions. Empirical results demonstrate\nthat the proposed method could outperform most baselines and achieve\nstate-of-the-art performance, particularly showing the benefits under the label\nshift conditions.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 17:36:35 GMT"}], "update_date": "2021-03-04", "authors_parsed": [["Zhou", "Fan", ""], ["Chaib-draa", "Brahim", ""], ["Wang", "Boyu", ""]]}, {"id": "2103.02603", "submitter": "Joseph K J", "authors": "K J Joseph, Salman Khan, Fahad Shahbaz Khan, Vineeth N Balasubramanian", "title": "Towards Open World Object Detection", "comments": "To appear in CVPR 2021 as an ORAL paper. Code is available in\n  https://github.com/JosephKJ/OWOD", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Humans have a natural instinct to identify unknown object instances in their\nenvironments. The intrinsic curiosity about these unknown instances aids in\nlearning about them, when the corresponding knowledge is eventually available.\nThis motivates us to propose a novel computer vision problem called: `Open\nWorld Object Detection', where a model is tasked to: 1) identify objects that\nhave not been introduced to it as `unknown', without explicit supervision to do\nso, and 2) incrementally learn these identified unknown categories without\nforgetting previously learned classes, when the corresponding labels are\nprogressively received. We formulate the problem, introduce a strong evaluation\nprotocol and provide a novel solution, which we call ORE: Open World Object\nDetector, based on contrastive clustering and energy based unknown\nidentification. Our experimental evaluation and ablation studies analyze the\nefficacy of ORE in achieving Open World objectives. As an interesting\nby-product, we find that identifying and characterizing unknown instances helps\nto reduce confusion in an incremental object detection setting, where we\nachieve state-of-the-art performance, with no extra methodological effort. We\nhope that our work will attract further research into this newly identified,\nyet crucial research direction.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 18:58:18 GMT"}, {"version": "v2", "created": "Sun, 9 May 2021 06:50:56 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Joseph", "K J", ""], ["Khan", "Salman", ""], ["Khan", "Fahad Shahbaz", ""], ["Balasubramanian", "Vineeth N", ""]]}, {"id": "2103.02649", "submitter": "Xiaoyang Wang", "authors": "Xiaoyang Wang, Jonathan D Thomas, Robert J Piechocki, Shipra Kapoor,\n  Raul Santos-Rodriguez, Arjun Parekh", "title": "Self-play Learning Strategies for Resource Assignment in Open-RAN\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open Radio Access Network (ORAN) is being developed with an aim to\ndemocratise access and lower the cost of future mobile data networks,\nsupporting network services with various QoS requirements, such as massive IoT\nand URLLC. In ORAN, network functionality is dis-aggregated into remote units\n(RUs), distributed units (DUs) and central units (CUs), which allows flexible\nsoftware on Commercial-Off-The-Shelf (COTS) deployments. Furthermore, the\nmapping of variable RU requirements to local mobile edge computing centres for\nfuture centralized processing would significantly reduce the power consumption\nin cellular networks. In this paper, we study the RU-DU resource assignment\nproblem in an ORAN system, modelled as a 2D bin packing problem. A deep\nreinforcement learning-based self-play approach is proposed to achieve\nefficient RU-DU resource management, with AlphaGo Zero inspired neural\nMonte-Carlo Tree Search (MCTS). Experiments on representative 2D bin packing\nenvironment and real sites data show that the self-play learning strategy\nachieves intelligent RU-DU resource assignment for different network\nconditions.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 19:31:29 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Wang", "Xiaoyang", ""], ["Thomas", "Jonathan D", ""], ["Piechocki", "Robert J", ""], ["Kapoor", "Shipra", ""], ["Santos-Rodriguez", "Raul", ""], ["Parekh", "Arjun", ""]]}, {"id": "2103.02654", "submitter": "Yudi Dong", "authors": "Yudi Dong and Huaxia Wang and Yu-Dong Yao", "title": "A Robust Adversarial Network-Based End-to-End Communications System With\n  Strong Generalization Ability Against Adversarial Attacks", "comments": "5 pages letter", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a novel defensive mechanism based on a generative adversarial\nnetwork (GAN) framework to defend against adversarial attacks in end-to-end\ncommunications systems. Specifically, we utilize a generative network to model\na powerful adversary and enable the end-to-end communications system to combat\nthe generative attack network via a minimax game. We show that the proposed\nsystem not only works well against white-box and black-box adversarial attacks\nbut also possesses excellent generalization capabilities to maintain good\nperformance under no attacks. We also show that our GAN-based end-to-end system\noutperforms the conventional communications system and the end-to-end\ncommunications system with/without adversarial training.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 20:04:42 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Dong", "Yudi", ""], ["Wang", "Huaxia", ""], ["Yao", "Yu-Dong", ""]]}, {"id": "2103.02676", "submitter": "Alvi Ataur Khalil", "authors": "Alvi Ataur Khalil, Alexander J Byrne, Mohammad Ashiqur Rahman,\n  Mohammad Hossein Manshaei", "title": "Efficient UAV Trajectory-Planning using Economic Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Advances in unmanned aerial vehicle (UAV) design have opened up applications\nas varied as surveillance, firefighting, cellular networks, and delivery\napplications. Additionally, due to decreases in cost, systems employing fleets\nof UAVs have become popular. The uniqueness of UAVs in systems creates a novel\nset of trajectory or path planning and coordination problems. Environments\ninclude many more points of interest (POIs) than UAVs, with obstacles and\nno-fly zones. We introduce REPlanner, a novel multi-agent reinforcement\nlearning algorithm inspired by economic transactions to distribute tasks\nbetween UAVs. This system revolves around an economic theory, in particular an\nauction mechanism where UAVs trade assigned POIs. We formulate the path\nplanning problem as a multi-agent economic game, where agents can cooperate and\ncompete for resources. We then translate the problem into a Partially\nObservable Markov decision process (POMDP), which is solved using a\nreinforcement learning (RL) model deployed on each agent. As the system\ncomputes task distributions via UAV cooperation, it is highly resilient to any\nchange in the swarm size. Our proposed network and economic game architecture\ncan effectively coordinate the swarm as an emergent phenomenon while\nmaintaining the swarm's operation. Evaluation results prove that REPlanner\nefficiently outperforms conventional RL-based trajectory search.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 20:54:19 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Khalil", "Alvi Ataur", ""], ["Byrne", "Alexander J", ""], ["Rahman", "Mohammad Ashiqur", ""], ["Manshaei", "Mohammad Hossein", ""]]}, {"id": "2103.02696", "submitter": "Weilin Cong", "authors": "Weilin Cong, Morteza Ramezani, Mehrdad Mahdavi", "title": "On the Importance of Sampling in Learning Graph Convolutional Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph Convolutional Networks (GCNs) have achieved impressive empirical\nadvancement across a wide variety of graph-related applications. Despite their\ngreat success, training GCNs on large graphs suffers from computational and\nmemory issues. A potential path to circumvent these obstacles is sampling-based\nmethods, where at each layer a subset of nodes is sampled. Although recent\nstudies have empirically demonstrated the effectiveness of sampling-based\nmethods, these works lack theoretical convergence guarantees under realistic\nsettings and cannot fully leverage the information of evolving parameters\nduring optimization. In this paper, we describe and analyze a general\n\\textbf{\\textit{doubly variance reduction}} schema that can accelerate any\nsampling method under the memory budget. The motivating impetus for the\nproposed schema is a careful analysis for the variance of sampling methods\nwhere it is shown that the induced variance can be decomposed into node\nembedding approximation variance (\\emph{zeroth-order variance}) during forward\npropagation and layerwise-gradient variance (\\emph{first-order variance})\nduring backward propagation. We theoretically analyze the convergence of the\nproposed schema and show that it enjoys an $\\mathcal{O}(1/T)$ convergence rate.\nWe complement our theoretical results by integrating the proposed schema in\ndifferent sampling methods and applying them to different large real-world\ngraphs. Code is public available\nat~\\url{https://github.com/CongWeilin/SGCN.git}.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 21:31:23 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Cong", "Weilin", ""], ["Ramezani", "Morteza", ""], ["Mahdavi", "Mehrdad", ""]]}, {"id": "2103.02728", "submitter": "Cosmin Badea", "authors": "Cosmin Badea, Gregory Artus", "title": "Morality, Machines and the Interpretation Problem: A value-based,\n  Wittgensteinian approach to building Moral Agents", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We argue that the attempt to build morality into machines is subject to what\nwe call the Interpretation problem, whereby any rule we give the machine is\nopen to infinite interpretation in ways that we might morally disapprove of,\nand that the interpretation problem in Artificial Intelligence is an\nillustration of Wittgenstein's general claim that no rule can contain the\ncriteria for its own application. Using games as an example, we attempt to\ndefine the structure of normative spaces and argue that any rule-following\nwithin a normative space is guided by values that are external to that space\nand which cannot themselves be represented as rules. In light of this problem,\nwe analyse the types of mistakes an artificial moral agent could make and we\nmake suggestions about how to build morality into machines by getting them to\ninterpret the rules we give in accordance with these external values, through\nexplicit moral reasoning and the presence of structured values, the adjustment\nof causal power assigned to the agent and interaction with human agents, such\nthat the machine develops a virtuous character and the impact of the\ninterpretation problem is minimised.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 22:34:01 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Badea", "Cosmin", ""], ["Artus", "Gregory", ""]]}, {"id": "2103.02781", "submitter": "Zhiqun Zhao", "authors": "Zhiqun Zhao, Hengyou Wang, Hao Sun and Zhihai He", "title": "Structure-Preserving Progressive Low-rank Image Completion for Defending\n  Adversarial Attacks", "comments": "10 pages, 12 figures, submitted to Journal of Visual Communication\n  and Image Representation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks recognize objects by analyzing local image details and\nsummarizing their information along the inference layers to derive the final\ndecision. Because of this, they are prone to adversarial attacks. Small\nsophisticated noise in the input images can accumulate along the network\ninference path and produce wrong decisions at the network output. On the other\nhand, human eyes recognize objects based on their global structure and semantic\ncues, instead of local image textures. Because of this, human eyes can still\nclearly recognize objects from images which have been heavily damaged by\nadversarial attacks. This leads to a very interesting approach for defending\ndeep neural networks against adversarial attacks. In this work, we propose to\ndevelop a structure-preserving progressive low-rank image completion (SPLIC)\nmethod to remove unneeded texture details from the input images and shift the\nbias of deep neural networks towards global object structures and semantic\ncues. We formulate the problem into a low-rank matrix completion problem with\nprogressively smoothed rank functions to avoid local minimums during the\noptimization process. Our experimental results demonstrate that the proposed\nmethod is able to successfully remove the insignificant local image details\nwhile preserving important global object structures. On black-box, gray-box,\nand white-box attacks, our method outperforms existing defense methods (by up\nto 12.6%) and significantly improves the adversarial robustness of the network.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 01:24:15 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Zhao", "Zhiqun", ""], ["Wang", "Hengyou", ""], ["Sun", "Hao", ""], ["He", "Zhihai", ""]]}, {"id": "2103.02789", "submitter": "William Beksi", "authors": "Christopher Collander, William J. Beksi, Manfred Huber", "title": "Learning the Next Best View for 3D Point Clouds via Topological Features", "comments": "To be published in the 2021 IEEE International Conference on Robotics\n  and Automation (ICRA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we introduce a reinforcement learning approach utilizing a\nnovel topology-based information gain metric for directing the next best view\nof a noisy 3D sensor. The metric combines the disjoint sections of an observed\nsurface to focus on high-detail features such as holes and concave sections.\nExperimental results show that our approach can aid in establishing the\nplacement of a robotic sensor to optimize the information provided by its\nstreaming point cloud data. Furthermore, a labeled dataset of 3D objects, a CAD\ndesign for a custom robotic manipulator, and software for the transformation,\nunion, and registration of point clouds has been publicly released to the\nresearch community.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 02:19:12 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 03:39:17 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Collander", "Christopher", ""], ["Beksi", "William J.", ""], ["Huber", "Manfred", ""]]}, {"id": "2103.02827", "submitter": "Audrey Huang", "authors": "Audrey Huang, Liu Leqi, Zachary C. Lipton, Kamyar Azizzadenesheli", "title": "On the Convergence and Optimality of Policy Gradient for Markov Coherent\n  Risk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In order to model risk aversion in reinforcement learning, an emerging line\nof research adapts familiar algorithms to optimize coherent risk functionals, a\nclass that includes conditional value-at-risk (CVaR). Because optimizing the\ncoherent risk is difficult in Markov decision processes, recent work tends to\nfocus on the Markov coherent risk (MCR), a time-consistent surrogate. While,\npolicy gradient (PG) updates have been derived for this objective, it remains\nunclear (i) whether PG finds a global optimum for MCR; (ii) how to estimate the\ngradient in a tractable manner. In this paper, we demonstrate that, in general,\nMCR objectives (unlike the expected return) are not gradient dominated and that\nstationary points are not, in general, guaranteed to be globally optimal.\nMoreover, we present a tight upper bound on the suboptimality of the learned\npolicy, characterizing its dependence on the nonlinearity of the objective and\nthe degree of risk aversion. Addressing (ii), we propose a practical\nimplementation of PG that uses state distribution reweighting to overcome\nprevious limitations. Through experiments, we demonstrate that when the\noptimality gap is small, PG can learn risk-sensitive policies. However, we find\nthat instances with large suboptimality gaps are abundant and easy to\nconstruct, outlining an important challenge for future research.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 04:11:09 GMT"}, {"version": "v2", "created": "Fri, 5 Mar 2021 20:49:55 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Huang", "Audrey", ""], ["Leqi", "Liu", ""], ["Lipton", "Zachary C.", ""], ["Azizzadenesheli", "Kamyar", ""]]}, {"id": "2103.02828", "submitter": "David D. Fan", "authors": "David D. Fan, Kyohei Otsu, Yuki Kubo, Anushri Dixit, Joel Burdick, and\n  Ali-Akbar Agha-Mohammadi", "title": "STEP: Stochastic Traversability Evaluation and Planning for Risk-Aware\n  Off-road Navigation", "comments": "Accepted to Robotics: Science and Systems (RSS) 2021. Video link:\n  https://youtu.be/N97cv4eH5c8", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although ground robotic autonomy has gained widespread usage in structured\nand controlled environments, autonomy in unknown and off-road terrain remains a\ndifficult problem. Extreme, off-road, and unstructured environments such as\nundeveloped wilderness, caves, and rubble pose unique and challenging problems\nfor autonomous navigation. To tackle these problems we propose an approach for\nassessing traversability and planning a safe, feasible, and fast trajectory in\nreal-time. Our approach, which we name STEP (Stochastic Traversability\nEvaluation and Planning), relies on: 1) rapid uncertainty-aware mapping and\ntraversability evaluation, 2) tail risk assessment using the Conditional\nValue-at-Risk (CVaR), and 3) efficient risk and constraint-aware kinodynamic\nmotion planning using sequential quadratic programming-based (SQP) model\npredictive control (MPC). We analyze our method in simulation and validate its\nefficacy on wheeled and legged robotic platforms exploring extreme terrains\nincluding an abandoned subway and an underground lava tube.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 04:24:19 GMT"}, {"version": "v2", "created": "Fri, 25 Jun 2021 19:45:43 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Fan", "David D.", ""], ["Otsu", "Kyohei", ""], ["Kubo", "Yuki", ""], ["Dixit", "Anushri", ""], ["Burdick", "Joel", ""], ["Agha-Mohammadi", "Ali-Akbar", ""]]}, {"id": "2103.02835", "submitter": "Sifan Song", "authors": "Sifan Song, Daiyun Huang, Yalun Hu, Chunxiao Yang, Jia Meng, Fei Ma,\n  Jiaming Zhang, Jionglong Su", "title": "A Novel Application of Image-to-Image Translation: Chromosome\n  Straightening Framework by Learning from a Single Image", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In medical imaging, chromosome straightening plays a significant role in the\npathological study of chromosomes and in the development of cytogenetic maps.\nWhereas different approaches exist for the straightening task, they are mostly\ngeometric algorithms whose outputs are characterized by jagged edges or\nfragments with discontinued banding patterns. To address the flaws in the\ngeometric algorithms, we propose a novel framework based on image-to-image\ntranslation to learn a pertinent mapping dependence for synthesizing\nstraightened chromosomes with uninterrupted banding patterns and preserved\ndetails. In addition, to avoid the pitfall of deficient input chromosomes, we\nconstruct an augmented dataset using only one single curved chromosome image\nfor training models. Based on this framework, we apply two popular\nimage-to-image translation architectures, U-shape networks and conditional\ngenerative adversarial networks, to assess its efficacy. Experiments on a\ndataset comprising of 642 real-world chromosomes demonstrate the superiority of\nour framework as compared to the geometric method in straightening performance\nby rendering realistic and continued chromosome details. Furthermore, our\nstraightened results improve the chromosome classification, achieving\n0.98%-1.39% in mean accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 05:05:41 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Song", "Sifan", ""], ["Huang", "Daiyun", ""], ["Hu", "Yalun", ""], ["Yang", "Chunxiao", ""], ["Meng", "Jia", ""], ["Ma", "Fei", ""], ["Zhang", "Jiaming", ""], ["Su", "Jionglong", ""]]}, {"id": "2103.02854", "submitter": "Dexter Neo", "authors": "Vassilios Vonikakis, Dexter Neo, Stefan Winkler", "title": "Morphset:Augmenting categorical emotion datasets with dimensional affect\n  labels using face morphing", "comments": "in Proc IEEE International Conference on Image Processing (ICIP),\n  Anchorage, Sep.2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Emotion recognition and understanding is a vital component in human-machine\ninteraction. Dimensional models of affect such as those using valence and\narousal have advantages over traditional categorical ones due to the complexity\nof emotional states in humans. However, dimensional emotion annotations are\ndifficult and expensive to collect, therefore they are not as prevalent in the\naffective computing community. To address these issues, we propose a method to\ngenerate synthetic images from existing categorical emotion datasets using face\nmorphing as well as dimensional labels in the circumplex space with full\ncontrol over the resulting sample distribution, while achieving augmentation\nfactors of at least 20x or more.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 06:33:06 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 03:36:06 GMT"}], "update_date": "2021-06-17", "authors_parsed": [["Vonikakis", "Vassilios", ""], ["Neo", "Dexter", ""], ["Winkler", "Stefan", ""]]}, {"id": "2103.02866", "submitter": "Shalini Pandey", "authors": "Shalini Pandey, George Karypis and Jaideep Srivasatava", "title": "IACN: Influence-aware and Attention-based Co-evolutionary Network for\n  Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Recommending relevant items to users is a crucial task on online communities\nsuch as Reddit and Twitter. For recommendation system, representation learning\npresents a powerful technique that learns embeddings to represent user\nbehaviors and capture item properties. However, learning embeddings on online\ncommunities is a challenging task because the user interest keep evolving. This\nevolution can be captured from 1) interaction between user and item, 2)\ninfluence from other users in the community. The existing dynamic embedding\nmodels only consider either of the factors to update user embeddings. However,\nat a given time, user interest evolves due to a combination of the two factors.\nTo this end, we propose Influence-aware and Attention-based Co-evolutionary\nNetwork (IACN). Essentially, IACN consists of two key components: interaction\nmodeling and influence modeling layer. The interaction modeling layer is\nresponsible for updating the embedding of a user and an item when the user\ninteracts with the item. The influence modeling layer captures the temporal\nexcitation caused by interactions of other users. To integrate the signals\nobtained from the two layers, we design a novel fusion layer that effectively\ncombines interaction-based and influence-based embeddings to predict final user\nembedding. Our model outperforms the existing state-of-the-art models from\nvarious domains.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 07:08:20 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Pandey", "Shalini", ""], ["Karypis", "George", ""], ["Srivasatava", "Jaideep", ""]]}, {"id": "2103.02878", "submitter": "Shuangyong Song", "authors": "Shuangyong Song, Kexin Wang, Chao Wang, Haiqing Chen, Huan Chen", "title": "An Emotion-controlled Dialog Response Generation Model with Dynamic\n  Vocabulary", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In response generation task, proper sentimental expressions can obviously\nimprove the human-like level of the responses. However, for real application in\nonline systems, high QPS (queries per second, an indicator of the flow capacity\nof on-line systems) is required, and a dynamic vocabulary mechanism has been\nproved available in improving speed of generative models. In this paper, we\nproposed an emotion-controlled dialog response generation model based on the\ndynamic vocabulary mechanism, and the experimental results show the benefit of\nthis model.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 07:58:43 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Song", "Shuangyong", ""], ["Wang", "Kexin", ""], ["Wang", "Chao", ""], ["Chen", "Haiqing", ""], ["Chen", "Huan", ""]]}, {"id": "2103.02881", "submitter": "Michele Piana", "authors": "Sabrina Guastavino, Michele Piana, Federico Benvenuto", "title": "Bad and good errors: value-weighted skill scores in deep ensemble\n  learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a novel approach to realize forecast verification.\nSpecifically, we introduce a strategy for assessing the severity of forecast\nerrors based on the evidence that, on the one hand, a false alarm just\nanticipating an occurring event is better than one in the middle of consecutive\nnon-occurring events, and that, on the other hand, a miss of an isolated event\nhas a worse impact than a miss of a single event, which is part of several\nconsecutive occurrences. Relying on this idea, we introduce a novel definition\nof confusion matrix and skill scores giving greater importance to the value of\nthe prediction rather than to its quality. Then, we introduce a deep ensemble\nlearning procedure for binary classification, in which the probabilistic\noutcomes of a neural network are clustered via optimization of these\nvalue-weighted skill scores. We finally show the performances of this approach\nin the case of three applications concerned with pollution, space weather and\nstock prize forecasting.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 08:05:13 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Guastavino", "Sabrina", ""], ["Piana", "Michele", ""], ["Benvenuto", "Federico", ""]]}, {"id": "2103.02943", "submitter": "Jose Maria Font", "authors": "Jose M. Font and Tobias Mahlmann", "title": "The Dota 2 Bot Competition", "comments": "6 pages", "journal-ref": "IEEE Transactions on Games 2018", "doi": "10.1109/TG.2018.2834566", "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multiplayer Online Battle Area (MOBA) games are a recent huge success both in\nthe video game industry and the international eSports scene. These games\nencourage team coordination and cooperation, short and long-term planning,\nwithin a real-time combined action and strategy gameplay.\n  Artificial Intelligence and Computational Intelligence in Games research\ncompetitions offer a wide variety of challenges regarding the study and\napplication of AI techniques to different game genres. These events are widely\naccepted by the AI/CI community as a sort of AI benchmarking that strongly\ninfluences many other research areas in the field.\n  This paper presents and describes in detail the Dota 2 Bot competition and\nthe Dota 2 AI framework that supports it. This challenge aims to join both,\nMOBAs and AI/CI game competitions, inviting participants to submit AI\ncontrollers for the successful MOBA \\textit{Defense of the Ancients 2} (Dota 2)\nto play in 1v1 matches, which aims for fostering research on AI techniques for\nreal-time games. The Dota 2 AI framework makes use of the actual Dota 2 game\nmodding capabilities to enable to connect external AI controllers to actual\nDota 2 game matches using the original Free-to-Play game.se of the actual Dota\n2 game modding capabilities to enable to connect external AI controllers to\nactual Dota 2 game matches using the original Free-to-Play game.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 10:49:47 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Font", "Jose M.", ""], ["Mahlmann", "Tobias", ""]]}, {"id": "2103.02957", "submitter": "Wei-Cheng Tseng", "authors": "Wei-Cheng Tseng, Jin-Siang Lin, Yao-Min Feng, Min Sun", "title": "Toward Robust Long Range Policy Transfer", "comments": "Accepted by AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans can master a new task within a few trials by drawing upon skills\nacquired through prior experience. To mimic this capability, hierarchical\nmodels combining primitive policies learned from prior tasks have been\nproposed. However, these methods fall short comparing to the human's range of\ntransferability. We propose a method, which leverages the hierarchical\nstructure to train the combination function and adapt the set of diverse\nprimitive polices alternatively, to efficiently produce a range of complex\nbehaviors on challenging new tasks. We also design two regularization terms to\nimprove the diversity and utilization rate of the primitives in the\npre-training phase. We demonstrate that our method outperforms other recent\npolicy transfer methods by combining and adapting these reusable primitives in\ntasks with continuous action space. The experiment results further show that\nour approach provides a broader transferring range. The ablation study also\nshows the regularization terms are critical for long range policy transfer.\nFinally, we show that our method consistently outperforms other methods when\nthe quality of the primitives varies.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 11:17:03 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Tseng", "Wei-Cheng", ""], ["Lin", "Jin-Siang", ""], ["Feng", "Yao-Min", ""], ["Sun", "Min", ""]]}, {"id": "2103.02958", "submitter": "Yuncheng Wu", "authors": "Yuncheng Wu, Tien Tuan Anh Dinh, Guoyu Hu, Meihui Zhang, Yeow Meng\n  Chee, Beng Chin Ooi", "title": "Serverless Model Serving for Data Science", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.DB cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) is an important part of modern data science\napplications. Data scientists today have to manage the end-to-end ML life cycle\nthat includes both model training and model serving, the latter of which is\nessential, as it makes their works available to end-users. Systems for model\nserving require high performance, low cost, and ease of management. Cloud\nproviders are already offering model serving options, including managed\nservices and self-rented servers. Recently, serverless computing, whose\nadvantages include high elasticity and fine-grained cost model, brings another\npossibility for model serving.\n  In this paper, we study the viability of serverless as a mainstream model\nserving platform for data science applications. We conduct a comprehensive\nevaluation of the performance and cost of serverless against other model\nserving systems on two clouds: Amazon Web Service (AWS) and Google Cloud\nPlatform (GCP). We find that serverless outperforms many cloud-based\nalternatives with respect to cost and performance. More interestingly, under\nsome circumstances, it can even outperform GPU-based systems for both average\nlatency and cost. These results are different from previous works' claim that\nserverless is not suitable for model serving, and are contrary to the\nconventional wisdom that GPU-based systems are better for ML workloads than\nCPU-based systems. Other findings include a large gap in cold start time\nbetween AWS and GCP serverless functions, and serverless' low sensitivity to\nchanges in workloads or models. Our evaluation results indicate that serverless\nis a viable option for model serving. Finally, we present several practical\nrecommendations for data scientists on how to use serverless for scalable and\ncost-effective model serving.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 11:23:01 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Wu", "Yuncheng", ""], ["Dinh", "Tien Tuan Anh", ""], ["Hu", "Guoyu", ""], ["Zhang", "Meihui", ""], ["Chee", "Yeow Meng", ""], ["Ooi", "Beng Chin", ""]]}, {"id": "2103.03000", "submitter": "Paula Harder", "authors": "Paula Harder, Franz-Josef Pfreundt, Margret Keuper, Janis Keuper", "title": "SpectralDefense: Detecting Adversarial Attacks on CNNs in the Fourier\n  Domain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the success of convolutional neural networks (CNNs) in many computer\nvision and image analysis tasks, they remain vulnerable against so-called\nadversarial attacks: Small, crafted perturbations in the input images can lead\nto false predictions. A possible defense is to detect adversarial examples. In\nthis work, we show how analysis in the Fourier domain of input images and\nfeature maps can be used to distinguish benign test samples from adversarial\nimages. We propose two novel detection methods: Our first method employs the\nmagnitude spectrum of the input images to detect an adversarial attack. This\nsimple and robust classifier can successfully detect adversarial perturbations\nof three commonly used attack methods. The second method builds upon the first\nand additionally extracts the phase of Fourier coefficients of feature-maps at\ndifferent layers of the network. With this extension, we are able to improve\nadversarial detection rates compared to state-of-the-art detectors on five\ndifferent attack methods.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 12:48:28 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 09:35:41 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Harder", "Paula", ""], ["Pfreundt", "Franz-Josef", ""], ["Keuper", "Margret", ""], ["Keuper", "Janis", ""]]}, {"id": "2103.03014", "submitter": "Lucas Liebenwein", "authors": "Lucas Liebenwein, Cenk Baykal, Brandon Carter, David Gifford, Daniela\n  Rus", "title": "Lost in Pruning: The Effects of Pruning Neural Networks beyond Test\n  Accuracy", "comments": "Published in MLSys 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural network pruning is a popular technique used to reduce the inference\ncosts of modern, potentially overparameterized, networks. Starting from a\npre-trained network, the process is as follows: remove redundant parameters,\nretrain, and repeat while maintaining the same test accuracy. The result is a\nmodel that is a fraction of the size of the original with comparable predictive\nperformance (test accuracy). Here, we reassess and evaluate whether the use of\ntest accuracy alone in the terminating condition is sufficient to ensure that\nthe resulting model performs well across a wide spectrum of \"harder\" metrics\nsuch as generalization to out-of-distribution data and resilience to noise.\nAcross evaluations on varying architectures and data sets, we find that pruned\nnetworks effectively approximate the unpruned model, however, the prune ratio\nat which pruned networks achieve commensurate performance varies significantly\nacross tasks. These results call into question the extent of \\emph{genuine}\noverparameterization in deep learning and raise concerns about the\npracticability of deploying pruned networks, specifically in the context of\nsafety-critical systems, unless they are widely evaluated beyond test accuracy\nto reliably predict their performance. Our code is available at\nhttps://github.com/lucaslie/torchprune.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 13:22:16 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Liebenwein", "Lucas", ""], ["Baykal", "Cenk", ""], ["Carter", "Brandon", ""], ["Gifford", "David", ""], ["Rus", "Daniela", ""]]}, {"id": "2103.03059", "submitter": "Samuel Earp", "authors": "Samuel W. F. Earp and Aubin Samacoits and Sanjana Jain and Pavit\n  Noinongyao and Siwa Boonpunmongkol", "title": "Sub-pixel face landmarks using heatmaps and a bag of tricks", "comments": "10 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Accurate face landmark localization is an essential part of face recognition,\nreconstruction and morphing. To accurately localize face landmarks, we present\nour heatmap regression approach. Each model consists of a MobileNetV2 backbone\nfollowed by several upscaling layers, with different tricks to optimize both\nperformance and inference cost. We use five na\\\"ive face landmarks from a\npublicly available face detector to position and align the face instead of\nusing the bounding box like traditional methods. Moreover, we show by adding\nrandom rotation, displacement and scaling -- after alignment -- that the model\nis more sensitive to the face position than orientation. We also show that it\nis possible to reduce the upscaling complexity by using a mixture of\ndeconvolution and pixel-shuffle layers without impeding localization\nperformance. We present our state-of-the-art face landmark localization model\n(ranking second on The 2nd Grand Challenge of 106-Point Facial Landmark\nLocalization validation set). Finally, we test the effect on face recognition\nusing these landmarks, using a publicly available model and benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 14:34:20 GMT"}, {"version": "v2", "created": "Mon, 8 Mar 2021 02:55:39 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Earp", "Samuel W. F.", ""], ["Samacoits", "Aubin", ""], ["Jain", "Sanjana", ""], ["Noinongyao", "Pavit", ""], ["Boonpunmongkol", "Siwa", ""]]}, {"id": "2103.03060", "submitter": "Junaid Malik", "authors": "Junaid Malik, Serkan Kiranyaz, Mehmet Yamac, Moncef Gabbouj", "title": "BM3D vs 2-Layer ONN", "comments": "Submitted for review in ICIP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Despite their recent success on image denoising, the need for deep and\ncomplex architectures still hinders the practical usage of CNNs. Older but\ncomputationally more efficient methods such as BM3D remain a popular choice,\nespecially in resource-constrained scenarios. In this study, we aim to find out\nwhether compact neural networks can learn to produce competitive results as\ncompared to BM3D for AWGN image denoising. To this end, we configure networks\nwith only two hidden layers and employ different neuron models and layer widths\nfor comparing the performance with BM3D across different AWGN noise levels. Our\nresults conclusively show that the recently proposed self-organized variant of\noperational neural networks based on a generative neuron model (Self-ONNs) is\nnot only a better choice as compared to CNNs, but also provide competitive\nresults as compared to BM3D and even significantly surpass it for high noise\nlevels.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 14:37:23 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Malik", "Junaid", ""], ["Kiranyaz", "Serkan", ""], ["Yamac", "Mehmet", ""], ["Gabbouj", "Moncef", ""]]}, {"id": "2103.03096", "submitter": "Rudresh Dwivedi", "authors": "Devam Dave, Het Naik, Smiti Singhal, Rudresh Dwivedi, Pankesh Patel", "title": "Towards Designing Computer Vision-based Explainable-AI Solution: A Use\n  Case of Livestock Mart Industry", "comments": "8 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The objective of an online Mart is to match buyers and sellers, to weigh\nanimals and to oversee their sale. A reliable pricing method can be developed\nby ML models that can read through historical sales data. However, when AI\nmodels suggest or recommend a price, that in itself does not reveal too much\n(i.e., it acts like a black box) about the qualities and the abilities of an\nanimal. An interested buyer would like to know more about the salient features\nof an animal before making the right choice based on his requirements. A model\ncapable of explaining the different factors that impact the price point is\nessential for the needs of the market. It can also inspire confidence in buyers\nand sellers about the price point offered. To achieve these objectives, we have\nbeen working with the team at MartEye, a startup based in Portershed in Galway\nCity, Ireland. Through this paper, we report our work-in-progress research\ntowards building a smart video analytic platform, leveraging Explainable AI\ntechniques.\n", "versions": [{"version": "v1", "created": "Mon, 8 Feb 2021 17:11:19 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Dave", "Devam", ""], ["Naik", "Het", ""], ["Singhal", "Smiti", ""], ["Dwivedi", "Rudresh", ""], ["Patel", "Pankesh", ""]]}, {"id": "2103.03097", "submitter": "Jindong Wang", "authors": "Jindong Wang, Cuiling Lan, Chang Liu, Yidong Ouyang, Wenjun Zeng, Tao\n  Qin", "title": "Generalizing to Unseen Domains: A Survey on Domain Generalization", "comments": "15 pages; short version (6 pages) has been accepted by IJCAI-21\n  survey track; codebase:\n  https://github.com/jindongwang/transferlearning/tree/master/code/DeepDG", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning systems generally assume that the training and testing\ndistributions are the same. To this end, a key requirement is to develop models\nthat can generalize to unseen distributions. Domain generalization (DG), i.e.,\nout-of-distribution generalization, has attracted increasing interests in\nrecent years. Domain generalization deals with a challenging setting where one\nor several different but related domain(s) are given, and the goal is to learn\na model that can generalize to an unseen test domain. Great progress has been\nmade in the area of domain generalization for years. This paper presents the\nfirst review of recent advances in this area. First, we provide a formal\ndefinition of domain generalization and discuss several related fields. We then\nthoroughly review the theories related to domain generalization and carefully\nanalyze the theory behind generalization. We categorize recent algorithms into\nthree classes: data manipulation, representation learning, and learning\nstrategy, and present several popular algorithms in detail for each category.\nThird, we introduce the commonly used datasets and applications. Finally, we\nsummarize existing literature and present some potential research topics for\nthe future.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 06:04:11 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 06:11:06 GMT"}, {"version": "v3", "created": "Sat, 1 May 2021 02:21:03 GMT"}, {"version": "v4", "created": "Tue, 13 Jul 2021 03:31:28 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Wang", "Jindong", ""], ["Lan", "Cuiling", ""], ["Liu", "Chang", ""], ["Ouyang", "Yidong", ""], ["Zeng", "Wenjun", ""], ["Qin", "Tao", ""]]}, {"id": "2103.03102", "submitter": "Wei Dai", "authors": "Wei Dai, Daniel Berleant", "title": "Benchmarking Deep Learning Classifiers: Beyond Accuracy", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.PF", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Previous research evaluating deep learning (DL) classifiers has often used\ntop-1/top-5 accuracy. However, the accuracy of DL classifiers is unstable in\nthat it often changes significantly when retested on imperfect or adversarial\nimages. This paper adds to the small but fundamental body of work on\nbenchmarking the robustness of DL classifiers on imperfect images by proposing\na two-dimensional metric, consisting of mean accuracy and coefficient of\nvariation, to measure the robustness of DL classifiers. Spearman's rank\ncorrelation coefficient and Pearson's correlation coefficient are used and\ntheir independence evaluated. A statistical plot we call mCV is presented which\naims to help visualize the robustness of the performance of DL classifiers\nacross varying amounts of imperfection in tested images. Finally, we\ndemonstrate that defective images corrupted by two-factor corruption could be\nused to improve the robustness of DL classifiers. All source codes and related\nimage sets are shared on a website (http://www.animpala.com) to support future\nresearch projects.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 02:10:54 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Dai", "Wei", ""], ["Berleant", "Daniel", ""]]}, {"id": "2103.03108", "submitter": "Hamidreza Ghader", "authors": "Hamidreza Ghader", "title": "An empirical analysis of phrase-based and neural machine translation", "comments": "PhD thesis, University of Amsterdam, October 2020.\n  https://pure.uva.nl/ws/files/51388868/Thesis.pdf", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Two popular types of machine translation (MT) are phrase-based and neural\nmachine translation systems. Both of these types of systems are composed of\nmultiple complex models or layers. Each of these models and layers learns\ndifferent linguistic aspects of the source language. However, for some of these\nmodels and layers, it is not clear which linguistic phenomena are learned or\nhow this information is learned. For phrase-based MT systems, it is often clear\nwhat information is learned by each model, and the question is rather how this\ninformation is learned, especially for its phrase reordering model. For neural\nmachine translation systems, the situation is even more complex, since for many\ncases it is not exactly clear what information is learned and how it is\nlearned.\n  To shed light on what linguistic phenomena are captured by MT systems, we\nanalyze the behavior of important models in both phrase-based and neural MT\nsystems. We consider phrase reordering models from phrase-based MT systems to\ninvestigate which words from inside of a phrase have the biggest impact on\ndefining the phrase reordering behavior. Additionally, to contribute to the\ninterpretability of neural MT systems we study the behavior of the attention\nmodel, which is a key component in neural MT systems and the closest model in\nfunctionality to phrase reordering models in phrase-based systems. The\nattention model together with the encoder hidden state representations form the\nmain components to encode source side linguistic information in neural MT. To\nthis end, we also analyze the information captured in the encoder hidden state\nrepresentations of a neural MT system. We investigate the extent to which\nsyntactic and lexical-semantic information from the source side is captured by\nhidden state representations of different neural MT architectures.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 15:28:28 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Ghader", "Hamidreza", ""]]}, {"id": "2103.03113", "submitter": "Wei Huang", "authors": "Wei Huang, Yayong Li, Weitao Du, Richard Yi Da Xu, Jie Yin, and Ling\n  Chen", "title": "Wide Graph Neural Networks: Aggregation Provably Leads to Exponentially\n  Trainability Loss", "comments": "23 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph convolutional networks (GCNs) and their variants have achieved great\nsuccess in dealing with graph-structured data. However, it is well known that\ndeep GCNs will suffer from over-smoothing problem, where node representations\ntend to be indistinguishable as we stack up more layers. Although extensive\nresearch has confirmed this prevailing understanding, few theoretical analyses\nhave been conducted to study the expressivity and trainability of deep GCNs. In\nthis work, we demonstrate these characterizations by studying the Gaussian\nProcess Kernel (GPK) and Graph Neural Tangent Kernel (GNTK) of an\ninfinitely-wide GCN, corresponding to the analysis on expressivity and\ntrainability, respectively. We first prove the expressivity of infinitely-wide\nGCNs decaying at an exponential rate by applying the mean-field theory on GPK.\nBesides, we formulate the asymptotic behaviors of GNTK in the large depth,\nwhich enables us to reveal the dropping trainability of wide and deep GCNs at\nan exponential rate. Additionally, we extend our theoretical framework to\nanalyze residual connection-resemble techniques. We found that these techniques\ncan mildly mitigate exponential decay, but they failed to overcome it\nfundamentally. Finally, all theoretical results in this work are corroborated\nexperimentally on a variety of graph-structured datasets.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 11:06:12 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Huang", "Wei", ""], ["Li", "Yayong", ""], ["Du", "Weitao", ""], ["Da Xu", "Richard Yi", ""], ["Yin", "Jie", ""], ["Chen", "Ling", ""]]}, {"id": "2103.03116", "submitter": "Linfeng Liu", "authors": "Linfeng Liu, Hoan Nguyen, George Karypis, Srinivasan Sengamedu", "title": "Universal Representation for Code", "comments": "PAKDD 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.PL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning from source code usually requires a large amount of labeled data.\nDespite the possible scarcity of labeled data, the trained model is highly\ntask-specific and lacks transferability to different tasks. In this work, we\npresent effective pre-training strategies on top of a novel graph-based code\nrepresentation, to produce universal representations for code. Specifically,\nour graph-based representation captures important semantics between code\nelements (e.g., control flow and data flow). We pre-train graph neural networks\non the representation to extract universal code properties. The pre-trained\nmodel then enables the possibility of fine-tuning to support various downstream\napplications. We evaluate our model on two real-world datasets -- spanning over\n30M Java methods and 770K Python methods. Through visualization, we reveal\ndiscriminative properties in our universal code representation. By comparing\nmultiple benchmarks, we demonstrate that the proposed framework achieves\nstate-of-the-art results on method name prediction and code graph link\nprediction.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 15:39:25 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Liu", "Linfeng", ""], ["Nguyen", "Hoan", ""], ["Karypis", "George", ""], ["Sengamedu", "Srinivasan", ""]]}, {"id": "2103.03125", "submitter": "Zhuosheng Zhang", "authors": "Zhuosheng Zhang and Hai Zhao", "title": "Advances in Multi-turn Dialogue Comprehension: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training machines to understand natural language and interact with humans is\nan elusive and essential task in the field of artificial intelligence. In\nrecent years, a diversity of dialogue systems has been designed with the rapid\ndevelopment of deep learning researches, especially the recent pre-trained\nlanguage models. Among these studies, the fundamental yet challenging part is\ndialogue comprehension whose role is to teach the machines to read and\ncomprehend the dialogue context before responding. In this paper, we review the\nprevious methods from the perspective of dialogue modeling. We summarize the\ncharacteristics and challenges of dialogue comprehension in contrast to\nplain-text reading comprehension. Then, we discuss three typical patterns of\ndialogue modeling that are widely-used in dialogue comprehension tasks such as\nresponse selection and conversation question-answering, as well as\ndialogue-related language modeling techniques to enhance PrLMs in dialogue\nscenarios. Finally, we highlight the technical advances in recent years and\npoint out the lessons we can learn from the empirical analysis and the\nprospects towards a new frontier of researches.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 15:50:17 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Zhang", "Zhuosheng", ""], ["Zhao", "Hai", ""]]}, {"id": "2103.03133", "submitter": "\\v{S}imon Bil\\'ik", "authors": "Simon Bilik, Lukas Kratochvila, Adam Ligocki, Ondrej Bostik, Tomas\n  Zemcik, Matous Hybl, Karel Horak, Ludek Zalud", "title": "Visual diagnosis of the Varroa destructor parasitic mite in honeybees\n  using object detector techniques", "comments": null, "journal-ref": null, "doi": "10.3390/s21082764", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Varroa destructor mite is one of the most dangerous Honey Bee (Apis\nmellifera) parasites worldwide and the bee colonies have to be regularly\nmonitored in order to control its spread. Here we present an object detector\nbased method for health state monitoring of bee colonies. This method has the\npotential for online measurement and processing. In our experiment, we compare\nthe YOLO and SSD object detectors along with the Deep SVDD anomaly detector.\nBased on the custom dataset with 600 ground-truth images of healthy and\ninfected bees in various scenes, the detectors reached a high F1 score up to\n0.874 in the infected bee detection and up to 0.727 in the detection of the\nVarroa Destructor mite itself. The results demonstrate the potential of this\napproach, which will be later used in the real-time computer vision based honey\nbee inspection system. To the best of our knowledge, this study is the first\none using object detectors for this purpose. We expect that performance of\nthose object detectors will enable us to inspect the health status of the honey\nbee colonies.\n", "versions": [{"version": "v1", "created": "Fri, 26 Feb 2021 11:01:31 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Bilik", "Simon", ""], ["Kratochvila", "Lukas", ""], ["Ligocki", "Adam", ""], ["Bostik", "Ondrej", ""], ["Zemcik", "Tomas", ""], ["Hybl", "Matous", ""], ["Horak", "Karel", ""], ["Zalud", "Ludek", ""]]}, {"id": "2103.03206", "submitter": "Andrew Jaegle", "authors": "Andrew Jaegle and Felix Gimeno and Andrew Brock and Andrew Zisserman\n  and Oriol Vinyals and Joao Carreira", "title": "Perceiver: General Perception with Iterative Attention", "comments": "ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.SD eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biological systems perceive the world by simultaneously processing\nhigh-dimensional inputs from modalities as diverse as vision, audition, touch,\nproprioception, etc. The perception models used in deep learning on the other\nhand are designed for individual modalities, often relying on domain-specific\nassumptions such as the local grid structures exploited by virtually all\nexisting vision models. These priors introduce helpful inductive biases, but\nalso lock models to individual modalities. In this paper we introduce the\nPerceiver - a model that builds upon Transformers and hence makes few\narchitectural assumptions about the relationship between its inputs, but that\nalso scales to hundreds of thousands of inputs, like ConvNets. The model\nleverages an asymmetric attention mechanism to iteratively distill inputs into\na tight latent bottleneck, allowing it to scale to handle very large inputs. We\nshow that this architecture is competitive with or outperforms strong,\nspecialized models on classification tasks across various modalities: images,\npoint clouds, audio, video, and video+audio. The Perceiver obtains performance\ncomparable to ResNet-50 and ViT on ImageNet without 2D convolutions by directly\nattending to 50,000 pixels. It is also competitive in all modalities in\nAudioSet.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 18:20:50 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 00:25:31 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Jaegle", "Andrew", ""], ["Gimeno", "Felix", ""], ["Brock", "Andrew", ""], ["Zisserman", "Andrew", ""], ["Vinyals", "Oriol", ""], ["Carreira", "Joao", ""]]}, {"id": "2103.03216", "submitter": "Hadi Nekoei", "authors": "Hadi Nekoei, Akilesh Badrinaaraayanan, Aaron Courville, Sarath Chandar", "title": "Continuous Coordination As a Realistic Scenario for Lifelong Learning", "comments": "19 pages with supplementary materials. Added results for Lifelong RL\n  methods and some future work. Accepted to ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Current deep reinforcement learning (RL) algorithms are still highly\ntask-specific and lack the ability to generalize to new environments. Lifelong\nlearning (LLL), however, aims at solving multiple tasks sequentially by\nefficiently transferring and using knowledge between tasks. Despite a surge of\ninterest in lifelong RL in recent years, the lack of a realistic testbed makes\nrobust evaluation of LLL algorithms difficult. Multi-agent RL (MARL), on the\nother hand, can be seen as a natural scenario for lifelong RL due to its\ninherent non-stationarity, since the agents' policies change over time. In this\nwork, we introduce a multi-agent lifelong learning testbed that supports both\nzero-shot and few-shot settings. Our setup is based on Hanabi -- a\npartially-observable, fully cooperative multi-agent game that has been shown to\nbe challenging for zero-shot coordination. Its large strategy space makes it a\ndesirable environment for lifelong RL tasks. We evaluate several recent MARL\nmethods, and benchmark state-of-the-art LLL algorithms in limited memory and\ncomputation regimes to shed light on their strengths and weaknesses. This\ncontinual learning paradigm also provides us with a pragmatic way of going\nbeyond centralized training which is the most commonly used training protocol\nin MARL. We empirically show that the agents trained in our setup are able to\ncoordinate well with unseen agents, without any additional assumptions made by\nprevious works. The code and all pre-trained models are available at\nhttps://github.com/chandar-lab/Lifelong-Hanabi.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 18:44:03 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 17:56:50 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Nekoei", "Hadi", ""], ["Badrinaaraayanan", "Akilesh", ""], ["Courville", "Aaron", ""], ["Chandar", "Sarath", ""]]}, {"id": "2103.03223", "submitter": "Tobias Schumacher", "authors": "Tobias Schumacher, Markus Strohmaier, Florian Lemmerich", "title": "A Comparative Evaluation of Quantification Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Quantification represents the problem of predicting class distributions in a\ngiven target set. It also represents a growing research field in supervised\nmachine learning, for which a large variety of different algorithms has been\nproposed in recent years. However, a comprehensive empirical comparison of\nquantification methods that supports algorithm selection is not available yet.\nIn this work, we close this research gap by conducting a thorough empirical\nperformance comparison of 24 different quantification methods. To consider a\nbroad range of different scenarios for binary as well as multiclass\nquantification settings, we carried out almost 3 million experimental runs on\n40 data sets. We observe that no single algorithm generally outperforms all\ncompetitors, but identify a group of methods including the Median Sweep and the\nDyS framework that perform significantly better in binary settings. For the\nmulticlass setting, we observe that a different, broad group of algorithms\nyields good performance, including the Generalized Probabilistic Adjusted\nCount, the readme method, the energy distance minimization method, the EM\nalgorithm for quantification, and Friedman's method. More generally, we find\nthat the performance on multiclass quantification is inferior to the results\nobtained in the binary setting. Our results can guide practitioners who intend\nto apply quantification algorithms and help researchers to identify\nopportunities for future research.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 18:51:06 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Schumacher", "Tobias", ""], ["Strohmaier", "Markus", ""], ["Lemmerich", "Florian", ""]]}, {"id": "2103.03227", "submitter": "E K", "authors": "E.Kurshan, H. Shen", "title": "Graph Computing for Financial Crime and Fraud Detection: Trends,\n  Challenges and Outlook", "comments": "arXiv admin note: substantial text overlap with arXiv:2103.01854", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.CY cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rise of digital payments has caused consequential changes in the\nfinancial crime landscape. As a result, traditional fraud detection approaches\nsuch as rule-based systems have largely become ineffective. AI and machine\nlearning solutions using graph computing principles have gained significant\ninterest in recent years. Graph-based techniques provide unique solution\nopportunities for financial crime detection. However, implementing such\nsolutions at industrial-scale in real-time financial transaction processing\nsystems has brought numerous application challenges to light. In this paper, we\ndiscuss the implementation difficulties current and next-generation graph\nsolutions face. Furthermore, financial crime and digital payments trends\nindicate emerging challenges in the continued effectiveness of the detection\ntechniques. We analyze the threat landscape and argue that it provides key\ninsights for developing graph-based solutions.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 21:14:44 GMT"}], "update_date": "2021-03-05", "authors_parsed": [["Kurshan", "E.", ""], ["Shen", "H.", ""]]}, {"id": "2103.03230", "submitter": "Jure Zbontar", "authors": "Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, St\\'ephane Deny", "title": "Barlow Twins: Self-Supervised Learning via Redundancy Reduction", "comments": "13 pages, 6 figures, to appear at ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised learning (SSL) is rapidly closing the gap with supervised\nmethods on large computer vision benchmarks. A successful approach to SSL is to\nlearn embeddings which are invariant to distortions of the input sample.\nHowever, a recurring issue with this approach is the existence of trivial\nconstant solutions. Most current methods avoid such solutions by careful\nimplementation details. We propose an objective function that naturally avoids\ncollapse by measuring the cross-correlation matrix between the outputs of two\nidentical networks fed with distorted versions of a sample, and making it as\nclose to the identity matrix as possible. This causes the embedding vectors of\ndistorted versions of a sample to be similar, while minimizing the redundancy\nbetween the components of these vectors. The method is called Barlow Twins,\nowing to neuroscientist H. Barlow's redundancy-reduction principle applied to a\npair of identical networks. Barlow Twins does not require large batches nor\nasymmetry between the network twins such as a predictor network, gradient\nstopping, or a moving average on the weight updates. Intriguingly it benefits\nfrom very high-dimensional output vectors. Barlow Twins outperforms previous\nmethods on ImageNet for semi-supervised classification in the low-data regime,\nand is on par with current state of the art for ImageNet classification with a\nlinear classifier head, and for transfer tasks of classification and object\ndetection.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 18:55:09 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 09:36:29 GMT"}, {"version": "v3", "created": "Mon, 14 Jun 2021 14:09:43 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Zbontar", "Jure", ""], ["Jing", "Li", ""], ["Misra", "Ishan", ""], ["LeCun", "Yann", ""], ["Deny", "St\u00e9phane", ""]]}, {"id": "2103.03279", "submitter": "Ayush Sekhari", "authors": "Ayush Sekhari, Jayadev Acharya, Gautam Kamath, Ananda Theertha Suresh", "title": "Remember What You Want to Forget: Algorithms for Machine Unlearning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We study the problem of unlearning datapoints from a learnt model. The\nlearner first receives a dataset $S$ drawn i.i.d. from an unknown distribution,\nand outputs a model $\\widehat{w}$ that performs well on unseen samples from the\nsame distribution. However, at some point in the future, any training datapoint\n$z \\in S$ can request to be unlearned, thus prompting the learner to modify its\noutput model while still ensuring the same accuracy guarantees. We initiate a\nrigorous study of generalization in machine unlearning, where the goal is to\nperform well on previously unseen datapoints. Our focus is on both\ncomputational and storage complexity.\n  For the setting of convex losses, we provide an unlearning algorithm that can\nunlearn up to $O(n/d^{1/4})$ samples, where $d$ is the problem dimension. In\ncomparison, in general, differentially private learning (which implies\nunlearning) only guarantees deletion of $O(n/d^{1/2})$ samples. This\ndemonstrates a novel separation between differential privacy and machine\nunlearning.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 19:28:57 GMT"}, {"version": "v2", "created": "Thu, 22 Jul 2021 17:45:56 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Sekhari", "Ayush", ""], ["Acharya", "Jayadev", ""], ["Kamath", "Gautam", ""], ["Suresh", "Ananda Theertha", ""]]}, {"id": "2103.03305", "submitter": "Kevin Xu", "authors": "Mohammadreza Nemati, Haonan Zhang, Michael Sloma, Dulat Bekbolsynov,\n  Hong Wang, Stanislaw Stepkowski, and Kevin S. Xu", "title": "Predicting Kidney Transplant Survival using Multiple Feature\n  Representations for HLAs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kidney transplantation can significantly enhance living standards for people\nsuffering from end-stage renal disease. A significant factor that affects graft\nsurvival time (the time until the transplant fails and the patient requires\nanother transplant) for kidney transplantation is the compatibility of the\nHuman Leukocyte Antigens (HLAs) between the donor and recipient. In this paper,\nwe propose new biologically-relevant feature representations for incorporating\nHLA information into machine learning-based survival analysis algorithms. We\nevaluate our proposed HLA feature representations on a database of over 100,000\ntransplants and find that they improve prediction accuracy by about 1%, modest\nat the patient level but potentially significant at a societal level. Accurate\nprediction of survival times can improve transplant survival outcomes, enabling\nbetter allocation of donors to recipients and reducing the number of\nre-transplants due to graft failure with poorly matched donors.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 20:22:47 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Nemati", "Mohammadreza", ""], ["Zhang", "Haonan", ""], ["Sloma", "Michael", ""], ["Bekbolsynov", "Dulat", ""], ["Wang", "Hong", ""], ["Stepkowski", "Stanislaw", ""], ["Xu", "Kevin S.", ""]]}, {"id": "2103.03359", "submitter": "Amol Kelkar", "authors": "Amol Kelkar", "title": "Cognitive Homeostatic Agents", "comments": "Accepted at AAMAS2021 Blue Sky Ideas Track", "journal-ref": "In Proc. of the 20th International Conference on Autonomous Agents\n  and Multiagent Systems (AAMAS 2021), Online, May 3-7, 2021, IFAAMAS, 5 pages", "doi": "10.5555/3461017.3461021", "report-no": null, "categories": "cs.AI cs.LG cs.MA cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Human brain has been used as an inspiration for building autonomous agents,\nbut it is not obvious what level of computational description of the brain one\nshould use. This has led to overly opinionated symbolic approaches and overly\nunstructured connectionist approaches. We propose that using homeostasis as the\ncomputational description provides a good compromise. Similar to how\nphysiological homeostasis is the regulation of certain homeostatic variables,\ncognition can be interpreted as the regulation of certain 'cognitive\nhomeostatic variables'. We present an outline of a Cognitive Homeostatic Agent,\nbuilt as a hierarchy of physiological and cognitive homeostatic subsystems and\ndescribe structures and processes to guide future exploration. We expect this\nto be a fruitful line of investigation towards building sophisticated\nartificial agents that can act flexibly in complex environments, and produce\nbehaviors indicating planning, thinking and feelings.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 07:29:43 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Kelkar", "Amol", ""]]}, {"id": "2103.03361", "submitter": "Natesh Ganesh", "authors": "Natesh Ganesh", "title": "From Quantifying Vagueness To Pan-niftyism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this short paper, we will introduce a simple model for quantifying\nphilosophical vagueness. There is growing interest in this endeavor to quantify\nvague concepts of consciousness, agency, etc. We will then discuss some of the\nimplications of this model including the conditions under which the\nquantification of `nifty' leads to pan-nifty-ism. Understanding this leads to\nan interesting insight - the reason a framework to quantify consciousness like\nIntegrated Information Theory implies (forms of) panpsychism is because there\nis favorable structure already implicitly encoded in the construction of the\nquantification metric.\n", "versions": [{"version": "v1", "created": "Mon, 1 Mar 2021 17:00:52 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Ganesh", "Natesh", ""]]}, {"id": "2103.03373", "submitter": "Sunghyun Park", "authors": "Han Li, Sunghyun Park, Aswarth Dara, Jinseok Nam, Sungjin Lee,\n  Young-Bum Kim, Spyros Matsoukas, Ruhi Sarikaya", "title": "Neural model robustness for skill routing in large-scale conversational\n  AI systems: A design choice exploration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current state-of-the-art large-scale conversational AI or intelligent digital\nassistant systems in industry comprises a set of components such as Automatic\nSpeech Recognition (ASR) and Natural Language Understanding (NLU). For some of\nthese systems that leverage a shared NLU ontology (e.g., a centralized\nintent/slot schema), there exists a separate skill routing component to\ncorrectly route a request to an appropriate skill, which is either a\nfirst-party or third-party application that actually executes on a user\nrequest. The skill routing component is needed as there are thousands of skills\nthat can either subscribe to the same intent and/or subscribe to an intent\nunder specific contextual conditions (e.g., device has a screen). Ensuring\nmodel robustness or resilience in the skill routing component is an important\nproblem since skills may dynamically change their subscription in the ontology\nafter the skill routing model has been deployed to production. We show how\ndifferent modeling design choices impact the model robustness in the context of\nskill routing on a state-of-the-art commercial conversational AI system,\nspecifically on the choices around data augmentation, model architecture, and\noptimization method. We show that applying data augmentation can be a very\neffective and practical way to drastically improve model robustness.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 22:54:33 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Li", "Han", ""], ["Park", "Sunghyun", ""], ["Dara", "Aswarth", ""], ["Nam", "Jinseok", ""], ["Lee", "Sungjin", ""], ["Kim", "Young-Bum", ""], ["Matsoukas", "Spyros", ""], ["Sarikaya", "Ruhi", ""]]}, {"id": "2103.03390", "submitter": "Nikola Zubi\\'c", "authors": "Nikola Zubi\\'c, Pietro Li\\`o", "title": "An Effective Loss Function for Generating 3D Models from Single 2D Image\n  without Rendering", "comments": "21 page, 13 figures, 6 tables, to appear as a full paper with oral\n  contribution in AIAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Differentiable rendering is a very successful technique that applies to a\nSingle-View 3D Reconstruction. Current renderers use losses based on pixels\nbetween a rendered image of some 3D reconstructed object and ground-truth\nimages from given matched viewpoints to optimise parameters of the 3D shape.\n  These models require a rendering step, along with visibility handling and\nevaluation of the shading model. The main goal of this paper is to demonstrate\nthat we can avoid these steps and still get reconstruction results as other\nstate-of-the-art models that are equal or even better than existing\ncategory-specific reconstruction methods. First, we use the same CNN\narchitecture for the prediction of a point cloud shape and pose prediction like\nthe one used by Insafutdinov & Dosovitskiy. Secondly, we propose the novel\neffective loss function that evaluates how well the projections of\nreconstructed 3D point clouds cover the ground truth object's silhouette. Then\nwe use Poisson Surface Reconstruction to transform the reconstructed point\ncloud into a 3D mesh. Finally, we perform a GAN-based texture mapping on a\nparticular 3D mesh and produce a textured 3D mesh from a single 2D image. We\nevaluate our method on different datasets (including ShapeNet, CUB-200-2011,\nand Pascal3D+) and achieve state-of-the-art results, outperforming all the\nother supervised and unsupervised methods and 3D representations, all in terms\nof performance, accuracy, and training time.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 00:02:18 GMT"}, {"version": "v2", "created": "Fri, 30 Apr 2021 09:47:39 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Zubi\u0107", "Nikola", ""], ["Li\u00f2", "Pietro", ""]]}, {"id": "2103.03411", "submitter": "Kanthi Sarpatwar", "authors": "Kanthi Sarpatwar and Karthik Nandakumar and Nalini Ratha and James\n  Rayfield and Karthikeyan Shanmugam and Sharath Pankanti and Roman Vaculin", "title": "Efficient Encrypted Inference on Ensembles of Decision Trees", "comments": "9 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data privacy concerns often prevent the use of cloud-based machine learning\nservices for sensitive personal data. While homomorphic encryption (HE) offers\na potential solution by enabling computations on encrypted data, the challenge\nis to obtain accurate machine learning models that work within the\nmultiplicative depth constraints of a leveled HE scheme. Existing approaches\nfor encrypted inference either make ad-hoc simplifications to a pre-trained\nmodel (e.g., replace hard comparisons in a decision tree with soft comparators)\nat the cost of accuracy or directly train a new depth-constrained model using\nthe original training set. In this work, we propose a framework to transfer\nknowledge extracted by complex decision tree ensembles to shallow neural\nnetworks (referred to as DTNets) that are highly conducive to encrypted\ninference. Our approach minimizes the accuracy loss by searching for the best\nDTNet architecture that operates within the given depth constraints and\ntraining this DTNet using only synthetic data sampled from the training data\ndistribution. Extensive experiments on real-world datasets demonstrate that\nthese characteristics are critical in ensuring that DTNet accuracy approaches\nthat of the original tree ensemble. Our system is highly scalable and can\nperform efficient inference on batched encrypted (134 bits of security) data\nwith amortized time in milliseconds. This is approximately three orders of\nmagnitude faster than the standard approach of applying soft comparison at the\ninternal nodes of the ensemble trees.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 01:06:30 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Sarpatwar", "Kanthi", ""], ["Nandakumar", "Karthik", ""], ["Ratha", "Nalini", ""], ["Rayfield", "James", ""], ["Shanmugam", "Karthikeyan", ""], ["Pankanti", "Sharath", ""], ["Vaculin", "Roman", ""]]}, {"id": "2103.03412", "submitter": "Shuang Yang", "authors": "Zhigang Hua, Feng Qi, Gan Liu and Shuang Yang", "title": "Learning to Schedule DAG Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scheduling computational tasks represented by directed acyclic graphs (DAGs)\nis challenging because of its complexity. Conventional scheduling algorithms\nrely heavily on simple heuristics such as shortest job first (SJF) and critical\npath (CP), and are often lacking in scheduling quality. In this paper, we\npresent a novel learning-based approach to scheduling DAG tasks. The algorithm\nemploys a reinforcement learning agent to iteratively add directed edges to the\nDAG, one at a time, to enforce ordering (i.e., priorities of execution and\nresource allocation) of \"tricky\" job nodes. By doing so, the original DAG\nscheduling problem is dramatically reduced to a much simpler proxy problem, on\nwhich heuristic scheduling algorithms such as SJF and CP can be efficiently\nimproved. Our approach can be easily applied to any existing heuristic\nscheduling algorithms. On the benchmark dataset of TPC-H, we show that our\nlearning based approach can significantly improve over popular heuristic\nalgorithms and consistently achieves the best performance among several methods\nunder a variety of settings.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 01:10:24 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Hua", "Zhigang", ""], ["Qi", "Feng", ""], ["Liu", "Gan", ""], ["Yang", "Shuang", ""]]}, {"id": "2103.03413", "submitter": "Yi-Lin Tsai", "authors": "Yi-Lin Tsai (1), Chetanya Rastogi (2), Peter K. Kitanidis (1, 3, and\n  4), Christopher B. Field (3, 5, and 6) ((1) Department of Civil and\n  Environmental Engineering, Stanford University, Stanford, CA, USA, (2)\n  Department of Computer Science, Stanford University, Stanford, CA, USA, (3)\n  Woods Institute for the Environment, Stanford University, Stanford, CA, USA,\n  (4) Institute for Computational and Mathematical Engineering, Stanford\n  University, Stanford, CA, USA, (5) Department of Biology, Stanford\n  University, Stanford, CA, USA, (6) Department of Earth System Science,\n  Stanford University, Stanford, CA, USA)", "title": "Routing algorithms as tools for integrating social distancing with\n  emergency evacuation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore the implications of integrating social distancing with emergency\nevacuation, as would be expected when a hurricane approaches a city during the\nCOVID-19 pandemic. Specifically, we compare DNN (Deep Neural Network)-based and\nnon-DNN methods for generating evacuation strategies that minimize evacuation\ntime while allowing for social distancing in emergency vehicles. A central\nquestion is whether a DNN-based method provides sufficient extra routing\nefficiency to accommodate increased social distancing in a time-constrained\nevacuation operation. We describe the problem as a Capacitated Vehicle Routing\nProblem and solve it using a non-DNN solution (Sweep Algorithm) and a DNN-based\nsolution (Deep Reinforcement Learning). The DNN-based solution can provide\ndecision-makers with more efficient routing than the typical non-DNN routing\nsolution. However, it does not come close to compensating for the extra time\nrequired for social distancing, and its advantage disappears as the emergency\nvehicle capacity approaches the number of people per household.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 01:12:31 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 22:43:07 GMT"}, {"version": "v3", "created": "Mon, 10 May 2021 02:26:53 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Tsai", "Yi-Lin", "", "1, 3, and\n  4"], ["Rastogi", "Chetanya", "", "1, 3, and\n  4"], ["Kitanidis", "Peter K.", "", "1, 3, and\n  4"], ["Field", "Christopher B.", "", "3, 5, and 6"]]}, {"id": "2103.03429", "submitter": "Xiaowei Zhou", "authors": "Xiaowei Zhou, Jie Yin, Ivor Tsang and Chen Wang", "title": "Human-Understandable Decision Making for Visual Recognition", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The widespread use of deep neural networks has achieved substantial success\nin many tasks. However, there still exists a huge gap between the operating\nmechanism of deep learning models and human-understandable decision making, so\nthat humans cannot fully trust the predictions made by these models. To date,\nlittle work has been done on how to align the behaviors of deep learning models\nwith human perception in order to train a human-understandable model. To fill\nthis gap, we propose a new framework to train a deep neural network by\nincorporating the prior of human perception into the model learning process.\nOur proposed model mimics the process of perceiving conceptual parts from\nimages and assessing their relative contributions towards the final\nrecognition. The effectiveness of our proposed model is evaluated on two\nclassical visual recognition tasks. The experimental results and analysis\nconfirm our model is able to provide interpretable explanations for its\npredictions, but also maintain competitive recognition accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 02:07:33 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Zhou", "Xiaowei", ""], ["Yin", "Jie", ""], ["Tsang", "Ivor", ""], ["Wang", "Chen", ""]]}, {"id": "2103.03438", "submitter": "Tao Zhang", "authors": "Mengting Xu, Tao Zhang, Zhongnian Li, Mingxia Liu, Daoqiang Zhang", "title": "Towards Evaluating the Robustness of Deep Diagnostic Models by\n  Adversarial Attack", "comments": "This version was accepted in the journal Medical Image Analysis\n  (MedIA)", "journal-ref": "Medical Image Analysis 69 (2021): 101977", "doi": "10.1016/j.media.2021.101977", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning models (with neural networks) have been widely used in\nchallenging tasks such as computer-aided disease diagnosis based on medical\nimages. Recent studies have shown deep diagnostic models may not be robust in\nthe inference process and may pose severe security concerns in clinical\npractice. Among all the factors that make the model not robust, the most\nserious one is adversarial examples. The so-called \"adversarial example\" is a\nwell-designed perturbation that is not easily perceived by humans but results\nin a false output of deep diagnostic models with high confidence. In this\npaper, we evaluate the robustness of deep diagnostic models by adversarial\nattack. Specifically, we have performed two types of adversarial attacks to\nthree deep diagnostic models in both single-label and multi-label\nclassification tasks, and found that these models are not reliable when\nattacked by adversarial example. We have further explored how adversarial\nexamples attack the models, by analyzing their quantitative classification\nresults, intermediate features, discriminability of features and correlation of\nestimated labels for both original/clean images and those adversarial ones. We\nhave also designed two new defense methods to handle adversarial examples in\ndeep diagnostic models, i.e., Multi-Perturbations Adversarial Training (MPAdvT)\nand Misclassification-Aware Adversarial Training (MAAdvT). The experimental\nresults have shown that the use of defense methods can significantly improve\nthe robustness of deep diagnostic models against adversarial attacks.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 02:24:47 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Xu", "Mengting", ""], ["Zhang", "Tao", ""], ["Li", "Zhongnian", ""], ["Liu", "Mingxia", ""], ["Zhang", "Daoqiang", ""]]}, {"id": "2103.03454", "submitter": "Wenguan Wang", "authors": "Hanqing Wang, Wenguan Wang, Wei Liang, Caiming Xiong, Jianbing Shen", "title": "Structured Scene Memory for Vision-Language Navigation", "comments": "Accepted on CVPR2021; Implementation will be available at\n  https://github.com/HanqingWangAI/SSM-VLN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, numerous algorithms have been developed to tackle the problem of\nvision-language navigation (VLN), i.e., entailing an agent to navigate 3D\nenvironments through following linguistic instructions. However, current VLN\nagents simply store their past experiences/observations as latent states in\nrecurrent networks, failing to capture environment layouts and make long-term\nplanning. To address these limitations, we propose a crucial architecture,\ncalled Structured Scene Memory (SSM). It is compartmentalized enough to\naccurately memorize the percepts during navigation. It also serves as a\nstructured scene representation, which captures and disentangles visual and\ngeometric cues in the environment. SSM has a collect-read controller that\nadaptively collects information for supporting current decision making and\nmimics iterative algorithms for long-range reasoning. As SSM provides a\ncomplete action space, i.e., all the navigable places on the map, a\nfrontier-exploration based navigation decision making strategy is introduced to\nenable efficient and global planning. Experiment results on two VLN datasets\n(i.e., R2R and R4R) show that our method achieves state-of-the-art performance\non several metrics.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 03:41:00 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Wang", "Hanqing", ""], ["Wang", "Wenguan", ""], ["Liang", "Wei", ""], ["Xiong", "Caiming", ""], ["Shen", "Jianbing", ""]]}, {"id": "2103.03457", "submitter": "Lijun Wu", "authors": "Jinhua Zhu, Lijun Wu, Yingce Xia, Shufang Xie, Tao Qin, Wengang Zhou,\n  Houqiang Li, Tie-Yan Liu", "title": "IOT: Instance-wise Layer Reordering for Transformer Structures", "comments": "Accepted at ICLR-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With sequentially stacked self-attention, (optional) encoder-decoder\nattention, and feed-forward layers, Transformer achieves big success in natural\nlanguage processing (NLP), and many variants have been proposed. Currently,\nalmost all these models assume that the layer order is fixed and kept the same\nacross data samples. We observe that different data samples actually favor\ndifferent orders of the layers. Based on this observation, in this work, we\nbreak the assumption of the fixed layer order in the Transformer and introduce\ninstance-wise layer reordering into the model structure. Our Instance-wise\nOrdered Transformer (IOT) can model variant functions by reordered layers,\nwhich enables each sample to select the better one to improve the model\nperformance under the constraint of almost the same number of parameters. To\nachieve this, we introduce a light predictor with negligible parameter and\ninference cost to decide the most capable and favorable layer order for any\ninput sequence. Experiments on 3 tasks (neural machine translation, abstractive\nsummarization, and code generation) and 9 datasets demonstrate consistent\nimprovements of our method. We further show that our method can also be applied\nto other architectures beyond Transformer. Our code is released at Github.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 03:44:42 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Zhu", "Jinhua", ""], ["Wu", "Lijun", ""], ["Xia", "Yingce", ""], ["Xie", "Shufang", ""], ["Qin", "Tao", ""], ["Zhou", "Wengang", ""], ["Li", "Houqiang", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "2103.03482", "submitter": "William Wagner", "authors": "William Wagner, Anna \\'Zakowska, Clement Aladi, Joseph Santhosh", "title": "Pilot Investigation for a Comprehensive Taxonomy of Autonomous Entities", "comments": "15 pages, 4 figures, 7 tables, 2 appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.LO cs.NE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper documents an exploratory pilot study to define the term Autonomous\nEntity, and any characteristics that are required to identify or classify an\nAutonomous Entity. Our solution builds on previous work with regard to\nphilosophical and scientific classification methods but focuses on a novel\nDesign Science Research Methodology (DSRM) and model to help identify those\ncharacteristics which make any autonomous entity similar or different from\nothers. We have solved the problem of not having an existing term to define our\nlens by creating a new combinatorial term: \"Riskyishness\". We present a DSRM\nand instrument for initial investigation, as well as observational and\nstatistical descriptions of their use in the real world to solicit domain\nexpertise and statistical evidence. Further, we demonstrate a specific\napplication of the methodology by creating a second artifact - a tool to score\nexisting and future technologies based on Riskyishness. The first artifact also\nprovides a technique to disentangle miscellaneous existing technologies or add\ndimensions to the tools to capture future additions and paradigm shifts.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 05:51:40 GMT"}, {"version": "v2", "created": "Mon, 26 Apr 2021 07:33:26 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Wagner", "William", ""], ["\u0179akowska", "Anna", ""], ["Aladi", "Clement", ""], ["Santhosh", "Joseph", ""]]}, {"id": "2103.03488", "submitter": "Daniel Leite", "authors": "Daniel Leite, Volnei Frigeri Jr., Rodrigo Medeiros", "title": "Adaptive Gaussian Fuzzy Classifier for Real-Time Emotion Recognition in\n  Computer Games", "comments": "7 pages, 6 figures, Fuzz-IEEE 2021, Luxembourg", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Human emotion recognition has become a need for more realistic and\ninteractive machines and computer systems. The greatest challenge is the\navailability of high-performance algorithms to effectively manage individual\ndifferences and nonstationarities in physiological data streams, i.e.,\nalgorithms that self-customize to a user with no subject-specific calibration\ndata. We describe an evolving Gaussian Fuzzy Classifier (eGFC), which is\nsupported by an online semi-supervised learning algorithm to recognize emotion\npatterns from electroencephalogram (EEG) data streams. We extract features from\nthe Fourier spectrum of EEG data. The data are provided by 28 individuals\nplaying the games 'Train Sim World', 'Unravel', 'Slender The Arrival', and\n'Goat Simulator' - a public dataset. Different emotions prevail, namely,\nboredom, calmness, horror and joy. We analyze the effect of individual\nelectrodes, time window lengths, and frequency bands on the accuracy of\nuser-independent eGFCs. We conclude that both brain hemispheres may assist\nclassification, especially electrodes on the frontal (Af3-Af4), occipital\n(O1-O2), and temporal (T7-T8) areas. We observe that patterns may be eventually\nfound in any frequency band; however, the Alpha (8-13Hz), Delta (1-4Hz), and\nTheta (4-8Hz) bands, in this order, are the highest correlated with emotion\nclasses. eGFC has shown to be effective for real-time learning of EEG data. It\nreaches a 72.2% accuracy using a variable rule base, 10-second windows, and\n1.8ms/sample processing time in a highly-stochastic time-varying 4-class\nclassification problem.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 06:27:04 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Leite", "Daniel", ""], ["Frigeri", "Volnei", "Jr."], ["Medeiros", "Rodrigo", ""]]}, {"id": "2103.03501", "submitter": "Giang Truong", "authors": "Giang Truong, Huu Le, David Suter, Erchuan Zhang, Syed Zulqarnain\n  Gilani", "title": "Unsupervised Learning for Robust Fitting:A Reinforcement Learning\n  Approach", "comments": "The preprint of paper accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust model fitting is a core algorithm in a large number of computer vision\napplications. Solving this problem efficiently for datasets highly contaminated\nwith outliers is, however, still challenging due to the underlying\ncomputational complexity. Recent literature has focused on learning-based\nalgorithms. However, most approaches are supervised which require a large\namount of labelled training data. In this paper, we introduce a novel\nunsupervised learning framework that learns to directly solve robust model\nfitting. Unlike other methods, our work is agnostic to the underlying input\nfeatures, and can be easily generalized to a wide variety of LP-type problems\nwith quasi-convex residuals. We empirically show that our method outperforms\nexisting unsupervised learning approaches, and achieves competitive results\ncompared to traditional methods on several important computer vision problems.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 07:14:00 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Truong", "Giang", ""], ["Le", "Huu", ""], ["Suter", "David", ""], ["Zhang", "Erchuan", ""], ["Gilani", "Syed Zulqarnain", ""]]}, {"id": "2103.03526", "submitter": "Hugo Siqueira Gomes", "authors": "Hugo Siqueira Gomes, Benjamin L\\'eger and Christian Gagn\\'e", "title": "Meta Learning Black-Box Population-Based Optimizers", "comments": "9 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The no free lunch theorem states that no model is better suited to every\nproblem. A question that arises from this is how to design methods that propose\noptimizers tailored to specific problems achieving state-of-the-art\nperformance. This paper addresses this issue by proposing the use of\nmeta-learning to infer population-based black-box optimizers that can\nautomatically adapt to specific classes of problems. We suggest a general\nmodeling of population-based algorithms that result in Learning-to-Optimize\nPOMDP (LTO-POMDP), a meta-learning framework based on a specific partially\nobservable Markov decision process (POMDP). From that framework's formulation,\nwe propose to parameterize the algorithm using deep recurrent neural networks\nand use a meta-loss function based on stochastic algorithms' performance to\ntrain efficient data-driven optimizers over several related optimization tasks.\nThe learned optimizers' performance based on this implementation is assessed on\nvarious black-box optimization tasks and hyperparameter tuning of machine\nlearning models. Our results revealed that the meta-loss function encourages a\nlearned algorithm to alter its search behavior so that it can easily fit into a\nnew context. Thus, it allows better generalization and higher sample efficiency\nthan state-of-the-art generic optimization algorithms, such as the Covariance\nmatrix adaptation evolution strategy (CMA-ES).\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 08:13:25 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Gomes", "Hugo Siqueira", ""], ["L\u00e9ger", "Benjamin", ""], ["Gagn\u00e9", "Christian", ""]]}, {"id": "2103.03539", "submitter": "Pengfei Qu", "authors": "Xintian Wu, Pengfei Qu, Shaofei Wang, Lin Xie and Jie Dong", "title": "Extend the FFmpeg Framework to Analyze Media Content", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper introduces a new set of video analytics plugins developed for the\nFFmpeg framework. Multimedia applications that increasingly utilize the FFmpeg\nmedia features for its comprehensive media encoding, decoding, muxing, and\ndemuxing capabilities can now additionally analyze the video content based on\nAI models. The plugins are thread optimized for best performance overcoming\ncertain FFmpeg threading limitations. The plugins utilize the Intel OpenVINO\nToolkit inference engine as the backend. The analytics workloads are\naccelerated on different platforms such as CPU, GPU, FPGA or specialized\nanalytics accelerators. With our reference implementation, the feature of\nOpenVINO as inference backend has been pushed into FFmpeg mainstream\nrepository. We plan to submit more patches later.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 08:39:47 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Wu", "Xintian", ""], ["Qu", "Pengfei", ""], ["Wang", "Shaofei", ""], ["Xie", "Lin", ""], ["Dong", "Jie", ""]]}, {"id": "2103.03544", "submitter": "Dejan Ni\\v{c}kovi\\'c", "authors": "Nadja Marko, Eike M\\\"ohlmann, Dejan Ni\\v{c}kovi\\'c, J\\\"urgen Niehaus,\n  Peter Priller, Martijn Rooker", "title": "Challenges of engineering safe and secure highly automated vehicles", "comments": "13 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After more than a decade of intense focus on automated vehicles, we are still\nfacing huge challenges for the vision of fully autonomous driving to become a\nreality. The same \"disillusionment\" is true in many other domains, in which\nautonomous Cyber-Physical Systems (CPS) could considerably help to overcome\nsocietal challenges and be highly beneficial to society and individuals. Taking\nthe automotive domain, i.e. highly automated vehicles (HAV), as an example,\nthis paper sets out to summarize the major challenges that are still to\novercome for achieving safe, secure, reliable and trustworthy highly automated\nresp. autonomous CPS. We constrain ourselves to technical challenges,\nacknowledging the importance of (legal) regulations, certification,\nstandardization, ethics, and societal acceptance, to name but a few, without\ndelving deeper into them as this is beyond the scope of this paper. Four\nchallenges have been identified as being the main obstacles to realizing HAV:\nRealization of continuous, post-deployment systems improvement, handling of\nuncertainties and incomplete information, verification of HAV with machine\nlearning components, and prediction. Each of these challenges is described in\ndetail, including sub-challenges and, where appropriate, possible approaches to\novercome them. By working together in a common effort between industry and\nacademy and focusing on these challenges, the authors hope to contribute to\novercome the \"disillusionment\" for realizing HAV.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 08:52:31 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 21:27:51 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Marko", "Nadja", ""], ["M\u00f6hlmann", "Eike", ""], ["Ni\u010dkovi\u0107", "Dejan", ""], ["Niehaus", "J\u00fcrgen", ""], ["Priller", "Peter", ""], ["Rooker", "Martijn", ""]]}, {"id": "2103.03568", "submitter": "Jiaye Teng", "authors": "Jiaye Teng, Weiran Huang, Haowei He", "title": "Can Pretext-Based Self-Supervised Learning Be Boosted by Downstream\n  Data? A Theoretical Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pretext-based self-supervised learning aims to learn the semantic\nrepresentation via a handcrafted pretext task over unlabeled data and then use\nthe learned representation for downstream prediction tasks. It is proved that\npretext-based self-supervised learning can effectively reduce the sample\ncomplexity of downstream tasks under Conditional Independence (CI) between the\ncomponents of the pretext task conditional on the downstream label. However,\nthe downstream sample complexity will get much worse if the CI condition does\nnot hold. One interesting question is whether we can make the CI condition hold\nby using downstream data to refine the unlabeled data to boost self-supervised\nlearning. At first glance, one might think that seeing downstream data in\nadvance would always boost the downstream performance. However, we show that it\nis not intuitively true and point out that in some cases, it will hurt the\nfinal performance instead. In particular, we prove both model-free and\nmodel-dependent lower bounds of the number of downstream samples used for data\nrefinement. Moreover, we conduct several experiments on both synthetic and\nreal-world datasets to verify our theoretical results.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 09:53:10 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 14:36:34 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Teng", "Jiaye", ""], ["Huang", "Weiran", ""], ["He", "Haowei", ""]]}, {"id": "2103.03572", "submitter": "Kirill Glinskiy", "authors": "Kirill Glinskiy, Evgeny Khorov, Alexey Kureev", "title": "SDR-based Testbed for Real-time CQI Prediction for URLLC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ultra-reliable Low-Latency Communication (URLLC) is a key feature of 5G\nsystems. The quality of service (QoS) requirements imposed by URLLC are less\nthan 10ms delay and less than $10^{-5}$ packet loss rate (PLR). To satisfy such\nstrict requirements with minimal channel resource consumption, the devices need\nto accurately predict the channel quality and select Modulation and Coding\nScheme (MCS) for URLLC in a proper way.\n  This paper presents a novel real-time channel prediction system based on\nSoftware-Defined Radio that uses a neural network. The paper also describes and\nshares an open channel measurement dataset that can be used to compare various\nchannel prediction approaches in different mobility scenarios in future\nresearch on URLLC\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 10:17:36 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Glinskiy", "Kirill", ""], ["Khorov", "Evgeny", ""], ["Kureev", "Alexey", ""]]}, {"id": "2103.03580", "submitter": "Sara Durrani", "authors": "Sara Durrani, Muhammad Umair Arshad", "title": "Transfer Learning based Speech Affect Recognition in Urdu", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been established that Speech Affect Recognition for low resource\nlanguages is a difficult task. Here we present a Transfer learning based Speech\nAffect Recognition approach in which: we pre-train a model for high resource\nlanguage affect recognition task and fine tune the parameters for low resource\nlanguage using Deep Residual Network. Here we use standard four data sets to\ndemonstrate that transfer learning can solve the problem of data scarcity for\nAffect Recognition task. We demonstrate that our approach is efficient by\nachieving 74.7 percent UAR on RAVDESS as source and Urdu data set as a target.\nThrough an ablation study, we have identified that pre-trained model adds most\nof the features information, improvement in results and solves less data\nissues. Using this knowledge, we have also experimented on SAVEE and EMO-DB\ndata set by setting Urdu as target language where only 400 utterances of data\nis available. This approach achieves high Unweighted Average Recall (UAR) when\ncompared with existing algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 10:30:58 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Durrani", "Sara", ""], ["Arshad", "Muhammad Umair", ""]]}, {"id": "2103.03587", "submitter": "Paula Gomez Duran", "authors": "Paula G\\'omez Duran, Alexandros Karatzoglou, Jordi Vitri\\`a, Xin Xin,\n  Ioannis Arapakis", "title": "Graph Convolutional Embeddings for Recommender Systems", "comments": "10 pages, 4 figures, SIGIR July 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modern recommender systems (RS) work by processing a number of signals that\ncan be inferred from large sets of user-item interaction data. The main signal\nto analyze stems from the raw matrix that represents interactions. However, we\ncan increase the performance of RS by considering other kinds of signals like\nthe context of interactions, which could be, for example, the time or date of\nthe interaction, the user location, or sequential data corresponding to the\nhistorical interactions of the user with the system. These complex,\ncontext-based interaction signals are characterized by a rich relational\nstructure that can be represented by a multi-partite graph. Graph Convolutional\nNetworks (GCNs) have been used successfully in collaborative filtering with\nsimple user-item interaction data. In this work, we generalize the use of GCNs\nfor N-partite graphs by considering N multiple context dimensions and propose a\nsimple way for their seamless integration in modern deep learning RS\narchitectures. More specifically, we define a graph convolutional embedding\nlayer for N-partite graphs that processes user-item-context interactions, and\nconstructs node embeddings by leveraging their relational structure.\nExperiments on several datasets from recommender systems to drug re-purposing\nshow the benefits of the introduced GCN embedding layer by measuring the\nperformance of different context-enriched tasks.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 10:46:16 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Duran", "Paula G\u00f3mez", ""], ["Karatzoglou", "Alexandros", ""], ["Vitri\u00e0", "Jordi", ""], ["Xin", "Xin", ""], ["Arapakis", "Ioannis", ""]]}, {"id": "2103.03598", "submitter": "Bhavya Ghai", "authors": "Bhavya Ghai, Md Naimul Hoque, Klaus Mueller", "title": "WordBias: An Interactive Visual Tool for Discovering Intersectional\n  Biases Encoded in Word Embeddings", "comments": "Accepted to ACM SIGCHI 2021 LBW", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intersectional bias is a bias caused by an overlap of multiple social factors\nlike gender, sexuality, race, disability, religion, etc. A recent study has\nshown that word embedding models can be laden with biases against\nintersectional groups like African American females, etc. The first step\ntowards tackling such intersectional biases is to identify them. However,\ndiscovering biases against different intersectional groups remains a\nchallenging task. In this work, we present WordBias, an interactive visual tool\ndesigned to explore biases against intersectional groups encoded in static word\nembeddings. Given a pretrained static word embedding, WordBias computes the\nassociation of each word along different groups based on race, age, etc. and\nthen visualizes them using a novel interactive interface. Using a case study,\nwe demonstrate how WordBias can help uncover biases against intersectional\ngroups like Black Muslim Males, Poor Females, etc. encoded in word embedding.\nIn addition, we also evaluate our tool using qualitative feedback from expert\ninterviews. The source code for this tool can be publicly accessed for\nreproducibility at github.com/bhavyaghai/WordBias.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 11:04:35 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Ghai", "Bhavya", ""], ["Hoque", "Md Naimul", ""], ["Mueller", "Klaus", ""]]}, {"id": "2103.03602", "submitter": "Ahmed Rasheed", "authors": "Ahmed Rasheed, Muhammad Shahzad Younis, Junaid Qadir and Muhammad\n  Bilal", "title": "Use of Transfer Learning and Wavelet Transform for Breast Cancer\n  Detection", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Breast cancer is one of the most common cause of deaths among women.\nMammography is a widely used imaging modality that can be used for cancer\ndetection in its early stages. Deep learning is widely used for the detection\nof cancerous masses in the images obtained via mammography. The need to improve\naccuracy remains constant due to the sensitive nature of the datasets so we\nintroduce segmentation and wavelet transform to enhance the important features\nin the image scans. Our proposed system aids the radiologist in the screening\nphase of cancer detection by using a combination of segmentation and wavelet\ntransforms as pre-processing augmentation that leads to transfer learning in\nneural networks. The proposed system with these pre-processing techniques\nsignificantly increases the accuracy of detection on Mini-MIAS.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 11:08:56 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Rasheed", "Ahmed", ""], ["Younis", "Muhammad Shahzad", ""], ["Qadir", "Junaid", ""], ["Bilal", "Muhammad", ""]]}, {"id": "2103.03610", "submitter": "Iain Barclay", "authors": "Iain Barclay, Harrison Taylor, Alun Preece, Ian Taylor, Dinesh Verma,\n  Geeth de Mel", "title": "A framework for fostering transparency in shared artificial intelligence\n  models by increasing visibility of contributions", "comments": "This is the pre-peer reviewed version of the following article:\n  Barclay I, Taylor H, Preece A, Taylor I, Verma D, de Mel G. A framework for\n  fostering transparency in shared artificial intelligence models by increasing\n  visibility of contributions. Concurrency Computat Pract Exper. 2020;e6129.\n  arXiv admin note: substantial text overlap with arXiv:1907.03483", "journal-ref": null, "doi": "10.1002/cpe.6129", "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Increased adoption of artificial intelligence (AI) systems into scientific\nworkflows will result in an increasing technical debt as the distance between\nthe data scientists and engineers who develop AI system components and\nscientists, researchers and other users grows. This could quickly become\nproblematic, particularly where guidance or regulations change and\nonce-acceptable best practice becomes outdated, or where data sources are later\ndiscredited as biased or inaccurate. This paper presents a novel method for\nderiving a quantifiable metric capable of ranking the overall transparency of\nthe process pipelines used to generate AI systems, such that users, auditors\nand other stakeholders can gain confidence that they will be able to validate\nand trust the data sources and contributors in the AI systems that they rely\non. The methodology for calculating the metric, and the type of criteria that\ncould be used to make judgements on the visibility of contributions to systems\nare evaluated through models published at ModelHub and PyTorch Hub, popular\narchives for sharing science resources, and is found to be helpful in driving\nconsideration of the contributions made to generating AI systems and approaches\ntowards effective documentation and improving transparency in machine learning\nassets shared within scientific communities.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 11:28:50 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Barclay", "Iain", ""], ["Taylor", "Harrison", ""], ["Preece", "Alun", ""], ["Taylor", "Ian", ""], ["Verma", "Dinesh", ""], ["de Mel", "Geeth", ""]]}, {"id": "2103.03636", "submitter": "Peijun Tang", "authors": "Lili Pan, Peijun Tang, Zhiyong Chen, Zenglin Xu", "title": "Contrastive Disentanglement in Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disentanglement is defined as the problem of learninga representation that\ncan separate the distinct, informativefactors of variations of data. Learning\nsuch a representa-tion may be critical for developing explainable and\nhuman-controllable Deep Generative Models (DGMs) in artificialintelligence.\nHowever, disentanglement in GANs is not a triv-ial task, as the absence of\nsample likelihood and posteriorinference for latent variables seems to prohibit\nthe forwardstep. Inspired by contrastive learning (CL), this paper, froma new\nperspective, proposes contrastive disentanglement ingenerative adversarial\nnetworks (CD-GAN). It aims at dis-entangling the factors of inter-class\nvariation of visual datathrough contrasting image features, since the same\nfactorvalues produce images in the same class. More importantly,we probe a\nnovel way to make use of limited amount ofsupervision to the largest extent, to\npromote inter-class dis-entanglement performance. Extensive experimental\nresultson many well-known datasets demonstrate the efficacy ofCD-GAN for\ndisentangling inter-class variation.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 12:44:22 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Pan", "Lili", ""], ["Tang", "Peijun", ""], ["Chen", "Zhiyong", ""], ["Xu", "Zenglin", ""]]}, {"id": "2103.03638", "submitter": "Mark Niklas M\\\"uller", "authors": "Mark Niklas M\\\"uller, Gleb Makarchuk, Gagandeep Singh, Markus\n  P\\\"uschel, Martin Vechev", "title": "PRIMA: Precise and General Neural Network Certification via Multi-Neuron\n  Convex Relaxations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Formal verification of neural networks is critical for their safe and secure\nadoption in real-world applications. However, designing a precise and scalable\nverifier which can handle different activation functions, realistic network\narchitectures and relevant specifications remains an open and difficult\nchallenge. In this paper, we take a major step in addressing this challenge and\npresent a new verification framework, called PRIMA. PRIMA is both (i) general:\nit handles any non-linear activation function, and (ii) precise: it computes\nprecise convex approximations involving multiple neurons via novel convex hull\napproximation algorithms that leverage concepts from computational geometry.\nThe algorithms have polynomial complexity, yield fewer constraints, and\nminimize precision loss. We evaluate the effectiveness of PRIMA on a variety of\nchallenging image classifiers from prior work. Our results show that PRIMA is\nsignificantly more precise than state-of-the-art, verifying robustness for up\nto 14%, 30%, and 34% more images than existing work on ReLU-, Sigmoid-, and\nTanh-based networks, respectively. Further, PRIMA enables, for the first time,\nprecise verification of a realistic neural network for autonomous driving\nwithin a few minutes.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 12:53:24 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 15:42:07 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["M\u00fcller", "Mark Niklas", ""], ["Makarchuk", "Gleb", ""], ["Singh", "Gagandeep", ""], ["P\u00fcschel", "Markus", ""], ["Vechev", "Martin", ""]]}, {"id": "2103.03656", "submitter": "Yoshihiro Okawa", "authors": "Yoshihiro Okawa, Tomotake Sasaki and Hidenao Iwane", "title": "Automatic Exploration Process Adjustment for Safe Reinforcement Learning\n  with Joint Chance Constraint Satisfaction", "comments": "Accepted to the 21st IFAC World Congress (IFAC-V 2020)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SY eess.SY stat.ML", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In reinforcement learning (RL) algorithms, exploratory control inputs are\nused during learning to acquire knowledge for decision making and control,\nwhile the true dynamics of a controlled object is unknown. However, this\nexploring property sometimes causes undesired situations by violating\nconstraints regarding the state of the controlled object. In this paper, we\npropose an automatic exploration process adjustment method for safe RL in\ncontinuous state and action spaces utilizing a linear nominal model of the\ncontrolled object. Specifically, our proposed method automatically selects\nwhether the exploratory input is used or not at each time depending on the\nstate and its predicted value as well as adjusts the variance-covariance matrix\nused in the Gaussian policy for exploration. We also show that our exploration\nprocess adjustment method theoretically guarantees the satisfaction of the\nconstraints with the pre-specified probability, that is, the satisfaction of a\njoint chance constraint at every time. Finally, we illustrate the validity and\nthe effectiveness of our method through numerical simulation.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 13:30:53 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Okawa", "Yoshihiro", ""], ["Sasaki", "Tomotake", ""], ["Iwane", "Hidenao", ""]]}, {"id": "2103.03662", "submitter": "Dani\\\"el Willemsen Willemsen", "authors": "Dani\\\"el Willemsen, Mario Coppola and Guido C.H.E. de Croon", "title": "MAMBPO: Sample-efficient multi-robot reinforcement learning using\n  learned world models", "comments": "Submitted to 2021 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-robot systems can benefit from reinforcement learning (RL) algorithms\nthat learn behaviours in a small number of trials, a property known as sample\nefficiency. This research thus investigates the use of learned world models to\nimprove sample efficiency. We present a novel multi-agent model-based RL\nalgorithm: Multi-Agent Model-Based Policy Optimization (MAMBPO), utilizing the\nCentralized Learning for Decentralized Execution (CLDE) framework. CLDE\nalgorithms allow a group of agents to act in a fully decentralized manner after\ntraining. This is a desirable property for many systems comprising of multiple\nrobots. MAMBPO uses a learned world model to improve sample efficiency compared\nto model-free Multi-Agent Soft Actor-Critic (MASAC). We demonstrate this on two\nsimulated multi-robot tasks, where MAMBPO achieves a similar performance to\nMASAC, but requires far fewer samples to do so. Through this, we take an\nimportant step towards making real-life learning for multi-robot systems\npossible.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 13:37:23 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Willemsen", "Dani\u00ebl", ""], ["Coppola", "Mario", ""], ["de Croon", "Guido C. H. E.", ""]]}, {"id": "2103.03666", "submitter": "Benedikt Kleppmann", "authors": "Benedikt T. Kleppmann", "title": "Tree of Knowledge: an Online Platform for Learning the Behaviour of\n  Complex Systems", "comments": "10 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many social sciences such as psychology and economics try to learn the\nbehaviour of complex agents such as humans, organisations and countries. The\ncurrent statistical methods used for learning this behaviour try to infer\ngenerally valid behaviour, but can only learn from one type of study at a time.\nFurthermore, only data from carefully designed studies can be used, as the\nphenomenon of interest has to be isolated and confounding factors accounted\nfor. These restrictions limit the robustness and accuracy of insights that can\nbe gained from social/economic systems. Here we present the online platform\nTreeOfKnowledge which implements a new methodology specifically designed for\nlearning complex behaviours from complex systems: agent-based behaviour\nlearning. With agent-based behaviour learning it is possible to gain more\naccurate and robust insights as it does not have the restriction of\nconventional statistics. It learns agent behaviour from many heterogenous\ndatasets and can learn from these datasets even if the phenomenon of interest\nis not directly observed, but appears deep within complex systems. This new\nmethodology shows how the internet and advances in computational power allow\nfor more accurate and powerful mathematical models.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 19:39:14 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Kleppmann", "Benedikt T.", ""]]}, {"id": "2103.03697", "submitter": "Ali Ghadirzadeh", "authors": "Ali Ghadirzadeh, Xi Chen, Petra Poklukar, Chelsea Finn, M{\\aa}rten\n  Bj\\\"orkman and Danica Kragic", "title": "Bayesian Meta-Learning for Few-Shot Policy Adaptation Across Robotic\n  Platforms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning methods can achieve significant performance but\nrequire a large amount of training data collected on the same robotic platform.\nA policy trained with expensive data is rendered useless after making even a\nminor change to the robot hardware. In this paper, we address the challenging\nproblem of adapting a policy, trained to perform a task, to a novel robotic\nhardware platform given only few demonstrations of robot motion trajectories on\nthe target robot. We formulate it as a few-shot meta-learning problem where the\ngoal is to find a meta-model that captures the common structure shared across\ndifferent robotic platforms such that data-efficient adaptation can be\nperformed. We achieve such adaptation by introducing a learning framework\nconsisting of a probabilistic gradient-based meta-learning algorithm that\nmodels the uncertainty arising from the few-shot setting with a low-dimensional\nlatent variable. We experimentally evaluate our framework on a simulated\nreaching and a real-robot picking task using 400 simulated robots generated by\nvarying the physical parameters of an existing set of robotic platforms. Our\nresults show that the proposed method can successfully adapt a trained policy\nto different robotic platforms with novel physical parameters and the\nsuperiority of our meta-learning algorithm compared to state-of-the-art methods\nfor the introduced few-shot policy adaptation problem.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 14:16:20 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Ghadirzadeh", "Ali", ""], ["Chen", "Xi", ""], ["Poklukar", "Petra", ""], ["Finn", "Chelsea", ""], ["Bj\u00f6rkman", "M\u00e5rten", ""], ["Kragic", "Danica", ""]]}, {"id": "2103.03699", "submitter": "Yongge Wang", "authors": "Yongge Wang", "title": "Implementing Automated Market Makers with Constant Circle", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describe the implementation details of constant ellipse based\nautomated market makers (CoinSwap). A CoinSwap prototype has been implemented\nat http://coinswapapp.io/ and the source codes are available at\nhttps://github.com/coinswapapp/\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 14:19:33 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Wang", "Yongge", ""]]}, {"id": "2103.03701", "submitter": "Omid Aramoon", "authors": "Omid Aramoon, Pin-Yu Chen, Gang Qu", "title": "Don't Forget to Sign the Gradients!", "comments": "Accepted to MLSys 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Engineering a top-notch deep learning model is an expensive procedure that\ninvolves collecting data, hiring human resources with expertise in machine\nlearning, and providing high computational resources. For that reason, deep\nlearning models are considered as valuable Intellectual Properties (IPs) of the\nmodel vendors. To ensure reliable commercialization of deep learning models, it\nis crucial to develop techniques to protect model vendors against IP\ninfringements. One of such techniques that recently has shown great promise is\ndigital watermarking. However, current watermarking approaches can embed very\nlimited amount of information and are vulnerable against watermark removal\nattacks. In this paper, we present GradSigns, a novel watermarking framework\nfor deep neural networks (DNNs). GradSigns embeds the owner's signature into\nthe gradient of the cross-entropy cost function with respect to inputs to the\nmodel. Our approach has a negligible impact on the performance of the protected\nmodel and it allows model vendors to remotely verify the watermark through\nprediction APIs. We evaluate GradSigns on DNNs trained for different image\nclassification tasks using CIFAR-10, SVHN, and YTF datasets. Experimental\nresults show that GradSigns is robust against all known counter-watermark\nattacks and can embed a large amount of information into DNNs.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 14:24:32 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Aramoon", "Omid", ""], ["Chen", "Pin-Yu", ""], ["Qu", "Gang", ""]]}, {"id": "2103.03717", "submitter": "Flavio de Barros Vidal", "authors": "Andre da Silva Abade, Lucas Faria Porto, Paulo Afonso Ferreira, Flavio\n  de Barros Vidal", "title": "NemaNet: A convolutional neural network model for identification of\n  nematodes soybean crop in brazil", "comments": "21 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Phytoparasitic nematodes (or phytonematodes) are causing severe damage to\ncrops and generating large-scale economic losses worldwide. In soybean crops,\nannual losses are estimated at 10.6% of world production. Besides, identifying\nthese species through microscopic analysis by an expert with taxonomy knowledge\nis often laborious, time-consuming, and susceptible to failure. In this\nperspective, robust and automatic approaches are necessary for identifying\nphytonematodes capable of providing correct diagnoses for the classification of\nspecies and subsidizing the taking of all control and prevention measures. This\nwork presents a new public data set called NemaDataset containing 3,063\nmicroscopic images from five nematode species with the most significant damage\nrelevance for the soybean crop. Additionally, we propose a new Convolutional\nNeural Network (CNN) model defined as NemaNet and a comparative assessment with\nthirteen popular models of CNNs, all of them representing the state of the art\nclassification and recognition. The general average calculated for each model,\non a from-scratch training, the NemaNet model reached 96.99% accuracy, while\nthe best evaluation fold reached 98.03%. In training with transfer learning,\nthe average accuracy reached 98.88\\%. The best evaluation fold reached 99.34%\nand achieve an overall accuracy improvement over 6.83% and 4.1%, for\nfrom-scratch and transfer learning training, respectively, when compared to\nother popular models.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 14:47:00 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Abade", "Andre da Silva", ""], ["Porto", "Lucas Faria", ""], ["Ferreira", "Paulo Afonso", ""], ["Vidal", "Flavio de Barros", ""]]}, {"id": "2103.03730", "submitter": "Masayu Leylia Khodra", "authors": "Adylan Roaffa Ilmy and Masayu Leylia Khodra", "title": "Parsing Indonesian Sentence into Abstract Meaning Representation using\n  Machine Learning Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Abstract Meaning Representation (AMR) provides many information of a sentence\nsuch as semantic relations, coreferences, and named entity relation in one\nrepresentation. However, research on AMR parsing for Indonesian sentence is\nfairly limited. In this paper, we develop a system that aims to parse an\nIndonesian sentence using a machine learning approach. Based on Zhang et al.\nwork, our system consists of three steps: pair prediction, label prediction,\nand graph construction. Pair prediction uses dependency parsing component to\nget the edges between the words for the AMR. The result of pair prediction is\npassed to the label prediction process which used a supervised learning\nalgorithm to predict the label between the edges of the AMR. We used simple\nsentence dataset that is gathered from articles and news article sentences. Our\nmodel achieved the SMATCH score of 0.820 for simple sentence test data.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 15:01:59 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Ilmy", "Adylan Roaffa", ""], ["Khodra", "Masayu Leylia", ""]]}, {"id": "2103.03764", "submitter": "Arniel Labrada", "authors": "Arniel Labrada, Benjamin Bustos, Ivan Sipiran", "title": "A Convolutional Architecture for 3D Model Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  During the last years, many advances have been made in tasks like3D model\nretrieval, 3D model classification, and 3D model segmentation.The typical 3D\nrepresentations such as point clouds, voxels, and poly-gon meshes are mostly\nsuitable for rendering purposes, while their use forcognitive processes\n(retrieval, classification, segmentation) is limited dueto their high\nredundancy and complexity. We propose a deep learningarchitecture to handle 3D\nmodels as an input. We combine this architec-ture with other standard\narchitectures like Convolutional Neural Networksand autoencoders for computing\n3D model embeddings. Our goal is torepresent a 3D model as a vector with enough\ninformation to substitutethe 3D model for high-level tasks. Since this vector\nis a learned repre-sentation which tries to capture the relevant information of\na 3D model,we show that the embedding representation conveys semantic\ninformationthat helps to deal with the similarity assessment of 3D objects. Our\nex-periments show the benefit of computing the embeddings of a 3D modeldata set\nand use them for effective 3D Model Retrieval.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 15:46:47 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Labrada", "Arniel", ""], ["Bustos", "Benjamin", ""], ["Sipiran", "Ivan", ""]]}, {"id": "2103.03786", "submitter": "Shuai Wang", "authors": "Zijian Zhang, Shuai Wang, Yuncong Hong, Liangkai Zhou, and Qi Hao", "title": "Distributed Dynamic Map Fusion via Federated Learning for Intelligent\n  Networked Vehicles", "comments": "12 pages, 5 figures, to appear in 2021 IEEE International Conference\n  on Robotics and Automation (ICRA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The technology of dynamic map fusion among networked vehicles has been\ndeveloped to enlarge sensing ranges and improve sensing accuracies for\nindividual vehicles. This paper proposes a federated learning (FL) based\ndynamic map fusion framework to achieve high map quality despite unknown\nnumbers of objects in fields of view (FoVs), various sensing and model\nuncertainties, and missing data labels for online learning. The novelty of this\nwork is threefold: (1) developing a three-stage fusion scheme to predict the\nnumber of objects effectively and to fuse multiple local maps with fidelity\nscores; (2) developing an FL algorithm which fine-tunes feature models (i.e.,\nrepresentation learning networks for feature extraction) distributively by\naggregating model parameters; (3) developing a knowledge distillation method to\ngenerate FL training labels when data labels are unavailable. The proposed\nframework is implemented in the Car Learning to Act (CARLA) simulation\nplatform. Extensive experimental results are provided to verify the superior\nperformance and robustness of the developed map fusion and FL schemes.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 16:28:46 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Zhang", "Zijian", ""], ["Wang", "Shuai", ""], ["Hong", "Yuncong", ""], ["Zhou", "Liangkai", ""], ["Hao", "Qi", ""]]}, {"id": "2103.03793", "submitter": "Jonas Kiemel", "authors": "Jonas C. Kiemel and Torsten Kr\\\"oger", "title": "Learning Collision-free and Torque-limited Robot Trajectories based on\n  Alternative Safe Behaviors", "comments": "8 pages; 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an approach to learn online generation of collision-free\nand torque-limited trajectories for industrial robots. A neural network, which\nis trained via reinforcement learning, is periodically invoked to predict\nfuture motions. For each robot joint, the network outputs the kinematic state\nthat is desired at the end of the current time interval. Compliance with\nkinematic joint limits is ensured by the design of the action space. Given the\ncurrent kinematic state and the network prediction, a trajectory for the\ncurrent time interval can be computed. The main idea of our paper is to execute\nthe predicted motion only if a collision-free and torque-limited way to\ncontinue the trajectory is known. In practice, the predicted motion is expanded\nby a braking trajectory and simulated using a physics engine. If the simulated\ntrajectory complies with all safety constraints, the predicted motion is\ncarried out. Otherwise, the braking trajectory calculated in the previous\ndecision step serves as an alternative safe behavior. For evaluation, up to\nthree simulated robots are trained to reach as many randomly placed target\npoints as possible. We show that our method reliably prevents collisions with\nstatic obstacles and collisions between the robots, while generating motions\nthat respect both torque limits and kinematic joint limits. Experiments with a\nreal robot demonstrate that safe trajectories can be generated in real-time.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 16:50:57 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Kiemel", "Jonas C.", ""], ["Kr\u00f6ger", "Torsten", ""]]}, {"id": "2103.03796", "submitter": "Ruidong Yan", "authors": "Ruidong Yan, Rui Jiang, Bin Jia, Diange Yang, and Jin Huang", "title": "Hybrid Car-Following Strategy based on Deep Deterministic Policy\n  Gradient and Cooperative Adaptive Cruise Control", "comments": "9 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep deterministic policy gradient (DDPG) based car-following strategy can\nbreak through the constraints of the differential equation model due to the\nability of exploration on complex environments. However, the car-following\nperformance of DDPG is usually degraded by unreasonable reward function design,\ninsufficient training and low sampling efficiency. In order to solve this kind\nof problem, a hybrid car-following strategy based on DDPG and cooperative\nadaptive cruise control (CACC) is proposed. Firstly, the car-following process\nis modeled as markov decision process to calculate CACC and DDPG simultaneously\nat each frame. Given a current state, two actions are obtained from CACC and\nDDPG, respectively. Then an optimal action, corresponding to the one offering a\nlarger reward, is chosen as the output of the hybrid strategy. Meanwhile, a\nrule is designed to ensure that the change rate of acceleration is smaller than\nthe desired value. Therefore, the proposed strategy not only guarantees the\nbasic performance of car-following through CACC, but also makes full use of the\nadvantages of exploration on complex environments via DDPG. Finally, simulation\nresults show that the car-following performance of proposed strategy is\nimproved significantly as compared with that of DDPG and CACC in the whole\nstate space.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 17:37:47 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Yan", "Ruidong", ""], ["Jiang", "Rui", ""], ["Jia", "Bin", ""], ["Yang", "Diange", ""], ["Huang", "Jin", ""]]}, {"id": "2103.03798", "submitter": "Vlad Firoiu", "authors": "Vlad Firoiu, Eser Aygun, Ankit Anand, Zafarali Ahmed, Xavier Glorot,\n  Laurent Orseau, Lei Zhang, Doina Precup, Shibl Mourad", "title": "Training a First-Order Theorem Prover from Synthetic Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge in applying machine learning to automated theorem proving\nis the scarcity of training data, which is a key ingredient in training\nsuccessful deep learning models. To tackle this problem, we propose an approach\nthat relies on training purely with synthetically generated theorems, without\nany human data aside from axioms. We use these theorems to train a\nneurally-guided saturation-based prover. Our neural prover outperforms the\nstate-of-the-art E-prover on this synthetic data in both time and search steps,\nand shows significant transfer to the unseen human-written theorems from the\nTPTP library, where it solves 72\\% of first-order problems without equality.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 17:01:34 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 18:41:02 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Firoiu", "Vlad", ""], ["Aygun", "Eser", ""], ["Anand", "Ankit", ""], ["Ahmed", "Zafarali", ""], ["Glorot", "Xavier", ""], ["Orseau", "Laurent", ""], ["Zhang", "Lei", ""], ["Precup", "Doina", ""], ["Mourad", "Shibl", ""]]}, {"id": "2103.03806", "submitter": "Moulay Akhloufi", "authors": "Abir Rahali and Moulay A. Akhloufi", "title": "MalBERT: Using Transformers for Cybersecurity and Malicious Software\n  Detection", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years we have witnessed an increase in cyber threats and malicious\nsoftware attacks on different platforms with important consequences to persons\nand businesses. It has become critical to find automated machine learning\ntechniques to proactively defend against malware. Transformers, a category of\nattention-based deep learning techniques, have recently shown impressive\nresults in solving different tasks mainly related to the field of Natural\nLanguage Processing (NLP). In this paper, we propose the use of a Transformers'\narchitecture to automatically detect malicious software. We propose a model\nbased on BERT (Bidirectional Encoder Representations from Transformers) which\nperforms a static analysis on the source code of Android applications using\npreprocessed features to characterize existing malware and classify it into\ndifferent representative malware categories. The obtained results are promising\nand show the high performance obtained by Transformer-based models for\nmalicious software detection.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 17:09:46 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Rahali", "Abir", ""], ["Akhloufi", "Moulay A.", ""]]}, {"id": "2103.03809", "submitter": "Xuezixiang Li", "authors": "Xuezixiang Li, Qu Yu, Heng Yin", "title": "PalmTree: Learning an Assembly Language Model for Instruction Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has demonstrated its strengths in numerous binary analysis\ntasks, including function boundary detection, binary code search, function\nprototype inference, value set analysis, etc. When applying deep learning to\nbinary analysis tasks, we need to decide what input should be fed into the\nneural network model. More specifically, we need to answer how to represent an\ninstruction in a fixed-length vector. The idea of automatically learning\ninstruction representations is intriguing, however the existing schemes fail to\ncapture the unique characteristics of disassembly. These schemes ignore the\ncomplex intra-instruction structures and mainly rely on control flow in which\nthe contextual information is noisy and can be influenced by compiler\noptimizations.\n  In this paper, we propose to pre-train an assembly language model called\nPalmTree for generating general-purpose instruction embeddings by conducting\nself-supervised training on large-scale unlabeled binary corpora. PalmTree\nutilizes three pre-training tasks to capture various characteristics of\nassembly language. These training tasks overcome the problems in existing\nschemes, thus can help to generate high-quality representations. We conduct\nboth intrinsic and extrinsic evaluations, and compare PalmTree with other\ninstruction embedding schemes. PalmTree has the best performance for intrinsic\nmetrics, and outperforms the other instruction embedding schemes for all\ndownstream tasks.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jan 2021 22:30:01 GMT"}, {"version": "v2", "created": "Fri, 7 May 2021 19:48:48 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Li", "Xuezixiang", ""], ["Yu", "Qu", ""], ["Yin", "Heng", ""]]}, {"id": "2103.03862", "submitter": "Jacob Whitehill", "authors": "Anand Ramakrishnan, Minh Pham, and Jacob Whitehill", "title": "Harnessing Geometric Constraints from Emotion Labels to improve Face\n  Verification", "comments": "8 pages, 3 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CG cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  For the task of face verification, we explore the utility of harnessing\nauxiliary facial emotion labels to impose explicit geometric constraints on the\nembedding space when training deep embedding models. We introduce several novel\nloss functions that, in conjunction with a standard Triplet Loss [43], or\nArcFace loss [10], provide geometric constraints on the embedding space; the\nlabels for our loss functions can be provided using either manually annotated\nor automatically detected auxiliary emotion labels. Our method is implemented\npurely in terms of the loss function and does not require any changes to the\nneural network backbone of the embedding function.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 18:27:38 GMT"}, {"version": "v2", "created": "Mon, 3 May 2021 14:17:43 GMT"}, {"version": "v3", "created": "Thu, 22 Jul 2021 15:45:31 GMT"}], "update_date": "2021-07-23", "authors_parsed": [["Ramakrishnan", "Anand", ""], ["Pham", "Minh", ""], ["Whitehill", "Jacob", ""]]}, {"id": "2103.03872", "submitter": "Ethan Perez", "authors": "Ethan Perez, Douwe Kiela, Kyunghyun Cho", "title": "Rissanen Data Analysis: Examining Dataset Characteristics via\n  Description Length", "comments": "Code at https://github.com/ethanjperez/rda along with a script to run\n  RDA on your own dataset", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a method to determine if a certain capability helps to achieve\nan accurate model of given data. We view labels as being generated from the\ninputs by a program composed of subroutines with different capabilities, and we\nposit that a subroutine is useful if and only if the minimal program that\ninvokes it is shorter than the one that does not. Since minimum program length\nis uncomputable, we instead estimate the labels' minimum description length\n(MDL) as a proxy, giving us a theoretically-grounded method for analyzing\ndataset characteristics. We call the method Rissanen Data Analysis (RDA) after\nthe father of MDL, and we showcase its applicability on a wide variety of\nsettings in NLP, ranging from evaluating the utility of generating subquestions\nbefore answering a question, to analyzing the value of rationales and\nexplanations, to investigating the importance of different parts of speech, and\nuncovering dataset gender bias.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 18:58:32 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Perez", "Ethan", ""], ["Kiela", "Douwe", ""], ["Cho", "Kyunghyun", ""]]}, {"id": "2103.03873", "submitter": "Tobias Czempiel", "authors": "Tobias Czempiel, Magdalini Paschali, Daniel Ostler, Seong Tae Kim,\n  Benjamin Busam, Nassir Navab", "title": "OperA: Attention-Regularized Transformers for Surgical Phase Recognition", "comments": "10 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce OperA, a transformer-based model that accurately\npredicts surgical phases from long video sequences. A novel attention\nregularization loss encourages the model to focus on high-quality frames during\ntraining. Moreover, the attention weights are utilized to identify\ncharacteristic high attention frames for each surgical phase, which could\nfurther be used for surgery summarization. OperA is thoroughly evaluated on two\ndatasets of laparoscopic cholecystectomy videos, outperforming various\nstate-of-the-art temporal refinement approaches.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 18:59:14 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Czempiel", "Tobias", ""], ["Paschali", "Magdalini", ""], ["Ostler", "Daniel", ""], ["Kim", "Seong Tae", ""], ["Busam", "Benjamin", ""], ["Navab", "Nassir", ""]]}, {"id": "2103.03874", "submitter": "Dan Hendrycks", "authors": "Dan Hendrycks and Collin Burns and Saurav Kadavath and Akul Arora and\n  Steven Basart and Eric Tang and Dawn Song and Jacob Steinhardt", "title": "Measuring Mathematical Problem Solving With the MATH Dataset", "comments": "Code and the MATH dataset is available at\n  https://github.com/hendrycks/math/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many intellectual endeavors require mathematical problem solving, but this\nskill remains beyond the capabilities of computers. To measure this ability in\nmachine learning models, we introduce MATH, a new dataset of 12,500 challenging\ncompetition mathematics problems. Each problem in MATH has a full step-by-step\nsolution which can be used to teach models to generate answer derivations and\nexplanations. To facilitate future research and increase accuracy on MATH, we\nalso contribute a large auxiliary pretraining dataset which helps teach models\nthe fundamentals of mathematics. Even though we are able to increase accuracy\non MATH, our results show that accuracy remains relatively low, even with\nenormous Transformer models. Moreover, we find that simply increasing budgets\nand model parameter counts will be impractical for achieving strong\nmathematical reasoning if scaling trends continue. While scaling Transformers\nis automatically solving most other text-based tasks, scaling is not currently\nsolving MATH. To have more traction on mathematical problem solving we will\nlikely need new algorithmic advancements from the broader research community.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 18:59:39 GMT"}], "update_date": "2021-03-08", "authors_parsed": [["Hendrycks", "Dan", ""], ["Burns", "Collin", ""], ["Kadavath", "Saurav", ""], ["Arora", "Akul", ""], ["Basart", "Steven", ""], ["Tang", "Eric", ""], ["Song", "Dawn", ""], ["Steinhardt", "Jacob", ""]]}, {"id": "2103.03905", "submitter": "Jason Ramapuram", "authors": "Jason Ramapuram, Yan Wu, Alexandros Kalousis", "title": "Kanerva++: extending The Kanerva Machine with differentiable, locally\n  block allocated latent memory", "comments": null, "journal-ref": "ICLR 2021", "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Episodic and semantic memory are critical components of the human memory\nmodel. The theory of complementary learning systems (McClelland et al., 1995)\nsuggests that the compressed representation produced by a serial event\n(episodic memory) is later restructured to build a more generalized form of\nreusable knowledge (semantic memory). In this work we develop a new principled\nBayesian memory allocation scheme that bridges the gap between episodic and\nsemantic memory via a hierarchical latent variable model. We take inspiration\nfrom traditional heap allocation and extend the idea of locally contiguous\nmemory to the Kanerva Machine, enabling a novel differentiable block allocated\nlatent memory. In contrast to the Kanerva Machine, we simplify the process of\nmemory writing by treating it as a fully feed forward deterministic process,\nrelying on the stochasticity of the read key distribution to disperse\ninformation within the memory. We demonstrate that this allocation scheme\nimproves performance in memory conditional image generation, resulting in new\nstate-of-the-art conditional likelihood values on binarized MNIST (<=41.58\nnats/image) , binarized Omniglot (<=66.24 nats/image), as well as presenting\ncompetitive performance on CIFAR10, DMLab Mazes, Celeb-A and ImageNet32x32.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 18:40:40 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 09:38:06 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Ramapuram", "Jason", ""], ["Wu", "Yan", ""], ["Kalousis", "Alexandros", ""]]}, {"id": "2103.03912", "submitter": "Albert Dulian", "authors": "Albert Dulian, John C. Murray", "title": "Multi-modal anticipation of stochastic trajectories in a dynamic\n  environment with Conditional Variational Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Forecasting short-term motion of nearby vehicles presents an inherently\nchallenging issue as the space of their possible future movements is not\nstrictly limited to a set of single trajectories. Recently proposed techniques\nthat demonstrate plausible results concentrate primarily on forecasting a fixed\nnumber of deterministic predictions, or on classifying over a wide variety of\ntrajectories that were previously generated using e.g. dynamic model. This\npaper focuses on addressing the uncertainty associated with the discussed task\nby utilising the stochastic nature of generative models in order to produce a\ndiverse set of plausible paths with regards to tracked vehicles. More\nspecifically, we propose to account for the multi-modality of the problem with\nuse of Conditional Variational Autoencoder (C-VAE) conditioned on an agent's\npast motion as well as a rasterised scene context encoded with Capsule Network\n(CapsNet). In addition, we demonstrate advantages of employing the Minimum over\nN (MoN) cost function which measures the distance between ground truth and N\ngenerated samples and tries to minimise the loss with respect to the closest\nsample, effectively leading to more diverse predictions. We examine our network\non a publicly available dataset against recent state-of-the-art methods and\nshow that our approach outperforms these techniques in numerous scenarios\nwhilst significantly reducing the number of trainable parameters as well as\nallowing to sample an arbitrary amount of diverse trajectories.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 19:38:26 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Dulian", "Albert", ""], ["Murray", "John C.", ""]]}, {"id": "2103.03918", "submitter": "Runhua Xu", "authors": "Runhua Xu, Nathalie Baracaldo, Yi Zhou, Ali Anwar, James Joshi, Heiko\n  Ludwig", "title": "FedV: Privacy-Preserving Federated Learning over Vertically Partitioned\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.DC", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Federated learning (FL) has been proposed to allow collaborative training of\nmachine learning (ML) models among multiple parties where each party can keep\nits data private. In this paradigm, only model updates, such as model weights\nor gradients, are shared. Many existing approaches have focused on horizontal\nFL, where each party has the entire feature set and labels in the training data\nset. However, many real scenarios follow a vertically-partitioned FL setup,\nwhere a complete feature set is formed only when all the datasets from the\nparties are combined, and the labels are only available to a single party.\nPrivacy-preserving vertical FL is challenging because complete sets of labels\nand features are not owned by one entity. Existing approaches for vertical FL\nrequire multiple peer-to-peer communications among parties, leading to lengthy\ntraining times, and are restricted to (approximated) linear models and just two\nparties. To close this gap, we propose FedV, a framework for secure gradient\ncomputation in vertical settings for several widely used ML models such as\nlinear models, logistic regression, and support vector machines. FedV removes\nthe need for peer-to-peer communication among parties by using functional\nencryption schemes; this allows FedV to achieve faster training times. It also\nworks for larger and changing sets of parties. We empirically demonstrate the\napplicability for multiple types of ML models and show a reduction of 10%-70%\nof training time and 80% to 90% in data transfer with respect to the\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 19:59:29 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 20:31:33 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Xu", "Runhua", ""], ["Baracaldo", "Nathalie", ""], ["Zhou", "Yi", ""], ["Anwar", "Ali", ""], ["Joshi", "James", ""], ["Ludwig", "Heiko", ""]]}, {"id": "2103.03938", "submitter": "Pedro Alejandro Ortega", "authors": "Gr\\'egoire D\\'eletang, Jordi Grau-Moya, Miljan Martic, Tim Genewein,\n  Tom McGrath, Vladimir Mikulik, Markus Kunesch, Shane Legg, Pedro A. Ortega", "title": "Causal Analysis of Agent Behavior for AI Safety", "comments": "16 pages, 16 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As machine learning systems become more powerful they also become\nincreasingly unpredictable and opaque. Yet, finding human-understandable\nexplanations of how they work is essential for their safe deployment. This\ntechnical report illustrates a methodology for investigating the causal\nmechanisms that drive the behaviour of artificial agents. Six use cases are\ncovered, each addressing a typical question an analyst might ask about an\nagent. In particular, we show that each question cannot be addressed by pure\nobservation alone, but instead requires conducting experiments with\nsystematically chosen manipulations so as to generate the correct causal\nevidence.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 20:51:12 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["D\u00e9letang", "Gr\u00e9goire", ""], ["Grau-Moya", "Jordi", ""], ["Martic", "Miljan", ""], ["Genewein", "Tim", ""], ["McGrath", "Tom", ""], ["Mikulik", "Vladimir", ""], ["Kunesch", "Markus", ""], ["Legg", "Shane", ""], ["Ortega", "Pedro A.", ""]]}, {"id": "2103.03953", "submitter": "Supreeth Mysore Shivanandamurthy", "authors": "Supreeth Mysore Shivanandamurthy, Ishan. G. Thakkar, Sayed Ahmad\n  Salehi", "title": "ODIN: A Bit-Parallel Stochastic Arithmetic Based Accelerator for In-Situ\n  Neural Network Processing in Phase Change RAM", "comments": "6 pages, 6 Figures, 4 Tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.AI cs.LG", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Due to the very rapidly growing use of Artificial Neural Networks (ANNs) in\nreal-world applications related to machine learning and Artificial Intelligence\n(AI), several hardware accelerator de-signs for ANNs have been proposed\nrecently. In this paper, we present a novel processing-in-memory (PIM) engine\ncalled ODIN that employs hybrid binary-stochastic bit-parallel arithmetic\nin-side phase change RAM (PCRAM) to enable a low-overhead in-situ acceleration\nof all essential ANN functions such as multiply-accumulate (MAC), nonlinear\nactivation, and pooling. We mapped four ANN benchmark applications on ODIN to\ncompare its performance with a conventional processor-centric design and a\ncrossbar-based in-situ ANN accelerator from prior work. The results of our\nanalysis for the considered ANN topologies indicate that our ODIN accelerator\ncan be at least 5.8x faster and 23.2x more energy-efficient, and up to 90.8x\nfaster and 1554x more energy-efficient, compared to the crossbar-based in-situ\nANN accelerator from prior work.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 21:47:48 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Shivanandamurthy", "Supreeth Mysore", ""], ["Thakkar", "Ishan. G.", ""], ["Salehi", "Sayed Ahmad", ""]]}, {"id": "2103.03959", "submitter": "Krzysztof Sornat", "authors": "Krzysztof Sornat, Virginia Vassilevska Williams, Yinzhan Xu", "title": "Fine-Grained Complexity and Algorithms for the Schulze Voting Method", "comments": "19 pages, 2 algorithms, 2 tables. A previous version of this work\n  appears in EC 2021. In this version we strengthen Theorem 6.2 which now holds\n  also for the problem of finding a Schulze winner", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study computational aspects of a well-known single-winner voting rule\ncalled the Schulze method [Schulze, 2003] which is used broadly in practice. In\nthis method the voters give (weak) ordinal preference ballots which are used to\ndefine the weighted majority graph (WMG) of direct comparisons between pairs of\ncandidates. The choice of the winner comes from indirect comparisons in the\ngraph, and more specifically from considering directed paths instead of direct\ncomparisons between candidates.\n  When the input is the WMG, to our knowledge, the fastest algorithm for\ncomputing all winners in the Schulze method uses a folklore reduction to the\nAll-Pairs Bottleneck Paths problem and runs in $O(m^{2.69})$ time, where $m$ is\nthe number of candidates. It is an interesting open question whether this can\nbe improved. Our first result is a combinatorial algorithm with a nearly\nquadratic running time for computing all winners. This running time is\nessentially optimal. If the input to the Schulze winners problem is not the WMG\nbut the preference profile, then constructing the WMG is a bottleneck that\nincreases the running time significantly; in the special case when there are\n$m$ candidates and $n=O(m)$ voters, the running time is $O(m^{2.69})$, or\n$O(m^{2.5})$ if there is a nearly-linear time algorithm for multiplying dense\nsquare matrices. To address this bottleneck, we prove a formal equivalence\nbetween the well-studied Dominance Product problem and the problem of computing\nthe WMG. We prove a similar connection between the so called Dominating Pairs\nproblem and the problem of finding a winner in the Schulze method.\n  Our paper is the first to bring fine-grained complexity into the field of\ncomputational social choice. Using it we can identify voting protocols that are\nunlikely to be practical for large numbers of candidates and/or voters, as\ntheir complexity is likely, say at least cubic.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 22:27:36 GMT"}, {"version": "v2", "created": "Thu, 24 Jun 2021 17:06:08 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Sornat", "Krzysztof", ""], ["Williams", "Virginia Vassilevska", ""], ["Xu", "Yinzhan", ""]]}, {"id": "2103.03987", "submitter": "Tyler Hayes", "authors": "Tyler L. Hayes and Christopher Kanan", "title": "Selective Replay Enhances Learning in Online Continual Analogical\n  Reasoning", "comments": "To appear in the IEEE Conference on Computer Vision and Pattern\n  Recognition Workshop (CVPR-W) on Continual Learning in Computer Vision\n  (CLVision) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In continual learning, a system learns from non-stationary data streams or\nbatches without catastrophic forgetting. While this problem has been heavily\nstudied in supervised image classification and reinforcement learning,\ncontinual learning in neural networks designed for abstract reasoning has not\nyet been studied. Here, we study continual learning of analogical reasoning.\nAnalogical reasoning tests such as Raven's Progressive Matrices (RPMs) are\ncommonly used to measure non-verbal abstract reasoning in humans, and recently\noffline neural networks for the RPM problem have been proposed. In this paper,\nwe establish experimental baselines, protocols, and forward and backward\ntransfer metrics to evaluate continual learners on RPMs. We employ experience\nreplay to mitigate catastrophic forgetting. Prior work using replay for image\nclassification tasks has found that selectively choosing the samples to replay\noffers little, if any, benefit over random selection. In contrast, we find that\nselective replay can significantly outperform random selection for the RPM\ntask.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 00:04:10 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 15:38:49 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Hayes", "Tyler L.", ""], ["Kanan", "Christopher", ""]]}, {"id": "2103.03991", "submitter": "Brendan Tidd", "authors": "Brendan Tidd, Akansel Cosgun, Jurgen Leitner, and Nicolas Hudson", "title": "Passing Through Narrow Gaps with Deep Reinforcement Learning", "comments": "Submitted to 2021 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The DARPA subterranean challenge requires teams of robots to traverse\ndifficult and diverse underground environments. Traversing small gaps is one of\nthe challenging scenarios that robots encounter. Imperfect sensor information\nmakes it difficult for classical navigation methods, where behaviours require\nsignificant manual fine tuning. In this paper we present a deep reinforcement\nlearning method for autonomously navigating through small gaps, where contact\nbetween the robot and the gap may be required. We first learn a gap behaviour\npolicy to get through small gaps (only centimeters wider than the robot). We\nthen learn a goal-conditioned behaviour selection policy that determines when\nto activate the gap behaviour policy. We train our policies in simulation and\ndemonstrate their effectiveness with a large tracked robot in simulation and on\nthe real platform. In simulation experiments, our approach achieves 93% success\nrate when the gap behaviour is activated manually by an operator, and 67% with\nautonomous activation using the behaviour selection policy. In real robot\nexperiments, our approach achieves a success rate of 73% with manual\nactivation, and 40% with autonomous behaviour selection. While we show the\nfeasibility of our approach in simulation, the difference in performance\nbetween simulated and real world scenarios highlight the difficulty of direct\nsim-to-real transfer for deep reinforcement learning policies. In both the\nsimulated and real world environments alternative methods were unable to\ntraverse the gap.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 00:10:41 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Tidd", "Brendan", ""], ["Cosgun", "Akansel", ""], ["Leitner", "Jurgen", ""], ["Hudson", "Nicolas", ""]]}, {"id": "2103.04000", "submitter": "Hengyuan Hu", "authors": "Hengyuan Hu, Adam Lerer, Brandon Cui, David Wu, Luis Pineda, Noam\n  Brown, Jakob Foerster", "title": "Off-Belief Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The standard problem setting in Dec-POMDPs is self-play, where the goal is to\nfind a set of policies that play optimally together. Policies learned through\nself-play may adopt arbitrary conventions and implicitly rely on multi-step\nreasoning based on fragile assumptions about other agents' actions and thus\nfail when paired with humans or independently trained agents at test time. To\naddress this, we present off-belief learning (OBL). At each timestep OBL agents\nfollow a policy $\\pi_1$ that is optimized assuming past actions were taken by a\ngiven, fixed policy ($\\pi_0$), but assuming that future actions will be taken\nby $\\pi_1$. When $\\pi_0$ is uniform random, OBL converges to an optimal policy\nthat does not rely on inferences based on other agents' behavior (an optimal\ngrounded policy). OBL can be iterated in a hierarchy, where the optimal policy\nfrom one level becomes the input to the next, thereby introducing multi-level\ncognitive reasoning in a controlled manner. Unlike existing approaches, which\nmay converge to any equilibrium policy, OBL converges to a unique policy,\nmaking it suitable for zero-shot coordination (ZSC). OBL can be scaled to\nhigh-dimensional settings with a fictitious transition mechanism and shows\nstrong performance in both a toy-setting and the benchmark human-AI & ZSC\nproblem Hanabi.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 01:09:55 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 03:26:08 GMT"}, {"version": "v3", "created": "Tue, 29 Jun 2021 18:05:19 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Hu", "Hengyuan", ""], ["Lerer", "Adam", ""], ["Cui", "Brandon", ""], ["Wu", "David", ""], ["Pineda", "Luis", ""], ["Brown", "Noam", ""], ["Foerster", "Jakob", ""]]}, {"id": "2103.04019", "submitter": "Jianing Qiu", "authors": "Jianing Qiu, Frank P.-W. Lo, Xiao Gu, Yingnan Sun, Shuo Jiang, and\n  Benny Lo", "title": "Indoor Future Person Localization from an Egocentric Wearable Camera", "comments": "accepted as conference paper in 2021 IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Accurate prediction of future person location and movement trajectory from an\negocentric wearable camera can benefit a wide range of applications, such as\nassisting visually impaired people in navigation, and the development of\nmobility assistance for people with disability. In this work, a new egocentric\ndataset was constructed using a wearable camera, with 8,250 short clips of a\ntargeted person either walking 1) toward, 2) away, or 3) across the camera\nwearer in indoor environments, or 4) staying still in the scene, and 13,817\nperson bounding boxes were manually labelled. Apart from the bounding boxes,\nthe dataset also contains the estimated pose of the targeted person as well as\nthe IMU signal of the wearable camera at each time point. An LSTM-based\nencoder-decoder framework was designed to predict the future location and\nmovement trajectory of the targeted person in this egocentric setting.\nExtensive experiments have been conducted on the new dataset, and have shown\nthat the proposed method is able to reliably and better predict future person\nlocation and trajectory in egocentric videos captured by the wearable camera\ncompared to three baselines.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 03:32:42 GMT"}, {"version": "v2", "created": "Wed, 28 Jul 2021 09:47:11 GMT"}], "update_date": "2021-07-29", "authors_parsed": [["Qiu", "Jianing", ""], ["Lo", "Frank P. -W.", ""], ["Gu", "Xiao", ""], ["Sun", "Yingnan", ""], ["Jiang", "Shuo", ""], ["Lo", "Benny", ""]]}, {"id": "2103.04038", "submitter": "Yiming Li", "authors": "Yiming Li, Yanjie Li, Yalei Lv, Yong Jiang, Shu-Tao Xia", "title": "Hidden Backdoor Attack against Semantic Segmentation Models", "comments": "This is a 6-pages short version of our ongoing work. It is accepted\n  by the non-archival ICLR workshop on Security and Safety in Machine Learning\n  Systems, 2021. The first two authors contributed equally to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks (DNNs) are vulnerable to the \\emph{backdoor attack},\nwhich intends to embed hidden backdoors in DNNs by poisoning training data. The\nattacked model behaves normally on benign samples, whereas its prediction will\nbe changed to a particular target label if hidden backdoors are activated. So\nfar, backdoor research has mostly been conducted towards classification tasks.\nIn this paper, we reveal that this threat could also happen in semantic\nsegmentation, which may further endanger many mission-critical applications\n($e.g.$, autonomous driving). Except for extending the existing attack paradigm\nto maliciously manipulate the segmentation models from the image-level, we\npropose a novel attack paradigm, the \\emph{fine-grained attack}, where we treat\nthe target label ($i.e.$, annotation) from the object-level instead of the\nimage-level to achieve more sophisticated manipulation. In the annotation of\npoisoned samples generated by the fine-grained attack, only pixels of specific\nobjects will be labeled with the attacker-specified target class while others\nare still with their ground-truth ones. Experiments show that the proposed\nmethods can successfully attack semantic segmentation models by poisoning only\na small proportion of training data. Our method not only provides a new\nperspective for designing novel attacks but also serves as a strong baseline\nfor improving the robustness of semantic segmentation methods.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 05:50:29 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 10:47:02 GMT"}, {"version": "v3", "created": "Sat, 3 Apr 2021 05:07:33 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Li", "Yiming", ""], ["Li", "Yanjie", ""], ["Lv", "Yalei", ""], ["Jiang", "Yong", ""], ["Xia", "Shu-Tao", ""]]}, {"id": "2103.04044", "submitter": "Zijie Wang", "authors": "Zijie J. Wang, Dongjin Choi, Shenyu Xu, Diyi Yang", "title": "Putting Humans in the Natural Language Processing Loop: A Survey", "comments": "The paper is accepted to the HCI+NLP workshop at EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How can we design Natural Language Processing (NLP) systems that learn from\nhuman feedback? There is a growing research body of Human-in-the-loop (HITL)\nNLP frameworks that continuously integrate human feedback to improve the model\nitself. HITL NLP research is nascent but multifarious -- solving various NLP\nproblems, collecting diverse feedback from different people, and applying\ndifferent methods to learn from collected feedback. We present a survey of HITL\nNLP work from both Machine Learning (ML) and Human-Computer Interaction (HCI)\ncommunities that highlights its short yet inspiring history, and thoroughly\nsummarize recent frameworks focusing on their tasks, goals, human interactions,\nand feedback learning methods. Finally, we discuss future directions for\nintegrating human feedback in the NLP development loop.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 06:26:00 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Wang", "Zijie J.", ""], ["Choi", "Dongjin", ""], ["Xu", "Shenyu", ""], ["Yang", "Diyi", ""]]}, {"id": "2103.04047", "submitter": "Xiuyuan Lu", "authors": "Xiuyuan Lu, Benjamin Van Roy, Vikranth Dwaracherla, Morteza Ibrahimi,\n  Ian Osband, Zheng Wen", "title": "Reinforcement Learning, Bit by Bit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning agents have demonstrated remarkable achievements in\nsimulated environments. Data efficiency poses an impediment to carrying this\nsuccess over to real environments. The design of data-efficient agents calls\nfor a deeper understanding of information acquisition and representation. We\ndevelop concepts and establish a regret bound that together offer principled\nguidance. The bound sheds light on questions of what information to seek, how\nto seek that information, and it what information to retain. To illustrate\nconcepts, we design simple agents that build on them and present computational\nresults that demonstrate improvements in data efficiency.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 06:37:46 GMT"}, {"version": "v2", "created": "Sun, 14 Mar 2021 05:58:17 GMT"}, {"version": "v3", "created": "Mon, 12 Apr 2021 18:42:28 GMT"}, {"version": "v4", "created": "Tue, 11 May 2021 01:03:05 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Lu", "Xiuyuan", ""], ["Van Roy", "Benjamin", ""], ["Dwaracherla", "Vikranth", ""], ["Ibrahimi", "Morteza", ""], ["Osband", "Ian", ""], ["Wen", "Zheng", ""]]}, {"id": "2103.04077", "submitter": "Xiaofeng Gao", "authors": "Xiaofeng Gao, Luyao Yuan, Tianmin Shu, Hongjing Lu, Song-Chun Zhu", "title": "Show Me What You Can Do: Capability Calibration on Reachable Workspace\n  for Human-Robot Collaboration", "comments": "8 pages, 6 figures, under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aligning humans' assessment of what a robot can do with its true capability\nis crucial for establishing a common ground between human and robot partners\nwhen they collaborate on a joint task. In this work, we propose an approach to\ncalibrate humans' estimate of a robot's reachable workspace through a small\nnumber of demonstrations before collaboration. We develop a novel motion\nplanning method, REMP (Reachability-Expressive Motion Planning), which jointly\noptimizes the physical cost and the expressiveness of robot motion to reveal\nthe robot's motion capability to a human observer. Our experiments with human\nparticipants demonstrate that a short calibration using REMP can effectively\nbridge the gap between what a non-expert user thinks a robot can reach and the\nground-truth. We show that this calibration procedure not only results in\nbetter user perception, but also promotes more efficient human-robot\ncollaborations in a subsequent joint task.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 09:14:30 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Gao", "Xiaofeng", ""], ["Yuan", "Luyao", ""], ["Shu", "Tianmin", ""], ["Lu", "Hongjing", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "2103.04097", "submitter": "No\\'e Tits", "authors": "No\\'e Tits, Kevin El Haddad and Thierry Dutoit", "title": "Analysis and Assessment of Controllability of an Expressive Deep\n  Learning-based TTS system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.CL cs.HC eess.AS", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper, we study the controllability of an Expressive TTS system\ntrained on a dataset for a continuous control. The dataset is the Blizzard 2013\ndataset based on audiobooks read by a female speaker containing a great\nvariability in styles and expressiveness. Controllability is evaluated with\nboth an objective and a subjective experiment. The objective assessment is\nbased on a measure of correlation between acoustic features and the dimensions\nof the latent space representing expressiveness. The subjective assessment is\nbased on a perceptual experiment in which users are shown an interface for\nControllable Expressive TTS and asked to retrieve a synthetic utterance whose\nexpressiveness subjectively corresponds to that a reference utterance.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 11:06:13 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Tits", "No\u00e9", ""], ["Haddad", "Kevin El", ""], ["Dutoit", "Thierry", ""]]}, {"id": "2103.04132", "submitter": "Zhenwang Qin Mr.", "authors": "Zhenwang Qin, Wensheng Wang, Karl-Heinz Dammer, Leifeng Guo and Zhen\n  Cao", "title": "A Real-time Low-cost Artificial Intelligence System for Autonomous\n  Spraying in Palm Plantations", "comments": "19 pages,18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In precision crop protection, (target-orientated) object detection in image\nprocessing can help navigate Unmanned Aerial Vehicles (UAV, crop protection\ndrones) to the right place to apply the pesticide. Unnecessary application of\nnon-target areas could be avoided. Deep learning algorithms dominantly use in\nmodern computer vision tasks which require high computing time, memory\nfootprint, and power consumption. Based on the Edge Artificial Intelligence, we\ninvestigate the main three paths that lead to dealing with this problem,\nincluding hardware accelerators, efficient algorithms, and model compression.\nFinally, we integrate them and propose a solution based on a light deep neural\nnetwork (DNN), called Ag-YOLO, which can make the crop protection UAV have the\nability to target detection and autonomous operation. This solution is\nrestricted in size, cost, flexible, fast, and energy-effective. The hardware is\nonly 18 grams in weight and 1.5 watts in energy consumption, and the developed\nDNN model needs only 838 kilobytes of disc space. We tested the developed\nhardware and software in comparison to the tiny version of the state-of-art\nYOLOv3 framework, known as YOLOv3-Tiny to detect individual palm in a\nplantation. An average F1 score of 0.9205 at the speed of 36.5 frames per\nsecond (in comparison to similar accuracy at 18 frames per second and 8.66\nmegabytes of the YOLOv3-Tiny algorithm) was reached. This developed detection\nsystem is easily plugged into any machines already purchased as long as the\nmachines have USB ports and run Linux Operating System.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 15:05:14 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Qin", "Zhenwang", ""], ["Wang", "Wensheng", ""], ["Dammer", "Karl-Heinz", ""], ["Guo", "Leifeng", ""], ["Cao", "Zhen", ""]]}, {"id": "2103.04146", "submitter": "Bart Van Oort", "authors": "Bart van Oort, Lu\\'is Cruz, Maur\\'icio Aniche, Arie van Deursen", "title": "The Prevalence of Code Smells in Machine Learning projects", "comments": "Submitted and accepted to 2021 IEEE/ACM 1st Workshop on AI\n  Engineering - Software Engineering for AI (WAIN)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Artificial Intelligence (AI) and Machine Learning (ML) are pervasive in the\ncurrent computer science landscape. Yet, there still exists a lack of software\nengineering experience and best practices in this field. One such best\npractice, static code analysis, can be used to find code smells, i.e.,\n(potential) defects in the source code, refactoring opportunities, and\nviolations of common coding standards. Our research set out to discover the\nmost prevalent code smells in ML projects. We gathered a dataset of 74\nopen-source ML projects, installed their dependencies and ran Pylint on them.\nThis resulted in a top 20 of all detected code smells, per category. Manual\nanalysis of these smells mainly showed that code duplication is widespread and\nthat the PEP8 convention for identifier naming style may not always be\napplicable to ML code due to its resemblance with mathematical notation. More\ninterestingly, however, we found several major obstructions to the\nmaintainability and reproducibility of ML projects, primarily related to the\ndependency management of Python projects. We also found that Pylint cannot\nreliably check for correct usage of imported dependencies, including prominent\nML libraries such as PyTorch.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 16:01:54 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["van Oort", "Bart", ""], ["Cruz", "Lu\u00eds", ""], ["Aniche", "Maur\u00edcio", ""], ["van Deursen", "Arie", ""]]}, {"id": "2103.04167", "submitter": "Hongwei Li", "authors": "Hongwei Li, Fei-Fei Xue, Krishna Chaitanya, Shengda Luo, Ivan Ezhov,\n  Benedikt Wiestler, Jianguo Zhang, Bjoern Menze", "title": "Imbalance-Aware Self-Supervised Learning for 3D Radiomic Representations", "comments": "camera-ready version in MICCAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Radiomic representations can quantify properties of regions of interest in\nmedical image data. Classically, they account for pre-defined statistics of\nshape, texture, and other low-level image features. Alternatively, deep\nlearning-based representations are derived from supervised learning but require\nexpensive annotations from experts and often suffer from overfitting and data\nimbalance issues. In this work, we address the challenge of learning\nrepresentations of 3D medical images for an effective quantification under data\nimbalance. We propose a \\emph{self-supervised} representation learning\nframework to learn high-level features of 3D volumes as a complement to\nexisting radiomics features. Specifically, we demonstrate how to learn image\nrepresentations in a self-supervised fashion using a 3D Siamese network. More\nimportantly, we deal with data imbalance by exploiting two unsupervised\nstrategies: a) sample re-weighting, and b) balancing the composition of\ntraining batches. When combining our learned self-supervised feature with\ntraditional radiomics, we show significant improvement in brain tumor\nclassification and lung cancer staging tasks covering MRI and CT imaging\nmodalities.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 18:17:03 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 11:21:19 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Li", "Hongwei", ""], ["Xue", "Fei-Fei", ""], ["Chaitanya", "Krishna", ""], ["Luo", "Shengda", ""], ["Ezhov", "Ivan", ""], ["Wiestler", "Benedikt", ""], ["Zhang", "Jianguo", ""], ["Menze", "Bjoern", ""]]}, {"id": "2103.04174", "submitter": "Bohan Wu", "authors": "Bohan Wu, Suraj Nair, Roberto Martin-Martin, Li Fei-Fei, Chelsea Finn", "title": "Greedy Hierarchical Variational Autoencoders for Large-Scale Video\n  Prediction", "comments": "Equal advising and contribution for last two authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A video prediction model that generalizes to diverse scenes would enable\nintelligent agents such as robots to perform a variety of tasks via planning\nwith the model. However, while existing video prediction models have produced\npromising results on small datasets, they suffer from severe underfitting when\ntrained on large and diverse datasets. To address this underfitting challenge,\nwe first observe that the ability to train larger video prediction models is\noften bottlenecked by the memory constraints of GPUs or TPUs. In parallel, deep\nhierarchical latent variable models can produce higher quality predictions by\ncapturing the multi-level stochasticity of future observations, but end-to-end\noptimization of such models is notably difficult. Our key insight is that\ngreedy and modular optimization of hierarchical autoencoders can simultaneously\naddress both the memory constraints and the optimization challenges of\nlarge-scale video prediction. We introduce Greedy Hierarchical Variational\nAutoencoders (GHVAEs), a method that learns high-fidelity video predictions by\ngreedily training each level of a hierarchical autoencoder. In comparison to\nstate-of-the-art models, GHVAEs provide 17-55% gains in prediction performance\non four video datasets, a 35-40% higher success rate on real robot tasks, and\ncan improve performance monotonically by simply adding more modules.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 18:58:56 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 18:37:14 GMT"}, {"version": "v3", "created": "Sat, 19 Jun 2021 07:25:28 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Wu", "Bohan", ""], ["Nair", "Suraj", ""], ["Martin-Martin", "Roberto", ""], ["Fei-Fei", "Li", ""], ["Finn", "Chelsea", ""]]}, {"id": "2103.04176", "submitter": "Razvan Bunescu", "authors": "Mike Chen and Razvan Bunescu", "title": "Changing the Narrative Perspective: From Deictic to Anaphoric Point of\n  View", "comments": "To appear in Information Processing & Management, Special Issue on\n  Creative Language Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We introduce the task of changing the narrative point of view, where\ncharacters are assigned a narrative perspective that is different from the one\noriginally used by the writer. The resulting shift in the narrative point of\nview alters the reading experience and can be used as a tool in fiction writing\nor to generate types of text ranging from educational to self-help and\nself-diagnosis. We introduce a benchmark dataset containing a wide range of\ntypes of narratives annotated with changes in point of view from deictic (first\nor second person) to anaphoric (third person) and describe a pipeline for\nprocessing raw text that relies on a neural architecture for mention selection.\nEvaluations on the new benchmark dataset show that the proposed architecture\nsubstantially outperforms the baselines by generating mentions that are less\nambiguous and more natural.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 19:03:42 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Chen", "Mike", ""], ["Bunescu", "Razvan", ""]]}, {"id": "2103.04178", "submitter": "Lahiru D. Chamain Hewa Gamage", "authors": "Lahiru D. Chamain, Fabien Racap\\'e, Jean B\\'egaint, Akshay Pushparaja\n  and Simon Feltman", "title": "End-to-end optimized image compression for multiple machine tasks", "comments": "supplement is added to the same document", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An increasing share of captured images and videos are transmitted for storage\nand remote analysis by computer vision algorithms, rather than to be viewed by\nhumans. Contrary to traditional standard codecs with engineered tools, neural\nnetwork based codecs can be trained end-to-end to optimally compress images\nwith respect to a target rate and any given differentiable performance metric.\nAlthough it is possible to train such compression tools to achieve better\nrate-accuracy performance for a particular computer vision task, it could be\npractical and relevant to re-use the compressed bit-stream for multiple machine\ntasks. For this purpose, we introduce 'Connectors' that are inserted between\nthe decoder and the task algorithms to enable a direct transformation of the\ncompressed content, which was previously optimized for a specific task, to\nmultiple other machine tasks. We demonstrate the effectiveness of the proposed\nmethod by achieving significant rate-accuracy performance improvement for both\nimage classification and object segmentation, using the same bit-stream,\noriginally optimized for object detection.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 19:09:05 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Chamain", "Lahiru D.", ""], ["Racap\u00e9", "Fabien", ""], ["B\u00e9gaint", "Jean", ""], ["Pushparaja", "Akshay", ""], ["Feltman", "Simon", ""]]}, {"id": "2103.04200", "submitter": "Ruwan Tennakoon", "authors": "Ruwan Tennakoon, David Suter, Erchuan Zhang, Tat-Jun Chin, Alireza\n  Bab-Hadiashar", "title": "Consensus Maximisation Using Influences of Monotone Boolean Functions", "comments": "To appear in CVPR 2021 as an ORAL paper. arXiv admin note: text\n  overlap with arXiv:2005.05490", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Consensus maximisation (MaxCon), which is widely used for robust fitting in\ncomputer vision, aims to find the largest subset of data that fits the model\nwithin some tolerance level. In this paper, we outline the connection between\nMaxCon problem and the abstract problem of finding the maximum upper zero of a\nMonotone Boolean Function (MBF) defined over the Boolean Cube. Then, we link\nthe concept of influences (in a MBF) to the concept of outlier (in MaxCon) and\nshow that influences of points belonging to the largest structure in data would\ngenerally be smaller under certain conditions. Based on this observation, we\npresent an iterative algorithm to perform consensus maximisation. Results for\nboth synthetic and real visual data experiments show that the MBF based\nalgorithm is capable of generating a near optimal solution relatively quickly.\nThis is particularly important where there are large number of outliers (gross\nor pseudo) in the observed data.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 22:01:06 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Tennakoon", "Ruwan", ""], ["Suter", "David", ""], ["Zhang", "Erchuan", ""], ["Chin", "Tat-Jun", ""], ["Bab-Hadiashar", "Alireza", ""]]}, {"id": "2103.04225", "submitter": "Ife Adebara", "authors": "Ife Adebara, Muhammad Abdul-Mageed, Miikka Silfverberg", "title": "Translating the Unseen? Yoruba-English MT in Low-Resource,\n  Morphologically-Unmarked Settings", "comments": "Accepted at AfricanNLP @ EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Translating between languages where certain features are marked\nmorphologically in one but absent or marked contextually in the other is an\nimportant test case for machine translation. When translating into English\nwhich marks (in)definiteness morphologically, from Yor\\`ub\\'a which uses bare\nnouns but marks these features contextually, ambiguities arise. In this work,\nwe perform fine-grained analysis on how an SMT system compares with two NMT\nsystems (BiLSTM and Transformer) when translating bare nouns in Yor\\`ub\\'a into\nEnglish. We investigate how the systems what extent they identify BNs,\ncorrectly translate them, and compare with human translation patterns. We also\nanalyze the type of errors each model makes and provide a linguistic\ndescription of these errors. We glean insights for evaluating model performance\nin low-resource settings. In translating bare nouns, our results show the\ntransformer model outperforms the SMT and BiLSTM models for 4 categories, the\nBiLSTM outperforms the SMT model for 3 categories while the SMT outperforms the\nNMT models for 1 category.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 01:24:09 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 04:46:10 GMT"}, {"version": "v3", "created": "Tue, 6 Apr 2021 17:55:46 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Adebara", "Ife", ""], ["Abdul-Mageed", "Muhammad", ""], ["Silfverberg", "Miikka", ""]]}, {"id": "2103.04243", "submitter": "Xiaoxiao Li", "authors": "Xiaoxiao Li, Ziteng Cui, Yifan Wu, Lin Gu, Tatsuya Harada", "title": "Estimating and Improving Fairness with Adversarial Learning", "comments": "12 pages, 2 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Fairness and accountability are two essential pillars for trustworthy\nArtificial Intelligence (AI) in healthcare. However, the existing AI model may\nbe biased in its decision marking. To tackle this issue, we propose an\nadversarial multi-task training strategy to simultaneously mitigate and detect\nbias in the deep learning-based medical image analysis system. Specifically, we\npropose to add a discrimination module against bias and a critical module that\npredicts unfairness within the base classification model. We further impose an\northogonality regularization to force the two modules to be independent during\ntraining. Hence, we can keep these deep learning tasks distinct from one\nanother, and avoid collapsing them into a singular point on the manifold.\nThrough this adversarial training method, the data from the underprivileged\ngroup, which is vulnerable to bias because of attributes such as sex and skin\ntone, are transferred into a domain that is neutral relative to these\nattributes. Furthermore, the critical module can predict fairness scores for\nthe data with unknown sensitive attributes. We evaluate our framework on a\nlarge-scale public-available skin lesion dataset under various fairness\nevaluation metrics. The experiments demonstrate the effectiveness of our\nproposed method for estimating and improving fairness in the deep\nlearning-based medical image analysis system.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 03:10:32 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 14:16:04 GMT"}], "update_date": "2021-05-12", "authors_parsed": [["Li", "Xiaoxiao", ""], ["Cui", "Ziteng", ""], ["Wu", "Yifan", ""], ["Gu", "Lin", ""], ["Harada", "Tatsuya", ""]]}, {"id": "2103.04244", "submitter": "Catarina Moreira", "authors": "Yu-Liang Chou and Catarina Moreira and Peter Bruza and Chun Ouyang and\n  Joaquim Jorge", "title": "Counterfactuals and Causability in Explainable Artificial Intelligence:\n  Theory, Algorithms, and Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There has been a growing interest in model-agnostic methods that can make\ndeep learning models more transparent and explainable to a user. Some\nresearchers recently argued that for a machine to achieve a certain degree of\nhuman-level explainability, this machine needs to provide human causally\nunderstandable explanations, also known as causability. A specific class of\nalgorithms that have the potential to provide causability are counterfactuals.\nThis paper presents an in-depth systematic review of the diverse existing body\nof literature on counterfactuals and causability for explainable artificial\nintelligence. We performed an LDA topic modelling analysis under a PRISMA\nframework to find the most relevant literature articles. This analysis resulted\nin a novel taxonomy that considers the grounding theories of the surveyed\nalgorithms, together with their underlying properties and applications in\nreal-world data. This research suggests that current model-agnostic\ncounterfactual algorithms for explainable AI are not grounded on a causal\ntheoretical formalism and, consequently, cannot promote causability to a human\ndecision-maker. Our findings suggest that the explanations derived from major\nalgorithms in the literature provide spurious correlations rather than\ncause/effects relationships, leading to sub-optimal, erroneous or even biased\nexplanations. This paper also advances the literature with new directions and\nchallenges on promoting causability in model-agnostic approaches for\nexplainable artificial intelligence.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 03:11:39 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 06:50:02 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Chou", "Yu-Liang", ""], ["Moreira", "Catarina", ""], ["Bruza", "Peter", ""], ["Ouyang", "Chun", ""], ["Jorge", "Joaquim", ""]]}, {"id": "2103.04246", "submitter": "Alvin Chan", "authors": "Alvin Chan, Anna Korsakova, Yew-Soon Ong, Fernaldo Richtia Winnerdy,\n  Kah Wai Lim, Anh Tuan Phan", "title": "RNA Alternative Splicing Prediction with Discrete Compositional Energy\n  Network", "comments": "ACM CHIL 2021 Camera-Ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A single gene can encode for different protein versions through a process\ncalled alternative splicing. Since proteins play major roles in cellular\nfunctions, aberrant splicing profiles can result in a variety of diseases,\nincluding cancers. Alternative splicing is determined by the gene's primary\nsequence and other regulatory factors such as RNA-binding protein levels. With\nthese as input, we formulate the prediction of RNA splicing as a regression\ntask and build a new training dataset (CAPD) to benchmark learned models. We\npropose discrete compositional energy network (DCEN) which leverages the\nhierarchical relationships between splice sites, junctions and transcripts to\napproach this task. In the case of alternative splicing prediction, DCEN models\nmRNA transcript probabilities through its constituent splice junctions' energy\nvalues. These transcript probabilities are subsequently mapped to relative\nabundance values of key nucleotides and trained with ground-truth experimental\nmeasurements. Through our experiments on CAPD, we show that DCEN outperforms\nbaselines and ablation variants.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 03:15:10 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Chan", "Alvin", ""], ["Korsakova", "Anna", ""], ["Ong", "Yew-Soon", ""], ["Winnerdy", "Fernaldo Richtia", ""], ["Lim", "Kah Wai", ""], ["Phan", "Anh Tuan", ""]]}, {"id": "2103.04260", "submitter": "Dongxu Li", "authors": "Dongxu Li, Chenchen Xu, Kaihao Zhang, Xin Yu, Yiran Zhong, Wenqi Ren,\n  Hanna Suominen, Hongdong Li", "title": "ARVo: Learning All-Range Volumetric Correspondence for Video Deblurring", "comments": "Preprint for CVPR 2021 Poster", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Video deblurring models exploit consecutive frames to remove blurs from\ncamera shakes and object motions. In order to utilize neighboring sharp\npatches, typical methods rely mainly on homography or optical flows to\nspatially align neighboring blurry frames. However, such explicit approaches\nare less effective in the presence of fast motions with large pixel\ndisplacements. In this work, we propose a novel implicit method to learn\nspatial correspondence among blurry frames in the feature space. To construct\ndistant pixel correspondences, our model builds a correlation volume pyramid\namong all the pixel-pairs between neighboring frames. To enhance the features\nof the reference frame, we design a correlative aggregation module that\nmaximizes the pixel-pair correlations with its neighbors based on the volume\npyramid. Finally, we feed the aggregated features into a reconstruction module\nto obtain the restored frame. We design a generative adversarial paradigm to\noptimize the model progressively. Our proposed method is evaluated on the\nwidely-adopted DVD dataset, along with a newly collected High-Frame-Rate (1000\nfps) Dataset for Video Deblurring (HFR-DVD). Quantitative and qualitative\nexperiments show that our model performs favorably on both datasets against\nprevious state-of-the-art methods, confirming the benefit of modeling all-range\nspatial correspondence for video deblurring.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 04:33:13 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Li", "Dongxu", ""], ["Xu", "Chenchen", ""], ["Zhang", "Kaihao", ""], ["Yu", "Xin", ""], ["Zhong", "Yiran", ""], ["Ren", "Wenqi", ""], ["Suominen", "Hanna", ""], ["Li", "Hongdong", ""]]}, {"id": "2103.04289", "submitter": "Ran Tian", "authors": "Ran Tian, Masayoshi Tomizuka, and Liting Sun", "title": "Learning Human Rewards by Inferring Their Latent Intelligence Levels in\n  Multi-Agent Games: A Theory-of-Mind Approach with Application to Driving Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reward function, as an incentive representation that recognizes humans'\nagency and rationalizes humans' actions, is particularly appealing for modeling\nhuman behavior in human-robot interaction. Inverse Reinforcement Learning is an\neffective way to retrieve reward functions from demonstrations. However, it has\nalways been challenging when applying it to multi-agent settings since the\nmutual influence between agents has to be appropriately modeled. To tackle this\nchallenge, previous work either exploits equilibrium solution concepts by\nassuming humans as perfectly rational optimizers with unbounded intelligence or\npre-assigns humans' interaction strategies a priori. In this work, we advocate\nthat humans are bounded rational and have different intelligence levels when\nreasoning about others' decision-making process, and such an inherent and\nlatent characteristic should be accounted for in reward learning algorithms.\nHence, we exploit such insights from Theory-of-Mind and propose a new\nmulti-agent Inverse Reinforcement Learning framework that reasons about humans'\nlatent intelligence levels during learning. We validate our approach in both\nzero-sum and general-sum games with synthetic agents and illustrate a practical\napplication to learning human drivers' reward functions from real driving data.\nWe compare our approach with two baseline algorithms. The results show that by\nreasoning about humans' latent intelligence levels, the proposed approach has\nmore flexibility and capability to retrieve reward functions that explain\nhumans' driving behaviors better.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 07:48:31 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Tian", "Ran", ""], ["Tomizuka", "Masayoshi", ""], ["Sun", "Liting", ""]]}, {"id": "2103.04303", "submitter": "Nguyen Van Huynh", "authors": "Nguyen Van Huynh, Dinh Thai Hoang, Diep N. Nguyen, and Eryk Dutkiewicz", "title": "Joint Coding and Scheduling Optimization for Distributed Learning over\n  Wireless Edge Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike theoretical distributed learning (DL), DL over wireless edge networks\nfaces the inherent dynamics/uncertainty of wireless connections and edge nodes,\nmaking DL less efficient or even inapplicable under the highly dynamic wireless\nedge networks (e.g., using mmW interfaces). This article addresses these\nproblems by leveraging recent advances in coded computing and the deep dueling\nneural network architecture. By introducing coded structures/redundancy, a\ndistributed learning task can be completed without waiting for straggling\nnodes. Unlike conventional coded computing that only optimizes the code\nstructure, coded distributed learning over the wireless edge also requires to\noptimize the selection/scheduling of wireless edge nodes with heterogeneous\nconnections, computing capability, and straggling effects. However, even\nneglecting the aforementioned dynamics/uncertainty, the resulting joint\noptimization of coding and scheduling to minimize the distributed learning time\nturns out to be NP-hard. To tackle this and to account for the dynamics and\nuncertainty of wireless connections and edge nodes, we reformulate the problem\nas a Markov Decision Process and then design a novel deep reinforcement\nlearning algorithm that employs the deep dueling neural network architecture to\nfind the jointly optimal coding scheme and the best set of edge nodes for\ndifferent learning tasks without explicit information about the wireless\nenvironment and edge nodes' straggling parameters. Simulations show that the\nproposed framework reduces the average learning delay in wireless edge\ncomputing up to 66% compared with other DL approaches. The jointly optimal\nframework in this article is also applicable to any distributed learning scheme\nwith heterogeneous and uncertain computing nodes.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 08:57:09 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 04:20:00 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Van Huynh", "Nguyen", ""], ["Hoang", "Dinh Thai", ""], ["Nguyen", "Diep N.", ""], ["Dutkiewicz", "Eryk", ""]]}, {"id": "2103.04314", "submitter": "Jeremy Straub", "authors": "Jeremy Straub", "title": "Expert System Gradient Descent Style Training: Development of a\n  Defensible Artificial Intelligence Technique", "comments": null, "journal-ref": "Knowledge Based-Systems (2021)", "doi": "10.1016/j.knosys.2021.107275", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Artificial intelligence systems, which are designed with a capability to\nlearn from the data presented to them, are used throughout society. These\nsystems are used to screen loan applicants, make sentencing recommendations for\ncriminal defendants, scan social media posts for disallowed content and more.\nBecause these systems don't assign meaning to their complex learned correlation\nnetwork, they can learn associations that don't equate to causality, resulting\nin non-optimal and indefensible decisions being made. In addition to making\ndecisions that are sub-optimal, these systems may create legal liability for\ntheir designers and operators by learning correlations that violate\nanti-discrimination and other laws regarding what factors can be used in\ndifferent types of decision making. This paper presents the use of a machine\nlearning expert system, which is developed with meaning-assigned nodes (facts)\nand correlations (rules). Multiple potential implementations are considered and\nevaluated under different conditions, including different network error and\naugmentation levels and different training levels. The performance of these\nsystems is compared to random and fully connected networks.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 10:09:50 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Straub", "Jeremy", ""]]}, {"id": "2103.04318", "submitter": "Patrick Reiser", "authors": "Patrick Reiser, Andre Eberhard and Pascal Friederich", "title": "Implementing graph neural networks with TensorFlow-Keras", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Graph neural networks are a versatile machine learning architecture that\nreceived a lot of attention recently. In this technical report, we present an\nimplementation of convolution and pooling layers for TensorFlow-Keras models,\nwhich allows a seamless and flexible integration into standard Keras layers to\nset up graph models in a functional way. This implies the usage of mini-batches\nas the first tensor dimension, which can be realized via the new RaggedTensor\nclass of TensorFlow best suited for graphs. We developed the Keras Graph\nConvolutional Neural Network Python package kgcnn based on TensorFlow-Keras\nthat provides a set of Keras layers for graph networks which focus on a\ntransparent tensor structure passed between layers and an ease-of-use mindset.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 10:46:02 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Reiser", "Patrick", ""], ["Eberhard", "Andre", ""], ["Friederich", "Pascal", ""]]}, {"id": "2103.04337", "submitter": "Pingping Zhang Dr", "authors": "Xuehu Liu and Pingping Zhang and Chenyang Yu and Huchuan Lu and\n  Xiaoyun Yang", "title": "Watching You: Global-guided Reciprocal Learning for Video-based Person\n  Re-identification", "comments": "This is the camera-ready version of our Poster paper in CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video-based person re-identification (Re-ID) aims to automatically retrieve\nvideo sequences of the same person under non-overlapping cameras. To achieve\nthis goal, it is the key to fully utilize abundant spatial and temporal cues in\nvideos. Existing methods usually focus on the most conspicuous image regions,\nthus they may easily miss out fine-grained clues due to the person varieties in\nimage sequences. To address above issues, in this paper, we propose a novel\nGlobal-guided Reciprocal Learning (GRL) framework for video-based person Re-ID.\nSpecifically, we first propose a Global-guided Correlation Estimation (GCE) to\ngenerate feature correlation maps of local features and global features, which\nhelp to localize the high- and low-correlation regions for identifying the same\nperson. After that, the discriminative features are disentangled into\nhigh-correlation features and low-correlation features under the guidance of\nthe global representations. Moreover, a novel Temporal Reciprocal Learning\n(TRL) mechanism is designed to sequentially enhance the high-correlation\nsemantic information and accumulate the low-correlation sub-critical clues.\nExtensive experiments are conducted on three public benchmarks. The\nexperimental results indicate that our approach can achieve better performance\nthan other state-of-the-art approaches. The code is released at\nhttps://github.com/flysnowtiger/GRL.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 12:27:42 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 05:17:58 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Liu", "Xuehu", ""], ["Zhang", "Pingping", ""], ["Yu", "Chenyang", ""], ["Lu", "Huchuan", ""], ["Yang", "Xiaoyun", ""]]}, {"id": "2103.04351", "submitter": "David Hoeller", "authors": "David Hoeller, Lorenz Wellhausen, Farbod Farshidian, Marco Hutter", "title": "Learning a State Representation and Navigation in Cluttered and Dynamic\n  Environments", "comments": "8 pages, 8 figures, 2 tables", "journal-ref": "IEEE Robotics and Automation Letters 2021", "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present a learning-based pipeline to realise local\nnavigation with a quadrupedal robot in cluttered environments with static and\ndynamic obstacles. Given high-level navigation commands, the robot is able to\nsafely locomote to a target location based on frames from a depth camera\nwithout any explicit mapping of the environment. First, the sequence of images\nand the current trajectory of the camera are fused to form a model of the world\nusing state representation learning. The output of this lightweight module is\nthen directly fed into a target-reaching and obstacle-avoiding policy trained\nwith reinforcement learning. We show that decoupling the pipeline into these\ncomponents results in a sample efficient policy learning stage that can be\nfully trained in simulation in just a dozen minutes. The key part is the state\nrepresentation, which is trained to not only estimate the hidden state of the\nworld in an unsupervised fashion, but also helps bridging the reality gap,\nenabling successful sim-to-real transfer. In our experiments with the\nquadrupedal robot ANYmal in simulation and in reality, we show that our system\ncan handle noisy depth images, avoid dynamic obstacles unseen during training,\nand is endowed with local spatial awareness.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 13:19:06 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Hoeller", "David", ""], ["Wellhausen", "Lorenz", ""], ["Farshidian", "Farbod", ""], ["Hutter", "Marco", ""]]}, {"id": "2103.04364", "submitter": "Zaid Tahir", "authors": "Zaid Tahir, Rob Alexander", "title": "Coverage based testing for V&V and Safety Assurance of Self-driving\n  Autonomous Vehicles: A Systematic Literature Review", "comments": null, "journal-ref": "IEEE International Conference On Artificial Intelligence Testing\n  (AITest), Oxford, UK, 2020", "doi": "10.1109/AITEST49225.2020.00011", "report-no": null, "categories": "cs.AI cs.RO cs.SE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Self-driving Autonomous Vehicles (SAVs) are gaining more interest each\npassing day by the industry as well as the general public. Tech and automobile\ncompanies are investing huge amounts of capital in research and development of\nSAVs to make sure they have a head start in the SAV market in the future. One\nof the major hurdles in the way of SAVs making it to the public roads is the\nlack of confidence of public in the safety aspect of SAVs. In order to assure\nsafety and provide confidence to the public in the safety of SAVs, researchers\naround the world have used coverage-based testing for Verification and\nValidation (V&V) and safety assurance of SAVs. The objective of this paper is\nto investigate the coverage criteria proposed and coverage maximizing\ntechniques used by researchers in the last decade up till now, to assure safety\nof SAVs. We conduct a Systematic Literature Review (SLR) for this investigation\nin our paper. We present a classification of existing research based on the\ncoverage criteria used. Several research gaps and research directions are also\nprovided in this SLR to enable further research in this domain. This paper\nprovides a body of knowledge in the domain of safety assurance of SAVs. We\nbelieve the results of this SLR will be helpful in the progression of V&V and\nsafety assurance of SAVs.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 14:23:04 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Tahir", "Zaid", ""], ["Alexander", "Rob", ""]]}, {"id": "2103.04412", "submitter": "Pablo Lanillos", "authors": "Cristian Meo and Pablo Lanillos", "title": "Multimodal VAE Active Inference Controller", "comments": "Submitted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active inference, a theoretical construct inspired by brain processing, is a\npromising alternative to control artificial agents. However, current methods do\nnot yet scale to high-dimensional inputs in continuous control. Here we present\na novel active inference torque controller for industrial arms that maintains\nthe adaptive characteristics of previous proprioceptive approaches but also\nenables large-scale multimodal integration (e.g., raw images). We extended our\nprevious mathematical formulation by including multimodal state representation\nlearning using a linearly coupled multimodal variational autoencoder. We\nevaluated our model on a simulated 7DOF Franka Emika Panda robot arm and\ncompared its behavior with a previous active inference baseline and the Panda\nbuilt-in optimized controller. Results showed improved tracking and control in\ngoal-directed reaching due to the increased representation power, high\nrobustness to noise and adaptability in changes on the environmental conditions\nand robot parameters without the need to relearn the generative models nor\nparameters retuning.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 18:00:27 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Meo", "Cristian", ""], ["Lanillos", "Pablo", ""]]}, {"id": "2103.04430", "submitter": "Chen Chen", "authors": "Wenxuan Wang, Chen Chen, Meng Ding, Jiangyun Li, Hong Yu, Sen Zha", "title": "TransBTS: Multimodal Brain Tumor Segmentation Using Transformer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer, which can benefit from global (long-range) information modeling\nusing self-attention mechanisms, has been successful in natural language\nprocessing and 2D image classification recently. However, both local and global\nfeatures are crucial for dense prediction tasks, especially for 3D medical\nimage segmentation. In this paper, we for the first time exploit Transformer in\n3D CNN for MRI Brain Tumor Segmentation and propose a novel network named\nTransBTS based on the encoder-decoder structure. To capture the local 3D\ncontext information, the encoder first utilizes 3D CNN to extract the\nvolumetric spatial feature maps. Meanwhile, the feature maps are reformed\nelaborately for tokens that are fed into Transformer for global feature\nmodeling. The decoder leverages the features embedded by Transformer and\nperforms progressive upsampling to predict the detailed segmentation map.\nExtensive experimental results on both BraTS 2019 and 2020 datasets show that\nTransBTS achieves comparable or higher results than previous state-of-the-art\n3D methods for brain tumor segmentation on 3D MRI scans. The source code is\navailable at https://github.com/Wenxuan-1119/TransBTS\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 19:12:14 GMT"}, {"version": "v2", "created": "Sat, 26 Jun 2021 23:58:17 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Wang", "Wenxuan", ""], ["Chen", "Chen", ""], ["Ding", "Meng", ""], ["Li", "Jiangyun", ""], ["Yu", "Hong", ""], ["Zha", "Sen", ""]]}, {"id": "2103.04439", "submitter": "Tianwei Ni", "authors": "Tianwei Ni, Huao Li, Siddharth Agrawal, Suhas Raja, Fan Jia, Yikang\n  Gui, Dana Hughes, Michael Lewis, Katia Sycara", "title": "Adaptive Agent Architecture for Real-time Human-Agent Teaming", "comments": "The first three authors contributed equally. In AAAI 2021 Workshop on\n  Plan, Activity, and Intent Recognition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Teamwork is a set of interrelated reasoning, actions and behaviors of team\nmembers that facilitate common objectives. Teamwork theory and experiments have\nresulted in a set of states and processes for team effectiveness in both\nhuman-human and agent-agent teams. However, human-agent teaming is less well\nstudied because it is so new and involves asymmetry in policy and intent not\npresent in human teams. To optimize team performance in human-agent teaming, it\nis critical that agents infer human intent and adapt their polices for smooth\ncoordination. Most literature in human-agent teaming builds agents referencing\na learned human model. Though these agents are guaranteed to perform well with\nthe learned model, they lay heavy assumptions on human policy such as\noptimality and consistency, which is unlikely in many real-world scenarios. In\nthis paper, we propose a novel adaptive agent architecture in human-model-free\nsetting on a two-player cooperative game, namely Team Space Fortress (TSF).\nPrevious human-human team research have shown complementary policies in TSF\ngame and diversity in human players' skill, which encourages us to relax the\nassumptions on human policy. Therefore, we discard learning human models from\nhuman data, and instead use an adaptation strategy on a pre-trained library of\nexemplar policies composed of RL algorithms or rule-based methods with minimal\nassumptions of human behavior. The adaptation strategy relies on a novel\nsimilarity metric to infer human policy and then selects the most complementary\npolicy in our library to maximize the team performance. The adaptive agent\narchitecture can be deployed in real-time and generalize to any off-the-shelf\nstatic agents. We conducted human-agent experiments to evaluate the proposed\nadaptive agent framework, and demonstrated the suboptimality, diversity, and\nadaptability of human policies in human-agent teams.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 20:08:09 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Ni", "Tianwei", ""], ["Li", "Huao", ""], ["Agrawal", "Siddharth", ""], ["Raja", "Suhas", ""], ["Jia", "Fan", ""], ["Gui", "Yikang", ""], ["Hughes", "Dana", ""], ["Lewis", "Michael", ""], ["Sycara", "Katia", ""]]}, {"id": "2103.04485", "submitter": "Ming Zhu", "authors": "Xiao-Yang Liu, Ming Zhu", "title": "Convolutional Graph-Tensor Net for Graph Data Completion", "comments": null, "journal-ref": "IJCAI 2021", "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph data completion is a fundamentally important issue as data generally\nhas a graph structure, e.g., social networks, recommendation systems, and the\nInternet of Things. We consider a graph where each node has a data matrix,\nrepresented as a \\textit{graph-tensor} by stacking the data matrices in the\nthird dimension. In this paper, we propose a \\textit{Convolutional Graph-Tensor\nNet} (\\textit{Conv GT-Net}) for the graph data completion problem, which uses\ndeep neural networks to learn the general transform of graph-tensors. The\nexperimental results on the ego-Facebook data sets show that the proposed\n\\textit{Conv GT-Net} achieves significant improvements on both completion\naccuracy (50\\% higher) and completion speed (3.6x $\\sim$ 8.1x faster) over the\nexisting algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 23:33:38 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Liu", "Xiao-Yang", ""], ["Zhu", "Ming", ""]]}, {"id": "2103.04496", "submitter": "Pavel Surynek", "authors": "Pavel Surynek", "title": "Sparsification for Fast Optimal Multi-Robot Path Planning in Lazy\n  Compilation Schemes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Path planning for multiple robots (MRPP) represents a task of finding\nnon-colliding paths for robots through which they can navigate from their\ninitial positions to specified goal positions. The problem is usually modeled\nusing undirected graphs where robots move between vertices across edges.\nContemporary optimal solving algorithms include dedicated search-based methods,\nthat solve the problem directly, and compilation-based algorithms that reduce\nMRPP to a different formalism for which an efficient solver exists, such as\nconstraint programming (CP), mixed integer programming (MIP), or Boolean\nsatisfiability (SAT). In this paper, we enhance existing SAT-based algorithm\nfor MRPP via spartification of the set of candidate paths for each robot from\nwhich target Boolean encoding is derived. Suggested sparsification of the set\nof paths led to smaller target Boolean formulae that can be constructed and\nsolved faster while optimality guarantees of the approach have been kept.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 00:57:42 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Surynek", "Pavel", ""]]}, {"id": "2103.04498", "submitter": "Katarzyna Pasternak", "authors": "Katarzyna Pasternak, Zishi Wu, Ubbo Visser, and Christine Lisetti", "title": "Let's be friends! A rapport-building 3D embodied conversational agent\n  for the Human Support Robot", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Partial subtle mirroring of nonverbal behaviors during conversations (also\nknown as mimicking or parallel empathy), is essential for rapport building,\nwhich in turn is essential for optimal human-human communication outcomes.\nMirroring has been studied in interactions between robots and humans, and in\ninteractions between Embodied Conversational Agents (ECAs) and humans. However,\nvery few studies examine interactions between humans and ECAs that are\nintegrated with robots, and none of them examine the effect of mirroring\nnonverbal behaviors in such interactions. Our research question is whether\nintegrating an ECA able to mirror its interlocutor's facial expressions and\nhead movements (continuously or intermittently) with a human-service robot will\nimprove the user's experience with the support robot that is able to perform\nuseful mobile manipulative tasks (e.g. at home). Our contribution is the\ncomplex integration of an expressive ECA, able to track its interlocutor's\nface, and to mirror his/her facial expressions and head movements in real time,\nintegrated with a human support robot such that the robot and the agent are\nfully aware of each others', and of the users', nonverbals cues. We also\ndescribe a pilot study we conducted towards answering our research question,\nwhich shows promising results for our forthcoming larger user study.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 01:02:41 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Pasternak", "Katarzyna", ""], ["Wu", "Zishi", ""], ["Visser", "Ubbo", ""], ["Lisetti", "Christine", ""]]}, {"id": "2103.04516", "submitter": "Zhongqiang Ren", "authors": "Zhongqiang Ren, Sivakumar Rathinam and Howie Choset", "title": "Loosely Synchronized Search for Multi-agent Path Finding with\n  Asynchronous Actions", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-agent path finding (MAPF) determines an ensemble of collision-free\npaths for multiple agents between their respective start and goal locations.\nAmong the available MAPF planners for workspaces modeled as a graph, A*-based\napproaches have been widely investigated and have demonstrated their efficiency\nin numerous scenarios. However, almost all of these A*-based approaches assume\nthat each agent executes an action concurrently in that all agents start and\nstop together. This article presents a natural generalization of MAPF with\nasynchronous actions where agents do not necessarily start and stop\nconcurrently. The main contribution of the work is a proposed approach called\nLoosely Synchronized Search (LSS) that extends A*-based MAPF planners to handle\nasynchronous actions. We show LSS is complete and finds an optimal solution if\none exists. We also combine LSS with other existing MAPF methods that aims to\ntrade-off optimality for computational efficiency. Extensive numerical results\nare presented to corroborate the performance of the proposed approaches.\nFinally, we also verify the applicability of our method in the Robotarium, a\nremotely accessible swarm robotics research platform.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 02:34:17 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Ren", "Zhongqiang", ""], ["Rathinam", "Sivakumar", ""], ["Choset", "Howie", ""]]}, {"id": "2103.04541", "submitter": "Tu Gu", "authors": "Tu Gu, Kaiyu Feng, Gao Cong, Cheng Long, Zheng Wang, Sheng Wang", "title": "The RLR-Tree: A Reinforcement Learning Based R-Tree for Spatial Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learned indices have been proposed to replace classic index structures like\nB-Tree with machine learning (ML) models. They require to replace both the\nindices and query processing algorithms currently deployed by the databases,\nand such a radical departure is likely to encounter challenges and obstacles.\nIn contrast, we propose a fundamentally different way of using ML techniques to\nimprove on the query performance of the classic R-Tree without the need of\nchanging its structure or query processing algorithms. Specifically, we develop\nreinforcement learning (RL) based models to decide how to choose a subtree for\ninsertion and how to split a node, instead of relying on hand-crafted heuristic\nrules as R-Tree and its variants. Experiments on real and synthetic datasets\nwith up to 100 million spatial objects clearly show that our RL based index\noutperforms R-Tree and its variants.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 04:29:58 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Gu", "Tu", ""], ["Feng", "Kaiyu", ""], ["Cong", "Gao", ""], ["Long", "Cheng", ""], ["Wang", "Zheng", ""], ["Wang", "Sheng", ""]]}, {"id": "2103.04555", "submitter": "Zhiwei Qin", "authors": "Yan Jiao, Xiaocheng Tang, Zhiwei Qin, Shuaiji Li, Fan Zhang, Hongtu\n  Zhu and Jieping Ye", "title": "Real-world Ride-hailing Vehicle Repositioning using Deep Reinforcement\n  Learning", "comments": null, "journal-ref": "Transportation Research: Part C, 2021", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We present a new practical framework based on deep reinforcement learning and\ndecision-time planning for real-world vehicle repositioning on ride-hailing (a\ntype of mobility-on-demand, MoD) platforms. Our approach learns the\nspatiotemporal state-value function using a batch training algorithm with deep\nvalue networks. The optimal repositioning action is generated on-demand through\nvalue-based policy search, which combines planning and bootstrapping with the\nvalue networks. For the large-fleet problems, we develop several algorithmic\nfeatures that we incorporate into our framework and that we demonstrate to\ninduce coordination among the algorithmically-guided vehicles. We benchmark our\nalgorithm with baselines in a ride-hailing simulation environment to\ndemonstrate its superiority in improving income efficiency meausred by\nincome-per-hour. We have also designed and run a real-world experiment program\nwith regular drivers on a major ride-hailing platform. We have observed\nsignificantly positive results on key metrics comparing our method with\nexperienced drivers who performed idle-time repositioning based on their own\nexpertise.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 05:34:05 GMT"}, {"version": "v2", "created": "Sat, 22 May 2021 00:14:19 GMT"}, {"version": "v3", "created": "Thu, 8 Jul 2021 06:32:31 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Jiao", "Yan", ""], ["Tang", "Xiaocheng", ""], ["Qin", "Zhiwei", ""], ["Li", "Shuaiji", ""], ["Zhang", "Fan", ""], ["Zhu", "Hongtu", ""], ["Ye", "Jieping", ""]]}, {"id": "2103.04556", "submitter": "Jiaye Teng", "authors": "Jiaye Teng, Zeren Tan, Yang Yuan", "title": "T-SCI: A Two-Stage Conformal Inference Algorithm with Guaranteed\n  Coverage for Cox-MLP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is challenging to deal with censored data, where we only have access to\nthe incomplete information of survival time instead of its exact value.\nFortunately, under linear predictor assumption, people can obtain guaranteed\ncoverage for the confidence band of survival time using methods like Cox\nRegression. However, when relaxing the linear assumption with neural networks\n(e.g., Cox-MLP (Katzman et al., 2018; Kvamme et al., 2019)), we lose the\nguaranteed coverage. To recover the guaranteed coverage without linear\nassumption, we propose two algorithms based on conformal inference. In the\nfirst algorithm WCCI, we revisit weighted conformal inference and introduce a\nnew non-conformity score based on partial likelihood. We then propose a\ntwo-stage algorithm T-SCI, where we run WCCI in the first stage and apply\nquantile conformal inference to calibrate the results in the second stage.\nTheoretical analysis shows that T-SCI returns guaranteed coverage under milder\nassumptions than WCCI. We conduct extensive experiments on synthetic data and\nreal data using different methods, which validate our analysis.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 05:42:05 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 07:20:34 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Teng", "Jiaye", ""], ["Tan", "Zeren", ""], ["Yuan", "Yang", ""]]}, {"id": "2103.04564", "submitter": "Chao Yu", "authors": "Zhenggang Tang, Chao Yu, Boyuan Chen, Huazhe Xu, Xiaolong Wang, Fei\n  Fang, Simon Du, Yu Wang, Yi Wu", "title": "Discovering Diverse Multi-Agent Strategic Behavior via Reward\n  Randomization", "comments": "Accepted paper on ICLR 2021. First two authors share equal\n  contribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a simple, general and effective technique, Reward Randomization\nfor discovering diverse strategic policies in complex multi-agent games.\nCombining reward randomization and policy gradient, we derive a new algorithm,\nReward-Randomized Policy Gradient (RPG). RPG is able to discover multiple\ndistinctive human-interpretable strategies in challenging temporal trust\ndilemmas, including grid-world games and a real-world game Agar.io, where\nmultiple equilibria exist but standard multi-agent policy gradient algorithms\nalways converge to a fixed one with a sub-optimal payoff for every player even\nusing state-of-the-art exploration techniques. Furthermore, with the set of\ndiverse strategies from RPG, we can (1) achieve higher payoffs by fine-tuning\nthe best policy from the set; and (2) obtain an adaptive agent by using this\nset of strategies as its training opponents. The source code and example videos\ncan be found in our website: https://sites.google.com/view/staghuntrpg.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 06:26:55 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 02:38:01 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Tang", "Zhenggang", ""], ["Yu", "Chao", ""], ["Chen", "Boyuan", ""], ["Xu", "Huazhe", ""], ["Wang", "Xiaolong", ""], ["Fang", "Fei", ""], ["Du", "Simon", ""], ["Wang", "Yu", ""], ["Wu", "Yi", ""]]}, {"id": "2103.04578", "submitter": "Chih-Hong Cheng", "authors": "Chih-Hong Cheng, Rongjie Yan", "title": "Testing Autonomous Systems with Believed Equivalence Refinement", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous engineering of autonomous driving functions commonly requires\ndeploying vehicles in road testing to obtain inputs that cause problematic\ndecisions. Although the discovery leads to producing an improved system, it\nalso challenges the foundation of testing using equivalence classes and the\nassociated relative test coverage criterion. In this paper, we propose believed\nequivalence, where the establishment of an equivalence class is initially based\non expert belief and is subject to a set of available test cases having a\nconsistent valuation. Upon a newly encountered test case that breaks the\nconsistency, one may need to refine the established categorization in order to\nsplit the originally believed equivalence into two. Finally, we focus on\nmodules implemented using deep neural networks where every category partitions\nan input over the real domain. We establish new equivalence classes by guiding\nthe new test cases following directions suggested by its k-nearest neighbors,\ncomplemented by local robustness testing. The concept is demonstrated in a\nlane-keeping assist module indicating the potential of our proposed approach.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 07:25:20 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Cheng", "Chih-Hong", ""], ["Yan", "Rongjie", ""]]}, {"id": "2103.04590", "submitter": "Pranav Rajpurkar", "authors": "Siyu Shi, Ishaan Malhi, Kevin Tran, Andrew Y. Ng, Pranav Rajpurkar", "title": "CheXseen: Unseen Disease Detection for Deep Learning Interpretation of\n  Chest X-rays", "comments": "Accepted at MIDL Conference 2021. Previous version accepted at ACM\n  Conference on Health, Inference, and Learning (ACM-CHIL) Workshop 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We systematically evaluate the performance of deep learning models in the\npresence of diseases not labeled for or present during training. First, we\nevaluate whether deep learning models trained on a subset of diseases (seen\ndiseases) can detect the presence of any one of a larger set of diseases. We\nfind that models tend to falsely classify diseases outside of the subset\n(unseen diseases) as \"no disease\". Second, we evaluate whether models trained\non seen diseases can detect seen diseases when co-occurring with diseases\noutside the subset (unseen diseases). We find that models are still able to\ndetect seen diseases even when co-occurring with unseen diseases. Third, we\nevaluate whether feature representations learned by models may be used to\ndetect the presence of unseen diseases given a small labeled set of unseen\ndiseases. We find that the penultimate layer of the deep neural network\nprovides useful features for unseen disease detection. Our results can inform\nthe safe clinical deployment of deep learning models trained on a\nnon-exhaustive set of disease classes.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 08:13:21 GMT"}, {"version": "v2", "created": "Mon, 17 May 2021 05:15:55 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["Shi", "Siyu", ""], ["Malhi", "Ishaan", ""], ["Tran", "Kevin", ""], ["Ng", "Andrew Y.", ""], ["Rajpurkar", "Pranav", ""]]}, {"id": "2103.04616", "submitter": "Johann Lange", "authors": "Marian K\\\"orber, Johann Lange, Stephan Rediske, Simon Steinmann,\n  Roland Gl\\\"uck", "title": "Comparing Popular Simulation Environments in the Scope of Robotics and\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This letter compares the performance of four different, popular simulation\nenvironments for robotics and reinforcement learning (RL) through a series of\nbenchmarks. The benchmarked scenarios are designed carefully with current\nindustrial applications in mind. Given the need to run simulations as fast as\npossible to reduce the real-world training time of the RL agents, the\ncomparison includes not only different simulation environments but also\ndifferent hardware configurations, ranging from an entry-level notebook up to a\ndual CPU high performance server. We show that the chosen simulation\nenvironments benefit the most from single core performance. Yet, using a multi\ncore system, multiple simulations could be run in parallel to increase the\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 09:08:53 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["K\u00f6rber", "Marian", ""], ["Lange", "Johann", ""], ["Rediske", "Stephan", ""], ["Steinmann", "Simon", ""], ["Gl\u00fcck", "Roland", ""]]}, {"id": "2103.04623", "submitter": "Jihoon Tack", "authors": "Jihoon Tack, Sihyun Yu, Jongheon Jeong, Minseon Kim, Sung Ju Hwang,\n  Jinwoo Shin", "title": "Consistency Regularization for Adversarial Robustness", "comments": "17 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adversarial training (AT) is currently one of the most successful methods to\nobtain the adversarial robustness of deep neural networks. However, the\nphenomenon of robust overfitting, i.e., the robustness starts to decrease\nsignificantly during AT, has been problematic, not only making practitioners\nconsider a bag of tricks for a successful training, e.g., early stopping, but\nalso incurring a significant generalization gap in the robustness. In this\npaper, we propose an effective regularization technique that prevents robust\noverfitting by optimizing an auxiliary 'consistency' regularization loss during\nAT. Specifically, it forces the predictive distributions after attacking from\ntwo different augmentations of the same instance to be similar with each other.\nOur experimental results demonstrate that such a simple regularization\ntechnique brings significant improvements in the test robust accuracy of a wide\nrange of AT methods. More remarkably, we also show that our method could\nsignificantly help the model to generalize its robustness against unseen\nadversaries, e.g., other types or larger perturbations compared to those used\nduring training. Code is available at\nhttps://github.com/alinlab/consistency-adversarial.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 09:21:41 GMT"}, {"version": "v2", "created": "Mon, 28 Jun 2021 07:53:46 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Tack", "Jihoon", ""], ["Yu", "Sihyun", ""], ["Jeong", "Jongheon", ""], ["Kim", "Minseon", ""], ["Hwang", "Sung Ju", ""], ["Shin", "Jinwoo", ""]]}, {"id": "2103.04693", "submitter": "Antonia Creswell", "authors": "Antonia Creswell, Rishabh Kabra, Chris Burgess, Murray Shanahan", "title": "Unsupervised Object-Based Transition Models for 3D Partially Observable\n  Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a slot-wise, object-based transition model that decomposes a scene\ninto objects, aligns them (with respect to a slot-wise object memory) to\nmaintain a consistent order across time, and predicts how those objects evolve\nover successive frames. The model is trained end-to-end without supervision\nusing losses at the level of the object-structured representation rather than\npixels. Thanks to its alignment module, the model deals properly with two\nissues that are not handled satisfactorily by other transition models, namely\nobject persistence and object identity. We show that the combination of an\nobject-level loss and correct object alignment over time enables the model to\noutperform a state-of-the-art baseline, and allows it to deal well with object\nocclusion and re-appearance in partially observable environments.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 12:10:02 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Creswell", "Antonia", ""], ["Kabra", "Rishabh", ""], ["Burgess", "Chris", ""], ["Shanahan", "Murray", ""]]}, {"id": "2103.04710", "submitter": "Peter Steiner", "authors": "Peter Steiner (1), Azarakhsh Jalalvand (2 and 3), Peter Birkholz (1)\n  ((1) Institute for Acoustics and Speech Communication, Technische\n  Universit\\\"at Dresden, 01069 Dresden, Germany, (2) IDLab, Ghent University,\n  Belgium, (3) Aerospace Engineering department, Princeton University, USA)", "title": "Cluster-based Input Weight Initialization for Echo State Networks", "comments": "Submitted to IEEE Transactions on Neural Network and Learning System\n  (TNNLS), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Echo State Networks (ESNs) are a special type of recurrent neural networks\n(RNNs), in which the input and recurrent connections are traditionally\ngenerated randomly, and only the output weights are trained. Despite the recent\nsuccess of ESNs in various tasks of audio, image and radar recognition, we\npostulate that a purely random initialization is not the ideal way of\ninitializing ESNs. The aim of this work is to propose an unsupervised\ninitialization of the input connections using the K-Means algorithm on the\ntraining data. We show that this initialization performs equivalently or\nsuperior than a randomly initialized ESN whilst needing significantly less\nreservoir neurons (2000 vs. 4000 for spoken digit recognition, and 300 vs. 8000\nneurons for f0 extraction) and thus reducing the amount of training time.\nFurthermore, we discuss that this approach provides the opportunity to estimate\nthe suitable size of the reservoir based on the prior knowledge about the data.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 12:39:57 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Steiner", "Peter", "", "2 and 3"], ["Jalalvand", "Azarakhsh", "", "2 and 3"], ["Birkholz", "Peter", ""]]}, {"id": "2103.04730", "submitter": "Aditya Mate", "authors": "Aditya Mate, Arpita Biswas, Christoph Siebenbrunner, Milind Tambe", "title": "Efficient Algorithms for Finite Horizon and Streaming Restless\n  Multi-Armed Bandit Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Restless Multi-Armed Bandits (RMABs) have been popularly used to model\nlimited resource allocation problems. Recently, these have been employed for\nhealth monitoring and intervention planning problems. However, the existing\napproaches fail to account for the arrival of new patients and the departure of\nenrolled patients from a treatment program. To address this challenge, we\nformulate a streaming bandit (S-RMAB) framework, a generalization of RMABs\nwhere heterogeneous arms arrive and leave under possibly random streams. We\npropose a new and scalable approach to computing index-based solutions. We\nstart by proving that index values decrease for short residual lifetimes, a\nphenomenon that we call index decay. We then provide algorithms designed to\ncapture index decay without having to solve the costly finite horizon problem,\nthereby lowering the computational complexity compared to existing methods.We\nevaluate our approach via simulations run on real-world data obtained from a\ntuberculosis intervention planning task as well as multiple other synthetic\ndomains. Our algorithms achieve an over 150x speed-up over existing methods in\nthese tasks without loss in performance. These findings are robust across\nmultiple domains.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 13:10:31 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Mate", "Aditya", ""], ["Biswas", "Arpita", ""], ["Siebenbrunner", "Christoph", ""], ["Tambe", "Milind", ""]]}, {"id": "2103.04747", "submitter": "Benjamin Goertzel", "authors": "Ben Goertzel", "title": "Info-Evo: Using Information Geometry to Guide Evolutionary Program\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel optimization strategy, Info-Evo, is described, in which natural\ngradient search using nonparametric Fisher information is used to provide\nongoing guidance to an evolutionary learning algorithm, so that the\nevolutionary process preferentially moves in the directions identified as\n\"shortest paths\" according to the natural gradient. Some specifics regarding\nthe application of this approach to automated program learning are reviewed,\nincluding a strategy for integrating Info-Evo into the MOSES program learning\nframework.\n", "versions": [{"version": "v1", "created": "Sat, 20 Feb 2021 09:36:00 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Goertzel", "Ben", ""]]}, {"id": "2103.04749", "submitter": "Eduard Paul Enoiu", "authors": "Eduard Enoiu, Robert Feldt", "title": "Towards Human-Like Automated Test Generation: Perspectives from\n  Cognition and Problem Solving", "comments": "preprint; accepted by CHASE 2020 as a note paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated testing tools typically create test cases that are different from\nwhat human testers create. This often makes the tools less effective, the\ncreated tests harder to understand, and thus results in tools providing less\nsupport to human testers. Here, we propose a framework based on cognitive\nscience and, in particular, an analysis of approaches to problem-solving, for\nidentifying cognitive processes of testers. The framework helps map test design\nsteps and criteria used in human test activities and thus to better understand\nhow effective human testers perform their tasks. Ultimately, our goal is to be\nable to mimic how humans create test cases and thus to design more human-like\nautomated test generation systems. We posit that such systems can better\naugment and support testers in a way that is meaningful to them.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 13:43:55 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Enoiu", "Eduard", ""], ["Feldt", "Robert", ""]]}, {"id": "2103.04751", "submitter": "Avijit Basak", "authors": "Avijit Basak", "title": "A Memory Optimized Data Structure for Binary Chromosomes in Genetic\n  Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a memory-optimized metadata-based data structure for\nimplementation of binary chromosome in Genetic Algorithm. In GA different types\nof genotypes are used depending on the problem domain. Among these, binary\ngenotype is the most popular one for non-enumerated encoding owing to its\nrepresentational and computational simplicity. This paper proposes a\nmemory-optimized implementation approach of binary genotype. The approach\nimproves the memory utilization as well as capacity of retaining alleles.\nMathematical proof has been provided to establish the same.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 15:49:11 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Basak", "Avijit", ""]]}, {"id": "2103.04757", "submitter": "Jakob Schoeffer", "authors": "Jakob Schoeffer, Yvette Machowski, Niklas Kuehl", "title": "A Study on Fairness and Trust Perceptions in Automated Decision Making", "comments": "Joint Proceedings of the ACM IUI 2021 Workshops, April 13--17, 2021,\n  College Station, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automated decision systems are increasingly used for consequential decision\nmaking -- for a variety of reasons. These systems often rely on sophisticated\nyet opaque models, which do not (or hardly) allow for understanding how or why\na given decision was arrived at. This is not only problematic from a legal\nperspective, but non-transparent systems are also prone to yield undesirable\n(e.g., unfair) outcomes because their sanity is difficult to assess and\ncalibrate in the first place. In this work, we conduct a study to evaluate\ndifferent attempts of explaining such systems with respect to their effect on\npeople's perceptions of fairness and trustworthiness towards the underlying\nmechanisms. A pilot study revealed surprising qualitative insights as well as\npreliminary significant effects, which will have to be verified, extended and\nthoroughly discussed in the larger main study.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 13:57:31 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Schoeffer", "Jakob", ""], ["Machowski", "Yvette", ""], ["Kuehl", "Niklas", ""]]}, {"id": "2103.04768", "submitter": "Liya Wang", "authors": "Liya Wang, Panta Lucic, Keith Campbell, and Craig Wanke", "title": "Helicopter Track Identification with Autoencoder", "comments": "Draft for ICNS conference. arXiv admin note: text overlap with\n  arXiv:2011.01464", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Computing power, big data, and advancement of algorithms have led to a\nrenewed interest in artificial intelligence (AI), especially in deep learning\n(DL). The success of DL largely lies on data representation because different\nrepresentations can indicate to a degree the different explanatory factors of\nvariation behind the data. In the last few year, the most successful story in\nDL is supervised learning. However, to apply supervised learning, one challenge\nis that data labels are expensive to get, noisy, or only partially available.\nWith consideration that we human beings learn in an unsupervised way;\nself-supervised learning methods have garnered a lot of attention recently. A\ndominant force in self-supervised learning is the autoencoder, which has\nmultiple uses (e.g., data representation, anomaly detection, denoise). This\nresearch explored the application of an autoencoder to learn effective data\nrepresentation of helicopter flight track data, and then to support helicopter\ntrack identification. Our testing results are promising. For example, at\nPhoenix Deer Valley (DVT) airport, where 70% of recorded flight tracks have\nmissing aircraft types, the autoencoder can help to identify twenty-two times\nmore helicopters than otherwise detectable using rule-based methods; for Grand\nCanyon West Airport (1G4) airport, the autoencoder can identify thirteen times\nmore helicopters than a current rule-based approach. Our approach can also\nidentify mislabeled aircraft types in the flight track data and find true types\nfor records with pseudo aircraft type labels such as HELO. With improved\nlabelling, studies using these data sets can produce more reliable results.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 22:32:39 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Wang", "Liya", ""], ["Lucic", "Panta", ""], ["Campbell", "Keith", ""], ["Wanke", "Craig", ""]]}, {"id": "2103.04780", "submitter": "Wilkie Olin-Ammentorp", "authors": "Wilkie Olin-Ammentorp, Yury Sokolov, Maxim Bazhenov", "title": "A Dual-Memory Architecture for Reinforcement Learning on Neuromorphic\n  Platforms", "comments": "20 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning (RL) is a foundation of learning in biological systems\nand provides a framework to address numerous challenges with real-world\nartificial intelligence applications. Efficient implementations of RL\ntechniques could allow for agents deployed in edge-use cases to gain novel\nabilities, such as improved navigation, understanding complex situations and\ncritical decision making. Towards this goal, we describe a flexible\narchitecture to carry out reinforcement learning on neuromorphic platforms.\nThis architecture was implemented using an Intel neuromorphic processor and\ndemonstrated solving a variety of tasks using spiking dynamics. Our study\nproposes a usable energy efficient solution for real-world RL applications and\ndemonstrates applicability of the neuromorphic platforms for RL problems.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 01:54:22 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Olin-Ammentorp", "Wilkie", ""], ["Sokolov", "Yury", ""], ["Bazhenov", "Maxim", ""]]}, {"id": "2103.04781", "submitter": "Ahmed Rasheed", "authors": "Ahmed Rasheed, Muhammad Shahzad Younis, Farooq Ahmad, Junaid Qadir,\n  and Muhammad Kashif", "title": "District Wise Price Forecasting of Wheat in Pakistan using Deep Learning", "comments": "9 pages, submitted to IEEE Access", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Wheat is the main agricultural crop of Pakistan and is a staple food\nrequirement of almost every Pakistani household making it the main strategic\ncommodity of the country whose availability and affordability is the\ngovernment's main priority. Wheat food availability can be vastly affected by\nmultiple factors included but not limited to the production, consumption,\nfinancial crisis, inflation, or volatile market. The government ensures food\nsecurity by particular policy and monitory arrangements, which keeps up\npurchase parity for the poor. Such arrangements can be made more effective if a\ndynamic analysis is carried out to estimate the future yield based on certain\ncurrent factors. Future planning of commodity pricing is achievable by\nforecasting their future price anticipated by the current circumstances. This\npaper presents a wheat price forecasting methodology, which uses the price,\nweather, production, and consumption trends for wheat prices taken over the\npast few years and analyzes them with the help of advance neural networks\narchitecture Long Short Term Memory (LSTM) networks. The proposed methodology\npresented significantly improved results versus other conventional machine\nlearning and statistical time series analysis methods.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 06:13:51 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Rasheed", "Ahmed", ""], ["Younis", "Muhammad Shahzad", ""], ["Ahmad", "Farooq", ""], ["Qadir", "Junaid", ""], ["Kashif", "Muhammad", ""]]}, {"id": "2103.04786", "submitter": "Maximilian Ilse", "authors": "Maximilian Ilse, Patrick Forr\\'e, Max Welling, Joris M. Mooij", "title": "Efficient Causal Inference from Combined Observational and\n  Interventional Data through Causal Reductions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.ME", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unobserved confounding is one of the main challenges when estimating causal\neffects. We propose a novel causal reduction method that replaces an arbitrary\nnumber of possibly high-dimensional latent confounders with a single latent\nconfounder that lives in the same space as the treatment variable without\nchanging the observational and interventional distributions entailed by the\ncausal model. After the reduction, we parameterize the reduced causal model\nusing a flexible class of transformations, so-called normalizing flows. We\npropose a learning algorithm to estimate the parameterized reduced model\njointly from observational and interventional data. This allows us to estimate\nthe causal effect in a principled way from combined data. We perform a series\nof experiments on data simulated using nonlinear causal mechanisms and find\nthat we can often substantially reduce the number of interventional samples\nwhen adding observational training samples without sacrificing accuracy. Thus,\nadding observational data may help to more accurately estimate causal effects\neven in the presence of unobserved confounders.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 14:29:07 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Ilse", "Maximilian", ""], ["Forr\u00e9", "Patrick", ""], ["Welling", "Max", ""], ["Mooij", "Joris M.", ""]]}, {"id": "2103.04826", "submitter": "Jamal Toutouh", "authors": "Diego Gabriel Rossit, Jamal Toutouh, and Sergio Nesmachnow", "title": "Exact and heuristic approaches for multi-objective garbage accumulation\n  points location in real scenarios", "comments": "This article has been accepted for publication in the Waste\n  Management journal", "journal-ref": "Waste Management. 105:467-481 (2020)", "doi": "10.1016/j.wasman.2020.02.016", "report-no": null, "categories": "cs.OH cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Municipal solid waste management is a major challenge for nowadays urban\nsocieties, because it accounts for a large proportion of public budget and,\nwhen mishandled, it can lead to environmental and social problems. This work\nfocuses on the problem of locating waste bins in an urban area, which is\nconsidered to have a strong influence in the overall efficiency of the reverse\nlogistic chain. This article contributes with an exact multiobjective approach\nto solve the waste bin location in which the optimization criteria that are\nconsidered are: the accessibility to the system (as quality of service\nmeasure), the investment cost, and the required frequency of waste removal from\nthe bins (as a proxy of the posterior routing costs). In this approach,\ndifferent methods to obtain the objectives ideal and nadir values over the\nPareto front are proposed and compared. Then, a family of heuristic methods\nbased on the PageRank algorithm is proposed which aims to optimize the\naccessibility to the system, the amount of collected waste and the installation\ncost. The experimental evaluation was performed on real-world scenarios of the\ncities of Montevideo, Uruguay, and Bah\\'ia Blanca, Argentina. The obtained\nresults show the competitiveness of the proposed approaches for constructing a\nset of candidate solutions that considers the different trade-offs between the\noptimization criteria.\n", "versions": [{"version": "v1", "created": "Fri, 5 Mar 2021 13:47:21 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 18:05:15 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Rossit", "Diego Gabriel", ""], ["Toutouh", "Jamal", ""], ["Nesmachnow", "Sergio", ""]]}, {"id": "2103.04845", "submitter": "Mo Han", "authors": "Mo Han, Sezen Ya{\\u{g}}mur G\\\"unay, Gunar Schirner, Ta\\c{s}k{\\i}n\n  Pad{\\i}r, Deniz Erdo{\\u{g}}mu\\c{s}", "title": "HANDS: A Multimodal Dataset for Modeling Towards Human Grasp Intent\n  Inference in Prosthetic Hands", "comments": null, "journal-ref": null, "doi": "10.1007/s11370-019-00293-8", "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Upper limb and hand functionality is critical to many activities of daily\nliving and the amputation of one can lead to significant functionality loss for\nindividuals. From this perspective, advanced prosthetic hands of the future are\nanticipated to benefit from improved shared control between a robotic hand and\nits human user, but more importantly from the improved capability to infer\nhuman intent from multimodal sensor data to provide the robotic hand perception\nabilities regarding the operational context. Such multimodal sensor data may\ninclude various environment sensors including vision, as well as human\nphysiology and behavior sensors including electromyography and inertial\nmeasurement units. A fusion methodology for environmental state and human\nintent estimation can combine these sources of evidence in order to help\nprosthetic hand motion planning and control.\n  In this paper, we present a dataset of this type that was gathered with the\nanticipation of cameras being built into prosthetic hands, and computer vision\nmethods will need to assess this hand-view visual evidence in order to estimate\nhuman intent. Specifically, paired images from human eye-view and hand-view of\nvarious objects placed at different orientations have been captured at the\ninitial state of grasping trials, followed by paired video, EMG and IMU from\nthe arm of the human during a grasp, lift, put-down, and retract style trial\nstructure. For each trial, based on eye-view images of the scene showing the\nhand and object on a table, multiple humans were asked to sort in decreasing\norder of preference, five grasp types appropriate for the object in its given\nconfiguration relative to the hand. The potential utility of paired eye-view\nand hand-view images was illustrated by training a convolutional neural network\nto process hand-view images in order to predict eye-view labels assigned by\nhumans.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 15:51:03 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Han", "Mo", ""], ["G\u00fcnay", "Sezen Ya{\u011f}mur", ""], ["Schirner", "Gunar", ""], ["Pad\u0131r", "Ta\u015fk\u0131n", ""], ["Erdo{\u011f}mu\u015f", "Deniz", ""]]}, {"id": "2103.04846", "submitter": "Fan Fu", "authors": "Fan Fu, Tingting Xie, Ioannis Patras, Sepehr Jalali", "title": "Relationship-based Neural Baby Talk", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding interactions between objects in an image is an important\nelement for generating captions. In this paper, we propose a relationship-based\nneural baby talk (R-NBT) model to comprehensively investigate several types of\npairwise object interactions by encoding each image via three different\nrelationship-based graph attention networks (GATs). We study three main\nrelationships: \\textit{spatial relationships} to explore geometric\ninteractions, \\textit{semantic relationships} to extract semantic interactions,\nand \\textit{implicit relationships} to capture hidden information that could\nnot be modelled explicitly as above. We construct three relationship graphs\nwith the objects in an image as nodes, and the mutual relationships of pairwise\nobjects as edges. By exploring features of neighbouring regions individually\nvia GATs, we integrate different types of relationships into visual features of\neach node. Experiments on COCO dataset show that our proposed R-NBT model\noutperforms state-of-the-art models trained on COCO dataset in three image\ncaption generation tasks.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 15:51:24 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Fu", "Fan", ""], ["Xie", "Tingting", ""], ["Patras", "Ioannis", ""], ["Jalali", "Sepehr", ""]]}, {"id": "2103.04847", "submitter": "Linus Gissl\\'en", "authors": "Linus Gissl\\'en, Andy Eakins, Camilo Gordillo, Joakim Bergdahl, Konrad\n  Tollmar", "title": "Adversarial Reinforcement Learning for Procedural Content Generation", "comments": "8 pages, 6 figures (11 subfigures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach ARLPCG: Adversarial Reinforcement Learning for\nProcedural Content Generation, which procedurally generates and tests\npreviously unseen environments with an auxiliary input as a control variable.\nTraining RL agents over novel environments is a notoriously difficult task. One\npopular approach is to procedurally generate different environments to increase\nthe generalizability of the trained agents. ARLPCG instead deploys an\nadversarial model with one PCG RL agent (called Generator) and one solving RL\nagent (called Solver). The Generator receives a reward signal based on the\nSolver's performance, which encourages the environment design to be challenging\nbut not impossible. To further drive diversity and control of the environment\ngeneration, we propose using auxiliary inputs for the Generator. The benefit is\ntwo-fold: Firstly, the Solver achieves better generalization through the\nGenerator's generated challenges. Secondly, the trained Generator can be used\nas a creator of novel environments that, together with the Solver, can be shown\nto be solvable. We create two types of 3D environments to validate our model,\nrepresenting two popular game genres: a third-person platformer and a racing\ngame. In these cases, we shows that ARLPCG has a significantly better solve\nratio, and that the auxiliary inputs renders the levels creation controllable\nto a certain degree. For a video compilation of the results please visit\nhttps://youtu.be/z7q2PtVsT0I.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 15:51:42 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 11:00:30 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Gissl\u00e9n", "Linus", ""], ["Eakins", "Andy", ""], ["Gordillo", "Camilo", ""], ["Bergdahl", "Joakim", ""], ["Tollmar", "Konrad", ""]]}, {"id": "2103.04852", "submitter": "E K", "authors": "Eren Kurshan, Hai Li, Mingoo Seok, Yuan Xie", "title": "A Case for 3D Integrated System Design for Neuromorphic Computing & AI\n  Applications", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AI cs.AR cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last decade, artificial intelligence has found many applications\nareas in the society. As AI solutions have become more sophistication and the\nuse cases grew, they highlighted the need to address performance and energy\nefficiency challenges faced during the implementation process. To address these\nchallenges, there has been growing interest in neuromorphic chips. Neuromorphic\ncomputing relies on non von Neumann architectures as well as novel devices,\ncircuits and manufacturing technologies to mimic the human brain. Among such\ntechnologies, 3D integration is an important enabler for AI hardware and the\ncontinuation of the scaling laws. In this paper, we overview the unique\nopportunities 3D integration provides in neuromorphic chip design, discuss the\nemerging opportunities in next generation neuromorphic architectures and review\nthe obstacles. Neuromorphic architectures, which relied on the brain for\ninspiration and emulation purposes, face grand challenges due to the limited\nunderstanding of the functionality and the architecture of the human brain.\nYet, high-levels of investments are dedicated to develop neuromorphic chips. We\nargue that 3D integration not only provides strategic advantages to the\ncost-effective and flexible design of neuromorphic chips, it may provide design\nflexibility in incorporating advanced capabilities to further benefits the\ndesigns in the future.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 21:50:12 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Kurshan", "Eren", ""], ["Li", "Hai", ""], ["Seok", "Mingoo", ""], ["Xie", "Yuan", ""]]}, {"id": "2103.04854", "submitter": "Mohammadhossein Bahari", "authors": "Mohammadhossein Bahari, Ismail Nejjar, Alexandre Alahi", "title": "Injecting Knowledge in Data-driven Vehicle Trajectory Predictors", "comments": "To be published in Transportation Research: Part C", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vehicle trajectory prediction tasks have been commonly tackled from two\ndistinct perspectives: either with knowledge-driven methods or more recently\nwith data-driven ones. On the one hand, we can explicitly implement\ndomain-knowledge or physical priors such as anticipating that vehicles will\nfollow the middle of the roads. While this perspective leads to feasible\noutputs, it has limited performance due to the difficulty to hand-craft complex\ninteractions in urban environments. On the other hand, recent works use\ndata-driven approaches which can learn complex interactions from the data\nleading to superior performance. However, generalization, \\textit{i.e.}, having\naccurate predictions on unseen data, is an issue leading to unrealistic\noutputs. In this paper, we propose to learn a \"Realistic Residual Block\" (RRB),\nwhich effectively connects these two perspectives. Our RRB takes any\noff-the-shelf knowledge-driven model and finds the required residuals to add to\nthe knowledge-aware trajectory. Our proposed method outputs realistic\npredictions by confining the residual range and taking into account its\nuncertainty. We also constrain our output with Model Predictive Control (MPC)\nto satisfy kinematic constraints. Using a publicly available dataset, we show\nthat our method outperforms previous works in terms of accuracy and\ngeneralization to new scenes. We will release our code and data split here:\nhttps://github.com/vita-epfl/RRB.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 16:03:09 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Bahari", "Mohammadhossein", ""], ["Nejjar", "Ismail", ""], ["Alahi", "Alexandre", ""]]}, {"id": "2103.04863", "submitter": "Mo Han", "authors": "Mo Han, Sezen Ya{\\u{g}}mur G\\\"unay, \\.Ilkay Y{\\i}ld{\\i}z, Paolo\n  Bonato, Cagdas D. Onal, Ta\\c{s}k{\\i}n Pad{\\i}r, Gunar Schirner, Deniz\n  Erdo{\\u{g}}mu\\c{s}", "title": "From Hand-Perspective Visual Information to Grasp Type Probabilities:\n  Deep Learning via Ranking Labels", "comments": null, "journal-ref": null, "doi": "10.1145/3316782.3316794", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Limb deficiency severely affects the daily lives of amputees and drives\nefforts to provide functional robotic prosthetic hands to compensate this\ndeprivation. Convolutional neural network-based computer vision control of the\nprosthetic hand has received increased attention as a method to replace or\ncomplement physiological signals due to its reliability by training visual\ninformation to predict the hand gesture. Mounting a camera into the palm of a\nprosthetic hand is proved to be a promising approach to collect visual data.\nHowever, the grasp type labelled from the eye and hand perspective may differ\nas object shapes are not always symmetric. Thus, to represent this difference\nin a realistic way, we employed a dataset containing synchronous images from\neye- and hand- view, where the hand-perspective images are used for training\nwhile the eye-view images are only for manual labelling. Electromyogram (EMG)\nactivity and movement kinematics data from the upper arm are also collected for\nmulti-modal information fusion in future work. Moreover, in order to include\nhuman-in-the-loop control and combine the computer vision with physiological\nsignal inputs, instead of making absolute positive or negative predictions, we\nbuild a novel probabilistic classifier according to the Plackett-Luce model. To\npredict the probability distribution over grasps, we exploit the statistical\nmodel over label rankings to solve the permutation domain problems via a\nmaximum likelihood estimation, utilizing the manually ranked lists of grasps as\na new form of label. We indicate that the proposed model is applicable to the\nmost popular and productive convolutional neural network frameworks.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 16:12:38 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Han", "Mo", ""], ["G\u00fcnay", "Sezen Ya{\u011f}mur", ""], ["Y\u0131ld\u0131z", "\u0130lkay", ""], ["Bonato", "Paolo", ""], ["Onal", "Cagdas D.", ""], ["Pad\u0131r", "Ta\u015fk\u0131n", ""], ["Schirner", "Gunar", ""], ["Erdo{\u011f}mu\u015f", "Deniz", ""]]}, {"id": "2103.04876", "submitter": "Sam Kriegman", "authors": "Sam Kriegman, Amir Mohammadi Nasab, Douglas Blackiston, Hannah Steele,\n  Michael Levin, Rebecca Kramer-Bottiglio, Josh Bongard", "title": "Scale invariant robot behavior with fractals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Robots deployed at orders of magnitude different size scales, and that retain\nthe same desired behavior at any of those scales, would greatly expand the\nenvironments in which the robots could operate. However it is currently not\nknown whether such robots exist, and, if they do, how to design them. Since\nself similar structures in nature often exhibit self similar behavior at\ndifferent scales, we hypothesize that there may exist robot designs that have\nthe same property. Here we demonstrate that this is indeed the case for some,\nbut not all, modular soft robots: there are robot designs that exhibit a\ndesired behavior at a small size scale, and if copies of that robot are\nattached together to realize the same design at higher scales, those larger\nrobots exhibit similar behavior. We show how to find such designs in simulation\nusing an evolutionary algorithm. Further, when fractal attachment is not\nassumed and attachment geometries must thus be evolved along with the design of\nthe base robot unit, scale invariant behavior is not achieved, demonstrating\nthat structural self similarity, when combined with appropriate designs, is a\nuseful path to realizing scale invariant robot behavior. We validate our\nfindings by demonstrating successful transferal of self similar structure and\nbehavior to pneumatically-controlled soft robots. Finally, we show that biobots\ncan spontaneously exhibit self similar attachment geometries, thereby\nsuggesting that self similar behavior via self similar structure may be\nrealizable across a wide range of robot platforms in future.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 16:27:07 GMT"}, {"version": "v2", "created": "Fri, 25 Jun 2021 04:30:06 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Kriegman", "Sam", ""], ["Nasab", "Amir Mohammadi", ""], ["Blackiston", "Douglas", ""], ["Steele", "Hannah", ""], ["Levin", "Michael", ""], ["Kramer-Bottiglio", "Rebecca", ""], ["Bongard", "Josh", ""]]}, {"id": "2103.04909", "submitter": "Ramin Hasani", "authors": "Axel Brunnbauer, Luigi Berducci, Andreas Brandst\\\"atter, Mathias\n  Lechner, Ramin Hasani, Daniela Rus, Radu Grosu", "title": "Model-based versus Model-free Deep Reinforcement Learning for Autonomous\n  Racing Cars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the rich theoretical foundation of model-based deep reinforcement\nlearning (RL) agents, their effectiveness in real-world robotics-applications\nis less studied and understood. In this paper, we, therefore, investigate how\nsuch agents generalize to real-world autonomous-vehicle control-tasks, where\nadvanced model-free deep RL algorithms fail. In particular, we set up a series\nof time-lap tasks for an F1TENTH racing robot, equipped with high-dimensional\nLiDAR sensors, on a set of test tracks with a gradual increase in their\ncomplexity. In this continuous-control setting, we show that model-based agents\ncapable of learning in imagination, substantially outperform model-free agents\nwith respect to performance, sample efficiency, successful task completion, and\ngeneralization. Moreover, we show that the generalization ability of\nmodel-based agents strongly depends on the observation-model choice. Finally,\nwe provide extensive empirical evidence for the effectiveness of model-based\nagents provided with long enough memory horizons in sim2real tasks.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 17:15:23 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Brunnbauer", "Axel", ""], ["Berducci", "Luigi", ""], ["Brandst\u00e4tter", "Andreas", ""], ["Lechner", "Mathias", ""], ["Hasani", "Ramin", ""], ["Rus", "Daniela", ""], ["Grosu", "Radu", ""]]}, {"id": "2103.04918", "submitter": "Jiafei Duan", "authors": "Jiafei Duan, Samson Yu, Hui Li Tan, Hongyuan Zhu and Cheston Tan", "title": "A Survey of Embodied AI: From Simulators to Research Tasks", "comments": "Submitted for CVIU review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  There has been an emerging paradigm shift from the era of \"internet AI\" to\n\"embodied AI\", whereby AI algorithms and agents no longer simply learn from\ndatasets of images, videos or text curated primarily from the internet.\nInstead, they learn through embodied physical interactions with their\nenvironments, whether real or simulated. Consequently, there has been\nsubstantial growth in the demand for embodied AI simulators to support a\ndiversity of embodied AI research tasks. This growing interest in embodied AI\nis beneficial to the greater pursuit of artificial general intelligence, but\nthere is no contemporary and comprehensive survey of this field. This paper\ncomprehensively surveys state-of-the-art embodied AI simulators and research,\nmapping connections between these. By benchmarking nine state-of-the-art\nembodied AI simulators in terms of seven features, this paper aims to\nunderstand the simulators in their provision for use in embodied AI research.\nFinally, based upon the simulators and a pyramidal hierarchy of embodied AI\nresearch tasks, this paper surveys the main research tasks in embodied AI --\nvisual exploration, visual navigation and embodied question answering (QA),\ncovering the state-of-the-art approaches, evaluation and datasets.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 17:31:19 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 02:33:07 GMT"}, {"version": "v3", "created": "Wed, 10 Mar 2021 02:16:01 GMT"}, {"version": "v4", "created": "Sun, 14 Mar 2021 03:27:27 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Duan", "Jiafei", ""], ["Yu", "Samson", ""], ["Tan", "Hui Li", ""], ["Zhu", "Hongyuan", ""], ["Tan", "Cheston", ""]]}, {"id": "2103.04931", "submitter": "Bartosz Sawicki", "authors": "Maciej \\'Swiechowski, Konrad Godlewski, Bartosz Sawicki, Jacek\n  Ma\\'ndziuk", "title": "Monte Carlo Tree Search: A Review of Recent Modifications and\n  Applications", "comments": "84 pages, submitted to AI Review journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo Tree Search (MCTS) is a powerful approach to designing\ngame-playing bots or solving sequential decision problems. The method relies on\nintelligent tree search that balances exploration and exploitation. MCTS\nperforms random sampling in the form of simulations and stores statistics of\nactions to make more educated choices in each subsequent iteration. The method\nhas become a state-of-the-art technique for combinatorial games, however, in\nmore complex games (e.g. those with high branching factor or real-time ones),\nas well as in various practical domains (e.g. transportation, scheduling or\nsecurity) an efficient MCTS application often requires its problem-dependent\nmodification or integration with other techniques. Such domain-specific\nmodifications and hybrid approaches are the main focus of this survey. The last\nmajor MCTS survey has been published in 2012. Contributions that appeared since\nits release are of particular interest for this review.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 17:44:15 GMT"}, {"version": "v2", "created": "Tue, 9 Mar 2021 13:04:22 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["\u015awiechowski", "Maciej", ""], ["Godlewski", "Konrad", ""], ["Sawicki", "Bartosz", ""], ["Ma\u0144dziuk", "Jacek", ""]]}, {"id": "2103.04947", "submitter": "Ruosong Wang", "authors": "Ruosong Wang, Yifan Wu, Ruslan Salakhutdinov, Sham M. Kakade", "title": "Instabilities of Offline RL with Pre-Trained Neural Representation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In offline reinforcement learning (RL), we seek to utilize offline data to\nevaluate (or learn) policies in scenarios where the data are collected from a\ndistribution that substantially differs from that of the target policy to be\nevaluated. Recent theoretical advances have shown that such sample-efficient\noffline RL is indeed possible provided certain strong representational\nconditions hold, else there are lower bounds exhibiting exponential error\namplification (in the problem horizon) unless the data collection distribution\nhas only a mild distribution shift relative to the target policy. This work\nstudies these issues from an empirical perspective to gauge how stable offline\nRL methods are. In particular, our methodology explores these ideas when using\nfeatures from pre-trained neural networks, in the hope that these\nrepresentations are powerful enough to permit sample efficient offline RL.\nThrough extensive experiments on a range of tasks, we see that substantial\nerror amplification does occur even when using such pre-trained representations\n(trained on the same task itself); we find offline RL is stable only under\nextremely mild distribution shift. The implications of these results, both from\na theoretical and an empirical perspective, are that successful offline RL\n(where we seek to go beyond the low distribution shift regime) requires\nsubstantially stronger conditions beyond those which suffice for successful\nsupervised learning.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 18:06:44 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Wang", "Ruosong", ""], ["Wu", "Yifan", ""], ["Salakhutdinov", "Ruslan", ""], ["Kakade", "Sham M.", ""]]}, {"id": "2103.04951", "submitter": "Jamie Duell MSc.", "authors": "Jamie Andrew Duell", "title": "A Comparative Approach to Explainable Artificial Intelligence Methods in\n  Application to High-Dimensional Electronic Health Records: Examining the\n  Usability of XAI", "comments": "18 Pages, 11 Figures, Supplementary work supporting a proposal - the\n  existing paper is open to future modification and further development post\n  arXiv submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Explainable Artificial Intelligence (XAI) is a rising field in AI. It aims to\nproduce a demonstrative factor of trust, which for human subjects is achieved\nthrough communicative means, which Machine Learning (ML) algorithms cannot\nsolely produce, illustrating the necessity of an extra layer producing support\nto the model output. When approaching the medical field, we can see challenges\narise when dealing with the involvement of human-subjects, the ideology behind\ntrusting a machine to tend towards the livelihood of a human poses an ethical\nconundrum - leaving trust as the basis of the human-expert in acceptance to the\nmachines decision. The aim of this paper is to apply XAI methods to demonstrate\nthe usability of explainable architectures as a tertiary layer for the medical\ndomain supporting ML predictions and human-expert opinion, XAI methods produce\nvisualization of the feature contribution towards a given models output on both\na local and global level. The work in this paper uses XAI to determine feature\nimportance towards high-dimensional data-driven questions to inform\ndomain-experts of identifiable trends with a comparison of model-agnostic\nmethods in application to ML algorithms. The performance metrics for a\nglass-box method is also provided as a comparison against black-box capability\nfor tabular data. Future work will aim to produce a user-study using metrics to\nevaluate human-expert usability and opinion of the given models.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 18:15:52 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Duell", "Jamie Andrew", ""]]}, {"id": "2103.04957", "submitter": "Yan Zhang", "authors": "Yan Zhang", "title": "Learning to Represent and Predict Sets with Deep Neural Networks", "comments": "PhD thesis submitted December 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this thesis, we develop various techniques for working with sets in\nmachine learning. Each input or output is not an image or a sequence, but a\nset: an unordered collection of multiple objects, each object described by a\nfeature vector. Their unordered nature makes them suitable for modeling a wide\nvariety of data, ranging from objects in images to point clouds to graphs. Deep\nlearning has recently shown great success on other types of structured data, so\nwe aim to build the necessary structures for sets into deep neural networks.\n  The first focus of this thesis is the learning of better set representations\n(sets as input). Existing approaches have bottlenecks that prevent them from\nproperly modeling relations between objects within the set. To address this\nissue, we develop a variety of techniques for different scenarios and show that\nalleviating the bottleneck leads to consistent improvements across many\nexperiments.\n  The second focus of this thesis is the prediction of sets (sets as output).\nCurrent approaches do not take the unordered nature of sets into account\nproperly. We determine that this results in a problem that causes discontinuity\nissues with many set prediction tasks and prevents them from learning some\nextremely simple datasets. To avoid this problem, we develop two models that\nproperly take the structure of sets into account. Various experiments show that\nour set prediction techniques can significantly benefit over existing\napproaches.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 18:27:08 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["Zhang", "Yan", ""]]}, {"id": "2103.04982", "submitter": "Kevin McKee", "authors": "Kevin R. McKee, Edward Hughes, Tina O. Zhu, Martin J. Chadwick,\n  Raphael Koster, Antonio Garcia Castaneda, Charlie Beattie, Thore Graepel,\n  Matt Botvinick, Joel Z. Leibo", "title": "Deep reinforcement learning models the emergent dynamics of human\n  cooperation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collective action demands that individuals efficiently coordinate how much,\nwhere, and when to cooperate. Laboratory experiments have extensively explored\nthe first part of this process, demonstrating that a variety of\nsocial-cognitive mechanisms influence how much individuals choose to invest in\ngroup efforts. However, experimental research has been unable to shed light on\nhow social cognitive mechanisms contribute to the where and when of collective\naction. We leverage multi-agent deep reinforcement learning to model how a\nsocial-cognitive mechanism--specifically, the intrinsic motivation to achieve a\ngood reputation--steers group behavior toward specific spatial and temporal\nstrategies for collective action in a social dilemma. We also collect\nbehavioral data from groups of human participants challenged with the same\ndilemma. The model accurately predicts spatial and temporal patterns of group\nbehavior: in this public goods dilemma, the intrinsic motivation for reputation\ncatalyzes the development of a non-territorial, turn-taking strategy to\ncoordinate collective action.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 18:58:40 GMT"}], "update_date": "2021-03-09", "authors_parsed": [["McKee", "Kevin R.", ""], ["Hughes", "Edward", ""], ["Zhu", "Tina O.", ""], ["Chadwick", "Martin J.", ""], ["Koster", "Raphael", ""], ["Castaneda", "Antonio Garcia", ""], ["Beattie", "Charlie", ""], ["Graepel", "Thore", ""], ["Botvinick", "Matt", ""], ["Leibo", "Joel Z.", ""]]}, {"id": "2103.05079", "submitter": "Edoardo Cetin", "authors": "Edoardo Cetin and Oya Celiktutan", "title": "Domain-Robust Visual Imitation Learning with Mutual Information\n  Constraints", "comments": "Presented at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human beings are able to understand objectives and learn by simply observing\nothers perform a task. Imitation learning methods aim to replicate such\ncapabilities, however, they generally depend on access to a full set of optimal\nstates and actions taken with the agent's actuators and from the agent's point\nof view. In this paper, we introduce a new algorithm - called Disentangling\nGenerative Adversarial Imitation Learning (DisentanGAIL) - with the purpose of\nbypassing such constraints. Our algorithm enables autonomous agents to learn\ndirectly from high dimensional observations of an expert performing a task, by\nmaking use of adversarial learning with a latent representation inside the\ndiscriminator network. Such latent representation is regularized through mutual\ninformation constraints to incentivize learning only features that encode\ninformation about the completion levels of the task being demonstrated. This\nallows to obtain a shared feature space to successfully perform imitation while\ndisregarding the differences between the expert's and the agent's domains.\nEmpirically, our algorithm is able to efficiently imitate in a diverse range of\ncontrol problems including balancing, manipulation and locomotive tasks, while\nbeing robust to various domain differences in terms of both environment\nappearance and agent embodiment.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 21:18:58 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Cetin", "Edoardo", ""], ["Celiktutan", "Oya", ""]]}, {"id": "2103.05101", "submitter": "Aytekin Nebisoy", "authors": "Aytekin Nebisoy and Saber Malekzadeh", "title": "Video Action Recognition Using spatio-temporal optical flow video frames", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recognizing human actions based on videos has became one of the most popular\nareas of research in computer vision in recent years. This area has many\napplications such as surveillance, robotics, health care, video search and\nhuman-computer interaction. There are many problems associated with recognizing\nhuman actions in videos such as cluttered backgrounds, obstructions, viewpoints\nvariation, execution speed and camera movement. A large number of methods have\nbeen proposed to solve the problems. This paper focus on spatial and temporal\npattern recognition for the classification of videos using Deep Neural\nNetworks. This model takes RGB images and Optical Flow as input data and\noutputs an action class number. The final recognition accuracy was about 94%.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2021 19:46:49 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Nebisoy", "Aytekin", ""], ["Malekzadeh", "Saber", ""]]}, {"id": "2103.05103", "submitter": "Shikha Dubey", "authors": "Farrukh Olimov, Shikha Dubey, Labina Shrestha, Tran Trung Tin, Moongu\n  Jeon", "title": "Image Captioning using Multiple Transformers for Self-Attention\n  Mechanism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time image captioning, along with adequate precision, is the main\nchallenge of this research field. The present work, Multiple Transformers for\nSelf-Attention Mechanism (MTSM), utilizes multiple transformers to address\nthese problems. The proposed algorithm, MTSM, acquires region proposals using a\ntransformer detector (DETR). Consequently, MTSM achieves the self-attention\nmechanism by transferring these region proposals and their visual and\ngeometrical features through another transformer and learns the objects' local\nand global interconnections. The qualitative and quantitative results of the\nproposed algorithm, MTSM, are shown on the MSCOCO dataset.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2021 05:35:54 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Olimov", "Farrukh", ""], ["Dubey", "Shikha", ""], ["Shrestha", "Labina", ""], ["Tin", "Tran Trung", ""], ["Jeon", "Moongu", ""]]}, {"id": "2103.05108", "submitter": "Jessica Cooper", "authors": "Jessica Cooper, Ognjen Arandjelovi\\'c, David J Harrison", "title": "Believe The HiPe: Hierarchical Perturbation for Fast, Robust and\n  Model-Agnostic Explanations", "comments": "github.com/jessicamarycooper/Hierarchical-Perturbation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding the predictions made by Artificial Intelligence (AI) systems is\nbecoming more and more important as deep learning models are used for\nincreasingly complex and high-stakes tasks. Saliency mapping - an easily\ninterpretable visual attribution method - is one important tool for this, but\nexisting formulations are limited by either computational cost or architectural\nconstraints. We therefore propose Hierarchical Perturbation, a very fast and\ncompletely model-agnostic method for explaining model predictions with robust\nsaliency maps. Using standard benchmarks and datasets, we show that our\nsaliency maps are of competitive or superior quality to those generated by\nexisting model-agnostic methods - and are over 20X faster to compute.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 18:22:56 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 10:15:44 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Cooper", "Jessica", ""], ["Arandjelovi\u0107", "Ognjen", ""], ["Harrison", "David J", ""]]}, {"id": "2103.05112", "submitter": "Adriano Lucieri", "authors": "Adriano Lucieri, Andreas Dengel and Sheraz Ahmed", "title": "Deep Learning Based Decision Support for Medicine -- A Case Study on\n  Skin Cancer Diagnosis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Early detection of skin cancers like melanoma is crucial to ensure high\nchances of survival for patients. Clinical application of Deep Learning\n(DL)-based Decision Support Systems (DSS) for skin cancer screening has the\npotential to improve the quality of patient care. The majority of work in the\nmedical AI community focuses on a diagnosis setting that is mainly relevant for\nautonomous operation. Practical decision support should, however, go beyond\nplain diagnosis and provide explanations. This paper provides an overview of\nworks towards explainable, DL-based decision support in medical applications\nwith the example of skin cancer diagnosis from clinical, dermoscopic and\nhistopathologic images. Analysis reveals that comparably little attention is\npayed to the explanation of histopathologic skin images and that current work\nis dominated by visual relevance maps as well as dermoscopic feature\nidentification. We conclude that future work should focus on meeting the\nstakeholder's cognitive concepts, providing exhaustive explanations that\ncombine global and local approaches and leverage diverse modalities. Moreover,\nthe possibility to intervene and guide models in case of misbehaviour is\nidentified as a major step towards successful deployment of AI as DL-based DSS\nand beyond.\n", "versions": [{"version": "v1", "created": "Tue, 2 Mar 2021 11:07:49 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Lucieri", "Adriano", ""], ["Dengel", "Andreas", ""], ["Ahmed", "Sheraz", ""]]}, {"id": "2103.05113", "submitter": "Erim Yanik", "authors": "Erim Yanik, Xavier Intes, Uwe Kruger, Pingkun Yan, David Miller, Brian\n  Van Voorst, Basiel Makled, Jack Norfleet, Suvranu De", "title": "Deep Neural Networks for the Assessment of Surgical Skills: A Systematic\n  Review", "comments": "23 pages, 3 figures, 3 tables, Journal Paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surgical training in medical school residency programs has followed the\napprenticeship model. The learning and assessment process is inherently\nsubjective and time-consuming. Thus, there is a need for objective methods to\nassess surgical skills. Here, we use the Preferred Reporting Items for\nSystematic Reviews and Meta-Analyses (PRISMA) guidelines to systematically\nsurvey the literature on the use of Deep Neural Networks for automated and\nobjective surgical skill assessment, with a focus on kinematic data as putative\nmarkers of surgical competency. There is considerable recent interest in deep\nneural networks (DNN) due to the availability of powerful algorithms, multiple\ndatasets, some of which are publicly available, as well as efficient\ncomputational hardware to train and host them. We have reviewed 530 papers, of\nwhich we selected 25 for this systematic review. Based on this review, we\nconcluded that DNNs are powerful tools for automated, objective surgical skill\nassessment using both kinematic and video data. The field would benefit from\nlarge, publicly available, annotated datasets that are representative of the\nsurgical trainee and expert demographics and multimodal data beyond kinematics\nand videos.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 10:08:37 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Yanik", "Erim", ""], ["Intes", "Xavier", ""], ["Kruger", "Uwe", ""], ["Yan", "Pingkun", ""], ["Miller", "David", ""], ["Van Voorst", "Brian", ""], ["Makled", "Basiel", ""], ["Norfleet", "Jack", ""], ["De", "Suvranu", ""]]}, {"id": "2103.05121", "submitter": "Jevgenij Gamper", "authors": "Jevgenij Gamper, Nasir Rajpoot", "title": "Multiple Instance Captioning: Learning Representations from\n  Histopathology Textbooks and Articles", "comments": "Accepted at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present ARCH, a computational pathology (CP) multiple instance captioning\ndataset to facilitate dense supervision of CP tasks. Existing CP datasets focus\non narrow tasks; ARCH on the other hand contains dense diagnostic and\nmorphological descriptions for a range of stains, tissue types and pathologies.\nUsing intrinsic dimensionality estimation, we show that ARCH is the only CP\ndataset to (ARCH-)rival its computer vision analog MS-COCO Captions. We\nconjecture that an encoder pre-trained on dense image captions learns\ntransferable representations for most CP tasks. We support the conjecture with\nevidence that ARCH representation transfers to a variety of pathology sub-tasks\nbetter than ImageNet features or representations obtained via self-supervised\nor multi-task learning on pathology images alone. We release our best model and\ninvite other researchers to test it on their CP tasks.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 22:18:36 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Gamper", "Jevgenij", ""], ["Rajpoot", "Nasir", ""]]}, {"id": "2103.05124", "submitter": "Piotr Szwed PhD", "authors": "Piotr Szwed", "title": "Classification and Feature Transformation with Fuzzy Cognitive Maps", "comments": null, "journal-ref": null, "doi": "10.1016/j.asoc.2021.107271", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fuzzy Cognitive Maps (FCMs) are considered a soft computing technique\ncombining elements of fuzzy logic and recurrent neural networks. They found\nmultiple application in such domains as modeling of system behavior, prediction\nof time series, decision making and process control. Less attention, however,\nhas been turned towards using them in pattern classification. In this work we\npropose an FCM based classifier with a fully connected map structure. In\ncontrast to methods that expect reaching a steady system state during\nreasoning, we chose to execute a few FCM iterations (steps) before collecting\noutput labels. Weights were learned with a gradient algorithm and logloss or\ncross-entropy were used as the cost function. Our primary goal was to verify,\nwhether such design would result in a descent general purpose classifier, with\nperformance comparable to off the shelf classical methods. As the preliminary\nresults were promising, we investigated the hypothesis that the performance of\n$d$-step classifier can be attributed to a fact that in previous $d-1$ steps it\ntransforms the feature space by grouping observations belonging to a given\nclass, so that they became more compact and separable. To verify this\nhypothesis we calculated three clustering scores for the transformed feature\nspace. We also evaluated performance of pipelines built from FCM-based data\ntransformer followed by a classification algorithm. The standard statistical\nanalyzes confirmed both the performance of FCM based classifier and its\ncapability to improve data. The supporting prototype software was implemented\nin Python using TensorFlow library.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 22:26:24 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Szwed", "Piotr", ""]]}, {"id": "2103.05127", "submitter": "Jian Pei", "authors": "Xia Hu, Lingyang Chu, Jian Pei, Weiqing Liu and Jiang Bian", "title": "Model Complexity of Deep Learning: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Model complexity is a fundamental problem in deep learning. In this paper we\nconduct a systematic overview of the latest studies on model complexity in deep\nlearning. Model complexity of deep learning can be categorized into expressive\ncapacity and effective model complexity. We review the existing studies on\nthose two categories along four important factors, including model framework,\nmodel size, optimization process and data complexity. We also discuss the\napplications of deep learning model complexity including understanding model\ngeneralization capability, model optimization, and model selection and design.\nWe conclude by proposing several interesting future directions.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 22:39:32 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Hu", "Xia", ""], ["Chu", "Lingyang", ""], ["Pei", "Jian", ""], ["Liu", "Weiqing", ""], ["Bian", "Jiang", ""]]}, {"id": "2103.05132", "submitter": "Bonaventure F. P. Dossou", "authors": "Bonaventure F. P. Dossou and Mohammed Sabry", "title": "AfriVEC: Word Embedding Models for African Languages. Case Study of Fon\n  and Nobiin", "comments": null, "journal-ref": "Africa NLP, EACL 2021", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  From Word2Vec to GloVe, word embedding models have played key roles in the\ncurrent state-of-the-art results achieved in Natural Language Processing.\nDesigned to give significant and unique vectorized representations of words and\nentities, those models have proven to efficiently extract similarities and\nestablish relationships reflecting semantic and contextual meaning among words\nand entities. African Languages, representing more than 31% of the worldwide\nspoken languages, have recently been subject to lots of research. However, to\nthe best of our knowledge, there are currently very few to none word embedding\nmodels for those languages words and entities, and none for the languages under\nstudy in this paper. After describing Glove, Word2Vec, and Poincar\\'e\nembeddings functionalities, we build Word2Vec and Poincar\\'e word embedding\nmodels for Fon and Nobiin, which show promising results. We test the\napplicability of transfer learning between these models as a landmark for\nAfrican Languages to jointly involve in mitigating the scarcity of their\nresources, and attempt to provide linguistic and social interpretations of our\nresults. Our main contribution is to arouse more interest in creating word\nembedding models proper to African Languages, ready for use, and that can\nsignificantly improve the performances of Natural Language Processing\ndownstream tasks on them. The official repository and implementation is at\nhttps://github.com/bonaventuredossou/afrivec\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 22:58:20 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 05:35:22 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Dossou", "Bonaventure F. P.", ""], ["Sabry", "Mohammed", ""]]}, {"id": "2103.05137", "submitter": "Ali Borji", "authors": "Ali Borji", "title": "Contemplating real-world object classification", "comments": "to appear in iclr 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep object recognition models have been very successful over benchmark\ndatasets such as ImageNet. How accurate and robust are they to distribution\nshifts arising from natural and synthetic variations in datasets? Prior\nresearch on this problem has primarily focused on ImageNet variations (e.g.,\nImageNetV2, ImageNet-A). To avoid potential inherited biases in these studies,\nwe take a different approach. Specifically, we reanalyze the ObjectNet dataset\nrecently proposed by Barbu et al. containing objects in daily life situations.\nThey showed a dramatic performance drop of the state of the art object\nrecognition models on this dataset. Due to the importance and implications of\ntheir results regarding the generalization ability of deep models, we take a\nsecond look at their analysis. We find that applying deep models to the\nisolated objects, rather than the entire scene as is done in the original\npaper, results in around 20-30% performance improvement. Relative to the\nnumbers reported in Barbu et al., around 10-15% of the performance loss is\nrecovered, without any test time data augmentation. Despite this gain, however,\nwe conclude that deep models still suffer drastically on the ObjectNet dataset.\nWe also investigate the robustness of models against synthetic image\nperturbations such as geometric transformations (e.g., scale, rotation,\ntranslation), natural image distortions (e.g., impulse noise, blur) as well as\nadversarial attacks (e.g., FGSM and PGD-5). Our results indicate that limiting\nthe object area as much as possible (i.e., from the entire image to the\nbounding box to the segmentation mask) leads to consistent improvement in\naccuracy and robustness.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 23:29:59 GMT"}, {"version": "v2", "created": "Sat, 27 Mar 2021 18:50:02 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Borji", "Ali", ""]]}, {"id": "2103.05140", "submitter": "Wenzhao Lian", "authors": "Wenzhao Lian, Tim Kelch, Dirk Holz, Adam Norton, and Stefan Schaal", "title": "Benchmarking Off-The-Shelf Solutions to Robotic Assembly Tasks", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, many learning based approaches have been studied to realize\nrobotic manipulation and assembly tasks, often including vision and\nforce/tactile feedback. However, it remains frequently unclear what is the\nbaseline state-of-the-art performance and what are the bottleneck problems. In\nthis work, we evaluate some off-the-shelf (OTS) industrial solutions on a\nrecently introduced benchmark, the National Institute of Standards and\nTechnology (NIST) Assembly Task Boards. A set of assembly tasks are introduced\nand baseline methods are provided to understand their intrinsic difficulty.\nMultiple sensor-based robotic solutions are then evaluated, including hybrid\nforce/motion control and 2D/3D pattern matching algorithms. An end-to-end\nintegrated solution that accomplishes the tasks is also provided. The results\nand findings throughout the study reveal a few noticeable factors that impede\nthe adoptions of the OTS solutions: expertise dependent, limited applicability,\nlack of interoperability, no scene awareness or error recovery mechanisms, and\nhigh cost. This paper also provides a first attempt of an objective benchmark\nperformance on the NIST Assembly Task Boards as a reference comparison for\nfuture works on this problem.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 23:46:48 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Lian", "Wenzhao", ""], ["Kelch", "Tim", ""], ["Holz", "Dirk", ""], ["Norton", "Adam", ""], ["Schaal", "Stefan", ""]]}, {"id": "2103.05147", "submitter": "Qingfeng Lan", "authors": "Qingfeng Lan, A. Rupam Mahmood", "title": "Model-free Policy Learning with Reward Gradients", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Policy gradient methods estimate the gradient of a policy objective solely\nbased on either the likelihood ratio (LR) estimator or the reparameterization\n(RP) estimator for estimating gradients. Many policy gradient methods based on\nthe LR estimator can be unified under the policy gradient theorem (Sutton et\nal., 2000). However, such a unifying theorem does not exist for policy gradient\nmethods based on the RP estimator. Moreover, no existing method requires and\nuses both estimators beyond a trivial interpolation between them. In this\npaper, we provide a theoretical framework that unifies several existing policy\ngradient methods based on the RP estimator. Utilizing our framework, we\nintroduce a novel strategy to compute the policy gradient that, for the first\ntime, incorporates both the LR and RP estimators and can be unbiased only when\nboth estimators are present. Based on this strategy, we develop a new on-policy\nalgorithm called the Reward Policy Gradient algorithm, which is the first\nmodel-free policy gradient method to utilize reward gradients. Using an\nidealized environment, we show that policy gradient solely based on the RP\nestimator for rewards are biased even with true rewards whereas our combined\nestimator is not. Finally, we show that our method either performs comparably\nwith or outperforms Proximal Policy Optimization -- an LR-based on-policy\nmethod -- on several continuous control tasks.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 00:14:13 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Lan", "Qingfeng", ""], ["Mahmood", "A. Rupam", ""]]}, {"id": "2103.05152", "submitter": "Ahmed Taha", "authors": "Ahmed Taha, Abhinav Shrivastava, Larry Davis", "title": "Knowledge Evolution in Neural Networks", "comments": "CVPR Oral 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning relies on the availability of a large corpus of data (labeled\nor unlabeled). Thus, one challenging unsettled question is: how to train a deep\nnetwork on a relatively small dataset? To tackle this question, we propose an\nevolution-inspired training approach to boost performance on relatively small\ndatasets. The knowledge evolution (KE) approach splits a deep network into two\nhypotheses: the fit-hypothesis and the reset-hypothesis. We iteratively evolve\nthe knowledge inside the fit-hypothesis by perturbing the reset-hypothesis for\nmultiple generations. This approach not only boosts performance, but also\nlearns a slim network with a smaller inference cost. KE integrates seamlessly\nwith both vanilla and residual convolutional networks. KE reduces both\noverfitting and the burden for data collection.\n  We evaluate KE on various network architectures and loss functions. We\nevaluate KE using relatively small datasets (e.g., CUB-200) and randomly\ninitialized deep networks. KE achieves an absolute 21% improvement margin on a\nstate-of-the-art baseline. This performance improvement is accompanied by a\nrelative 73% reduction in inference cost. KE achieves state-of-the-art results\non classification and metric learning benchmarks. Code available at\nhttp://bit.ly/3uLgwYb\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 00:25:34 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Taha", "Ahmed", ""], ["Shrivastava", "Abhinav", ""], ["Davis", "Larry", ""]]}, {"id": "2103.05154", "submitter": "Daniel Omeiza A", "authors": "Daniel Omeiza, Helena Webb, Marina Jirotka, Lars Kunze", "title": "Explanations in Autonomous Driving: A Survey", "comments": "18 pages, 5 Tables and 3 Figures. Submitted to the IEEE Transaction\n  on Intelligent Transportation Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The automotive industry is seen to have witnessed an increasing level of\ndevelopment in the past decades; from manufacturing manually operated vehicles\nto manufacturing vehicles with high level of automation. With the recent\ndevelopments in Artificial Intelligence (AI), automotive companies now employ\nhigh performance AI models to enable vehicles to perceive their environment and\nmake driving decisions with little or no influence from a human. With the hope\nto deploy autonomous vehicles (AV) on a commercial scale, the acceptance of AV\nby society becomes paramount and may largely depend on their degree of\ntransparency, trustworthiness, and compliance to regulations. The assessment of\nthese acceptance requirements can be facilitated through the provision of\nexplanations for AVs' behaviour. Explainability is therefore seen as an\nimportant requirement for AVs. AVs should be able to explain what they have\n'seen', done and might do in environments where they operate. In this paper, we\nprovide a comprehensive survey of the existing work in explainable autonomous\ndriving. First, we open by providing a motivation for explanations and\nexamining existing standards related to AVs. Second, we identify and categorise\nthe different stakeholders involved in the development, use, and regulation of\nAVs and show their perceived need for explanation. Third, we provide a taxonomy\nof explanations and reviewed previous work on explanation in the different AV\noperations. Finally, we draw a close by pointing out pertinent challenges and\nfuture research directions. This survey serves to provide fundamental knowledge\nrequired of researchers who are interested in explanation in autonomous\ndriving.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 00:31:30 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 15:51:59 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Omeiza", "Daniel", ""], ["Webb", "Helena", ""], ["Jirotka", "Marina", ""], ["Kunze", "Lars", ""]]}, {"id": "2103.05158", "submitter": "Hakdong Kim", "authors": "Hakdong Kim, Heonyeong Lim, Minkyu Jee, Yurim Lee, Jisoo Jeong, Kyudam\n  Choi, MinSung Yoon, and Cheongwon Kim", "title": "Deep Learning-based High-precision Depth Map Estimation from Missing\n  Viewpoints for 360 Degree Digital Holography", "comments": "12 pages, 10 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel, convolutional neural network model to\nextract highly precise depth maps from missing viewpoints, especially well\napplicable to generate holographic 3D contents. The depth map is an essential\nelement for phase extraction which is required for synthesis of\ncomputer-generated hologram (CGH). The proposed model called the HDD Net uses\nMSE for the better performance of depth map estimation as loss function, and\nutilizes the bilinear interpolation in up sampling layer with the Relu as\nactivation function. We design and prepare a total of 8,192 multi-view images,\neach resolution of 640 by 360 for the deep learning study. The proposed model\nestimates depth maps through extracting features, up sampling. For quantitative\nassessment, we compare the estimated depth maps with the ground truths by using\nthe PSNR, ACC, and RMSE. We also compare the CGH patterns made from estimated\ndepth maps with ones made from ground truths. Furthermore, we demonstrate the\nexperimental results to test the quality of estimated depth maps through\ndirectly reconstructing holographic 3D image scenes from the CGHs.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 00:38:23 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Kim", "Hakdong", ""], ["Lim", "Heonyeong", ""], ["Jee", "Minkyu", ""], ["Lee", "Yurim", ""], ["Jeong", "Jisoo", ""], ["Choi", "Kyudam", ""], ["Yoon", "MinSung", ""], ["Kim", "Cheongwon", ""]]}, {"id": "2103.05196", "submitter": "Shaoming He", "authors": "Zichao Liu, Jiang Wang, Shaoming He, Hyo-Sang Shin and Antonios\n  Tsourdos", "title": "A Learning-Based Computational Impact Time Guidance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper investigates the problem of impact-time-control and proposes a\nlearning-based computational guidance algorithm to solve this problem. The\nproposed guidance algorithm is developed based on a general\nprediction-correction concept: the exact time-to-go under proportional\nnavigation guidance with realistic aerodynamic characteristics is estimated by\na deep neural network and a biased command to nullify the impact time error is\ndeveloped by utilizing the emerging reinforcement learning techniques. The deep\nneural network is augmented into the reinforcement learning block to resolve\nthe issue of sparse reward that has been observed in typical reinforcement\nlearning formulation. Extensive numerical simulations are conducted to support\nthe proposed algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 03:10:42 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 12:55:14 GMT"}, {"version": "v3", "created": "Fri, 28 May 2021 01:55:45 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Liu", "Zichao", ""], ["Wang", "Jiang", ""], ["He", "Shaoming", ""], ["Shin", "Hyo-Sang", ""], ["Tsourdos", "Antonios", ""]]}, {"id": "2103.05213", "submitter": "Mingyuan Meng", "authors": "Mingyuan Meng, Lei Bi, Michael Fulham, David Dagan Feng, and Jinman\n  Kim", "title": "Enhancing Medical Image Registration via Appearance Adjustment Networks", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI eess.IV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deformable image registration is fundamental for many medical image analyses.\nA key obstacle for accurate image registration is the variations in image\nappearance. Recently, deep learning-based registration methods (DLRs), using\ndeep neural networks, have computational efficiency that is several orders of\nmagnitude greater than traditional optimization-based registration methods\n(ORs). A major drawback, however, of DLRs is a disregard for the\ntarget-pair-specific optimization that is inherent in ORs and instead they rely\non a globally optimized network that is trained with a set of training samples\nto achieve faster registration. Thus, DLRs inherently have degraded ability to\nadapt to appearance variations and perform poorly, compared to ORs, when image\npairs (fixed/moving images) have large differences in appearance. Hence, we\npropose an Appearance Adjustment Network (AAN) where we leverage anatomy edges,\nthrough an anatomy-constrained loss function, to generate an anatomy-preserving\nappearance transformation. We designed the AAN so that it can be readily\ninserted into a wide range of DLRs, to reduce the appearance differences\nbetween the fixed and moving images. Our AAN and DLR's network can be trained\ncooperatively in an unsupervised and end-to-end manner. We evaluated our AAN\nwith two widely used DLRs - Voxelmorph (VM) and FAst IMage registration (FAIM)\n- on three public 3D brain magnetic resonance (MR) image datasets - IBSR18,\nMindboggle101, and LPBA40. The results show that DLRs, using the AAN, improved\nperformance and achieved higher results than state-of-the-art ORs.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 04:24:48 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Meng", "Mingyuan", ""], ["Bi", "Lei", ""], ["Fulham", "Michael", ""], ["Feng", "David Dagan", ""], ["Kim", "Jinman", ""]]}, {"id": "2103.05225", "submitter": "Harel Yedidsion", "authors": "Harel Yedidsion, Jennifer Suriadinata, Zifan Xu, Stefan Debruyn, Peter\n  Stone", "title": "A Scavenger Hunt for Service Robots", "comments": "6 pages + references + Appendix", "journal-ref": "the 2021 IEEE International Conference on Robotics and Automation\n  (ICRA), May 30 - June 5, 2021, Xi'an, China", "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Creating robots that can perform general-purpose service tasks in a\nhuman-populated environment has been a longstanding grand challenge for AI and\nRobotics research. One particularly valuable skill that is relevant to a wide\nvariety of tasks is the ability to locate and retrieve objects upon request.\nThis paper models this skill as a Scavenger Hunt (SH) game, which we formulate\nas a variation of the NP-hard stochastic traveling purchaser problem. In this\nproblem, the goal is to find a set of objects as quickly as possible, given\nprobability distributions of where they may be found. We investigate the\nperformance of several solution algorithms for the SH problem, both in\nsimulation and on a real mobile robot. We use Reinforcement Learning (RL) to\ntrain an agent to plan a minimal cost path, and show that the RL agent can\noutperform a range of heuristic algorithms, achieving near optimal performance.\nIn order to stimulate research on this problem, we introduce a publicly\navailable software stack and associated website that enable users to upload\nscavenger hunts which robots can download, perform, and learn from to\ncontinually improve their performance on future hunts.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 05:06:47 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 05:47:08 GMT"}, {"version": "v3", "created": "Mon, 29 Mar 2021 20:57:58 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Yedidsion", "Harel", ""], ["Suriadinata", "Jennifer", ""], ["Xu", "Zifan", ""], ["Debruyn", "Stefan", ""], ["Stone", "Peter", ""]]}, {"id": "2103.05247", "submitter": "Kevin Lu", "authors": "Kevin Lu, Aditya Grover, Pieter Abbeel, Igor Mordatch", "title": "Pretrained Transformers as Universal Computation Engines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate the capability of a transformer pretrained on natural language\nto generalize to other modalities with minimal finetuning -- in particular,\nwithout finetuning of the self-attention and feedforward layers of the residual\nblocks. We consider such a model, which we call a Frozen Pretrained Transformer\n(FPT), and study finetuning it on a variety of sequence classification tasks\nspanning numerical computation, vision, and protein fold prediction. In\ncontrast to prior works which investigate finetuning on the same modality as\nthe pretraining dataset, we show that pretraining on natural language can\nimprove performance and compute efficiency on non-language downstream tasks.\nAdditionally, we perform an analysis of the architecture, comparing the\nperformance of a random initialized transformer to a random LSTM. Combining the\ntwo insights, we find language-pretrained transformers can obtain strong\nperformance on a variety of non-language tasks.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 06:39:56 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 17:34:46 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Lu", "Kevin", ""], ["Grover", "Aditya", ""], ["Abbeel", "Pieter", ""], ["Mordatch", "Igor", ""]]}, {"id": "2103.05266", "submitter": "Yunfeng Diao", "authors": "Yunfeng Diao and Tianjia Shao and Yong-Liang Yang and Kun Zhou and He\n  Wang", "title": "BASAR:Black-box Attack on Skeletal Action Recognition", "comments": "Accepted in CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Skeletal motion plays a vital role in human activity recognition as either an\nindependent data source or a complement. The robustness of skeleton-based\nactivity recognizers has been questioned recently, which shows that they are\nvulnerable to adversarial attacks when the full-knowledge of the recognizer is\naccessible to the attacker. However, this white-box requirement is overly\nrestrictive in most scenarios and the attack is not truly threatening. In this\npaper, we show that such threats do exist under black-box settings too. To this\nend, we propose the first black-box adversarial attack method BASAR. Through\nBASAR, we show that adversarial attack is not only truly a threat but also can\nbe extremely deceitful, because on-manifold adversarial samples are rather\ncommon in skeletal motions, in contrast to the common belief that adversarial\nsamples only exist off-manifold. Through exhaustive evaluation and comparison,\nwe show that BASAR can deliver successful attacks across models, data, and\nattack modes. Through harsh perceptual studies, we show that it achieves\neffective yet imperceptible attacks. By analyzing the attack on different\nactivity recognizers, BASAR helps identify the potential causes of their\nvulnerability and provides insights on what classifiers are likely to be more\nrobust against attack. Code is available at\nhttps://github.com/realcrane/BASAR-Black-box-Attack-on-Skeletal-Action-Recognition.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 07:29:35 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 15:44:58 GMT"}, {"version": "v3", "created": "Fri, 19 Mar 2021 07:48:47 GMT"}, {"version": "v4", "created": "Thu, 29 Apr 2021 11:23:45 GMT"}, {"version": "v5", "created": "Thu, 13 May 2021 08:34:51 GMT"}, {"version": "v6", "created": "Mon, 26 Jul 2021 00:58:45 GMT"}], "update_date": "2021-07-27", "authors_parsed": [["Diao", "Yunfeng", ""], ["Shao", "Tianjia", ""], ["Yang", "Yong-Liang", ""], ["Zhou", "Kun", ""], ["Wang", "He", ""]]}, {"id": "2103.05277", "submitter": "Kinjal Basu", "authors": "Rohan Ramanath, Sathiya Keerthi, Yao Pan, Konstantin Salomatin, Kinjal\n  Basu", "title": "Efficient Algorithms for Global Inference in Internet Marketplaces", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Matching demand to supply in internet marketplaces (e-commerce, ride-sharing,\nfood delivery, professional services, advertising) is a global inference\nproblem that can be formulated as a Linear Program (LP) with (millions of)\ncoupling constraints and (up to a billion) non-coupling polytope constraints.\nUntil recently, solving such problems on web-scale data with an LP formulation\nwas intractable. Recent work (Basu et al., 2020) developed a dual\ndecomposition-based approach to solve such problems when the polytope\nconstraints are simple. In this work, we motivate the need to go beyond these\nsimple polytopes and show real-world internet marketplaces that require more\ncomplex structured polytope constraints. We expand on the recent literature\nwith novel algorithms that are more broadly applicable to global inference\nproblems. We derive an efficient incremental algorithm using a theoretical\ninsight on the nature of solutions on the polytopes to project onto any\narbitrary polytope, that shows massive improvements in performance. Using\nbetter optimization routines along with an adaptive algorithm to control the\nsmoothness of the objective, improves the speed of the solution even further.\nWe showcase the efficacy of our approach via experimental results on web-scale\nmarketplace data.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 08:00:58 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 16:48:54 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Ramanath", "Rohan", ""], ["Keerthi", "Sathiya", ""], ["Pan", "Yao", ""], ["Salomatin", "Konstantin", ""], ["Basu", "Kinjal", ""]]}, {"id": "2103.05293", "submitter": "Tianhao Zhang", "authors": "Tianhao Zhang and Yueheng Li and Shuai Li and Qiwei Ye and Chen Wang\n  and Guangming Xie", "title": "Decentralized Circle Formation Control for Fish-like Robots in the\n  Real-world via Reinforcement Learning", "comments": "to be published in ICRA2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, the circle formation control problem is addressed for a group\nof cooperative underactuated fish-like robots involving unknown nonlinear\ndynamics and disturbances. Based on the reinforcement learning and cognitive\nconsistency theory, we propose a decentralized controller without the knowledge\nof the dynamics of the fish-like robots. The proposed controller can be\ntransferred from simulation to reality. It is only trained in our established\nsimulation environment, and the trained controller can be deployed to real\nrobots without any manual tuning. Simulation results confirm that the proposed\nmodel-free robust formation control method is scalable with respect to the\ngroup size of the robots and outperforms other representative RL algorithms.\nSeveral experiments in the real world verify the effectiveness of our RL-based\napproach for circle formation control.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 08:38:28 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Zhang", "Tianhao", ""], ["Li", "Yueheng", ""], ["Li", "Shuai", ""], ["Ye", "Qiwei", ""], ["Wang", "Chen", ""], ["Xie", "Guangming", ""]]}, {"id": "2103.05343", "submitter": "Mario Coppola", "authors": "Mario Coppola, Jian Guo, Eberhard Gill, Guido C. H. E. de Croon", "title": "A model-based framework for learning transparent swarm behaviors", "comments": "23 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a model-based framework to automatically and efficiently\ndesign understandable and verifiable behaviors for swarms of robots. The\nframework is based on the automatic extraction of two distinct models: 1) a\nneural network model trained to estimate the relationship between the robots'\nsensor readings and the global performance of the swarm, and 2) a probabilistic\nstate transition model that explicitly models the local state transitions\n(i.e., transitions in observations from the perspective of a single robot in\nthe swarm) given a policy. The models can be trained from a data set of\nsimulated runs featuring random policies. The first model is used to\nautomatically extract a set of local states that are expected to maximize the\nglobal performance. These local states are referred to as desired local states.\nThe second model is used to optimize a stochastic policy so as to increase the\nprobability that the robots in the swarm observe one of the desired local\nstates. Following these steps, the framework proposed in this paper can\nefficiently lead to effective controllers. This is tested on four case studies,\nfeaturing aggregation and foraging tasks. Importantly, thanks to the models,\nthe framework allows us to understand and inspect a swarm's behavior. To this\nend, we propose verification checks to identify some potential issues that may\nprevent the swarm from achieving the desired global objective. In addition, we\nexplore how the framework can be used in combination with a \"standard\"\nevolutionary robotics strategy (i.e., where performance is measured via\nsimulation), or with online learning.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 10:45:57 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Coppola", "Mario", ""], ["Guo", "Jian", ""], ["Gill", "Eberhard", ""], ["de Croon", "Guido C. H. E.", ""]]}, {"id": "2103.05349", "submitter": "Ali Shafti", "authors": "Nat Wannawas, Ali Shafti, A. Aldo Faisal", "title": "I am Robot: Neuromuscular Reinforcement Learning to Actuate Human Limbs\n  through Functional Electrical Stimulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human movement disorders or paralysis lead to the loss of control of muscle\nactivation and thus motor control. Functional Electrical Stimulation (FES) is\nan established and safe technique for contracting muscles by stimulating the\nskin above a muscle to induce its contraction. However, an open challenge\nremains on how to restore motor abilities to human limbs through FES, as the\nproblem of controlling the stimulation is unclear. We are taking a robotics\nperspective on this problem, by developing robot learning algorithms that\ncontrol the ultimate humanoid robot, the human body, through electrical muscle\nstimulation. Human muscles are not trivial to control as actuators due to their\nforce production being non-stationary as a result of fatigue and other internal\nstate changes, in contrast to robot actuators which are well-understood and\nstationary over broad operation ranges. We present our Deep Reinforcement\nLearning approach to the control of human muscles with FES, using a recurrent\nneural network for dynamic state representation, to overcome the unobserved\nelements of the behaviour of human muscles under external stimulation. We\ndemonstrate our technique both in neuromuscular simulations but also\nexperimentally on a human. Our results show that our controller can learn to\nmanipulate human muscles, applying appropriate levels of stimulation to achieve\nthe given tasks while compensating for advancing muscle fatigue which arises\nthroughout the tasks. Additionally, our technique can learn quickly enough to\nbe implemented in real-world human-in-the-loop settings.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 10:58:51 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Wannawas", "Nat", ""], ["Shafti", "Ali", ""], ["Faisal", "A. Aldo", ""]]}, {"id": "2103.05376", "submitter": "Lu Yang", "authors": "Lu Yang, Hongbang Liu, Jinghao Zhou, Lingqiao Liu, Lei Zhang, Peng\n  Wang and Yanning Zhang", "title": "Pluggable Weakly-Supervised Cross-View Learning for Accurate Vehicle\n  Re-Identification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning cross-view consistent feature representation is the key for accurate\nvehicle Re-identification (ReID), since the visual appearance of vehicles\nchanges significantly under different viewpoints. To this end, most existing\napproaches resort to the supervised cross-view learning using extensive extra\nviewpoints annotations, which however, is difficult to deploy in real\napplications due to the expensive labelling cost and the continous viewpoint\nvariation that makes it hard to define discrete viewpoint labels. In this\nstudy, we present a pluggable Weakly-supervised Cross-View Learning (WCVL)\nmodule for vehicle ReID. Through hallucinating the cross-view samples as the\nhardest positive counterparts in feature domain, we can learn the consistent\nfeature representation via minimizing the cross-view feature distance based on\nvehicle IDs only without using any viewpoint annotation. More importantly, the\nproposed method can be seamlessly plugged into most existing vehicle ReID\nbaselines for cross-view learning without re-training the baselines. To\ndemonstrate its efficacy, we plug the proposed method into a bunch of\noff-the-shelf baselines and obtain significant performance improvement on four\npublic benchmark datasets, i.e., VeRi-776, VehicleID, VRIC and VRAI.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 11:51:09 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Yang", "Lu", ""], ["Liu", "Hongbang", ""], ["Zhou", "Jinghao", ""], ["Liu", "Lingqiao", ""], ["Zhang", "Lei", ""], ["Wang", "Peng", ""], ["Zhang", "Yanning", ""]]}, {"id": "2103.05401", "submitter": "Marc Tuscher", "authors": "Marc Tuscher, Julian H\\\"orz, Danny Driess, Marc Toussaint", "title": "Deep 6-DoF Tracking of Unknown Objects for Reactive Grasping", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Robotic manipulation of unknown objects is an important field of research.\nPractical applications occur in many real-world settings where robots need to\ninteract with an unknown environment. We tackle the problem of reactive\ngrasping by proposing a method for unknown object tracking, grasp point\nsampling and dynamic trajectory planning. Our object tracking method combines\nSiamese Networks with an Iterative Closest Point approach for pointcloud\nregistration into a method for 6-DoF unknown object tracking. The method does\nnot require further training and is robust to noise and occlusion. We propose a\nrobotic manipulation system, which is able to grasp a wide variety of formerly\nunseen objects and is robust against object perturbations and inferior grasping\npoints.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 12:51:17 GMT"}, {"version": "v2", "created": "Wed, 10 Mar 2021 05:23:11 GMT"}, {"version": "v3", "created": "Thu, 25 Mar 2021 16:18:37 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Tuscher", "Marc", ""], ["H\u00f6rz", "Julian", ""], ["Driess", "Danny", ""], ["Toussaint", "Marc", ""]]}, {"id": "2103.05434", "submitter": "Tae Wan Kim", "authors": "Tae Wan Kim, Tong (Joy) Lu, Kyusong Lee, Zhaoqi Cheng, Yanhan Tang,\n  and John Hooker", "title": "When is it permissible for artificial intelligence to lie? A trust-based\n  approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Conversational Artificial Intelligence (AI) used in industry settings can be\ntrained to closely mimic human behaviors, including lying and deception.\nHowever, lying is often a necessary part of negotiation. To address this, we\ndevelop a normative framework for when it is ethical or unethical for a\nconversational AI to lie to humans, based on whether there is what we call\n\"invitation of trust\" in a particular scenario. Importantly, cultural norms\nplay an important role in determining whether there is invitation of trust\nacross negotiation settings, and thus an AI trained in one culture may not be\ngeneralizable to others. Moreover, individuals may have different expectations\nregarding the invitation of trust and propensity to lie for human vs. AI\nnegotiators, and these expectations may vary across cultures as well. Finally,\nwe outline how a conversational chatbot can be trained to negotiate ethically\nby applying autoregressive models to large dialog and negotiations datasets.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 14:24:29 GMT"}, {"version": "v2", "created": "Sun, 14 Mar 2021 00:24:07 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Kim", "Tae Wan", "", "Joy"], ["Tong", "", "", "Joy"], ["Lu", "", ""], ["Lee", "Kyusong", ""], ["Cheng", "Zhaoqi", ""], ["Tang", "Yanhan", ""], ["Hooker", "John", ""]]}, {"id": "2103.05481", "submitter": "Damien Pellier", "authors": "Damien Pellier, Humbert Fiorino", "title": "From Classical to Hierarchical: benchmarks for the HTN Track of the\n  International Planning Competition", "comments": null, "journal-ref": "Proceedings of the International Planning Competition, ICAPS,\n  Nancy, France, 2020", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this short paper, we outline nine classical benchmarks submitted to the\nfirst hierarchical planning track of the International Planning competition in\n2020. All of these benchmarks are based on the HDDL language. The choice of the\nbenchmarks was based on a questionnaire sent to the HTN community. They are the\nfollowing: Barman, Childsnack, Rover, Satellite, Blocksworld, Depots, Gripper,\nand Hiking. In the rest of the paper we give a short description of these\nbenchmarks. All are totally ordered.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 15:11:51 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Pellier", "Damien", ""], ["Fiorino", "Humbert", ""]]}, {"id": "2103.05552", "submitter": "Tharindu Ranasinghe Mr", "authors": "Tommi Jauhiainen, Tharindu Ranasinghe, Marcos Zampieri", "title": "Comparing Approaches to Dravidian Language Identification", "comments": "Accepted to VarDial 2021 @ EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper describes the submissions by team HWR to the Dravidian Language\nIdentification (DLI) shared task organized at VarDial 2021 workshop. The DLI\ntraining set includes 16,674 YouTube comments written in Roman script\ncontaining code-mixed text with English and one of the three South Dravidian\nlanguages: Kannada, Malayalam, and Tamil. We submitted results generated using\ntwo models, a Naive Bayes classifier with adaptive language models, which has\nshown to obtain competitive performance in many language and dialect\nidentification tasks, and a transformer-based model which is widely regarded as\nthe state-of-the-art in a number of NLP tasks. Our first submission was sent in\nthe closed submission track using only the training set provided by the shared\ntask organisers, whereas the second submission is considered to be open as it\nused a pretrained model trained with external data. Our team attained shared\nsecond position in the shared task with the submission based on Naive Bayes.\nOur results reinforce the idea that deep learning methods are not as\ncompetitive in language identification related tasks as they are in many other\ntext classification tasks.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 16:58:55 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Jauhiainen", "Tommi", ""], ["Ranasinghe", "Tharindu", ""], ["Zampieri", "Marcos", ""]]}, {"id": "2103.05561", "submitter": "Biplav Srivastava", "authors": "Biplav Srivastava", "title": "Did Chatbots Miss Their 'Apollo Moment'? A Survey of the Potential, Gaps\n  and Lessons from Using Collaboration Assistants During COVID-19", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Artificial Intelligence (AI) technologies have long been positioned as a tool\nto provide crucial data-driven decision support to people. In this survey\npaper, we look at how AI in general, and collaboration assistants (CAs or\nchatbots for short) in particular, have been used during a true global exigency\n- the COVID-19 pandemic. The key observation is that chatbots missed their\n\"Apollo moment\" when they could have really provided contextual, personalized,\nreliable decision support at scale that the state-of-the-art makes possible. We\nreview the existing capabilities that are feasible and methods, identify the\npotential that chatbots could have met, the use-cases they were deployed on,\nthe challenges they faced and gaps that persisted, and draw lessons that, if\nimplemented, would make them more relevant in future health emergencies.\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 19:08:54 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Srivastava", "Biplav", ""]]}, {"id": "2103.05563", "submitter": "Ahmet Orun", "authors": "Ahmet Orun", "title": "Low-level cognitive skill transfer between two individuals' minds via\n  computer game-based framework", "comments": "8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The novel technique introduced here aims to accomplish the first stage of\ntransferring low-level cognitive skills between two individuals (e.g. from\nexpert to learner) to ease the consecutive higher level declarative learning\nprocess for the target \"learner\" individual in a game environment. Such\nlow-level cognitive skill is associated with the procedural knowledge and\nestablished at low-level of mind which can be unveiled and transferred by only\na novel technique (rather than by a traditional educational environment ) like\na highly interactive computer game domain in which a user exposes his/her\nunconscious mind behaviors via the game-hero non-deliberately during the game\nsessions. The cognitive data exposed by the game-hero would be recorded, and\nthen be modelled by the artificial intelligence technique like Bayesian\nnetworks for an early stage of cognitive skill transfer and the cognitive\nstimuli are also generated to be used as game agents to train the learner.\n", "versions": [{"version": "v1", "created": "Wed, 3 Mar 2021 01:52:16 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Orun", "Ahmet", ""]]}, {"id": "2103.05564", "submitter": "Marco Pegoraro", "authors": "Marco Pegoraro, Merih Seran Uysal, Wil M.P. van der Aalst", "title": "PROVED: A Tool for Graph Representation and Analysis of Uncertain Event\n  Data", "comments": "11 pages, 6 figures, 1 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The discipline of process mining aims to study processes in a data-driven\nmanner by analyzing historical process executions, often employing Petri nets.\nEvent data, extracted from information systems (e.g. SAP), serve as the\nstarting point for process mining. Recently, novel types of event data have\ngathered interest among the process mining community, including uncertain event\ndata. Uncertain events, process traces and logs contain attributes that are\ncharacterized by quantified imprecisions, e.g., a set of possible attribute\nvalues. The PROVED tool helps to explore, navigate and analyze such uncertain\nevent data by abstracting the uncertain information using behavior graphs and\nnets, which have Petri nets semantics. Based on these constructs, the tool\nenables discovery and conformance checking.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 17:11:54 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Pegoraro", "Marco", ""], ["Uysal", "Merih Seran", ""], ["van der Aalst", "Wil M. P.", ""]]}, {"id": "2103.05577", "submitter": "Sofiene Jerbi", "authors": "Sofiene Jerbi, Casper Gyurik, Simon Marshall, Hans J. Briegel, Vedran\n  Dunjko", "title": "Variational quantum policies for reinforcement learning", "comments": "27 pages, 8 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational quantum circuits have recently gained popularity as quantum\nmachine learning models. While considerable effort has been invested to train\nthem in supervised and unsupervised learning settings, relatively little\nattention has been given to their potential use in reinforcement learning. In\nthis work, we leverage the understanding of quantum policy gradient algorithms\nin a number of ways. First, we investigate how to construct and train\nreinforcement learning policies based on variational quantum circuits. We\npropose several designs for quantum policies, provide their learning\nalgorithms, and test their performance on classical benchmarking environments.\nSecond, we show the existence of task environments with a provable separation\nin performance between quantum learning agents and any polynomial-time\nclassical learner, conditioned on the widely-believed classical hardness of the\ndiscrete logarithm problem. We also consider more natural settings, in which we\nshow an empirical quantum advantage of our quantum policies over standard\nneural-network policies. Our results constitute a first step towards\nestablishing a practical near-term quantum advantage in a reinforcement\nlearning setting. Additionally, we believe that some of our design choices for\nvariational quantum policies may also be beneficial to other models based on\nvariational quantum circuits, such as quantum classifiers and quantum\nregression models.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 17:33:09 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Jerbi", "Sofiene", ""], ["Gyurik", "Casper", ""], ["Marshall", "Simon", ""], ["Briegel", "Hans J.", ""], ["Dunjko", "Vedran", ""]]}, {"id": "2103.05612", "submitter": "Elsa Riachi", "authors": "Elsa Riachi, Muhammad Mamdani, Michael Fralick, Frank Rudzicz", "title": "Challenges for Reinforcement Learning in Healthcare", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many healthcare decisions involve navigating through a multitude of treatment\noptions in a sequential and iterative manner to find an optimal treatment\npathway with the goal of an optimal patient outcome. Such optimization problems\nmay be amenable to reinforcement learning. A reinforcement learning agent could\nbe trained to provide treatment recommendations for physicians, acting as a\ndecision support tool. However, a number of difficulties arise when using RL\nbeyond benchmark environments, such as specifying the reward function, choosing\nan appropriate state representation and evaluating the learned policy.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 18:34:54 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Riachi", "Elsa", ""], ["Mamdani", "Muhammad", ""], ["Fralick", "Michael", ""], ["Rudzicz", "Frank", ""]]}, {"id": "2103.05633", "submitter": "Mohammad Yaghini", "authors": "Hengrui Jia, Mohammad Yaghini, Christopher A. Choquette-Choo, Natalie\n  Dullerud, Anvith Thudi, Varun Chandrasekaran, Nicolas Papernot", "title": "Proof-of-Learning: Definitions and Practice", "comments": "To appear in the 42nd IEEE Symposium on Security and Privacy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training machine learning (ML) models typically involves expensive iterative\noptimization. Once the model's final parameters are released, there is\ncurrently no mechanism for the entity which trained the model to prove that\nthese parameters were indeed the result of this optimization procedure. Such a\nmechanism would support security of ML applications in several ways. For\ninstance, it would simplify ownership resolution when multiple parties contest\nownership of a specific model. It would also facilitate the distributed\ntraining across untrusted workers where Byzantine workers might otherwise mount\na denial-of-service by returning incorrect model updates.\n  In this paper, we remediate this problem by introducing the concept of\nproof-of-learning in ML. Inspired by research on both proof-of-work and\nverified computations, we observe how a seminal training algorithm, stochastic\ngradient descent, accumulates secret information due to its stochasticity. This\nproduces a natural construction for a proof-of-learning which demonstrates that\na party has expended the compute require to obtain a set of model parameters\ncorrectly. In particular, our analyses and experiments show that an adversary\nseeking to illegitimately manufacture a proof-of-learning needs to perform *at\nleast* as much work than is needed for gradient descent itself.\n  We also instantiate a concrete proof-of-learning mechanism in both of the\nscenarios described above. In model ownership resolution, it protects the\nintellectual property of models released publicly. In distributed training, it\npreserves availability of the training procedure. Our empirical evaluation\nvalidates that our proof-of-learning mechanism is robust to variance induced by\nthe hardware (ML accelerators) and software stacks.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 18:59:54 GMT"}], "update_date": "2021-03-10", "authors_parsed": [["Jia", "Hengrui", ""], ["Yaghini", "Mohammad", ""], ["Choquette-Choo", "Christopher A.", ""], ["Dullerud", "Natalie", ""], ["Thudi", "Anvith", ""], ["Chandrasekaran", "Varun", ""], ["Papernot", "Nicolas", ""]]}, {"id": "2103.05636", "submitter": "Jack Kendall", "authors": "Jack Kendall", "title": "A Gradient Estimator for Time-Varying Electrical Networks with\n  Non-Linear Dissipation", "comments": "12 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a method for extending the technique of equilibrium propagation\nfor estimating gradients in fixed-point neural networks to the more general\nsetting of directed, time-varying neural networks by modeling them as\nelectrical circuits. We use electrical circuit theory to construct a Lagrangian\ncapable of describing deep, directed neural networks modeled using nonlinear\ncapacitors and inductors, linear resistors and sources, and a special class of\nnonlinear dissipative elements called fractional memristors. We then derive an\nestimator for the gradient of the physical parameters of the network, such as\nsynapse conductances, with respect to an arbitrary loss function. This\nestimator is entirely local, in that it only depends on information locally\navailable to each synapse. We conclude by suggesting methods for extending\nthese results to networks of biologically plausible neurons, e.g.\nHodgkin-Huxley neurons.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 02:07:39 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Kendall", "Jack", ""]]}, {"id": "2103.05639", "submitter": "Israel Abebe Azime", "authors": "Israel Abebe Azime and Nebil Mohammed", "title": "An Amharic News Text classification Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In NLP, text classification is one of the primary problems we try to solve\nand its uses in language analyses are indisputable. The lack of labeled\ntraining data made it harder to do these tasks in low resource languages like\nAmharic. The task of collecting, labeling, annotating, and making valuable this\nkind of data will encourage junior researchers, schools, and machine learning\npractitioners to implement existing classification models in their language. In\nthis short paper, we aim to introduce the Amharic text classification dataset\nthat consists of more than 50k news articles that were categorized into 6\nclasses. This dataset is made available with easy baseline performances to\nencourage studies and better performance experiments.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 16:36:39 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Azime", "Israel Abebe", ""], ["Mohammed", "Nebil", ""]]}, {"id": "2103.05661", "submitter": "Anca Dragan", "authors": "Liting Sun, Xiaogang Jia, Anca D. Dragan", "title": "On complementing end-to-end human behavior predictors with planning", "comments": null, "journal-ref": "Robotics: Science and Systems, 2021", "doi": null, "report-no": null, "categories": "cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  High capacity end-to-end approaches for human motion (behavior) prediction\nhave the ability to represent subtle nuances in human behavior, but struggle\nwith robustness to out of distribution inputs and tail events. Planning-based\nprediction, on the other hand, can reliably output decent-but-not-great\npredictions: it is much more stable in the face of distribution shift (as we\nverify in this work), but it has high inductive bias, missing important aspects\nthat drive human decisions, and ignoring cognitive biases that make human\nbehavior suboptimal. In this work, we analyze one family of approaches that\nstrive to get the best of both worlds: use the end-to-end predictor on common\ncases, but do not rely on it for tail events / out-of-distribution inputs --\nswitch to the planning-based predictor there. We contribute an analysis of\ndifferent approaches for detecting when to make this switch, using an\nautonomous driving domain. We find that promising approaches based on\nensembling or generative modeling of the training distribution might not be\nreliable, but that there very simple methods which can perform surprisingly\nwell -- including training a classifier to pick up on tell-tale issues in\npredicted trajectories.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 19:02:45 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 01:24:55 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Sun", "Liting", ""], ["Jia", "Xiaogang", ""], ["Dragan", "Anca D.", ""]]}, {"id": "2103.05704", "submitter": "Christiane Gresse Von Wangenheim", "authors": "Daniel Baul\\'e, Christiane Gresse von Wangenheim, Aldo von Wangenheim,\n  Jean C. R. Hauck, Edson C. Vargas J\\'unior", "title": "Automatic code generation from sketches of mobile applications in\n  end-user development using Deep Learning", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A common need for mobile application development by end-users or in computing\neducation is to transform a sketch of a user interface into wireframe code\nusing App Inventor, a popular block-based programming environment. As this task\nis challenging and time-consuming, we present the Sketch2aia approach that\nautomates this process. Sketch2aia employs deep learning to detect the most\nfrequent user interface components and their position on a hand-drawn sketch\ncreating an intermediate representation of the user interface and then\nautomatically generates the App Inventor code of the wireframe. The approach\nachieves an average user interface component classification accuracy of 87,72%\nand results of a preliminary user evaluation indicate that it generates\nwireframes that closely mirror the sketches in terms of visual similarity. The\napproach has been implemented as a web tool and can be used to support the\nend-user development of mobile applications effectively and efficiently as well\nas the teaching of user interface design in K-12.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 20:32:20 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Baul\u00e9", "Daniel", ""], ["von Wangenheim", "Christiane Gresse", ""], ["von Wangenheim", "Aldo", ""], ["Hauck", "Jean C. R.", ""], ["J\u00fanior", "Edson C. Vargas", ""]]}, {"id": "2103.05723", "submitter": "Fabio Valerio Massoli", "authors": "Fabio Valerio Massoli, Donato Cafarelli, Giuseppe Amato, Fabrizio\n  Falchi", "title": "A Multi-resolution Approach to Expression Recognition in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Facial expressions play a fundamental role in human communication. Indeed,\nthey typically reveal the real emotional status of people beyond the spoken\nlanguage. Moreover, the comprehension of human affect based on visual patterns\nis a key ingredient for any human-machine interaction system and, for such\nreasons, the task of Facial Expression Recognition (FER) draws both scientific\nand industrial interest. In the recent years, Deep Learning techniques reached\nvery high performance on FER by exploiting different architectures and learning\nparadigms. In such a context, we propose a multi-resolution approach to solve\nthe FER task. We ground our intuition on the observation that often faces\nimages are acquired at different resolutions. Thus, directly considering such\nproperty while training a model can help achieve higher performance on\nrecognizing facial expressions. To our aim, we use a ResNet-like architecture,\nequipped with Squeeze-and-Excitation blocks, trained on the Affect-in-the-Wild\n2 dataset. Not being available a test set, we conduct tests and models\nselection by employing the validation set only on which we achieve more than\n90\\% accuracy on classifying the seven expressions that the dataset comprises.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 21:21:02 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Massoli", "Fabio Valerio", ""], ["Cafarelli", "Donato", ""], ["Amato", "Giuseppe", ""], ["Falchi", "Fabrizio", ""]]}, {"id": "2103.05737", "submitter": "Corban Rivera", "authors": "Edward W. Staley, Corban G.Rivera, Ashley J. Llorens", "title": "The AI Arena: A Framework for Distributed Multi-Agent Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Advances in reinforcement learning (RL) have resulted in recent breakthroughs\nin the application of artificial intelligence (AI) across many different\ndomains. An emerging landscape of development environments is making powerful\nRL techniques more accessible for a growing community of researchers. However,\nmost existing frameworks do not directly address the problem of learning in\ncomplex operating environments, such as dense urban settings or defense-related\nscenarios, that incorporate distributed, heterogeneous teams of agents. To help\nenable AI research for this important class of applications, we introduce the\nAI Arena: a scalable framework with flexible abstractions for distributed\nmulti-agent reinforcement learning. The AI Arena extends the OpenAI Gym\ninterface to allow greater flexibility in learning control policies across\nmultiple agents with heterogeneous learning strategies and localized views of\nthe environment. To illustrate the utility of our framework, we present\nexperimental results that demonstrate performance gains due to a distributed\nmulti-agent learning approach over commonly-used RL techniques in several\ndifferent learning environments.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 22:16:19 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Staley", "Edward W.", ""], ["Rivera", "Corban G.", ""], ["Llorens", "Ashley J.", ""]]}, {"id": "2103.05746", "submitter": "Andrea Bajcsy", "authors": "Andrea Bajcsy, Anand Siththaranjan, Claire J. Tomlin, Anca D. Dragan", "title": "Analyzing Human Models that Adapt Online", "comments": "ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predictive human models often need to adapt their parameters online from\nhuman data. This raises previously ignored safety-related questions for robots\nrelying on these models such as what the model could learn online and how\nquickly could it learn it. For instance, when will the robot have a confident\nestimate in a nearby human's goal? Or, what parameter initializations guarantee\nthat the robot can learn the human's preferences in a finite number of\nobservations? To answer such analysis questions, our key idea is to model the\nrobot's learning algorithm as a dynamical system where the state is the current\nmodel parameter estimate and the control is the human data the robot observes.\nThis enables us to leverage tools from reachability analysis and optimal\ncontrol to compute the set of hypotheses the robot could learn in finite time,\nas well as the worst and best-case time it takes to learn them. We demonstrate\nthe utility of our analysis tool in four human-robot domains, including\nautonomous driving and indoor navigation.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 22:38:46 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Bajcsy", "Andrea", ""], ["Siththaranjan", "Anand", ""], ["Tomlin", "Claire J.", ""], ["Dragan", "Anca D.", ""]]}, {"id": "2103.05753", "submitter": "Bradly Alicea", "authors": "Bradly Alicea, Rishabh Chakrabarty, Akshara Gopi, Anson Lim, and Jesse\n  Parent", "title": "Embodied Continual Learning Across Developmental Time Via Developmental\n  Braitenberg Vehicles", "comments": "14 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is much to learn through synthesis of Developmental Biology, Cognitive\nScience and Computational Modeling. One lesson we can learn from this\nperspective is that the initialization of intelligent programs cannot solely\nrely on manipulation of numerous parameters. Our path forward is to present a\ndesign for developmentally-inspired learning agents based on the Braitenberg\nVehicle. Using these agents to exemplify artificial embodied intelligence, we\nmove closer to modeling embodied experience and morphogenetic growth as\ncomponents of cognitive developmental capacity. We consider various factors\nregarding biological and cognitive development which influence the generation\nof adult phenotypes and the contingency of available developmental pathways.\nThese mechanisms produce emergent connectivity with shifting weights and\nadaptive network topography, thus illustrating the importance of developmental\nprocesses in training neural networks. This approach provides a blueprint for\nadaptive agent behavior that might result from a developmental approach: namely\nby exploiting critical periods or growth and acquisition, an explicitly\nembodied network architecture, and a distinction between the assembly of neural\nnetworks and active learning on these networks.\n", "versions": [{"version": "v1", "created": "Sun, 7 Mar 2021 07:22:49 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Alicea", "Bradly", ""], ["Chakrabarty", "Rishabh", ""], ["Gopi", "Akshara", ""], ["Lim", "Anson", ""], ["Parent", "Jesse", ""]]}, {"id": "2103.05760", "submitter": "Tarik A. Rashid", "authors": "Hardi M. Mohammed, Zrar Kh. Abdul, Tarik A. Rashid, Abeer Alsadoon,\n  Nebojsa Bacanin", "title": "A New K means Grey Wolf Algorithm for Engineering Problems", "comments": "15 pages. World Journal of Engineering, 2021", "journal-ref": null, "doi": "10.1108/WJE-10-2020-0527", "report-no": null, "categories": "cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Purpose: The development of metaheuristic algorithms has increased by\nresearchers to use them extensively in the field of business, science, and\nengineering. One of the common metaheuristic optimization algorithms is called\nGrey Wolf Optimization (GWO). The algorithm works based on imitation of the\nwolves' searching and the process of attacking grey wolves. The main purpose of\nthis paper to overcome the GWO problem which is trapping into local optima.\n  Design or Methodology or Approach: In this paper, the K-means clustering\nalgorithm is used to enhance the performance of the original Grey Wolf\nOptimization by dividing the population into different parts. The proposed\nalgorithm is called K-means clustering Grey Wolf Optimization (KMGWO).\n  Findings: Results illustrate the efficiency of KMGWO is superior to GWO. To\nevaluate the performance of the KMGWO, KMGWO applied to solve 10 CEC2019\nbenchmark test functions. Results prove that KMGWO is better compared to GWO.\nKMGWO is also compared to Cat Swarm Optimization (CSO), Whale Optimization\nAlgorithm-Bat Algorithm (WOA-BAT), and WOA, so, KMGWO achieves the first rank\nin terms of performance. Statistical results proved that KMGWO achieved a\nhigher significant value compared to the compared algorithms. Also, the KMGWO\nis used to solve a pressure vessel design problem and it has outperformed\nresults.\n  Originality/value: Results prove that KMGWO is superior to GWO. KMGWO is also\ncompared to cat swarm optimization (CSO), whale optimization algorithm-bat\nalgorithm (WOA-BAT), WOA, and GWO so KMGWO achieved the first rank in terms of\nperformance. Also, the KMGWO is used to solve a classical engineering problem\nand it is superior\n", "versions": [{"version": "v1", "created": "Sat, 27 Feb 2021 04:29:07 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Mohammed", "Hardi M.", ""], ["Abdul", "Zrar Kh.", ""], ["Rashid", "Tarik A.", ""], ["Alsadoon", "Abeer", ""], ["Bacanin", "Nebojsa", ""]]}, {"id": "2103.05767", "submitter": "Shao-En Weng", "authors": "Lei Chen, Shao-En Weng, Chu-Jun Peng, Hong-Han Shuai, and Wen-Huang\n  Cheng", "title": "ZYELL-NCTU NetTraffic-1.0: A Large-Scale Dataset for Real-World Network\n  Anomaly Detection", "comments": "2 pages, 3 tables, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network security has been an active research topic for long. One critical\nissue is improving the anomaly detection capability of intrusion detection\nsystems (IDSs), such as firewalls. However, existing network anomaly datasets\nare out of date (i.e., being collected many years ago) or IP-anonymized, making\nthe data characteristics differ from today's network. Therefore, this work\nintroduces a new, large-scale, and real-world dataset, ZYELL-NCTU\nNetTraffic-1.0, which is collected from the raw output of firewalls in a real\nnetwork, with the objective to advance the development of network security\nresearches.\n", "versions": [{"version": "v1", "created": "Mon, 8 Mar 2021 15:18:29 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Chen", "Lei", ""], ["Weng", "Shao-En", ""], ["Peng", "Chu-Jun", ""], ["Shuai", "Hong-Han", ""], ["Cheng", "Wen-Huang", ""]]}, {"id": "2103.05823", "submitter": "Aysja Johnson", "authors": "Aysja Johnson, Wai Keen Vong, Brenden M. Lake, Todd M. Gureckis", "title": "Fast and flexible: Human program induction in abstract reasoning tasks", "comments": "7 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Abstraction and Reasoning Corpus (ARC) is a challenging program induction\ndataset that was recently proposed by Chollet (2019). Here, we report the first\nset of results collected from a behavioral study of humans solving a subset of\ntasks from ARC (40 out of 1000). Although this subset of tasks contains\nconsiderable variation, our results showed that humans were able to infer the\nunderlying program and generate the correct test output for a novel test input\nexample, with an average of 80% of tasks solved per participant, and with 65%\nof tasks being solved by more than 80% of participants. Additionally, we find\ninteresting patterns of behavioral consistency and variability within the\naction sequences during the generation process, the natural language\ndescriptions to describe the transformations for each task, and the errors\npeople made. Our findings suggest that people can quickly and reliably\ndetermine the relevant features and properties of a task to compose a correct\nsolution. Future modeling work could incorporate these findings, potentially by\nconnecting the natural language descriptions we collected here to the\nunderlying semantics of ARC.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 02:18:21 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Johnson", "Aysja", ""], ["Vong", "Wai Keen", ""], ["Lake", "Brenden M.", ""], ["Gureckis", "Todd M.", ""]]}, {"id": "2103.05825", "submitter": "Suvir Mirchandani", "authors": "Suvir Mirchandani, Siddharth Karamcheti, Dorsa Sadigh", "title": "ELLA: Exploration through Learned Language Abstraction", "comments": "13 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building agents capable of understanding language instructions is critical to\neffective and robust human-AI collaboration. Recent work focuses on training\nthese instruction following agents via reinforcement learning in environments\nwith synthetic language; however, these instructions often define long-horizon,\nsparse-reward tasks, and learning policies requires many episodes of\nexperience. To this end, we introduce ELLA: Exploration through Learned\nLanguage Abstraction, a reward shaping approach that correlates high-level\ninstructions with simpler low-level instructions to enrich the sparse rewards\nafforded by the environment. ELLA has two key elements: 1) A termination\nclassifier that identifies when agents complete low-level instructions, and 2)\nA relevance classifier that correlates low-level instructions with success on\nhigh-level tasks. We learn the termination classifier offline from pairs of\ninstructions and terminal states. Notably, in departure from prior work in\nlanguage and abstraction, we learn the relevance classifier online, without\nrelying on an explicit decomposition of high-level instructions to low-level\ninstructions. On a suite of complex grid world environments with varying\ninstruction complexities and reward sparsity, ELLA shows a significant gain in\nsample efficiency across several environments compared to competitive\nlanguage-based reward shaping and no-shaping methods.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 02:18:46 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Mirchandani", "Suvir", ""], ["Karamcheti", "Siddharth", ""], ["Sadigh", "Dorsa", ""]]}, {"id": "2103.05847", "submitter": "Yongming He", "authors": "Yongming He, Guohua Wu, Yingwu Chen and Witold Pedrycz", "title": "A Two-stage Framework and Reinforcement Learning-based Optimization\n  Algorithms for Complex Scheduling Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There hardly exists a general solver that is efficient for scheduling\nproblems due to their diversity and complexity. In this study, we develop a\ntwo-stage framework, in which reinforcement learning (RL) and traditional\noperations research (OR) algorithms are combined together to efficiently deal\nwith complex scheduling problems. The scheduling problem is solved in two\nstages, including a finite Markov decision process (MDP) and a mixed-integer\nprogramming process, respectively. This offers a novel and general paradigm\nthat combines RL with OR approaches to solving scheduling problems, which\nleverages the respective strengths of RL and OR: The MDP narrows down the\nsearch space of the original problem through an RL method, while the\nmixed-integer programming process is settled by an OR algorithm. These two\nstages are performed iteratively and interactively until the termination\ncriterion has been met. Under this idea, two implementation versions of the\ncombination methods of RL and OR are put forward. The agile Earth observation\nsatellite scheduling problem is selected as an example to demonstrate the\neffectiveness of the proposed scheduling framework and methods. The convergence\nand generalization capability of the methods are verified by the performance of\ntraining scenarios, while the efficiency and accuracy are tested in 50\nuntrained scenarios. The results show that the proposed algorithms could stably\nand efficiently obtain satisfactory scheduling schemes for agile Earth\nobservation satellite scheduling problems. In addition, it can be found that\nRL-based optimization algorithms have stronger scalability than non-learning\nalgorithms. This work reveals the advantage of combining reinforcement learning\nmethods with heuristic methods or mathematical programming methods for solving\ncomplex combinatorial optimization problems.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 03:16:12 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["He", "Yongming", ""], ["Wu", "Guohua", ""], ["Chen", "Yingwu", ""], ["Pedrycz", "Witold", ""]]}, {"id": "2103.05863", "submitter": "Denis Gudovskiy", "authors": "Denis Gudovskiy, Luca Rigazio, Shun Ishizaka, Kazuki Kozuka, Sotaro\n  Tsukizawa", "title": "AutoDO: Robust AutoAugment for Biased Data with Label Noise via Scalable\n  Probabilistic Implicit Differentiation", "comments": "Accepted to CVPR 2021. Preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  AutoAugment has sparked an interest in automated augmentation methods for\ndeep learning models. These methods estimate image transformation policies for\ntrain data that improve generalization to test data. While recent papers\nevolved in the direction of decreasing policy search complexity, we show that\nthose methods are not robust when applied to biased and noisy data. To overcome\nthese limitations, we reformulate AutoAugment as a generalized automated\ndataset optimization (AutoDO) task that minimizes the distribution shift\nbetween test data and distorted train dataset. In our AutoDO model, we\nexplicitly estimate a set of per-point hyperparameters to flexibly change\ndistribution of train data. In particular, we include hyperparameters for\naugmentation, loss weights, and soft-labels that are jointly estimated using\nimplicit differentiation. We develop a theoretical probabilistic interpretation\nof this framework using Fisher information and show that its complexity scales\nlinearly with the dataset size. Our experiments on SVHN, CIFAR-10/100, and\nImageNet classification show up to 9.3% improvement for biased datasets with\nlabel noise compared to prior methods and, importantly, up to 36.6% gain for\nunderrepresented SVHN classes.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 04:05:33 GMT"}, {"version": "v2", "created": "Thu, 11 Mar 2021 22:15:41 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Gudovskiy", "Denis", ""], ["Rigazio", "Luca", ""], ["Ishizaka", "Shun", ""], ["Kozuka", "Kazuki", ""], ["Tsukizawa", "Sotaro", ""]]}, {"id": "2103.05898", "submitter": "Collin Burns", "authors": "Collin Burns and Jacob Steinhardt", "title": "Limitations of Post-Hoc Feature Alignment for Robustness", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature alignment is an approach to improving robustness to distribution\nshift that matches the distribution of feature activations between the training\ndistribution and test distribution. A particularly simple but effective\napproach to feature alignment involves aligning the batch normalization\nstatistics between the two distributions in a trained neural network. This\ntechnique has received renewed interest lately because of its impressive\nperformance on robustness benchmarks. However, when and why this method works\nis not well understood. We investigate the approach in more detail and identify\nseveral limitations. We show that it only significantly helps with a narrow set\nof distribution shifts and we identify several settings in which it even\ndegrades performance. We also explain why these limitations arise by\npinpointing why this approach can be so effective in the first place. Our\nfindings call into question the utility of this approach and Unsupervised\nDomain Adaptation more broadly for improving robustness in practice.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 06:55:41 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Burns", "Collin", ""], ["Steinhardt", "Jacob", ""]]}, {"id": "2103.05900", "submitter": "Shaowei Wang", "authors": "Shaowei Wang, LingLing Zhang, Xuan Luo, Yi Yang, Xin Hu, and Jun Liu", "title": "RL-CSDia: Representation Learning of Computer Science Diagrams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies on computer vision mainly focus on natural images that express\nreal-world scenes. They achieve outstanding performance on diverse tasks such\nas visual question answering. Diagram is a special form of visual expression\nthat frequently appears in the education field and is of great significance for\nlearners to understand multimodal knowledge. Current research on diagrams\npreliminarily focuses on natural disciplines such as Biology and Geography,\nwhose expressions are still similar to natural images. Another type of diagrams\nsuch as from Computer Science is composed of graphics containing complex\ntopologies and relations, and research on this type of diagrams is still blank.\nThe main challenges of graphic diagrams understanding are the rarity of data\nand the confusion of semantics, which are mainly reflected in the diversity of\nexpressions. In this paper, we construct a novel dataset of graphic diagrams\nnamed Computer Science Diagrams (CSDia). It contains more than 1,200 diagrams\nand exhaustive annotations of objects and relations. Considering the visual\nnoises caused by the various expressions in diagrams, we introduce the topology\nof diagrams to parse topological structure. After that, we propose Diagram\nParsing Net (DPN) to represent the diagram from three branches: topology,\nvisual feature, and text, and apply the model to the diagram classification\ntask to evaluate the ability of diagrams understanding. The results show the\neffectiveness of the proposed DPN on diagrams understanding.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 07:01:07 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Wang", "Shaowei", ""], ["Zhang", "LingLing", ""], ["Luo", "Xuan", ""], ["Yang", "Yi", ""], ["Hu", "Xin", ""], ["Liu", "Jun", ""]]}, {"id": "2103.05908", "submitter": "Freddy C. Chua", "authors": "Freddy C. Chua, Nigel P. Duffy", "title": "DeepCPCFG: Deep Learning and Context Free Grammars for End-to-End\n  Information Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We address the challenge of extracting structured information from business\ndocuments without detailed annotations. We propose Deep Conditional\nProbabilistic Context Free Grammars (DeepCPCFG) to parse two-dimensional\ncomplex documents and use Recursive Neural Networks to create an end-to-end\nsystem for finding the most probable parse that represents the structured\ninformation to be extracted. This system is trained end-to-end with scanned\ndocuments as input and only relational-records as labels. The\nrelational-records are extracted from existing databases avoiding the cost of\nannotating documents by hand. We apply this approach to extract information\nfrom scanned invoices achieving state-of-the-art results despite using no\nhand-annotations.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 07:35:21 GMT"}, {"version": "v2", "created": "Sat, 5 Jun 2021 15:30:46 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Chua", "Freddy C.", ""], ["Duffy", "Nigel P.", ""]]}, {"id": "2103.05923", "submitter": "XinZhou Dong", "authors": "Xinzhou Dong, Beihong Jin, Wei Zhuo, Beibei Li, Taofeng Xue", "title": "Improving Sequential Recommendation with Attribute-augmented Graph\n  Neural Networks", "comments": null, "journal-ref": "The 25th Pacific-Asia Conference on Knowledge Discovery and Data\n  Mining (PAKDD-2021), May 11-14, 2021, Delhi, India", "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many practical recommender systems provide item recommendation for different\nusers only via mining user-item interactions but totally ignoring the rich\nattribute information of items that users interact with. In this paper, we\npropose an attribute-augmented graph neural network model named Murzim. Murzim\ntakes as input the graphs constructed from the user-item interaction sequences\nand corresponding item attribute sequences. By combining the GNNs with node\naggregation and an attention network, Murzim can capture user preference\npatterns, generate embeddings for user-item interaction sequences, and then\ngenerate recommendations through next-item prediction. We conduct extensive\nexperiments on multiple datasets. Experimental results show that Murzim\noutperforms several state-of-the-art methods in terms of recall and MRR, which\nillustrates that Murzim can make use of item attribute information to produce\nbetter recommendations. At present, Murzim has been deployed in MX Player, one\nof India's largest streaming platforms, and is recommending videos for tens of\nthousands of users.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 08:29:49 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Dong", "Xinzhou", ""], ["Jin", "Beihong", ""], ["Zhuo", "Wei", ""], ["Li", "Beibei", ""], ["Xue", "Taofeng", ""]]}, {"id": "2103.05927", "submitter": "James Lo", "authors": "Shi-Wei Lo, Jyh-Horng Wu, Jo-Yu Chang, Chien-Hao Tseng, Meng-Wei Lin,\n  Fang-Pang Lin", "title": "Deep Sensing of Urban Waterlogging", "comments": "19 pages, 14 figures, under submitting and patenting", "journal-ref": null, "doi": null, "report-no": "revise-2021-05-25", "categories": "cs.CV cs.AI cs.CY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In the monsoon season, sudden flood events occur frequently in urban areas,\nwhich hamper the social and economic activities and may threaten the\ninfrastructure and lives. The use of an efficient large-scale waterlogging\nsensing and information system can provide valuable real-time disaster\ninformation to facilitate disaster management and enhance awareness of the\ngeneral public to alleviate losses during and after flood disasters. Therefore,\nin this study, a visual sensing approach driven by deep neural networks and\ninformation and communication technology was developed to provide an end-to-end\nmechanism to realize waterlogging sensing and event-location mapping. The use\nof a deep sensing system in the monsoon season in Taiwan was demonstrated, and\nwaterlogging events were predicted on the island-wide scale. The system could\nsense approximately 2379 vision sources through an internet of video things\nframework and transmit the event-location information in 5 min. The proposed\napproach can sense waterlogging events at a national scale and provide an\nefficient and highly scalable alternative to conventional waterlogging sensing\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 08:34:37 GMT"}, {"version": "v2", "created": "Wed, 26 May 2021 03:43:52 GMT"}], "update_date": "2021-05-27", "authors_parsed": [["Lo", "Shi-Wei", ""], ["Wu", "Jyh-Horng", ""], ["Chang", "Jo-Yu", ""], ["Tseng", "Chien-Hao", ""], ["Lin", "Meng-Wei", ""], ["Lin", "Fang-Pang", ""]]}, {"id": "2103.06076", "submitter": "Hanna Wallach", "authors": "Solon Barocas, Anhong Guo, Ece Kamar, Jacquelyn Krones, Meredith\n  Ringel Morris, Jennifer Wortman Vaughan, Duncan Wadsworth, Hanna Wallach", "title": "Designing Disaggregated Evaluations of AI Systems: Choices,\n  Considerations, and Tradeoffs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several pieces of work have uncovered performance disparities by conducting\n\"disaggregated evaluations\" of AI systems. We build on these efforts by\nfocusing on the choices that must be made when designing a disaggregated\nevaluation, as well as some of the key considerations that underlie these\ndesign choices and the tradeoffs between these considerations. We argue that a\ndeeper understanding of the choices, considerations, and tradeoffs involved in\ndesigning disaggregated evaluations will better enable researchers,\npractitioners, and the public to understand the ways in which AI systems may be\nunderperforming for particular groups of people.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 14:26:14 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Barocas", "Solon", ""], ["Guo", "Anhong", ""], ["Kamar", "Ece", ""], ["Krones", "Jacquelyn", ""], ["Morris", "Meredith Ringel", ""], ["Vaughan", "Jennifer Wortman", ""], ["Wadsworth", "Duncan", ""], ["Wallach", "Hanna", ""]]}, {"id": "2103.06084", "submitter": "Loann Giovannangeli", "authors": "Loann Giovannangeli, Romain Giot, David Auber and Romain Bourqui", "title": "Impacts of the Numbers of Colors and Shapes on Outlier Detection: from\n  Automated to User Evaluation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The design of efficient representations is well established as a fruitful way\nto explore and analyze complex or large data. In these representations, data\nare encoded with various visual attributes depending on the needs of the\nrepresentation itself. To make coherent design choices about visual attributes,\nthe visual search field proposes guidelines based on the human brain perception\nof features. However, information visualization representations frequently need\nto depict more data than the amount these guidelines have been validated on.\nSince, the information visualization community has extended these guidelines to\na wider parameter space.\n  This paper contributes to this theme by extending visual search theories to\nan information visualization context. We consider a visual search task where\nsubjects are asked to find an unknown outlier in a grid of randomly laid out\ndistractor. Stimuli are defined by color and shape features for the purpose of\nvisually encoding categorical data. The experimental protocol is made of a\nparameters space reduction step (i.e., sub-sampling) based on a machine\nlearning model, and a user evaluation to measure capacity limits and validate\nhypotheses. The results show that the major difficulty factor is the number of\nvisual attributes that are used to encode the outlier. When redundantly\nencoded, the display heterogeneity has no effect on the task. When encoded with\none attribute, the difficulty depends on that attribute heterogeneity until its\ncapacity limit (7 for color, 5 for shape) is reached. Finally, when encoded\nwith two attributes simultaneously, performances drop drastically even with\nminor heterogeneity.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 14:35:53 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Giovannangeli", "Loann", ""], ["Giot", "Romain", ""], ["Auber", "David", ""], ["Bourqui", "Romain", ""]]}, {"id": "2103.06105", "submitter": "Zi-Yuan Hu", "authors": "Zi-Yuan Hu, Jin Huang, Zhi-Hong Deng, Chang-Dong Wang, Ling Huang,\n  Jian-Huang Lai and Philip S. Yu", "title": "BCFNet: A Balanced Collaborative Filtering Network with Attention\n  Mechanism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative Filtering (CF) based recommendation methods have been widely\nstudied, which can be generally categorized into two types, i.e.,\nrepresentation learning-based CF methods and matching function learning-based\nCF methods. Representation learning tries to learn a common low dimensional\nspace for the representations of users and items. In this case, a user and item\nmatch better if they have higher similarity in that common space. Matching\nfunction learning tries to directly learn the complex matching function that\nmaps user-item pairs to matching scores. Although both methods are well\ndeveloped, they suffer from two fundamental flaws, i.e., the representation\nlearning resorts to applying a dot product which has limited expressiveness on\nthe latent features of users and items, while the matching function learning\nhas weakness in capturing low-rank relations. To overcome such flaws, we\npropose a novel recommendation model named Balanced Collaborative Filtering\nNetwork (BCFNet), which has the strengths of the two types of methods. In\naddition, an attention mechanism is designed to better capture the hidden\ninformation within implicit feedback and strengthen the learning ability of the\nneural network. Furthermore, a balance module is designed to alleviate the\nover-fitting issue in DNNs. Extensive experiments on eight real-world datasets\ndemonstrate the effectiveness of the proposed model.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 14:59:23 GMT"}, {"version": "v2", "created": "Sun, 11 Apr 2021 11:30:34 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Hu", "Zi-Yuan", ""], ["Huang", "Jin", ""], ["Deng", "Zhi-Hong", ""], ["Wang", "Chang-Dong", ""], ["Huang", "Ling", ""], ["Lai", "Jian-Huang", ""], ["Yu", "Philip S.", ""]]}, {"id": "2103.06123", "submitter": "Hiroshi Yamakawa", "authors": "Hiroshi Yamakawa", "title": "The whole brain architecture approach: Accelerating the development of\n  artificial general intelligence by referring to the brain", "comments": "28 pages, 10 figures, Preprint submitted to Neural Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE q-bio.NC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The vastness of the design space created by the combination of a large number\nof computational mechanisms, including machine learning, is an obstacle to\ncreating an artificial general intelligence (AGI). Brain-inspired AGI\ndevelopment, in other words, cutting down the design space to look more like a\nbiological brain, which is an existing model of a general intelligence, is a\npromising plan for solving this problem. However, it is difficult for an\nindividual to design a software program that corresponds to the entire brain\nbecause the neuroscientific data required to understand the architecture of the\nbrain are extensive and complicated. The whole-brain architecture approach\ndivides the brain-inspired AGI development process into the task of designing\nthe brain reference architecture (BRA) -- the flow of information and the\ndiagram of corresponding components -- and the task of developing each\ncomponent using the BRA. This is called BRA-driven development. Another\ndifficulty lies in the extraction of the operating principles necessary for\nreproducing the cognitive-behavioral function of the brain from neuroscience\ndata. Therefore, this study proposes the Structure-constrained Interface\nDecomposition (SCID) method, which is a hypothesis-building method for creating\na hypothetical component diagram consistent with neuroscientific findings. The\napplication of this approach has begun for building various regions of the\nbrain. Moving forward, we will examine methods of evaluating the biological\nplausibility of brain-inspired software. This evaluation will also be used to\nprioritize different computational mechanisms, which should be merged,\nassociated with the same regions of the brain.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 04:58:12 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Yamakawa", "Hiroshi", ""]]}, {"id": "2103.06124", "submitter": "Aditya Desai", "authors": "Aditya Desai, Yanzhou Pan, Kuangyuan Sun, Li Chou, Anshumali\n  Shrivastava", "title": "Semantically Constrained Memory Allocation (SCMA) for Embedding in\n  Efficient Recommendation Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning-based models are utilized to achieve state-of-the-art\nperformance for recommendation systems. A key challenge for these models is to\nwork with millions of categorical classes or tokens. The standard approach is\nto learn end-to-end, dense latent representations or embeddings for each token.\nThe resulting embeddings require large amounts of memory that blow up with the\nnumber of tokens. Training and inference with these models create storage, and\nmemory bandwidth bottlenecks leading to significant computing and energy\nconsumption when deployed in practice. To this end, we present the problem of\n\\textit{Memory Allocation} under budget for embeddings and propose a novel\nformulation of memory shared embedding, where memory is shared in proportion to\nthe overlap in semantic information. Our formulation admits a practical and\nefficient randomized solution with Locality sensitive hashing based Memory\nAllocation (LMA). We demonstrate a significant reduction in the memory\nfootprint while maintaining performance. In particular, our LMA embeddings\nachieve the same performance compared to standard embeddings with a 16$\\times$\nreduction in memory footprint. Moreover, LMA achieves an average improvement of\nover 0.003 AUC across different memory regimes than standard DLRM models on\nCriteo and Avazu datasets\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 19:55:49 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Desai", "Aditya", ""], ["Pan", "Yanzhou", ""], ["Sun", "Kuangyuan", ""], ["Chou", "Li", ""], ["Shrivastava", "Anshumali", ""]]}, {"id": "2103.06132", "submitter": "Alexandre Rame", "authors": "Alexandre Rame, Remy Sun, Matthieu Cord", "title": "MixMo: Mixing Multiple Inputs for Multiple Outputs via Deep Subnetworks", "comments": "8 pages, 10 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent strategies achieved ensembling \"for free\" by fitting concurrently\ndiverse subnetworks inside a single base network. The main idea during training\nis that each subnetwork learns to classify only one of the multiple inputs\nsimultaneously provided. However, the question of how to best mix these\nmultiple inputs has not been studied so far. In this paper, we introduce MixMo,\na new generalized framework for learning multi-input multi-output deep\nsubnetworks. Our key motivation is to replace the suboptimal summing operation\nhidden in previous approaches by a more appropriate mixing mechanism. For that\npurpose, we draw inspiration from successful mixed sample data augmentations.\nWe show that binary mixing in features - particularly with rectangular patches\nfrom CutMix - enhances results by making subnetworks stronger and more diverse.\nWe improve state of the art for image classification on CIFAR-100 and Tiny\nImageNet datasets. Our easy to implement models notably outperform data\naugmented deep ensembles, without the inference and memory overheads. As we\noperate in features and simply better leverage the expressiveness of large\nnetworks, we open a new line of research complementary to previous works.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 15:31:02 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 11:49:24 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Rame", "Alexandre", ""], ["Sun", "Remy", ""], ["Cord", "Matthieu", ""]]}, {"id": "2103.06145", "submitter": "Abhishek Singh", "authors": "Abhishek Narain Singh", "title": "GraphBreak: Tool for Network Community based Regulatory Medicine, Gene\n  co-expression, Linkage Disequilibrium analysis, functional annotation and\n  more", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.AI cs.DC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Graph network science is becoming increasingly popular, notably in big-data\nperspective where understanding individual entities for individual functional\nroles is complex and time consuming. It is likely when a set of genes are\nregulated by a set of genetic variants, the genes set is recruited for a common\nor related functional purpose. Grouping and extracting communities from network\nof associations becomes critical to understand system complexity, thus\nprioritizing genes for dis-ease and functional associations. Workload is\nreduced when studying entities one at a time. For this, we present GraphBreak,\na suite of tools for community detection application, such as for gene\nco-expression, protein interaction, regulation network, etc.Although developed\nfor use case of eQTLs regulatory genomic net-work community study -- results\nshown with our analysis with sample eQTL data. Graphbreak can be deployed for\nother studies if input data has been fed in requisite format, including but not\nlimited to gene co-expression networks, protein-protein interaction network,\nsignaling pathway and metabolic network. Graph-Break showed critical use case\nvalue in its downstream analysis for disease association of communities\ndetected. If all independent steps of community detection and analysis are a\nstep-by-step sub-part of the algorithm, GraphBreak can be considered a new\nalgorithm for community based functional characterization. Combination of\nvarious algorithmic implementation modules into a single script for this\npurpose illustrates GraphBreak novelty. Compared to other similar tools, with\nGraphBreak we can better detect communities with over-representation of its\nmember genes for statistical association with diseases, therefore target genes\nwhich can be prioritized for drug-positioning or drug-re-positioning as the\ncase be.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2021 15:16:38 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Singh", "Abhishek Narain", ""]]}, {"id": "2103.06157", "submitter": "Sunil Kumar Kopparapu Dr", "authors": "Ayush Tripathi and Swapnil Bhosale and Sunil Kumar Kopparapu", "title": "Automatic Speaker Independent Dysarthric Speech Intelligibility\n  Assessment System", "comments": "29 pages, 2 figures, Computer Speech & Language 2021", "journal-ref": null, "doi": "10.1016/j.csl.2021.101213", "report-no": null, "categories": "cs.SD cs.AI eess.AS", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Dysarthria is a condition which hampers the ability of an individual to\ncontrol the muscles that play a major role in speech delivery. The loss of fine\ncontrol over muscles that assist the movement of lips, vocal chords, tongue and\ndiaphragm results in abnormal speech delivery. One can assess the severity\nlevel of dysarthria by analyzing the intelligibility of speech spoken by an\nindividual. Continuous intelligibility assessment helps speech language\npathologists not only study the impact of medication but also allows them to\nplan personalized therapy. It helps the clinicians immensely if the\nintelligibility assessment system is reliable, automatic, simple for (a)\npatients to undergo and (b) clinicians to interpret. Lack of availability of\ndysarthric data has resulted in development of speaker dependent automatic\nintelligibility assessment systems which requires patients to speak a large\nnumber of utterances. In this paper, we propose (a) a cost minimization\nprocedure to select an optimal (small) number of utterances that need to be\nspoken by the dysarthric patient, (b) four different speaker independent\nintelligibility assessment systems which require the patient to speak a small\nnumber of words, and (c) the assessment score is close to the perceptual score\nthat the Speech Language Pathologist (SLP) can relate to. The need for small\nnumber of utterances to be spoken by the patient and the score being relatable\nto the SLP benefits both the dysarthric patient and the clinician from\nusability perspective.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 16:15:32 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Tripathi", "Ayush", ""], ["Bhosale", "Swapnil", ""], ["Kopparapu", "Sunil Kumar", ""]]}, {"id": "2103.06160", "submitter": "Chao Zhang", "authors": "Chao Zhang, Shihan Wang, Henk Aarts and Mehdi Dastani", "title": "Using Cognitive Models to Train Warm Start Reinforcement Learning Agents\n  for Human-Computer Interactions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reinforcement learning (RL) agents in human-computer interactions\napplications require repeated user interactions before they can perform well.\nTo address this \"cold start\" problem, we propose a novel approach of using\ncognitive models to pre-train RL agents before they are applied to real users.\nAfter briefly reviewing relevant cognitive models, we present our general\nmethodological approach, followed by two case studies from our previous and\nongoing projects. We hope this position paper stimulates conversations between\nRL, HCI, and cognitive science researchers in order to explore the full\npotential of the approach.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 16:20:02 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Zhang", "Chao", ""], ["Wang", "Shihan", ""], ["Aarts", "Henk", ""], ["Dastani", "Mehdi", ""]]}, {"id": "2103.06179", "submitter": "Christian Reimers", "authors": "Christian Reimers and Paul Bodesheim and Jakob Runge and Joachim\n  Denzler", "title": "Towards Learning an Unbiased Classifier from Biased Data via Conditional\n  Adversarial Debiasing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bias in classifiers is a severe issue of modern deep learning methods,\nespecially for their application in safety- and security-critical areas. Often,\nthe bias of a classifier is a direct consequence of a bias in the training\ndataset, frequently caused by the co-occurrence of relevant features and\nirrelevant ones. To mitigate this issue, we require learning algorithms that\nprevent the propagation of bias from the dataset into the classifier. We\npresent a novel adversarial debiasing method, which addresses a feature that is\nspuriously connected to the labels of training images but statistically\nindependent of the labels for test images. Thus, the automatic identification\nof relevant features during training is perturbed by irrelevant features. This\nis the case in a wide range of bias-related problems for many computer vision\ntasks, such as automatic skin cancer detection or driver assistance. We argue\nby a mathematical proof that our approach is superior to existing techniques\nfor the abovementioned bias. Our experiments show that our approach performs\nbetter than state-of-the-art techniques on a well-known benchmark dataset with\nreal-world images of cats and dogs.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 16:50:42 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Reimers", "Christian", ""], ["Bodesheim", "Paul", ""], ["Runge", "Jakob", ""], ["Denzler", "Joachim", ""]]}, {"id": "2103.06198", "submitter": "Radha Kopparti", "authors": "Radha Kopparti and Tillman Weyde", "title": "Relational Weight Priors in Neural Networks for Abstract Pattern\n  Learning and Language Modelling", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep neural networks have become the dominant approach in natural language\nprocessing (NLP). However, in recent years, it has become apparent that there\nare shortcomings in systematicity that limit the performance and data\nefficiency of deep learning in NLP. These shortcomings can be clearly shown in\nlower-level artificial tasks, mostly on synthetic data. Abstract patterns are\nthe best known examples of a hard problem for neural networks in terms of\ngeneralisation to unseen data. They are defined by relations between items,\nsuch as equality, rather than their values. It has been argued that these\nlow-level problems demonstrate the inability of neural networks to learn\nsystematically. In this study, we propose Embedded Relation Based Patterns\n(ERBP) as a novel way to create a relational inductive bias that encourages\nlearning equality and distance-based relations for abstract patterns. ERBP is\nbased on Relation Based Patterns (RBP), but modelled as a Bayesian prior on\nnetwork weights and implemented as a regularisation term in otherwise standard\nnetwork learning. ERBP is is easy to integrate into standard neural networks\nand does not affect their learning capacity. In our experiments, ERBP priors\nlead to almost perfect generalisation when learning abstract patterns from\nsynthetic noise-free sequences. ERBP also improves natural language models on\nthe word and character level and pitch prediction in melodies with RNN, GRU and\nLSTM networks. We also find improvements in in the more complex tasks of\nlearning of graph edit distance and compositional sentence entailment. ERBP\nconsistently improves over RBP and over standard networks, showing that it\nenables abstract pattern learning which contributes to performance in natural\nlanguage tasks.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 17:21:16 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Kopparti", "Radha", ""], ["Weyde", "Tillman", ""]]}, {"id": "2103.06220", "submitter": "Daniel O\\~noro-Rubio", "authors": "Anjany Sekuboyina, Daniel O\\~noro-Rubio, Jens Kleesiek and Brandon\n  Malone", "title": "A Relational-learning Perspective to Multi-label Chest X-ray\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-label classification of chest X-ray images is frequently performed\nusing discriminative approaches, i.e. learning to map an image directly to its\nbinary labels. Such approaches make it challenging to incorporate auxiliary\ninformation such as annotation uncertainty or a dependency among the labels.\nBuilding towards this, we propose a novel knowledge graph reformulation of\nmulti-label classification, which not only readily increases predictive\nperformance of an encoder but also serves as a general framework for\nintroducing new domain knowledge.\n  Specifically, we construct a multi-modal knowledge graph out of the chest\nX-ray images and its labels and pose multi-label classification as a link\nprediction problem. Incorporating auxiliary information can then simply be\nachieved by adding additional nodes and relations among them. When tested on a\npublicly-available radiograph dataset (CheXpert), our relational-reformulation\nusing a naive knowledge graph outperforms the state-of-art by achieving an\narea-under-ROC curve of 83.5%, an improvement of \"sim 1\" over a purely\ndiscriminative approach.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 17:44:59 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Sekuboyina", "Anjany", ""], ["O\u00f1oro-Rubio", "Daniel", ""], ["Kleesiek", "Jens", ""], ["Malone", "Brandon", ""]]}, {"id": "2103.06232", "submitter": "Samuel Yen-Chi Chen", "authors": "William M Watkins, Samuel Yen-Chi Chen, Shinjae Yoo", "title": "Quantum machine learning with differential privacy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.AI cs.CR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Quantum machine learning (QML) can complement the growing trend of using\nlearned models for a myriad of classification tasks, from image recognition to\nnatural speech processing. A quantum advantage arises due to the intractability\nof quantum operations on a classical computer. Many datasets used in machine\nlearning are crowd sourced or contain some private information. To the best of\nour knowledge, no current QML models are equipped with privacy-preserving\nfeatures, which raises concerns as it is paramount that models do not expose\nsensitive information. Thus, privacy-preserving algorithms need to be\nimplemented with QML. One solution is to make the machine learning algorithm\ndifferentially private, meaning the effect of a single data point on the\ntraining dataset is minimized. Differentially private machine learning models\nhave been investigated, but differential privacy has yet to be studied in the\ncontext of QML. In this study, we develop a hybrid quantum-classical model that\nis trained to preserve privacy using differentially private optimization\nalgorithm. This marks the first proof-of-principle demonstration of\nprivacy-preserving QML. The experiments demonstrate that differentially private\nQML can protect user-sensitive information without diminishing model accuracy.\nAlthough the quantum model is simulated and tested on a classical computer, it\ndemonstrates potential to be efficiently implemented on near-term quantum\ndevices (noisy intermediate-scale quantum [NISQ]). The approach's success is\nillustrated via the classification of spatially classed two-dimensional\ndatasets and a binary MNIST classification. This implementation of\nprivacy-preserving QML will ensure confidentiality and accurate learning on\nNISQ technology.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 18:06:15 GMT"}], "update_date": "2021-03-11", "authors_parsed": [["Watkins", "William M", ""], ["Chen", "Samuel Yen-Chi", ""], ["Yoo", "Shinjae", ""]]}, {"id": "2103.06304", "submitter": "Letitia Parcalabescu", "authors": "Letitia Parcalabescu, Nils Trost, Anette Frank", "title": "What is Multimodality?", "comments": "Paper accepted for publication at MMSR 2021; 10 pages, 5 figures", "journal-ref": "Proceedings of the 1st Workshop on Multimodal Semantic\n  Representations (MMSR), 2021, Groningen, Netherlands (Online), Association\n  for Computational Linguistics, p. 1--10", "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CV cs.GL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The last years have shown rapid developments in the field of multimodal\nmachine learning, combining e.g., vision, text or speech. In this position\npaper we explain how the field uses outdated definitions of multimodality that\nprove unfit for the machine learning era. We propose a new task-relative\ndefinition of (multi)modality in the context of multimodal machine learning\nthat focuses on representations and information that are relevant for a given\nmachine learning task. With our new definition of multimodality we aim to\nprovide a missing foundation for multimodal research, an important component of\nlanguage grounding and a crucial milestone towards NLU.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 19:14:07 GMT"}, {"version": "v2", "created": "Sat, 1 May 2021 09:17:44 GMT"}, {"version": "v3", "created": "Thu, 10 Jun 2021 19:32:33 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Parcalabescu", "Letitia", ""], ["Trost", "Nils", ""], ["Frank", "Anette", ""]]}, {"id": "2103.06312", "submitter": "Daniel Zhang", "authors": "Daniel Zhang, Saurabh Mishra, Erik Brynjolfsson, John Etchemendy, Deep\n  Ganguli, Barbara Grosz, Terah Lyons, James Manyika, Juan Carlos Niebles,\n  Michael Sellitto, Yoav Shoham, Jack Clark, Raymond Perrault", "title": "The AI Index 2021 Annual Report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Welcome to the fourth edition of the AI Index Report. This year we\nsignificantly expanded the amount of data available in the report, worked with\na broader set of external organizations to calibrate our data, and deepened our\nconnections with the Stanford Institute for Human-Centered Artificial\nIntelligence (HAI). The AI Index Report tracks, collates, distills, and\nvisualizes data related to artificial intelligence. Its mission is to provide\nunbiased, rigorously vetted, and globally sourced data for policymakers,\nresearchers, executives, journalists, and the general public to develop\nintuitions about the complex field of AI. The report aims to be the most\ncredible and authoritative source for data and insights about AI in the world.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 02:29:44 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Zhang", "Daniel", ""], ["Mishra", "Saurabh", ""], ["Brynjolfsson", "Erik", ""], ["Etchemendy", "John", ""], ["Ganguli", "Deep", ""], ["Grosz", "Barbara", ""], ["Lyons", "Terah", ""], ["Manyika", "James", ""], ["Niebles", "Juan Carlos", ""], ["Sellitto", "Michael", ""], ["Shoham", "Yoav", ""], ["Clark", "Jack", ""], ["Perrault", "Raymond", ""]]}, {"id": "2103.06342", "submitter": "Umberto Michieli", "authors": "Umberto Michieli and Pietro Zanuttigh", "title": "Continual Semantic Segmentation via Repulsion-Attraction of Sparse and\n  Disentangled Latent Representations", "comments": "CVPR 2021. 22 pages, 10 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep neural networks suffer from the major limitation of catastrophic\nforgetting old tasks when learning new ones. In this paper we focus on class\nincremental continual learning in semantic segmentation, where new categories\nare made available over time while previous training data is not retained. The\nproposed continual learning scheme shapes the latent space to reduce forgetting\nwhilst improving the recognition of novel classes. Our framework is driven by\nthree novel components which we also combine on top of existing techniques\neffortlessly. First, prototypes matching enforces latent space consistency on\nold classes, constraining the encoder to produce similar latent representation\nfor previously seen classes in the subsequent steps. Second, features\nsparsification allows to make room in the latent space to accommodate novel\nclasses. Finally, contrastive learning is employed to cluster features\naccording to their semantics while tearing apart those of different classes.\nExtensive evaluation on the Pascal VOC2012 and ADE20K datasets demonstrates the\neffectiveness of our approach, significantly outperforming state-of-the-art\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 21:02:05 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 19:58:33 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Michieli", "Umberto", ""], ["Zanuttigh", "Pietro", ""]]}, {"id": "2103.06364", "submitter": "Himan Abdollahpouri", "authors": "Himan Abdollahpouri, Masoud Mansoury, Robin Burke, Bamshad Mobasher,\n  Edward Malthouse", "title": "User-centered Evaluation of Popularity Bias in Recommender Systems", "comments": "Proceedings of the 29th ACM Conference on User Modeling, Adaptation\n  and Personalization (UMAP '21), June 21--25, 2021, Utrecht, Netherlands.\n  arXiv admin note: text overlap with arXiv:2007.12230", "journal-ref": null, "doi": "10.1145/3450613.3456821", "report-no": null, "categories": "cs.IR cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recommendation and ranking systems are known to suffer from popularity bias;\nthe tendency of the algorithm to favor a few popular items while\nunder-representing the majority of other items. Prior research has examined\nvarious approaches for mitigating popularity bias and enhancing the\nrecommendation of long-tail, less popular, items. The effectiveness of these\napproaches is often assessed using different metrics to evaluate the extent to\nwhich over-concentration on popular items is reduced. However, not much\nattention has been given to the user-centered evaluation of this bias; how\ndifferent users with different levels of interest towards popular items are\naffected by such algorithms. In this paper, we show the limitations of the\nexisting metrics to evaluate popularity bias mitigation when we want to assess\nthese algorithms from the users' perspective and we propose a new metric that\ncan address these limitations. In addition, we present an effective approach\nthat mitigates popularity bias from the user-centered point of view. Finally,\nwe investigate several state-of-the-art approaches proposed in recent years to\nmitigate popularity bias and evaluate their performances using the existing\nmetrics and also from the users' perspective. Our experimental results using\ntwo publicly-available datasets show that existing popularity bias mitigation\ntechniques ignore the users' tolerance towards popular items. Our proposed\nuser-centered method can tackle popularity bias effectively for different users\nwhile also improving the existing metrics.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 22:12:51 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Abdollahpouri", "Himan", ""], ["Mansoury", "Masoud", ""], ["Burke", "Robin", ""], ["Mobasher", "Bamshad", ""], ["Malthouse", "Edward", ""]]}, {"id": "2103.06369", "submitter": "Alexander Jones", "authors": "Alex Jones and Derry Tanti Wijaya", "title": "Majority Voting with Bidirectional Pre-translation For Bitext Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Obtaining high-quality parallel corpora is of paramount importance for\ntraining NMT systems. However, as many language pairs lack adequate\ngold-standard training data, a popular approach has been to mine so-called\n\"pseudo-parallel\" sentences from paired documents in two languages. In this\npaper, we outline some problems with current methods, propose computationally\neconomical solutions to those problems, and demonstrate success with novel\nmethods on the Tatoeba similarity search benchmark and on a downstream task,\nnamely NMT. We uncover the effect of resource-related factors (i.e. how much\nmonolingual/bilingual data is available for a given language) on the optimal\nchoice of bitext mining approach, and echo problems with the oft-used BUCC\ndataset that have been observed by others. We make the code and data used for\nour experiments publicly available.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 22:24:01 GMT"}, {"version": "v2", "created": "Fri, 12 Mar 2021 14:59:49 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Jones", "Alex", ""], ["Wijaya", "Derry Tanti", ""]]}, {"id": "2103.06370", "submitter": "Govardana Sachithanandam Ramachandran", "authors": "Govardana Sachithanandam Ramachandran, Kazuma Hashimoto, Caiming Xiong", "title": "Causal-aware Safe Policy Improvement for Task-oriented dialogue", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent success of reinforcement learning's (RL) in solving complex tasks\nis most often attributed to its capacity to explore and exploit an environment\nwhere it has been trained. Sample efficiency is usually not an issue since\ncheap simulators are available to sample data on-policy. On the other hand,\ntask oriented dialogues are usually learnt from offline data collected using\nhuman demonstrations. Collecting diverse demonstrations and annotating them is\nexpensive. Unfortunately, use of RL methods trained on off-policy data are\nprone to issues of bias and generalization, which are further exacerbated by\nstochasticity in human response and non-markovian belief state of a dialogue\nmanagement system. To this end, we propose a batch RL framework for task\noriented dialogue policy learning: causal aware safe policy improvement\n(CASPI). This method gives guarantees on dialogue policy's performance and also\nlearns to shape rewards according to intentions behind human responses, rather\nthan just mimicking demonstration data; this couple with batch-RL helps overall\nwith sample efficiency of the framework. We demonstrate the effectiveness of\nthis framework on a dialogue-context-to-text Generation and end-to-end dialogue\ntask of the Multiwoz2.0 dataset. The proposed method outperforms the current\nstate of the art on these metrics, in both case. In the end-to-end case, our\nmethod trained only on 10\\% of the data was able to out perform current state\nin three out of four evaluation metrics.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 22:34:28 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Ramachandran", "Govardana Sachithanandam", ""], ["Hashimoto", "Kazuma", ""], ["Xiong", "Caiming", ""]]}, {"id": "2103.06371", "submitter": "Himanshu Sahni", "authors": "Himanshu Sahni and Charles Isbell", "title": "Hard Attention Control By Mutual Information Maximization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Biological agents have adopted the principle of attention to limit the rate\nof incoming information from the environment. One question that arises is if an\nartificial agent has access to only a limited view of its surroundings, how can\nit control its attention to effectively solve tasks? We propose an approach for\nlearning how to control a hard attention window by maximizing the mutual\ninformation between the environment state and the attention location at each\nstep. The agent employs an internal world model to make predictions about its\nstate and focuses attention towards where the predictions may be wrong.\nAttention is trained jointly with a dynamic memory architecture that stores\npartial observations and keeps track of the unobserved state. We demonstrate\nthat our approach is effective in predicting the full state from a sequence of\npartial observations. We also show that the agent's internal representation of\nthe surroundings, a live mental map, can be used for control in two partially\nobservable reinforcement learning tasks. Videos of the trained agent can be\nfound at https://sites.google.com/view/hard-attention-control.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 22:38:28 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Sahni", "Himanshu", ""], ["Isbell", "Charles", ""]]}, {"id": "2103.06375", "submitter": "Wenting Zhao", "authors": "Wenting Zhao, Shufeng Kong, Junwen Bai, Daniel Fink, and Carla Gomes", "title": "HOT-VAE: Learning High-Order Label Correlation for Multi-Label\n  Classification via Attention-Based Variational Autoencoders", "comments": "accepted at AAAI'21 AISI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Understanding how environmental characteristics affect bio-diversity\npatterns, from individual species to communities of species, is critical for\nmitigating effects of global change. A central goal for conservation planning\nand monitoring is the ability to accurately predict the occurrence of species\ncommunities and how these communities change over space and time. This in turn\nleads to a challenging and long-standing problem in the field of computer\nscience - how to perform ac-curate multi-label classification with hundreds of\nlabels? The key challenge of this problem is its exponential-sized output space\nwith regards to the number of labels to be predicted.Therefore, it is essential\nto facilitate the learning process by exploiting correlations (or dependency)\namong labels. Previous methods mostly focus on modelling the correlation on\nlabel pairs; however, complex relations between real-world objects often go\nbeyond second order. In this paper, we pro-pose a novel framework for\nmulti-label classification, High-order Tie-in Variational Autoencoder\n(HOT-VAE), which per-forms adaptive high-order label correlation learning. We\nexperimentally verify that our model outperforms the existing state-of-the-art\napproaches on a bird distribution dataset on both conventional F1 scores and a\nvariety of ecological metrics. To show our method is general, we also perform\nempirical analysis on seven other public real-world datasets in several\napplication domains, and Hot-VAE exhibits superior performance to previous\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 9 Mar 2021 04:30:28 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Zhao", "Wenting", ""], ["Kong", "Shufeng", ""], ["Bai", "Junwen", ""], ["Fink", "Daniel", ""], ["Gomes", "Carla", ""]]}, {"id": "2103.06386", "submitter": "Bernie Wang", "authors": "Bernie Wang, Simon Xu, Kurt Keutzer, Yang Gao, Bichen Wu", "title": "Improving Context-Based Meta-Reinforcement Learning with Self-Supervised\n  Trajectory Contrastive Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Meta-reinforcement learning typically requires orders of magnitude more\nsamples than single task reinforcement learning methods. This is because\nmeta-training needs to deal with more diverse distributions and train extra\ncomponents such as context encoders. To address this, we propose a novel\nself-supervised learning task, which we named Trajectory Contrastive Learning\n(TCL), to improve meta-training. TCL adopts contrastive learning and trains a\ncontext encoder to predict whether two transition windows are sampled from the\nsame trajectory. TCL leverages the natural hierarchical structure of\ncontext-based meta-RL and makes minimal assumptions, allowing it to be\ngenerally applicable to context-based meta-RL algorithms. It accelerates the\ntraining of context encoders and improves meta-training overall. Experiments\nshow that TCL performs better or comparably than a strong meta-RL baseline in\nmost of the environments on both meta-RL MuJoCo (5 of 6) and Meta-World\nbenchmarks (44 out of 50).\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 23:31:19 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Wang", "Bernie", ""], ["Xu", "Simon", ""], ["Keutzer", "Kurt", ""], ["Gao", "Yang", ""], ["Wu", "Bichen", ""]]}, {"id": "2103.06403", "submitter": "Amir Ehsan Niaraki Asli", "authors": "Jeremy Roghair, Kyungtae Ko, Amir Ehsan Niaraki Asli and Ali Jannesari", "title": "A Vision Based Deep Reinforcement Learning Algorithm for UAV Obstacle\n  Avoidance", "comments": "12 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integration of reinforcement learning with unmanned aerial vehicles (UAVs) to\nachieve autonomous flight has been an active research area in recent years. An\nimportant part focuses on obstacle detection and avoidance for UAVs navigating\nthrough an environment. Exploration in an unseen environment can be tackled\nwith Deep Q-Network (DQN). However, value exploration with uniform sampling of\nactions may lead to redundant states, where often the environments inherently\nbear sparse rewards. To resolve this, we present two techniques for improving\nexploration for UAV obstacle avoidance. The first is a convergence-based\napproach that uses convergence error to iterate through unexplored actions and\ntemporal threshold to balance exploration and exploitation. The second is a\nguidance-based approach using a Domain Network which uses a Gaussian mixture\ndistribution to compare previously seen states to a predicted next state in\norder to select the next action. Performance and evaluation of these approaches\nwere implemented in multiple 3-D simulation environments, with variation in\ncomplexity. The proposed approach demonstrates a two-fold improvement in\naverage rewards compared to state of the art.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 01:15:26 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Roghair", "Jeremy", ""], ["Ko", "Kyungtae", ""], ["Asli", "Amir Ehsan Niaraki", ""], ["Jannesari", "Ali", ""]]}, {"id": "2103.06417", "submitter": "Yasunori Ozaki", "authors": "Yuki Tamaru, Yasunori Ozaki, Yuki Okafuji, Jun Baba, Junya Nakanishi,\n  Yuichiro Yoshikawa", "title": "3D Head-Position Prediction in First-Person View by Considering Head\n  Pose for Human-Robot Eye Contact", "comments": "Submit to IROS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  For a humanoid robot to make eye contact to initiate communication with a\nhuman, it is necessary to estimate the human's head position.However, eye\ncontact becomes difficult due to the mechanical delay of the robot while the\nsubject with whom the robot is interacting with is moving. Owing to these\nissues, it is important to perform head-position prediction to mitigate the\neffect of the delay in the robot's motion. Based on the fact that humans turn\ntheir heads before changing direction while walking, we hypothesized that the\naccuracy of three-dimensional(3D) head-position prediction from the\nfirst-person view can be improved by considering the head pose into account.We\ncompared our method with the conventional Kalman filter-based method, and found\nour method to be more accurate. The experimental results show that considering\nthe head pose helps improve the accuracy of 3D head-position prediction.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 02:16:53 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Tamaru", "Yuki", ""], ["Ozaki", "Yasunori", ""], ["Okafuji", "Yuki", ""], ["Baba", "Jun", ""], ["Nakanishi", "Junya", ""], ["Yoshikawa", "Yuichiro", ""]]}, {"id": "2103.06426", "submitter": "Stephen McAleer", "authors": "Stephen McAleer, John Lanier, Pierre Baldi, Roy Fox", "title": "XDO: A Double Oracle Algorithm for Extensive-Form Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.LG cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Policy Space Response Oracles (PSRO) is a deep reinforcement learning\nalgorithm for two-player zero-sum games that has empirically found approximate\nNash equilibria in large games. Although PSRO is guaranteed to converge to a\nNash equilibrium, it may take an exponential number of iterations as the number\nof infostates grows. We propose Extensive-Form Double Oracle (XDO), an\nextensive-form double oracle algorithm that is guaranteed to converge to an\napproximate Nash equilibrium linearly in the number of infostates. Unlike PSRO,\nwhich mixes best responses at the root of the game, XDO mixes best responses at\nevery infostate. We also introduce Neural XDO (NXDO), where the best response\nis learned through deep RL. In tabular experiments on Leduc poker, we find that\nXDO achieves an approximate Nash equilibrium in a number of iterations 1-2\norders of magnitude smaller than PSRO. In experiments on a modified Leduc poker\ngame, we show that tabular XDO achieves over 11x lower exploitability than CFR\nand over 82x lower exploitability than PSRO and XFP in the same amount of time.\nWe also show that NXDO beats PSRO and is competitive with NFSP on a large\nno-limit poker game.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 03:05:44 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["McAleer", "Stephen", ""], ["Lanier", "John", ""], ["Baldi", "Pierre", ""], ["Fox", "Roy", ""]]}, {"id": "2103.06434", "submitter": "Rohola Zandie", "authors": "Rohola Zandie and Mohammad H. Mahoor", "title": "Topical Language Generation using Transformers", "comments": "Accepted in the Journal of Natural Language Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large-scale transformer-based language models (LMs) demonstrate impressive\ncapabilities in open text generation. However, controlling the generated text's\nproperties such as the topic, style, and sentiment is challenging and often\nrequires significant changes to the model architecture or retraining and\nfine-tuning the model on new supervised data. This paper presents a novel\napproach for Topical Language Generation (TLG) by combining a pre-trained LM\nwith topic modeling information. We cast the problem using Bayesian probability\nformulation with topic probabilities as a prior, LM probabilities as the\nlikelihood, and topical language generation probability as the posterior. In\nlearning the model, we derive the topic probability distribution from the\nuser-provided document's natural structure. Furthermore, we extend our model by\nintroducing new parameters and functions to influence the quantity of the\ntopical features presented in the generated text. This feature would allow us\nto easily control the topical properties of the generated text. Our\nexperimental results demonstrate that our model outperforms the\nstate-of-the-art results on coherency, diversity, and fluency while being\nfaster in decoding.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 03:45:24 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Zandie", "Rohola", ""], ["Mahoor", "Mohammad H.", ""]]}, {"id": "2103.06442", "submitter": "Yoshinobu Hagiwara Dr.", "authors": "Yoshinobu Hagiwara and Keishiro Taguchi and Satoshi Ishibushi and\n  Akira Taniguchi and Tadahiro Taniguchi", "title": "Hierarchical Bayesian Model for the Transfer of Knowledge on Spatial\n  Concepts based on Multimodal Information", "comments": "17 pages, 12 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a hierarchical Bayesian model based on spatial concepts\nthat enables a robot to transfer the knowledge of places from experienced\nenvironments to a new environment. The transfer of knowledge based on spatial\nconcepts is modeled as the calculation process of the posterior distribution\nbased on the observations obtained in each environment with the parameters of\nspatial concepts generalized to environments as prior knowledge. We conducted\nexperiments to evaluate the generalization performance of spatial knowledge for\ngeneral places such as kitchens and the adaptive performance of spatial\nknowledge for unique places such as `Emma's room' in a new environment. In the\nexperiments, the accuracies of the proposed method and conventional methods\nwere compared in the prediction task of location names from an image and a\nposition, and the prediction task of positions from a location name. The\nexperimental results demonstrated that the proposed method has a higher\nprediction accuracy of location names and positions than the conventional\nmethod owing to the transfer of knowledge.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 04:10:23 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Hagiwara", "Yoshinobu", ""], ["Taguchi", "Keishiro", ""], ["Ishibushi", "Satoshi", ""], ["Taniguchi", "Akira", ""], ["Taniguchi", "Tadahiro", ""]]}, {"id": "2103.06443", "submitter": "Sourav Garg", "authors": "Sourav Garg, Tobias Fischer and Michael Milford", "title": "Where is your place, Visual Place Recognition?", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual Place Recognition (VPR) is often characterized as being able to\nrecognize the same place despite significant changes in appearance and\nviewpoint. VPR is a key component of Spatial Artificial Intelligence, enabling\nrobotic platforms and intelligent augmentation platforms such as augmented\nreality devices to perceive and understand the physical world. In this paper,\nwe observe that there are three \"drivers\" that impose requirements on spatially\nintelligent agents and thus VPR systems: 1) the particular agent including its\nsensors and computational resources, 2) the operating environment of this\nagent, and 3) the specific task that the artificial agent carries out. In this\npaper, we characterize and survey key works in the VPR area considering those\ndrivers, including their place representation and place matching choices. We\nalso provide a new definition of VPR based on the visual overlap -- akin to\nspatial view cells in the brain -- that enables us to find similarities and\ndifferences to other research areas in the robotics and computer vision fields.\nWe identify numerous open challenges and suggest areas that require more\nin-depth attention in future works.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 04:11:04 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Garg", "Sourav", ""], ["Fischer", "Tobias", ""], ["Milford", "Michael", ""]]}, {"id": "2103.06450", "submitter": "Sumeet Sohan Singh", "authors": "Sumeet S. Singh, Sergey Karayev", "title": "Full Page Handwriting Recognition via Image to Sequence Extraction", "comments": "To appear in ICDAR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a Neural Network based Handwritten Text Recognition (HTR) model\narchitecture that can be trained to recognize full pages of handwritten or\nprinted text without image segmentation. Being based on Image to Sequence\narchitecture, it can extract text present in an image and then sequence it\ncorrectly without imposing any constraints regarding orientation, layout and\nsize of text and non-text. Further, it can also be trained to generate\nauxiliary markup related to formatting, layout and content. We use character\nlevel vocabulary, thereby enabling language and terminology of any subject. The\nmodel achieves a new state-of-art in paragraph level recognition on the IAM\ndataset. When evaluated on scans of real world handwritten free form test\nanswers - beset with curved and slanted lines, drawings, tables, math,\nchemistry and other symbols - it performs better than all commercially\navailable HTR cloud APIs. It is deployed in production as part of a commercial\nweb application.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 04:37:29 GMT"}, {"version": "v2", "created": "Fri, 21 May 2021 18:52:44 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Singh", "Sumeet S.", ""], ["Karayev", "Sergey", ""]]}, {"id": "2103.06459", "submitter": "Linlin Liu", "authors": "Linlin Liu, Thien Hai Nguyen, Shafiq Joty, Lidong Bing, Luo Si", "title": "Towards Multi-Sense Cross-Lingual Alignment of Contextual Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-lingual word embeddings (CLWE) have been proven useful in many\ncross-lingual tasks. However, most existing approaches to learn CLWE including\nthe ones with contextual embeddings are sense agnostic. In this work, we\npropose a novel framework to align contextual embeddings at the sense level by\nleveraging cross-lingual signal from bilingual dictionaries only. We\noperationalize our framework by first proposing a novel sense-aware cross\nentropy loss to model word senses explicitly. The monolingual ELMo and BERT\nmodels pretrained with our sense-aware cross entropy loss demonstrate\nsignificant performance improvement for word sense disambiguation tasks. We\nthen propose a sense alignment objective on top of the sense-aware cross\nentropy loss for cross-lingual model pretraining, and pretrain cross-lingual\nmodels for several language pairs (English to German/Spanish/Japanese/Chinese).\nCompared with the best baseline results, our cross-lingual models achieve\n0.52%, 2.09% and 1.29% average performance improvements on zero-shot\ncross-lingual NER, sentiment classification and XNLI tasks, respectively.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 04:55:35 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Liu", "Linlin", ""], ["Nguyen", "Thien Hai", ""], ["Joty", "Shafiq", ""], ["Bing", "Lidong", ""], ["Si", "Luo", ""]]}, {"id": "2103.06460", "submitter": "Huan Wang", "authors": "Huan Wang, Can Qin, Yulun Zhang, Yun Fu", "title": "Emerging Paradigms of Neural Network Pruning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over-parameterization of neural networks benefits the optimization and\ngeneralization yet brings cost in practice. Pruning is adopted as a\npost-processing solution to this problem, which aims to remove unnecessary\nparameters in a neural network with little performance compromised. It has been\nbroadly believed the resulted sparse neural network cannot be trained from\nscratch to comparable accuracy. However, several recent works (e.g., [Frankle\nand Carbin, 2019a]) challenge this belief by discovering random sparse networks\nwhich can be trained to match the performance with their dense counterpart.\nThis new pruning paradigm later inspires more new methods of pruning at\ninitialization. In spite of the encouraging progress, how to coordinate these\nnew pruning fashions with the traditional pruning has not been explored yet.\nThis survey seeks to bridge the gap by proposing a general pruning framework so\nthat the emerging pruning paradigms can be accommodated well with the\ntraditional one. With it, we systematically reflect the major differences and\nnew insights brought by these new pruning fashions, with representative works\ndiscussed at length. Finally, we summarize the open questions as worthy future\ndirections.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 05:01:52 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Wang", "Huan", ""], ["Qin", "Can", ""], ["Zhang", "Yulun", ""], ["Fu", "Yun", ""]]}, {"id": "2103.06469", "submitter": "Hao Hu", "authors": "Hao Hu, Jianing Ye, Guangxiang Zhu, Zhizhou Ren, Chongjie Zhang", "title": "Generalizable Episodic Memory for Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Episodic memory-based methods can rapidly latch onto past successful\nstrategies by a non-parametric memory and improve sample efficiency of\ntraditional reinforcement learning. However, little effort is put into the\ncontinuous domain, where a state is never visited twice, and previous episodic\nmethods fail to efficiently aggregate experience across trajectories. To\naddress this problem, we propose Generalizable Episodic Memory (GEM), which\neffectively organizes the state-action values of episodic memory in a\ngeneralizable manner and supports implicit planning on memorized trajectories.\nGEM utilizes a double estimator to reduce the overestimation bias induced by\nvalue propagation in the planning process. Empirical evaluation shows that our\nmethod significantly outperforms existing trajectory-based methods on various\nMuJoCo continuous control tasks. To further show the general applicability, we\nevaluate our method on Atari games with discrete action space, which also shows\na significant improvement over baseline algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 05:31:21 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 11:03:45 GMT"}, {"version": "v3", "created": "Fri, 11 Jun 2021 14:27:01 GMT"}], "update_date": "2021-06-14", "authors_parsed": [["Hu", "Hao", ""], ["Ye", "Jianing", ""], ["Zhu", "Guangxiang", ""], ["Ren", "Zhizhou", ""], ["Zhang", "Chongjie", ""]]}, {"id": "2103.06473", "submitter": "Malik Aqeel Anwar", "authors": "Aqeel Anwar, Arijit Raychowdhury", "title": "Multi-Task Federated Reinforcement Learning with Adversaries", "comments": "14 pages, 19 figures, 4 tables, journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reinforcement learning algorithms, just like any other Machine learning\nalgorithm pose a serious threat from adversaries. The adversaries can\nmanipulate the learning algorithm resulting in non-optimal policies. In this\npaper, we analyze the Multi-task Federated Reinforcement Learning algorithms,\nwhere multiple collaborative agents in various environments are trying to\nmaximize the sum of discounted return, in the presence of adversarial agents.\nWe argue that the common attack methods are not guaranteed to carry out a\nsuccessful attack on Multi-task Federated Reinforcement Learning and propose an\nadaptive attack method with better attack performance. Furthermore, we modify\nthe conventional federated reinforcement learning algorithm to address the\nissue of adversaries that works equally well with and without the adversaries.\nExperimentation on different small to mid-size reinforcement learning problems\nshow that the proposed attack method outperforms other general attack methods\nand the proposed modification to federated reinforcement learning algorithm was\nable to achieve near-optimal policies in the presence of adversarial agents.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 05:39:52 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Anwar", "Aqeel", ""], ["Raychowdhury", "Arijit", ""]]}, {"id": "2103.06474", "submitter": "Bang Lin", "authors": "Bang Lin, Xiuchong Wang, Yu Dong, Chengfu Huo, Weijun Ren, Chuanyu Xu", "title": "Metapaths guided Neighbors aggregated Network for?Heterogeneous Graph\n  Reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most real-world datasets are inherently heterogeneous graphs, which involve a\ndiversity of node and relation types. Heterogeneous graph embedding is to learn\nthe structure and semantic information from the graph, and then embed it into\nthe low-dimensional node representation. Existing methods usually capture the\ncomposite relation of a heterogeneous graph by defining metapath, which\nrepresent a semantic of the graph. However, these methods either ignore node\nattributes, or discard the local and global information of the graph, or only\nconsider one metapath. To address these limitations, we propose a\nMetapaths-guided Neighbors-aggregated Heterogeneous Graph Neural Network(MHN)\nto improve performance. Specially, MHN employs node base embedding to\nencapsulate node attributes, BFS and DFS neighbors aggregation within a\nmetapath to capture local and global information, and metapaths aggregation to\ncombine different semantics of the heterogeneous graph. We conduct extensive\nexperiments for the proposed MHN on three real-world heterogeneous graph\ndatasets, including node classification, link prediction and online A/B test on\nAlibaba mobile application. Results demonstrate that MHN performs better than\nother state-of-the-art baselines.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 05:42:06 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Lin", "Bang", ""], ["Wang", "Xiuchong", ""], ["Dong", "Yu", ""], ["Huo", "Chengfu", ""], ["Ren", "Weijun", ""], ["Xu", "Chuanyu", ""]]}, {"id": "2103.06487", "submitter": "Haowen Liu", "authors": "Haowen Liu, Ping Yi, Hsiao-Ying Lin, Jie Shi, Weidong Qiu", "title": "DAFAR: Defending against Adversaries by Feedback-Autoencoder\n  Reconstruction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has shown impressive performance on challenging perceptual\ntasks and has been widely used in software to provide intelligent services.\nHowever, researchers found deep neural networks vulnerable to adversarial\nexamples. Since then, many methods are proposed to defend against adversaries\nin inputs, but they are either attack-dependent or shown to be ineffective with\nnew attacks. And most of existing techniques have complicated structures or\nmechanisms that cause prohibitively high overhead or latency, impractical to\napply on real software.\n  We propose DAFAR, a feedback framework that allows deep learning models to\ndetect/purify adversarial examples in high effectiveness and universality, with\nlow area and time overhead. DAFAR has a simple structure, containing a victim\nmodel, a plug-in feedback network, and a detector. The key idea is to import\nthe high-level features from the victim model's feature extraction layers into\nthe feedback network to reconstruct the input. This data stream forms a\nfeedback autoencoder. For strong attacks, it transforms the imperceptible\nattack on the victim model into the obvious reconstruction-error attack on the\nfeedback autoencoder directly, which is much easier to detect; for weak\nattacks, the reformation process destroys the structure of adversarial\nexamples. Experiments are conducted on MNIST and CIFAR-10 data-sets, showing\nthat DAFAR is effective against popular and arguably most advanced attacks\nwithout losing performance on legitimate samples, with high effectiveness and\nuniversality across attack methods and parameters.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 06:18:50 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 14:49:12 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Liu", "Haowen", ""], ["Yi", "Ping", ""], ["Lin", "Hsiao-Ying", ""], ["Shi", "Jie", ""], ["Qiu", "Weidong", ""]]}, {"id": "2103.06490", "submitter": "Parag Dutta", "authors": "Rishi Hazra, Parag Dutta, Shubham Gupta, Mohammed Abdul Qaathir,\n  Ambedkar Dukkipati", "title": "Active$^2$ Learning: Actively reducing redundancies in Active Learning\n  methods for Sequence Tagging and Machine Translation", "comments": "Two of the authors had published similar manuscripts on arXiv. So\n  withdrawing this one. All further updations will be reflected at\n  arXiv:1911.00234", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While deep learning is a powerful tool for natural language processing (NLP)\nproblems, successful solutions to these problems rely heavily on large amounts\nof annotated samples. However, manually annotating data is expensive and\ntime-consuming. Active Learning (AL) strategies reduce the need for huge\nvolumes of labeled data by iteratively selecting a small number of examples for\nmanual annotation based on their estimated utility in training the given model.\nIn this paper, we argue that since AL strategies choose examples independently,\nthey may potentially select similar examples, all of which may not contribute\nsignificantly to the learning process. Our proposed approach,\nActive$\\mathbf{^2}$ Learning (A$\\mathbf{^2}$L), actively adapts to the deep\nlearning model being trained to eliminate further such redundant examples\nchosen by an AL strategy. We show that A$\\mathbf{^2}$L is widely applicable by\nusing it in conjunction with several different AL strategies and NLP tasks. We\nempirically demonstrate that the proposed approach is further able to reduce\nthe data requirements of state-of-the-art AL strategies by an absolute\npercentage reduction of $\\approx\\mathbf{3-25\\%}$ on multiple NLP tasks while\nachieving the same performance with no additional computation overhead.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 06:27:31 GMT"}, {"version": "v2", "created": "Sat, 3 Apr 2021 13:49:59 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Hazra", "Rishi", ""], ["Dutta", "Parag", ""], ["Gupta", "Shubham", ""], ["Qaathir", "Mohammed Abdul", ""], ["Dukkipati", "Ambedkar", ""]]}, {"id": "2103.06498", "submitter": "Xiangyu Xu", "authors": "Xiangyu Xu, Hao Chen, Francesc Moreno-Noguer, Laszlo A. Jeni, Fernando\n  De la Torre", "title": "3D Human Pose, Shape and Texture from Low-Resolution Images and Videos", "comments": "arXiv admin note: substantial text overlap with arXiv:2007.13666", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D human pose and shape estimation from monocular images has been an active\nresearch area in computer vision. Existing deep learning methods for this task\nrely on high-resolution input, which however, is not always available in many\nscenarios such as video surveillance and sports broadcasting. Two common\napproaches to deal with low-resolution images are applying super-resolution\ntechniques to the input, which may result in unpleasant artifacts, or simply\ntraining one model for each resolution, which is impractical in many realistic\napplications.\n  To address the above issues, this paper proposes a novel algorithm called\nRSC-Net, which consists of a Resolution-aware network, a Self-supervision loss,\nand a Contrastive learning scheme. The proposed method is able to learn 3D body\npose and shape across different resolutions with one single model. The\nself-supervision loss enforces scale-consistency of the output, and the\ncontrastive learning scheme enforces scale-consistency of the deep features. We\nshow that both these new losses provide robustness when learning in a\nweakly-supervised manner. Moreover, we extend the RSC-Net to handle\nlow-resolution videos and apply it to reconstruct textured 3D pedestrians from\nlow-resolution input. Extensive experiments demonstrate that the RSC-Net can\nachieve consistently better results than the state-of-the-art methods for\nchallenging low-resolution images.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 06:52:12 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Xu", "Xiangyu", ""], ["Chen", "Hao", ""], ["Moreno-Noguer", "Francesc", ""], ["Jeni", "Laszlo A.", ""], ["De la Torre", "Fernando", ""]]}, {"id": "2103.06504", "submitter": "Ranjie Duan", "authors": "Ranjie Duan, Xiaofeng Mao, A. K. Qin, Yun Yang, Yuefeng Chen, Shaokai\n  Ye, Yuan He", "title": "Adversarial Laser Beam: Effective Physical-World Attack to DNNs in a\n  Blink", "comments": "Accepted to CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Though it is well known that the performance of deep neural networks (DNNs)\ndegrades under certain light conditions, there exists no study on the threats\nof light beams emitted from some physical source as adversarial attacker on\nDNNs in a real-world scenario. In this work, we show by simply using a laser\nbeam that DNNs are easily fooled. To this end, we propose a novel attack method\ncalled Adversarial Laser Beam ($AdvLB$), which enables manipulation of laser\nbeam's physical parameters to perform adversarial attack. Experiments\ndemonstrate the effectiveness of our proposed approach in both digital- and\nphysical-settings. We further empirically analyze the evaluation results and\nreveal that the proposed laser beam attack may lead to some interesting\nprediction errors of the state-of-the-art DNNs. We envisage that the proposed\n$AdvLB$ method enriches the current family of adversarial attacks and builds\nthe foundation for future robustness studies for light.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 07:03:21 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Duan", "Ranjie", ""], ["Mao", "Xiaofeng", ""], ["Qin", "A. K.", ""], ["Yang", "Yun", ""], ["Chen", "Yuefeng", ""], ["Ye", "Shaokai", ""], ["He", "Yuan", ""]]}, {"id": "2103.06506", "submitter": "Corey Lammie", "authors": "Corey Lammie, Jason K. Eshraghian, Wei D. Lu, Mostafa Rahimi Azghadi", "title": "Memristive Stochastic Computing for Deep Learning Parameter Optimization", "comments": "Accepted by IEEE Transactions on Circuits and Systems Part II:\n  Express Briefs", "journal-ref": "IEEE Transactions on Circuits and Systems Part II: Express Briefs,\n  2021", "doi": "10.1109/TCSII.2021.3065932", "report-no": null, "categories": "cs.ET cs.AI cs.AR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic Computing (SC) is a computing paradigm that allows for the\nlow-cost and low-power computation of various arithmetic operations using\nstochastic bit streams and digital logic. In contrast to conventional\nrepresentation schemes used within the binary domain, the sequence of bit\nstreams in the stochastic domain is inconsequential, and computation is usually\nnon-deterministic. In this brief, we exploit the stochasticity during switching\nof probabilistic Conductive Bridging RAM (CBRAM) devices to efficiently\ngenerate stochastic bit streams in order to perform Deep Learning (DL)\nparameter optimization, reducing the size of Multiply and Accumulate (MAC)\nunits by 5 orders of magnitude. We demonstrate that in using a 40-nm\nComplementary Metal Oxide Semiconductor (CMOS) process our scalable\narchitecture occupies 1.55mm$^2$ and consumes approximately 167$\\mu$W when\noptimizing parameters of a Convolutional Neural Network (CNN) while it is being\ntrained for a character recognition task, observing no notable reduction in\naccuracy post-training.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 07:10:32 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Lammie", "Corey", ""], ["Eshraghian", "Jason K.", ""], ["Lu", "Wei D.", ""], ["Azghadi", "Mostafa Rahimi", ""]]}, {"id": "2103.06541", "submitter": "Trisha Mittal", "authors": "Trisha Mittal, Puneet Mathur, Aniket Bera, Dinesh Manocha", "title": "Affect2MM: Affective Analysis of Multimedia Content Using Emotion\n  Causality", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present Affect2MM, a learning method for time-series emotion prediction\nfor multimedia content. Our goal is to automatically capture the varying\nemotions depicted by characters in real-life human-centric situations and\nbehaviors. We use the ideas from emotion causation theories to computationally\nmodel and determine the emotional state evoked in clips of movies. Affect2MM\nexplicitly models the temporal causality using attention-based methods and\nGranger causality. We use a variety of components like facial features of\nactors involved, scene understanding, visual aesthetics, action/situation\ndescription, and movie script to obtain an affective-rich representation to\nunderstand and perceive the scene. We use an LSTM-based learning model for\nemotion perception. To evaluate our method, we analyze and compare our\nperformance on three datasets, SENDv1, MovieGraphs, and the LIRIS-ACCEDE\ndataset, and observe an average of 10-15% increase in the performance over SOTA\nmethods for all three datasets.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 09:07:25 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Mittal", "Trisha", ""], ["Mathur", "Puneet", ""], ["Bera", "Aniket", ""], ["Manocha", "Dinesh", ""]]}, {"id": "2103.06544", "submitter": "Zhaolong Ling", "authors": "Zhaolong Ling, Kui Yu, Yiwen Zhang, Lin Liu, and Jiuyong Li", "title": "Causal Learner: A Toolbox for Causal Structure and Markov Blanket\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal Learner is a toolbox for learning causal structure and Markov blanket\n(MB) from data. It integrates functions for generating simulated Bayesian\nnetwork data, a set of state-of-the-art global causal structure learning\nalgorithms, a set of state-of-the-art local causal structure learning\nalgorithms, a set of state-of-the-art MB learning algorithms, and functions for\nevaluating algorithms. The data generation part of Causal Learner is written in\nR, and the rest of Causal Learner is written in MATLAB. Causal Learner aims to\nprovide researchers and practitioners with an open-source platform for causal\nlearning from data and for the development and evaluation of new causal\nlearning algorithms. The Causal Learner project is available at\nhttp://bigdata.ahu.edu.cn/causal-learner.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 09:10:55 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Ling", "Zhaolong", ""], ["Yu", "Kui", ""], ["Zhang", "Yiwen", ""], ["Liu", "Lin", ""], ["Li", "Jiuyong", ""]]}, {"id": "2103.06546", "submitter": "Li Fan", "authors": "Fan Li, Yongming Li, Pin Wang, Jie Xiao, Fang Yan, Xinke Li", "title": "Integrated Age Estimation Mechanism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine-learning-based age estimation has received lots of attention.\nTraditional age estimation mechanism focuses estimation age error, but ignores\nthat there is a deviation between the estimated age and real age due to\ndisease. Pathological age estimation mechanism the author proposed before\nintroduces age deviation to solve the above problem and improves classification\ncapability of the estimated age significantly. However,it does not consider the\nage estimation error of the normal control (NC) group and results in a larger\nerror between the estimated age and real age of NC group. Therefore, an\nintegrated age estimation mechanism based on Decision-Level fusion of error and\ndeviation orientation model is proposed to solve the problem.Firstly, the\ntraditional age estimation and pathological age estimation mechanisms are\nweighted together.Secondly, their optimal weights are obtained by minimizing\nmean absolute error (MAE) between the estimated age and real age of normal\npeople. In the experimental section, several representative age-related\ndatasets are used for verification of the proposed method. The results show\nthat the proposed age estimation mechanism achieves a good tradeoff effect of\nage estimation. It not only improves the classification ability of the\nestimated age, but also reduces the age estimation error of the NC group. In\ngeneral, the proposed age estimation mechanism is effective. Additionally, the\nmechanism is a framework mechanism that can be used to construct different\nspecific age estimation algorithms, contributing to relevant research.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 09:14:10 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Li", "Fan", ""], ["Li", "Yongming", ""], ["Wang", "Pin", ""], ["Xiao", "Jie", ""], ["Yan", "Fang", ""], ["Li", "Xinke", ""]]}, {"id": "2103.06602", "submitter": "Alexandros Nikou PhD", "authors": "Alexandros Nikou, Anusha Mujumdar, Marin Orlic, Aneta Vulgarakis\n  Feljan", "title": "Symbolic Reinforcement Learning for Safe RAN Control", "comments": "The paper has been accepted to be presented in 20th International\n  Conference on Autonomous Agents and Multiagent Systems (AAMAS 2021), May 3-7,\n  London, UK (demo track)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we demonstrate a Symbolic Reinforcement Learning (SRL)\narchitecture for safe control in Radio Access Network (RAN) applications. In\nour automated tool, a user can select a high-level safety specifications\nexpressed in Linear Temporal Logic (LTL) to shield an RL agent running in a\ngiven cellular network with aim of optimizing network performance, as measured\nthrough certain Key Performance Indicators (KPIs). In the proposed\narchitecture, network safety shielding is ensured through model-checking\ntechniques over combined discrete system models (automata) that are abstracted\nthrough reinforcement learning. We demonstrate the user interface (UI) helping\nthe user set intent specifications to the architecture and inspect the\ndifference in allowed and blocked actions.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 10:56:49 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Nikou", "Alexandros", ""], ["Mujumdar", "Anusha", ""], ["Orlic", "Marin", ""], ["Feljan", "Aneta Vulgarakis", ""]]}, {"id": "2103.06617", "submitter": "Matthias Weissenbacher", "authors": "Matthias Weissenbacher and Yoshinobu Kawahara", "title": "A Quadratic Actor Network for Model-Free Reinforcement Learning", "comments": "8 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work we discuss the incorporation of quadratic neurons into policy\nnetworks in the context of model-free actor-critic reinforcement learning.\nQuadratic neurons admit an explicit quadratic function approximation in\ncontrast to conventional approaches where the the non-linearity is induced by\nthe activation functions. We perform empiric experiments on several MuJoCo\ncontinuous control tasks and find that when quadratic neurons are added to MLP\npolicy networks those outperform the baseline MLP whilst admitting a smaller\nnumber of parameters. The top returned reward is in average increased by\n$5.8\\%$ while being about $21\\%$ more sample efficient. Moreover, it can\nmaintain its advantage against added action and observation noise.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 11:36:28 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Weissenbacher", "Matthias", ""], ["Kawahara", "Yoshinobu", ""]]}, {"id": "2103.06624", "submitter": "Huan Zhang", "authors": "Shiqi Wang, Huan Zhang, Kaidi Xu, Xue Lin, Suman Jana, Cho-Jui Hsieh,\n  J. Zico Kolter", "title": "Beta-CROWN: Efficient Bound Propagation with Per-neuron Split\n  Constraints for Complete and Incomplete Neural Network Verification", "comments": "Shiqi Wang, Huan Zhang and Kaidi Xu contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works in neural network verification show that cheap incomplete\nverifiers such as CROWN, based upon bound propagations, can effectively be used\nin Branch-and-Bound (BaB) methods to accelerate complete verification,\nachieving significant speedups compared to expensive linear programming (LP)\nbased techniques. However, they cannot fully handle the per-neuron split\nconstraints introduced by BaB like LP verifiers do, leading to looser bounds\nand hurting their verification efficiency. In this work, we develop\n$\\beta$-CROWN, a new bound propagation based method that can fully encode\nper-neuron splits via optimizable parameters $\\beta$. When the optimizable\nparameters are jointly optimized in intermediate layers, $\\beta$-CROWN has the\npotential of producing better bounds than typical LP verifiers with neuron\nsplit constraints, while being efficiently parallelizable on GPUs. Applied to\nthe complete verification setting, $\\beta$-CROWN is close to three orders of\nmagnitude faster than LP-based BaB methods for robustness verification, and\nalso over twice faster than state-of-the-art GPU-based complete verifiers with\nsimilar timeout rates. By terminating BaB early, our method can also be used\nfor incomplete verification. Compared to the state-of-the-art\nsemidefinite-programming (SDP) based verifier, we show a substantial leap\nforward by greatly reducing the gap between verified accuracy and empirical\nadversarial attack accuracy, from 35% (SDP) to 12% on an adversarially trained\nMNIST network ($\\epsilon=0.3$), while being 47 times faster. Our code is\navailable at https://github.com/KaidiXu/Beta-CROWN\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 11:56:54 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Wang", "Shiqi", ""], ["Zhang", "Huan", ""], ["Xu", "Kaidi", ""], ["Lin", "Xue", ""], ["Jana", "Suman", ""], ["Hsieh", "Cho-Jui", ""], ["Kolter", "J. Zico", ""]]}, {"id": "2103.06733", "submitter": "Simon Carbonnelle", "authors": "Carbonnelle Simon and Christophe De Vleeschouwer", "title": "Intraclass clustering: an implicit learning ability that regularizes\n  DNNs", "comments": "Published as a conference paper at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Several works have shown that the regularization mechanisms underlying deep\nneural networks' generalization performances are still poorly understood. In\nthis paper, we hypothesize that deep neural networks are regularized through\ntheir ability to extract meaningful clusters among the samples of a class. This\nconstitutes an implicit form of regularization, as no explicit training\nmechanisms or supervision target such behaviour. To support our hypothesis, we\ndesign four different measures of intraclass clustering, based on the neuron-\nand layer-level representations of the training data. We then show that these\nmeasures constitute accurate predictors of generalization performance across\nvariations of a large set of hyperparameters (learning rate, batch size,\noptimizer, weight decay, dropout rate, data augmentation, network depth and\nwidth).\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 15:26:27 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Simon", "Carbonnelle", ""], ["De Vleeschouwer", "Christophe", ""]]}, {"id": "2103.06752", "submitter": "Daniel Vollmers", "authors": "Daniel Vollmers (1), Rricha Jalota (1), Diego Moussallem (1), Hardik\n  Topiwala (1), Axel-Cyrille Ngonga Ngomo (1), and Ricardo Usbeck (2) ((1) Data\n  Science Group, Paderborn University, Germany, (2) Fraunhofer IAIS, Dresden,\n  Germany)", "title": "Knowledge Graph Question Answering using Graph-Pattern Isomorphism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Knowledge Graph Question Answering (KGQA) systems are based on machine\nlearning algorithms, requiring thousands of question-answer pairs as training\nexamples or natural language processing pipelines that need module fine-tuning.\nIn this paper, we present a novel QA approach, dubbed TeBaQA. Our approach\nlearns to answer questions based on graph isomorphisms from basic graph\npatterns of SPARQL queries. Learning basic graph patterns is efficient due to\nthe small number of possible patterns. This novel paradigm reduces the amount\nof training data necessary to achieve state-of-the-art performance. TeBaQA also\nspeeds up the domain adaption process by transforming the QA system development\ntask into a much smaller and easier data compilation task. In our evaluation,\nTeBaQA achieves state-of-the-art performance on QALD-8 and delivers comparable\nresults on QALD-9 and LC-QuAD v1. Additionally, we performed a fine-grained\nevaluation on complex queries that deal with aggregation and superlative\nquestions as well as an ablation study, highlighting future research\nchallenges.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 16:03:24 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Vollmers", "Daniel", ""], ["Jalota", "Rricha", ""], ["Moussallem", "Diego", ""], ["Topiwala", "Hardik", ""], ["Ngomo", "Axel-Cyrille Ngonga", ""], ["Usbeck", "Ricardo", ""]]}, {"id": "2103.06757", "submitter": "Nicol\\'as Cardozo", "authors": "Nicol\\'as Cardozo and Ivana Dusparic", "title": "Auto-COP: Adaptation Generation in Context-Oriented Programming using\n  Reinforcement Learning Options", "comments": "Submitted to The Art, Science, and Engineering of Programming\n  Journal. 22 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI cs.SE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Self-adaptive software systems continuously adapt in response to internal and\nexternal changes in their execution environment, captured as contexts. The COP\nparadigm posits a technique for the development of self-adaptive systems,\ncapturing their main characteristics with specialized programming language\nconstructs. COP adaptations are specified as independent modules composed in\nand out of the base system as contexts are activated and deactivated in\nresponse to sensed circumstances from the surrounding environment. However, the\ndefinition of adaptations, their contexts and associated specialized behavior,\nneed to be specified at design time. In complex CPS this is intractable due to\nnew unpredicted operating conditions. We propose Auto-COP, a new technique to\nenable generation of adaptations at run time. Auto-COP uses RL options to build\naction sequences, based on the previous instances of the system execution.\nOptions are explored in interaction with the environment, and the most suitable\noptions for each context are used to generate adaptations exploiting COP. To\nvalidate Auto-COP, we present two case studies exhibiting different system\ncharacteristics and application domains: a driving assistant and a robot\ndelivery system. We present examples of Auto-COP code generated at run time, to\nillustrate the types of circumstances (contexts) requiring adaptation, and the\ncorresponding generated adaptations for each context. We confirm that the\ngenerated adaptations exhibit correct system behavior measured by\ndomain-specific performance metrics, while reducing the number of required\nexecution/actuation steps by a factor of two showing that the adaptations are\nregularly selected by the running system as adaptive behavior is more\nappropriate than the execution of primitive actions.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 16:14:56 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Cardozo", "Nicol\u00e1s", ""], ["Dusparic", "Ivana", ""]]}, {"id": "2103.06758", "submitter": "Tuhin Chakrabarty Mr", "authors": "Tuhin Chakrabarty, Christopher Hidey, Smaranda Muresan", "title": "ENTRUST: Argument Reframing with Language Models and Entailment", "comments": "NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Framing involves the positive or negative presentation of an argument or\nissue depending on the audience and goal of the speaker (Entman 1983).\nDifferences in lexical framing, the focus of our work, can have large effects\non peoples' opinions and beliefs. To make progress towards reframing arguments\nfor positive effects, we create a dataset and method for this task. We use a\nlexical resource for \"connotations\" to create a parallel corpus and propose a\nmethod for argument reframing that combines controllable text generation\n(positive connotation) with a post-decoding entailment component (same\ndenotation). Our results show that our method is effective compared to strong\nbaselines along the dimensions of fluency, meaning, and\ntrustworthiness/reduction of fear.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 16:15:13 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 15:25:23 GMT"}, {"version": "v3", "created": "Sun, 11 Apr 2021 00:06:43 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Chakrabarty", "Tuhin", ""], ["Hidey", "Christopher", ""], ["Muresan", "Smaranda", ""]]}, {"id": "2103.06769", "submitter": "Pierre-Yves Oudeyer", "authors": "Manfred Eppe and Pierre-Yves Oudeyer", "title": "Intelligent behavior depends on the ecological niche: Scaling up AI to\n  human-like intelligence in socio-cultural environments", "comments": "Keywords: developmental AI, general artificial intelligence,\n  human-like AI, embodiment, cultural evolution, language, socio-cultural\n  skills", "journal-ref": "KI - K\\\"unstliche Intelligenz KI - K\\\"unstliche Intelligenz\n  (German Journal of Artificial Intelligence), 2021", "doi": "10.1007/s13218-020-00696-1", "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper outlines a perspective on the future of AI, discussing directions\nfor machines models of human-like intelligence. We explain how developmental\nand evolutionary theories of human cognition should further inform artificial\nintelligence. We emphasize the role of ecological niches in sculpting\nintelligent behavior, and in particular that human intelligence was\nfundamentally shaped to adapt to a constantly changing socio-cultural\nenvironment. We argue that a major limit of current work in AI is that it is\nmissing this perspective, both theoretically and experimentally. Finally, we\ndiscuss the promising approach of developmental artificial intelligence,\nmodeling infant development through multi-scale interaction between\nintrinsically motivated learning, embodiment and a fastly changing\nsocio-cultural environment. This paper takes the form of an interview of\nPierre-Yves Oudeyer by Mandred Eppe, organized within the context of a KI -\nK{\\\"{u}}nstliche Intelligenz special issue in developmental robotics.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 16:24:00 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Eppe", "Manfred", ""], ["Oudeyer", "Pierre-Yves", ""]]}, {"id": "2103.06804", "submitter": "Ljubisa Stankovic", "authors": "Ljubisa Stankovic, Milos Brajovic, Danilo Mandic, Isidora Stankovic,\n  Milos Dakovic", "title": "Improved Coherence Index-Based Bound in Compressive Sensing", "comments": "5 pages, 1 figure", "journal-ref": null, "doi": "10.1109/LSP.2021.3084559", "report-no": null, "categories": "cs.IT cs.AI math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Within the Compressive Sensing (CS) paradigm, sparse signals can be\nreconstructed based on a reduced set of measurements. Reliability of the\nsolution is determined by the uniqueness condition. With its mathematically\ntractable and feasible calculation, coherence index is one of very few CS\nmetrics with a considerable practical importance. In this paper, we propose an\nimprovement of the coherence based uniqueness relation for the matching pursuit\nalgorithms. Starting from a simple and intuitive derivation of the standard\nuniqueness condition based on the coherence index, we derive a less\nconservative coherence index-based lower bound for signal sparsity. The results\nare generalized to the uniqueness condition of the $l_0$-norm minimization for\na signal represented in two orthonormal bases.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 17:19:06 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Stankovic", "Ljubisa", ""], ["Brajovic", "Milos", ""], ["Mandic", "Danilo", ""], ["Stankovic", "Isidora", ""], ["Dakovic", "Milos", ""]]}, {"id": "2103.06807", "submitter": "Kashyap Todi", "authors": "Kashyap Todi, Gilles Bailly, Luis A. Leiva, Antti Oulasvirta", "title": "Adapting User Interfaces with Model-based Reinforcement Learning", "comments": "13 pages, 10 figures, ACM CHI 2021 Full Paper", "journal-ref": null, "doi": "10.1145/3411764.3445497", "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adapting an interface requires taking into account both the positive and\nnegative effects that changes may have on the user. A carelessly picked\nadaptation may impose high costs to the user -- for example, due to surprise or\nrelearning effort -- or \"trap\" the process to a suboptimal design immaturely.\nHowever, effects on users are hard to predict as they depend on factors that\nare latent and evolve over the course of interaction. We propose a novel\napproach for adaptive user interfaces that yields a conservative adaptation\npolicy: It finds beneficial changes when there are such and avoids changes when\nthere are none. Our model-based reinforcement learning method plans sequences\nof adaptations and consults predictive HCI models to estimate their effects. We\npresent empirical and simulation results from the case of adaptive menus,\nshowing that the method outperforms both a non-adaptive and a frequency-based\npolicy.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 17:24:34 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Todi", "Kashyap", ""], ["Bailly", "Gilles", ""], ["Leiva", "Luis A.", ""], ["Oulasvirta", "Antti", ""]]}, {"id": "2103.06816", "submitter": "Arko Barman", "authors": "Hannah Lei (1), Weiqi Lu (1), Alan Ji (1), Emmett Bertram (1), Paul\n  Gao (1), Xiaoqian Jiang (2), Arko Barman (1) ((1) Rice University, Houston,\n  United States, (2) The University of Texas Health Science Center at Houston,\n  United States)", "title": "COVID-19 Smart Chatbot Prototype for Patient Monitoring", "comments": "This manuscript is under consideration for the AMIA 2021 Annual\n  Symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many COVID-19 patients developed prolonged symptoms after the infection,\nincluding fatigue, delirium, and headache. The long-term health impact of these\nconditions is still not clear. It is necessary to develop a way to follow up\nwith these patients for monitoring their health status to support timely\nintervention and treatment. In the lack of sufficient human resources to follow\nup with patients, we propose a novel smart chatbot solution backed with machine\nlearning to collect information (i.e., generating digital diary) in a\npersonalized manner. In this article, we describe the design framework and\ncomponents of our prototype.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 17:37:55 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Lei", "Hannah", ""], ["Lu", "Weiqi", ""], ["Ji", "Alan", ""], ["Bertram", "Emmett", ""], ["Gao", "Paul", ""], ["Jiang", "Xiaoqian", ""], ["Barman", "Arko", ""]]}, {"id": "2103.06846", "submitter": "Nicolas Bredeche", "authors": "Paul Ecoffet, Nicolas Fontbonne, Jean-Baptiste Andr\\'e, Nicolas\n  Bredeche", "title": "Policy Search with Rare Significant Events: Choosing the Right Partner\n  to Cooperate with", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper focuses on a class of reinforcement learning problems where\nsignificant events are rare and limited to a single positive reward per\nepisode. A typical example is that of an agent who has to choose a partner to\ncooperate with, while a large number of partners are simply not interested in\ncooperating, regardless of what the agent has to offer. We address this problem\nin a continuous state and action space with two different kinds of search\nmethods: a gradient policy search method and a direct policy search method\nusing an evolution strategy. We show that when significant events are rare,\ngradient information is also scarce, making it difficult for policy gradient\nsearch methods to find an optimal policy, with or without a deep neural\narchitecture. On the other hand, we show that direct policy search methods are\ninvariant to the rarity of significant events, which is yet another\nconfirmation of the unique role evolutionary algorithms has to play as a\nreinforcement learning method.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 18:14:41 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Ecoffet", "Paul", ""], ["Fontbonne", "Nicolas", ""], ["Andr\u00e9", "Jean-Baptiste", ""], ["Bredeche", "Nicolas", ""]]}, {"id": "2103.06854", "submitter": "Laura Giordano", "authors": "Laura Giordano, Valentina Gliozzi, Daniele Theseider Dupr\\'e", "title": "A conditional, a fuzzy and a probabilistic interpretation of\n  self-organising maps", "comments": "28 pages, 1 figure. arXiv admin note: substantial text overlap with\n  arXiv:2008.13278", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we establish a link between preferential semantics for\ndescription logics and self-organising maps, which have been proposed as\npossible candidates to explain the psychological mechanisms underlying category\ngeneralisation. In particular, we show that a concept-wise multipreference\nsemantics, which takes into account preferences with respect to different\nconcepts and has been recently proposed for defeasible description logics, can\nbe used to to provide a logical interpretation of SOMs. We also provide a\nlogical interpretation of SOMs in terms of a fuzzy description logic as well as\na probabilistic account.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 18:31:00 GMT"}], "update_date": "2021-03-12", "authors_parsed": [["Giordano", "Laura", ""], ["Gliozzi", "Valentina", ""], ["Dupr\u00e9", "Daniele Theseider", ""]]}, {"id": "2103.06859", "submitter": "Beren Millidge Mr", "authors": "Beren Millidge, Alexander Tschantz, Anil Seth, Christopher Buckley", "title": "Understanding the Origin of Information-Seeking Exploration in\n  Probabilistic Objectives for Control", "comments": "11-03-21 initial upload. 14-03-21 fix Charnov citation. 16-03-21\n  another fix. 25-06-21 more fixes plus numerical simulations. 30-06-21 minor\n  fixes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The exploration-exploitation trade-off is central to the description of\nadaptive behaviour in fields ranging from machine learning, to biology, to\neconomics. While many approaches have been taken, one approach to solving this\ntrade-off has been to equip or propose that agents possess an intrinsic\n'exploratory drive' which is often implemented in terms of maximizing the\nagents information gain about the world -- an approach which has been widely\nstudied in machine learning and cognitive science. In this paper we\nmathematically investigate the nature and meaning of such approaches and\ndemonstrate that this combination of utility maximizing and information-seeking\nbehaviour arises from the minimization of an entirely difference class of\nobjectives we call divergence objectives. We propose a dichotomy in the\nobjective functions underlying adaptive behaviour between \\emph{evidence}\nobjectives, which correspond to well-known reward or utility maximizing\nobjectives in the literature, and \\emph{divergence} objectives which instead\nseek to minimize the divergence between the agent's expected and desired\nfutures, and argue that this new class of divergence objectives could form the\nmathematical foundation for a much richer understanding of the exploratory\ncomponents of adaptive and intelligent action, beyond simply greedy utility\nmaximization.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 18:42:39 GMT"}, {"version": "v2", "created": "Sun, 14 Mar 2021 14:31:46 GMT"}, {"version": "v3", "created": "Tue, 16 Mar 2021 13:07:41 GMT"}, {"version": "v4", "created": "Fri, 25 Jun 2021 13:05:59 GMT"}, {"version": "v5", "created": "Wed, 30 Jun 2021 17:20:43 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Millidge", "Beren", ""], ["Tschantz", "Alexander", ""], ["Seth", "Anil", ""], ["Buckley", "Christopher", ""]]}, {"id": "2103.06879", "submitter": "Fabio Pizzati", "authors": "Fabio Pizzati, Pietro Cerri, Raoul de Charette", "title": "CoMoGAN: continuous model-guided image-to-image translation", "comments": "CVPR 2021 oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CoMoGAN is a continuous GAN relying on the unsupervised reorganization of the\ntarget data on a functional manifold. To that matter, we introduce a new\nFunctional Instance Normalization layer and residual mechanism, which together\ndisentangle image content from position on target manifold. We rely on naive\nphysics-inspired models to guide the training while allowing private\nmodel/translations features. CoMoGAN can be used with any GAN backbone and\nallows new types of image translation, such as cyclic image translation like\ntimelapse generation, or detached linear translation. On all datasets, it\noutperforms the literature. Our code is available at\nhttp://github.com/cv-rits/CoMoGAN .\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 18:59:50 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 17:59:57 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Pizzati", "Fabio", ""], ["Cerri", "Pietro", ""], ["de Charette", "Raoul", ""]]}, {"id": "2103.06908", "submitter": "Ivana Dusparic", "authors": "Ivana Dusparic, Nicolas Cardozo", "title": "Adaptation to Unknown Situations as the Holy Grail of Learning-Based\n  Self-Adaptive Systems: Research Directions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Self-adaptive systems continuously adapt to changes in their execution\nenvironment. Capturing all possible changes to define suitable behaviour\nbeforehand is unfeasible, or even impossible in the case of unknown changes,\nhence human intervention may be required. We argue that adapting to unknown\nsituations is the ultimate challenge for self-adaptive systems. Learning-based\napproaches are used to learn the suitable behaviour to exhibit in the case of\nunknown situations, to minimize or fully remove human intervention. While such\napproaches can, to a certain extent, generalize existing adaptations to new\nsituations, there is a number of breakthroughs that need to be achieved before\nsystems can adapt to general unknown and unforeseen situations. We posit the\nresearch directions that need to be explored to achieve unanticipated\nadaptation from the perspective of learning-based self-adaptive systems. At\nminimum, systems need to define internal representations of previously unseen\nsituations on-the-fly, extrapolate the relationship to the previously\nencountered situations to evolve existing adaptations, and reason about the\nfeasibility of achieving their intrinsic goals in the new set of conditions. We\nclose discussing whether, even when we can, we should indeed build systems that\ndefine their own behaviour and adapt their goals, without involving a human\nsupervisor.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 19:07:02 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Dusparic", "Ivana", ""], ["Cardozo", "Nicolas", ""]]}, {"id": "2103.06967", "submitter": "Vijay Gupta", "authors": "Martin Figura, Krishna Chaitanya Kosaraju, and Vijay Gupta", "title": "Adversarial attacks in consensus-based multi-agent reinforcement\n  learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.AI cs.LG cs.SY stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, many cooperative distributed multi-agent reinforcement learning\n(MARL) algorithms have been proposed in the literature. In this work, we study\nthe effect of adversarial attacks on a network that employs a consensus-based\nMARL algorithm. We show that an adversarial agent can persuade all the other\nagents in the network to implement policies that optimize an objective that it\ndesires. In this sense, the standard consensus-based MARL algorithms are\nfragile to attacks.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 21:44:18 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Figura", "Martin", ""], ["Kosaraju", "Krishna Chaitanya", ""], ["Gupta", "Vijay", ""]]}, {"id": "2103.07009", "submitter": "Pengtao Xie", "authors": "Parth Sheth, Yueyu Jiang, Pengtao Xie", "title": "Learning by Teaching, with Application to Neural Architecture Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In human learning, an effective skill in improving learning outcomes is\nlearning by teaching: a learner deepens his/her understanding of a topic by\nteaching this topic to others. In this paper, we aim to borrow this\nteaching-driven learning methodology from humans and leverage it to train more\nperformant machine learning models, by proposing a novel ML framework referred\nto as learning by teaching (LBT). In the LBT framework, a teacher model\nimproves itself by teaching a student model to learn well. Specifically, the\nteacher creates a pseudo-labeled dataset and uses it to train a student model.\nBased on how the student performs on a validation dataset, the teacher\nre-learns its model and re-teaches the student until the student achieves great\nvalidation performance. Our framework is based on three-level optimization\nwhich contains three stages: teacher learns; teacher teaches student; teacher\nre-learns based on how well the student performs. A simple but efficient\nalgorithm is developed to solve the three-level optimization problem. We apply\nLBT to search neural architectures on CIFAR-10, CIFAR-100, and ImageNet. The\nefficacy of our method is demonstrated in various experiments.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 23:50:38 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Sheth", "Parth", ""], ["Jiang", "Yueyu", ""], ["Xie", "Pengtao", ""]]}, {"id": "2103.07011", "submitter": "Liang Qiu", "authors": "Liang Qiu, Yizhou Zhao, Yuan Liang, Pan Lu, Weiyan Shi, Zhou Yu,\n  Song-Chun Zhu", "title": "Towards Socially Intelligent Agents with Mental State Transition and\n  Human Utility", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building a socially intelligent agent involves many challenges, one of which\nis to track the agent's mental state transition and teach the agent to make\nrational decisions guided by its utility like a human. Towards this end, we\npropose to incorporate a mental state parser and utility model into dialogue\nagents. The hybrid mental state parser extracts information from both the\ndialogue and event observations and maintains a graphical representation of the\nagent's mind; Meanwhile, the utility model is a ranking model that learns human\npreferences from a crowd-sourced social commonsense dataset, Social IQA.\nEmpirical results show that the proposed model attains state-of-the-art\nperformance on the dialogue/action/emotion prediction task in the fantasy\ntext-adventure game dataset, LIGHT. We also show example cases to demonstrate:\n(\\textit{i}) how the proposed mental state parser can assist agent's decision\nby grounding on the context like locations and objects, and (\\textit{ii}) how\nthe utility model can help the agent make reasonable decisions in a dilemma. To\nthe best of our knowledge, we are the first work that builds a socially\nintelligent agent by incorporating a hybrid mental state parser for both\ndiscrete events and continuous dialogues parsing and human-like utility\nmodeling.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 00:06:51 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Qiu", "Liang", ""], ["Zhao", "Yizhou", ""], ["Liang", "Yuan", ""], ["Lu", "Pan", ""], ["Shi", "Weiyan", ""], ["Yu", "Zhou", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "2103.07013", "submitter": "Brennan Shacklett", "authors": "Brennan Shacklett, Erik Wijmans, Aleksei Petrenko, Manolis Savva,\n  Dhruv Batra, Vladlen Koltun, Kayvon Fatahalian", "title": "Large Batch Simulation for Deep Reinforcement Learning", "comments": "Published as a conference paper at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We accelerate deep reinforcement learning-based training in visually complex\n3D environments by two orders of magnitude over prior work, realizing\nend-to-end training speeds of over 19,000 frames of experience per second on a\nsingle GPU and up to 72,000 frames per second on a single eight-GPU machine.\nThe key idea of our approach is to design a 3D renderer and embodied navigation\nsimulator around the principle of \"batch simulation\": accepting and executing\nlarge batches of requests simultaneously. Beyond exposing large amounts of work\nat once, batch simulation allows implementations to amortize in-memory storage\nof scene assets, rendering work, data loading, and synchronization costs across\nmany simulation requests, dramatically improving the number of simulated agents\nper GPU and overall simulation throughput. To balance DNN inference and\ntraining costs with faster simulation, we also build a computationally\nefficient policy DNN that maintains high task performance, and modify training\nalgorithms to maintain sample efficiency when training with large mini-batches.\nBy combining batch simulation and DNN performance optimizations, we demonstrate\nthat PointGoal navigation agents can be trained in complex 3D environments on a\nsingle GPU in 1.5 days to 97% of the accuracy of agents trained on a prior\nstate-of-the-art system using a 64-GPU cluster over three days. We provide\nopen-source reference implementations of our batch 3D renderer and simulator to\nfacilitate incorporation of these ideas into RL systems.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 00:22:50 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Shacklett", "Brennan", ""], ["Wijmans", "Erik", ""], ["Petrenko", "Aleksei", ""], ["Savva", "Manolis", ""], ["Batra", "Dhruv", ""], ["Koltun", "Vladlen", ""], ["Fatahalian", "Kayvon", ""]]}, {"id": "2103.07018", "submitter": "Pengtao Xie", "authors": "Hao Ban, Pengtao Xie", "title": "Interleaving Learning, with Application to Neural Architecture Search", "comments": "arXiv admin note: substantial text overlap with arXiv:2012.04863", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Interleaving learning is a human learning technique where a learner\ninterleaves the studies of multiple topics, which increases long-term retention\nand improves ability to transfer learned knowledge. Inspired by the\ninterleaving learning technique of humans, in this paper we explore whether\nthis learning methodology is beneficial for improving the performance of\nmachine learning models as well. We propose a novel machine learning framework\nreferred to as interleaving learning (IL). In our framework, a set of models\ncollaboratively learn a data encoder in an interleaving fashion: the encoder is\ntrained by model 1 for a while, then passed to model 2 for further training,\nthen model 3, and so on; after trained by all models, the encoder returns back\nto model 1 and is trained again, then moving to model 2, 3, etc. This process\nrepeats for multiple rounds. Our framework is based on multi-level optimization\nconsisting of multiple inter-connected learning stages. An efficient\ngradient-based algorithm is developed to solve the multi-level optimization\nproblem. We apply interleaving learning to search neural architectures for\nimage classification on CIFAR-10, CIFAR-100, and ImageNet. The effectiveness of\nour method is strongly demonstrated by the experimental results.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 00:54:22 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Ban", "Hao", ""], ["Xie", "Pengtao", ""]]}, {"id": "2103.07040", "submitter": "Yusen Lin", "authors": "Yusen Lin, Jiayong Lin, Shuaicheng Zhang, Haoying Dai", "title": "Bilingual Dictionary-based Language Model Pretraining for Neural Machine\n  Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent studies have demonstrated a perceivable improvement on the performance\nof neural machine translation by applying cross-lingual language model\npretraining (Lample and Conneau, 2019), especially the Translation Language\nModeling (TLM). To alleviate the need for expensive parallel corpora by TLM, in\nthis work, we incorporate the translation information from dictionaries into\nthe pretraining process and propose a novel Bilingual Dictionary-based Language\nModel (BDLM). We evaluate our BDLM in Chinese, English, and Romanian. For\nChinese-English, we obtained a 55.0 BLEU on WMT-News19 (Tiedemann, 2012) and a\n24.3 BLEU on WMT20 news-commentary, outperforming the Vanilla Transformer\n(Vaswani et al., 2017) by more than 8.4 BLEU and 2.3 BLEU, respectively.\nAccording to our results, the BDLM also has advantages on convergence speed and\npredicting rare words. The increase in BLEU for WMT16 Romanian-English also\nshows its effectiveness in low-resources language translation.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 02:01:22 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Lin", "Yusen", ""], ["Lin", "Jiayong", ""], ["Zhang", "Shuaicheng", ""], ["Dai", "Haoying", ""]]}, {"id": "2103.07080", "submitter": "Chris Connell", "authors": "Chris Connell and Yang Wang", "title": "DynACPD Embedding Algorithm for Prediction Tasks in Dynamic Networks", "comments": "35 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical network embeddings create a low dimensional representation of the\nlearned relationships between features across nodes. Such embeddings are\nimportant for tasks such as link prediction and node classification. In the\ncurrent paper, we consider low dimensional embeddings of dynamic networks, that\nis a family of time varying networks where there exist both temporal and\nspatial link relationships between nodes. We present novel embedding methods\nfor a dynamic network based on higher order tensor decompositions for tensorial\nrepresentations of the dynamic network. In one sense, our embeddings are\nanalogous to spectral embedding methods for static networks. We provide a\nrationale for our algorithms via a mathematical analysis of some potential\nreasons for their effectiveness. Finally, we demonstrate the power and\nefficiency of our approach by comparing our algorithms' performance on the link\nprediction task against an array of current baseline methods across three\ndistinct real-world dynamic networks.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 04:36:42 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Connell", "Chris", ""], ["Wang", "Yang", ""]]}, {"id": "2103.07084", "submitter": "Takayuki Osa", "authors": "Takayuki Osa, Voot Tangkaratt and Masashi Sugiyama", "title": "Discovering Diverse Solutions in Deep Reinforcement Learning", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Reinforcement learning (RL) algorithms are typically limited to learning a\nsingle solution of a specified task, even though there often exists diverse\nsolutions to a given task. Compared with learning a single solution, learning a\nset of diverse solutions is beneficial because diverse solutions enable robust\nfew-shot adaptation and allow the user to select a preferred solution. Although\nprevious studies have showed that diverse behaviors can be modeled with a\npolicy conditioned on latent variables, an approach for modeling an infinite\nset of diverse solutions with continuous latent variables has not been\ninvestigated. In this study, we propose an RL method that can learn infinitely\nmany solutions by training a policy conditioned on a continuous or discrete\nlow-dimensional latent variable. Through continuous control tasks, we\ndemonstrate that our method can learn diverse solutions in a data-efficient\nmanner and that the solutions can be used for few-shot adaptation to solve\nunseen tasks.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 04:54:31 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Osa", "Takayuki", ""], ["Tangkaratt", "Voot", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "2103.07110", "submitter": "Dattaraj Rao", "authors": "Shraddha Mane, Dattaraj Rao", "title": "Explaining Network Intrusion Detection System Using Explainable AI\n  Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cybersecurity is a domain where the data distribution is constantly changing\nwith attackers exploring newer patterns to attack cyber infrastructure.\nIntrusion detection system is one of the important layers in cyber safety in\ntoday's world. Machine learning based network intrusion detection systems\nstarted showing effective results in recent years. With deep learning models,\ndetection rates of network intrusion detection system are improved. More\naccurate the model, more the complexity and hence less the interpretability.\nDeep neural networks are complex and hard to interpret which makes difficult to\nuse them in production as reasons behind their decisions are unknown. In this\npaper, we have used deep neural network for network intrusion detection and\nalso proposed explainable AI framework to add transparency at every stage of\nmachine learning pipeline. This is done by leveraging Explainable AI algorithms\nwhich focus on making ML models less of black boxes by providing explanations\nas to why a prediction is made. Explanations give us measurable factors as to\nwhat features influence the prediction of a cyberattack and to what degree.\nThese explanations are generated from SHAP, LIME, Contrastive Explanations\nMethod, ProtoDash and Boolean Decision Rules via Column Generation. We apply\nthese approaches to NSL KDD dataset for intrusion detection system and\ndemonstrate results.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 07:15:09 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Mane", "Shraddha", ""], ["Rao", "Dattaraj", ""]]}, {"id": "2103.07116", "submitter": "Jiaoyang Li", "authors": "Jiaoyang Li, Daniel Harabor, Peter J. Stuckey, Sven Koenig", "title": "Pairwise Symmetry Reasoning for Multi-Agent Path Finding Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-Agent Path Finding (MAPF) is a challenging combinatorial problem that\nasks us to plan collision-free paths for a team of cooperative agents. In this\nwork, we show that one of the reasons why MAPF is so hard to solve is due to a\nphenomenon called pairwise symmetry, which occurs when two agents have many\ndifferent paths to their target locations, all of which appear promising, but\nevery combination of them results in a collision. We identify several classes\nof pairwise symmetries and show that each one arises commonly in practice and\ncan produce an exponential explosion in the space of possible collision\nresolutions, leading to unacceptable runtimes for current state-of-the-art\n(bounded-sub)optimal MAPF algorithms. We propose a variety of reasoning\ntechniques that detect the symmetries efficiently as they arise and resolve\nthem by using specialized constraints to eliminate all permutations of pairwise\ncolliding paths in a single branching step. We implement these ideas in the\ncontext of the leading optimal MAPF algorithm CBS and show that the addition of\nthe symmetry reasoning techniques can have a dramatic positive effect on its\nperformance - we report a reduction in the number of node expansions by up to\nfour orders of magnitude and an increase in scalability by up to thirty times.\nThese gains allow us to solve to optimality a variety of challenging MAPF\ninstances previously considered out of reach for CBS.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 07:27:35 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Li", "Jiaoyang", ""], ["Harabor", "Daniel", ""], ["Stuckey", "Peter J.", ""], ["Koenig", "Sven", ""]]}, {"id": "2103.07150", "submitter": "Renhao Lu", "authors": "Renhao Lu, Weizhe Zhang, Qiong Li, Xiaoxiong Zhong and Athanasios V.\n  Vasilakos", "title": "Auction Based Clustered Federated Learning in Mobile Edge Computing\n  System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, mobile clients' computing ability and storage capacity have\ngreatly improved, efficiently dealing with some applications locally. Federated\nlearning is a promising distributed machine learning solution that uses local\ncomputing and local data to train the Artificial Intelligence (AI) model.\nCombining local computing and federated learning can train a powerful AI model\nunder the premise of ensuring local data privacy while making full use of\nmobile clients' resources. However, the heterogeneity of local data, that is,\nNon-independent and identical distribution (Non-IID) and imbalance of local\ndata size, may bring a bottleneck hindering the application of federated\nlearning in mobile edge computing (MEC) system. Inspired by this, we propose a\ncluster-based clients selection method that can generate a federated virtual\ndataset that satisfies the global distribution to offset the impact of data\nheterogeneity and proved that the proposed scheme could converge to an\napproximate optimal solution. Based on the clustering method, we propose an\nauction-based clients selection scheme within each cluster that fully considers\nthe system's energy heterogeneity and gives the Nash equilibrium solution of\nthe proposed scheme for balance the energy consumption and improving the\nconvergence rate. The simulation results show that our proposed selection\nmethods and auction-based federated learning can achieve better performance\nwith the Convolutional Neural Network model (CNN) under different data\ndistributions.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 08:54:27 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Lu", "Renhao", ""], ["Zhang", "Weizhe", ""], ["Li", "Qiong", ""], ["Zhong", "Xiaoxiong", ""], ["Vasilakos", "Athanasios V.", ""]]}, {"id": "2103.07184", "submitter": "Anahita Farhang Ghahfarokhi", "authors": "Anahita Farhang Ghahfarokhi, Alessandro Berti, Wil M.P. van der Aalst", "title": "Process Comparison Using Object-Centric Process Cubes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Process mining provides ways to analyze business processes. Common process\nmining techniques consider the process as a whole. However, in real-life\nbusiness processes, different behaviors exist that make the overall process too\ncomplex to interpret. Process comparison is a branch of process mining that\nisolates different behaviors of the process from each other by using process\ncubes. Process cubes organize event data using different dimensions. Each cell\ncontains a set of events that can be used as an input to apply process mining\ntechniques. Existing work on process cubes assume single case notions. However,\nin real processes, several case notions (e.g., order, item, package, etc.) are\nintertwined. Object-centric process mining is a new branch of process mining\naddressing multiple case notions in a process. To make a bridge between\nobject-centric process mining and process comparison, we propose a process cube\nframework, which supports process cube operations such as slice and dice on\nobject-centric event logs. To facilitate the comparison, the framework is\nintegrated with several object-centric process discovery approaches.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 10:08:28 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Ghahfarokhi", "Anahita Farhang", ""], ["Berti", "Alessandro", ""], ["van der Aalst", "Wil M. P.", ""]]}, {"id": "2103.07224", "submitter": "Fu Song", "authors": "Yedi Zhang and Zhe Zhao and Guangke Chen and Fu Song and Taolue Chen", "title": "BDD4BNN: A BDD-based Quantitative Analysis Framework for Binarized\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.LO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Verifying and explaining the behavior of neural networks is becoming\nincreasingly important, especially when they are deployed in safety-critical\napplications. In this paper, we study verification problems for Binarized\nNeural Networks (BNNs), the 1-bit quantization of general real-numbered neural\nnetworks. Our approach is to encode BNNs into Binary Decision Diagrams (BDDs),\nwhich is done by exploiting the internal structure of the BNNs. In particular,\nwe translate the input-output relation of blocks in BNNs to cardinality\nconstraints which are then encoded by BDDs. Based on the encoding, we develop a\nquantitative verification framework for BNNs where precise and comprehensive\nanalysis of BNNs can be performed. We demonstrate the application of our\nframework by providing quantitative robustness analysis and interpretability\nfor BNNs. We implement a prototype tool BDD4BNN and carry out extensive\nexperiments which confirm the effectiveness and efficiency of our approach.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 12:02:41 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Zhang", "Yedi", ""], ["Zhao", "Zhe", ""], ["Chen", "Guangke", ""], ["Song", "Fu", ""], ["Chen", "Taolue", ""]]}, {"id": "2103.07230", "submitter": "Chaorong Li", "authors": "Chaorong Li, Malu Zhang, Wei Huang, Fengqing Qin, Anping Zeng,\n  Yuanyuan Huang", "title": "Sequential Random Network for Fine-grained Image Classification", "comments": "The performance of the model is very severely affected by the order\n  of the test samples", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep Convolutional Neural Network (DCNN) and Transformer have achieved\nremarkable successes in image recognition. However, their performance in\nfine-grained image recognition is still difficult to meet the requirements of\nactual needs. This paper proposes a Sequence Random Network (SRN) to enhance\nthe performance of DCNN. The output of DCNN is one-dimensional features. This\none-dimensional feature abstractly represents image information, but it does\nnot express well the detailed information of image. To address this issue, we\nuse the proposed SRN which composed of BiLSTM and several Tanh-Dropout blocks\n(called BiLSTM-TDN), to further process DCNN one-dimensional features for\nhighlighting the detail information of image. After the feature transform by\nBiLSTM-TDN, the recognition performance has been greatly improved. We conducted\nthe experiments on six fine-grained image datasets. Except for FGVC-Aircraft,\nthe accuracies of the proposed methods on the other datasets exceeded 99%.\nExperimental results show that BiLSTM-TDN is far superior to the existing\nstate-of-the-art methods. In addition to DCNN, BiLSTM-TDN can also be extended\nto other models, such as Transformer.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 12:16:03 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 09:59:50 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Li", "Chaorong", ""], ["Zhang", "Malu", ""], ["Huang", "Wei", ""], ["Qin", "Fengqing", ""], ["Zeng", "Anping", ""], ["Huang", "Yuanyuan", ""]]}, {"id": "2103.07241", "submitter": "Giovani Guizzo", "authors": "Giovani Guizzo, Federica Sarro, Jens Krinke, Silvia Regina Vergilio", "title": "Sentinel: A Hyper-Heuristic for the Generation of Mutant Reduction\n  Strategies", "comments": "in IEEE Transactions on Software Engineering", "journal-ref": null, "doi": "10.1109/TSE.2020.3002496", "report-no": null, "categories": "cs.SE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mutation testing is an effective approach to evaluate and strengthen software\ntest suites, but its adoption is currently limited by the mutants' execution\ncomputational cost. Several strategies have been proposed to reduce this cost\n(a.k.a. mutation cost reduction strategies), however none of them has proven to\nbe effective for all scenarios since they often need an ad-hoc manual selection\nand configuration depending on the software under test (SUT). In this paper, we\npropose a novel multi-objective evolutionary hyper-heuristic approach, dubbed\nSentinel, to automate the generation of optimal cost reduction strategies for\nevery new SUT. We evaluate Sentinel by carrying out a thorough empirical study\ninvolving 40 releases of 10 open-source real-world software systems and both\nbaseline and state-of-the-art strategies as a benchmark. We execute a total of\n4,800 experiments, and evaluate their results with both quality indicators and\nstatistical significance tests, following the most recent best practice in the\nliterature. The results show that strategies generated by Sentinel outperform\nthe baseline strategies in 95% of the cases always with large effect sizes.\nThey also obtain statistically significantly better results than\nstate-of-the-art strategies in 88% of the cases, with large effect sizes for\n95% of them. Also, our study reveals that the mutation strategies generated by\nSentinel for a given software version can be used without any loss in quality\nfor subsequently developed versions in 95% of the cases. These results show\nthat Sentinel is able to automatically generate mutation strategies that reduce\nmutation testing cost without affecting its testing effectiveness (i.e.\nmutation score), thus taking off from the tester's shoulders the burden of\nmanually selecting and configuring strategies for each SUT.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 12:38:51 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Guizzo", "Giovani", ""], ["Sarro", "Federica", ""], ["Krinke", "Jens", ""], ["Vergilio", "Silvia Regina", ""]]}, {"id": "2103.07250", "submitter": "Pedro Ramaciotti Morales", "authors": "Pedro Ramaciotti Morales, Jean-Philippe Cointet and Julio Laborde", "title": "Your most telling friends: Propagating latent ideological features on\n  Twitter using neighborhood coherence", "comments": "8 pages, 2020 ASONAM Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multidimensional scaling in networks allows for the discovery of latent\ninformation about their structure by embedding nodes in some feature space.\nIdeological scaling for users in social networks such as Twitter is an example,\nbut similar settings can include diverse applications in other networks and\neven media platforms or e-commerce. A growing literature of ideology scaling\nmethods in social networks restricts the scaling procedure to nodes that\nprovide interpretability of the feature space: on Twitter, it is common to\nconsider the sub-network of parliamentarians and their followers. This allows\nto interpret inferred latent features as indices for ideology-related concepts\ninspecting the position of members of parliament. While effective in inferring\nmeaningful features, this is generally restrained to these sub-networks,\nlimiting interesting applications such as country-wide measurement of\npolarization and its evolution. We propose two methods to propagate ideological\nfeatures beyond these sub-networks: one based on homophily (linked users have\nsimilar ideology), and the other on structural similarity (nodes with similar\nneighborhoods have similar ideologies). In our methods, we leverage the concept\nof neighborhood ideological coherence as a parameter for propagation. Using\nTwitter data, we produce an ideological scaling for 370K users, and analyze the\ntwo families of propagation methods on a population of 6.5M users. We find\nthat, when coherence is considered, the ideology of a user is better estimated\nfrom those with similar neighborhoods, than from their immediate neighbors.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 13:01:59 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Morales", "Pedro Ramaciotti", ""], ["Cointet", "Jean-Philippe", ""], ["Laborde", "Julio", ""]]}, {"id": "2103.07268", "submitter": "Ferhat Ozgur Catak", "authors": "Evren Catak, Ferhat Ozgur Catak, Arild Moldsvor", "title": "Adversarial Machine Learning Security Problems for 6G: mmWave Beam\n  Prediction Use-Case", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  6G is the next generation for the communication systems. In recent years,\nmachine learning algorithms have been applied widely in various fields such as\nhealth, transportation, and the autonomous car. The predictive algorithms will\nbe used in 6G problems. With the rapid developments of deep learning\ntechniques, it is critical to take the security concern into account to apply\nthe algorithms. While machine learning offers significant advantages for 6G, AI\nmodels' security is ignored. Since it has many applications in the real world,\nsecurity is a vital part of the algorithms. This paper has proposed a\nmitigation method for adversarial attacks against proposed 6G machine learning\nmodels for the millimeter-wave (mmWave) beam prediction with adversarial\nlearning. The main idea behind adversarial attacks against machine learning\nmodels is to produce faulty results by manipulating trained deep learning\nmodels for 6G applications for mmWave beam prediction use case. We have also\npresented the adversarial learning mitigation method's performance for 6G\nsecurity in millimeter-wave beam prediction application with fast gradient sign\nmethod attack. The mean square errors of the defended model and undefended\nmodel are very close.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 13:42:25 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Catak", "Evren", ""], ["Catak", "Ferhat Ozgur", ""], ["Moldsvor", "Arild", ""]]}, {"id": "2103.07295", "submitter": "Shuai Zheng", "authors": "Shuai Zheng, Zhenfeng Zhu, Zhizhe Liu, Shuiwang Ji, Jian Cheng, Yao\n  Zhao", "title": "Adversarial Graph Disentanglement", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  A real-world graph has a complex topological structure, which is often formed\nby the interaction of different latent factors. Disentanglement of these latent\nfactors can effectively improve the robustness and expressiveness of node\nrepresentation of graph. However, most existing methods lack consideration of\nthe intrinsic differences in relations between nodes caused by factor\nentanglement. In this paper, we propose an Adversarial Disentangled Graph\nConvolutional Network (ADGCN) for disentangled graph representation learning.\nSpecifically, a component-specific aggregation approach is proposed to achieve\nmicro-disentanglement by inferring latent components that caused the links\nbetween nodes. On the basis of micro-disentanglement, we further propose a\nmacro-disentanglement adversarial regularizer to improve the separability among\ncomponent distributions, thus restricting the interdependence among components.\nAdditionally, to reveal the topological graph structure, a diversity-preserving\nnode sampling approach is proposed, by which the graph structure can be\nprogressively refined in a way of local structure awareness. The experimental\nresults on various real-world graph data verify that our ADGCN obtains more\nfavorable performance over currently available alternatives.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 14:11:36 GMT"}, {"version": "v2", "created": "Tue, 6 Jul 2021 13:44:26 GMT"}], "update_date": "2021-07-07", "authors_parsed": [["Zheng", "Shuai", ""], ["Zhu", "Zhenfeng", ""], ["Liu", "Zhizhe", ""], ["Ji", "Shuiwang", ""], ["Cheng", "Jian", ""], ["Zhao", "Yao", ""]]}, {"id": "2103.07297", "submitter": "Xavier Ferrer Aran", "authors": "Danny S. Guam\\'an, Xavier Ferrer, Jose M. del Alamo, Jose Such", "title": "Automating the GDPR Compliance Assessment for Cross-border Personal Data\n  Transfers in Android Applications", "comments": "Author's copy of the submitted manuscript", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The General Data Protection Regulation (GDPR) aims to ensure that all\npersonal data processing activities are fair and transparent for the European\nUnion (EU) citizens, regardless of whether these are carried out within the EU\nor anywhere else. To this end, it sets strict requirements to transfer personal\ndata outside the EU. However, checking these requirements is a daunting task\nfor supervisory authorities, particularly in the mobile app domain due to the\nhuge number of apps available and their dynamic nature. In this paper, we\npropose a fully automated method to assess the compliance of mobile apps with\nthe GDPR requirements for cross-border personal data transfers. We have applied\nthe method to the top-free 10,080 apps from the Google Play Store. The results\nreveal that there is still a very significant gap between what app providers\nand third-party recipients do in practice and what is intended by the GDPR. A\nsubstantial 56% of analysed apps are potentially non-compliant with the GDPR\ncross-border transfer requirements.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 14:13:26 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Guam\u00e1n", "Danny S.", ""], ["Ferrer", "Xavier", ""], ["del Alamo", "Jose M.", ""], ["Such", "Jose", ""]]}, {"id": "2103.07356", "submitter": "Akira Taniguchi", "authors": "Akira Taniguchi, Ayako Fukawa, Hiroshi Yamakawa", "title": "Hippocampal formation-inspired probabilistic generative model", "comments": "Submitted to Neural Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We constructed a hippocampal formation (HPF)-inspired probabilistic\ngenerative model (HPF-PGM) using the structure-constrained interface\ndecomposition method. By modeling brain regions with PGMs, this model is\npositioned as a module that can be integrated as a whole-brain PGM. We discuss\nthe relationship between simultaneous localization and mapping (SLAM) in\nrobotics and the findings of HPF in neuroscience. Furthermore, we survey the\nmodeling for HPF and various computational models, including brain-inspired\nSLAM, spatial concept formation, and deep generative models. The HPF-PGM is a\ncomputational model that is highly consistent with the anatomical structure and\nfunctions of the HPF, in contrast to typical conventional SLAM models. By\nreferencing the brain, we suggest the importance of the integration of\negocentric/allocentric information from the entorhinal cortex to the\nhippocampus and the use of discrete-event queues.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 15:46:52 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Taniguchi", "Akira", ""], ["Fukawa", "Ayako", ""], ["Yamakawa", "Hiroshi", ""]]}, {"id": "2103.07364", "submitter": "Quanshi Zhang", "authors": "Jie Ren, Die Zhang, Yisen Wang, Lu Chen, Zhanpeng Zhou, Xu Cheng, Xin\n  Wang, Yiting Chen, Jie Shi, Quanshi Zhang", "title": "Game-theoretic Understanding of Adversarially Learned Features", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper aims to understand adversarial attacks and defense from a new\nperspecitve, i.e., the signal-processing behavior of DNNs. We novelly define\nthe multi-order interaction in game theory, which satisfies six properties.\nWith the multi-order interaction, we discover that adversarial attacks mainly\naffect high-order interactions to fool the DNN. Furthermore, we find that the\nrobustness of adversarially trained DNNs comes from category-specific low-order\ninteractions. Our findings provide more insights into and make a revision of\nprevious understanding for the shape bias of adversarially learned features.\nBesides, the multi-order interaction can also explain the recoverability of\nadversarial examples.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 15:56:28 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Ren", "Jie", ""], ["Zhang", "Die", ""], ["Wang", "Yisen", ""], ["Chen", "Lu", ""], ["Zhou", "Zhanpeng", ""], ["Cheng", "Xu", ""], ["Wang", "Xin", ""], ["Chen", "Yiting", ""], ["Shi", "Jie", ""], ["Zhang", "Quanshi", ""]]}, {"id": "2103.07371", "submitter": "Huizi Mao", "authors": "Huizi Mao, Sibo Zhu, Song Han, William J. Dally", "title": "PatchNet -- Short-range Template Matching for Efficient Video Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Object recognition is a fundamental problem in many video processing tasks,\naccurately locating seen objects at low computation cost paves the way for\non-device video recognition. We propose PatchNet, an efficient convolutional\nneural network to match objects in adjacent video frames. It learns the\npatchwise correlation features instead of pixel features. PatchNet is very\ncompact, running at just 58MFLOPs, $5\\times$ simpler than MobileNetV2. We\ndemonstrate its application on two tasks, video object detection and visual\nobject tracking. On ImageNet VID, PatchNet reduces the flops of R-FCN\nResNet-101 by 5x and EfficientDet-D0 by 3.4x with less than 1% mAP loss. On\nOTB2015, PatchNet reduces SiamFC and SiamRPN by 2.5x with no accuracy loss.\nExperiments on Jetson Nano further demonstrate 2.8x to 4.3x speed-ups\nassociated with flops reduction. Code is open sourced at\nhttps://github.com/RalphMao/PatchNet.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 20:56:07 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Mao", "Huizi", ""], ["Zhu", "Sibo", ""], ["Han", "Song", ""], ["Dally", "William J.", ""]]}, {"id": "2103.07403", "submitter": "Zahra Ghodsi", "authors": "Zahra Ghodsi, Siva Kumar Sastry Hari, Iuri Frosio, Timothy Tsai,\n  Alejandro Troccoli, Stephen W. Keckler, Siddharth Garg, Anima Anandkumar", "title": "Generating and Characterizing Scenarios for Safety Testing of Autonomous\n  Vehicles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Extracting interesting scenarios from real-world data as well as generating\nfailure cases is important for the development and testing of autonomous\nsystems. We propose efficient mechanisms to both characterize and generate\ntesting scenarios using a state-of-the-art driving simulator. For any scenario,\nour method generates a set of possible driving paths and identifies all the\npossible safe driving trajectories that can be taken starting at different\ntimes, to compute metrics that quantify the complexity of the scenario. We use\nour method to characterize real driving data from the Next Generation\nSimulation (NGSIM) project, as well as adversarial scenarios generated in\nsimulation. We rank the scenarios by defining metrics based on the complexity\nof avoiding accidents and provide insights into how the AV could have minimized\nthe probability of incurring an accident. We demonstrate a strong correlation\nbetween the proposed metrics and human intuition.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 17:00:23 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Ghodsi", "Zahra", ""], ["Hari", "Siva Kumar Sastry", ""], ["Frosio", "Iuri", ""], ["Tsai", "Timothy", ""], ["Troccoli", "Alejandro", ""], ["Keckler", "Stephen W.", ""], ["Garg", "Siddharth", ""], ["Anandkumar", "Anima", ""]]}, {"id": "2103.07449", "submitter": "Hongyin Luo", "authors": "Hongyin Luo, Shang-Wen Li, Seunghak Yu, James Glass", "title": "Cooperative Learning of Zero-Shot Machine Reading Comprehension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pretrained language models have significantly improved the performance of\ndown-stream language understanding tasks, including extractive question\nanswering, by providing high-quality contextualized word embeddings. However,\nlearning question answering models still need large-scaled data annotation in\nspecific domains. In this work, we propose a cooperative, self-play learning\nframework, REGEX, for question generation and answering. REGEX is built upon a\nmasked answer extraction task with an interactive learning environment\ncontaining an answer entity REcognizer, a question Generator, and an answer\nEXtractor. Given a passage with a masked entity, the generator generates a\nquestion around the entity, and the extractor is trained to extract the masked\nentity with the generated question and raw texts. The framework allows the\ntraining of question generation and answering models on any text corpora\nwithout annotation. We further leverage a reinforcement learning technique to\nreward generating high-quality questions and to improve the answer extraction\nmodel's performance. Experiment results show that REGEX outperforms the\nstate-of-the-art (SOTA) pretrained language models and zero-shot approaches on\nstandard question-answering benchmarks, and yields the new SOTA performance\nunder the zero-shot setting.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 18:22:28 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 07:05:01 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Luo", "Hongyin", ""], ["Li", "Shang-Wen", ""], ["Yu", "Seunghak", ""], ["Glass", "James", ""]]}, {"id": "2103.07460", "submitter": "Matteo Camilli Ph.D.", "authors": "Matteo Camilli, Michael Felderer, Andrea Giusti, Dominik T. Matt, Anna\n  Perini, Barbara Russo, Angelo Susi", "title": "Towards Risk Modeling for Collaborative AI", "comments": "4 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Collaborative AI systems aim at working together with humans in a shared\nspace to achieve a common goal. This setting imposes potentially hazardous\ncircumstances due to contacts that could harm human beings. Thus, building such\nsystems with strong assurances of compliance with requirements domain specific\nstandards and regulations is of greatest importance. Challenges associated with\nthe achievement of this goal become even more severe when such systems rely on\nmachine learning components rather than such as top-down rule-based AI. In this\npaper, we introduce a risk modeling approach tailored to Collaborative AI\nsystems. The risk model includes goals, risk events and domain specific\nindicators that potentially expose humans to hazards. The risk model is then\nleveraged to drive assurance methods that feed in turn the risk model through\ninsights extracted from run-time evidence. Our envisioned approach is described\nby means of a running example in the domain of Industry 4.0, where a robotic\narm endowed with a visual perception component, implemented with machine\nlearning, collaborates with a human operator for a production-relevant task.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 18:53:06 GMT"}], "update_date": "2021-03-15", "authors_parsed": [["Camilli", "Matteo", ""], ["Felderer", "Michael", ""], ["Giusti", "Andrea", ""], ["Matt", "Dominik T.", ""], ["Perini", "Anna", ""], ["Russo", "Barbara", ""], ["Susi", "Angelo", ""]]}, {"id": "2103.07491", "submitter": "Pallika Kanani", "authors": "Pallika Kanani, Virendra J. Marathe, Daniel Peterson, Rave Harpaz,\n  Steve Bright", "title": "Private Cross-Silo Federated Learning for Extracting Vaccine Adverse\n  Event Mentions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated Learning (FL) is quickly becoming a goto distributed training\nparadigm for users to jointly train a global model without physically sharing\ntheir data. Users can indirectly contribute to, and directly benefit from a\nmuch larger aggregate data corpus used to train the global model. However,\nliterature on successful application of FL in real-world problem settings is\nsomewhat sparse. In this paper, we describe our experience applying a FL based\nsolution to the Named Entity Recognition (NER) task for an adverse event\ndetection application in the context of mass scale vaccination programs. We\npresent a comprehensive empirical analysis of various dimensions of benefits\ngained with FL based training. Furthermore, we investigate effects of tighter\nDifferential Privacy (DP) constraints in highly sensitive settings where\nfederation users must enforce Local DP to ensure strict privacy guarantees. We\nshow that local DP can severely cripple the global model's prediction accuracy,\nthus dis-incentivizing users from participating in the federation. In response,\nwe demonstrate how recent innovation on personalization methods can help\nsignificantly recover the lost accuracy. We focus our analysis on the Federated\nFine-Tuning algorithm, FedFT, and prove that it is not PAC Identifiable, thus\nmaking it even more attractive for FL-based training.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 19:20:33 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Kanani", "Pallika", ""], ["Marathe", "Virendra J.", ""], ["Peterson", "Daniel", ""], ["Harpaz", "Rave", ""], ["Bright", "Steve", ""]]}, {"id": "2103.07492", "submitter": "Andrea Cossu", "authors": "Andrea Cossu, Antonio Carta, Vincenzo Lomonaco, Davide Bacciu", "title": "Continual Learning for Recurrent Neural Networks: an Empirical\n  Evaluation", "comments": "In submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning continuously during all model lifetime is fundamental to deploy\nmachine learning solutions robust to drifts in the data distribution. Advances\nin Continual Learning (CL) with recurrent neural networks could pave the way to\na large number of applications where incoming data is non stationary, like\nnatural language processing and robotics. However, the existing body of work on\nthe topic is still fragmented, with approaches which are application-specific\nand whose assessment is based on heterogeneous learning protocols and datasets.\nIn this paper, we organize the literature on CL for sequential data processing\nby providing a categorization of the contributions and a review of the\nbenchmarks. We propose two new benchmarks for CL with sequential data based on\nexisting datasets, whose characteristics resemble real-world applications. We\nalso provide a broad empirical evaluation of CL and Recurrent Neural Networks\nin class-incremental scenario, by testing their ability to mitigate forgetting\nwith a number of different strategies which are not specific to sequential data\nprocessing. Our results highlight the key role played by the sequence length\nand the importance of a clear specification of the CL scenario.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 19:25:28 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 11:10:43 GMT"}, {"version": "v3", "created": "Fri, 28 May 2021 08:25:39 GMT"}], "update_date": "2021-05-31", "authors_parsed": [["Cossu", "Andrea", ""], ["Carta", "Antonio", ""], ["Lomonaco", "Vincenzo", ""], ["Bacciu", "Davide", ""]]}, {"id": "2103.07494", "submitter": "Soumi Chattopadhyay", "authors": "Soumi Chattopadhyay, Chandranath Adak, Ranjana Roy Chowdhury", "title": "FES: A Fast Efficient Scalable QoS Prediction Framework", "comments": "13 pages, 15 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Quality-of-Service prediction of web service is an integral part of services\ncomputing due to its diverse applications in the various facets of a service\nlife cycle, such as service composition, service selection, service\nrecommendation. One of the primary objectives of designing a QoS prediction\nalgorithm is to achieve satisfactory prediction accuracy. However, accuracy is\nnot the only criteria to meet while developing a QoS prediction algorithm. The\nalgorithm has to be faster in terms of prediction time so that it can be\nintegrated into a real-time recommendation or composition system. The other\nimportant factor to consider while designing the prediction algorithm is\nscalability to ensure that the prediction algorithm can tackle large-scale\ndatasets. The existing algorithms on QoS prediction often compromise on one\ngoal while ensuring the others. In this paper, we propose a semi-offline QoS\nprediction model to achieve three important goals simultaneously: higher\naccuracy, faster prediction time, scalability. Here, we aim to predict the QoS\nvalue of service that varies across users. Our framework consists of\nmulti-phase prediction algorithms: preprocessing-phase prediction, online\nprediction, and prediction using the pre-trained model. In the preprocessing\nphase, we first apply multi-level clustering on the dataset to obtain\ncorrelated users and services. We then preprocess the clusters using\ncollaborative filtering to remove the sparsity of the given QoS invocation log\nmatrix. Finally, we create a two-staged, semi-offline regression model using\nneural networks to predict the QoS value of service to be invoked by a user in\nreal-time. Our experimental results on four publicly available WS-DREAM\ndatasets show the efficiency in terms of accuracy, scalability, fast\nresponsiveness of our framework as compared to the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 19:28:17 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 04:11:46 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Chattopadhyay", "Soumi", ""], ["Adak", "Chandranath", ""], ["Chowdhury", "Ranjana Roy", ""]]}, {"id": "2103.07512", "submitter": "Francisco Baeta", "authors": "Francisco Baeta, Jo\\~ao Correia, Tiago Martins and Penousal Machado", "title": "TensorGP -- Genetic Programming Engine in TensorFlow", "comments": "To be published in the 24th International Conference on the\n  Applications of Evolutionary Computation proceedings. 16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we resort to the TensorFlow framework to investigate the\nbenefits of applying data vectorization and fitness caching methods to domain\nevaluation in Genetic Programming. For this purpose, an independent engine was\ndeveloped, TensorGP, along with a testing suite to extract comparative timing\nresults across different architectures and amongst both iterative and\nvectorized approaches. Our performance benchmarks demonstrate that by\nexploiting the TensorFlow eager execution model, performance gains of up to two\norders of magnitude can be achieved on a parallel approach running on dedicated\nhardware when compared to a standard iterative approach.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 20:19:37 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Baeta", "Francisco", ""], ["Correia", "Jo\u00e3o", ""], ["Martins", "Tiago", ""], ["Machado", "Penousal", ""]]}, {"id": "2103.07544", "submitter": "Priyam Parashar", "authors": "Priyam Parashar, Aayush Naik, Jiaming Hu and Henrik I. Christensen", "title": "Meta-Modeling of Assembly Contingencies and Planning for Repair", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The World Robotics Challenge (2018 & 2020) was designed to challenge teams to\ndesign systems that are easy to adapt to new tasks and to ensure robust\noperation in a semi-structured environment. We present a layered strategy to\ntransform missions into tasks and actions and provide a set of strategies to\naddress simple and complex failures. We propose a model for characterizing\nfailures using this model and discuss repairs. Simple failures are by far the\nmost common in our WRC system and we also present how we repaired them.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 21:44:55 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Parashar", "Priyam", ""], ["Naik", "Aayush", ""], ["Hu", "Jiaming", ""], ["Christensen", "Henrik I.", ""]]}, {"id": "2103.07575", "submitter": "Yusen Lin", "authors": "Yusen Lin", "title": "A Review on Semi-Supervised Relation Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Relation extraction (RE) plays an important role in extracting knowledge from\nunstructured text but requires a large amount of labeled corpus. To reduce the\nexpensive annotation efforts, semisupervised learning aims to leverage both\nlabeled and unlabeled data. In this paper, we review and compare three typical\nmethods in semi-supervised RE with deep learning or meta-learning:\nself-ensembling, which forces consistent under perturbations but may confront\ninsufficient supervision; self-training, which iteratively generates pseudo\nlabels and retrain itself with the enlarged labeled set; dual learning, which\nleverages a primal task and a dual task to give mutual feedback. Mean-teacher\n(Tarvainen and Valpola, 2017), LST (Li et al., 2019), and DualRE (Lin et al.,\n2019) are elaborated as the representatives to alleviate the weakness of these\nthree methods, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 23:43:23 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Lin", "Yusen", ""]]}, {"id": "2103.07597", "submitter": "Amirali Salehi-Abari", "authors": "Sarina Sajadi Ghaemmaghami and Amirali Salehi-Abari", "title": "DeepGroup: Representation Learning for Group Recommendation with\n  Implicit Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Group recommender systems facilitate group decision making for a set of\nindividuals (e.g., a group of friends, a team, a corporation, etc.). Many of\nthese systems, however, either assume that (i) user preferences can be elicited\n(or inferred) and then aggregated into group preferences or (ii) group\npreferences are partially observed/elicited. We focus on making recommendations\nfor a new group of users whose preferences are unknown, but we are given the\ndecisions/choices of other groups. By formulating this problem as group\nrecommendation from group implicit feedback, we focus on two of its practical\ninstances: group decision prediction and reverse social choice. Given a set of\ngroups and their observed decisions, group decision prediction intends to\npredict the decision of a new group of users, whereas reverse social choice\naims to infer the preferences of those users involved in observed group\ndecisions. These two problems are of interest to not only group recommendation,\nbut also to personal privacy when the users intend to conceal their personal\npreferences but have participated in group decisions. To tackle these two\nproblems, we propose and study DeepGroup -- a deep learning approach for group\nrecommendation with group implicit data. We empirically assess the predictive\npower of DeepGroup on various real-world datasets, group conditions (e.g.,\nhomophily or heterophily), and group decision (or voting) rules. Our extensive\nexperiments not only demonstrate the efficacy of DeepGroup, but also shed light\non the privacy-leakage concerns of some decision making processes.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 02:05:26 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Ghaemmaghami", "Sarina Sajadi", ""], ["Salehi-Abari", "Amirali", ""]]}, {"id": "2103.07601", "submitter": "Ruiqi Zhong", "authors": "Charlie Snell, Ruiqi Zhong, Dan Klein, Jacob Steinhardt", "title": "Approximating How Single Head Attention Learns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Why do models often attend to salient words, and how does this evolve\nthroughout training? We approximate model training as a two stage process:\nearly on in training when the attention weights are uniform, the model learns\nto translate individual input word `i` to `o` if they co-occur frequently.\nLater, the model learns to attend to `i` while the correct output is $o$\nbecause it knows `i` translates to `o`. To formalize, we define a model\nproperty, Knowledge to Translate Individual Words (KTIW) (e.g. knowing that `i`\ntranslates to `o`), and claim that it drives the learning of the attention.\nThis claim is supported by the fact that before the attention mechanism is\nlearned, KTIW can be learned from word co-occurrence statistics, but not the\nother way around. Particularly, we can construct a training distribution that\nmakes KTIW hard to learn, the learning of the attention fails, and the model\ncannot even learn the simple task of copying the input words to the output. Our\napproximation explains why models sometimes attend to salient words, and\ninspires a toy example where a multi-head attention model can overcome the\nabove hard training distribution by improving learning dynamics rather than\nexpressiveness.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 02:32:19 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Snell", "Charlie", ""], ["Zhong", "Ruiqi", ""], ["Klein", "Dan", ""], ["Steinhardt", "Jacob", ""]]}, {"id": "2103.07612", "submitter": "Matloob Khushi Dr", "authors": "Mimi Mukherjee and Matloob Khushi", "title": "SMOTE-ENC: A novel SMOTE-based method to generate synthetic data for\n  nominal and continuous features", "comments": null, "journal-ref": "Appl. Syst. Innov. 2021, 4, 18", "doi": "10.3390/asi4010018", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Real world datasets are heavily skewed where some classes are significantly\noutnumbered by the other classes. In these situations, machine learning\nalgorithms fail to achieve substantial efficacy while predicting these\nunder-represented instances. To solve this problem, many variations of\nsynthetic minority over-sampling methods (SMOTE) have been proposed to balance\nthe dataset which deals with continuous features. However, for datasets with\nboth nominal and continuous features, SMOTE-NC is the only SMOTE-based\nover-sampling technique to balance the data. In this paper, we present a novel\nminority over-sampling method, SMOTE-ENC (SMOTE - Encoded Nominal and\nContinuous), in which, nominal features are encoded as numeric values and the\ndifference between two such numeric value reflects the amount of change of\nassociation with minority class. Our experiments show that the classification\nmodel using SMOTE-ENC method offers better prediction than model using SMOTE-NC\nwhen the dataset has a substantial number of nominal features and also when\nthere is some association between the categorical features and the target\nclass. Additionally, our proposed method addressed one of the major limitations\nof SMOTE-NC algorithm. SMOTE-NC can be applied only on mixed datasets that have\nfeatures consisting of both continuous and nominal features and cannot function\nif all the features of the dataset are nominal. Our novel method has been\ngeneralized to be applied on both mixed datasets and on nominal only datasets.\nThe code is available from mkhushi.github.io\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 04:16:17 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Mukherjee", "Mimi", ""], ["Khushi", "Matloob", ""]]}, {"id": "2103.07633", "submitter": "Fu Song", "authors": "Zhe Zhao, Guangke Chen, Jingyi Wang, Yiwei Yang, Fu Song, Jun Sun", "title": "Attack as Defense: Characterizing Adversarial Examples using Robustness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a new programming paradigm, deep learning has expanded its application to\nmany real-world problems. At the same time, deep learning based software are\nfound to be vulnerable to adversarial attacks. Though various defense\nmechanisms have been proposed to improve robustness of deep learning software,\nmany of them are ineffective against adaptive attacks. In this work, we propose\na novel characterization to distinguish adversarial examples from benign ones\nbased on the observation that adversarial examples are significantly less\nrobust than benign ones. As existing robustness measurement does not scale to\nlarge networks, we propose a novel defense framework, named attack as defense\n(A2D), to detect adversarial examples by effectively evaluating an example's\nrobustness. A2D uses the cost of attacking an input for robustness evaluation\nand identifies those less robust examples as adversarial since less robust\nexamples are easier to attack. Extensive experiment results on MNIST, CIFAR10\nand ImageNet show that A2D is more effective than recent promising approaches.\nWe also evaluate our defence against potential adaptive attacks and show that\nA2D is effective in defending carefully designed adaptive attacks, e.g., the\nattack success rate drops to 0% on CIFAR10.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 06:29:13 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Zhao", "Zhe", ""], ["Chen", "Guangke", ""], ["Wang", "Jingyi", ""], ["Yang", "Yiwei", ""], ["Song", "Fu", ""], ["Sun", "Jun", ""]]}, {"id": "2103.07636", "submitter": "Juntong Liu", "authors": "Juntong Liu, Yong Xiao, Yingyu Li, Guangming Shiyz, Walid Saad, and H.\n  Vincent Poor", "title": "Spatio-temporal Modeling for Large-scale Vehicular Networks Using Graph\n  Convolutional Networks", "comments": "6 pages, 5 figures, conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SP cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The effective deployment of connected vehicular networks is contingent upon\nmaintaining a desired performance across spatial and temporal domains. In this\npaper, a graph-based framework, called SMART, is proposed to model and keep\ntrack of the spatial and temporal statistics of vehicle-to-infrastructure (V2I)\ncommunication latency across a large geographical area. SMART first formulates\nthe spatio-temporal performance of a vehicular network as a graph in which each\nvertex corresponds to a subregion consisting of a set of neighboring location\npoints with similar statistical features of V2I latency and each edge\nrepresents the spatio-correlation between latency statistics of two connected\nvertices. Motivated by the observation that the complete temporal and spatial\nlatency performance of a vehicular network can be reconstructed from a limited\nnumber of vertices and edge relations, we develop a graph reconstruction-based\napproach using a graph convolutional network integrated with a deep Q-networks\nalgorithm in order to capture the spatial and temporal statistic of feature map\npf latency performance for a large-scale vehicular network. Extensive\nsimulations have been conducted based on a five-month latency measurement study\non a commercial LTE network. Our results show that the proposed method can\nsignificantly improve both the accuracy and efficiency for modeling and\nreconstructing the latency performance of large vehicular networks.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 06:56:29 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Liu", "Juntong", ""], ["Xiao", "Yong", ""], ["Li", "Yingyu", ""], ["Shiyz", "Guangming", ""], ["Saad", "Walid", ""], ["Poor", "H. Vincent", ""]]}, {"id": "2103.07674", "submitter": "Azra Abtahi", "authors": "Seyed Majid Naji, Azra Abtahi, Farokh Marvasti", "title": "Efficient Sparse Artificial Neural Networks", "comments": "Submitted in IEEE Transactions on Neural Networks and Learning\n  Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The brain, as the source of inspiration for Artificial Neural Networks (ANN),\nis based on a sparse structure. This sparse structure helps the brain to\nconsume less energy, learn easier and generalize patterns better than any other\nANN. In this paper, two evolutionary methods for adopting sparsity to ANNs are\nproposed. In the proposed methods, the sparse structure of a network as well as\nthe values of its parameters are trained and updated during the learning\nprocess. The simulation results show that these two methods have better\naccuracy and faster convergence while they need fewer training samples compared\nto their sparse and non-sparse counterparts. Furthermore, the proposed methods\nsignificantly improve the generalization power and reduce the number of\nparameters. For example, the sparsification of the ResNet47 network by\nexploiting our proposed methods for the image classification of ImageNet\ndataset uses 40 % fewer parameters while the top-1 accuracy of the model\nimproves by 12% and 5% compared to the dense network and their sparse\ncounterpart, respectively. As another example, the proposed methods for the\nCIFAR10 dataset converge to their final structure 7 times faster than its\nsparse counterpart, while the final accuracy increases by 6%.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 10:03:41 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Naji", "Seyed Majid", ""], ["Abtahi", "Azra", ""], ["Marvasti", "Farokh", ""]]}, {"id": "2103.07719", "submitter": "Defu Cao", "authors": "Defu Cao, Yujing Wang, Juanyong Duan, Ce Zhang, Xia Zhu, Conguri\n  Huang, Yunhai Tong, Bixiong Xu, Jing Bai, Jie Tong, Qi Zhang", "title": "Spectral Temporal Graph Neural Network for Multivariate Time-series\n  Forecasting", "comments": "Accepted by NeurIPS 2020. 20 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate time-series forecasting plays a crucial role in many real-world\napplications. It is a challenging problem as one needs to consider both\nintra-series temporal correlations and inter-series correlations\nsimultaneously. Recently, there have been multiple works trying to capture both\ncorrelations, but most, if not all of them only capture temporal correlations\nin the time domain and resort to pre-defined priors as inter-series\nrelationships.\n  In this paper, we propose Spectral Temporal Graph Neural Network (StemGNN) to\nfurther improve the accuracy of multivariate time-series forecasting. StemGNN\ncaptures inter-series correlations and temporal dependencies \\textit{jointly}\nin the \\textit{spectral domain}. It combines Graph Fourier Transform (GFT)\nwhich models inter-series correlations and Discrete Fourier Transform (DFT)\nwhich models temporal dependencies in an end-to-end framework. After passing\nthrough GFT and DFT, the spectral representations hold clear patterns and can\nbe predicted effectively by convolution and sequential learning modules.\nMoreover, StemGNN learns inter-series correlations automatically from the data\nwithout using pre-defined priors. We conduct extensive experiments on ten\nreal-world datasets to demonstrate the effectiveness of StemGNN. Code is\navailable at https://github.com/microsoft/StemGNN/\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 13:44:20 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Cao", "Defu", ""], ["Wang", "Yujing", ""], ["Duan", "Juanyong", ""], ["Zhang", "Ce", ""], ["Zhu", "Xia", ""], ["Huang", "Conguri", ""], ["Tong", "Yunhai", ""], ["Xu", "Bixiong", ""], ["Bai", "Jing", ""], ["Tong", "Jie", ""], ["Zhang", "Qi", ""]]}, {"id": "2103.07754", "submitter": "EL-Hachemi Guerrout", "authors": "EL-Hachemi Guerrout, Ramdane Mahiou, Randa Boukabene, and Assia Ouali", "title": "Image Segmentation Methods for Non-destructive testing Applications", "comments": "10 pages, 3 figures, the article is just accepted in the conference\n  JERI 2020 but the conference stopped because of Covid so the article non\n  published", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we present new image segmentation methods based on hidden\nMarkov random fields (HMRFs) and cuckoo search (CS) variants. HMRFs model the\nsegmentation problem as a minimization of an energy function. CS algorithm is\none of the recent powerful optimization techniques. Therefore, five variants of\nthe CS algorithm are used to compute a solution. Through tests, we conduct a\nstudy to choose the CS variant with parameters that give good results\n(execution time and quality of segmentation). CS variants are evaluated and\ncompared with non-destructive testing (NDT) images using a misclassification\nerror (ME) criterion.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 17:13:33 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Guerrout", "EL-Hachemi", ""], ["Mahiou", "Ramdane", ""], ["Boukabene", "Randa", ""], ["Ouali", "Assia", ""]]}, {"id": "2103.07762", "submitter": "Bonaventure F. P. Dossou", "authors": "Bonaventure F. P. Dossou and Chris C. Emezue", "title": "OkwuGb\\'e: End-to-End Speech Recognition for Fon and Igbo", "comments": null, "journal-ref": "African NLP, EACL 2021", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Language is inherent and compulsory for human communication. Whether\nexpressed in a written or spoken way, it ensures understanding between people\nof the same and different regions. With the growing awareness and effort to\ninclude more low-resourced languages in NLP research, African languages have\nrecently been a major subject of research in machine translation, and other\ntext-based areas of NLP. However, there is still very little comparable\nresearch in speech recognition for African languages. Interestingly, some of\nthe unique properties of African languages affecting NLP, like their\ndiacritical and tonal complexities, have a major root in their speech,\nsuggesting that careful speech interpretation could provide more intuition on\nhow to deal with the linguistic complexities of African languages for\ntext-based NLP. OkwuGb\\'e is a step towards building speech recognition systems\nfor African low-resourced languages. Using Fon and Igbo as our case study, we\nconduct a comprehensive linguistic analysis of each language and describe the\ncreation of end-to-end, deep neural network-based speech recognition models for\nboth languages. We present a state-of-art ASR model for Fon, as well as\nbenchmark ASR model results for Igbo. Our linguistic analyses (for Fon and\nIgbo) provide valuable insights and guidance into the creation of speech\nrecognition models for other African low-resourced languages, as well as guide\nfuture NLP research for Fon and Igbo. The Fon and Igbo models source code have\nbeen made publicly available.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 18:02:44 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 04:35:06 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Dossou", "Bonaventure F. P.", ""], ["Emezue", "Chris C.", ""]]}, {"id": "2103.07768", "submitter": "Robin Swezey", "authors": "Robin Swezey, Bruno Charron", "title": "Large-scale Recommendation for Portfolio Optimization", "comments": null, "journal-ref": "In Proceedings of the 12th ACM Conference on Recommender Systems\n  (RecSys 2018). Association for Computing Machinery, New York, NY, USA,\n  382-386", "doi": "10.1145/3240323.3240386", "report-no": null, "categories": "cs.AI cs.CE cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Individual investors are now massively using online brokers to trade stocks\nwith convenient interfaces and low fees, albeit losing the advice and\npersonalization traditionally provided by full-service brokers. We frame the\nproblem faced by online brokers of replicating this level of service in a\nlow-cost and automated manner for a very large number of users. Because of the\ncare required in recommending financial products, we focus on a risk-management\napproach tailored to each user's portfolio and risk profile. We show that our\nhybrid approach, based on Modern Portfolio Theory and Collaborative Filtering,\nprovides a sound and effective solution. The method is applicable to stocks as\nwell as other financial assets, and can be easily combined with various\nfinancial forecasting models. We validate our proposal by comparing it with\nseveral baselines in a domain expert-based study.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 18:22:48 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Swezey", "Robin", ""], ["Charron", "Bruno", ""]]}, {"id": "2103.07769", "submitter": "Preslav Nakov", "authors": "Preslav Nakov, David Corney, Maram Hasanain, Firoj Alam, Tamer\n  Elsayed, Alberto Barr\\'on-Cede\\~no, Paolo Papotti, Shaden Shaar, Giovanni Da\n  San Martino", "title": "Automated Fact-Checking for Assisting Human Fact-Checkers", "comments": "fact-checking, fact-checkers, check-worthiness, detecting previously\n  fact-checked claims, evidence retrieval", "journal-ref": "IJCAI-2021", "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.CR cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reporting and the analysis of current events around the globe has\nexpanded from professional, editor-lead journalism all the way to citizen\njournalism. Nowadays, politicians and other key players enjoy direct access to\ntheir audiences through social media, bypassing the filters of official cables\nor traditional media. However, the multiple advantages of free speech and\ndirect communication are dimmed by the misuse of media to spread inaccurate or\nmisleading claims. These phenomena have led to the modern incarnation of the\nfact-checker -- a professional whose main aim is to examine claims using\navailable evidence and to assess their veracity. As in other text forensics\ntasks, the amount of information available makes the work of the fact-checker\nmore difficult. With this in mind, starting from the perspective of the\nprofessional fact-checker, we survey the available intelligent technologies\nthat can support the human expert in the different steps of her fact-checking\nendeavor. These include identifying claims worth fact-checking, detecting\nrelevant previously fact-checked claims, retrieving relevant evidence to\nfact-check a claim, and actually verifying a claim. In each case, we pay\nattention to the challenges in future work and the potential impact on\nreal-world fact-checking.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 18:29:14 GMT"}, {"version": "v2", "created": "Sat, 22 May 2021 12:27:05 GMT"}], "update_date": "2021-05-25", "authors_parsed": [["Nakov", "Preslav", ""], ["Corney", "David", ""], ["Hasanain", "Maram", ""], ["Alam", "Firoj", ""], ["Elsayed", "Tamer", ""], ["Barr\u00f3n-Cede\u00f1o", "Alberto", ""], ["Papotti", "Paolo", ""], ["Shaar", "Shaden", ""], ["Martino", "Giovanni Da San", ""]]}, {"id": "2103.07779", "submitter": "Robin Swezey", "authors": "Robin Swezey, Young-joo Chung", "title": "Recommending Short-lived Dynamic Packages for Golf Booking Services", "comments": null, "journal-ref": "In Proceedings of the 24th ACM International on Conference on\n  Information and Knowledge Management (CIKM 2015). Association for Computing\n  Machinery, New York, NY, USA, 1779-1782", "doi": "10.1145/2806416.2806608", "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce an approach to recommending short-lived dynamic packages for\ngolf booking services. Two challenges are addressed in this work. The first is\nthe short life of the items, which puts the system in a state of a permanent\ncold start. The second is the uninformative nature of the package attributes,\nwhich makes clustering or figuring latent packages challenging. Although such\nsettings are fairly pervasive, they have not been studied in traditional\nrecommendation research, and there is thus a call for original approaches for\nrecommender systems. In this paper, we introduce a hybrid method that leverages\nuser analysis and its relation to the packages, as well as package pricing and\nenvironmental analysis, and traditional collaborative filtering. The proposed\napproach achieved appreciable improvement in precision compared with baselines.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 19:48:04 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Swezey", "Robin", ""], ["Chung", "Young-joo", ""]]}, {"id": "2103.07780", "submitter": "Yaodong Yang Mr.", "authors": "Le Cong Dinh, Yaodong Yang, Zheng Tian, Nicolas Perez Nieves, Oliver\n  Slumbers, David Henry Mguni, Haitham Bou Ammar, Jun Wang", "title": "Online Double Oracle", "comments": "yaodong.yang@outlook.com", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Solving strategic games with huge action space is a critical yet\nunder-explored topic in economics, operations research and artificial\nintelligence. This paper proposes new learning algorithms for solving\ntwo-player zero-sum normal-form games where the number of pure strategies is\nprohibitively large. Specifically, we combine no-regret analysis from online\nlearning with Double Oracle (DO) methods from game theory. Our method --\n\\emph{Online Double Oracle (ODO)} -- is provably convergent to a Nash\nequilibrium (NE). Most importantly, unlike normal DO methods, ODO is\n\\emph{rationale} in the sense that each agent in ODO can exploit strategic\nadversary with a regret bound of $\\mathcal{O}(\\sqrt{T k \\log(k)})$ where $k$ is\nnot the total number of pure strategies, but rather the size of \\emph{effective\nstrategy set} that is linearly dependent on the support size of the NE. On tens\nof different real-world games, ODO outperforms DO, PSRO methods, and no-regret\nalgorithms such as Multiplicative Weight Update by a significant margin, both\nin terms of convergence rate to a NE and average payoff against strategic\nadversaries.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 19:48:27 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 14:34:47 GMT"}, {"version": "v3", "created": "Fri, 4 Jun 2021 22:50:56 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Dinh", "Le Cong", ""], ["Yang", "Yaodong", ""], ["Tian", "Zheng", ""], ["Nieves", "Nicolas Perez", ""], ["Slumbers", "Oliver", ""], ["Mguni", "David Henry", ""], ["Ammar", "Haitham Bou", ""], ["Wang", "Jun", ""]]}, {"id": "2103.07789", "submitter": "Yuval Shahar", "authors": "Avner Hatsek and Yuval Shahar", "title": "A Methodology for Bi-Directional Knowledge-Based Assessment of\n  Compliance to Continuous Application of Clinical Guidelines", "comments": "25 pages; 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Clinicians often do not sufficiently adhere to evidence-based clinical\nguidelines in a manner sensitive to the context of each patient. It is\nimportant to detect such deviations, typically including redundant or missing\nactions, even when the detection is performed retrospectively, so as to inform\nboth the attending clinician and policy makers. Furthermore, it would be\nbeneficial to detect such deviations in a manner proportional to the level of\nthe deviation, and not to simply use arbitrary cut-off values. In this study,\nwe introduce a new approach for automated guideline-based quality assessment of\nthe care process, the bidirectional knowledge-based assessment of compliance\n(BiKBAC) method. Our BiKBAC methodology assesses the degree of compliance when\napplying clinical guidelines, with respect to multiple different aspects of the\nguideline (e.g., the guideline's process and outcome objectives). The\nassessment is performed through a highly detailed, automated quality-assessment\nretrospective analysis, which compares a formal representation of the guideline\nand of its process and outcome intentions (we use the Asbru language for that\npurpose) with the longitudinal electronic medical record of its continuous\napplication over a significant time period, using both a top-down and a\nbottom-up approach, which we explain in detail. Partial matches of the data to\nthe process and to the outcome objectives are resolved using fuzzy temporal\nlogic. We also introduce the DiscovErr system, which implements the BiKBAC\napproach, and present its detailed architecture. The DiscovErr system was\nevaluated in a separate study in the type 2 diabetes management domain, by\ncomparing its performance to a panel of three clinicians, with highly\nencouraging results with respect to the completeness and correctness of its\ncomments.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 20:43:45 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Hatsek", "Avner", ""], ["Shahar", "Yuval", ""]]}, {"id": "2103.07802", "submitter": "Bernd Ulmann", "authors": "Mirko Holzer, Bernd Ulmann", "title": "Hybrid computer approach to train a machine learning system", "comments": "Book chapter", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.ET", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This book chapter describes a novel approach to training machine learning\nsystems by means of a hybrid computer setup i.e. a digital computer tightly\ncoupled with an analog computer. As an example a reinforcement learning system\nis trained to balance an inverted pendulum which is simulated on an analog\ncomputer, thus demonstrating a solution to the major challenge of adequately\nsimulating the environment for reinforcement learning.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 22:01:47 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Holzer", "Mirko", ""], ["Ulmann", "Bernd", ""]]}, {"id": "2103.07805", "submitter": "Subhajit Das", "authors": "Subhajit Das and Alex Endert", "title": "CACTUS: Detecting and Resolving Conflicts in Objective Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning (ML) models are constructed by expert ML practitioners using\nvarious coding languages, in which they tune and select models hyperparameters\nand learning algorithms for a given problem domain. They also carefully design\nan objective function or loss function (often with multiple objectives) that\ncaptures the desired output for a given ML task such as classification,\nregression, etc. In multi-objective optimization, conflicting objectives and\nconstraints is a major area of concern. In such problems, several competing\nobjectives are seen for which no single optimal solution is found that\nsatisfies all desired objectives simultaneously. In the past VA systems have\nallowed users to interactively construct objective functions for a classifier.\nIn this paper, we extend this line of work by prototyping a technique to\nvisualize multi-objective objective functions either defined in a Jupyter\nnotebook or defined using an interactive visual interface to help users to: (1)\nperceive and interpret complex mathematical terms in it and (2) detect and\nresolve conflicting objectives. Visualization of the objective function\nenlightens potentially conflicting objectives that obstructs selecting correct\nsolution(s) for the desired ML task or goal. We also present an enumeration of\npotential conflicts in objective specification in multi-objective objective\nfunctions for classifier selection. Furthermore, we demonstrate our approach in\na VA system that helps users in specifying meaningful objective functions to a\nclassifier by detecting and resolving conflicting objectives and constraints.\nThrough a within-subject quantitative and qualitative user study, we present\nresults showing that our technique helps users interactively specify meaningful\nobjective functions by resolving potential conflicts for a classification task.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 22:38:47 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Das", "Subhajit", ""], ["Endert", "Alex", ""]]}, {"id": "2103.07808", "submitter": "Youngwoo Kim", "authors": "Youngwoo Kim, Cheng Li, Bingyang Ye, Amir Tahmasebi and Javed Aslam", "title": "Supervised Learning in the Presence of Noise: Application in ICD-10 Code\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  ICD coding is the international standard for capturing and reporting health\nconditions and diagnosis for revenue cycle management in healthcare. Manually\nassigning ICD codes is prone to human error due to the large code vocabulary\nand the similarities between codes. Since machine learning based approaches\nrequire ground truth training data, the inconsistency among human coders is\nmanifested as noise in labeling, which makes the training and evaluation of ICD\nclassifiers difficult in presence of such noise. This paper investigates the\ncharacteristics of such noise in manually-assigned ICD-10 codes and\nfurthermore, proposes a method to train robust ICD-10 classifiers in the\npresence of labeling noise. Our research concluded that the nature of such\nnoise is systematic. Most of the existing methods for handling label noise\nassume that the noise is completely random and independent of features or\nlabels, which is not the case for ICD data. Therefore, we develop a new method\nfor training robust classifiers in the presence of systematic noise. We first\nidentify ICD-10 codes that human coders tend to misuse or confuse, based on the\ncodes' locations in the ICD-10 hierarchy, the types of the codes, and baseline\nclassifier's prediction behaviors; we then develop a novel training strategy\nthat accounts for such noise. We compared our method with the baseline that\ndoes not handle label noise and the baseline methods that assume random noise,\nand demonstrated that our proposed method outperforms all baselines when\nevaluated on expert validated labels.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 23:05:50 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Kim", "Youngwoo", ""], ["Li", "Cheng", ""], ["Ye", "Bingyang", ""], ["Tahmasebi", "Amir", ""], ["Aslam", "Javed", ""]]}, {"id": "2103.07815", "submitter": "Arjun Sripathy", "authors": "Arjun Sripathy, Andreea Bobu, Daniel S. Brown, and Anca D. Dragan", "title": "Dynamically Switching Human Prediction Models for Efficient Planning", "comments": "ICRA '21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As environments involving both robots and humans become increasingly common,\nso does the need to account for people during planning. To plan effectively,\nrobots must be able to respond to and sometimes influence what humans do. This\nrequires a human model which predicts future human actions. A simple model may\nassume the human will continue what they did previously; a more complex one\nmight predict that the human will act optimally, disregarding the robot;\nwhereas an even more complex one might capture the robot's ability to influence\nthe human. These models make different trade-offs between computational time\nand performance of the resulting robot plan. Using only one model of the human\neither wastes computational resources or is unable to handle critical\nsituations. In this work, we give the robot access to a suite of human models\nand enable it to assess the performance-computation trade-off online. By\nestimating how an alternate model could improve human prediction and how that\nmay translate to performance gain, the robot can dynamically switch human\nmodels whenever the additional computation is justified. Our experiments in a\ndriving simulator showcase how the robot can achieve performance comparable to\nalways using the best human model, but with greatly reduced computation.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 23:48:09 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Sripathy", "Arjun", ""], ["Bobu", "Andreea", ""], ["Brown", "Daniel S.", ""], ["Dragan", "Anca D.", ""]]}, {"id": "2103.07825", "submitter": "Xu Dong", "authors": "Xu Dong, Binnan Zhuang, Yunxiang Mao, Langechuan Liu", "title": "Radar Camera Fusion via Representation Learning in Autonomous Driving", "comments": null, "journal-ref": "In Proceedings of the IEEE/CVF Conference on Computer Vision and\n  Pattern Recognition, pp. 1672-1681. 2021", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Radars and cameras are mature, cost-effective, and robust sensors and have\nbeen widely used in the perception stack of mass-produced autonomous driving\nsystems. Due to their complementary properties, outputs from radar detection\n(radar pins) and camera perception (2D bounding boxes) are usually fused to\ngenerate the best perception results. The key to successful radar-camera fusion\nis the accurate data association. The challenges in the radar-camera\nassociation can be attributed to the complexity of driving scenes, the noisy\nand sparse nature of radar measurements, and the depth ambiguity from 2D\nbounding boxes. Traditional rule-based association methods are susceptible to\nperformance degradation in challenging scenarios and failure in corner cases.\nIn this study, we propose to address radar-camera association via deep\nrepresentation learning, to explore feature-level interaction and global\nreasoning. Additionally, we design a loss sampling mechanism and an innovative\nordinal loss to overcome the difficulty of imperfect labeling and to enforce\ncritical human-like reasoning. Despite being trained with noisy labels\ngenerated by a rule-based algorithm, our proposed method achieves a performance\nof 92.2% F1 score, which is 11.6% higher than the rule-based teacher. Moreover,\nthis data-driven method also lends itself to continuous improvement via corner\ncase mining.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 01:32:03 GMT"}, {"version": "v2", "created": "Sun, 18 Apr 2021 21:02:47 GMT"}, {"version": "v3", "created": "Fri, 18 Jun 2021 15:48:11 GMT"}], "update_date": "2021-06-21", "authors_parsed": [["Dong", "Xu", ""], ["Zhuang", "Binnan", ""], ["Mao", "Yunxiang", ""], ["Liu", "Langechuan", ""]]}, {"id": "2103.07845", "submitter": "Hui Li", "authors": "Chen Lin, Zhichao Ouyang, Junqing Zhuang, Jianqiang Chen, Hui Li,\n  Rongxin Wu", "title": "Improving Code Summarization with Block-wise Abstract Syntax Tree\n  Splitting", "comments": "Accepted in 29th IEEE/ACM International Conference on Program\n  Comprehension (ICPC 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic code summarization frees software developers from the heavy burden\nof manual commenting and benefits software development and maintenance.\nAbstract Syntax Tree (AST), which depicts the source code's syntactic\nstructure, has been incorporated to guide the generation of code summaries.\nHowever, existing AST based methods suffer from the difficulty of training and\ngenerate inadequate code summaries. In this paper, we present the Block-wise\nAbstract Syntax Tree Splitting method (BASTS for short), which fully utilizes\nthe rich tree-form syntax structure in ASTs, for improving code summarization.\nBASTS splits the code of a method based on the blocks in the dominator tree of\nthe Control Flow Graph, and generates a split AST for each code split. Each\nsplit AST is then modeled by a Tree-LSTM using a pre-training strategy to\ncapture local non-linear syntax encoding. The learned syntax encoding is\ncombined with code encoding, and fed into Transformer to generate high-quality\ncode summaries. Comprehensive experiments on benchmarks have demonstrated that\nBASTS significantly outperforms state-of-the-art approaches in terms of various\nevaluation metrics. To facilitate reproducibility, our implementation is\navailable at https://github.com/XMUDM/BASTS.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 05:04:06 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 11:15:11 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Lin", "Chen", ""], ["Ouyang", "Zhichao", ""], ["Zhuang", "Junqing", ""], ["Chen", "Jianqiang", ""], ["Li", "Hui", ""], ["Wu", "Rongxin", ""]]}, {"id": "2103.07877", "submitter": "Xinliang Wu", "authors": "Xinliang Wu and Mengying Jiang and Guizhong Liu", "title": "R-GSN: The Relation-based Graph Similar Network for Heterogeneous Graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Heterogeneous graph is a kind of data structure widely existing in real life.\nNowadays, the research of graph neural network on heterogeneous graph has\nbecome more and more popular. The existing heterogeneous graph neural network\nalgorithms mainly have two ideas, one is based on meta-path and the other is\nnot. The idea based on meta-path often requires a lot of manual preprocessing,\nat the same time it is difficult to extend to large scale graphs. In this\npaper, we proposed the general heterogeneous message passing paradigm and\ndesigned R-GSN that does not need meta-path, which is much improved compared to\nthe baseline R-GCN. Experiments have shown that our R-GSN algorithm achieves\nthe state-of-the-art performance on the ogbn-mag large scale heterogeneous\ngraph dataset.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 09:25:36 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 17:40:24 GMT"}, {"version": "v3", "created": "Fri, 25 Jun 2021 09:36:05 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Wu", "Xinliang", ""], ["Jiang", "Mengying", ""], ["Liu", "Guizhong", ""]]}, {"id": "2103.07903", "submitter": "Mustafa Gunel", "authors": "Anil Ozturk, Mustafa Burak Gunel, Resul Dagdanov, Mirac Ekim Vural,\n  Ferhat Yurdakul, Melih Dal, Nazim Kemal Ure", "title": "Investigating Value of Curriculum Reinforcement Learning in Autonomous\n  Driving Under Diverse Road and Weather Conditions", "comments": "8 pages, 9 figures, IV2021 Workshop submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Applications of reinforcement learning (RL) are popular in autonomous driving\ntasks. That being said, tuning the performance of an RL agent and guaranteeing\nthe generalization performance across variety of different driving scenarios is\nstill largely an open problem. In particular, getting good performance on\ncomplex road and weather conditions require exhaustive tuning and computation\ntime. Curriculum RL, which focuses on solving simpler automation tasks in order\nto transfer knowledge to complex tasks, is attracting attention in RL\ncommunity. The main contribution of this paper is a systematic study for\ninvestigating the value of curriculum reinforcement learning in autonomous\ndriving applications. For this purpose, we setup several different driving\nscenarios in a realistic driving simulator, with varying road complexity and\nweather conditions. Next, we train and evaluate performance of RL agents on\ndifferent sequences of task combinations and curricula. Results show that\ncurriculum RL can yield significant gains in complex driving tasks, both in\nterms of driving performance and sample complexity. Results also demonstrate\nthat different curricula might enable different benefits, which hints future\nresearch directions for automated curriculum training.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 12:05:05 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 11:59:48 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Ozturk", "Anil", ""], ["Gunel", "Mustafa Burak", ""], ["Dagdanov", "Resul", ""], ["Vural", "Mirac Ekim", ""], ["Yurdakul", "Ferhat", ""], ["Dal", "Melih", ""], ["Ure", "Nazim Kemal", ""]]}, {"id": "2103.07919", "submitter": "Donghwan Lee", "authors": "Donghwan Lee, Niao He, Seungjae Lee, Panagiota Karava, Jianghai Hu", "title": "Simulation Studies on Deep Reinforcement Learning for Building Control\n  with Human Interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The building sector consumes the largest energy in the world, and there have\nbeen considerable research interests in energy consumption and comfort\nmanagement of buildings. Inspired by recent advances in reinforcement learning\n(RL), this paper aims at assessing the potential of RL in building climate\ncontrol problems with occupant interaction. We apply a recent RL approach,\ncalled DDPG (deep deterministic policy gradient), for the continuous building\ncontrol tasks and assess its performance with simulation studies in terms of\nits ability to handle (a) the partial state observability due to sensor\nlimitations; (b) complex stochastic system with high-dimensional state-spaces,\nwhich are jointly continuous and discrete; (c) uncertainties due to ambient\nweather conditions, occupant's behavior, and comfort feelings. Especially, the\npartial observability and uncertainty due to the occupant interaction\nsignificantly complicate the control problem. Through simulation studies, the\npolicy learned by DDPG demonstrates reasonable performance and computational\ntractability.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 13:04:04 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Lee", "Donghwan", ""], ["He", "Niao", ""], ["Lee", "Seungjae", ""], ["Karava", "Panagiota", ""], ["Hu", "Jianghai", ""]]}, {"id": "2103.07927", "submitter": "Yaodong Yang Mr.", "authors": "Nicolas Perez Nieves, Yaodong Yang, Oliver Slumbers, David Henry\n  Mguni, Ying Wen, Jun Wang", "title": "Modelling Behavioural Diversity for Learning in Open-Ended Games", "comments": "corresponds to <yaodong.yang@cs.ucl.ac.uk>", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Promoting behavioural diversity is critical for solving games with\nnon-transitive dynamics where strategic cycles exist, and there is no\nconsistent winner (e.g., Rock-Paper-Scissors). Yet, there is a lack of rigorous\ntreatment for defining diversity and constructing diversity-aware learning\ndynamics. In this work, we offer a geometric interpretation of behavioural\ndiversity in games and introduce a novel diversity metric based on\ndeterminantal point processes (DPP). By incorporating the diversity metric into\nbest-response dynamics, we develop diverse fictitious play and diverse\npolicy-space response oracle for solving normal-form games and open-ended\ngames. We prove the uniqueness of the diverse best response and the convergence\nof our algorithms on two-player games. Importantly, we show that maximising the\nDPP-based diversity metric guarantees to enlarge the gamescape -- convex\npolytopes spanned by agents' mixtures of strategies. To validate our\ndiversity-aware solvers, we test on tens of games that show strong\nnon-transitivity. Results suggest that our methods achieve at least the same,\nand in most games, lower exploitability than PSRO solvers by finding effective\nand diverse strategies.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 13:42:39 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 10:03:15 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Nieves", "Nicolas Perez", ""], ["Yang", "Yaodong", ""], ["Slumbers", "Oliver", ""], ["Mguni", "David Henry", ""], ["Wen", "Ying", ""], ["Wang", "Jun", ""]]}, {"id": "2103.07938", "submitter": "Amir Masoud Ghalamzan Esfahani", "authors": "Amir Ghalamzan-E", "title": "Learning needle insertion from sample task executions", "comments": "Submitted to IEEE/RSJ IROS 2021!", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automating a robotic task, e.g., robotic suturing can be very complex and\ntime-consuming. Learning a task model to autonomously perform the task is\ninvaluable making the technology, robotic surgery, accessible for a wider\ncommunity. The data of robotic surgery can be easily logged where the collected\ndata can be used to learn task models. This will result in reduced time and\ncost of robotic surgery in which a surgeon can supervise the robot operation or\ngive high-level commands instead of low-level control of the tools. We present\na data-set of needle insertion in soft tissue with two arms where Arm 1 inserts\nthe needle into the tissue and Arm 2 actively manipulate the soft tissue to\nensure the desired and actual exit points are the same. This is important in\nreal-surgery because suturing without active manipulation of tissue may yield\nfailure of the suturing as the stitch may not grip enough tissue to resist the\nforce applied for the suturing. We present a needle insertion dataset including\n60 successful trials recorded by 3 pair of stereo cameras. Moreover, we present\nDeep-robot Learning from Demonstrations that predicts the desired state of the\nrobot at the time step after t (which the optimal action taken at t yields) by\nlooking at the video of the past time steps, i.e. n step time history where N\nis the memory time window, of the task execution. The experimental results\nillustrate our proposed deep model architecture is outperforming the existing\nmethods. Although the solution is not yet ready to be deployed on a real robot,\nthe results indicate the possibility of future development for real robot\ndeployment.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 14:23:17 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Ghalamzan-E", "Amir", ""]]}, {"id": "2103.07945", "submitter": "Yann Ollivier", "authors": "Ahmed Touati and Yann Ollivier", "title": "Learning One Representation to Optimize All Rewards", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the forward-backward (FB) representation of the dynamics of a\nreward-free Markov decision process. It provides explicit near-optimal policies\nfor any reward specified a posteriori. During an unsupervised phase, we use\nreward-free interactions with the environment to learn two representations via\noff-the-shelf deep learning methods and temporal difference (TD) learning. In\nthe test phase, a reward representation is estimated either from observations\nor an explicit reward description (e.g., a target state). The optimal policy\nfor that reward is directly obtained from these representations, with no\nplanning. We assume access to an exploration scheme or replay buffer for the\nfirst phase.\n  The unsupervised FB loss is well-principled: if training is perfect, the\npolicies obtained are provably optimal for any reward function. With imperfect\ntraining, the sub-optimality is proportional to the unsupervised approximation\nerror. The FB representation learns long-range relationships between states and\nactions, via a predictive occupancy map, without having to synthesize states as\nin model-based approaches.\n  This is a step towards learning controllable agents in arbitrary black-box\nstochastic environments. This approach compares well to goal-oriented RL\nalgorithms on discrete and continuous mazes, pixel-based MsPacman, and the\nFetchReach virtual robot arm. We also illustrate how the agent can immediately\nadapt to new tasks beyond goal-oriented RL.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 15:00:08 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 16:40:33 GMT"}], "update_date": "2021-06-07", "authors_parsed": [["Touati", "Ahmed", ""], ["Ollivier", "Yann", ""]]}, {"id": "2103.07950", "submitter": "Karthik Vaidhyanathan", "authors": "Henry Muccini and Karthik Vaidhyanathan", "title": "Software Architecture for ML-based Systems: What Exists and What Lies\n  Ahead", "comments": "About to appear in the proceedings of 1st International Workshop on\n  Software Engineering - AI Engineering (WAIN) 2021, workshop of ICSE 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The increasing usage of machine learning (ML) coupled with the software\narchitectural challenges of the modern era has resulted in two broad research\nareas: i) software architecture for ML-based systems, which focuses on\ndeveloping architectural techniques for better developing ML-based software\nsystems, and ii) ML for software architectures, which focuses on developing ML\ntechniques to better architect traditional software systems. In this work, we\nfocus on the former side of the spectrum with a goal to highlight the different\narchitecting practices that exist in the current scenario for architecting\nML-based software systems. We identify four key areas of software architecture\nthat need the attention of both the ML and software practitioners to better\ndefine a standard set of practices for architecting ML-based software systems.\nWe base these areas in light of our experience in architecting an ML-based\nsoftware system for solving queuing challenges in one of the largest museums in\nItaly.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 15:20:46 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 07:11:31 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Muccini", "Henry", ""], ["Vaidhyanathan", "Karthik", ""]]}, {"id": "2103.07953", "submitter": "David F. N. Oliveira", "authors": "David F. N. Oliveira, Lucio F. Vismari, Alexandre M. Nascimento, Jorge\n  R. de Almeida Jr, Paulo S. Cugnasca, Joao B. Camargo Jr, Leandro Almeida,\n  Rafael Gripp, Marcelo Neves", "title": "A new interpretable unsupervised anomaly detection method based on\n  residual explanation", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Despite the superior performance in modeling complex patterns to address\nchallenging problems, the black-box nature of Deep Learning (DL) methods impose\nlimitations to their application in real-world critical domains. The lack of a\nsmooth manner for enabling human reasoning about the black-box decisions hinder\nany preventive action to unexpected events, in which may lead to catastrophic\nconsequences. To tackle the unclearness from black-box models, interpretability\nbecame a fundamental requirement in DL-based systems, leveraging trust and\nknowledge by providing ways to understand the model's behavior. Although a\ncurrent hot topic, further advances are still needed to overcome the existing\nlimitations of the current interpretability methods in unsupervised DL-based\nmodels for Anomaly Detection (AD). Autoencoders (AE) are the core of\nunsupervised DL-based for AD applications, achieving best-in-class performance.\nHowever, due to their hybrid aspect to obtain the results (by requiring\nadditional calculations out of network), only agnostic interpretable methods\ncan be applied to AE-based AD. These agnostic methods are computationally\nexpensive to process a large number of parameters. In this paper we present the\nRXP (Residual eXPlainer), a new interpretability method to deal with the\nlimitations for AE-based AD in large-scale systems. It stands out for its\nimplementation simplicity, low computational cost and deterministic behavior,\nin which explanations are obtained through the deviation analysis of\nreconstructed input features. In an experiment using data from a real\nheavy-haul railway line, the proposed method achieved superior performance\ncompared to SHAP, demonstrating its potential to support decision making in\nlarge scale critical systems.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 15:35:45 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Oliveira", "David F. N.", ""], ["Vismari", "Lucio F.", ""], ["Nascimento", "Alexandre M.", ""], ["Almeida", "Jorge R. de", "Jr"], ["Cugnasca", "Paulo S.", ""], ["Camargo", "Joao B.", "Jr"], ["Almeida", "Leandro", ""], ["Gripp", "Rafael", ""], ["Neves", "Marcelo", ""]]}, {"id": "2103.07963", "submitter": "S\\'ebastien Le Digabel", "authors": "Dounia Lakhmiri and S\\'ebastien Le Digabel", "title": "Use of static surrogates in hyperparameter optimization", "comments": "http://www.optimization-online.org/DB_HTML/2021/03/8296.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimizing the hyperparameters and architecture of a neural network is a long\nyet necessary phase in the development of any new application. This consuming\nprocess can benefit from the elaboration of strategies designed to quickly\ndiscard low quality configurations and focus on more promising candidates. This\nwork aims at enhancing HyperNOMAD, a library that adapts a direct search\nderivative-free optimization algorithm to tune both the architecture and the\ntraining of a neural network simultaneously, by targeting two keys steps of its\nexecution and exploiting cheap approximations in the form of static surrogates\nto trigger the early stopping of the evaluation of a configuration and the\nranking of pools of candidates. These additions to HyperNOMAD are shown to\nimprove on its resources consumption without harming the quality of the\nproposed solutions.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 16:15:53 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Lakhmiri", "Dounia", ""], ["Digabel", "S\u00e9bastien Le", ""]]}, {"id": "2103.07966", "submitter": "Jeremy Gordon", "authors": "Jeremy Gordon and John Chuang", "title": "Active Dynamical Prospection: Modeling Mental Simulation as Particle\n  Filtering for Sensorimotor Control during Pathfinding", "comments": "8 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI q-bio.NC", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  What do humans do when confronted with a common challenge: we know where we\nwant to go but we are not yet sure the best way to get there, or even if we\ncan. This is the problem posed to agents during spatial navigation and\npathfinding, and its solution may give us clues about the more abstract domain\nof planning in general. In this work, we model pathfinding behavior in a\ncontinuous, explicitly exploratory paradigm. In our task, participants (and\nagents) must coordinate both visual exploration and navigation within a\npartially observable environment. Our contribution has three primary\ncomponents: 1) an analysis of behavioral data from 81 human participants in a\nnovel pathfinding paradigm conducted as an online experiment, 2) a proposal to\nmodel prospective mental simulation during navigation as particle filtering,\nand 3) an instantiation of this proposal in a computational agent. We show that\nour model, Active Dynamical Prospection, demonstrates similar patterns of map\nsolution rate, path selection, and trial duration, as well as attentional\nbehavior (at both aggregate and individual levels) when compared with data from\nhuman participants. We also find that both distal attention and delay prior to\nfirst move (both potential correlates of prospective simulation) are predictive\nof task performance.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 16:26:33 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Gordon", "Jeremy", ""], ["Chuang", "John", ""]]}, {"id": "2103.07969", "submitter": "Sinisa Stekovic", "authors": "Shreyas Hampali, Sinisa Stekovic, Sayan Deb Sarkar, Chetan Srinivasa\n  Kumar, Friedrich Fraundorfer, Vincent Lepetit", "title": "Monte Carlo Scene Search for 3D Scene Understanding", "comments": "To be presented at CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore how a general AI algorithm can be used for 3D scene understanding\nto reduce the need for training data. More exactly, we propose a modification\nof the Monte Carlo Tree Search (MCTS) algorithm to retrieve objects and room\nlayouts from noisy RGB-D scans. While MCTS was developed as a game-playing\nalgorithm, we show it can also be used for complex perception problems. Our\nadapted MCTS algorithm has few easy-to-tune hyperparameters and can optimise\ngeneral losses. We use it to optimise the posterior probability of objects and\nroom layout hypotheses given the RGB-D data. This results in an\nanalysis-by-synthesis approach that explores the solution space by rendering\nthe current solution and comparing it to the RGB-D observations. To perform\nthis exploration even more efficiently, we propose simple changes to the\nstandard MCTS' tree construction and exploration policy. We demonstrate our\napproach on the ScanNet dataset. Our method often retrieves configurations that\nare better than some manual annotations, especially on layouts.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 16:33:28 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 09:39:56 GMT"}, {"version": "v3", "created": "Wed, 5 May 2021 10:03:03 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Hampali", "Shreyas", ""], ["Stekovic", "Sinisa", ""], ["Sarkar", "Sayan Deb", ""], ["Kumar", "Chetan Srinivasa", ""], ["Fraundorfer", "Friedrich", ""], ["Lepetit", "Vincent", ""]]}, {"id": "2103.07974", "submitter": "Cheng Luo", "authors": "Cheng Luo, Lei Qu, Youshan Miao, Peng Cheng, Yongqiang Xiong", "title": "CrossoverScheduler: Overlapping Multiple Distributed Training\n  Applications in a Crossover Manner", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Distributed deep learning workloads include throughput-intensive training\ntasks on the GPU clusters, where the Distributed Stochastic Gradient Descent\n(SGD) incurs significant communication delays after backward propagation,\nforces workers to wait for the gradient synchronization via a centralized\nparameter server or directly in decentralized workers. We present\nCrossoverScheduler, an algorithm that enables communication cycles of a\ndistributed training application to be filled by other applications through\npipelining communication and computation. With CrossoverScheduler, the running\nperformance of distributed training can be significantly improved without\nsacrificing convergence rate and network accuracy. We achieve so by introducing\nCrossover Synchronization which allows multiple distributed deep learning\napplications to time-share the same GPU alternately. The prototype of\nCrossoverScheduler is built and integrated with Horovod. Experiments on a\nvariety of distributed tasks show that CrossoverScheduler achieves 20% \\times\nspeedup for image classification tasks on ImageNet dataset.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 17:01:15 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Luo", "Cheng", ""], ["Qu", "Lei", ""], ["Miao", "Youshan", ""], ["Cheng", "Peng", ""], ["Xiong", "Yongqiang", ""]]}, {"id": "2103.07986", "submitter": "Arthur Venter Mr", "authors": "Arthur E. W. Venter and Marthinus W. Theunissen and Marelie H. Davel", "title": "Pre-interpolation loss behaviour in neural networks", "comments": "11 pages, 8 figures. Presented at the 2021 SACAIR online conference\n  in February 2021", "journal-ref": "Communications in Computer and Information Science, volume 1342,\n  year 2021, pages 296-309", "doi": "10.1007/978-3-030-66151-9_19", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  When training neural networks as classifiers, it is common to observe an\nincrease in average test loss while still maintaining or improving the overall\nclassification accuracy on the same dataset. In spite of the ubiquity of this\nphenomenon, it has not been well studied and is often dismissively attributed\nto an increase in borderline correct classifications. We present an empirical\ninvestigation that shows how this phenomenon is actually a result of the\ndifferential manner by which test samples are processed. In essence: test loss\ndoes not increase overall, but only for a small minority of samples. Large\nrepresentational capacities allow losses to decrease for the vast majority of\ntest samples at the cost of extreme increases for others. This effect seems to\nbe mainly caused by increased parameter values relating to the correctly\nprocessed sample features. Our findings contribute to the practical\nunderstanding of a common behaviour of deep neural networks. We also discuss\nthe implications of this work for network optimisation and generalisation.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 18:08:59 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Venter", "Arthur E. W.", ""], ["Theunissen", "Marthinus W.", ""], ["Davel", "Marelie H.", ""]]}, {"id": "2103.08001", "submitter": "Amartya Hatua", "authors": "Amartya Hatua, Arjun Mukherjee and Rakesh M. Verma", "title": "Claim Verification using a Multi-GAN based Model", "comments": "Paper is submitted at LDK 2021 3rd Conference on Language, Data and\n  Knowledge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes research on claim verification carried out using a\nmultiple GAN-based model. The proposed model consists of three pairs of\ngenerators and discriminators. The generator and discriminator pairs are\nresponsible for generating synthetic data for supported and refuted claims and\nclaim labels. A theoretical discussion about the proposed model is provided to\nvalidate the equilibrium state of the model. The proposed model is applied to\nthe FEVER dataset, and a pre-trained language model is used for the input text\ndata. The synthetically generated data helps to gain information which helps\nthe model to perform better than state of the art models and other standard\nclassifiers.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 19:15:53 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 22:17:50 GMT"}, {"version": "v3", "created": "Tue, 20 Jul 2021 05:02:28 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Hatua", "Amartya", ""], ["Mukherjee", "Arjun", ""], ["Verma", "Rakesh M.", ""]]}, {"id": "2103.08022", "submitter": "Naoki Yokoyama", "authors": "Naoki Yokoyama, Sehoon Ha, Dhruv Batra", "title": "Success Weighted by Completion Time: A Dynamics-Aware Evaluation\n  Criteria for Embodied Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Success weighted by Completion Time (SCT), a new metric for\nevaluating navigation performance for mobile robots. Several related works on\nnavigation have used Success weighted by Path Length (SPL) as the primary\nmethod of evaluating the path an agent makes to a goal location, but SPL is\nlimited in its ability to properly evaluate agents with complex dynamics. In\ncontrast, SCT explicitly takes the agent's dynamics model into consideration,\nand aims to accurately capture how well the agent has approximated the fastest\nnavigation behavior afforded by its dynamics. While several embodied navigation\nworks use point-turn dynamics, we focus on unicycle-cart dynamics for our\nagent, which better exemplifies the dynamics model of popular mobile robotics\nplatforms (e.g., LoCoBot, TurtleBot, Fetch, etc.). We also present\nRRT*-Unicycle, an algorithm for unicycle dynamics that estimates the fastest\ncollision-free path and completion time from a starting pose to a goal location\nin an environment containing obstacles. We experiment with deep reinforcement\nlearning and reward shaping to train and compare the navigation performance of\nagents with different dynamics models. In evaluating these agents, we show that\nin contrast to SPL, SCT is able to capture the advantages in navigation speed a\nunicycle model has over a simpler point-turn model of dynamics. Lastly, we show\nthat we can successfully deploy our trained models and algorithms outside of\nsimulation in the real world. We embody our agents in an real robot to navigate\nan apartment, and show that they can generalize in a zero-shot manner.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 20:13:06 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Yokoyama", "Naoki", ""], ["Ha", "Sehoon", ""], ["Batra", "Dhruv", ""]]}, {"id": "2103.08052", "submitter": "Bonaventure F. P. Dossou", "authors": "Bonaventure F. P. Dossou and Chris C. Emezue", "title": "Crowdsourced Phrase-Based Tokenization for Low-Resourced Neural Machine\n  Translation: The Case of Fon Language", "comments": null, "journal-ref": "African NLP, EACL 2021", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Building effective neural machine translation (NMT) models for very\nlow-resourced and morphologically rich African indigenous languages is an open\nchallenge. Besides the issue of finding available resources for them, a lot of\nwork is put into preprocessing and tokenization. Recent studies have shown that\nstandard tokenization methods do not always adequately deal with the\ngrammatical, diacritical, and tonal properties of some African languages. That,\ncoupled with the extremely low availability of training samples, hinders the\nproduction of reliable NMT models. In this paper, using Fon language as a case\nstudy, we revisit standard tokenization methods and introduce\nWord-Expressions-Based (WEB) tokenization, a human-involved super-words\ntokenization strategy to create a better representative vocabulary for\ntraining. Furthermore, we compare our tokenization strategy to others on the\nFon-French and French-Fon translation tasks.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 22:12:14 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 13:00:28 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Dossou", "Bonaventure F. P.", ""], ["Emezue", "Chris C.", ""]]}, {"id": "2103.08057", "submitter": "ChihWei Hsu", "authors": "Martin Mladenov, Chih-Wei Hsu, Vihan Jain, Eugene Ie, Christopher\n  Colby, Nicolas Mayoraz, Hubert Pham, Dustin Tran, Ivan Vendrov, Craig\n  Boutilier", "title": "RecSim NG: Toward Principled Uncertainty Modeling for Recommender\n  Ecosystems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The development of recommender systems that optimize multi-turn interaction\nwith users, and model the interactions of different agents (e.g., users,\ncontent providers, vendors) in the recommender ecosystem have drawn increasing\nattention in recent years. Developing and training models and algorithms for\nsuch recommenders can be especially difficult using static datasets, which\noften fail to offer the types of counterfactual predictions needed to evaluate\npolicies over extended horizons. To address this, we develop RecSim NG, a\nprobabilistic platform for the simulation of multi-agent recommender systems.\nRecSim NG is a scalable, modular, differentiable simulator implemented in\nEdward2 and TensorFlow. It offers: a powerful, general probabilistic\nprogramming language for agent-behavior specification; tools for probabilistic\ninference and latent-variable model learning, backed by automatic\ndifferentiation and tracing; and a TensorFlow-based runtime for running\nsimulations on accelerated hardware. We describe RecSim NG and illustrate how\nit can be used to create transparent, configurable, end-to-end models of a\nrecommender ecosystem, complemented by a small set of simple use cases that\ndemonstrate how RecSim NG can help both researchers and practitioners easily\ndevelop and train novel algorithms for recommender systems.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 22:37:42 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Mladenov", "Martin", ""], ["Hsu", "Chih-Wei", ""], ["Jain", "Vihan", ""], ["Ie", "Eugene", ""], ["Colby", "Christopher", ""], ["Mayoraz", "Nicolas", ""], ["Pham", "Hubert", ""], ["Tran", "Dustin", ""], ["Vendrov", "Ivan", ""], ["Boutilier", "Craig", ""]]}, {"id": "2103.08067", "submitter": "Kalesha Bullard", "authors": "Kalesha Bullard, Douwe Kiela, Franziska Meier, Joelle Pineau, Jakob\n  Foerster", "title": "Quasi-Equivalence Discovery for Zero-Shot Emergent Communication", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Effective communication is an important skill for enabling information\nexchange in multi-agent settings and emergent communication is now a vibrant\nfield of research, with common settings involving discrete cheap-talk channels.\nSince, by definition, these settings involve arbitrary encoding of information,\ntypically they do not allow for the learned protocols to generalize beyond\ntraining partners. In contrast, in this work, we present a novel problem\nsetting and the Quasi-Equivalence Discovery (QED) algorithm that allows for\nzero-shot coordination (ZSC), i.e., discovering protocols that can generalize\nto independently trained agents. Real world problem settings often contain\ncostly communication channels, e.g., robots have to physically move their\nlimbs, and a non-uniform distribution over intents. We show that these two\nfactors lead to unique optimal ZSC policies in referential games, where agents\nuse the energy cost of the messages to communicate intent. Other-Play was\nrecently introduced for learning optimal ZSC policies, but requires prior\naccess to the symmetries of the problem. Instead, QED can iteratively discovers\nthe symmetries in this setting and converges to the optimal ZSC policy.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 23:42:37 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 14:52:57 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Bullard", "Kalesha", ""], ["Kiela", "Douwe", ""], ["Meier", "Franziska", ""], ["Pineau", "Joelle", ""], ["Foerster", "Jakob", ""]]}, {"id": "2103.08079", "submitter": "Katie Seaborn", "authors": "Katie Seaborn, Peter Pennefather, Norihisa P. Miyake, Mihoko\n  Otake-Matsuura", "title": "Crossing the Tepper Line: An Emerging Ontology for Describing the\n  Dynamic Sociality of Embodied AI", "comments": "Accepted at CHI EA '21", "journal-ref": null, "doi": "10.1145/3411763.3451783", "report-no": null, "categories": "cs.HC cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Artificial intelligences (AI) are increasingly being embodied and embedded in\nthe world to carry out tasks and support decision-making with and for people.\nRobots, recommender systems, voice assistants, virtual humans - do these\ndisparate types of embodied AI have something in common? Here we show how they\ncan manifest as \"socially embodied AI.\" We define this as the state that\nembodied AI \"circumstantially\" take on within interactive contexts when\nperceived as both social and agentic by people. We offer a working ontology\nthat describes how embodied AI can dynamically transition into socially\nembodied AI. We propose an ontological heuristic for describing the threshold:\nthe Tepper line. We reinforce our theoretical work with expert insights from a\ncard sort workshop. We end with two case studies to illustrate the dynamic and\ncontextual nature of this heuristic.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 00:45:44 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Seaborn", "Katie", ""], ["Pennefather", "Peter", ""], ["Miyake", "Norihisa P.", ""], ["Otake-Matsuura", "Mihoko", ""]]}, {"id": "2103.08083", "submitter": "Wahab Hamou-Lhadj PhD", "authors": "Md Shariful Islam, Abdelwahab Hamou-Lhadj, Korosh K. Sabor, Mohammad\n  Hamdaqa, Haipeng Cai", "title": "EnHMM: On the Use of Ensemble HMMs and Stack Traces to Predict the\n  Reassignment of Bug Report Fields", "comments": "Published in Proceedings of the 28th IEEE International Conference on\n  Software Analysis, Evolution and Reengineering (SANER 2021), 11 pages, 7\n  figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Bug reports (BR) contain vital information that can help triaging teams\nprioritize and assign bugs to developers who will provide the fixes. However,\nstudies have shown that BR fields often contain incorrect information that need\nto be reassigned, which delays the bug fixing process. There exist approaches\nfor predicting whether a BR field should be reassigned or not. These studies\nuse mainly BR descriptions and traditional machine learning algorithms (SVM,\nKNN, etc.). As such, they do not fully benefit from the sequential order of\ninformation in BR data, such as function call sequences in BR stack traces,\nwhich may be valuable for improving the prediction accuracy. In this paper, we\npropose a novel approach, called EnHMM, for predicting the reassignment of BR\nfields using ensemble Hidden Markov Models (HMMs), trained on stack traces.\nEnHMM leverages the natural ability of HMMs to represent sequential data to\nmodel the temporal order of function calls in BR stack traces. When applied to\nEclipse and Gnome BR repositories, EnHMM achieves an average precision, recall,\nand F-measure of 54%, 76%, and 60% on Eclipse dataset and 41%, 69%, and 51% on\nGnome dataset. We also found that EnHMM improves over the best single HMM by\n36% for Eclipse and 76% for Gnome. Finally, when comparing EnHMM to Im.ML.KNN,\na recent approach in the field, we found that the average F-measure score of\nEnHMM improves the average F-measure of Im.ML.KNN by 6.80% and improves the\naverage recall of Im.ML.KNN by 36.09%. However, the average precision of EnHMM\nis lower than that of Im.ML.KNN (53.93% as opposed to 56.71%).\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 01:00:45 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Islam", "Md Shariful", ""], ["Hamou-Lhadj", "Abdelwahab", ""], ["Sabor", "Korosh K.", ""], ["Hamdaqa", "Mohammad", ""], ["Cai", "Haipeng", ""]]}, {"id": "2103.08115", "submitter": "Muhao Chen", "authors": "Junheng Hao, Muhao Chen, Wenchao Yu, Yizhou Sun, Wei Wang", "title": "Universal Representation Learning of Knowledge Bases by Jointly\n  Embedding Instances and Ontological Concepts", "comments": "KDD-19", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many large-scale knowledge bases simultaneously represent two views of\nknowledge graphs (KGs): an ontology view for abstract and commonsense concepts,\nand an instance view for specific entities that are instantiated from\nontological concepts. Existing KG embedding models, however, merely focus on\nrepresenting one of the two views alone. In this paper, we propose a novel\ntwo-view KG embedding model, JOIE, with the goal to produce better knowledge\nembedding and enable new applications that rely on multi-view knowledge. JOIE\nemploys both cross-view and intra-view modeling that learn on multiple facets\nof the knowledge base. The cross-view association model is learned to bridge\nthe embeddings of ontological concepts and their corresponding instance-view\nentities. The intra-view models are trained to capture the structured knowledge\nof instance and ontology views in separate embedding spaces, with a\nhierarchy-aware encoding technique enabled for ontologies with hierarchies. We\nexplore multiple representation techniques for the two model components and\ninvestigate with nine variants of JOIE. Our model is trained on large-scale\nknowledge bases that consist of massive instances and their corresponding\nontological concepts connected via a (small) set of cross-view links.\nExperimental results on public datasets show that the best variant of JOIE\nsignificantly outperforms previous models on instance-view triple prediction\ntask as well as ontology population on ontologyview KG. In addition, our model\nsuccessfully extends the use of KG embeddings to entity typing with promising\nperformance.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 03:24:37 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Hao", "Junheng", ""], ["Chen", "Muhao", ""], ["Yu", "Wenchao", ""], ["Sun", "Yizhou", ""], ["Wang", "Wei", ""]]}, {"id": "2103.08137", "submitter": "Solvi Arnold", "authors": "Solvi Arnold (1), Daisuke Tanaka (1), Kimitoshi Yamazaki (1) ((1)\n  Shinshu University)", "title": "Cloth Manipulation Planning on Basis of Mesh Representations with\n  Incomplete Domain Knowledge and Voxel-to-Mesh Estimation", "comments": "27 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of open-goal planning for robotic cloth manipulation.\nCore of our system is a neural network trained as a forward model of cloth\nbehaviour under manipulation, with planning performed through backpropagation.\nWe introduce a neural network-based routine for estimating mesh representations\nfrom voxel input, and perform planning in mesh format internally. We address\nthe problem of planning with incomplete domain knowledge by means of an\nexplicit epistemic uncertainty signal. This signal is calculated from\nprediction divergence between two instances of the forward model network and\nused to avoid epistemic uncertainty during planning. Finally, we introduce\nlogic for handling restriction of grasp points to a discrete set of candidates,\nin order to accommodate graspability constraints imposed by robotic hardware.\nWe evaluate the system's mesh estimation, prediction, and planning ability on\nsimulated cloth for sequences of one to three manipulations. Comparative\nexperiments confirm that planning on basis of estimated meshes improves\naccuracy compared to voxel-based planning, and that epistemic uncertainty\navoidance improves performance under conditions of incomplete domain knowledge.\nWe additionally present qualitative results on robot hardware.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 04:59:14 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Arnold", "Solvi", ""], ["Tanaka", "Daisuke", ""], ["Yamazaki", "Kimitoshi", ""]]}, {"id": "2103.08143", "submitter": "Olga Lukyanova", "authors": "Oleg Nikitin and Olga Lukyanova and Alex Kunin", "title": "Constrained plasticity reserve as a natural way to control frequency and\n  weights in spiking neural networks", "comments": "24 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Biological neurons have adaptive nature and perform complex computations\ninvolving the filtering of redundant information. However, most common neural\ncell models, including biologically plausible, such as Hodgkin-Huxley or\nIzhikevich, do not possess predictive dynamics on a single-cell level.\nMoreover, the modern rules of synaptic plasticity or interconnections weights\nadaptation also do not provide grounding for the ability of neurons to adapt to\nthe ever-changing input signal intensity. While natural neuron synaptic growth\nis precisely controlled and restricted by protein supply and recycling, weight\ncorrection rules such as widely used STDP are efficiently unlimited in change\nrate and scale. The present article introduces new mechanics of interconnection\nbetween neuron firing rate homeostasis and weight change through STDP growth\nbounded by abstract protein reserve, controlled by the intracellular\noptimization algorithm. We show how these cellular dynamics help neurons filter\nout the intense noise signals to help neurons keep a stable firing rate. We\nalso examine that such filtering does not affect the ability of neurons to\nrecognize the correlated inputs in unsupervised mode. Such an approach might be\nused in the machine learning domain to improve the robustness of AI systems.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 05:22:14 GMT"}, {"version": "v2", "created": "Sun, 20 Jun 2021 10:56:05 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Nikitin", "Oleg", ""], ["Lukyanova", "Olga", ""], ["Kunin", "Alex", ""]]}, {"id": "2103.08155", "submitter": "Kenny Chour", "authors": "Kenny Chour, Sivakumar Rathinam, Ramamoorthi Ravi", "title": "S$^*$: A Heuristic Information-Based Approximation Framework for\n  Multi-Goal Path Finding", "comments": "In Proceedings of the 31st International Conference on Automated\n  Planning and Scheduling (ICAPS 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We combine ideas from uni-directional and bi-directional heuristic search,\nand approximation algorithms for the Traveling Salesman Problem, to develop a\nnovel framework for a Multi-Goal Path Finding (MGPF) problem that provides a\n2-approximation guarantee. MGPF aims to find a least-cost path from an origin\nto a destination such that each node in a given set of goals is visited at\nleast once along the path. We present numerical results to illustrate the\nadvantages of our framework over conventional alternates in terms of the number\nof expanded nodes and run time.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 06:27:37 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 03:12:06 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Chour", "Kenny", ""], ["Rathinam", "Sivakumar", ""], ["Ravi", "Ramamoorthi", ""]]}, {"id": "2103.08175", "submitter": "Babak Nouri-Moghaddam", "authors": "Jafar Abdollahi, Babak Nouri-Moghaddam", "title": "Feature selection for medical diagnosis: Evaluation for using a hybrid\n  Stacked-Genetic approach in the diagnosis of heart disease", "comments": "11 pages, 2 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Background and purpose: Heart disease has been one of the most important\ncauses of death in the last 10 years, so the use of classification methods to\ndiagnose and predict heart disease is very important. If this disease is\npredicted before menstruation, it is possible to prevent high mortality of the\ndisease and provide more accurate and efficient treatment methods. Materials\nand Methods: Due to the selection of input features, the use of basic\nalgorithms can be very time-consuming. Reducing dimensions or choosing a good\nsubset of features, without risking accuracy, has great importance for basic\nalgorithms for successful use in the region. In this paper, we propose an\nensemble-genetic learning method using wrapper feature reduction to select\nfeatures in disease classification. Findings: The development of a medical\ndiagnosis system based on ensemble learning to predict heart disease provides a\nmore accurate diagnosis than the traditional method and reduces the cost of\ntreatment. Conclusion: The results showed that Thallium Scan and vascular\nocclusion were the most important features in the diagnosis of heart disease\nand can distinguish between sick and healthy people with 97.57% accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 07:31:14 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Abdollahi", "Jafar", ""], ["Nouri-Moghaddam", "Babak", ""]]}, {"id": "2103.08178", "submitter": "Babak Nouri-Moghaddam", "authors": "Jafar Abdollahi, Amir Jalili Irani, Babak Nouri-Moghaddam", "title": "Modeling and forecasting Spread of COVID-19 epidemic in Iran until Sep\n  22, 2021, based on deep learning", "comments": "9 Pages, 5 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The recent global outbreak of covid-19 is affecting many countries around the\nworld. Due to the growing number of newly infected individuals and the\nhealth-care system bottlenecks, it will be useful to predict the upcoming\nnumber of patients. This study aims to efficiently forecast the is used to\nestimate new cases, number of deaths, and number of recovered patients in Iran\nfor 180 days, using the official dataset of the Iranian Ministry of Health and\nMedical Education and the impact of control measures on the spread of COVID-19.\nFour different types of forecasting techniques, time series, and machine\nlearning algorithms, are developed and the best performing method for the given\ncase study is determined. Under the time series, we consider the four\nalgorithms including Prophet, Long short-term memory, Autoregressive,\nAutoregressive Integrated Moving Average models. On comparing the different\ntechniques, we found that deep learning methods yield better results than time\nseries forecasting algorithms. More specifically, the least value of the error\nmeasures is observed in seasonal ANN and LSTM models. Our findings showed that\nif precautionary measures are taken seriously, the number of new cases and\ndeaths will decrease, and the number of deaths in September 2021 will reach\nzero.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 07:36:12 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Abdollahi", "Jafar", ""], ["Irani", "Amir Jalili", ""], ["Nouri-Moghaddam", "Babak", ""]]}, {"id": "2103.08182", "submitter": "Babak Nouri-Moghaddam", "authors": "Jafar Abdollahi, Babak Nouri-Moghaddam, Mehdi Ghazanfari", "title": "Deep Neural Network Based Ensemble learning Algorithms for the\n  healthcare system (diagnosis of chronic diseases)", "comments": "16 pages, 12 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  learning algorithms. In this paper, we review the classification algorithms\nused in the health care system (chronic diseases) and present the neural\nnetwork-based Ensemble learning method. We briefly describe the commonly used\nalgorithms and describe their critical properties. Materials and Methods: In\nthis study, modern classification algorithms used in healthcare, examine the\nprinciples of these methods and guidelines, and to accurately diagnose and\npredict chronic diseases, superior machine learning algorithms with the neural\nnetwork-based ensemble learning Is used. To do this, we use experimental data,\nreal data on chronic patients (diabetes, heart, cancer) available on the UCI\nsite. Results: We found that group algorithms designed to diagnose chronic\ndiseases can be more effective than baseline algorithms. It also identifies\nseveral challenges to further advancing the classification of machine learning\nin the diagnosis of chronic diseases. Conclusion: The results show the high\nperformance of the neural network-based Ensemble learning approach for the\ndiagnosis and prediction of chronic diseases, which in this study reached 98.5,\n99, and 100% accuracy, respectively.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 07:41:54 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Abdollahi", "Jafar", ""], ["Nouri-Moghaddam", "Babak", ""], ["Ghazanfari", "Mehdi", ""]]}, {"id": "2103.08183", "submitter": "Tadahiro Taniguchi", "authors": "Tadahiro Taniguchi, Hiroshi Yamakawa, Takayuki Nagai, Kenji Doya,\n  Masamichi Sakagami, Masahiro Suzuki, Tomoaki Nakamura, Akira Taniguchi", "title": "Whole brain Probabilistic Generative Model toward Realizing Cognitive\n  Architecture for Developmental Robots", "comments": "55 pages, 8 figures, submitted to Neural Networks", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Building a humanlike integrative artificial cognitive system, that is, an\nartificial general intelligence, is one of the goals in artificial intelligence\nand developmental robotics. Furthermore, a computational model that enables an\nartificial cognitive system to achieve cognitive development will be an\nexcellent reference for brain and cognitive science. This paper describes the\ndevelopment of a cognitive architecture using probabilistic generative models\n(PGMs) to fully mirror the human cognitive system. The integrative model is\ncalled a whole-brain PGM (WB-PGM). It is both brain-inspired and PGMbased. In\nthis paper, the process of building the WB-PGM and learning from the human\nbrain to build cognitive architectures is described.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 07:42:04 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Taniguchi", "Tadahiro", ""], ["Yamakawa", "Hiroshi", ""], ["Nagai", "Takayuki", ""], ["Doya", "Kenji", ""], ["Sakagami", "Masamichi", ""], ["Suzuki", "Masahiro", ""], ["Nakamura", "Tomoaki", ""], ["Taniguchi", "Akira", ""]]}, {"id": "2103.08186", "submitter": "Babak Nouri-Moghaddam", "authors": "Jafar Abdollahi, Babak Nouri-Moghaddam", "title": "Hybrid stacked ensemble combined with genetic algorithms for Prediction\n  of Diabetes", "comments": "12 pages, 5 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Diabetes is currently one of the most common, dangerous, and costly diseases\nin the world that is caused by an increase in blood sugar or a decrease in\ninsulin in the body. Diabetes can have detrimental effects on people's health\nif diagnosed late. Today, diabetes has become one of the challenges for health\nand government officials. Prevention is a priority, and taking care of people's\nhealth without compromising their comfort is an essential need. In this study,\nthe Ensemble training methodology based on genetic algorithms are used to\naccurately diagnose and predict the outcomes of diabetes mellitus. In this\nstudy, we use the experimental data, real data on Indian diabetics on the\nUniversity of California website. Current developments in ICT, such as the\nInternet of Things, machine learning, and data mining, allow us to provide\nhealth strategies with more intelligent capabilities to accurately predict the\noutcomes of the disease in daily life and the hospital and prevent the\nprogression of this disease and its many complications. The results show the\nhigh performance of the proposed method in diagnosing the disease, which has\nreached 98.8%, and 99% accuracy in this study.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 07:47:23 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Abdollahi", "Jafar", ""], ["Nouri-Moghaddam", "Babak", ""]]}, {"id": "2103.08199", "submitter": "Tadahiro Taniguchi", "authors": "Yasuaki Okuda, Ryo Ozaki, and Tadahiro Taniguchi", "title": "Double Articulation Analyzer with Prosody for Unsupervised Word and\n  Phoneme Discovery", "comments": "11 pages, Submitted to IEEE Transactions on Cognitive and\n  Developmental Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Infants acquire words and phonemes from unsegmented speech signals using\nsegmentation cues, such as distributional, prosodic, and co-occurrence cues.\nMany pre-existing computational models that represent the process tend to focus\non distributional or prosodic cues. This paper proposes a nonparametric\nBayesian probabilistic generative model called the prosodic hierarchical\nDirichlet process-hidden language model (Prosodic HDP-HLM). Prosodic HDP-HLM,\nan extension of HDP-HLM, considers both prosodic and distributional cues within\na single integrative generative model. We conducted three experiments on\ndifferent types of datasets, and demonstrate the validity of the proposed\nmethod. The results show that the Prosodic DAA successfully uses prosodic cues\nand outperforms a method that solely uses distributional cues. The main\ncontributions of this study are as follows: 1) We develop a probabilistic\ngenerative model for time series data including prosody that potentially has a\ndouble articulation structure; 2) We propose the Prosodic DAA by deriving the\ninference procedure for Prosodic HDP-HLM and show that Prosodic DAA can\ndiscover words directly from continuous human speech signals using statistical\ninformation and prosodic information in an unsupervised manner; 3) We show that\nprosodic cues contribute to word segmentation more in naturally distributed\ncase words, i.e., they follow Zipf's law.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 08:17:44 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Okuda", "Yasuaki", ""], ["Ozaki", "Ryo", ""], ["Taniguchi", "Tadahiro", ""]]}, {"id": "2103.08200", "submitter": "Jiaxin Pan", "authors": "Jiaxin Pan, Min Peng, Yiyan Zhang", "title": "Mention-centered Graph Neural Network for Document-level Relation\n  Extraction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Document-level relation extraction aims to discover relations between\nentities across a whole document. How to build the dependency of entities from\ndifferent sentences in a document remains to be a great challenge. Current\napproaches either leverage syntactic trees to construct document-level graphs\nor aggregate inference information from different sentences. In this paper, we\nbuild cross-sentence dependencies by inferring compositional relations between\ninter-sentence mentions. Adopting aggressive linking strategy, intermediate\nrelations are reasoned on the document-level graphs by mention convolution. We\nfurther notice the generalization problem of NA instances, which is caused by\nincomplete annotation and worsened by fully-connected mention pairs. An\nimproved ranking loss is proposed to attend this problem. Experiments show the\nconnections between different mentions are crucial to document-level relation\nextraction, which enables the model to extract more meaningful higher-level\ncompositional relations.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 08:19:44 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Pan", "Jiaxin", ""], ["Peng", "Min", ""], ["Zhang", "Yiyan", ""]]}, {"id": "2103.08201", "submitter": "Adil Rasheed Professor", "authors": "Tiril Sundby, Julia Maria Graham, Adil Rasheed, Mandar Tabib, Omer San", "title": "Geometric Change Detection in Digital Twins using 3D Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital twins are meant to bridge the gap between real-world physical systems\nand virtual representations. Both stand-alone and descriptive digital twins\nincorporate 3D geometric models, which are the physical representations of\nobjects in the digital replica. Digital twin applications are required to\nrapidly update internal parameters with the evolution of their physical\ncounterpart. Due to an essential need for having high-quality geometric models\nfor accurate physical representations, the storage and bandwidth requirements\nfor storing 3D model information can quickly exceed the available storage and\nbandwidth capacity. In this work, we demonstrate a novel approach to geometric\nchange detection in the context of a digital twin. We address the issue through\na combined solution of Dynamic Mode Decomposition (DMD) for motion detection,\nYOLOv5 for object detection, and 3D machine learning for pose estimation. DMD\nis applied for background subtraction, enabling detection of moving foreground\nobjects in real-time. The video frames containing detected motion are extracted\nand used as input to the change detection network. The object detection\nalgorithm YOLOv5 is applied to extract the bounding boxes of detected objects\nin the video frames. Furthermore, the rotational pose of each object is\nestimated in a 3D pose estimation network. A series of convolutional neural\nnetworks conducts feature extraction from images and 3D model shapes. Then, the\nnetwork outputs the estimated Euler angles of the camera orientation with\nrespect to the object in the input image. By only storing data associated with\na detected change in pose, we minimize necessary storage and bandwidth\nrequirements while still being able to recreate the 3D scene on demand.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 08:20:16 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Sundby", "Tiril", ""], ["Graham", "Julia Maria", ""], ["Rasheed", "Adil", ""], ["Tabib", "Mandar", ""], ["San", "Omer", ""]]}, {"id": "2103.08228", "submitter": "Zhihao Ma", "authors": "Zhihao Ma, Yuzheng Zhuang, Paul Weng, Hankz Hankui Zhuo, Dong Li,\n  Wulong Liu, Jianye Hao", "title": "Learning Symbolic Rules for Interpretable Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in deep reinforcement learning (DRL) can be largely\nattributed to the use of neural networks. However, this black-box approach\nfails to explain the learned policy in a human understandable way. To address\nthis challenge and improve the transparency, we propose a Neural Symbolic\nReinforcement Learning framework by introducing symbolic logic into DRL. This\nframework features a fertilization of reasoning and learning modules, enabling\nend-to-end learning with prior symbolic knowledge. Moreover, interpretability\nis achieved by extracting the logical rules learned by the reasoning module in\na symbolic rule space. The experimental results show that our framework has\nbetter interpretability, along with competing performance in comparison to\nstate-of-the-art approaches.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 09:26:00 GMT"}, {"version": "v2", "created": "Tue, 16 Mar 2021 05:32:42 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Ma", "Zhihao", ""], ["Zhuang", "Yuzheng", ""], ["Weng", "Paul", ""], ["Zhuo", "Hankz Hankui", ""], ["Li", "Dong", ""], ["Liu", "Wulong", ""], ["Hao", "Jianye", ""]]}, {"id": "2103.08233", "submitter": "Thanh Nguyen Xuan", "authors": "Thanh Nguyen, Tung Luu, Trung Pham, Sanzhar Rakhimkul, Chang D. Yoo", "title": "Robust MAML: Prioritization task buffer with adaptive learning process\n  for model-agnostic meta-learning", "comments": null, "journal-ref": "ICASSP 2021 - 2021 IEEE International Conference on Acoustics,\n  Speech and Signal Processing (ICASSP)", "doi": "10.1109/ICASSP39728.2021.9413446", "report-no": null, "categories": "cs.LG cs.AI cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Model agnostic meta-learning (MAML) is a popular state-of-the-art\nmeta-learning algorithm that provides good weight initialization of a model\ngiven a variety of learning tasks. The model initialized by provided weight can\nbe fine-tuned to an unseen task despite only using a small amount of samples\nand within a few adaptation steps. MAML is simple and versatile but requires\ncostly learning rate tuning and careful design of the task distribution which\naffects its scalability and generalization. This paper proposes a more robust\nMAML based on an adaptive learning scheme and a prioritization task buffer(PTB)\nreferred to as Robust MAML (RMAML) for improving scalability of training\nprocess and alleviating the problem of distribution mismatch. RMAML uses\ngradient-based hyper-parameter optimization to automatically find the optimal\nlearning rate and uses the PTB to gradually adjust train-ing task distribution\ntoward testing task distribution over the course of training. Experimental\nresults on meta reinforcement learning environments demonstrate a substantial\nperformance gain as well as being less sensitive to hyper-parameter choice and\nrobust to distribution mismatch.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 09:34:34 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 13:56:07 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Nguyen", "Thanh", ""], ["Luu", "Tung", ""], ["Pham", "Trung", ""], ["Rakhimkul", "Sanzhar", ""], ["Yoo", "Chang D.", ""]]}, {"id": "2103.08236", "submitter": "Lars V\\\"ogtlin", "authors": "Lars V\\\"ogtlin, Manuel Drazyk, Vinaychandran Pondenkandath, Michele\n  Alberti, Rolf Ingold", "title": "Generating Synthetic Handwritten Historical Documents With OCR\n  Constrained GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework to generate synthetic historical documents with\nprecise ground truth using nothing more than a collection of unlabeled\nhistorical images. Obtaining large labeled datasets is often the limiting\nfactor to effectively use supervised deep learning methods for Document Image\nAnalysis (DIA). Prior approaches towards synthetic data generation either\nrequire expertise or result in poor accuracy in the synthetic documents. To\nachieve high precision transformations without requiring expertise, we tackle\nthe problem in two steps. First, we create template documents with\nuser-specified content and structure. Second, we transfer the style of a\ncollection of unlabeled historical images to these template documents while\npreserving their text and layout. We evaluate the use of our synthetic\nhistorical documents in a pre-training setting and find that we outperform the\nbaselines (randomly initialized and pre-trained). Additionally, with visual\nexamples, we demonstrate a high-quality synthesis that makes it possible to\ngenerate large labeled historical document datasets with precise ground truth.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 09:39:17 GMT"}, {"version": "v2", "created": "Sun, 16 May 2021 18:52:10 GMT"}], "update_date": "2021-05-18", "authors_parsed": [["V\u00f6gtlin", "Lars", ""], ["Drazyk", "Manuel", ""], ["Pondenkandath", "Vinaychandran", ""], ["Alberti", "Michele", ""], ["Ingold", "Rolf", ""]]}, {"id": "2103.08249", "submitter": "Zhaoyang Hai", "authors": "Zhaoyang Hai, Xiabi Liu", "title": "Evolving parametrized Loss for Image Classification Learning on Small\n  Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a meta-learning approach to evolving a parametrized loss\nfunction, which is called Meta-Loss Network (MLN), for training the image\nclassification learning on small datasets. In our approach, the MLN is embedded\nin the framework of classification learning as a differentiable objective\nfunction. The MLN is evolved with the Evolutionary Strategy algorithm (ES) to\nan optimized loss function, such that a classifier, which optimized to minimize\nthis loss, will achieve a good generalization effect. A classifier learns on a\nsmall training dataset to minimize MLN with Stochastic Gradient Descent (SGD),\nand then the MLN is evolved with the precision of the small-dataset-updated\nclassifier on a large validation dataset. In order to evaluate our approach,\nthe MLN is trained with a large number of small sample learning tasks sampled\nfrom FashionMNIST and tested on validation tasks sampled from FashionMNIST and\nCIFAR10. Experiment results demonstrate that the MLN effectively improved\ngeneralization compared to classical cross-entropy error and mean squared\nerror.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 10:00:18 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Hai", "Zhaoyang", ""], ["Liu", "Xiabi", ""]]}, {"id": "2103.08251", "submitter": "Xueshuang Xiang", "authors": "Wei Bao, Meiyu Huang, Yaqin Zhang, Yao Xu, Xuejiao Liu, Xueshuang\n  Xiang", "title": "Boosting ship detection in SAR images with complementary pretraining\n  techniques", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning methods have made significant progress in ship detection in\nsynthetic aperture radar (SAR) images. The pretraining technique is usually\nadopted to support deep neural networks-based SAR ship detectors due to the\nscarce labeled SAR images. However, directly leveraging ImageNet pretraining is\nhardly to obtain a good ship detector because of different imaging perspective\nand geometry. In this paper, to resolve the problem of inconsistent imaging\nperspective between ImageNet and earth observations, we propose an optical ship\ndetector (OSD) pretraining technique, which transfers the characteristics of\nships in earth observations to SAR images from a large-scale aerial image\ndataset. On the other hand, to handle the problem of different imaging geometry\nbetween optical and SAR images, we propose an optical-SAR matching (OSM)\npretraining technique, which transfers plentiful texture features from optical\nimages to SAR images by common representation learning on the optical-SAR\nmatching task. Finally, observing that the OSD pretraining based SAR ship\ndetector has a better recall on sea area while the OSM pretraining based SAR\nship detector can reduce false alarms on land area, we combine the predictions\nof the two detectors through weighted boxes fusion to further improve detection\nresults. Extensive experiments on four SAR ship detection datasets and two\nrepresentative CNN-based detection benchmarks are conducted to show the\neffectiveness and complementarity of the two proposed detectors, and the\nstate-of-the-art performance of the combination of the two detectors. The\nproposed method won the sixth place of ship detection in SAR images in 2020\nGaofen challenge.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 10:03:04 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Bao", "Wei", ""], ["Huang", "Meiyu", ""], ["Zhang", "Yaqin", ""], ["Xu", "Yao", ""], ["Liu", "Xuejiao", ""], ["Xiang", "Xueshuang", ""]]}, {"id": "2103.08255", "submitter": "Thanh Nguyen Xuan", "authors": "Thanh Nguyen, Tung M. Luu, Thang Vu and Chang D. Yoo", "title": "Sample-efficient Reinforcement Learning Representation Learning with\n  Curiosity Contrastive Forward Dynamics Model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Developing an agent in reinforcement learning (RL) that is capable of\nperforming complex control tasks directly from high-dimensional observation\nsuch as raw pixels is yet a challenge as efforts are made towards improving\nsample efficiency and generalization. This paper considers a learning framework\nfor Curiosity Contrastive Forward Dynamics Model (CCFDM) in achieving a more\nsample-efficient RL based directly on raw pixels. CCFDM incorporates a forward\ndynamics model (FDM) and performs contrastive learning to train its deep\nconvolutional neural network-based image encoder (IE) to extract conducive\nspatial and temporal information for achieving a more sample efficiency for RL.\nIn addition, during training, CCFDM provides intrinsic rewards, produced based\non FDM prediction error, encourages the curiosity of the RL agent to improve\nexploration. The diverge and less-repetitive observations provide by both our\nexploration strategy and data augmentation available in contrastive learning\nimprove not only the sample efficiency but also the generalization. Performance\nof existing model-free RL methods such as Soft Actor-Critic built on top of\nCCFDM outperforms prior state-of-the-art pixel-based RL methods on the DeepMind\nControl Suite benchmark.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 10:08:52 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Nguyen", "Thanh", ""], ["Luu", "Tung M.", ""], ["Vu", "Thang", ""], ["Yoo", "Chang D.", ""]]}, {"id": "2103.08306", "submitter": "Bushra Sabir", "authors": "Bushra Sabir, M. Ali Babar, Raj Gaire", "title": "ReinforceBug: A Framework to Generate Adversarial Textual Examples", "comments": "Accepted in NAACL-HLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Adversarial Examples (AEs) generated by perturbing original training examples\nare useful in improving the robustness of Deep Learning (DL) based models. Most\nprior works, generate AEs that are either unconscionable due to lexical errors\nor semantically or functionally deviant from original examples. In this paper,\nwe present ReinforceBug, a reinforcement learning framework, that learns a\npolicy that is transferable on unseen datasets and generates utility-preserving\nand transferable (on other models) AEs. Our results show that our method is on\naverage 10% more successful as compared to the state-of-the-art attack\nTextFooler. Moreover, the target models have on average 73.64% confidence in\nthe wrong prediction, the generated AEs preserve the functional equivalence and\nsemantic similarity (83.38% ) to their original counterparts, and are\ntransferable on other models with an average success rate of 46%.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 05:35:51 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Sabir", "Bushra", ""], ["Babar", "M. Ali", ""], ["Gaire", "Raj", ""]]}, {"id": "2103.08313", "submitter": "Ping Guo", "authors": "Ping Guo, Kaizhu Huang, and Zenglin Xu", "title": "Partial Differential Equations is All You Need for Generating Neural\n  Architectures -- A Theory for Physical Artificial Intelligence Systems", "comments": "15 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this work, we generalize the reaction-diffusion equation in statistical\nphysics, Schr\\\"odinger equation in quantum mechanics, Helmholtz equation in\nparaxial optics into the neural partial differential equations (NPDE), which\ncan be considered as the fundamental equations in the field of artificial\nintelligence research. We take finite difference method to discretize NPDE for\nfinding numerical solution, and the basic building blocks of deep neural\nnetwork architecture, including multi-layer perceptron, convolutional neural\nnetwork and recurrent neural networks, are generated. The learning strategies,\nsuch as Adaptive moment estimation, L-BFGS, pseudoinverse learning algorithms\nand partial differential equation constrained optimization, are also presented.\nWe believe it is of significance that presented clear physical image of\ninterpretable deep neural networks, which makes it be possible for applying to\nanalog computing device design, and pave the road to physical artificial\nintelligence.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 00:05:46 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Guo", "Ping", ""], ["Huang", "Kaizhu", ""], ["Xu", "Zenglin", ""]]}, {"id": "2103.08315", "submitter": "Eric Allen", "authors": "Eric E. Allen", "title": "Neural Networks and Denotation", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a framework for reasoning about what meaning is captured by the\nneurons in a trained neural network. We provide a strategy for discovering\nmeaning by training a second model (referred to as an observer model) to\nclassify the state of the model it observes (an object model) in relation to\nattributes of the underlying dataset. We implement and evaluate observer models\nin the context of a specific set of classification problems, employ heat maps\nfor visualizing the relevance of components of an object model in the context\nof linear observer models, and use these visualizations to extract insights\nabout the manner in which neural networks identify salient characteristics of\ntheir inputs. We identify important properties captured decisively in trained\nneural networks; some of these properties are denoted by individual neurons.\nFinally, we observe that the label proportion of a property denoted by a neuron\nis dependent on the depth of a neuron within a network; we analyze these\ndependencies, and provide an interpretation of them.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 15:32:22 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Allen", "Eric E.", ""]]}, {"id": "2103.08327", "submitter": "Saibal Majumder", "authors": "Saibal Majumder", "title": "Some Network Optimization Models under Diverse Uncertain Environments", "comments": "Thesis document", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE math.OC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Network models provide an efficient way to represent many real life problems\nmathematically. In the last few decades, the field of network optimization has\nwitnessed an upsurge of interest among researchers and practitioners. The\nnetwork models considered in this thesis are broadly classified into four types\nincluding transportation problem, shortest path problem, minimum spanning tree\nproblem and maximum flow problem. Quite often, we come across situations, when\nthe decision parameters of network optimization problems are not precise and\ncharacterized by various forms of uncertainties arising from the factors, like\ninsufficient or incomplete data, lack of evidence, inappropriate judgements and\nrandomness. Considering the deterministic environment, there exist several\nstudies on network optimization problems. However, in the literature, not many\ninvestigations on single and multi objective network optimization problems are\nobserved under diverse uncertain frameworks. This thesis proposes seven\ndifferent network models under different uncertain paradigms. Here, the\nuncertain programming techniques used to formulate the uncertain network models\nare (i) expected value model, (ii) chance constrained model and (iii) dependent\nchance constrained model. Subsequently, the corresponding crisp equivalents of\nthe uncertain network models are solved using different solution methodologies.\nThe solution methodologies used in this thesis can be broadly categorized as\nclassical methods and evolutionary algorithms. The classical methods, used in\nthis thesis, are Dijkstra and Kruskal algorithms, modified rough Dijkstra\nalgorithm, global criterion method, epsilon constraint method and fuzzy\nprogramming method. Whereas, among the evolutionary algorithms, we have\nproposed the varying population genetic algorithm with indeterminate crossover\nand considered two multi objective evolutionary algorithms.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2021 13:48:15 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Majumder", "Saibal", ""]]}, {"id": "2103.08389", "submitter": "Nuno Louren\\c{c}o", "authors": "Jessica M\\'egane, Nuno Louren\\c{c}o, Penousal Machado", "title": "Probabilistic Grammatical Evolution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Grammatical Evolution (GE) is one of the most popular Genetic Programming\n(GP) variants, and it has been used with success in several problem domains.\nSince the original proposal, many enhancements have been proposed to GE in\norder to address some of its main issues and improve its performance.\n  In this paper we propose Probabilistic Grammatical Evolution (PGE), which\nintroduces a new genotypic representation and new mapping mechanism for GE.\nSpecifically, we resort to a Probabilistic Context-Free Grammar (PCFG) where\nits probabilities are adapted during the evolutionary process, taking into\naccount the productions chosen to construct the fittest individual. The\ngenotype is a list of real values, where each value represents the likelihood\nof selecting a derivation rule. We evaluate the performance of PGE in two\nregression problems and compare it with GE and Structured Grammatical Evolution\n(SGE).\n  The results show that PGE has a a better performance than GE, with\nstatistically significant differences, and achieved similar performance when\ncomparing with SGE.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 13:54:26 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["M\u00e9gane", "Jessica", ""], ["Louren\u00e7o", "Nuno", ""], ["Machado", "Penousal", ""]]}, {"id": "2103.08391", "submitter": "Blai Bonet", "authors": "Ivan D. Rodriguez and Blai Bonet and Sebastian Sardina and Hector\n  Geffner", "title": "Flexible FOND Planning with Explicit Fairness Assumptions", "comments": "Extended version of ICAPS-21 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider the problem of reaching a propositional goal condition in\nfully-observable non-deterministic (FOND) planning under a general class of\nfairness assumptions that are given explicitly. The fairness assumptions are of\nthe form A/B and say that state trajectories that contain infinite occurrences\nof an action a from A in a state s and finite occurrence of actions from B,\nmust also contain infinite occurrences of action a in s followed by each one of\nits possible outcomes. The infinite trajectories that violate this condition\nare deemed as unfair, and the solutions are policies for which all the fair\ntrajectories reach a goal state. We show that strong and strong-cyclic FOND\nplanning, as well as QNP planning, a planning model introduced recently for\ngeneralized planning, are all special cases of FOND planning with fairness\nassumptions of this form which can also be combined. FOND+ planning, as this\nform of planning is called, combines the syntax of FOND planning with some of\nthe versatility of LTL for expressing fairness constraints. A new planner is\nimplemented by reducing FOND+ planning to answer set programs, and the\nperformance of the planner is evaluated in comparison with FOND and QNP\nplanners, and LTL synthesis tools.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 13:57:07 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Rodriguez", "Ivan D.", ""], ["Bonet", "Blai", ""], ["Sardina", "Sebastian", ""], ["Geffner", "Hector", ""]]}, {"id": "2103.08396", "submitter": "Mingwei Shi", "authors": "Mingwei Shi", "title": "Gradient Policy on \"CartPole\" game and its' expansibility to F1Tenth\n  Autonomous Vehicles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Policy gradient is an effective way to estimate continuous action on the\nenvironment. This paper, it about explaining the mathematical formula and code\nimplementation. In the end, comparing between the rotation angle of the stick\non CartPole , and the angle of the Autonomous vehicle when turning, and\nutilizing the Bicycle Model, a simple Kinematic dynamic model, are the purpose\nto discover the similarity between these two models, so as to facilitate the\nmodel transfer from CartPole to the F1tenth Autonomous vehicle.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 14:09:25 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Shi", "Mingwei", ""]]}, {"id": "2103.08408", "submitter": "Mital Raithatha", "authors": "Mital Raithatha, Aizaz U. Chaudhry, Roshdy H.M. Hafez, John W.\n  Chinneck", "title": "A Fast Heuristic for Gateway Location in Wireless Backhaul of 5G\n  Ultra-Dense Networks", "comments": "Accepted Journal paper in IEEE access", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In 5G Ultra-Dense Networks, a distributed wireless backhaul is an attractive\nsolution for forwarding traffic to the core. The macro-cell coverage area is\ndivided into many small cells. A few of these cells are designated as gateways\nand are linked to the core by high-capacity fiber optic links. Each small cell\nis associated with one gateway and all small cells forward their traffic to\ntheir respective gateway through multi-hop mesh networks. We investigate the\ngateway location problem and show that finding near-optimal gateway locations\nimproves the backhaul network capacity. An exact p-median integer linear\nprogram is formulated for comparison with our novel K-GA heuristic that\ncombines a Genetic Algorithm (GA) with K-means clustering to find near-optimal\ngateway locations. We compare the performance of KGA with six other approaches\nin terms of average number of hops and backhaul network capacity at different\nnode densities through extensive Monte Carlo simulations. All approaches are\ntested in various user distribution scenarios, including uniform distribution,\nbivariate Gaussian distribution, and cluster distribution. In all cases K-GA\nprovides near-optimal results, achieving average number of hops and backhaul\nnetwork capacity within 2% of optimal while saving an average of 95% of the\nexecution time.\n", "versions": [{"version": "v1", "created": "Mon, 22 Feb 2021 17:34:49 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Raithatha", "Mital", ""], ["Chaudhry", "Aizaz U.", ""], ["Hafez", "Roshdy H. M.", ""], ["Chinneck", "John W.", ""]]}, {"id": "2103.08457", "submitter": "Zhiwei Xu", "authors": "Zhiwei Xu, Thalaiyasingam Ajanthan, Vibhav Vineet, Richard Hartley", "title": "RANP: Resource Aware Neuron Pruning at Initialization for 3D CNNs", "comments": "this is an extension of our 3DV2020 conference paper RANP. arXiv\n  admin note: substantial text overlap with arXiv:2010.02488", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although 3D Convolutional Neural Networks are essential for most learning\nbased applications involving dense 3D data, their applicability is limited due\nto excessive memory and computational requirements. Compressing such networks\nby pruning therefore becomes highly desirable. However, pruning 3D CNNs is\nlargely unexplored possibly because of the complex nature of typical pruning\nalgorithms that embeds pruning into an iterative optimization paradigm. In this\nwork, we introduce a Resource Aware Neuron Pruning (RANP) algorithm that prunes\n3D CNNs at initialization to high sparsity levels. Specifically, the core idea\nis to obtain an importance score for each neuron based on their sensitivity to\nthe loss function. This neuron importance is then reweighted according to the\nneuron resource consumption related to FLOPs or memory. We demonstrate the\neffectiveness of our pruning method on 3D semantic segmentation with widely\nused 3D-UNets on ShapeNet and BraTS'18 datasets, video classification with\nMobileNetV2 and I3D on UCF101 dataset, and two-view stereo matching with\nPyramid Stereo Matching (PSM) network on SceneFlow dataset. In these\nexperiments, our RANP leads to roughly 50%-95% reduction in FLOPs and 35%-80%\nreduction in memory with negligible loss in accuracy compared to the unpruned\nnetworks. This significantly reduces the computational resources required to\ntrain 3D CNNs. The pruned network obtained by our algorithm can also be easily\nscaled up and transferred to another dataset for training.\n", "versions": [{"version": "v1", "created": "Tue, 9 Feb 2021 04:35:29 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Xu", "Zhiwei", ""], ["Ajanthan", "Thalaiyasingam", ""], ["Vineet", "Vibhav", ""], ["Hartley", "Richard", ""]]}, {"id": "2103.08458", "submitter": "Shaina Raza Ms", "authors": "Shaina Raza, Chen Ding", "title": "Deep Dynamic Neural Network to trade-off between Accuracy and Diversity\n  in a News Recommender System", "comments": "under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The news recommender systems are marked by a few unique challenges specific\nto the news domain. These challenges emerge from rapidly evolving readers'\ninterests over dynamically generated news items that continuously change over\ntime. News reading is also driven by a blend of a reader's long-term and\nshort-term interests. In addition, diversity is required in a news recommender\nsystem, not only to keep the reader engaged in the reading process but to get\nthem exposed to different views and opinions. In this paper, we propose a deep\nneural network that jointly learns informative news and readers' interests into\na unified framework. We learn the news representation (features) from the\nheadlines, snippets (body) and taxonomy (category, subcategory) of news. We\nlearn a reader's long-term interests from the reader's click history,\nshort-term interests from the recent clicks via LSTMSs and the diversified\nreader's interests through the attention mechanism. We also apply different\nlevels of attention to our model. We conduct extensive experiments on two news\ndatasets to demonstrate the effectiveness of our approach.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 15:30:25 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 00:30:07 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Raza", "Shaina", ""], ["Ding", "Chen", ""]]}, {"id": "2103.08494", "submitter": "Chao Li", "authors": "Chao Li, Yiran Wei, Xi Chen, Carola-Bibiane Schonlieb", "title": "BrainNetGAN: Data augmentation of brain connectivity using generative\n  adversarial network for dementia classification", "comments": "accepted by DGM4MICCAI workshop 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI eess.IV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Alzheimer's disease (AD) is the most common age-related dementia. It remains\na challenge to identify the individuals at risk of dementia for precise\nmanagement. Brain MRI offers a noninvasive biomarker to detect brain aging.\nPrevious evidence shows that the brain structural change detected by diffusion\nMRI is associated with dementia. Mounting studies has conceptualised the brain\nas a complex network, which has shown the utility of this approach in\ncharacterising various neurological and psychiatric disorders. Therefore, the\nstructural connectivity shows promise in dementia classification. The proposed\nBrainNetGAN is a generative adversarial network variant to augment the brain\nstructural connectivity matrices for binary dementia classification tasks.\nStructural connectivity matrices between separated brain regions are\nconstructed using tractography on diffusion MRI data. The BrainNetGAN model is\ntrained to generate fake brain connectivity matrices, which are expected to\nreflect latent distribution of the real brain network data. Finally, a\nconvolutional neural network classifier is proposed for binary dementia\nclassification. Numerical results show that the binary classification\nperformance in the testing set was improved using the BrainNetGAN augmented\ndataset. The proposed methodology allows quick synthesis of an arbitrary number\nof augmented connectivity matrices and can be easily transferred to similar\nclassification tasks.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 23:44:53 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2021 19:30:36 GMT"}, {"version": "v3", "created": "Fri, 23 Jul 2021 08:43:13 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Li", "Chao", ""], ["Wei", "Yiran", ""], ["Chen", "Xi", ""], ["Schonlieb", "Carola-Bibiane", ""]]}, {"id": "2103.08504", "submitter": "Mohammad Reza Mohebbian", "authors": "Mohammad Reza Mohebbian, Seyed Shahim Vedaei, Khan A. Wahid and Paul\n  Babyn", "title": "Siamese Network Features for Endoscopy Image and Video Localization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Conventional Endoscopy (CE) and Wireless Capsule Endoscopy (WCE) are known\ntools for diagnosing gastrointestinal (GI) tract disorders. Localizing frames\nprovide valuable information about the anomaly location and also can help\nclinicians determine a more appropriate treatment plan. There are many\nautomated algorithms to detect the anomaly. However, very few of the existing\nworks address the issue of localization. In this study, we present a\ncombination of meta-learning and deep learning for localizing both endoscopy\nimages and video. A dataset is collected from 10 different anatomical positions\nof human GI tract. In the meta-learning section, the system was trained using\n78 CE and 27 WCE annotated frames with a modified Siamese Neural Network (SNN)\nto predict the location of one single image/frame. Then, a postprocessing\nsection using bidirectional long short-term memory is proposed for localizing a\nsequence of frames. Here, we have employed feature vector, distance and\npredicted location obtained from a trained SNN. The postprocessing section is\ntrained and tested on 1,028 and 365 seconds of CE and WCE videos using hold-out\nvalidation (50%), and achieved F1-score of 86.3% and 83.0%, respectively. In\naddition, we performed subjective evaluation using nine gastroenterologists.\nThe results show that the computer-aided methods can outperform\ngastroenterologists assessment of localization. The proposed method is compared\nwith various approaches, such as support vector machine with hand-crafted\nfeatures, convolutional neural network and the transfer learning-based methods,\nand showed better results. Therefore, it can be used in frame localization,\nwhich can help in video summarization and anomaly detection.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 16:24:30 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Mohebbian", "Mohammad Reza", ""], ["Vedaei", "Seyed Shahim", ""], ["Wahid", "Khan A.", ""], ["Babyn", "Paul", ""]]}, {"id": "2103.08508", "submitter": "Mohammad Reza Mohebbian", "authors": "Mohammad Reza Mohebbian, Seyed Shahim Vedaei, Khan A. Wahid and Paul\n  Babyn", "title": "Multiclass Anomaly Detection in GI Endoscopic Images using Optimized\n  Deep One-class Classification in an Imbalanced Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Wireless Capsule Endoscopy helps physicians examine the gastrointestinal (GI)\ntract noninvasively, with the cost of generating many images. Many available\ndatasets, such as KID2 and Kvasir, suffer from imbalance issue which make it\ndifficult to train an effective artificial intelligence (AI) system. Moreover,\nincreasing number of classes makes the problem worse. In this study, an\nensemble of one-class classifiers is used for detecting anomaly. This method\nfocuses on learning single models using samples from only one class, and\nensemble all models for multiclass classification. A total of 1,778 normal, 227\ninflammation, 303 vascular diseases, and 44 polyp images have been used from\nthe KID2 dataset. In the first step, deep features are extracted based on an\nautoencoder architecture from the preprocessed images. Then, these features are\noversampled using Synthetic Minority Over-sampling Technique and clustered\nusing Ordering Points to Identify the Clustering Structure. To create one-class\nclassification model, the Support Vector Data Descriptions are trained on each\ncluster with the help of Ant Colony Optimization, which is also used for tuning\nclustering parameters for improving F1-score. This process is applied on each\nclasses and ensemble of final models used for multiclass classification. The\nentire algorithm ran 5 times and obtained F1-score 96.3 +- 0.2% and\nmacro-average F1-score 85.0 +- 0.4%, for anomaly detection and multiclass\nclassification, respectively. The results are compared with GoogleNet, AlexNet,\nResnet50, VGG16 and other published algorithms, and demonstrate that the\nproposed method is a competitive choice for multiclass class anomaly detection\nin GI images.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 16:28:42 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Mohebbian", "Mohammad Reza", ""], ["Vedaei", "Seyed Shahim", ""], ["Wahid", "Khan A.", ""], ["Babyn", "Paul", ""]]}, {"id": "2103.08545", "submitter": "Miruna Clinciu", "authors": "Miruna Clinciu, Arash Eshghi, and Helen Hastie", "title": "A Study of Automatic Metrics for the Evaluation of Natural Language\n  Explanations", "comments": "Accepted at EACL 2021", "journal-ref": null, "doi": null, "report-no": "2021.eacl-main.202", "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  As transparency becomes key for robotics and AI, it will be necessary to\nevaluate the methods through which transparency is provided, including\nautomatically generated natural language (NL) explanations. Here, we explore\nparallels between the generation of such explanations and the much-studied\nfield of evaluation of Natural Language Generation (NLG). Specifically, we\ninvestigate which of the NLG evaluation measures map well to explanations. We\npresent the ExBAN corpus: a crowd-sourced corpus of NL explanations for\nBayesian Networks. We run correlations comparing human subjective ratings with\nNLG automatic measures. We find that embedding-based automatic NLG evaluation\nmethods, such as BERTScore and BLEURT, have a higher correlation with human\nratings, compared to word-overlap metrics, such as BLEU and ROUGE. This work\nhas implications for Explainable AI and transparent robotic and autonomous\nsystems.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 17:10:39 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Clinciu", "Miruna", ""], ["Eshghi", "Arash", ""], ["Hastie", "Helen", ""]]}, {"id": "2103.08562", "submitter": "Kai Packh\\\"auser", "authors": "Kai Packh\\\"auser, Sebastian G\\\"undel, Nicolas M\\\"unster, Christopher\n  Syben, Vincent Christlein, Andreas Maier", "title": "Is Medical Chest X-ray Data Anonymous?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the rise and ever-increasing potential of deep learning techniques in\nrecent years, publicly available medical datasets became a key factor to enable\nreproducible development of diagnostic algorithms in the medical domain.\nMedical data contains sensitive patient-related information and is therefore\nusually anonymized by removing patient identifiers, e.g., patient names before\npublication. To the best of our knowledge, we are the first to show that a\nwell-trained deep learning system is able to recover the patient identity from\nchest X-ray data. We demonstrate this using the publicly available large-scale\nChestX-ray14 dataset, a collection of 112,120 frontal-view chest X-ray images\nfrom 30,805 unique patients. Our verification system is able to identify\nwhether two frontal chest X-ray images are from the same person with an AUC of\n0.9940 and a classification accuracy of 95.55%. We further highlight that the\nproposed system is able to reveal the same person even ten and more years after\nthe initial scan. When pursuing a retrieval approach, we observe an mAP@R of\n0.9748 and a precision@1 of 0.9963. Furthermore, we achieve an AUC of up to\n0.9870 and a precision@1 of up to 0.9444 when evaluating our trained networks\non CheXpert and the COVID-19 Image Data Collection. Based on this high\nidentification rate, a potential attacker may leak patient-related information\nand additionally cross-reference images to obtain more information. Thus, there\nis a great risk of sensitive content falling into unauthorized hands or being\ndisseminated against the will of the concerned patients. Especially during the\nCOVID-19 pandemic, numerous chest X-ray datasets have been published to advance\nresearch. Therefore, such data may be vulnerable to potential attacks by deep\nlearning-based re-identification algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 17:26:43 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 17:22:04 GMT"}, {"version": "v3", "created": "Tue, 1 Jun 2021 10:36:57 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Packh\u00e4user", "Kai", ""], ["G\u00fcndel", "Sebastian", ""], ["M\u00fcnster", "Nicolas", ""], ["Syben", "Christopher", ""], ["Christlein", "Vincent", ""], ["Maier", "Andreas", ""]]}, {"id": "2103.08588", "submitter": "Teodoro Baldazzi", "authors": "Teodoro Baldazzi (Universit\\`a Roma Tre), Luigi Bellomarini (Banca\n  d'Italia), Emanuel Sallinger (University of Oxford and TU Wien), Paolo Atzeni\n  (Universit\\`a Roma Tre)", "title": "iWarded: A System for Benchmarking Datalog+/- Reasoning (technical\n  report)", "comments": "17 pages, 14 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen increasing popularity of logic-based reasoning\nsystems, with research and industrial interest as well as many flourishing\napplications in the area of Knowledge Graphs. Despite that, one can observe a\nsubstantial lack of specific tools able to generate nontrivial reasoning\nsettings and benchmark scenarios. As a consequence, evaluating, analysing and\ncomparing reasoning systems is a complex task, especially when they embody\nsophisticated optimizations and execution techniques that leverage the\ntheoretical underpinnings of the adopted logic fragment. In this paper, we aim\nat filling this gap by introducing iWarded, a system that can generate very\nlarge, complex, realistic reasoning settings to be used for the benchmarking of\nlogic-based reasoning systems adopting Datalog+/-, a family of extensions of\nDatalog that has seen a resurgence in the last few years. In particular,\niWarded generates reasoning settings for Warded Datalog+/-, a language with a\nvery good tradeoff between computational complexity and expressive power. In\nthe paper, we present the iWarded system and a set of novel theoretical results\nadopted to generate effective scenarios. As Datalog-based languages are of\ngeneral interest and see increasing adoption, we believe that iWarded is a step\nforward in the empirical evaluation of current and future systems.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 17:56:46 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Baldazzi", "Teodoro", "", "Universit\u00e0 Roma Tre"], ["Bellomarini", "Luigi", "", "Banca\n  d'Italia"], ["Sallinger", "Emanuel", "", "University of Oxford and TU Wien"], ["Atzeni", "Paolo", "", "Universit\u00e0 Roma Tre"]]}, {"id": "2103.08624", "submitter": "Yunlong Song", "authors": "Yunlong Song, Mats Steinweg, Elia Kaufmann, and Davide Scaramuzza", "title": "Autonomous Drone Racing with Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many robotic tasks, such as drone racing, the goal is to travel through a\nset of waypoints as fast as possible. A key challenge for this task is planning\nthe minimum-time trajectory, which is typically solved by assuming perfect\nknowledge of the waypoints to pass in advance. The resulting solutions are\neither highly specialized for a single-track layout, or suboptimal due to\nsimplifying assumptions about the platform dynamics. In this work, a new\napproach to minimum-time trajectory generation for quadrotors is presented.\nLeveraging deep reinforcement learning and relative gate observations, this\napproach can adaptively compute near-time-optimal trajectories for random track\nlayouts. Our method exhibits a significant computational advantage over\napproaches based on trajectory optimization for non-trivial track\nconfigurations. The proposed approach is evaluated on a set of race tracks in\nsimulation and the real world, achieving speeds of up to 17 m/s with a physical\nquadrotor.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 18:05:49 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Song", "Yunlong", ""], ["Steinweg", "Mats", ""], ["Kaufmann", "Elia", ""], ["Scaramuzza", "Davide", ""]]}, {"id": "2103.08673", "submitter": "Mingyue Zhang", "authors": "Mingyue Zhang", "title": "System Component-Level Self-Adaptations for Security via Bayesian Games", "comments": "Published in International Conference on Software Engineering,\n  Companion Volume", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Security attacks present unique challenges to self-adaptive system design due\nto the adversarial nature of the environment. However, modeling the system as a\nsingle player, as done in prior works in security domain, is insufficient for\nthe system under partial compromise and for the design of fine-grained\ndefensive strategies where the rest of the system with autonomy can cooperate\nto mitigate the impact of attacks. To deal with such issues, we propose a new\nself-adaptive framework incorporating Bayesian game and model the defender\n(i.e., the system) at the granularity of components in system architecture. The\nsystem architecture model is translated into a Bayesian multi-player game,\nwhere each component is modeled as an independent player while security attacks\nare encoded as variant types for the components. The defensive strategy for the\nsystem is dynamically computed by solving the pure equilibrium to achieve the\nbest possible system utility, improving the resiliency of the system against\nsecurity attacks.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 16:20:59 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Zhang", "Mingyue", ""]]}, {"id": "2103.08733", "submitter": "Nikolaos Kondylidis", "authors": "Nikolaos Kondylidis, Jie Zou and Evangelos Kanoulas", "title": "Category Aware Explainable Conversational Recommendation", "comments": "Workshop on Mixed-Initiative ConveRsatiOnal Systems (MICROS) @ECIR,\n  2021", "journal-ref": "Workshop on Mixed-Initiative ConveRsatiOnal Systems (MICROS)\n  @ECIR, 2021", "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most conversational recommendation approaches are either not explainable, or\nthey require external user's knowledge for explaining or their explanations\ncannot be applied in real time due to computational limitations. In this work,\nwe present a real time category based conversational recommendation approach,\nwhich can provide concise explanations without prior user knowledge being\nrequired. We first perform an explainable user model in the form of preferences\nover the items' categories, and then use the category preferences to recommend\nitems. The user model is performed by applying a BERT-based neural architecture\non the conversation. Then, we translate the user model into item recommendation\nscores using a Feed Forward Network. User preferences during the conversation\nin our approach are represented by category vectors which are directly\ninterpretable. The experimental results on the real conversational\nrecommendation dataset ReDial demonstrate comparable performance to the\nstate-of-the-art, while our approach is explainable. We also show the potential\npower of our framework by involving an oracle setting of category preference\nprediction.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 21:45:13 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Kondylidis", "Nikolaos", ""], ["Zou", "Jie", ""], ["Kanoulas", "Evangelos", ""]]}, {"id": "2103.08747", "submitter": "Ya Xiao", "authors": "Ya Xiao, Salman Ahmed, Wenjia Song, Xinyang Ge, Bimal Viswanath,\n  Danfeng Yao", "title": "Embedding Code Contexts for Cryptographic API Suggestion:New\n  Methodologies and Comparisons", "comments": "12 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite recent research efforts, the vision of automatic code generation\nthrough API recommendation has not been realized. Accuracy and expressiveness\nchallenges of API recommendation needs to be systematically addressed. We\npresent a new neural network-based approach, Multi-HyLSTM for API\nrecommendation --targeting cryptography-related code. Multi-HyLSTM leverages\nprogram analysis to guide the API embedding and recommendation. By analyzing\nthe data dependence paths of API methods, we train embedding and specialize a\nmulti-path neural network architecture for API recommendation tasks that\naccurately predict the next API method call. We address two previously\nunreported programming language-specific challenges, differentiating\nfunctionally similar APIs and capturing low-frequency long-range influences.\nOur results confirm the effectiveness of our design choices, including\nprogram-analysis-guided embedding, multi-path code suggestion architecture, and\nlow-frequency long-range-enhanced sequence learning, with high accuracy on\ntop-1 recommendations. We achieve a top-1 accuracy of 91.41% compared with\n77.44% from the state-of-the-art tool SLANG. In an analysis of 245 test cases,\ncompared with the commercial tool Codota, we achieve a top-1 recommendation\naccuracy of 88.98%, which is significantly better than Codota's accuracy of\n64.90%. We publish our data and code as a large Java cryptographic code\ndataset.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 22:27:57 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 02:16:07 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Xiao", "Ya", ""], ["Ahmed", "Salman", ""], ["Song", "Wenjia", ""], ["Ge", "Xinyang", ""], ["Viswanath", "Bimal", ""], ["Yao", "Danfeng", ""]]}, {"id": "2103.08780", "submitter": "Maximilian Kupi", "authors": "Maximilian Kupi, Michael Bodnar, Nikolas Schmidt, and Carlos Eduardo\n  Posada", "title": "dictNN: A Dictionary-Enhanced CNN Approach for Classifying Hate Speech\n  on Twitter", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Hate speech on social media is a growing concern, and automated methods have\nso far been sub-par at reliably detecting it. A major challenge lies in the\npotentially evasive nature of hate speech due to the ambiguity and fast\nevolution of natural language. To tackle this, we introduce a vectorisation\nbased on a crowd-sourced and continuously updated dictionary of hate words and\npropose fusing this approach with standard word embedding in order to improve\nthe classification performance of a CNN model. To train and test our model we\nuse a merge of two established datasets (110,748 tweets in total). By adding\nthe dictionary-enhanced input, we are able to increase the CNN model's\npredictive power and increase the F1 macro score by seven percentage points.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 00:27:33 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Kupi", "Maximilian", ""], ["Bodnar", "Michael", ""], ["Schmidt", "Nikolas", ""], ["Posada", "Carlos Eduardo", ""]]}, {"id": "2103.08786", "submitter": "Jessie J. Smith", "authors": "Nasim Sonboli and Jessie J. Smith, Florencia Cabral Berenfus, Robin\n  Burke, Casey Fiesler", "title": "Fairness and Transparency in Recommendation: The Users' Perspective", "comments": null, "journal-ref": null, "doi": "10.1145/3450613.3456835", "report-no": null, "categories": "cs.IR cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Though recommender systems are defined by personalization, recent work has\nshown the importance of additional, beyond-accuracy objectives, such as\nfairness. Because users often expect their recommendations to be purely\npersonalized, these new algorithmic objectives must be communicated\ntransparently in a fairness-aware recommender system. While explanation has a\nlong history in recommender systems research, there has been little work that\nattempts to explain systems that use a fairness objective. Even though the\nprevious work in other branches of AI has explored the use of explanations as a\ntool to increase fairness, this work has not been focused on recommendation.\nHere, we consider user perspectives of fairness-aware recommender systems and\ntechniques for enhancing their transparency. We describe the results of an\nexploratory interview study that investigates user perceptions of fairness,\nrecommender systems, and fairness-aware objectives. We propose three features\n-- informed by the needs of our participants -- that could improve user\nunderstanding of and trust in fairness-aware recommender systems.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 00:42:09 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Sonboli", "Nasim", ""], ["Smith", "Jessie J.", ""], ["Berenfus", "Florencia Cabral", ""], ["Burke", "Robin", ""], ["Fiesler", "Casey", ""]]}, {"id": "2103.08796", "submitter": "Xiaojun Li", "authors": "Xiaojun Li, Jianwei Li, Ali Abdollahi and Trevor Jones", "title": "Data-driven Thermal Anomaly Detection for Batteries using Unsupervised\n  Shape Clustering", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.AI cs.LG cs.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  For electric vehicles (EV) and energy storage (ES) batteries, thermal runaway\nis a critical issue as it can lead to uncontrollable fires or even explosions.\nThermal anomaly detection can identify problematic battery packs that may\neventually undergo thermal runaway. However, there are common challenges like\ndata unavailability, environment and configuration variations, and battery\naging. We propose a data-driven method to detect battery thermal anomaly based\non comparing shape-similarity between thermal measurements. Based on their\nshapes, the measurements are continuously being grouped into different\nclusters. Anomaly is detected by monitoring deviations within the clusters.\nUnlike model-based or other data-driven methods, the proposed method is robust\nto data loss and requires minimal reference data for different pack\nconfigurations. As the initial experimental results show, the method not only\ncan be more accurate than the onboard BMS and but also can detect unforeseen\nanomalies at the early stage.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 01:29:41 GMT"}, {"version": "v2", "created": "Wed, 19 May 2021 23:56:30 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Li", "Xiaojun", ""], ["Li", "Jianwei", ""], ["Abdollahi", "Ali", ""], ["Jones", "Trevor", ""]]}, {"id": "2103.08800", "submitter": "Sajjad Fouladvand", "authors": "Sajjad Fouladvand, Jeffery Talbert, Linda P. Dwoskin, Heather Bush,\n  Amy Lynn Meadows, Lars E. Peterson, Ramakanth Kavuluru, Jin Chen", "title": "Predicting Opioid Use Disorder from Longitudinal Healthcare Data using\n  Multi-stream Transformer", "comments": "This manuscript has been accepted by AMIA 2021 for oral presentation\n  on November 1, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Opioid Use Disorder (OUD) is a public health crisis costing the US billions\nof dollars annually in healthcare, lost workplace productivity, and crime.\nAnalyzing longitudinal healthcare data is critical in addressing many\nreal-world problems in healthcare. Leveraging the real-world longitudinal\nhealthcare data, we propose a novel multi-stream transformer model called MUPOD\nfor OUD identification. MUPOD is designed to simultaneously analyze multiple\ntypes of healthcare data streams, such as medications and diagnoses, by\nattending to segments within and across these data streams. Our model tested on\nthe data from 392,492 patients with long-term back pain problems showed\nsignificantly better performance than the traditional models and recently\ndeveloped deep learning models.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 01:44:21 GMT"}, {"version": "v2", "created": "Wed, 7 Jul 2021 14:12:48 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Fouladvand", "Sajjad", ""], ["Talbert", "Jeffery", ""], ["Dwoskin", "Linda P.", ""], ["Bush", "Heather", ""], ["Meadows", "Amy Lynn", ""], ["Peterson", "Lars E.", ""], ["Kavuluru", "Ramakanth", ""], ["Chen", "Jin", ""]]}, {"id": "2103.08820", "submitter": "Yingqi Liu", "authors": "Yingqi Liu, Guangyu Shen, Guanhong Tao, Zhenting Wang, Shiqing Ma,\n  Xiangyu Zhang", "title": "EX-RAY: Distinguishing Injected Backdoor from Natural Features in Neural\n  Networks by Examining Differential Feature Symmetry", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Backdoor attack injects malicious behavior to models such that inputs\nembedded with triggers are misclassified to a target label desired by the\nattacker. However, natural features may behave like triggers, causing\nmisclassification once embedded. While they are inevitable, mis-recognizing\nthem as injected triggers causes false warnings in backdoor scanning. A\nprominent challenge is hence to distinguish natural features and injected\nbackdoors. We develop a novel symmetric feature differencing method that\nidentifies a smallest set of features separating two classes. A backdoor is\nconsidered injected if the corresponding trigger consists of features different\nfrom the set of features distinguishing the victim and target classes. We\nevaluate the technique on thousands of models, including both clean and\ntrojaned models, from the TrojAI rounds 2-4 competitions and a number of models\non ImageNet. Existing backdoor scanning techniques may produce hundreds of\nfalse positives (i.e., clean models recognized as trojaned). Our technique\nremoves 78-100% of the false positives (by a state-of-the-art scanner ABS) with\na small increase of false negatives by 0-30%, achieving 17-41% overall accuracy\nimprovement, and facilitates achieving top performance on the leaderboard. It\nalso boosts performance of other scanners. It outperforms false positive\nremoval methods using L2 distance and attribution techniques. We also\ndemonstrate its potential in detecting a number of semantic backdoor attacks.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 03:07:31 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 04:15:58 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Liu", "Yingqi", ""], ["Shen", "Guangyu", ""], ["Tao", "Guanhong", ""], ["Wang", "Zhenting", ""], ["Ma", "Shiqing", ""], ["Zhang", "Xiangyu", ""]]}, {"id": "2103.08829", "submitter": "Ryan Mukherjee", "authors": "Ryan Mukherjee, Derek Rollend, Gordon Christie, Armin Hadzic, Sally\n  Matson, Anshu Saksena, Marisa Hughes", "title": "Towards Indirect Top-Down Road Transport Emissions Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Road transportation is one of the largest sectors of greenhouse gas (GHG)\nemissions affecting climate change. Tackling climate change as a global\ncommunity will require new capabilities to measure and inventory road transport\nemissions. However, the large scale and distributed nature of vehicle emissions\nmake this sector especially challenging for existing inventory methods. In this\nwork, we develop machine learning models that use satellite imagery to perform\nindirect top-down estimation of road transport emissions. Our initial\nexperiments focus on the United States, where a bottom-up inventory was\navailable for training our models. We achieved a mean absolute error (MAE) of\n39.5 kg CO$_{2}$ of annual road transport emissions, calculated on a\npixel-by-pixel (100 m$^{2}$) basis in Sentinel-2 imagery. We also discuss key\nmodel assumptions and challenges that need to be addressed to develop models\ncapable of generalizing to global geography. We believe this work is the first\npublished approach for automated indirect top-down estimation of road transport\nsector emissions using visual imagery and represents a critical step towards\nscalable, global, near-real-time road transportation emissions inventories that\nare measured both independently and objectively.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 03:30:53 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Mukherjee", "Ryan", ""], ["Rollend", "Derek", ""], ["Christie", "Gordon", ""], ["Hadzic", "Armin", ""], ["Matson", "Sally", ""], ["Saksena", "Anshu", ""], ["Hughes", "Marisa", ""]]}, {"id": "2103.08835", "submitter": "Julian Yarkony", "authors": "Naveed Haghani, Jiaoyang Li, Sven Koenig, Gautam Kunapuli, Claudio\n  Contardo, Amelia Regan, Julian Yarkony", "title": "Multi-Robot Routing with Time Windows: A Column Generation Approach", "comments": "arXiv admin note: substantial text overlap with arXiv:2006.04856", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robots performing tasks in warehouses provide the first example of\nwide-spread adoption of autonomous vehicles in transportation and logistics.\nThe efficiency of these operations, which can vary widely in practice, are a\nkey factor in the success of supply chains. In this work we consider the\nproblem of coordinating a fleet of robots performing picking operations in a\nwarehouse so as to maximize the net profit achieved within a time period while\nrespecting problem- and robot-specific constraints. We formulate the problem as\na weighted set packing problem where the elements in consideration are items on\nthe warehouse floor that can be picked up and delivered within specified time\nwindows. We enforce the constraint that robots must not collide, that each item\nis picked up and delivered by at most one robot, and that the number of robots\nactive at any time does not exceed the total number available. Since the set of\nroutes is exponential in the size of the input, we attack optimization of the\nresulting integer linear program using column generation, where pricing amounts\nto solving an elementary resource-constrained shortest-path problem. We propose\nan efficient optimization scheme that avoids consideration of every increment\nwithin the time windows. We also propose a heuristic pricing algorithm that can\nefficiently solve the pricing subproblem. While this itself is an important\nproblem, the insights gained from solving these problems effectively can lead\nto new advances in other time-widow constrained vehicle routing problems.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 03:39:42 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Haghani", "Naveed", ""], ["Li", "Jiaoyang", ""], ["Koenig", "Sven", ""], ["Kunapuli", "Gautam", ""], ["Contardo", "Claudio", ""], ["Regan", "Amelia", ""], ["Yarkony", "Julian", ""]]}, {"id": "2103.08863", "submitter": "Peike Li", "authors": "Peike Li, Xin Yu, Yi Yang", "title": "Super-Resolving Cross-Domain Face Miniatures by Peeking at One-Shot\n  Exemplar", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Conventional face super-resolution methods usually assume testing\nlow-resolution (LR) images lie in the same domain as the training ones. Due to\ndifferent lighting conditions and imaging hardware, domain gaps between\ntraining and testing images inevitably occur in many real-world scenarios.\nNeglecting those domain gaps would lead to inferior face super-resolution (FSR)\nperformance. However, how to transfer a trained FSR model to a target domain\nefficiently and effectively has not been investigated. To tackle this problem,\nwe develop a Domain-Aware Pyramid-based Face Super-Resolution network, named\nDAP-FSR network. Our DAP-FSR is the first attempt to super-resolve LR faces\nfrom a target domain by exploiting only a pair of high-resolution (HR) and LR\nexemplar in the target domain. To be specific, our DAP-FSR firstly employs its\nencoder to extract the multi-scale latent representations of the input LR face.\nConsidering only one target domain example is available, we propose to augment\nthe target domain data by mixing the latent representations of the target\ndomain face and source domain ones, and then feed the mixed representations to\nthe decoder of our DAP-FSR. The decoder will generate new face images\nresembling the target domain image style. The generated HR faces in turn are\nused to optimize our decoder to reduce the domain gap. By iteratively updating\nthe latent representations and our decoder, our DAP-FSR will be adapted to the\ntarget domain, thus achieving authentic and high-quality upsampled HR faces.\nExtensive experiments on three newly constructed benchmarks validate the\neffectiveness and superior performance of our DAP-FSR compared to the\nstate-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 05:47:26 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Li", "Peike", ""], ["Yu", "Xin", ""], ["Yang", "Yi", ""]]}, {"id": "2103.08870", "submitter": "Lusine Abrahamyan", "authors": "Lusine Abrahamyan, Yiming Chen, Giannis Bekoulis and Nikos Deligiannis", "title": "Learned Gradient Compression for Distributed Deep Learning", "comments": "15 pages 24 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep neural networks on large datasets containing high-dimensional\ndata requires a large amount of computation. A solution to this problem is\ndata-parallel distributed training, where a model is replicated into several\ncomputational nodes that have access to different chunks of the data. This\napproach, however, entails high communication rates and latency because of the\ncomputed gradients that need to be shared among nodes at every iteration. The\nproblem becomes more pronounced in the case that there is wireless\ncommunication between the nodes (i.e. due to the limited network bandwidth). To\naddress this problem, various compression methods have been proposed including\nsparsification, quantization, and entropy encoding of the gradients. Existing\nmethods leverage the intra-node information redundancy, that is, they compress\ngradients at each node independently. In contrast, we advocate that the\ngradients across the nodes are correlated and propose methods to leverage this\ninter-node redundancy to improve compression efficiency. Depending on the node\ncommunication protocol (parameter server or ring-allreduce), we propose two\ninstances of the LGC approach that we coin Learned Gradient Compression (LGC).\nOur methods exploit an autoencoder (i.e. trained during the first stages of the\ndistributed training) to capture the common information that exists in the\ngradients of the distributed nodes. We have tested our LGC methods on the image\nclassification and semantic segmentation tasks using different convolutional\nneural networks (ResNet50, ResNet101, PSPNet) and multiple datasets (ImageNet,\nCifar10, CamVid). The ResNet101 model trained for image classification on\nCifar10 achieved an accuracy of 93.57%, which is lower than the baseline\ndistributed training with uncompressed gradients only by 0.18%.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 06:42:36 GMT"}, {"version": "v2", "created": "Wed, 17 Mar 2021 05:55:33 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Abrahamyan", "Lusine", ""], ["Chen", "Yiming", ""], ["Bekoulis", "Giannis", ""], ["Deligiannis", "Nikos", ""]]}, {"id": "2103.08877", "submitter": "Djordje Miladinovic", "authors": "{\\DJ}or{\\dj}e Miladinovi\\'c, Aleksandar Stani\\'c, Stefan Bauer,\n  J\\\"urgen Schmidhuber, Joachim M. Buhmann", "title": "Spatial Dependency Networks: Neural Layers for Improved Generative Image\n  Modeling", "comments": null, "journal-ref": "International Conference on Learning Representations (2021);", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How to improve generative modeling by better exploiting spatial regularities\nand coherence in images? We introduce a novel neural network for building image\ngenerators (decoders) and apply it to variational autoencoders (VAEs). In our\nspatial dependency networks (SDNs), feature maps at each level of a deep neural\nnet are computed in a spatially coherent way, using a sequential gating-based\nmechanism that distributes contextual information across 2-D space. We show\nthat augmenting the decoder of a hierarchical VAE by spatial dependency layers\nconsiderably improves density estimation over baseline convolutional\narchitectures and the state-of-the-art among the models within the same class.\nFurthermore, we demonstrate that SDN can be applied to large images by\nsynthesizing samples of high quality and coherence. In a vanilla VAE setting,\nwe find that a powerful SDN decoder also improves learning disentangled\nrepresentations, indicating that neural architectures play an important role in\nthis task. Our results suggest favoring spatial dependency over convolutional\nlayers in various VAE settings. The accompanying source code is given at\nhttps://github.com/djordjemila/sdn.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 07:01:08 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Miladinovi\u0107", "\u0110or\u0111e", ""], ["Stani\u0107", "Aleksandar", ""], ["Bauer", "Stefan", ""], ["Schmidhuber", "J\u00fcrgen", ""], ["Buhmann", "Joachim M.", ""]]}, {"id": "2103.08878", "submitter": "Gabriel Silva", "authors": "Vivek Kurien George, Vikash Morar, Weiwei Yang, Jonathan Larson, Bryan\n  Tower, Shweti Mahajan, Arkin Gupta, Christopher White, Gabriel A. Silva", "title": "Learning without gradient descent encoded by the dynamics of a\n  neurobiological model", "comments": "Version 2 includes a new subsection 4.1 and associated table and\n  figure benchmarking our biologically-inspired neural network against a\n  traditional ANN", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The success of state-of-the-art machine learning is essentially all based on\ndifferent variations of gradient descent algorithms that minimize some version\nof a cost or loss function. A fundamental limitation, however, is the need to\ntrain these systems in either supervised or unsupervised ways by exposing them\nto typically large numbers of training examples. Here, we introduce a\nfundamentally novel conceptual approach to machine learning that takes\nadvantage of a neurobiologically derived model of dynamic signaling,\nconstrained by the geometric structure of a network. We show that MNIST images\ncan be uniquely encoded and classified by the dynamics of geometric networks\nwith nearly state-of-the-art accuracy in an unsupervised way, and without the\nneed for any training.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 07:03:04 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 20:55:19 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["George", "Vivek Kurien", ""], ["Morar", "Vikash", ""], ["Yang", "Weiwei", ""], ["Larson", "Jonathan", ""], ["Tower", "Bryan", ""], ["Mahajan", "Shweti", ""], ["Gupta", "Arkin", ""], ["White", "Christopher", ""], ["Silva", "Gabriel A.", ""]]}, {"id": "2103.08886", "submitter": "Haiqin Yang", "authors": "Zengfeng Zeng, Dan Ma, Haiqin Yang, Zhen Gou and Jianping Shen", "title": "Automatic Intent-Slot Induction for Dialogue Systems", "comments": "12 pages, 11 figures, 6 tables, in WWW'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Automatically and accurately identifying user intents and filling the\nassociated slots from their spoken language are critical to the success of\ndialogue systems. Traditional methods require manually defining the\nDOMAIN-INTENT-SLOT schema and asking many domain experts to annotate the\ncorresponding utterances, upon which neural models are trained. This procedure\nbrings the challenges of information sharing hindering, out-of-schema, or data\nsparsity in open-domain dialogue systems. To tackle these challenges, we\nexplore a new task of {\\em automatic intent-slot induction} and propose a novel\ndomain-independent tool. That is, we design a coarse-to-fine three-step\nprocedure including Role-labeling, Concept-mining, And Pattern-mining (RCAP):\n(1) role-labeling: extracting keyphrases from users' utterances and classifying\nthem into a quadruple of coarsely-defined intent-roles via sequence labeling;\n(2) concept-mining: clustering the extracted intent-role mentions and naming\nthem into abstract fine-grained concepts; (3) pattern-mining: applying the\nApriori algorithm to mine intent-role patterns and automatically inferring the\nintent-slot using these coarse-grained intent-role labels and fine-grained\nconcepts. Empirical evaluations on both real-world in-domain and out-of-domain\ndatasets show that: (1) our RCAP can generate satisfactory SLU schema and\noutperforms the state-of-the-art supervised learning method; (2) our RCAP can\nbe directly applied to out-of-domain datasets and gain at least 76\\%\nimprovement of F1-score on intent detection and 41\\% improvement of F1-score on\nslot filling; (3) our RCAP exhibits its power in generic intent-slot\nextractions with less manual effort, which opens pathways for schema induction\non new domains and unseen intent-slot discovery for generalizable dialogue\nsystems.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 07:21:31 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Zeng", "Zengfeng", ""], ["Ma", "Dan", ""], ["Yang", "Haiqin", ""], ["Gou", "Zhen", ""], ["Shen", "Jianping", ""]]}, {"id": "2103.08893", "submitter": "Haiqin Yang", "authors": "Yiying Yang, Xi Yin, Haiqin Yang, Xingjian Fei, Hao Peng, Kaijie Zhou,\n  Kunfeng Lai, and Jianping Shen", "title": "KGSynNet: A Novel Entity Synonyms Discovery Framework with Knowledge\n  Graph", "comments": "16 pages, 3 figures, 5 tables, in DASFAA'21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Entity synonyms discovery is crucial for entity-leveraging applications.\nHowever, existing studies suffer from several critical issues: (1) the input\nmentions may be out-of-vocabulary (OOV) and may come from a different semantic\nspace of the entities; (2) the connection between mentions and entities may be\nhidden and cannot be established by surface matching; and (3) some entities\nrarely appear due to the long-tail effect. To tackle these challenges, we\nfacilitate knowledge graphs and propose a novel entity synonyms discovery\nframework, named \\emph{KGSynNet}. Specifically, we pre-train subword embeddings\nfor mentions and entities using a large-scale domain-specific corpus while\nlearning the knowledge embeddings of entities via a joint TransC-TransE model.\nMore importantly, to obtain a comprehensive representation of entities, we\nemploy a specifically designed \\emph{fusion gate} to adaptively absorb the\nentities' knowledge information into their semantic features. We conduct\nextensive experiments to demonstrate the effectiveness of our \\emph{KGSynNet}\nin leveraging the knowledge graph. The experimental results show that the\n\\emph{KGSynNet} improves the state-of-the-art methods by 14.7\\% in terms of\nhits@3 in the offline evaluation and outperforms the BERT model by 8.3\\% in the\npositive feedback rate of an online A/B test on the entity linking module of a\nquestion answering system.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 07:32:33 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 08:41:06 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Yang", "Yiying", ""], ["Yin", "Xi", ""], ["Yang", "Haiqin", ""], ["Fei", "Xingjian", ""], ["Peng", "Hao", ""], ["Zhou", "Kaijie", ""], ["Lai", "Kunfeng", ""], ["Shen", "Jianping", ""]]}, {"id": "2103.08894", "submitter": "Medha Atre", "authors": "Medha Atre and Birendra Jha and Ashwini Rao", "title": "Distributed Deep Learning Using Volunteer Computing-Like Paradigm", "comments": null, "journal-ref": "ScaDL workshop at IEEE International Parallel and Distributed\n  Processing Symposium 2021", "doi": "10.1109/IPDPSW52791.2021.00144", "report-no": null, "categories": "cs.DC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Use of Deep Learning (DL) in commercial applications such as image\nclassification, sentiment analysis and speech recognition is increasing. When\ntraining DL models with large number of parameters and/or large datasets, cost\nand speed of training can become prohibitive. Distributed DL training solutions\nthat split a training job into subtasks and execute them over multiple nodes\ncan decrease training time. However, the cost of current solutions, built\npredominantly for cluster computing systems, can still be an issue. In contrast\nto cluster computing systems, Volunteer Computing (VC) systems can lower the\ncost of computing, but applications running on VC systems have to handle fault\ntolerance, variable network latency and heterogeneity of compute nodes, and the\ncurrent solutions are not designed to do so. We design a distributed solution\nthat can run DL training on a VC system by using a data parallel approach. We\nimplement a novel asynchronous SGD scheme called VC-ASGD suited for VC systems.\nIn contrast to traditional VC systems that lower cost by using untrustworthy\nvolunteer devices, we lower cost by leveraging preemptible computing instances\non commercial cloud platforms. By using preemptible instances that require\napplications to be fault tolerant, we lower cost by 70-90% and improve data\nsecurity.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 07:32:58 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 12:50:05 GMT"}, {"version": "v3", "created": "Thu, 27 May 2021 06:41:45 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Atre", "Medha", ""], ["Jha", "Birendra", ""], ["Rao", "Ashwini", ""]]}, {"id": "2103.08971", "submitter": "Tsing Zhang", "authors": "Jianqing Zhang (1), Dongjing Wang (1), Dongjin Yu (1) ((1) School of\n  Computer Science and Technology, Hangzhou Dianzi University, China)", "title": "TLSAN: Time-aware Long- and Short-term Attention Network for Next-item\n  Recommendation", "comments": null, "journal-ref": "Neurocomputing, Volume 441, 21 June 2021, Pages 179-191", "doi": "10.1016/j.neucom.2021.02.015", "report-no": null, "categories": "cs.IR cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recently, deep neural networks are widely applied in recommender systems for\ntheir effectiveness in capturing/modeling users' preferences. Especially, the\nattention mechanism in deep learning enables recommender systems to incorporate\nvarious features in an adaptive way. Specifically, as for the next item\nrecommendation task, we have the following three observations: 1) users'\nsequential behavior records aggregate at time positions (\"time-aggregation\"),\n2) users have personalized taste that is related to the \"time-aggregation\"\nphenomenon (\"personalized time-aggregation\"), and 3) users' short-term\ninterests play an important role in the next item prediction/recommendation. In\nthis paper, we propose a new Time-aware Long- and Short-term Attention Network\n(TLSAN) to address those observations mentioned above. Specifically, TLSAN\nconsists of two main components. Firstly, TLSAN models \"personalized\ntime-aggregation\" and learn user-specific temporal taste via trainable\npersonalized time position embeddings with category-aware correlations in\nlong-term behaviors. Secondly, long- and short-term feature-wise attention\nlayers are proposed to effectively capture users' long- and short-term\npreferences for accurate recommendation. Especially, the attention mechanism\nenables TLSAN to utilize users' preferences in an adaptive way, and its usage\nin long- and short-term layers enhances TLSAN's ability of dealing with sparse\ninteraction data. Extensive experiments are conducted on Amazon datasets from\ndifferent fields (also with different size), and the results show that TLSAN\noutperforms state-of-the-art baselines in both capturing users' preferences and\nperforming time-sensitive next-item recommendation.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 10:51:57 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Zhang", "Jianqing", ""], ["Wang", "Dongjing", ""], ["Yu", "Dongjin", ""]]}, {"id": "2103.08976", "submitter": "BaiRan Fu", "authors": "Bairan Fu and Wenming Zhang and Guangneng Hu and Xinyu Dai and Shujian\n  Huang and Jiajun Chen", "title": "Dual Side Deep Context-aware Modulation for Social Recommendation", "comments": "Accepted by WWW2021 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social recommendation is effective in improving the recommendation\nperformance by leveraging social relations from online social networking\nplatforms. Social relations among users provide friends' information for\nmodeling users' interest in candidate items and help items expose to potential\nconsumers (i.e., item attraction). However, there are two issues haven't been\nwell-studied: Firstly, for the user interests, existing methods typically\naggregate friends' information contextualized on the candidate item only, and\nthis shallow context-aware aggregation makes them suffer from the limited\nfriends' information. Secondly, for the item attraction, if the item's past\nconsumers are the friends of or have a similar consumption habit to the\ntargeted user, the item may be more attractive to the targeted user, but most\nexisting methods neglect the relation enhanced context-aware item attraction.\nTo address the above issues, we proposed DICER (Dual Side Deep Context-aware\nModulation for SocialRecommendation). Specifically, we first proposed a novel\ngraph neural network to model the social relation and collaborative relation,\nand on top of high-order relations, a dual side deep context-aware modulation\nis introduced to capture the friends' information and item attraction.\nEmpirical results on two real-world datasets show the effectiveness of the\nproposed model and further experiments are conducted to help understand how the\ndual context-aware modulation works.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 11:08:30 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Fu", "Bairan", ""], ["Zhang", "Wenming", ""], ["Hu", "Guangneng", ""], ["Dai", "Xinyu", ""], ["Huang", "Shujian", ""], ["Chen", "Jiajun", ""]]}, {"id": "2103.09031", "submitter": "Yuval Shahar", "authors": "Avner Hatsek, Irit Hochberg, Deeb Daoud Naccache, Aya Biderman, and\n  Yuval Shahar", "title": "Evaluation of a Bi-Directional Methodology for Automated Assessment of\n  Compliance to Continuous Application of Clinical Guidelines, in the Type 2\n  Diabetes-Management Domain", "comments": "25 pages; 4 figures, 6 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We evaluated the DiscovErr system, in which we had previously implemented a\nnew methodology for assessment of compliance to continuous application of\nevidence-based clinical guidelines, based on a bidirectional search from the\nguideline objectives to the patient's longitudinal data, and vice versa. We\ncompared the system comments on 1584 transactions regarding the management,\nover a mean of 5.23 years, of 10 randomly selected Type 2 diabetes patients, to\nthose of two diabetes experts and a senior family practitioner. After providing\ntheir own comments, the experts assessed both the correctness (precision) and\nthe importance of each of the DiscovErr system comments. The completeness\n(recall or coverage) of the system was computed by comparing its comments to\nthose made by the experts. The system made 279 comments. The experts made 181\nunique comments. The completeness of the system was 91% compared to comments\nmade by at least two experts, and 98% when compared to comments made by all\nthree. 172 comments were evaluated by the experts for correctness and\nimportance: All 114 medication-related comments, and a random 35% of the 165\nmonitoring-related comments. The system's correctness was 81% compared to\ncomments judged as correct by both diabetes experts, and 91% compared to\ncomments judged as correct by a diabetes expert and at least as partially\ncorrect by the other. 89% of the comments were judged as important by both\ndiabetes experts, 8% were judged as important by one expert, 3% were judged as\nless important by both experts. The completeness scores of the three experts\n(compared to the comments of all experts plus the validated system comments)\nwere 75%, 60%, and 55%; the experts' correctness scores (compared to their\nmajority) were respectively 99%, 91%, and 88%. Conclusion: Systems such as\nDiscovErr can assess the quality of continuous guideline-based care.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 13:02:07 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Hatsek", "Avner", ""], ["Hochberg", "Irit", ""], ["Naccache", "Deeb Daoud", ""], ["Biderman", "Aya", ""], ["Shahar", "Yuval", ""]]}, {"id": "2103.09051", "submitter": "Markus Borg", "authors": "Markus Borg, Joshua Bronson, Linus Christensson, Fredrik Olsson, Olof\n  Lennartsson, Elias Sonnsj\\\"o, Hamid Ebabi, Martin Karsberg", "title": "Exploring the Assessment List for Trustworthy AI in the Context of\n  Advanced Driver-Assistance Systems", "comments": "Accepted for publication in the Proc. of the 2nd Workshop on Ethics\n  in Software Engineering Research and Practice", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Intelligence (AI) is increasingly used in critical applications.\nThus, the need for dependable AI systems is rapidly growing. In 2018, the\nEuropean Commission appointed experts to a High-Level Expert Group on AI\n(AI-HLEG). AI-HLEG defined Trustworthy AI as 1) lawful, 2) ethical, and 3)\nrobust and specified seven corresponding key requirements. To help development\norganizations, AI-HLEG recently published the Assessment List for Trustworthy\nAI (ALTAI). We present an illustrative case study from applying ALTAI to an\nongoing development project of an Advanced Driver-Assistance System (ADAS) that\nrelies on Machine Learning (ML). Our experience shows that ALTAI is largely\napplicable to ADAS development, but specific parts related to human agency and\ntransparency can be disregarded. Moreover, bigger questions related to societal\nand environmental impact cannot be tackled by an ADAS supplier in isolation. We\npresent how we plan to develop the ADAS to ensure ALTAI-compliance. Finally, we\nprovide three recommendations for the next revision of ALTAI, i.e., life-cycle\nvariants, domain-specific adaptations, and removed redundancy.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 21:48:11 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Borg", "Markus", ""], ["Bronson", "Joshua", ""], ["Christensson", "Linus", ""], ["Olsson", "Fredrik", ""], ["Lennartsson", "Olof", ""], ["Sonnsj\u00f6", "Elias", ""], ["Ebabi", "Hamid", ""], ["Karsberg", "Martin", ""]]}, {"id": "2103.09071", "submitter": "Akira Taniguchi", "authors": "Yuki Katsumata, Akinori Kanechika, Akira Taniguchi, Lotfi El Hafi,\n  Yoshinobu Hagiwara, Tadahiro Taniguchi", "title": "Map completion from partial observation using the global structure of\n  multiple environmental maps", "comments": "Submitted to Advanced Robotics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using the spatial structure of various indoor environments as prior\nknowledge, the robot would construct the map more efficiently. Autonomous\nmobile robots generally apply simultaneous localization and mapping (SLAM)\nmethods to understand the reachable area in newly visited environments.\nHowever, conventional mapping approaches are limited by only considering sensor\nobservation and control signals to estimate the current environment map. This\npaper proposes a novel SLAM method, map completion network-based SLAM\n(MCN-SLAM), based on a probabilistic generative model incorporating deep neural\nnetworks for map completion. These map completion networks are primarily\ntrained in the framework of generative adversarial networks (GANs) to extract\nthe global structure of large amounts of existing map data. We show in\nexperiments that the proposed method can estimate the environment map 1.3 times\nbetter than the previous SLAM methods in the situation of partial observation.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 13:48:37 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Katsumata", "Yuki", ""], ["Kanechika", "Akinori", ""], ["Taniguchi", "Akira", ""], ["Hafi", "Lotfi El", ""], ["Hagiwara", "Yoshinobu", ""], ["Taniguchi", "Tadahiro", ""]]}, {"id": "2103.09072", "submitter": "Giulia Belgiovine", "authors": "Jonas Gonzalez-Billandon, Giulia Belgiovine, Alessandra Sciutti,\n  Giulio Sandini, Francesco Rea", "title": "Cognitive architecture aided by working-memory for self-supervised\n  multi-modal humans recognition", "comments": "Submitted to the IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to recognize human partners is an important social skill to build\npersonalized and long-term human-robot interactions, especially in scenarios\nlike education, care-giving, and rehabilitation. Faces and voices constitute\ntwo important sources of information to enable artificial systems to reliably\nrecognize individuals. Deep learning networks have achieved state-of-the-art\nresults and demonstrated to be suitable tools to address such a task. However,\nwhen those networks are applied to different and unprecedented scenarios not\nincluded in the training set, they can suffer a drop in performance. For\nexample, with robotic platforms in ever-changing and realistic environments,\nwhere always new sensory evidence is acquired, the performance of those models\ndegrades. One solution is to make robots learn from their first-hand sensory\ndata with self-supervision. This allows coping with the inherent variability of\nthe data gathered in realistic and interactive contexts. To this aim, we\npropose a cognitive architecture integrating low-level perceptual processes\nwith a spatial working memory mechanism. The architecture autonomously\norganizes the robot's sensory experience into a structured dataset suitable for\nhuman recognition. Our results demonstrate the effectiveness of our\narchitecture and show that it is a promising solution in the quest of making\nrobots more autonomous in their learning process.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 13:50:24 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Gonzalez-Billandon", "Jonas", ""], ["Belgiovine", "Giulia", ""], ["Sciutti", "Alessandra", ""], ["Sandini", "Giulio", ""], ["Rea", "Francesco", ""]]}, {"id": "2103.09098", "submitter": "Yusen Lin", "authors": "Yusen Lin, Jinming Xue, Louiqa Raschid", "title": "Predicting the Behavior of Dealers in Over-The-Counter Corporate Bond\n  Markets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.ST cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Trading in Over-The-Counter (OTC) markets is facilitated by broker-dealers,\nin comparison to public exchanges, e.g., the New York Stock Exchange (NYSE).\nDealers play an important role in stabilizing prices and providing liquidity in\nOTC markets. We apply machine learning methods to model and predict the trading\nbehavior of OTC dealers for US corporate bonds. We create sequences of daily\nhistorical transaction reports for each dealer over a vocabulary of US\ncorporate bonds. Using this history of dealer activity, we predict the future\ntrading decisions of the dealer. We consider a range of neural network-based\nprediction models. We propose an extension, the Pointwise-Product ReZero (PPRZ)\nTransformer model, and demonstrate the improved performance of our model. We\nshow that individual history provides the best predictive model for the most\nactive dealers. For less active dealers, a collective model provides improved\nperformance. Further, clustering dealers based on their similarity can improve\nperformance. Finally, prediction accuracy varies based on the activity level of\nboth the bond and the dealer.\n", "versions": [{"version": "v1", "created": "Fri, 12 Mar 2021 04:22:07 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Lin", "Yusen", ""], ["Xue", "Jinming", ""], ["Raschid", "Louiqa", ""]]}, {"id": "2103.09118", "submitter": "Joseph Robinson", "authors": "Joseph P Robinson and Can Qin and Yann Henon and Samson Timoner and\n  Yun Fu", "title": "Balancing Biases and Preserving Privacy on Balanced Faces in the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are demographic biases in current models used for facial recognition\n(FR). Our Balanced Faces In the Wild (BFW) dataset serves as a proxy to measure\nbias across ethnicity and gender subgroups, allowing one to characterize FR\nperformances per subgroup. We show performances are non-optimal when a single\nscore threshold is used to determine whether sample pairs are genuine or\nimposter. Across subgroups, performance ratings vary from the reported across\nthe entire dataset. Thus, claims of specific error rates only hold true for\npopulations matching that of the validation data. We mitigate the imbalanced\nperformances using a novel domain adaptation learning scheme on the facial\nfeatures extracted using state-of-the-art. Not only does this technique balance\nperformance, but it also boosts the overall performance. A benefit of the\nproposed is to preserve identity information in facial features while removing\ndemographic knowledge in the lower dimensional features. The removal of\ndemographic knowledge prevents future potential biases from being injected into\ndecision-making. This removal satisfies privacy concerns. We explore why this\nworks qualitatively; we also show quantitatively that subgroup classifiers can\nno longer learn from the features mapped by the proposed.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 15:05:49 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 12:21:18 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Robinson", "Joseph P", ""], ["Qin", "Can", ""], ["Henon", "Yann", ""], ["Timoner", "Samson", ""], ["Fu", "Yun", ""]]}, {"id": "2103.09159", "submitter": "Yaodong Yang Mr.", "authors": "David Mguni, Jianhong Wang, Taher Jafferjee, Nicolas Perez-Nieves,\n  Wenbin Song, Yaodong Yang, Feifei Tong, Hui Chen, Jiangcheng Zhu, Jun Wang", "title": "Learning to Shape Rewards using a Game of Switching Controls", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reward shaping (RS) is a powerful method in reinforcement learning (RL) for\novercoming the problem of sparse or uninformative rewards. However, RS\ntypically relies on manually engineered shaping-reward functions whose\nconstruction is time-consuming and error-prone. It also requires domain\nknowledge which runs contrary to the goal of autonomous learning. We introduce\nReinforcement Learning Optimal Shaping Algorithm (ROSA), an automated RS\nframework in which the shaping-reward function is constructed in a novel Markov\ngame between two agents. A reward-shaping agent (Shaper) uses switching\ncontrols to determine which states to add shaping rewards and their optimal\nvalues while the other agent (Controller) learns the optimal policy for the\ntask using these shaped rewards. We prove that ROSA, which easily adopts\nexisting RL algorithms, learns to construct a shaping-reward function that is\ntailored to the task thus ensuring efficient convergence to high performance\npolicies. We demonstrate ROSA's congenial properties in three carefully\ndesigned experiments and show its superior performance against state-of-the-art\nRS algorithms in challenging sparse reward environments.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 15:56:57 GMT"}, {"version": "v2", "created": "Wed, 16 Jun 2021 18:32:39 GMT"}], "update_date": "2021-06-18", "authors_parsed": [["Mguni", "David", ""], ["Wang", "Jianhong", ""], ["Jafferjee", "Taher", ""], ["Perez-Nieves", "Nicolas", ""], ["Song", "Wenbin", ""], ["Yang", "Yaodong", ""], ["Tong", "Feifei", ""], ["Chen", "Hui", ""], ["Zhu", "Jiangcheng", ""], ["Wang", "Jun", ""]]}, {"id": "2103.09160", "submitter": "Jingdao Chen", "authors": "Jingdao Chen, Zsolt Kira, and Yong K. Cho", "title": "LRGNet: Learnable Region Growing for Class-Agnostic Point Cloud\n  Segmentation", "comments": null, "journal-ref": "IEEE Robotics and Automation Letters 2021", "doi": "10.1109/LRA.2021.3062607", "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  3D point cloud segmentation is an important function that helps robots\nunderstand the layout of their surrounding environment and perform tasks such\nas grasping objects, avoiding obstacles, and finding landmarks. Current\nsegmentation methods are mostly class-specific, many of which are tuned to work\nwith specific object categories and may not be generalizable to different types\nof scenes. This research proposes a learnable region growing method for\nclass-agnostic point cloud segmentation, specifically for the task of instance\nlabel prediction. The proposed method is able to segment any class of objects\nusing a single deep neural network without any assumptions about their shapes\nand sizes. The deep neural network is trained to predict how to add or remove\npoints from a point cloud region to morph it into incrementally more complete\nregions of an object instance. Segmentation results on the S3DIS and ScanNet\ndatasets show that the proposed method outperforms competing methods by 1%-9%\non 6 different evaluation metrics.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 15:58:01 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Chen", "Jingdao", ""], ["Kira", "Zsolt", ""], ["Cho", "Yong K.", ""]]}, {"id": "2103.09173", "submitter": "Chang Liu", "authors": "Chang Liu, Lixin Fan, Kam Woh Ng, Yilun Jin, Ce Ju, Tianyu Zhang, Chee\n  Seng Chan, Qiang Yang", "title": "Ternary Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a novel ternary hash encoding for learning to hash\nmethods, which provides a principled more efficient coding scheme with\nperformances better than those of the state-of-the-art binary hashing\ncounterparts. Two kinds of axiomatic ternary logic, Kleene logic and\n{\\L}ukasiewicz logic are adopted to calculate the Ternary Hamming Distance\n(THD) for both the learning/encoding and testing/querying phases. Our work\ndemonstrates that, with an efficient implementation of ternary logic on\nstandard binary machines, the proposed ternary hashing is compared favorably to\nthe binary hashing methods with consistent improvements of retrieval mean\naverage precision (mAP) ranging from 1\\% to 5.9\\% as shown in CIFAR10, NUS-WIDE\nand ImageNet100 datasets.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 16:20:54 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 12:39:32 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Liu", "Chang", ""], ["Fan", "Lixin", ""], ["Ng", "Kam Woh", ""], ["Jin", "Yilun", ""], ["Ju", "Ce", ""], ["Zhang", "Tianyu", ""], ["Chan", "Chee Seng", ""], ["Yang", "Qiang", ""]]}, {"id": "2103.09185", "submitter": "Hatem Haddad", "authors": "Aymen Ben Elhaj Mabrouk, Moez Ben Haj Hmida, Chayma Fourati, Hatem\n  Haddad, Abir Messaoudi", "title": "A Multilingual African Embedding for FAQ Chatbots", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Searching for an available, reliable, official, and understandable\ninformation is not a trivial task due to scattered information across the\ninternet, and the availability lack of governmental communication channels\ncommunicating with African dialects and languages. In this paper, we introduce\nan Artificial Intelligence Powered chatbot for crisis communication that would\nbe omnichannel, multilingual and multi dialectal. We present our work on\nmodified StarSpace embedding tailored for African dialects for the\nquestion-answering task along with the architecture of the proposed chatbot\nsystem and a description of the different layers. English, French, Arabic,\nTunisian, Igbo,Yor\\`ub\\'a, and Hausa are used as languages and dialects.\nQuantitative and qualitative evaluation results are obtained for our real\ndeployed Covid-19 chatbot. Results show that users are satisfied and the\nconversation with the chatbot is meeting customer needs.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 16:36:40 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Mabrouk", "Aymen Ben Elhaj", ""], ["Hmida", "Moez Ben Haj", ""], ["Fourati", "Chayma", ""], ["Haddad", "Hatem", ""], ["Messaoudi", "Abir", ""]]}, {"id": "2103.09189", "submitter": "Pranav Agarwal", "authors": "Pranav Agarwal, Pierre de Beaucorps and Raoul de Charette", "title": "Sparse Curriculum Reinforcement Learning for End-to-End Driving", "comments": "6 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep reinforcement Learning for end-to-end driving is limited by the need of\ncomplex reward engineering. Sparse rewards can circumvent this challenge but\nsuffers from long training time and leads to sub-optimal policy. In this work,\nwe explore driving using only goal conditioned sparse rewards and propose a\ncurriculum learning approach for end to end driving using only navigation view\nmaps that benefit from small virtual-to-real domain gap. To address the\ncomplexity of multiple driving policies, we learn concurrent individual\npolicies which are selected at inference by a navigation system. We demonstrate\nthe ability of our proposal to generalize on unseen road layout, and to drive\nlonger than in the training.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 16:39:09 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Agarwal", "Pranav", ""], ["de Beaucorps", "Pierre", ""], ["de Charette", "Raoul", ""]]}, {"id": "2103.09230", "submitter": "Harshit Sikchi", "authors": "Harshit Sikchi, Wenxuan Zhou, David Held", "title": "Lyapunov Barrier Policy Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deploying Reinforcement Learning (RL) agents in the real-world require that\nthe agents satisfy safety constraints. Current RL agents explore the\nenvironment without considering these constraints, which can lead to damage to\nthe hardware or even other agents in the environment. We propose a new method,\nLBPO, that uses a Lyapunov-based barrier function to restrict the policy update\nto a safe set for each training iteration. Our method also allows the user to\ncontrol the conservativeness of the agent with respect to the constraints in\nthe environment. LBPO significantly outperforms state-of-the-art baselines in\nterms of the number of constraint violations during training while being\ncompetitive in terms of performance. Further, our analysis reveals that\nbaselines like CPO and SDDPG rely mostly on backtracking to ensure safety\nrather than safe projection, which provides insight into why previous methods\nmight not have effectively limit the number of constraint violations.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 17:58:27 GMT"}], "update_date": "2021-03-17", "authors_parsed": [["Sikchi", "Harshit", ""], ["Zhou", "Wenxuan", ""], ["Held", "David", ""]]}, {"id": "2103.09265", "submitter": "Harshitha Machiraju", "authors": "Harshitha Machiraju, Oh-Hyeon Choung, Pascal Frossard, Michael. H\n  Herzog", "title": "Bio-inspired Robustness: A Review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Deep convolutional neural networks (DCNNs) have revolutionized computer\nvision and are often advocated as good models of the human visual system.\nHowever, there are currently many shortcomings of DCNNs, which preclude them as\na model of human vision. For example, in the case of adversarial attacks, where\nadding small amounts of noise to an image, including an object, can lead to\nstrong misclassification of that object. But for humans, the noise is often\ninvisible. If vulnerability to adversarial noise cannot be fixed, DCNNs cannot\nbe taken as serious models of human vision. Many studies have tried to add\nfeatures of the human visual system to DCNNs to make them robust against\nadversarial attacks. However, it is not fully clear whether human vision\ninspired components increase robustness because performance evaluations of\nthese novel components in DCNNs are often inconclusive. We propose a set of\ncriteria for proper evaluation and analyze different models according to these\ncriteria. We finally sketch future efforts to make DCCNs one step closer to the\nmodel of human vision.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 18:20:29 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Machiraju", "Harshitha", ""], ["Choung", "Oh-Hyeon", ""], ["Frossard", "Pascal", ""], ["Herzog", "Michael. H", ""]]}, {"id": "2103.09287", "submitter": "Swati Gupta", "authors": "Jad Salem, Swati Gupta, Vijay Kamble", "title": "Taming Wild Price Fluctuations: Monotone Stochastic Convex Optimization\n  with Bandit Feedback", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prices generated by automated price experimentation algorithms often display\nwild fluctuations, leading to unfavorable customer perceptions and violations\nof individual fairness: e.g., the price seen by a customer can be significantly\nhigher than what was seen by her predecessors, only to fall once again later.\nTo address this concern, we propose demand learning under a monotonicity\nconstraint on the sequence of prices, within the framework of stochastic convex\noptimization with bandit feedback.\n  Our main contribution is the design of the first sublinear-regret algorithms\nfor monotonic price experimentation for smooth and strongly concave revenue\nfunctions under noisy as well as noiseless bandit feedback. The monotonicity\nconstraint presents a unique challenge: since any increase (or decrease) in the\ndecision-levels is final, an algorithm needs to be cautious in its exploration\nto avoid over-shooting the optimum. At the same time, minimizing regret\nrequires that progress be made towards the optimum at a sufficient pace.\nBalancing these two goals is particularly challenging under noisy feedback,\nwhere obtaining sufficiently accurate gradient estimates is expensive. Our key\ninnovation is to utilize conservative gradient estimates to adaptively tailor\nthe degree of caution to local gradient information, being aggressive far from\nthe optimum and being increasingly cautious as the prices approach the optimum.\nImportantly, we show that our algorithms guarantee the same regret rates (up to\nlogarithmic factors) as the best achievable rates of regret without the\nmonotonicity requirement.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 19:06:28 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Salem", "Jad", ""], ["Gupta", "Swati", ""], ["Kamble", "Vijay", ""]]}, {"id": "2103.09311", "submitter": "Arash Shaban-Nejad", "authors": "Nariman Ammar, James E Bailey, Robert L Davis, Arash Shaban-Nejad", "title": "Using a Personal Health Library-Enabled mHealth Recommender System for\n  Self-Management of Diabetes Among Underserved Populations: Use Case for\n  Knowledge Graphs and Linked Data", "comments": "21 Pages, 13 Figures", "journal-ref": "JMIR Form Res. 2021 March 16;5(3):e24738", "doi": "10.2196/24738", "report-no": null, "categories": "cs.AI cs.DL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Personal health libraries (PHLs) provide a single point of secure access to\npatients digital health data and enable the integration of knowledge stored in\ntheir digital health profiles with other sources of global knowledge. PHLs can\nhelp empower caregivers and health care providers to make informed decisions\nabout patients health by understanding medical events in the context of their\nlives. This paper reports the implementation of a mobile health digital\nintervention that incorporates both digital health data stored in patients PHLs\nand other sources of contextual knowledge to deliver tailored recommendations\nfor improving self-care behaviors in diabetic adults. We conducted a thematic\nassessment of patient functional and nonfunctional requirements that are\nmissing from current EHRs based on evidence from the literature. We used the\nresults to identify the technologies needed to address those requirements. We\ndescribe the technological infrastructures used to construct, manage, and\nintegrate the types of knowledge stored in the PHL. We leverage the Social\nLinked Data (Solid) platform to design a fully decentralized and privacy-aware\nplatform that supports interoperability and care integration. We provided an\ninitial prototype design of a PHL and drafted a use case scenario that involves\nfour actors to demonstrate how the proposed prototype can be used to address\nuser requirements, including the construction and management of the PHL and its\nutilization for developing a mobile app that queries the knowledge stored and\nintegrated into the PHL in a private and fully decentralized manner to provide\nbetter recommendations. The proposed PHL helps patients and their caregivers\ntake a central role in making decisions regarding their health and equips their\nhealth care providers with informatics tools that support the collection and\ninterpretation of the collected knowledge.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 20:43:17 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Ammar", "Nariman", ""], ["Bailey", "James E", ""], ["Davis", "Robert L", ""], ["Shaban-Nejad", "Arash", ""]]}, {"id": "2103.09354", "submitter": "Denis Dimitrov", "authors": "Mark Potanin, Denis Dimitrov, Alex Shonenkov, Vladimir Bataev, Denis\n  Karachev and Maxim Novopoltsev", "title": "Digital Peter: Dataset, Competition and Handwriting Recognition Methods", "comments": "17 pages, 7 figures, submitted to ICDAR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a new dataset of Peter the Great's manuscripts and\ndescribes a segmentation procedure that converts initial images of documents\ninto the lines. The new dataset may be useful for researchers to train\nhandwriting text recognition models as a benchmark for comparing different\nmodels. It consists of 9 694 images and text files corresponding to lines in\nhistorical documents. The open machine learning competition Digital Peter was\nheld based on the considered dataset. The baseline solution for this\ncompetition as well as more advanced methods on handwritten text recognition\nare described in the article. Full dataset and all code are publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 22:37:22 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Potanin", "Mark", ""], ["Dimitrov", "Denis", ""], ["Shonenkov", "Alex", ""], ["Bataev", "Vladimir", ""], ["Karachev", "Denis", ""], ["Novopoltsev", "Maxim", ""]]}, {"id": "2103.09360", "submitter": "Ashesh Chattopadhyay", "authors": "Ashesh Chattopadhyay, Mustafa Mustafa, Pedram Hassanzadeh, Eviatar\n  Bach, Karthik Kashinath", "title": "Towards physically consistent data-driven weather forecasting:\n  Integrating data assimilation with equivariance-preserving deep spatial\n  transformers", "comments": "Under review in Geoscientific Model Development", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.ao-ph cs.AI cs.LG physics.comp-ph", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There is growing interest in data-driven weather prediction (DDWP), for\nexample using convolutional neural networks such as U-NETs that are trained on\ndata from models or reanalysis. Here, we propose 3 components to integrate with\ncommonly used DDWP models in order to improve their physical consistency and\nforecast accuracy. These components are 1) a deep spatial transformer added to\nthe latent space of the U-NETs to preserve a property called equivariance,\nwhich is related to correctly capturing rotations and scalings of features in\nspatio-temporal data, 2) a data-assimilation (DA) algorithm to ingest noisy\nobservations and improve the initial conditions for next forecasts, and 3) a\nmulti-time-step algorithm, which combines forecasts from DDWP models with\ndifferent time steps through DA, improving the accuracy of forecasts at short\nintervals. To show the benefit/feasibility of each component, we use\ngeopotential height at 500~hPa (Z500) from ERA5 reanalysis and examine the\nshort-term forecast accuracy of specific setups of the DDWP framework. Results\nshow that the equivariance-preserving networks (U-STNs) clearly outperform the\nU-NETs, for example improving the forecast skill by $45\\%$. Using a sigma-point\nensemble Kalman (SPEnKF) algorithm for DA and U-STN as the forward model, we\nshow that stable, accurate DA cycles are achieved even with high observation\nnoise. The DDWP+DA framework substantially benefits from large ($O(1000)$)\nensembles that are inexpensively generated with the data-driven forward model\nin each DA cycle. The multi-time-step DDWP+DA framework also shows promises,\ne.g., it reduces the average error by factors of 2-3.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 23:15:00 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Chattopadhyay", "Ashesh", ""], ["Mustafa", "Mustafa", ""], ["Hassanzadeh", "Pedram", ""], ["Bach", "Eviatar", ""], ["Kashinath", "Karthik", ""]]}, {"id": "2103.09382", "submitter": "Chuang Niu", "authors": "Chuang Niu and Ge Wang", "title": "SPICE: Semantic Pseudo-labeling for Image Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents SPICE, a Semantic Pseudo-labeling framework for Image\nClustEring. Instead of using indirect loss functions required by the recently\nproposed methods, SPICE generates pseudo-labels via self-learning and directly\nuses the pseudo-label-based classification loss to train a deep clustering\nnetwork. The basic idea of SPICE is to synergize the discrepancy among semantic\nclusters, the similarity among instance samples, and the semantic consistency\nof local samples in an embedding space to optimize the clustering network in a\nsemantically-driven paradigm. Specifically, a semantic-similarity-based\npseudo-labeling algorithm is first proposed to train a clustering network\nthrough unsupervised representation learning. Given the initial clustering\nresults, a local semantic consistency principle is used to select a set of\nreliably labeled samples, and a semi-pseudo-labeling algorithm is adapted for\nperformance boosting. Extensive experiments demonstrate that SPICE clearly\noutperforms the state-of-the-art methods on six common benchmark datasets\nincluding STL10, Cifar10, Cifar100-20, ImageNet-10, ImageNet-Dog, and\nTiny-ImageNet. On average, our SPICE method improves the current best results\nby about 10% in terms of adjusted rand index, normalized mutual information,\nand clustering accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 00:52:27 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Niu", "Chuang", ""], ["Wang", "Ge", ""]]}, {"id": "2103.09396", "submitter": "Ali Borji", "authors": "Ali Borji", "title": "Pros and Cons of GAN Evaluation Measures: New Developments", "comments": "NA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This work is an update of a previous paper on the same topic published a few\nyears ago. With the dramatic progress in generative modeling, a suite of new\nquantitative and qualitative techniques to evaluate models has emerged.\nAlthough some measures such as Inception Score, Frechet Inception Distance,\nPrecision-Recall, and Perceptual Path Length are relatively more popular, GAN\nevaluation is not a settled issue and there is still room for improvement.\nHere, I describe new dimensions that are becoming important in assessing models\n(e.g. bias and fairness) and discuss the connection between GAN evaluation and\ndeepfakes. These are important areas of concern in the machine learning\ncommunity today and progress in GAN evaluation can help mitigate them.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 01:48:34 GMT"}, {"version": "v2", "created": "Wed, 30 Jun 2021 07:51:09 GMT"}], "update_date": "2021-07-01", "authors_parsed": [["Borji", "Ali", ""]]}, {"id": "2103.09402", "submitter": "Kanata Suzuki", "authors": "Kanata Suzuki, Momomi Kanamura, Yuki Suga, Hiroki Mori, Tetsuya Ogata", "title": "In-air Knotting of Rope using Dual-Arm Robot based on Deep Learning", "comments": "Submitted to IROS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this study, we report the successful execution of in-air knotting of rope\nusing a dual-arm two-finger robot based on deep learning. Owing to its\nflexibility, the state of the rope was in constant flux during the operation of\nthe robot. This required the robot control system to dynamically correspond to\nthe state of the object at all times. However, a manual description of\nappropriate robot motions corresponding to all object states is difficult to be\nprepared in advance. To resolve this issue, we constructed a model that\ninstructed the robot to perform bowknots and overhand knots based on two deep\nneural networks trained using the data gathered from its sensorimotor,\nincluding visual and proximity sensors. The resultant model was verified to be\ncapable of predicting the appropriate robot motions based on the sensory\ninformation available online. In addition, we designed certain task motions\nbased on the Ian knot method using the dual-arm two-fingers robot. The designed\nknotting motions do not require a dedicated workbench or robot hand, thereby\nenhancing the versatility of the proposed method. Finally, experiments were\nperformed to estimate the knotting performance of the real robot while\nexecuting overhand knots and bowknots on rope and its success rate. The\nexperimental results established the effectiveness and high performance of the\nproposed method.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 02:11:58 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Suzuki", "Kanata", ""], ["Kanamura", "Momomi", ""], ["Suga", "Yuki", ""], ["Mori", "Hiroki", ""], ["Ogata", "Tetsuya", ""]]}, {"id": "2103.09419", "submitter": "Haoyu Liu", "authors": "Haoyu Liu, Fenglong Ma, Shibo He, Jiming Chen, Jing Gao", "title": "Fairness-aware Outlier Ensemble", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outlier ensemble methods have shown outstanding performance on the discovery\nof instances that are significantly different from the majority of the data.\nHowever, without the awareness of fairness, their applicability in the ethical\nscenarios, such as fraud detection and judiciary judgement system, could be\ndegraded. In this paper, we propose to reduce the bias of the outlier ensemble\nresults through a fairness-aware ensemble framework. Due to the lack of ground\ntruth in the outlier detection task, the key challenge is how to mitigate the\ndegradation in the detection performance with the improvement of fairness. To\naddress this challenge, we define a distance measure based on the output of\nconventional outlier ensemble techniques to estimate the possible cost\nassociated with detection performance degradation. Meanwhile, we propose a\npost-processing framework to tune the original ensemble results through a\nstacking process so that we can achieve a trade off between fairness and\ndetection performance. Detection performance is measured by the area under ROC\ncurve (AUC) while fairness is measured at both group and individual level.\nExperiments on eight public datasets are conducted. Results demonstrate the\neffectiveness of the proposed framework in improving fairness of outlier\nensemble results. We also analyze the trade-off between AUC and fairness.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 03:21:24 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Liu", "Haoyu", ""], ["Ma", "Fenglong", ""], ["He", "Shibo", ""], ["Chen", "Jiming", ""], ["Gao", "Jing", ""]]}, {"id": "2103.09439", "submitter": "Zhou Xian", "authors": "Zhou Xian, Shamit Lal, Hsiao-Yu Tung, Emmanouil Antonios Platanios,\n  Katerina Fragkiadaki", "title": "HyperDynamics: Meta-Learning Object and Agent Dynamics with\n  Hypernetworks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose HyperDynamics, a dynamics meta-learning framework that conditions\non an agent's interactions with the environment and optionally its visual\nobservations, and generates the parameters of neural dynamics models based on\ninferred properties of the dynamical system. Physical and visual properties of\nthe environment that are not part of the low-dimensional state yet affect its\ntemporal dynamics are inferred from the interaction history and visual\nobservations, and are implicitly captured in the generated parameters. We test\nHyperDynamics on a set of object pushing and locomotion tasks. It outperforms\nexisting dynamics models in the literature that adapt to environment variations\nby learning dynamics over high dimensional visual observations, capturing the\ninteractions of the agent in recurrent state representations, or using\ngradient-based meta-optimization. We also show our method matches the\nperformance of an ensemble of separately trained experts, while also being able\nto generalize well to unseen environment variations at test time. We attribute\nits good performance to the multiplicative interactions between the inferred\nsystem properties -- captured in the generated parameters -- and the\nlow-dimensional state representation of the dynamical system.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 04:48:43 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Xian", "Zhou", ""], ["Lal", "Shamit", ""], ["Tung", "Hsiao-Yu", ""], ["Platanios", "Emmanouil Antonios", ""], ["Fragkiadaki", "Katerina", ""]]}, {"id": "2103.09474", "submitter": "Keon Lee", "authors": "Keon Lee, Kyumin Park, Daeyoung Kim", "title": "STYLER: Style Factor Modeling with Rapidity and Robustness via Speech\n  Decomposition for Expressive and Controllable Neural Text to Speech", "comments": "5 pages, 2 figures, Accepted to Interspeech 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.AI cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous works on neural text-to-speech (TTS) have been addressed on limited\nspeed in training and inference time, robustness for difficult synthesis\nconditions, expressiveness, and controllability. Although several approaches\nresolve some limitations, there has been no attempt to solve all weaknesses at\nonce. In this paper, we propose STYLER, an expressive and controllable TTS\nframework with high-speed and robust synthesis. Our novel audio-text aligning\nmethod called Mel Calibrator and excluding autoregressive decoding enable rapid\ntraining and inference and robust synthesis on unseen data. Also, disentangled\nstyle factor modeling under supervision enlarges the controllability in\nsynthesizing process leading to expressive TTS. On top of it, a novel noise\nmodeling pipeline using domain adversarial training and Residual Decoding\nempowers noise-robust style transfer, decomposing the noise without any\nadditional label. Various experiments demonstrate that STYLER is more effective\nin speed and robustness than expressive TTS with autoregressive decoding and\nmore expressive and controllable than reading style non-autoregressive TTS.\nSynthesis samples and experiment results are provided via our demo page, and\ncode is available publicly.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 07:11:09 GMT"}, {"version": "v2", "created": "Sun, 28 Mar 2021 03:10:05 GMT"}, {"version": "v3", "created": "Sun, 4 Apr 2021 03:47:56 GMT"}, {"version": "v4", "created": "Fri, 25 Jun 2021 01:55:08 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Lee", "Keon", ""], ["Park", "Kyumin", ""], ["Kim", "Daeyoung", ""]]}, {"id": "2103.09488", "submitter": "Wenxin Yu", "authors": "Wenxin Yu, Bin Hu, Yucheng Hu, Tianxiang Lan, Yuanfan You, Dong Yin", "title": "Revisiting the Loss Weight Adjustment in Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  By definition, object detection requires a multi-task loss in order to solve\nclassification and regression tasks simultaneously. However, loss weight tends\nto be set manually in actuality. Therefore, a very practical problem that has\nnot been studied so far arises: how to quickly find the loss weight that fits\nthe current loss functions. In addition, when we choose different regression\nloss functions, whether the loss weight need to be adjusted and if so, how\nshould it be adjusted still is a problem demanding prompt solution. In this\npaper, through experiments and theoretical analysis of prediction box shifting,\nwe firstly find out three important conclusions about optimal loss weight\nallocation strategy, including (1) the classification loss curve decays faster\nthan regression loss curve; (2) loss weight is less than 1; (3) the gap between\nclassification and regression loss weight should not be too large. Then, based\non the above conclusions, we propose an Adaptive Loss Weight Adjustment(ALWA)\nto solve the above two problems by dynamically adjusting the loss weight in the\ntraining process, according to statistical characteristics of loss values. By\nincorporating ALWA into both one-stage and two-stage object detectors, we show\na consistent improvement on their performance using L1, SmoothL1 and CIoU loss,\nperformance measures on popular object detection benchmarks including PASCAL\nVOC and MS COCO. The code is available at https://github.com/ywx-hub/ALWA.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 07:45:06 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 05:48:04 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Yu", "Wenxin", ""], ["Hu", "Bin", ""], ["Hu", "Yucheng", ""], ["Lan", "Tianxiang", ""], ["You", "Yuanfan", ""], ["Yin", "Dong", ""]]}, {"id": "2103.09499", "submitter": "Hui Li", "authors": "Yanlin Wang, Hui Li", "title": "Code Completion by Modeling Flattened Abstract Syntax Trees as Graphs", "comments": "Accepted in AAAI 2021. This version contains the appendix for the\n  derivation of Eq. 12", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Code completion has become an essential component of integrated development\nenvironments. Contemporary code completion methods rely on the abstract syntax\ntree (AST) to generate syntactically correct code. However, they cannot fully\ncapture the sequential and repetitive patterns of writing code and the\nstructural information of the AST. To alleviate these problems, we propose a\nnew code completion approach named CCAG, which models the flattened sequence of\na partial AST as an AST graph. CCAG uses our proposed AST Graph Attention Block\nto capture different dependencies in the AST graph for representation learning\nin code completion. The sub-tasks of code completion are optimized via\nmulti-task learning in CCAG, and the task balance is automatically achieved\nusing uncertainty without the need to tune task weights. The experimental\nresults show that CCAG has superior performance than state-of-the-art\napproaches and it is able to provide intelligent code completion.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 08:11:09 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Wang", "Yanlin", ""], ["Li", "Hui", ""]]}, {"id": "2103.09548", "submitter": "Yang-Yin Lee", "authors": "Lee-Hsun Hsieh and Yang-Yin Lee and Ee-Peng Lim", "title": "ENCONTER: Entity Constrained Progressive Sequence Generation via\n  Insertion-based Transformer", "comments": "EACL 2021 long paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pretrained using large amount of data, autoregressive language models are\nable to generate high quality sequences. However, these models do not perform\nwell under hard lexical constraints as they lack fine control of content\ngeneration process. Progressive insertion-based transformers can overcome the\nabove limitation and efficiently generate a sequence in parallel given some\ninput tokens as constraint. These transformers however may fail to support hard\nlexical constraints as their generation process is more likely to terminate\nprematurely. The paper analyses such early termination problems and proposes\nthe Entity-constrained insertion transformer (ENCONTER), a new insertion\ntransformer that addresses the above pitfall without compromising much\ngeneration efficiency. We introduce a new training strategy that considers\npredefined hard lexical constraints (e.g., entities to be included in the\ngenerated sequence). Our experiments show that ENCONTER outperforms other\nbaseline models in several performance metrics rendering it more suitable in\npractical applications. Our code is available at\nhttps://github.com/LARC-CMU-SMU/Enconter\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 10:24:10 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Hsieh", "Lee-Hsun", ""], ["Lee", "Yang-Yin", ""], ["Lim", "Ee-Peng", ""]]}, {"id": "2103.09568", "submitter": "Roxana R\\u{a}dulescu", "authors": "Conor F. Hayes, Roxana R\\u{a}dulescu, Eugenio Bargiacchi, Johan\n  K\\\"allstr\\\"om, Matthew Macfarlane, Mathieu Reymond, Timothy Verstraeten,\n  Luisa M. Zintgraf, Richard Dazeley, Fredrik Heintz, Enda Howley, Athirai A.\n  Irissappane, Patrick Mannion, Ann Now\\'e, Gabriel Ramos, Marcello Restelli,\n  Peter Vamplew, Diederik M. Roijers", "title": "A Practical Guide to Multi-Objective Reinforcement Learning and Planning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world decision-making tasks are generally complex, requiring trade-offs\nbetween multiple, often conflicting, objectives. Despite this, the majority of\nresearch in reinforcement learning and decision-theoretic planning either\nassumes only a single objective, or that multiple objectives can be adequately\nhandled via a simple linear combination. Such approaches may oversimplify the\nunderlying problem and hence produce suboptimal results. This paper serves as a\nguide to the application of multi-objective methods to difficult problems, and\nis aimed at researchers who are already familiar with single-objective\nreinforcement learning and planning methods who wish to adopt a multi-objective\nperspective on their research, as well as practitioners who encounter\nmulti-objective decision problems in practice. It identifies the factors that\nmay influence the nature of the desired solution, and illustrates by example\nhow these influence the design of multi-objective decision-making systems for\ncomplex problems.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 11:07:28 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Hayes", "Conor F.", ""], ["R\u0103dulescu", "Roxana", ""], ["Bargiacchi", "Eugenio", ""], ["K\u00e4llstr\u00f6m", "Johan", ""], ["Macfarlane", "Matthew", ""], ["Reymond", "Mathieu", ""], ["Verstraeten", "Timothy", ""], ["Zintgraf", "Luisa M.", ""], ["Dazeley", "Richard", ""], ["Heintz", "Fredrik", ""], ["Howley", "Enda", ""], ["Irissappane", "Athirai A.", ""], ["Mannion", "Patrick", ""], ["Now\u00e9", "Ann", ""], ["Ramos", "Gabriel", ""], ["Restelli", "Marcello", ""], ["Vamplew", "Peter", ""], ["Roijers", "Diederik M.", ""]]}, {"id": "2103.09588", "submitter": "Houtan Ghaffari", "authors": "Houtan Ghaffari", "title": "An Efficient Method for the Classification of Croplands in Scarce-Label\n  Regions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Two of the main challenges for cropland classification by satellite\ntime-series images are insufficient ground-truth data and inaccessibility of\nhigh-quality hyperspectral images for under-developed areas. Unlabeled\nmedium-resolution satellite images are abundant, but how to benefit from them\nis an open question. We will show how to leverage their potential for cropland\nclassification using self-supervised tasks. Self-supervision is an approach\nwhere we provide simple training signals for the samples, which are apparent\nfrom the data's structure. Hence, they are cheap to acquire and explain a\nsimple concept about the data. We introduce three self-supervised tasks for\ncropland classification. They reduce epistemic uncertainty, and the resulting\nmodel shows superior accuracy in a wide range of settings compared to SVM and\nRandom Forest. Subsequently, we use the self-supervised tasks to perform\nunsupervised domain adaptation and benefit from the labeled samples in other\nregions. It is crucial to know what information to transfer to avoid degrading\nthe performance. We show how to automate the information selection and transfer\nprocess in cropland classification even when the source and target areas have a\nvery different feature distribution. We improved the model by about 24%\ncompared to a baseline architecture without any labeled sample in the target\ndomain. Our method is amenable to gradual improvement, works with\nmedium-resolution satellite images, and does not require complicated models.\nCode and data are available.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 12:10:11 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Ghaffari", "Houtan", ""]]}, {"id": "2103.09593", "submitter": "Samson Tan", "authors": "Samson Tan, Shafiq Joty", "title": "Code-Mixing on Sesame Street: Dawn of the Adversarial Polyglots", "comments": "To be presented at NAACL-HLT 2021. Abstract also published in the\n  Rising Stars Track of the Workshop on Computational Approaches to Linguistic\n  Code-Switching (CALCS 2021)", "journal-ref": "2021.naacl-main.282", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multilingual models have demonstrated impressive cross-lingual transfer\nperformance. However, test sets like XNLI are monolingual at the example level.\nIn multilingual communities, it is common for polyglots to code-mix when\nconversing with each other. Inspired by this phenomenon, we present two strong\nblack-box adversarial attacks (one word-level, one phrase-level) for\nmultilingual models that push their ability to handle code-mixed sentences to\nthe limit. The former uses bilingual dictionaries to propose perturbations and\ntranslations of the clean example for sense disambiguation. The latter directly\naligns the clean example with its translations before extracting phrases as\nperturbations. Our phrase-level attack has a success rate of 89.75% against\nXLM-R-large, bringing its average accuracy of 79.85 down to 8.18 on XNLI.\nFinally, we propose an efficient adversarial training scheme that trains in the\nsame number of steps as the original model and show that it improves model\naccuracy.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 12:20:53 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 09:30:27 GMT"}, {"version": "v3", "created": "Sat, 5 Jun 2021 02:02:07 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Tan", "Samson", ""], ["Joty", "Shafiq", ""]]}, {"id": "2103.09606", "submitter": "Youri Van Der Zee", "authors": "Youri van der Zee, Jan C. Scholtes, Marcel Westerhoud, Julien Rossi", "title": "Code Word Detection in Fraud Investigations using a Deep-Learning\n  Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In modern litigation, fraud investigators often face an overwhelming number\nof documents that must be reviewed throughout a matter. In the majority of\nlegal cases, fraud investigators do not know beforehand, exactly what they are\nlooking for, nor where to find it. In addition, fraudsters may use deception to\nhide their behaviour and intentions by using code words. Effectively, this\nmeans fraud investigators are looking for a needle in the haystack without\nknowing what the needle looks like.\n  As part of a larger research program, we use a framework to expedite the\ninvestigation process applying text-mining and machine learning techniques. We\nstructure this framework using three well-known methods in fraud\ninvestigations: (i) the fraud triangle (ii) the golden (\"W\") investigation\nquestions, and (iii) the analysis of competing hypotheses. With this framework,\nit is possible to automatically organize investigative data, so it is easier\nfor investigators to find answers to typical investigative questions.\n  In this research, we focus on one of the components of this framework: the\nidentification of the usage of code words by fraudsters. Here for, a novel\n(annotated) synthetic data set is created containing such code words, hidden in\nnormal email communication. Subsequently, a range of machine learning\ntechniques are employed to detect such code words. We show that the\nstate-of-the-art BERT model significantly outperforms other methods on this\ntask. With this result, we demonstrate that deep neural language models can\nreliably (F1 score of 0.9) be applied in fraud investigations for the detection\nof code words.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 12:49:55 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["van der Zee", "Youri", ""], ["Scholtes", "Jan C.", ""], ["Westerhoud", "Marcel", ""], ["Rossi", "Julien", ""]]}, {"id": "2103.09627", "submitter": "Keisuke Fujii", "authors": "Kosuke Toda, Masakiyo Teranishi, Keisuke Kushiro, Keisuke Fujii", "title": "Evaluation of soccer team defense based on prediction models of ball\n  recovery and being attacked", "comments": "12 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  With the development of measurement technology, data on the movements of\nactual games in various sports are available and are expected to be used for\nplanning and evaluating the tactics and strategy. In particular, defense in\nteam sports is generally difficult to be evaluated because of the lack of\nstatistical data. Conventional evaluation methods based on predictions of\nscores are considered unreliable and predict rare events throughout the entire\ngame, and it is difficult to evaluate various plays leading up to a score. On\nthe other hand, evaluation methods based on certain plays that lead to scoring\nand dominant regions are sometimes unsuitable to evaluate the performance\n(e.g., goals scored) of players and teams. In this study, we propose a method\nto evaluate team defense from a comprehensive perspective related to team\nperformance based on the prediction of ball recovery and being attacked, which\noccur more frequently than goals, using player actions and positional data of\nall players and the ball. Using data from 45 soccer matches, we examined the\nrelationship between the proposed index and team performance in actual matches\nand throughout a season. Results show that the proposed classifiers more\naccurately predicted the true events than the existing classifiers which were\nbased on rare events (i.e., goals). Also, the proposed index had a moderate\ncorrelation with the long-term outcomes of the season. These results suggest\nthat the proposed index might be a more reliable indicator rather than winning\nor losing with the inclusion of accidental factors.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 13:15:41 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 00:42:56 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Toda", "Kosuke", ""], ["Teranishi", "Masakiyo", ""], ["Kushiro", "Keisuke", ""], ["Fujii", "Keisuke", ""]]}, {"id": "2103.09656", "submitter": "Mateusz Jurewicz", "authors": "Mateusz Jurewicz, Leon Str{\\o}mberg-Derczynski", "title": "Set-to-Sequence Methods in Machine Learning: a Review", "comments": "46 pages of text, with 10 pages of references. Contains 2 tables and\n  4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine learning on sets towards sequential output is an important and\nubiquitous task, with applications ranging from language modelling and\nmeta-learning to multi-agent strategy games and power grid optimization.\nCombining elements of representation learning and structured prediction, its\ntwo primary challenges include obtaining a meaningful, permutation invariant\nset representation and subsequently utilizing this representation to output a\ncomplex target permutation. This paper provides a comprehensive introduction to\nthe field as well as an overview of important machine learning methods tackling\nboth of these key challenges, with a detailed qualitative comparison of\nselected model architectures.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 13:52:33 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Jurewicz", "Mateusz", ""], ["Str\u00f8mberg-Derczynski", "Leon", ""]]}, {"id": "2103.09662", "submitter": "Shashank Reddy Vadyala", "authors": "Shashank Reddy Vadyala, Sai Nethra Betgeri", "title": "Physics-Informed Neural Network Method for Solving One-Dimensional\n  Advection Equation Using PyTorch", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Numerical solutions to the equation for advection are determined using\ndifferent finite-difference approximations and physics-informed neural networks\n(PINNs) under conditions that allow an analytical solution. Their accuracy is\nexamined by comparing them to the analytical solution. We used a machine\nlearning framework like PyTorch to implement PINNs. PINNs approach allows\ntraining neural networks while respecting the PDEs as a strong constraint in\nthe optimization as apposed to making them part of the loss function. In\nstandard small-scale circulation simulations, it is shown that the conventional\napproach incorporates a pseudo diffusive effect that is almost as large as the\neffect of the turbulent diffusion model; hence the numerical solution is\nrendered inconsistent with the PDEs. This oscillation causes inaccuracy and\ncomputational uncertainty. Of all the schemes tested, only the PINNs\napproximation accurately predicted the outcome. We assume that the PINNs\napproach can transform the physics simulation area by allowing real-time\nphysics simulation and geometry optimization without costly and time-consuming\nsimulations on large supercomputers.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 05:39:17 GMT"}, {"version": "v2", "created": "Thu, 18 Mar 2021 04:50:59 GMT"}, {"version": "v3", "created": "Sun, 21 Mar 2021 04:50:39 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Vadyala", "Shashank Reddy", ""], ["Betgeri", "Sai Nethra", ""]]}, {"id": "2103.09704", "submitter": "Jiaye Li", "authors": "Shichao Zhang and Jiaye Li", "title": "Z Distance Function for KNN Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper proposes a new distance metric function, called Z distance, for\nKNN classification. The Z distance function is not a geometric direct-line\ndistance between two data points. It gives a consideration to the class\nattribute of a training dataset when measuring the affinity between data\npoints. Concretely speaking, the Z distance of two data points includes their\nclass center distance and real distance. And its shape looks like \"Z\". In this\nway, the affinity of two data points in the same class is always stronger than\nthat in different classes. Or, the intraclass data points are always closer\nthan those interclass data points. We evaluated the Z distance with\nexperiments, and demonstrated that the proposed distance function achieved\nbetter performance in KNN classification.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 15:01:17 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Zhang", "Shichao", ""], ["Li", "Jiaye", ""]]}, {"id": "2103.09712", "submitter": "Xiaojie Gao", "authors": "Xiaojie Gao, Yueming Jin, Yonghao Long, Qi Dou, Pheng-Ann Heng", "title": "Trans-SVNet: Accurate Phase Recognition from Surgical Videos via Hybrid\n  Embedding Aggregation Transformer", "comments": "MICCAI2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time surgical phase recognition is a fundamental task in modern\noperating rooms. Previous works tackle this task relying on architectures\narranged in spatio-temporal order, however, the supportive benefits of\nintermediate spatial features are not considered. In this paper, we introduce,\nfor the first time in surgical workflow analysis, Transformer to reconsider the\nignored complementary effects of spatial and temporal features for accurate\nsurgical phase recognition. Our hybrid embedding aggregation Transformer fuses\ncleverly designed spatial and temporal embeddings by allowing for active\nqueries based on spatial information from temporal embedding sequences. More\nimportantly, our framework processes the hybrid embeddings in parallel to\nachieve a high inference speed. Our method is thoroughly validated on two large\nsurgical video datasets, i.e., Cholec80 and M2CAI16 Challenge datasets, and\noutperforms the state-of-the-art approaches at a processing speed of 91 fps.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 15:12:55 GMT"}, {"version": "v2", "created": "Mon, 12 Jul 2021 12:18:39 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Gao", "Xiaojie", ""], ["Jin", "Yueming", ""], ["Long", "Yonghao", ""], ["Dou", "Qi", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "2103.09713", "submitter": "Boxiang Dong", "authors": "Boxiang Dong, Hui (Wendy) Wang, Aparna S. Varde, Dawei Li, Bharath K.\n  Samanthula, Weifeng Sun, Liang Zhao", "title": "Cyber Intrusion Detection by Using Deep Neural Networks with\n  Attack-sharing Loss", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cyber attacks pose crucial threats to computer system security, and put\ndigital treasuries at excessive risks. This leads to an urgent call for an\neffective intrusion detection system that can identify the intrusion attacks\nwith high accuracy. It is challenging to classify the intrusion events due to\nthe wide variety of attacks. Furthermore, in a normal network environment, a\nmajority of the connections are initiated by benign behaviors. The class\nimbalance issue in intrusion detection forces the classifier to be biased\ntoward the majority/benign class, thus leave many attack incidents undetected.\nSpurred by the success of deep neural networks in computer vision and natural\nlanguage processing, in this paper, we design a new system named DeepIDEA that\ntakes full advantage of deep learning to enable intrusion detection and\nclassification. To achieve high detection accuracy on imbalanced data, we\ndesign a novel attack-sharing loss function that can effectively move the\ndecision boundary towards the attack classes and eliminates the bias towards\nthe majority/benign class. By using this loss function, DeepIDEA respects the\nfact that the intrusion mis-classification should receive higher penalty than\nthe attack mis-classification. Extensive experimental results on three\nbenchmark datasets demonstrate the high detection accuracy of DeepIDEA. In\nparticular, compared with eight state-of-the-art approaches, DeepIDEA always\nprovides the best class-balanced accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 15:15:12 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Dong", "Boxiang", "", "Wendy"], ["Hui", "", "", "Wendy"], ["Wang", "", ""], ["Varde", "Aparna S.", ""], ["Li", "Dawei", ""], ["Samanthula", "Bharath K.", ""], ["Sun", "Weifeng", ""], ["Zhao", "Liang", ""]]}, {"id": "2103.09714", "submitter": "Boxiang Dong", "authors": "Boxiang Dong, Aparna S. Varde, Danilo Stevanovic, Jiayin Wang, Liang\n  Zhao", "title": "Interpretable Distance Metric Learning for Handwritten Chinese Character\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Handwriting recognition is of crucial importance to both Human Computer\nInteraction (HCI) and paperwork digitization. In the general field of Optical\nCharacter Recognition (OCR), handwritten Chinese character recognition faces\ntremendous challenges due to the enormously large character sets and the\namazing diversity of writing styles. Learning an appropriate distance metric to\nmeasure the difference between data inputs is the foundation of accurate\nhandwritten character recognition. Existing distance metric learning approaches\neither produce unacceptable error rates, or provide little interpretability in\nthe results. In this paper, we propose an interpretable distance metric\nlearning approach for handwritten Chinese character recognition. The learned\nmetric is a linear combination of intelligible base metrics, and thus provides\nmeaningful insights to ordinary users. Our experimental results on a benchmark\ndataset demonstrate the superior efficiency, accuracy and interpretability of\nour proposed approach.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 15:17:02 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Dong", "Boxiang", ""], ["Varde", "Aparna S.", ""], ["Stevanovic", "Danilo", ""], ["Wang", "Jiayin", ""], ["Zhao", "Liang", ""]]}, {"id": "2103.09720", "submitter": "Georgios Tziafas", "authors": "Giorgos Tziafas and Hamidreza Kasaei", "title": "Few-Shot Visual Grounding for Natural Human-Robot Interaction", "comments": "6 pages, 4 figures, ICARSC2021 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Natural Human-Robot Interaction (HRI) is one of the key components for\nservice robots to be able to work in human-centric environments. In such\ndynamic environments, the robot needs to understand the intention of the user\nto accomplish a task successfully. Towards addressing this point, we propose a\nsoftware architecture that segments a target object from a crowded scene,\nindicated verbally by a human user. At the core of our system, we employ a\nmulti-modal deep neural network for visual grounding. Unlike most grounding\nmethods that tackle the challenge using pre-trained object detectors via a\ntwo-stepped process, we develop a single stage zero-shot model that is able to\nprovide predictions in unseen data. We evaluate the performance of the proposed\nmodel on real RGB-D data collected from public scene datasets. Experimental\nresults showed that the proposed model performs well in terms of accuracy and\nspeed, while showcasing robustness to variation in the natural language input.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 15:24:02 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 14:13:29 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Tziafas", "Giorgos", ""], ["Kasaei", "Hamidreza", ""]]}, {"id": "2103.09742", "submitter": "Jongheon Jeong", "authors": "Jongheon Jeong and Jinwoo Shin", "title": "Training GANs with Stronger Augmentations via Contrastive Discriminator", "comments": "23 pages; ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent works in Generative Adversarial Networks (GANs) are actively\nrevisiting various data augmentation techniques as an effective way to prevent\ndiscriminator overfitting. It is still unclear, however, that which\naugmentations could actually improve GANs, and in particular, how to apply a\nwider range of augmentations in training. In this paper, we propose a novel way\nto address these questions by incorporating a recent contrastive representation\nlearning scheme into the GAN discriminator, coined ContraD. This \"fusion\"\nenables the discriminators to work with much stronger augmentations without\nincreasing their training instability, thereby preventing the discriminator\noverfitting issue in GANs more effectively. Even better, we observe that the\ncontrastive learning itself also benefits from our GAN training, i.e., by\nmaintaining discriminative features between real and fake samples, suggesting a\nstrong coherence between the two worlds: good contrastive representations are\nalso good for GAN discriminators, and vice versa. Our experimental results show\nthat GANs with ContraD consistently improve FID and IS compared to other recent\ntechniques incorporating data augmentations, still maintaining highly\ndiscriminative features in the discriminator in terms of the linear evaluation.\nFinally, as a byproduct, we also show that our GANs trained in an unsupervised\nmanner (without labels) can induce many conditional generative models via a\nsimple latent sampling, leveraging the learned features of ContraD. Code is\navailable at https://github.com/jh-jeong/ContraD.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 16:04:54 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Jeong", "Jongheon", ""], ["Shin", "Jinwoo", ""]]}, {"id": "2103.09756", "submitter": "Aldo Pacchiano", "authors": "Aldo Pacchiano, Jonathan Lee, Peter Bartlett, Ofir Nachum", "title": "Near Optimal Policy Optimization via REPS", "comments": "8 main pages, 37 total pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since its introduction a decade ago, \\emph{relative entropy policy search}\n(REPS) has demonstrated successful policy learning on a number of simulated and\nreal-world robotic domains, not to mention providing algorithmic components\nused by many recently proposed reinforcement learning (RL) algorithms. While\nREPS is commonly known in the community, there exist no guarantees on its\nperformance when using stochastic and gradient-based solvers. In this paper we\naim to fill this gap by providing guarantees and convergence rates for the\nsub-optimality of a policy learned using first-order optimization methods\napplied to the REPS objective. We first consider the setting in which we are\ngiven access to exact gradients and demonstrate how near-optimality of the\nobjective translates to near-optimality of the policy. We then consider the\npractical setting of stochastic gradients, and introduce a technique that uses\n\\emph{generative} access to the underlying Markov decision process to compute\nparameter updates that maintain favorable convergence to the optimal\nregularized policy.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 16:22:59 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Pacchiano", "Aldo", ""], ["Lee", "Jonathan", ""], ["Bartlett", "Peter", ""], ["Nachum", "Ofir", ""]]}, {"id": "2103.09783", "submitter": "Justus Bogner", "authors": "Justus Bogner, Roberto Verdecchia, Ilias Gerostathopoulos", "title": "Characterizing Technical Debt and Antipatterns in AI-Based Systems: A\n  Systematic Mapping Study", "comments": "Accepted at the 4th International Conference on Technical Debt\n  (TechDebt 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: With the rising popularity of Artificial Intelligence (AI), there\nis a growing need to build large and complex AI-based systems in a\ncost-effective and manageable way. Like with traditional software, Technical\nDebt (TD) will emerge naturally over time in these systems, therefore leading\nto challenges and risks if not managed appropriately. The influence of data\nscience and the stochastic nature of AI-based systems may also lead to new\ntypes of TD or antipatterns, which are not yet fully understood by researchers\nand practitioners. Objective: The goal of our study is to provide a clear\noverview and characterization of the types of TD (both established and new\nones) that appear in AI-based systems, as well as the antipatterns and related\nsolutions that have been proposed. Method: Following the process of a\nsystematic mapping study, 21 primary studies are identified and analyzed.\nResults: Our results show that (i) established TD types, variations of them,\nand four new TD types (data, model, configuration, and ethics debt) are present\nin AI-based systems, (ii) 72 antipatterns are discussed in the literature, the\nmajority related to data and model deficiencies, and (iii) 46 solutions have\nbeen proposed, either to address specific TD types, antipatterns, or TD in\ngeneral. Conclusions: Our results can support AI professionals with reasoning\nabout and communicating aspects of TD present in their systems. Additionally,\nthey can serve as a foundation for future research to further our understanding\nof TD in AI-based systems.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 17:11:43 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Bogner", "Justus", ""], ["Verdecchia", "Roberto", ""], ["Gerostathopoulos", "Ilias", ""]]}, {"id": "2103.09847", "submitter": "Lin Chen", "authors": "Lin Chen, Bruno Scherrer, Peter L. Bartlett", "title": "Infinite-Horizon Offline Reinforcement Learning with Linear Function\n  Approximation: Curse of Dimensionality and Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the sample complexity of policy evaluation in\ninfinite-horizon offline reinforcement learning (also known as the off-policy\nevaluation problem) with linear function approximation. We identify a hard\nregime $d\\gamma^{2}>1$, where $d$ is the dimension of the feature vector and\n$\\gamma$ is the discount rate. In this regime, for any $q\\in[\\gamma^{2},1]$, we\ncan construct a hard instance such that the smallest eigenvalue of its feature\ncovariance matrix is $q/d$ and it requires\n$\\Omega\\left(\\frac{d}{\\gamma^{2}\\left(q-\\gamma^{2}\\right)\\varepsilon^{2}}\\exp\\left(\\Theta\\left(d\\gamma^{2}\\right)\\right)\\right)$\nsamples to approximate the value function up to an additive error\n$\\varepsilon$. Note that the lower bound of the sample complexity is\nexponential in $d$. If $q=\\gamma^{2}$, even infinite data cannot suffice. Under\nthe low distribution shift assumption, we show that there is an algorithm that\nneeds at most $O\\left(\\max\\left\\{ \\frac{\\left\\Vert \\theta^{\\pi}\\right\\Vert\n_{2}^{4}}{\\varepsilon^{4}}\\log\\frac{d}{\\delta},\\frac{1}{\\varepsilon^{2}}\\left(d+\\log\\frac{1}{\\delta}\\right)\\right\\}\n\\right)$ samples ($\\theta^{\\pi}$ is the parameter of the policy in linear\nfunction approximation) and guarantees approximation to the value function up\nto an additive error of $\\varepsilon$ with probability at least $1-\\delta$.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 18:18:57 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Chen", "Lin", ""], ["Scherrer", "Bruno", ""], ["Bartlett", "Peter L.", ""]]}, {"id": "2103.09879", "submitter": "Andrew Carr", "authors": "Andrew N Carr, Quentin Berthet, Mathieu Blondel, Olivier Teboul, Neil\n  Zeghidour", "title": "Self-Supervised Learning of Audio Representations from Permutations with\n  Differentiable Ranking", "comments": null, "journal-ref": null, "doi": "10.1109/LSP.2021.3067635", "report-no": null, "categories": "cs.SD cs.AI eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Self-supervised pre-training using so-called \"pretext\" tasks has recently\nshown impressive performance across a wide range of modalities. In this work,\nwe advance self-supervised learning from permutations, by pre-training a model\nto reorder shuffled parts of the spectrogram of an audio signal, to improve\ndownstream classification performance. We make two main contributions. First,\nwe overcome the main challenges of integrating permutation inversions into an\nend-to-end training scheme, using recent advances in differentiable ranking.\nThis was heretofore sidestepped by casting the reordering task as\nclassification, fundamentally reducing the space of permutations that can be\nexploited. Our experiments validate that learning from all possible\npermutations improves the quality of the pre-trained representations over using\na limited, fixed set. Second, we show that inverting permutations is a\nmeaningful pretext task for learning audio representations in an unsupervised\nfashion. In particular, we improve instrument classification and pitch\nestimation of musical notes by reordering spectrogram patches in the\ntime-frequency space.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 19:36:04 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Carr", "Andrew N", ""], ["Berthet", "Quentin", ""], ["Blondel", "Mathieu", ""], ["Teboul", "Olivier", ""], ["Zeghidour", "Neil", ""]]}, {"id": "2103.09891", "submitter": "Matthew Inkawhich", "authors": "Matthew Inkawhich, Nathan Inkawhich, Eric Davis, Hai Li and Yiran Chen", "title": "The Untapped Potential of Off-the-Shelf Convolutional Neural Networks", "comments": "12 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over recent years, a myriad of novel convolutional network architectures have\nbeen developed to advance state-of-the-art performance on challenging\nrecognition tasks. As computational resources improve, a great deal of effort\nhas been placed in efficiently scaling up existing designs and generating new\narchitectures with Neural Architecture Search (NAS) algorithms. While network\ntopology has proven to be a critical factor for model performance, we show that\nsignificant gains are being left on the table by keeping topology static at\ninference-time. Due to challenges such as scale variation, we should not expect\nstatic models configured to perform well across a training dataset to be\noptimally configured to handle all test data. In this work, we seek to expose\nthe exciting potential of inference-time-dynamic models. By allowing just four\nlayers to dynamically change configuration at inference-time, we show that\nexisting off-the-shelf models like ResNet-50 are capable of over 95% accuracy\non ImageNet. This level of performance currently exceeds that of models with\nover 20x more parameters and significantly more complex training procedures.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 20:04:46 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Inkawhich", "Matthew", ""], ["Inkawhich", "Nathan", ""], ["Davis", "Eric", ""], ["Li", "Hai", ""], ["Chen", "Yiran", ""]]}, {"id": "2103.09903", "submitter": "Md. Akmal Haidar", "authors": "Md Akmal Haidar, Chao Xing, Mehdi Rezagholizadeh", "title": "Transformer-based ASR Incorporating Time-reduction Layer and Fine-tuning\n  with Self-Knowledge Distillation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  End-to-end automatic speech recognition (ASR), unlike conventional ASR, does\nnot have modules to learn the semantic representation from speech encoder.\nMoreover, the higher frame-rate of speech representation prevents the model to\nlearn the semantic representation properly. Therefore, the models that are\nconstructed by the lower frame-rate of speech encoder lead to better\nperformance. For Transformer-based ASR, the lower frame-rate is not only\nimportant for learning better semantic representation but also for reducing the\ncomputational complexity due to the self-attention mechanism which has O(n^2)\norder of complexity in both training and inference. In this paper, we propose a\nTransformer-based ASR model with the time reduction layer, in which we\nincorporate time reduction layer inside transformer encoder layers in addition\nto traditional sub-sampling methods to input features that further reduce the\nframe-rate. This can help in reducing the computational cost of the\nself-attention process for training and inference with performance improvement.\nMoreover, we introduce a fine-tuning approach for pre-trained ASR models using\nself-knowledge distillation (S-KD) which further improves the performance of\nour ASR model. Experiments on LibriSpeech datasets show that our proposed\nmethods outperform all other Transformer-based ASR systems. Furthermore, with\nlanguage model (LM) fusion, we achieve new state-of-the-art word error rate\n(WER) results for Transformer-based ASR models with just 30 million parameters\ntrained without any external data.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 21:02:36 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Haidar", "Md Akmal", ""], ["Xing", "Chao", ""], ["Rezagholizadeh", "Mehdi", ""]]}, {"id": "2103.09942", "submitter": "Shreyansh Daftry", "authors": "Shreyansh Daftry, Barry Ridge, William Seto, Tu-Hoa Pham, Peter\n  Ilhardt, Gerard Maggiolino, Mark Van der Merwe, Alex Brinkman, John Mayo,\n  Eric Kulczyski and Renaud Detry", "title": "Machine Vision based Sample-Tube Localization for Mars Sample Return", "comments": "IEEE Aerospace Conference, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A potential Mars Sample Return (MSR) architecture is being jointly studied by\nNASA and ESA. As currently envisioned, the MSR campaign consists of a series of\n3 missions: sample cache, fetch and return to Earth. In this paper, we focus on\nthe fetch part of the MSR, and more specifically the problem of autonomously\ndetecting and localizing sample tubes deposited on the Martian surface. Towards\nthis end, we study two machine-vision based approaches: First, a\ngeometry-driven approach based on template matching that uses hard-coded\nfilters and a 3D shape model of the tube; and second, a data-driven approach\nbased on convolutional neural networks (CNNs) and learned features.\nFurthermore, we present a large benchmark dataset of sample-tube images,\ncollected in representative outdoor environments and annotated with ground\ntruth segmentation masks and locations. The dataset was acquired systematically\nacross different terrain, illumination conditions and dust-coverage; and\nbenchmarking was performed to study the feasibility of each approach, their\nrelative strengths and weaknesses, and robustness in the presence of adverse\nenvironmental conditions.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 23:09:28 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Daftry", "Shreyansh", ""], ["Ridge", "Barry", ""], ["Seto", "William", ""], ["Pham", "Tu-Hoa", ""], ["Ilhardt", "Peter", ""], ["Maggiolino", "Gerard", ""], ["Van der Merwe", "Mark", ""], ["Brinkman", "Alex", ""], ["Mayo", "John", ""], ["Kulczyski", "Eric", ""], ["Detry", "Renaud", ""]]}, {"id": "2103.09957", "submitter": "Andy Kim", "authors": "Emma Chen, Andy Kim, Rayan Krishnan, Jin Long, Andrew Y. Ng, Pranav\n  Rajpurkar", "title": "CheXbreak: Misclassification Identification for Deep Learning Models\n  Interpreting Chest X-rays", "comments": "In Proceedings of the 2021 Conference on Machine Learning for Health\n  Care, 2021. In ACM Conference on Health, Inference, and Learning (ACM-CHIL)\n  Workshop 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A major obstacle to the integration of deep learning models for chest x-ray\ninterpretation into clinical settings is the lack of understanding of their\nfailure modes. In this work, we first investigate whether there are patient\nsubgroups that chest x-ray models are likely to misclassify. We find that\npatient age and the radiographic finding of lung lesion, pneumothorax or\nsupport devices are statistically relevant features for predicting\nmisclassification for some chest x-ray models. Second, we develop\nmisclassification predictors on chest x-ray models using their outputs and\nclinical features. We find that our best performing misclassification\nidentifier achieves an AUROC close to 0.9 for most diseases. Third, employing\nour misclassification identifiers, we develop a corrective algorithm to\nselectively flip model predictions that have high likelihood of\nmisclassification at inference time. We observe F1 improvement on the\nprediction of Consolidation (0.008 [95% CI 0.005, 0.010]) and Edema (0.003,\n[95% CI 0.001, 0.006]). By carrying out our investigation on ten distinct and\nhigh-performing chest x-ray models, we are able to derive insights across model\narchitectures and offer a generalizable framework applicable to other medical\nimaging tasks.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 00:30:19 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 20:10:14 GMT"}, {"version": "v3", "created": "Tue, 20 Jul 2021 17:20:35 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Chen", "Emma", ""], ["Kim", "Andy", ""], ["Krishnan", "Rayan", ""], ["Long", "Jin", ""], ["Ng", "Andrew Y.", ""], ["Rajpurkar", "Pranav", ""]]}, {"id": "2103.09977", "submitter": "Prithviraj Ammanabrolu", "authors": "Prithviraj Ammanabrolu and Mark O. Riedl", "title": "Situated Language Learning via Interactive Narratives", "comments": "Preprint. Under journal review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a roadmap that explores the question of how to imbue\nlearning agents with the ability to understand and generate contextually\nrelevant natural language in service of achieving a goal. We hypothesize that\ntwo key components in creating such agents are interactivity and environment\ngrounding, shown to be vital parts of language learning in humans, and posit\nthat interactive narratives should be the environments of choice for such\ntraining these agents. These games are simulations in which an agent interacts\nwith the world through natural language -- \"perceiving\", \"acting upon\", and\n\"talking to\" the world using textual descriptions, commands, and dialogue --\nand as such exist at the intersection of natural language processing,\nstorytelling, and sequential decision making. We discuss the unique challenges\na text games' puzzle-like structure combined with natural language\nstate-and-action spaces provides: knowledge representation, commonsense\nreasoning, and exploration. Beyond the challenges described so far, progress in\nthe realm of interactive narratives can be applied in adjacent problem domains.\nThese applications provide interesting challenges of their own as well as\nextensions to those discussed so far. We describe three of them in detail: (1)\nevaluating AI system's commonsense understanding by automatically creating\ninteractive narratives; (2) adapting abstract text-based policies to include\nother modalities such as vision; and (3) enabling multi-agent and human-AI\ncollaboration in shared, situated worlds.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 01:55:16 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Ammanabrolu", "Prithviraj", ""], ["Riedl", "Mark O.", ""]]}, {"id": "2103.09979", "submitter": "Zhongqiang Ren", "authors": "Zhongqiang Ren, Sivakumar Rathinam and Howie Choset", "title": "MS*: A New Exact Algorithm for Multi-agent Simultaneous Multi-goal\n  Sequencing and Path Finding", "comments": "6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multi-agent applications such as surveillance and logistics, fleets of\nmobile agents are often expected to coordinate and safely visit a large number\nof goal locations as efficiently as possible. The multi-agent planning problem\nin these applications involves allocating and sequencing goals for each agent\nwhile simultaneously producing conflict-free paths for the agents. In this\narticle, we introduce a new algorithm called MS* which computes an optimal\nsolution for this multi-agent problem by fusing and advancing state of the art\nsolvers for multi-agent path finding (MAPF) and multiple travelling salesman\nproblem (mTSP). MS* leverages our prior subdimensional expansion approach for\nMAPF and embeds the mTSP solvers to optimally allocate and sequence goals for\nagents. Numerical results show that our new algorithm can solve the multi-agent\nproblem with 20 agents and 50 goals in a minute of CPU time on a standard\nlaptop.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 01:57:35 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Ren", "Zhongqiang", ""], ["Rathinam", "Sivakumar", ""], ["Choset", "Howie", ""]]}, {"id": "2103.09990", "submitter": "Zahra Zahedi", "authors": "Zahra Zahedi and Subbarao Kambhampati", "title": "Human-AI Symbiosis: A Survey of Current Approaches", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we aim at providing a comprehensive outline of the different\nthreads of work in human-AI collaboration. By highlighting various aspects of\nworks on the human-AI team such as the flow of complementing, task horizon,\nmodel representation, knowledge level, and teaming goal, we make a taxonomy of\nrecent works according to these dimensions. We hope that the survey will\nprovide a more clear connection between the works in the human-AI team and\nguidance to new researchers in this area.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 02:39:28 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Zahedi", "Zahra", ""], ["Kambhampati", "Subbarao", ""]]}, {"id": "2103.10069", "submitter": "Luan Thanh Nguyen", "authors": "Luan Thanh Nguyen, Kiet Van Nguyen, Ngan Luu-Thuy Nguyen", "title": "Constructive and Toxic Speech Detection for Open-domain Social Media\n  Comments in Vietnamese", "comments": "Accepted as a FULL PAPER for The 34th International Conference on\n  Industrial, Engineering & Other Applications of Applied Intelligent Systems\n  (IEA/AIE 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The rise of social media has led to the increasing of comments on online\nforums. However, there still exists invalid comments which are not informative\nfor users. Moreover, those comments are also quite toxic and harmful to people.\nIn this paper, we create a dataset for constructive and toxic speech detection,\nnamed UIT-ViCTSD (Vietnamese Constructive and Toxic Speech Detection dataset)\nwith 10,000 human-annotated comments. For these tasks, we propose a system for\nconstructive and toxic speech detection with the state-of-the-art transfer\nlearning model in Vietnamese NLP as PhoBERT. With this system, we obtain\nF1-scores of 78.59% and 59.40% for classifying constructive and toxic comments,\nrespectively. Besides, we implement various baseline models as traditional\nMachine Learning and Deep Neural Network-Based models to evaluate the dataset.\nWith the results, we can solve several tasks on the online discussions and\ndevelop the framework for identifying constructiveness and toxicity of\nVietnamese social media comments automatically.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 08:04:12 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 08:55:12 GMT"}, {"version": "v3", "created": "Wed, 24 Mar 2021 14:49:19 GMT"}, {"version": "v4", "created": "Fri, 16 Apr 2021 14:50:06 GMT"}], "update_date": "2021-04-19", "authors_parsed": [["Nguyen", "Luan Thanh", ""], ["Van Nguyen", "Kiet", ""], ["Nguyen", "Ngan Luu-Thuy", ""]]}, {"id": "2103.10070", "submitter": "Clemence Reda", "authors": "Cl\\'emence R\\'eda (UP M\\'edecine Paris Nord, INSERM), Emilie Kaufmann\n  (CNRS, Lille DECCID SID), Andr\\'ee Delahaye-Duriez (UP M\\'edecine Paris Nord,\n  INSERM)", "title": "Top-m identification for linear bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.ST q-bio.QM stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by an application to drug repurposing, we propose the first\nalgorithms to tackle the identification of the m $\\ge$ 1 arms with largest\nmeans in a linear bandit model, in the fixed-confidence setting. These\nalgorithms belong to the generic family of Gap-Index Focused Algorithms (GIFA)\nthat we introduce for Top-m identification in linear bandits. We propose a\nunified analysis of these algorithms, which shows how the use of features might\ndecrease the sample complexity. We further validate these algorithms\nempirically on simulated data and on a simple drug repurposing task.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 08:04:45 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["R\u00e9da", "Cl\u00e9mence", "", "UP M\u00e9decine Paris Nord, INSERM"], ["Kaufmann", "Emilie", "", "CNRS, Lille DECCID SID"], ["Delahaye-Duriez", "Andr\u00e9e", "", "UP M\u00e9decine Paris Nord,\n  INSERM"]]}, {"id": "2103.10133", "submitter": "Aili Shen", "authors": "Aili Shen, Meladel Mistica, Bahar Salehi, Hang Li, Timothy Baldwin,\n  and Jianzhong Qi", "title": "Evaluating Document Coherence Modelling", "comments": "accepted to TACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While pretrained language models (\"LM\") have driven impressive gains over\nmorpho-syntactic and semantic tasks, their ability to model discourse and\npragmatic phenomena is less clear. As a step towards a better understanding of\ntheir discourse modelling capabilities, we propose a sentence intrusion\ndetection task. We examine the performance of a broad range of pretrained LMs\non this detection task for English. Lacking a dataset for the task, we\nintroduce INSteD, a novel intruder sentence detection dataset, containing\n170,000+ documents constructed from English Wikipedia and CNN news articles.\nOur experiments show that pretrained LMs perform impressively in in-domain\nevaluation, but experience a substantial drop in the cross-domain setting,\nindicating limited generalisation capacity. Further results over a novel\nlinguistic probe dataset show that there is substantial room for improvement,\nespecially in the cross-domain setting.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 10:05:06 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Shen", "Aili", ""], ["Mistica", "Meladel", ""], ["Salehi", "Bahar", ""], ["Li", "Hang", ""], ["Baldwin", "Timothy", ""], ["Qi", "Jianzhong", ""]]}, {"id": "2103.10142", "submitter": "Florian Rehm", "authors": "Florian Rehm, Sofia Vallecorsa, Vikram Saletore, Hans Pabst, Adel\n  Chaibi, Valeriu Codreanu, Kerstin Borras, Dirk Kr\\\"ucker", "title": "Reduced Precision Strategies for Deep Learning: A High Energy Physics\n  Generative Adversarial Network Use Case", "comments": "Submitted at ICPRAM 2021; from CERN openlab - Intel collaboration", "journal-ref": "ICPRAM 2021", "doi": "10.5220/0010245002510258", "report-no": null, "categories": "physics.data-an cs.AI hep-ex", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning is finding its way into high energy physics by replacing\ntraditional Monte Carlo simulations. However, deep learning still requires an\nexcessive amount of computational resources. A promising approach to make deep\nlearning more efficient is to quantize the parameters of the neural networks to\nreduced precision. Reduced precision computing is extensively used in modern\ndeep learning and results to lower execution inference time, smaller memory\nfootprint and less memory bandwidth. In this paper we analyse the effects of\nlow precision inference on a complex deep generative adversarial network model.\nThe use case which we are addressing is calorimeter detector simulations of\nsubatomic particle interactions in accelerator based high energy physics. We\nemploy the novel Intel low precision optimization tool (iLoT) for quantization\nand compare the results to the quantized model from TensorFlow Lite. In the\nperformance benchmark we gain a speed-up of 1.73x on Intel hardware for the\nquantized iLoT model compared to the initial, not quantized, model. With\ndifferent physics-inspired self-developed metrics, we validate that the\nquantized iLoT model shows a lower loss of physical accuracy in comparison to\nthe TensorFlow Lite model.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 10:20:23 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Rehm", "Florian", ""], ["Vallecorsa", "Sofia", ""], ["Saletore", "Vikram", ""], ["Pabst", "Hans", ""], ["Chaibi", "Adel", ""], ["Codreanu", "Valeriu", ""], ["Borras", "Kerstin", ""], ["Kr\u00fccker", "Dirk", ""]]}, {"id": "2103.10150", "submitter": "James Townsend", "authors": "James Townsend, Iain Murray", "title": "Lossless compression with state space models using bits back coding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We generalize the 'bits back with ANS' method to time-series models with a\nlatent Markov structure. This family of models includes hidden Markov models\n(HMMs), linear Gaussian state space models (LGSSMs) and many more. We provide\nexperimental evidence that our method is effective for small scale models, and\ndiscuss its applicability to larger scale settings such as video compression.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 10:34:57 GMT"}, {"version": "v2", "created": "Fri, 19 Mar 2021 10:53:45 GMT"}, {"version": "v3", "created": "Tue, 4 May 2021 09:43:19 GMT"}], "update_date": "2021-05-05", "authors_parsed": [["Townsend", "James", ""], ["Murray", "Iain", ""]]}, {"id": "2103.10159", "submitter": "Pratik Jawanpuria", "authors": "Karthik S. Gurumoorthy and Pratik Jawanpuria and Bamdev Mishra", "title": "SPOT: A framework for selection of prototypes using optimal transport", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we develop an optimal transport (OT) based framework to select\ninformative prototypical examples that best represent a given target dataset.\nSummarizing a given target dataset via representative examples is an important\nproblem in several machine learning applications where human understanding of\nthe learning models and underlying data distribution is essential for decision\nmaking. We model the prototype selection problem as learning a sparse\n(empirical) probability distribution having the minimum OT distance from the\ntarget distribution. The learned probability measure supported on the chosen\nprototypes directly corresponds to their importance in representing the target\ndata. We show that our objective function enjoys a key property of\nsubmodularity and propose an efficient greedy method that is both\ncomputationally fast and possess deterministic approximation guarantees.\nEmpirical results on several real world benchmarks illustrate the efficacy of\nour approach.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 10:50:14 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 13:58:27 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Gurumoorthy", "Karthik S.", ""], ["Jawanpuria", "Pratik", ""], ["Mishra", "Bamdev", ""]]}, {"id": "2103.10176", "submitter": "Guy Tennenholtz", "authors": "Nir Baram, Guy Tennenholtz, Shie Mannor", "title": "Maximum Entropy Reinforcement Learning with Mixture Policies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixture models are an expressive hypothesis class that can approximate a rich\nset of policies. However, using mixture policies in the Maximum Entropy\n(MaxEnt) framework is not straightforward. The entropy of a mixture model is\nnot equal to the sum of its components, nor does it have a closed-form\nexpression in most cases. Using such policies in MaxEnt algorithms, therefore,\nrequires constructing a tractable approximation of the mixture entropy. In this\npaper, we derive a simple, low-variance mixture-entropy estimator. We show that\nit is closely related to the sum of marginal entropies. Equipped with our\nentropy estimator, we derive an algorithmic variant of Soft Actor-Critic (SAC)\nto the mixture policy case and evaluate it on a series of continuous control\ntasks.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 11:23:39 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Baram", "Nir", ""], ["Tennenholtz", "Guy", ""], ["Mannor", "Shie", ""]]}, {"id": "2103.10206", "submitter": "Buyu Li", "authors": "Buyu Li, Yongchi Zhao, Lu Sheng", "title": "DanceNet3D: Music Based Dance Generation with Parametric Motion\n  Transformer", "comments": "Add project link in abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we propose a novel deep learning framework that can generate a\nvivid dance from a whole piece of music. In contrast to previous works that\ndefine the problem as generation of frames of motion state parameters, we\nformulate the task as a prediction of motion curves between key poses, which is\ninspired by the animation industry practice. The proposed framework, named\nDanceNet3D, first generates key poses on beats of the given music and then\npredicts the in-between motion curves. DanceNet3D adopts the encoder-decoder\narchitecture and the adversarial schemes for training. The decoders in\nDanceNet3D are constructed on MoTrans, a transformer tailored for motion\ngeneration. In MoTrans we introduce the kinematic correlation by the Kinematic\nChain Networks, and we also propose the Learned Local Attention module to take\nthe temporal local correlation of human motion into consideration. Furthermore,\nwe propose PhantomDance, the first large-scale dance dataset produced by\nprofessional animatiors, with accurate synchronization with music. Extensive\nexperiments demonstrate that the proposed approach can generate fluent,\nelegant, performative and beat-synchronized 3D dances, which significantly\nsurpasses previous works quantitatively and qualitatively.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 12:17:38 GMT"}, {"version": "v2", "created": "Sat, 20 Mar 2021 07:28:26 GMT"}, {"version": "v3", "created": "Thu, 25 Mar 2021 09:27:22 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Li", "Buyu", ""], ["Zhao", "Yongchi", ""], ["Sheng", "Lu", ""]]}, {"id": "2103.10213", "submitter": "Jianhua He", "authors": "Zheng Huang, Kai Chen, Jianhua He, Xiang Bai, Dimosthenis Karatzas,\n  Shjian Lu, and C.V. Jawahar", "title": "ICDAR2019 Competition on Scanned Receipt OCR and Information Extraction", "comments": null, "journal-ref": null, "doi": "10.1109/ICDAR.2019.00244", "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scanned receipts OCR and key information extraction (SROIE) represent the\nprocesseses of recognizing text from scanned receipts and extracting key texts\nfrom them and save the extracted tests to structured documents. SROIE plays\ncritical roles for many document analysis applications and holds great\ncommercial potentials, but very little research works and advances have been\npublished in this area. In recognition of the technical challenges, importance\nand huge commercial potentials of SROIE, we organized the ICDAR 2019\ncompetition on SROIE. In this competition, we set up three tasks, namely,\nScanned Receipt Text Localisation (Task 1), Scanned Receipt OCR (Task 2) and\nKey Information Extraction from Scanned Receipts (Task 3). A new dataset with\n1000 whole scanned receipt images and annotations is created for the\ncompetition. In this report we will presents the motivation, competition\ndatasets, task definition, evaluation protocol, submission statistics,\nperformance of submitted methods and results analysis.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 12:33:41 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Huang", "Zheng", ""], ["Chen", "Kai", ""], ["He", "Jianhua", ""], ["Bai", "Xiang", ""], ["Karatzas", "Dimosthenis", ""], ["Lu", "Shjian", ""], ["Jawahar", "C. V.", ""]]}, {"id": "2103.10245", "submitter": "Ashish Rana", "authors": "Ashish Rana, Avleen Malhi", "title": "Building Safer Autonomous Agents by Leveraging Risky Driving Behavior\n  Knowledge", "comments": "8 Pages, 7 Figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Simulation environments are good for learning different driving tasks like\nlane changing, parking or handling intersections etc. in an abstract manner.\nHowever, these simulation environments often restrict themselves to operate\nunder conservative interactions behavior amongst different vehicles. But, as we\nknow that the real driving tasks often involves very high risk scenarios where\nother drivers often don't behave in the expected sense. There can be many\nreasons for this behavior like being tired or inexperienced. The simulation\nenvironments doesn't take this information into account while training the\nnavigation agent. Therefore, in this study we especially focus on\nsystematically creating these risk prone scenarios with heavy traffic and\nunexpected random behavior for creating better model-free learning agents. We\ngenerate multiple autonomous driving scenarios by creating new custom Markov\nDecision Process (MDP) environment iterations in highway-env simulation\npackage. The behavior policy is learnt by agents trained with the help from\ndeep reinforcement learning models. Our behavior policy is deliberated to\nhandle collisions and risky randomized driver behavior. We train model free\nlearning agents with supplement information of risk prone driving scenarios and\ncompare their performance with baseline agents. Finally, we casually measure\nthe impact of adding these perturbations in the training process to precisely\naccount for the performance improvement attained from utilizing the learnings\nfrom these scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 23:39:33 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 14:35:14 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Rana", "Ashish", ""], ["Malhi", "Avleen", ""]]}, {"id": "2103.10248", "submitter": "Lu\\'is Cruz", "authors": "Yuanhao Xie, Lu\\'is Cruz, Petra Heck, Jan S. Rellermeyer", "title": "Systematic Mapping Study on the Machine Learning Lifecycle", "comments": "Accepted at WAIN21: 1st Workshop on AI Engineering - Software\n  Engineering for AI", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The development of artificial intelligence (AI) has made various industries\neager to explore the benefits of AI. There is an increasing amount of research\nsurrounding AI, most of which is centred on the development of new AI\nalgorithms and techniques. However, the advent of AI is bringing an increasing\nset of practical problems related to AI model lifecycle management that need to\nbe investigated. We address this gap by conducting a systematic mapping study\non the lifecycle of AI model. Through quantitative research, we provide an\noverview of the field, identify research opportunities, and provide suggestions\nfor future research. Our study yields 405 publications published from 2005 to\n2020, mapped in 5 different main research topics, and 31 sub-topics. We observe\nthat only a minority of publications focus on data management and model\nproduction problems, and that more studies should address the AI lifecycle from\na holistic perspective.\n", "versions": [{"version": "v1", "created": "Thu, 11 Mar 2021 11:44:23 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Xie", "Yuanhao", ""], ["Cruz", "Lu\u00eds", ""], ["Heck", "Petra", ""], ["Rellermeyer", "Jan S.", ""]]}, {"id": "2103.10252", "submitter": "Jeffrey Cheng", "authors": "Jeffrey Cheng, Ari Benjamin, Benjamin Lansdell, Konrad Paul Kordin", "title": "Augmenting Supervised Learning by Meta-learning Unsupervised Local Rules", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The brain performs unsupervised learning and (perhaps) simultaneous\nsupervised learning. This raises the question as to whether a hybrid of\nsupervised and unsupervised methods will produce better learning. Inspired by\nthe rich space of Hebbian learning rules, we set out to directly learn the\nunsupervised learning rule on local information that best augments a supervised\nsignal. We present the Hebbian-augmented training algorithm (HAT) for combining\ngradient-based learning with an unsupervised rule on pre-synpatic activity,\npost-synaptic activities, and current weights. We test HAT's effect on a simple\nproblem (Fashion-MNIST) and find consistently higher performance than\nsupervised learning alone. This finding provides empirical evidence that\nunsupervised learning on synaptic activities provides a strong signal that can\nbe used to augment gradient-based methods.\n  We further find that the meta-learned update rule is a time-varying function;\nthus, it is difficult to pinpoint an interpretable Hebbian update rule that\naids in training. We do find that the meta-learner eventually degenerates into\na non-Hebbian rule that preserves important weights so as not to disturb the\nlearner's convergence.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 05:25:21 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Cheng", "Jeffrey", ""], ["Benjamin", "Ari", ""], ["Lansdell", "Benjamin", ""], ["Kordin", "Konrad Paul", ""]]}, {"id": "2103.10257", "submitter": "Yusuf Mesbah", "authors": "Yusuf Mesbah, Youssef Youssry Ibrahim, Adil Mehood Khan", "title": "Domain Generalization using Ensemble Learning", "comments": "11 pages, 3 figures, 4 tables, summited to IntelliSys 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain generalization is a sub-field of transfer learning that aims at\nbridging the gap between two different domains in the absence of any knowledge\nabout the target domain. Our approach tackles the problem of a model's weak\ngeneralization when it is trained on a single source domain. From this\nperspective, we build an ensemble model on top of base deep learning models\ntrained on a single source to enhance the generalization of their collective\nprediction. The results achieved thus far have demonstrated promising\nimprovements of the ensemble over any of its base learners.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 13:50:36 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Mesbah", "Yusuf", ""], ["Ibrahim", "Youssef Youssry", ""], ["Khan", "Adil Mehood", ""]]}, {"id": "2103.10270", "submitter": "Hans-Martin Heyn", "authors": "Hans-Martin Heyn, Eric Knauss, Amna Pir Muhammad, Olof Eriksson,\n  Jennifer Linder, Padmini Subbiah, Shameer Kumar Pradhan, Sagar Tungal", "title": "Requirement Engineering Challenges for AI-intense Systems Development", "comments": "Contribution to the WAIN'21 1st Workshop on AI Engineering during the\n  43rd International Conference on Software Engineering (ICSE21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Availability of powerful computation and communication technology as well as\nadvances in artificial intelligence enable a new generation of complex,\nAI-intense systems and applications. Such systems and applications promise\nexciting improvements on a societal level, yet they also bring with them new\nchallenges for their development. In this paper we argue that significant\nchallenges relate to defining and ensuring behaviour and quality attributes of\nsuch systems and applications. We specifically derive four challenge areas from\nrelevant use cases of complex, AI-intense systems and applications related to\nindustry, transportation, and home automation: understanding, determining, and\nspecifying (i) contextual definitions and requirements, (ii) data attributes\nand requirements, (iii) performance definition and monitoring, and (iv) the\nimpact of human factors on system acceptance and success. Solving these\nchallenges will imply process support that integrates new requirements\nengineering methods into development approaches for complex, AI-intense systems\nand applications. We present these challenges in detail and propose a research\nroadmap.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 14:06:13 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 07:29:58 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Heyn", "Hans-Martin", ""], ["Knauss", "Eric", ""], ["Muhammad", "Amna Pir", ""], ["Eriksson", "Olof", ""], ["Linder", "Jennifer", ""], ["Subbiah", "Padmini", ""], ["Pradhan", "Shameer Kumar", ""], ["Tungal", "Sagar", ""]]}, {"id": "2103.10277", "submitter": "Sergio Martiradonna", "authors": "Sergio Martiradonna, Andrea Abrardo, Marco Moretti, Giuseppe Piro,\n  Gennaro Boggia", "title": "Deep Reinforcement Learning-Aided RAN Slicing Enforcement for B5G\n  Latency Sensitive Services", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI cs.LG cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The combination of cloud computing capabilities at the network edge and\nartificial intelligence promise to turn future mobile networks into service-\nand radio-aware entities, able to address the requirements of upcoming\nlatency-sensitive applications. In this context, a challenging research goal is\nto exploit edge intelligence to dynamically and optimally manage the Radio\nAccess Network Slicing (that is a less mature and more complex technology than\nfifth-generation Network Slicing) and Radio Resource Management, which is a\nvery complex task due to the mostly unpredictably nature of the wireless\nchannel. This paper presents a novel architecture that leverages Deep\nReinforcement Learning at the edge of the network in order to address Radio\nAccess Network Slicing and Radio Resource Management optimization supporting\nlatency-sensitive applications. The effectiveness of our proposal against\nbaseline methodologies is investigated through computer simulation, by\nconsidering an autonomous-driving use-case.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 14:18:34 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Martiradonna", "Sergio", ""], ["Abrardo", "Andrea", ""], ["Moretti", "Marco", ""], ["Piro", "Giuseppe", ""], ["Boggia", "Gennaro", ""]]}, {"id": "2103.10321", "submitter": "Kevin Tierney", "authors": "Carlos Ansotegui, Meinolf Sellmann, Tapan Shah, Kevin Tierney", "title": "Learning How to Optimize Black-Box Functions With Extreme Limits on the\n  Number of Function Evaluations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider black-box optimization in which only an extremely limited number\nof function evaluations, on the order of around 100, are affordable and the\nfunction evaluations must be performed in even fewer batches of a limited\nnumber of parallel trials. This is a typical scenario when optimizing variable\nsettings that are very costly to evaluate, for example in the context of\nsimulation-based optimization or machine learning hyperparameterization. We\npropose an original method that uses established approaches to propose a set of\npoints for each batch and then down-selects from these candidate points to the\nnumber of trials that can be run in parallel. The key novelty of our approach\nlies in the introduction of a hyperparameterized method for down-selecting the\nnumber of candidates to the allowed batch-size, which is optimized offline\nusing automated algorithm configuration. We tune this method for black box\noptimization and then evaluate on classical black box optimization benchmarks.\nOur results show that it is possible to learn how to combine evaluation points\nsuggested by highly diverse black box optimization methods conditioned on the\nprogress of the optimization. Compared with the state of the art in black box\nminimization and various other methods specifically geared towards few-shot\nminimization, we achieve an average reduction of 50\\% of normalized cost, which\nis a highly significant improvement in performance.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 15:30:15 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Ansotegui", "Carlos", ""], ["Sellmann", "Meinolf", ""], ["Shah", "Tapan", ""], ["Tierney", "Kevin", ""]]}, {"id": "2103.10350", "submitter": "Mohammad Hosseini", "authors": "Mohammad Hosseini, Mahmudul Hasan", "title": "The Case for High-Accuracy Classification: Think Small, Think Many!", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To facilitate implementation of high-accuracy deep neural networks especially\non resource-constrained devices, maintaining low computation requirements is\ncrucial. Using very deep models for classification purposes not only decreases\nthe neural network training speed and increases the inference time, but also\nneed more data for higher prediction accuracy and to mitigate false positives.\n  In this paper, we propose an efficient and lightweight deep classification\nensemble structure based on a combination of simple color features, which is\nparticularly designed for \"high-accuracy\" image classifications with low false\npositives. We designed, implemented, and evaluated our approach for explosion\ndetection use-case applied to images and videos. Our evaluation results based\non a large test test show considerable improvements on the prediction accuracy\ncompared to the popular ResNet-50 model, while benefiting from 7.64x faster\ninference and lower computation cost.\n  While we applied our approach to explosion detection, our approach is general\nand can be applied to other similar classification use cases as well. Given the\ninsight gained from our experiments, we hence propose a \"think small, think\nmany\" philosophy in classification scenarios: that transforming a single,\nlarge, monolithic deep model into a verification-based step model ensemble of\nmultiple small, simple, lightweight models with narrowed-down color spaces can\npossibly lead to predictions with higher accuracy.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 16:15:31 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Hosseini", "Mohammad", ""], ["Hasan", "Mahmudul", ""]]}, {"id": "2103.10360", "submitter": "Zhengxiao Du", "authors": "Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin\n  Yang, Jie Tang", "title": "All NLP Tasks Are Generation Tasks: A General Pretraining Framework", "comments": "14 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There have been various types of pretraining architectures including\nautoregressive models (e.g., GPT), autoencoding models (e.g., BERT), and\nencoder-decoder models (e.g., T5). On the other hand, NLP tasks are different\nin nature, with three main categories being classification, unconditional\ngeneration, and conditional generation. However, none of the pretraining\nframeworks performs the best for all tasks, which introduces inconvenience for\nmodel development and selection. We propose a novel pretraining framework GLM\n(General Language Model) to address this challenge. Compared to previous work,\nour architecture has three major benefits: (1) it performs well on\nclassification, unconditional generation, and conditional generation tasks with\none single pretrained model; (2) it outperforms BERT-like models on\nclassification due to improved pretrain-finetune consistency; (3) it naturally\nhandles variable-length blank filling which is crucial for many downstream\ntasks. Empirically, GLM substantially outperforms BERT on the SuperGLUE natural\nlanguage understanding benchmark with the same amount of pre-training data.\nMoreover, GLM with 1.25x parameters of BERT-Large achieves the best performance\nin NLU, conditional and unconditional generation at the same time, which\ndemonstrates its generalizability to different downstream tasks.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 16:30:26 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Du", "Zhengxiao", ""], ["Qian", "Yujie", ""], ["Liu", "Xiao", ""], ["Ding", "Ming", ""], ["Qiu", "Jiezhong", ""], ["Yang", "Zhilin", ""], ["Tang", "Jie", ""]]}, {"id": "2103.10369", "submitter": "Sebastian Curi", "authors": "Sebastian Curi, Ilija Bogunovic, Andreas Krause", "title": "Combining Pessimism with Optimism for Robust and Efficient Model-Based\n  Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In real-world tasks, reinforcement learning (RL) agents frequently encounter\nsituations that are not present during training time. To ensure reliable\nperformance, the RL agents need to exhibit robustness against worst-case\nsituations. The robust RL framework addresses this challenge via a worst-case\noptimization between an agent and an adversary. Previous robust RL algorithms\nare either sample inefficient, lack robustness guarantees, or do not scale to\nlarge problems. We propose the Robust Hallucinated Upper-Confidence RL\n(RH-UCRL) algorithm to provably solve this problem while attaining near-optimal\nsample complexity guarantees. RH-UCRL is a model-based reinforcement learning\n(MBRL) algorithm that effectively distinguishes between epistemic and aleatoric\nuncertainty and efficiently explores both the agent and adversary decision\nspaces during policy learning. We scale RH-UCRL to complex tasks via neural\nnetworks ensemble models as well as neural network policies. Experimentally, we\ndemonstrate that RH-UCRL outperforms other robust deep RL algorithms in a\nvariety of adversarial environments.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 16:50:17 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Curi", "Sebastian", ""], ["Bogunovic", "Ilija", ""], ["Krause", "Andreas", ""]]}, {"id": "2103.10374", "submitter": "Chen Chen", "authors": "Weiping Yu, Sijie Zhu, Taojiannan Yang, Chen Chen, Mengyuan Liu", "title": "Consistency-based Active Learning for Object Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning aims to improve the performance of task model by selecting\nthe most informative samples with a limited budget. Unlike most recent works\nthat focused on applying active learning for image classification, we propose\nan effective Consistency-based Active Learning method for object Detection\n(CALD), which fully explores the consistency between original and augmented\ndata. CALD has three appealing benefits. (i) CALD is systematically designed by\ninvestigating the weaknesses of existing active learning methods, which do not\ntake the unique challenges of object detection into account. (ii) CALD unifies\nbox regression and classification with a single metric, which is not concerned\nby active learning methods for classification. CALD also focuses on the most\ninformative local region rather than the whole image, which is beneficial for\nobject detection. (iii) CALD not only gauges individual information for sample\nselection, but also leverages mutual information to encourage a balanced data\ndistribution. Extensive experiments show that CALD significantly outperforms\nexisting state-of-the-art task-agnostic and detection-specific active learning\nmethods on general object detection datasets. Based on the Faster R-CNN\ndetector, CALD consistently surpasses the baseline method (random selection) by\n2.9/2.8/0.8 mAP on average on PASCAL VOC 2007, PASCAL VOC 2012, and MS COCO.\nCode is available at \\url{https://github.com/we1pingyu/CALD}\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 17:00:34 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 15:53:32 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Yu", "Weiping", ""], ["Zhu", "Sijie", ""], ["Yang", "Taojiannan", ""], ["Chen", "Chen", ""], ["Liu", "Mengyuan", ""]]}, {"id": "2103.10394", "submitter": "Dogan Corus", "authors": "Dogan Corus and Andrei Lissovoi and Pietro S. Oliveto and Carsten Witt", "title": "On Steady-State Evolutionary Algorithms and Selective Pressure: Why\n  Inverse Rank-Based Allocation of Reproductive Trials is Best", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We analyse the impact of the selective pressure for the global optimisation\ncapabilities of steady-state EAs. For the standard bimodal benchmark function\n\\twomax we rigorously prove that using uniform parent selection leads to\nexponential runtimes with high probability to locate both optima for the\nstandard ($\\mu$+1)~EA and ($\\mu$+1)~RLS with any polynomial population sizes.\nOn the other hand, we prove that selecting the worst individual as parent leads\nto efficient global optimisation with overwhelming probability for reasonable\npopulation sizes. Since always selecting the worst individual may have\ndetrimental effects for escaping from local optima, we consider the performance\nof stochastic parent selection operators with low selective pressure for a\nfunction class called \\textsc{TruncatedTwoMax} where one slope is shorter than\nthe other. An experimental analysis shows that the EAs equipped with inverse\ntournament selection, where the loser is selected for reproduction and small\ntournament sizes, globally optimise \\textsc{TwoMax} efficiently and effectively\nescape from local optima of \\textsc{TruncatedTwoMax} with high probability.\nThus they identify both optima efficiently while uniform (or stronger)\nselection fails in theory and in practice. We then show the power of inverse\nselection on function classes from the literature where populations are\nessential by providing rigorous proofs or experimental evidence that it\noutperforms uniform selection equipped with or without a restart strategy. We\nconclude the paper by confirming our theoretical insights with an empirical\nanalysis of the different selective pressures on standard benchmarks of the\nclassical MaxSat and Multidimensional Knapsack Problems.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 17:27:05 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Corus", "Dogan", ""], ["Lissovoi", "Andrei", ""], ["Oliveto", "Pietro S.", ""], ["Witt", "Carsten", ""]]}, {"id": "2103.10410", "submitter": "Takfarinas Saber", "authors": "Takfarinas Saber, Anthony Ventresque, Joao Marques-Silva, James\n  Thorburn, Liam Murphy", "title": "MILP for the Multi-objective VM Reassignment Problem", "comments": null, "journal-ref": null, "doi": "10.1109/ICTAI.2015.20", "report-no": null, "categories": "cs.AI cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Machine Reassignment is a challenging problem for constraint programming (CP)\nand mixed-integer linear programming (MILP) approaches, especially given the\nsize of data centres. The multi-objective version of the Machine Reassignment\nProblem is even more challenging and it seems unlikely for CP or MILP to obtain\ngood results in this context. As a result, the first approaches to address this\nproblem have been based on other optimisation methods, including\nmetaheuristics. In this paper we study under which conditions a mixed-integer\noptimisation solver, such as IBM ILOG CPLEX, can be used for the\nMulti-objective Machine Reassignment Problem. We show that it is useful only\nfor small or medium-scale data centres and with some relaxations, such as an\noptimality tolerance gap and a limited number of directions explored in the\nsearch space. Building on this study, we also investigate a hybrid approach,\nfeeding a metaheuristic with the results of CPLEX, and we show that the gains\nare important in terms of quality of the set of Pareto solutions (+126.9%\nagainst the metaheuristic alone and +17.8% against CPLEX alone) and number of\nsolutions (8.9 times more than CPLEX), while the processing time increases only\nby 6% in comparison to CPLEX for execution times larger than 100 seconds.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 17:46:57 GMT"}], "update_date": "2021-03-19", "authors_parsed": [["Saber", "Takfarinas", ""], ["Ventresque", "Anthony", ""], ["Marques-Silva", "Joao", ""], ["Thorburn", "James", ""], ["Murphy", "Liam", ""]]}, {"id": "2103.10453", "submitter": "Olivier Goudet Dr", "authors": "Olivier Goudet and Jin-Kao Hao", "title": "Massively parallel hybrid search for the partial Latin square extension\n  problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The partial Latin square extension problem is to fill as many as possible\nempty cells of a partially filled Latin square. This problem is a useful model\nfor a wide range of relevant applications in diverse domains. This paper\npresents the first massively parallel hybrid search algorithm for this\ncomputationally challenging problem based on a transformation of the problem to\npartial graph coloring. The algorithm features the following original elements.\nBased on a very large population (with more than $10^4$ individuals) and modern\ngraphical processing units, the algorithm performs many local searches in\nparallel to ensure an intensified exploitation of the search space. It employs\na dedicated crossover with a specific parent matching strategy to create a\nlarge number of diversified and information-preserving offspring at each\ngeneration. Extensive experiments on 1800 benchmark instances show a high\ncompetitiveness of the algorithm compared with the current best performing\nmethods. Competitive results are also reported on the related Latin square\ncompletion problem. Analyses are performed to shed lights on the understanding\nof the main algorithmic components. The code of the algorithm will be made\npublicly available.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 18:09:50 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Goudet", "Olivier", ""], ["Hao", "Jin-Kao", ""]]}, {"id": "2103.10455", "submitter": "Chen Chen", "authors": "Ce Zheng, Sijie Zhu, Matias Mendieta, Taojiannan Yang, Chen Chen,\n  Zhengming Ding", "title": "3D Human Pose Estimation with Spatial and Temporal Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer architectures have become the model of choice in natural language\nprocessing and are now being introduced into computer vision tasks such as\nimage classification, object detection, and semantic segmentation. However, in\nthe field of human pose estimation, convolutional architectures still remain\ndominant. In this work, we present PoseFormer, a purely transformer-based\napproach for 3D human pose estimation in videos without convolutional\narchitectures involved. Inspired by recent developments in vision transformers,\nwe design a spatial-temporal transformer structure to comprehensively model the\nhuman joint relations within each frame as well as the temporal correlations\nacross frames, then output an accurate 3D human pose of the center frame. We\nquantitatively and qualitatively evaluate our method on two popular and\nstandard benchmark datasets: Human3.6M and MPI-INF-3DHP. Extensive experiments\nshow that PoseFormer achieves state-of-the-art performance on both datasets.\nCode is available at \\url{https://github.com/zczcwh/PoseFormer}\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 18:14:37 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 16:54:14 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Zheng", "Ce", ""], ["Zhu", "Sijie", ""], ["Mendieta", "Matias", ""], ["Yang", "Taojiannan", ""], ["Chen", "Chen", ""], ["Ding", "Zhengming", ""]]}, {"id": "2103.10492", "submitter": "Jakaria Rabbi", "authors": "Md. Tahmid Hasan Fuad, Awal Ahmed Fime, Delowar Sikder, Md. Akil\n  Raihan Iftee, Jakaria Rabbi, Mabrook S. Al-rakhami, Abdu Gumae, Ovishake Sen,\n  Mohtasim Fuad, and Md. Nazrul Islam", "title": "Recent Advances in Deep Learning Techniques for Face Recognition", "comments": "32 pages and citation: M. T. H. Fuad et al., \"Recent Advances in Deep\n  Learning Techniques for Face Recognition,\" in IEEE Access, vol. 9, pp.\n  99112-99142, 2021, doi: 10.1109/ACCESS.2021.3096136", "journal-ref": "in IEEE Access, vol. 9, pp. 99112-99142, 2021", "doi": "10.1109/ACCESS.2021.3096136", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, researchers have proposed many deep learning (DL) methods\nfor various tasks, and particularly face recognition (FR) made an enormous leap\nusing these techniques. Deep FR systems benefit from the hierarchical\narchitecture of the DL methods to learn discriminative face representation.\nTherefore, DL techniques significantly improve state-of-the-art performance on\nFR systems and encourage diverse and efficient real-world applications. In this\npaper, we present a comprehensive analysis of various FR systems that leverage\nthe different types of DL techniques, and for the study, we summarize 168\nrecent contributions from this area. We discuss the papers related to different\nalgorithms, architectures, loss functions, activation functions, datasets,\nchallenges, improvement ideas, current and future trends of DL-based FR\nsystems. We provide a detailed discussion of various DL methods to understand\nthe current state-of-the-art, and then we discuss various activation and loss\nfunctions for the methods. Additionally, we summarize different datasets used\nwidely for FR tasks and discuss challenges related to illumination, expression,\npose variations, and occlusion. Finally, we discuss improvement ideas, current\nand future trends of FR tasks.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 19:39:12 GMT"}, {"version": "v2", "created": "Wed, 21 Jul 2021 16:31:53 GMT"}], "update_date": "2021-07-22", "authors_parsed": [["Fuad", "Md. Tahmid Hasan", ""], ["Fime", "Awal Ahmed", ""], ["Sikder", "Delowar", ""], ["Iftee", "Md. Akil Raihan", ""], ["Rabbi", "Jakaria", ""], ["Al-rakhami", "Mabrook S.", ""], ["Gumae", "Abdu", ""], ["Sen", "Ovishake", ""], ["Fuad", "Mohtasim", ""], ["Islam", "Md. Nazrul", ""]]}, {"id": "2103.10507", "submitter": "Alessandro Gianola", "authors": "Paolo Felli and Alessandro Gianola and Marco Montali and Andrey Rivkin\n  and Sarah Winkler", "title": "CoCoMoT: Conformance Checking of Multi-Perspective Processes via SMT\n  (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conformance checking is a key process mining task for comparing the expected\nbehavior captured in a process model and the actual behavior recorded in a log.\nWhile this problem has been extensively studied for pure control-flow\nprocesses, conformance checking with multi-perspective processes is still at\nits infancy. In this paper, we attack this challenging problem by considering\nprocesses that combine the data and control-flow dimensions. In particular, we\nadopt data Petri nets (DPNs) as the underlying reference formalism, and show\nhow solid, well-established automated reasoning techniques can be effectively\nemployed for computing conformance metrics and data-aware alignments. We do so\nby introducing the CoCoMoT (Computing Conformance Modulo Theories) framework,\nwith a fourfold contribution. First, we show how SAT-based encodings studied in\nthe pure control-flow setting can be lifted to our data-aware case, using SMT\nas the underlying formal and algorithmic framework. Second, we introduce a\nnovel preprocessing technique based on a notion of property-preserving\nclustering, to speed up the computation of conformance checking outputs. Third,\nwe provide a proof-of-concept implementation that uses a state-of-the-art SMT\nsolver and report on preliminary experiments. Finally, we discuss how CoCoMoT\ndirectly lends itself to a number of further tasks, like multi- and\nanti-alignments, log analysis by clustering, and model repair.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 20:22:50 GMT"}, {"version": "v2", "created": "Mon, 19 Apr 2021 12:26:50 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Felli", "Paolo", ""], ["Gianola", "Alessandro", ""], ["Montali", "Marco", ""], ["Rivkin", "Andrey", ""], ["Winkler", "Sarah", ""]]}, {"id": "2103.10510", "submitter": "Chong Huang", "authors": "Chong Huang, Arash Nourian, Kevin Griest", "title": "Hidden Technical Debts for Fair Machine Learning in Financial Services", "comments": "Presented at NeurIPS 2020 Fair AI in Finance Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The recent advancements in machine learning (ML) have demonstrated the\npotential for providing a powerful solution to build complex prediction systems\nin a short time. However, in highly regulated industries, such as the financial\ntechnology (Fintech), people have raised concerns about the risk of ML systems\ndiscriminating against specific protected groups or individuals. To address\nthese concerns, researchers have introduced various mathematical fairness\nmetrics and bias mitigation algorithms. This paper discusses hidden technical\ndebts and challenges of building fair ML systems in a production environment\nfor Fintech. We explore various stages that require attention for fairness in\nthe ML system development and deployment life cycle. To identify hidden\ntechnical debts that exist in building fair ML system for Fintech, we focus on\nkey pipeline stages including data preparation, model development, system\nmonitoring and integration in production. Our analysis shows that enforcing\nfairness for production-ready ML systems in Fintech requires specific\nengineering commitments at different stages of ML system life cycle. We also\npropose several initial starting points to mitigate these technical debts for\ndeploying fair ML systems in production.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 20:27:34 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 01:11:01 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Huang", "Chong", ""], ["Nourian", "Arash", ""], ["Griest", "Kevin", ""]]}, {"id": "2103.10518", "submitter": "Qi Liu", "authors": "Qi Liu, Lei Yu, Laura Rimell, Phil Blunsom", "title": "Pretraining the Noisy Channel Model for Task-Oriented Dialogue", "comments": "Accepted to TACL, pre MIT Press publication version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Direct decoding for task-oriented dialogue is known to suffer from the\nexplaining-away effect, manifested in models that prefer short and generic\nresponses. Here we argue for the use of Bayes' theorem to factorize the\ndialogue task into two models, the distribution of the context given the\nresponse, and the prior for the response itself. This approach, an\ninstantiation of the noisy channel model, both mitigates the explaining-away\neffect and allows the principled incorporation of large pretrained models for\nthe response prior. We present extensive experiments showing that a noisy\nchannel model decodes better responses compared to direct decoding and that a\ntwo stage pretraining strategy, employing both open-domain and task-oriented\ndialogue data, improves over randomly initialized models.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 20:52:49 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Liu", "Qi", ""], ["Yu", "Lei", ""], ["Rimell", "Laura", ""], ["Blunsom", "Phil", ""]]}, {"id": "2103.10529", "submitter": "Franck Mamalet Dr", "authors": "Herv\\'e Delseny, Christophe Gabreau, Adrien Gauffriau, Bernard\n  Beaudouin, Ludovic Ponsolle, Lucian Alecu, Hugues Bonnin, Brice Beltran,\n  Didier Duchel, Jean-Brice Ginestet, Alexandre Hervieu, Ghilaine Martinez,\n  Sylvain Pasquet, Kevin Delmas, Claire Pagetti, Jean-Marc Gabriel, Camille\n  Chapdelaine, Sylvaine Picard, Mathieu Damour, Cyril Cappi, Laurent Gard\\`es,\n  Florence De Grancey, Eric Jenn, Baptiste Lefevre, Gregory Flandin,\n  S\\'ebastien Gerchinovitz, Franck Mamalet, Alexandre Albore", "title": "White Paper Machine Learning in Certified Systems", "comments": "113 pages, White paper", "journal-ref": null, "doi": null, "report-no": "S079L03T00-005", "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Learning (ML) seems to be one of the most promising solution to\nautomate partially or completely some of the complex tasks currently realized\nby humans, such as driving vehicles, recognizing voice, etc. It is also an\nopportunity to implement and embed new capabilities out of the reach of\nclassical implementation techniques. However, ML techniques introduce new\npotential risks. Therefore, they have only been applied in systems where their\nbenefits are considered worth the increase of risk. In practice, ML techniques\nraise multiple challenges that could prevent their use in systems submitted to\ncertification constraints. But what are the actual challenges? Can they be\novercome by selecting appropriate ML techniques, or by adopting new engineering\nor certification practices? These are some of the questions addressed by the ML\nCertification 3 Workgroup (WG) set-up by the Institut de Recherche\nTechnologique Saint Exup\\'ery de Toulouse (IRT), as part of the DEEL Project.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 21:14:30 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Delseny", "Herv\u00e9", ""], ["Gabreau", "Christophe", ""], ["Gauffriau", "Adrien", ""], ["Beaudouin", "Bernard", ""], ["Ponsolle", "Ludovic", ""], ["Alecu", "Lucian", ""], ["Bonnin", "Hugues", ""], ["Beltran", "Brice", ""], ["Duchel", "Didier", ""], ["Ginestet", "Jean-Brice", ""], ["Hervieu", "Alexandre", ""], ["Martinez", "Ghilaine", ""], ["Pasquet", "Sylvain", ""], ["Delmas", "Kevin", ""], ["Pagetti", "Claire", ""], ["Gabriel", "Jean-Marc", ""], ["Chapdelaine", "Camille", ""], ["Picard", "Sylvaine", ""], ["Damour", "Mathieu", ""], ["Cappi", "Cyril", ""], ["Gard\u00e8s", "Laurent", ""], ["De Grancey", "Florence", ""], ["Jenn", "Eric", ""], ["Lefevre", "Baptiste", ""], ["Flandin", "Gregory", ""], ["Gerchinovitz", "S\u00e9bastien", ""], ["Mamalet", "Franck", ""], ["Albore", "Alexandre", ""]]}, {"id": "2103.10534", "submitter": "Mayank Mittal", "authors": "Mayank Mittal, David Hoeller, Farbod Farshidian, Marco Hutter, Animesh\n  Garg", "title": "Articulated Object Interaction in Unknown Scenes with Whole-Body Mobile\n  Manipulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A kitchen assistant needs to operate human-scale objects, such as cabinets\nand ovens, in unmapped environments with dynamic obstacles. Autonomous\ninteractions in such real-world environments require integrating dexterous\nmanipulation and fluid mobility. While mobile manipulators in different\nform-factors provide an extended workspace, their real-world adoption has been\nlimited. This limitation is in part due to two main reasons: 1) inability to\ninteract with unknown human-scale objects such as cabinets and ovens, and 2)\ninefficient coordination between the arm and the mobile base. Executing a\nhigh-level task for general objects requires a perceptual understanding of the\nobject as well as adaptive whole-body control among dynamic obstacles. In this\npaper, we propose a two-stage architecture for autonomous interaction with\nlarge articulated objects in unknown environments. The first stage uses a\nlearned model to estimate the articulated model of a target object from an\nRGB-D input and predicts an action-conditional sequence of states for\ninteraction. The second stage comprises of a whole-body motion controller to\nmanipulate the object along the generated kinematic plan. We show that our\nproposed pipeline can handle complicated static and dynamic kitchen settings.\nMoreover, we demonstrate that the proposed approach achieves better performance\nthan commonly used control methods in mobile manipulation. For additional\nmaterial, please check: https://www.pair.toronto.edu/articulated-mm/ .\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 21:32:18 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Mittal", "Mayank", ""], ["Hoeller", "David", ""], ["Farshidian", "Farbod", ""], ["Hutter", "Marco", ""], ["Garg", "Animesh", ""]]}, {"id": "2103.10547", "submitter": "Dravyansh Sharma", "authors": "Maria-Florina Balcan, Dravyansh Sharma", "title": "Data driven algorithms for limited labeled data learning", "comments": "33 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider a novel data driven approach for designing learning algorithms\nthat can effectively learn with only a small number of labeled examples. This\nis crucial for modern machine learning applications where labels are scarce or\nexpensive to obtain. We focus on graph-based techniques, where the unlabeled\nexamples are connected in a graph under the implicit assumption that similar\nnodes likely have similar labels. Over the past decades, several elegant\ngraph-based semi-supervised and active learning algorithms for how to infer the\nlabels of the unlabeled examples given the graph and a few labeled examples\nhave been proposed. However, the problem of how to create the graph (which\nimpacts the practical usefulness of these methods significantly) has been\nrelegated to domain-specific art and heuristics and no general principles have\nbeen proposed. In this work we present a novel data driven approach for\nlearning the graph and provide strong formal guarantees in both the\ndistributional and online learning formalizations.\n  We show how to leverage problem instances coming from an underlying problem\ndomain to learn the graph hyperparameters from commonly used parametric\nfamilies of graphs that perform well on new instances coming from the same\ndomain. We obtain low regret and efficient algorithms in the online setting,\nand generalization guarantees in the distributional setting. We also show how\nto combine several very different similarity metrics and learn multiple\nhyperparameters, providing general techniques to apply to large classes of\nproblems. We expect some of the tools and techniques we develop along the way\nto be of interest beyond semi-supervised and active learning, for data driven\nalgorithms for combinatorial problems more generally.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 22:19:19 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 22:35:07 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Balcan", "Maria-Florina", ""], ["Sharma", "Dravyansh", ""]]}, {"id": "2103.10600", "submitter": "Hao Gao", "authors": "Hao Gao, Yongqing Wang, Shanshan Lyu, Huawei Shen, Xueqi Cheng", "title": "GCN-ALP: Addressing Matching Collisions in Anchor Link Prediction", "comments": "8 pages, 5figures, ICKG 2020", "journal-ref": null, "doi": "10.1109/ICBK50248.2020.00065", "report-no": null, "categories": "cs.SI cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Nowadays online users prefer to join multiple social media for the purpose of\nsocialized online service. The problem \\textit{anchor link prediction} is\nformalized to link user data with the common ground on user profile, content\nand network structure across social networks. Most of the traditional works\nconcentrated on learning matching function with explicit or implicit features\non observed user data. However, the low quality of observed user data confuses\nthe judgment on anchor links, resulting in the matching collision problem in\npractice. In this paper, we explore local structure consistency and then\nconstruct a matching graph in order to circumvent matching collisions.\nFurthermore, we propose graph convolution networks with mini-batch strategy,\nefficiently solving anchor link prediction on matching graph. The experimental\nresults on three real application scenarios show the great potentials of our\nproposed method in both prediction accuracy and efficiency. In addition, the\nvisualization of learned embeddings provides us a qualitative way to understand\nthe inference of anchor links on the matching graph.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 02:41:55 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Gao", "Hao", ""], ["Wang", "Yongqing", ""], ["Lyu", "Shanshan", ""], ["Shen", "Huawei", ""], ["Cheng", "Xueqi", ""]]}, {"id": "2103.10642", "submitter": "Sergio A. Serrano", "authors": "Sergio A. Serrano, Elizabeth Santiago, Jose Martinez-Carranza, Eduardo\n  Morales, L. Enrique Sucar", "title": "Knowledge-Based Hierarchical POMDPs for Task Planning", "comments": null, "journal-ref": "Journal of Intelligent & Robotic Systems 101 (2021) 1-30", "doi": "10.1007/s10846-021-01348-8", "report-no": null, "categories": "cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The main goal in task planning is to build a sequence of actions that takes\nan agent from an initial state to a goal state. In robotics, this is\nparticularly difficult because actions usually have several possible results,\nand sensors are prone to produce measurements with error. Partially observable\nMarkov decision processes (POMDPs) are commonly employed, thanks to their\ncapacity to model the uncertainty of actions that modify and monitor the state\nof a system. However, since solving a POMDP is computationally expensive, their\nusage becomes prohibitive for most robotic applications. In this paper, we\npropose a task planning architecture for service robotics. In the context of\nservice robot design, we present a scheme to encode knowledge about the robot\nand its environment, that promotes the modularity and reuse of information.\nAlso, we introduce a new recursive definition of a POMDP that enables our\narchitecture to autonomously build a hierarchy of POMDPs, so that it can be\nused to generate and execute plans that solve the task at hand. Experimental\nresults show that, in comparison to baseline methods, by following a recursive\nhierarchical approach the architecture is able to significantly reduce the\nplanning time, while maintaining (or even improving) the robustness under\nseveral scenarios that vary in uncertainty and size.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 05:45:05 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 17:33:30 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Serrano", "Sergio A.", ""], ["Santiago", "Elizabeth", ""], ["Martinez-Carranza", "Jose", ""], ["Morales", "Eduardo", ""], ["Sucar", "L. Enrique", ""]]}, {"id": "2103.10656", "submitter": "Nicolas Gillis", "authors": "Maryam Abdolali, Nicolas Gillis", "title": "Beyond Linear Subspace Clustering: A Comparative Study of Nonlinear\n  Manifold Clustering Algorithms", "comments": "55 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Subspace clustering is an important unsupervised clustering approach. It is\nbased on the assumption that the high-dimensional data points are approximately\ndistributed around several low-dimensional linear subspaces. The majority of\nthe prominent subspace clustering algorithms rely on the representation of the\ndata points as linear combinations of other data points, which is known as a\nself-expressive representation. To overcome the restrictive linearity\nassumption, numerous nonlinear approaches were proposed to extend successful\nsubspace clustering approaches to data on a union of nonlinear manifolds. In\nthis comparative study, we provide a comprehensive overview of nonlinear\nsubspace clustering approaches proposed in the last decade. We introduce a new\ntaxonomy to classify the state-of-the-art approaches into three categories,\nnamely locality preserving, kernel based, and neural network based. The major\nrepresentative algorithms within each category are extensively compared on\ncarefully designed synthetic and real-world data sets. The detailed analysis of\nthese approaches unfolds potential research directions and unsolved challenges\nin this field.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 06:34:34 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Abdolali", "Maryam", ""], ["Gillis", "Nicolas", ""]]}, {"id": "2103.10670", "submitter": "Zhengwen Li", "authors": "Zhengwen Li, Xiabi Liu", "title": "Improving Image co-segmentation via Deep Metric Learning", "comments": "11 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Metric Learning (DML) is helpful in computer vision tasks. In this\npaper, we firstly introduce DML into image co-segmentation. We propose a novel\nTriplet loss for Image Segmentation, called IS-Triplet loss for short, and\ncombine it with traditional image segmentation loss. Different from the general\nDML task which learns the metric between pictures, we treat each pixel as a\nsample, and use their embedded features in high-dimensional space to form\ntriples, then we tend to force the distance between pixels of different\ncategories greater than of the same category by optimizing IS-Triplet loss so\nthat the pixels from different categories are easier to be distinguished in the\nhigh-dimensional feature space. We further present an efficient triple sampling\nstrategy to make a feasible computation of IS-Triplet loss. Finally, the\nIS-Triplet loss is combined with 3 traditional image segmentation losses to\nperform image segmentation. We apply the proposed approach to image\nco-segmentation and test it on the SBCoseg dataset and the Internet dataset.\nThe experimental result shows that our approach can effectively improve the\ndiscrimination of pixels' categories in high-dimensional space and thus help\ntraditional loss achieve better performance of image segmentation with fewer\ntraining epochs.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 07:30:42 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Li", "Zhengwen", ""], ["Liu", "Xiabi", ""]]}, {"id": "2103.10685", "submitter": "Xu Zou", "authors": "Xu Zou, Da Yin, Qingyang Zhong, Ming Ding, Zhilin Yang, Jie Tang", "title": "Controllable Generation from Pre-trained Language Models via Inverse\n  Prompting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Large-scale pre-trained language models have demonstrated strong capabilities\nof generating realistic text. However, it remains challenging to control the\ngeneration results. Previous approaches such as prompting are far from\nsufficient, which limits the usage of language models. To tackle this\nchallenge, we propose an innovative method, inverse prompting, to better\ncontrol text generation. The core idea of inverse prompting is to use generated\ntext to inversely predict the prompt during beam search, which enhances the\nrelevance between the prompt and the generated text and provides better\ncontrollability. Empirically, we pre-train a large-scale Chinese language model\nto perform a systematic study using human evaluation on the tasks of\nopen-domain poem generation and open-domain long-form question answering. Our\nresults show that our proposed method substantially outperforms the baselines\nand that our generation quality is close to human performance on some of the\ntasks.\n  Narrators can try our poem generation demo at\nhttps://pretrain.aminer.cn/apps/poetry.html, while our QA demo can be found at\nhttps://pretrain.aminer.cn/app/qa. For researchers, the code is provided in\nhttps://github.com/THUDM/InversePrompting.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 08:36:52 GMT"}, {"version": "v2", "created": "Sat, 12 Jun 2021 17:51:13 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Zou", "Xu", ""], ["Yin", "Da", ""], ["Zhong", "Qingyang", ""], ["Ding", "Ming", ""], ["Yang", "Zhilin", ""], ["Tang", "Jie", ""]]}, {"id": "2103.10694", "submitter": "Amb Mis", "authors": "Sarika Jain and Archana Patel", "title": "Semantic Contextual Reasoning to Provide Human Behavior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, the world has witnessed various primitives pertaining to the\ncomplexity of human behavior. Identifying an event in the presence of\ninsufficient, incomplete, or tentative premises along with the constraints on\nresources such as time, data and memory is a vital aspect of an intelligent\nsystem. Data explosion presents one of the most challenging research issues for\nintelligent systems; to optimally represent and store this heterogeneous and\nvoluminous data semantically to provide human behavior. There is a requirement\nof intelligent but personalized human behavior subject to constraints on\nresources and priority of the user. Knowledge, when represented in the form of\nan ontology, procures an intelligent response to a query posed by users; but it\ndoes not offer content in accordance with the user context. To this aim, we\npropose a model to quantify the user context and provide semantic contextual\nreasoning. A diagnostic belief algorithm (DBA) is also presented that\nidentifies a given event and also computes the confidence of the decision as a\nfunction of available resources, premises, exceptions, and desired specificity.\nWe conduct an empirical study in the domain of day-to-day routine queries and\nthe experimental results show that the answer to queries and also its\nconfidence varies with user context.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 09:02:38 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Jain", "Sarika", ""], ["Patel", "Archana", ""]]}, {"id": "2103.10703", "submitter": "Petra Heck", "authors": "Petra Heck and Gerard Schouten", "title": "Lessons Learned from Educating AI Engineers", "comments": "Acccepted for the 1st International Workshop on AI Engineering\n  (WAIN21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past three years we have built a practice-oriented, bachelor level,\neducational programme for software engineers to specialize as AI engineers. The\nexperience with this programme and the practical assignments our students\nexecute in industry has given us valuable insights on the profession of AI\nengineer. In this paper we discuss our programme and the lessons learned for\nindustry and research.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 09:34:52 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Heck", "Petra", ""], ["Schouten", "Gerard", ""]]}, {"id": "2103.10741", "submitter": "Chandan Gautam", "authors": "Chandan Gautam, Sethupathy Parameswaran, Ashish Mishra, Suresh\n  Sundaram", "title": "Online Lifelong Generalized Zero-Shot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Methods proposed in the literature for zero-shot learning (ZSL) are typically\nsuitable for offline learning and cannot continually learn from sequential\nstreaming data. The sequential data comes in the form of tasks during training.\nRecently, a few attempts have been made to handle this issue and develop\ncontinual ZSL (CZSL) methods. However, these CZSL methods require clear\ntask-boundary information between the tasks during training, which is not\npractically possible. This paper proposes a task-free (i.e., task-agnostic)\nCZSL method, which does not require any task information during continual\nlearning. The proposed task-free CZSL method employs a variational autoencoder\n(VAE) for performing ZSL. To develop the CZSL method, we combine the concept of\nexperience replay with knowledge distillation and regularization. Here,\nknowledge distillation is performed using the training sample's dark knowledge,\nwhich essentially helps overcome the catastrophic forgetting issue. Further, it\nis enabled for task-free learning using short-term memory. Finally, a\nclassifier is trained on the synthetic features generated at the latent space\nof the VAE. Moreover, the experiments are conducted in a challenging and\npractical ZSL setup, i.e., generalized ZSL (GZSL). These experiments are\nconducted for two kinds of single-head continual learning settings: (i) mild\nsetting-: task-boundary is known only during training but not during testing;\n(ii) strict setting-: task-boundary is not known at training, as well as\ntesting. Experimental results on five benchmark datasets exhibit the validity\nof the approach for CZSL.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 11:24:05 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 03:05:03 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Gautam", "Chandan", ""], ["Parameswaran", "Sethupathy", ""], ["Mishra", "Ashish", ""], ["Sundaram", "Suresh", ""]]}, {"id": "2103.10763", "submitter": "Jiayan Pei", "authors": "Jiayan Pei, Yimin Wu, Zishan Qin, Yao Cong, Jingtao Guan", "title": "Attention-based model for predicting question relatedness on Stack\n  Overflow", "comments": "11 pages, 4 figures, IEEE/ACM MSR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stack Overflow is one of the most popular Programming Community-based\nQuestion Answering (PCQA) websites that has attracted more and more users in\nrecent years. When users raise or inquire questions in Stack Overflow,\nproviding related questions can help them solve problems. Although there are\nmany approaches based on deep learning that can automatically predict the\nrelatedness between questions, those approaches are limited since interaction\ninformation between two questions may be lost. In this paper, we adopt the deep\nlearning technique, propose an Attention-based Sentence pair Interaction Model\n(ASIM) to predict the relatedness between questions on Stack Overflow\nautomatically. We adopt the attention mechanism to capture the semantic\ninteraction information between the questions. Besides, we have pre-trained and\nreleased word embeddings specific to the software engineering domain for this\ntask, which may also help other related tasks. The experiment results\ndemonstrate that ASIM has made significant improvement over the baseline\napproaches in Precision, Recall, and Micro-F1 evaluation metrics, achieving\nstate-of-the-art performance in this task. Our model also performs well in the\nduplicate question detection task of AskUbuntu, which is a similar but\ndifferent task, proving its generalization and robustness.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 12:18:03 GMT"}, {"version": "v2", "created": "Mon, 22 Mar 2021 09:12:02 GMT"}, {"version": "v3", "created": "Thu, 25 Mar 2021 13:06:57 GMT"}, {"version": "v4", "created": "Sat, 27 Mar 2021 06:44:49 GMT"}, {"version": "v5", "created": "Thu, 1 Apr 2021 11:57:51 GMT"}, {"version": "v6", "created": "Mon, 5 Apr 2021 10:37:13 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Pei", "Jiayan", ""], ["Wu", "Yimin", ""], ["Qin", "Zishan", ""], ["Cong", "Yao", ""], ["Guan", "Jingtao", ""]]}, {"id": "2103.10790", "submitter": "Adam Katona", "authors": "Adam Katona, Daniel W. Franks, James Alfred Walker", "title": "Quality Evolvability ES: Evolving Individuals With a Distribution of\n  Well Performing and Diverse Offspring", "comments": "2021 Conference on Artificial Life", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most important lessons from the success of deep learning is that\nlearned representations tend to perform much better at any task compared to\nrepresentations we design by hand. Yet evolution of evolvability algorithms,\nwhich aim to automatically learn good genetic representations, have received\nrelatively little attention, perhaps because of the large amount of\ncomputational power they require. The recent method Evolvability ES allows\ndirect selection for evolvability with little computation. However, it can only\nbe used to solve problems where evolvability and task performance are aligned.\nWe propose Quality Evolvability ES, a method that simultaneously optimizes for\ntask performance and evolvability and without this restriction. Our proposed\napproach Quality Evolvability has similar motivation to Quality Diversity\nalgorithms, but with some important differences. While Quality Diversity aims\nto find an archive of diverse and well-performing, but potentially genetically\ndistant individuals, Quality Evolvability aims to find a single individual with\na diverse and well-performing distribution of offspring. By doing so Quality\nEvolvability is forced to discover more evolvable representations. We\ndemonstrate on robotic locomotion control tasks that Quality Evolvability ES,\nsimilarly to Quality Diversity methods, can learn faster than objective-based\nmethods and can handle deceptive problems.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 13:22:22 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 11:28:41 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Katona", "Adam", ""], ["Franks", "Daniel W.", ""], ["Walker", "James Alfred", ""]]}, {"id": "2103.10798", "submitter": "Sicheng Zhao", "authors": "Sicheng Zhao, Quanwei Huang, Youbao Tang, Xingxu Yao, Jufeng Yang,\n  Guiguang Ding, Bj\\\"orn W. Schuller", "title": "Computational Emotion Analysis From Images: Recent Advances and Future\n  Directions", "comments": "Accepted chapter in the book \"Human Perception of Visual Information\n  Psychological and Computational Perspective\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.HC cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotions are usually evoked in humans by images. Recently, extensive research\nefforts have been dedicated to understanding the emotions of images. In this\nchapter, we aim to introduce image emotion analysis (IEA) from a computational\nperspective with the focus on summarizing recent advances and suggesting future\ndirections. We begin with commonly used emotion representation models from\npsychology. We then define the key computational problems that the researchers\nhave been trying to solve and provide supervised frameworks that are generally\nused for different IEA tasks. After the introduction of major challenges in\nIEA, we present some representative methods on emotion feature extraction,\nsupervised classifier learning, and domain adaptation. Furthermore, we\nintroduce available datasets for evaluation and summarize some main results.\nFinally, we discuss some open questions and future directions that researchers\ncan pursue.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 13:33:34 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Zhao", "Sicheng", ""], ["Huang", "Quanwei", ""], ["Tang", "Youbao", ""], ["Yao", "Xingxu", ""], ["Yang", "Jufeng", ""], ["Ding", "Guiguang", ""], ["Schuller", "Bj\u00f6rn W.", ""]]}, {"id": "2103.10805", "submitter": "Lukas Harsch", "authors": "Lukas Harsch, Johannes Burgbacher, Stefan Riedelbauch", "title": "Transferable Model for Shape Optimization subject to Physical\n  Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI physics.flu-dyn", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The interaction of neural networks with physical equations offers a wide\nrange of applications. We provide a method which enables a neural network to\ntransform objects subject to given physical constraints. Therefore an U-Net\narchitecture is used to learn the underlying physical behaviour of fluid flows.\nThe network is used to infer the solution of flow simulations, which will be\nshown for a wide range of generic channel flow simulations. Physical meaningful\nquantities can be computed on the obtained solution, e.g. the total pressure\ndifference or the forces on the objects. A Spatial Transformer Network with\nthin-plate-splines is used for the interaction between the physical constraints\nand the geometric representation of the objects. Thus, a transformation from an\ninitial to a target geometry is performed such that the object is fulfilling\nthe given constraints. This method is fully differentiable i.e., gradient\ninformations can be used for the transformation. This can be seen as an inverse\ndesign process. The advantage of this method over many other proposed methods\nis, that the physical constraints are based on the inferred flow field\nsolution. Thus, we have a transferable model which can be applied to varying\nproblem setups and is not limited to a given set of geometry parameters or\nphysical quantities.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 13:49:21 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Harsch", "Lukas", ""], ["Burgbacher", "Johannes", ""], ["Riedelbauch", "Stefan", ""]]}, {"id": "2103.10844", "submitter": "David Fernandez-Llorca", "authors": "David Fern\\'andez Llorca", "title": "From driving automation systems to autonomous vehicles: clarifying the\n  terminology", "comments": "6 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The terminological landscape is rather cluttered when referring to autonomous\ndriving or vehicles. A plethora of terms are used interchangeably, leading to\nmisuse and confusion. With its technological, social and legal progress, it is\nincreasingly imperative to establish a clear terminology that allows each\nconcept to be placed in its corresponding place.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 14:53:15 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Llorca", "David Fern\u00e1ndez", ""]]}, {"id": "2103.10897", "submitter": "Gaurav Mahajan", "authors": "Simon S. Du, Sham M. Kakade, Jason D. Lee, Shachar Lovett, Gaurav\n  Mahajan, Wen Sun and Ruosong Wang", "title": "Bilinear Classes: A Structural Framework for Provable Generalization in\n  RL", "comments": "Expanded extension section to include generalized linear bellman\n  complete and changed related work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work introduces Bilinear Classes, a new structural framework, which\npermit generalization in reinforcement learning in a wide variety of settings\nthrough the use of function approximation. The framework incorporates nearly\nall existing models in which a polynomial sample complexity is achievable, and,\nnotably, also includes new models, such as the Linear $Q^*/V^*$ model in which\nboth the optimal $Q$-function and the optimal $V$-function are linear in some\nknown feature space. Our main result provides an RL algorithm which has\npolynomial sample complexity for Bilinear Classes; notably, this sample\ncomplexity is stated in terms of a reduction to the generalization error of an\nunderlying supervised learning sub-problem. These bounds nearly match the best\nknown sample complexity bounds for existing models. Furthermore, this framework\nalso extends to the infinite dimensional (RKHS) setting: for the the Linear\n$Q^*/V^*$ model, linear MDPs, and linear mixture MDPs, we provide sample\ncomplexities that have no explicit dependence on the explicit feature dimension\n(which could be infinite), but instead depends only on information theoretic\nquantities.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 16:34:20 GMT"}, {"version": "v2", "created": "Fri, 4 Jun 2021 06:29:35 GMT"}, {"version": "v3", "created": "Sun, 11 Jul 2021 22:29:02 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Du", "Simon S.", ""], ["Kakade", "Sham M.", ""], ["Lee", "Jason D.", ""], ["Lovett", "Shachar", ""], ["Mahajan", "Gaurav", ""], ["Sun", "Wen", ""], ["Wang", "Ruosong", ""]]}, {"id": "2103.10909", "submitter": "Xiaoyu Yang", "authors": "Xiaoyu Yang and Huiyun Li", "title": "IA Planner: Motion Planning Using Instantaneous Analysis for Autonomous\n  Vehicle in the Dense Dynamic Scenarios on Highways", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In dense and dynamic scenarios, planning a safe and comfortable trajectory is\nfull of challenges when traffic participants are driving at high speed. The\nclassic graph search and sampling methods first perform path planning and then\nconfigure the corresponding speed, which lacks a strategy to deal with the\nhigh-speed obstacles. Decoupling optimization methods perform motion planning\nin the S-L and S-T domains respectively. These methods require a large free\nconfiguration space to plan the lane change trajectory. In dense dynamic\nscenes, it is easy to cause the failure of trajectory planning and be cut in by\nothers, causing slow driving speed and bring safety hazards. We analyze the\ncollision relationship in the spatio-temporal domain, and propose an\ninstantaneous analysis model which only analyzes the collision relationship at\nthe same time. In the model, the collision-free constraints in 3D\nspatio-temporal domain is projected to the 2D space domain to remove redundant\nconstraints and reduce computational complexity. Experimental results show that\nour method can plan a safe and comfortable lane-changing trajectory in dense\ndynamic scenarios. At the same time, it improves traffic efficiency and\nincreases ride comfort.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 17:10:50 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Yang", "Xiaoyu", ""], ["Li", "Huiyun", ""]]}, {"id": "2103.10911", "submitter": "Kaoutar El Maghraoui", "authors": "Kauotar El Maghraoui and Lorraine M. Herger and Chekuri Choudary and\n  Kim Tran and Todd Deshane and David Hanson", "title": "Performance Analysis of Deep Learning Workloads on a Composable System", "comments": "Submitted to IPDPS ScaDL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A composable infrastructure is defined as resources, such as compute,\nstorage, accelerators and networking, that are shared in a pool and that can be\ngrouped in various configurations to meet application requirements. This\nfreedom to 'mix and match' resources dynamically allows for experimentation\nearly in the design cycle, prior to the final architectural design or hardware\nimplementation of a system. This design provides flexibility to serve a variety\nof workloads and provides a dynamic co-design platform that allows experiments\nand measurements in a controlled manner. For instance, key performance\nbottlenecks can be revealed early on in the experimentation phase thus avoiding\ncostly and time consuming mistakes. Additionally, various system-level\ntopologies can be evaluated when experimenting with new System on Chip (SoCs)\nand new accelerator types. This paper details the design of an enterprise\ncomposable infrastructure that we have implemented and made available to our\npartners in the IBM Research AI Hardware Center (AIHC). Our experimental\nevaluations on the composable system give insights into how the system works\nand evaluates the impact of various resource aggregations and reconfigurations\non representative deep learning benchmarks.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 17:15:42 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Maghraoui", "Kauotar El", ""], ["Herger", "Lorraine M.", ""], ["Choudary", "Chekuri", ""], ["Tran", "Kim", ""], ["Deshane", "Todd", ""], ["Hanson", "David", ""]]}, {"id": "2103.10928", "submitter": "Yun Zhao", "authors": "Yun Zhao, Qinghang Hong, Xinlu Zhang, Yu Deng, Yuqing Wang, and Linda\n  Petzold", "title": "BERTSurv: BERT-Based Survival Models for Predicting Outcomes of Trauma\n  Patients", "comments": "ICDM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Survival analysis is a technique to predict the times of specific outcomes,\nand is widely used in predicting the outcomes for intensive care unit (ICU)\ntrauma patients. Recently, deep learning models have drawn increasing attention\nin healthcare. However, there is a lack of deep learning methods that can model\nthe relationship between measurements, clinical notes and mortality outcomes.\nIn this paper we introduce BERTSurv, a deep learning survival framework which\napplies Bidirectional Encoder Representations from Transformers (BERT) as a\nlanguage representation model on unstructured clinical notes, for mortality\nprediction and survival analysis. We also incorporate clinical measurements in\nBERTSurv. With binary cross-entropy (BCE) loss, BERTSurv can predict mortality\nas a binary outcome (mortality prediction). With partial log-likelihood (PLL)\nloss, BERTSurv predicts the probability of mortality as a time-to-event outcome\n(survival analysis). We apply BERTSurv on Medical Information Mart for\nIntensive Care III (MIMIC III) trauma patient data. For mortality prediction,\nBERTSurv obtained an area under the curve of receiver operating characteristic\ncurve (AUC-ROC) of 0.86, which is an improvement of 3.6% over baseline of\nmultilayer perceptron (MLP) without notes. For survival analysis, BERTSurv\nachieved a concordance index (C-index) of 0.7. In addition, visualizations of\nBERT's attention heads help to extract patterns in clinical notes and improve\nmodel interpretability by showing how the model assigns weights to different\ninputs.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 17:49:00 GMT"}], "update_date": "2021-03-22", "authors_parsed": [["Zhao", "Yun", ""], ["Hong", "Qinghang", ""], ["Zhang", "Xinlu", ""], ["Deng", "Yu", ""], ["Wang", "Yuqing", ""], ["Petzold", "Linda", ""]]}, {"id": "2103.10929", "submitter": "Yun Zhao", "authors": "Yuqing Wang, Yun Zhao, Rachael Callcut, and Linda Petzold", "title": "Empirical Analysis of Machine Learning Configurations for Prediction of\n  Multiple Organ Failure in Trauma Patients", "comments": "ICDM 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Multiple organ failure (MOF) is a life-threatening condition. Due to its\nurgency and high mortality rate, early detection is critical for clinicians to\nprovide appropriate treatment. In this paper, we perform quantitative analysis\non early MOF prediction with comprehensive machine learning (ML)\nconfigurations, including data preprocessing (missing value treatment, label\nbalancing, feature scaling), feature selection, classifier choice, and\nhyperparameter tuning. Results show that classifier choice impacts both the\nperformance improvement and variation most among all the configurations. In\ngeneral, complex classifiers including ensemble methods can provide better\nperformance than simple classifiers. However, blindly pursuing complex\nclassifiers is unwise as it also brings the risk of greater performance\nvariation.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 17:49:22 GMT"}, {"version": "v2", "created": "Thu, 22 Apr 2021 19:57:06 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Wang", "Yuqing", ""], ["Zhao", "Yun", ""], ["Callcut", "Rachael", ""], ["Petzold", "Linda", ""]]}, {"id": "2103.10951", "submitter": "David Bau iii", "authors": "David Bau, Alex Andonian, Audrey Cui, YeonHwan Park, Ali Jahanian,\n  Aude Oliva, Antonio Torralba", "title": "Paint by Word", "comments": "10 pages, 9 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of zero-shot semantic image painting. Instead of\npainting modifications into an image using only concrete colors or a finite set\nof semantic concepts, we ask how to create semantic paint based on open\nfull-text descriptions: our goal is to be able to point to a location in a\nsynthesized image and apply an arbitrary new concept such as \"rustic\" or\n\"opulent\" or \"happy dog.\" To do this, our method combines a state-of-the art\ngenerative model of realistic images with a state-of-the-art text-image\nsemantic similarity network. We find that, to make large changes, it is\nimportant to use non-gradient methods to explore latent space, and it is\nimportant to relax the computations of the GAN to target changes to a specific\nregion. We conduct user studies to compare our methods to several baselines.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 17:59:08 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 05:46:17 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Bau", "David", ""], ["Andonian", "Alex", ""], ["Cui", "Audrey", ""], ["Park", "YeonHwan", ""], ["Jahanian", "Ali", ""], ["Oliva", "Aude", ""], ["Torralba", "Antonio", ""]]}, {"id": "2103.10972", "submitter": "Yuchen Lu", "authors": "Yuchen Lu, Yikang Shen, Siyuan Zhou, Aaron Courville, Joshua B.\n  Tenenbaum, Chuang Gan", "title": "Learning Task Decomposition with Ordered Memory Policy Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Many complex real-world tasks are composed of several levels of sub-tasks.\nHumans leverage these hierarchical structures to accelerate the learning\nprocess and achieve better generalization. In this work, we study the inductive\nbias and propose Ordered Memory Policy Network (OMPN) to discover subtask\nhierarchy by learning from demonstration. The discovered subtask hierarchy\ncould be used to perform task decomposition, recovering the subtask boundaries\nin an unstruc-tured demonstration. Experiments on Craft and Dial demonstrate\nthat our modelcan achieve higher task decomposition performance under both\nunsupervised and weakly supervised settings, comparing with strong baselines.\nOMPN can also bedirectly applied to partially observable environments and still\nachieve higher task decomposition performance. Our visualization further\nconfirms that the subtask hierarchy can emerge in our model.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 18:13:35 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Lu", "Yuchen", ""], ["Shen", "Yikang", ""], ["Zhou", "Siyuan", ""], ["Courville", "Aaron", ""], ["Tenenbaum", "Joshua B.", ""], ["Gan", "Chuang", ""]]}, {"id": "2103.10975", "submitter": "Kevin Luna", "authors": "Kevin Luna, Katherine Klymko, Johannes P. Blaschke", "title": "Accelerating GMRES with Deep Learning in Real-Time", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GMRES is a powerful numerical solver used to find solutions to extremely\nlarge systems of linear equations. These systems of equations appear in many\napplications in science and engineering. Here we demonstrate a real-time\nmachine learning algorithm that can be used to accelerate the time-to-solution\nfor GMRES. Our framework is novel in that is integrates the deep learning\nalgorithm in an in situ fashion: the AI-accelerator gradually learns how to\noptimizes the time to solution without requiring user input (such as a\npre-trained data set). We describe how our algorithm collects data and\noptimizes GMRES. We demonstrate our algorithm by implementing an accelerated\n(MLGMRES) solver in Python. We then use MLGMRES to accelerate a solver for the\nPoisson equation -- a class of linear problems that appears in may\napplications.\n  Informed by the properties of formal solutions to the Poisson equation, we\ntest the performance of different neural networks. Our key takeaway is that\nnetworks which are capable of learning non-local relationships perform well,\nwithout needing to be scaled with the input problem size, making them good\ncandidates for the extremely large problems encountered in high-performance\ncomputing. For the inputs studied, our method provides a roughly 2$\\times$\nacceleration.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 18:21:38 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Luna", "Kevin", ""], ["Klymko", "Katherine", ""], ["Blaschke", "Johannes P.", ""]]}, {"id": "2103.11006", "submitter": "Mariano Rivera", "authors": "Hanna Ehrlich and Mariano Rivera", "title": "AxonNet: A self-supervised Deep Neural Network for Intravoxel Structure\n  Estimation from DW-MRI", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We present a method for estimating intravoxel parameters from a DW-MRI based\non deep learning techniques. We show that neural networks (DNNs) have the\npotential to extract information from diffusion-weighted signals to reconstruct\ncerebral tracts. We present two DNN models: one that estimates the axonal\nstructure in the form of a voxel and the other to calculate the structure of\nthe central voxel using the voxel neighborhood. Our methods are based on a\nproposed parameter representation suitable for the problem. Since it is\npractically impossible to have real tagged data for any acquisition protocol,\nwe used a self-supervised strategy. Experiments with synthetic data and real\ndata show that our approach is competitive, and the computational times show\nthat our approach is faster than the SOTA methods, even if training times are\nconsidered. This computational advantage increases if we consider the\nprediction of multiple images with the same acquisition protocol.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 20:11:03 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Ehrlich", "Hanna", ""], ["Rivera", "Mariano", ""]]}, {"id": "2103.11011", "submitter": "Dani Kiyasseh", "authors": "Dani Kiyasseh, Tingting Zhu, David Clifton", "title": "Let Your Heart Speak in its Mother Tongue: Multilingual Captioning of\n  Cardiac Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Cardiac signals, such as the electrocardiogram, convey a significant amount\nof information about the health status of a patient which is typically\nsummarized by a clinician in the form of a clinical report, a cumbersome\nprocess that is prone to errors. To streamline this routine process, we propose\na deep neural network capable of captioning cardiac signals; it receives a\ncardiac signal as input and generates a clinical report as output. We extend\nthis further to generate multilingual reports. To that end, we create and make\npublicly available a multilingual clinical report dataset. In the absence of\nsufficient labelled data, deep neural networks can benefit from a warm-start,\nor pre-training, procedure in which parameters are first learned in an\narbitrary task. We propose such a task in the form of discriminative\nmultilingual pre-training where tokens from clinical reports are randomly\nreplaced with those from other languages and the network is tasked with\npredicting the language of all tokens. We show that our method performs on par\nwith state-of-the-art pre-training methods such as MLM, ELECTRA, and MARGE,\nwhile simultaneously generating diverse and plausible clinical reports. We also\ndemonstrate that multilingual models can outperform their monolingual\ncounterparts, informally terming this beneficial phenomenon as the blessing of\nmultilinguality.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 20:30:13 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Kiyasseh", "Dani", ""], ["Zhu", "Tingting", ""], ["Clifton", "David", ""]]}, {"id": "2103.11018", "submitter": "Curtis Bright", "authors": "Noah Rubin, Curtis Bright, Kevin K. H. Cheung, Brett Stevens", "title": "Integer and Constraint Programming Revisited for Mutually Orthogonal\n  Latin Squares", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.AI math.CO math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide results on using integer programming (IP) and\nconstraint programming (CP) to search for sets of mutually orthogonal latin\nsquares (MOLS). Both programming paradigms have previously successfully been\nused to search for MOLS, but solvers for IP and CP solvers have significantly\nimproved in recent years and data on how modern IP and CP solvers perform on\nthe MOLS problem is lacking. Using state-of-the-art solvers as black boxes we\nwere able to quickly find pairs of MOLS (or prove their nonexistence) in all\norders up to ten. Moreover, we improve the effectiveness of the solvers by\nformulating an extended symmetry breaking method as well as an improvement to\nthe straightforward CP encoding. We also analyze the effectiveness of using CP\nand IP solvers to search for triples of MOLS, compare our timings to those\nwhich have been previously published, and estimate the running time of using\nthis approach to resolve the longstanding open problem of determining the\nexistence of a triple of MOLS of order ten.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 20:45:55 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Rubin", "Noah", ""], ["Bright", "Curtis", ""], ["Cheung", "Kevin K. H.", ""], ["Stevens", "Brett", ""]]}, {"id": "2103.11029", "submitter": "Denis Newman-Griffis", "authors": "Denis Newman-Griffis, Venkatesh Sivaraman, Adam Perer, Eric\n  Fosler-Lussier, Harry Hochheiser", "title": "TextEssence: A Tool for Interactive Analysis of Semantic Shifts Between\n  Corpora", "comments": "Accepted as a Systems Demonstration at NAACL-HLT 2021. Video\n  demonstration at https://youtu.be/1xEEfsMwL0k", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Embeddings of words and concepts capture syntactic and semantic regularities\nof language; however, they have seen limited use as tools to study\ncharacteristics of different corpora and how they relate to one another. We\nintroduce TextEssence, an interactive system designed to enable comparative\nanalysis of corpora using embeddings. TextEssence includes visual,\nneighbor-based, and similarity-based modes of embedding analysis in a\nlightweight, web-based interface. We further propose a new measure of embedding\nconfidence based on nearest neighborhood overlap, to assist in identifying\nhigh-quality embeddings for corpus analysis. A case study on COVID-19\nscientific literature illustrates the utility of the system. TextEssence is\navailable from https://github.com/drgriffis/text-essence.\n", "versions": [{"version": "v1", "created": "Fri, 19 Mar 2021 21:26:28 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Newman-Griffis", "Denis", ""], ["Sivaraman", "Venkatesh", ""], ["Perer", "Adam", ""], ["Fosler-Lussier", "Eric", ""], ["Hochheiser", "Harry", ""]]}, {"id": "2103.11072", "submitter": "Siwen Luo", "authors": "Siwen Luo and Hamish Ivison and Caren Han and Josiah Poon", "title": "Local Interpretations for Explainable Natural Language Processing: A\n  Survey", "comments": "This work is an initial draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As the use of deep learning techniques has grown across various fields over\nthe past decade, complaints about the opaqueness of the black-box models have\nincreased, resulting in an increased focus on transparency in deep learning\nmodels. This work investigates various methods to improve the interpretability\nof deep neural networks for natural language processing (NLP) tasks, including\nmachine translation and sentiment analysis. We provide a comprehensive\ndiscussion on the definition of the term \\textit{interpretability} and its\nvarious aspects at the beginning of this work. The methods collected and\nsummarised in this survey are only associated with local interpretation and are\ndivided into three categories: 1) explaining the model's predictions through\nrelated input features; 2) explaining through natural language explanation; 3)\nprobing the hidden states of models and word representations.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 02:28:33 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Luo", "Siwen", ""], ["Ivison", "Hamish", ""], ["Han", "Caren", ""], ["Poon", "Josiah", ""]]}, {"id": "2103.11083", "submitter": "Hong-Ning Dai Prof.", "authors": "Ke Zhang, Hanbo Ying, Hong-Ning Dai, Lin Li, Yuangyuang Peng, Keyi\n  Guo, Hongfang Yu", "title": "Compacting Deep Neural Networks for Internet of Things: Methods and\n  Applications", "comments": "25 pages, 11 figures", "journal-ref": "IEEE Internet of Things Journal, 2021", "doi": "10.1109/JIOT.2021.3063497", "report-no": null, "categories": "cs.LG cs.AI cs.NI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Deep Neural Networks (DNNs) have shown great success in completing complex\ntasks. However, DNNs inevitably bring high computational cost and storage\nconsumption due to the complexity of hierarchical structures, thereby hindering\ntheir wide deployment in Internet-of-Things (IoT) devices, which have limited\ncomputational capability and storage capacity. Therefore, it is a necessity to\ninvestigate the technologies to compact DNNs. Despite tremendous advances in\ncompacting DNNs, few surveys summarize compacting-DNNs technologies, especially\nfor IoT applications. Hence, this paper presents a comprehensive study on\ncompacting-DNNs technologies. We categorize compacting-DNNs technologies into\nthree major types: 1) network model compression, 2) Knowledge Distillation\n(KD), 3) modification of network structures. We also elaborate on the diversity\nof these approaches and make side-by-side comparisons. Moreover, we discuss the\napplications of compacted DNNs in various IoT applications and outline future\ndirections.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 03:18:42 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Zhang", "Ke", ""], ["Ying", "Hanbo", ""], ["Dai", "Hong-Ning", ""], ["Li", "Lin", ""], ["Peng", "Yuangyuang", ""], ["Guo", "Keyi", ""], ["Yu", "Hongfang", ""]]}, {"id": "2103.11089", "submitter": "Liangyou Li", "authors": "Liangyou Li and Andy Way and Qun Liu", "title": "Dependency Graph-to-String Statistical Machine Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present graph-based translation models which translate source graphs into\ntarget strings. Source graphs are constructed from dependency trees with extra\nlinks so that non-syntactic phrases are connected. Inspired by phrase-based\nmodels, we first introduce a translation model which segments a graph into a\nsequence of disjoint subgraphs and generates a translation by combining\nsubgraph translations left-to-right using beam search. However, similar to\nphrase-based models, this model is weak at phrase reordering. Therefore, we\nfurther introduce a model based on a synchronous node replacement grammar which\nlearns recursive translation rules. We provide two implementations of the model\nwith different restrictions so that source graphs can be parsed efficiently.\nExperiments on Chinese--English and German--English show that our graph-based\nmodels are significantly better than corresponding sequence- and tree-based\nbaselines.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 04:20:56 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Li", "Liangyou", ""], ["Way", "Andy", ""], ["Liu", "Qun", ""]]}, {"id": "2103.11139", "submitter": "Yang Liu", "authors": "Yang Liu, Fei Wang, Baigui Sun, Hao Li", "title": "MogFace: Rethinking Scale Augmentation on the Face Detector", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Face detector frequently confronts extreme scale variance challenge. The\nfamous solutions are Multi-scale training, Data-anchor-sampling and Random crop\nstrategy. In this paper, we indicate 2 significant elements to resolve extreme\nscale variance problem by investigating the difference among the previous\nsolutions, including the fore-ground and back-ground information of an image\nand the scale information. However, current excellent solutions can only\nutilize the former information while neglecting to absorb the latter one\neffectively. In order to help the detector utilize the scale information\nefficiently, we analyze the relationship between the detector performance and\nthe scale distribution of the training data. Based on this analysis, we propose\na Selective Scale Enhancement (SSE) strategy which can assimilate these two\ninformation efficiently and simultaneously. Finally, our method achieves\nstate-of-the-art detection performance on all common face detection benchmarks,\nincluding AFW, PASCAL face, FDDB and Wider Face datasets. Note that our result\nachieves six champions on the Wider Face dataset.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 09:17:04 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 03:08:44 GMT"}, {"version": "v3", "created": "Mon, 29 Mar 2021 07:32:03 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Liu", "Yang", ""], ["Wang", "Fei", ""], ["Sun", "Baigui", ""], ["Li", "Hao", ""]]}, {"id": "2103.11144", "submitter": "Carmel Rabinovitz", "authors": "Carmel Rabinovitz, Niko Grupen and Aviv Tamar", "title": "Unsupervised Feature Learning for Manipulation with Contrastive Domain\n  Randomization", "comments": "Accepted to ICRA 2021, code can be found at\n  https://github.com/carmelrabinov/cdr", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Robotic tasks such as manipulation with visual inputs require image features\nthat capture the physical properties of the scene, e.g., the position and\nconfiguration of objects. Recently, it has been suggested to learn such\nfeatures in an unsupervised manner from simulated, self-supervised, robot\ninteraction; the idea being that high-level physical properties are well\ncaptured by modern physical simulators, and their representation from visual\ninputs may transfer well to the real world. In particular, learning methods\nbased on noise contrastive estimation have shown promising results. To\nrobustify the simulation-to-real transfer, domain randomization (DR) was\nsuggested for learning features that are invariant to irrelevant visual\nproperties such as textures or lighting. In this work, however, we show that a\nnaive application of DR to unsupervised learning based on contrastive\nestimation does not promote invariance, as the loss function maximizes mutual\ninformation between the features and both the relevant and irrelevant visual\nproperties. We propose a simple modification of the contrastive loss to fix\nthis, exploiting the fact that we can control the simulated randomization of\nvisual properties. Our approach learns physical features that are significantly\nmore robust to visual domain variation, as we demonstrate using both rigid and\nnon-rigid objects.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 09:54:45 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 12:52:56 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Rabinovitz", "Carmel", ""], ["Grupen", "Niko", ""], ["Tamar", "Aviv", ""]]}, {"id": "2103.11148", "submitter": "George Papakostas Prof.", "authors": "G.G. Samatas, S.S. Moumgiakmas, G.A. Papakostas", "title": "Predictive Maintenance -- Bridging Artificial Intelligence and IoT", "comments": "7 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NI cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper highlights the trends in the field of predictive maintenance with\nthe use of machine learning. With the continuous development of the Fourth\nIndustrial Revolution, through IoT, the technologies that use artificial\nintelligence are evolving. As a result, industries have been using these\ntechnologies to optimize their production. Through scientific research\nconducted for this paper, conclusions were drawn about the trends in Predictive\nMaintenance applications with the use of machine learning bridging Artificial\nIntelligence and IoT. These trends are related to the types of industries in\nwhich Predictive Maintenance was applied, the models of artificial intelligence\nwere implemented, mainly of machine learning and the types of sensors that are\napplied through the IoT to the applications. Six sectors were presented and the\nproduction sector was dominant as it accounted for 54.54% of total\npublications. In terms of artificial intelligence models, the most prevalent\namong ten were the Artificial Neural Networks, Support Vector Machine and\nRandom Forest with 27.84%, 17.72% and 13.92% respectively. Finally, twelve\ncategories of sensors emerged, of which the most widely used were the sensors\nof temperature and vibration with percentages of 60.71% and 46.42%\ncorrespondingly.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 10:01:40 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 17:50:03 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Samatas", "G. G.", ""], ["Moumgiakmas", "S. S.", ""], ["Papakostas", "G. A.", ""]]}, {"id": "2103.11155", "submitter": "Junchi Yu", "authors": "Junchi Yu, Tingyang Xu, Yu Rong, Yatao Bian, Junzhou Huang, Ran He", "title": "Recognizing Predictive Substructures with Subgraph Information\n  Bottleneck", "comments": "arXiv admin note: substantial text overlap with arXiv:2010.05563", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The emergence of Graph Convolutional Network (GCN) has greatly boosted the\nprogress of graph learning. However, two disturbing factors, noise and\nredundancy in graph data, and lack of interpretation for prediction results,\nimpede further development of GCN. One solution is to recognize a predictive\nyet compressed subgraph to get rid of the noise and redundancy and obtain the\ninterpretable part of the graph. This setting of subgraph is similar to the\ninformation bottleneck (IB) principle, which is less studied on\ngraph-structured data and GCN. Inspired by the IB principle, we propose a novel\nsubgraph information bottleneck (SIB) framework to recognize such subgraphs,\nnamed IB-subgraph. However, the intractability of mutual information and the\ndiscrete nature of graph data makes the objective of SIB notoriously hard to\noptimize. To this end, we introduce a bilevel optimization scheme coupled with\na mutual information estimator for irregular graphs. Moreover, we propose a\ncontinuous relaxation for subgraph selection with a connectivity loss for\nstabilization. We further theoretically prove the error bound of our estimation\nscheme for mutual information and the noise-invariant nature of IB-subgraph.\nExtensive experiments on graph learning and large-scale point cloud tasks\ndemonstrate the superior property of IB-subgraph.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 11:19:43 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Yu", "Junchi", ""], ["Xu", "Tingyang", ""], ["Rong", "Yu", ""], ["Bian", "Yatao", ""], ["Huang", "Junzhou", ""], ["He", "Ran", ""]]}, {"id": "2103.11161", "submitter": "Sinisa Stekovic", "authors": "Sinisa Stekovic, Mahdi Rad, Friedrich Fraundorfer, Vincent Lepetit", "title": "MonteFloor: Extending MCTS for Reconstructing Accurate Large-Scale Floor\n  Plans", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel method for reconstructing floor plans from noisy 3D point\nclouds. Our main contribution is a principled approach that relies on the Monte\nCarlo Tree Search (MCTS) algorithm to maximize a suitable objective function\nefficiently despite the complexity of the problem. Like previous work, we first\nproject the input point cloud to a top view to create a density map and extract\nroom proposals from it. Our method selects and optimizes the polygonal shapes\nof these room proposals jointly to fit the density map and outputs an accurate\nvectorized floor map even for large complex scenes. To do this, we adapted\nMCTS, an algorithm originally designed to learn to play games, to select the\nroom proposals by maximizing an objective function combining the fitness with\nthe density map as predicted by a deep network and regularizing terms on the\nroom shapes. We also introduce a refinement step to MCTS that adjusts the shape\nof the room proposals. For this step, we propose a novel differentiable method\nfor rendering the polygonal shapes of these proposals. We evaluate our method\non the recent and challenging Structured3D and Floor-SP datasets and show a\nsignificant improvement over the state-of-the-art, without imposing any hard\nconstraints nor assumptions on the floor plan configurations.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 11:36:49 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Stekovic", "Sinisa", ""], ["Rad", "Mahdi", ""], ["Fraundorfer", "Friedrich", ""], ["Lepetit", "Vincent", ""]]}, {"id": "2103.11168", "submitter": "Jinwook Huh", "authors": "Jinwook Huh, Daniel D. Lee and Volkan Isler", "title": "Learning Continuous Cost-to-Go Functions for Non-holonomic Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a supervised learning method to generate continuous\ncost-to-go functions of non-holonomic systems directly from the workspace\ndescription. Supervision from informative examples reduces training time and\nimproves network performance. The manifold representing the optimal\ntrajectories of a non-holonomic system has high-curvature regions which can not\nbe efficiently captured with uniform sampling. To address this challenge, we\npresent an adaptive sampling method which makes use of sampling-based planners\nalong with local, closed-form solutions to generate training samples. The\ncost-to-go function over a specific workspace is represented as a neural\nnetwork whose weights are generated by a second, higher order network. The\nnetworks are trained in an end-to-end fashion. In our previous work, this\narchitecture was shown to successfully learn to generate the cost-to-go\nfunctions of holonomic systems using uniform sampling. In this work, we show\nthat uniform sampling fails for non-holonomic systems. However, with the\nproposed adaptive sampling methodology, our network can generate near-optimal\ntrajectories for non-holonomic systems while avoiding obstacles. Experiments\nshow that our method is two orders of magnitude faster compared to traditional\napproaches in cluttered environments.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 12:31:08 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Huh", "Jinwook", ""], ["Lee", "Daniel D.", ""], ["Isler", "Volkan", ""]]}, {"id": "2103.11177", "submitter": "Xianqi Chen", "authors": "Xianqi Chen (1 and 2), Xiaoyu Zhao (2), Zhiqiang Gong (2), Jun Zhang\n  (2), Weien Zhou (2), Xiaoqian Chen (2), Wen Yao (2) ((1) College of Aerospace\n  Science and Engineering, National University of Defense Technology, (2)\n  National Innovation Institute of Defense Technology, Chinese Academy of\n  Military Science)", "title": "A Deep Neural Network Surrogate Modeling Benchmark for Temperature Field\n  Prediction of Heat Source Layout", "comments": "31 pages, 25 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Thermal issue is of great importance during layout design of heat source\ncomponents in systems engineering, especially for high functional-density\nproducts. Thermal analysis generally needs complex simulation, which leads to\nan unaffordable computational burden to layout optimization as it iteratively\nevaluates different schemes. Surrogate modeling is an effective way to\nalleviate computation complexity. However, temperature field prediction (TFP)\nwith complex heat source layout (HSL) input is an ultra-high dimensional\nnonlinear regression problem, which brings great difficulty to traditional\nregression models. The Deep neural network (DNN) regression method is a\nfeasible way for its good approximation performance. However, it faces great\nchallenges in both data preparation for sample diversity and uniformity in the\nlayout space with physical constraints, and proper DNN model selection and\ntraining for good generality, which necessitates efforts of both layout\ndesigner and DNN experts. To advance this cross-domain research, this paper\nproposes a DNN based HSL-TFP surrogate modeling task benchmark. With\nconsideration for engineering applicability, sample generation, dataset\nevaluation, DNN model, and surrogate performance metrics, are thoroughly\nstudied. Experiments are conducted with ten representative state-of-the-art DNN\nmodels. Detailed discussion on baseline results is provided and future\nprospects are analyzed for DNN based HSL-TFP tasks.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 13:26:21 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Chen", "Xianqi", "", "1 and 2"], ["Zhao", "Xiaoyu", ""], ["Gong", "Zhiqiang", ""], ["Zhang", "Jun", ""], ["Zhou", "Weien", ""], ["Chen", "Xiaoqian", ""], ["Yao", "Wen", ""]]}, {"id": "2103.11189", "submitter": "Jonne S\\\"alev\\\"a", "authors": "Jonne S\\\"alev\\\"a and Constantine Lignos", "title": "The Effectiveness of Morphology-aware Segmentation in Low-Resource\n  Neural Machine Translation", "comments": "EACL 2021 Student Research Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper evaluates the performance of several modern subword segmentation\nmethods in a low-resource neural machine translation setting. We compare\nsegmentations produced by applying BPE at the token or sentence level with\nmorphologically-based segmentations from LMVR and MORSEL. We evaluate\ntranslation tasks between English and each of Nepali, Sinhala, and Kazakh, and\npredict that using morphologically-based segmentation methods would lead to\nbetter performance in this setting. However, comparing to BPE, we find that no\nconsistent and reliable differences emerge between the segmentation methods.\nWhile morphologically-based methods outperform BPE in a few cases, what\nperforms best tends to vary across tasks, and the performance of segmentation\nmethods is often statistically indistinguishable.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 14:39:25 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["S\u00e4lev\u00e4", "Jonne", ""], ["Lignos", "Constantine", ""]]}, {"id": "2103.11190", "submitter": "Yue Lu", "authors": "Congqi Cao, Yue Lu, Yifan Zhang, Dongmei Jiang and Yanning Zhang", "title": "Efficient Spatialtemporal Context Modeling for Action Recognition", "comments": "16 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Contextual information plays an important role in action recognition. Local\noperations have difficulty to model the relation between two elements with a\nlong-distance interval. However, directly modeling the contextual information\nbetween any two points brings huge cost in computation and memory, especially\nfor action recognition, where there is an additional temporal dimension.\nInspired from 2D criss-cross attention used in segmentation task, we propose a\nrecurrent 3D criss-cross attention (RCCA-3D) module to model the dense\nlong-range spatiotemporal contextual information in video for action\nrecognition. The global context is factorized into sparse relation maps. We\nmodel the relationship between points in the same line along the direction of\nhorizon, vertical and depth at each time, which forms a 3D criss-cross\nstructure, and duplicate the same operation with recurrent mechanism to\ntransmit the relation between points in a line to a plane finally to the whole\nspatiotemporal space. Compared with the non-local method, the proposed RCCA-3D\nmodule reduces the number of parameters and FLOPs by 25% and 30% for video\ncontext modeling. We evaluate the performance of RCCA-3D with two latest action\nrecognition networks on three datasets and make a thorough analysis of the\narchitecture, obtaining the optimal way to factorize and fuse the relation\nmaps. Comparisons with other state-of-the-art methods demonstrate the\neffectiveness and efficiency of our model.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 14:48:12 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 04:40:12 GMT"}], "update_date": "2021-04-07", "authors_parsed": [["Cao", "Congqi", ""], ["Lu", "Yue", ""], ["Zhang", "Yifan", ""], ["Jiang", "Dongmei", ""], ["Zhang", "Yanning", ""]]}, {"id": "2103.11214", "submitter": "Biraja Ghoshal", "authors": "Bhargab Ghoshal, Biraja Ghoshal, Stephen Swift, Allan Tucker", "title": "Uncertainty Estimation in SARS-CoV-2 B-cell Epitope Prediction for\n  Vaccine Development", "comments": "Paper accepted for the 19th International Conference on Artificial\n  Intelligence in Medicine", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  B-cell epitopes play a key role in stimulating B-cells, triggering the\nprimary immune response which results in antibody production as well as the\nestablishment of long-term immunity in the form of memory cells. Consequently,\nbeing able to accurately predict appropriate linear B-cell epitope regions\nwould pave the way for the development of new protein-based vaccines. Knowing\nhow much confidence there is in a prediction is also essential for gaining\nclinicians' trust in the technology. In this article, we propose a calibrated\nuncertainty estimation in deep learning to approximate variational Bayesian\ninference using MC-DropWeights to predict epitope regions using the data from\nthe immune epitope database. Having applied this onto SARS-CoV-2, it can more\nreliably predict B-cell epitopes than standard methods. This will be able to\nidentify safe and effective vaccine candidates against Covid-19.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 17:10:49 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Ghoshal", "Bhargab", ""], ["Ghoshal", "Biraja", ""], ["Swift", "Stephen", ""], ["Tucker", "Allan", ""]]}, {"id": "2103.11218", "submitter": "Amin Jalali", "authors": "Amin Jalali", "title": "Evaluating Perceived Usefulness and Ease of Use of CMMN and DCR", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Case Management has been gradually evolving to support Knowledge-intensive\nbusiness process management, which resulted in developing different modeling\nlanguages, e.g., Declare, Dynamic Condition Response (DCR), and Case Management\nModel and Notation (CMMN). A language will die if users do not accept and use\nit in practice - similar to extinct human languages. Thus, it is important to\nevaluate how users perceive languages to determine if there is a need for\nimprovement. Although some studies have investigated how the process designers\nperceived Declare and DCR, there is a lack of research on how they perceive\nCMMN. Therefore, this study investigates how the process designers perceive the\nusefulness and ease of use of CMMN and DCR based on the Technology Acceptance\nModel. DCR is included to enable comparing the study result with previous ones.\nThe study is performed by educating master level students with these languages\nover eight weeks by giving feedback on their assignments to reduce perceptions\nbiases. The students' perceptions are collected through questionnaires before\nand after sending feedback on their final practice in the exam. Thus, the\nresult shows how the perception of participants can change by receiving\nfeedback - despite being well trained. The reliability of responses is tested\nusing Cronbach's alpha, and the result indicates that both languages have an\nacceptable level for both perceived usefulness and ease of use.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 17:57:19 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 19:41:14 GMT"}, {"version": "v3", "created": "Mon, 3 May 2021 10:22:36 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Jalali", "Amin", ""]]}, {"id": "2103.11241", "submitter": "Alvaro Leandro Cavalcante Carneiro", "authors": "Alvaro Leandro Cavalcante Carneiro, Lucas de Brito Silva, Marisa\n  Silveira Almeida Renaud Faulin", "title": "Artificial intelligence for detection and quantification of rust and\n  leaf miner in coffee crop", "comments": "10 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pest and disease control plays a key role in agriculture since the damage\ncaused by these agents are responsible for a huge economic loss every year.\nBased on this assumption, we create an algorithm capable of detecting rust\n(Hemileia vastatrix) and leaf miner (Leucoptera coffeella) in coffee leaves\n(Coffea arabica) and quantify disease severity using a mobile application as a\nhigh-level interface for the model inferences. We used different convolutional\nneural network architectures to create the object detector, besides the OpenCV\nlibrary, k-means, and three treatments: the RGB and value to quantification,\nand the AFSoft software, in addition to the analysis of variance, where we\ncompare the three methods. The results show an average precision of 81,5% in\nthe detection and that there was no significant statistical difference between\ntreatments to quantify the severity of coffee leaves, proposing a\ncomputationally less costly method. The application, together with the trained\nmodel, can detect the pest and disease over different image conditions and\ninfection stages and also estimate the disease infection stage.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 20:52:11 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 22:41:10 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Carneiro", "Alvaro Leandro Cavalcante", ""], ["Silva", "Lucas de Brito", ""], ["Faulin", "Marisa Silveira Almeida Renaud", ""]]}, {"id": "2103.11249", "submitter": "Mohammad Reza Besharati", "authors": "Nafiseh Jafari, Mohammad Reza Besharati, Mohammad Izadi, Maryam\n  Hourali", "title": "SELM: Software Engineering of Machine Learning Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the pillars of any machine learning model is its concepts. Using\nsoftware engineering, we can engineer these concepts and then develop and\nexpand them. In this article, we present a SELM framework for Software\nEngineering of machine Learning Models. We then evaluate this framework through\na case study. Using the SELM framework, we can improve a machine learning\nprocess efficiency and provide more accuracy in learning with less processing\nhardware resources and a smaller training dataset. This issue highlights the\nimportance of an interdisciplinary approach to machine learning. Therefore, in\nthis article, we have provided interdisciplinary teams' proposals for machine\nlearning.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 21:43:24 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Jafari", "Nafiseh", ""], ["Besharati", "Mohammad Reza", ""], ["Izadi", "Mohammad", ""], ["Hourali", "Maryam", ""]]}, {"id": "2103.11264", "submitter": "M. Saquib Sarfraz", "authors": "M. Saquib Sarfraz, Naila Murray, Vivek Sharma, Ali Diba, Luc Van Gool,\n  Rainer Stiefelhagen", "title": "Temporally-Weighted Hierarchical Clustering for Unsupervised Action\n  Segmentation", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Action segmentation refers to inferring boundaries of semantically consistent\nvisual concepts in videos and is an important requirement for many video\nunderstanding tasks. For this and other video understanding tasks, supervised\napproaches have achieved encouraging performance but require a high volume of\ndetailed frame-level annotations. We present a fully automatic and unsupervised\napproach for segmenting actions in a video that does not require any training.\nOur proposal is an effective temporally-weighted hierarchical clustering\nalgorithm that can group semantically consistent frames of the video. Our main\nfinding is that representing a video with a 1-nearest neighbor graph by taking\ninto account the time progression is sufficient to form semantically and\ntemporally consistent clusters of frames where each cluster may represent some\naction in the video. Additionally, we establish strong unsupervised baselines\nfor action segmentation and show significant performance improvements over\npublished unsupervised methods on five challenging action segmentation\ndatasets. Our code is available at\nhttps://github.com/ssarfraz/FINCH-Clustering/tree/master/TW-FINCH\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 23:30:01 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 08:16:53 GMT"}, {"version": "v3", "created": "Thu, 25 Mar 2021 12:39:28 GMT"}, {"version": "v4", "created": "Sat, 27 Mar 2021 16:40:25 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Sarfraz", "M. Saquib", ""], ["Murray", "Naila", ""], ["Sharma", "Vivek", ""], ["Diba", "Ali", ""], ["Van Gool", "Luc", ""], ["Stiefelhagen", "Rainer", ""]]}, {"id": "2103.11276", "submitter": "Erkan Kayacan", "authors": "Zhongzhong Zhang, Erkan Kayacan, Benjamin Thompson and Girish\n  Chowdhary", "title": "High precision control and deep learning-based corn stand counting\n  algorithms for agricultural robot", "comments": "14 pages, 9 figures", "journal-ref": "Autonomous Robots, volume 44, pages 1289-1302, 2020", "doi": "10.1007/s10514-020-09915-y", "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents high precision control and deep learning-based corn stand\ncounting algorithms for a low-cost, ultra-compact 3D printed and autonomous\nfield robot for agricultural operations. Currently, plant traits, such as\nemergence rate, biomass, vigor, and stand counting, are measured manually. This\nis highly labor-intensive and prone to errors. The robot, termed TerraSentia,\nis designed to automate the measurement of plant traits for efficient\nphenotyping as an alternative to manual measurements. In this paper, we\nformulate a Nonlinear Moving Horizon Estimator (NMHE) that identifies key\nterrain parameters using onboard robot sensors and a learning-based Nonlinear\nModel Predictive Control (NMPC) that ensures high precision path tracking in\nthe presence of unknown wheel-terrain interaction. Moreover, we develop a\nmachine vision algorithm designed to enable an ultra-compact ground robot to\ncount corn stands by driving through the fields autonomously. The algorithm\nleverages a deep network to detect corn plants in images, and a visual tracking\nmodel to re-identify detected objects at different time steps. We collected\ndata from 53 corn plots in various fields for corn plants around 14 days after\nemergence (stage V3 - V4). The robot predictions have agreed well with the\nground truth with $C_{robot}=1.02 \\times C_{human}-0.86$ and a correlation\ncoefficient $R=0.96$. The mean relative error given by the algorithm is\n$-3.78\\%$, and the standard deviation is $6.76\\%$. These results indicate a\nfirst and significant step towards autonomous robot-based real-time phenotyping\nusing low-cost, ultra-compact ground robots for corn and potentially other\ncrops.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 01:13:38 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Zhang", "Zhongzhong", ""], ["Kayacan", "Erkan", ""], ["Thompson", "Benjamin", ""], ["Chowdhary", "Girish", ""]]}, {"id": "2103.11285", "submitter": "Charles (A.) Kantor", "authors": "Charles A. Kantor, Marta Skreta, Brice Rauby, L\\'eonard Boussioux,\n  Emmanuel Jehanno, Alexandra Luccioni, David Rolnick, Hugues Talbot", "title": "Geo-Spatiotemporal Features and Shape-Based Prior Knowledge for\n  Fine-grained Imbalanced Data Classification", "comments": "Copyright by the authors. All rights reserved to authors only.\n  Correspondence to: ckantor (at) stanford [dot] edu", "journal-ref": "Proc. IJCAI 2021, Workshop on AI for Social Good, Harvard\n  University (2021)", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Fine-grained classification aims at distinguishing between items with similar\nglobal perception and patterns, but that differ by minute details. Our primary\nchallenges come from both small inter-class variations and large intra-class\nvariations. In this article, we propose to combine several innovations to\nimprove fine-grained classification within the use-case of wildlife, which is\nof practical interest for experts. We utilize geo-spatiotemporal data to enrich\nthe picture information and further improve the performance. We also\ninvestigate state-of-the-art methods for handling the imbalanced data issue.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 02:01:38 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Kantor", "Charles A.", ""], ["Skreta", "Marta", ""], ["Rauby", "Brice", ""], ["Boussioux", "L\u00e9onard", ""], ["Jehanno", "Emmanuel", ""], ["Luccioni", "Alexandra", ""], ["Rolnick", "David", ""], ["Talbot", "Hugues", ""]]}, {"id": "2103.11297", "submitter": "Ryan Rossi", "authors": "Camille Harris, Ryan A. Rossi, Sana Malik, Jane Hoffswell, Fan Du, Tak\n  Yeon Lee, Eunyee Koh, Handong Zhao", "title": "Insight-centric Visualization Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visualization recommendation systems simplify exploratory data analysis (EDA)\nand make understanding data more accessible to users of all skill levels by\nautomatically generating visualizations for users to explore. However, most\nexisting visualization recommendation systems focus on ranking all\nvisualizations into a single list or set of groups based on particular\nattributes or encodings. This global ranking makes it difficult and\ntime-consuming for users to find the most interesting or relevant insights. To\naddress these limitations, we introduce a novel class of visualization\nrecommendation systems that automatically rank and recommend both groups of\nrelated insights as well as the most important insights within each group. Our\nproposed approach combines results from many different learning-based methods\nto discover insights automatically. A key advantage is that this approach\ngeneralizes to a wide variety of attribute types such as categorical,\nnumerical, and temporal, as well as complex non-trivial combinations of these\ndifferent attribute types. To evaluate the effectiveness of our approach, we\nimplemented a new insight-centric visualization recommendation system,\nSpotLight, which generates and ranks annotated visualizations to explain each\ninsight. We conducted a user study with 12 participants and two datasets which\nshowed that users are able to quickly understand and find relevant insights in\nunfamiliar data.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 03:30:22 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Harris", "Camille", ""], ["Rossi", "Ryan A.", ""], ["Malik", "Sana", ""], ["Hoffswell", "Jane", ""], ["Du", "Fan", ""], ["Lee", "Tak Yeon", ""], ["Koh", "Eunyee", ""], ["Zhao", "Handong", ""]]}, {"id": "2103.11302", "submitter": "Aparna Varde", "authors": "Onyeka Emebo, Aparna S. Varde, Olawande Daramola", "title": "Common Sense Knowledge, Ontology and Text Mining for Implicit\n  Requirements", "comments": "7 pages, 3 figures, 2 tables, conference: DMIN 2016 conference by\n  CSREA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The ability of a system to meet its requirements is a strong determinant of\nsuccess. Thus effective requirements specification is crucial. Explicit\nRequirements are well-defined needs for a system to execute. IMplicit\nRequirements (IMRs) are assumed needs that a system is expected to fulfill\nthough not elicited during requirements gathering. Studies have shown that a\nmajor factor in the failure of software systems is the presence of unhandled\nIMRs. Since relevance of IMRs is important for efficient system functionality,\nthere are methods developed to aid the identification and management of IMRs.\nIn this paper, we emphasize that Common Sense Knowledge, in the field of\nKnowledge Representation in AI, would be useful to automatically identify and\nmanage IMRs. This paper is aimed at identifying the sources of IMRs and also\nproposing an automated support tool for managing IMRs within an organizational\ncontext. Since this is found to be a present gap in practice, our work makes a\ncontribution here. We propose a novel approach for identifying and managing\nIMRs based on combining three core technologies: common sense knowledge, text\nmining and ontology. We claim that discovery and handling of unknown and\nnon-elicited requirements would reduce risks and costs in software development.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 04:32:58 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Emebo", "Onyeka", ""], ["Varde", "Aparna S.", ""], ["Daramola", "Olawande", ""]]}, {"id": "2103.11338", "submitter": "Aparna Varde", "authors": "Anita Pampoore-Thampi, Aparna S. Varde, Danlin Yu", "title": "Mining GIS Data to Predict Urban Sprawl", "comments": "8 Pages, 13 figures, KDD 2014 conference Bloomberg track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB stat.ML", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  This paper addresses the interesting problem of processing and analyzing data\nin geographic information systems (GIS) to achieve a clear perspective on urban\nsprawl. The term urban sprawl refers to overgrowth and expansion of low-density\nareas with issues such as car dependency and segregation between residential\nversus commercial use. Sprawl has impacts on the environment and public health.\nIn our work, spatiotemporal features related to real GIS data on urban sprawl\nsuch as population growth and demographics are mined to discover knowledge for\ndecision support. We adapt data mining algorithms, Apriori for association rule\nmining and J4.8 for decision tree classification to geospatial analysis,\ndeploying the ArcGIS tool for mapping. Knowledge discovered by mining this\nspatiotemporal data is used to implement a prototype spatial decision support\nsystem (SDSS). This SDSS predicts whether urban sprawl is likely to occur.\nFurther, it estimates the values of pertinent variables to understand how the\nvariables impact each other. The SDSS can help decision-makers identify\nproblems and create solutions for avoiding future sprawl occurrence and\nconducting urban planning where sprawl already occurs, thus aiding sustainable\ndevelopment. This work falls in the broad realm of geospatial intelligence and\nsets the stage for designing a large scale SDSS to process big data in complex\nenvironments, which constitutes part of our future work.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 08:41:35 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Pampoore-Thampi", "Anita", ""], ["Varde", "Aparna S.", ""], ["Yu", "Danlin", ""]]}, {"id": "2103.11345", "submitter": "Vincent Thomas", "authors": "Vincent Thomas, G\\'er\\'emy Hutin, Olivier Buffet", "title": "Monte Carlo Information-Oriented Planning", "comments": "9 pages, revised version of ECAI 2020 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we discuss how to solve information-gathering problems\nexpressed as rho-POMDPs, an extension of Partially Observable Markov Decision\nProcesses (POMDPs) whose reward rho depends on the belief state. Point-based\napproaches used for solving POMDPs have been extended to solving rho-POMDPs as\nbelief MDPs when its reward rho is convex in B or when it is\nLipschitz-continuous. In the present paper, we build on the POMCP algorithm to\npropose a Monte Carlo Tree Search for rho-POMDPs, aiming for an efficient\non-line planner which can be used for any rho function. Adaptations are\nrequired due to the belief-dependent rewards to (i) propagate more than one\nstate at a time, and (ii) prevent biases in value estimates. An asymptotic\nconvergence proof to epsilon-optimal values is given when rho is continuous.\nExperiments are conducted to analyze the algorithms at hand and show that they\noutperform myopic approaches.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 09:09:27 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Thomas", "Vincent", ""], ["Hutin", "G\u00e9r\u00e9my", ""], ["Buffet", "Olivier", ""]]}, {"id": "2103.11356", "submitter": "Dongsheng Wang", "authors": "Dongsheng Wang, Prayag Tiwari, Sahil Garg, Hongyin Zhu, Peter Bruza", "title": "Structural block driven - enhanced convolutional neural representation\n  for relation extraction", "comments": null, "journal-ref": null, "doi": "10.1016/j.asoc.2019.105913", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel lightweight relation extraction approach of\nstructural block driven - convolutional neural learning. Specifically, we\ndetect the essential sequential tokens associated with entities through\ndependency analysis, named as a structural block, and only encode the block on\na block-wise and an inter-block-wise representation, utilizing multi-scale\nCNNs. This is to 1) eliminate the noisy from irrelevant part of a sentence;\nmeanwhile 2) enhance the relevant block representation with both block-wise and\ninter-block-wise semantically enriched representation. Our method has the\nadvantage of being independent of long sentence context since we only encode\nthe sequential tokens within a block boundary. Experiments on two datasets\ni.e., SemEval2010 and KBP37, demonstrate the significant advantages of our\nmethod. In particular, we achieve the new state-of-the-art performance on the\nKBP37 dataset; and comparable performance with the state-of-the-art on the\nSemEval2010 dataset.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 10:23:44 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Wang", "Dongsheng", ""], ["Tiwari", "Prayag", ""], ["Garg", "Sahil", ""], ["Zhu", "Hongyin", ""], ["Bruza", "Peter", ""]]}, {"id": "2103.11357", "submitter": "Andreas Holzinger", "authors": "Andr\\'e M. Carrington, Douglas G. Manuel, Paul W. Fieguth, Tim Ramsay,\n  Venet Osmani, Bernhard Wernly, Carol Bennett, Steven Hawken, Matthew McInnes,\n  Olivia Magwood, Yusuf Sheikh, Andreas Holzinger", "title": "Deep ROC Analysis and AUC as Balanced Average Accuracy to Improve Model\n  Selection, Understanding and Interpretation", "comments": "14 pages, 6 Figures, submitted to IEEE Transactions on Pattern\n  Analysis and Machine Intelligence (TPAMI), currently under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Optimal performance is critical for decision-making tasks from medicine to\nautonomous driving, however common performance measures may be too general or\ntoo specific. For binary classifiers, diagnostic tests or prognosis at a\ntimepoint, measures such as the area under the receiver operating\ncharacteristic curve, or the area under the precision recall curve, are too\ngeneral because they include unrealistic decision thresholds. On the other\nhand, measures such as accuracy, sensitivity or the F1 score are measures at a\nsingle threshold that reflect an individual single probability or predicted\nrisk, rather than a range of individuals or risk. We propose a method in\nbetween, deep ROC analysis, that examines groups of probabilities or predicted\nrisks for more insightful analysis. We translate esoteric measures into\nfamiliar terms: AUC and the normalized concordant partial AUC are balanced\naverage accuracy (a new finding); the normalized partial AUC is average\nsensitivity; and the normalized horizontal partial AUC is average specificity.\nAlong with post-test measures, we provide a method that can improve model\nselection in some cases and provide interpretation and assurance for patients\nin each risk group. We demonstrate deep ROC analysis in two case studies and\nprovide a toolkit in Python.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 10:27:35 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Carrington", "Andr\u00e9 M.", ""], ["Manuel", "Douglas G.", ""], ["Fieguth", "Paul W.", ""], ["Ramsay", "Tim", ""], ["Osmani", "Venet", ""], ["Wernly", "Bernhard", ""], ["Bennett", "Carol", ""], ["Hawken", "Steven", ""], ["McInnes", "Matthew", ""], ["Magwood", "Olivia", ""], ["Sheikh", "Yusuf", ""], ["Holzinger", "Andreas", ""]]}, {"id": "2103.11362", "submitter": "Jelica Vasiljevi\\'c", "authors": "Jelica Vasiljevi\\'c, Friedrich Feuerhake, C\\'edric Wemmert, Thomas\n  Lampert", "title": "Self adversarial attack as an augmentation method for\n  immunohistochemical stainings", "comments": "Accepted to ISBI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  It has been shown that unpaired image-to-image translation methods\nconstrained by cycle-consistency hide the information necessary for accurate\ninput reconstruction as imperceptible noise. We demonstrate that, when applied\nto histopathology data, this hidden noise appears to be related to stain\nspecific features and show that this is the case with two immunohistochemical\nstainings during translation to Periodic acid- Schiff (PAS), a histochemical\nstaining method commonly applied in renal pathology. Moreover, by perturbing\nthis hidden information, the translation models produce different, plausible\noutputs. We demonstrate that this property can be used as an augmentation\nmethod which, in a case of supervised glomeruli segmentation, leads to improved\nperformance.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 10:48:40 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Vasiljevi\u0107", "Jelica", ""], ["Feuerhake", "Friedrich", ""], ["Wemmert", "C\u00e9dric", ""], ["Lampert", "Thomas", ""]]}, {"id": "2103.11372", "submitter": "Sadaf Gulshad", "authors": "Sadaf Gulshad and Arnold Smeulders", "title": "Natural Perturbed Training for General Robustness of Neural Network\n  Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We focus on the robustness of neural networks for classification. To permit a\nfair comparison between methods to achieve robustness, we first introduce a\nstandard based on the mensuration of a classifier's degradation. Then, we\npropose natural perturbed training to robustify the network. Natural\nperturbations will be encountered in practice: the difference of two images of\nthe same object may be approximated by an elastic deformation (when they have\nslightly different viewing angles), by occlusions (when they hide differently\nbehind objects), or by saturation, Gaussian noise etc. Training some fraction\nof the epochs on random versions of such variations will help the classifier to\nlearn better. We conduct extensive experiments on six datasets of varying sizes\nand granularity. Natural perturbed learning show better and much faster\nperformance than adversarial training on clean, adversarial as well as natural\nperturbed images. It even improves general robustness on perturbations not seen\nduring the training. For Cifar-10 and STL-10 natural perturbed training even\nimproves the accuracy for clean data and reaches the state of the art\nperformance. Ablation studies verify the effectiveness of natural perturbed\ntraining.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 11:47:38 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Gulshad", "Sadaf", ""], ["Smeulders", "Arnold", ""]]}, {"id": "2103.11388", "submitter": "Antonios Liapis", "authors": "Konstantinos Sfikas and Antonios Liapis", "title": "Collaborative Agent Gameplay in the Pandemic Board Game", "comments": "11 pages", "journal-ref": "Proceedings of the Foundations of Digital Games Conference, 2020", "doi": "10.1145/3402942.3402943", "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While artificial intelligence has been applied to control players' decisions\nin board games for over half a century, little attention is given to games with\nno player competition. Pandemic is an exemplar collaborative board game where\nall players coordinate to overcome challenges posed by events occurring during\nthe game's progression. This paper proposes an artificial agent which controls\nall players' actions and balances chances of winning versus risk of losing in\nthis highly stochastic environment. The agent applies a Rolling Horizon\nEvolutionary Algorithm on an abstraction of the game-state that lowers the\nbranching factor and simulates the game's stochasticity. Results show that the\nproposed algorithm can find winning strategies more consistently in different\ngames of varying difficulty. The impact of a number of state evaluation metrics\nis explored, balancing between optimistic strategies that favor winning and\npessimistic strategies that guard against losing.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 13:18:20 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Sfikas", "Konstantinos", ""], ["Liapis", "Antonios", ""]]}, {"id": "2103.11402", "submitter": "Qiang Zhou", "authors": "Qiang Zhou, Chaohui Yu, Zhibin Wang, Qi Qian, Hao Li", "title": "Instant-Teaching: An End-to-End Semi-Supervised Object Detection\n  Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Supervised learning based object detection frameworks demand plenty of\nlaborious manual annotations, which may not be practical in real applications.\nSemi-supervised object detection (SSOD) can effectively leverage unlabeled data\nto improve the model performance, which is of great significance for the\napplication of object detection models. In this paper, we revisit SSOD and\npropose Instant-Teaching, a completely end-to-end and effective SSOD framework,\nwhich uses instant pseudo labeling with extended weak-strong data augmentations\nfor teaching during each training iteration. To alleviate the confirmation bias\nproblem and improve the quality of pseudo annotations, we further propose a\nco-rectify scheme based on Instant-Teaching, denoted as Instant-Teaching$^*$.\nExtensive experiments on both MS-COCO and PASCAL VOC datasets substantiate the\nsuperiority of our framework. Specifically, our method surpasses\nstate-of-the-art methods by 4.2 mAP on MS-COCO when using $2\\%$ labeled data.\nEven with full supervised information of MS-COCO, the proposed method still\noutperforms state-of-the-art methods by about 1.0 mAP. On PASCAL VOC, we can\nachieve more than 5 mAP improvement by applying VOC07 as labeled data and VOC12\nas unlabeled data.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 14:03:36 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Zhou", "Qiang", ""], ["Yu", "Chaohui", ""], ["Wang", "Zhibin", ""], ["Qian", "Qi", ""], ["Li", "Hao", ""]]}, {"id": "2103.11405", "submitter": "Yu Bao", "authors": "Yu Bao, Shujian Huang, Tong Xiao, Dongqi Wang, Xinyu Dai, Jiajun Chen", "title": "Non-Autoregressive Translation by Learning Target Categorical Codes", "comments": "11 pages, 3 figures, 7 tables. Accepted by NAACL-2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Non-autoregressive Transformer is a promising text generation model. However,\ncurrent non-autoregressive models still fall behind their autoregressive\ncounterparts in translation quality. We attribute this accuracy gap to the lack\nof dependency modeling among decoder inputs. In this paper, we propose CNAT,\nwhich learns implicitly categorical codes as latent variables into the\nnon-autoregressive decoding. The interaction among these categorical codes\nremedies the missing dependencies and improves the model capacity. Experiment\nresults show that our model achieves comparable or better performance in\nmachine translation tasks, compared with several strong baselines.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 14:12:34 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Bao", "Yu", ""], ["Huang", "Shujian", ""], ["Xiao", "Tong", ""], ["Wang", "Dongqi", ""], ["Dai", "Xinyu", ""], ["Chen", "Jiajun", ""]]}, {"id": "2103.11441", "submitter": "Tao Gui", "authors": "Tao Gui, Xiao Wang, Qi Zhang, Qin Liu, Yicheng Zou, Xin Zhou, Rui\n  Zheng, Chong Zhang, Qinzhuo Wu, Jiacheng Ye, Zexiong Pang, Yongxin Zhang,\n  Zhengyan Li, Ruotian Ma, Zichu Fei, Ruijian Cai, Jun Zhao, Xingwu Hu, Zhiheng\n  Yan, Yiding Tan, Yuan Hu, Qiyuan Bian, Zhihua Liu, Bolin Zhu, Shan Qin,\n  Xiaoyu Xing, Jinlan Fu, Yue Zhang, Minlong Peng, Xiaoqing Zheng, Yaqian Zhou,\n  Zhongyu Wei, Xipeng Qiu and Xuanjing Huang", "title": "TextFlint: Unified Multilingual Robustness Evaluation Toolkit for\n  Natural Language Processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Various robustness evaluation methodologies from different perspectives have\nbeen proposed for different natural language processing (NLP) tasks. These\nmethods have often focused on either universal or task-specific generalization\ncapabilities. In this work, we propose a multilingual robustness evaluation\nplatform for NLP tasks (TextFlint) that incorporates universal text\ntransformation, task-specific transformation, adversarial attack,\nsubpopulation, and their combinations to provide comprehensive robustness\nanalysis. TextFlint enables practitioners to automatically evaluate their\nmodels from all aspects or to customize their evaluations as desired with just\na few lines of code. To guarantee user acceptability, all the text\ntransformations are linguistically based, and we provide a human evaluation for\neach one. TextFlint generates complete analytical reports as well as targeted\naugmented data to address the shortcomings of the model's robustness. To\nvalidate TextFlint's utility, we performed large-scale empirical evaluations\n(over 67,000 evaluations) on state-of-the-art deep learning models, classic\nsupervised methods, and real-world systems. Almost all models showed\nsignificant performance degradation, including a decline of more than 50% of\nBERT's prediction accuracy on tasks such as aspect-level sentiment\nclassification, named entity recognition, and natural language inference.\nTherefore, we call for the robustness to be included in the model evaluation,\nso as to promote the healthy development of NLP technology.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 17:20:38 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 09:56:50 GMT"}, {"version": "v3", "created": "Wed, 5 May 2021 09:31:54 GMT"}], "update_date": "2021-05-06", "authors_parsed": [["Gui", "Tao", ""], ["Wang", "Xiao", ""], ["Zhang", "Qi", ""], ["Liu", "Qin", ""], ["Zou", "Yicheng", ""], ["Zhou", "Xin", ""], ["Zheng", "Rui", ""], ["Zhang", "Chong", ""], ["Wu", "Qinzhuo", ""], ["Ye", "Jiacheng", ""], ["Pang", "Zexiong", ""], ["Zhang", "Yongxin", ""], ["Li", "Zhengyan", ""], ["Ma", "Ruotian", ""], ["Fei", "Zichu", ""], ["Cai", "Ruijian", ""], ["Zhao", "Jun", ""], ["Hu", "Xingwu", ""], ["Yan", "Zhiheng", ""], ["Tan", "Yiding", ""], ["Hu", "Yuan", ""], ["Bian", "Qiyuan", ""], ["Liu", "Zhihua", ""], ["Zhu", "Bolin", ""], ["Qin", "Shan", ""], ["Xing", "Xiaoyu", ""], ["Fu", "Jinlan", ""], ["Zhang", "Yue", ""], ["Peng", "Minlong", ""], ["Zheng", "Xiaoqing", ""], ["Zhou", "Yaqian", ""], ["Wei", "Zhongyu", ""], ["Qiu", "Xipeng", ""], ["Huang", "Xuanjing", ""]]}, {"id": "2103.11470", "submitter": "David D. Fan", "authors": "Ali Agha, Kyohei Otsu, Benjamin Morrell, David D. Fan, Rohan Thakker,\n  Angel Santamaria-Navarro, Sung-Kyun Kim, Amanda Bouman, Xianmei Lei, Jeffrey\n  Edlund, Muhammad Fadhil Ginting, Kamak Ebadi, Matthew Anderson, Torkom\n  Pailevanian, Edward Terry, Michael Wolf, Andrea Tagliabue, Tiago Stegun\n  Vaquero, Matteo Palieri, Scott Tepsuporn, Yun Chang, Arash Kalantari,\n  Fernando Chavez, Brett Lopez, Nobuhiro Funabiki, Gregory Miles, Thomas Touma,\n  Alessandro Buscicchio, Jesus Tordesillas, Nikhilesh Alatur, Jeremy Nash,\n  William Walsh, Sunggoo Jung, Hanseob Lee, Christoforos Kanellakis, John Mayo,\n  Scott Harper, Marcel Kaufmann, Anushri Dixit, Gustavo Correa, Carlyn Lee, Jay\n  Gao, Gene Merewether, Jairo Maldonado-Contreras, Gautam Salhotra, Maira\n  Saboia Da Silva, Benjamin Ramtoula, Yuki Kubo, Seyed Fakoorian, Alexander\n  Hatteland, Taeyeon Kim, Tara Bartlett, Alex Stephens, Leon Kim, Chuck Bergh,\n  Eric Heiden, Thomas Lew, Abhishek Cauligi, Tristan Heywood, Andrew Kramer,\n  Henry A. Leopold, Chris Choi, Shreyansh Daftry, Olivier Toupet, Inhwan Wee,\n  Abhishek Thakur, Micah Feras, Giovanni Beltrame, George Nikolakopoulos, David\n  Shim, Luca Carlone, Joel Burdick", "title": "NeBula: Quest for Robotic Autonomy in Challenging Environments; TEAM\n  CoSTAR at the DARPA Subterranean Challenge", "comments": "For team website, see https://costar.jpl.nasa.gov/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents and discusses algorithms, hardware, and software\narchitecture developed by the TEAM CoSTAR (Collaborative SubTerranean\nAutonomous Robots), competing in the DARPA Subterranean Challenge.\nSpecifically, it presents the techniques utilized within the Tunnel (2019) and\nUrban (2020) competitions, where CoSTAR achieved 2nd and 1st place,\nrespectively. We also discuss CoSTAR's demonstrations in Martian-analog surface\nand subsurface (lava tubes) exploration. The paper introduces our autonomy\nsolution, referred to as NeBula (Networked Belief-aware Perceptual Autonomy).\nNeBula is an uncertainty-aware framework that aims at enabling resilient and\nmodular autonomy solutions by performing reasoning and decision making in the\nbelief space (space of probability distributions over the robot and world\nstates). We discuss various components of the NeBula framework, including: (i)\ngeometric and semantic environment mapping; (ii) a multi-modal positioning\nsystem; (iii) traversability analysis and local planning; (iv) global motion\nplanning and exploration behavior; (i) risk-aware mission planning; (vi)\nnetworking and decentralized reasoning; and (vii) learning-enabled adaptation.\nWe discuss the performance of NeBula on several robot types (e.g. wheeled,\nlegged, flying), in various environments. We discuss the specific results and\nlessons learned from fielding this solution in the challenging courses of the\nDARPA Subterranean Challenge competition.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 19:42:26 GMT"}, {"version": "v2", "created": "Sun, 28 Mar 2021 16:34:14 GMT"}, {"version": "v3", "created": "Wed, 7 Jul 2021 02:00:34 GMT"}], "update_date": "2021-07-08", "authors_parsed": [["Agha", "Ali", ""], ["Otsu", "Kyohei", ""], ["Morrell", "Benjamin", ""], ["Fan", "David D.", ""], ["Thakker", "Rohan", ""], ["Santamaria-Navarro", "Angel", ""], ["Kim", "Sung-Kyun", ""], ["Bouman", "Amanda", ""], ["Lei", "Xianmei", ""], ["Edlund", "Jeffrey", ""], ["Ginting", "Muhammad Fadhil", ""], ["Ebadi", "Kamak", ""], ["Anderson", "Matthew", ""], ["Pailevanian", "Torkom", ""], ["Terry", "Edward", ""], ["Wolf", "Michael", ""], ["Tagliabue", "Andrea", ""], ["Vaquero", "Tiago Stegun", ""], ["Palieri", "Matteo", ""], ["Tepsuporn", "Scott", ""], ["Chang", "Yun", ""], ["Kalantari", "Arash", ""], ["Chavez", "Fernando", ""], ["Lopez", "Brett", ""], ["Funabiki", "Nobuhiro", ""], ["Miles", "Gregory", ""], ["Touma", "Thomas", ""], ["Buscicchio", "Alessandro", ""], ["Tordesillas", "Jesus", ""], ["Alatur", "Nikhilesh", ""], ["Nash", "Jeremy", ""], ["Walsh", "William", ""], ["Jung", "Sunggoo", ""], ["Lee", "Hanseob", ""], ["Kanellakis", "Christoforos", ""], ["Mayo", "John", ""], ["Harper", "Scott", ""], ["Kaufmann", "Marcel", ""], ["Dixit", "Anushri", ""], ["Correa", "Gustavo", ""], ["Lee", "Carlyn", ""], ["Gao", "Jay", ""], ["Merewether", "Gene", ""], ["Maldonado-Contreras", "Jairo", ""], ["Salhotra", "Gautam", ""], ["Da Silva", "Maira Saboia", ""], ["Ramtoula", "Benjamin", ""], ["Kubo", "Yuki", ""], ["Fakoorian", "Seyed", ""], ["Hatteland", "Alexander", ""], ["Kim", "Taeyeon", ""], ["Bartlett", "Tara", ""], ["Stephens", "Alex", ""], ["Kim", "Leon", ""], ["Bergh", "Chuck", ""], ["Heiden", "Eric", ""], ["Lew", "Thomas", ""], ["Cauligi", "Abhishek", ""], ["Heywood", "Tristan", ""], ["Kramer", "Andrew", ""], ["Leopold", "Henry A.", ""], ["Choi", "Chris", ""], ["Daftry", "Shreyansh", ""], ["Toupet", "Olivier", ""], ["Wee", "Inhwan", ""], ["Thakur", "Abhishek", ""], ["Feras", "Micah", ""], ["Beltrame", "Giovanni", ""], ["Nikolakopoulos", "George", ""], ["Shim", "David", ""], ["Carlone", "Luca", ""], ["Burdick", "Joel", ""]]}, {"id": "2103.11477", "submitter": "Yoli Shavit", "authors": "Yoli Shavit, Ron Ferens, Yosi Keller", "title": "Paying Attention to Activation Maps in Camera Pose Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Camera pose regression methods apply a single forward pass to the query image\nto estimate the camera pose. As such, they offer a fast and light-weight\nalternative to traditional localization schemes based on image retrieval. Pose\nregression approaches simultaneously learn two regression tasks, aiming to\njointly estimate the camera position and orientation using a single embedding\nvector computed by a convolutional backbone. We propose an attention-based\napproach for pose regression, where the convolutional activation maps are used\nas sequential inputs. Transformers are applied to encode the sequential\nactivation maps as latent vectors, used for camera pose regression. This allows\nus to pay attention to spatially-varying deep features. Using two Transformer\nheads, we separately focus on the features for camera position and orientation,\nbased on how informative they are per task. Our proposed approach is shown to\ncompare favorably to contemporary pose regressors schemes and achieves\nstate-of-the-art accuracy across multiple outdoor and indoor benchmarks. In\nparticular, to the best of our knowledge, our approach is the only method to\nattain sub-meter average accuracy across outdoor scenes. We make our code\npublicly available from here.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 20:10:15 GMT"}, {"version": "v2", "created": "Sun, 11 Apr 2021 19:55:34 GMT"}], "update_date": "2021-04-13", "authors_parsed": [["Shavit", "Yoli", ""], ["Ferens", "Ron", ""], ["Keller", "Yosi", ""]]}, {"id": "2103.11505", "submitter": "Laurent Orseau", "authors": "Laurent Orseau, Levi H. S. Lelis", "title": "Policy-Guided Heuristic Search with Guarantees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of a policy and a heuristic function for guiding search can be quite\neffective in adversarial problems, as demonstrated by AlphaGo and its\nsuccessors, which are based on the PUCT search algorithm. While PUCT can also\nbe used to solve single-agent deterministic problems, it lacks guarantees on\nits search effort and it can be computationally inefficient in practice.\nCombining the A* algorithm with a learned heuristic function tends to work\nbetter in these domains, but A* and its variants do not use a policy. Moreover,\nthe purpose of using A* is to find solutions of minimum cost, while we seek\ninstead to minimize the search loss (e.g., the number of search steps). LevinTS\nis guided by a policy and provides guarantees on the number of search steps\nthat relate to the quality of the policy, but it does not make use of a\nheuristic function. In this work we introduce Policy-guided Heuristic Search\n(PHS), a novel search algorithm that uses both a heuristic function and a\npolicy and has theoretical guarantees on the search loss that relates to both\nthe quality of the heuristic and of the policy. We show empirically on the\nsliding-tile puzzle, Sokoban, and a puzzle from the commercial game `The\nWitness' that PHS enables the rapid learning of both a policy and a heuristic\nfunction and compares favorably with A*, Weighted A*, Greedy Best-First Search,\nLevinTS, and PUCT in terms of number of problems solved and search time in all\nthree domains tested.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 22:30:57 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Orseau", "Laurent", ""], ["Lelis", "Levi H. S.", ""]]}, {"id": "2103.11511", "submitter": "Dan Kondratyuk", "authors": "Dan Kondratyuk, Liangzhe Yuan, Yandong Li, Li Zhang, Mingxing Tan,\n  Matthew Brown, Boqing Gong", "title": "MoViNets: Mobile Video Networks for Efficient Video Recognition", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Mobile Video Networks (MoViNets), a family of computation and\nmemory efficient video networks that can operate on streaming video for online\ninference. 3D convolutional neural networks (CNNs) are accurate at video\nrecognition but require large computation and memory budgets and do not support\nonline inference, making them difficult to work on mobile devices. We propose a\nthree-step approach to improve computational efficiency while substantially\nreducing the peak memory usage of 3D CNNs. First, we design a video network\nsearch space and employ neural architecture search to generate efficient and\ndiverse 3D CNN architectures. Second, we introduce the Stream Buffer technique\nthat decouples memory from video clip duration, allowing 3D CNNs to embed\narbitrary-length streaming video sequences for both training and inference with\na small constant memory footprint. Third, we propose a simple ensembling\ntechnique to improve accuracy further without sacrificing efficiency. These\nthree progressive techniques allow MoViNets to achieve state-of-the-art\naccuracy and efficiency on the Kinetics, Moments in Time, and Charades video\naction recognition datasets. For instance, MoViNet-A5-Stream achieves the same\naccuracy as X3D-XL on Kinetics 600 while requiring 80% fewer FLOPs and 65% less\nmemory. Code will be made available at\nhttps://github.com/tensorflow/models/tree/master/official/vision.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 23:06:38 GMT"}, {"version": "v2", "created": "Sun, 18 Apr 2021 18:04:58 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Kondratyuk", "Dan", ""], ["Yuan", "Liangzhe", ""], ["Li", "Yandong", ""], ["Zhang", "Li", ""], ["Tan", "Mingxing", ""], ["Brown", "Matthew", ""], ["Gong", "Boqing", ""]]}, {"id": "2103.11512", "submitter": "Jianlan Luo", "authors": "Jianlan Luo, Oleg Sushkov, Rugile Pevceviciute, Wenzhao Lian, Chang\n  Su, Mel Vecerik, Ning Ye, Stefan Schaal, Jon Scholz", "title": "Robust Multi-Modal Policies for Industrial Assembly via Reinforcement\n  Learning and Demonstrations: A Large-Scale Study", "comments": "RSS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Over the past several years there has been a considerable research investment\ninto learning-based approaches to industrial assembly, but despite significant\nprogress these techniques have yet to be adopted by industry. We argue that it\nis the prohibitively large design space for Deep Reinforcement Learning (DRL),\nrather than algorithmic limitations per se, that are truly responsible for this\nlack of adoption. Pushing these techniques into the industrial mainstream\nrequires an industry-oriented paradigm which differs significantly from the\nacademic mindset. In this paper we define criteria for industry-oriented DRL,\nand perform a thorough comparison according to these criteria of one family of\nlearning approaches, DRL from demonstration, against a professional industrial\nintegrator on the recently established NIST assembly benchmark. We explain the\ndesign choices, representing several years of investigation, which enabled our\nDRL system to consistently outperform the integrator baseline in terms of both\nspeed and reliability. Finally, we conclude with a competition between our DRL\nsystem and a human on a challenge task of insertion into a randomly moving\ntarget. This study suggests that DRL is capable of outperforming not only\nestablished engineered approaches, but the human motor system as well, and that\nthere remains significant room for improvement. Videos can be found on our\nproject website: https://sites.google.com/view/shield-nist.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 23:14:27 GMT"}, {"version": "v2", "created": "Tue, 23 Mar 2021 08:37:39 GMT"}, {"version": "v3", "created": "Fri, 25 Jun 2021 04:51:15 GMT"}], "update_date": "2021-06-28", "authors_parsed": [["Luo", "Jianlan", ""], ["Sushkov", "Oleg", ""], ["Pevceviciute", "Rugile", ""], ["Lian", "Wenzhao", ""], ["Su", "Chang", ""], ["Vecerik", "Mel", ""], ["Ye", "Ning", ""], ["Schaal", "Stefan", ""], ["Scholz", "Jon", ""]]}, {"id": "2103.11516", "submitter": "Guansong Pang", "authors": "Guansong Pang, Longbing Cao, Ling Chen", "title": "Homophily Outlier Detection in Non-IID Categorical Data", "comments": "To appear in Data Ming and Knowledge Discovery Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of existing outlier detection methods assume that the outlier factors\n(i.e., outlierness scoring measures) of data entities (e.g., feature values and\ndata objects) are Independent and Identically Distributed (IID). This\nassumption does not hold in real-world applications where the outlierness of\ndifferent entities is dependent on each other and/or taken from different\nprobability distributions (non-IID). This may lead to the failure of detecting\nimportant outliers that are too subtle to be identified without considering the\nnon-IID nature. The issue is even intensified in more challenging contexts,\ne.g., high-dimensional data with many noisy features. This work introduces a\nnovel outlier detection framework and its two instances to identify outliers in\ncategorical data by capturing non-IID outlier factors. Our approach first\ndefines and incorporates distribution-sensitive outlier factors and their\ninterdependence into a value-value graph-based representation. It then models\nan outlierness propagation process in the value graph to learn the outlierness\nof feature values. The learned value outlierness allows for either direct\noutlier detection or outlying feature selection. The graph representation and\nmining approach is employed here to well capture the rich non-IID\ncharacteristics. Our empirical results on 15 real-world data sets with\ndifferent levels of data complexities show that (i) the proposed outlier\ndetection methods significantly outperform five state-of-the-art methods at the\n95%/99% confidence level, achieving 10%-28% AUC improvement on the 10 most\ncomplex data sets; and (ii) the proposed feature selection methods\nsignificantly outperform three competing methods in enabling subsequent outlier\ndetection of two different existing detectors.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 23:29:33 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Pang", "Guansong", ""], ["Cao", "Longbing", ""], ["Chen", "Ling", ""]]}, {"id": "2103.11517", "submitter": "Prashank Kadam", "authors": "Prashank Kadam, Ruiyang Xu, Karl Lieberherr", "title": "Dual Monte Carlo Tree Search", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  AlphaZero, using a combination of Deep Neural Networks and Monte Carlo Tree\nSearch (MCTS), has successfully trained reinforcement learning agents in a\ntabula-rasa way. The neural MCTS algorithm has been successful in finding\nnear-optimal strategies for games through self-play. However, the AlphaZero\nalgorithm has a significant drawback; it takes a long time to converge and\nrequires high computational power due to complex neural networks for solving\ngames like Chess, Go, Shogi, etc. Owing to this, it is very difficult to pursue\nneural MCTS research without cutting-edge hardware, which is a roadblock for\nmany aspiring neural MCTS researchers. In this paper, we propose a new neural\nMCTS algorithm, called Dual MCTS, which helps overcome these drawbacks. Dual\nMCTS uses two different search trees, a single deep neural network, and a new\nupdate technique for the search trees using a combination of the PUCB, a\nsliding-window, and the epsilon-greedy algorithm. This technique is applicable\nto any MCTS based algorithm to reduce the number of updates to the tree. We\nshow that Dual MCTS performs better than one of the most widely used neural\nMCTS algorithms, AlphaZero, for various symmetric and asymmetric games.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 23:34:11 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Kadam", "Prashank", ""], ["Xu", "Ruiyang", ""], ["Lieberherr", "Karl", ""]]}, {"id": "2103.11520", "submitter": "Gabriel Bertocco", "authors": "Gabriel Bertocco and Fernanda Andal\\'o and Anderson Rocha", "title": "Unsupervised and self-adaptative techniques for cross-domain person\n  re-identification", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Person Re-Identification (ReID) across non-overlapping cameras is a\nchallenging task and, for this reason, most works in the prior art rely on\nsupervised feature learning from a labeled dataset to match the same person in\ndifferent views. However, it demands the time-consuming task of labeling the\nacquired data, prohibiting its fast deployment, specially in forensic\nscenarios. Unsupervised Domain Adaptation (UDA) emerges as a promising\nalternative, as it performs feature-learning adaptation from a model trained on\na source to a target domain without identity-label annotation. However, most\nUDA-based algorithms rely upon a complex loss function with several\nhyper-parameters, which hinders the generalization to different scenarios.\nMoreover, as UDA depends on the translation between domains, it is important to\nselect the most reliable data from the unseen domain, thus avoiding error\npropagation caused by noisy examples on the target data -- an often overlooked\nproblem. In this sense, we propose a novel UDA-based ReID method that optimizes\na simple loss function with only one hyper-parameter and that takes advantage\nof triplets of samples created by a new offline strategy based on the diversity\nof cameras within a cluster. This new strategy adapts the model and also\nregularizes it, avoiding overfitting on the target domain. We also introduce a\nnew self-ensembling strategy, in which weights from different iterations are\naggregated to create a final model combining knowledge from distinct moments of\nthe adaptation. For evaluation, we consider three well-known deep learning\narchitectures and combine them for final decision-making. The proposed method\ndoes not use person re-ranking nor any label on the target domain, and\noutperforms the state of the art, with a much simpler setup, on the Market to\nDuke, the challenging Market1501 to MSMT17, and Duke to MSMT17 adaptation\nscenarios.\n", "versions": [{"version": "v1", "created": "Sun, 21 Mar 2021 23:58:39 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 18:22:33 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Bertocco", "Gabriel", ""], ["Andal\u00f3", "Fernanda", ""], ["Rocha", "Anderson", ""]]}, {"id": "2103.11542", "submitter": "Jian Wang", "authors": "Jian Wang and Chen Xu and Rong Li and Yiqun Ge and Jun Wang", "title": "Smart Scheduling based on Deep Reinforcement Learning for Cellular\n  Networks", "comments": "14 figures, submitted to a journal for possible publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To improve the system performance towards the Shannon limit, advanced radio\nresource management mechanisms play a fundamental role. In particular,\nscheduling should receive much attention, because it allocates radio resources\namong different users in terms of their channel conditions and QoS\nrequirements. The difficulties of scheduling algorithms are the tradeoffs need\nto be made among multiple objectives, such as throughput, fairness and packet\ndrop rate. We propose a smart scheduling scheme based on deep reinforcement\nlearning (DRL). We not only verify the performance gain achieved, but also\nprovide implementation-friend designs, i.e., a scalable neural network design\nfor the agent and a virtual environment training framework. With the scalable\nneural network design, the DRL agent can easily handle the cases when the\nnumber of active users is time-varying without the need to redesign and retrain\nthe DRL agent. Training the DRL agent in a virtual environment offline first\nand using it as the initial version in the practical usage helps to prevent the\nsystem from suffering from performance and robustness degradation due to the\ntime-consuming training. Through both simulations and field tests, we show that\nthe DRL-based smart scheduling outperforms the conventional scheduling method\nand can be adopted in practical systems.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 02:09:16 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Wang", "Jian", ""], ["Xu", "Chen", ""], ["Li", "Rong", ""], ["Ge", "Yiqun", ""], ["Wang", "Jun", ""]]}, {"id": "2103.11562", "submitter": "Wei Wang", "authors": "Wei Wang, Pedro P. B. de Gusmo, Bo Yang, Andrew Markham, and Niki\n  Trigoni", "title": "RadarLoc: Learning to Relocalize in FMCW Radar", "comments": "To appear in ICRA 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Relocalization is a fundamental task in the field of robotics and computer\nvision. There is considerable work in the field of deep camera relocalization,\nwhich directly estimates poses from raw images. However, learning-based methods\nhave not yet been applied to the radar sensory data. In this work, we\ninvestigate how to exploit deep learning to predict global poses from Emerging\nFrequency-Modulated Continuous Wave (FMCW) radar scans. Specifically, we\npropose a novel end-to-end neural network with self-attention, termed RadarLoc,\nwhich is able to estimate 6-DoF global poses directly. We also propose to\nimprove the localization performance by utilizing geometric constraints between\nradar scans. We validate our approach on the recently released challenging\noutdoor dataset Oxford Radar RobotCar. Comprehensive experiments demonstrate\nthat the proposed method outperforms radar-based localization and deep camera\nrelocalization methods by a significant margin.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 03:22:37 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Wang", "Wei", ""], ["de Gusmo", "Pedro P. B.", ""], ["Yang", "Bo", ""], ["Markham", "Andrew", ""], ["Trigoni", "Niki", ""]]}, {"id": "2103.11576", "submitter": "Ying Xu", "authors": "Ying Xu, Xu Zhong, Antonio Jimeno Yepes, Jey Han Lau", "title": "Grey-box Adversarial Attack And Defence For Sentiment Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a grey-box adversarial attack and defence framework for\nsentiment classification. We address the issues of differentiability, label\npreservation and input reconstruction for adversarial attack and defence in one\nunified framework. Our results show that once trained, the attacking model is\ncapable of generating high-quality adversarial examples substantially faster\n(one order of magnitude less in time) than state-of-the-art attacking methods.\nThese examples also preserve the original sentiment according to human\nevaluation. Additionally, our framework produces an improved classifier that is\nrobust in defending against multiple adversarial attacking methods. Code is\navailable at: https://github.com/ibm-aur-nlp/adv-def-text-dist.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 04:05:17 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Xu", "Ying", ""], ["Zhong", "Xu", ""], ["Yepes", "Antonio Jimeno", ""], ["Lau", "Jey Han", ""]]}, {"id": "2103.11624", "submitter": "Liu Yicheng", "authors": "Yicheng Liu, Jinghuai Zhang, Liangji Fang, Qinhong Jiang, Bolei Zhou", "title": "Multimodal Motion Prediction with Stacked Transformers", "comments": "CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting multiple plausible future trajectories of the nearby vehicles is\ncrucial for the safety of autonomous driving. Recent motion prediction\napproaches attempt to achieve such multimodal motion prediction by implicitly\nregularizing the feature or explicitly generating multiple candidate proposals.\nHowever, it remains challenging since the latent features may concentrate on\nthe most frequent mode of the data while the proposal-based methods depend\nlargely on the prior knowledge to generate and select the proposals. In this\nwork, we propose a novel transformer framework for multimodal motion\nprediction, termed as mmTransformer. A novel network architecture based on\nstacked transformers is designed to model the multimodality at feature level\nwith a set of fixed independent proposals. A region-based training strategy is\nthen developed to induce the multimodality of the generated proposals.\nExperiments on Argoverse dataset show that the proposed model achieves the\nstate-of-the-art performance on motion prediction, substantially improving the\ndiversity and the accuracy of the predicted trajectories. Demo video and code\nare available at https://decisionforce.github.io/mmTransformer.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 07:25:54 GMT"}, {"version": "v2", "created": "Wed, 24 Mar 2021 06:37:16 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Liu", "Yicheng", ""], ["Zhang", "Jinghuai", ""], ["Fang", "Liangji", ""], ["Jiang", "Qinhong", ""], ["Zhou", "Bolei", ""]]}, {"id": "2103.11647", "submitter": "Ning Ding", "authors": "Ning Ding, Xiaobin Wang, Yao Fu, Guangwei Xu, Rui Wang, Pengjun Xie,\n  Ying Shen, Fei Huang, Hai-Tao Zheng, Rui Zhang", "title": "Prototypical Representation Learning for Relation Extraction", "comments": "Accepted by ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recognizing relations between entities is a pivotal task of relational\nlearning. Learning relation representations from distantly-labeled datasets is\ndifficult because of the abundant label noise and complicated expressions in\nhuman language. This paper aims to learn predictive, interpretable, and robust\nrelation representations from distantly-labeled data that are effective in\ndifferent settings, including supervised, distantly supervised, and few-shot\nlearning. Instead of solely relying on the supervision from noisy labels, we\npropose to learn prototypes for each relation from contextual information to\nbest explore the intrinsic semantics of relations. Prototypes are\nrepresentations in the feature space abstracting the essential semantics of\nrelations between entities in sentences. We learn prototypes based on\nobjectives with clear geometric interpretation, where the prototypes are unit\nvectors uniformly dispersed in a unit ball, and statement embeddings are\ncentered at the end of their corresponding prototype vectors on the surface of\nthe ball. This approach allows us to learn meaningful, interpretable prototypes\nfor the final classification. Results on several relation learning tasks show\nthat our model significantly outperforms the previous state-of-the-art models.\nWe further demonstrate the robustness of the encoder and the interpretability\nof prototypes with extensive experiments.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 08:11:43 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Ding", "Ning", ""], ["Wang", "Xiaobin", ""], ["Fu", "Yao", ""], ["Xu", "Guangwei", ""], ["Wang", "Rui", ""], ["Xie", "Pengjun", ""], ["Shen", "Ying", ""], ["Huang", "Fei", ""], ["Zheng", "Hai-Tao", ""], ["Zhang", "Rui", ""]]}, {"id": "2103.11658", "submitter": "Shiyu Xuan", "authors": "Shiyu Xuan, Shiliang Zhang", "title": "Intra-Inter Camera Similarity for Unsupervised Person Re-Identification", "comments": "CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of unsupervised person Re-Identification (Re-ID) works produce\npseudo-labels by measuring the feature similarity without considering the\ndistribution discrepancy among cameras, leading to degraded accuracy in label\ncomputation across cameras. This paper targets to address this challenge by\nstudying a novel intra-inter camera similarity for pseudo-label generation. We\ndecompose the sample similarity computation into two stage, i.e., the\nintra-camera and inter-camera computations, respectively. The intra-camera\ncomputation directly leverages the CNN features for similarity computation\nwithin each camera. Pseudo-labels generated on different cameras train the\nre-id model in a multi-branch network. The second stage considers the\nclassification scores of each sample on different cameras as a new feature\nvector. This new feature effectively alleviates the distribution discrepancy\namong cameras and generates more reliable pseudo-labels. We hence train our\nre-id model in two stages with intra-camera and inter-camera pseudo-labels,\nrespectively. This simple intra-inter camera similarity produces surprisingly\ngood performance on multiple datasets, e.g., achieves rank-1 accuracy of 89.5%\non the Market1501 dataset, outperforming the recent unsupervised works by 9+%,\nand is comparable with the latest transfer learning works that leverage extra\nannotations.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 08:29:04 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Xuan", "Shiyu", ""], ["Zhang", "Shiliang", ""]]}, {"id": "2103.11686", "submitter": "Wei Zhang", "authors": "Wei Zhang, Yunfeng Zhang and Ning Liu", "title": "Enhancing the Generalization Performance and Speed Up Training for\n  DRL-based Mapless Navigation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training an agent to navigate with DRL is data-hungry, which requires\nmillions of training steps. Besides, the DRL agents performing well in training\nscenarios are found to perform poorly in some unseen real-world scenarios. In\nthis paper, we discuss why the DRL agent fails in such unseen scenarios and\nfind the representation of LiDAR readings is the key factor behind the agent's\nperformance degradation. Moreover, we propose an easy, but efficient input\npre-processing (IP) approach to accelerate training and enhance the performance\nof the DRL agent in such scenarios. The proposed IP functions can highlight the\nimportant short-distance values of laser scans and compress the range of\nless-important long-distance values. Extensive comparative experiments are\ncarried out, and the experimental results demonstrate the high performance of\nthe proposed IP approaches.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 09:36:51 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Zhang", "Wei", ""], ["Zhang", "Yunfeng", ""], ["Liu", "Ning", ""]]}, {"id": "2103.11692", "submitter": "Ramon Fraga Pereira", "authors": "Ramon Fraga Pereira, Francesco Fuggitti, and Giuseppe De Giacomo", "title": "Recognizing LTLf/PLTLf Goals in Fully Observable Non-Deterministic\n  Domain Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Goal Recognition is the task of discerning the correct intended goal that an\nagent aims to achieve, given a set of possible goals, a domain model, and a\nsequence of observations as a sample of the plan being executed in the\nenvironment. Existing approaches assume that the possible goals are formalized\nas a conjunction in deterministic settings. In this paper, we develop a novel\napproach that is capable of recognizing temporally extended goals in Fully\nObservable Non-Deterministic (FOND) planning domain models, focusing on goals\non finite traces expressed in Linear Temporal Logic (LTLf) and (Pure) Past\nLinear Temporal Logic (PLTLf). We empirically evaluate our goal recognition\napproach using different LTLf and PLTLf goals over six common FOND planning\ndomain models, and show that our approach is accurate to recognize temporally\nextended goals at several levels of observability.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 09:46:03 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Pereira", "Ramon Fraga", ""], ["Fuggitti", "Francesco", ""], ["De Giacomo", "Giuseppe", ""]]}, {"id": "2103.11696", "submitter": "Dong Chen", "authors": "Dong Chen and Duoqian Miao", "title": "Control Distance IoU and Control Distance IoU Loss Function for Better\n  Bounding Box Regression", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Numerous improvements for feedback mechanisms have contributed to the great\nprogress in object detection. In this paper, we first present an\nevaluation-feedback module, which is proposed to consist of evaluation system\nand feedback mechanism. Then we analyze and summarize the disadvantages and\nimprovements of traditional evaluation-feedback module. Finally, we focus on\nboth the evaluation system and the feedback mechanism, and propose Control\nDistance IoU and Control Distance IoU loss function (or CDIoU and CDIoU loss\nfor short) without increasing parameters or FLOPs in models, which show\ndifferent significant enhancements on several classical and emerging models.\nSome experiments and comparative tests show that coordinated\nevaluation-feedback module can effectively improve model performance. CDIoU and\nCDIoU loss have different excellent performances in several models such as\nFaster R-CNN, YOLOv4, RetinaNet and ATSS. There is a maximum AP improvement of\n1.9% and an average AP of 0.8% improvement on MS COCO dataset, compared to\ntraditional evaluation-feedback modules.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 09:57:25 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Chen", "Dong", ""], ["Miao", "Duoqian", ""]]}, {"id": "2103.11715", "submitter": "Antonios Liapis", "authors": "Antonios Liapis, Hector P. Martinez, Julian Togelius and Georgios N.\n  Yannakakis", "title": "Transforming Exploratory Creativity with DeLeNoX", "comments": "8 pages", "journal-ref": "Proceedings of the Fourth International Conference on\n  Computational Creativity, 2013, pages 56-63", "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce DeLeNoX (Deep Learning Novelty Explorer), a system that\nautonomously creates artifacts in constrained spaces according to its own\nevolving interestingness criterion. DeLeNoX proceeds in alternating phases of\nexploration and transformation. In the exploration phases, a version of novelty\nsearch augmented with constraint handling searches for maximally diverse\nartifacts using a given distance function. In the transformation phases, a deep\nlearning autoencoder learns to compress the variation between the found\nartifacts into a lower-dimensional space. The newly trained encoder is then\nused as the basis for a new distance function, transforming the criteria for\nthe next exploration phase. In the current paper, we apply DeLeNoX to the\ncreation of spaceships suitable for use in two-dimensional arcade-style\ncomputer games, a representative problem in procedural content generation in\ngames. We also situate DeLeNoX in relation to the distinction between\nexploratory and transformational creativity, and in relation to Schmidhuber's\ntheory of creativity through the drive for compression progress.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 10:39:29 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Liapis", "Antonios", ""], ["Martinez", "Hector P.", ""], ["Togelius", "Julian", ""], ["Yannakakis", "Georgios N.", ""]]}, {"id": "2103.11726", "submitter": "Antonios Liapis", "authors": "Panagiotis Migkotzidis and Antonios Liapis", "title": "SuSketch: Surrogate Models of Gameplay as a Design Assistant", "comments": "To be published in IEEE Transactions on Games, 11 pages", "journal-ref": null, "doi": "10.1109/TG.2021.3068360", "report-no": null, "categories": "cs.AI cs.HC cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces SuSketch, a design tool for first person shooter\nlevels. SuSketch provides the designer with gameplay predictions for two\ncompeting players of specific character classes. The interface allows the\ndesigner to work side-by-side with an artificially intelligent creator and to\nreceive varied types of feedback such as path information, predicted balance\nbetween players in a complete playthrough, or a predicted heatmap of the\nlocations of player deaths. The system also proactively designs alternatives to\nthe level and class pairing, and presents them to the designer as suggestions\nthat improve the predicted balance of the game. SuSketch offers a new way of\nintegrating machine learning into mixed-initiative co-creation tools, as a\nsurrogate of human play trained on a large corpus of artificial playtraces. A\nuser study with 16 game developers indicated that the tool was easy to use, but\nalso highlighted a need to make SuSketch more accessible and more explainable.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 11:05:27 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Migkotzidis", "Panagiotis", ""], ["Liapis", "Antonios", ""]]}, {"id": "2103.11731", "submitter": "Gongjie Zhang", "authors": "Gongjie Zhang, Zhipeng Luo, Kaiwen Cui, Shijian Lu", "title": "Meta-DETR: Few-Shot Object Detection via Unified Image-Level\n  Meta-Learning", "comments": "Codes and data will be made publicly available in the future", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Few-shot object detection aims at detecting novel objects with only a few\nannotated examples. Prior works have proved meta-learning a promising solution,\nand most of them essentially address detection by meta-learning over regions\nfor their classification and location fine-tuning. However, these methods\nsubstantially rely on initially well-located region proposals, which are\nusually hard to obtain under the few-shot settings. This paper presents a novel\nmeta-detector framework, namely Meta-DETR, which eliminates region-wise\nprediction and instead meta-learns object localization and classification at\nimage level in a unified and complementary manner. Specifically, it first\nencodes both support and query images into category-specific features and then\nfeeds them into a category-agnostic decoder to directly generate predictions\nfor specific categories. To facilitate meta-learning with deep networks, we\ndesign a simple but effective Semantic Alignment Mechanism (SAM), which aligns\nhigh-level and low-level feature semantics to improve the generalization of\nmeta-learned representations. Experiments over multiple few-shot object\ndetection benchmarks show that Meta-DETR outperforms state-of-the-art methods\nby large margins.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 11:14:00 GMT"}, {"version": "v2", "created": "Thu, 25 Mar 2021 04:54:43 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Zhang", "Gongjie", ""], ["Luo", "Zhipeng", ""], ["Cui", "Kaiwen", ""], ["Lu", "Shijian", ""]]}, {"id": "2103.11744", "submitter": "Peng Zhao", "authors": "Hongying Liu, Peng Zhao, Zhubo Ruan, Fanhua Shang, and Yuanyuan Liu", "title": "Large Motion Video Super-Resolution with Dual Subnet and Multi-Stage\n  Communicated Upsampling", "comments": "Accepted by AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video super-resolution (VSR) aims at restoring a video in low-resolution (LR)\nand improving it to higher-resolution (HR). Due to the characteristics of video\ntasks, it is very important that motion information among frames should be well\nconcerned, summarized and utilized for guidance in a VSR algorithm. Especially,\nwhen a video contains large motion, conventional methods easily bring\nincoherent results or artifacts. In this paper, we propose a novel deep neural\nnetwork with Dual Subnet and Multi-stage Communicated Upsampling (DSMC) for\nsuper-resolution of videos with large motion. We design a new module named\nU-shaped residual dense network with 3D convolution (U3D-RDN) for fine implicit\nmotion estimation and motion compensation (MEMC) as well as coarse spatial\nfeature extraction. And we present a new Multi-Stage Communicated Upsampling\n(MSCU) module to make full use of the intermediate results of upsampling for\nguiding the VSR. Moreover, a novel dual subnet is devised to aid the training\nof our DSMC, whose dual loss helps to reduce the solution space as well as\nenhance the generalization ability. Our experimental results confirm that our\nmethod achieves superior performance on videos with large motion compared to\nstate-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 11:52:12 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Liu", "Hongying", ""], ["Zhao", "Peng", ""], ["Ruan", "Zhubo", ""], ["Shang", "Fanhua", ""], ["Liu", "Yuanyuan", ""]]}, {"id": "2103.11750", "submitter": "Andrea Cossu", "authors": "Antonio Carta, Andrea Cossu, Federico Errica, Davide Bacciu", "title": "Catastrophic Forgetting in Deep Graph Networks: an Introductory\n  Benchmark for Graph Classification", "comments": "Accepted at the 2021 Web Conference Workshop on Graph Learning\n  Benchmarks (GLB 2021). Code available at\n  https://github.com/diningphil/continual_learning_for_graphs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the phenomenon of catastrophic forgetting in the graph\nrepresentation learning scenario. The primary objective of the analysis is to\nunderstand whether classical continual learning techniques for flat and\nsequential data have a tangible impact on performances when applied to graph\ndata. To do so, we experiment with a structure-agnostic model and a deep graph\nnetwork in a robust and controlled environment on three different datasets. The\nbenchmark is complemented by an investigation on the effect of\nstructure-preserving regularization techniques on catastrophic forgetting. We\nfind that replay is the most effective strategy in so far, which also benefits\nthe most from the use of regularization. Our findings suggest interesting\nfuture research at the intersection of the continual and graph representation\nlearning fields. Finally, we provide researchers with a flexible software\nframework to reproduce our results and carry out further experiments.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 12:07:21 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Carta", "Antonio", ""], ["Cossu", "Andrea", ""], ["Errica", "Federico", ""], ["Bacciu", "Davide", ""]]}, {"id": "2103.11761", "submitter": "Adrian Rebmann", "authors": "Adrian Rebmann and Han van der Aa", "title": "Extracting Semantic Process Information from the Natural Language in\n  Event Logs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Process mining focuses on the analysis of recorded event data in order to\ngain insights about the true execution of business processes. While\nfoundational process mining techniques treat such data as sequences of abstract\nevents, more advanced techniques depend on the availability of specific kinds\nof information, such as resources in organizational mining and business objects\nin artifact-centric analysis. However, this information is generally not\nreadily available, but rather associated with events in an ad hoc manner, often\neven as part of unstructured textual attributes. Given the size and complexity\nof event logs, this calls for automated support to extract such process\ninformation and, thereby, enable advanced process mining techniques. In this\npaper, we present an approach that achieves this through so-called semantic\nrole labeling of event data. We combine the analysis of textual attribute\nvalues, based on a state-of-the-art language model, with a novel attribute\nclassification technique. In this manner, our approach extracts information\nabout up to eight semantic roles per event. We demonstrate the approach's\nefficacy through a quantitative evaluation using a broad range of event logs\nand demonstrate the usefulness of the extracted information in a case study.\n", "versions": [{"version": "v1", "created": "Sat, 6 Mar 2021 08:39:04 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Rebmann", "Adrian", ""], ["van der Aa", "Han", ""]]}, {"id": "2103.11764", "submitter": "Sara Durrani", "authors": "Sara Durrani and Umair Arshad", "title": "Transfer learning from High-Resource to Low-Resource Language Improves\n  Speech Affect Recognition Classification Accuracy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Speech Affect Recognition is a problem of extracting emotional affects from\naudio data. Low resource languages corpora are rear and affect recognition is a\ndifficult task in cross-corpus settings. We present an approach in which the\nmodel is trained on high resource language and fine-tune to recognize affects\nin low resource language. We train the model in same corpus setting on SAVEE,\nEMOVO, Urdu, and IEMOCAP by achieving baseline accuracy of 60.45, 68.05, 80.34,\nand 56.58 percent respectively. For capturing the diversity of affects in\nlanguages cross-corpus evaluations are discussed in detail. We find that\naccuracy improves by adding the domain target data into the training data.\nFinally, we show that performance is improved for low resource language speech\naffect recognition by achieving the UAR OF 69.32 and 68.2 for Urdu and Italian\nspeech affects.\n", "versions": [{"version": "v1", "created": "Thu, 4 Mar 2021 08:17:19 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Durrani", "Sara", ""], ["Arshad", "Umair", ""]]}, {"id": "2103.11782", "submitter": "Aidong Yang", "authors": "Aidong Yang, Xinlang Yue, Ye Ouyang", "title": "Reinforcement Learning Assisted Beamforming for Inter-cell Interference\n  Mitigation in 5G Massive MIMO Networks", "comments": "There is an error in section 4 about what are the definitions of\n  states and actions, which will affect the performance of the algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AI cs.NI math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Beamforming is an essential technology in the 5G massive\nmultiple-input-multiple-output (MMIMO) communications, which are subject to\nmany impairments due to the nature of wireless transmission channel, i.e. the\nair. The inter-cell interference (ICI) is one of the main impairments faced by\n5G communications due to frequency-reuse technologies. In this paper, we\npropose a reinforcement learning (RL) assisted full dynamic beamforming for ICI\nmitigation in 5G downlink. The proposed algorithm is a joint of beamforming and\nfull dynamic Q-learning technology to minimize the ICI, and results in a\nlow-complexity method without channel estimation. Performance analysis shows\nthe quality of service improvement in terms of\nsignal-to-interference-plus-noise-ratio (SINR) and computational complexity\ncompared to other algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jan 2021 07:18:07 GMT"}, {"version": "v2", "created": "Fri, 2 Jul 2021 02:13:52 GMT"}], "update_date": "2021-07-05", "authors_parsed": [["Yang", "Aidong", ""], ["Yue", "Xinlang", ""], ["Ouyang", "Ye", ""]]}, {"id": "2103.11793", "submitter": "Xiang Pan", "authors": "Wanjun Huang, Xiang Pan, Minghua Chen, and Steven H. Low", "title": "DeepOPF-V: Solving AC-OPF Problems Efficiently", "comments": "4 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.AI cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AC optimal power flow (AC-OPF) problems need to be solved more frequently in\nthe future to maintain stable and economic power system operation. To tackle\nthis challenge, a deep neural network-based voltage-constrained approach\n(DeepOPF-V) is proposed to solve AC-OPF problems with high computational\nefficiency. Its unique design predicts voltages of all buses and then uses them\nto reconstruct the remaining variables without solving non-linear AC power flow\nequations. A fast post-processing process is developed to enforce the box\nconstraints. The effectiveness of DeepOPF-V is validated by simulations on IEEE\n118/300-bus systems and a 2000-bus test system. Compared with existing studies,\nDeepOPF-V achieves decent computation speedup up to four orders of magnitude\nand comparable performance in optimality gap and preserving the feasibility of\nthe solution.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 12:59:06 GMT"}, {"version": "v2", "created": "Mon, 19 Jul 2021 03:15:44 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Huang", "Wanjun", ""], ["Pan", "Xiang", ""], ["Chen", "Minghua", ""], ["Low", "Steven H.", ""]]}, {"id": "2103.11795", "submitter": "Bojun Huang", "authors": "Fei Yuan, Longtu Zhang, Huang Bojun, Yaobo Liang", "title": "Simpson's Bias in NLP Training", "comments": "AAAI 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In most machine learning tasks, we evaluate a model $M$ on a given data\npopulation $S$ by measuring a population-level metric $F(S;M)$. Examples of\nsuch evaluation metric $F$ include precision/recall for (binary) recognition,\nthe F1 score for multi-class classification, and the BLEU metric for language\ngeneration. On the other hand, the model $M$ is trained by optimizing a\nsample-level loss $G(S_t;M)$ at each learning step $t$, where $S_t$ is a subset\nof $S$ (a.k.a. the mini-batch). Popular choices of $G$ include cross-entropy\nloss, the Dice loss, and sentence-level BLEU scores. A fundamental assumption\nbehind this paradigm is that the mean value of the sample-level loss $G$, if\naveraged over all possible samples, should effectively represent the\npopulation-level metric $F$ of the task, such as, that $\\mathbb{E}[ G(S_t;M) ]\n\\approx F(S;M)$.\n  In this paper, we systematically investigate the above assumption in several\nNLP tasks. We show, both theoretically and experimentally, that some popular\ndesigns of the sample-level loss $G$ may be inconsistent with the true\npopulation-level metric $F$ of the task, so that models trained to optimize the\nformer can be substantially sub-optimal to the latter, a phenomenon we call it,\nSimpson's bias, due to its deep connections with the classic paradox known as\nSimpson's reversal paradox in statistics and social sciences.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 06:19:37 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Yuan", "Fei", ""], ["Zhang", "Longtu", ""], ["Bojun", "Huang", ""], ["Liang", "Yaobo", ""]]}, {"id": "2103.11807", "submitter": "Kenji Suzuki", "authors": "Kenji Suzuki, Yoshiyuki Kobayashi, Takuya Narihira", "title": "Data Cleansing for Deep Neural Networks with Storage-efficient\n  Approximation of Influence Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Identifying the influence of training data for data cleansing can improve the\naccuracy of deep learning. An approach with stochastic gradient descent (SGD)\ncalled SGD-influence to calculate the influence scores was proposed, but, the\ncalculation costs are expensive. It is necessary to temporally store the\nparameters of the model during training phase for inference phase to calculate\ninfluence sores. In close connection with the previous method, we propose a\nmethod to reduce cache files to store the parameters in training phase for\ncalculating inference score. We only adopt the final parameters in last epoch\nfor influence functions calculation. In our experiments on classification, the\ncache size of training using MNIST dataset with our approach is 1.236 MB. On\nthe other hand, the previous method used cache size of 1.932 GB in last epoch.\nIt means that cache size has been reduced to 1/1,563. We also observed the\naccuracy improvement by data cleansing with removal of negatively influential\ndata using our approach as well as the previous method. Moreover, our simple\nand general proposed method to calculate influence scores is available on our\nauto ML tool without programing, Neural Network Console. The source code is\nalso available.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 13:08:46 GMT"}, {"version": "v2", "created": "Tue, 1 Jun 2021 08:23:25 GMT"}], "update_date": "2021-06-02", "authors_parsed": [["Suzuki", "Kenji", ""], ["Kobayashi", "Yoshiyuki", ""], ["Narihira", "Takuya", ""]]}, {"id": "2103.11811", "submitter": "David Adelani", "authors": "David Ifeoluwa Adelani, Jade Abbott, Graham Neubig, Daniel D'souza,\n  Julia Kreutzer, Constantine Lignos, Chester Palen-Michel, Happy Buzaaba,\n  Shruti Rijhwani, Sebastian Ruder, Stephen Mayhew, Israel Abebe Azime,\n  Shamsuddeen Muhammad, Chris Chinenye Emezue, Joyce Nakatumba-Nabende, Perez\n  Ogayo, Anuoluwapo Aremu, Catherine Gitau, Derguene Mbaye, Jesujoba Alabi,\n  Seid Muhie Yimam, Tajuddeen Gwadabe, Ignatius Ezeani, Rubungo Andre\n  Niyongabo, Jonathan Mukiibi, Verrah Otiende, Iroro Orife, Davis David, Samba\n  Ngom, Tosin Adewumi, Paul Rayson, Mofetoluwa Adeyemi, Gerald Muriuki,\n  Emmanuel Anebi, Chiamaka Chukwuneke, Nkiruka Odu, Eric Peter Wairagala,\n  Samuel Oyerinde, Clemencia Siro, Tobius Saul Bateesa, Temilola Oloyede,\n  Yvonne Wambui, Victor Akinode, Deborah Nabagereka, Maurice Katusiime, Ayodele\n  Awokoya, Mouhamadane MBOUP, Dibora Gebreyohannes, Henok Tilaye, Kelechi\n  Nwaike, Degaga Wolde, Abdoulaye Faye, Blessing Sibanda, Orevaoghene Ahia,\n  Bonaventure F. P. Dossou, Kelechi Ogueji, Thierno Ibrahima DIOP, Abdoulaye\n  Diallo, Adewale Akinfaderin, Tendai Marengereke, and Salomey Osei", "title": "MasakhaNER: Named Entity Recognition for African Languages", "comments": "Accepted to TACL 2021, pre-MIT Press publication version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We take a step towards addressing the under-representation of the African\ncontinent in NLP research by creating the first large publicly available\nhigh-quality dataset for named entity recognition (NER) in ten African\nlanguages, bringing together a variety of stakeholders. We detail\ncharacteristics of the languages to help researchers understand the challenges\nthat these languages pose for NER. We analyze our datasets and conduct an\nextensive empirical evaluation of state-of-the-art methods across both\nsupervised and transfer learning settings. We release the data, code, and\nmodels in order to inspire future research on African NLP.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 13:12:44 GMT"}, {"version": "v2", "created": "Mon, 5 Jul 2021 15:14:32 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Adelani", "David Ifeoluwa", ""], ["Abbott", "Jade", ""], ["Neubig", "Graham", ""], ["D'souza", "Daniel", ""], ["Kreutzer", "Julia", ""], ["Lignos", "Constantine", ""], ["Palen-Michel", "Chester", ""], ["Buzaaba", "Happy", ""], ["Rijhwani", "Shruti", ""], ["Ruder", "Sebastian", ""], ["Mayhew", "Stephen", ""], ["Azime", "Israel Abebe", ""], ["Muhammad", "Shamsuddeen", ""], ["Emezue", "Chris Chinenye", ""], ["Nakatumba-Nabende", "Joyce", ""], ["Ogayo", "Perez", ""], ["Aremu", "Anuoluwapo", ""], ["Gitau", "Catherine", ""], ["Mbaye", "Derguene", ""], ["Alabi", "Jesujoba", ""], ["Yimam", "Seid Muhie", ""], ["Gwadabe", "Tajuddeen", ""], ["Ezeani", "Ignatius", ""], ["Niyongabo", "Rubungo Andre", ""], ["Mukiibi", "Jonathan", ""], ["Otiende", "Verrah", ""], ["Orife", "Iroro", ""], ["David", "Davis", ""], ["Ngom", "Samba", ""], ["Adewumi", "Tosin", ""], ["Rayson", "Paul", ""], ["Adeyemi", "Mofetoluwa", ""], ["Muriuki", "Gerald", ""], ["Anebi", "Emmanuel", ""], ["Chukwuneke", "Chiamaka", ""], ["Odu", "Nkiruka", ""], ["Wairagala", "Eric Peter", ""], ["Oyerinde", "Samuel", ""], ["Siro", "Clemencia", ""], ["Bateesa", "Tobius Saul", ""], ["Oloyede", "Temilola", ""], ["Wambui", "Yvonne", ""], ["Akinode", "Victor", ""], ["Nabagereka", "Deborah", ""], ["Katusiime", "Maurice", ""], ["Awokoya", "Ayodele", ""], ["MBOUP", "Mouhamadane", ""], ["Gebreyohannes", "Dibora", ""], ["Tilaye", "Henok", ""], ["Nwaike", "Kelechi", ""], ["Wolde", "Degaga", ""], ["Faye", "Abdoulaye", ""], ["Sibanda", "Blessing", ""], ["Ahia", "Orevaoghene", ""], ["Dossou", "Bonaventure F. P.", ""], ["Ogueji", "Kelechi", ""], ["DIOP", "Thierno Ibrahima", ""], ["Diallo", "Abdoulaye", ""], ["Akinfaderin", "Adewale", ""], ["Marengereke", "Tendai", ""], ["Osei", "Salomey", ""]]}, {"id": "2103.11845", "submitter": "Hua Wei", "authors": "Hua Wei, Chacha Chen, Chang Liu, Guanjie Zheng, Zhenhui Li", "title": "Learning to Simulate on Sparse Trajectory Data", "comments": "Accepted by ECML-PKDD 2020, Best Applied Data Science Paper. 16\n  pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Simulation of the real-world traffic can be used to help validate the\ntransportation policies. A good simulator means the simulated traffic is\nsimilar to real-world traffic, which often requires dense traffic trajectories\n(i.e., with a high sampling rate) to cover dynamic situations in the real\nworld. However, in most cases, the real-world trajectories are sparse, which\nmakes simulation challenging. In this paper, we present a novel framework\nImInGAIL to address the problem of learning to simulate the driving behavior\nfrom sparse real-world data. The proposed architecture incorporates data\ninterpolation with the behavior learning process of imitation learning. To the\nbest of our knowledge, we are the first to tackle the data sparsity issue for\nbehavior learning problems. We investigate our framework on both synthetic and\nreal-world trajectory datasets of driving vehicles, showing that our method\noutperforms various baselines and state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 13:42:11 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Wei", "Hua", ""], ["Chen", "Chacha", ""], ["Liu", "Chang", ""], ["Zheng", "Guanjie", ""], ["Li", "Zhenhui", ""]]}, {"id": "2103.11863", "submitter": "Zahra Nili Ahmadabadi", "authors": "Karan Sridharan, Zahra Nili Ahmadabadi, Jeffrey Hudack", "title": "Online search of unknown terrains using a dynamical system-based path\n  planning approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Surveillance and exploration of large environments is a tedious task. In\nspaces with limited environmental cues, random-like search appears to be an\neffective approach as it allows the robot to perform online coverage of\nenvironments using a simple design. One way to generate random-like scanning is\nto use nonlinear dynamical systems to impart chaos into the robot's controller.\nThis will result in generation of unpredictable but at the same time\ndeterministic trajectories, allowing the designer to control the system and\nachieve a high scanning coverage. However, the unpredictability comes at the\ncost of increased coverage time and lack of scalability, both of which have\nbeen ignored by the state-of-the-art chaotic path planners. This study\nintroduces a new scalable technique that helps a robot to steer away from the\nobstacles and cover the entire space in a short period of time. The technique\ninvolves coupling and manipulating two chaotic systems to minimize the coverage\ntime and enable scanning of unknown environments with different properties\nonline. Using this technique resulted in 49% boost, on average, in the robot's\nperformance compared to the state-of-the-art planners. While ensuring\nunpredictability in the paths, the overall performance of the chaotic planner\nremained comparable to optimal systems.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 14:00:04 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Sridharan", "Karan", ""], ["Ahmadabadi", "Zahra Nili", ""], ["Hudack", "Jeffrey", ""]]}, {"id": "2103.11878", "submitter": "Yuchen Jiang", "authors": "Yuchen Jiang, Shuming Ma, Dongdong Zhang, Jian Yang, Haoyang Huang and\n  Ming Zhou", "title": "BlonD: An Automatic Evaluation Metric for Document-level\n  MachineTranslation", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Standard automatic metrics (such as BLEU) are problematic for document-level\nMT evaluation. They can neither distinguish document-level improvements in\ntranslation quality from sentence-level ones nor can they identify the specific\ndiscourse phenomena that caused the translation errors. To address these\nproblems, we propose an automatic metric BlonD for document-level machine\ntranslation evaluation. BlonD takes discourse coherence into consideration by\ncalculating the recall and distance of check-pointing phrases and tags, and\nfurther provides comprehensive evaluation scores by combining with n-gram.\nExtensive comparisons between BlonD and existing evaluation metrics are\nconducted to illustrate their critical distinctions. Experimental results show\nthat BlonD has a much higher document-level sensitivity with respect to\nprevious metrics. The human evaluation also reveals high Pearson R correlation\nvalues between BlonD scores and manual quality judgments.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 14:14:58 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Jiang", "Yuchen", ""], ["Ma", "Shuming", ""], ["Zhang", "Dongdong", ""], ["Yang", "Jian", ""], ["Huang", "Haoyang", ""], ["Zhou", "Ming", ""]]}, {"id": "2103.11887", "submitter": "Wandong Zhang", "authors": "Yimin Yang, Wandong Zhang, Jonathan Wu, Will Zhao, Ao Chen", "title": "Deconvolution-and-convolution Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  2D Convolutional neural network (CNN) has arguably become the de facto\nstandard for computer vision tasks. Recent findings, however, suggest that CNN\nmay not be the best option for 1D pattern recognition, especially for datasets\nwith over 1 M training samples, e.g., existing CNN-based methods for 1D signals\nare highly reliant on human pre-processing. Common practices include utilizing\ndiscrete Fourier transform (DFT) to reconstruct 1D signal into 2D array. To add\nto extant knowledge, in this paper, a novel 1D data processing algorithm is\nproposed for 1D big data analysis through learning a deep\ndeconvolutional-convolutional network. Rather than resorting to human-based\ntechniques, we employed deconvolution layers to convert 1 D signals into 2D\ndata. On top of the deconvolution model, the data was identified by a 2D CNN.\nCompared with the existing 1D signal processing algorithms, DCNet boasts the\nadvantages of less human-made inference and higher generalization performance.\nOur experimental results from a varying number of training patterns (50 K to 11\nM) from classification and regression demonstrate the desirability of our new\napproach.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 14:32:09 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Yang", "Yimin", ""], ["Zhang", "Wandong", ""], ["Wu", "Jonathan", ""], ["Zhao", "Will", ""], ["Chen", "Ao", ""]]}, {"id": "2103.11909", "submitter": "Jan Philip Wahle", "authors": "Jan Philip Wahle, Terry Ruas, Tom\\'a\\v{s} Folt\\'ynek, Norman Meuschke,\n  Bela Gipp", "title": "Identifying Machine-Paraphrased Plagiarism", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Employing paraphrasing tools to conceal plagiarized text is a severe threat\nto academic integrity. To enable the detection of machine-paraphrased text, we\nevaluate the effectiveness of five pre-trained word embedding models combined\nwith machine learning classifiers and state-of-the-art neural language models.\nWe analyze preprints of research papers, graduation theses, and Wikipedia\narticles, which we paraphrased using different configurations of the tools\nSpinBot and SpinnerChief. The best performing technique, Longformer, achieved\nan average F1 score of 80.99% (F1=99.68% for SpinBot and F1=71.64% for\nSpinnerChief cases), while human evaluators achieved F1=78.4% for SpinBot and\nF1=65.6% for SpinnerChief cases. We show that the automated classification\nalleviates shortcomings of widely-used text-matching systems, such as Turnitin\nand PlagScan. To facilitate future research, all data, code, and two web\napplications showcasing our contributions are openly available.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 14:54:54 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Wahle", "Jan Philip", ""], ["Ruas", "Terry", ""], ["Folt\u00fdnek", "Tom\u00e1\u0161", ""], ["Meuschke", "Norman", ""], ["Gipp", "Bela", ""]]}, {"id": "2103.11912", "submitter": "Gon\\c{c}alo Mordido", "authors": "Gon\\c{c}alo Mordido, Haojin Yang, Christoph Meinel", "title": "Evaluating Post-Training Compression in GANs using Locality-Sensitive\n  Hashing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The analysis of the compression effects in generative adversarial networks\n(GANs) after training, i.e. without any fine-tuning, remains an unstudied,\nalbeit important, topic with the increasing trend of their computation and\nmemory requirements. While existing works discuss the difficulty of compressing\nGANs during training, requiring novel methods designed with the instability of\nGANs training in mind, we show that existing compression methods (namely\nclipping and quantization) may be directly applied to compress GANs\npost-training, without any additional changes. High compression levels may\ndistort the generated set, likely leading to an increase of outliers that may\nnegatively affect the overall assessment of existing k-nearest neighbor (KNN)\nbased metrics. We propose two new precision and recall metrics based on\nlocality-sensitive hashing (LSH), which, on top of increasing the outlier\nrobustness, decrease the complexity of assessing an evaluation sample against\n$n$ reference samples from $O(n)$ to $O(\\log(n))$, if using LSH and KNN, and to\n$O(1)$, if only applying LSH. We show that low-bit compression of several\npre-trained GANs on multiple datasets induces a trade-off between precision and\nrecall, retaining sample quality while sacrificing sample diversity.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 14:55:24 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Mordido", "Gon\u00e7alo", ""], ["Yang", "Haojin", ""], ["Meinel", "Christoph", ""]]}, {"id": "2103.11927", "submitter": "Zhixin Pan", "authors": "Zhixin Pan and Prabhat Mishra", "title": "Hardware Acceleration of Explainable Machine Learning using Tensor\n  Processing Units", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Machine learning (ML) is successful in achieving human-level performance in\nvarious fields. However, it lacks the ability to explain an outcome due to its\nblack-box nature. While existing explainable ML is promising, almost all of\nthese methods focus on formatting interpretability as an optimization problem.\nSuch a mapping leads to numerous iterations of time-consuming complex\ncomputations, which limits their applicability in real-time applications. In\nthis paper, we propose a novel framework for accelerating explainable ML using\nTensor Processing Units (TPUs). The proposed framework exploits the synergy\nbetween matrix convolution and Fourier transform, and takes full advantage of\nTPU's natural ability in accelerating matrix computations. Specifically, this\npaper makes three important contributions. (1) To the best of our knowledge,\nour proposed work is the first attempt in enabling hardware acceleration of\nexplainable ML using TPUs. (2) Our proposed approach is applicable across a\nwide variety of ML algorithms, and effective utilization of TPU-based\nacceleration can lead to real-time outcome interpretation. (3) Extensive\nexperimental results demonstrate that our proposed approach can provide an\norder-of-magnitude speedup in both classification time (25x on average) and\ninterpretation time (13x on average) compared to state-of-the-art techniques.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 15:11:45 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Pan", "Zhixin", ""], ["Mishra", "Prabhat", ""]]}, {"id": "2103.11943", "submitter": "Mikhail Koroteev Mr.", "authors": "M. V. Koroteev", "title": "BERT: A Review of Applications in Natural Language Processing and\n  Understanding", "comments": "18 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  In this review, we describe the application of one of the most popular deep\nlearning-based language models - BERT. The paper describes the mechanism of\noperation of this model, the main areas of its application to the tasks of text\nanalytics, comparisons with similar models in each task, as well as a\ndescription of some proprietary models. In preparing this review, the data of\nseveral dozen original scientific articles published over the past few years,\nwhich attracted the most attention in the scientific community, were\nsystematized. This survey will be useful to all students and researchers who\nwant to get acquainted with the latest advances in the field of natural\nlanguage text analysis.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 15:34:39 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Koroteev", "M. V.", ""]]}, {"id": "2103.11944", "submitter": "Manuel Camargo", "authors": "Manuel Camargo, Marlon Dumas, Oscar Gonz\\'alez-Rojas", "title": "Learning Accurate Business Process Simulation Models from Event Logs via\n  Automated Process Discovery and Deep Learning", "comments": "16 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Business process simulation is a well-known approach to estimate the impact\nof changes to a process with respect to time and cost measures -- a practice\nknown as what-if process analysis. The usefulness of such estimations hinges on\nthe accuracy of the underlying simulation model. Data-Driven Simulation (DDS)\nmethods combine automated process discovery and enhancement techniques to learn\nprocess simulation models from event logs. Empirical studies have shown that,\nwhile DDS models adequately capture the observed sequences of activities and\ntheir frequencies, they fail to capture the temporal dynamics of real-life\nprocesses. In contrast, parallel work has shown that generative Deep Learning\n(DL) models are able to accurately capture such temporal dynamics. The drawback\nof these latter models is that users cannot alter them for what-if analysis due\nto their black-box nature. This paper presents a hybrid approach to learn\nprocess simulation models from event logs wherein a (stochastic) process model\nis extracted from a log using automated process discovery and enhancement\ntechniques, and this model is then combined with a DL model to generate\ntimestamped event sequences (traces). An experimental evaluation shows that the\nresulting hybrid simulation models match the temporal accuracy of pure DL\nmodels, while retaining the what-if analysis capability of DDS approaches.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 15:34:57 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Camargo", "Manuel", ""], ["Dumas", "Marlon", ""], ["Gonz\u00e1lez-Rojas", "Oscar", ""]]}, {"id": "2103.11951", "submitter": "Endri Kacupaj", "authors": "Aynur Guluzade, Endri Kacupaj, Maria Maleshkova", "title": "Demographic Aware Probabilistic Medical Knowledge Graph Embeddings of\n  Electronic Medical Records", "comments": "Artificial Intelligence in Medicine 2021 (AIME 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Medical knowledge graphs (KGs) constructed from Electronic Medical Records\n(EMR) contain abundant information about patients and medical entities. The\nutilization of KG embedding models on these data has proven to be efficient for\ndifferent medical tasks. However, existing models do not properly incorporate\npatient demographics and most of them ignore the probabilistic features of the\nmedical KG. In this paper, we propose DARLING (Demographic Aware pRobabiListic\nmedIcal kNowledge embeddinG), a demographic-aware medical KG embedding\nframework that explicitly incorporates demographics in the medical entities\nspace by associating patient demographics with a corresponding hyperplane. Our\nframework leverages the probabilistic features within the medical entities for\nlearning their representations through demographic guidance. We evaluate\nDARLING through link prediction for treatments and medicines, on a medical KG\nconstructed from EMR data, and illustrate its superior performance compared to\nexisting KG embedding models.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 15:45:05 GMT"}, {"version": "v2", "created": "Sat, 3 Apr 2021 22:57:44 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Guluzade", "Aynur", ""], ["Kacupaj", "Endri", ""], ["Maleshkova", "Maria", ""]]}, {"id": "2103.11955", "submitter": "Derek Tam", "authors": "Derek Tam, Rakesh R Menon, Mohit Bansal, Shashank Srivastava, Colin\n  Raffel", "title": "Improving and Simplifying Pattern Exploiting Training", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, pre-trained language models (LMs) have achieved strong performance\nwhen fine-tuned on difficult benchmarks like SuperGLUE. However, performance\ncan suffer when there are very few labeled examples available for fine-tuning.\nPattern Exploiting Training (PET) is a recent approach that leverages patterns\nfor few-shot learning. However, PET uses task-specific unlabeled data. In this\npaper, we focus on few shot learning without any unlabeled data and introduce\nADAPET, which modifies PET's objective to provide denser supervision during\nfine-tuning. As a result, ADAPET outperforms PET on SuperGLUE without any\ntask-specific unlabeled data. Our code can be found at\nhttps://github.com/rrmenon10/ADAPET.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 15:52:45 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Tam", "Derek", ""], ["Menon", "Rakesh R", ""], ["Bansal", "Mohit", ""], ["Srivastava", "Shashank", ""], ["Raffel", "Colin", ""]]}, {"id": "2103.11961", "submitter": "Noah Klarmann", "authors": "Noah Klarmann", "title": "Artificial Intelligence Narratives: An Objective Perspective on Current\n  Developments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This work provides a starting point for researchers interested in gaining a\ndeeper understanding of the big picture of artificial intelligence (AI). To\nthis end, a narrative is conveyed that allows the reader to develop an\nobjective view on current developments that is free from false promises that\ndominate public communication. An essential takeaway for the reader is that AI\nmust be understood as an umbrella term encompassing a plethora of different\nmethods, schools of thought, and their respective historical movements.\nConsequently, a bottom-up strategy is pursued in which the field of AI is\nintroduced by presenting various aspects that are characteristic of the\nsubject. This paper is structured in three parts: (i) Discussion of current\ntrends revealing false public narratives, (ii) an introduction to the history\nof AI focusing on recurring patterns and main characteristics, and (iii) a\ncritical discussion on the limitations of current methods in the context of the\npotential emergence of a strong(er) AI. It should be noted that this work does\nnot cover any of these aspects holistically; rather, the content addressed is a\nselection made by the author and subject to a didactic strategy.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 17:33:00 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Klarmann", "Noah", ""]]}, {"id": "2103.11972", "submitter": "Sainyam Galhotra", "authors": "Sainyam Galhotra, Romila Pradhan, Babak Salimi", "title": "Explaining Black-Box Algorithms Using Probabilistic Contrastive\n  Counterfactuals", "comments": "Proceedings of the 2021 International Conference on Management of\n  Data. ACM, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There has been a recent resurgence of interest in explainable artificial\nintelligence (XAI) that aims to reduce the opaqueness of AI-based\ndecision-making systems, allowing humans to scrutinize and trust them. Prior\nwork in this context has focused on the attribution of responsibility for an\nalgorithm's decisions to its inputs wherein responsibility is typically\napproached as a purely associational concept. In this paper, we propose a\nprincipled causality-based approach for explaining black-box decision-making\nsystems that addresses limitations of existing methods in XAI. At the core of\nour framework lies probabilistic contrastive counterfactuals, a concept that\ncan be traced back to philosophical, cognitive, and social foundations of\ntheories on how humans generate and select explanations. We show how such\ncounterfactuals can quantify the direct and indirect influences of a variable\non decisions made by an algorithm, and provide actionable recourse for\nindividuals negatively affected by the algorithm's decision. Unlike prior work,\nour system, LEWIS: (1)can compute provably effective explanations and recourse\nat local, global and contextual levels (2)is designed to work with users with\nvarying levels of background knowledge of the underlying causal model and\n(3)makes no assumptions about the internals of an algorithmic system except for\nthe availability of its input-output data. We empirically evaluate LEWIS on\nthree real-world datasets and show that it generates human-understandable\nexplanations that improve upon state-of-the-art approaches in XAI, including\nthe popular LIME and SHAP. Experiments on synthetic data further demonstrate\nthe correctness of LEWIS's explanations and the scalability of its recourse\nalgorithm.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 16:20:21 GMT"}, {"version": "v2", "created": "Wed, 23 Jun 2021 06:33:03 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Galhotra", "Sainyam", ""], ["Pradhan", "Romila", ""], ["Salimi", "Babak", ""]]}, {"id": "2103.12010", "submitter": "Samuel Yen-Chi Chen", "authors": "Samuel Yen-Chi Chen, Shinjae Yoo", "title": "Federated Quantum Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.AI cs.CR cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed training across several quantum computers could significantly\nimprove the training time and if we could share the learned model, not the\ndata, it could potentially improve the data privacy as the training would\nhappen where the data is located. However, to the best of our knowledge, no\nwork has been done in quantum machine learning (QML) in federation setting yet.\nIn this work, we present the federated training on hybrid quantum-classical\nmachine learning models although our framework could be generalized to pure\nquantum machine learning model. Specifically, we consider the quantum neural\nnetwork (QNN) coupled with classical pre-trained convolutional model. Our\ndistributed federated learning scheme demonstrated almost the same level of\ntrained model accuracies and yet significantly faster distributed training. It\ndemonstrates a promising future research direction for scaling and privacy\naspects.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 17:00:19 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Chen", "Samuel Yen-Chi", ""], ["Yoo", "Shinjae", ""]]}, {"id": "2103.12016", "submitter": "Christopher Starke", "authors": "Christopher Starke, Janine Baleis, Birte Keller, Frank Marcinkowski", "title": "Fairness Perceptions of Algorithmic Decision-Making: A Systematic Review\n  of the Empirical Literature", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Algorithmic decision-making (ADM) increasingly shapes people's daily lives.\nGiven that such autonomous systems can cause severe harm to individuals and\nsocial groups, fairness concerns have arisen. A human-centric approach demanded\nby scholars and policymakers requires taking people's fairness perceptions into\naccount when designing and implementing ADM. We provide a comprehensive,\nsystematic literature review synthesizing the existing empirical insights on\nperceptions of algorithmic fairness from 39 empirical studies spanning multiple\ndomains and scientific disciplines. Through thorough coding, we systemize the\ncurrent empirical literature along four dimensions: (a) algorithmic predictors,\n(b) human predictors, (c) comparative effects (human decision-making vs.\nalgorithmic decision-making), and (d) consequences of ADM. While we identify\nmuch heterogeneity around the theoretical concepts and empirical measurements\nof algorithmic fairness, the insights come almost exclusively from\nWestern-democratic contexts. By advocating for more interdisciplinary research\nadopting a society-in-the-loop framework, we hope our work will contribute to\nfairer and more responsible ADM.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 17:12:45 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Starke", "Christopher", ""], ["Baleis", "Janine", ""], ["Keller", "Birte", ""], ["Marcinkowski", "Frank", ""]]}, {"id": "2103.12021", "submitter": "Paria Rashidinejad", "authors": "Paria Rashidinejad, Banghua Zhu, Cong Ma, Jiantao Jiao, Stuart Russell", "title": "Bridging Offline Reinforcement Learning and Imitation Learning: A Tale\n  of Pessimism", "comments": "84 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC math.ST stat.ML stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Offline (or batch) reinforcement learning (RL) algorithms seek to learn an\noptimal policy from a fixed dataset without active data collection. Based on\nthe composition of the offline dataset, two main categories of methods are\nused: imitation learning which is suitable for expert datasets and vanilla\noffline RL which often requires uniform coverage datasets. From a practical\nstandpoint, datasets often deviate from these two extremes and the exact data\ncomposition is usually unknown a priori. To bridge this gap, we present a new\noffline RL framework that smoothly interpolates between the two extremes of\ndata composition, hence unifying imitation learning and vanilla offline RL. The\nnew framework is centered around a weak version of the concentrability\ncoefficient that measures the deviation from the behavior policy to the expert\npolicy alone.\n  Under this new framework, we further investigate the question on algorithm\ndesign: can one develop an algorithm that achieves a minimax optimal rate and\nalso adapts to unknown data composition? To address this question, we consider\na lower confidence bound (LCB) algorithm developed based on pessimism in the\nface of uncertainty in offline RL. We study finite-sample properties of LCB as\nwell as information-theoretic limits in multi-armed bandits, contextual\nbandits, and Markov decision processes (MDPs). Our analysis reveals surprising\nfacts about optimality rates. In particular, in all three settings, LCB\nachieves a faster rate of $1/N$ for nearly-expert datasets compared to the\nusual rate of $1/\\sqrt{N}$ in offline RL, where $N$ is the number of samples in\nthe batch dataset. In the case of contextual bandits with at least two\ncontexts, we prove that LCB is adaptively optimal for the entire data\ncomposition range, achieving a smooth transition from imitation learning to\noffline RL. We further show that LCB is almost adaptively optimal in MDPs.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 17:27:08 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Rashidinejad", "Paria", ""], ["Zhu", "Banghua", ""], ["Ma", "Cong", ""], ["Jiao", "Jiantao", ""], ["Russell", "Stuart", ""]]}, {"id": "2103.12028", "submitter": "Isaac Caswell", "authors": "Isaac Caswell, Julia Kreutzer, Lisa Wang, Ahsan Wahab, Daan van Esch,\n  Nasanbayar Ulzii-Orshikh, Allahsera Tapo, Nishant Subramani, Artem Sokolov,\n  Claytone Sikasote, Monang Setyawan, Supheakmungkol Sarin, Sokhar Samb,\n  Beno\\^it Sagot, Clara Rivera, Annette Rios, Isabel Papadimitriou, Salomey\n  Osei, Pedro Javier Ortiz Su\\'arez, Iroro Orife, Kelechi Ogueji, Rubungo Andre\n  Niyongabo, Toan Q. Nguyen, Mathias M\\\"uller, Andr\\'e M\\\"uller, Shamsuddeen\n  Hassan Muhammad, Nanda Muhammad, Ayanda Mnyakeni, Jamshidbek Mirzakhalov,\n  Tapiwanashe Matangira, Colin Leong, Nze Lawson, Sneha Kudugunta, Yacine\n  Jernite, Mathias Jenny, Orhan Firat, Bonaventure F. P. Dossou, Sakhile\n  Dlamini, Nisansa de Silva, Sakine \\c{C}abuk Ball{\\i}, Stella Biderman,\n  Alessia Battisti, Ahmed Baruwa, Ankur Bapna, Pallavi Baljekar, Israel Abebe\n  Azime, Ayodele Awokoya, Duygu Ataman, Orevaoghene Ahia, Oghenefego Ahia,\n  Sweta Agrawal, Mofetoluwa Adeyemi", "title": "Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets", "comments": "10 pages paper; 10 pages appendix; AfricaNLP 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the success of large-scale pre-training and multilingual modeling in\nNatural Language Processing (NLP), recent years have seen a proliferation of\nlarge, web-mined text datasets covering hundreds of languages. However, to date\nthere has been no systematic analysis of the quality of these publicly\navailable datasets, or whether the datasets actually contain content in the\nlanguages they claim to represent. In this work, we manually audit the quality\nof 205 language-specific corpora released with five major public datasets\n(CCAligned, ParaCrawl, WikiMatrix, OSCAR, mC4), and audit the correctness of\nlanguage codes in a sixth (JW300). We find that lower-resource corpora have\nsystematic issues: at least 15 corpora are completely erroneous, and a\nsignificant fraction contains less than 50% sentences of acceptable quality.\nSimilarly, we find 82 corpora that are mislabeled or use nonstandard/ambiguous\nlanguage codes. We demonstrate that these issues are easy to detect even for\nnon-speakers of the languages in question, and supplement the human judgements\nwith automatic analyses. Inspired by our analysis, we recommend techniques to\nevaluate and improve multilingual corpora and discuss the risks that come with\nlow-quality data releases.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 17:30:33 GMT"}, {"version": "v2", "created": "Fri, 23 Apr 2021 19:38:25 GMT"}], "update_date": "2021-04-27", "authors_parsed": [["Caswell", "Isaac", ""], ["Kreutzer", "Julia", ""], ["Wang", "Lisa", ""], ["Wahab", "Ahsan", ""], ["van Esch", "Daan", ""], ["Ulzii-Orshikh", "Nasanbayar", ""], ["Tapo", "Allahsera", ""], ["Subramani", "Nishant", ""], ["Sokolov", "Artem", ""], ["Sikasote", "Claytone", ""], ["Setyawan", "Monang", ""], ["Sarin", "Supheakmungkol", ""], ["Samb", "Sokhar", ""], ["Sagot", "Beno\u00eet", ""], ["Rivera", "Clara", ""], ["Rios", "Annette", ""], ["Papadimitriou", "Isabel", ""], ["Osei", "Salomey", ""], ["Su\u00e1rez", "Pedro Javier Ortiz", ""], ["Orife", "Iroro", ""], ["Ogueji", "Kelechi", ""], ["Niyongabo", "Rubungo Andre", ""], ["Nguyen", "Toan Q.", ""], ["M\u00fcller", "Mathias", ""], ["M\u00fcller", "Andr\u00e9", ""], ["Muhammad", "Shamsuddeen Hassan", ""], ["Muhammad", "Nanda", ""], ["Mnyakeni", "Ayanda", ""], ["Mirzakhalov", "Jamshidbek", ""], ["Matangira", "Tapiwanashe", ""], ["Leong", "Colin", ""], ["Lawson", "Nze", ""], ["Kudugunta", "Sneha", ""], ["Jernite", "Yacine", ""], ["Jenny", "Mathias", ""], ["Firat", "Orhan", ""], ["Dossou", "Bonaventure F. P.", ""], ["Dlamini", "Sakhile", ""], ["de Silva", "Nisansa", ""], ["Ball\u0131", "Sakine \u00c7abuk", ""], ["Biderman", "Stella", ""], ["Battisti", "Alessia", ""], ["Baruwa", "Ahmed", ""], ["Bapna", "Ankur", ""], ["Baljekar", "Pallavi", ""], ["Azime", "Israel Abebe", ""], ["Awokoya", "Ayodele", ""], ["Ataman", "Duygu", ""], ["Ahia", "Orevaoghene", ""], ["Ahia", "Oghenefego", ""], ["Agrawal", "Sweta", ""], ["Adeyemi", "Mofetoluwa", ""]]}, {"id": "2103.12045", "submitter": "Fan Shi", "authors": "Fan Shi, Bin Li, Xiangyang Xue", "title": "Raven's Progressive Matrices Completion with Latent Gaussian Process\n  Priors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstract reasoning ability is fundamental to human intelligence. It enables\nhumans to uncover relations among abstract concepts and further deduce implicit\nrules from the relations. As a well-known abstract visual reasoning task,\nRaven's Progressive Matrices (RPM) are widely used in human IQ tests. Although\nextensive research has been conducted on RPM solvers with machine intelligence,\nfew studies have considered further advancing the standard answer-selection\n(classification) problem to a more challenging answer-painting (generating)\nproblem, which can verify whether the model has indeed understood the implicit\nrules. In this paper we aim to solve the latter one by proposing a deep latent\nvariable model, in which multiple Gaussian processes are employed as priors of\nlatent variables to separately learn underlying abstract concepts from RPMs;\nthus the proposed model is interpretable in terms of concept-specific latent\nvariables. The latent Gaussian process also provides an effective way of\nextrapolation for answer painting based on the learned concept-changing rules.\nWe evaluate the proposed model on RPM-like datasets with multiple\ncontinuously-changing visual concepts. Experimental results demonstrate that\nour model requires only few training samples to paint high-quality answers,\ngenerate novel RPM panels, and achieve interpretability through\nconcept-specific latent variables.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 17:48:44 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Shi", "Fan", ""], ["Li", "Bin", ""], ["Xue", "Xiangyang", ""]]}, {"id": "2103.12051", "submitter": "Vikash Sehwag", "authors": "Vikash Sehwag, Mung Chiang, Prateek Mittal", "title": "SSD: A Unified Framework for Self-Supervised Outlier Detection", "comments": "ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We ask the following question: what training information is required to\ndesign an effective outlier/out-of-distribution (OOD) detector, i.e., detecting\nsamples that lie far away from the training distribution? Since unlabeled data\nis easily accessible for many applications, the most compelling approach is to\ndevelop detectors based on only unlabeled in-distribution data. However, we\nobserve that most existing detectors based on unlabeled data perform poorly,\noften equivalent to a random prediction. In contrast, existing state-of-the-art\nOOD detectors achieve impressive performance but require access to fine-grained\ndata labels for supervised training. We propose SSD, an outlier detector based\non only unlabeled in-distribution data. We use self-supervised representation\nlearning followed by a Mahalanobis distance based detection in the feature\nspace. We demonstrate that SSD outperforms most existing detectors based on\nunlabeled data by a large margin. Additionally, SSD even achieves performance\non par, and sometimes even better, with supervised training based detectors.\nFinally, we expand our detection framework with two key extensions. First, we\nformulate few-shot OOD detection, in which the detector has access to only one\nto five samples from each class of the targeted OOD dataset. Second, we extend\nour framework to incorporate training data labels, if available. We find that\nour novel detection framework based on SSD displays enhanced performance with\nthese extensions, and achieves state-of-the-art performance. Our code is\npublicly available at https://github.com/inspire-group/SSD.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 17:51:35 GMT"}], "update_date": "2021-03-23", "authors_parsed": [["Sehwag", "Vikash", ""], ["Chiang", "Mung", ""], ["Mittal", "Prateek", ""]]}, {"id": "2103.12057", "submitter": "Pedro Lara-Ben\\'itez", "authors": "Pedro Lara-Ben\\'itez, Manuel Carranza-Garc\\'ia and Jos\\'e C. Riquelme", "title": "An Experimental Review on Deep Learning Architectures for Time Series\n  Forecasting", "comments": null, "journal-ref": "International Journal of Neural Systems, Vol. 31, No. 3 (2021)\n  2130001", "doi": "10.1142/S0129065721300011", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, deep learning techniques have outperformed traditional\nmodels in many machine learning tasks. Deep neural networks have successfully\nbeen applied to address time series forecasting problems, which is a very\nimportant topic in data mining. They have proved to be an effective solution\ngiven their capacity to automatically learn the temporal dependencies present\nin time series. However, selecting the most convenient type of deep neural\nnetwork and its parametrization is a complex task that requires considerable\nexpertise. Therefore, there is a need for deeper studies on the suitability of\nall existing architectures for different forecasting tasks. In this work, we\nface two main challenges: a comprehensive review of the latest works using deep\nlearning for time series forecasting; and an experimental study comparing the\nperformance of the most popular architectures. The comparison involves a\nthorough analysis of seven types of deep learning models in terms of accuracy\nand efficiency. We evaluate the rankings and distribution of results obtained\nwith the proposed models under many different architecture configurations and\ntraining hyperparameters. The datasets used comprise more than 50000 time\nseries divided into 12 different forecasting problems. By training more than\n38000 models on these data, we provide the most extensive deep learning study\nfor time series forecasting. Among all studied models, the results show that\nlong short-term memory (LSTM) and convolutional networks (CNN) are the best\nalternatives, with LSTMs obtaining the most accurate forecasts. CNNs achieve\ncomparable performance with less variability of results under different\nparameter configurations, while also being more efficient.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 17:58:36 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 16:59:09 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Lara-Ben\u00edtez", "Pedro", ""], ["Carranza-Garc\u00eda", "Manuel", ""], ["Riquelme", "Jos\u00e9 C.", ""]]}, {"id": "2103.12069", "submitter": "Kieran Greer Dr", "authors": "Kieran Greer", "title": "Exemplars can Reciprocate Principal Components", "comments": null, "journal-ref": "WSEAS Transactions on Computers, ISSN / E-ISSN: 1109-2750 /\n  2224-2872, Volume 20, 2021, Art. #4, pp. 30-38", "doi": "10.37394/23205.2021.20.4", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a clustering algorithm that is an extension of the\nCategory Trees algorithm. Category Trees is a clustering method that creates\ntree structures that branch on category type and not feature. The development\nin this paper is to consider a secondary order of clustering that is not the\ncategory to which the data row belongs, but the tree, representing a single\nclassifier, that it is eventually clustered with. Each tree branches to store\nsubsets of other categories, but the rows in those subsets may also be related.\nThis paper is therefore concerned with looking at that second level of\nclustering between the other category subsets, to try to determine if there is\nany consistency over it. It is argued that Principal Components may be a\nrelated and reciprocal type of structure, and there is an even bigger question\nabout the relation between exemplars and principal components, in general. The\ntheory is demonstrated using the Portugal Forest Fires dataset as a case study.\nThe Category Trees are then combined with other Self-Organising algorithms from\nthe author and it is suggested that they all belong to the same family type,\nwhich is an Entropy-style of classifier.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 12:46:29 GMT"}, {"version": "v2", "created": "Wed, 7 Apr 2021 13:19:12 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Greer", "Kieran", ""]]}, {"id": "2103.12070", "submitter": "Christoph Killing", "authors": "Christoph Killing, Adam Villaflor, John M. Dolan", "title": "Learning to Robustly Negotiate Bi-Directional Lane Usage in\n  High-Conflict Driving Scenarios", "comments": "7 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA cs.RO", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Recently, autonomous driving has made substantial progress in addressing the\nmost common traffic scenarios like intersection navigation and lane changing.\nHowever, most of these successes have been limited to scenarios with\nwell-defined traffic rules and require minimal negotiation with other vehicles.\nIn this paper, we introduce a previously unconsidered, yet everyday,\nhigh-conflict driving scenario requiring negotiations between agents of equal\nrights and priorities. There exists no centralized control structure and we do\nnot allow communications. Therefore, it is unknown if other drivers are willing\nto cooperate, and if so to what extent. We train policies to robustly negotiate\nwith opposing vehicles of an unobservable degree of cooperativeness using\nmulti-agent reinforcement learning (MARL). We propose Discrete Asymmetric Soft\nActor-Critic (DASAC), a maximum-entropy off-policy MARL algorithm allowing for\ncentralized training with decentralized execution. We show that using DASAC we\nare able to successfully negotiate and traverse the scenario considered over\n99% of the time. Our agents are robust to an unknown timing of opponent\ndecisions, an unobservable degree of cooperativeness of the opposing vehicle,\nand previously unencountered policies. Furthermore, they learn to exhibit\nhuman-like behaviors such as defensive driving, anticipating solution options\nand interpreting the behavior of other agents.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 14:46:43 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Killing", "Christoph", ""], ["Villaflor", "Adam", ""], ["Dolan", "John M.", ""]]}, {"id": "2103.12095", "submitter": "Fabricio Murai", "authors": "Davi Pedrosa de Aguiar and Ot\\'avio Augusto Silva and Fabricio Murai", "title": "Am I fit for this physical activity? Neural embedding of physical\n  conditioning from inertial sensors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Inertial Measurement Unit (IMU) sensors are becoming increasingly ubiquitous\nin everyday devices such as smartphones, fitness watches, etc. As a result, the\narray of health-related applications that tap onto this data has been growing,\nas well as the importance of designing accurate prediction models for tasks\nsuch as human activity recognition (HAR). However, one important task that has\nreceived little attention is the prediction of an individual's heart rate when\nundergoing a physical activity using IMU data. This could be used, for example,\nto determine which activities are safe for a person without having him/her\nactually perform them. We propose a neural architecture for this task composed\nof convolutional and LSTM layers, similarly to the state-of-the-art techniques\nfor the closely related task of HAR. However, our model includes a\nconvolutional network that extracts, based on sensor data from a previously\nexecuted activity, a physical conditioning embedding (PCE) of the individual to\nbe used as the LSTM's initial hidden state. We evaluate the proposed model,\ndubbed PCE-LSTM, when predicting the heart rate of 23 subjects performing a\nvariety of physical activities from IMU-sensor data available in public\ndatasets (PAMAP2, PPG-DaLiA). For comparison, we use as baselines the only\nmodel specifically proposed for this task, and an adapted state-of-the-art\nmodel for HAR. PCE-LSTM yields over 10% lower mean absolute error. We\ndemonstrate empirically that this error reduction is in part due to the use of\nthe PCE. Last, we use the two datasets (PPG-DaLiA, WESAD) to show that PCE-LSTM\ncan also be successfully applied when photoplethysmography (PPG) sensors are\navailable to rectify heart rate measurement errors caused by movement,\noutperforming the state-of-the-art deep learning baselines by more than 30%.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 18:00:27 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["de Aguiar", "Davi Pedrosa", ""], ["Silva", "Ot\u00e1vio Augusto", ""], ["Murai", "Fabricio", ""]]}, {"id": "2103.12142", "submitter": "Dmitrii Krasheninnikov", "authors": "Dmitrii Krasheninnikov, Rohin Shah, Herke van Hoof", "title": "Combining Reward Information from Multiple Sources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given two sources of evidence about a latent variable, one can combine the\ninformation from both by multiplying the likelihoods of each piece of evidence.\nHowever, when one or both of the observation models are misspecified, the\ndistributions will conflict. We study this problem in the setting with two\nconflicting reward functions learned from different sources. In such a setting,\nwe would like to retreat to a broader distribution over reward functions, in\norder to mitigate the effects of misspecification. We assume that an agent will\nmaximize expected reward given this distribution over reward functions, and\nidentify four desiderata for this setting. We propose a novel algorithm,\nMultitask Inverse Reward Design (MIRD), and compare it to a range of simple\nbaselines. While all methods must trade off between conservatism and\ninformativeness, through a combination of theory and empirical results on a toy\nenvironment, we find that MIRD and its variant MIRD-IF strike a good balance\nbetween the two.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 19:23:24 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Krasheninnikov", "Dmitrii", ""], ["Shah", "Rohin", ""], ["van Hoof", "Herke", ""]]}, {"id": "2103.12144", "submitter": "Vahideh Hayyolalam", "authors": "Vahideh Hayyolalam, Moayad Aloqaily, Oznur Ozkasap, Mohsen Guizani", "title": "Edge Intelligence for Empowering IoT-based Healthcare Systems", "comments": "This paper has been accepted in IEEE Wireless Communication Magazine", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The demand for real-time, affordable, and efficient smart healthcare services\nis increasing exponentially due to the technological revolution and burst of\npopulation. To meet the increasing demands on this critical infrastructure,\nthere is a need for intelligent methods to cope with the existing obstacles in\nthis area. In this regard, edge computing technology can reduce latency and\nenergy consumption by moving processes closer to the data sources in comparison\nto the traditional centralized cloud and IoT-based healthcare systems. In\naddition, by bringing automated insights into the smart healthcare systems,\nartificial intelligence (AI) provides the possibility of detecting and\npredicting high-risk diseases in advance, decreasing medical costs for\npatients, and offering efficient treatments. The objective of this article is\nto highlight the benefits of the adoption of edge intelligent technology, along\nwith AI in smart healthcare systems. Moreover, a novel smart healthcare model\nis proposed to boost the utilization of AI and edge technology in smart\nhealthcare systems. Additionally, the paper discusses issues and research\ndirections arising when integrating these different technologies together.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 19:35:06 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Hayyolalam", "Vahideh", ""], ["Aloqaily", "Moayad", ""], ["Ozkasap", "Oznur", ""], ["Guizani", "Mohsen", ""]]}, {"id": "2103.12155", "submitter": "Satvik Garg", "authors": "Satvik Garg and Somya Garg", "title": "Prediction of lung and colon cancer through analysis of\n  histopathological images by utilizing Pre-trained CNN models with\n  visualization of class activation and saliency maps", "comments": "This Paper has been accepted and presented in 2nd Asia Pacific\n  Digital Image Processing (ADIP) Workshop of 3rd Artificial Intelligence and\n  Cloud Computing Conference (AICCC 2020), ACM, Japan. 2020 December 3-5. The\n  publication can be accessed from the proceedings of the AICCC 2020\n  conference. (https://dl.acm.org/doi/10.1145/3442536.3442543)", "journal-ref": null, "doi": "10.1145/3442536.3442543", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Colon and Lung cancer is one of the most perilous and dangerous ailments that\nindividuals are enduring worldwide and has become a general medical problem. To\nlessen the risk of death, a legitimate and early finding is particularly\nrequired. In any case, it is a truly troublesome task that depends on the\nexperience of histopathologists. If a histologist is under-prepared it may even\nhazard the life of a patient. As of late, deep learning has picked up energy,\nand it is being valued in the analysis of Medical Imaging. This paper intends\nto utilize and alter the current pre-trained CNN-based model to identify lung\nand colon cancer utilizing histopathological images with better augmentation\ntechniques. In this paper, eight distinctive Pre-trained CNN models, VGG16,\nNASNetMobile, InceptionV3, InceptionResNetV2, ResNet50, Xception, MobileNet,\nand DenseNet169 are trained on LC25000 dataset. The model performances are\nassessed on precision, recall, f1score, accuracy, and auroc score. The results\nexhibit that all eight models accomplished noteworthy results ranging from 96%\nto 100% accuracy. Subsequently, GradCAM and SmoothGrad are also used to picture\nthe attention images of Pre-trained CNN models classifying malignant and benign\nimages.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 20:06:27 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Garg", "Satvik", ""], ["Garg", "Somya", ""]]}, {"id": "2103.12169", "submitter": "Stefan Kuhn", "authors": "Stefan Kuhn, Eda Tumer, Simon Colreavy-Donnelly, Ricardo Moreira\n  Borges", "title": "A Pilot Study For Fragment Identification Using 2D NMR and Deep Learning", "comments": "11 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a method to identify substructures in NMR spectra of\nmixtures, specifically 2D spectra, using a bespoke image-based Convolutional\nNeural Network application. This is done using HSQC and HMBC spectra separately\nand in combination. The application can reliably detect substructures in pure\ncompounds, using a simple network. It can work for mixtures when trained on\npure compounds only. HMBC data and the combination of HMBC and HSQC show better\nresults than HSQC alone.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 20:25:41 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Kuhn", "Stefan", ""], ["Tumer", "Eda", ""], ["Colreavy-Donnelly", "Simon", ""], ["Borges", "Ricardo Moreira", ""]]}, {"id": "2103.12171", "submitter": "Tianlong Chen", "authors": "Tianlong Chen, Yu Cheng, Zhe Gan, Jianfeng Wang, Lijuan Wang,\n  Zhangyang Wang, Jingjing Liu", "title": "Adversarial Feature Augmentation and Normalization for Visual\n  Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in computer vision take advantage of adversarial data\naugmentation to ameliorate the generalization ability of classification models.\nHere, we present an effective and efficient alternative that advocates\nadversarial augmentation on intermediate feature embeddings, instead of relying\non computationally-expensive pixel-level perturbations. We propose Adversarial\nFeature Augmentation and Normalization (A-FAN), which (i) first augments visual\nrecognition models with adversarial features that integrate flexible scales of\nperturbation strengths, (ii) then extracts adversarial feature statistics from\nbatch normalization, and re-injects them into clean features through feature\nnormalization. We validate the proposed approach across diverse visual\nrecognition tasks with representative backbone networks, including ResNets and\nEfficientNets for classification, Faster-RCNN for detection, and Deeplab V3+\nfor segmentation. Extensive experiments show that A-FAN yields consistent\ngeneralization improvement over strong baselines across various datasets for\nclassification, detection and segmentation tasks, such as CIFAR-10, CIFAR-100,\nImageNet, Pascal VOC2007, Pascal VOC2012, COCO2017, and Cityspaces.\nComprehensive ablation studies and detailed analyses also demonstrate that\nadding perturbations to specific modules and layers of\nclassification/detection/segmentation backbones yields optimal performance.\nCodes and pre-trained models will be made available at:\nhttps://github.com/VITA-Group/CV_A-FAN.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 20:36:34 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Chen", "Tianlong", ""], ["Cheng", "Yu", ""], ["Gan", "Zhe", ""], ["Wang", "Jianfeng", ""], ["Wang", "Lijuan", ""], ["Wang", "Zhangyang", ""], ["Liu", "Jingjing", ""]]}, {"id": "2103.12204", "submitter": "Long Chen", "authors": "Long Chen, Zhihong Jiang, Jun Xiao, Wei Liu", "title": "Human-like Controllable Image Captioning with Verb-specific Semantic\n  Roles", "comments": "Accepted by CVPR 2021. The code is available at:\n  https://github.com/mad-red/VSR-guided-CIC", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Controllable Image Captioning (CIC) -- generating image descriptions\nfollowing designated control signals -- has received unprecedented attention\nover the last few years. To emulate the human ability in controlling caption\ngeneration, current CIC studies focus exclusively on control signals concerning\nobjective properties, such as contents of interest or descriptive patterns.\nHowever, we argue that almost all existing objective control signals have\noverlooked two indispensable characteristics of an ideal control signal: 1)\nEvent-compatible: all visual contents referred to in a single sentence should\nbe compatible with the described activity. 2) Sample-suitable: the control\nsignals should be suitable for a specific image sample. To this end, we propose\na new control signal for CIC: Verb-specific Semantic Roles (VSR). VSR consists\nof a verb and some semantic roles, which represents a targeted activity and the\nroles of entities involved in this activity. Given a designated VSR, we first\ntrain a grounded semantic role labeling (GSRL) model to identify and ground all\nentities for each role. Then, we propose a semantic structure planner (SSP) to\nlearn human-like descriptive semantic structures. Lastly, we use a role-shift\ncaptioning model to generate the captions. Extensive experiments and ablations\ndemonstrate that our framework can achieve better controllability than several\nstrong baselines on two challenging CIC benchmarks. Besides, we can generate\nmulti-level diverse captions easily. The code is available at:\nhttps://github.com/mad-red/VSR-guided-CIC.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 22:17:42 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Chen", "Long", ""], ["Jiang", "Zhihong", ""], ["Xiao", "Jun", ""], ["Liu", "Wei", ""]]}, {"id": "2103.12256", "submitter": "Ya Zhang", "authors": "Mingming Lu, Ya Zhang", "title": "Spatio-Temporal Sparsification for General Robust Graph Convolution\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Graph Neural Networks (GNNs) have attracted increasing attention due to its\nsuccessful applications on various graph-structure data. However, recent\nstudies have shown that adversarial attacks are threatening the functionality\nof GNNs. Although numerous works have been proposed to defend adversarial\nattacks from various perspectives, most of them can be robust against the\nattacks only on specific scenarios. To address this shortage of robust\ngeneralization, we propose to defend the adversarial attacks on GNN through\napplying the Spatio-Temporal sparsification (called ST-Sparse) on the GNN\nhidden node representation. ST-Sparse is similar to the Dropout regularization\nin spirit. Through intensive experiment evaluation with GCN as the target GNN\nmodel, we identify the benefits of ST-Sparse as follows: (1) ST-Sparse shows\nthe defense performance improvement in most cases, as it can effectively\nincrease the robust accuracy by up to 6\\% improvement; (2) ST-Sparse\nillustrates its robust generalization capability by integrating with the\nexisting defense methods, similar to the integration of Dropout into various\ndeep learning models as a standard regularization technique; (3) ST-Sparse also\nshows its ordinary generalization capability on clean datasets, in that\nST-SparseGCN (the integration of ST-Sparse and the original GCN) even\noutperform the original GCN, while the other three representative defense\nmethods are inferior to the original GCN.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 02:03:11 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Lu", "Mingming", ""], ["Zhang", "Ya", ""]]}, {"id": "2103.12270", "submitter": "Xianzhi Du", "authors": "Abdullah Rashwan and Xianzhi Du and Xiaoqi Yin and Jing Li", "title": "Dilated SpineNet for Semantic Segmentation", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Scale-permuted networks have shown promising results on object bounding box\ndetection and instance segmentation. Scale permutation and cross-scale fusion\nof features enable the network to capture multi-scale semantics while\npreserving spatial resolution. In this work, we evaluate this meta-architecture\ndesign on semantic segmentation - another vision task that benefits from high\nspatial resolution and multi-scale feature fusion at different network stages.\nBy further leveraging dilated convolution operations, we propose SpineNet-Seg,\na network discovered by NAS that is searched from the DeepLabv3 system.\nSpineNet-Seg is designed with a better scale-permuted network topology with\ncustomized dilation ratios per block on a semantic segmentation task.\nSpineNet-Seg models outperform the DeepLabv3/v3+ baselines at all model scales\non multiple popular benchmarks in speed and accuracy. In particular, our\nSpineNet-S143+ model achieves the new state-of-the-art on the popular\nCityscapes benchmark at 83.04% mIoU and attained strong performance on the\nPASCAL VOC2012 benchmark at 85.56% mIoU. SpineNet-Seg models also show\npromising results on a challenging Street View segmentation dataset. Code and\ncheckpoints will be open-sourced.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 02:39:04 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Rashwan", "Abdullah", ""], ["Du", "Xianzhi", ""], ["Yin", "Xiaoqi", ""], ["Li", "Jing", ""]]}, {"id": "2103.12277", "submitter": "Han Li", "authors": "Han Li, Long Chen, Hu Han, S. Kevin Zhou", "title": "Conditional Training with Bounding Map for Universal Lesion Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Universal Lesion Detection (ULD) in computed tomography plays an essential\nrole in computer-aided diagnosis. Promising ULD results have been reported by\ncoarse-to-fine two-stage detection approaches, but such two-stage ULD methods\nstill suffer from issues like imbalance of positive v.s. negative anchors\nduring object proposal and insufficient supervision problem during localization\nregression and classification of the region of interest (RoI) proposals. While\nleveraging pseudo segmentation masks such as bounding map (BM) can reduce the\nabove issues to some degree, it is still an open problem to effectively handle\nthe diverse lesion shapes and sizes in ULD. In this paper, we propose a\nBM-based conditional training for two-stage ULD, which can (i) reduce positive\nvs. negative anchor imbalance via BM-based conditioning (BMC) mechanism for\nanchor sampling instead of traditional IoU-based rule; and (ii) adaptively\ncompute size-adaptive BM (ABM) from lesion bounding box, which is used for\nimproving lesion localization accuracy via ABMsupervised segmentation.\nExperiments with four state-of-the-art methods show that the proposed approach\ncan bring an almost free detection accuracy improvement without requiring\nexpensive lesion mask annotations.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 03:04:13 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Li", "Han", ""], ["Chen", "Long", ""], ["Han", "Hu", ""], ["Zhou", "S. Kevin", ""]]}, {"id": "2103.12300", "submitter": "Jaekyeom Kim", "authors": "Jaekyeom Kim, Minjung Kim, Dongyeon Woo, Gunhee Kim", "title": "Drop-Bottleneck: Learning Discrete Compressed Representation for\n  Noise-Robust Exploration", "comments": "Accepted to ICLR 2021. Code at http://vision.snu.ac.kr/projects/db", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel information bottleneck (IB) method named Drop-Bottleneck,\nwhich discretely drops features that are irrelevant to the target variable.\nDrop-Bottleneck not only enjoys a simple and tractable compression objective\nbut also additionally provides a deterministic compressed representation of the\ninput variable, which is useful for inference tasks that require consistent\nrepresentation. Moreover, it can jointly learn a feature extractor and select\nfeatures considering each feature dimension's relevance to the target task,\nwhich is unattainable by most neural network-based IB methods. We propose an\nexploration method based on Drop-Bottleneck for reinforcement learning tasks.\nIn a multitude of noisy and reward sparse maze navigation tasks in VizDoom\n(Kempka et al., 2016) and DMLab (Beattie et al., 2016), our exploration method\nachieves state-of-the-art performance. As a new IB framework, we demonstrate\nthat Drop-Bottleneck outperforms Variational Information Bottleneck (VIB)\n(Alemi et al., 2017) in multiple aspects including adversarial robustness and\ndimensionality reduction.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 04:31:28 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Kim", "Jaekyeom", ""], ["Kim", "Minjung", ""], ["Woo", "Dongyeon", ""], ["Kim", "Gunhee", ""]]}, {"id": "2103.12308", "submitter": "Alina Jade Barnett", "authors": "Alina Jade Barnett, Fides Regina Schwartz, Chaofan Tao, Chaofan Chen,\n  Yinhao Ren, Joseph Y. Lo and Cynthia Rudin", "title": "IAIA-BL: A Case-based Interpretable Deep Learning Model for\n  Classification of Mass Lesions in Digital Mammography", "comments": "24 pages, 5 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interpretability in machine learning models is important in high-stakes\ndecisions, such as whether to order a biopsy based on a mammographic exam.\nMammography poses important challenges that are not present in other computer\nvision tasks: datasets are small, confounding information is present, and it\ncan be difficult even for a radiologist to decide between watchful waiting and\nbiopsy based on a mammogram alone. In this work, we present a framework for\ninterpretable machine learning-based mammography. In addition to predicting\nwhether a lesion is malignant or benign, our work aims to follow the reasoning\nprocesses of radiologists in detecting clinically relevant semantic features of\neach image, such as the characteristics of the mass margins. The framework\nincludes a novel interpretable neural network algorithm that uses case-based\nreasoning for mammography. Our algorithm can incorporate a combination of data\nwith whole image labelling and data with pixel-wise annotations, leading to\nbetter accuracy and interpretability even with a small number of images. Our\ninterpretable models are able to highlight the classification-relevant parts of\nthe image, whereas other methods highlight healthy tissue and confounding\ninformation. Our models are decision aids, rather than decision makers, aimed\nat better overall human-machine collaboration. We do not observe a loss in mass\nmargin classification accuracy over a black box neural network trained on the\nsame data.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 05:00:21 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Barnett", "Alina Jade", ""], ["Schwartz", "Fides Regina", ""], ["Tao", "Chaofan", ""], ["Chen", "Chaofan", ""], ["Ren", "Yinhao", ""], ["Lo", "Joseph Y.", ""], ["Rudin", "Cynthia", ""]]}, {"id": "2103.12312", "submitter": "Jingxuan Tu", "authors": "Jingxuan Tu and Constantine Lignos", "title": "TMR: Evaluating NER Recall on Tough Mentions", "comments": "To appear in the 2021 EACL Student Research Workshop (SRW)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the Tough Mentions Recall (TMR) metrics to supplement traditional\nnamed entity recognition (NER) evaluation by examining recall on specific\nsubsets of \"tough\" mentions: unseen mentions, those whose tokens or token/type\ncombination were not observed in training, and type-confusable mentions, token\nsequences with multiple entity types in the test data. We demonstrate the\nusefulness of these metrics by evaluating corpora of English, Spanish, and\nDutch using five recent neural architectures. We identify subtle differences\nbetween the performance of BERT and Flair on two English NER corpora and\nidentify a weak spot in the performance of current models in Spanish. We\nconclude that the TMR metrics enable differentiation between otherwise\nsimilar-scoring systems and identification of patterns in performance that\nwould go unnoticed from overall precision, recall, and F1.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 05:04:14 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Tu", "Jingxuan", ""], ["Lignos", "Constantine", ""]]}, {"id": "2103.12329", "submitter": "Mohit Prabhushankar", "authors": "Mohit Prabhushankar and Ghassan AlRegib", "title": "Contrastive Reasoning in Neural Networks", "comments": "Submitted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Neural networks represent data as projections on trained weights in a high\ndimensional manifold. The trained weights act as a knowledge base consisting of\ncausal class dependencies. Inference built on features that identify these\ndependencies is termed as feed-forward inference. Such inference mechanisms are\njustified based on classical cause-to-effect inductive reasoning models.\nInductive reasoning based feed-forward inference is widely used due to its\nmathematical simplicity and operational ease. Nevertheless, feed-forward models\ndo not generalize well to untrained situations. To alleviate this\ngeneralization challenge, we propose using an effect-to-cause inference model\nthat reasons abductively. Here, the features represent the change from existing\nweight dependencies given a certain effect. We term this change as contrast and\nthe ensuing reasoning mechanism as contrastive reasoning. In this paper, we\nformalize the structure of contrastive reasoning and propose a methodology to\nextract a neural network's notion of contrast. We demonstrate the value of\ncontrastive reasoning in two stages of a neural network's reasoning pipeline :\nin inferring and visually explaining decisions for the application of object\nrecognition. We illustrate the value of contrastively recognizing images under\ndistortions by reporting an improvement of 3.47%, 2.56%, and 5.48% in average\naccuracy under the proposed contrastive framework on CIFAR-10C, noisy STL-10,\nand VisDA datasets respectively.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 05:54:36 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Prabhushankar", "Mohit", ""], ["AlRegib", "Ghassan", ""]]}, {"id": "2103.12370", "submitter": "Agnes Cseh", "authors": "Haris Aziz, Hau Chan, \\'Agnes Cseh, Bo Li, Fahimeh Ramezani, Chenhao\n  Wang", "title": "Multi-Robot Task Allocation -- Complexity and Approximation", "comments": "AAMAS 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-robot task allocation is one of the most fundamental classes of\nproblems in robotics and is crucial for various real-world robotic applications\nsuch as search, rescue and area exploration. We consider the Single-Task robots\nand Multi-Robot tasks Instantaneous Assignment (ST-MR-IA) setting where each\ntask requires at least a certain number of robots and each robot can work on at\nmost one task and incurs an operational cost for each task. Our aim is to\nconsider a natural computational problem of allocating robots to complete the\nmaximum number of tasks subject to budget constraints. We consider budget\nconstraints of three different kinds: (1) total budget, (2) task budget, and\n(3) robot budget. We provide a detailed complexity analysis including results\non approximations as well as polynomial-time algorithms for the general setting\nand important restricted settings.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 08:12:27 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Aziz", "Haris", ""], ["Chan", "Hau", ""], ["Cseh", "\u00c1gnes", ""], ["Li", "Bo", ""], ["Ramezani", "Fahimeh", ""], ["Wang", "Chenhao", ""]]}, {"id": "2103.12399", "submitter": "Antonio Emanuele Cin\\`a", "authors": "Antonio Emanuele Cin\\`a, Sebastiano Vascon, Ambra Demontis, Battista\n  Biggio, Fabio Roli, Marcello Pelillo", "title": "The Hammer and the Nut: Is Bilevel Optimization Really Needed to Poison\n  Linear Classifiers?", "comments": "8 pages, 7 figures, Submitted to IJCNN 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most concerning threats for modern AI systems is data poisoning,\nwhere the attacker injects maliciously crafted training data to corrupt the\nsystem's behavior at test time. Availability poisoning is a particularly\nworrisome subset of poisoning attacks where the attacker aims to cause a\nDenial-of-Service (DoS) attack. However, the state-of-the-art algorithms are\ncomputationally expensive because they try to solve a complex bi-level\noptimization problem (the \"hammer\"). We observed that in particular conditions,\nnamely, where the target model is linear (the \"nut\"), the usage of\ncomputationally costly procedures can be avoided. We propose a\ncounter-intuitive but efficient heuristic that allows contaminating the\ntraining set such that the target system's performance is highly compromised.\nWe further suggest a re-parameterization trick to decrease the number of\nvariables to be optimized. Finally, we demonstrate that, under the considered\nsettings, our framework achieves comparable, or even better, performances in\nterms of the attacker's objective while being significantly more\ncomputationally efficient.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 09:08:10 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Cin\u00e0", "Antonio Emanuele", ""], ["Vascon", "Sebastiano", ""], ["Demontis", "Ambra", ""], ["Biggio", "Battista", ""], ["Roli", "Fabio", ""], ["Pelillo", "Marcello", ""]]}, {"id": "2103.12419", "submitter": "Artur Sokolovsky", "authors": "Artur Sokolovsky, Luca Arnaboldi, Jaume Bacardit, Thomas Gross", "title": "Explainable Machine Learning-driven Strategy for Automated Trading\n  Pattern Extraction", "comments": "The reproducibility package available at:\n  https://doi.org/10.5281/zenodo.4629567", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SY eess.SY q-fin.TR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Financial markets are a source of non-stationary multidimensional time series\nwhich has been drawing attention for decades. Each financial instrument has its\nspecific changing over time properties, making their analysis a complex task.\nImprovement of understanding and development of methods for financial time\nseries analysis is essential for successful operation on financial markets. In\nthis study we propose a volume-based data pre-processing method for making\nfinancial time series more suitable for machine learning pipelines. We use a\nstatistical approach for assessing the performance of the method. Namely, we\nformally state the hypotheses, set up associated classification tasks, compute\neffect sizes with confidence intervals, and run statistical tests to validate\nthe hypotheses. We additionally assess the trading performance of the proposed\nmethod on historical data and compare it to a previously published approach.\nOur analysis shows that the proposed volume-based method allows successful\nclassification of the financial time series patterns, and also leads to better\nclassification performance than a price action-based method, excelling\nspecifically on more liquid financial instruments. Finally, we propose an\napproach for obtaining feature interactions directly from tree-based models on\nexample of CatBoost estimator, as well as formally assess the relatedness of\nthe proposed approach and SHAP feature interactions with a positive outcome.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 09:55:46 GMT"}, {"version": "v2", "created": "Tue, 13 Apr 2021 14:36:44 GMT"}, {"version": "v3", "created": "Thu, 13 May 2021 14:35:25 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Sokolovsky", "Artur", ""], ["Arnaboldi", "Luca", ""], ["Bacardit", "Jaume", ""], ["Gross", "Thomas", ""]]}, {"id": "2103.12450", "submitter": "Jan Philip Wahle", "authors": "Jan Philip Wahle, Terry Ruas, Norman Meuschke, Bela Gipp", "title": "Are Neural Language Models Good Plagiarists? A Benchmark for Neural\n  Paraphrase Detection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DL", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  The rise of language models such as BERT allows for high-quality text\nparaphrasing. This is a problem to academic integrity, as it is difficult to\ndifferentiate between original and machine-generated content. We propose a\nbenchmark consisting of paraphrased articles using recent language models\nrelying on the Transformer architecture. Our contribution fosters future\nresearch of paraphrase detection systems as it offers a large collection of\naligned original and paraphrased documents, a study regarding its structure,\nclassification experiments with state-of-the-art systems, and we make our\nfindings publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 11:01:35 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Wahle", "Jan Philip", ""], ["Ruas", "Terry", ""], ["Meuschke", "Norman", ""], ["Gipp", "Bela", ""]]}, {"id": "2103.12469", "submitter": "Hao Huang", "authors": "Hao Huang, Yongtao Wang, Zhaoyu Chen, Zhi Tang, Wenqiang Zhang and\n  Kai-Kuang Ma", "title": "RPATTACK: Refined Patch Attack on General Object Detectors", "comments": "6 pages, 4 figures, IEEE International Conference on Multimedia and\n  Expo (ICME) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Nowadays, general object detectors like YOLO and Faster R-CNN as well as\ntheir variants are widely exploited in many applications. Many works have\nrevealed that these detectors are extremely vulnerable to adversarial patch\nattacks. The perturbed regions generated by previous patch-based attack works\non object detectors are very large which are not necessary for attacking and\nperceptible for human eyes. To generate much less but more efficient\nperturbation, we propose a novel patch-based method for attacking general\nobject detectors. Firstly, we propose a patch selection and refining scheme to\nfind the pixels which have the greatest importance for attack and remove the\ninconsequential perturbations gradually. Then, for a stable ensemble attack, we\nbalance the gradients of detectors to avoid over-optimizing one of them during\nthe training phase. Our RPAttack can achieve an amazing missed detection rate\nof 100% for both Yolo v4 and Faster R-CNN while only modifies 0.32% pixels on\nVOC 2007 test set. Our code is available at\nhttps://github.com/VDIGPKU/RPAttack.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 11:45:41 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Huang", "Hao", ""], ["Wang", "Yongtao", ""], ["Chen", "Zhaoyu", ""], ["Tang", "Zhi", ""], ["Zhang", "Wenqiang", ""], ["Ma", "Kai-Kuang", ""]]}, {"id": "2103.12489", "submitter": "Chenguo Lin", "authors": "Ruowei Wang, Chenguo Lin, Qijun Zhao, Feiyu Zhu", "title": "Watermark Faker: Towards Forgery of Digital Image Watermarking", "comments": "6 pages; accepted by ICME2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digital watermarking has been widely used to protect the copyright and\nintegrity of multimedia data. Previous studies mainly focus on designing\nwatermarking techniques that are robust to attacks of destroying the embedded\nwatermarks. However, the emerging deep learning based image generation\ntechnology raises new open issues that whether it is possible to generate fake\nwatermarked images for circumvention. In this paper, we make the first attempt\nto develop digital image watermark fakers by using generative adversarial\nlearning. Suppose that a set of paired images of original and watermarked\nimages generated by the targeted watermarker are available, we use them to\ntrain a watermark faker with U-Net as the backbone, whose input is an original\nimage, and after a domain-specific preprocessing, it outputs a fake watermarked\nimage. Our experiments show that the proposed watermark faker can effectively\ncrack digital image watermarkers in both spatial and frequency domains,\nsuggesting the risk of such forgery attacks.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 12:28:00 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Wang", "Ruowei", ""], ["Lin", "Chenguo", ""], ["Zhao", "Qijun", ""], ["Zhu", "Feiyu", ""]]}, {"id": "2103.12496", "submitter": "Uehwan Kim", "authors": "Ue-Hwan Kim, Jong-Hwan Kim", "title": "Revisiting Self-Supervised Monocular Depth Estimation", "comments": "14 pages, 3 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Self-supervised learning of depth map prediction and motion estimation from\nmonocular video sequences is of vital importance -- since it realizes a broad\nrange of tasks in robotics and autonomous vehicles. A large number of research\nefforts have enhanced the performance by tackling illumination variation,\nocclusions, and dynamic objects, to name a few. However, each of those efforts\ntargets individual goals and endures as separate works. Moreover, most of\nprevious works have adopted the same CNN architecture, not reaping\narchitectural benefits. Therefore, the need to investigate the inter-dependency\nof the previous methods and the effect of architectural factors remains. To\nachieve these objectives, we revisit numerous previously proposed\nself-supervised methods for joint learning of depth and motion, perform a\ncomprehensive empirical study, and unveil multiple crucial insights.\nFurthermore, we remarkably enhance the performance as a result of our study --\noutperforming previous state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 12:45:00 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Kim", "Ue-Hwan", ""], ["Kim", "Jong-Hwan", ""]]}, {"id": "2103.12528", "submitter": "Nicola De Cao", "authors": "Nicola De Cao, Ledell Wu, Kashyap Popat, Mikel Artetxe, Naman Goyal,\n  Mikhail Plekhanov, Luke Zettlemoyer, Nicola Cancedda, Sebastian Riedel, Fabio\n  Petroni", "title": "Multilingual Autoregressive Entity Linking", "comments": "20 pages, 8 figures, and 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present mGENRE, a sequence-to-sequence system for the Multilingual Entity\nLinking (MEL) problem -- the task of resolving language-specific mentions to a\nmultilingual Knowledge Base (KB). For a mention in a given language, mGENRE\npredicts the name of the target entity left-to-right, token-by-token in an\nautoregressive fashion. The autoregressive formulation allows us to effectively\ncross-encode mention string and entity names to capture more interactions than\nthe standard dot product between mention and entity vectors. It also enables\nfast search within a large KB even for mentions that do not appear in mention\ntables and with no need for large-scale vector indices. While prior MEL works\nuse a single representation for each entity, we match against entity names of\nas many languages as possible, which allows exploiting language connections\nbetween source input and target name. Moreover, in a zero-shot setting on\nlanguages with no training data at all, mGENRE treats the target language as a\nlatent variable that is marginalized at prediction time. This leads to over 50%\nimprovements in average accuracy. We show the efficacy of our approach through\nextensive evaluation including experiments on three popular MEL benchmarks\nwhere mGENRE establishes new state-of-the-art results. Code and pre-trained\nmodels at https://github.com/facebookresearch/GENRE.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 13:25:55 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["De Cao", "Nicola", ""], ["Wu", "Ledell", ""], ["Popat", "Kashyap", ""], ["Artetxe", "Mikel", ""], ["Goyal", "Naman", ""], ["Plekhanov", "Mikhail", ""], ["Zettlemoyer", "Luke", ""], ["Cancedda", "Nicola", ""], ["Riedel", "Sebastian", ""], ["Petroni", "Fabio", ""]]}, {"id": "2103.12535", "submitter": "Muhammad Usman", "authors": "Muhammad Usman, Divya Gopinath, Youcheng Sun, Yannic Noller and Corina\n  Pasareanu", "title": "NNrepair: Constraint-based Repair of Neural Network Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present NNrepair, a constraint-based technique for repairing neural\nnetwork classifiers. The technique aims to fix the logic of the network at an\nintermediate layer or at the last layer. NNrepair first uses fault localization\nto find potentially faulty network parameters (such as the weights) and then\nperforms repair using constraint solving to apply small modifications to the\nparameters to remedy the defects. We present novel strategies to enable precise\nyet efficient repair such as inferring correctness specifications to act as\noracles for intermediate layer repair, and generation of experts for each\nclass. We demonstrate the technique in the context of three different\nscenarios: (1) Improving the overall accuracy of a model, (2) Fixing security\nvulnerabilities caused by poisoning of training data and (3) Improving the\nrobustness of the network against adversarial attacks. Our evaluation on MNIST\nand CIFAR-10 models shows that NNrepair can improve the accuracy by 45.56\npercentage points on poisoned data and 10.40 percentage points on adversarial\ndata. NNrepair also provides small improvement in the overall accuracy of\nmodels, without requiring new data or re-training.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 13:44:01 GMT"}, {"version": "v2", "created": "Tue, 15 Jun 2021 02:57:39 GMT"}], "update_date": "2021-06-16", "authors_parsed": [["Usman", "Muhammad", ""], ["Gopinath", "Divya", ""], ["Sun", "Youcheng", ""], ["Noller", "Yannic", ""], ["Pasareanu", "Corina", ""]]}, {"id": "2103.12537", "submitter": "Shaina Raza Ms", "authors": "Shaina Raza", "title": "A News Recommender System Considering Temporal Dynamics and Diversity", "comments": "A doctoral symposium", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In a news recommender system, a reader's preferences change over time. Some\npreferences drift quite abruptly (short-term preferences), while others change\nover a longer period of time (long-term preferences). Although the existing\nnews recommender systems consider the reader's full history, they often ignore\nthe dynamics in the reader's behavior. Thus, they cannot meet the demand of the\nnews readers for their time-varying preferences. In addition, the\nstate-of-the-art news recommendation models are often focused on providing\naccurate predictions, which can work well in traditional recommendation\nscenarios. However, in a news recommender system, diversity is essential, not\nonly to keep news readers engaged, but also to play a key role in a democratic\nsociety. In this PhD dissertation, our goal is to build a news recommender\nsystem to address these two challenges. Our system should be able to: (i)\naccommodate the dynamics in reader behavior; and (ii) consider both accuracy\nand diversity in the design of the recommendation model. Our news recommender\nsystem can also work for unprofiled, anonymous and short-term readers, by\nleveraging the rich side information of the news items and by including the\nimplicit feedback in our model. We evaluate our model with multiple evaluation\nmeasures (both accuracy and diversity-oriented metrics) to demonstrate the\neffectiveness of our methods.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 13:45:34 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Raza", "Shaina", ""]]}, {"id": "2103.12541", "submitter": "Preslav Nakov", "authors": "Firoj Alam, Stefano Cresci, Tanmoy Chakraborty, Fabrizio Silvestri,\n  Dimiter Dimitrov, Giovanni Da San Martino, Shaden Shaar, Hamed Firooz,\n  Preslav Nakov", "title": "A Survey on Multimodal Disinformation Detection", "comments": "disinformation, misinformation, factuality, harmfulness, fake news,\n  propaganda, multimodality, text, images, videos, network structure,\n  temporality", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.AI cs.CL cs.CR cs.CY cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed the proliferation of fake news, propaganda,\nmisinformation, and disinformation online. While initially this was mostly\nabout textual content, over time images and videos gained popularity, as they\nare much easier to consume, attract much more attention, and spread further\nthan simple text. As a result, researchers started targeting different\nmodalities and combinations thereof. As different modalities are studied in\ndifferent research communities, with insufficient interaction, here we offer a\nsurvey that explores the state-of-the-art on multimodal disinformation\ndetection covering various combinations of modalities: text, images, audio,\nvideo, network structure, and temporal information. Moreover, while some\nstudies focused on factuality, others investigated how harmful the content is.\nWhile these two components in the definition of disinformation -- (i)\nfactuality and (ii) harmfulness, are equally important, they are typically\nstudied in isolation. Thus, we argue for the need to tackle disinformation\ndetection by taking into account multiple modalities as well as both factuality\nand harmfulness, in the same framework. Finally, we discuss current challenges\nand future research directions.\n", "versions": [{"version": "v1", "created": "Sat, 13 Mar 2021 18:04:17 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Alam", "Firoj", ""], ["Cresci", "Stefano", ""], ["Chakraborty", "Tanmoy", ""], ["Silvestri", "Fabrizio", ""], ["Dimitrov", "Dimiter", ""], ["Martino", "Giovanni Da San", ""], ["Shaar", "Shaden", ""], ["Firooz", "Hamed", ""], ["Nakov", "Preslav", ""]]}, {"id": "2103.12544", "submitter": "Ripon Patgiri", "authors": "Ripon Patgiri, Anupam Biswas and Sabuzima Nayak", "title": "deepBF: Malicious URL detection using Learned Bloom Filter and\n  Evolutionary Deep Learning", "comments": "This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.CV cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Malicious URL detection is an emerging research area due to continuous\nmodernization of various systems, for instance, Edge Computing. In this\narticle, we present a novel malicious URL detection technique, called deepBF\n(deep learning and Bloom Filter). deepBF is presented in two-fold. Firstly, we\npropose a learned Bloom Filter using 2-dimensional Bloom Filter. We\nexperimentally decide the best non-cryptography string hash function. Then, we\nderive a modified non-cryptography string hash function from the selected hash\nfunction for deepBF by introducing biases in the hashing method and compared\namong the string hash functions. The modified string hash function is compared\nto other variants of diverse non-cryptography string hash functions. It is also\ncompared with various filters, particularly, counting Bloom Filter, Kirsch\n\\textit{et al.}, and Cuckoo Filter using various use cases. The use cases\nunearth weakness and strength of the filters. Secondly, we propose a malicious\nURL detection mechanism using deepBF. We apply the evolutionary convolutional\nneural network to identify the malicious URLs. The evolutionary convolutional\nneural network is trained and tested with malicious URL datasets. The output is\ntested in deepBF for accuracy. We have achieved many conclusions from our\nexperimental evaluation and results and are able to reach various conclusive\ndecisions which are presented in the article.\n", "versions": [{"version": "v1", "created": "Thu, 18 Mar 2021 21:53:22 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Patgiri", "Ripon", ""], ["Biswas", "Anupam", ""], ["Nayak", "Sabuzima", ""]]}, {"id": "2103.12545", "submitter": "Edwin Pan", "authors": "Edwin Pan, Anthony Vento", "title": "MetaHDR: Model-Agnostic Meta-Learning for HDR Image Reconstruction", "comments": "7 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Capturing scenes with a high dynamic range is crucial to reproducing images\nthat appear similar to those seen by the human visual system. Despite progress\nin developing data-driven deep learning approaches for converting low dynamic\nrange images to high dynamic range images, existing approaches are limited by\nthe assumption that all conversions are governed by the same nonlinear mapping.\nTo address this problem, we propose \"Model-Agnostic Meta-Learning for HDR Image\nReconstruction\" (MetaHDR), which applies meta-learning to the LDR-to-HDR\nconversion problem using existing HDR datasets. Our key novelty is the\nreinterpretation of LDR-to-HDR conversion scenes as independently sampled tasks\nfrom a common LDR-to-HDR conversion task distribution. Naturally, we use a\nmeta-learning framework that learns a set of meta-parameters which capture the\ncommon structure consistent across all LDR-to-HDR conversion tasks. Finally, we\nperform experimentation with MetaHDR to demonstrate its capacity to tackle\nchallenging LDR-to-HDR image conversions. Code and pretrained models are\navailable at https://github.com/edwin-pan/MetaHDR.\n", "versions": [{"version": "v1", "created": "Sat, 20 Mar 2021 07:56:45 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Pan", "Edwin", ""], ["Vento", "Anthony", ""]]}, {"id": "2103.12558", "submitter": "Majid Mazouchi", "authors": "Aquib Mustafa, Majid Mazouchi, Subramanya Nageshrao, Hamidreza Modares", "title": "Assured Learning-enabled Autonomy: A Metacognitive Reinforcement\n  Learning Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning (RL) agents with pre-specified reward functions cannot\nprovide guaranteed safety across variety of circumstances that an uncertain\nsystem might encounter. To guarantee performance while assuring satisfaction of\nsafety constraints across variety of circumstances, an assured autonomous\ncontrol framework is presented in this paper by empowering RL algorithms with\nmetacognitive learning capabilities. More specifically, adapting the reward\nfunction parameters of the RL agent is performed in a metacognitive\ndecision-making layer to assure the feasibility of RL agent. That is, to assure\nthat the learned policy by the RL agent satisfies safety constraints specified\nby signal temporal logic while achieving as much performance as possible. The\nmetacognitive layer monitors any possible future safety violation under the\nactions of the RL agent and employs a higher-layer Bayesian RL algorithm to\nproactively adapt the reward function for the lower-layer RL agent. To minimize\nthe higher-layer Bayesian RL intervention, a fitness function is leveraged by\nthe metacognitive layer as a metric to evaluate success of the lower-layer RL\nagent in satisfaction of safety and liveness specifications, and the\nhigher-layer Bayesian RL intervenes only if there is a risk of lower-layer RL\nfailure. Finally, a simulation example is provided to validate the\neffectiveness of the proposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 14:01:35 GMT"}, {"version": "v2", "created": "Sat, 17 Apr 2021 19:01:31 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Mustafa", "Aquib", ""], ["Mazouchi", "Majid", ""], ["Nageshrao", "Subramanya", ""], ["Modares", "Hamidreza", ""]]}, {"id": "2103.12564", "submitter": "Huy Nguyen BSc", "authors": "Huy Le Nguyen, Dominique Chu", "title": "Linear Constraints Learning for Spiking Neurons", "comments": "17 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Encoding information with precise spike timings using spike-coded neurons has\nbeen shown to be more computationally powerful than rate-coded approaches.\nHowever, most existing supervised learning algorithms for spiking neurons are\ncomplicated and offer poor time complexity. To address these limitations, we\npropose a supervised multi-spike learning algorithm which reduces the required\nnumber of training iterations. We achieve this by formulating a large number of\nweight updates as a linear constraint satisfaction problem, which can be solved\nefficiently. Experimental results show this method offers better efficiency\ncompared to existing algorithms on the MNIST dataset. Additionally, we provide\nexperimental results on the classification capacity of the LIF neuron model,\nrelative to several parameters of the system.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 13:54:05 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Nguyen", "Huy Le", ""], ["Chu", "Dominique", ""]]}, {"id": "2103.12576", "submitter": "Negar Safinianaini", "authors": "Negar Safinianaini and Henrik Bostr\\\"om", "title": "Towards interpretability of Mixtures of Hidden Markov Models", "comments": null, "journal-ref": "AAAI Workshop XAI (2021) 4-10", "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixtures of Hidden Markov Models (MHMMs) are frequently used for clustering\nof sequential data. An important aspect of MHMMs, as of any clustering\napproach, is that they can be interpretable, allowing for novel insights to be\ngained from the data. However, without a proper way of measuring\ninterpretability, the evaluation of novel contributions is difficult and it\nbecomes practically impossible to devise techniques that directly optimize this\nproperty. In this work, an information-theoretic measure (entropy) is proposed\nfor interpretability of MHMMs, and based on that, a novel approach to improve\nmodel interpretability is proposed, i.e., an entropy-regularized Expectation\nMaximization (EM) algorithm. The new approach aims for reducing the entropy of\nthe Markov chains (involving state transition matrices) within an MHMM, i.e.,\nassigning higher weights to common state transitions during clustering. It is\nargued that this entropy reduction, in general, leads to improved\ninterpretability since the most influential and important state transitions of\nthe clusters can be more easily identified. An empirical investigation shows\nthat it is possible to improve the interpretability of MHMMs, as measured by\nentropy, without sacrificing (but rather improving) clustering performance and\ncomputational costs, as measured by the v-measure and number of EM iterations,\nrespectively.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 14:25:03 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Safinianaini", "Negar", ""], ["Bostr\u00f6m", "Henrik", ""]]}, {"id": "2103.12670", "submitter": "Bruno Henrique Pachulski Camara", "authors": "B. H. P. Camara, M. A. G. Silva, A. T. Endo, S. R. Vergilio", "title": "What is the Vocabulary of Flaky Tests? An Extended Replication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software systems have been continuously evolved and delivered with high\nquality due to the widespread adoption of automated tests. A recurring issue\nhurting this scenario is the presence of flaky tests, a test case that may pass\nor fail non-deterministically. A promising, but yet lacking more empirical\nevidence, approach is to collect static data of automated tests and use them to\npredict their flakiness. In this paper, we conducted an empirical study to\nassess the use of code identifiers to predict test flakiness. To do so, we\nfirst replicate most parts of the previous study of Pinto~et~al.~(MSR~2020).\nThis replication was extended by using a different ML Python platform\n(Scikit-learn) and adding different learning algorithms in the analyses. Then,\nwe validated the performance of trained models using datasets with other flaky\ntests and from different projects. We successfully replicated the results of\nPinto~et~al.~(2020), with minor differences using Scikit-learn; different\nalgorithms had performance similar to the ones used previously. Concerning the\nvalidation, we noticed that the recall of the trained models was smaller, and\nclassifiers presented a varying range of decreases. This was observed in both\nintra-project and inter-projects test flakiness prediction.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 16:42:22 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Camara", "B. H. P.", ""], ["Silva", "M. A. G.", ""], ["Endo", "A. T.", ""], ["Vergilio", "S. R.", ""]]}, {"id": "2103.12679", "submitter": "Antonio Andriella", "authors": "Antonio Andriella, Alessandra Rossi, Silvia Rossi, Anouk van Maris", "title": "The Road to a Successful HRI: AI, Trust and ethicS-TRAITS", "comments": "TRAITS Workshop Proceedings including 9 articles", "journal-ref": "Companion of the 2021 ACM/IEEE International Conference on\n  Human-Robot Interaction, March 2021, Pages 709-711", "doi": "10.1145/3434074.3444872", "report-no": null, "categories": "cs.RO cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The aim of this workshop is to give researchers from academia and industry\nthe possibility to discuss the inter-and multi-disciplinary nature of the\nrelationships between people and robots towards effective and long-lasting\ncollaborations. This workshop will provide a forum for the HRI and robotics\ncommunities to explore successful human-robot interaction (HRI) to analyse the\ndifferent aspects of HRI that impact its success. Particular focus are the AI\nalgorithms required to implement autonomous interactions, and the factors that\nenhance, undermine, or recover humans' trust in robots. Finally, potential\nethical and legal concerns, and how they can be addressed will be considered.\nWebsite: https://sites.google.com/view/traits-hri\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 16:52:12 GMT"}, {"version": "v2", "created": "Sat, 27 Mar 2021 18:27:25 GMT"}, {"version": "v3", "created": "Mon, 12 Apr 2021 12:24:20 GMT"}, {"version": "v4", "created": "Tue, 20 Apr 2021 12:48:23 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Andriella", "Antonio", ""], ["Rossi", "Alessandra", ""], ["Rossi", "Silvia", ""], ["van Maris", "Anouk", ""]]}, {"id": "2103.12685", "submitter": "Paulina Grnarova", "authors": "Paulina Grnarova, Yannic Kilcher, Kfir Y. Levy, Aurelien Lucchi,\n  Thomas Hofmann", "title": "Generative Minimization Networks: Training GANs Without Competition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many applications in machine learning can be framed as minimization problems\nand solved efficiently using gradient-based techniques. However, recent\napplications of generative models, particularly GANs, have triggered interest\nin solving min-max games for which standard optimization techniques are often\nnot suitable. Among known problems experienced by practitioners is the lack of\nconvergence guarantees or convergence to a non-optimum cycle. At the heart of\nthese problems is the min-max structure of the GAN objective which creates\nnon-trivial dependencies between the players. We propose to address this\nproblem by optimizing a different objective that circumvents the min-max\nstructure using the notion of duality gap from game theory. We provide novel\nconvergence guarantees on this objective and demonstrate why the obtained limit\npoint solves the problem better than known techniques.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 17:01:08 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Grnarova", "Paulina", ""], ["Kilcher", "Yannic", ""], ["Levy", "Kfir Y.", ""], ["Lucchi", "Aurelien", ""], ["Hofmann", "Thomas", ""]]}, {"id": "2103.12690", "submitter": "Yuanhao Wang", "authors": "Yuanhao Wang, Ruosong Wang, Sham M. Kakade", "title": "An Exponential Lower Bound for Linearly-Realizable MDPs with Constant\n  Suboptimality Gap", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  A fundamental question in the theory of reinforcement learning is: suppose\nthe optimal $Q$-function lies in the linear span of a given $d$ dimensional\nfeature mapping, is sample-efficient reinforcement learning (RL) possible? The\nrecent and remarkable result of Weisz et al. (2020) resolved this question in\nthe negative, providing an exponential (in $d$) sample size lower bound, which\nholds even if the agent has access to a generative model of the environment.\nOne may hope that this information theoretic barrier for RL can be circumvented\nby further supposing an even more favorable assumption: there exists a\n\\emph{constant suboptimality gap} between the optimal $Q$-value of the best\naction and that of the second-best action (for all states). The hope is that\nhaving a large suboptimality gap would permit easier identification of optimal\nactions themselves, thus making the problem tractable; indeed, provided the\nagent has access to a generative model, sample-efficient RL is in fact possible\nwith the addition of this more favorable assumption.\n  This work focuses on this question in the standard online reinforcement\nlearning setting, where our main result resolves this question in the negative:\nour hardness result shows that an exponential sample complexity lower bound\nstill holds even if a constant suboptimality gap is assumed in addition to\nhaving a linearly realizable optimal $Q$-function. Perhaps surprisingly, this\nimplies an exponential separation between the online RL setting and the\ngenerative model setting. Complementing our negative hardness result, we give\ntwo positive results showing that provably sample-efficient RL is possible\neither under an additional low-variance assumption or under a novel\nhypercontractivity assumption (both implicitly place stronger conditions on the\nunderlying dynamics model).\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 17:05:54 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Wang", "Yuanhao", ""], ["Wang", "Ruosong", ""], ["Kakade", "Sham M.", ""]]}, {"id": "2103.12701", "submitter": "Zhaoxing Bu", "authors": "Zhaoxing Bu and Richard E. Korf", "title": "A*+BFHS: A Hybrid Heuristic Search Algorithm", "comments": "9 pages, 1 figure, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new algorithm A*+BFHS for solving hard problems where A* and\nIDA* fail due to memory limitations and/or the existence of many short cycles.\nA*+BFHS is based on A* and breadth-first heuristic search (BFHS). A*+BFHS\ncombines advantages from both algorithms, namely A*'s node ordering, BFHS's\nmemory savings, and both algorithms' duplicate detection. On easy problems,\nA*+BFHS behaves the same as A*. On hard problems, it is slower than A* but\nsaves a large amount of memory. Compared to BFIDA*, A*+BFHS reduces the search\ntime and/or memory requirement by several times on a variety of planning\ndomains.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 17:22:03 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Bu", "Zhaoxing", ""], ["Korf", "Richard E.", ""]]}, {"id": "2103.12703", "submitter": "Alexander Ku", "authors": "Alexander Ku and Peter Anderson and Jordi Pont-Tuset and Jason\n  Baldridge", "title": "PanGEA: The Panoramic Graph Environment Annotation Toolkit", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  PanGEA, the Panoramic Graph Environment Annotation toolkit, is a lightweight\ntoolkit for collecting speech and text annotations in photo-realistic 3D\nenvironments. PanGEA immerses annotators in a web-based simulation and allows\nthem to move around easily as they speak and/or listen. It includes database\nand cloud storage integration, plus utilities for automatically aligning\nrecorded speech with manual transcriptions and the virtual pose of the\nannotators. Out of the box, PanGEA supports two tasks -- collecting navigation\ninstructions and navigation instruction following -- and it could be easily\nadapted for annotating walking tours, finding and labeling landmarks or\nobjects, and similar tasks. We share best practices learned from using PanGEA\nin a 20,000 hour annotation effort to collect the Room-Across-Room dataset. We\nhope that our open-source annotation toolkit and insights will both expedite\nfuture data collection efforts and spur innovation on the kinds of grounded\nlanguage tasks such environments can support.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 17:24:12 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Ku", "Alexander", ""], ["Anderson", "Peter", ""], ["Pont-Tuset", "Jordi", ""], ["Baldridge", "Jason", ""]]}, {"id": "2103.12710", "submitter": "Jimmy Wu", "authors": "Jimmy Wu, Xingyuan Sun, Andy Zeng, Shuran Song, Szymon Rusinkiewicz,\n  Thomas Funkhouser", "title": "Spatial Intention Maps for Multi-Agent Mobile Manipulation", "comments": "To appear at IEEE International Conference on Robotics and Automation\n  (ICRA), 2021. Project page: https://spatial-intention-maps.cs.princeton.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The ability to communicate intention enables decentralized multi-agent robots\nto collaborate while performing physical tasks. In this work, we present\nspatial intention maps, a new intention representation for multi-agent\nvision-based deep reinforcement learning that improves coordination between\ndecentralized mobile manipulators. In this representation, each agent's\nintention is provided to other agents, and rendered into an overhead 2D map\naligned with visual observations. This synergizes with the recently proposed\nspatial action maps framework, in which state and action representations are\nspatially aligned, providing inductive biases that encourage emergent\ncooperative behaviors requiring spatial coordination, such as passing objects\nto each other or avoiding collisions. Experiments across a variety of\nmulti-agent environments, including heterogeneous robot teams with different\nabilities (lifting, pushing, or throwing), show that incorporating spatial\nintention maps improves performance for different mobile manipulation tasks\nwhile significantly enhancing cooperative behaviors.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 17:31:14 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Wu", "Jimmy", ""], ["Sun", "Xingyuan", ""], ["Zeng", "Andy", ""], ["Song", "Shuran", ""], ["Rusinkiewicz", "Szymon", ""], ["Funkhouser", "Thomas", ""]]}, {"id": "2103.12715", "submitter": "Andr\\'e Cruz", "authors": "Andr\\'e F. Cruz, Pedro Saleiro, Catarina Bel\\'em, Carlos Soares, Pedro\n  Bizarro", "title": "Promoting Fairness through Hyperparameter Optimization", "comments": "arXiv admin note: substantial text overlap with arXiv:2010.03665", "journal-ref": "ICLR 2021 Workshop on Responsible AI", "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Considerable research effort has been guided towards algorithmic fairness but\nreal-world adoption of bias reduction techniques is still scarce. Existing\nmethods are either metric- or model-specific, require access to sensitive\nattributes at inference time, or carry high development and deployment costs.\nThis work explores, in the context of a real-world fraud detection application,\nthe unfairness that emerges from traditional ML model development, and how to\nmitigate it with a simple and easily deployed intervention: fairness-aware\nhyperparameter optimization (HO). We propose and evaluate fairness-aware\nvariants of three popular HO algorithms: Fair Random Search, Fair TPE, and\nFairband. Our method enables practitioners to adapt pre-existing business\noperations to accommodate fairness objectives in a frictionless way and with\ncontrollable fairness-accuracy trade-offs. Additionally, it can be coupled with\nexisting bias reduction techniques to tune their hyperparameters. We validate\nour approach on a real-world bank account opening fraud use case, as well as on\nthree datasets from the fairness literature. Results show that, without extra\ntraining cost, it is feasible to find models with 111% average fairness\nincrease and just 6% decrease in predictive accuracy, when compared to standard\nfairness-blind HO.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 17:36:22 GMT"}], "update_date": "2021-04-29", "authors_parsed": [["Cruz", "Andr\u00e9 F.", ""], ["Saleiro", "Pedro", ""], ["Bel\u00e9m", "Catarina", ""], ["Soares", "Carlos", ""], ["Bizarro", "Pedro", ""]]}, {"id": "2103.12719", "submitter": "Chaitanya Ryali", "authors": "Chaitanya K. Ryali, David J. Schwab, Ari S. Morcos", "title": "Leveraging background augmentations to encourage semantic focus in\n  self-supervised contrastive learning", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised representation learning is an important challenge in computer\nvision, with self-supervised learning methods recently closing the gap to\nsupervised representation learning. An important ingredient in high-performing\nself-supervised methods is the use of data augmentation by training models to\nplace different augmented views of the same image nearby in embedding space.\nHowever, commonly used augmentation pipelines treat images holistically,\ndisregarding the semantic relevance of parts of an image-e.g. a subject vs. a\nbackground-which can lead to the learning of spurious correlations. Our work\naddresses this problem by investigating a class of simple, yet highly effective\n\"background augmentations\", which encourage models to focus on\nsemantically-relevant content by discouraging them from focusing on image\nbackgrounds. Background augmentations lead to substantial improvements (+1-2%\non ImageNet-1k) in performance across a spectrum of state-of-the art\nself-supervised methods (MoCov2, BYOL, SwAV) on a variety of tasks, allowing us\nto reach within 0.3% of supervised performance. We also demonstrate that\nbackground augmentations improve robustness to a number of out of distribution\nsettings, including natural adversarial examples, the backgrounds challenge,\nadversarial attacks, and ReaL ImageNet.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 17:39:16 GMT"}], "update_date": "2021-03-24", "authors_parsed": [["Ryali", "Chaitanya K.", ""], ["Schwab", "David J.", ""], ["Morcos", "Ari S.", ""]]}, {"id": "2103.12726", "submitter": "Hiroki Furuta", "authors": "Hiroki Furuta, Tatsuya Matsushima, Tadashi Kozuno, Yutaka Matsuo,\n  Sergey Levine, Ofir Nachum, Shixiang Shane Gu", "title": "Policy Information Capacity: Information-Theoretic Measure for Task\n  Complexity in Deep Reinforcement Learning", "comments": "Accepted to ICML2021. The code is available at:\n  https://github.com/frt03/pic", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Progress in deep reinforcement learning (RL) research is largely enabled by\nbenchmark task environments. However, analyzing the nature of those\nenvironments is often overlooked. In particular, we still do not have agreeable\nways to measure the difficulty or solvability of a task, given that each has\nfundamentally different actions, observations, dynamics, rewards, and can be\ntackled with diverse RL algorithms. In this work, we propose policy information\ncapacity (PIC) -- the mutual information between policy parameters and episodic\nreturn -- and policy-optimal information capacity (POIC) -- between policy\nparameters and episodic optimality -- as two environment-agnostic,\nalgorithm-agnostic quantitative metrics for task difficulty. Evaluating our\nmetrics across toy environments as well as continuous control benchmark tasks\nfrom OpenAI Gym and DeepMind Control Suite, we empirically demonstrate that\nthese information-theoretic metrics have higher correlations with normalized\ntask solvability scores than a variety of alternatives. Lastly, we show that\nthese metrics can also be used for fast and compute-efficient optimizations of\nkey design parameters such as reward shaping, policy architectures, and MDP\nproperties for better solvability by RL algorithms without ever running full RL\nexperiments.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 17:49:50 GMT"}, {"version": "v2", "created": "Mon, 31 May 2021 12:12:08 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Furuta", "Hiroki", ""], ["Matsushima", "Tatsuya", ""], ["Kozuno", "Tadashi", ""], ["Matsuo", "Yutaka", ""], ["Levine", "Sergey", ""], ["Nachum", "Ofir", ""], ["Gu", "Shixiang Shane", ""]]}, {"id": "2103.12777", "submitter": "Sonal Garg", "authors": "Sonal Garg, Sumanth Prabhu, Hemant Misra, and G. Srinivasaraghavan", "title": "Unsupervised Contextual Paraphrase Generation using Lexical Control and\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Customer support via chat requires agents to resolve customer queries with\nminimum wait time and maximum customer satisfaction. Given that the agents as\nwell as the customers can have varying levels of literacy, the overall quality\nof responses provided by the agents tend to be poor if they are not predefined.\nBut using only static responses can lead to customer detraction as the\ncustomers tend to feel that they are no longer interacting with a human. Hence,\nit is vital to have variations of the static responses to reduce monotonicity\nof the responses. However, maintaining a list of such variations can be\nexpensive. Given the conversation context and the agent response, we propose an\nunsupervised frame-work to generate contextual paraphrases using autoregressive\nmodels. We also propose an automated metric based on Semantic Similarity,\nTextual Entailment, Expression Diversity and Fluency to evaluate the quality of\ncontextual paraphrases and demonstrate performance improvement with\nReinforcement Learning (RL) fine-tuning using the automated metric as the\nreward function.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 18:22:03 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Garg", "Sonal", ""], ["Prabhu", "Sumanth", ""], ["Misra", "Hemant", ""], ["Srinivasaraghavan", "G.", ""]]}, {"id": "2103.12854", "submitter": "Jo\\v{z}e Ro\\v{z}anec", "authors": "Jo\\v{z}e M. Ro\\v{z}anec, Jinzhi Lu, Jan Rupnik, Maja \\v{S}krjanc,\n  Dunja Mladeni\\'c, Bla\\v{z} Fortuna, Xiaochen Zheng, Dimitris Kiritsis", "title": "Actionable Cognitive Twins for Decision Making in Manufacturing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Actionable Cognitive Twins are the next generation Digital Twins enhanced\nwith cognitive capabilities through a knowledge graph and artificial\nintelligence models that provide insights and decision-making options to the\nusers. The knowledge graph describes the domain-specific knowledge regarding\nentities and interrelationships related to a manufacturing setting. It also\ncontains information on possible decision-making options that can assist\ndecision-makers, such as planners or logisticians. In this paper, we propose a\nknowledge graph modeling approach to construct actionable cognitive twins for\ncapturing specific knowledge related to demand forecasting and production\nplanning in a manufacturing plant. The knowledge graph provides semantic\ndescriptions and contextualization of the production lines and processes,\nincluding data identification and simulation or artificial intelligence\nalgorithms and forecasts used to support them. Such semantics provide ground\nfor inferencing, relating different knowledge types: creative, deductive,\ndefinitional, and inductive. To develop the knowledge graph models for\ndescribing the use case completely, systems thinking approach is proposed to\ndesign and verify the ontology, develop a knowledge graph and build an\nactionable cognitive twin. Finally, we evaluate our approach in two use cases\ndeveloped for a European original equipment manufacturer related to the\nautomotive industry as part of the European Horizon 2020 project FACTLOG.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 21:32:07 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Ro\u017eanec", "Jo\u017ee M.", ""], ["Lu", "Jinzhi", ""], ["Rupnik", "Jan", ""], ["\u0160krjanc", "Maja", ""], ["Mladeni\u0107", "Dunja", ""], ["Fortuna", "Bla\u017e", ""], ["Zheng", "Xiaochen", ""], ["Kiritsis", "Dimitris", ""]]}, {"id": "2103.12871", "submitter": "Jaeyeon Jang Dr.", "authors": "Jaeyeon Jang and Chang Ouk Kim", "title": "Teacher-Explorer-Student Learning: A Novel Learning Method for Open Set\n  Recognition", "comments": "12 pages, 13 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  If an unknown example that is not seen during training appears, most\nrecognition systems usually produce overgeneralized results and determine that\nthe example belongs to one of the known classes. To address this problem,\nteacher-explorer-student (T/E/S) learning, which adopts the concept of open set\nrecognition (OSR) that aims to reject unknown samples while minimizing the loss\nof classification performance on known samples, is proposed in this study. In\nthis novel learning method, overgeneralization of deep learning classifiers is\nsignificantly reduced by exploring various possibilities of unknowns. Here, the\nteacher network extracts some hints about unknowns by distilling the pretrained\nknowledge about knowns and delivers this distilled knowledge to the student.\nAfter learning the distilled knowledge, the student network shares the learned\ninformation with the explorer network. Then, the explorer network shares its\nexploration results by generating unknown-like samples and feeding the samples\nto the student network. By repeating this alternating learning process, the\nstudent network experiences a variety of synthetic unknowns, reducing\novergeneralization. Extensive experiments were conducted, and the experimental\nresults showed that each component proposed in this paper significantly\ncontributes to the improvement in OSR performance. As a result, the proposed\nT/E/S learning method outperformed current state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 22:32:32 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Jang", "Jaeyeon", ""], ["Kim", "Chang Ouk", ""]]}, {"id": "2103.12872", "submitter": "Stella Biderman", "authors": "Louis Castricato and Stella Biderman and Rogelio E. Cardona-Rivera and\n  David Thue", "title": "Towards a Formal Model of Narratives", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose the beginnings of a formal framework for modeling\nnarrative \\textit{qua} narrative. Our framework affords the ability to discuss\nkey qualities of stories and their communication, including the flow of\ninformation from a Narrator to a Reader, the evolution of a Reader's story\nmodel over time, and Reader uncertainty. We demonstrate its applicability to\ncomputational narratology by giving explicit algorithms for measuring the\naccuracy with which information was conveyed to the Reader and two novel\nmeasurements of story coherence.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 22:33:23 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Castricato", "Louis", ""], ["Biderman", "Stella", ""], ["Cardona-Rivera", "Rogelio E.", ""], ["Thue", "David", ""]]}, {"id": "2103.12894", "submitter": "Bart de Keijzer", "authors": "Bart de Keijzer and Dominik Wojtczak", "title": "Facility Reallocation on the Line", "comments": "28 Pages, 5 Figures. A prelimininary version of the paper, with most\n  proofs omitted, has appeared at IJCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a multi-stage facility reallocation problems on the real line,\nwhere a facility is being moved between time stages based on the locations\nreported by $n$ agents. The aim of the reallocation algorithm is to minimise\nthe social cost, i.e., the sum over the total distance between the facility and\nall agents at all stages, plus the cost incurred for moving the facility. We\nstudy this problem both in the offline setting and online setting. In the\noffline case the algorithm has full knowledge of the agent locations in all\nfuture stages, and in the online setting the algorithm does not know these\nfuture locations and must decide the location of the facility on a\nstage-per-stage basis. We derive the optimal algorithm in both cases. For the\nonline setting we show that its competitive ratio is $(n+2)/(n+1)$. As neither\nof these algorithms turns out to yield a strategy-proof mechanism, we propose\nanother strategy-proof mechanism which has a competitive ratio of $(n+3)/(n+1)$\nfor odd $n$ and $(n+4)/n$ for even $n$, which we conjecture to be the best\npossible. We also consider a generalisation with multiple facilities and\nweighted agents, for which we show that the optimum can be computed in\npolynomial time for a fixed number of facilities.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 23:48:45 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["de Keijzer", "Bart", ""], ["Wojtczak", "Dominik", ""]]}, {"id": "2103.12947", "submitter": "Jun-Hyun Bae", "authors": "Jun-Hyun Bae, Inchul Choi, Minho Lee", "title": "Meta-Learned Invariant Risk Minimization", "comments": "12 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Empirical Risk Minimization (ERM) based machine learning algorithms have\nsuffered from weak generalization performance on data obtained from\nout-of-distribution (OOD). To address this problem, Invariant Risk Minimization\n(IRM) objective was suggested to find invariant optimal predictor which is less\naffected by the changes in data distribution. However, even with such progress,\nIRMv1, the practical formulation of IRM, still shows performance degradation\nwhen there are not enough training data, and even fails to generalize to OOD,\nif the number of spurious correlations is larger than the number of\nenvironments. In this paper, to address such problems, we propose a novel\nmeta-learning based approach for IRM. In this method, we do not assume the\nlinearity of classifier for the ease of optimization, and solve ideal bi-level\nIRM objective with Model-Agnostic Meta-Learning (MAML) framework. Our method is\nmore robust to the data with spurious correlations and can provide an invariant\noptimal classifier even when data from each distribution are scarce. In\nexperiments, we demonstrate that our algorithm not only has better OOD\ngeneralization performance than IRMv1 and all IRM variants, but also addresses\nthe weakness of IRMv1 with improved stability.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 02:52:48 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Bae", "Jun-Hyun", ""], ["Choi", "Inchul", ""], ["Lee", "Minho", ""]]}, {"id": "2103.12951", "submitter": "Amit Verma Dr.", "authors": "Amit Verma and Mark Lewis", "title": "Goal Seeking Quadratic Unconstrained Binary Optimization", "comments": "Benchmark problems used are available from the first author", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI math.OC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Quadratic Unconstrained Binary Optimization (QUBO) modeling and solution\nframework is a requirement for quantum and digital annealers. However\noptimality for QUBO problems of any practical size is extremely difficult to\nachieve. In order to incorporate the problem-specific insights, a diverse set\nof solutions meeting an acceptable target metric or goal is the preference in\nhigh level decision making. In this paper, we present two alternatives for\ngoal-seeking QUBO for minimizing the deviation from a given target as well as a\nrange of values around a target. Experimental results illustrate the efficacy\nof the proposed approach over Constraint Programming for quickly finding a\nsatisficing set of solutions.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 03:03:13 GMT"}, {"version": "v2", "created": "Tue, 11 May 2021 22:34:40 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Verma", "Amit", ""], ["Lewis", "Mark", ""]]}, {"id": "2103.12975", "submitter": "Yining Hong", "authors": "Yining Hong, Qing Li, Song-Chun Zhu, Siyuan Huang", "title": "VLGrammar: Grounded Grammar Induction of Vision and Language", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cognitive grammar suggests that the acquisition of language grammar is\ngrounded within visual structures. While grammar is an essential representation\nof natural language, it also exists ubiquitously in vision to represent the\nhierarchical part-whole structure. In this work, we study grounded grammar\ninduction of vision and language in a joint learning framework. Specifically,\nwe present VLGrammar, a method that uses compound probabilistic context-free\ngrammars (compound PCFGs) to induce the language grammar and the image grammar\nsimultaneously. We propose a novel contrastive learning framework to guide the\njoint learning of both modules. To provide a benchmark for the grounded grammar\ninduction task, we collect a large-scale dataset, \\textsc{PartIt}, which\ncontains human-written sentences that describe part-level semantics for 3D\nobjects. Experiments on the \\textsc{PartIt} dataset show that VLGrammar\noutperforms all baselines in image grammar induction and language grammar\ninduction. The learned VLGrammar naturally benefits related downstream tasks.\nSpecifically, it improves the image unsupervised clustering accuracy by 30\\%,\nand performs well in image retrieval and text retrieval. Notably, the induced\ngrammar shows superior generalizability by easily generalizing to unseen\ncategories.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 04:05:08 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Hong", "Yining", ""], ["Li", "Qing", ""], ["Zhu", "Song-Chun", ""], ["Huang", "Siyuan", ""]]}, {"id": "2103.12982", "submitter": "Wen-Yun Yang", "authors": "Rui Li, Yunjiang Jiang, Wenyun Yang, Guoyu Tang, Songlin Wang, Chaoyi\n  Ma, Wei He, Xi Xiong, Yun Xiao, Eric Yihong Zhao", "title": "From Semantic Retrieval to Pairwise Ranking: Applying Deep Learning in\n  E-commerce Search", "comments": "Accepted in SIGIR 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce deep learning models to the two most important stages in product\nsearch at JD.com, one of the largest e-commerce platforms in the world.\nSpecifically, we outline the design of a deep learning system that retrieves\nsemantically relevant items to a query within milliseconds, and a pairwise deep\nre-ranking system, which learns subtle user preferences. Compared to\ntraditional search systems, the proposed approaches are better at semantic\nretrieval and personalized ranking, achieving significant improvements.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 04:37:32 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Li", "Rui", ""], ["Jiang", "Yunjiang", ""], ["Yang", "Wenyun", ""], ["Tang", "Guoyu", ""], ["Wang", "Songlin", ""], ["Ma", "Chaoyi", ""], ["He", "Wei", ""], ["Xiong", "Xi", ""], ["Xiao", "Yun", ""], ["Zhao", "Eric Yihong", ""]]}, {"id": "2103.12983", "submitter": "Tri Minh Nguyen", "authors": "Tri Minh Nguyen, Thomas P Quinn, Thin Nguyen, Truyen Tran", "title": "Counterfactual Explanation with Multi-Agent Reinforcement Learning for\n  Drug Target Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Motivation: Many high-performance DTA models have been proposed, but they are\nmostly black-box and thus lack human interpretability. Explainable AI (XAI) can\nmake DTA models more trustworthy, and can also enable scientists to distill\nbiological knowledge from the models. Counterfactual explanation is one popular\napproach to explaining the behaviour of a deep neural network, which works by\nsystematically answering the question \"How would the model output change if the\ninputs were changed in this way?\". Most counterfactual explanation methods only\noperate on single input data. It remains an open problem how to extend\ncounterfactual-based XAI methods to DTA models, which have two inputs, one for\ndrug and one for target, that also happen to be discrete in nature.\n  Methods: We propose a multi-agent reinforcement learning framework,\nMulti-Agent Counterfactual Drug target binding Affinity (MACDA), to generate\ncounterfactual explanations for the drug-protein complex. Our proposed\nframework provides human-interpretable counterfactual instances while\noptimizing both the input drug and target for counterfactual generation at the\nsame time.\n  Results: We benchmark the proposed MACDA framework using the Davis dataset\nand find that our framework produces more parsimonious explanations with no\nloss in explanation validity, as measured by encoding similarity and QED. We\nthen present a case study involving ABL1 and Nilotinib to demonstrate how MACDA\ncan explain the behaviour of a DTA model in the underlying substructure\ninteraction between inputs in its prediction, revealing mechanisms that align\nwith prior domain knowledge.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 04:38:38 GMT"}, {"version": "v2", "created": "Wed, 2 Jun 2021 01:28:49 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Nguyen", "Tri Minh", ""], ["Quinn", "Thomas P", ""], ["Nguyen", "Thin", ""], ["Tran", "Truyen", ""]]}, {"id": "2103.12998", "submitter": "Markus Lange-Hegermann", "authors": "Tom Hammerbacher, Markus Lange-Hegermann, Gorden Platz", "title": "Including Sparse Production Knowledge into Variational Autoencoders to\n  Increase Anomaly Detection Reliability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Digitalization leads to data transparency for production systems that we can\nbenefit from with data-driven analysis methods like neural networks. For\nexample, automated anomaly detection enables saving resources and optimizing\nthe production. We study using rarely occurring information about labeled\nanomalies into Variational Autoencoder neural network structures to overcome\ninformation deficits of supervised and unsupervised approaches. This method\noutperforms all other models in terms of accuracy, precision, and recall. We\nevaluate the following methods: Principal Component Analysis, Isolation Forest,\nClassifying Neural Networks, and Variational Autoencoders on seven time series\ndatasets to find the best performing detection methods. We extend this idea to\ninclude more infrequently occurring meta information about production\nprocesses. This use of sparse labels, both of anomalies or production data,\nallows to harness any additional information available for increasing anomaly\ndetection performance.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 05:54:12 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 11:26:45 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Hammerbacher", "Tom", ""], ["Lange-Hegermann", "Markus", ""], ["Platz", "Gorden", ""]]}, {"id": "2103.12999", "submitter": "Allan Zhou", "authors": "Behzad Haghgoo, Allan Zhou, Archit Sharma, Chelsea Finn", "title": "Discriminator Augmented Model-Based Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  By planning through a learned dynamics model, model-based reinforcement\nlearning (MBRL) offers the prospect of good performance with little environment\ninteraction. However, it is common in practice for the learned model to be\ninaccurate, impairing planning and leading to poor performance. This paper aims\nto improve planning with an importance sampling framework that accounts and\ncorrects for discrepancy between the true and learned dynamics. This framework\nalso motivates an alternative objective for fitting the dynamics model: to\nminimize the variance of value estimation during planning. We derive and\nimplement this objective, which encourages better prediction on trajectories\nwith larger returns. We observe empirically that our approach improves the\nperformance of current MBRL algorithms on two stochastic control problems, and\nprovide a theoretical basis for our method.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 06:01:55 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 06:49:28 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Haghgoo", "Behzad", ""], ["Zhou", "Allan", ""], ["Sharma", "Archit", ""], ["Finn", "Chelsea", ""]]}, {"id": "2103.13020", "submitter": "Yue Yu", "authors": "Chen Zeng, Yue Yu, Shanshan Li, Xin Xia, Zhiming Wang, Mingyang Geng,\n  Bailin Xiao, Wei Dong, Xiangke Liao", "title": "deGraphCS: Embedding Variable-based Flow Graph for Neural Code Search", "comments": "32 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the rapid increase in the amount of public code repositories, developers\nmaintain a great desire to retrieve precise code snippets by using natural\nlanguage. Despite existing deep learning based approaches(e.g., DeepCS and\nMMAN) have provided the end-to-end solutions (i.e., accepts natural language as\nqueries and shows related code fragments retrieved directly from code corpus),\nthe accuracy of code search in the large-scale repositories is still limited by\nthe code representation (e.g., AST) and modeling (e.g., directly fusing the\nfeatures in the attention stage). In this paper, we propose a novel learnable\ndeep Graph for Code Search (calleddeGraphCS), to transfer source code into\nvariable-based flow graphs based on the intermediate representation technique,\nwhich can model code semantics more precisely compared to process the code as\ntext directly or use the syntactic tree representation. Furthermore, we propose\na well-designed graph optimization mechanism to refine the code representation,\nand apply an improved gated graph neural network to model variable-based flow\ngraphs. To evaluate the effectiveness of deGraphCS, we collect a large-scale\ndataset from GitHub containing 41,152 code snippets written in C language, and\nreproduce several typical deep code search methods for comparison. Besides, we\ndesign a qualitative user study to verify the practical value of our approach.\nThe experimental results have shown that deGraphCS can achieve state-of-the-art\nperformances, and accurately retrieve code snippets satisfying the needs of the\nusers.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 06:57:44 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Zeng", "Chen", ""], ["Yu", "Yue", ""], ["Li", "Shanshan", ""], ["Xia", "Xin", ""], ["Wang", "Zhiming", ""], ["Geng", "Mingyang", ""], ["Xiao", "Bailin", ""], ["Dong", "Wei", ""], ["Liao", "Xiangke", ""]]}, {"id": "2103.13026", "submitter": "Xing Xu", "authors": "Xing Xu and Rongpeng Li and Zhifeng Zhao and Honggang Zhang", "title": "The Gradient Convergence Bound of Federated Multi-Agent Reinforcement\n  Learning with Efficient Communication", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper considers a distributed version of deep reinforcement learning\n(DRL) for multi-agent decision-making process in the paradigm of federated\nlearning. Since the deep neural network models in federated learning are\ntrained locally and aggregated iteratively through a central server, frequent\ninformation exchange incurs a large amount of communication overheads. Besides,\ndue to the heterogeneity of agents, Markov state transition trajectories from\ndifferent agents are usually unsynchronized within the same time interval,\nwhich will further influence the convergence bound of the aggregated deep\nneural network models. Therefore, it is of vital importance to reasonably\nevaluate the effectiveness of different optimization methods. Accordingly, this\npaper proposes a utility function to consider the balance between reducing\ncommunication overheads and improving convergence performance. Meanwhile, this\npaper develops two new optimization methods on top of variation-aware periodic\naveraging methods: 1) the decay-based method which gradually decreases the\nweight of the model's local gradients within the progress of local updating,\nand 2) the consensus-based method which introduces the consensus algorithm into\nfederated learning for the exchange of the model's local gradients. This paper\nalso provides novel convergence guarantees for both developed methods and\ndemonstrates their effectiveness and efficiency through theoretical analysis\nand numerical simulation results.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 07:21:43 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Xu", "Xing", ""], ["Li", "Rongpeng", ""], ["Zhao", "Zhifeng", ""], ["Zhang", "Honggang", ""]]}, {"id": "2103.13027", "submitter": "Siyuan Li", "authors": "Zicheng Liu, Siyuan Li, Di Wu, Zhiyuan Chen, Lirong Wu, Jianzhu Guo,\n  Stan Z. Li", "title": "AutoMix: Unveiling the Power of Mixup", "comments": "The first version of AutoMix. 13 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Mixup-based data augmentation has achieved great success as regularizer for\ndeep neural networks. However, existing mixup methods require explicitly\ndesigned mixup policies. In this paper, we present a flexible, general\nAutomatic Mixup (AutoMix) framework which utilizes discriminative features to\nlearn a sample mixing policy adaptively. We regard mixup as a pretext task and\nsplit it into two sub-problems: mixed samples generation and mixup\nclassification. To this end, we design a lightweight mix block to generate\nsynthetic samples based on feature maps and mix labels. Since the two\nsub-problems are in the nature of Expectation-Maximization (EM), we also\npropose a momentum training pipeline to optimize the mixup process and mixup\nclassification process alternatively in an end-to-end fashion. Extensive\nexperiments on six popular classification benchmarks show that AutoMix\nconsistently outperforms other leading mixup methods and improves\ngeneralization abilities to downstream tasks. We hope AutoMix will motivate the\ncommunity to rethink the role of mixup in representation learning. The code\nwill be released soon.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 07:21:53 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Liu", "Zicheng", ""], ["Li", "Siyuan", ""], ["Wu", "Di", ""], ["Chen", "Zhiyuan", ""], ["Wu", "Lirong", ""], ["Guo", "Jianzhu", ""], ["Li", "Stan Z.", ""]]}, {"id": "2103.13107", "submitter": "Francesco Ponzio", "authors": "Francesco Ponzio, Enrico Macii, Elisa Ficarra, Santa Di Cataldo", "title": "W2WNet: a two-module probabilistic Convolutional Neural Network with\n  embedded data cleansing functionality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Convolutional Neural Networks (CNNs) are supposed to be fed with only\nhigh-quality annotated datasets. Nonetheless, in many real-world scenarios,\nsuch high quality is very hard to obtain, and datasets may be affected by any\nsort of image degradation and mislabelling issues. This negatively impacts the\nperformance of standard CNNs, both during the training and the inference phase.\nTo address this issue we propose Wise2WipedNet (W2WNet), a new two-module\nConvolutional Neural Network, where a Wise module exploits Bayesian inference\nto identify and discard spurious images during the training, and a Wiped module\ntakes care of the final classification while broadcasting information on the\nprediction confidence at inference time. The goodness of our solution is\ndemonstrated on a number of public benchmarks addressing different image\nclassification tasks, as well as on a real-world case study on histological\nimage analysis. Overall, our experiments demonstrate that W2WNet is able to\nidentify image degradation and mislabelling issues both at training and at\ninference time, with a positive impact on the final classification accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 11:28:59 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Ponzio", "Francesco", ""], ["Macii", "Enrico", ""], ["Ficarra", "Elisa", ""], ["Di Cataldo", "Santa", ""]]}, {"id": "2103.13128", "submitter": "Martin Molina", "authors": "Martin Molina, Pablo Santamaria", "title": "Behavior coordination for self-adaptive robots using constraint-based\n  configuration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Autonomous robots may be able to adapt their behavior in response to changes\nin the environment. This is useful, for example, to efficiently handle limited\nresources or to respond appropriately to unexpected events such as faults. The\narchitecture of a self-adaptive robot is complex because it should include\nautomatic mechanisms to dynamically configure the elements that control robot\nbehaviors. To facilitate the construction of this type of architectures, it is\nuseful to have general solutions in the form of software tools that may be\napplicable to different robotic systems. This paper presents an original\nalgorithm to dynamically configure the control architecture, which is\napplicable to the development of self-adaptive autonomous robots. This\nalgorithm uses a constraint-based configuration approach to decide which basic\nrobot behaviors should be activated in response to both reactive and\ndeliberative events. The algorithm uses specific search heuristics and\ninitialization procedures to achieve the performance required by robotic\nsystems. The solution has been implemented as a software development tool\ncalled Behavior Coordinator CBC (Constraint-Based Configuration), which is\nbased on ROS and open source, available to the general public. This tool has\nbeen successfully used for building multiple applications of autonomous aerial\nrobots.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 12:09:44 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Molina", "Martin", ""], ["Santamaria", "Pablo", ""]]}, {"id": "2103.13136", "submitter": "Avijit Thawani", "authors": "Avijit Thawani, Jay Pujara, Pedro A. Szekely, Filip Ilievski", "title": "Representing Numbers in NLP: a Survey and a Vision", "comments": "Accepted at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  NLP systems rarely give special consideration to numbers found in text. This\nstarkly contrasts with the consensus in neuroscience that, in the brain,\nnumbers are represented differently from words. We arrange recent NLP work on\nnumeracy into a comprehensive taxonomy of tasks and methods. We break down the\nsubjective notion of numeracy into 7 subtasks, arranged along two dimensions:\ngranularity (exact vs approximate) and units (abstract vs grounded). We analyze\nthe myriad representational choices made by 18 previously published number\nencoders and decoders. We synthesize best practices for representing numbers in\ntext and articulate a vision for holistic numeracy in NLP, comprised of design\ntrade-offs and a unified evaluation.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 12:28:22 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Thawani", "Avijit", ""], ["Pujara", "Jay", ""], ["Szekely", "Pedro A.", ""], ["Ilievski", "Filip", ""]]}, {"id": "2103.13137", "submitter": "Chuming Lin", "authors": "Chuming Lin, Chengming Xu, Donghao Luo, Yabiao Wang, Ying Tai,\n  Chengjie Wang, Jilin Li, Feiyue Huang, Yanwei Fu", "title": "Learning Salient Boundary Feature for Anchor-free Temporal Action\n  Localization", "comments": "Accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Temporal action localization is an important yet challenging task in video\nunderstanding. Typically, such a task aims at inferring both the action\ncategory and localization of the start and end frame for each action instance\nin a long, untrimmed video.While most current models achieve good results by\nusing pre-defined anchors and numerous actionness, such methods could be\nbothered with both large number of outputs and heavy tuning of locations and\nsizes corresponding to different anchors. Instead, anchor-free methods is\nlighter, getting rid of redundant hyper-parameters, but gains few attention. In\nthis paper, we propose the first purely anchor-free temporal localization\nmethod, which is both efficient and effective. Our model includes (i) an\nend-to-end trainable basic predictor, (ii) a saliency-based refinement module\nto gather more valuable boundary features for each proposal with a novel\nboundary pooling, and (iii) several consistency constraints to make sure our\nmodel can find the accurate boundary given arbitrary proposals. Extensive\nexperiments show that our method beats all anchor-based and actionness-guided\nmethods with a remarkable margin on THUMOS14, achieving state-of-the-art\nresults, and comparable ones on ActivityNet v1.3. Code is available at\nhttps://github.com/TencentYoutuResearch/ActionDetection-AFSD.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 12:28:32 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Lin", "Chuming", ""], ["Xu", "Chengming", ""], ["Luo", "Donghao", ""], ["Wang", "Yabiao", ""], ["Tai", "Ying", ""], ["Wang", "Chengjie", ""], ["Li", "Jilin", ""], ["Huang", "Feiyue", ""], ["Fu", "Yanwei", ""]]}, {"id": "2103.13192", "submitter": "Tanya Ignatenko", "authors": "Tanya Ignatenko, Kirill Kondrashov, Marco Cox, Bert de Vries", "title": "On Sequential Bayesian Optimization with Pairwise Comparison", "comments": "13 pages, 5 figures (15 with subfigures), submitted for EEE\n  Transactions on Pattern Analysis and Machine Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the problem of user preference learning on the example\nof parameter setting for a hearing aid (HA). We propose to use an agent that\ninteracts with a HA user, in order to collect the most informative data, and\nlearns user preferences for HA parameter settings, based on these data. We\nmodel the HA system as two interacting sub-systems, one representing a user\nwith his/her preferences and another one representing an agent. In this system,\nthe user responses to HA settings, proposed by the agent. In our user model,\nthe responses are driven by a parametric user preference function. The agent\ncomprises the sequential mechanisms for user model inference and HA parameter\nproposal generation. To infer the user model (preference function), Bayesian\napproximate inference is used in the agent. Here we propose the normalized\nweighted Kullback-Leibler (KL) divergence between true and agent-assigned\npredictive user response distributions as a metric to assess the quality of\nlearned preferences. Moreover, our agent strategy for generating HA parameter\nproposals is to generate HA settings, responses to which help resolving\nuncertainty associated with prediction of the user responses the most. The\nresulting data, consequently, allows for efficient user model learning. The\nnormalized weighted KL-divergence plays an important role here as well, since\nit characterizes the informativeness of the data to be used for probing the\nuser. The efficiency of our approach is validated by numerical simulations.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 13:46:27 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Ignatenko", "Tanya", ""], ["Kondrashov", "Kirill", ""], ["Cox", "Marco", ""], ["de Vries", "Bert", ""]]}, {"id": "2103.13268", "submitter": "Michele Colledanchise", "authors": "Michele Colledanchise", "title": "A New Paradigm of Threats in Robotics Behaviors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robots applications in our daily life increase at an unprecedented pace. As\nrobots will soon operate \"out in the wild\", we must identify the safety and\nsecurity vulnerabilities they will face. Robotics researchers and manufacturers\nfocus their attention on new, cheaper, and more reliable applications. Still,\nthey often disregard the operability in adversarial environments where a\ntrusted or untrusted user can jeopardize or even alter the robot's task.\n  In this paper, we identify a new paradigm of security threats in the next\ngeneration of robots. These threats fall beyond the known hardware or\nnetwork-based ones, and we must find new solutions to address them. These new\nthreats include malicious use of the robot's privileged access, tampering with\nthe robot sensors system, and tricking the robot's deliberation into harmful\nbehaviors. We provide a taxonomy of attacks that exploit these vulnerabilities\nwith realistic examples, and we outline effective countermeasures to prevent\nbetter, detect, and mitigate them.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 15:33:49 GMT"}], "update_date": "2021-03-25", "authors_parsed": [["Colledanchise", "Michele", ""]]}, {"id": "2103.13332", "submitter": "Johannes Stern Dr.", "authors": "Johannes Stern", "title": "Truth and Subjunctive Theories of Knowledge: No Luck?", "comments": "The notion of stabilizing ordinal is not well-defined, i.e.,\n  Definition 14 is flawed. As a consequence the results presented in the paper\n  are either incorrect or remain unproved", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The paper explores applications of Kripke's theory of truth to semantics for\nanti-luck epistemology, that is, to subjunctive theories of knowledge.\nSubjunctive theories put forward modal or subjunctive conditions to rule out\nknowledge by mere luck as to be found in Gettier-style counterexamples to the\nanalysis of knowledge as justified true belief. Because of the subjunctive\nnature of these conditions the resulting semantics turns out to be\nnon-monotone, even if it is based on non-classical evaluation schemes such as\nstrong Kleene or FDE. This blocks the usual road to fixed-point results for\nKripke's theory of truth within these semantics and consequently the paper is\npredominantly an exploration of fixed point results for Kripke's theory of\ntruth within non-monotone semantics. Using the theory of quasi-inductive\ndefinitions we show that in case of the subjunctive theories of knowledge the\nso-called Kripke jump will have fixed points despite the non-monotonicity of\nthe semantics: Kripke's theory of truth can be successfully applied in the\nframework of subjunctive theories of knowledge.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 16:40:42 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 16:05:20 GMT"}, {"version": "v3", "created": "Thu, 1 Apr 2021 16:29:24 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Stern", "Johannes", ""]]}, {"id": "2103.13355", "submitter": "Yangkun Wang", "authors": "Yangkun Wang, Jiarui Jin, Weinan Zhang, Yong Yu, Zheng Zhang, David\n  Wipf", "title": "Bag of Tricks for Node Classification with Graph Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Over the past few years, graph neural networks (GNN) and label\npropagation-based methods have made significant progress in addressing node\nclassification tasks on graphs. However, in addition to their reliance on\nelaborate architectures and algorithms, there are several key technical details\nthat are frequently overlooked, and yet nonetheless can play a vital role in\nachieving satisfactory performance. In this paper, we first summarize a series\nof existing tricks-of-the-trade, and then propose several new ones related to\nlabel usage, loss function formulation, and model design that can significantly\nimprove various GNN architectures. We empirically evaluate their impact on\nfinal node classification accuracy by conducting ablation studies and\ndemonstrate consistently-improved performance, often to an extent that\noutweighs the gains from more dramatic changes in the underlying GNN\narchitecture. Notably, many of the top-ranked models on the Open Graph\nBenchmark (OGB) leaderboard and KDDCUP 2021 Large-Scale Challenge MAG240M-LSC\nbenefit from these techniques.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 17:24:26 GMT"}, {"version": "v2", "created": "Thu, 3 Jun 2021 18:37:04 GMT"}, {"version": "v3", "created": "Mon, 5 Jul 2021 12:02:57 GMT"}], "update_date": "2021-07-06", "authors_parsed": [["Wang", "Yangkun", ""], ["Jin", "Jiarui", ""], ["Zhang", "Weinan", ""], ["Yu", "Yong", ""], ["Zhang", "Zheng", ""], ["Wipf", "David", ""]]}, {"id": "2103.13420", "submitter": "Jingxi Xu", "authors": "Jingxi Xu, Da Tang, Tony Jebara", "title": "Active Multitask Learning with Committees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cost of annotating training data has traditionally been a bottleneck for\nsupervised learning approaches. The problem is further exacerbated when\nsupervised learning is applied to a number of correlated tasks simultaneously\nsince the amount of labels required scales with the number of tasks. To\nmitigate this concern, we propose an active multitask learning algorithm that\nachieves knowledge transfer between tasks. The approach forms a so-called\ncommittee for each task that jointly makes decisions and directly shares data\nacross similar tasks. Our approach reduces the number of queries needed during\ntraining while maintaining high accuracy on test data. Empirical results on\nbenchmark datasets show significant improvements on both accuracy and number of\nquery requests.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 18:07:23 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Xu", "Jingxi", ""], ["Tang", "Da", ""], ["Jebara", "Tony", ""]]}, {"id": "2103.13425", "submitter": "Xiaohan Ding", "authors": "Xiaohan Ding, Xiangyu Zhang, Jungong Han, Guiguang Ding", "title": "Diverse Branch Block: Building a Convolution as an Inception-like Unit", "comments": "CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a universal building block of Convolutional Neural Network\n(ConvNet) to improve the performance without any inference-time costs. The\nblock is named Diverse Branch Block (DBB), which enhances the representational\ncapacity of a single convolution by combining diverse branches of different\nscales and complexities to enrich the feature space, including sequences of\nconvolutions, multi-scale convolutions, and average pooling. After training, a\nDBB can be equivalently converted into a single conv layer for deployment.\nUnlike the advancements of novel ConvNet architectures, DBB complicates the\ntraining-time microstructure while maintaining the macro architecture, so that\nit can be used as a drop-in replacement for regular conv layers of any\narchitecture. In this way, the model can be trained to reach a higher level of\nperformance and then transformed into the original inference-time structure for\ninference. DBB improves ConvNets on image classification (up to 1.9% higher\ntop-1 accuracy on ImageNet), object detection and semantic segmentation. The\nPyTorch code and models are released at\nhttps://github.com/DingXiaoH/DiverseBranchBlock.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 18:12:00 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 13:00:50 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Ding", "Xiaohan", ""], ["Zhang", "Xiangyu", ""], ["Han", "Jungong", ""], ["Ding", "Guiguang", ""]]}, {"id": "2103.13427", "submitter": "Eleonora Giunchiglia", "authors": "Eleonora Giunchiglia and Thomas Lukasiewicz", "title": "Multi-Label Classification Neural Networks with Hard Logical Constraints", "comments": "arXiv admin note: text overlap with arXiv:2010.10151", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-label classification (MC) is a standard machine learning problem in\nwhich a data point can be associated with a set of classes. A more challenging\nscenario is given by hierarchical multi-label classification (HMC) problems, in\nwhich every prediction must satisfy a given set of hard constraints expressing\nsubclass relationships between classes. In this paper, we propose C-HMCNN(h), a\nnovel approach for solving HMC problems, which, given a network h for the\nunderlying MC problem, exploits the hierarchy information in order to produce\npredictions coherent with the constraints and to improve performance.\nFurthermore, we extend the logic used to express HMC constraints in order to be\nable to specify more complex relations among the classes and propose a new\nmodel CCN(h), which extends C-HMCNN(h) and is again able to satisfy and exploit\nthe constraints to improve performance. We conduct an extensive experimental\nanalysis showing the superior performance of both C-HMCNN(h) and CCN(h) when\ncompared to state-of-the-art models in both the HMC and the general MC setting\nwith hard logical constraints.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 18:13:56 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Giunchiglia", "Eleonora", ""], ["Lukasiewicz", "Thomas", ""]]}, {"id": "2103.13452", "submitter": "Anh Tuan Nguyen", "authors": "Anh Tuan Nguyen, Markus W. Drealan, Diu Khue Luu, Ming Jiang, Jian Xu,\n  Jonathan Cheng, Qi Zhao, Edward W. Keefer, Zhi Yang", "title": "A Portable, Self-Contained Neuroprosthetic Hand with Deep Learning-Based\n  Finger Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Objective: Deep learning-based neural decoders have emerged as the prominent\napproach to enable dexterous and intuitive control of neuroprosthetic hands.\nYet few studies have materialized the use of deep learning in clinical settings\ndue to its high computational requirements. Methods: Recent advancements of\nedge computing devices bring the potential to alleviate this problem. Here we\npresent the implementation of a neuroprosthetic hand with embedded deep\nlearning-based control. The neural decoder is designed based on the recurrent\nneural network (RNN) architecture and deployed on the NVIDIA Jetson Nano - a\ncompacted yet powerful edge computing platform for deep learning inference.\nThis enables the implementation of the neuroprosthetic hand as a portable and\nself-contained unit with real-time control of individual finger movements.\nResults: The proposed system is evaluated on a transradial amputee using\nperipheral nerve signals (ENG) with implanted intrafascicular microelectrodes.\nThe experiment results demonstrate the system's capabilities of providing\nrobust, high-accuracy (95-99%) and low-latency (50-120 msec) control of\nindividual finger movements in various laboratory and real-world environments.\nConclusion: Modern edge computing platforms enable the effective use of deep\nlearning-based neural decoders for neuroprosthesis control as an autonomous\nsystem. Significance: This work helps pioneer the deployment of deep neural\nnetworks in clinical applications underlying a new class of wearable biomedical\ndevices with embedded artificial intelligence.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 19:11:58 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Nguyen", "Anh Tuan", ""], ["Drealan", "Markus W.", ""], ["Luu", "Diu Khue", ""], ["Jiang", "Ming", ""], ["Xu", "Jian", ""], ["Cheng", "Jonathan", ""], ["Zhao", "Qi", ""], ["Keefer", "Edward W.", ""], ["Yang", "Zhi", ""]]}, {"id": "2103.13455", "submitter": "Chandan Singh", "authors": "Chandan Singh, Guha Balakrishnan, Pietro Perona", "title": "Matched sample selection with GANs for mitigating attribute confounding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.AP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Measuring biases of vision systems with respect to protected attributes like\ngender and age is critical as these systems gain widespread use in society.\nHowever, significant correlations between attributes in benchmark datasets make\nit difficult to separate algorithmic bias from dataset bias. To mitigate such\nattribute confounding during bias analysis, we propose a matching approach that\nselects a subset of images from the full dataset with balanced attribute\ndistributions across protected attributes. Our matching approach first projects\nreal images onto a generative adversarial network (GAN)'s latent space in a\nmanner that preserves semantic attributes. It then finds image matches in this\nlatent space across a chosen protected attribute, yielding a dataset where\nsemantic and perceptual attributes are balanced across the protected attribute.\nWe validate projection and matching strategies with qualitative, quantitative,\nand human annotation experiments. We demonstrate our work in the context of\ngender bias in multiple open-source facial-recognition classifiers and find\nthat bias persists after removing key confounders via matching. Code and\ndocumentation to reproduce the results here and apply the methods to new data\nis available at https://github.com/csinva/matching-with-gans .\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 19:18:44 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Singh", "Chandan", ""], ["Balakrishnan", "Guha", ""], ["Perona", "Pietro", ""]]}, {"id": "2103.13460", "submitter": "Abhinav Grover", "authors": "Abhinav Grover, Christopher Grebe, Philippe Nadeau, Jonathan Kelly", "title": "Under Pressure: Learning to Detect Slip with Barometric Tactile Sensors", "comments": "Submitted to th RoboTac Workshop in the IEEE/RSJ International\n  Conference on Intelligent Robotics and Systems (IROS'21), Prague, Czech\n  Republic, Sept 27- Oct 1, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the utility of tactile information, tactile sensors have yet to be\nwidely deployed in industrial robotics settings -- part of the challenge lies\nin identifying slip and other key events from the tactile data stream. In this\npaper, we present a learning-based method to detect slip using barometric\ntactile sensors. Although these sensors have a low resolution, they have many\nother desirable properties including high reliability and durability, a very\nslim profile, and a low cost. We are able to achieve slip detection accuracies\nof greater than 91% while being robust to the speed and direction of the slip\nmotion. Further, we test our detector on two robot manipulation tasks involving\ncommon household objects and demonstrate successful generalization to\nreal-world scenarios not seen during training. We show that barometric tactile\nsensing technology, combined with data-driven learning, is potentially suitable\nfor complex manipulation tasks such as slip compensation.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 19:29:03 GMT"}, {"version": "v2", "created": "Fri, 23 Jul 2021 04:36:43 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Grover", "Abhinav", ""], ["Grebe", "Christopher", ""], ["Nadeau", "Philippe", ""], ["Kelly", "Jonathan", ""]]}, {"id": "2103.13496", "submitter": "Jordan Meadows", "authors": "Jordan Meadows, Andr\\'e Freitas", "title": "Similarity-Based Equational Inference in Physics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automating the derivation of published results is a challenge, in part due to\nthe informal use of mathematics by physicists, compared to that of\nmathematicians. Following demand, we describe a method for converting informal\nhand-written derivations into datasets, and present an example dataset crafted\nfrom a contemporary result in condensed matter. We define an equation\nreconstruction task completed by rederiving an unknown intermediate equation\nposed as a state, taken from three consecutive equational states within a\nderivation. Derivation automation is achieved by applying string-based\nCAS-reliant actions to states, which mimic mathematical operations and induce\nstate transitions. We implement a symbolic similarity-based heuristic search to\nsolve the equation reconstruction task as an early step towards multi-hop\nequational inference in physics.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 21:36:39 GMT"}, {"version": "v2", "created": "Sun, 27 Jun 2021 02:09:15 GMT"}], "update_date": "2021-06-29", "authors_parsed": [["Meadows", "Jordan", ""], ["Freitas", "Andr\u00e9", ""]]}, {"id": "2103.13511", "submitter": "Praveer Singh", "authors": "Sharut Gupta, Praveer Singh, Ken Chang, Liangqiong Qu, Mehak Aggarwal,\n  Nishanth Arun, Ashwin Vaswani, Shruti Raghavan, Vibha Agarwal, Mishka\n  Gidwani, Katharina Hoebel, Jay Patel, Charles Lu, Christopher P. Bridge,\n  Daniel L. Rubin, Jayashree Kalpathy-Cramer", "title": "Addressing catastrophic forgetting for medical domain expansion", "comments": "First three authors contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model brittleness is a key concern when deploying deep learning models in\nreal-world medical settings. A model that has high performance at one\ninstitution may suffer a significant decline in performance when tested at\nother institutions. While pooling datasets from multiple institutions and\nretraining may provide a straightforward solution, it is often infeasible and\nmay compromise patient privacy. An alternative approach is to fine-tune the\nmodel on subsequent institutions after training on the original institution.\nNotably, this approach degrades model performance at the original institution,\na phenomenon known as catastrophic forgetting. In this paper, we develop an\napproach to address catastrophic forget-ting based on elastic weight\nconsolidation combined with modulation of batch normalization statistics under\ntwo scenarios: first, for expanding the domain from one imaging system's data\nto another imaging system's, and second, for expanding the domain from a large\nmulti-institutional dataset to another single institution dataset. We show that\nour approach outperforms several other state-of-the-art approaches and provide\ntheoretical justification for the efficacy of batch normalization modulation.\nThe results of this study are generally applicable to the deployment of any\nclinical deep learning model which requires domain expansion.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 22:33:38 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Gupta", "Sharut", ""], ["Singh", "Praveer", ""], ["Chang", "Ken", ""], ["Qu", "Liangqiong", ""], ["Aggarwal", "Mehak", ""], ["Arun", "Nishanth", ""], ["Vaswani", "Ashwin", ""], ["Raghavan", "Shruti", ""], ["Agarwal", "Vibha", ""], ["Gidwani", "Mishka", ""], ["Hoebel", "Katharina", ""], ["Patel", "Jay", ""], ["Lu", "Charles", ""], ["Bridge", "Christopher P.", ""], ["Rubin", "Daniel L.", ""], ["Kalpathy-Cramer", "Jayashree", ""]]}, {"id": "2103.13512", "submitter": "Frank Guerin", "authors": "Frank Guerin", "title": "Projection: A Mechanism for Human-like Reasoning in Artificial\n  Intelligence", "comments": "25 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.RO", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Artificial Intelligence systems cannot yet match human abilities to apply\nknowledge to situations that vary from what they have been programmed for, or\ntrained for. In visual object recognition methods of inference exploiting\ntop-down information (from a model) have been shown to be effective for\nrecognising entities in difficult conditions. Here this type of inference,\ncalled `projection', is shown to be a key mechanism to solve the problem of\napplying knowledge to varied or challenging situations, across a range of AI\ndomains, such as vision, robotics, or language. Finally the relevance of\nprojection to tackling the commonsense knowledge problem is discussed.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 22:33:51 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Guerin", "Frank", ""]]}, {"id": "2103.13520", "submitter": "Amit Sheth", "authors": "Amit Sheth and Krishnaprasad Thirunarayan", "title": "The Duality of Data and Knowledge Across the Three Waves of AI", "comments": "A version of this will appear as (cite as): IT Professional Magazine\n  (special section to commemorate the 75th Anniversary of IEEE Computer\n  Society), 23 (3) April-May 2021", "journal-ref": "IT Professional, 23 (3), April-May 2021", "doi": "10.1109/MITP.2021.3070985", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss how over the last 30 to 50 years, Artificial Intelligence (AI)\nsystems that focused only on data have been handicapped, and how knowledge has\nbeen critical in developing smarter, intelligent, and more effective systems.\nIn fact, the vast progress in AI can be viewed in terms of the three waves of\nAI as identified by DARPA. During the first wave, handcrafted knowledge has\nbeen at the center-piece, while during the second wave, the data-driven\napproaches supplanted knowledge. Now we see a strong role and resurgence of\nknowledge fueling major breakthroughs in the third wave of AI underpinning\nfuture intelligent systems as they attempt human-like decision making, and seek\nto become trusted assistants and companions for humans. We find a wider\navailability of knowledge created from diverse sources, using manual to\nautomated means both by repurposing as well as by extraction. Using knowledge\nwith statistical learning is becoming increasingly indispensable to help make\nAI systems more transparent and auditable. We will draw a parallel with the\nrole of knowledge and experience in human intelligence based on cognitive\nscience, and discuss emerging neuro-symbolic or hybrid AI systems in which\nknowledge is the critical enabler for combining capabilities of the\ndata-intensive statistical AI systems with those of symbolic AI systems,\nresulting in more capable AI systems that support more human-like intelligence.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 23:07:47 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 19:57:57 GMT"}], "update_date": "2021-04-16", "authors_parsed": [["Sheth", "Amit", ""], ["Thirunarayan", "Krishnaprasad", ""]]}, {"id": "2103.13526", "submitter": "Francesco Osborne", "authors": "Thiviyan Thanapalasingam, Francesco Osborne, Aliaksandr Birukou and\n  Enrico Motta", "title": "Ontology-Based Recommendation of Editorial Products", "comments": "In: The Semantic Web - ISWC 2018. Lecture Notes in Computer Science,\n  vol 11137. Springer, Cham", "journal-ref": null, "doi": "10.1007/978-3-030-00668-6_21", "report-no": null, "categories": "cs.DL cs.AI cs.IR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Major academic publishers need to be able to analyse their vast catalogue of\nproducts and select the best items to be marketed in scientific venues. This is\na complex exercise that requires characterising with a high precision the\ntopics of thousands of books and matching them with the interests of the\nrelevant communities. In Springer Nature, this task has been traditionally\nhandled manually by publishing editors. However, the rapid growth in the number\nof scientific publications and the dynamic nature of the Computer Science\nlandscape has made this solution increasingly inefficient. We have addressed\nthis issue by creating Smart Book Recommender (SBR), an ontology-based\nrecommender system developed by The Open University (OU) in collaboration with\nSpringer Nature, which supports their Computer Science editorial team in\nselecting the products to market at specific venues. SBR recommends books,\njournals, and conference proceedings relevant to a conference by taking\nadvantage of a semantically enhanced representation of about 27K editorial\nproducts. This is based on the Computer Science Ontology, a very large-scale,\nautomatically generated taxonomy of research areas. SBR also allows users to\ninvestigate why a certain publication was suggested by the system. It does so\nby means of an interactive graph view that displays the topic taxonomy of the\nrecommended editorial product and compares it with the topic-centric\ncharacterization of the input conference. An evaluation carried out with seven\nSpringer Nature editors and seven OU researchers has confirmed the\neffectiveness of the solution.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 23:23:53 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Thanapalasingam", "Thiviyan", ""], ["Osborne", "Francesco", ""], ["Birukou", "Aliaksandr", ""], ["Motta", "Enrico", ""]]}, {"id": "2103.13527", "submitter": "Francesco Osborne", "authors": "Angelo A. Salatino, Francesco Osborne, Aliaksandr Birukou and Enrico\n  Motta", "title": "Improving Editorial Workflow and Metadata Quality at Springer Nature", "comments": "In: The Semantic Web - ISWC 2019. Lecture Notes in Computer Science,\n  vol 11779. Springer, Cham", "journal-ref": null, "doi": "10.1007/978-3-030-30796-7_31", "report-no": null, "categories": "cs.DL cs.AI cs.IR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Identifying the research topics that best describe the scope of a scientific\npublication is a crucial task for editors, in particular because the quality of\nthese annotations determine how effectively users are able to discover the\nright content in online libraries. For this reason, Springer Nature, the\nworld's largest academic book publisher, has traditionally entrusted this task\nto their most expert editors. These editors manually analyse all new books,\npossibly including hundreds of chapters, and produce a list of the most\nrelevant topics. Hence, this process has traditionally been very expensive,\ntime-consuming, and confined to a few senior editors. For these reasons, back\nin 2016 we developed Smart Topic Miner (STM), an ontology-driven application\nthat assists the Springer Nature editorial team in annotating the volumes of\nall books covering conference proceedings in Computer Science. Since then STM\nhas been regularly used by editors in Germany, China, Brazil, India, and Japan,\nfor a total of about 800 volumes per year. Over the past three years the\ninitial prototype has iteratively evolved in response to feedback from the\nusers and evolving requirements. In this paper we present the most recent\nversion of the tool and describe the evolution of the system over the years,\nthe key lessons learnt, and the impact on the Springer Nature workflow. In\nparticular, our solution has drastically reduced the time needed to annotate\nproceedings and significantly improved their discoverability, resulting in 9.3\nmillion additional downloads. We also present a user study involving 9 editors,\nwhich yielded excellent results in term of usability, and report an evaluation\nof the new topic classifier used by STM, which outperforms previous versions in\nrecall and F-measure.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 23:23:59 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Salatino", "Angelo A.", ""], ["Osborne", "Francesco", ""], ["Birukou", "Aliaksandr", ""], ["Motta", "Enrico", ""]]}, {"id": "2103.13533", "submitter": "Miguel Lerma", "authors": "Miguel Lerma and Mirtha Lucas", "title": "Symmetry-Preserving Paths in Integrated Gradients", "comments": "7 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide rigorous proofs that the Integrated Gradients (IG) attribution\nmethod for deep networks satisfies completeness and symmetry-preserving\nproperties. We also study the uniqueness of IG as a path method preserving\nsymmetry.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 00:09:09 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Lerma", "Miguel", ""], ["Lucas", "Mirtha", ""]]}, {"id": "2103.13544", "submitter": "Zheng Tong", "authors": "Zheng Tong, Philippe Xu, Thierry Den{\\oe}ux", "title": "Evidential fully convolutional network for semantic segmentation", "comments": "34 pages, 21 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a hybrid architecture composed of a fully convolutional network\n(FCN) and a Dempster-Shafer layer for image semantic segmentation. In the\nso-called evidential FCN (E-FCN), an encoder-decoder architecture first\nextracts pixel-wise feature maps from an input image. A Dempster-Shafer layer\nthen computes mass functions at each pixel location based on distances to\nprototypes. Finally, a utility layer performs semantic segmentation from mass\nfunctions and allows for imprecise classification of ambiguous pixels and\noutliers. We propose an end-to-end learning strategy for jointly updating the\nnetwork parameters, which can make use of soft (imprecise) labels. Experiments\nusing three databases (Pascal VOC 2011, MIT-scene Parsing and SIFT Flow) show\nthat the proposed combination improves the accuracy and calibration of semantic\nsegmentation by assigning confusing pixels to multi-class sets.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 01:21:22 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Tong", "Zheng", ""], ["Xu", "Philippe", ""], ["Den\u0153ux", "Thierry", ""]]}, {"id": "2103.13549", "submitter": "Zheng Tong", "authors": "Zheng Tong, Philippe Xu, Thierry Den{\\oe}ux", "title": "An evidential classifier based on Dempster-Shafer theory and deep\n  learning", "comments": null, "journal-ref": "Neurocomputing, Vol. 450, pages 275-293, 2021", "doi": "10.1016/j.neucom.2021.03.066", "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new classifier based on Dempster-Shafer (DS) theory and a\nconvolutional neural network (CNN) architecture for set-valued classification.\nIn this classifier, called the evidential deep-learning classifier,\nconvolutional and pooling layers first extract high-dimensional features from\ninput data. The features are then converted into mass functions and aggregated\nby Dempster's rule in a DS layer. Finally, an expected utility layer performs\nset-valued classification based on mass functions. We propose an end-to-end\nlearning strategy for jointly updating the network parameters. Additionally, an\napproach for selecting partial multi-class acts is proposed. Experiments on\nimage recognition, signal processing, and semantic-relationship classification\ntasks demonstrate that the proposed combination of deep CNN, DS layer, and\nexpected utility layer makes it possible to improve classification accuracy and\nto make cautious decisions by assigning confusing patterns to multi-class sets.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 01:29:05 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["Tong", "Zheng", ""], ["Xu", "Philippe", ""], ["Den\u0153ux", "Thierry", ""]]}, {"id": "2103.13552", "submitter": "Shunyu Yao", "authors": "Shunyu Yao, Karthik Narasimhan, Matthew Hausknecht", "title": "Reading and Acting while Blindfolded: The Need for Semantics in Text\n  Game Agents", "comments": "NAACL 2021. Project page: https://blindfolded.cs.princeton.edu", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Text-based games simulate worlds and interact with players using natural\nlanguage. Recent work has used them as a testbed for autonomous\nlanguage-understanding agents, with the motivation being that understanding the\nmeanings of words or semantics is a key component of how humans understand,\nreason, and act in these worlds. However, it remains unclear to what extent\nartificial agents utilize semantic understanding of the text. To this end, we\nperform experiments to systematically reduce the amount of semantic information\navailable to a learning agent. Surprisingly, we find that an agent is capable\nof achieving high scores even in the complete absence of language semantics,\nindicating that the currently popular experimental setup and models may be\npoorly designed to understand and leverage game texts. To remedy this\ndeficiency, we propose an inverse dynamics decoder to regularize the\nrepresentation space and encourage exploration, which shows improved\nperformance on several games including Zork I. We discuss the implications of\nour findings for designing future agents with stronger semantic understanding.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 01:35:27 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 18:41:30 GMT"}], "update_date": "2021-05-03", "authors_parsed": [["Yao", "Shunyu", ""], ["Narasimhan", "Karthik", ""], ["Hausknecht", "Matthew", ""]]}, {"id": "2103.13558", "submitter": "Vinay Verma Kumar", "authors": "Vinay Kumar Verma, Kevin J Liang, Nikhil Mehta, Piyush Rai, Lawrence\n  Carin", "title": "Efficient Feature Transformations for Discriminative and Generative\n  Continual Learning", "comments": "Accepted in CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  As neural networks are increasingly being applied to real-world applications,\nmechanisms to address distributional shift and sequential task learning without\nforgetting are critical. Methods incorporating network expansion have shown\npromise by naturally adding model capacity for learning new tasks while\nsimultaneously avoiding catastrophic forgetting. However, the growth in the\nnumber of additional parameters of many of these types of methods can be\ncomputationally expensive at larger scales, at times prohibitively so. Instead,\nwe propose a simple task-specific feature map transformation strategy for\ncontinual learning, which we call Efficient Feature Transformations (EFTs).\nThese EFTs provide powerful flexibility for learning new tasks, achieved with\nminimal parameters added to the base architecture. We further propose a feature\ndistance maximization strategy, which significantly improves task prediction in\nclass incremental settings, without needing expensive generative models. We\ndemonstrate the efficacy and efficiency of our method with an extensive set of\nexperiments in discriminative (CIFAR-100 and ImageNet-1K) and generative (LSUN,\nCUB-200, Cats) sequences of tasks. Even with low single-digit parameter growth\nrates, EFTs can outperform many other continual learning methods in a wide\nrange of settings.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 01:48:14 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Verma", "Vinay Kumar", ""], ["Liang", "Kevin J", ""], ["Mehta", "Nikhil", ""], ["Rai", "Piyush", ""], ["Carin", "Lawrence", ""]]}, {"id": "2103.13565", "submitter": "Haobing Liu", "authors": "Haobing Liu, Yanmin Zhu, Tianzi Zang, Yanan Xu, Jiadi Yu, Feilong Tang", "title": "Jointly Modeling Heterogeneous Student Behaviors and Interactions Among\n  Multiple Prediction Tasks", "comments": null, "journal-ref": "ACM TKDD2021", "doi": "10.1145/3458023", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prediction tasks about students have practical significance for both student\nand college. Making multiple predictions about students is an important part of\na smart campus. For instance, predicting whether a student will fail to\ngraduate can alert the student affairs office to take predictive measures to\nhelp the student improve his/her academic performance. With the development of\ninformation technology in colleges, we can collect digital footprints which\nencode heterogeneous behaviors continuously. In this paper, we focus on\nmodeling heterogeneous behaviors and making multiple predictions together,\nsince some prediction tasks are related and learning the model for a specific\ntask may have the data sparsity problem. To this end, we propose a variant of\nLSTM and a soft-attention mechanism. The proposed LSTM is able to learn the\nstudent profile-aware representation from heterogeneous behavior sequences. The\nproposed soft-attention mechanism can dynamically learn different importance\ndegrees of different days for every student. In this way, heterogeneous\nbehaviors can be well modeled. In order to model interactions among multiple\nprediction tasks, we propose a co-attention mechanism based unit. With the help\nof the stacked units, we can explicitly control the knowledge transfer among\nmultiple tasks. We design three motivating behavior prediction tasks based on a\nreal-world dataset collected from a college. Qualitative and quantitative\nexperiments on the three prediction tasks have demonstrated the effectiveness\nof our model.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 02:01:58 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Liu", "Haobing", ""], ["Zhu", "Yanmin", ""], ["Zang", "Tianzi", ""], ["Xu", "Yanan", ""], ["Yu", "Jiadi", ""], ["Tang", "Feilong", ""]]}, {"id": "2103.13581", "submitter": "Rui Wang", "authors": "Rui Wang, Zhihua Wei, Haoran Duan, Shouling Ji, and Zhen Hong", "title": "EfficientTDNN: Efficient Architecture Search for Speaker Recognition in\n  the Wild", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Speaker recognition refers to audio biometrics that utilizes acoustic\ncharacteristics. These systems have emerged as an essential means of\nauthenticating identity in various areas such as smart homes, general business\ninteractions, e-commerce applications, and forensics. The mismatch between\ndevelopment and real-world data causes a shift of speaker embedding space and\nseverely degrades the performance of speaker recognition. Extensive efforts\nhave been devoted to address speaker recognition in the wild, but these often\nneglect computation and storage requirements. In this work, we propose an\nefficient time-delay neural network (EfficientTDNN) based on neural\narchitecture search to improve inference efficiency while maintaining\nrecognition accuracy. The proposed EfficientTDNN contains three phases:\nsupernet design, progressive training, and architecture search. Firstly, we\nborrow the design of TDNN to construct a supernet that enables sampling subnets\nwith different depth, kernel, and width. Secondly, the supernet is\nprogressively trained with multi-condition data augmentation to mitigate\ninterference between subnets and overcome the challenge of optimizing a huge\nsearch space. Thirdly, an accuracy predictor and efficiency estimator are\nproposed to use in the architecture search to derive the specialized subnet\nunder the given efficiency constraints. Experimental results on the VoxCeleb\ndataset show EfficientTDNN achieves 1.55% equal error rate (EER) and 0.138\ndetection cost function (DCF$_{0.01}$) with 565M multiply-accumulate operations\n(MACs) as well as 0.96% EER and 0.108 DCF$_{0.01}$ with 1.46G MACs.\nComprehensive investigations suggest that the trained supernet generalizes\nsubnets not sampled during training and obtains a favorable trade-off between\naccuracy and efficiency.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 03:28:07 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 01:07:33 GMT"}, {"version": "v3", "created": "Tue, 20 Apr 2021 07:55:01 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Wang", "Rui", ""], ["Wei", "Zhihua", ""], ["Duan", "Haoran", ""], ["Ji", "Shouling", ""], ["Hong", "Zhen", ""]]}, {"id": "2103.13582", "submitter": "Chengming Xu", "authors": "Chengming Xu, Chen Liu, Li Zhang, Chengjie Wang, Jilin Li, Feiyue\n  Huang, Xiangyang Xue, Yanwei Fu", "title": "Learning Dynamic Alignment via Meta-filter for Few-shot Learning", "comments": "accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Few-shot learning (FSL), which aims to recognise new classes by adapting the\nlearned knowledge with extremely limited few-shot (support) examples, remains\nan important open problem in computer vision. Most of the existing methods for\nfeature alignment in few-shot learning only consider image-level or\nspatial-level alignment while omitting the channel disparity. Our insight is\nthat these methods would lead to poor adaptation with redundant matching, and\nleveraging channel-wise adjustment is the key to well adapting the learned\nknowledge to new classes. Therefore, in this paper, we propose to learn a\ndynamic alignment, which can effectively highlight both query regions and\nchannels according to different local support information. Specifically, this\nis achieved by first dynamically sampling the neighbourhood of the feature\nposition conditioned on the input few shot, based on which we further predict a\nboth position-dependent and channel-dependent Dynamic Meta-filter. The filter\nis used to align the query feature with position-specific and channel-specific\nknowledge. Moreover, we adopt Neural Ordinary Differential Equation (ODE) to\nenable a more accurate control of the alignment. In such a sense our model is\nable to better capture fine-grained semantic context of the few-shot example\nand thus facilitates dynamical knowledge adaptation for few-shot learning. The\nresulting framework establishes the new state-of-the-arts on major few-shot\nvisual recognition benchmarks, including miniImageNet and tieredImageNet.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 03:29:33 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Xu", "Chengming", ""], ["Liu", "Chen", ""], ["Zhang", "Li", ""], ["Wang", "Chengjie", ""], ["Li", "Jilin", ""], ["Huang", "Feiyue", ""], ["Xue", "Xiangyang", ""], ["Fu", "Yanwei", ""]]}, {"id": "2103.13606", "submitter": "Pedram Hosseini", "authors": "Pedram Hosseini, David A. Broniatowski, Mona Diab", "title": "Predicting Directionality in Causal Relations in Text", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we test the performance of two bidirectional transformer-based\nlanguage models, BERT and SpanBERT, on predicting directionality in causal\npairs in the textual content. Our preliminary results show that predicting\ndirection for inter-sentence and implicit causal relations is more challenging.\nAnd, SpanBERT performs better than BERT on causal samples with longer span\nlength. We also introduce CREST which is a framework for unifying a collection\nof scattered datasets of causal relations.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 04:49:01 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Hosseini", "Pedram", ""], ["Broniatowski", "David A.", ""], ["Diab", "Mona", ""]]}, {"id": "2103.13607", "submitter": "Gautam Gare", "authors": "Gautam Rajendrakumar Gare and John Michael Galeotti", "title": "Exploiting Class Similarity for Machine Learning with Confidence Labels\n  and Projective Loss Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Class labels used for machine learning are relatable to each other, with\ncertain class labels being more similar to each other than others (e.g. images\nof cats and dogs are more similar to each other than those of cats and cars).\nSuch similarity among classes is often the cause of poor model performance due\nto the models confusing between them. Current labeling techniques fail to\nexplicitly capture such similarity information. In this paper, we instead\nexploit the similarity between classes by capturing the similarity information\nwith our novel confidence labels. Confidence labels are probabilistic labels\ndenoting the likelihood of similarity, or confusability, between the classes.\nOften even after models are trained to differentiate between classes in the\nfeature space, the similar classes' latent space still remains clustered. We\nview this type of clustering as valuable information and exploit it with our\nnovel projective loss functions. Our projective loss functions are designed to\nwork with confidence labels with an ability to relax the loss penalty for\nerrors that confuse similar classes. We use our approach to train neural\nnetworks with noisy labels, as we believe noisy labels are partly a result of\nconfusability arising from class similarity. We show improved performance\ncompared to the use of standard loss functions. We conduct a detailed analysis\nusing the CIFAR-10 dataset and show our proposed methods' applicability to\nlarger datasets, such as ImageNet and Food-101N.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 04:49:44 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Gare", "Gautam Rajendrakumar", ""], ["Galeotti", "John Michael", ""]]}, {"id": "2103.13620", "submitter": "Simyung Chang", "authors": "Simyung Chang, Hyoungwoo Park, Janghoon Cho, Hyunsin Park, Sungrack\n  Yun, Kyuwoong Hwang", "title": "SubSpectral Normalization for Neural Audio Data Processing", "comments": "4 pages, ICASSP '21 accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Convolutional Neural Networks are widely used in various machine learning\ndomains. In image processing, the features can be obtained by applying 2D\nconvolution to all spatial dimensions of the input. However, in the audio case,\nfrequency domain input like Mel-Spectrogram has different and unique\ncharacteristics in the frequency dimension. Thus, there is a need for a method\nthat allows the 2D convolution layer to handle the frequency dimension\ndifferently. In this work, we introduce SubSpectral Normalization (SSN), which\nsplits the input frequency dimension into several groups (sub-bands) and\nperforms a different normalization for each group. SSN also includes an affine\ntransformation that can be applied to each group. Our method removes the\ninter-frequency deflection while the network learns a frequency-aware\ncharacteristic. In the experiments with audio data, we observed that SSN can\nefficiently improve the network's performance.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 05:55:48 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Chang", "Simyung", ""], ["Park", "Hyoungwoo", ""], ["Cho", "Janghoon", ""], ["Park", "Hyunsin", ""], ["Yun", "Sungrack", ""], ["Hwang", "Kyuwoong", ""]]}, {"id": "2103.13628", "submitter": "Peizhuo Lv", "authors": "Peizhuo Lv, Pan Li, Shengzhi Zhang, Kai Chen, Ruigang Liang, Yue Zhao,\n  Yingjiu Li", "title": "HufuNet: Embedding the Left Piece as Watermark and Keeping the Right\n  Piece for Ownership Verification in Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Due to the wide use of highly-valuable and large-scale deep neural networks\n(DNNs), it becomes crucial to protect the intellectual property of DNNs so that\nthe ownership of disputed or stolen DNNs can be verified. Most existing\nsolutions embed backdoors in DNN model training such that DNN ownership can be\nverified by triggering distinguishable model behaviors with a set of secret\ninputs. However, such solutions are vulnerable to model fine-tuning and\npruning. They also suffer from fraudulent ownership claim as attackers can\ndiscover adversarial samples and use them as secret inputs to trigger\ndistinguishable behaviors from stolen models. To address these problems, we\npropose a novel DNN watermarking solution, named HufuNet, for protecting the\nownership of DNN models. We evaluate HufuNet rigorously on four benchmark\ndatasets with five popular DNN models, including convolutional neural network\n(CNN) and recurrent neural network (RNN). The experiments demonstrate HufuNet\nis highly robust against model fine-tuning/pruning, kernels cutoff/supplement,\nfunctionality-equivalent attack, and fraudulent ownership claims, thus highly\npromising to protect large-scale DNN models in the real-world.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 06:55:22 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Lv", "Peizhuo", ""], ["Li", "Pan", ""], ["Zhang", "Shengzhi", ""], ["Chen", "Kai", ""], ["Liang", "Ruigang", ""], ["Zhao", "Yue", ""], ["Li", "Yingjiu", ""]]}, {"id": "2103.13674", "submitter": "Seung-Hun Nam", "authors": "Minseok Yoon, Seung-Hun Nam, In-Jae Yu, Wonhyuk Ahn, Myung-Joon Kwon,\n  Heung-Kyu Lee", "title": "Frame-rate Up-conversion Detection Based on Convolutional Neural Network\n  for Learning Spatiotemporal Features", "comments": "preprint; under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MM cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advance in user-friendly and powerful video editing tools, anyone\ncan easily manipulate videos without leaving prominent visual traces.\nFrame-rate up-conversion (FRUC), a representative temporal-domain operation,\nincreases the motion continuity of videos with a lower frame-rate and is used\nby malicious counterfeiters in video tampering such as generating fake\nframe-rate video without improving the quality or mixing temporally spliced\nvideos. FRUC is based on frame interpolation schemes and subtle artifacts that\nremain in interpolated frames are often difficult to distinguish. Hence,\ndetecting such forgery traces is a critical issue in video forensics. This\npaper proposes a frame-rate conversion detection network (FCDNet) that learns\nforensic features caused by FRUC in an end-to-end fashion. The proposed network\nuses a stack of consecutive frames as the input and effectively learns\ninterpolation artifacts using network blocks to learn spatiotemporal features.\nThis study is the first attempt to apply a neural network to the detection of\nFRUC. Moreover, it can cover the following three types of frame interpolation\nschemes: nearest neighbor interpolation, bilinear interpolation, and\nmotion-compensated interpolation. In contrast to existing methods that exploit\nall frames to verify integrity, the proposed approach achieves a high detection\nspeed because it observes only six frames to test its authenticity. Extensive\nexperiments were conducted with conventional forensic methods and neural\nnetworks for video forensic tasks to validate our research. The proposed\nnetwork achieved state-of-the-art performance in terms of detecting the\ninterpolated artifacts of FRUC. The experimental results also demonstrate that\nour trained model is robust for an unseen dataset, unlearned frame-rate, and\nunlearned quality factor.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 08:47:46 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Yoon", "Minseok", ""], ["Nam", "Seung-Hun", ""], ["Yu", "In-Jae", ""], ["Ahn", "Wonhyuk", ""], ["Kwon", "Myung-Joon", ""], ["Lee", "Heung-Kyu", ""]]}, {"id": "2103.13686", "submitter": "Hugo Manuel Proen\\c{c}a", "authors": "Hugo Manuel Proen\\c{c}a, Thomas B\\\"ack, Matthijs van Leeuwen", "title": "Robust subgroup discovery", "comments": "For associated code, see https://github.com/HMProenca/RuleList ;\n  submitted to Data Mining and Knowledge Discovery Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the problem of robust subgroup discovery, i.e., finding a set of\ninterpretable descriptions of subsets that 1) stand out with respect to one or\nmore target attributes, 2) are statistically robust, and 3) non-redundant. Many\nattempts have been made to mine either locally robust subgroups or to tackle\nthe pattern explosion, but we are the first to address both challenges at the\nsame time from a global perspective.\n  First, we formulate a broad model class of subgroup lists, i.e., ordered sets\nof subgroups, for univariate and multivariate targets that can consist of\nnominal or numeric variables. This novel model class allows us to formalize the\nproblem of optimal robust subgroup discovery using the Minimum Description\nLength (MDL) principle, where we resort to optimal Normalized Maximum\nLikelihood and Bayesian encodings for nominal and numeric targets,\nrespectively. Notably, we show that our problem definition is equal to mining\nthe top-1 subgroup with an information-theoretic quality measure plus a penalty\nfor complexity.\n  Second, as finding optimal subgroup lists is NP-hard, we propose RSD, a\ngreedy heuristic that finds good subgroup lists and guarantees that the most\nsignificant subgroup found according to the MDL criterion is added in each\niteration, which is shown to be equivalent to a Bayesian one-sample\nproportions, multinomial, or t-test between the subgroup and dataset marginal\ntarget distributions plus a multiple hypothesis testing penalty. We empirically\nshow on 54 datasets that RSD outperforms previous subgroup set discovery\nmethods in terms of quality and subgroup list size.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 09:04:13 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Proen\u00e7a", "Hugo Manuel", ""], ["B\u00e4ck", "Thomas", ""], ["van Leeuwen", "Matthijs", ""]]}, {"id": "2103.13694", "submitter": "Ana Ozaki", "authors": "Ana Ozaki", "title": "On the Complexity of Learning Description Logic Ontologies", "comments": "Presented at the Reasoning Web Summer School 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC cs.LG cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Ontologies are a popular way of representing domain knowledge, in particular,\nknowledge in domains related to life sciences. (Semi-)automating the process of\nbuilding an ontology has attracted researchers from different communities into\na field called \"Ontology Learning\". We provide a formal specification of the\nexact and the probably approximately correct learning models from computational\nlearning theory. Then, we recall from the literature complexity results for\nlearning lightweight description logic (DL) ontologies in these models.\nFinally, we highlight other approaches proposed in the literature for learning\nDL ontologies.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 09:18:12 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Ozaki", "Ana", ""]]}, {"id": "2103.13733", "submitter": "Zhi Yuan Wu", "authors": "Zhiyuan Wu, Yu Jiang, Chupeng Cui, Zongmin Yang, Xinhui Xue, Hong Qi", "title": "Spirit Distillation: Precise Real-time Semantic Segmentation of Road\n  Scenes with Insufficient Data", "comments": "12 pages, 10 figures, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Semantic segmentation of road scenes is one of the key technologies for\nrealizing autonomous driving scene perception, and the effectiveness of deep\nConvolutional Neural Networks(CNNs) for this task has been demonstrated.\nState-of-art CNNs for semantic segmentation suffer from excessive computations\nas well as large-scale training data requirement. Inspired by the ideas of\nFine-tuning-based Transfer Learning (FTT) and feature-based knowledge\ndistillation, we propose a new knowledge distillation method for cross-domain\nknowledge transference and efficient data-insufficient network training, named\nSpirit Distillation(SD), which allow the student network to mimic the teacher\nnetwork to extract general features, so that a compact and accurate student\nnetwork can be trained for real-time semantic segmentation of road scenes.\nThen, in order to further alleviate the trouble of insufficient data and\nimprove the robustness of the student, an Enhanced Spirit Distillation (ESD)\nmethod is proposed, which commits to exploit a more comprehensive general\nfeatures extraction capability by considering images from both the target and\nthe proximity domains as input. To our knowledge, this paper is a pioneering\nwork on the application of knowledge distillation to few-shot learning.\nPersuasive experiments conducted on Cityscapes semantic segmentation with the\nprior knowledge transferred from COCO2017 and KITTI demonstrate that our\nmethods can train a better student network (mIOU and high-precision accuracy\nboost by 1.4% and 8.2% respectively, with 78.2% segmentation variance) with\nonly 41.8% FLOPs (see Fig. 1).\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 10:23:30 GMT"}, {"version": "v2", "created": "Sat, 17 Apr 2021 00:40:53 GMT"}], "update_date": "2021-04-20", "authors_parsed": [["Wu", "Zhiyuan", ""], ["Jiang", "Yu", ""], ["Cui", "Chupeng", ""], ["Yang", "Zongmin", ""], ["Xue", "Xinhui", ""], ["Qi", "Hong", ""]]}, {"id": "2103.13740", "submitter": "Thorir Mar Ingolfsson", "authors": "Thorir Mar Ingolfsson, Xiaying Wang, Michael Hersche, Alessio\n  Burrello, Lukas Cavigelli, Luca Benini", "title": "ECG-TCN: Wearable Cardiac Arrhythmia Detection with a Temporal\n  Convolutional Network", "comments": "4 pages, 1 figure, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Personalized ubiquitous healthcare solutions require energy-efficient\nwearable platforms that provide an accurate classification of bio-signals while\nconsuming low average power for long-term battery-operated use. Single lead\nelectrocardiogram (ECG) signals provide the ability to detect, classify, and\neven predict cardiac arrhythmia. In this paper, we propose a novel temporal\nconvolutional network (TCN) that achieves high accuracy while still being\nfeasible for wearable platform use. Experimental results on the ECG5000 dataset\nshow that the TCN has a similar accuracy (94.2%) score as the state-of-the-art\n(SoA) network while achieving an improvement of 16.5% in the balanced accuracy\nscore. This accurate classification is done with 27 times fewer parameters and\n37 times less multiply-accumulate operations. We test our implementation on two\npublicly available platforms, the STM32L475, which is based on ARM Cortex M4F,\nand the GreenWaves Technologies GAP8 on the GAPuino board, based on 1+8 RISC-V\nCV32E40P cores. Measurements show that the GAP8 implementation respects the\nreal-time constraints while consuming 0.10 mJ per inference. With 9.91\nGMAC/s/W, it is 23.0 times more energy-efficient and 46.85 times faster than an\nimplementation on the ARM Cortex M4F (0.43 GMAC/s/W). Overall, we obtain 8.1%\nhigher accuracy while consuming 19.6 times less energy and being 35.1 times\nfaster compared to a previous SoA embedded implementation.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 10:39:54 GMT"}, {"version": "v2", "created": "Mon, 14 Jun 2021 09:05:28 GMT"}], "update_date": "2021-06-15", "authors_parsed": [["Ingolfsson", "Thorir Mar", ""], ["Wang", "Xiaying", ""], ["Hersche", "Michael", ""], ["Burrello", "Alessio", ""], ["Cavigelli", "Lukas", ""], ["Benini", "Luca", ""]]}, {"id": "2103.13807", "submitter": "Aurore Guillevic", "authors": "Myriam Guillevic (EMPA), Aurore Guillevic (CARAMBA), Martin Vollmer\n  (EMPA), Paul Schlauri (EMPA), Matthias Hill (EMPA), Lukas Emmenegger (EMPA),\n  Stefan Reimann (EMPA)", "title": "Automated fragment identification for electron ionisation mass\n  spectrometry: application to atmospheric measurements of halocarbons", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "physics.comp-ph cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: Non-target screening consists in searching a sample for all\npresent substances, suspected or unknown, with very little prior knowledge\nabout the sample. This approach has been introduced more than a decade ago in\nthe field of water analysis, but is still very scarce for indoor and\natmospheric trace gas measurements, despite the clear need for a better\nunderstanding of the atmospheric trace gas composition. For a systematic\ndetection of emerging trace gases in the atmosphere, a new and powerful\nanalytical method is gas chromatography (GC) of preconcentrated samples,\nfollowed by electron ionisation, high resolution mass spectrometry (EI-HRMS).\nIn this work, we present data analysis tools to enable automated identification\nof unknown compounds measured by GC-EI-HRMS. Results: Based on co-eluting\nmass/charge fragments, we developed an innovative data analysis method to\nreliably reconstruct the chemical formulae of the fragments, using efficient\ncombinatorics and graph theory. The method (i) does not to require the presence\nof the molecular ion, which is absent in $\\sim$40% of EI spectra, and (ii)\npermits to use all measured data while giving more weight to mass/charge ratios\nmeasured with better precision. Our method has been trained and validated on\n>50 halocarbons and hydrocarbons with a molar masses of 30-330 g mol-1 ,\nmeasured with a mass resolution of approx. 3500. For >90% of the compounds,\nmore than 90% of the reconstructed signal is correct. Cases of wrong\nidentification can be attributed to the scarcity of detected fragments per\ncompound (less than six measured mass/charge) or the lack of isotopic constrain\n(no rare isotopocule detected). Conclusions: Our method enables to reconstruct\nmost probable chemical formulae independently from spectral databases.\nTherefore, it demonstrates the suitability of EI-HRMS data for non-target\nanalysis and paves the way for the identification of substances for which no EI\nmass spectrum is registered in databases. We illustrate the performances of our\nmethod for atmospheric trace gases and suggest that it may be well suited for\nmany other types of samples.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 09:35:10 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Guillevic", "Myriam", "", "EMPA"], ["Guillevic", "Aurore", "", "CARAMBA"], ["Vollmer", "Martin", "", "EMPA"], ["Schlauri", "Paul", "", "EMPA"], ["Hill", "Matthias", "", "EMPA"], ["Emmenegger", "Lukas", "", "EMPA"], ["Reimann", "Stefan", "", "EMPA"]]}, {"id": "2103.13810", "submitter": "Zhaolong Ling", "authors": "Zhaolong Ling, Kui Yu, Hao Wang, Lin Liu, and Jiuyong Li", "title": "Any Part of Bayesian Network Structure Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an interesting and challenging problem, learning any part of a\nBayesian network (BN) structure. In this challenge, it will be computationally\ninefficient using existing global BN structure learning algorithms to find an\nentire BN structure to achieve the part of a BN structure in which we are\ninterested. And local BN structure learning algorithms encounter the false edge\norientation problem when they are directly used to tackle this challenging\nproblem. In this paper, we first present a new concept of Expand-Backtracking\nto explain why local BN structure learning methods have the false edge\norientation problem, then propose APSL, an efficient and accurate Any Part of\nBN Structure Learning algorithm. Specifically, APSL divides the V-structures in\na Markov blanket (MB) into two types: collider V-structure and non-collider\nV-structure, then it starts from a node of interest and recursively finds both\ncollider V-structures and non-collider V-structures in the found MBs, until the\npart of a BN structure in which we are interested are oriented. To improve the\nefficiency of APSL, we further design the APSL-FS algorithm using Feature\nSelection, APSL-FS. Using six benchmark BNs, the extensive experiments have\nvalidated the efficiency and accuracy of our methods.\n", "versions": [{"version": "v1", "created": "Tue, 23 Mar 2021 10:03:31 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Ling", "Zhaolong", ""], ["Yu", "Kui", ""], ["Wang", "Hao", ""], ["Liu", "Lin", ""], ["Li", "Jiuyong", ""]]}, {"id": "2103.13823", "submitter": "Sunil Kumar Kopparapu Dr", "authors": "Ayush Tripathi and Rupayan Chakraborty and Sunil Kumar Kopparapu", "title": "A Novel Adaptive Minority Oversampling Technique for Improved\n  Classification in Data Imbalanced Scenarios", "comments": "8 pages", "journal-ref": "ICPR 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Imbalance in the proportion of training samples belonging to different\nclasses often poses performance degradation of conventional classifiers. This\nis primarily due to the tendency of the classifier to be biased towards the\nmajority classes in the imbalanced dataset. In this paper, we propose a novel\nthree step technique to address imbalanced data. As a first step we\nsignificantly oversample the minority class distribution by employing the\ntraditional Synthetic Minority OverSampling Technique (SMOTE) algorithm using\nthe neighborhood of the minority class samples and in the next step we\npartition the generated samples using a Gaussian-Mixture Model based clustering\nalgorithm. In the final step synthetic data samples are chosen based on the\nweight associated with the cluster, the weight itself being determined by the\ndistribution of the majority class samples. Extensive experiments on several\nstandard datasets from diverse domains shows the usefulness of the proposed\ntechnique in comparison with the original SMOTE and its state-of-the-art\nvariants algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 09:58:02 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 18:12:45 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Tripathi", "Ayush", ""], ["Chakraborty", "Rupayan", ""], ["Kopparapu", "Sunil Kumar", ""]]}, {"id": "2103.13834", "submitter": "Sha Luo", "authors": "Sha Luo, Hamidreza Kasaei, Lambert Schomaker", "title": "Self-Imitation Learning by Planning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Imitation learning (IL) enables robots to acquire skills quickly by\ntransferring expert knowledge, which is widely adopted in reinforcement\nlearning (RL) to initialize exploration. However, in long-horizon motion\nplanning tasks, a challenging problem in deploying IL and RL methods is how to\ngenerate and collect massive, broadly distributed data such that these methods\ncan generalize effectively. In this work, we solve this problem using our\nproposed approach called {self-imitation learning by planning (SILP)}, where\ndemonstration data are collected automatically by planning on the visited\nstates from the current policy. SILP is inspired by the observation that\nsuccessfully visited states in the early reinforcement learning stage are\ncollision-free nodes in the graph-search based motion planner, so we can plan\nand relabel robot's own trials as demonstrations for policy learning. Due to\nthese self-generated demonstrations, we relieve the human operator from the\nlaborious data preparation process required by IL and RL methods in solving\ncomplex motion planning tasks. The evaluation results show that our SILP method\nachieves higher success rates and enhances sample efficiency compared to\nselected baselines, and the policy learned in simulation performs well in a\nreal-world placement task with changing goals and obstacles.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 13:28:38 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 21:41:14 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Luo", "Sha", ""], ["Kasaei", "Hamidreza", ""], ["Schomaker", "Lambert", ""]]}, {"id": "2103.13843", "submitter": "Yang Tan", "authors": "Yang Tan, Yang Li, Shao-Lun Huang", "title": "OTCE: A Transferability Metric for Cross-Domain Cross-Task\n  Representations", "comments": "13 pages, accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Transfer learning across heterogeneous data distributions (a.k.a. domains)\nand distinct tasks is a more general and challenging problem than conventional\ntransfer learning, where either domains or tasks are assumed to be the same.\nWhile neural network based feature transfer is widely used in transfer learning\napplications, finding the optimal transfer strategy still requires\ntime-consuming experiments and domain knowledge. We propose a transferability\nmetric called Optimal Transport based Conditional Entropy (OTCE), to\nanalytically predict the transfer performance for supervised classification\ntasks in such cross-domain and cross-task feature transfer settings. Our OTCE\nscore characterizes transferability as a combination of domain difference and\ntask difference, and explicitly evaluates them from data in a unified\nframework. Specifically, we use optimal transport to estimate domain difference\nand the optimal coupling between source and target distributions, which is then\nused to derive the conditional entropy of the target task (task difference).\nExperiments on the largest cross-domain dataset DomainNet and Office31\ndemonstrate that OTCE shows an average of 21% gain in the correlation with the\nground truth transfer accuracy compared to state-of-the-art methods. We also\ninvestigate two applications of the OTCE score including source model selection\nand multi-source feature fusion.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 13:51:33 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Tan", "Yang", ""], ["Li", "Yang", ""], ["Huang", "Shao-Lun", ""]]}, {"id": "2103.13859", "submitter": "Qing-Long Zhang", "authors": "Qinglong Zhang, Lu Rao, Yubin Yang", "title": "Group-CAM: Group Score-Weighted Visual Explanations for Deep\n  Convolutional Networks", "comments": "Group-CAM is an efficient region-based saliency method, which can be\n  used as an effective data augmentation trick", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this paper, we propose an efficient saliency map generation method, called\nGroup score-weighted Class Activation Mapping (Group-CAM), which adopts the\n\"split-transform-merge\" strategy to generate saliency maps. Specifically, for\nan input image, the class activations are firstly split into groups. In each\ngroup, the sub-activations are summed and de-noised as an initial mask. After\nthat, the initial masks are transformed with meaningful perturbations and then\napplied to preserve sub-pixels of the input (i.e., masked inputs), which are\nthen fed into the network to calculate the confidence scores. Finally, the\ninitial masks are weighted summed to form the final saliency map, where the\nweights are confidence scores produced by the masked inputs. Group-CAM is\nefficient yet effective, which only requires dozens of queries to the network\nwhile producing target-related saliency maps. As a result, Group-CAM can be\nserved as an effective data augment trick for fine-tuning the networks. We\ncomprehensively evaluate the performance of Group-CAM on common-used\nbenchmarks, including deletion and insertion tests on ImageNet-1k, and pointing\ngame tests on COCO2017. Extensive experimental results demonstrate that\nGroup-CAM achieves better visual performance than the current state-of-the-art\nexplanation approaches. The code is available at\nhttps://github.com/wofmanaf/Group-CAM.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 14:16:02 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 08:56:42 GMT"}, {"version": "v3", "created": "Sun, 30 May 2021 14:51:34 GMT"}, {"version": "v4", "created": "Sat, 19 Jun 2021 09:40:17 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Zhang", "Qinglong", ""], ["Rao", "Lu", ""], ["Yang", "Yubin", ""]]}, {"id": "2103.13860", "submitter": "Domenico Maisto Dr", "authors": "Domenico Maisto, Francesco Gregoretti, Karl Friston, Giovanni Pezzulo", "title": "Active Tree Search in Large POMDPs", "comments": "35 pages, 8 figures, 1 pseudocode, preprint", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI math.PR q-bio.NC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Model-based planning and prospection are widely studied in both cognitive\nneuroscience and artificial intelligence (AI), but from different perspectives\n- and with different desiderata in mind (biological realism versus scalability)\nthat are difficult to reconcile. Here, we introduce a novel method to plan in\nlarge POMDPs - Active Tree Search - that combines the normative character and\nbiological realism of a leading planning theory in neuroscience (Active\nInference) and the scalability of Monte-Carlo methods in AI. This unification\nis beneficial for both approaches. On the one hand, using Monte-Carlo planning\npermits scaling up the biologically grounded approach of Active Inference to\nlarge-scale problems. On the other hand, the theory of Active Inference\nprovides a principled solution to the balance of exploration and exploitation,\nwhich is often addressed heuristically in Monte-Carlo methods. Our simulations\nshow that Active Tree Search successfully navigates binary trees that are\nchallenging for sampling-based methods, problems that require adaptive\nexploration, and the large POMDP problem Rocksample. Furthermore, we illustrate\nhow Active Tree Search can be used to simulate neurophysiological responses\n(e.g., in the hippocampus and prefrontal cortex) of humans and other animals\nthat contain large planning problems. These simulations show that Active Tree\nSearch is a principled realisation of neuroscientific and AI theories of\nplanning, which offers both biological realism and scalability.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 14:17:09 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Maisto", "Domenico", ""], ["Gregoretti", "Francesco", ""], ["Friston", "Karl", ""], ["Pezzulo", "Giovanni", ""]]}, {"id": "2103.13861", "submitter": "Briti Gangopadhyay", "authors": "Briti Gangopadhyay, Harshit Soora, Pallab Dasgupta", "title": "Hierarchical Program-Triggered Reinforcement Learning Agents For\n  Automated Driving", "comments": "The paper is under consideration in Transactions on Intelligent\n  Transportation Systems", "journal-ref": null, "doi": "10.1109/TITS.2021.3096998", "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in Reinforcement Learning (RL) combined with Deep Learning\n(DL) have demonstrated impressive performance in complex tasks, including\nautonomous driving. The use of RL agents in autonomous driving leads to a\nsmooth human-like driving experience, but the limited interpretability of Deep\nReinforcement Learning (DRL) creates a verification and certification\nbottleneck. Instead of relying on RL agents to learn complex tasks, we propose\nHPRL - Hierarchical Program-triggered Reinforcement Learning, which uses a\nhierarchy consisting of a structured program along with multiple RL agents,\neach trained to perform a relatively simple task. The focus of verification\nshifts to the master program under simple guarantees from the RL agents,\nleading to a significantly more interpretable and verifiable implementation as\ncompared to a complex RL agent. The evaluation of the framework is demonstrated\non different driving tasks, and NHTSA precrash scenarios using CARLA, an\nopen-source dynamic urban simulation environment.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 14:19:54 GMT"}], "update_date": "2021-07-30", "authors_parsed": [["Gangopadhyay", "Briti", ""], ["Soora", "Harshit", ""], ["Dasgupta", "Pallab", ""]]}, {"id": "2103.13868", "submitter": "Yuzhong Wang", "authors": "Yuzhong Wang, Chaojun Xiao, Shirong Ma, Haoxi Zhong, Cunchao Tu,\n  Tianyang Zhang, Zhiyuan Liu, Maosong Sun", "title": "Equality before the Law: Legal Judgment Consistency Analysis for\n  Fairness", "comments": "15 pages, 4 figures, 10 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a legal system, judgment consistency is regarded as one of the most\nimportant manifestations of fairness. However, due to the complexity of factual\nelements that impact sentencing in real-world scenarios, few works have been\ndone on quantitatively measuring judgment consistency towards real-world data.\nIn this paper, we propose an evaluation metric for judgment inconsistency,\nLegal Inconsistency Coefficient (LInCo), which aims to evaluate inconsistency\nbetween data groups divided by specific features (e.g., gender, region, race).\nWe propose to simulate judges from different groups with legal judgment\nprediction (LJP) models and measure the judicial inconsistency with the\ndisagreement of the judgment results given by LJP models trained on different\ngroups. Experimental results on the synthetic data verify the effectiveness of\nLInCo. We further employ LInCo to explore the inconsistency in real cases and\ncome to the following observations: (1) Both regional and gender inconsistency\nexist in the legal system, but gender inconsistency is much less than regional\ninconsistency; (2) The level of regional inconsistency varies little across\ndifferent time periods; (3) In general, judicial inconsistency is negatively\ncorrelated with the severity of the criminal charges. Besides, we use LInCo to\nevaluate the performance of several de-bias methods, such as adversarial\nlearning, and find that these mechanisms can effectively help LJP models to\navoid suffering from data bias.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 14:28:00 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Wang", "Yuzhong", ""], ["Xiao", "Chaojun", ""], ["Ma", "Shirong", ""], ["Zhong", "Haoxi", ""], ["Tu", "Cunchao", ""], ["Zhang", "Tianyang", ""], ["Liu", "Zhiyuan", ""], ["Sun", "Maosong", ""]]}, {"id": "2103.13870", "submitter": "Tianxiang Zhan", "authors": "Tianxiang Zhan, Fuyuan Xiao", "title": "A novel weighted approach for time series forecasting based on\n  visibility graph", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Time series have attracted widespread attention in many fields today. Based\non the analysis of complex networks and visibility graph theory, a new time\nseries forecasting method is proposed. In time series analysis, visibility\ngraph theory transforms time series data into a network model. In the network\nmodel, the node similarity index is an important factor. On the basis of\ndirectly using the node prediction method with the largest similarity, the node\nsimilarity index is used as the weight coefficient to optimize the prediction\nalgorithm. Compared with the single-node sampling node prediction algorithm,\nthe multi-node sampling prediction algorithm can provide more accurate\nprediction values when the data set is sufficient. According to results of\nexperiments on four real-world representative datasets, the method has more\naccurate forecasting ability and can provide more accurate forecasts in the\nfield of time series and actual scenes.\n", "versions": [{"version": "v1", "created": "Sun, 14 Mar 2021 01:01:41 GMT"}, {"version": "v2", "created": "Fri, 26 Mar 2021 01:11:32 GMT"}, {"version": "v3", "created": "Sat, 17 Jul 2021 08:46:51 GMT"}], "update_date": "2021-07-20", "authors_parsed": [["Zhan", "Tianxiang", ""], ["Xiao", "Fuyuan", ""]]}, {"id": "2103.13883", "submitter": "Yaqi Duan", "authors": "Yaqi Duan, Chi Jin, Zhiyuan Li", "title": "Risk Bounds and Rademacher Complexity in Batch Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers batch Reinforcement Learning (RL) with general value\nfunction approximation. Our study investigates the minimal assumptions to\nreliably estimate/minimize Bellman error, and characterizes the generalization\nperformance by (local) Rademacher complexities of general function classes,\nwhich makes initial steps in bridging the gap between statistical learning\ntheory and batch RL. Concretely, we view the Bellman error as a surrogate loss\nfor the optimality gap, and prove the followings: (1) In double sampling\nregime, the excess risk of Empirical Risk Minimizer (ERM) is bounded by the\nRademacher complexity of the function class. (2) In the single sampling regime,\nsample-efficient risk minimization is not possible without further assumptions,\nregardless of algorithms. However, with completeness assumptions, the excess\nrisk of FQI and a minimax style algorithm can be again bounded by the\nRademacher complexity of the corresponding function classes. (3) Fast\nstatistical rates can be achieved by using tools of local Rademacher\ncomplexity. Our analysis covers a wide range of function classes, including\nfinite classes, linear spaces, kernel spaces, sparse linear features, etc.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 14:45:29 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Duan", "Yaqi", ""], ["Jin", "Chi", ""], ["Li", "Zhiyuan", ""]]}, {"id": "2103.13901", "submitter": "Pedro Zuidberg Dos Martires", "authors": "Ivan Miosic, Pedro Zuidberg Dos Martires", "title": "Measure Theoretic Weighted Model Integration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Weighted model counting (WMC) is a popular framework to perform probabilistic\ninference with discrete random variables. Recently, WMC has been extended to\nweighted model integration (WMI) in order to additionally handle continuous\nvariables. At their core, WMI problems consist of computing integrals and sums\nover weighted logical formulas. From a theoretical standpoint, WMI has been\nformulated by patching the sum over weighted formulas, which is already present\nin WMC, with Riemann integration. A more principled approach to integration,\nwhich is rooted in measure theory, is Lebesgue integration. Lebesgue\nintegration allows one to treat discrete and continuous variables on equal\nfooting in a principled fashion. We propose a theoretically sound measure\ntheoretic formulation of weighted model integration, which naturally reduces to\nweighted model counting in the absence of continuous variables. Instead of\nregarding weighted model integration as an extension of weighted model\ncounting, WMC emerges as a special case of WMI in our formulation.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 15:11:11 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Miosic", "Ivan", ""], ["Martires", "Pedro Zuidberg Dos", ""]]}, {"id": "2103.13924", "submitter": "David Benrimoh", "authors": "David Benrimoh, Ely Sibarium, Andrew Sheldon, Albert Powers", "title": "Computational Mechanism for the Effect of Psychosis Community Treatment:\n  A Conceptual Review from Neurobiology to Social Interaction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The computational underpinnings of positive psychotic symptoms have recently\nreceived significant attention. Candidate mechanisms include some combination\nof maladaptive priors and reduced updating of these priors during perception. A\npotential benefit of models with such mechanisms is their ability to link\nmultiple levels of explanation. This is key to improving how we understand the\nexperience of psychosis. Moreover, it points us towards more comprehensive\navenues for therapeutic research by providing a putative mechanism that could\nallow for the generation of new treatments from first principles. In order to\ndemonstrate this, our conceptual paper will discuss the application of the\ninsights from previous computational models to an important and complex set of\nevidence-based clinical interventions with strong social elements, such as\ncoordinated specialty care clinics in early psychosis and assertive community\ntreatment. These interventions may include but also go beyond\npsychopharmacology, providing, we argue, structure and predictability for\npatients experiencing psychosis. We develop the argument that this structure\nand predictability directly counteract the relatively low precision afforded to\nsensory information in psychosis, while also providing the patient more access\nto external cognitive resources in the form of providers and the structure of\nthe programs themselves. We discuss how computational models explain the\nresulting reduction in symptoms, as well as the predictions these models make\nabout potential responses of patients to modifications or to different\nvariations of these interventions. We also link, via the framework of\ncomputational models, the experiences of patients and response to interventions\nto putative neurobiology.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 15:35:47 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Benrimoh", "David", ""], ["Sibarium", "Ely", ""], ["Sheldon", "Andrew", ""], ["Powers", "Albert", ""]]}, {"id": "2103.14023", "submitter": "Ye Yuan", "authors": "Ye Yuan, Xinshuo Weng, Yanglan Ou, Kris Kitani", "title": "AgentFormer: Agent-Aware Transformers for Socio-Temporal Multi-Agent\n  Forecasting", "comments": "Project page: https://www.ye-yuan.com/agentformer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting accurate future trajectories of multiple agents is essential for\nautonomous systems, but is challenging due to the complex agent interaction and\nthe uncertainty in each agent's future behavior. Forecasting multi-agent\ntrajectories requires modeling two key dimensions: (1) time dimension, where we\nmodel the influence of past agent states over future states; (2) social\ndimension, where we model how the state of each agent affects others. Most\nprior methods model these two dimensions separately; e.g., first using a\ntemporal model to summarize features over time for each agent independently and\nthen modeling the interaction of the summarized features with a social model.\nThis approach is suboptimal since independent feature encoding over either the\ntime or social dimension can result in a loss of information. Instead, we would\nprefer a method that allows an agent's state at one time to directly affect\nanother agent's state at a future time. To this end, we propose a new\nTransformer, AgentFormer, that jointly models the time and social dimensions.\nThe model leverages a sequence representation of multi-agent trajectories by\nflattening trajectory features across time and agents. Since standard attention\noperations disregard the agent identity of each element in the sequence,\nAgentFormer uses a novel agent-aware attention mechanism that preserves agent\nidentities by attending to elements of the same agent differently than elements\nof other agents. Based on AgentFormer, we propose a stochastic multi-agent\ntrajectory prediction model that can attend to features of any agent at any\nprevious timestep when inferring an agent's future position. The latent intent\nof all agents is also jointly modeled, allowing the stochasticity in one\nagent's behavior to affect other agents. Our method significantly improves the\nstate of the art on well-established pedestrian and autonomous driving\ndatasets.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 17:59:01 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Yuan", "Ye", ""], ["Weng", "Xinshuo", ""], ["Ou", "Yanglan", ""], ["Kitani", "Kris", ""]]}, {"id": "2103.14025", "submitter": "Chuang Gan", "authors": "Chuang Gan, Siyuan Zhou, Jeremy Schwartz, Seth Alter, Abhishek\n  Bhandwaldar, Dan Gutfreund, Daniel L.K. Yamins, James J DiCarlo, Josh\n  McDermott, Antonio Torralba, Joshua B. Tenenbaum", "title": "The ThreeDWorld Transport Challenge: A Visually Guided Task-and-Motion\n  Planning Benchmark for Physically Realistic Embodied AI", "comments": "Project page: http://tdw-transport.csail.mit.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a visually-guided and physics-driven task-and-motion planning\nbenchmark, which we call the ThreeDWorld Transport Challenge. In this\nchallenge, an embodied agent equipped with two 9-DOF articulated arms is\nspawned randomly in a simulated physical home environment. The agent is\nrequired to find a small set of objects scattered around the house, pick them\nup, and transport them to a desired final location. We also position containers\naround the house that can be used as tools to assist with transporting objects\nefficiently. To complete the task, an embodied agent must plan a sequence of\nactions to change the state of a large number of objects in the face of\nrealistic physical constraints. We build this benchmark challenge using the\nThreeDWorld simulation: a virtual 3D environment where all objects respond to\nphysics, and where can be controlled using fully physics-driven navigation and\ninteraction API. We evaluate several existing agents on this benchmark.\nExperimental results suggest that: 1) a pure RL model struggles on this\nchallenge; 2) hierarchical planning-based agents can transport some objects but\nstill far from solving this task. We anticipate that this benchmark will\nempower researchers to develop more intelligent physics-driven robots for the\nphysical world.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 17:59:08 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Gan", "Chuang", ""], ["Zhou", "Siyuan", ""], ["Schwartz", "Jeremy", ""], ["Alter", "Seth", ""], ["Bhandwaldar", "Abhishek", ""], ["Gutfreund", "Dan", ""], ["Yamins", "Daniel L. K.", ""], ["DiCarlo", "James J", ""], ["McDermott", "Josh", ""], ["Torralba", "Antonio", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "2103.14033", "submitter": "Parthasarathy Suryanarayanan", "authors": "Parthasarathy Suryanarayanan, Sundar Saranathan, Shilpa Mahatma, Divya\n  Pathak", "title": "A Novel Methodology For Crowdsourcing AI Models in an Enterprise", "comments": "Presented at Challenges in Machine Learning workshop 2020 (CiML2020)\n  @NeurIPS (http://ciml.chalearn.org/)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The evolution of AI is advancing rapidly, creating both challenges and\nopportunities for industry-community collaboration. In this work, we present a\nnovel methodology aiming to facilitate this collaboration through crowdsourcing\nof AI models. Concretely, we have implemented a system and a process that any\norganization can easily adopt to host AI competitions. The system allows them\nto automatically harvest and evaluate the submitted models against in-house\nproprietary data and also to incorporate them as reusable services in a\nproduct.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 18:27:51 GMT"}], "update_date": "2021-03-26", "authors_parsed": [["Suryanarayanan", "Parthasarathy", ""], ["Saranathan", "Sundar", ""], ["Mahatma", "Shilpa", ""], ["Pathak", "Divya", ""]]}, {"id": "2103.14036", "submitter": "David Smith", "authors": "David Smith, Frederik Geth, Elliott Vercoe, Andrew Feutrill, Ming\n  Ding, Jonathan Chan, James Foster and Thierry Rakotoarivelo", "title": "Realistic Differentially-Private Transmission Power Flow Data Release", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the modeling, design and planning of future energy transmission networks,\nit is vital for stakeholders to access faithful and useful power flow data,\nwhile provably maintaining the privacy of business confidentiality of service\nproviders. This critical challenge has recently been somewhat addressed in [1].\nThis paper significantly extends this existing work. First, we reduce the\npotential leakage information by proposing a fundamentally different\npost-processing method, using public information of grid losses rather than\npower dispatch, which achieve a higher level of privacy protection. Second, we\nprotect more sensitive parameters, i.e., branch shunt susceptance in addition\nto series impedance (complete pi-model). This protects power flow data for the\ntransmission high-voltage networks, using differentially private\ntransformations that maintain the optimal power flow consistent with, and\nfaithful to, expected model behaviour. Third, we tested our approach at a\nlarger scale than previous work, using the PGLib-OPF test cases [10]. This\nresulted in the successful obfuscation of up to a 4700-bus system, which can be\nsuccessfully solved with faithfulness of parameters and good utility to data\nanalysts. Our approach addresses a more feasible and realistic scenario, and\nprovides higher than state-of-the-art privacy guarantees, while maintaining\nsolvability, fidelity and feasibility of the system.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 04:04:12 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Smith", "David", ""], ["Geth", "Frederik", ""], ["Vercoe", "Elliott", ""], ["Feutrill", "Andrew", ""], ["Ding", "Ming", ""], ["Chan", "Jonathan", ""], ["Foster", "James", ""], ["Rakotoarivelo", "Thierry", ""]]}, {"id": "2103.14051", "submitter": "Hadi Jamali-Rad", "authors": "Attila Szabo, Hadi Jamali-Rad, Siva-Datta Mannava", "title": "Tilted Cross Entropy (TCE): Promoting Fairness in Semantic Segmentation", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional empirical risk minimization (ERM) for semantic segmentation can\ndisproportionately advantage or disadvantage certain target classes in favor of\nan (unfair but) improved overall performance. Inspired by the recently\nintroduced tilted ERM (TERM), we propose tilted cross-entropy (TCE) loss and\nadapt it to the semantic segmentation setting to minimize performance disparity\namong target classes and promote fairness. Through quantitative and qualitative\nperformance analyses, we demonstrate that the proposed Stochastic TCE for\nsemantic segmentation can efficiently improve the low-performing classes of\nCityscapes and ADE20k datasets trained with multi-class cross-entropy (MCCE),\nand also results in improved overall fairness.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 18:00:50 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Szabo", "Attila", ""], ["Jamali-Rad", "Hadi", ""], ["Mannava", "Siva-Datta", ""]]}, {"id": "2103.14068", "submitter": "Rachel Cummings", "authors": "Chris Waites and Rachel Cummings", "title": "Differentially Private Normalizing Flows for Privacy-Preserving Density\n  Estimation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.DS stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Normalizing flow models have risen as a popular solution to the problem of\ndensity estimation, enabling high-quality synthetic data generation as well as\nexact probability density evaluation. However, in contexts where individuals\nare directly associated with the training data, releasing such a model raises\nprivacy concerns. In this work, we propose the use of normalizing flow models\nthat provide explicit differential privacy guarantees as a novel approach to\nthe problem of privacy-preserving density estimation. We evaluate the efficacy\nof our approach empirically using benchmark datasets, and we demonstrate that\nour method substantially outperforms previous state-of-the-art approaches. We\nadditionally show how our algorithm can be applied to the task of\ndifferentially private anomaly detection.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 18:39:51 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Waites", "Chris", ""], ["Cummings", "Rachel", ""]]}, {"id": "2103.14082", "submitter": "Zhouzheng Li", "authors": "Zhouzheng Li and Kun Feng", "title": "Learning Stable Representations with Full Encoder", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the beta-VAE family is aiming to find disentangled representations and\nacquire human-interpretable generative factors, like what an ICA (from the\nlinear domain) does, we propose Full Encoder, a novel unified autoencoder\nframework as a correspondence to PCA in the non-linear domain. The idea is to\ntrain an autoencoder with one latent variable first, then involve more latent\nvariables progressively to refine the reconstruction results. The Full Encoder\nis also a latent variable predictive model that the latent variables acquired\nare stable and robust, as they always learn the same representation regardless\nof the network initial states. Full Encoder can be used to determine the\ndegrees of freedom in a simple non-linear system and can be useful for data\ncompression or anomaly detection. Full Encoder can also be combined with the\nbeta-VAE framework to sort out the importance of the generative factors,\nproviding more insights for non-linear system analysis. These qualities will\nmake FE useful for analyzing real-life industrial non-linear systems. To\nvalidate, we created a toy dataset with a custom-made non-linear system to test\nit and compare its properties to those of VAE and beta-VAE's.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 18:57:30 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 10:41:18 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Li", "Zhouzheng", ""], ["Feng", "Kun", ""]]}, {"id": "2103.14101", "submitter": "Grace Lewis", "authors": "Grace A. Lewis, Stephany Bellomo, Ipek Ozkaya", "title": "Characterizing and Detecting Mismatch in Machine-Learning-Enabled\n  Systems", "comments": "1st Workshop on AI Engineering: Software Engineering for AI (WAIN\n  2021) held at the 2021 IEEE/ACM 43rd International Conference on Software\n  Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Increasing availability of machine learning (ML) frameworks and tools, as\nwell as their promise to improve solutions to data-driven decision problems,\nhas resulted in popularity of using ML techniques in software systems. However,\nend-to-end development of ML-enabled systems, as well as their seamless\ndeployment and operations, remain a challenge. One reason is that development\nand deployment of ML-enabled systems involves three distinct workflows,\nperspectives, and roles, which include data science, software engineering, and\noperations. These three distinct perspectives, when misaligned due to incorrect\nassumptions, cause ML mismatches which can result in failed systems. We\nconducted an interview and survey study where we collected and validated common\ntypes of mismatches that occur in end-to-end development of ML-enabled systems.\nOur analysis shows that how each role prioritizes the importance of relevant\nmismatches varies, potentially contributing to these mismatched assumptions. In\naddition, the mismatch categories we identified can be specified as machine\nreadable descriptors contributing to improved ML-enabled system development. In\nthis paper, we report our findings and their implications for improving\nend-to-end ML-enabled system development.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 19:40:29 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Lewis", "Grace A.", ""], ["Bellomo", "Stephany", ""], ["Ozkaya", "Ipek", ""]]}, {"id": "2103.14115", "submitter": "Md Munir Hasan", "authors": "Md Munir Hasan and Jeremy Holleman", "title": "Training Neural Networks Using the Property of Negative Feedback to\n  Inverse a Function", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With high forward gain, a negative feedback system has the ability to perform\nthe inverse of a linear or non linear function that is in the feedback path.\nThis property of negative feedback systems has been widely used in analog\ncircuits to construct precise closed-loop functions. This paper describes how\nthe property of a negative feedback system to perform inverse of a function can\nbe used for training neural networks. This method does not require that the\ncost or activation functions be differentiable. Hence, it is able to learn a\nclass of non-differentiable functions as well where a gradient descent-based\nmethod fails. We also show that gradient descent emerges as a special case of\nthe proposed method. We have applied this method to the MNIST dataset and\nobtained results that shows the method is viable for neural network training.\nThis method, to the best of our knowledge, is novel in machine learning.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 20:13:53 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Hasan", "Md Munir", ""], ["Holleman", "Jeremy", ""]]}, {"id": "2103.14137", "submitter": "Joao Marcos Correia Marques", "authors": "Joao Marcos Correia Marques, Ramya Ramalingam, Zherong Pan, and Kris\n  Hauser", "title": "Optimized Coverage Planning for UV Surface Disinfection", "comments": "13 pages, 18 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  UV radiation has been used as a disinfection strategy to deactivate a wide\nrange of pathogens, but existing irradiation strategies do not ensure\nsufficient exposure of all environmental surfaces and/or require long\ndisinfection times. We present a near-optimal coverage planner for mobile UV\ndisinfection robots. The formulation optimizes the irradiation time efficiency,\nwhile ensuring that a sufficient dosage of radiation is received by each\nsurface. The trajectory and dosage plan are optimized taking collision and\nlight occlusion constraints into account. We propose a two-stage scheme to\napproximate the solution of the induced NP-hard optimization, and, for\nefficiency, perform key irradiance and occlusion calculations on a GPU.\nEmpirical results show that our technique achieves more coverage for the same\nexposure time as strategies for existing UV robots, can be used to compare UV\nrobot designs, and produces near-optimal plans. This is an extended version of\nthe paper originally contributed to ICRA2021.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 21:16:25 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Marques", "Joao Marcos Correia", ""], ["Ramalingam", "Ramya", ""], ["Pan", "Zherong", ""], ["Hauser", "Kris", ""]]}, {"id": "2103.14161", "submitter": "Thanh Nguyen", "authors": "Thanh Nguyen-Duc, Natasha Mulligan, Gurdeep S. Mannu, Joao H.\n  Bettencourt-Silva", "title": "Deep EHR Spotlight: a Framework and Mechanism to Highlight Events in\n  Electronic Health Records for Explainable Predictions", "comments": "AMIA 2021 Virtual Informatics Summit", "journal-ref": "AMIA 2021 Virtual Informatics Summit", "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The wide adoption of Electronic Health Records (EHR) has resulted in large\namounts of clinical data becoming available, which promises to support service\ndelivery and advance clinical and informatics research. Deep learning\ntechniques have demonstrated performance in predictive analytic tasks using\nEHRs yet they typically lack model result transparency or explainability\nfunctionalities and require cumbersome pre-processing tasks. Moreover, EHRs\ncontain heterogeneous and multi-modal data points such as text, numbers and\ntime series which further hinder visualisation and interpretability. This paper\nproposes a deep learning framework to: 1) encode patient pathways from EHRs\ninto images, 2) highlight important events within pathway images, and 3) enable\nmore complex predictions with additional intelligibility. The proposed method\nrelies on a deep attention mechanism for visualisation of the predictions and\nallows predicting multiple sequential outcomes.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 22:30:14 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Nguyen-Duc", "Thanh", ""], ["Mulligan", "Natasha", ""], ["Mannu", "Gurdeep S.", ""], ["Bettencourt-Silva", "Joao H.", ""]]}, {"id": "2103.14172", "submitter": "Shreyas Ramakrishna", "authors": "Matthew Burruss, Shreyas Ramakrishna and Abhishek Dubey", "title": "Deep-RBF Networks for Anomaly Detection in Automotive Cyber-Physical\n  Systems", "comments": "Submitted to Smartcomp 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks (DNNs) are popularly used for implementing autonomy\nrelated tasks in automotive Cyber-Physical Systems (CPSs). However, these\nnetworks have been shown to make erroneous predictions to anomalous inputs,\nwhich manifests either due to Out-of-Distribution (OOD) data or adversarial\nattacks. To detect these anomalies, a separate DNN called assurance monitor is\noften trained and used in parallel to the controller DNN, increasing the\nresource burden and latency. We hypothesize that a single network that can\nperform controller predictions and anomaly detection is necessary to reduce the\nresource requirements. Deep-Radial Basis Function (RBF) networks provide a\nrejection class alongside the class predictions, which can be utilized for\ndetecting anomalies at runtime. However, the use of RBF activation functions\nlimits the applicability of these networks to only classification tasks. In\nthis paper, we show how the deep-RBF network can be used for detecting\nanomalies in CPS regression tasks such as continuous steering predictions.\nFurther, we design deep-RBF networks using popular DNNs such as NVIDIA DAVE-II,\nand ResNet20, and then use the resulting rejection class for detecting\nadversarial attacks such as a physical attack and data poison attack. Finally,\nwe evaluate these attacks and the trained deep-RBF networks using a hardware\nCPS testbed called DeepNNCar and a real-world German Traffic Sign Benchmark\n(GTSB) dataset. Our results show that the deep-RBF networks can robustly detect\nthese attacks in a short time without additional resource requirements.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 23:10:32 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Burruss", "Matthew", ""], ["Ramakrishna", "Shreyas", ""], ["Dubey", "Abhishek", ""]]}, {"id": "2103.14208", "submitter": "Ju-Chiang Wang", "authors": "Jiawen Huang, Ju-Chiang Wang, Jordan B. L. Smith, Xuchen Song, Yuxuan\n  Wang", "title": "Modeling the Compatibility of Stem Tracks to Generate Music Mashups", "comments": "This is a preprint of the paper accepted by AAAI-21. Please cite the\n  version included in the Proceedings of the 35th AAAI Conference on Artificial\n  Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI eess.AS", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A music mashup combines audio elements from two or more songs to create a new\nwork. To reduce the time and effort required to make them, researchers have\ndeveloped algorithms that predict the compatibility of audio elements. Prior\nwork has focused on mixing unaltered excerpts, but advances in source\nseparation enable the creation of mashups from isolated stems (e.g., vocals,\ndrums, bass, etc.). In this work, we take advantage of separated stems not just\nfor creating mashups, but for training a model that predicts the mutual\ncompatibility of groups of excerpts, using self-supervised and semi-supervised\nmethods. Specifically, we first produce a random mashup creation pipeline that\ncombines stem tracks obtained via source separation, with key and tempo\nautomatically adjusted to match, since these are prerequisites for high-quality\nmashups. To train a model to predict compatibility, we use stem tracks obtained\nfrom the same song as positive examples, and random combinations of stems with\nkey and/or tempo unadjusted as negative examples. To improve the model and use\nmore data, we also train on \"average\" examples: random combinations with\nmatching key and tempo, where we treat them as unlabeled data as their true\ncompatibility is unknown. To determine whether the combined signal or the set\nof stem signals is more indicative of the quality of the result, we experiment\non two model architectures and train them using semi-supervised learning\ntechnique. Finally, we conduct objective and subjective evaluations of the\nsystem, comparing them to a standard rule-based system.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 01:51:11 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Huang", "Jiawen", ""], ["Wang", "Ju-Chiang", ""], ["Smith", "Jordan B. L.", ""], ["Song", "Xuchen", ""], ["Wang", "Yuxuan", ""]]}, {"id": "2103.14230", "submitter": "Chi Zhang", "authors": "Chi Zhang, Baoxiong Jia, Song-Chun Zhu, Yixin Zhu", "title": "Abstract Spatial-Temporal Reasoning via Probabilistic Abduction and\n  Execution", "comments": "CVPR 2021 paper. Supplementary:\n  http://wellyzhang.github.io/attach/cvpr21zhang_prae_supp.pdf Project:\n  http://wellyzhang.github.io/project/prae.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spatial-temporal reasoning is a challenging task in Artificial Intelligence\n(AI) due to its demanding but unique nature: a theoretic requirement on\nrepresenting and reasoning based on spatial-temporal knowledge in mind, and an\napplied requirement on a high-level cognitive system capable of navigating and\nacting in space and time. Recent works have focused on an abstract reasoning\ntask of this kind -- Raven's Progressive Matrices (RPM). Despite the\nencouraging progress on RPM that achieves human-level performance in terms of\naccuracy, modern approaches have neither a treatment of human-like reasoning on\ngeneralization, nor a potential to generate answers. To fill in this gap, we\npropose a neuro-symbolic Probabilistic Abduction and Execution (PrAE) learner;\ncentral to the PrAE learner is the process of probabilistic abduction and\nexecution on a probabilistic scene representation, akin to the mental\nmanipulation of objects. Specifically, we disentangle perception and reasoning\nfrom a monolithic model. The neural visual perception frontend predicts\nobjects' attributes, later aggregated by a scene inference engine to produce a\nprobabilistic scene representation. In the symbolic logical reasoning backend,\nthe PrAE learner uses the representation to abduce the hidden rules. An answer\nis predicted by executing the rules on the probabilistic representation. The\nentire system is trained end-to-end in an analysis-by-synthesis manner without\nany visual attribute annotations. Extensive experiments demonstrate that the\nPrAE learner improves cross-configuration generalization and is capable of\nrendering an answer, in contrast to prior works that merely make a categorical\nchoice from candidates.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 02:42:18 GMT"}, {"version": "v2", "created": "Fri, 14 May 2021 01:47:55 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Zhang", "Chi", ""], ["Jia", "Baoxiong", ""], ["Zhu", "Song-Chun", ""], ["Zhu", "Yixin", ""]]}, {"id": "2103.14231", "submitter": "Chi Zhang", "authors": "Xu Xie, Chi Zhang, Yixin Zhu, Ying Nian Wu, Song-Chun Zhu", "title": "Congestion-aware Multi-agent Trajectory Prediction for Collision\n  Avoidance", "comments": "ICRA 2021 paper. Project:\n  https://xuxie1031.github.io/projects/GTA/GTAProj.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Predicting agents' future trajectories plays a crucial role in modern AI\nsystems, yet it is challenging due to intricate interactions exhibited in\nmulti-agent systems, especially when it comes to collision avoidance. To\naddress this challenge, we propose to learn congestion patterns as contextual\ncues explicitly and devise a novel \"Sense--Learn--Reason--Predict\" framework by\nexploiting advantages of three different doctrines of thought, which yields the\nfollowing desirable benefits: (i) Representing congestion as contextual cues\nvia latent factors subsumes the concept of social force commonly used in\nphysics-based approaches and implicitly encodes the distance as a cost, similar\nto the way a planning-based method models the environment. (ii) By decomposing\nthe learning phases into two stages, a \"student\" can learn contextual cues from\na \"teacher\" while generating collision-free trajectories. To make the framework\ncomputationally tractable, we formulate it as an optimization problem and\nderive an upper bound by leveraging the variational parametrization. In\nexperiments, we demonstrate that the proposed model is able to generate\ncollision-free trajectory predictions in a synthetic dataset designed for\ncollision avoidance evaluation and remains competitive on the commonly used\nNGSIM US-101 highway dataset.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 02:42:33 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Xie", "Xu", ""], ["Zhang", "Chi", ""], ["Zhu", "Yixin", ""], ["Wu", "Ying Nian", ""], ["Zhu", "Song-Chun", ""]]}, {"id": "2103.14232", "submitter": "Chi Zhang", "authors": "Chi Zhang, Baoxiong Jia, Mark Edmonds, Song-Chun Zhu, Yixin Zhu", "title": "ACRE: Abstract Causal REasoning Beyond Covariation", "comments": "CVPR 2021 paper. Supplementary:\n  http://wellyzhang.github.io/attach/cvpr21zhang_acre_supp.pdf Project:\n  http://wellyzhang.github.io/project/acre.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal induction, i.e., identifying unobservable mechanisms that lead to the\nobservable relations among variables, has played a pivotal role in modern\nscientific discovery, especially in scenarios with only sparse and limited\ndata. Humans, even young toddlers, can induce causal relationships surprisingly\nwell in various settings despite its notorious difficulty. However, in contrast\nto the commonplace trait of human cognition is the lack of a diagnostic\nbenchmark to measure causal induction for modern Artificial Intelligence (AI)\nsystems. Therefore, in this work, we introduce the Abstract Causal REasoning\n(ACRE) dataset for systematic evaluation of current vision systems in causal\ninduction. Motivated by the stream of research on causal discovery in Blicket\nexperiments, we query a visual reasoning system with the following four types\nof questions in either an independent scenario or an interventional scenario:\ndirect, indirect, screening-off, and backward-blocking, intentionally going\nbeyond the simple strategy of inducing causal relationships by covariation. By\nanalyzing visual reasoning architectures on this testbed, we notice that pure\nneural models tend towards an associative strategy under their chance-level\nperformance, whereas neuro-symbolic combinations struggle in backward-blocking\nreasoning. These deficiencies call for future research in models with a more\ncomprehensive capability of causal induction.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 02:42:38 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Zhang", "Chi", ""], ["Jia", "Baoxiong", ""], ["Edmonds", "Mark", ""], ["Zhu", "Song-Chun", ""], ["Zhu", "Yixin", ""]]}, {"id": "2103.14245", "submitter": "Yi Shi", "authors": "Congyi Wang, Yu Chen, Bin Wang, Yi Shi", "title": "Improve GAN-based Neural Vocoder using Pointwise Relativistic\n  LeastSquare GAN", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.CL eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  GAN-based neural vocoders, such as Parallel WaveGAN and MelGAN have attracted\ngreat interest due to their lightweight and parallel structures, enabling them\nto generate high fidelity waveform in a real-time manner. In this paper,\ninspired by Relativistic GAN, we introduce a novel variant of the LSGAN\nframework under the context of waveform synthesis, named Pointwise Relativistic\nLSGAN (PRLSGAN). In this approach, we take the truism score distribution into\nconsideration and combine the original MSE loss with the proposed pointwise\nrelative discrepancy loss to increase the difficulty of the generator to fool\nthe discriminator, leading to improved generation quality. Moreover, PRLSGAN is\na general-purposed framework that can be combined with any GAN-based neural\nvocoder to enhance its generation quality. Experiments have shown a consistent\nperformance boost based on Parallel WaveGAN and MelGAN, demonstrating the\neffectiveness and strong generalization ability of our proposed PRLSGAN neural\nvocoders.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 03:35:22 GMT"}, {"version": "v2", "created": "Mon, 29 Mar 2021 03:00:21 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Wang", "Congyi", ""], ["Chen", "Yu", ""], ["Wang", "Bin", ""], ["Shi", "Yi", ""]]}, {"id": "2103.14250", "submitter": "Rohitash Chandra", "authors": "Rohitash Chandra, Shaurya Goyal, Rishabh Gupta", "title": "Evaluation of deep learning models for multi-step ahead time series\n  prediction", "comments": null, "journal-ref": "IEEE Access, 2021", "doi": "10.1109/ACCESS.2021.3085085", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Time series prediction with neural networks has been the focus of much\nresearch in the past few decades. Given the recent deep learning revolution,\nthere has been much attention in using deep learning models for time series\nprediction, and hence it is important to evaluate their strengths and\nweaknesses. In this paper, we present an evaluation study that compares the\nperformance of deep learning models for multi-step ahead time series\nprediction. The deep learning methods comprise simple recurrent neural\nnetworks, long short-term memory (LSTM) networks, bidirectional LSTM networks,\nencoder-decoder LSTM networks, and convolutional neural networks. We provide a\nfurther comparison with simple neural networks that use stochastic gradient\ndescent and adaptive moment estimation (Adam) for training. We focus on\nunivariate time series for multi-step-ahead prediction from benchmark\ntime-series datasets and provide a further comparison of the results with\nrelated methods from the literature. The results show that the bidirectional\nand encoder-decoder LSTM network provides the best performance in accuracy for\nthe given time series problems.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 04:07:11 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 10:43:11 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Chandra", "Rohitash", ""], ["Goyal", "Shaurya", ""], ["Gupta", "Rishabh", ""]]}, {"id": "2103.14253", "submitter": "Ju-Chiang Wang", "authors": "Ju-Chiang Wang, Jordan B.L. Smith, Jitong Chen, Xuchen Song, Yuxuan\n  Wang", "title": "Supervised Chorus Detection for Popular Music Using Convolutional Neural\n  Network and Multi-task Learning", "comments": "This version is a preprint of an accepted paper by ICASSP2021. Please\n  cite the publication in the Proceedings of IEEE International Conference on\n  Acoustics, Speech, & Signal Processing", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.AS cs.AI cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a novel supervised approach to detecting the chorus\nsegments in popular music. Traditional approaches to this task are mostly\nunsupervised, with pipelines designed to target some quality that is assumed to\ndefine \"chorusness,\" which usually means seeking the loudest or most frequently\nrepeated sections. We propose to use a convolutional neural network with a\nmulti-task learning objective, which simultaneously fits two temporal\nactivation curves: one indicating \"chorusness\" as a function of time, and the\nother the location of the boundaries. We also propose a post-processing method\nthat jointly takes into account the chorus and boundary predictions to produce\nbinary output. In experiments using three datasets, we compare our system to a\nset of public implementations of other segmentation and chorus-detection\nalgorithms, and find our approach performs significantly better.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 04:32:08 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 06:00:19 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Wang", "Ju-Chiang", ""], ["Smith", "Jordan B. L.", ""], ["Chen", "Jitong", ""], ["Song", "Xuchen", ""], ["Wang", "Yuxuan", ""]]}, {"id": "2103.14256", "submitter": "Saumya Saxena", "authors": "Saumya Saxena, Alex LaGrassa, Oliver Kroemer", "title": "Learning Reactive and Predictive Differentiable Controllers for\n  Switching Linear Dynamical Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans leverage the dynamics of the environment and their own bodies to\naccomplish challenging tasks such as grasping an object while walking past it\nor pushing off a wall to turn a corner. Such tasks often involve switching\ndynamics as the robot makes and breaks contact. Learning these dynamics is a\nchallenging problem and prone to model inaccuracies, especially near contact\nregions. In this work, we present a framework for learning composite dynamical\nbehaviors from expert demonstrations. We learn a switching linear dynamical\nmodel with contacts encoded in switching conditions as a close approximation of\nour system dynamics. We then use discrete-time LQR as the differentiable policy\nclass for data-efficient learning of control to develop a control strategy that\noperates over multiple dynamical modes and takes into account discontinuities\ndue to contact. In addition to predicting interactions with the environment,\nour policy effectively reacts to inaccurate predictions such as unanticipated\ncontacts. Through simulation and real world experiments, we demonstrate\ngeneralization of learned behaviors to different scenarios and robustness to\nmodel inaccuracies during execution.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 04:40:24 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Saxena", "Saumya", ""], ["LaGrassa", "Alex", ""], ["Kroemer", "Oliver", ""]]}, {"id": "2103.14262", "submitter": "Zhe Xu", "authors": "Zhe Xu and Xiaoming Duan", "title": "Robust Pandemic Control Synthesis with Formal Specifications: A Case\n  Study on COVID-19 Pandemic", "comments": "arXiv admin note: text overlap with arXiv:2007.15114", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.AI cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pandemics can bring a range of devastating consequences to public health and\nthe world economy. Identifying the most effective control strategies has been\nthe imperative task all around the world. Various public health control\nstrategies have been proposed and tested against pandemic diseases (e.g.,\nCOVID-19). We study two specific pandemic control models: the susceptible,\nexposed, infectious, recovered (SEIR) model with vaccination control; and the\nSEIR model with shield immunity control. We express the pandemic control\nrequirement in metric temporal logic (MTL) formulas. We then develop an\niterative approach for synthesizing the optimal control strategies with MTL\nspecifications. We provide simulation results in two different scenarios for\nrobust control of the COVID-19 pandemic: one for vaccination control, and\nanother for shield immunity control, with the model parameters estimated from\ndata in Lombardy, Italy. The results show that the proposed synthesis approach\ncan generate control inputs such that the time-varying numbers of individuals\nin each category (e.g., infectious, immune) satisfy the MTL specifications with\nrobustness against initial state and parameter uncertainties.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 04:46:04 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Xu", "Zhe", ""], ["Duan", "Xiaoming", ""]]}, {"id": "2103.14264", "submitter": "Zhe Xu", "authors": "Zhe Xu and Yichen Zhang", "title": "Provably Correct Controller Synthesis of Switched Stochastic Systems\n  with Metric Temporal Logic Specifications: A Case Study on Power Systems", "comments": "arXiv admin note: substantial text overlap with arXiv:1911.11347", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.AI cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a provably correct controller synthesis approach\nfor switched stochastic control systems with metric temporal logic (MTL)\nspecifications with provable probabilistic guarantees. We first present the\nstochastic control bisimulation function for switched stochastic control\nsystems, which bounds the trajectory divergence between the switched stochastic\ncontrol system and its nominal deterministic control system in a probabilistic\nfashion. We then develop a method to compute optimal control inputs by solving\nan optimization problem for the nominal trajectory of the deterministic control\nsystem with robustness against initial state variations and stochastic\nuncertainties. We implement our robust stochastic controller synthesis approach\non both a four-bus power system and a nine-bus power system under generation\nloss disturbances, with MTL specifications expressing requirements for the grid\nfrequency deviations, wind turbine generator rotor speed variations and the\npower flow constraints at different power lines.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 04:50:29 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Xu", "Zhe", ""], ["Zhang", "Yichen", ""]]}, {"id": "2103.14282", "submitter": "Shaofei Cai", "authors": "Shaofei Cai, Liang Li, Jincan Deng, Beichen Zhang, Zheng-Jun Zha, Li\n  Su and Qingming Huang", "title": "Rethinking Graph Neural Architecture Search from Message-passing", "comments": "This paper has been accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Graph neural networks (GNNs) emerged recently as a standard toolkit for\nlearning from data on graphs. Current GNN designing works depend on immense\nhuman expertise to explore different message-passing mechanisms, and require\nmanual enumeration to determine the proper message-passing depth. Inspired by\nthe strong searching capability of neural architecture search (NAS) in CNN,\nthis paper proposes Graph Neural Architecture Search (GNAS) with novel-designed\nsearch space. The GNAS can automatically learn better architecture with the\noptimal depth of message passing on the graph. Specifically, we design Graph\nNeural Architecture Paradigm (GAP) with tree-topology computation procedure and\ntwo types of fine-grained atomic operations (feature filtering and neighbor\naggregation) from message-passing mechanism to construct powerful graph network\nsearch space. Feature filtering performs adaptive feature selection, and\nneighbor aggregation captures structural information and calculates neighbors'\nstatistics. Experiments show that our GNAS can search for better GNNs with\nmultiple message-passing mechanisms and optimal message-passing depth. The\nsearched network achieves remarkable improvement over state-of-the-art manual\ndesigned and search-based GNNs on five large-scale datasets at three classical\ngraph tasks. Codes can be found at https://github.com/phython96/GNAS-MP.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 06:10:41 GMT"}, {"version": "v2", "created": "Sun, 18 Apr 2021 08:41:39 GMT"}, {"version": "v3", "created": "Sat, 8 May 2021 06:42:31 GMT"}, {"version": "v4", "created": "Thu, 24 Jun 2021 01:56:25 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Cai", "Shaofei", ""], ["Li", "Liang", ""], ["Deng", "Jincan", ""], ["Zhang", "Beichen", ""], ["Zha", "Zheng-Jun", ""], ["Su", "Li", ""], ["Huang", "Qingming", ""]]}, {"id": "2103.14283", "submitter": "Lin Shao", "authors": "Yifan You, Lin Shao, Toki Migimatsu, Jeannette Bohg", "title": "OmniHang: Learning to Hang Arbitrary Objects using Contact Point\n  Correspondences and Neural Collision Estimation", "comments": "Accepted to IEEE International Conference on Robotics and Automation\n  (ICRA) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper, we explore whether a robot can learn to hang arbitrary objects\nonto a diverse set of supporting items such as racks or hooks. Endowing robots\nwith such an ability has applications in many domains such as domestic\nservices, logistics, or manufacturing. Yet, it is a challenging manipulation\ntask due to the large diversity of geometry and topology of everyday objects.\nIn this paper, we propose a system that takes partial point clouds of an object\nand a supporting item as input and learns to decide where and how to hang the\nobject stably. Our system learns to estimate the contact point correspondences\nbetween the object and supporting item to get an estimated stable pose. We then\nrun a deep reinforcement learning algorithm to refine the predicted stable\npose. Then, the robot needs to find a collision-free path to move the object\nfrom its initial pose to stable hanging pose. To this end, we train a neural\nnetwork based collision estimator that takes as input partial point clouds of\nthe object and supporting item. We generate a new and challenging, large-scale,\nsynthetic dataset annotated with stable poses of objects hung on various\nsupporting items and their contact point correspondences. In this dataset, we\nshow that our system is able to achieve a 68.3% success rate of predicting\nstable object poses and has a 52.1% F1 score in terms of finding feasible\npaths. Supplemental material and videos are available on our project webpage.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 06:11:05 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["You", "Yifan", ""], ["Shao", "Lin", ""], ["Migimatsu", "Toki", ""], ["Bohg", "Jeannette", ""]]}, {"id": "2103.14295", "submitter": "Zhongyu Li", "authors": "Zhongyu Li, Xuxin Cheng, Xue Bin Peng, Pieter Abbeel, Sergey Levine,\n  Glen Berseth, Koushil Sreenath", "title": "Reinforcement Learning for Robust Parameterized Locomotion Control of\n  Bipedal Robots", "comments": "To appear on 2021 International Conference on Robotics and Automation\n  (ICRA 2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Developing robust walking controllers for bipedal robots is a challenging\nendeavor. Traditional model-based locomotion controllers require simplifying\nassumptions and careful modelling; any small errors can result in unstable\ncontrol. To address these challenges for bipedal locomotion, we present a\nmodel-free reinforcement learning framework for training robust locomotion\npolicies in simulation, which can then be transferred to a real bipedal Cassie\nrobot. To facilitate sim-to-real transfer, domain randomization is used to\nencourage the policies to learn behaviors that are robust across variations in\nsystem dynamics. The learned policies enable Cassie to perform a set of diverse\nand dynamic behaviors, while also being more robust than traditional\ncontrollers and prior learning-based methods that use residual control. We\ndemonstrate this on versatile walking behaviors such as tracking a target\nwalking velocity, walking height, and turning yaw.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 07:14:01 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Li", "Zhongyu", ""], ["Cheng", "Xuxin", ""], ["Peng", "Xue Bin", ""], ["Abbeel", "Pieter", ""], ["Levine", "Sergey", ""], ["Berseth", "Glen", ""], ["Sreenath", "Koushil", ""]]}, {"id": "2103.14330", "submitter": "Hao Li", "authors": "Hao Li, Xueliang Zhang, Guanglai Gao", "title": "Guided Training: A Simple Method for Single-channel Speaker Separation", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI eess.AS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep learning has shown a great potential for speech separation, especially\nfor speech and non-speech separation. However, it encounters permutation\nproblem for multi-speaker separation where both target and interference are\nspeech. Permutation Invariant training (PIT) was proposed to solve this problem\nby permuting the order of the multiple speakers. Another way is to use an\nanchor speech, a short speech of the target speaker, to model the speaker\nidentity. In this paper, we propose a simple strategy to train a long\nshort-term memory (LSTM) model to solve the permutation problem in speaker\nseparation. Specifically, we insert a short speech of target speaker at the\nbeginning of a mixture as guide information. So, the first appearing speaker is\ndefined as the target. Due to the powerful capability on sequence modeling,\nLSTM can use its memory cells to track and separate target speech from\ninterfering speech. Experimental results show that the proposed training\nstrategy is effective for speaker separation.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 08:46:50 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Li", "Hao", ""], ["Zhang", "Xueliang", ""], ["Gao", "Guanglai", ""]]}, {"id": "2103.14331", "submitter": "Farbod Farshidian", "authors": "Alexander Reske, Jan Carius, Yuntao Ma, Farbod Farshidian, Marco\n  Hutter", "title": "Imitation Learning from MPC for Quadrupedal Multi-Gait Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a learning algorithm for training a single policy that imitates\nmultiple gaits of a walking robot. To achieve this, we use and extend MPC-Net,\nwhich is an Imitation Learning approach guided by Model Predictive Control\n(MPC). The strategy of MPC-Net differs from many other approaches since its\nobjective is to minimize the control Hamiltonian, which derives from the\nprinciple of optimality. To represent the policies, we employ a\nmixture-of-experts network (MEN) and observe that the performance of a policy\nimproves if each expert of a MEN specializes in controlling exactly one mode of\na hybrid system, such as a walking robot. We introduce new loss functions for\nsingle- and multi-gait policies to achieve this kind of expert selection\nbehavior. Moreover, we benchmark our algorithm against Behavioral Cloning and\nthe original MPC implementation on various rough terrain scenarios. We validate\nour approach on hardware and show that a single learned policy can replace its\nteacher to control multiple gaits.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 08:48:53 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Reske", "Alexander", ""], ["Carius", "Jan", ""], ["Ma", "Yuntao", ""], ["Farshidian", "Farbod", ""], ["Hutter", "Marco", ""]]}, {"id": "2103.14337", "submitter": "Yangyang Qin", "authors": "Yangyang Qin, Hefei Ling, Zhenghai He, Yuxuan Shi, Lei Wu", "title": "Hands-on Guidance for Distilling Object Detectors", "comments": "Accepted at ICME2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge distillation can lead to deploy-friendly networks against the\nplagued computational complexity problem, but previous methods neglect the\nfeature hierarchy in detectors. Motivated by this, we propose a general\nframework for detection distillation. Our method, called Hands-on Guidance\nDistillation, distills the latent knowledge of all stage features for imposing\nmore comprehensive supervision, and focuses on the essence simultaneously for\npromoting more intense knowledge absorption. Specifically, a series of novel\nmechanisms are designed elaborately, including correspondence establishment for\nconsistency, hands-on imitation loss measure and re-weighted optimization from\nboth micro and macro perspectives. We conduct extensive evaluations with\ndifferent distillation configurations over VOC and COCO datasets, which show\nbetter performance on accuracy and speed trade-offs. Meanwhile, feasibility\nexperiments on different structural networks further prove the robustness of\nour HGD.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 09:00:23 GMT"}, {"version": "v2", "created": "Wed, 12 May 2021 09:14:27 GMT"}], "update_date": "2021-05-13", "authors_parsed": [["Qin", "Yangyang", ""], ["Ling", "Hefei", ""], ["He", "Zhenghai", ""], ["Shi", "Yuxuan", ""], ["Wu", "Lei", ""]]}, {"id": "2103.14339", "submitter": "Akshay Smit", "authors": "Akshay Smit, Damir Vrabac, Yujie He, Andrew Y. Ng, Andrew L. Beam,\n  Pranav Rajpurkar", "title": "MedSelect: Selective Labeling for Medical Image Classification Combining\n  Meta-Learning with Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a selective learning method using meta-learning and deep\nreinforcement learning for medical image interpretation in the setting of\nlimited labeling resources. Our method, MedSelect, consists of a trainable deep\nlearning selector that uses image embeddings obtained from contrastive\npretraining for determining which images to label, and a non-parametric\nselector that uses cosine similarity to classify unseen images. We demonstrate\nthat MedSelect learns an effective selection strategy outperforming baseline\nselection strategies across seen and unseen medical conditions for chest X-ray\ninterpretation. We also perform an analysis of the selections performed by\nMedSelect comparing the distribution of latent embeddings and clinical\nfeatures, and find significant differences compared to the strongest performing\nbaseline. We believe that our method may be broadly applicable across medical\nimaging settings where labels are expensive to acquire.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 09:09:34 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Smit", "Akshay", ""], ["Vrabac", "Damir", ""], ["He", "Yujie", ""], ["Ng", "Andrew Y.", ""], ["Beam", "Andrew L.", ""], ["Rajpurkar", "Pranav", ""]]}, {"id": "2103.14371", "submitter": "Ramya C", "authors": "Dr. Ramya C, Dr. Shreedhara K S", "title": "A PSO Strategy of Finding Relevant Web Documents using a New Similarity\n  Measure", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.DB", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the world of the Internet and World Wide Web, which offers a tremendous\namount of information, an increasing emphasis is being given to searching\nservices and functionality. Currently, a majority of web portals offer their\nsearching utilities, be it better or worse. These can search for the content\nwithin the sites, mainly text the textual content of documents. In this paper a\nnovel similarity measure called SMDR (Similarity Measure for Documents\nRetrieval) is proposed to help retrieve more similar documents from the\nrepository thus contributing considerably to the effectiveness of Web\nInformation Retrieval (WIR) process. Bio-inspired PSO methodology is used with\nthe intent to reduce the response time of the system and optimizes WIR process,\nhence contributes to the efficiency of the system. This paper also demonstrates\na comparative study of the proposed system with the existing method in terms of\naccuracy, sensitivity, F-measure and specificity. Finally, extensive\nexperiments are conducted on CACM collections. Better precision-recall rates\nare achieved than the existing system. Experimental results demonstrate the\neffectiveness and efficiency of the proposed system.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 10:19:38 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["C", "Dr. Ramya", ""], ["S", "Dr. Shreedhara K", ""]]}, {"id": "2103.14409", "submitter": "Lars Bjertnes", "authors": "Lars Bjertnes, Jacob O. T{\\o}rring, Anne C. Elster", "title": "LS-CAT: A Large-Scale CUDA AutoTuning Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The effectiveness of Machine Learning (ML) methods depend on access to large\nsuitable datasets. In this article, we present how we build the LS-CAT\n(Large-Scale CUDA AutoTuning) dataset sourced from GitHub for the purpose of\ntraining NLP-based ML models. Our dataset includes 19 683 CUDA kernels focused\non linear algebra. In addition to the CUDA codes, our LS-CAT dataset contains 5\n028 536 associated runtimes, with different combinations of kernels, block\nsizes and matrix sizes. The runtime are GPU benchmarks on both Nvidia GTX 980\nand Nvidia T4 systems. This information creates a foundation upon which\nNLP-based models can find correlations between source-code features and optimal\nchoice of thread block sizes.\n  There are several results that can be drawn out of our LS-CAT database. E.g.,\nour experimental results show that an optimal choice in thread block size can\ngain an average of 6% for the average case. We thus also analyze how much\nperformance increase can be achieved in general, finding that in 10% of the\ncases more than 20% performance increase can be achieved by using the optimal\nblock. A description of current and future work is also included.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 11:33:48 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Bjertnes", "Lars", ""], ["T\u00f8rring", "Jacob O.", ""], ["Elster", "Anne C.", ""]]}, {"id": "2103.14410", "submitter": "Tanmoy Chakraborty", "authors": "Ayan Sengupta, William Scott Paka, Suman Roy, Gaurav Ranjan, Tanmoy\n  Chakraborty", "title": "An Embedding-based Joint Sentiment-Topic Model for Short Texts", "comments": "Accepted in International AAAI Conference on Web and Social Media\n  (ICWSM), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Short text is a popular avenue of sharing feedback, opinions and reviews on\nsocial media, e-commerce platforms, etc. Many companies need to extract\nmeaningful information (which may include thematic content as well as semantic\npolarity) out of such short texts to understand users' behaviour. However,\nobtaining high quality sentiment-associated and human interpretable themes\nstill remains a challenge for short texts. In this paper we develop ELJST, an\nembedding enhanced generative joint sentiment-topic model that can discover\nmore coherent and diverse topics from short texts. It uses Markov Random Field\nRegularizer that can be seen as a generalisation of skip-gram based models.\nFurther, it can leverage higher-order semantic information appearing in word\nembedding, such as self-attention weights in graphical models. Our results show\nan average improvement of 10% in topic coherence and 5% in topic\ndiversification over baselines. Finally, ELJST helps understand users'\nbehaviour at more granular levels which can be explained. All these can bring\nsignificant values to the service and healthcare industries often dealing with\ncustomers.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 11:41:21 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Sengupta", "Ayan", ""], ["Paka", "William Scott", ""], ["Roy", "Suman", ""], ["Ranjan", "Gaurav", ""], ["Chakraborty", "Tanmoy", ""]]}, {"id": "2103.14422", "submitter": "Tamir Blum", "authors": "Tamir Blum, Gabin Paillet, Watcharawut Masawat, Mickael Laine and\n  Kazuya Yoshida", "title": "SegVisRL: Visuomotor Development for a Lunar Rover for Hazard Avoidance\n  using Camera Images", "comments": "9 pages including references. 8 images, 2 tables. Workshop submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The visuomotor system of any animal is critical for its survival, and the\ndevelopment of a complex one within humans is large factor in our success as a\nspecies on Earth. This system is an essential part of our ability to adapt to\nour environment. We use this system continuously throughout the day, when\npicking something up, or walking around while avoiding bumping into objects.\nEquipping robots with such capabilities will help produce more intelligent\nlocomotion with the ability to more easily understand their surroundings and to\nmove safely. In particular, such capabilities are desirable for traversing the\nlunar surface, as it is full of hazardous obstacles, such as rocks. These\nobstacles need to be identified and avoided in real time. This paper seeks to\ndemonstrate the development of a visuomotor system within a robot for\nnavigation and obstacle avoidance, with complex rock shaped objects\nrepresenting hazards. Our approach uses deep reinforcement learning with only\nimage data. In this paper, we compare the results from several neural network\narchitectures and a preprocessing methodology which includes producing a\nsegmented image and downsampling.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 12:01:42 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Blum", "Tamir", ""], ["Paillet", "Gabin", ""], ["Masawat", "Watcharawut", ""], ["Laine", "Mickael", ""], ["Yoshida", "Kazuya", ""]]}, {"id": "2103.14434", "submitter": "Javier Segovia Aguas", "authors": "Javier Segovia-Aguas, Sergio Jim\\'enez and Anders Jonsson", "title": "Generalized Planning as Heuristic Search", "comments": "Accepted at ICAPS-21", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although heuristic search is one of the most successful approaches to\nclassical planning, this planning paradigm does not apply straightforwardly to\nGeneralized Planning (GP). Planning as heuristic search traditionally addresses\nthe computation of sequential plans by searching in a grounded state-space. On\nthe other hand GP aims at computing algorithm-like plans, that can branch and\nloop, and that generalize to a (possibly infinite) set of classical planning\ninstances. This paper adapts the planning as heuristic search paradigm to the\nparticularities of GP, and presents the first native heuristic search approach\nto GP. First, the paper defines a novel GP solution space that is independent\nof the number of planning instances in a GP problem, and the size of these\ninstances. Second, the paper defines different evaluation and heuristic\nfunctions for guiding a combinatorial search in our GP solution space. Lastly\nthe paper defines a GP algorithm, called Best-First Generalized Planning\n(BFGP), that implements a best-first search in the solution space guided by our\nevaluation/heuristic functions.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 12:35:10 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Segovia-Aguas", "Javier", ""], ["Jim\u00e9nez", "Sergio", ""], ["Jonsson", "Anders", ""]]}, {"id": "2103.14443", "submitter": "Damai Dai", "authors": "Damai Dai, Hua Zheng, Zhifang Sui, Baobao Chang", "title": "Incorporating Connections Beyond Knowledge Embeddings: A Plug-and-Play\n  Module to Enhance Commonsense Reasoning in Machine Reading Comprehension", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional Machine Reading Comprehension (MRC) has been well-addressed by\npattern matching, but the ability of commonsense reasoning remains a gap\nbetween humans and machines. Previous methods tackle this problem by enriching\nword representations via pre-trained Knowledge Graph Embeddings (KGE). However,\nthey make limited use of a large number of connections between nodes in\nKnowledge Graphs (KG), which could be pivotal cues to build the commonsense\nreasoning chains. In this paper, we propose a Plug-and-play module to\nIncorporatE Connection information for commonsEnse Reasoning (PIECER). Beyond\nenriching word representations with knowledge embeddings, PIECER constructs a\njoint query-passage graph to explicitly guide commonsense reasoning by the\nknowledge-oriented connections between words. Further, PIECER has high\ngeneralizability since it can be plugged into suitable positions in any MRC\nmodel. Experimental results on ReCoRD, a large-scale public MRC dataset\nrequiring commonsense reasoning, show that PIECER introduces stable performance\nimprovements for four representative base MRC models, especially in\nlow-resource settings.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 12:55:19 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Dai", "Damai", ""], ["Zheng", "Hua", ""], ["Sui", "Zhifang", ""], ["Chang", "Baobao", ""]]}, {"id": "2103.14452", "submitter": "Benjamin Alt", "authors": "Benjamin Alt, Darko Katic, Rainer J\\\"akel, Asil Kaan Bozcuoglu,\n  Michael Beetz", "title": "Robot Program Parameter Inference via Differentiable Shadow Program\n  Inversion", "comments": "7 pages, 7 figures, accepted at IEEE International Conference on\n  Robotics and Automation (ICRA), Xi'an, China, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Challenging manipulation tasks can be solved effectively by combining\nindividual robot skills, which must be parameterized for the concrete physical\nenvironment and task at hand. This is time-consuming and difficult for human\nprogrammers, particularly for force-controlled skills. To this end, we present\nShadow Program Inversion (SPI), a novel approach to infer optimal skill\nparameters directly from data. SPI leverages unsupervised learning to train an\nauxiliary differentiable program representation (\"shadow program\") and realizes\nparameter inference via gradient-based model inversion. Our method enables the\nuse of efficient first-order optimizers to infer optimal parameters for\noriginally non-differentiable skills, including many skill variants currently\nused in production. SPI zero-shot generalizes across task objectives, meaning\nthat shadow programs do not need to be retrained to infer parameters for\ndifferent task variants. We evaluate our methods on three different robots and\nskill frameworks in industrial and household scenarios. Code and examples are\navailable at https://innolab.artiminds.com/icra2021.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 13:16:01 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Alt", "Benjamin", ""], ["Katic", "Darko", ""], ["J\u00e4kel", "Rainer", ""], ["Bozcuoglu", "Asil Kaan", ""], ["Beetz", "Michael", ""]]}, {"id": "2103.14453", "submitter": "Markus Bayer", "authors": "Markus Bayer, Marc-Andr\\'e Kaufhold, Bj\\\"orn Buchhold, Marcel Keller,\n  J\\\"org Dallmeyer and Christian Reuter", "title": "Data Augmentation in Natural Language Processing: A Novel Text\n  Generation Approach for Long and Short Text Classifiers", "comments": "20 pages, 1 figure, 5 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many cases of machine learning, research suggests that the development of\ntraining data might have a higher relevance than the choice and modelling of\nclassifiers themselves. Thus, data augmentation methods have been developed to\nimprove classifiers by artificially created training data. In NLP, there is the\nchallenge of establishing universal rules for text transformations which\nprovide new linguistic patterns. In this paper, we present and evaluate a text\ngeneration method suitable to increase the performance of classifiers for long\nand short texts. We achieved promising improvements when evaluating short as\nwell as long text tasks with the enhancement by our text generation method. In\na simulated low data regime additive accuracy gains of up to 15.53% are\nachieved. As the current track of these constructed regimes is not universally\napplicable, we also show major improvements in several real world low data\ntasks (up to +4.84 F1 score). Since we are evaluating the method from many\nperspectives, we also observe situations where the method might not be\nsuitable. We discuss implications and patterns for the successful application\nof our approach on different types of datasets.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 13:16:07 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Bayer", "Markus", ""], ["Kaufhold", "Marc-Andr\u00e9", ""], ["Buchhold", "Bj\u00f6rn", ""], ["Keller", "Marcel", ""], ["Dallmeyer", "J\u00f6rg", ""], ["Reuter", "Christian", ""]]}, {"id": "2103.14465", "submitter": "Kamil Bujel", "authors": "Kamil Bujel, Helen Yannakoudakis, Marek Rei", "title": "Zero-shot Sequence Labeling for Transformer-based Sentence Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We investigate how sentence-level transformers can be modified into effective\nsequence labelers at the token level without any direct supervision. Existing\napproaches to zero-shot sequence labeling do not perform well when applied on\ntransformer-based architectures. As transformers contain multiple layers of\nmulti-head self-attention, information in the sentence gets distributed between\nmany tokens, negatively affecting zero-shot token-level performance. We find\nthat a soft attention module which explicitly encourages sharpness of attention\nweights can significantly outperform existing methods.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 13:35:43 GMT"}, {"version": "v2", "created": "Tue, 8 Jun 2021 19:03:26 GMT"}], "update_date": "2021-06-10", "authors_parsed": [["Bujel", "Kamil", ""], ["Yannakoudakis", "Helen", ""], ["Rei", "Marek", ""]]}, {"id": "2103.14489", "submitter": "Jie Fu", "authors": "Jie Fu", "title": "Probabilistic Planning with Preferences over Temporal Goals", "comments": "6 pages, 8 figures, Accepted by American Control Conference (ACC)\n  2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.FL cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a formal language for specifying qualitative preferences over\ntemporal goals and a preference-based planning method in stochastic systems.\nUsing automata-theoretic modeling, the proposed specification allows us to\nexpress preferences over different sets of outcomes, where each outcome\ndescribes a set of temporal sequences of subgoals. We define the value of\npreference satisfaction given a stochastic process over possible outcomes and\ndevelop an algorithm for time-constrained probabilistic planning in labeled\nMarkov decision processes where an agent aims to maximally satisfy its\npreference formula within a pre-defined finite time duration. We present\nexperimental results using a stochastic gridworld example and discuss possible\nextensions of the proposed preference model.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 14:26:40 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Fu", "Jie", ""]]}, {"id": "2103.14561", "submitter": "Huimin Peng", "authors": "Huimin Peng", "title": "User-Oriented Smart General AI System under Causal Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  General AI system solves a wide range of tasks with high performance in an\nautomated fashion. The best general AI algorithm designed by one individual is\ndifferent from that devised by another. The best performance records achieved\nby different users are also different. An inevitable component of general AI is\ntacit knowledge that depends upon user-specific comprehension of task\ninformation and individual model design preferences that are related to user\ntechnical experiences. Tacit knowledge affects model performance but cannot be\nautomatically optimized in general AI algorithms. In this paper, we propose\nUser-Oriented Smart General AI System under Causal Inference, abbreviated as\nUOGASuCI, where UOGAS represents User-Oriented General AI System and uCI means\nunder the framework of causal inference. User characteristics that have a\nsignificant influence upon tacit knowledge can be extracted from observed model\ntraining experiences of many users in external memory modules. Under the\nframework of causal inference, we manage to identify the optimal value of user\ncharacteristics that are connected with the best model performance designed by\nusers. We make suggestions to users about how different user characteristics\ncan improve the best model performance achieved by users. By recommending\nupdating user characteristics associated with individualized tacit knowledge\ncomprehension and technical preferences, UOGAS helps users design models with\nbetter performance.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 08:34:35 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Peng", "Huimin", ""]]}, {"id": "2103.14586", "submitter": "Daniel Glasner", "authors": "Srinadh Bhojanapalli, Ayan Chakrabarti, Daniel Glasner, Daliang Li,\n  Thomas Unterthiner, Andreas Veit", "title": "Understanding Robustness of Transformers for Image Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep Convolutional Neural Networks (CNNs) have long been the architecture of\nchoice for computer vision tasks. Recently, Transformer-based architectures\nlike Vision Transformer (ViT) have matched or even surpassed ResNets for image\nclassification. However, details of the Transformer architecture -- such as the\nuse of non-overlapping patches -- lead one to wonder whether these networks are\nas robust. In this paper, we perform an extensive study of a variety of\ndifferent measures of robustness of ViT models and compare the findings to\nResNet baselines. We investigate robustness to input perturbations as well as\nrobustness to model perturbations. We find that when pre-trained with a\nsufficient amount of data, ViT models are at least as robust as the ResNet\ncounterparts on a broad range of perturbations. We also find that Transformers\nare robust to the removal of almost any single layer, and that while\nactivations from later layers are highly correlated with each other, they\nnevertheless play an important role in classification.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 16:47:55 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Bhojanapalli", "Srinadh", ""], ["Chakrabarti", "Ayan", ""], ["Glasner", "Daniel", ""], ["Li", "Daliang", ""], ["Unterthiner", "Thomas", ""], ["Veit", "Andreas", ""]]}, {"id": "2103.14610", "submitter": "Daniel Tanneberg", "authors": "Daniel Tanneberg and Kai Ploeger and Elmar Rueckert and Jan Peters", "title": "SKID RAW: Skill Discovery from Raw Trajectories", "comments": "IEEE Robotics and Automation Letters", "journal-ref": null, "doi": "10.1109/LRA.2021.3068891", "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Integrating robots in complex everyday environments requires a multitude of\nproblems to be solved. One crucial feature among those is to equip robots with\na mechanism for teaching them a new task in an easy and natural way. When\nteaching tasks that involve sequences of different skills, with varying order\nand number of these skills, it is desirable to only demonstrate full task\nexecutions instead of all individual skills. For this purpose, we propose a\nnovel approach that simultaneously learns to segment trajectories into\nreoccurring patterns and the skills to reconstruct these patterns from\nunlabelled demonstrations without further supervision. Moreover, the approach\nlearns a skill conditioning that can be used to understand possible sequences\nof skills, a practical mechanism to be used in, for example,\nhuman-robot-interactions for a more intelligent and adaptive robot behaviour.\nThe Bayesian and variational inference based approach is evaluated on synthetic\nand real human demonstrations with varying complexities and dimensionality,\nshowing the successful learning of segmentations and skill libraries from\nunlabelled data.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 17:27:13 GMT"}], "update_date": "2021-03-29", "authors_parsed": [["Tanneberg", "Daniel", ""], ["Ploeger", "Kai", ""], ["Rueckert", "Elmar", ""], ["Peters", "Jan", ""]]}, {"id": "2103.14625", "submitter": "Zijie Wang", "authors": "Zijie J. Wang, Robert Turko, Duen Horng Chau", "title": "Dodrio: Exploring Transformer Models with Interactive Visualization", "comments": "10 pages, 8 figures, Accepted to ACL 2021. For a demo video, see\n  https://youtu.be/qB-T9j7UTgE . For a live demo, see\n  https://poloclub.github.io/dodrio/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Why do large pre-trained transformer-based models perform so well across a\nwide variety of NLP tasks? Recent research suggests the key may lie in\nmulti-headed attention mechanism's ability to learn and represent linguistic\ninformation. Understanding how these models represent both syntactic and\nsemantic knowledge is vital to investigate why they succeed and fail, what they\nhave learned, and how they can improve. We present Dodrio, an open-source\ninteractive visualization tool to help NLP researchers and practitioners\nanalyze attention mechanisms in transformer-based models with linguistic\nknowledge. Dodrio tightly integrates an overview that summarizes the roles of\ndifferent attention heads, and detailed views that help users compare attention\nweights with the syntactic structure and semantic information in the input\ntext. To facilitate the visual comparison of attention weights and linguistic\nknowledge, Dodrio applies different graph visualization techniques to represent\nattention weights scalable to longer input text. Case studies highlight how\nDodrio provides insights into understanding the attention mechanism in\ntransformer-based models. Dodrio is available at\nhttps://poloclub.github.io/dodrio/.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 17:39:37 GMT"}, {"version": "v2", "created": "Mon, 12 Apr 2021 17:42:50 GMT"}, {"version": "v3", "created": "Sat, 5 Jun 2021 14:51:10 GMT"}], "update_date": "2021-06-08", "authors_parsed": [["Wang", "Zijie J.", ""], ["Turko", "Robert", ""], ["Chau", "Duen Horng", ""]]}, {"id": "2103.14651", "submitter": "Limor Gultchin", "authors": "David Watson, Limor Gultchin, Ankur Taly, Luciano Floridi", "title": "Local Explanations via Necessity and Sufficiency: Unifying Theory and\n  Practice", "comments": null, "journal-ref": "37th Conference on Uncertainty in Artificial Intelligence (UAI\n  2021)", "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Necessity and sufficiency are the building blocks of all successful\nexplanations. Yet despite their importance, these notions have been\nconceptually underdeveloped and inconsistently applied in explainable\nartificial intelligence (XAI), a fast-growing research area that is so far\nlacking in firm theoretical foundations. Building on work in logic,\nprobability, and causality, we establish the central role of necessity and\nsufficiency in XAI, unifying seemingly disparate methods in a single formal\nframework. We provide a sound and complete algorithm for computing explanatory\nfactors with respect to a given context, and demonstrate its flexibility and\ncompetitive performance against state of the art alternatives on various tasks.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 01:58:53 GMT"}, {"version": "v2", "created": "Thu, 10 Jun 2021 17:53:48 GMT"}], "update_date": "2021-06-11", "authors_parsed": [["Watson", "David", ""], ["Gultchin", "Limor", ""], ["Taly", "Ankur", ""], ["Floridi", "Luciano", ""]]}, {"id": "2103.14659", "submitter": "Zachary Kenton", "authors": "Zachary Kenton, Tom Everitt, Laura Weidinger, Iason Gabriel, Vladimir\n  Mikulik, Geoffrey Irving", "title": "Alignment of Language Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For artificial intelligence to be beneficial to humans the behaviour of AI\nagents needs to be aligned with what humans want. In this paper we discuss some\nbehavioural issues for language agents, arising from accidental\nmisspecification by the system designer. We highlight some ways that\nmisspecification can occur and discuss some behavioural issues that could arise\nfrom misspecification, including deceptive or manipulative language, and review\nsome approaches for avoiding these issues.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 18:01:48 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Kenton", "Zachary", ""], ["Everitt", "Tom", ""], ["Weidinger", "Laura", ""], ["Gabriel", "Iason", ""], ["Mikulik", "Vladimir", ""], ["Irving", "Geoffrey", ""]]}, {"id": "2103.14660", "submitter": "Dominik M\\\"uller", "authors": "Dominik M\\\"uller, I\\~naki Soto-Rey and Frank Kramer", "title": "Multi-Disease Detection in Retinal Imaging based on Ensembling\n  Heterogeneous Deep Learning Models", "comments": "Code repository: https://github.com/frankkramer-lab/riadd.aucmedi\n  Appendix: https://doi.org/10.5281/zenodo.4573990", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Preventable or undiagnosed visual impairment and blindness affect billion of\npeople worldwide. Automated multi-disease detection models offer great\npotential to address this problem via clinical decision support in diagnosis.\nIn this work, we proposed an innovative multi-disease detection pipeline for\nretinal imaging which utilizes ensemble learning to combine the predictive\ncapabilities of several heterogeneous deep convolutional neural network models.\nOur pipeline includes state-of-the-art strategies like transfer learning, class\nweighting, real-time image augmentation and Focal loss utilization.\nFurthermore, we integrated ensemble learning techniques like heterogeneous deep\nlearning models, bagging via 5-fold cross-validation and stacked logistic\nregression models. Through internal and external evaluation, we were able to\nvalidate and demonstrate high accuracy and reliability of our pipeline, as well\nas the comparability with other state-of-the-art pipelines for retinal disease\nprediction.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 18:02:17 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["M\u00fcller", "Dominik", ""], ["Soto-Rey", "I\u00f1aki", ""], ["Kramer", "Frank", ""]]}, {"id": "2103.14666", "submitter": "Yunlong Song", "authors": "Yunlong Song, HaoChih Lin, Elia Kaufmann, Peter Duerr, Davide\n  Scaramuzza", "title": "Autonomous Overtaking in Gran Turismo Sport Using Curriculum\n  Reinforcement Learning", "comments": "Accepted for publication at the IEEE International Conference on\n  Robotics and Automation (ICRA), Xi An, 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Professional race-car drivers can execute extreme overtaking maneuvers.\nHowever, existing algorithms for autonomous overtaking either rely on\nsimplified assumptions about the vehicle dynamics or try to solve expensive\ntrajectory-optimization problems online. When the vehicle approaches its\nphysical limits, existing model-based controllers struggle to handle highly\nnonlinear dynamics, and cannot leverage the large volume of data generated by\nsimulation or real-world driving. To circumvent these limitations, we propose a\nnew learning-based method to tackle the autonomous overtaking problem. We\nevaluate our approach in the popular car racing game Gran Turismo Sport, which\nis known for its detailed modeling of various cars and tracks. By leveraging\ncurriculum learning, our approach leads to faster convergence as well as\nincreased performance compared to vanilla reinforcement learning. As a result,\nthe trained controller outperforms the built-in model-based game AI and\nachieves comparable overtaking performance with an experienced human driver.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 18:06:50 GMT"}, {"version": "v2", "created": "Sun, 9 May 2021 17:19:03 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Song", "Yunlong", ""], ["Lin", "HaoChih", ""], ["Kaufmann", "Elia", ""], ["Duerr", "Peter", ""], ["Scaramuzza", "Davide", ""]]}, {"id": "2103.14689", "submitter": "Loris Nanni", "authors": "Loris Nanni, Gianluca Maguolo, Alessandra Lumini", "title": "Exploiting Adam-like Optimization Algorithms to Improve the Performance\n  of Convolutional Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Stochastic gradient descent (SGD) is the main approach for training deep\nnetworks: it moves towards the optimum of the cost function by iteratively\nupdating the parameters of a model in the direction of the gradient of the loss\nevaluated on a minibatch. Several variants of SGD have been proposed to make\nadaptive step sizes for each parameter (adaptive gradient) and take into\naccount the previous updates (momentum). Among several alternative of SGD the\nmost popular are AdaGrad, AdaDelta, RMSProp and Adam which scale coordinates of\nthe gradient by square roots of some form of averaging of the squared\ncoordinates in the past gradients and automatically adjust the learning rate on\na parameter basis. In this work, we compare Adam based variants based on the\ndifference between the present and the past gradients, the step size is\nadjusted for each parameter. We run several tests benchmarking proposed methods\nusing medical image data. The experiments are performed using ResNet50\narchitecture neural network. Moreover, we have tested ensemble of networks and\nthe fusion with ResNet50 trained with stochastic gradient descent. To combine\nthe set of ResNet50 the simple sum rule has been applied. Proposed ensemble\nobtains very high performance, it obtains accuracy comparable or better than\nactual state of the art. To improve reproducibility and research efficiency the\nMATLAB source code used for this research is available at GitHub:\nhttps://github.com/LorisNanni.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 18:55:08 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Nanni", "Loris", ""], ["Maguolo", "Gianluca", ""], ["Lumini", "Alessandra", ""]]}, {"id": "2103.14712", "submitter": "Arijit Ray", "authors": "Arijit Ray, Michael Cogswell, Xiao Lin, Kamran Alipour, Ajay\n  Divakaran, Yi Yao, Giedrius Burachas", "title": "Knowing What VQA Does Not: Pointing to Error-Inducing Regions to Improve\n  Explanation Helpfulness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Attention maps, a popular heatmap-based explanation method for Visual\nQuestion Answering (VQA), are supposed to help users understand the model by\nhighlighting portions of the image/question used by the model to infer answers.\nHowever, we see that users are often misled by current attention map\nvisualizations that point to relevant regions despite the model producing an\nincorrect answer. Hence, we propose Error Maps that clarify the error by\nhighlighting image regions where the model is prone to err. Error maps can\nindicate when a correctly attended region may be processed incorrectly leading\nto an incorrect answer, and hence, improve users' understanding of those cases.\nTo evaluate our new explanations, we further introduce a metric that simulates\nusers' interpretation of explanations to evaluate their potential helpfulness\nto understand model correctness. We finally conduct user studies to see that\nour new explanations help users understand model correctness better than\nbaselines by an expected 30% and that our proxy helpfulness metrics correlate\nstrongly ($\\rho$>0.97) with how well users can predict model correctness.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 19:52:32 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 21:15:40 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Ray", "Arijit", ""], ["Cogswell", "Michael", ""], ["Lin", "Xiao", ""], ["Alipour", "Kamran", ""], ["Divakaran", "Ajay", ""], ["Yao", "Yi", ""], ["Burachas", "Giedrius", ""]]}, {"id": "2103.14718", "submitter": "Eshagh Kargar", "authors": "Eshagh Kargar and Ville Kyrki", "title": "Increasing the Efficiency of Policy Learning for Autonomous Vehicles by\n  Multi-Task Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Driving in a dynamic, multi-agent, and complex urban environment is a\ndifficult task requiring a complex decision-making policy. The learning of such\na policy requires a state representation that can encode the entire\nenvironment. Mid-level representations that encode a vehicle's environment as\nimages have become a popular choice. Still, they are quite high-dimensional,\nlimiting their use in data-hungry approaches such as reinforcement learning. In\nthis article, we propose to learn a low-dimensional and rich latent\nrepresentation of the environment by leveraging the knowledge of relevant\nsemantic factors. To do this, we train an encoder-decoder deep neural network\nto predict multiple application-relevant factors such as the trajectories of\nother agents and the ego car. We also propose a hazard signal in addition to\nthe learned latent representation as input to a down-stream policy. We\ndemonstrate that using the multi-head encoder-decoder neural network results in\na more informative representation than a standard single-head model. In\nparticular, the proposed representation learning and the hazard signal help\nreinforcement learning to learn faster, with increased performance and less\ndata than baseline methods.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 20:16:59 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Kargar", "Eshagh", ""], ["Kyrki", "Ville", ""]]}, {"id": "2103.14727", "submitter": "Mohamadreza Ahmadi", "authors": "Mohamadreza Ahmadi, Anushri Dixit, Joel W. Burdick, and Aaron D. Ames", "title": "Risk-Averse Stochastic Shortest Path Planning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.AI cs.SY math.OC math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the stochastic shortest path planning problem in MDPs, i.e., the\nproblem of designing policies that ensure reaching a goal state from a given\ninitial state with minimum accrued cost. In order to account for rare but\nimportant realizations of the system, we consider a nested dynamic coherent\nrisk total cost functional rather than the conventional risk-neutral total\nexpected cost. Under some assumptions, we show that optimal, stationary,\nMarkovian policies exist and can be found via a special Bellman's equation. We\npropose a computational technique based on difference convex programs (DCPs) to\nfind the associated value functions and therefore the risk-averse policies. A\nrover navigation MDP is used to illustrate the proposed methodology with\nconditional-value-at-risk (CVaR) and entropic-value-at-risk (EVaR) coherent\nrisk measures.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 20:49:14 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Ahmadi", "Mohamadreza", ""], ["Dixit", "Anushri", ""], ["Burdick", "Joel W.", ""], ["Ames", "Aaron D.", ""]]}, {"id": "2103.14749", "submitter": "Curtis Northcutt", "authors": "Curtis G. Northcutt, Anish Athalye, Jonas Mueller", "title": "Pervasive Label Errors in Test Sets Destabilize Machine Learning\n  Benchmarks", "comments": null, "journal-ref": "ICLR 2021 RobustML and Weakly Supervised Learning Workshops;\n  NeurIPS 2020 Workshop on Dataset Curation and Security", "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We algorithmically identify label errors in the test sets of 10 of the most\ncommonly-used computer vision, natural language, and audio datasets, and\nsubsequently study the potential for these label errors to affect benchmark\nresults. Errors in test sets are numerous and widespread: we estimate an\naverage of 3.4% errors across the 10 datasets, where for example 2916 label\nerrors comprise 6% of the ImageNet validation set. Putative label errors are\nfound using confident learning and then human-validated via crowdsourcing (54%\nof the algorithmically-flagged candidates are indeed erroneously labeled).\nSurprisingly, we find that lower capacity models may be practically more useful\nthan higher capacity models in real-world datasets with high proportions of\nerroneously labeled data. For example, on ImageNet with corrected labels:\nResNet-18 outperforms ResNet-50 if the prevalence of originally mislabeled test\nexamples increases by just 6%. On CIFAR-10 with corrected labels: VGG-11\noutperforms VGG-19 if the prevalence of originally mislabeled test examples\nincreases by 5%. Traditionally, ML practitioners choose which model to deploy\nbased on test accuracy -- our findings advise caution here, proposing that\njudging models over correctly labeled test sets may be more useful, especially\nfor noisy real-world datasets.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 21:54:36 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 02:32:02 GMT"}, {"version": "v3", "created": "Thu, 8 Apr 2021 19:41:55 GMT"}], "update_date": "2021-04-12", "authors_parsed": [["Northcutt", "Curtis G.", ""], ["Athalye", "Anish", ""], ["Mueller", "Jonas", ""]]}, {"id": "2103.14757", "submitter": "Ikechukwu Onyenwe", "authors": "Chidinma A. Nwafor and Ikechukwu E. Onyenwe", "title": "An Automated Multiple-Choice Question Generation Using Natural Language\n  Processing Techniques", "comments": "Recently accepted by the International Journal on Natural Language\n  Computing (IJNLC) awaiting publication, 11 pages, 4 figures, 5 tables", "journal-ref": "International Journal on Natural Language Computing(IJNLC), April\n  2021", "doi": "10.5121/ijnlc.2021.10201", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic multiple-choice question generation (MCQG) is a useful yet\nchallenging task in Natural Language Processing (NLP). It is the task of\nautomatic generation of correct and relevant questions from textual data.\nDespite its usefulness, manually creating sizeable, meaningful and relevant\nquestions is a time-consuming and challenging task for teachers. In this paper,\nwe present an NLP-based system for automatic MCQG for Computer-Based Testing\nExamination (CBTE).We used NLP technique to extract keywords that are important\nwords in a given lesson material. To validate that the system is not perverse,\nfive lesson materials were used to check the effectiveness and efficiency of\nthe system. The manually extracted keywords by the teacher were compared to the\nauto-generated keywords and the result shows that the system was capable of\nextracting keywords from lesson materials in setting examinable questions. This\noutcome is presented in a user-friendly interface for easy accessibility.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 22:39:59 GMT"}], "update_date": "2021-05-04", "authors_parsed": [["Nwafor", "Chidinma A.", ""], ["Onyenwe", "Ikechukwu E.", ""]]}, {"id": "2103.14770", "submitter": "Artan Sheshmani", "authors": "Artan Sheshmani and Yizhuang You", "title": "Categorical Representation Learning: Morphism is All You Need", "comments": "Fixed some typos. 16 pages. Comments are welcome", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cond-mat.dis-nn cs.AI math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a construction for categorical representation learning and\nintroduce the foundations of \"$\\textit{categorifier}$\". The central theme in\nrepresentation learning is the idea of $\\textbf{everything to vector}$. Every\nobject in a dataset $\\mathcal{S}$ can be represented as a vector in\n$\\mathbb{R}^n$ by an $\\textit{encoding map}$ $E:\n\\mathcal{O}bj(\\mathcal{S})\\to\\mathbb{R}^n$. More importantly, every morphism\ncan be represented as a matrix $E:\n\\mathcal{H}om(\\mathcal{S})\\to\\mathbb{R}^{n}_{n}$. The encoding map $E$ is\ngenerally modeled by a $\\textit{deep neural network}$. The goal of\nrepresentation learning is to design appropriate tasks on the dataset to train\nthe encoding map (assuming that an encoding is optimal if it universally\noptimizes the performance on various tasks). However, the latter is still a\n$\\textit{set-theoretic}$ approach. The goal of the current article is to\npromote the representation learning to a new level via a\n$\\textit{category-theoretic}$ approach. As a proof of concept, we provide an\nexample of a text translator equipped with our technology, showing that our\ncategorical learning model outperforms the current deep learning models by 17\ntimes. The content of the current article is part of the recent US patent\nproposal (patent application number: 63110906).\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 23:47:15 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 17:34:05 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Sheshmani", "Artan", ""], ["You", "Yizhuang", ""]]}, {"id": "2103.14804", "submitter": "Xuejiao Tang", "authors": "Xin Huang, Wenbin Zhang, Yiyi Huang, Xuejiao Tang, Mingli Zhang,\n  Jayachander Surbiryala, Vasileios Iosifidis, Zhen Liu and Ji Zhang", "title": "LSTM Based Sentiment Analysis for Cryptocurrency Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Recent studies in big data analytics and natural language processing develop\nautomatic techniques in analyzing sentiment in the social media information. In\naddition, the growing user base of social media and the high volume of posts\nalso provide valuable sentiment information to predict the price fluctuation of\nthe cryptocurrency. This research is directed to predicting the volatile price\nmovement of cryptocurrency by analyzing the sentiment in social media and\nfinding the correlation between them. While previous work has been developed to\nanalyze sentiment in English social media posts, we propose a method to\nidentify the sentiment of the Chinese social media posts from the most popular\nChinese social media platform Sina-Weibo. We develop the pipeline to capture\nWeibo posts, describe the creation of the crypto-specific sentiment dictionary,\nand propose a long short-term memory (LSTM) based recurrent neural network\nalong with the historical cryptocurrency price movement to predict the price\ntrend for future time frames. The conducted experiments demonstrate the\nproposed approach outperforms the state of the art auto regressive based model\nby 18.5% in precision and 15.4% in recall.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 04:08:37 GMT"}, {"version": "v2", "created": "Sat, 3 Apr 2021 03:22:42 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Huang", "Xin", ""], ["Zhang", "Wenbin", ""], ["Huang", "Yiyi", ""], ["Tang", "Xuejiao", ""], ["Zhang", "Mingli", ""], ["Surbiryala", "Jayachander", ""], ["Iosifidis", "Vasileios", ""], ["Liu", "Zhen", ""], ["Zhang", "Ji", ""]]}, {"id": "2103.14823", "submitter": "Kunpeng Ning", "authors": "Kun-Peng Ning, Hu Xu, Kun Zhu, Sheng-Jun Huang", "title": "Co-Imitation Learning without Expert Demonstration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Imitation learning is a primary approach to improve the efficiency of\nreinforcement learning by exploiting the expert demonstrations. However, in\nmany real scenarios, obtaining expert demonstrations could be extremely\nexpensive or even impossible. To overcome this challenge, in this paper, we\npropose a novel learning framework called Co-Imitation Learning (CoIL) to\nexploit the past good experiences of the agents themselves without expert\ndemonstration. Specifically, we train two different agents via letting each of\nthem alternately explore the environment and exploit the peer agent's\nexperience. While the experiences could be valuable or misleading, we propose\nto estimate the potential utility of each piece of experience with the expected\ngain of the value function. Thus the agents can selectively imitate from each\nother by emphasizing the more useful experiences while filtering out noisy\nones. Experimental results on various tasks show significant superiority of the\nproposed Co-Imitation Learning framework, validating that the agents can\nbenefit from each other without external supervision.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 06:58:40 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Ning", "Kun-Peng", ""], ["Xu", "Hu", ""], ["Zhu", "Kun", ""], ["Huang", "Sheng-Jun", ""]]}, {"id": "2103.14852", "submitter": "Ilhan Aslan", "authors": "Muhammad Mehran Sunny, Moritz Berghofer, Ilhan Aslan", "title": "Towards Tool-Support for Interactive-Machine Learning Applications in\n  the Android Ecosystem", "comments": "4 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Consumer applications are becoming increasingly smarter and most of them have\nto run on device ecosystems. Potential benefits are for example enabling\ncross-device interaction and seamless user experiences. Essential for today's\nsmart solutions with high performance are machine learning models. However,\nthese models are often developed separately by AI engineers for one specific\ndevice and do not consider the challenges and potentials associated with a\ndevice ecosystem in which their models have to run. We believe that there is a\nneed for tool-support for AI engineers to address the challenges of\nimplementing, testing, and deploying machine learning models for a next\ngeneration of smart interactive consumer applications. This paper presents\npreliminary results of a series of inquiries, including interviews with AI\nengineers and experiments for an interactive machine learning use case with a\nSmartwatch and Smartphone. We identified the themes through interviews and\nhands-on experience working on our use case and proposed features, such as data\ncollection from sensors and easy testing of the resources consumption of\nrunning pre-processing code on the target device, which will serve as\ntool-support for AI engineers.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 09:28:40 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Sunny", "Muhammad Mehran", ""], ["Berghofer", "Moritz", ""], ["Aslan", "Ilhan", ""]]}, {"id": "2103.14874", "submitter": "Andrea Bontempelli", "authors": "Andrea Bontempelli, Fausto Giunchiglia, Andrea Passerini, Stefano Teso", "title": "Human-in-the-loop Handling of Knowledge Drift", "comments": "8 pages, figures 4, includes supplementary material (3 pages, 4\n  figures), code: https://gitlab.com/abonte/handling-knowledge-drift", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce and study knowledge drift (KD), a complex form of drift that\noccurs in hierarchical classification. Under KD the vocabulary of concepts,\ntheir individual distributions, and the is-a relations between them can all\nchange over time. The main challenge is that, since the ground-truth concept\nhierarchy is unobserved, it is hard to tell apart different forms of KD. For\ninstance, introducing a new is-a relation between two concepts might be\nconfused with individual changes to those concepts, but it is far from\nequivalent. Failure to identify the right kind of KD compromises the concept\nhierarchy used by the classifier, leading to systematic prediction errors. Our\nkey observation is that in many human-in-the-loop applications (like smart\npersonal assistants) the user knows whether and what kind of drift occurred\nrecently. Motivated by this, we introduce TRCKD, a novel approach that combines\nautomated drift detection and adaptation with an interactive stage in which the\nuser is asked to disambiguate between different kinds of KD. In addition, TRCKD\nimplements a simple but effective knowledge-aware adaptation strategy. Our\nsimulations show that often a handful of queries to the user are enough to\nsubstantially improve prediction performance on both synthetic and realistic\ndata.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 10:20:18 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Bontempelli", "Andrea", ""], ["Giunchiglia", "Fausto", ""], ["Passerini", "Andrea", ""], ["Teso", "Stefano", ""]]}, {"id": "2103.14886", "submitter": "Jenia Jitsev", "authors": "Marcel Aach, Jens Henrik Goebbert, Jenia Jitsev", "title": "Generalization over different cellular automata rules learned by a deep\n  feed-forward neural network", "comments": "Preprint. In submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE nlin.AO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  To test generalization ability of a class of deep neural networks, we\nrandomly generate a large number of different rule sets for 2-D cellular\nautomata (CA), based on John Conway's Game of Life. Using these rules, we\ncompute several trajectories for each CA instance. A deep convolutional\nencoder-decoder network with short and long range skip connections is trained\non various generated CA trajectories to predict the next CA state given its\nprevious states. Results show that the network is able to learn the rules of\nvarious, complex cellular automata and generalize to unseen configurations. To\nsome extent, the network shows generalization to rule sets and neighborhood\nsizes that were not seen during the training at all.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 12:12:07 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Aach", "Marcel", ""], ["Goebbert", "Jens Henrik", ""], ["Jitsev", "Jenia", ""]]}, {"id": "2103.14891", "submitter": "Zijian Gao", "authors": "Zijian Gao, Kele Xu, Bo Ding, Huaimin Wang, Yiying Li, Hongda Jia", "title": "KnowRU: Knowledge Reusing via Knowledge Distillation in Multi-agent\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, deep Reinforcement Learning (RL) algorithms have achieved\ndramatically progress in the multi-agent area. However, training the\nincreasingly complex tasks would be time-consuming and resources-exhausting. To\nalleviate this problem, efficient leveraging the historical experience is\nessential, which is under-explored in previous studies as most of the exiting\nmethods may fail to achieve this goal in a continuously variational system due\nto their complicated design and environmental dynamics. In this paper, we\npropose a method, named \"KnowRU\" for knowledge reusing which can be easily\ndeployed in the majority of the multi-agent reinforcement learning algorithms\nwithout complicated hand-coded design. We employ the knowledge distillation\nparadigm to transfer the knowledge among agents with the goal to accelerate the\ntraining phase for new tasks, while improving the asymptotic performance of\nagents. To empirically demonstrate the robustness and effectiveness of KnowRU,\nwe perform extensive experiments on state-of-the-art multi-agent reinforcement\nlearning (MARL) algorithms on collaborative and competitive scenarios. The\nresults show that KnowRU can outperform the recently reported methods, which\nemphasizes the importance of the proposed knowledge reusing for MARL.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 12:38:01 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Gao", "Zijian", ""], ["Xu", "Kele", ""], ["Ding", "Bo", ""], ["Wang", "Huaimin", ""], ["Li", "Yiying", ""], ["Jia", "Hongda", ""]]}, {"id": "2103.14908", "submitter": "Sungyeon Kim", "authors": "Sungyeon Kim, Dongwon Kim, Minsu Cho, Suha Kwak", "title": "Embedding Transfer with Label Relaxation for Improved Metric Learning", "comments": "Accepted to CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a novel method for embedding transfer, a task of\ntransferring knowledge of a learned embedding model to another. Our method\nexploits pairwise similarities between samples in the source embedding space as\nthe knowledge, and transfers them through a loss used for learning target\nembedding models. To this end, we design a new loss called relaxed contrastive\nloss, which employs the pairwise similarities as relaxed labels for\ninter-sample relations. Our loss provides a rich supervisory signal beyond\nclass equivalence, enables more important pairs to contribute more to training,\nand imposes no restriction on manifolds of target embedding spaces. Experiments\non metric learning benchmarks demonstrate that our method largely improves\nperformance, or reduces sizes and output dimensions of target models\neffectively. We further show that it can be also used to enhance quality of\nself-supervised representation and performance of classification models. In all\nthe experiments, our method clearly outperforms existing embedding transfer\ntechniques.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 13:35:03 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Kim", "Sungyeon", ""], ["Kim", "Dongwon", ""], ["Cho", "Minsu", ""], ["Kwak", "Suha", ""]]}, {"id": "2103.14919", "submitter": "Dongfang Li", "authors": "Dongfang Li, Jingcong Tao, Qingcai Chen, Baotian Hu", "title": "You Can Do Better! If You Elaborate the Reason When Making Prediction", "comments": "14 pages, 1 figure, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural predictive models have achieved remarkable performance improvements in\nvarious natural language processing tasks. However, most neural predictive\nmodels suffer from the lack of explainability of predictions, limiting their\npractical utility. This paper proposes a neural predictive approach to make a\nprediction and generate its corresponding explanation simultaneously. It\nleverages the knowledge entailed in explanations as an additional distillation\nsignal for more efficient learning. We conduct a preliminary study on Chinese\nmedical multiple-choice question answering, English natural language inference,\nand commonsense question answering tasks. The experimental results show that\nthe proposed approach can generate reasonable explanations for its predictions\neven with a small-scale training corpus. The proposed method also achieves\nimproved prediction accuracy on three datasets, which indicates that making\npredictions can benefit from generating the explanation in the decision\nprocess.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 14:55:19 GMT"}, {"version": "v2", "created": "Sat, 29 May 2021 15:53:32 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Li", "Dongfang", ""], ["Tao", "Jingcong", ""], ["Chen", "Qingcai", ""], ["Hu", "Baotian", ""]]}, {"id": "2103.14930", "submitter": "Kai Wang", "authors": "Kai Wang, Yu Liu, Quan Z. Sheng", "title": "High-efficiency Euclidean-based Models for Low-dimensional Knowledge\n  Graph Embeddings", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent knowledge graph embedding (KGE) models based on hyperbolic geometry\nhave shown great potential in a low-dimensional embedding space. However, the\nnecessity of hyperbolic space in KGE is still questionable, because the\ncalculation based on hyperbolic geometry is much more complicated than\nEuclidean operations. In this paper, based on the state-of-the-art\nhyperbolic-based model RotH, we develop two lightweight Euclidean-based models,\ncalled RotL and Rot2L. The RotL model simplifies the hyperbolic operations\nwhile keeping the flexible normalization effect. Utilizing a novel two-layer\nstacked transformation and based on RotL, the Rot2L model obtains an improved\nrepresentation capability, yet costs fewer parameters and calculations than\nRotH. The experiments on link prediction show that Rot2L achieves the\nstate-of-the-art performance on two widely-used datasets in low-dimensional\nknowledge graph embeddings. Furthermore, RotL achieves similar performance as\nRotH but only requires half of the training time.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 15:34:32 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Wang", "Kai", ""], ["Liu", "Yu", ""], ["Sheng", "Quan Z.", ""]]}, {"id": "2103.14950", "submitter": "Michael Green", "authors": "Christoph Salge, Michael Cerny Green, Rodrigo Canaan, Filip Skwarski,\n  Rafael Fritsch, Adrian Brightmoore, Shaofang Ye, Changxing Cao and Julian\n  Togelius", "title": "The AI Settlement Generation Challenge in Minecraft: First Year Report", "comments": "14 pages, 9 figures, published in KI-K\\\"unstliche Intelligenz", "journal-ref": "KI-K\\\"unstliche Intelligenz 2020", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article outlines what we learned from the first year of the AI\nSettlement Generation Competition in Minecraft, a competition about producing\nAI programs that can generate interesting settlements in Minecraft for an\nunseen map. This challenge seeks to focus research into adaptive and holistic\nprocedural content generation. Generating Minecraft towns and villages given\nexisting maps is a suitable task for this, as it requires the generated content\nto be adaptive, functional, evocative and aesthetic at the same time. Here, we\npresent the results from the first iteration of the competition. We discuss the\nevaluation methodology, present the different technical approaches by the\ncompetitors, and outline the open problems.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 17:27:05 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Salge", "Christoph", ""], ["Green", "Michael Cerny", ""], ["Canaan", "Rodrigo", ""], ["Skwarski", "Filip", ""], ["Fritsch", "Rafael", ""], ["Brightmoore", "Adrian", ""], ["Ye", "Shaofang", ""], ["Cao", "Changxing", ""], ["Togelius", "Julian", ""]]}, {"id": "2103.14953", "submitter": "John Jewell", "authors": "John Taylor Jewell, Vahid Reza Khazaie, Yalda Mohsenzadeh", "title": "OLED: One-Class Learned Encoder-Decoder Network with Adversarial Context\n  Masking for Novelty Detection", "comments": "11 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Novelty detection is the task of recognizing samples that do not belong to\nthe distribution of the target class. During training, the novelty class is\nabsent, preventing the use of traditional classification approaches. Deep\nautoencoders have been widely used as a base of many unsupervised novelty\ndetection methods. In particular, context autoencoders have been successful in\nthe novelty detection task because of the more effective representations they\nlearn by reconstructing original images from randomly masked images. However, a\nsignificant drawback of context autoencoders is that random masking fails to\nconsistently cover important structures of the input image, leading to\nsuboptimal representations - especially for the novelty detection task. In this\npaper, to optimize input masking, we have designed a framework consisting of\ntwo competing networks, a Mask Module and a Reconstructor. The Mask Module is a\nconvolutional autoencoder that learns to generate optimal masks that cover the\nmost important parts of images. Alternatively, the Reconstructor is a\nconvolutional encoder-decoder that aims to reconstruct unperturbed images from\nmasked images. The networks are trained in an adversarial manner in which the\nMask Module generates masks that are applied to images given to the\nReconstructor. In this way, the Mask Module seeks to maximize the\nreconstruction error that the Reconstructor is minimizing. When applied to\nnovelty detection, the proposed approach learns semantically richer\nrepresentations compared to context autoencoders and enhances novelty detection\nat test time through more optimal masking. Novelty detection experiments on the\nMNIST and CIFAR-10 image datasets demonstrate the proposed approach's\nsuperiority over cutting-edge methods. In a further experiment on the UCSD\nvideo dataset for novelty detection, the proposed approach achieves\nstate-of-the-art results.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 17:59:40 GMT"}, {"version": "v2", "created": "Thu, 8 Apr 2021 07:29:35 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Jewell", "John Taylor", ""], ["Khazaie", "Vahid Reza", ""], ["Mohsenzadeh", "Yalda", ""]]}, {"id": "2103.14969", "submitter": "Teofilo Zosa", "authors": "Teofilo E. Zosa", "title": "Catalyzing Clinical Diagnostic Pipelines Through Volumetric Medical\n  Image Segmentation Using Deep Neural Networks: Past, Present, & Future", "comments": "Review paper written for the UCSD PhD Research Mastery Exam; June 7,\n  2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.AI cs.CV cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning has made a remarkable impact in the field of natural image\nprocessing over the past decade. Consequently, there is a great deal of\ninterest in replicating this success across unsolved tasks in related domains,\nsuch as medical image analysis. Core to medical image analysis is the task of\nsemantic segmentation which enables various clinical workflows. Due to the\nchallenges inherent in manual segmentation, many decades of research have been\ndevoted to discovering extensible, automated, expert-level segmentation\ntechniques. Given the groundbreaking performance demonstrated by recent neural\nnetwork-based techniques, deep learning seems poised to achieve what classic\nmethods have historically been unable. This paper will briefly overview some of\nthe state-of-the-art (SoTA) neural network-based segmentation algorithms with a\nparticular emphasis on the most recent architectures, comparing and contrasting\nthe contributions and characteristics of each network topology. Using\nultrasonography as a motivating example, it will also demonstrate important\nclinical implications of effective deep learning-based solutions, articulate\nchallenges unique to the modality, and discuss novel approaches developed in\nresponse to those challenges, concluding with the proposal of future directions\nin the field. Given the generally observed ephemerality of the best deep\nlearning approaches (i.e. the extremely quick succession of the SoTA), the main\ncontributions of the paper are its contextualization of modern deep learning\narchitectures with historical background and the elucidation of the current\ntrajectory of volumetric medical image segmentation research.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 19:05:11 GMT"}, {"version": "v2", "created": "Thu, 13 May 2021 00:35:48 GMT"}], "update_date": "2021-05-14", "authors_parsed": [["Zosa", "Teofilo E.", ""]]}, {"id": "2103.14973", "submitter": "Hua Shen", "authors": "Hua Shen, Ting-Hao 'Kenneth' Huang", "title": "Explaining the Road Not Taken", "comments": "Accepted by The 2021 ACM CHI Workshop on Operationalizing\n  Human-Centered Perspectives in Explainable AI (CHI 2021 HCXAI Workshop). For\n  associated website, see https://human-centered-exnlp.github.io", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is unclear if existing interpretations of deep neural network models\nrespond effectively to the needs of users. This paper summarizes the common\nforms of explanations (such as feature attribution, decision rules, or probes)\nused in over 200 recent papers about natural language processing (NLP), and\ncompares them against user questions collected in the XAI Question Bank. We\nfound that although users are interested in explanations for the road not taken\n-- namely, why the model chose one result and not a well-defined, seemly\nsimilar legitimate counterpart -- most model interpretations cannot answer\nthese questions.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 19:47:06 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 04:51:50 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Shen", "Hua", ""], ["Huang", "Ting-Hao 'Kenneth'", ""]]}, {"id": "2103.14986", "submitter": "Ildar Batyrshin Z.", "authors": "Ildar Batyrshin, Luis Alfonso Villa-Vargas, Marco Antonio\n  Ramirez-Salinas, Moises Salinas-Rosales, Nailya Kubysheva", "title": "Generating Negations of Probability Distributions", "comments": "10 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently it was introduced a negation of a probability distribution. The need\nfor such negation arises when a knowledge-based system can use the terms like\nNOT HIGH, where HIGH is represented by a probability distribution (pd). For\nexample, HIGH PROFIT or HIGH PRICE can be considered. The application of this\nnegation in Dempster-Shafer theory was considered in many works. Although\nseveral negations of probability distributions have been proposed, it was not\nclear how to construct other negations. In this paper, we consider negations of\nprobability distributions as point-by-point transformations of pd using\ndecreasing functions defined on [0,1] called negators. We propose the general\nmethod of generation of negators and corresponding negations of pd, and study\ntheir properties. We give a characterization of linear negators as a convex\ncombination of Yager and uniform negators.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 20:24:10 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Batyrshin", "Ildar", ""], ["Villa-Vargas", "Luis Alfonso", ""], ["Ramirez-Salinas", "Marco Antonio", ""], ["Salinas-Rosales", "Moises", ""], ["Kubysheva", "Nailya", ""]]}, {"id": "2103.14991", "submitter": "Min Chen", "authors": "Min Chen and Zhikun Zhang and Tianhao Wang and Michael Backes and\n  Mathias Humbert and Yang Zhang", "title": "Graph Unlearning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The right to be forgotten states that a data subject has the right to erase\ntheir data from an entity storing it. In the context of machine learning (ML),\nit requires the ML model provider to remove the data subject's data from the\ntraining set used to build the ML model, a process known as \\textit{machine\nunlearning}. While straightforward and legitimate, retraining the ML model from\nscratch upon receiving unlearning requests incurs high computational overhead\nwhen the training set is large. To address this issue, a number of approximate\nalgorithms have been proposed in the domain of image and text data, among which\nSISA is the state-of-the-art solution. It randomly partitions the training set\ninto multiple shards and trains a constituent model for each shard. However,\ndirectly applying SISA to the graph data can severely damage the graph\nstructural information, and thereby the resulting ML model utility.\n  In this paper, we propose GraphEraser, a novel machine unlearning method\ntailored to graph data. Its contributions include two novel graph partition\nalgorithms, and a learning-based aggregation method. We conduct extensive\nexperiments on five real-world datasets to illustrate the unlearning efficiency\nand model utility of GraphEraser. We observe that GraphEraser achieves\n2.06$\\times$ (small dataset) to 35.94$\\times$ (large dataset) unlearning time\nimprovement compared to retraining from scratch. On the other hand, GraphEraser\nachieves up to $62.5\\%$ higher F1 score than that of random partitioning. In\naddition, our proposed learning-based aggregation method achieves up to $112\\%$\nhigher F1 score than that of the majority vote aggregation.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 20:38:25 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Chen", "Min", ""], ["Zhang", "Zhikun", ""], ["Wang", "Tianhao", ""], ["Backes", "Michael", ""], ["Humbert", "Mathias", ""], ["Zhang", "Yang", ""]]}, {"id": "2103.14995", "submitter": "Sanjin Gumbarevic", "authors": "Sanjin Gumbarevi\\'c, Bojan Milovanovi\\'c, Mergim Ga\\v{s}i, Marina\n  Bagari\\'c", "title": "Thermal transmittance prediction based on the application of artificial\n  neural networks on heat flux method results", "comments": "Submitted to International Building Physics Conference 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI eess.SP", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep energy renovation of building stock came more into focus in the European\nUnion due to energy efficiency related directives. Many buildings that must\nundergo deep energy renovation are old and may lack design/renovation\ndocumentation, or possible degradation of materials might have occurred in\nbuilding elements over time. Thermal transmittance (i.e. U-value) is one of the\nmost important parameters for determining the transmission heat losses through\nbuilding envelope elements. It depends on the thickness and thermal properties\nof all the materials that form a building element. In-situ U-value can be\ndetermined by ISO 9869-1 standard (Heat Flux Method - HFM). Still, measurement\nduration is one of the reasons why HFM is not widely used in field testing\nbefore the renovation design process commences. This paper analyzes the\npossibility of reducing the measurement time by conducting parallel\nmeasurements with one heat-flux sensor. This parallelization could be achieved\nby applying a specific class of the Artificial Neural Network (ANN) on HFM\nresults to predict unknown heat flux based on collected interior and exterior\nair temperatures. After the satisfying prediction is achieved, HFM sensor can\nbe relocated to another measuring location. Paper shows a comparison of four\nANN cases applied to HFM results for a measurement held on one multi-layer wall\n- multilayer perceptron with three neurons in one hidden layer, long short-term\nmemory with 100 units, gated recurrent unit with 100 units and combination of\n50 long short-term memory units and 50 gated recurrent units. The analysis gave\npromising results in term of predicting the heat flux rate based on the two\ninput temperatures. Additional analysis on another wall showed possible\nlimitations of the method that serves as a direction for further research on\nthis topic.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 21:02:31 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Gumbarevi\u0107", "Sanjin", ""], ["Milovanovi\u0107", "Bojan", ""], ["Ga\u0161i", "Mergim", ""], ["Bagari\u0107", "Marina", ""]]}, {"id": "2103.15004", "submitter": "Carolin Wienrich Prof. Dr.", "authors": "Carolin Wienrich and Marc Erich Latoschik", "title": "eXtended Artificial Intelligence: New Prospects of Human-AI Interaction\n  Research", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Artificial Intelligence (AI) covers a broad spectrum of computational\nproblems and use cases. Many of those implicate profound and sometimes\nintricate questions of how humans interact or should interact with AIs.\nMoreover, many users or future users do have abstract ideas of what AI is,\nsignificantly depending on the specific embodiment of AI applications.\nHuman-centered-design approaches would suggest evaluating the impact of\ndifferent embodiments on human perception of and interaction with AI. An\napproach that is difficult to realize due to the sheer complexity of\napplication fields and embodiments in reality. However, here XR opens new\npossibilities to research human-AI interactions. The article's contribution is\ntwofold: First, it provides a theoretical treatment and model of human-AI\ninteraction based on an XR-AI continuum as a framework for and a perspective of\ndifferent approaches of XR-AI combinations. It motivates XR-AI combinations as\na method to learn about the effects of prospective human-AI interfaces and\nshows why the combination of XR and AI fruitfully contributes to a valid and\nsystematic investigation of human-AI interactions and interfaces. Second, the\narticle provides two exemplary experiments investigating the aforementioned\napproach for two distinct AI-systems. The first experiment reveals an\ninteresting gender effect in human-robot interaction, while the second\nexperiment reveals an Eliza effect of a recommender system. Here the article\nintroduces two paradigmatic implementations of the proposed XR testbed for\nhuman-AI interactions and interfaces and shows how a valid and systematic\ninvestigation can be conducted. In sum, the article opens new perspectives on\nhow XR benefits human-centered AI design and development.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 22:12:06 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 11:18:14 GMT"}, {"version": "v3", "created": "Mon, 5 Apr 2021 16:10:19 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Wienrich", "Carolin", ""], ["Latoschik", "Marc Erich", ""]]}, {"id": "2103.15017", "submitter": "Sergiu Oprea", "authors": "Sergiu Oprea, Giorgos Karvounas, Pablo Martinez-Gonzalez, Nikolaos\n  Kyriazis, Sergio Orts-Escolano, Iason Oikonomidis, Alberto Garcia-Garcia,\n  Aggeliki Tsoli, Jose Garcia-Rodriguez, Antonis Argyros", "title": "H-GAN: the power of GANs in your Hands", "comments": "Paper accepted at The International Joint Conference on Neural\n  Networks (IJCNN) 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present HandGAN (H-GAN), a cycle-consistent adversarial learning approach\nimplementing multi-scale perceptual discriminators. It is designed to translate\nsynthetic images of hands to the real domain. Synthetic hands provide complete\nground-truth annotations, yet they are not representative of the target\ndistribution of real-world data. We strive to provide the perfect blend of a\nrealistic hand appearance with synthetic annotations. Relying on image-to-image\ntranslation, we improve the appearance of synthetic hands to approximate the\nstatistical distribution underlying a collection of real images of hands. H-GAN\ntackles not only the cross-domain tone mapping but also structural differences\nin localized areas such as shading discontinuities. Results are evaluated on a\nqualitative and quantitative basis improving previous works. Furthermore, we\nrelied on the hand classification task to claim our generated hands are\nstatistically similar to the real domain of hands.\n", "versions": [{"version": "v1", "created": "Sat, 27 Mar 2021 23:46:27 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 16:16:41 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Oprea", "Sergiu", ""], ["Karvounas", "Giorgos", ""], ["Martinez-Gonzalez", "Pablo", ""], ["Kyriazis", "Nikolaos", ""], ["Orts-Escolano", "Sergio", ""], ["Oikonomidis", "Iason", ""], ["Garcia-Garcia", "Alberto", ""], ["Tsoli", "Aggeliki", ""], ["Garcia-Rodriguez", "Jose", ""], ["Argyros", "Antonis", ""]]}, {"id": "2103.15049", "submitter": "Song Liu", "authors": "Song Liu and Haoqi Fan and Shengsheng Qian and Yiru Chen and Wenkui\n  Ding and Zhongyuan Wang", "title": "HiT: Hierarchical Transformer with Momentum Contrast for Video-Text\n  Retrieval", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Video-Text Retrieval has been a hot research topic with the explosion of\nmultimedia data on the Internet. Transformer for video-text learning has\nattracted increasing attention due to the promising performance.However,\nexisting cross-modal transformer approaches typically suffer from two major\nlimitations: 1) Limited exploitation of the transformer architecture where\ndifferent layers have different feature characteristics. 2) End-to-end training\nmechanism limits negative interactions among samples in a mini-batch. In this\npaper, we propose a novel approach named Hierarchical Transformer (HiT) for\nvideo-text retrieval. HiT performs hierarchical cross-modal contrastive\nmatching in feature-level and semantic-level to achieve multi-view and\ncomprehensive retrieval results. Moreover, inspired by MoCo, we propose\nMomentum Cross-modal Contrast for cross-modal learning to enable large-scale\nnegative interactions on-the-fly, which contributes to the generation of more\nprecise and discriminative representations. Experimental results on three major\nVideo-Text Retrieval benchmark datasets demonstrate the advantages of our\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 04:52:25 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Liu", "Song", ""], ["Fan", "Haoqi", ""], ["Qian", "Shengsheng", ""], ["Chen", "Yiru", ""], ["Ding", "Wenkui", ""], ["Wang", "Zhongyuan", ""]]}, {"id": "2103.15059", "submitter": "Miao Li", "authors": "Rui Zhang, Bayu Distiawan Trisedy, Miao Li, Yong Jiang, Jianzhong Qi", "title": "A Comprehensive Survey on Knowledge Graph Entity Alignment via\n  Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the last few years, the interest in knowledge bases has grown\nexponentially in both the research community and the industry due to their\nessential role in AI applications. Entity alignment is an important task for\nenriching knowledge bases. This paper provides a comprehensive tutorial-type\nsurvey on representative entity alignment techniques that use the new approach\nof representation learning. We present a framework for capturing the key\ncharacteristics of these techniques, propose two datasets to address the\nlimitation of existing benchmark datasets, and conduct extensive experiments\nusing the proposed datasets. The framework gives a clear picture of how the\ntechniques work. The experiments yield important results about the empirical\nperformance of the techniques and how various factors affect the performance.\nOne important observation not stressed by previous work is that techniques\nmaking good use of attribute triples and relation predicates as features stand\nout as winners.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 06:23:48 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhang", "Rui", ""], ["Trisedy", "Bayu Distiawan", ""], ["Li", "Miao", ""], ["Jiang", "Yong", ""], ["Qi", "Jianzhong", ""]]}, {"id": "2103.15073", "submitter": "Min Wang", "authors": "Min Wang, Shanchen Pang, Tong Ding, Sibo Qiao, Xue Zhai, Shuo Wang,\n  Neal N. Xiong, Zhengwen Huang", "title": "IUP: An Intelligent Utility Prediction Scheme for Solid-State\n  Fermentation in 5G IoT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  At present, SOILD-STATE Fermentation (SSF) is mainly controlled by artificial\nexperience, and the product quality and yield are not stable. Accurately\npredicting the quality and yield of SSF is of great significance for improving\nhuman food security and supply. In this paper, we propose an Intelligent\nUtility Prediction (IUP) scheme for SSF in 5G Industrial Internet of Things\n(IoT), including parameter collection and utility prediction of SSF process.\nThis IUP scheme is based on the environmental perception and intelligent\nlearning algorithms of the 5G Industrial IoT. We build a workflow model based\non rewritable petri net to verify the correctness of the system model function\nand process. In addition, we design a utility prediction model for SSF based on\nthe Generative Adversarial Networks (GAN) and Fully Connected Neural Network\n(FCNN). We design a GAN with constraint of mean square error (MSE-GAN) to solve\nthe problem of few-shot learning of SSF, and then combine with the FCNN to\nrealize the utility prediction (usually use the alcohol) of SSF. Based on the\nproduction of liquor in laboratory, the experiments show that the proposed\nmethod is more accurate than the other prediction methods in the utility\nprediction of SSF, and provide the basis for the numerical analysis of the\nproportion of preconfigured raw materials and the appropriate setting of cellar\ntemperature.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 07:36:48 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Wang", "Min", ""], ["Pang", "Shanchen", ""], ["Ding", "Tong", ""], ["Qiao", "Sibo", ""], ["Zhai", "Xue", ""], ["Wang", "Shuo", ""], ["Xiong", "Neal N.", ""], ["Huang", "Zhengwen", ""]]}, {"id": "2103.15090", "submitter": "Antonios Liapis", "authors": "Konstantinos Sfikas and Antonios Liapis", "title": "Playing Against the Board: Rolling Horizon Evolutionary Algorithms\n  Against Pandemic", "comments": "Accepted to IEEE Transactions on Games, 11 pages, 7 figures. arXiv\n  admin note: text overlap with arXiv:2103.11388", "journal-ref": null, "doi": "10.1109/TG.2021.3069766", "report-no": null, "categories": "cs.AI cs.MA cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Competitive board games have provided a rich and diverse testbed for\nartificial intelligence. This paper contends that collaborative board games\npose a different challenge to artificial intelligence as it must balance\nshort-term risk mitigation with long-term winning strategies. Collaborative\nboard games task all players to coordinate their different powers or pool their\nresources to overcome an escalating challenge posed by the board and a\nstochastic ruleset. This paper focuses on the exemplary collaborative board\ngame Pandemic and presents a rolling horizon evolutionary algorithm designed\nspecifically for this game. The complex way in which the Pandemic game state\nchanges in a stochastic but predictable way required a number of specially\ndesigned forward models, macro-action representations for decision-making, and\nrepair functions for the genetic operations of the evolutionary algorithm.\nVariants of the algorithm which explore optimistic versus pessimistic game\nstate evaluations, different mutation rates and event horizons are compared\nagainst a baseline hierarchical policy agent. Results show that an evolutionary\napproach via short-horizon rollouts can better account for the future dangers\nthat the board may introduce, and guard against them. Results highlight the\ntypes of challenges that collaborative board games pose to artificial\nintelligence, especially for handling multi-player collaboration interactions.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 09:22:10 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Sfikas", "Konstantinos", ""], ["Liapis", "Antonios", ""]]}, {"id": "2103.15100", "submitter": "Benjamin Goertzel", "authors": "Ben Goertzel", "title": "The General Theory of General Intelligence: A Pragmatic Patternist\n  Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A multi-decade exploration into the theoretical foundations of artificial and\nnatural general intelligence, which has been expressed in a series of books and\npapers and used to guide a series of practical and research-prototype software\nsystems, is reviewed at a moderate level of detail. The review covers\nunderlying philosophies (patternist philosophy of mind, foundational\nphenomenological and logical ontology), formalizations of the concept of\nintelligence, and a proposed high level architecture for AGI systems partly\ndriven by these formalizations and philosophies. The implementation of specific\ncognitive processes such as logical reasoning, program learning, clustering and\nattention allocation in the context and language of this high level\narchitecture is considered, as is the importance of a common (e.g. typed\nmetagraph based) knowledge representation for enabling \"cognitive synergy\"\nbetween the various processes. The specifics of human-like cognitive\narchitecture are presented as manifestations of these general principles, and\nkey aspects of machine consciousness and machine ethics are also treated in\nthis context. Lessons for practical implementation of advanced AGI in\nframeworks such as OpenCog Hyperon are briefly considered.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 10:11:25 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 01:30:34 GMT"}, {"version": "v3", "created": "Sun, 4 Apr 2021 04:30:42 GMT"}], "update_date": "2021-04-06", "authors_parsed": [["Goertzel", "Ben", ""]]}, {"id": "2103.15140", "submitter": "Felix Weitk\\\"amper", "authors": "Felix Weitk\\\"amper", "title": "Scaling the weight parameters in Markov logic networks and relational\n  logistic regression models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.LO", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We consider Markov logic networks and relational logistic regression as two\nfundamental representation formalisms in statistical relational artificial\nintelligence that use weighted formulas in their specification. However, Markov\nlogic networks are based on undirected graphs, while relational logistic\nregression is based on directed acyclic graphs. We show that when scaling the\nweight parameters with the domain size, the asymptotic behaviour of a\nrelational logistic regression model is transparently controlled by the\nparameters, and we supply an algorithm to compute asymptotic probabilities. We\nalso show using two examples that this is not true for Markov logic networks.\nWe also discuss using several examples, mainly from the literature, how the\napplication context can help the user to decide when such scaling is\nappropriate and when using the raw unscaled parameters might be preferable. We\nhighlight random sampling as a particularly promising area of application for\nscaled models and expound possible avenues for further research.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 14:24:26 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Weitk\u00e4mper", "Felix", ""]]}, {"id": "2103.15144", "submitter": "Ben Wycliff Mugalu", "authors": "Ben Wycliff Mugalu, Rodrick Calvin Wamala, Jonathan Serugunda, Andrew\n  Katumba", "title": "Face Recognition as a Method of Authentication in a Web-Based System", "comments": "7 pages, 9 figures, National Conference on Communications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Online information systems currently heavily rely on the username and\npassword traditional method for protecting information and controlling access.\nWith the advancement in biometric technology and popularity of fields like AI\nand Machine Learning, biometric security is becoming increasingly popular\nbecause of the usability advantage. This paper reports how machine learning\nbased face recognition can be integrated into a web-based system as a method of\nauthentication to reap the benefits of improved usability. This paper includes\na comparison of combinations of detection and classification algorithms with\nFaceNet for face recognition. The results show that a combination of MTCNN for\ndetection, Facenet for generating embeddings, and LinearSVC for classification\noutperforms other combinations with a 95% accuracy. The resulting classifier is\nintegrated into the web-based system and used for authenticating users.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 14:49:17 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Mugalu", "Ben Wycliff", ""], ["Wamala", "Rodrick Calvin", ""], ["Serugunda", "Jonathan", ""], ["Katumba", "Andrew", ""]]}, {"id": "2103.15171", "submitter": "Ramya Ramakrishnan", "authors": "Ramya Ramakrishnan, Vaibhav Unhelkar, Ece Kamar, Julie Shah", "title": "A Bayesian Approach to Identifying Representational Errors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Trained AI systems and expert decision makers can make errors that are often\ndifficult to identify and understand. Determining the root cause for these\nerrors can improve future decisions. This work presents Generative Error Model\n(GEM), a generative model for inferring representational errors based on\nobservations of an actor's behavior (either simulated agent, robot, or human).\nThe model considers two sources of error: those that occur due to\nrepresentational limitations -- \"blind spots\" -- and non-representational\nerrors, such as those caused by noise in execution or systematic errors present\nin the actor's policy. Disambiguating these two error types allows for targeted\nrefinement of the actor's policy (i.e., representational errors require\nperceptual augmentation, while other errors can be reduced through methods such\nas improved training or attention support). We present a Bayesian inference\nalgorithm for GEM and evaluate its utility in recovering representational\nerrors on multiple domains. Results show that our approach can recover blind\nspots of both reinforcement learning agents as well as human users.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 16:43:01 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Ramakrishnan", "Ramya", ""], ["Unhelkar", "Vaibhav", ""], ["Kamar", "Ece", ""], ["Shah", "Julie", ""]]}, {"id": "2103.15206", "submitter": "Sultan Mahmud", "authors": "Sultan Mahmud, Md. Mohsin, Ijaz Ahmed Khan, Ashraf Uddin Mian, Miah\n  Akib Zaman", "title": "Acceptance of COVID-19 Vaccine and Its Determinants in Bangladesh", "comments": "Original Research Paper, 18 pages, 3 Figures, 4 Tables, 49\n  references, 5 Sections", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Background: Bangladesh govt. launched a nationwide vaccination drive against\nSARS-CoV-2 infection from early February 2021. The objectives of this study\nwere to evaluate the acceptance of the COVID-19 vaccines and examine the\nfactors associated with the acceptance in Bangladesh.\n  Method: In between January 30 to February 6, 2021, we conducted a web-based\nanonymous cross-sectional survey among the Bangladeshi general population. The\nmultivariate logistic regression was used to identify the factors that\ninfluence the acceptance of the COVID-19 vaccination.\n  Results: 61.16% (370/605) of the respondents were willing to accept/take the\nCOVID-19 vaccine. Among the accepted group, only 35.14% showed the willingness\nto take the COVID-19 vaccine immediately, while 64.86% would delay the\nvaccination until they are confirmed about the vaccine's efficacy and safety or\nCOVID-19 become deadlier in Bangladesh. The regression results showed age,\ngender, location (urban/rural), level of education, income, perceived risk of\nbeing infected with COVID-19 in the future, perceived severity of infection,\nhaving previous vaccination experience after age 18, having higher knowledge\nabout COVID-19 and vaccination were significantly associated with the\nacceptance of COVID-19 vaccines.\n  Conclusion: The research reported a high prevalence of COVID-19 vaccine\nrefusal and hesitancy in Bangladesh. To diminish the vaccine hesitancy and\nincrease the uptake, the policymakers need to design a well-researched\nimmunization strategy to remove the vaccination barriers. To improve vaccine\nacceptance among people, false rumors and misconceptions about the COVID-19\nvaccines must be dispelled (especially on the internet) and people must be\nexposed to the actual scientific facts.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 19:37:47 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Mahmud", "Sultan", ""], ["Mohsin", "Md.", ""], ["Khan", "Ijaz Ahmed", ""], ["Mian", "Ashraf Uddin", ""], ["Zaman", "Miah Akib", ""]]}, {"id": "2103.15235", "submitter": "Ning Yu", "authors": "Ning Yu and Timothy Haskins", "title": "KNN, An Underestimated Model for Regional Rainfall Forecasting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Regional rainfall forecasting is an important issue in hydrology and\nmeteorology. This paper aims to design an integrated tool by applying various\nmachine learning algorithms, especially the state-of-the-art deep learning\nalgorithms including Deep Neural Network, Wide Neural Network, Deep and Wide\nNeural Network, Reservoir Computing, Long Short Term Memory, Support Vector\nMachine, K-Nearest Neighbor for forecasting regional precipitations over\ndifferent catchments in Upstate New York. Through the experimental results and\nthe comparison among machine learning models including classification and\nregression, we find that KNN is an outstanding model over other models to\nhandle the uncertainty in the precipitation data. The data normalization\nmethods such as ZScore and MinMax are also evaluated and discussed.\n", "versions": [{"version": "v1", "created": "Sun, 28 Mar 2021 22:25:29 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Yu", "Ning", ""], ["Haskins", "Timothy", ""]]}, {"id": "2103.15261", "submitter": "Vatsal Sharan", "authors": "Atish Agarwala, Abhimanyu Das, Brendan Juba, Rina Panigrahy, Vatsal\n  Sharan, Xin Wang, Qiuyi Zhang", "title": "One Network Fits All? Modular versus Monolithic Task Formulations in\n  Neural Networks", "comments": "30 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Can deep learning solve multiple tasks simultaneously, even when they are\nunrelated and very different? We investigate how the representations of the\nunderlying tasks affect the ability of a single neural network to learn them\njointly. We present theoretical and empirical findings that a single neural\nnetwork is capable of simultaneously learning multiple tasks from a combined\ndata set, for a variety of methods for representing tasks -- for example, when\nthe distinct tasks are encoded by well-separated clusters or decision trees\nover certain task-code attributes. More concretely, we present a novel analysis\nthat shows that families of simple programming-like constructs for the codes\nencoding the tasks are learnable by two-layer neural networks with standard\ntraining. We study more generally how the complexity of learning such combined\ntasks grows with the complexity of the task codes; we find that combining many\ntasks may incur a sample complexity penalty, even though the individual tasks\nare easy to learn. We provide empirical support for the usefulness of the\nlearning bounds by training networks on clusters, decision trees, and SQL-style\naggregation.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 01:16:42 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Agarwala", "Atish", ""], ["Das", "Abhimanyu", ""], ["Juba", "Brendan", ""], ["Panigrahy", "Rina", ""], ["Sharan", "Vatsal", ""], ["Wang", "Xin", ""], ["Zhang", "Qiuyi", ""]]}, {"id": "2103.15294", "submitter": "Bin Liu", "authors": "Bin Liu", "title": "\"Weak AI\" is Likely to Never Become \"Strong AI\", So What is its Greatest\n  Value for us?", "comments": "7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  AI has surpassed humans across a variety of tasks such as image\nclassification, playing games (e.g., go, \"Starcraft\" and poker), and protein\nstructure prediction. However, at the same time, AI is also bearing serious\ncontroversies. Many researchers argue that little substantial progress has been\nmade for AI in recent decades. In this paper, the author (1) explains why\ncontroversies about AI exist; (2) discriminates two paradigms of AI research,\ntermed \"weak AI\" and \"strong AI\" (a.k.a. artificial general intelligence); (3)\nclarifies how to judge which paradigm a research work should be classified\ninto; (4) discusses what is the greatest value of \"weak AI\" if it has no chance\nto develop into \"strong AI\".\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 02:57:48 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Liu", "Bin", ""]]}, {"id": "2103.15316", "submitter": "Jianlin Su", "authors": "Jianlin Su, Jiarun Cao, Weijie Liu, Yangyiwen Ou", "title": "Whitening Sentence Representations for Better Semantics and Faster\n  Retrieval", "comments": "The source code of this paper is available at\n  https://github.com/bojone/BERT-whitening", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Pre-training models such as BERT have achieved great success in many natural\nlanguage processing tasks. However, how to obtain better sentence\nrepresentation through these pre-training models is still worthy to exploit.\nPrevious work has shown that the anisotropy problem is an critical bottleneck\nfor BERT-based sentence representation which hinders the model to fully utilize\nthe underlying semantic features. Therefore, some attempts of boosting the\nisotropy of sentence distribution, such as flow-based model, have been applied\nto sentence representations and achieved some improvement. In this paper, we\nfind that the whitening operation in traditional machine learning can similarly\nenhance the isotropy of sentence representations and achieve competitive\nresults. Furthermore, the whitening technique is also capable of reducing the\ndimensionality of the sentence representation. Our experimental results show\nthat it can not only achieve promising performance but also significantly\nreduce the storage cost and accelerate the model retrieval speed.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 03:51:53 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Su", "Jianlin", ""], ["Cao", "Jiarun", ""], ["Liu", "Weijie", ""], ["Ou", "Yangyiwen", ""]]}, {"id": "2103.15332", "submitter": "Sharada Mohanty", "authors": "Sharada Mohanty, Jyotish Poonganam, Adrien Gaidon, Andrey Kolobov,\n  Blake Wulfe, Dipam Chakraborty, Gra\\v{z}vydas \\v{S}emetulskis, Jo\\~ao\n  Schapke, Jonas Kubilius, Jurgis Pa\\v{s}ukonis, Linas Klimas, Matthew\n  Hausknecht, Patrick MacAlpine, Quang Nhat Tran, Thomas Tumiel, Xiaocheng\n  Tang, Xinwei Chen, Christopher Hesse, Jacob Hilton, William Hebgen Guss,\n  Sahika Genc, John Schulman, Karl Cobbe", "title": "Measuring Sample Efficiency and Generalization in Reinforcement Learning\n  Benchmarks: NeurIPS 2020 Procgen Benchmark", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The NeurIPS 2020 Procgen Competition was designed as a centralized benchmark\nwith clearly defined tasks for measuring Sample Efficiency and Generalization\nin Reinforcement Learning. Generalization remains one of the most fundamental\nchallenges in deep reinforcement learning, and yet we do not have enough\nbenchmarks to measure the progress of the community on Generalization in\nReinforcement Learning. We present the design of a centralized benchmark for\nReinforcement Learning which can help measure Sample Efficiency and\nGeneralization in Reinforcement Learning by doing end to end evaluation of the\ntraining and rollout phases of thousands of user submitted code bases in a\nscalable way. We designed the benchmark on top of the already existing Procgen\nBenchmark by defining clear tasks and standardizing the end to end evaluation\nsetups. The design aims to maximize the flexibility available for researchers\nwho wish to design future iterations of such benchmarks, and yet imposes\nnecessary practical constraints to allow for a system like this to scale. This\npaper presents the competition setup and the details and analysis of the top\nsolutions identified through this setup in context of 2020 iteration of the\ncompetition at NeurIPS.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 05:00:14 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Mohanty", "Sharada", ""], ["Poonganam", "Jyotish", ""], ["Gaidon", "Adrien", ""], ["Kolobov", "Andrey", ""], ["Wulfe", "Blake", ""], ["Chakraborty", "Dipam", ""], ["\u0160emetulskis", "Gra\u017evydas", ""], ["Schapke", "Jo\u00e3o", ""], ["Kubilius", "Jonas", ""], ["Pa\u0161ukonis", "Jurgis", ""], ["Klimas", "Linas", ""], ["Hausknecht", "Matthew", ""], ["MacAlpine", "Patrick", ""], ["Tran", "Quang Nhat", ""], ["Tumiel", "Thomas", ""], ["Tang", "Xiaocheng", ""], ["Chen", "Xinwei", ""], ["Hesse", "Christopher", ""], ["Hilton", "Jacob", ""], ["Guss", "William Hebgen", ""], ["Genc", "Sahika", ""], ["Schulman", "John", ""], ["Cobbe", "Karl", ""]]}, {"id": "2103.15348", "submitter": "Zejiang Shen", "authors": "Zejiang Shen, Ruochen Zhang, Melissa Dell, Benjamin Charles Germain\n  Lee, Jacob Carlson, Weining Li", "title": "LayoutParser: A Unified Toolkit for Deep Learning Based Document Image\n  Analysis", "comments": "Accepted at ICDAR 2021, 16 pages, 6 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recent advances in document image analysis (DIA) have been primarily driven\nby the application of neural networks. Ideally, research outcomes could be\neasily deployed in production and extended for further investigation. However,\nvarious factors like loosely organized codebases and sophisticated model\nconfigurations complicate the easy reuse of important innovations by a wide\naudience. Though there have been on-going efforts to improve reusability and\nsimplify deep learning (DL) model development in disciplines like natural\nlanguage processing and computer vision, none of them are optimized for\nchallenges in the domain of DIA. This represents a major gap in the existing\ntoolkit, as DIA is central to academic research across a wide range of\ndisciplines in the social sciences and humanities. This paper introduces\nlayoutparser, an open-source library for streamlining the usage of DL in DIA\nresearch and applications. The core layoutparser library comes with a set of\nsimple and intuitive interfaces for applying and customizing DL models for\nlayout detection, character recognition, and many other document processing\ntasks. To promote extensibility, layoutparser also incorporates a community\nplatform for sharing both pre-trained models and full document digitization\npipelines. We demonstrate that layoutparser is helpful for both lightweight and\nlarge-scale digitization pipelines in real-word use cases. The library is\npublicly available at https://layout-parser.github.io/.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 05:55:08 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 16:24:36 GMT"}], "update_date": "2021-06-22", "authors_parsed": [["Shen", "Zejiang", ""], ["Zhang", "Ruochen", ""], ["Dell", "Melissa", ""], ["Lee", "Benjamin Charles Germain", ""], ["Carlson", "Jacob", ""], ["Li", "Weining", ""]]}, {"id": "2103.15358", "submitter": "Pengchuan Zhang", "authors": "Pengchuan Zhang, Xiyang Dai, Jianwei Yang, Bin Xiao, Lu Yuan, Lei\n  Zhang, Jianfeng Gao", "title": "Multi-Scale Vision Longformer: A New Vision Transformer for\n  High-Resolution Image Encoding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a new Vision Transformer (ViT) architecture Multi-Scale\nVision Longformer, which significantly enhances the ViT of\n\\cite{dosovitskiy2020image} for encoding high-resolution images using two\ntechniques. The first is the multi-scale model structure, which provides image\nencodings at multiple scales with manageable computational cost. The second is\nthe attention mechanism of vision Longformer, which is a variant of Longformer\n\\cite{beltagy2020longformer}, originally developed for natural language\nprocessing, and achieves a linear complexity w.r.t. the number of input tokens.\nA comprehensive empirical study shows that the new ViT significantly\noutperforms several strong baselines, including the existing ViT models and\ntheir ResNet counterparts, and the Pyramid Vision Transformer from a concurrent\nwork \\cite{wang2021pyramid}, on a range of vision tasks, including image\nclassification, object detection, and segmentation. The models and source code\nare released at \\url{https://github.com/microsoft/vision-longformer}.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 06:23:20 GMT"}, {"version": "v2", "created": "Thu, 27 May 2021 09:02:00 GMT"}], "update_date": "2021-05-28", "authors_parsed": [["Zhang", "Pengchuan", ""], ["Dai", "Xiyang", ""], ["Yang", "Jianwei", ""], ["Xiao", "Bin", ""], ["Yuan", "Lu", ""], ["Zhang", "Lei", ""], ["Gao", "Jianfeng", ""]]}, {"id": "2103.15361", "submitter": "Chen Lyu", "authors": "Chen Lyu, Ruyun Wang, Hongyu Zhang, Hanwen Zhang, Songlin Hu", "title": "Embedding API Dependency Graph for Neural Code Generation", "comments": null, "journal-ref": "Empir Software Eng 26, 61 (2021)", "doi": "10.1007/s10664-021-09968-2", "report-no": null, "categories": "cs.SE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of code generation from textual program descriptions has long\nbeen viewed as a grand challenge in software engineering. In recent years, many\ndeep learning based approaches have been proposed, which can generate a\nsequence of code from a sequence of textual program description. However, the\nexisting approaches ignore the global relationships among API methods, which\nare important for understanding the usage of APIs. In this paper, we propose to\nmodel the dependencies among API methods as an API dependency graph (ADG) and\nincorporate the graph embedding into a sequence-to-sequence (Seq2Seq) model. In\naddition to the existing encoder-decoder structure, a new module named\n``embedder\" is introduced. In this way, the decoder can utilize both global\nstructural dependencies and textual program description to predict the target\ncode. We conduct extensive code generation experiments on three public datasets\nand in two programming languages (Python and Java). Our proposed approach,\ncalled ADG-Seq2Seq, yields significant improvements over existing\nstate-of-the-art methods and maintains its performance as the length of the\ntarget code increases. Extensive ablation tests show that the proposed ADG\nembedding is effective and outperforms the baselines.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 06:26:38 GMT"}], "update_date": "2021-04-23", "authors_parsed": [["Lyu", "Chen", ""], ["Wang", "Ruyun", ""], ["Zhang", "Hongyu", ""], ["Zhang", "Hanwen", ""], ["Hu", "Songlin", ""]]}, {"id": "2103.15367", "submitter": "Xiangyu Zhang", "authors": "Xiangyu Zhang, Zhengming Zhang, and Luxi Yang", "title": "Joint User Association and Power Allocation in Heterogeneous Ultra Dense\n  Network via Semi-Supervised Representation Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Heterogeneous Ultra-Dense Network (HUDN) is one of the vital networking\narchitectures due to its ability to enable higher connectivity density and\nultra-high data rates. Rational user association and power control schedule in\nHUDN can reduce wireless interference. This paper proposes a novel idea for\nresolving the joint user association and power control problem: the optimal\nuser association and Base Station transmit power can be represented by channel\ninformation. Then, we solve this problem by formulating an optimal\nrepresentation function. We model the HUDNs as a heterogeneous graph and train\na Graph Neural Network (GNN) to approach this representation function by using\nsemi-supervised learning, in which the loss function is composed of the\nunsupervised part that helps the GNN approach the optimal representation\nfunction and the supervised part that utilizes the previous experience to\nreduce useless exploration. We separate the learning process into two parts,\nthe generalization-representation learning (GRL) part and the\nspecialization-representation learning (SRL) part, which train the GNN for\nlearning representation for generalized scenario quasi-static user distribution\nscenario, respectively. Simulation results demonstrate that the proposed\nGRL-based solution has higher computational efficiency than the traditional\noptimization algorithm, and the performance of SRL outperforms the GRL.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 06:39:51 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Zhang", "Xiangyu", ""], ["Zhang", "Zhengming", ""], ["Yang", "Luxi", ""]]}, {"id": "2103.15369", "submitter": "Mohammad Keshavarzi", "authors": "Mohammad Keshavarzi, Flaviano Christian Reyes, Ritika Shrivastava,\n  Oladapo Afolabi, Luisa Caldas, Allen Y. Yang", "title": "Contextual Scene Augmentation and Synthesis via GSACNet", "comments": "arXiv admin note: text overlap with arXiv:2009.12395 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Indoor scene augmentation has become an emerging topic in the field of\ncomputer vision and graphics with applications in augmented and virtual\nreality. However, current state-of-the-art systems using deep neural networks\nrequire large datasets for training. In this paper we introduce GSACNet, a\ncontextual scene augmentation system that can be trained with limited scene\npriors. GSACNet utilizes a novel parametric data augmentation method combined\nwith a Graph Attention and Siamese network architecture followed by an\nAutoencoder network to facilitate training with small datasets. We show the\neffectiveness of our proposed system by conducting ablation and comparative\nstudies with alternative systems on the Matterport3D dataset. Our results\nindicate that our scene augmentation outperforms prior art in scene synthesis\nwith limited scene priors available.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 06:47:01 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Keshavarzi", "Mohammad", ""], ["Reyes", "Flaviano Christian", ""], ["Shrivastava", "Ritika", ""], ["Afolabi", "Oladapo", ""], ["Caldas", "Luisa", ""], ["Yang", "Allen Y.", ""]]}, {"id": "2103.15370", "submitter": "Lebin Yu", "authors": "Lebin Yu, Jian Wang and Xudong Zhang", "title": "Robust Reinforcement Learning under model misspecification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning has achieved remarkable performance in a wide range of\ntasks these days. Nevertheless, some unsolved problems limit its applications\nin real-world control. One of them is model misspecification, a situation where\nan agent is trained and deployed in environments with different transition\ndynamics. We propose an novel framework that utilize history trajectory and\nPartial Observable Markov Decision Process Modeling to deal with this dilemma.\nAdditionally, we put forward an efficient adversarial attack method to assist\nrobust training. Our experiments in four gym domains validate the effectiveness\nof our framework.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 06:48:26 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Yu", "Lebin", ""], ["Wang", "Jian", ""], ["Zhang", "Xudong", ""]]}, {"id": "2103.15371", "submitter": "Tiejun Lv", "authors": "Shaoyang Wang and Tiejun Lv and Wei Ni and Norman C. Beaulieu and Y.\n  Jay Guo", "title": "Joint Resource Management for MC-NOMA: A Deep Reinforcement Learning\n  Approach", "comments": "14 pages, 10 figures, Accepted by IEEE Transactions on Wireless\n  Communications", "journal-ref": null, "doi": "10.1109/TWC.2021.3069240", "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a novel and effective deep reinforcement learning\n(DRL)-based approach to addressing joint resource management (JRM) in a\npractical multi-carrier non-orthogonal multiple access (MC-NOMA) system, where\nhardware sensitivity and imperfect successive interference cancellation (SIC)\nare considered. We first formulate the JRM problem to maximize the weighted-sum\nsystem throughput. Then, the JRM problem is decoupled into two iterative\nsubtasks: subcarrier assignment (SA, including user grouping) and power\nallocation (PA). Each subtask is a sequential decision process. Invoking a deep\ndeterministic policy gradient algorithm, our proposed DRL-based JRM (DRL-JRM)\napproach jointly performs the two subtasks, where the optimization objective\nand constraints of the subtasks are addressed by a new joint reward and\ninternal reward mechanism. A multi-agent structure and a convolutional neural\nnetwork are adopted to reduce the complexity of the PA subtask. We also tailor\nthe neural network structure for the stability and convergence of DRL-JRM.\nCorroborated by extensive experiments, the proposed DRL-JRM scheme is superior\nto existing alternatives in terms of system throughput and resistance to\ninterference, especially in the presence of many users and strong inter-cell\ninterference. DRL-JRM can flexibly meet individual service requirements of\nusers.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 06:52:19 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Wang", "Shaoyang", ""], ["Lv", "Tiejun", ""], ["Ni", "Wei", ""], ["Beaulieu", "Norman C.", ""], ["Guo", "Y. Jay", ""]]}, {"id": "2103.15447", "submitter": "Guotong Xue", "authors": "Guotong Xue, Ming Zhong, Jianxin Li, Jia Chen, Chengshuai Zhai,\n  Ruochen Kong", "title": "Dynamic Network Embedding Survey", "comments": "Neurocomputing accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since many real world networks are evolving over time, such as social\nnetworks and user-item networks, there are increasing research efforts on\ndynamic network embedding in recent years. They learn node representations from\na sequence of evolving graphs but not only the latest network, for preserving\nboth structural and temporal information from the dynamic networks. Due to the\nlack of comprehensive investigation of them, we give a survey of dynamic\nnetwork embedding in this paper. Our survey inspects the data model,\nrepresentation learning technique, evaluation and application of current\nrelated works and derives common patterns from them. Specifically, we present\ntwo basic data models, namely, discrete model and continuous model for dynamic\nnetworks. Correspondingly, we summarize two major categories of dynamic network\nembedding techniques, namely, structural-first and temporal-first that are\nadopted by most related works. Then we build a taxonomy that refines the\ncategory hierarchy by typical learning models. The popular experimental data\nsets and applications are also summarized. Lastly, we have a discussion of\nseveral distinct research topics in dynamic network embedding.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 09:27:53 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Xue", "Guotong", ""], ["Zhong", "Ming", ""], ["Li", "Jianxin", ""], ["Chen", "Jia", ""], ["Zhai", "Chengshuai", ""], ["Kong", "Ruochen", ""]]}, {"id": "2103.15452", "submitter": "Xin Mao", "authors": "Xin Mao, Wenting Wang, Yuanbin Wu, Man Lan", "title": "Boosting the Speed of Entity Alignment 10*: Dual Attention Matching\n  Network with Normalized Hard Sample Mining", "comments": "12 pages; Accepted by TheWebConf(WWW) 2021", "journal-ref": null, "doi": "10.1145/3442381.3449897", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Seeking the equivalent entities among multi-source Knowledge Graphs (KGs) is\nthe pivotal step to KGs integration, also known as \\emph{entity alignment}\n(EA). However, most existing EA methods are inefficient and poor in\nscalability. A recent summary points out that some of them even require several\ndays to deal with a dataset containing 200,000 nodes (DWY100K). We believe\nover-complex graph encoder and inefficient negative sampling strategy are the\ntwo main reasons. In this paper, we propose a novel KG encoder -- Dual\nAttention Matching Network (Dual-AMN), which not only models both intra-graph\nand cross-graph information smartly, but also greatly reduces computational\ncomplexity. Furthermore, we propose the Normalized Hard Sample Mining Loss to\nsmoothly select hard negative samples with reduced loss shift. The experimental\nresults on widely used public datasets indicate that our method achieves both\nhigh accuracy and high efficiency. On DWY100K, the whole running process of our\nmethod could be finished in 1,100 seconds, at least 10* faster than previous\nwork. The performances of our method also outperform previous works across all\ndatasets, where Hits@1 and MRR have been improved from 6% to 13%.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 09:35:07 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Mao", "Xin", ""], ["Wang", "Wenting", ""], ["Wu", "Yuanbin", ""], ["Lan", "Man", ""]]}, {"id": "2103.15456", "submitter": "Chih-Hong Cheng", "authors": "Yuhang Chen, Chih-Hong Cheng, Jun Yan, Rongjie Yan", "title": "Monitoring Object Detection Abnormalities via Data-Label and\n  Post-Algorithm Abstractions", "comments": "Work in progress report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While object detection modules are essential functionalities for any\nautonomous vehicle, the performance of such modules that are implemented using\ndeep neural networks can be, in many cases, unreliable. In this paper, we\ndevelop abstraction-based monitoring as a logical framework for filtering\npotentially erroneous detection results. Concretely, we consider two types of\nabstraction, namely data-label abstraction and post-algorithm abstraction.\nOperated on the training dataset, the construction of data-label abstraction\niterates each input, aggregates region-wise information over its associated\nlabels, and stores the vector under a finite history length. Post-algorithm\nabstraction builds an abstract transformer for the tracking algorithm. Elements\nbeing associated together by the abstract transformer can be checked against\nconsistency over their original values. We have implemented the overall\nframework to a research prototype and validated it using publicly available\nobject detection datasets.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 09:40:37 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Chen", "Yuhang", ""], ["Cheng", "Chih-Hong", ""], ["Yan", "Jun", ""], ["Yan", "Rongjie", ""]]}, {"id": "2103.15475", "submitter": "Salomey Osei", "authors": "Paul Azunre, Salomey Osei, Salomey Addo, Lawrence Asamoah Adu-Gyamfi,\n  Stephen Moore, Bernard Adabankah, Bernard Opoku, Clara Asare-Nyarko, Samuel\n  Nyarko, Cynthia Amoaba, Esther Dansoa Appiah, Felix Akwerh, Richard Nii Lante\n  Lawson, Joel Budu, Emmanuel Debrah, Nana Boateng, Wisdom Ofori, Edwin\n  Buabeng-Munkoh, Franklin Adjei, Isaac Kojo Essel Ampomah, Joseph Otoo,\n  Reindorf Borkor, Standylove Birago Mensah, Lucien Mensah, Mark Amoako Marcel,\n  Anokye Acheampong Amponsah, James Ben Hayfron-Acquah", "title": "NLP for Ghanaian Languages", "comments": "6 pages paper; Accepted at AfricaNLP @EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  NLP Ghana is an open-source non-profit organization aiming to advance the\ndevelopment and adoption of state-of-the-art NLP techniques and digital\nlanguage tools to Ghanaian languages and problems. In this paper, we first\npresent the motivation and necessity for the efforts of the organization; by\nintroducing some popular Ghanaian languages while presenting the state of NLP\nin Ghana. We then present the NLP Ghana organization and outline its aims,\nscope of work, some of the methods employed and contributions made thus far in\nthe NLP community in Ghana.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 10:16:52 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 09:01:29 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Azunre", "Paul", ""], ["Osei", "Salomey", ""], ["Addo", "Salomey", ""], ["Adu-Gyamfi", "Lawrence Asamoah", ""], ["Moore", "Stephen", ""], ["Adabankah", "Bernard", ""], ["Opoku", "Bernard", ""], ["Asare-Nyarko", "Clara", ""], ["Nyarko", "Samuel", ""], ["Amoaba", "Cynthia", ""], ["Appiah", "Esther Dansoa", ""], ["Akwerh", "Felix", ""], ["Lawson", "Richard Nii Lante", ""], ["Budu", "Joel", ""], ["Debrah", "Emmanuel", ""], ["Boateng", "Nana", ""], ["Ofori", "Wisdom", ""], ["Buabeng-Munkoh", "Edwin", ""], ["Adjei", "Franklin", ""], ["Ampomah", "Isaac Kojo Essel", ""], ["Otoo", "Joseph", ""], ["Borkor", "Reindorf", ""], ["Mensah", "Standylove Birago", ""], ["Mensah", "Lucien", ""], ["Marcel", "Mark Amoako", ""], ["Amponsah", "Anokye Acheampong", ""], ["Hayfron-Acquah", "James Ben", ""]]}, {"id": "2103.15514", "submitter": "Haomei Duan", "authors": "Haomei Duan and Jinghua Zhu", "title": "Context-aware short-term interest first model for session-based\n  recommendation", "comments": "15 pages,5 figures,8th International Conference on Computer Science\n  and Information Technology (CoSIT 2021), March 27 ~ 28, 2021, Sydney,\n  Australia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the case that user profiles are not available, the recommendation based on\nanonymous session is particularly important, which aims to predict the items\nthat the user may click at the next moment based on the user's access sequence\nover a while. In recent years, with the development of recurrent neural\nnetwork, attention mechanism, and graph neural network, the performance of\nsession-based recommendation has been greatly improved. However, the previous\nmethods did not comprehensively consider the context dependencies and\nshort-term interest first of the session. Therefore, we propose a context-aware\nshort-term interest first model (CASIF).The aim of this paper is improve the\naccuracy of recommendations by combining context and short-term interest. In\nCASIF, we dynamically construct a graph structure for session sequences and\ncapture rich context dependencies via graph neural network (GNN), latent\nfeature vectors are captured as inputs of the next step. Then we build the\nshort-term interest first module, which can to capture the user's general\ninterest from the session in the context of long-term memory, at the same time\nget the user's current interest from the item of the last click. In the end,\nthe short-term and long-term interest are combined as the final interest and\nmultiplied by the candidate vector to obtain the recommendation probability.\nFinally, a large number of experiments on two real-world datasets demonstrate\nthe effectiveness of our proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 11:36:00 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Duan", "Haomei", ""], ["Zhu", "Jinghua", ""]]}, {"id": "2103.15516", "submitter": "Piotr Kicki", "authors": "Piotr Kicki, Krzysztof {\\L}akomy, Ki Myung Brian Lee", "title": "Tuning of extended state observer with neural network-based control\n  performance assessment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.SY cs.AI cs.LG cs.SY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The extended state observer (ESO) is an inherent element of robust\nobserver-based control systems that allows estimating the impact of disturbance\non system dynamics. Proper tuning of ESO parameters is necessary to ensure a\ngood quality of estimated quantities and impacts the overall performance of the\nrobust control structure. In this paper, we propose a neural network (NN) based\ntuning procedure that allows the user to prioritize between selected quality\ncriteria such as the control and observation errors and the specified features\nof the control signal. The designed NN provides an accurate assessment of the\ncontrol system performance and returns a set of ESO parameters that delivers a\nnear-optimal solution to the user-defined cost function. The proposed tuning\nprocedure, using an estimated state from the single closed-loop experiment\nproduces near-optimal ESO gains within seconds.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 11:41:13 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 06:17:22 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Kicki", "Piotr", ""], ["\u0141akomy", "Krzysztof", ""], ["Lee", "Ki Myung Brian", ""]]}, {"id": "2103.15532", "submitter": "See Hian Lee", "authors": "See Hian Lee, Feng Ji, Wee Peng Tay", "title": "Learning on heterogeneous graphs using high-order relations", "comments": null, "journal-ref": null, "doi": "10.1109/ICASSP39728.2021.9413417", "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  A heterogeneous graph consists of different vertices and edges types.\nLearning on heterogeneous graphs typically employs meta-paths to deal with the\nheterogeneity by reducing the graph to a homogeneous network, guide random\nwalks or capture semantics. These methods are however sensitive to the choice\nof meta-paths, with suboptimal paths leading to poor performance. In this\npaper, we propose an approach for learning on heterogeneous graphs without\nusing meta-paths. Specifically, we decompose a heterogeneous graph into\ndifferent homogeneous relation-type graphs, which are then combined to create\nhigher-order relation-type representations. These representations preserve the\nheterogeneity of edges and retain their edge directions while capturing the\ninteraction of different vertex types multiple hops apart. This is then\ncomplemented with attention mechanisms to distinguish the importance of the\nrelation-type based neighbors and the relation-types themselves. Experiments\ndemonstrate that our model generally outperforms other state-of-the-art\nbaselines in the vertex classification task on three commonly studied\nheterogeneous graph datasets.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 12:02:47 GMT"}], "update_date": "2021-05-21", "authors_parsed": [["Lee", "See Hian", ""], ["Ji", "Feng", ""], ["Tay", "Wee Peng", ""]]}, {"id": "2103.15536", "submitter": "Ayan Das", "authors": "Ayan Das, Yongxin Yang, Timothy Hospedales, Tao Xiang and Yi-Zhe Song", "title": "Cloud2Curve: Generation and Vectorization of Parametric Sketches", "comments": "Accepted at CVPR 2021 (Poster)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Analysis of human sketches in deep learning has advanced immensely through\nthe use of waypoint-sequences rather than raster-graphic representations. We\nfurther aim to model sketches as a sequence of low-dimensional parametric\ncurves. To this end, we propose an inverse graphics framework capable of\napproximating a raster or waypoint based stroke encoded as a point-cloud with a\nvariable-degree B\\'ezier curve. Building on this module, we present\nCloud2Curve, a generative model for scalable high-resolution vector sketches\nthat can be trained end-to-end using point-cloud data alone. As a consequence,\nour model is also capable of deterministic vectorization which can map novel\nraster or waypoint based sketches to their corresponding high-resolution\nscalable B\\'ezier equivalent. We evaluate the generation and vectorization\ncapabilities of our model on Quick, Draw! and K-MNIST datasets.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 12:09:42 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Das", "Ayan", ""], ["Yang", "Yongxin", ""], ["Hospedales", "Timothy", ""], ["Xiang", "Tao", ""], ["Song", "Yi-Zhe", ""]]}, {"id": "2103.15551", "submitter": "A. M. Khalili", "authors": "Abdullah Khalili and Abdelhamid Bouchachia", "title": "Toward Building Science Discovery Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The dream of building machines that can do science has inspired scientists\nfor decades. Remarkable advances have been made recently; however, we are still\nfar from achieving this goal. In this paper, we focus on the scientific\ndiscovery process where a high level of reasoning and remarkable\nproblem-solving ability are required. We review different machine learning\ntechniques used in scientific discovery with their limitations. We survey and\ndiscuss the main principles driving the scientific discovery process. These\nprinciples are used in different fields and by different scientists to solve\nproblems and discover new knowledge. We provide many examples of the use of\nthese principles in different fields such as physics, mathematics, and biology.\nWe also review AI systems that attempt to implement some of these principles.\nWe argue that building science discovery machines should be guided by these\nprinciples as an alternative to the dominant approach of current AI systems\nthat focuses on narrow objectives. Building machines that fully incorporate\nthese principles in an automated way might open the doors for many\nadvancements.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 14:04:03 GMT"}, {"version": "v2", "created": "Mon, 5 Apr 2021 15:24:48 GMT"}, {"version": "v3", "created": "Tue, 1 Jun 2021 20:02:36 GMT"}], "update_date": "2021-06-03", "authors_parsed": [["Khalili", "Abdullah", ""], ["Bouchachia", "Abdelhamid", ""]]}, {"id": "2103.15552", "submitter": "Jamie Shelley Mr", "authors": "Jamie Nicholas Shelley, Optishell Consultancy", "title": "Energy Decay Network (EDeN)", "comments": null, "journal-ref": null, "doi": "10.31224/osf.io/dfyzn", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  This paper and accompanying Python and C++ Framework is the product of the\nauthors perceived problems with narrow (Discrimination based) AI. (Artificial\nIntelligence) The Framework attempts to develop a genetic transfer of\nexperience through potential structural expressions using a common\nregulation/exchange value (energy) to create a model whereby neural\narchitecture and all unit processes are co-dependently developed by genetic and\nreal time signal processing influences; successful routes are defined by\nstability of the spike distribution per epoch which is influenced by\ngenetically encoded morphological development biases.These principles are aimed\ntowards creating a diverse and robust network that is capable of adapting to\ngeneral tasks by training within a simulation designed for transfer learning to\nother mediums at scale.\n", "versions": [{"version": "v1", "created": "Wed, 10 Mar 2021 23:17:59 GMT"}, {"version": "v2", "created": "Sun, 11 Jul 2021 22:48:26 GMT"}], "update_date": "2021-07-13", "authors_parsed": [["Shelley", "Jamie Nicholas", ""], ["Consultancy", "Optishell", ""]]}, {"id": "2103.15553", "submitter": "Bradly Alicea", "authors": "Krishna Katyal, Jesse Parent, Bradly Alicea", "title": "Connectionism, Complexity, and Living Systems: a comparison of\n  Artificial and Biological Neural Networks", "comments": "14 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While Artificial Neural Networks (ANNs) have yielded impressive results in\nthe realm of simulated intelligent behavior, it is important to remember that\nthey are but sparse approximations of Biological Neural Networks (BNNs). We go\nbeyond comparison of ANNs and BNNs to introduce principles from BNNs that might\nguide the further development of ANNs as embodied neural models. These\nprinciples include representational complexity, complex network\nstructure/energetics, and robust function. We then consider these principles in\nways that might be implemented in the future development of ANNs. In\nconclusion, we consider the utility of this comparison, particularly in terms\nof building more robust and dynamic ANNs. This even includes constructing a\nmorphology and sensory apparatus to create an embodied ANN, which when\ncomplemented with the organizational and functional advantages of BNNs unlocks\nthe adaptive potential of lifelike networks.\n", "versions": [{"version": "v1", "created": "Mon, 15 Mar 2021 02:52:35 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Katyal", "Krishna", ""], ["Parent", "Jesse", ""], ["Alicea", "Bradly", ""]]}, {"id": "2103.15555", "submitter": "Cm Pintea", "authors": "Oliviu Matei, Erdei Rudolf, Camelia-M. Pintea", "title": "Selective Survey: Most Efficient Models and Solvers for Integrative\n  Multimodal Transport", "comments": "12 pages; Accepted: Informatica (ISSN 0868-4952)", "journal-ref": "Informatica, vol. 32, no. 2, pp. 371-396, 2021", "doi": "10.15388/21-INFOR449", "report-no": null, "categories": "cs.AI cs.CY math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the family of Intelligent Transportation Systems (ITS), Multimodal\nTransport Systems (MMTS) have placed themselves as a mainstream transportation\nmean of our time as a feasible integrative transportation process. The Global\nEconomy progressed with the help of transportation. The volume of goods and\ndistances covered have doubled in the last ten years, so there is a high demand\nof an optimized transportation, fast but with low costs, saving resources but\nalso safe, with low or zero emissions. Thus, it is important to have an\noverview of existing research in this field, to know what was already done and\nwhat is to be studied next. The main objective is to explore a beneficent\nselection of the existing research, methods and information in the field of\nmultimodal transportation research, to identify industry needs and gaps in\nresearch and provide context for future research. The selective survey covers\nmultimodal transport design and optimization in terms of: cost, time, and\nnetwork topology. The multimodal transport theoretical aspects, context and\nresources are also covering various aspects. The survey's selection includes\nnowadays best methods and solvers for Intelligent Transportation Systems (ITS).\nThe gap between theory and real-world applications should be further solved in\norder to optimize the global multimodal transportation system.\n", "versions": [{"version": "v1", "created": "Tue, 16 Mar 2021 08:31:44 GMT"}], "update_date": "2021-07-26", "authors_parsed": [["Matei", "Oliviu", ""], ["Rudolf", "Erdei", ""], ["Pintea", "Camelia-M.", ""]]}, {"id": "2103.15558", "submitter": "Huansheng Ning Prof", "authors": "Wenxi Wang, Huansheng Ning, Feifei Shi, Sahraoui Dhelim, Weishan\n  Zhang, Liming Chen", "title": "A Survey of Hybrid Human-Artificial Intelligence for Social Computing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Along with the development of modern computing technology and social\nsciences, both theoretical research and practical applications of social\ncomputing have been continuously extended. In particular with the boom of\nartificial intelligence (AI), social computing is significantly influenced by\nAI. However, the conventional technologies of AI have drawbacks in dealing with\nmore complicated and dynamic problems. Such deficiency can be rectified by\nhybrid human-artificial intelligence (H-AI) which integrates both human\nintelligence and AI into one unity, forming a new enhanced intelligence. H-AI\nin dealing with social problems shows the advantages that AI can not surpass.\nThis paper firstly introduces the concept of H-AI. AI is the intelligence in\nthe transition stage of H-AI, so the latest research progresses of AI in social\ncomputing are reviewed. Secondly, it summarizes typical challenges faced by AI\nin social computing, and makes it possible to introduce H-AI to solve these\nchallenges. Finally, the paper proposes a holistic framework of social\ncomputing combining with H-AI, which consists of four layers: object layer,\nbase layer, analysis layer, and application layer. It represents H-AI has\nsignificant advantages over AI in solving social problems.\n", "versions": [{"version": "v1", "created": "Wed, 17 Mar 2021 08:39:44 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Wang", "Wenxi", ""], ["Ning", "Huansheng", ""], ["Shi", "Feifei", ""], ["Dhelim", "Sahraoui", ""], ["Zhang", "Weishan", ""], ["Chen", "Liming", ""]]}, {"id": "2103.15559", "submitter": "Antonio Bucchiarone Dr.", "authors": "Antonio Bucchiarone, Antonio Cicchetti, Nelly Bencomo, Enrica Loria,\n  Annapaola Marconi", "title": "Gamified and Self-Adaptive Applications for the Common Good: Research\n  Challenges Ahead", "comments": "7 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivational digital systems offer capabilities to engage and motivate\nend-users to foster behavioral changes towards a common goal. In general these\nsystems use gamification principles in non-games contexts. Over the years,\ngamification has gained consensus among researchers and practitioners as a tool\nto motivate people to perform activities with the ultimate goal of promoting\nbehavioural change, or engaging the users to perform activities that can offer\nrelevant benefits but which can be seen as unrewarding and even tedious.\n  There exists a plethora of heterogeneous application scenarios towards\nreaching the common good that can benefit from gamification. However, an open\nproblem is how to effectively combine multiple motivational campaigns to\nmaximise the degree of participation without exposing the system to\ncounterproductive behaviours.\n  We conceive motivational digital systems as multi-agent systems:\nself-adaptation is a feature of the overall system, while individual agents may\nself-adapt in order to leverage other agents' resources, functionalities and\ncapabilities to perform tasks more efficiently and effectively. Consequently,\nmultiple campaigns can be run and adapted to reach common good. At the same\ntime, agents are grouped into micro-communities in which agents contribute with\ntheir own social capital and leverage others' capabilities to balance their\nweaknesses.\n  In this paper we propose our vision on how the principles at the base of the\nautonomous and multi-agent systems can be exploited to design multi-challenge\nmotivational systems to engage smart communities towards common goals. We\npresent an initial version of a general framework based on the MAPE-K loop and\na set of research challenges that characterise our research roadmap for the\nimplementation of our vision.\n", "versions": [{"version": "v1", "created": "Mon, 22 Mar 2021 18:56:44 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Bucchiarone", "Antonio", ""], ["Cicchetti", "Antonio", ""], ["Bencomo", "Nelly", ""], ["Loria", "Enrica", ""], ["Marconi", "Annapaola", ""]]}, {"id": "2103.15561", "submitter": "Arash Mehrjou", "authors": "Arash Mehrjou, Ashkan Soleymani, Amin Abyaneh, Samir Bhatt, Bernhard\n  Sch\\\"olkopf, Stefan Bauer", "title": "Pyfectious: An individual-level simulator to discover optimal\n  containment polices for epidemic diseases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.AI cs.LG cs.MA cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Simulating the spread of infectious diseases in human communities is critical\nfor predicting the trajectory of an epidemic and verifying various policies to\ncontrol the devastating impacts of the outbreak. Many existing simulators are\nbased on compartment models that divide people into a few subsets and simulate\nthe dynamics among those subsets using hypothesized differential equations.\nHowever, these models lack the requisite granularity to study the effect of\nintelligent policies that influence every individual in a particular way. In\nthis work, we introduce a simulator software capable of modeling a population\nstructure and controlling the disease's propagation at an individualistic\nlevel. In order to estimate the confidence of the conclusions drawn from the\nsimulator, we employ a comprehensive probabilistic approach where the entire\npopulation is constructed as a hierarchical random variable. This approach\nmakes the inferred conclusions more robust against sampling artifacts and gives\nconfidence bounds for decisions based on the simulation results. To showcase\npotential applications, the simulator parameters are set based on the formal\nstatistics of the COVID-19 pandemic, and the outcome of a wide range of control\nmeasures is investigated. Furthermore, the simulator is used as the environment\nof a reinforcement learning problem to find the optimal policies to control the\npandemic. The obtained experimental results indicate the simulator's\nadaptability and capacity in making sound predictions and a successful policy\nderivation example based on real-world data. As an exemplary application, our\nresults show that the proposed policy discovery method can lead to control\nmeasures that produce significantly fewer infected individuals in the\npopulation and protect the health system against saturation.\n", "versions": [{"version": "v1", "created": "Wed, 24 Mar 2021 10:54:46 GMT"}, {"version": "v2", "created": "Wed, 21 Apr 2021 00:13:57 GMT"}], "update_date": "2021-04-22", "authors_parsed": [["Mehrjou", "Arash", ""], ["Soleymani", "Ashkan", ""], ["Abyaneh", "Amin", ""], ["Bhatt", "Samir", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Bauer", "Stefan", ""]]}, {"id": "2103.15571", "submitter": "Xiaosen Wang", "authors": "Xiaosen Wang, Kun He", "title": "Enhancing the Transferability of Adversarial Attacks through Variance\n  Tuning", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are vulnerable to adversarial examples that mislead the\nmodels with imperceptible perturbations. Though adversarial attacks have\nachieved incredible success rates in the white-box setting, most existing\nadversaries often exhibit weak transferability in the black-box setting,\nespecially under the scenario of attacking models with defense mechanisms. In\nthis work, we propose a new method called variance tuning to enhance the class\nof iterative gradient based attack methods and improve their attack\ntransferability. Specifically, at each iteration for the gradient calculation,\ninstead of directly using the current gradient for the momentum accumulation,\nwe further consider the gradient variance of the previous iteration to tune the\ncurrent gradient so as to stabilize the update direction and escape from poor\nlocal optima. Empirical results on the standard ImageNet dataset demonstrate\nthat our method could significantly improve the transferability of\ngradient-based adversarial attacks. Besides, our method could be used to attack\nensemble models or be integrated with various input transformations.\nIncorporating variance tuning with input transformations on iterative\ngradient-based attacks in the multi-model setting, the integrated method could\nachieve an average success rate of 90.1% against nine advanced defense methods,\nimproving the current best attack performance significantly by 85.1% . Code is\navailable at https://github.com/JHL-HUST/VT.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 12:41:55 GMT"}, {"version": "v2", "created": "Tue, 20 Jul 2021 03:30:39 GMT"}], "update_date": "2021-07-21", "authors_parsed": [["Wang", "Xiaosen", ""], ["He", "Kun", ""]]}, {"id": "2103.15575", "submitter": "Benjamin Krarup", "authors": "Benjamin Krarup and Senka Krivic and Daniele Magazzeni and Derek Long\n  and Michael Cashmore and David E. Smith", "title": "Contrastive Explanations of Plans Through Model Restrictions", "comments": "80 pages, 32 figures, 7 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In automated planning, the need for explanations arises when there is a\nmismatch between a proposed plan and the user's expectation. We frame\nExplainable AI Planning in the context of the plan negotiation problem, in\nwhich a succession of hypothetical planning problems are generated and solved.\nThe object of the negotiation is for the user to understand and ultimately\narrive at a satisfactory plan. We present the results of a user study that\ndemonstrates that when users ask questions about plans, those questions are\ncontrastive, i.e. \"why A rather than B?\". We use the data from this study to\nconstruct a taxonomy of user questions that often arise during plan\nnegotiation. We formally define our approach to plan negotiation through model\nrestriction as an iterative process. This approach generates hypothetical\nproblems and contrastive plans by restricting the model through constraints\nimplied by user questions. We formally define model-based compilations in\nPDDL2.1 of each constraint derived from a user question in the taxonomy, and\nempirically evaluate the compilations in terms of computational complexity. The\ncompilations were implemented as part of an explanation framework that employs\niterative model restriction. We demonstrate its benefits in a second user\nstudy.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 12:47:15 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Krarup", "Benjamin", ""], ["Krivic", "Senka", ""], ["Magazzeni", "Daniele", ""], ["Long", "Derek", ""], ["Cashmore", "Michael", ""], ["Smith", "David E.", ""]]}, {"id": "2103.15578", "submitter": "Venkat Margapuri", "authors": "Venkat Margapuri and Mitchell Neilsen", "title": "Classification of Seeds using Domain Randomization on Self-Supervised\n  Learning Frameworks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The first step toward Seed Phenotyping i.e. the comprehensive assessment of\ncomplex seed traits such as growth, development, tolerance, resistance,\necology, yield, and the measurement of pa-rameters that form more complex\ntraits is the identification of seed type. Generally, a plant re-searcher\ninspects the visual attributes of a seed such as size, shape, area, color and\ntexture to identify the seed type, a process that is tedious and\nlabor-intensive. Advances in the areas of computer vision and deep learning\nhave led to the development of convolutional neural networks (CNN) that aid in\nclassification using images. While they classify efficiently, a key bottleneck\nis the need for an extensive amount of labelled data to train the CNN before it\ncan be put to the task of classification. The work leverages the concepts of\nContrastive Learning and Domain Randomi-zation in order to achieve the same.\nBriefly, domain randomization is the technique of applying models trained on\nimages containing simulated objects to real-world objects. The use of synthetic\nimages generated from a representational sample crop of real-world images\nalleviates the need for a large volume of test subjects. As part of the work,\nsynthetic image datasets of five different types of seed images namely, canola,\nrough rice, sorghum, soy and wheat are applied to three different\nself-supervised learning frameworks namely, SimCLR, Momentum Contrast (MoCo)\nand Build Your Own Latent (BYOL) where ResNet-50 is used as the backbone in\neach of the networks. When the self-supervised models are fine-tuned with only\n5% of the labels from the synthetic dataset, results show that MoCo, the model\nthat yields the best performance of the self-supervised learning frameworks in\nquestion, achieves an accuracy of 77% on the test dataset which is only ~13%\nless than the accuracy of 90% achieved by ResNet-50 trained on 100% of the\nlabels.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 12:50:06 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Margapuri", "Venkat", ""], ["Neilsen", "Mitchell", ""]]}, {"id": "2103.15592", "submitter": "Vladimir Ivanov", "authors": "V. K. Ivanov, N .V. Vinogradova, B. V. Palyukh, A. N. Sotnikov", "title": "Current Trends and Applications of Dempster-Shafer Theory (Review)", "comments": "11 pages, in Russian. Artificial intelligence and decision making.\n  2018. N 4", "journal-ref": null, "doi": "10.14357/20718594180403", "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The article provides a review of the publications on the current trends and\ndevelopments in Dempster-Shafer theory and its different applications in\nscience, engineering, and technologies. The review took account of the\nfollowing provisions with a focus on some specific aspects of the theory.\nFirstly, the article considers the research directions whose results are known\nnot only in scientific and academic community but understood by a wide circle\nof potential designers and developers of advanced engineering solutions and\ntechnologies. Secondly, the article shows the theory applications in some\nimportant areas of human activity such as manufacturing systems, diagnostics of\ntechnological processes, materials and products, building and construction,\nproduct quality control, economic and social systems. The particular attention\nis paid to the current state of research in the domains under consideration\nand, thus, the papers published, as a rule, in recent years and presenting the\nachievements of modern research on Dempster-Shafer theory and its application\nare selected and analyzed.\n", "versions": [{"version": "v1", "created": "Fri, 26 Mar 2021 09:37:28 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Ivanov", "V. K.", ""], ["Vinogradova", "N . V.", ""], ["Palyukh", "B. V.", ""], ["Sotnikov", "A. N.", ""]]}, {"id": "2103.15625", "submitter": "Salomey Osei", "authors": "Paul Azunre, Salomey Osei, Salomey Addo, Lawrence Asamoah Adu-Gyamfi,\n  Stephen Moore, Bernard Adabankah, Bernard Opoku, Clara Asare-Nyarko, Samuel\n  Nyarko, Cynthia Amoaba, Esther Dansoa Appiah, Felix Akwerh, Richard Nii Lante\n  Lawson, Joel Budu, Emmanuel Debrah, Nana Boateng, Wisdom Ofori, Edwin\n  Buabeng-Munkoh, Franklin Adjei, Isaac Kojo Essel Ampomah, Joseph Otoo,\n  Reindorf Borkor, Standylove Birago Mensah, Lucien Mensah, Mark Amoako Marcel,\n  Anokye Acheampong Amponsah, James Ben Hayfron-Acquah", "title": "English-Twi Parallel Corpus for Machine Translation", "comments": "9 pages paper, Accepted at African NLP workshop @EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a parallel machine translation training corpus for English and\nAkuapem Twi of 25,421 sentence pairs. We used a transformer-based translator to\ngenerate initial translations in Akuapem Twi, which were later verified and\ncorrected where necessary by native speakers to eliminate any occurrence of\ntranslationese. In addition, 697 higher quality crowd-sourced sentences are\nprovided for use as an evaluation set for downstream Natural Language\nProcessing (NLP) tasks. The typical use case for the larger human-verified\ndataset is for further training of machine translation models in Akuapem Twi.\nThe higher quality 697 crowd-sourced dataset is recommended as a testing\ndataset for machine translation of English to Twi and Twi to English models.\nFurthermore, the Twi part of the crowd-sourced data may also be used for other\ntasks, such as representation learning, classification, etc. We fine-tune the\ntransformer translation model on the training corpus and report benchmarks on\nthe crowd-sourced test set.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 14:04:57 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 07:12:46 GMT"}, {"version": "v3", "created": "Thu, 1 Apr 2021 15:31:23 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Azunre", "Paul", ""], ["Osei", "Salomey", ""], ["Addo", "Salomey", ""], ["Adu-Gyamfi", "Lawrence Asamoah", ""], ["Moore", "Stephen", ""], ["Adabankah", "Bernard", ""], ["Opoku", "Bernard", ""], ["Asare-Nyarko", "Clara", ""], ["Nyarko", "Samuel", ""], ["Amoaba", "Cynthia", ""], ["Appiah", "Esther Dansoa", ""], ["Akwerh", "Felix", ""], ["Lawson", "Richard Nii Lante", ""], ["Budu", "Joel", ""], ["Debrah", "Emmanuel", ""], ["Boateng", "Nana", ""], ["Ofori", "Wisdom", ""], ["Buabeng-Munkoh", "Edwin", ""], ["Adjei", "Franklin", ""], ["Ampomah", "Isaac Kojo Essel", ""], ["Otoo", "Joseph", ""], ["Borkor", "Reindorf", ""], ["Mensah", "Standylove Birago", ""], ["Mensah", "Lucien", ""], ["Marcel", "Mark Amoako", ""], ["Amponsah", "Anokye Acheampong", ""], ["Hayfron-Acquah", "James Ben", ""]]}, {"id": "2103.15670", "submitter": "Rulin Shao", "authors": "Rulin Shao, Zhouxing Shi, Jinfeng Yi, Pin-Yu Chen, Cho-Jui Hsieh", "title": "On the Adversarial Robustness of Visual Transformers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Following the success in advancing natural language processing and\nunderstanding, transformers are expected to bring revolutionary changes to\ncomputer vision. This work provides the first and comprehensive study on the\nrobustness of vision transformers (ViTs) against adversarial perturbations.\nTested on various white-box and transfer attack settings, we find that ViTs\npossess better adversarial robustness when compared with convolutional neural\nnetworks (CNNs). We summarize the following main observations contributing to\nthe improved robustness of ViTs:\n  1) Features learned by ViTs contain less low-level information and are more\ngeneralizable, which contributes to superior robustness against adversarial\nperturbations.\n  2) Introducing convolutional or tokens-to-token blocks for learning low-level\nfeatures in ViTs can improve classification accuracy but at the cost of\nadversarial robustness.\n  3) Increasing the proportion of transformers in the model structure (when the\nmodel consists of both transformer and CNN blocks) leads to better robustness.\nBut for a pure transformer model, simply increasing the size or adding layers\ncannot guarantee a similar effect.\n  4) Pre-training on larger datasets does not significantly improve adversarial\nrobustness though it is critical for training ViTs.\n  5) Adversarial training is also applicable to ViT for training robust models.\n  Furthermore, feature visualization and frequency analysis are conducted for\nexplanation. The results show that ViTs are less sensitive to high-frequency\nperturbations than CNNs and there is a high correlation between how well the\nmodel learns low-level features and its robustness against different\nfrequency-based perturbations.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 14:48:24 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Shao", "Rulin", ""], ["Shi", "Zhouxing", ""], ["Yi", "Jinfeng", ""], ["Chen", "Pin-Yu", ""], ["Hsieh", "Cho-Jui", ""]]}, {"id": "2103.15682", "submitter": "Mathias L\\\"owe", "authors": "Mathias L\\\"owe, Per Lunnemann Hansen, Sebastian Risi", "title": "Rapid Risk Minimization with Bayesian Models Through Deep Learning\n  Approximation", "comments": "8 pages, 3 figures. Accepted for publishing at IJCNN2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a novel combination of Bayesian Models (BMs) and Neural Networks\n(NNs) for making predictions with a minimum expected risk. Our approach\ncombines the best of both worlds, the data efficiency and interpretability of a\nBM with the speed of a NN. For a BM, making predictions with the lowest\nexpected loss requires integrating over the posterior distribution. When exact\ninference of the posterior predictive distribution is intractable,\napproximation methods are typically applied, e.g. Monte Carlo (MC) simulation.\nFor MC, the variance of the estimator decreases with the number of samples -\nbut at the expense of increased computational cost. Our approach removes the\nneed for iterative MC simulation on the CPU at prediction time. In brief, it\nworks by fitting a NN to synthetic data generated using the BM. In a single\nfeed-forward pass, the NN gives a set of point-wise approximations to the BM's\nposterior predictive distribution for a given observation. We achieve risk\nminimized predictions significantly faster than standard methods with a\nnegligible loss on the test dataset. We combine this approach with Active\nLearning to minimize the amount of data required for fitting the NN. This is\ndone by iteratively labeling more data in regions with high predictive\nuncertainty of the NN.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 15:08:25 GMT"}, {"version": "v2", "created": "Wed, 5 May 2021 19:49:00 GMT"}], "update_date": "2021-05-07", "authors_parsed": [["L\u00f6we", "Mathias", ""], ["Hansen", "Per Lunnemann", ""], ["Risi", "Sebastian", ""]]}, {"id": "2103.15692", "submitter": "Samuel Schmidgall", "authors": "Samuel Schmidgall", "title": "Self-Constructing Neural Networks Through Random Mutation", "comments": "Accepted to ICLR 'A Roadmap to Never-Ending RL' (NERL) 2021 Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The search for neural architecture is producing many of the most exciting\nresults in artificial intelligence. It has increasingly become apparent that\ntask-specific neural architecture plays a crucial role for effectively solving\nproblems. This paper presents a simple method for learning neural architecture\nthrough random mutation. This method demonstrates 1) neural architecture may be\nlearned during the agent's lifetime, 2) neural architecture may be constructed\nover a single lifetime without any initial connections or neurons, and 3)\narchitectural modifications enable rapid adaptation to dynamic and novel task\nscenarios. Starting without any neurons or connections, this method constructs\na neural architecture capable of high-performance on several tasks. The\nlifelong learning capabilities of this method are demonstrated in an\nenvironment without episodic resets, even learning with constantly changing\nmorphology, limb disablement, and changing task goals all without losing\nlocomotion capabilities.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 15:27:38 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Schmidgall", "Samuel", ""]]}, {"id": "2103.15721", "submitter": "Kushal Chawla", "authors": "Kushal Chawla, Jaysa Ramirez, Rene Clever, Gale Lucas, Jonathan May,\n  Jonathan Gratch", "title": "CaSiNo: A Corpus of Campsite Negotiation Dialogues for Automatic\n  Negotiation Systems", "comments": "Accepted at NAACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Automated systems that negotiate with humans have broad applications in\npedagogy and conversational AI. To advance the development of practical\nnegotiation systems, we present CaSiNo: a novel corpus of over a thousand\nnegotiation dialogues in English. Participants take the role of campsite\nneighbors and negotiate for food, water, and firewood packages for their\nupcoming trip. Our design results in diverse and linguistically rich\nnegotiations while maintaining a tractable, closed-domain environment. Inspired\nby the literature in human-human negotiations, we annotate persuasion\nstrategies and perform correlation analysis to understand how the dialogue\nbehaviors are associated with the negotiation performance. We further propose\nand evaluate a multi-task framework to recognize these strategies in a given\nutterance. We find that multi-task learning substantially improves the\nperformance for all strategy labels, especially for the ones that are the most\nskewed. We release the dataset, annotations, and the code to propel future work\nin human-machine negotiations: https://github.com/kushalchawla/CaSiNo\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 16:07:25 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 02:36:51 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Chawla", "Kushal", ""], ["Ramirez", "Jaysa", ""], ["Clever", "Rene", ""], ["Lucas", "Gale", ""], ["May", "Jonathan", ""], ["Gratch", "Jonathan", ""]]}, {"id": "2103.15737", "submitter": "Arpit Sharma", "authors": "Pratik Jayarao and Arpit Sharma", "title": "Retraining DistilBERT for a Voice Shopping Assistant by Using Universal\n  Dependencies", "comments": "Published in the Proceedings of The Fourth Workshop on Reasoning and\n  Learning for Human-Machine Dialogues at the Thirty-Fifth AAAI Conference on\n  Artificial Intelligence (AAAI-21)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this work, we retrained the distilled BERT language model for Walmart's\nvoice shopping assistant on retail domain-specific data. We also injected\nuniversal syntactic dependencies to improve the performance of the model\nfurther. The Natural Language Understanding (NLU) components of the voice\nassistants available today are heavily dependent on language models for various\ntasks. The generic language models such as BERT and RoBERTa are useful for\ndomain-independent assistants but have limitations when they cater to a\nspecific domain. For example, in the shopping domain, the token 'horizon' means\na brand instead of its literal meaning. Generic models are not able to capture\nsuch subtleties. So, in this work, we retrained a distilled version of the BERT\nlanguage model on retail domain-specific data for Walmart's voice shopping\nassistant. We also included universal dependency-based features in the\nretraining process further to improve the performance of the model on\ndownstream tasks. We evaluated the performance of the retrained language model\non four downstream tasks, including intent-entity detection, sentiment\nanalysis, voice title shortening and proactive intent suggestion. We observed\nan increase in the performance of all the downstream tasks of up to 1.31% on\naverage.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 16:24:00 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Jayarao", "Pratik", ""], ["Sharma", "Arpit", ""]]}, {"id": "2103.15739", "submitter": "Vivek Nallur", "authors": "Vivek Nallur and Martin Lloyd and Siani Pearson", "title": "Automation: An Essential Component Of Ethical AI?", "comments": "4 pages, 15th Multi Conference on Computer Science and Information\n  Systems, 20-23 July 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Ethics is sometimes considered to be too abstract to be meaningfully\nimplemented in artificial intelligence (AI). In this paper, we reflect on other\naspects of computing that were previously considered to be very abstract. Yet,\nthese are now accepted as being done very well by computers. These tasks have\nranged from multiple aspects of software engineering to mathematics to\nconversation in natural language with humans. This was done by automating the\nsimplest possible step and then building on it to perform more complex tasks.\nWe wonder if ethical AI might be similarly achieved and advocate the process of\nautomation as key step in making AI take ethical decisions. The key\ncontribution of this paper is to reflect on how automation was introduced into\ndomains previously considered too abstract for computers.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 16:25:58 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Nallur", "Vivek", ""], ["Lloyd", "Martin", ""], ["Pearson", "Siani", ""]]}, {"id": "2103.15746", "submitter": "Vivek Nallur", "authors": "Siani Pearson and Martin Lloyd and Vivek Nallur", "title": "Towards An Ethics-Audit Bot", "comments": "5 pages, short paper, 15th Multi Conference on Computer Science and\n  Information Systems, 20-23 July 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  In this paper we focus on artificial intelligence (AI) for governance, not\ngovernance for AI, and on just one aspect of governance, namely ethics audit.\nDifferent kinds of ethical audit bots are possible, but who makes the choices\nand what are the implications? In this paper, we do not provide\nethical/philosophical solutions, but rather focus on the technical aspects of\nwhat an AI-based solution for validating the ethical soundness of a target\nsystem would be like. We propose a system that is able to conduct an ethical\naudit of a target system, given certain socio-technical conditions. To be more\nspecific, we propose the creation of a bot that is able to support\norganisations in ensuring that their software development lifecycles contain\nprocesses that meet certain ethical standards.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 16:33:22 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Pearson", "Siani", ""], ["Lloyd", "Martin", ""], ["Nallur", "Vivek", ""]]}, {"id": "2103.15750", "submitter": "Callie Hao", "authors": "Cong Hao, Jordan Dotzel, Jinjun Xiong, Luca Benini, Zhiru Zhang,\n  Deming Chen", "title": "Enabling Design Methodologies and Future Trends for Edge AI:\n  Specialization and Co-design", "comments": "Accepted by IEEE Design & Test (D&T)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial intelligence (AI) technologies have dramatically advanced in\nrecent years, resulting in revolutionary changes in people's lives. Empowered\nby edge computing, AI workloads are migrating from centralized cloud\narchitectures to distributed edge systems, introducing a new paradigm called\nedge AI. While edge AI has the promise of bringing significant increases in\nautonomy and intelligence into everyday lives through common edge devices, it\nalso raises new challenges, especially for the development of its algorithms\nand the deployment of its services, which call for novel design methodologies\ncatered to these unique challenges. In this paper, we provide a comprehensive\nsurvey of the latest enabling design methodologies that span the entire edge AI\ndevelopment stack. We suggest that the key methodologies for effective edge AI\ndevelopment are single-layer specialization and cross-layer co-design. We\ndiscuss representative methodologies in each category in detail, including\non-device training methods, specialized software design, dedicated hardware\ndesign, benchmarking and design automation, software/hardware co-design,\nsoftware/compiler co-design, and compiler/hardware co-design. Moreover, we\nattempt to reveal hidden cross-layer design opportunities that can further\nboost the solution quality of future edge AI and provide insights into future\ndirections and emerging areas that require increased research focus.\n", "versions": [{"version": "v1", "created": "Thu, 25 Mar 2021 16:29:55 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 15:04:40 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Hao", "Cong", ""], ["Dotzel", "Jordan", ""], ["Xiong", "Jinjun", ""], ["Benini", "Luca", ""], ["Zhang", "Zhiru", ""], ["Chen", "Deming", ""]]}, {"id": "2103.15758", "submitter": "Sebastian Weichwald", "authors": "Eigil F. Rischel, Sebastian Weichwald", "title": "Compositional Abstraction Error and a Category of Causal Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.LO math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Interventional causal models describe joint distributions over some variables\nused to describe a system, one for each intervention setting. They provide a\nformal recipe for how to move between joint distributions and make predictions\nabout the variables upon intervening on the system. Yet, it is difficult to\nformalise how we may change the underlying variables used to describe the\nsystem, say from fine-grained to coarse-grained variables. Here, we argue that\ncompositionality is a desideratum for model transformations and the associated\nerrors. We develop a framework for model transformations and abstractions with\na notion of error that is compositional: when abstracting a reference model M\nmodularly, first obtaining M' and then further simplifying that to obtain M'',\nthen the composite transformation from M to M'' exists and its error can be\nbounded by the errors incurred by each individual transformation step. Category\ntheory, the study of mathematical objects via the compositional transformations\nbetween them, offers a natural language for developing our framework. We\nintroduce a category of finite interventional causal models and, leveraging\ntheory of enriched categories, prove that our framework enjoys the desired\ncompositionality properties.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 16:48:12 GMT"}], "update_date": "2021-04-09", "authors_parsed": [["Rischel", "Eigil F.", ""], ["Weichwald", "Sebastian", ""]]}, {"id": "2103.15764", "submitter": "Usha Lokala", "authors": "Usha Lokala, Francois Lamy, Triyasha Ghosh Dastidar, Kaushik Roy,\n  Raminta Daniulaityte, Srinivasan Parthasarathy, Amit Sheth", "title": "eDarkTrends: Harnessing Social Media Trends in Substance use disorders\n  for Opioid Listings on Cryptomarket", "comments": "6 pages, ICLR AI for Public Health Workshop 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Opioid and substance misuse is rampant in the United States today, with the\nphenomenon known as the opioid crisis. The relationship between substance use\nand mental health has been extensively studied, with one possible relationship\nbeing substance misuse causes poor mental health. However, the lack of evidence\non the relationship has resulted in opioids being largely inaccessible through\nlegal means. This study analyzes the substance misuse posts on social media\nwith the opioids being sold through crypto market listings. We use the Drug\nAbuse Ontology, state-of-the-art deep learning, and BERT-based models to\ngenerate sentiment and emotion for the social media posts to understand user\nperception on social media by investigating questions such as, which synthetic\nopioids people are optimistic, neutral, or negative about or what kind of drugs\ninduced fear and sorrow or what kind of drugs people love or thankful about or\nwhich drug people think negatively about or which opioids cause little to no\nsentimental reaction. We also perform topic analysis associated with the\ngenerated sentiments and emotions to understand which topics correlate with\npeople's responses to various drugs. Our findings can help shape policy to help\nisolate opioid use cases where timely intervention may be required to prevent\nadverse consequences, prevent overdose-related deaths, and worsen the epidemic.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 16:58:26 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Lokala", "Usha", ""], ["Lamy", "Francois", ""], ["Dastidar", "Triyasha Ghosh", ""], ["Roy", "Kaushik", ""], ["Daniulaityte", "Raminta", ""], ["Parthasarathy", "Srinivasan", ""], ["Sheth", "Amit", ""]]}, {"id": "2103.15792", "submitter": "Dimitrios Kollias", "authors": "Dimitrios Kollias and Stefanos Zafeiriou", "title": "Affect Analysis in-the-wild: Valence-Arousal, Expressions, Action Units\n  and a Unified Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Affect recognition based on subjects' facial expressions has been a topic of\nmajor research in the attempt to generate machines that can understand the way\nsubjects feel, act and react. In the past, due to the unavailability of large\namounts of data captured in real-life situations, research has mainly focused\non controlled environments. However, recently, social media and platforms have\nbeen widely used. Moreover, deep learning has emerged as a means to solve\nvisual analysis and recognition problems. This paper exploits these advances\nand presents significant contributions for affect analysis and recognition\nin-the-wild. Affect analysis and recognition can be seen as a dual knowledge\ngeneration problem, involving: i) creation of new, large and rich in-the-wild\ndatabases and ii) design and training of novel deep neural architectures that\nare able to analyse affect over these databases and to successfully generalise\ntheir performance on other datasets. The paper focuses on large in-the-wild\ndatabases, i.e., Aff-Wild and Aff-Wild2 and presents the design of two classes\nof deep neural networks trained with these databases. The first class refers to\nuni-task affect recognition, focusing on prediction of the valence and arousal\ndimensional variables. The second class refers to estimation of all main\nbehavior tasks, i.e. valence-arousal prediction; categorical emotion\nclassification in seven basic facial expressions; facial Action Unit detection.\nA novel multi-task and holistic framework is presented which is able to jointly\nlearn and effectively generalize and perform affect recognition over all\nexisting in-the-wild databases. Large experimental studies illustrate the\nachieved performance improvement over the existing state-of-the-art in affect\nrecognition.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 17:36:20 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Kollias", "Dimitrios", ""], ["Zafeiriou", "Stefanos", ""]]}, {"id": "2103.15793", "submitter": "Arthur Allshire", "authors": "Arthur Allshire, Roberto Mart\\'in-Mart\\'in, Charles Lin, Shawn Manuel,\n  Silvio Savarese, Animesh Garg", "title": "LASER: Learning a Latent Action Space for Efficient Reinforcement\n  Learning", "comments": "Accepted as a conference paper at ICRA 2021. 7 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The process of learning a manipulation task depends strongly on the action\nspace used for exploration: posed in the incorrect action space, solving a task\nwith reinforcement learning can be drastically inefficient. Additionally,\nsimilar tasks or instances of the same task family impose latent manifold\nconstraints on the most effective action space: the task family can be best\nsolved with actions in a manifold of the entire action space of the robot.\nCombining these insights we present LASER, a method to learn latent action\nspaces for efficient reinforcement learning. LASER factorizes the learning\nproblem into two sub-problems, namely action space learning and policy learning\nin the new action space. It leverages data from similar manipulation task\ninstances, either from an offline expert or online during policy learning, and\nlearns from these trajectories a mapping from the original to a latent action\nspace. LASER is trained as a variational encoder-decoder model to map raw\nactions into a disentangled latent action space while maintaining action\nreconstruction and latent space dynamic consistency. We evaluate LASER on two\ncontact-rich robotic tasks in simulation, and analyze the benefit of policy\nlearning in the generated latent action space. We show improved sample\nefficiency compared to the original action space from better alignment of the\naction space to the task space, as we observe with visualizations of the\nlearned action space manifold. Additional details:\nhttps://www.pair.toronto.edu/laser\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 17:40:02 GMT"}, {"version": "v2", "created": "Tue, 30 Mar 2021 12:19:29 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Allshire", "Arthur", ""], ["Mart\u00edn-Mart\u00edn", "Roberto", ""], ["Lin", "Charles", ""], ["Manuel", "Shawn", ""], ["Savarese", "Silvio", ""], ["Garg", "Animesh", ""]]}, {"id": "2103.15798", "submitter": "Mikhail Khodak", "authors": "Nicholas Roberts and Mikhail Khodak and Tri Dao and Liam Li and\n  Christopher R\\'e and Ameet Talwalkar", "title": "Rethinking Neural Operations for Diverse Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NA math.NA stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important goal of neural architecture search (NAS) is to automate-away the\ndesign of neural networks on new tasks in under-explored domains. Motivated by\nthis broader vision for NAS, we study the problem of enabling users to discover\nthe right neural operations given data from their specific domain. We introduce\na search space of neural operations called XD-Operations that mimic the\ninductive bias of standard multichannel convolutions while being much more\nexpressive: we prove that XD-operations include many named operations across\nseveral application areas. Starting with any standard backbone network such as\nLeNet or ResNet, we show how to transform it into an architecture search space\nover XD-operations and how to traverse the space using a simple weight-sharing\nscheme. On a diverse set of applications--image classification, solving partial\ndifferential equations (PDEs), and sequence modeling--our approach consistently\nyields models with lower error than baseline networks and sometimes even lower\nerror than expert-designed domain-specific approaches.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 17:50:39 GMT"}], "update_date": "2021-03-30", "authors_parsed": [["Roberts", "Nicholas", ""], ["Khodak", "Mikhail", ""], ["Dao", "Tri", ""], ["Li", "Liam", ""], ["R\u00e9", "Christopher", ""], ["Talwalkar", "Ameet", ""]]}, {"id": "2103.15819", "submitter": "Linus Gissl\\'en", "authors": "Joakim Bergdahl, Camilo Gordillo, Konrad Tollmar, Linus Gissl\\'en", "title": "Augmenting Automated Game Testing with Deep Reinforcement Learning", "comments": "4 pages, 6 figures, 2020 IEEE Conference on Games (CoG), 600-603", "journal-ref": "2020 IEEE Conference on Games (CoG), 600-603", "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  General game testing relies on the use of human play testers, play test\nscripting, and prior knowledge of areas of interest to produce relevant test\ndata. Using deep reinforcement learning (DRL), we introduce a self-learning\nmechanism to the game testing framework. With DRL, the framework is capable of\nexploring and/or exploiting the game mechanics based on a user-defined,\nreinforcing reward signal. As a result, test coverage is increased and\nunintended game play mechanics, exploits and bugs are discovered in a multitude\nof game types. In this paper, we show that DRL can be used to increase test\ncoverage, find exploits, test map difficulty, and to detect common problems\nthat arise in the testing of first-person shooter (FPS) games.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 11:55:15 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Bergdahl", "Joakim", ""], ["Gordillo", "Camilo", ""], ["Tollmar", "Konrad", ""], ["Gissl\u00e9n", "Linus", ""]]}, {"id": "2103.15851", "submitter": "Andrea Rosasco", "authors": "Andrea Rosasco, Antonio Carta, Andrea Cossu, Vincenzo Lomonaco, Davide\n  Bacciu", "title": "Distilled Replay: Overcoming Forgetting through Synthetic Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Replay strategies are Continual Learning techniques which mitigate\ncatastrophic forgetting by keeping a buffer of patterns from previous\nexperiences, which are interleaved with new data during training. The amount of\npatterns stored in the buffer is a critical parameter which largely influences\nthe final performance and the memory footprint of the approach. This work\nintroduces Distilled Replay, a novel replay strategy for Continual Learning\nwhich is able to mitigate forgetting by keeping a very small buffer (1 pattern\nper class) of highly informative samples. Distilled Replay builds the buffer\nthrough a distillation process which compresses a large dataset into a tiny set\nof informative examples. We show the effectiveness of our Distilled Replay\nagainst popular replay-based strategies on four Continual Learning benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 18:02:05 GMT"}, {"version": "v2", "created": "Tue, 22 Jun 2021 15:04:36 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Rosasco", "Andrea", ""], ["Carta", "Antonio", ""], ["Cossu", "Andrea", ""], ["Lomonaco", "Vincenzo", ""], ["Bacciu", "Davide", ""]]}, {"id": "2103.15871", "submitter": "Varun Kumar", "authors": "Luoxin Chen, Francisco Garcia, Varun Kumar, He Xie, Jianhua Lu", "title": "Industry Scale Semi-Supervised Learning for Natural Language\n  Understanding", "comments": "NAACL 2021 Industry track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a production Semi-Supervised Learning (SSL) pipeline\nbased on the student-teacher framework, which leverages millions of unlabeled\nexamples to improve Natural Language Understanding (NLU) tasks. We investigate\ntwo questions related to the use of unlabeled data in production SSL context:\n1) how to select samples from a huge unlabeled data pool that are beneficial\nfor SSL training, and 2) how do the selected data affect the performance of\ndifferent state-of-the-art SSL techniques. We compare four widely used SSL\ntechniques, Pseudo-Label (PL), Knowledge Distillation (KD), Virtual Adversarial\nTraining (VAT) and Cross-View Training (CVT) in conjunction with two data\nselection methods including committee-based selection and submodular\noptimization based selection. We further examine the benefits and drawbacks of\nthese techniques when applied to intent classification (IC) and named entity\nrecognition (NER) tasks, and provide guidelines specifying when each of these\nmethods might be beneficial to improve large scale NLU systems.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 18:24:02 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Chen", "Luoxin", ""], ["Garcia", "Francisco", ""], ["Kumar", "Varun", ""], ["Xie", "He", ""], ["Lu", "Jianhua", ""]]}, {"id": "2103.15877", "submitter": "Sai Koneru", "authors": "Sai Koneru, Danni Liu and Jan Niehues", "title": "Unsupervised Machine Translation On Dravidian Languages", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Unsupervised neural machine translation (UNMT) is beneficial especially for\nlow resource languages such as those from the Dravidian family. However, UNMT\nsystems tend to fail in realistic scenarios involving actual low resource\nlanguages. Recent works propose to utilize auxiliary parallel data and have\nachieved state-of-the-art results. In this work, we focus on unsupervised\ntranslation between English and Kannada, a low resource Dravidian language. We\nadditionally utilize a limited amount of auxiliary data between English and\nother related Dravidian languages. We show that unifying the writing systems is\nessential in unsupervised translation between the Dravidian languages. We\nexplore several model architectures that use the auxiliary data in order to\nmaximize knowledge sharing and enable UNMT for distant language pairs. Our\nexperiments demonstrate that it is crucial to include auxiliary languages that\nare similar to our focal language, Kannada. Furthermore, we propose a metric to\nmeasure language similarity and show that it serves as a good indicator for\nselecting the auxiliary languages.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 18:33:53 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Koneru", "Sai", ""], ["Liu", "Danni", ""], ["Niehues", "Jan", ""]]}, {"id": "2103.15898", "submitter": "Loris Nanni", "authors": "Loris Nanni, Gianluca Maguolo, Sheryl Brahnam, Michelangelo Paci", "title": "Comparison of different convolutional neural network activation\n  functions and methods for building ensembles", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently, much attention has been devoted to finding highly efficient and\npowerful activation functions for CNN layers. Because activation functions\ninject different nonlinearities between layers that affect performance, varying\nthem is one method for building robust ensembles of CNNs. The objective of this\nstudy is to examine the performance of CNN ensembles made with different\nactivation functions, including six new ones presented here: 2D Mexican ReLU,\nTanELU, MeLU+GaLU, Symmetric MeLU, Symmetric GaLU, and Flexible MeLU. The\nhighest performing ensemble was built with CNNs having different activation\nlayers that randomly replaced the standard ReLU. A comprehensive evaluation of\nthe proposed approach was conducted across fifteen biomedical data sets\nrepresenting various classification tasks. The proposed method was tested on\ntwo basic CNN architectures: Vgg16 and ResNet50. Results demonstrate the\nsuperiority in performance of this approach. The MATLAB source code for this\nstudy will be available at https://github.com/LorisNanni.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 19:12:41 GMT"}, {"version": "v2", "created": "Fri, 2 Apr 2021 02:09:13 GMT"}], "update_date": "2021-04-05", "authors_parsed": [["Nanni", "Loris", ""], ["Maguolo", "Gianluca", ""], ["Brahnam", "Sheryl", ""], ["Paci", "Michelangelo", ""]]}, {"id": "2103.15908", "submitter": "Ali el Hassouni", "authors": "Ali el Hassouni, Mark Hoogendoorn, Marketa Ciharova, Annet Kleiboer,\n  Khadicha Amarti, Vesa Muhonen, Heleen Riper, A. E. Eiben", "title": "pH-RL: A personalization architecture to bring reinforcement learning to\n  health practice", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While reinforcement learning (RL) has proven to be the approach of choice for\ntackling many complex problems, it remains challenging to develop and deploy RL\nagents in real-life scenarios successfully. This paper presents pH-RL\n(personalization in e-Health with RL) a general RL architecture for\npersonalization to bring RL to health practice. pH-RL allows for various levels\nof personalization in health applications and allows for online and batch\nlearning. Furthermore, we provide a general-purpose implementation framework\nthat can be integrated with various healthcare applications. We describe a\nstep-by-step guideline for the successful deployment of RL policies in a mobile\napplication. We implemented our open-source RL architecture and integrated it\nwith the MoodBuster mobile application for mental health to provide messages to\nincrease daily adherence to the online therapeutic modules. We then performed a\ncomprehensive study with human participants over a sustained period. Our\nexperimental results show that the developed policies learn to select\nappropriate actions consistently using only a few days' worth of data.\nFurthermore, we empirically demonstrate the stability of the learned policies\nduring the study.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 19:38:04 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 00:46:39 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Hassouni", "Ali el", ""], ["Hoogendoorn", "Mark", ""], ["Ciharova", "Marketa", ""], ["Kleiboer", "Annet", ""], ["Amarti", "Khadicha", ""], ["Muhonen", "Vesa", ""], ["Riper", "Heleen", ""], ["Eiben", "A. E.", ""]]}, {"id": "2103.15940", "submitter": "Valentina Popescu", "authors": "Valentina Popescu and Abhinav Venigalla and Di Wu and Robert Schreiber", "title": "Representation range needs for 16-bit neural network training", "comments": "7 pages, 3 figures, 4 tables; conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NA math.NA", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep learning has grown rapidly thanks to its state-of-the-art performance\nacross a wide range of real-world applications. While neural networks have been\ntrained using IEEE-754 binary32 arithmetic, the rapid growth of computational\ndemands in deep learning has boosted interest in faster, low precision\ntraining. Mixed-precision training that combines IEEE-754 binary16 with\nIEEE-754 binary32 has been tried, and other $16$-bit formats, for example\nGoogle's bfloat16, have become popular. In floating-point arithmetic there is a\ntradeoff between precision and representation range as the number of exponent\nbits changes; denormal numbers extend the representation range. This raises\nquestions of how much exponent range is needed, of whether there is a format\nbetween binary16 (5 exponent bits) and bfloat16 (8 exponent bits) that works\nbetter than either of them, and whether or not denormals are necessary.\n  In the current paper we study the need for denormal numbers for\nmixed-precision training, and we propose a 1/6/9 format, i.e., 6-bit exponent\nand 9-bit explicit mantissa, that offers a better range-precision tradeoff. We\nshow that 1/6/9 mixed-precision training is able to speed up training on\nhardware that incurs a performance slowdown on denormal operations or\neliminates the need for denormal numbers altogether. And, for a number of fully\nconnected and convolutional neural networks in computer vision and natural\nlanguage processing, 1/6/9 achieves numerical parity to standard\nmixed-precision.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 20:30:02 GMT"}, {"version": "v2", "created": "Tue, 6 Apr 2021 20:15:21 GMT"}], "update_date": "2021-04-08", "authors_parsed": [["Popescu", "Valentina", ""], ["Venigalla", "Abhinav", ""], ["Wu", "Di", ""], ["Schreiber", "Robert", ""]]}, {"id": "2103.15941", "submitter": "Bhaskar Ramasubramanian", "authors": "Baicen Xiao, Bhaskar Ramasubramanian, Radha Poovendran", "title": "Shaping Advice in Deep Multi-Agent Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SY eess.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multi-agent reinforcement learning involves multiple agents interacting with\neach other and a shared environment to complete tasks. When rewards provided by\nthe environment are sparse, agents may not receive immediate feedback on the\nquality of actions that they take, thereby affecting learning of policies. In\nthis paper, we propose a method called Shaping Advice in deep Multi-agent\nreinforcement learning (SAM) to augment the reward signal from the environment\nwith an additional reward termed shaping advice. The shaping advice is given by\na difference of potential functions at consecutive time-steps. Each potential\nfunction is a function of observations and actions of the agents. The shaping\nadvice needs to be specified only once at the start of training, and can be\neasily provided by non-experts. We show through theoretical analyses and\nexperimental validation that shaping advice provided by SAM does not distract\nagents from completing tasks specified by the environment reward.\nTheoretically, we prove that convergence of policy gradients and value\nfunctions when using SAM implies convergence of these quantities in the absence\nof SAM. Experimentally, we evaluate SAM on three tasks in the multi-agent\nParticle World environment that have sparse rewards. We observe that using SAM\nresults in agents learning policies to complete tasks faster, and obtain higher\nrewards than: i) using sparse rewards alone; ii) a state-of-the-art reward\nredistribution method.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 20:33:50 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Xiao", "Baicen", ""], ["Ramasubramanian", "Bhaskar", ""], ["Poovendran", "Radha", ""]]}, {"id": "2103.15951", "submitter": "Nare Karapetyan", "authors": "Nare Karapetyan, Jason Moulton, and Ioannis Rekleitis", "title": "Dynamic Autonomous Surface Vehicle Control and Applications in\n  Environmental Monitoring", "comments": "Published in OCEANS 2019 MTS/IEEE SEATTLE", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the problem of robotic operations in the presence of\nadversarial forces. We presents a complete framework for survey operations:\nwaypoint generation,modelling of forces and tuning the control. In many\napplications of environmental monitoring, search and exploration, and\nbathymetric mapping, the vehicle has to traverse in straight lines parallel to\neach other, ensuring there are no gaps and no redundant coverage. During\noperations with an Autonomous Surface Vehicle (ASV) however, the presence of\nwind and/or currents produces external forces acting on the vehicle which quite\noften divert it from its intended path. Similar issues have been encountered\nduring aerial or underwater operations. By measuring these phenomena, wind and\ncurrent, and modelling their impact on the vessel, actions can be taken to\nalleviate their effect and ensure the correct trajectory is followed.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 20:55:52 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Karapetyan", "Nare", ""], ["Moulton", "Jason", ""], ["Rekleitis", "Ioannis", ""]]}, {"id": "2103.15963", "submitter": "Salomey Osei", "authors": "Paul Azunre, Salomey Osei, Salomey Addo, Lawrence Asamoah Adu-Gyamfi,\n  Stephen Moore, Bernard Adabankah, Bernard Opoku, Clara Asare-Nyarko, Samuel\n  Nyarko, Cynthia Amoaba, Esther Dansoa Appiah, Felix Akwerh, Richard Nii Lante\n  Lawson, Joel Budu, Emmanuel Debrah, Nana Boateng, Wisdom Ofori, Edwin\n  Buabeng-Munkoh, Franklin Adjei, Isaac Kojo Essel Ampomah, Joseph Otoo,\n  Reindorf Borkor, Standylove Birago Mensah, Lucien Mensah, Mark Amoako Marcel,\n  Anokye Acheampong Amponsah, James Ben Hayfron-Acquah", "title": "Contextual Text Embeddings for Twi", "comments": "10 pages paper; Accepted at African NLP Workshop @ EACL 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transformer-based language models have been changing the modern Natural\nLanguage Processing (NLP) landscape for high-resource languages such as\nEnglish, Chinese, Russian, etc. However, this technology does not yet exist for\nany Ghanaian language. In this paper, we introduce the first of such models for\nTwi or Akan, the most widely spoken Ghanaian language. The specific\ncontribution of this research work is the development of several pretrained\ntransformer language models for the Akuapem and Asante dialects of Twi, paving\nthe way for advances in application areas such as Named Entity Recognition\n(NER), Neural Machine Translation (NMT), Sentiment Analysis (SA) and\nPart-of-Speech (POS) tagging. Specifically, we introduce four different\nflavours of ABENA -- A BERT model Now in Akan that is fine-tuned on a set of\nAkan corpora, and BAKO - BERT with Akan Knowledge only, which is trained from\nscratch. We open-source the model through the Hugging Face model hub and\ndemonstrate its use via a simple sentiment classification example.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 21:36:44 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 07:03:02 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Azunre", "Paul", ""], ["Osei", "Salomey", ""], ["Addo", "Salomey", ""], ["Adu-Gyamfi", "Lawrence Asamoah", ""], ["Moore", "Stephen", ""], ["Adabankah", "Bernard", ""], ["Opoku", "Bernard", ""], ["Asare-Nyarko", "Clara", ""], ["Nyarko", "Samuel", ""], ["Amoaba", "Cynthia", ""], ["Appiah", "Esther Dansoa", ""], ["Akwerh", "Felix", ""], ["Lawson", "Richard Nii Lante", ""], ["Budu", "Joel", ""], ["Debrah", "Emmanuel", ""], ["Boateng", "Nana", ""], ["Ofori", "Wisdom", ""], ["Buabeng-Munkoh", "Edwin", ""], ["Adjei", "Franklin", ""], ["Ampomah", "Isaac Kojo Essel", ""], ["Otoo", "Joseph", ""], ["Borkor", "Reindorf", ""], ["Mensah", "Standylove Birago", ""], ["Mensah", "Lucien", ""], ["Marcel", "Mark Amoako", ""], ["Amponsah", "Anokye Acheampong", ""], ["Hayfron-Acquah", "James Ben", ""]]}, {"id": "2103.15975", "submitter": "Dan Bohus", "authors": "Dan Bohus, Sean Andrist, Ashley Feniello, Nick Saw, Mihai Jalobeanu,\n  Patrick Sweeney, Anne Loomis Thompson, Eric Horvitz", "title": "Platform for Situated Intelligence", "comments": "29 pages, 14 figures, Microsoft Research Technical Report", "journal-ref": null, "doi": null, "report-no": "MSR-TR-2021-02", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Platform for Situated Intelligence, an open-source framework\ncreated to support the rapid development and study of multimodal,\nintegrative-AI systems. The framework provides infrastructure for sensing,\nfusing, and making inferences from temporal streams of data across different\nmodalities, a set of tools that enable visualization and debugging, and an\necosystem of components that encapsulate a variety of perception and processing\ntechnologies. These assets jointly provide the means for rapidly constructing\nand refining multimodal, integrative-AI systems, while retaining the efficiency\nand performance characteristics required for deployment in open-world settings.\n", "versions": [{"version": "v1", "created": "Mon, 29 Mar 2021 22:30:15 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Bohus", "Dan", ""], ["Andrist", "Sean", ""], ["Feniello", "Ashley", ""], ["Saw", "Nick", ""], ["Jalobeanu", "Mihai", ""], ["Sweeney", "Patrick", ""], ["Thompson", "Anne Loomis", ""], ["Horvitz", "Eric", ""]]}, {"id": "2103.15997", "submitter": "Gilberto Ochoa-Ruiz", "authors": "Juan Carlos Angeles Ceron, Leonardo Chang, Gilberto Ochoa-Ruiz and\n  Sharib Ali", "title": "Assessing YOLACT++ for real time and robust instance segmentation of\n  medical instruments in endoscopic procedures", "comments": "Preprint under review for EMBC 2021 following IEEE guidelines", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Image-based tracking of laparoscopic instruments plays a fundamental role in\ncomputer and robotic-assisted surgeries by aiding surgeons and increasing\npatient safety. Computer vision contests, such as the Robust Medical Instrument\nSegmentation (ROBUST-MIS) Challenge, seek to encourage the development of\nrobust models for such purposes, providing large, diverse, and annotated\ndatasets. To date, most of the existing models for instance segmentation of\nmedical instruments were based on two-stage detectors, which provide robust\nresults but are nowhere near to the real-time (5 frames-per-second (fps)at\nmost). However, in order for the method to be clinically applicable, real-time\ncapability is utmost required along with high accuracy. In this paper, we\npropose the addition of attention mechanisms to the YOLACT architecture that\nallows real-time instance segmentation of instrument with improved accuracy on\nthe ROBUST-MIS dataset. Our proposed approach achieves competitive performance\ncompared to the winner ofthe 2019 ROBUST-MIS challenge in terms of robustness\nscores,obtaining 0.313 MI_DSC and 0.338 MI_NSD, while achieving real-time\nperformance (37 fps)\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 00:09:55 GMT"}, {"version": "v2", "created": "Thu, 29 Apr 2021 01:39:43 GMT"}], "update_date": "2021-04-30", "authors_parsed": [["Ceron", "Juan Carlos Angeles", ""], ["Chang", "Leonardo", ""], ["Ochoa-Ruiz", "Gilberto", ""], ["Ali", "Sharib", ""]]}, {"id": "2103.16021", "submitter": "Keenon Werling", "authors": "Keenon Werling, Dalton Omens, Jeongseok Lee, Ioannis Exarchos, C.\n  Karen Liu", "title": "Fast and Feature-Complete Differentiable Physics for Articulated Rigid\n  Bodies with Contact", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.LG cs.SY eess.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a fast and feature-complete differentiable physics engine, Nimble\n(nimblephysics.org), that supports Lagrangian dynamics and hard contact\nconstraints for articulated rigid body simulation. Our differentiable physics\nengine offers a complete set of features that are typically only available in\nnon-differentiable physics simulators commonly used by robotics applications.\nWe solve contact constraints precisely using linear complementarity problems\n(LCPs). We present efficient and novel analytical gradients through the LCP\nformulation of inelastic contact that exploit the sparsity of the LCP solution.\nWe support complex contact geometry, and gradients approximating\ncontinuous-time elastic collision. We also introduce a novel method to compute\ncomplementarity-aware gradients that help downstream optimization tasks avoid\nstalling in saddle points. We show that an implementation of this combination\nin an existing physics engine (DART) is capable of a 87x single-core speedup\nover finite-differencing in computing analytical Jacobians for a single\ntimestep, while preserving all the expressiveness of original DART.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 01:48:29 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 16:59:11 GMT"}, {"version": "v3", "created": "Tue, 22 Jun 2021 23:36:54 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Werling", "Keenon", ""], ["Omens", "Dalton", ""], ["Lee", "Jeongseok", ""], ["Exarchos", "Ioannis", ""], ["Liu", "C. Karen", ""]]}, {"id": "2103.16080", "submitter": "James Wallbridge", "authors": "James Clift, Daniel Murfet, James Wallbridge", "title": "Geometry of Program Synthesis", "comments": "16 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We re-evaluate universal computation based on the synthesis of Turing\nmachines. This leads to a view of programs as singularities of analytic\nvarieties or, equivalently, as phases of the Bayesian posterior of a synthesis\nproblem. This new point of view reveals unexplored directions of research in\nprogram synthesis, of which neural networks are a subset, for example in\nrelation to phase transitions, complexity and generalisation. We also lay the\nempirical foundations for these new directions by reporting on our\nimplementation in code of some simple experiments.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 05:14:15 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Clift", "James", ""], ["Murfet", "Daniel", ""], ["Wallbridge", "James", ""]]}, {"id": "2103.16086", "submitter": "Xiao Wang", "authors": "Xiao Wang, Zhe Chen, Jin Tang, Bin Luo, Yaowei Wang, Yonghong Tian,\n  Feng Wu", "title": "Dynamic Attention guided Multi-Trajectory Analysis for Single Object\n  Tracking", "comments": "Accepted by IEEE T-CSVT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most of the existing single object trackers track the target in a unitary\nlocal search window, making them particularly vulnerable to challenging factors\nsuch as heavy occlusions and out-of-view movements. Despite the attempts to\nfurther incorporate global search, prevailing mechanisms that cooperate local\nand global search are relatively static, thus are still sub-optimal for\nimproving tracking performance. By further studying the local and global search\nresults, we raise a question: can we allow more dynamics for cooperating both\nresults? In this paper, we propose to introduce more dynamics by devising a\ndynamic attention-guided multi-trajectory tracking strategy. In particular, we\nconstruct dynamic appearance model that contains multiple target templates,\neach of which provides its own attention for locating the target in the new\nframe. Guided by different attention, we maintain diversified tracking results\nfor the target to build multi-trajectory tracking history, allowing more\ncandidates to represent the true target trajectory. After spanning the whole\nsequence, we introduce a multi-trajectory selection network to find the best\ntrajectory that delivers improved tracking performance. Extensive experimental\nresults show that our proposed tracking strategy achieves compelling\nperformance on various large-scale tracking benchmarks. The project page of\nthis paper can be found at https://sites.google.com/view/mt-track/.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 05:36:31 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Wang", "Xiao", ""], ["Chen", "Zhe", ""], ["Tang", "Jin", ""], ["Luo", "Bin", ""], ["Wang", "Yaowei", ""], ["Tian", "Yonghong", ""], ["Wu", "Feng", ""]]}, {"id": "2103.16089", "submitter": "Mateusz Ostaszewski", "authors": "Mateusz Ostaszewski, Lea M. Trenkwalder, Wojciech Masarczyk, Eleanor\n  Scerri, Vedran Dunjko", "title": "Reinforcement learning for optimization of variational quantum circuit\n  architectures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of Variational Quantum Eigensolvers (VQEs) has been in the\nspotlight in recent times as they may lead to real-world applications of\nnear-term quantum devices. However, their performance depends on the structure\nof the used variational ansatz, which requires balancing the depth and\nexpressivity of the corresponding circuit. In recent years, various methods for\nVQE structure optimization have been introduced but the capacities of machine\nlearning to aid with this problem has not yet been fully investigated. In this\nwork, we propose a reinforcement learning algorithm that autonomously explores\nthe space of possible ans{\\\"a}tze, identifying economic circuits which still\nyield accurate ground energy estimates. The algorithm is intrinsically\nmotivated, and it incrementally improves the accuracy of the result while\nminimizing the circuit depth. We showcase the performance of our algorithm on\nthe problem of estimating the ground-state energy of lithium hydride (LiH). In\nthis well-known benchmark problem, we achieve chemical accuracy, as well as\nstate-of-the-art results in terms of circuit depth.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 05:46:21 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Ostaszewski", "Mateusz", ""], ["Trenkwalder", "Lea M.", ""], ["Masarczyk", "Wojciech", ""], ["Scerri", "Eleanor", ""], ["Dunjko", "Vedran", ""]]}, {"id": "2103.16095", "submitter": "Muzhi Han", "authors": "Muzhi Han, Zeyu Zhang, Ziyuan Jiao, Xu Xie, Yixin Zhu, Song-Chun Zhu,\n  Hangxin Liu", "title": "Reconstructing Interactive 3D Scenes by Panoptic Mapping and CAD Model\n  Alignments", "comments": "ICRA 2021 paper. Project:\n  https://sites.google.com/view/icra2021-reconstruction", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we rethink the problem of scene reconstruction from an\nembodied agent's perspective: While the classic view focuses on the\nreconstruction accuracy, our new perspective emphasizes the underlying\nfunctions and constraints such that the reconstructed scenes provide\n\\em{actionable} information for simulating \\em{interactions} with agents. Here,\nwe address this challenging problem by reconstructing an interactive scene\nusing RGB-D data stream, which captures (i) the semantics and geometry of\nobjects and layouts by a 3D volumetric panoptic mapping module, and (ii) object\naffordance and contextual relations by reasoning over physical common sense\namong objects, organized by a graph-based scene representation. Crucially, this\nreconstructed scene replaces the object meshes in the dense panoptic map with\npart-based articulated CAD models for finer-grained robot interactions. In the\nexperiments, we demonstrate that (i) our panoptic mapping module outperforms\nprevious state-of-the-art methods, (ii) a high-performant physical reasoning\nprocedure that matches, aligns, and replaces objects' meshes with best-fitted\nCAD models, and (iii) reconstructed scenes are physically plausible and\nnaturally afford actionable interactions; without any manual labeling, they are\nseamlessly imported to ROS-based simulators and virtual environments for\ncomplex robot task executions.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 05:56:58 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Han", "Muzhi", ""], ["Zhang", "Zeyu", ""], ["Jiao", "Ziyuan", ""], ["Xie", "Xu", ""], ["Zhu", "Yixin", ""], ["Zhu", "Song-Chun", ""], ["Liu", "Hangxin", ""]]}, {"id": "2103.16099", "submitter": "Yanan Wu", "authors": "Zizhang Wu, Man Wang, Jason Wang, Wenkai Zhang, Muqing Fang, Tianhao\n  Xu", "title": "DeepWORD: A GCN-based Approach for Owner-Member Relationship Detection\n  in Autonomous Driving", "comments": "Accepted by IEEE ICME", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  It's worth noting that the owner-member relationship between wheels and\nvehicles has an significant contribution to the 3D perception of vehicles,\nespecially in the embedded environment. However, there are currently two main\nchallenges about the above relationship prediction: i) The traditional\nheuristic methods based on IoU can hardly deal with the traffic jam scenarios\nfor the occlusion. ii) It is difficult to establish an efficient applicable\nsolution for the vehicle-mounted system. To address these issues, we propose an\ninnovative relationship prediction method, namely DeepWORD, by designing a\ngraph convolution network (GCN). Specifically, we utilize the feature maps with\nlocal correlation as the input of nodes to improve the information richness.\nBesides, we introduce the graph attention network (GAT) to dynamically amend\nthe prior estimation deviation. Furthermore, we establish an annotated\nowner-member relationship dataset called WORD as a large-scale benchmark, which\nwill be available soon. The experiments demonstrate that our solution achieves\nstate-of-the-art accuracy and real-time in practice.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 06:12:29 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 14:13:53 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Wu", "Zizhang", ""], ["Wang", "Man", ""], ["Wang", "Jason", ""], ["Zhang", "Wenkai", ""], ["Fang", "Muqing", ""], ["Xu", "Tianhao", ""]]}, {"id": "2103.16108", "submitter": "Sandeep Kumar", "authors": "Sandeep Kumar, Koushik Biswas, Ashish Kumar Pandey", "title": "Predicting Landfall's Location and Time of a Tropical Cyclone Using\n  Reanalysis Data", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Landfall of a tropical cyclone is the event when it moves over the land after\ncrossing the coast of the ocean. It is important to know the characteristics of\nthe landfall in terms of location and time, well advance in time to take\npreventive measures timely. In this article, we develop a deep learning model\nbased on the combination of a Convolutional Neural network and a Long\nShort-Term memory network to predict the landfall's location and time of a\ntropical cyclone in six ocean basins of the world with high accuracy. We have\nused high-resolution spacial reanalysis data, ERA5, maintained by European\nCenter for Medium-Range Weather Forecasting (ECMWF). The model takes any 9\nhours, 15 hours, or 21 hours of data, during the progress of a tropical cyclone\nand predicts its landfall's location in terms of latitude and longitude and\ntime in hours. For 21 hours of data, we achieve mean absolute error for\nlandfall's location prediction in the range of 66.18 - 158.92 kilometers and\nfor landfall's time prediction in the range of 4.71 - 8.20 hours across all six\nocean basins. The model can be trained in just 30 to 45 minutes (based on ocean\nbasin) and can predict the landfall's location and time in a few seconds, which\nmakes it suitable for real time prediction.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 06:42:31 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Kumar", "Sandeep", ""], ["Biswas", "Koushik", ""], ["Pandey", "Ashish Kumar", ""]]}, {"id": "2103.16111", "submitter": "Giovanni Zappella", "authors": "Giovanni Zappella, David Salinas, C\\'edric Archambeau", "title": "A resource-efficient method for repeated HPO and NAS problems", "comments": "Accepted at AutoML workshop @ ICML 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we consider the problem of repeated hyperparameter and neural\narchitecture search (HNAS). We propose an extension of Successive Halving that\nis able to leverage information gained in previous HNAS problems with the goal\nof saving computational resources. We empirically demonstrate that our solution\nis able to drastically decrease costs while maintaining accuracy and being\nrobust to negative transfer. Our method is significantly simpler than competing\ntransfer learning approaches, setting a new baseline for transfer learning in\nHNAS.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 06:58:18 GMT"}, {"version": "v2", "created": "Tue, 13 Jul 2021 16:34:08 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Zappella", "Giovanni", ""], ["Salinas", "David", ""], ["Archambeau", "C\u00e9dric", ""]]}, {"id": "2103.16173", "submitter": "Zongyan Han", "authors": "Zongyan Han, Zhenyong Fu, Shuo Chen and Jian Yang", "title": "Contrastive Embedding for Generalized Zero-Shot Learning", "comments": "Accepted by CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized zero-shot learning (GZSL) aims to recognize objects from both\nseen and unseen classes, when only the labeled examples from seen classes are\nprovided. Recent feature generation methods learn a generative model that can\nsynthesize the missing visual features of unseen classes to mitigate the\ndata-imbalance problem in GZSL. However, the original visual feature space is\nsuboptimal for GZSL classification since it lacks discriminative information.\nTo tackle this issue, we propose to integrate the generation model with the\nembedding model, yielding a hybrid GZSL framework. The hybrid GZSL approach\nmaps both the real and the synthetic samples produced by the generation model\ninto an embedding space, where we perform the final GZSL classification.\nSpecifically, we propose a contrastive embedding (CE) for our hybrid GZSL\nframework. The proposed contrastive embedding can leverage not only the\nclass-wise supervision but also the instance-wise supervision, where the latter\nis usually neglected by existing GZSL researches. We evaluate our proposed\nhybrid GZSL framework with contrastive embedding, named CE-GZSL, on five\nbenchmark datasets. The results show that our CEGZSL method can outperform the\nstate-of-the-arts by a significant margin on three datasets. Our codes are\navailable on https://github.com/Hanzy1996/CE-GZSL.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 08:54:03 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Han", "Zongyan", ""], ["Fu", "Zhenyong", ""], ["Chen", "Shuo", ""], ["Yang", "Jian", ""]]}, {"id": "2103.16176", "submitter": "Ildar Batyrshin Z.", "authors": "Ildar Batyrshin", "title": "Contracting and Involutive Negations of Probability Distributions", "comments": "12 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A dozen papers have considered the concept of negation of probability\ndistributions (pd) introduced by Yager. Usually, such negations are generated\npoint-by-point by functions defined on a set of probability values and called\nhere negators. Recently it was shown that Yager negator plays a crucial role in\nthe definition of pd-independent linear negators: any linear negator is a\nfunction of Yager negator. Here, we prove that the sequence of multiple\nnegations of pd generated by a linear negator converges to the uniform\ndistribution with maximal entropy. We show that any pd-independent negator is\nnon-involutive, and any non-trivial linear negator is strictly contracting.\nFinally, we introduce an involutive negator in the class of pd-dependent\nnegators that generates an involutive negation of probability distributions.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 08:58:08 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Batyrshin", "Ildar", ""]]}, {"id": "2103.16177", "submitter": "Jo\\v{z}e Ro\\v{z}anec", "authors": "Patrik Zajec, Jo\\v{z}e M. Ro\\v{z}anec, Inna Novalija, Bla\\v{z}\n  Fortuna, Dunja Mladeni\\'c, Klemen Kenda", "title": "Towards Active Learning Based Smart Assistant for Manufacturing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A general approach for building a smart assistant that guides a user from a\nforecast generated by a machine learning model through a sequence of\ndecision-making steps is presented. We develop a methodology to build such a\nsystem. The system is demonstrated on a demand forecasting use case in\nmanufacturing. The methodology can be extended to several use cases in\nmanufacturing. The system provides means for knowledge acquisition, gathering\ndata from users. We envision active learning can be used to get data labels\nwhere labeled data is scarce.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 08:58:40 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Zajec", "Patrik", ""], ["Ro\u017eanec", "Jo\u017ee M.", ""], ["Novalija", "Inna", ""], ["Fortuna", "Bla\u017e", ""], ["Mladeni\u0107", "Dunja", ""], ["Kenda", "Klemen", ""]]}, {"id": "2103.16180", "submitter": "Sandeep Kumar", "authors": "Sandeep Kumar, Koushik Biswas, Ashish Kumar Pandey", "title": "Prediction of Landfall Intensity, Location, and Time of a Tropical\n  Cyclone", "comments": "Accepted for publication in conference AAAI (2021)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The prediction of the intensity, location and time of the landfall of a\ntropical cyclone well advance in time and with high accuracy can reduce human\nand material loss immensely. In this article, we develop a Long Short-Term\nmemory based Recurrent Neural network model to predict intensity (in terms of\nmaximum sustained surface wind speed), location (latitude and longitude), and\ntime (in hours after the observation period) of the landfall of a tropical\ncyclone which originates in the North Indian ocean. The model takes as input\nthe best track data of cyclone consisting of its location, pressure, sea\nsurface temperature, and intensity for certain hours (from 12 to 36 hours)\nanytime during the course of the cyclone as a time series and then provide\npredictions with high accuracy. For example, using 24 hours data of a cyclone\nanytime during its course, the model provides state-of-the-art results by\npredicting landfall intensity, time, latitude, and longitude with a mean\nabsolute error of 4.24 knots, 4.5 hours, 0.24 degree, and 0.37 degree\nrespectively, which resulted in a distance error of 51.7 kilometers from the\nlandfall location. We further check the efficacy of the model on three recent\ndevastating cyclones Bulbul, Fani, and Gaja, and achieved better results than\nthe test dataset.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 09:01:35 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Kumar", "Sandeep", ""], ["Biswas", "Koushik", ""], ["Pandey", "Ashish Kumar", ""]]}, {"id": "2103.16196", "submitter": "Can Cui Mr", "authors": "Can Cui, Wei Wang, Meihui Zhang, Gang Chen, Zhaojing Luo, Beng Chin\n  Ooi", "title": "AlphaEvolve: A Learning Framework to Discover Novel Alphas in\n  Quantitative Investment", "comments": "Accepted by SIGMOD 2021 Data Science and Engineering Track", "journal-ref": null, "doi": "10.1145/3448016.3457324", "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Alphas are stock prediction models capturing trading signals in a stock\nmarket. A set of effective alphas can generate weakly correlated high returns\nto diversify the risk. Existing alphas can be categorized into two classes:\nFormulaic alphas are simple algebraic expressions of scalar features, and thus\ncan generalize well and be mined into a weakly correlated set. Machine learning\nalphas are data-driven models over vector and matrix features. They are more\npredictive than formulaic alphas, but are too complex to mine into a weakly\ncorrelated set. In this paper, we introduce a new class of alphas to model\nscalar, vector, and matrix features which possess the strengths of these two\nexisting classes. The new alphas predict returns with high accuracy and can be\nmined into a weakly correlated set. In addition, we propose a novel alpha\nmining framework based on AutoML, called AlphaEvolve, to generate the new\nalphas. To this end, we first propose operators for generating the new alphas\nand selectively injecting relational domain knowledge to model the relations\nbetween stocks. We then accelerate the alpha mining by proposing a pruning\ntechnique for redundant alphas. Experiments show that AlphaEvolve can evolve\ninitial alphas into the new alphas with high returns and weak correlations.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 09:28:41 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 10:35:19 GMT"}], "update_date": "2021-04-02", "authors_parsed": [["Cui", "Can", ""], ["Wang", "Wei", ""], ["Zhang", "Meihui", ""], ["Chen", "Gang", ""], ["Luo", "Zhaojing", ""], ["Ooi", "Beng Chin", ""]]}, {"id": "2103.16201", "submitter": "Alexander Bartler", "authors": "Alexander Bartler, Andre B\\\"uhler, Felix Wiewel, Mario D\\\"obler and\n  Bin Yang", "title": "MT3: Meta Test-Time Training for Self-Supervised Test-Time Adaption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  An unresolved problem in Deep Learning is the ability of neural networks to\ncope with domain shifts during test-time, imposed by commonly fixing network\nparameters after training. Our proposed method Meta Test-Time Training (MT3),\nhowever, breaks this paradigm and enables adaption at test-time. We combine\nmeta-learning, self-supervision and test-time training to learn to adapt to\nunseen test distributions. By minimizing the self-supervised loss, we learn\ntask-specific model parameters for different tasks. A meta-model is optimized\nsuch that its adaption to the different task-specific models leads to higher\nperformance on those tasks. During test-time a single unlabeled image is\nsufficient to adapt the meta-model parameters. This is achieved by minimizing\nonly the self-supervised loss component resulting in a better prediction for\nthat image. Our approach significantly improves the state-of-the-art results on\nthe CIFAR-10-Corrupted image classification benchmark. Our implementation is\navailable on GitHub.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 09:33:38 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Bartler", "Alexander", ""], ["B\u00fchler", "Andre", ""], ["Wiewel", "Felix", ""], ["D\u00f6bler", "Mario", ""], ["Yang", "Bin", ""]]}, {"id": "2103.16215", "submitter": "Enrique Fernandez-Blanco", "authors": "Enrique Fernandez-Blanco, Daniel Rivero, Alejandro Pazos", "title": "Convolutional Neural Networks for Sleep Stage Scoring on a Two-Channel\n  EEG Signal", "comments": "20 pages, 4 figures, 4 tables", "journal-ref": "Soft Computing 24, 4067-4079 (2020)", "doi": "10.1007/s00500-019-04174-1", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Sleeping problems have become one of the major diseases all over the world.\nTo tackle this issue, the basic tool used by specialists is the Polysomnogram,\nwhich is a collection of different signals recorded during sleep. After its\nrecording, the specialists have to score the different signals according to one\nof the standard guidelines. This process is carried out manually, which can be\nhighly time-consuming and very prone to annotation errors. Therefore, over the\nyears, many approaches have been explored in an attempt to support the\nspecialists in this task. In this paper, an approach based on convolutional\nneural networks is presented, where an in-depth comparison is performed in\norder to determine the convenience of using more than one signal simultaneously\nas input. Additionally, the models were also used as parts of an ensemble model\nto check whether any useful information can be extracted from signal processing\na single signal at a time which the dual-signal model cannot identify. Tests\nhave been performed by using a well-known dataset called expanded sleep-EDF,\nwhich is the most commonly used dataset as the benchmark for this problem. The\ntests were carried out with a leave-one-out cross-validation over the patients,\nwhich ensures that there is no possible contamination between training and\ntesting. The resulting proposal is a network smaller than previously published\nones, but which overcomes the results of any previous models on the same\ndataset. The best result shows an accuracy of 92.67\\% and a Cohen's Kappa value\nover 0.84 compared to human experts.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 09:59:56 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Fernandez-Blanco", "Enrique", ""], ["Rivero", "Daniel", ""], ["Pazos", "Alejandro", ""]]}, {"id": "2103.16219", "submitter": "Xuning Shao", "authors": "Xuning Shao, Weidong Zhang", "title": "SPatchGAN: A Statistical Feature Based Discriminator for Unsupervised\n  Image-to-Image Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG eess.IV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For unsupervised image-to-image translation, we propose a discriminator\narchitecture which focuses on the statistical features instead of individual\npatches. The network is stabilized by distribution matching of key statistical\nfeatures at multiple scales. Unlike the existing methods which impose more and\nmore constraints on the generator, our method facilitates the shape deformation\nand enhances the fine details with a greatly simplified framework. We show that\nthe proposed method outperforms the existing state-of-the-art models in various\nchallenging applications including selfie-to-anime, male-to-female and glasses\nremoval. The code will be made publicly available.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 10:03:07 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Shao", "Xuning", ""], ["Zhang", "Weidong", ""]]}, {"id": "2103.16257", "submitter": "Qinbin Li", "authors": "Qinbin Li, Bingsheng He, Dawn Song", "title": "Model-Contrastive Federated Learning", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Federated learning enables multiple parties to collaboratively train a\nmachine learning model without communicating their local data. A key challenge\nin federated learning is to handle the heterogeneity of local data distribution\nacross parties. Although many studies have been proposed to address this\nchallenge, we find that they fail to achieve high performance in image datasets\nwith deep learning models. In this paper, we propose MOON: model-contrastive\nfederated learning. MOON is a simple and effective federated learning\nframework. The key idea of MOON is to utilize the similarity between model\nrepresentations to correct the local training of individual parties, i.e.,\nconducting contrastive learning in model-level. Our extensive experiments show\nthat MOON significantly outperforms the other state-of-the-art federated\nlearning algorithms on various image classification tasks.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 11:16:57 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Li", "Qinbin", ""], ["He", "Bingsheng", ""], ["Song", "Dawn", ""]]}, {"id": "2103.16285", "submitter": "Sahar Almahfouz Nasser", "authors": "Sahar A. Nasser, Debjani Paul, and Suyash P. Awate", "title": "Single Test Image-Based Automated Machine Learning System for\n  Distinguishing between Trait and Diseased Blood Samples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a machine learning-based method for fully automated diagnosis of\nsickle cell disease of poor-quality unstained images of a mobile microscope.\nOur method is capable of distinguishing between diseased, trait (carrier), and\nnormal samples unlike the previous methods that are limited to distinguishing\nthe normal from the abnormal samples only. The novelty of this method comes\nfrom distinguishing the trait and the diseased samples from challenging images\nthat have been captured directly in the field. The proposed approach contains\ntwo parts, the segmentation part followed by the classification part. We use a\nrandom forest algorithm to segment such challenging images acquitted through a\nmobile phone-based microscope. Then, we train two classifiers based on a random\nforest (RF) and a support vector machine (SVM) for classification. The results\nshow superior performances of both of the classifiers not only for images which\nhave been captured in the lab, but also for the ones which have been acquired\nin the field itself.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 12:29:50 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Nasser", "Sahar A.", ""], ["Paul", "Debjani", ""], ["Awate", "Suyash P.", ""]]}, {"id": "2103.16291", "submitter": "Weizhe Liu", "authors": "Weizhe Liu, Nikita Durasov, Pascal Fua", "title": "Leveraging Self-Supervision for Cross-Domain Crowd Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  State-of-the-art methods for counting people in crowded scenes rely on deep\nnetworks to estimate crowd density. While effective, these data-driven\napproaches rely on large amount of data annotation to achieve good performance,\nwhich stops these models from being deployed in emergencies during which data\nannotation is either too costly or cannot be obtained fast enough.\n  One popular solution is to use synthetic data for training. Unfortunately,\ndue to domain shift, the resulting models generalize poorly on real imagery. We\nremedy this shortcoming by training with both synthetic images, along with\ntheir associated labels, and unlabeled real images. To this end, we force our\nnetwork to learn perspective-aware features by training it to recognize\nupside-down real images from regular ones and incorporate into it the ability\nto predict its own uncertainty so that it can generate useful pseudo labels for\nfine-tuning purposes. This yields an algorithm that consistently outperforms\nstate-of-the-art cross-domain crowd counting ones without any extra computation\nat inference time.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 12:37:55 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Liu", "Weizhe", ""], ["Durasov", "Nikita", ""], ["Fua", "Pascal", ""]]}, {"id": "2103.16295", "submitter": "Siamak Layeghy", "authors": "Seyedehfaezeh Hosseininoorbin, Siamak Layeghy, Mohanad Sarhan, Raja\n  Jurdak, Marius Portmann", "title": "Exploring Edge TPU for Network Intrusion Detection in IoT", "comments": "22 pages, 11 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper explores Google's Edge TPU for implementing a practical network\nintrusion detection system (NIDS) at the edge of IoT, based on a deep learning\napproach. While there are a significant number of related works that explore\nmachine learning based NIDS for the IoT edge, they generally do not consider\nthe issue of the required computational and energy resources. The focus of this\npaper is the exploration of deep learning-based NIDS at the edge of IoT, and in\nparticular the computational and energy efficiency. In particular, the paper\nstudies Google's Edge TPU as a hardware platform, and considers the following\nthree key metrics: computation (inference) time, energy efficiency and the\ntraffic classification performance. Various scaled model sizes of two major\ndeep neural network architectures are used to investigate these three metrics.\nThe performance of the Edge TPU-based implementation is compared with that of\nan energy efficient embedded CPU (ARM Cortex A53). Our experimental evaluation\nshows some unexpected results, such as the fact that the CPU significantly\noutperforms the Edge TPU for small model sizes.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 12:43:57 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Hosseininoorbin", "Seyedehfaezeh", ""], ["Layeghy", "Siamak", ""], ["Sarhan", "Mohanad", ""], ["Jurdak", "Raja", ""], ["Portmann", "Marius", ""]]}, {"id": "2103.16327", "submitter": "Yueming Jin", "authors": "Yueming Jin, Yonghao Long, Cheng Chen, Zixu Zhao, Qi Dou, Pheng-Ann\n  Heng", "title": "Temporal Memory Relation Network for Workflow Recognition from Surgical\n  Video", "comments": "Accepted at IEEE Transactions on Medical Imaging (IEEE TMI); Code is\n  available at https://github.com/YuemingJin/TMRNet", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic surgical workflow recognition is a key component for developing\ncontext-aware computer-assisted systems in the operating theatre. Previous\nworks either jointly modeled the spatial features with short fixed-range\ntemporal information, or separately learned visual and long temporal cues. In\nthis paper, we propose a novel end-to-end temporal memory relation network\n(TMRNet) for relating long-range and multi-scale temporal patterns to augment\nthe present features. We establish a long-range memory bank to serve as a\nmemory cell storing the rich supportive information. Through our designed\ntemporal variation layer, the supportive cues are further enhanced by\nmulti-scale temporal-only convolutions. To effectively incorporate the two\ntypes of cues without disturbing the joint learning of spatio-temporal\nfeatures, we introduce a non-local bank operator to attentively relate the past\nto the present. In this regard, our TMRNet enables the current feature to view\nthe long-range temporal dependency, as well as tolerate complex temporal\nextents. We have extensively validated our approach on two benchmark surgical\nvideo datasets, M2CAI challenge dataset and Cholec80 dataset. Experimental\nresults demonstrate the outstanding performance of our method, consistently\nexceeding the state-of-the-art methods by a large margin (e.g., 67.0% v.s.\n78.9% Jaccard on Cholec80 dataset).\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 13:20:26 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Jin", "Yueming", ""], ["Long", "Yonghao", ""], ["Chen", "Cheng", ""], ["Zhao", "Zixu", ""], ["Dou", "Qi", ""], ["Heng", "Pheng-Ann", ""]]}, {"id": "2103.16329", "submitter": "Siamak Layeghy", "authors": "Wai Weng Lo, Siamak Layeghy, Mohanad Sarhan, Marcus Gallagher, Marius\n  Portmann", "title": "E-GraphSAGE: A Graph Neural Network based Intrusion Detection System", "comments": "13 pages, 7 figures, 8 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI cs.CR cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper presents a new network intrusion detection system (NIDS) based on\nGraph Neural Networks (GNNs). GNNs are a relatively new sub-field of deep\nneural networks, which have the unique ability to leverage the inherent\nstructure of graph-based data. Training and evaluation data for NIDSs are\ntypically represented as flow records, which can naturally be represented in a\ngraph format. This establishes the potential and motivation for exploring GNNs\nfor the purpose of network intrusion detection, which is the focus of this\npaper. E-GraphSAGE, our proposed new approach is based on the established\nGraphSAGE model, but provides the necessary modifications in order to support\nedge features for edge classification, and hence the classification of network\nflows into benign and attack classes. An extensive experimental evaluation\nbased on six recent NIDS benchmark datasets shows the excellent performance of\nour E-GraphSAGE based NIDS in comparison with the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 13:21:31 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 07:43:02 GMT"}, {"version": "v3", "created": "Thu, 13 May 2021 10:01:00 GMT"}, {"version": "v4", "created": "Fri, 14 May 2021 01:03:35 GMT"}], "update_date": "2021-05-17", "authors_parsed": [["Lo", "Wai Weng", ""], ["Layeghy", "Siamak", ""], ["Sarhan", "Mohanad", ""], ["Gallagher", "Marcus", ""], ["Portmann", "Marius", ""]]}, {"id": "2103.16370", "submitter": "Songyang Zhang", "authors": "Songyang Zhang, Zeming Li, Shipeng Yan, Xuming He, Jian Sun", "title": "Distribution Alignment: A Unified Framework for Long-tail Visual\n  Recognition", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Despite the recent success of deep neural networks, it remains challenging to\neffectively model the long-tail class distribution in visual recognition tasks.\nTo address this problem, we first investigate the performance bottleneck of the\ntwo-stage learning framework via ablative study. Motivated by our discovery, we\npropose a unified distribution alignment strategy for long-tail visual\nrecognition. Specifically, we develop an adaptive calibration function that\nenables us to adjust the classification scores for each data point. We then\nintroduce a generalized re-weight method in the two-stage learning to balance\nthe class prior, which provides a flexible and unified solution to diverse\nscenarios in visual recognition tasks. We validate our method by extensive\nexperiments on four tasks, including image classification, semantic\nsegmentation, object detection, and instance segmentation. Our approach\nachieves the state-of-the-art results across all four recognition tasks with a\nsimple and unified framework. The code and models will be made publicly\navailable at: https://github.com/Megvii-BaseDetection/DisAlign\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 14:09:53 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Zhang", "Songyang", ""], ["Li", "Zeming", ""], ["Yan", "Shipeng", ""], ["He", "Xuming", ""], ["Sun", "Jian", ""]]}, {"id": "2103.16378", "submitter": "Ferdinando Fioretto", "authors": "James Kotary, Ferdinando Fioretto, Pascal Van Hentenryck, Bryan Wilder", "title": "End-to-End Constrained Optimization Learning: A Survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper surveys the recent attempts at leveraging machine learning to\nsolve constrained optimization problems. It focuses on surveying the work on\nintegrating combinatorial solvers and optimization methods with machine\nlearning architectures. These approaches hold the promise to develop new hybrid\nmachine learning and optimization methods to predict fast, approximate,\nsolutions to combinatorial problems and to enable structural logical inference.\nThis paper presents a conceptual review of the recent advancements in this\nemerging area.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 14:19:30 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Kotary", "James", ""], ["Fioretto", "Ferdinando", ""], ["Van Hentenryck", "Pascal", ""], ["Wilder", "Bryan", ""]]}, {"id": "2103.16429", "submitter": "Hsuan Su", "authors": "Hsuan Su, Jiun-Hao Jhan, Fan-yun Sun, Saurav Sahay, Hung-yi Lee", "title": "Put Chatbot into Its Interlocutor's Shoes: New Framework to Learn\n  Chatbot Responding with Intention", "comments": "Accepted at NAACL-HLT 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Most chatbot literature that focuses on improving the fluency and coherence\nof a chatbot, is dedicated to making chatbots more human-like. However, very\nlittle work delves into what really separates humans from chatbots -- humans\nintrinsically understand the effect their responses have on the interlocutor\nand often respond with an intention such as proposing an optimistic view to\nmake the interlocutor feel better. This paper proposes an innovative framework\nto train chatbots to possess human-like intentions. Our framework includes a\nguiding chatbot and an interlocutor model that plays the role of humans. The\nguiding chatbot is assigned an intention and learns to induce the interlocutor\nto reply with responses matching the intention, for example, long responses,\njoyful responses, responses with specific words, etc. We examined our framework\nusing three experimental setups and evaluated the guiding chatbot with four\ndifferent metrics to demonstrate flexibility and performance advantages.\nAdditionally, we performed trials with human interlocutors to substantiate the\nguiding chatbot's effectiveness in influencing the responses of humans to a\ncertain extent. Code will be made available to the public.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 15:24:37 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 17:39:23 GMT"}, {"version": "v3", "created": "Thu, 1 Apr 2021 03:42:16 GMT"}, {"version": "v4", "created": "Mon, 12 Apr 2021 15:58:42 GMT"}, {"version": "v5", "created": "Fri, 23 Apr 2021 14:45:14 GMT"}], "update_date": "2021-04-26", "authors_parsed": [["Su", "Hsuan", ""], ["Jhan", "Jiun-Hao", ""], ["Sun", "Fan-yun", ""], ["Sahay", "Saurav", ""], ["Lee", "Hung-yi", ""]]}, {"id": "2103.16434", "submitter": "Georgios Papadopoulos Th.", "authors": "Georgios Th. Papadopoulos, Asterios Leonidis, Margherita Antona,\n  Constantine Stephanidis", "title": "User profile-driven large-scale multi-agent learning from demonstration\n  in federated human-robot collaborative environments", "comments": "arXiv admin note: substantial text overlap with arXiv:2012.08174", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Learning from Demonstration (LfD) has been established as the dominant\nparadigm for efficiently transferring skills from human teachers to robots. In\nthis context, the Federated Learning (FL) conceptualization has very recently\nbeen introduced for developing large-scale human-robot collaborative\nenvironments, targeting to robustly address, among others, the critical\nchallenges of multi-agent learning and long-term autonomy. In the current work,\nthe latter scheme is further extended and enhanced, by designing and\nintegrating a novel user profile formulation for providing a fine-grained\nrepresentation of the exhibited human behavior, adopting a Deep Learning\n(DL)-based formalism. In particular, a hierarchically organized set of key\ninformation sources is considered, including: a) User attributes (e.g.\ndemographic, anthropomorphic, educational, etc.), b) User state (e.g. fatigue\ndetection, stress detection, emotion recognition, etc.) and c)\nPsychophysiological measurements (e.g. gaze, electrodermal activity, heart\nrate, etc.) related data. Then, a combination of Long Short-Term Memory (LSTM)\nand stacked autoencoders, with appropriately defined neural network\narchitectures, is employed for the modelling step. The overall designed scheme\nenables both short- and long-term analysis/interpretation of the human behavior\n(as observed during the feedback capturing sessions), so as to adaptively\nadjust the importance of the collected feedback samples when aggregating\ninformation originating from the same and different human teachers,\nrespectively.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 15:33:21 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Papadopoulos", "Georgios Th.", ""], ["Leonidis", "Asterios", ""], ["Antona", "Margherita", ""], ["Stephanidis", "Constantine", ""]]}, {"id": "2103.16435", "submitter": "Omar Shaikh", "authors": "Omar Shaikh, Jon Saad-Falcon, Austin P Wright, Nilaksh Das, Scott\n  Freitas, Omar Isaac Asensio, Duen Horng Chau", "title": "EnergyVis: Interactively Tracking and Exploring Energy Consumption for\n  ML Models", "comments": "7 pages, 5 figures; CHI 2021 Extended Abstracts", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The advent of larger machine learning (ML) models have improved\nstate-of-the-art (SOTA) performance in various modeling tasks, ranging from\ncomputer vision to natural language. As ML models continue increasing in size,\nso does their respective energy consumption and computational requirements.\nHowever, the methods for tracking, reporting, and comparing energy consumption\nremain limited. We presentEnergyVis, an interactive energy consumption tracker\nfor ML models. Consisting of multiple coordinated views, EnergyVis enables\nresearchers to interactively track, visualize and compare model energy\nconsumption across key energy consumption and carbon footprint metrics (kWh and\nCO2), helping users explore alternative deployment locations and hardware that\nmay reduce carbon footprints. EnergyVis aims to raise awareness concerning\ncomputational sustainability by interactively highlighting excessive energy\nusage during model training; and by providing alternative training options to\nreduce energy usage.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 15:33:43 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Shaikh", "Omar", ""], ["Saad-Falcon", "Jon", ""], ["Wright", "Austin P", ""], ["Das", "Nilaksh", ""], ["Freitas", "Scott", ""], ["Asensio", "Omar Isaac", ""], ["Chau", "Duen Horng", ""]]}, {"id": "2103.16440", "submitter": "Chen Qiu", "authors": "Chen Qiu, Timo Pfrommer, Marius Kloft, Stephan Mandt, Maja Rudolph", "title": "Neural Transformation Learning for Deep Anomaly Detection Beyond Images", "comments": null, "journal-ref": "Proceedings of the 38th International Conference on Machine\n  Learning, 2021, volume:139, pages:8703--8714", "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data transformations (e.g. rotations, reflections, and cropping) play an\nimportant role in self-supervised learning. Typically, images are transformed\ninto different views, and neural networks trained on tasks involving these\nviews produce useful feature representations for downstream tasks, including\nanomaly detection. However, for anomaly detection beyond image data, it is\noften unclear which transformations to use. Here we present a simple end-to-end\nprocedure for anomaly detection with learnable transformations. The key idea is\nto embed the transformed data into a semantic space such that the transformed\ndata still resemble their untransformed form, while different transformations\nare easily distinguishable. Extensive experiments on time series demonstrate\nthat our proposed method outperforms existing approaches in the one-vs.-rest\nsetting and is competitive in the more challenging n-vs.-rest anomaly detection\ntask. On tabular datasets from the medical and cyber-security domains, our\nmethod learns domain-specific transformations and detects anomalies more\naccurately than previous work.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 15:38:18 GMT"}, {"version": "v2", "created": "Wed, 31 Mar 2021 15:09:56 GMT"}, {"version": "v3", "created": "Tue, 13 Jul 2021 13:25:36 GMT"}], "update_date": "2021-07-14", "authors_parsed": [["Qiu", "Chen", ""], ["Pfrommer", "Timo", ""], ["Kloft", "Marius", ""], ["Mandt", "Stephan", ""], ["Rudolph", "Maja", ""]]}, {"id": "2103.16490", "submitter": "Jakaria Rabbi", "authors": "Jakaria Rabbi, Md. Tahmid Hasan Fuad, Md. Abdul Awal", "title": "Human Activity Analysis and Recognition from Smartphones using Machine\n  Learning Techniques", "comments": "Submitted to the 10th International Conference on Informatics,\n  Electronics & Vision (ICIEV), 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human Activity Recognition (HAR) is considered a valuable research topic in\nthe last few decades. Different types of machine learning models are used for\nthis purpose, and this is a part of analyzing human behavior through machines.\nIt is not a trivial task to analyze the data from wearable sensors for complex\nand high dimensions. Nowadays, researchers mostly use smartphones or smart home\nsensors to capture these data. In our paper, we analyze these data using\nmachine learning models to recognize human activities, which are now widely\nused for many purposes such as physical and mental health monitoring. We apply\ndifferent machine learning models and compare performances. We use Logistic\nRegression (LR) as the benchmark model for its simplicity and excellent\nperformance on a dataset, and to compare, we take Decision Tree (DT), Support\nVector Machine (SVM), Random Forest (RF), and Artificial Neural Network (ANN).\nAdditionally, we select the best set of parameters for each model by grid\nsearch. We use the HAR dataset from the UCI Machine Learning Repository as a\nstandard dataset to train and test the models. Throughout the analysis, we can\nsee that the Support Vector Machine performed (average accuracy 96.33%) far\nbetter than the other methods. We also prove that the results are statistically\nsignificant by employing statistical significance test methods.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 16:46:40 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Rabbi", "Jakaria", ""], ["Fuad", "Md. Tahmid Hasan", ""], ["Awal", "Md. Abdul", ""]]}, {"id": "2103.16511", "submitter": "Manuel Schneider", "authors": "Florian Laurent, Manuel Schneider, Christian Scheller, Jeremy Watson,\n  Jiaoyang Li, Zhe Chen, Yi Zheng, Shao-Hung Chan, Konstantin Makhnev, Oleg\n  Svidchenko, Vladimir Egorov, Dmitry Ivanov, Aleksei Shpilman, Evgenija\n  Spirovska, Oliver Tanevski, Aleksandar Nikov, Ramon Grunder, David Galevski,\n  Jakov Mitrovski, Guillaume Sartoretti, Zhiyao Luo, Mehul Damani, Nilabha\n  Bhattacharya, Shivam Agarwal, Adrian Egli, Erik Nygren, Sharada Mohanty", "title": "Flatland Competition 2020: MAPF and MARL for Efficient Train\n  Coordination on a Grid World", "comments": "28 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Flatland competition aimed at finding novel approaches to solve the\nvehicle re-scheduling problem (VRSP). The VRSP is concerned with scheduling\ntrips in traffic networks and the re-scheduling of vehicles when disruptions\noccur, for example the breakdown of a vehicle. While solving the VRSP in\nvarious settings has been an active area in operations research (OR) for\ndecades, the ever-growing complexity of modern railway networks makes dynamic\nreal-time scheduling of traffic virtually impossible. Recently, multi-agent\nreinforcement learning (MARL) has successfully tackled challenging tasks where\nmany agents need to be coordinated, such as multiplayer video games. However,\nthe coordination of hundreds of agents in a real-life setting like a railway\nnetwork remains challenging and the Flatland environment used for the\ncompetition models these real-world properties in a simplified manner.\nSubmissions had to bring as many trains (agents) to their target stations in as\nlittle time as possible. While the best submissions were in the OR category,\nparticipants found many promising MARL approaches. Using both centralized and\ndecentralized learning based approaches, top submissions used graph\nrepresentations of the environment to construct tree-based observations.\nFurther, different coordination mechanisms were implemented, such as\ncommunication and prioritization between agents. This paper presents the\ncompetition setup, four outstanding solutions to the competition, and a\ncross-comparison between them.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 17:13:29 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Laurent", "Florian", ""], ["Schneider", "Manuel", ""], ["Scheller", "Christian", ""], ["Watson", "Jeremy", ""], ["Li", "Jiaoyang", ""], ["Chen", "Zhe", ""], ["Zheng", "Yi", ""], ["Chan", "Shao-Hung", ""], ["Makhnev", "Konstantin", ""], ["Svidchenko", "Oleg", ""], ["Egorov", "Vladimir", ""], ["Ivanov", "Dmitry", ""], ["Shpilman", "Aleksei", ""], ["Spirovska", "Evgenija", ""], ["Tanevski", "Oliver", ""], ["Nikov", "Aleksandar", ""], ["Grunder", "Ramon", ""], ["Galevski", "David", ""], ["Mitrovski", "Jakov", ""], ["Sartoretti", "Guillaume", ""], ["Luo", "Zhiyao", ""], ["Damani", "Mehul", ""], ["Bhattacharya", "Nilabha", ""], ["Agarwal", "Shivam", ""], ["Egli", "Adrian", ""], ["Nygren", "Erik", ""], ["Mohanty", "Sharada", ""]]}, {"id": "2103.16534", "submitter": "Mireille El Gheche", "authors": "Mireille El Gheche, Pascal Frossard", "title": "Multilayer Graph Clustering with Optimized Node Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We are interested in multilayer graph clustering, which aims at dividing the\ngraph nodes into categories or communities. To do so, we propose to learn a\nclustering-friendly embedding of the graph nodes by solving an optimization\nproblem that involves a fidelity term to the layers of a given multilayer\ngraph, and a regularization on the (single-layer) graph induced by the\nembedding. The fidelity term uses the contrastive loss to properly aggregate\nthe observed layers into a representative embedding. The regularization pushes\nfor a sparse and community-aware graph, and it is based on a measure of graph\nsparsification called \"effective resistance\", coupled with a penalization of\nthe first few eigenvalues of the representative graph Laplacian matrix to favor\nthe formation of communities. The proposed optimization problem is nonconvex\nbut fully differentiable, and thus can be solved via the descent gradient\nmethod. Experiments show that our method leads to a significant improvement\nw.r.t. state-of-the-art multilayer graph clustering algorithms.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 17:36:40 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Gheche", "Mireille El", ""], ["Frossard", "Pascal", ""]]}, {"id": "2103.16547", "submitter": "Xiaohan Chen", "authors": "Xiaohan Chen, Yu Cheng, Shuohang Wang, Zhe Gan, Jingjing Liu,\n  Zhangyang Wang", "title": "The Elastic Lottery Ticket Hypothesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lottery Ticket Hypothesis (LTH) raises keen attention to identifying sparse\ntrainable subnetworks, or winning tickets, of training, which can be trained in\nisolation to achieve similar or even better performance compared to the full\nmodels. Despite many efforts being made, the most effective method to identify\nsuch winning tickets is still Iterative Magnitude-based Pruning (IMP), which is\ncomputationally expensive and has to be run thoroughly for every different\nnetwork. A natural question that comes in is: can we \"transform\" the winning\nticket found in one network to another with a different architecture, yielding\na winning ticket for the latter at the beginning, without re-doing the\nexpensive IMP? Answering this question is not only practically relevant for\nefficient \"once-for-all\" winning ticket finding, but also theoretically\nappealing for uncovering inherently scalable sparse patterns in networks. We\nconduct extensive experiments on CIFAR-10 and ImageNet, and propose a variety\nof strategies to tweak the winning tickets found from different networks of the\nsame model family (e.g., ResNets). Based on these results, we articulate the\nElastic Lottery Ticket Hypothesis (E-LTH): by mindfully replicating (or\ndropping) and re-ordering layers for one network, its corresponding winning\nticket could be stretched (or squeezed) into a subnetwork for another deeper\n(or shallower) network from the same family, whose performance is nearly the\nsame competitive as the latter's winning ticket directly found by IMP. We have\nalso thoroughly compared E-LTH with pruning-at-initialization and dynamic\nsparse training methods, and discuss the generalizability of E-LTH to different\nmodel families, layer types, or across datasets. Code is available at\nhttps://github.com/VITA-Group/ElasticLTH.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 17:53:45 GMT"}, {"version": "v2", "created": "Mon, 7 Jun 2021 18:04:37 GMT"}], "update_date": "2021-06-09", "authors_parsed": [["Chen", "Xiaohan", ""], ["Cheng", "Yu", ""], ["Wang", "Shuohang", ""], ["Gan", "Zhe", ""], ["Liu", "Jingjing", ""], ["Wang", "Zhangyang", ""]]}, {"id": "2103.16561", "submitter": "Wanrong Zhu", "authors": "Wanrong Zhu, Yuankai Qi, Pradyumna Narayana, Kazoo Sone, Sugato Basu,\n  Xin Eric Wang, Qi Wu, Miguel Eckstein, William Yang Wang", "title": "Diagnosing Vision-and-Language Navigation: What Really Matters", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vision-and-language navigation (VLN) is a multimodal task where an agent\nfollows natural language instructions and navigates in visual environments.\nMultiple setups have been proposed, and researchers apply new model\narchitectures or training techniques to boost navigation performance. However,\nrecent studies witness a slow-down in the performance improvements in both\nindoor and outdoor VLN tasks, and the agents' inner mechanisms for making\nnavigation decisions remain unclear. To the best of our knowledge, the way the\nagents perceive the multimodal input is under-studied and clearly needs\ninvestigations. In this work, we conduct a series of diagnostic experiments to\nunveil agents' focus during navigation. Results show that indoor navigation\nagents refer to both object tokens and direction tokens in the instruction when\nmaking decisions. In contrast, outdoor navigation agents heavily rely on\ndirection tokens and have a poor understanding of the object tokens.\nFurthermore, instead of merely staring at surrounding objects, indoor\nnavigation agents can set their sights on objects further from the current\nviewpoint. When it comes to vision-and-language alignments, many models claim\nthat they are able to align object tokens with certain visual targets, but we\ncast doubt on the reliability of such alignments.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 17:59:07 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Zhu", "Wanrong", ""], ["Qi", "Yuankai", ""], ["Narayana", "Pradyumna", ""], ["Sone", "Kazoo", ""], ["Basu", "Sugato", ""], ["Wang", "Xin Eric", ""], ["Wu", "Qi", ""], ["Eckstein", "Miguel", ""], ["Wang", "William Yang", ""]]}, {"id": "2103.16564", "submitter": "Chuang Gan", "authors": "Zhenfang Chen, Jiayuan Mao, Jiajun Wu, Kwan-Yee Kenneth Wong, Joshua\n  B. Tenenbaum, Chuang Gan", "title": "Grounding Physical Concepts of Objects and Events Through Dynamic Visual\n  Reasoning", "comments": "ICLR 2021. Project page: http://dcl.csail.mit.edu/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG cs.SC", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We study the problem of dynamic visual reasoning on raw videos. This is a\nchallenging problem; currently, state-of-the-art models often require dense\nsupervision on physical object properties and events from simulation, which are\nimpractical to obtain in real life. In this paper, we present the Dynamic\nConcept Learner (DCL), a unified framework that grounds physical objects and\nevents from video and language. DCL first adopts a trajectory extractor to\ntrack each object over time and to represent it as a latent, object-centric\nfeature vector. Building upon this object-centric representation, DCL learns to\napproximate the dynamic interaction among objects using graph networks. DCL\nfurther incorporates a semantic parser to parse questions into semantic\nprograms and, finally, a program executor to run the program to answer the\nquestion, levering the learned dynamics model. After training, DCL can detect\nand associate objects across the frames, ground visual properties, and physical\nevents, understand the causal relationship between events, make future and\ncounterfactual predictions, and leverage these extracted presentations for\nanswering queries. DCL achieves state-of-the-art performance on CLEVRER, a\nchallenging causal video reasoning dataset, even without using ground-truth\nattributes and collision labels from simulations for training. We further test\nDCL on a newly proposed video-retrieval and event localization dataset derived\nfrom CLEVRER, showing its strong generalization capacity.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 17:59:48 GMT"}], "update_date": "2021-03-31", "authors_parsed": [["Chen", "Zhenfang", ""], ["Mao", "Jiayuan", ""], ["Wu", "Jiajun", ""], ["Wong", "Kwan-Yee Kenneth", ""], ["Tenenbaum", "Joshua B.", ""], ["Gan", "Chuang", ""]]}, {"id": "2103.16575", "submitter": "H.F. Stevance", "authors": "H. F. Stevance", "title": "Using Artificial Intelligence to Shed Light on the Star of Biscuits: The\n  Jaffa Cake", "comments": "April fools astro-ph submission - The topic is a joke but the numbers\n  and the analysis are real", "journal-ref": null, "doi": null, "report-no": null, "categories": "astro-ph.IM cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Before Brexit, one of the greatest causes of arguments amongst British\nfamilies was the question of the nature of Jaffa Cakes. Some argue that their\nsize and host environment (the biscuit aisle) should make them a biscuit in\ntheir own right. Others consider that their physical properties (e.g. they\nharden rather than soften on becoming stale) suggest that they are in fact\ncake. In order to finally put this debate to rest, we re-purpose technologies\nused to classify transient events. We train two classifiers (a Random Forest\nand a Support Vector Machine) on 100 recipes of traditional cakes and biscuits.\nOur classifiers have 95 percent and 91 percent accuracy respectively. Finally\nwe feed two Jaffa Cake recipes to the algorithms and find that Jaffa Cakes are,\nwithout a doubt, cakes. Finally, we suggest a new theory as to why some believe\nJaffa Cakes are biscuits.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 18:00:02 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Stevance", "H. F.", ""]]}, {"id": "2103.16591", "submitter": "Daniel Wu", "authors": "Daniel J. Wu, Avoy Datta", "title": "Continuous Weight Balancing", "comments": "4 pages, 2 figures, presented at the S2D-OLAD workshop at ICLR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  We propose a simple method by which to choose sample weights for problems\nwith highly imbalanced or skewed traits. Rather than naively discretizing\nregression labels to find binned weights, we take a more principled approach --\nwe derive sample weights from the transfer function between an estimated source\nand specified target distributions. Our method outperforms both unweighted and\ndiscretely-weighted models on both regression and classification tasks. We also\nopen-source our implementation of this method\n(https://github.com/Daniel-Wu/Continuous-Weight-Balancing) to the scientific\ncommunity.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 18:03:12 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Wu", "Daniel J.", ""], ["Datta", "Avoy", ""]]}, {"id": "2103.16624", "submitter": "Ikechukwu Onyenwe", "authors": "D.C. Asogwa, S.O. Anigbogu, I.E. Onyenwe, F.A. Sani", "title": "Text Classification Using Hybrid Machine Learning Algorithms on Big Data", "comments": "8 pages, 2 figures, 8 tables, Journal", "journal-ref": "International Journal of Trend in Research and Development, Volume\n  6(5), ISSN: 2394-9333, 2019", "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, there are unprecedented data growth originating from different\nonline platforms which contribute to big data in terms of volume, velocity,\nvariety and veracity (4Vs). Given this nature of big data which is\nunstructured, performing analytics to extract meaningful information is\ncurrently a great challenge to big data analytics. Collecting and analyzing\nunstructured textual data allows decision makers to study the escalation of\ncomments/posts on our social media platforms. Hence, there is need for\nautomatic big data analysis to overcome the noise and the non-reliability of\nthese unstructured dataset from the digital media platforms. However, current\nmachine learning algorithms used are performance driven focusing on the\nclassification/prediction accuracy based on known properties learned from the\ntraining samples. With the learning task in a large dataset, most machine\nlearning models are known to require high computational cost which eventually\nleads to computational complexity. In this work, two supervised machine\nlearning algorithms are combined with text mining techniques to produce a\nhybrid model which consists of Na\\\"ive Bayes and support vector machines (SVM).\nThis is to increase the efficiency and accuracy of the results obtained and\nalso to reduce the computational cost and complexity. The system also provides\nan open platform where a group of persons with a common interest can share\ntheir comments/messages and these comments classified automatically as legal or\nillegal. This improves the quality of conversation among users. The hybrid\nmodel was developed using WEKA tools and Java programming language. The result\nshows that the hybrid model gave 96.76% accuracy as against the 61.45% and\n69.21% of the Na\\\"ive Bayes and SVM models respectively.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 19:02:48 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Asogwa", "D. C.", ""], ["Anigbogu", "S. O.", ""], ["Onyenwe", "I. E.", ""], ["Sani", "F. A.", ""]]}, {"id": "2103.16634", "submitter": "Chengxi Ye", "authors": "Chengxi Ye, Xiong Zhou, Tristan McKinney, Yanfeng Liu, Qinggang Zhou,\n  Fedor Zhdanov", "title": "Exploiting Invariance in Training Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by two basic mechanisms in animal visual systems, we introduce a\nfeature transform technique that imposes invariance properties in the training\nof deep neural networks. The resulting algorithm requires less parameter\ntuning, trains well with an initial learning rate 1.0, and easily generalizes\nto different tasks. We enforce scale invariance with local statistics in the\ndata to align similar samples generated in diverse situations. To accelerate\nconvergence, we enforce a GL(n)-invariance property with global statistics\nextracted from a batch that the gradient descent solution should remain\ninvariant under basis change. Tested on ImageNet, MS COCO, and Cityscapes\ndatasets, our proposed technique requires fewer iterations to train, surpasses\nall baselines by a large margin, seamlessly works on both small and large batch\nsize training, and applies to different computer vision tasks of image\nclassification, object detection, and semantic segmentation.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 19:18:31 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Ye", "Chengxi", ""], ["Zhou", "Xiong", ""], ["McKinney", "Tristan", ""], ["Liu", "Yanfeng", ""], ["Zhou", "Qinggang", ""], ["Zhdanov", "Fedor", ""]]}, {"id": "2103.16652", "submitter": "Tobias Lorenz", "authors": "Tobias Lorenz, Anian Ruoss, Mislav Balunovi\\'c, Gagandeep Singh,\n  Martin Vechev", "title": "Robustness Certification for Point Cloud Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of deep 3D point cloud models in safety-critical applications, such\nas autonomous driving, dictates the need to certify the robustness of these\nmodels to semantic transformations. This is technically challenging as it\nrequires a scalable verifier tailored to point cloud models that handles a wide\nrange of semantic 3D transformations. In this work, we address this challenge\nand introduce 3DCertify, the first verifier able to certify robustness of point\ncloud models. 3DCertify is based on two key insights: (i) a generic relaxation\nbased on first-order Taylor approximations, applicable to any differentiable\ntransformation, and (ii) a precise relaxation for global feature pooling, which\nis more complex than pointwise activations (e.g., ReLU or sigmoid) but commonly\nemployed in point cloud models. We demonstrate the effectiveness of 3DCertify\nby performing an extensive evaluation on a wide range of 3D transformations\n(e.g., rotation, twisting) for both classification and part segmentation tasks.\nFor example, we can certify robustness against rotations by $\\pm60^\\circ$ for\n95.7% of point clouds, and our max pool relaxation increases certification by\nup to 15.6%.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 19:52:07 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Lorenz", "Tobias", ""], ["Ruoss", "Anian", ""], ["Balunovi\u0107", "Mislav", ""], ["Singh", "Gagandeep", ""], ["Vechev", "Martin", ""]]}, {"id": "2103.16692", "submitter": "Chao Gao", "authors": "Chao Gao", "title": "On AO*, Proof Number Search and Minimax Search", "comments": "6 pages, 1 page reference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss the interconnections between AO*, adversarial game-searching\nalgorithms, e.g., proof number search and minimax search. The former was\ndeveloped in the context of a general AND/OR graph model, while the latter were\nmostly presented in game-trees which are sometimes modeled using AND/OR trees.\nIt is thus worth investigating to what extent these algorithms are related and\nhow they are connected. In this paper, we explicate the interconnections\nbetween these search paradigms. We argue that generalized proof number search\nmight be regarded as a more informed replacement of AO* for solving arbitrary\nAND/OR graphs, and the minimax principle might also extended to use dual\nheuristics.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 21:27:40 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Gao", "Chao", ""]]}, {"id": "2103.16704", "submitter": "Hongjing Lu", "authors": "Hongjing Lu, Nicholas Ichien, Keith J. Holyoak", "title": "Probabilistic Analogical Mapping with Semantic Relation Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  The human ability to flexibly reason using analogies with domain-general\ncontent depends on mechanisms for identifying relations between concepts, and\nfor mapping concepts and their relations across analogs. Building on a recent\nmodel of how semantic relations can be learned from non-relational word\nembeddings, we present a new computational model of mapping between two\nanalogs. The model adopts a Bayesian framework for probabilistic graph\nmatching, operating on semantic relation networks constructed from distributed\nrepresentations of individual concepts and of relations between concepts.\nThrough comparisons of model predictions with human performance in a novel\nmapping task requiring integration of multiple relations, as well as in several\nclassic studies, we demonstrate that the model accounts for a broad range of\nphenomena involving analogical mapping by both adults and children. We also\nshow the potential for extending the model to deal with analog retrieval. Our\napproach demonstrates that human-like analogical mapping can emerge from\ncomparison mechanisms applied to rich semantic representations of individual\nconcepts and relations.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 22:14:13 GMT"}, {"version": "v2", "created": "Sat, 29 May 2021 20:52:03 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Lu", "Hongjing", ""], ["Ichien", "Nicholas", ""], ["Holyoak", "Keith J.", ""]]}, {"id": "2103.16710", "submitter": "Albert Zeyer", "authors": "Albert Zeyer, Ralf Schl\\\"uter, Hermann Ney", "title": "A study of latent monotonic attention variants", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  End-to-end models reach state-of-the-art performance for speech recognition,\nbut global soft attention is not monotonic, which might lead to convergence\nproblems, to instability, to bad generalisation, cannot be used for online\nstreaming, and is also inefficient in calculation. Monotonicity can potentially\nfix all of this. There are several ad-hoc solutions or heuristics to introduce\nmonotonicity, but a principled introduction is rarely found in literature so\nfar. In this paper, we present a mathematically clean solution to introduce\nmonotonicity, by introducing a new latent variable which represents the audio\nposition or segment boundaries. We compare several monotonic latent models to\nour global soft attention baseline such as a hard attention model, a local\nwindowed soft attention model, and a segmental soft attention model. We can\nshow that our monotonic models perform as good as the global soft attention\nmodel. We perform our experiments on Switchboard 300h. We carefully outline the\ndetails of our training and release our code and configs.\n", "versions": [{"version": "v1", "created": "Tue, 30 Mar 2021 22:35:56 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Zeyer", "Albert", ""], ["Schl\u00fcter", "Ralf", ""], ["Ney", "Hermann", ""]]}, {"id": "2103.16732", "submitter": "Wenyu Han", "authors": "Wenyu Han, Chen Feng, Haoran Wu, Alexander Gao, Armand Jordana, Dong\n  Liu, Lerrel Pinto, Ludovic Righetti", "title": "Simultaneous Navigation and Construction Benchmarking Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We need intelligent robots for mobile construction, the process of navigating\nin an environment and modifying its structure according to a geometric design.\nIn this task, a major robot vision and learning challenge is how to exactly\nachieve the design without GPS, due to the difficulty caused by the\nbi-directional coupling of accurate robot localization and navigation together\nwith strategic environment manipulation. However, many existing robot vision\nand learning tasks such as visual navigation and robot manipulation address\nonly one of these two coupled aspects. To stimulate the pursuit of a generic\nand adaptive solution, we reasonably simplify mobile construction as a\npartially observable Markov decision process (POMDP) in 1/2/3D grid worlds and\nbenchmark the performance of a handcrafted policy with basic localization and\nplanning, and state-of-the-art deep reinforcement learning (RL) methods. Our\nextensive experiments show that the coupling makes this problem very\nchallenging for those methods, and emphasize the need for novel task-specific\nsolutions.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 00:05:54 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Han", "Wenyu", ""], ["Feng", "Chen", ""], ["Wu", "Haoran", ""], ["Gao", "Alexander", ""], ["Jordana", "Armand", ""], ["Liu", "Dong", ""], ["Pinto", "Lerrel", ""], ["Righetti", "Ludovic", ""]]}, {"id": "2103.16746", "submitter": "Xiao Wang", "authors": "Xiao Wang, Xiujun Shu, Zhipeng Zhang, Bo Jiang, Yaowei Wang, Yonghong\n  Tian, Feng Wu", "title": "Towards More Flexible and Accurate Object Tracking with Natural\n  Language: Algorithms and Benchmark", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tracking by natural language specification is a new rising research topic\nthat aims at locating the target object in the video sequence based on its\nlanguage description. Compared with traditional bounding box (BBox) based\ntracking, this setting guides object tracking with high-level semantic\ninformation, addresses the ambiguity of BBox, and links local and global search\norganically together. Those benefits may bring more flexible, robust and\naccurate tracking performance in practical scenarios. However, existing natural\nlanguage initialized trackers are developed and compared on benchmark datasets\nproposed for tracking-by-BBox, which can't reflect the true power of\ntracking-by-language. In this work, we propose a new benchmark specifically\ndedicated to the tracking-by-language, including a large scale dataset, strong\nand diverse baseline methods. Specifically, we collect 2k video sequences\n(contains a total of 1,244,340 frames, 663 words) and split 1300/700 for the\ntrain/testing respectively. We densely annotate one sentence in English and\ncorresponding bounding boxes of the target object for each video. We also\nintroduce two new challenges into TNL2K for the object tracking task, i.e.,\nadversarial samples and modality switch. A strong baseline method based on an\nadaptive local-global-search scheme is proposed for future works to compare. We\nbelieve this benchmark will greatly boost related researches on natural\nlanguage guided tracking.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 00:57:32 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Wang", "Xiao", ""], ["Shu", "Xiujun", ""], ["Zhang", "Zhipeng", ""], ["Jiang", "Bo", ""], ["Wang", "Yaowei", ""], ["Tian", "Yonghong", ""], ["Wu", "Feng", ""]]}, {"id": "2103.16750", "submitter": "Tyler Weitzman", "authors": "Tyler Weitzman and Hoon Pyo (Tim) Jeon", "title": "CloneBot: Personalized Dialogue-Response Predictions", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our project task was to create a model that, given a speaker ID, chat\nhistory, and an utterance query, can predict the response utterance in a\nconversation. The model is personalized for each speaker. This task can be a\nuseful tool for building speech bots that talk in a human-like manner in a live\nconversation. Further, we succeeded at using dense-vector encoding clustering\nto be able to retrieve relevant historical dialogue context, a useful strategy\nfor overcoming the input limitations of neural-based models when predictions\nrequire longer-term references from the dialogue history. In this paper, we\nhave implemented a state-of-the-art model using pre-training and fine-tuning\ntechniques built on transformer architecture and multi-headed attention blocks\nfor the Switchboard corpus. We also show how efficient vector clustering\nalgorithms can be used for real-time utterance predictions that require no\ntraining and therefore work on offline and encrypted message histories.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 01:15:37 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Weitzman", "Tyler", "", "Tim"], ["Pyo", "Hoon", "", "Tim"], ["Jeon", "", ""]]}, {"id": "2103.16758", "submitter": "Jiesi Hu", "authors": "Jiesi Hu, Ganning Zhao, Suya You, C. C. Jay Kuo", "title": "Evaluation of Multimodal Semantic Segmentation using RGB-D Data", "comments": "12 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our goal is to develop stable, accurate, and robust semantic scene\nunderstanding methods for wide-area scene perception and understanding,\nespecially in challenging outdoor environments. To achieve this, we are\nexploring and evaluating a range of related technology and solutions, including\nAI-driven multimodal scene perception, fusion, processing, and understanding.\nThis work reports our efforts on the evaluation of a state-of-the-art approach\nfor semantic segmentation with multiple RGB and depth sensing data. We employ\nfour large datasets composed of diverse urban and terrain scenes and design\nvarious experimental methods and metrics. In addition, we also develop new\nstrategies of multi-datasets learning to improve the detection and recognition\nof unseen objects. Extensive experiments, implementations, and results are\nreported in the paper.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 01:43:43 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Hu", "Jiesi", ""], ["Zhao", "Ganning", ""], ["You", "Suya", ""], ["Kuo", "C. C. Jay", ""]]}, {"id": "2103.16775", "submitter": "Alana Santana Correia De", "authors": "Alana de Santana Correia, Esther Luna Colombini", "title": "Attention, please! A survey of Neural Attention Models in Deep Learning", "comments": "66 pages, 24 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In humans, Attention is a core property of all perceptual and cognitive\noperations. Given our limited ability to process competing sources, attention\nmechanisms select, modulate, and focus on the information most relevant to\nbehavior. For decades, concepts and functions of attention have been studied in\nphilosophy, psychology, neuroscience, and computing. For the last six years,\nthis property has been widely explored in deep neural networks. Currently, the\nstate-of-the-art in Deep Learning is represented by neural attention models in\nseveral application domains. This survey provides a comprehensive overview and\nanalysis of developments in neural attention models. We systematically reviewed\nhundreds of architectures in the area, identifying and discussing those in\nwhich attention has shown a significant impact. We also developed and made\npublic an automated methodology to facilitate the development of reviews in the\narea. By critically analyzing 650 works, we describe the primary uses of\nattention in convolutional, recurrent networks and generative models,\nidentifying common subgroups of uses and applications. Furthermore, we describe\nthe impact of attention in different application domains and their impact on\nneural networks' interpretability. Finally, we list possible trends and\nopportunities for further research, hoping that this review will provide a\nsuccinct overview of the main attentional models in the area and guide\nresearchers in developing future approaches that will drive further\nimprovements.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 02:42:28 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Correia", "Alana de Santana", ""], ["Colombini", "Esther Luna", ""]]}, {"id": "2103.16817", "submitter": "Suraj Nair", "authors": "Annie S. Chen, Suraj Nair, Chelsea Finn", "title": "Learning Generalizable Robotic Reward Functions from \"In-The-Wild\" Human\n  Videos", "comments": "https://sites.google.com/view/dvd-human-videos", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are motivated by the goal of generalist robots that can complete a wide\nrange of tasks across many environments. Critical to this is the robot's\nability to acquire some metric of task success or reward, which is necessary\nfor reinforcement learning, planning, or knowing when to ask for help. For a\ngeneral-purpose robot operating in the real world, this reward function must\nalso be able to generalize broadly across environments, tasks, and objects,\nwhile depending only on on-board sensor observations (e.g. RGB images). While\ndeep learning on large and diverse datasets has shown promise as a path towards\nsuch generalization in computer vision and natural language, collecting high\nquality datasets of robotic interaction at scale remains an open challenge. In\ncontrast, \"in-the-wild\" videos of humans (e.g. YouTube) contain an extensive\ncollection of people doing interesting tasks across a diverse range of\nsettings. In this work, we propose a simple approach, Domain-agnostic Video\nDiscriminator (DVD), that learns multitask reward functions by training a\ndiscriminator to classify whether two videos are performing the same task, and\ncan generalize by virtue of learning from a small amount of robot data with a\nbroad dataset of human videos. We find that by leveraging diverse human\ndatasets, this reward function (a) can generalize zero shot to unseen\nenvironments, (b) generalize zero shot to unseen tasks, and (c) can be combined\nwith visual model predictive control to solve robotic manipulation tasks on a\nreal WidowX200 robot in an unseen environment from a single human demo.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 05:25:05 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Chen", "Annie S.", ""], ["Nair", "Suraj", ""], ["Finn", "Chelsea", ""]]}, {"id": "2103.16828", "submitter": "Wing Yin Yu", "authors": "Wing-Yin Yu, Lai-Man Po, Yuzhi Zhao, Jingjing Xiong, Kin-Wai Lau", "title": "Spatial Content Alignment For Pose Transfer", "comments": "IEEE International Conference on Multimedia and Expo (ICME) 2021 Oral", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Due to unreliable geometric matching and content misalignment, most\nconventional pose transfer algorithms fail to generate fine-trained person\nimages. In this paper, we propose a novel framework Spatial Content Alignment\nGAN (SCAGAN) which aims to enhance the content consistency of garment textures\nand the details of human characteristics. We first alleviate the spatial\nmisalignment by transferring the edge content to the target pose in advance.\nSecondly, we introduce a new Content-Style DeBlk which can progressively\nsynthesize photo-realistic person images based on the appearance features of\nthe source image, the target pose heatmap and the prior transferred content in\nedge domain. We compare the proposed framework with several state-of-the-art\nmethods to show its superiority in quantitative and qualitative analysis.\nMoreover, detailed ablation study results demonstrate the efficacy of our\ncontributions. Codes are publicly available at\ngithub.com/rocketappslab/SCA-GAN.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 06:10:29 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Yu", "Wing-Yin", ""], ["Po", "Lai-Man", ""], ["Zhao", "Yuzhi", ""], ["Xiong", "Jingjing", ""], ["Lau", "Kin-Wai", ""]]}, {"id": "2103.16846", "submitter": "Viktor Csuvik", "authors": "Viktor Csuvik, D\\'aniel Horv\\'ath, M\\'ark Lajk\\'o, L\\'aszl\\'o Vid\\'acs", "title": "Exploring Plausible Patches Using Source Code Embeddings in JavaScript", "comments": "Paper accepted in APR2021 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Despite the immense popularity of the Automated Program Repair (APR) field,\nthe question of patch validation is still open. Most of the present-day\napproaches follow the so-called Generate-and-Validate approach, where first a\ncandidate solution is being generated and after validated against an oracle.\nThe latter, however, might not give a reliable result, because of the\nimperfections in such oracles; one of which is usually the test suite. Although\n(re-) running the test suite is right under one's nose, in real life\napplications the problem of over- and underfitting often occurs, resulting in\ninadequate patches. Efforts that have been made to tackle with this problem\ninclude patch filtering, test suite expansion, careful patch producing and many\nmore. Most approaches to date use post-filtering relying either on test\nexecution traces or make use of some similarity concept measured on the\ngenerated patches. Our goal is to investigate the nature of these\nsimilarity-based approaches. To do so, we trained a Doc2Vec model on an\nopen-source JavaScript project and generated 465 patches for 10 bugs in it.\nThese plausible patches alongside with the developer fix are then ranked based\non their similarity to the original program. We analyzed these similarity lists\nand found that plain document embeddings may lead to misclassification - it\nfails to capture nuanced code semantics. Nevertheless, in some cases it also\nprovided useful information, thus helping to better understand the area of\nAutomated Program Repair.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 06:57:10 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Csuvik", "Viktor", ""], ["Horv\u00e1th", "D\u00e1niel", ""], ["Lajk\u00f3", "M\u00e1rk", ""], ["Vid\u00e1cs", "L\u00e1szl\u00f3", ""]]}, {"id": "2103.16848", "submitter": "Hao Zhou", "authors": "Hao Zhou, Chongyang Zhang, Yan Luo, Yanjun Chen, Chuanping Hu", "title": "Embracing Uncertainty: Decoupling and De-bias for Robust Temporal\n  Grounding", "comments": "Accepted by CVPR 2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Temporal grounding aims to localize temporal boundaries within untrimmed\nvideos by language queries, but it faces the challenge of two types of\ninevitable human uncertainties: query uncertainty and label uncertainty. The\ntwo uncertainties stem from human subjectivity, leading to limited\ngeneralization ability of temporal grounding. In this work, we propose a novel\nDeNet (Decoupling and De-bias) to embrace human uncertainty: Decoupling - We\nexplicitly disentangle each query into a relation feature and a modified\nfeature. The relation feature, which is mainly based on skeleton-like words\n(including nouns and verbs), aims to extract basic and consistent information\nin the presence of query uncertainty. Meanwhile, modified feature assigned with\nstyle-like words (including adjectives, adverbs, etc) represents the subjective\ninformation, and thus brings personalized predictions; De-bias - We propose a\nde-bias mechanism to generate diverse predictions, aim to alleviate the bias\ncaused by single-style annotations in the presence of label uncertainty.\nMoreover, we put forward new multi-label metrics to diversify the performance\nevaluation. Extensive experiments show that our approach is more effective and\nrobust than state-of-the-arts on Charades-STA and ActivityNet Captions\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 07:00:56 GMT"}, {"version": "v2", "created": "Thu, 24 Jun 2021 07:29:25 GMT"}], "update_date": "2021-06-25", "authors_parsed": [["Zhou", "Hao", ""], ["Zhang", "Chongyang", ""], ["Luo", "Yan", ""], ["Chen", "Yanjun", ""], ["Hu", "Chuanping", ""]]}, {"id": "2103.16897", "submitter": "Giovanni Iacca Prof.", "authors": "Ahmed Hallawa, Anil Yaman, Giovanni Iacca, Gerd Ascheid", "title": "A Framework for Knowledge Integrated Evolutionary Algorithms", "comments": "Published in: Squillero G., Sim K. (eds) Applications of Evolutionary\n  Computation. EvoApplications 2017. Lecture Notes in Computer Science, vol\n  10199. Springer, Cham", "journal-ref": null, "doi": "10.1007/978-3-319-55849-3_42", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  One of the main reasons for the success of Evolutionary Algorithms (EAs) is\ntheir general-purposeness, i.e., the fact that they can be applied\nstraightforwardly to a broad range of optimization problems, without any\nspecific prior knowledge. On the other hand, it has been shown that\nincorporating a priori knowledge, such as expert knowledge or empirical\nfindings, can significantly improve the performance of an EA. However,\nintegrating knowledge in EAs poses numerous challenges. It is often the case\nthat the features of the search space are unknown, hence any knowledge\nassociated with the search space properties can be hardly used. In addition, a\npriori knowledge is typically problem-specific and hard to generalize. In this\npaper, we propose a framework, called Knowledge Integrated Evolutionary\nAlgorithm (KIEA), which facilitates the integration of existing knowledge into\nEAs. Notably, the KIEA framework is EA-agnostic (i.e., it works with any\nevolutionary algorithm), problem-independent (i.e., it is not dedicated to a\nspecific type of problems), expandable (i.e., its knowledge base can grow over\ntime). Furthermore, the framework integrates knowledge while the EA is running,\nthus optimizing the use of the needed computational power. In the preliminary\nexperiments shown here, we observe that the KIEA framework produces in the\nworst case an 80% improvement on the converge time, w.r.t. the corresponding\n\"knowledge-free\" EA counterpart.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 08:30:11 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Hallawa", "Ahmed", ""], ["Yaman", "Anil", ""], ["Iacca", "Giovanni", ""], ["Ascheid", "Gerd", ""]]}, {"id": "2103.16898", "submitter": "Wojciech Ozga", "authors": "Wojciech Ozga, Do Le Quoc, Christof Fetzer", "title": "Perun: Secure Multi-Stakeholder Machine Learning Framework with GPU\n  Support", "comments": null, "journal-ref": "The 35th Annual IFIP Conference on Data and Applications Security\n  and Privacy (DBSec 2021)", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Confidential multi-stakeholder machine learning (ML) allows multiple parties\nto perform collaborative data analytics while not revealing their intellectual\nproperty, such as ML source code, model, or datasets. State-of-the-art\nsolutions based on homomorphic encryption incur a large performance overhead.\nHardware-based solutions, such as trusted execution environments (TEEs),\nsignificantly improve the performance in inference computations but still\nsuffer from low performance in training computations, e.g., deep neural\nnetworks model training, because of limited availability of protected memory\nand lack of GPU support.\n  To address this problem, we designed and implemented Perun, a framework for\nconfidential multi-stakeholder machine learning that allows users to make a\ntrade-off between security and performance. Perun executes ML training on\nhardware accelerators (e.g., GPU) while providing security guarantees using\ntrusted computing technologies, such as trusted platform module and integrity\nmeasurement architecture. Less compute-intensive workloads, such as inference,\nexecute only inside TEE, thus at a lower trusted computing base. The evaluation\nshows that during the ML training on CIFAR-10 and real-world medical datasets,\nPerun achieved a 161x to 1560x speedup compared to a pure TEE-based approach.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 08:31:07 GMT"}], "update_date": "2021-06-04", "authors_parsed": [["Ozga", "Wojciech", ""], ["Quoc", "Do Le", ""], ["Fetzer", "Christof", ""]]}, {"id": "2103.16985", "submitter": "Mohamed Sana", "authors": "Mohamed Sana, Mattia Merluzzi, Nicola di Pietro, Emilio Calvanese\n  Strinati", "title": "Energy Efficient Edge Computing: When Lyapunov Meets Distributed\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA cs.NI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the problem of energy-efficient computation offloading\nenabled by edge computing. In the considered scenario, multiple users\nsimultaneously compete for limited radio and edge computing resources to get\noffloaded tasks processed under a delay constraint, with the possibility of\nexploiting low power sleep modes at all network nodes. The radio resource\nallocation takes into account inter- and intra-cell interference, and the duty\ncycles of the radio and computing equipment have to be jointly optimized to\nminimize the overall energy consumption. To address this issue, we formulate\nthe underlying problem as a dynamic long-term optimization. Then, based on\nLyapunov stochastic optimization tools, we decouple the formulated problem into\na CPU scheduling problem and a radio resource allocation problem to be solved\nin a per-slot basis. Whereas the first one can be optimally and efficiently\nsolved using a fast iterative algorithm, the second one is solved using\ndistributed multi-agent reinforcement learning due to its non-convexity and\nNP-hardness. The resulting framework achieves up to 96.5% performance of the\noptimal strategy based on exhaustive search, while drastically reducing\ncomplexity. The proposed solution also allows to increase the network's energy\nefficiency compared to a benchmark heuristic approach.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 11:02:29 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Sana", "Mohamed", ""], ["Merluzzi", "Mattia", ""], ["di Pietro", "Nicola", ""], ["Strinati", "Emilio Calvanese", ""]]}, {"id": "2103.17007", "submitter": "Vyacheslav Yukalov", "authors": "V.I. Yukalov", "title": "Tossing Quantum Coins and Dice", "comments": "26 pages", "journal-ref": "Laser Physics 31 (2021) 055201", "doi": "10.1088/1555-6611/abee8f", "report-no": null, "categories": "quant-ph cs.AI cs.IT math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The procedure of tossing quantum coins and dice is described. This case is an\nimportant example of a quantum procedure because it presents a typical\nframework employed in quantum information processing and quantum computing. The\nemphasis is on the clarification of the difference between quantum and\nclassical conditional probabilities. These probabilities are designed for\ncharacterizing different systems, either quantum or classical, and they,\ngenerally, cannot be reduced to each other. Thus the L\\\"{u}ders probability\ncannot be treated as a generalization of the classical conditional probability.\nThe analogies between quantum theory of measurements and quantum decision\ntheory are elucidated.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 11:39:56 GMT"}], "update_date": "2021-05-26", "authors_parsed": [["Yukalov", "V. I.", ""]]}, {"id": "2103.17045", "submitter": "Yusheng Peng", "authors": "Yusheng Peng, Gaofeng Zhang, Jun Shi, Benzhu Xu, Liping Zheng", "title": "SRA-LSTM: Social Relationship Attention LSTM for Human Trajectory\n  Prediction", "comments": "Submitted to Neural Computing and Applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pedestrian trajectory prediction for surveillance video is one of the\nimportant research topics in the field of computer vision and a key technology\nof intelligent surveillance systems. Social relationship among pedestrians is a\nkey factor influencing pedestrian walking patterns but was mostly ignored in\nthe literature. Pedestrians with different social relationships play different\nroles in the motion decision of target pedestrian. Motivated by this idea, we\npropose a Social Relationship Attention LSTM (SRA-LSTM) model to predict future\ntrajectories. We design a social relationship encoder to obtain the\nrepresentation of their social relationship through the relative position\nbetween each pair of pedestrians. Afterwards, the social relationship feature\nand latent movements are adopted to acquire the social relationship attention\nof this pair of pedestrians. Social interaction modeling is achieved by\nutilizing social relationship attention to aggregate movement information from\nneighbor pedestrians. Experimental results on two public walking pedestrian\nvideo datasets (ETH and UCY), our model achieves superior performance compared\nwith state-of-the-art methods. Contrast experiments with other attention\nmethods also demonstrate the effectiveness of social relationship attention.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 12:56:39 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Peng", "Yusheng", ""], ["Zhang", "Gaofeng", ""], ["Shi", "Jun", ""], ["Xu", "Benzhu", ""], ["Zheng", "Liping", ""]]}, {"id": "2103.17047", "submitter": "Yidong Liao", "authors": "Yidong Liao, Min-Hsiu Hsieh, Chris Ferrie", "title": "Quantum Optimization for Training Quantum Neural Networks", "comments": "35 pages, 30 figures, 1 animation", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-nd/4.0/", "abstract": "  Training quantum neural networks (QNNs) using gradient-based or gradient-free\nclassical optimisation approaches is severely impacted by the presence of\nbarren plateaus in the cost landscapes. In this paper, we devise a framework\nfor leveraging quantum optimisation algorithms to find optimal parameters of\nQNNs for certain tasks. To achieve this, we coherently encode the cost function\nof QNNs onto relative phases of a superposition state in the Hilbert space of\nthe network parameters. The parameters are tuned with an iterative quantum\noptimisation structure using adaptively selected Hamiltonians. The quantum\nmechanism of this framework exploits hidden structure in the QNN optimisation\nproblem and hence is expected to provide beyond-Grover speed up, mitigating the\nbarren plateau issue.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 13:06:30 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Liao", "Yidong", ""], ["Hsieh", "Min-Hsiu", ""], ["Ferrie", "Chris", ""]]}, {"id": "2103.17086", "submitter": "Peter Tan", "authors": "Dayu Tan, Zheng Huang, Xin Peng, Weimin Zhong, Vladimir Mahalec", "title": "Deep adaptive fuzzy clustering for evolutionary unsupervised\n  representation learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cluster assignment of large and complex images is a crucial but challenging\ntask in pattern recognition and computer vision. In this study, we explore the\npossibility of employing fuzzy clustering in a deep neural network framework.\nThus, we present a novel evolutionary unsupervised learning representation\nmodel with iterative optimization. It implements the deep adaptive fuzzy\nclustering (DAFC) strategy that learns a convolutional neural network\nclassifier from given only unlabeled data samples. DAFC consists of a deep\nfeature quality-verifying model and a fuzzy clustering model, where deep\nfeature representation learning loss function and embedded fuzzy clustering\nwith the weighted adaptive entropy is implemented. We joint fuzzy clustering to\nthe deep reconstruction model, in which fuzzy membership is utilized to\nrepresent a clear structure of deep cluster assignments and jointly optimize\nfor the deep representation learning and clustering. Also, the joint model\nevaluates current clustering performance by inspecting whether the re-sampled\ndata from estimated bottleneck space have consistent clustering properties to\nprogressively improve the deep clustering model. Comprehensive experiments on a\nvariety of datasets show that the proposed method obtains a substantially\nbetter performance for both reconstruction and clustering quality when compared\nto the other state-of-the-art deep clustering methods, as demonstrated with the\nin-depth analysis in the extensive experiments.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 13:58:10 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Tan", "Dayu", ""], ["Huang", "Zheng", ""], ["Peng", "Xin", ""], ["Zhong", "Weimin", ""], ["Mahalec", "Vladimir", ""]]}, {"id": "2103.17144", "submitter": "Mani Sotoodeh", "authors": "Mani Sotoodeh, Li Xiong and Joyce C. Ho", "title": "CrowdTeacher: Robust Co-teaching with Noisy Answers & Sample-specific\n  Perturbations for Tabular Data", "comments": null, "journal-ref": null, "doi": "10.1007/978-3-030-75765-6_15", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Samples with ground truth labels may not always be available in numerous\ndomains. While learning from crowdsourcing labels has been explored, existing\nmodels can still fail in the presence of sparse, unreliable, or diverging\nannotations. Co-teaching methods have shown promising improvements for computer\nvision problems with noisy labels by employing two classifiers trained on each\nothers' confident samples in each batch. Inspired by the idea of separating\nconfident and uncertain samples during the training process, we extend it for\nthe crowdsourcing problem. Our model, CrowdTeacher, uses the idea that\nperturbation in the input space model can improve the robustness of the\nclassifier for noisy labels. Treating crowdsourcing annotations as a source of\nnoisy labeling, we perturb samples based on the certainty from the aggregated\nannotations. The perturbed samples are fed to a Co-teaching algorithm tuned to\nalso accommodate smaller tabular data. We showcase the boost in predictive\npower attained using CrowdTeacher for both synthetic and real datasets across\nvarious label density settings. Our experiments reveal that our proposed\napproach beats baselines modeling individual annotations and then combining\nthem, methods simultaneously learning a classifier and inferring truth labels,\nand the Co-teaching algorithm with aggregated labels through common truth\ninference methods.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 15:09:38 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Sotoodeh", "Mani", ""], ["Xiong", "Li", ""], ["Ho", "Joyce C.", ""]]}, {"id": "2103.17171", "submitter": "Joona Pohjonen", "authors": "Joona Pohjonen, Carolin St\\\"urenberg, Antti Rannikko, Tuomas Mirtti,\n  Esa Pitk\\\"anen", "title": "Spectral decoupling allows training transferable neural networks in\n  medical imaging", "comments": "8 pages, 5 figures and 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "eess.IV cs.AI cs.CV cs.LG stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Many current neural networks for medical imaging generalise poorly to data\nunseen during training. Such behaviour can be caused by networks overfitting\neasy-to-learn, or statistically dominant, features while disregarding other\npotentially informative features. For example, indistinguishable differences in\nthe sharpness of the images from two different scanners can degrade the\nperformance of the network significantly. All neural networks intended for\nclinical practice need to be robust to variation in data caused by differences\nin imaging equipment, sample preparation and patient populations.\n  To address these challenges, we evaluate the utility of spectral decoupling\nas an implicit bias mitigation method. Spectral decoupling encourages the\nneural network to learn more features by simply regularising the networks'\nunnormalised prediction scores with an L2 penalty, thus having no added\ncomputational costs.\n  We show that spectral decoupling allows training neural networks on datasets\nwith strong spurious correlations. Networks trained without spectral decoupling\ndo not learn the original task and appear to make false predictions based on\nthe spurious correlations. Spectral decoupling also increases networks'\nrobustness for data distribution shifts. To validate our findings, we train\nnetworks with and without spectral decoupling to detect prostate cancer tissue\nslides and COVID-19 in chest radiographs. Networks trained with spectral\ndecoupling achieve substantially higher performance on all evaluation datasets.\n  Our results show that spectral decoupling helps with generalisation issues\nassociated with neural networks. We recommend using spectral decoupling as an\nimplicit bias mitigation method in any neural network intended for clinical\nuse.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 15:47:01 GMT"}, {"version": "v2", "created": "Fri, 9 Apr 2021 12:11:58 GMT"}, {"version": "v3", "created": "Thu, 8 Jul 2021 13:36:37 GMT"}], "update_date": "2021-07-09", "authors_parsed": [["Pohjonen", "Joona", ""], ["St\u00fcrenberg", "Carolin", ""], ["Rannikko", "Antti", ""], ["Mirtti", "Tuomas", ""], ["Pitk\u00e4nen", "Esa", ""]]}, {"id": "2103.17185", "submitter": "Dmytro Kotovenko", "authors": "Dmytro Kotovenko, Matthias Wright, Arthur Heimbrecht, Bj\\\"orn Ommer", "title": "Rethinking Style Transfer: From Pixels to Parameterized Brushstrokes", "comments": "Accepted at CVPR2021", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  There have been many successful implementations of neural style transfer in\nrecent years. In most of these works, the stylization process is confined to\nthe pixel domain. However, we argue that this representation is unnatural\nbecause paintings usually consist of brushstrokes rather than pixels. We\npropose a method to stylize images by optimizing parameterized brushstrokes\ninstead of pixels and further introduce a simple differentiable rendering\nmechanism. Our approach significantly improves visual quality and enables\nadditional control over the stylization process such as controlling the flow of\nbrushstrokes through user input. We provide qualitative and quantitative\nevaluations that show the efficacy of the proposed parameterized\nrepresentation.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 16:15:03 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Kotovenko", "Dmytro", ""], ["Wright", "Matthias", ""], ["Heimbrecht", "Arthur", ""], ["Ommer", "Bj\u00f6rn", ""]]}, {"id": "2103.17191", "submitter": "Pushkar Mishra", "authors": "Pushkar Mishra, Helen Yannakoudakis, Ekaterina Shutova", "title": "Modeling Users and Online Communities for Abuse Detection: A Position on\n  Ethics and Explainability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abuse on the Internet is an important societal problem of our time. Millions\nof Internet users face harassment, racism, personal attacks, and other types of\nabuse across various platforms. The psychological effects of abuse on\nindividuals can be profound and lasting. Consequently, over the past few years,\nthere has been a substantial research effort towards automated abusive language\ndetection in the field of NLP. In this position paper, we discuss the role that\nmodeling of users and online communities plays in abuse detection.\nSpecifically, we review and analyze the state of the art methods that leverage\nuser or community information to enhance the understanding and detection of\nabusive language. We then explore the ethical challenges of incorporating user\nand community information, laying out considerations to guide future research.\nFinally, we address the topic of explainability in abusive language detection,\nproposing properties that an explainable method should aim to exhibit. We\ndescribe how user and community information can facilitate the realization of\nthese properties and discuss the effective operationalization of explainability\nin view of the properties.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 16:20:37 GMT"}, {"version": "v2", "created": "Wed, 14 Apr 2021 12:35:02 GMT"}], "update_date": "2021-04-15", "authors_parsed": [["Mishra", "Pushkar", ""], ["Yannakoudakis", "Helen", ""], ["Shutova", "Ekaterina", ""]]}, {"id": "2103.17228", "submitter": "Antonio Norelli", "authors": "Antonio Norelli and Alessandro Panconesi", "title": "OLIVAW: Mastering Othello with neither Humans nor a Penny", "comments": "Presented at AAAI-21 Reinforcement Learning in Games Workshop, 7\n  pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce OLIVAW, an AI Othello player adopting the design principles of\nthe famous AlphaGo series. The main motivation behind OLIVAW was to attain\nexceptional competence in a non-trivial board game at a tiny fraction of the\ncost of its illustrious predecessors. In this paper, we show how the AlphaGo\nZero's paradigm can be successfully applied to the popular game of Othello\nusing only commodity hardware and free cloud services. While being simpler than\nChess or Go, Othello maintains a considerable search space and difficulty in\nevaluating board positions. To achieve this result, OLIVAW implements some\nimprovements inspired by recent works to accelerate the standard AlphaGo Zero\nlearning process. The main modification implies doubling the positions\ncollected per game during the training phase, by including also positions not\nplayed but largely explored by the agent. We tested the strength of OLIVAW in\nthree different ways: by pitting it against Edax, the strongest open-source\nOthello engine, by playing anonymous games on the web platform OthelloQuest,\nand finally in two in-person matches against top-notch human players: a\nnational champion and a former world champion.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 17:21:52 GMT"}, {"version": "v2", "created": "Mon, 21 Jun 2021 14:39:03 GMT"}, {"version": "v3", "created": "Tue, 22 Jun 2021 09:08:03 GMT"}], "update_date": "2021-06-23", "authors_parsed": [["Norelli", "Antonio", ""], ["Panconesi", "Alessandro", ""]]}, {"id": "2103.17245", "submitter": "Enis Karaarslan Dr.", "authors": "\\\"Ozg\\\"ur Dogan, Oguzhan Sahin, Enis Karaarslan", "title": "Digital Twin Based Disaster Management System Proposal: DT-DMS", "comments": "5 pages, 6 figures", "journal-ref": "Journal of Emerging Computer Technologies (JECT), 2021, Vol:1 (2),\n  25-30", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The damage and the impact of natural disasters are becoming more destructive\nwith the increase of urbanization. Today's metropolitan cities are not\nsufficiently prepared for the pre and post-disaster situations. Digital Twin\ntechnology can provide a solution. A virtual copy of the physical city could be\ncreated by collecting data from sensors of the Internet of Things (IoT) devices\nand stored on the cloud infrastructure. This virtual copy is kept current and\nup to date with the continuous flow of the data coming from the sensors. We\npropose a disaster management system utilizing machine learning called DT-DMS\nis used to support decision-making mechanisms. This study aims to show how to\neducate and prepare emergency center staff by simulating potential disaster\nsituations on the virtual copy. The event of a disaster will be simulated\nallowing emergency center staff to make decisions and depicting the potential\noutcomes of these decisions. A rescue operation after an earthquake is\nsimulated. Test results are promising and the simulation scope is planned to be\nextended.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 17:47:15 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Dogan", "\u00d6zg\u00fcr", ""], ["Sahin", "Oguzhan", ""], ["Karaarslan", "Enis", ""]]}, {"id": "2103.17258", "submitter": "Hiroki Furuta", "authors": "Hiroki Furuta, Tadashi Kozuno, Tatsuya Matsushima, Yutaka Matsuo,\n  Shixiang Shane Gu", "title": "Co-Adaptation of Algorithmic and Implementational Innovations in\n  Inference-based Deep Reinforcement Learning", "comments": "The implementation is available at:\n  https://github.com/frt03/inference-based-rl", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Recently many algorithms were devised for reinforcement learning (RL) with\nfunction approximation. While they have clear algorithmic distinctions, they\nalso have many implementation differences that are algorithm-independent and\nsometimes under-emphasized. Such mixing of algorithmic novelty and\nimplementation craftsmanship makes rigorous analyses of the sources of\nperformance improvements across algorithms difficult. In this work, we focus on\na series of off-policy inference-based actor-critic algorithms -- MPO, AWR, and\nSAC -- to decouple their algorithmic innovations and implementation decisions.\nWe present unified derivations through a single control-as-inference objective,\nwhere we can categorize each algorithm as based on either\nExpectation-Maximization (EM) or direct Kullback-Leibler (KL) divergence\nminimization and treat the rest of specifications as implementation details. We\nperformed extensive ablation studies, and identified substantial performance\ndrops whenever implementation details are mismatched for algorithmic choices.\nThese results show which implementation details are co-adapted and co-evolved\nwith algorithms, and which are transferable across algorithms: as examples, we\nidentified that tanh Gaussian policy and network sizes are highly adapted to\nalgorithmic types, while layer normalization and ELU are critical for MPO's\nperformances but also transfer to noticeable gains in SAC. We hope our work can\ninspire future work to further demystify sources of performance improvements\nacross multiple algorithms and allow researchers to build on one another's both\nalgorithmic and implementational innovations.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 17:55:20 GMT"}, {"version": "v2", "created": "Fri, 28 May 2021 21:27:30 GMT"}], "update_date": "2021-06-01", "authors_parsed": [["Furuta", "Hiroki", ""], ["Kozuno", "Tadashi", ""], ["Matsushima", "Tatsuya", ""], ["Matsuo", "Yutaka", ""], ["Gu", "Shixiang Shane", ""]]}, {"id": "2103.17267", "submitter": "Adrian Bulat", "authors": "Adrian Bulat and Georgios Tzimiropoulos", "title": "Bit-Mixer: Mixed-precision networks with runtime bit-width selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mixed-precision networks allow for a variable bit-width quantization for\nevery layer in the network. A major limitation of existing work is that the\nbit-width for each layer must be predefined during training time. This allows\nlittle flexibility if the characteristics of the device on which the network is\ndeployed change during runtime. In this work, we propose Bit-Mixer, the very\nfirst method to train a meta-quantized network where during test time any layer\ncan change its bid-width without affecting at all the overall network's ability\nfor highly accurate inference. To this end, we make 2 key contributions: (a)\nTransitional Batch-Norms, and (b) a 3-stage optimization process which is shown\ncapable of training such a network. We show that our method can result in mixed\nprecision networks that exhibit the desirable flexibility properties for\non-device deployment without compromising accuracy. Code will be made\navailable.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 17:58:47 GMT"}], "update_date": "2021-04-01", "authors_parsed": [["Bulat", "Adrian", ""], ["Tzimiropoulos", "Georgios", ""]]}, {"id": "2103.17268", "submitter": "Zhouxing Shi", "authors": "Zhouxing Shi, Yihan Wang, Huan Zhang, Jinfeng Yi, Cho-Jui Hsieh", "title": "Fast Certified Robust Training with Short Warmup", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, bound propagation based certified robust training methods have been\nproposed for training neural networks with certifiable robustness guarantees.\nDespite that state-of-the-art (SOTA) methods including interval bound\npropagation (IBP) and CROWN-IBP have per-batch training complexity similar to\nstandard neural network training, they usually use a long warmup schedule with\nhundreds or thousands epochs to reach SOTA performance and are thus still\ncostly. In this paper, we identify two important issues in existing methods,\nnamely exploded bounds at initialization, and the imbalance in ReLU activation\nstates. These two issues make certified training difficult and unstable, and\nthereby long warmup schedules were needed in prior works. To mitigate these\nissues and conduct certified training with shorter warmup, we propose three\nimprovements: 1) We derive a new weight initialization method for IBP training;\n2) We propose to fully add Batch Normalization (BN) to each layer in the model,\nsince we find BN can reduce the imbalance in ReLU activation states; 3) We also\ndesign regularization to explicitly tighten certified bounds and balance ReLU\nactivation states. In our experiments, we are able to obtain 65.03% verified\nerror on CIFAR-10 ($\\epsilon=\\frac{8}{255}$) and 82.36% verified error on\nTinyImageNet ($\\epsilon=\\frac{1}{255}$) using very short training schedules\n(160 and 80 total epochs, respectively), outperforming literature SOTA trained\nwith hundreds or thousands epochs under the same network architecture.\n", "versions": [{"version": "v1", "created": "Wed, 31 Mar 2021 17:58:58 GMT"}, {"version": "v2", "created": "Thu, 1 Apr 2021 17:35:36 GMT"}, {"version": "v3", "created": "Wed, 23 Jun 2021 16:07:37 GMT"}], "update_date": "2021-06-24", "authors_parsed": [["Shi", "Zhouxing", ""], ["Wang", "Yihan", ""], ["Zhang", "Huan", ""], ["Yi", "Jinfeng", ""], ["Hsieh", "Cho-Jui", ""]]}]