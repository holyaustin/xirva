[{"id": "1502.00062", "submitter": "Mallenahalli Naresh Kumar Prof. Dr.", "authors": "Vadrevu Sree Hari Rao and Mallenahalli Naresh Kumar", "title": "A New Intelligence Based Approach for Computer-Aided Diagnosis of Dengue\n  Fever", "comments": "7 pages, 5 figures. arXiv admin note: substantial text overlap with\n  arXiv:1501.07093", "journal-ref": "Information Technology in Biomedicine, IEEE Transactions on ,\n  vol.16, no.1, pp.112,118, Jan. 2012", "doi": "10.1109/TITB.2011.2171978", "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Identification of the influential clinical symptoms and laboratory features\nthat help in the diagnosis of dengue fever in early phase of the illness would\naid in designing effective public health management and virological\nsurveillance strategies. Keeping this as our main objective we develop in this\npaper, a new computational intelligence based methodology that predicts the\ndiagnosis in real time, minimizing the number of false positives and false\nnegatives. Our methodology consists of three major components (i) a novel\nmissing value imputation procedure that can be applied on any data set\nconsisting of categorical (nominal) and/or numeric (real or integer) (ii) a\nwrapper based features selection method with genetic search for extracting a\nsubset of most influential symptoms that can diagnose the illness and (iii) an\nalternating decision tree method that employs boosting for generating highly\naccurate decision rules. The predictive models developed using our methodology\nare found to be more accurate than the state-of-the-art methodologies used in\nthe diagnosis of the dengue fever.\n", "versions": [{"version": "v1", "created": "Sat, 31 Jan 2015 03:15:12 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Rao", "Vadrevu Sree Hari", ""], ["Kumar", "Mallenahalli Naresh", ""]]}, {"id": "1502.00130", "submitter": "Joseph Corneli", "authors": "Joseph Corneli and Ewen Maclean", "title": "The Search for Computational Intelligence", "comments": "8 pages. Submitted to Social Aspects of Cognition and Computing\n  symposium at AISB 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  We define and explore in simulation several rules for the local evolution of\ngenerative rules for 1D and 2D cellular automata. Our implementation uses\nstrategies from conceptual blending. We discuss potential applications to\nmodelling social dynamics.\n", "versions": [{"version": "v1", "created": "Sat, 31 Jan 2015 16:10:29 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Corneli", "Joseph", ""], ["Maclean", "Ewen", ""]]}, {"id": "1502.00152", "submitter": "Samantha Leung", "authors": "Joseph Y. Halpern, Samantha Leung", "title": "Minimizing Regret in Dynamic Decision Problems", "comments": "Full version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The menu-dependent nature of regret-minimization creates subtleties when it\nis applied to dynamic decision problems. Firstly, it is not clear whether\n\\emph{forgone opportunities} should be included in the \\emph{menu}, with\nrespect to which regrets are computed, at different points of the decision\nproblem. If forgone opportunities are included, however, we can characterize\nwhen a form of dynamic consistency is guaranteed. Secondly, more subtleties\narise when sophistication is used to deal with dynamic inconsistency. In the\nfull version of this paper, we examine, axiomatically and by common examples,\nthe implications of different menu definitions for sophisticated,\nregret-minimizing agents.\n", "versions": [{"version": "v1", "created": "Sat, 31 Jan 2015 19:17:54 GMT"}, {"version": "v2", "created": "Thu, 18 Jun 2015 16:35:53 GMT"}], "update_date": "2015-06-19", "authors_parsed": [["Halpern", "Joseph Y.", ""], ["Leung", "Samantha", ""]]}, {"id": "1502.00245", "submitter": "Christian Samuel Perone", "authors": "Christian S. Perone", "title": "Injury risk prediction for traffic accidents in Porto Alegre/RS, Brazil", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  This study describes the experimental application of Machine Learning\ntechniques to build prediction models that can assess the injury risk\nassociated with traffic accidents. This work uses an freely available data set\nof traffic accident records that took place in the city of Porto Alegre/RS\n(Brazil) during the year of 2013. This study also provides an analysis of the\nmost important attributes of a traffic accident that could produce an outcome\nof injury to the people involved in the accident.\n", "versions": [{"version": "v1", "created": "Sun, 1 Feb 2015 12:57:40 GMT"}], "update_date": "2015-02-03", "authors_parsed": [["Perone", "Christian S.", ""]]}, {"id": "1502.00725", "submitter": "Hongwei Li", "authors": "Hongwei Li and Qiang Liu", "title": "Cheaper and Better: Selecting Good Workers for Crowdsourcing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Crowdsourcing provides a popular paradigm for data collection at scale. We\nstudy the problem of selecting subsets of workers from a given worker pool to\nmaximize the accuracy under a budget constraint. One natural question is\nwhether we should hire as many workers as the budget allows, or restrict on a\nsmall number of top-quality workers. By theoretically analyzing the error rate\nof a typical setting in crowdsourcing, we frame the worker selection problem\ninto a combinatorial optimization problem and propose an algorithm to solve it\nefficiently. Empirical results on both simulated and real-world datasets show\nthat our algorithm is able to select a small number of high-quality workers,\nand performs as good as, sometimes even better than, the much larger crowds as\nthe budget allows.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 03:45:48 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Li", "Hongwei", ""], ["Liu", "Qiang", ""]]}, {"id": "1502.01075", "submitter": "Ehtibar Dzhafarov", "authors": "Damir D. Dzhafarov and Ehtibar N. Dzhafarov", "title": "Classificatory Sorites, Probabilistic Supervenience, and Rule-Making", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We view sorites in terms of stimuli acting upon a system and evoking this\nsystem's responses. Supervenience of responses on stimuli implies that they\neither lack tolerance (i.e., they change in every vicinity of some of the\nstimuli), or stimuli are not always connectable by finite chains of stimuli in\nwhich successive members are `very similar'. If supervenience does not hold,\nthe properties of tolerance and connectedness cannot be formulated and\ntherefore soritical sequences cannot be constructed. We hypothesize that\nsupervenience in empirical systems (such as people answering questions) is\nfundamentally probabilistic. The supervenience of probabilities of responses on\nstimuli is stable, in the sense that `higher-order' probability distributions\ncan always be reduced to `ordinary' ones. In making rules about which stimuli\nought to correspond to which responses, the main characterization of choices in\nsoritical situations is their arbitrariness. We argue that arbitrariness poses\nno problems for classical logic.\n", "versions": [{"version": "v1", "created": "Wed, 4 Feb 2015 01:15:00 GMT"}, {"version": "v2", "created": "Thu, 23 Apr 2015 23:56:31 GMT"}], "update_date": "2015-04-27", "authors_parsed": [["Dzhafarov", "Damir D.", ""], ["Dzhafarov", "Ehtibar N.", ""]]}, {"id": "1502.01321", "submitter": "Sukanta Nayak", "authors": "Sukanta Nayak and Snehashish Chakraverty", "title": "Numerical Solution of Fuzzy Stochastic Differential Equation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NA cs.AI math-ph math.MP math.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper an alternative approach to solve uncertain Stochastic\nDifferential Equation (SDE) is proposed. This uncertainty occurs due to the\ninvolved parameters in system and these are considered as Triangular Fuzzy\nNumbers (TFN). Here the proposed fuzzy arithmetic in [2] is used as a tool to\nhandle Fuzzy Stochastic Differential Equation (FSDE). In particular, a system\nof Ito stochastic differential equations is analysed with fuzzy parameters.\nFurther exact and Euler Maruyama approximation methods with fuzzy values are\ndemonstrated and solved some standard SDE.\n", "versions": [{"version": "v1", "created": "Tue, 3 Feb 2015 05:52:25 GMT"}], "update_date": "2015-02-11", "authors_parsed": [["Nayak", "Sukanta", ""], ["Chakraverty", "Snehashish", ""]]}, {"id": "1502.01497", "submitter": "Tomas Teijeiro", "authors": "Tom\\'as Teijeiro, Paulo F\\'elix and Jes\\'us Presedo", "title": "Using temporal abduction for biosignal interpretation: A case study on\n  QRS detection", "comments": "7 pages, Healthcare Informatics (ICHI), 2014 IEEE International\n  Conference on", "journal-ref": null, "doi": "10.1109/ICHI.2014.52", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we propose an abductive framework for biosignal interpretation,\nbased on the concept of Temporal Abstraction Patterns. A temporal abstraction\npattern defines an abstraction relation between an observation hypothesis and a\nset of observations constituting its evidence support. New observations are\ngenerated abductively from any subset of the evidence of a pattern, building an\nabstraction hierarchy of observations in which higher levels contain those\nobservations with greater interpretative value of the physiological processes\nunderlying a given signal. Non-monotonic reasoning techniques have been applied\nto this model in order to find the best interpretation of a set of initial\nobservations, permitting even to correct these observations by removing, adding\nor modifying them in order to make them consistent with the available domain\nknowledge. Some preliminary experiments have been conducted to apply this\nframework to a well known and bounded problem: the QRS detection on ECG\nsignals. The objective is not to provide a new better QRS detector, but to test\nthe validity of an abductive paradigm. These experiments show that a knowledge\nbase comprising just a few very simple rhythm abstraction patterns can enhance\nthe results of a state of the art algorithm by significantly improving its\ndetection F1-score, besides proving the ability of the abductive framework to\ncorrect both sensitivity and specificity failures.\n", "versions": [{"version": "v1", "created": "Thu, 5 Feb 2015 10:57:07 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Teijeiro", "Tom\u00e1s", ""], ["F\u00e9lix", "Paulo", ""], ["Presedo", "Jes\u00fas", ""]]}, {"id": "1502.01852", "submitter": "Kaiming He", "authors": "Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun", "title": "Delving Deep into Rectifiers: Surpassing Human-Level Performance on\n  ImageNet Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rectified activation units (rectifiers) are essential for state-of-the-art\nneural networks. In this work, we study rectifier neural networks for image\nclassification from two aspects. First, we propose a Parametric Rectified\nLinear Unit (PReLU) that generalizes the traditional rectified unit. PReLU\nimproves model fitting with nearly zero extra computational cost and little\noverfitting risk. Second, we derive a robust initialization method that\nparticularly considers the rectifier nonlinearities. This method enables us to\ntrain extremely deep rectified models directly from scratch and to investigate\ndeeper or wider network architectures. Based on our PReLU networks\n(PReLU-nets), we achieve 4.94% top-5 test error on the ImageNet 2012\nclassification dataset. This is a 26% relative improvement over the ILSVRC 2014\nwinner (GoogLeNet, 6.66%). To our knowledge, our result is the first to surpass\nhuman-level performance (5.1%, Russakovsky et al.) on this visual recognition\nchallenge.\n", "versions": [{"version": "v1", "created": "Fri, 6 Feb 2015 10:44:00 GMT"}], "update_date": "2015-02-09", "authors_parsed": [["He", "Kaiming", ""], ["Zhang", "Xiangyu", ""], ["Ren", "Shaoqing", ""], ["Sun", "Jian", ""]]}, {"id": "1502.01972", "submitter": "Michael Saint-Guillain", "authors": "Michael Saint-Guillain, Yves Deville and Christine Solnon", "title": "A Multistage Stochastic Programming Approach to the Dynamic and\n  Stochastic VRPTW - Extended version", "comments": "Extended version of the same-name study submitted for publication in\n  conference CPAIOR2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a dynamic vehicle routing problem with time windows and\nstochastic customers (DS-VRPTW), such that customers may request for services\nas vehicles have already started their tours. To solve this problem, the goal\nis to provide a decision rule for choosing, at each time step, the next action\nto perform in light of known requests and probabilistic knowledge on requests\nlikelihood. We introduce a new decision rule, called Global Stochastic\nAssessment (GSA) rule for the DS-VRPTW, and we compare it with existing\ndecision rules, such as MSA. In particular, we show that GSA fully integrates\nnonanticipativity constraints so that it leads to better decisions in our\nstochastic context. We describe a new heuristic approach for efficiently\napproximating our GSA rule. We introduce a new waiting strategy. Experiments on\ndynamic and stochastic benchmarks, which include instances of different degrees\nof dynamism, show that not only our approach is competitive with\nstate-of-the-art methods, but also enables to compute meaningful offline\nsolutions to fully dynamic problems where absolutely no a priori customer\nrequest is provided.\n", "versions": [{"version": "v1", "created": "Fri, 6 Feb 2015 18:11:09 GMT"}], "update_date": "2015-02-09", "authors_parsed": [["Saint-Guillain", "Michael", ""], ["Deville", "Yves", ""], ["Solnon", "Christine", ""]]}, {"id": "1502.02029", "submitter": "Lu\\'is Tarrataca", "authors": "Lu\\'is Tarrataca and Andreas Wichert", "title": "A Quantum Production Model", "comments": null, "journal-ref": "Quantum Information Processing, 2012,11:1, 189-209", "doi": "10.1007/s11128-011-0241-2", "report-no": null, "categories": "cs.AI quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The production system is a theoretical model of computation relevant to the\nartificial intelligence field allowing for problem solving procedures such as\nhierarchical tree search. In this work we explore some of the connections\nbetween artificial intelligence and quantum computation by presenting a model\nfor a quantum production system. Our approach focuses on initially developing a\nmodel for a reversible production system which is a simple mapping of Bennett's\nreversible Turing machine. We then expand on this result in order to\naccommodate for the requirements of quantum computation. We present the details\nof how our proposition can be used alongside Grover's algorithm in order to\nyield a speedup comparatively to its classical counterpart. We discuss the\nrequirements associated with such a speedup and how it compares against a\nsimilar quantum hierarchical search approach.\n", "versions": [{"version": "v1", "created": "Fri, 6 Feb 2015 17:18:03 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Tarrataca", "Lu\u00eds", ""], ["Wichert", "Andreas", ""]]}, {"id": "1502.02193", "submitter": "Liane Gabora", "authors": "Liane Gabora", "title": "The Silver Lining Around Fearful Living", "comments": "4 pages, Psychology Today (online).\n  https://www.psychologytoday.com/blog/mindbloggling/201502/the-silver-lining-around-fearful-living-0\n  (2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses in layperson's terms human and computational studies of\nthe impact of threat and fear on exploration and creativity. A first study\nshowed that both killifish from a lake with predators and from a lake without\npredators explore a new environment to the same degree and plotting number of\nnew spaces covered over time generates a hump-shaped curve. However, for the\nfish from the lake with predators the curve is shifted to the right; they take\nlonger. This pattern was replicated by a computer model of exploratory behavior\nvarying only one parameter, the fear parameter. A second study showed that\nstories inspired by threatening photographs were rated as more creative than\nstories inspired by non-threatening photographs. Various explanations for the\nfindings are discussed.\n", "versions": [{"version": "v1", "created": "Sat, 7 Feb 2015 23:27:48 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Gabora", "Liane", ""]]}, {"id": "1502.02298", "submitter": "Jamal Atif", "authors": "Marc Aiguier and Jamal Atif and Isabelle Bloch and C\\'eline Hudelot", "title": "Belief Revision, Minimal Change and Relaxation: A General Framework\n  based on Satisfaction Systems, and Applications to Description Logics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Belief revision of knowledge bases represented by a set of sentences in a\ngiven logic has been extensively studied but for specific logics, mainly\npropositional, and also recently Horn and description logics. Here, we propose\nto generalize this operation from a model-theoretic point of view, by defining\nrevision in an abstract model theory known under the name of satisfaction\nsystems. In this framework, we generalize to any satisfaction systems the\ncharacterization of the well known AGM postulates given by Katsuno and\nMendelzon for propositional logic in terms of minimal change among\ninterpretations. Moreover, we study how to define revision, satisfying the AGM\npostulates, from relaxation notions that have been first introduced in\ndescription logics to define dissimilarity measures between concepts, and the\nconsequence of which is to relax the set of models of the old belief until it\nbecomes consistent with the new pieces of knowledge. We show how the proposed\ngeneral framework can be instantiated in different logics such as\npropositional, first-order, description and Horn logics. In particular for\ndescription logics, we introduce several concrete relaxation operators tailored\nfor the description logic $\\ALC{}$ and its fragments $\\EL{}$ and $\\ELext{}$,\ndiscuss their properties and provide some illustrative examples.\n", "versions": [{"version": "v1", "created": "Sun, 8 Feb 2015 20:26:10 GMT"}, {"version": "v2", "created": "Fri, 13 Jan 2017 20:46:28 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Aiguier", "Marc", ""], ["Atif", "Jamal", ""], ["Bloch", "Isabelle", ""], ["Hudelot", "C\u00e9line", ""]]}, {"id": "1502.02414", "submitter": "Schiex Thomas", "authors": "David Allouche, Christian Bessiere, Patrice Boizumault, Simon de\n  Givry, Patricia Gutierrez, Jimmy H.M. Lee, Kam Lun Leung, Samir Loudni,\n  Jean-Philippe M\\'etivier, Thomas Schiex, Yi Wu", "title": "Tractability and Decompositions of Global Cost Functions", "comments": "45 pages for the main paper, extra Appendix with examples of\n  DAG-decomposed global cost functions", "journal-ref": null, "doi": "10.1016/j.artint.2016.06.005", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Enforcing local consistencies in cost function networks is performed by\napplying so-called Equivalent Preserving Transformations (EPTs) to the cost\nfunctions. As EPTs transform the cost functions, they may break the property\nthat was making local consistency enforcement tractable on a global cost\nfunction. A global cost function is called tractable projection-safe when\napplying an EPT to it is tractable and does not break the tractability\nproperty. In this paper, we prove that depending on the size r of the smallest\nscopes used for performing EPTs, the tractability of global cost functions can\nbe preserved (r = 0) or destroyed (r > 1). When r = 1, the answer is\nindefinite. We show that on a large family of cost functions, EPTs can be\ncomputed via dynamic programming-based algorithms, leading to tractable\nprojection-safety. We also show that when a global cost function can be\ndecomposed into a Berge acyclic network of bounded arity cost functions, soft\nlocal consistencies such as soft Directed or Virtual Arc Consistency can\ndirectly emulate dynamic programming. These different approaches to\ndecomposable cost functions are then embedded in a solver for extensive\nexperiments that confirm the feasibility and efficiency of our proposal.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 10:09:35 GMT"}, {"version": "v2", "created": "Wed, 29 Jun 2016 16:24:11 GMT"}, {"version": "v3", "created": "Thu, 30 Jun 2016 11:21:20 GMT"}], "update_date": "2016-07-29", "authors_parsed": [["Allouche", "David", ""], ["Bessiere", "Christian", ""], ["Boizumault", "Patrice", ""], ["de Givry", "Simon", ""], ["Gutierrez", "Patricia", ""], ["Lee", "Jimmy H. M.", ""], ["Leung", "Kam Lun", ""], ["Loudni", "Samir", ""], ["M\u00e9tivier", "Jean-Philippe", ""], ["Schiex", "Thomas", ""], ["Wu", "Yi", ""]]}, {"id": "1502.02417", "submitter": "Antonio Nicola", "authors": "Cecilia Camporeale, Antonio De Nicola, Maria Luisa Villani", "title": "Semantics-based services for a low carbon society: An application on\n  emissions trading system data and scenarios management", "comments": null, "journal-ref": "Environmental Modelling & Software, Vol 64, Feb 2015, Pages\n  124-142", "doi": "10.1016/j.envsoft.2014.11.007", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A low carbon society aims at fighting global warming by stimulating synergic\nefforts from governments, industry and scientific communities. Decision support\nsystems should be adopted to provide policy makers with possible scenarios,\noptions for prompt countermeasures in case of side effects on environment,\neconomy and society due to low carbon society policies, and also options for\ninformation management. A necessary precondition to fulfill this agenda is to\nface the complexity of this multi-disciplinary domain and to reach a common\nunderstanding on it as a formal specification. Ontologies are widely accepted\nmeans to share knowledge. Together with semantic rules, they enable advanced\nsemantic services to manage knowledge in a smarter way. Here we address the\nEuropean Emissions Trading System (EU-ETS) and we present a knowledge base\nconsisting of the EREON ontology and a catalogue of rules. Then we describe two\ninnovative semantic services to manage ETS data and information on ETS\nscenarios.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 10:19:18 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Camporeale", "Cecilia", ""], ["De Nicola", "Antonio", ""], ["Villani", "Maria Luisa", ""]]}, {"id": "1502.02454", "submitter": "Thuc Le Ph.D", "authors": "Thuc Duy Le, Tao Hoang, Jiuyong Li, Lin Liu, and Huawen Liu", "title": "A fast PC algorithm for high dimensional causal discovery with\n  multi-core PCs", "comments": "Thuc Le, Tao Hoang, Jiuyong Li, Lin Liu, Huawen Liu, Shu Hu, \"A fast\n  PC algorithm for high dimensional causal discovery with multi-core PCs\",\n  IEEE/ACM Transactions on Computational Biology and Bioinformatics,\n  doi:10.1109/TCBB.2016.2591526", "journal-ref": null, "doi": "10.1109/TCBB.2016.2591526", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discovering causal relationships from observational data is a crucial problem\nand it has applications in many research areas. The PC algorithm is the\nstate-of-the-art constraint based method for causal discovery. However, runtime\nof the PC algorithm, in the worst-case, is exponential to the number of nodes\n(variables), and thus it is inefficient when being applied to high dimensional\ndata, e.g. gene expression datasets. On another note, the advancement of\ncomputer hardware in the last decade has resulted in the widespread\navailability of multi-core personal computers. There is a significant\nmotivation for designing a parallelised PC algorithm that is suitable for\npersonal computers and does not require end users' parallel computing knowledge\nbeyond their competency in using the PC algorithm. In this paper, we develop\nparallel-PC, a fast and memory efficient PC algorithm using the parallel\ncomputing technique. We apply our method to a range of synthetic and real-world\nhigh dimensional datasets. Experimental results on a dataset from the DREAM 5\nchallenge show that the original PC algorithm could not produce any results\nafter running more than 24 hours; meanwhile, our parallel-PC algorithm managed\nto finish within around 12 hours with a 4-core CPU computer, and less than 6\nhours with a 8-core CPU computer. Furthermore, we integrate parallel-PC into a\ncausal inference method for inferring miRNA-mRNA regulatory relationships. The\nexperimental results show that parallel-PC helps improve both the efficiency\nand accuracy of the causal inference algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 12:15:21 GMT"}, {"version": "v2", "created": "Sat, 11 Jul 2015 03:03:16 GMT"}, {"version": "v3", "created": "Thu, 10 Nov 2016 12:23:48 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Le", "Thuc Duy", ""], ["Hoang", "Tao", ""], ["Li", "Jiuyong", ""], ["Liu", "Lin", ""], ["Liu", "Huawen", ""]]}, {"id": "1502.02467", "submitter": "Evgenij Thorstensen", "authors": "Evgenij Thorstensen", "title": "Structural Decompositions for Problems with Global Constraints", "comments": "The final publication is available at Springer via\n  http://dx.doi.org/10.1007/s10601-015-9181-2", "journal-ref": null, "doi": "10.1007/s10601-015-9181-2", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wide range of problems can be modelled as constraint satisfaction problems\n(CSPs), that is, a set of constraints that must be satisfied simultaneously.\nConstraints can either be represented extensionally, by explicitly listing\nallowed combinations of values, or implicitly, by special-purpose algorithms\nprovided by a solver.\n  Such implicitly represented constraints, known as global constraints, are\nwidely used; indeed, they are one of the key reasons for the success of\nconstraint programming in solving real-world problems. In recent years, a\nvariety of restrictions on the structure of CSP instances have been shown to\nyield tractable classes of CSPs. However, most such restrictions fail to\nguarantee tractability for CSPs with global constraints. We therefore study the\napplicability of structural restrictions to instances with such constraints.\n  We show that when the number of solutions to a CSP instance is bounded in key\nparts of the problem, structural restrictions can be used to derive new\ntractable classes. Furthermore, we show that this result extends to\ncombinations of instances drawn from known tractable classes, as well as to CSP\ninstances where constraints assign costs to satisfying assignments.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 12:55:36 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Thorstensen", "Evgenij", ""]]}, {"id": "1502.02493", "submitter": "Jose M. Such", "authors": "Natalia Criado, Jose M. Such", "title": "Implicit Contextual Integrity in Online Social Networks", "comments": "Authors Version of the paper accepted for publication in the\n  Information Sciences journal\n  (http://www.journals.elsevier.com/information-sciences/)", "journal-ref": "Information Sciences, Vol. 325 pp. 48-69 (2015)", "doi": "10.1016/j.ins.2015.07.013", "report-no": null, "categories": "cs.SI cs.AI cs.CR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real incidents demonstrate that users of Online Social Networks need\nmechanisms that help them manage their interactions by increasing the awareness\nof the different contexts that coexist in Online Social Networks and preventing\nthem from exchanging inappropriate information in those contexts or\ndisseminating sensitive information from some contexts to others. Contextual\nintegrity is a privacy theory that conceptualises the appropriateness of\ninformation sharing based on the contexts in which this information is to be\nshared. Computational models of Contextual Integrity assume the existence of\nwell-defined contexts, in which individuals enact pre-defined roles and\ninformation sharing is governed by an explicit set of norms. However, contexts\nin Online Social Networks are known to be implicit, unknown a priori and ever\nchanging; users relationships are constantly evolving; and the information\nsharing norms are implicit. This makes current Contextual Integrity models not\nsuitable for Online Social Networks.\n  In this paper, we propose the first computational model of Implicit\nContextual Integrity, presenting an information model and an Information\nAssistant Agent that uses the information model to learn implicit contexts,\nrelationships and the information sharing norms to help users avoid\ninappropriate information exchanges and undesired information disseminations.\nThrough an experimental evaluation, we validate the properties of Information\nAssistant Agents, which are shown to: infer the information sharing norms even\nif a small proportion of the users follow the norms and in presence of\nmalicious users; help reduce the exchange of inappropriate information and the\ndissemination of sensitive information with only a partial view of the system\nand the information received and sent by their users; and minimise the burden\nto the users in terms of raising unnecessary alerts.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 14:25:44 GMT"}, {"version": "v2", "created": "Mon, 6 Jul 2015 09:35:32 GMT"}], "update_date": "2016-06-14", "authors_parsed": [["Criado", "Natalia", ""], ["Such", "Jose M.", ""]]}, {"id": "1502.02535", "submitter": "Maria Paola Bonacina", "authors": "Maria Paola Bonacina, Ulrich Furbach, Viorica Sofronie-Stokkermans", "title": "On First-Order Model-Based Reasoning", "comments": "In Narciso Marti-Oliet, Peter Olveczky, and Carolyn Talcott (Eds.),\n  \"Logic, Rewriting, and Concurrency: Essays in Honor of Jose Meseguer\"\n  Springer, Lecture Notes in Computer Science 9200, September 2015, 24 pages.\n  Version v4 in arxiv fixes a typo on page 15 that remains in the version\n  published in the Springer book", "journal-ref": null, "doi": "10.1007/978-3-319-23165-5_8", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reasoning semantically in first-order logic is notoriously a challenge. This\npaper surveys a selection of semantically-guided or model-based methods that\naim at meeting aspects of this challenge. For first-order logic we touch upon\nresolution-based methods, tableaux-based methods, DPLL-inspired methods, and we\ngive a preview of a new method called SGGS, for Semantically-Guided\nGoal-Sensitive reasoning. For first-order theories we highlight hierarchical\nand locality-based methods, concluding with the recent Model-Constructing\nsatisfiability calculus.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 16:14:40 GMT"}, {"version": "v2", "created": "Tue, 9 Jun 2015 15:22:17 GMT"}, {"version": "v3", "created": "Fri, 31 Jul 2015 21:16:13 GMT"}, {"version": "v4", "created": "Wed, 20 Nov 2019 19:25:16 GMT"}], "update_date": "2019-11-22", "authors_parsed": [["Bonacina", "Maria Paola", ""], ["Furbach", "Ulrich", ""], ["Sofronie-Stokkermans", "Viorica", ""]]}, {"id": "1502.02606", "submitter": "Huy Nguyen", "authors": "Rafael da Ponte Barbosa and Alina Ene and Huy L. Nguyen and Justin\n  Ward", "title": "The Power of Randomization: Distributed Submodular Maximization on\n  Massive Datasets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A wide variety of problems in machine learning, including exemplar\nclustering, document summarization, and sensor placement, can be cast as\nconstrained submodular maximization problems. Unfortunately, the resulting\nsubmodular optimization problems are often too large to be solved on a single\nmachine. We develop a simple distributed algorithm that is embarrassingly\nparallel and it achieves provable, constant factor, worst-case approximation\nguarantees. In our experiments, we demonstrate its efficiency in large problems\nwith different kinds of constraints with objective values always close to what\nis achievable in the centralized setting.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 19:04:43 GMT"}, {"version": "v2", "created": "Wed, 22 Apr 2015 17:49:22 GMT"}], "update_date": "2015-04-23", "authors_parsed": [["Barbosa", "Rafael da Ponte", ""], ["Ene", "Alina", ""], ["Nguyen", "Huy L.", ""], ["Ward", "Justin", ""]]}, {"id": "1502.02643", "submitter": "Huy Nguyen", "authors": "Alina Ene and Huy L. Nguyen", "title": "Random Coordinate Descent Methods for Minimizing Decomposable Submodular\n  Functions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Submodular function minimization is a fundamental optimization problem that\narises in several applications in machine learning and computer vision. The\nproblem is known to be solvable in polynomial time, but general purpose\nalgorithms have high running times and are unsuitable for large-scale problems.\nRecent work have used convex optimization techniques to obtain very practical\nalgorithms for minimizing functions that are sums of ``simple\" functions. In\nthis paper, we use random coordinate descent methods to obtain algorithms with\nfaster linear convergence rates and cheaper iteration costs. Compared to\nalternating projection methods, our algorithms do not rely on full-dimensional\nvector operations and they converge in significantly fewer iterations.\n", "versions": [{"version": "v1", "created": "Mon, 9 Feb 2015 20:31:18 GMT"}], "update_date": "2015-02-10", "authors_parsed": [["Ene", "Alina", ""], ["Nguyen", "Huy L.", ""]]}, {"id": "1502.02761", "submitter": "Yujia Li", "authors": "Yujia Li, Kevin Swersky and Richard Zemel", "title": "Generative Moment Matching Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning deep generative models from data. We\nformulate a method that generates an independent sample via a single\nfeedforward pass through a multilayer perceptron, as in the recently proposed\ngenerative adversarial networks (Goodfellow et al., 2014). Training a\ngenerative adversarial network, however, requires careful optimization of a\ndifficult minimax program. Instead, we utilize a technique from statistical\nhypothesis testing known as maximum mean discrepancy (MMD), which leads to a\nsimple objective that can be interpreted as matching all orders of statistics\nbetween a dataset and samples from the model, and can be trained by\nbackpropagation. We further boost the performance of this approach by combining\nour generative network with an auto-encoder network, using MMD to learn to\ngenerate codes that can then be decoded to produce samples. We show that the\ncombination of these techniques yields excellent generative models compared to\nbaseline approaches as measured on MNIST and the Toronto Face Database.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2015 02:54:58 GMT"}], "update_date": "2015-02-11", "authors_parsed": [["Li", "Yujia", ""], ["Swersky", "Kevin", ""], ["Zemel", "Richard", ""]]}, {"id": "1502.02799", "submitter": "Yisong Wang", "authors": "Yisong Wang", "title": "On Forgetting in Tractable Propositional Fragments", "comments": "27 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distilling from a knowledge base only the part that is relevant to a subset\nof alphabet, which is recognized as forgetting, has attracted extensive\ninterests in AI community. In standard propositional logic, a general algorithm\nof forgetting and its computation-oriented investigation in various fragments\nwhose satisfiability are tractable are still lacking. The paper aims at filling\nthe gap. After exploring some basic properties of forgetting in propositional\nlogic, we present a resolution-based algorithm of forgetting for CNF fragment,\nand some complexity results about forgetting in Horn, renamable Horn, q-Horn,\nKrom, DNF and CNF fragments of propositional logic.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2015 07:05:56 GMT"}], "update_date": "2015-02-11", "authors_parsed": [["Wang", "Yisong", ""]]}, {"id": "1502.02840", "submitter": "Pablo Rodriguez-Mier", "authors": "Pablo Rodriguez-Mier, Carlos Pedrinaci, Manuel Lama, Manuel Mucientes", "title": "An Integrated Semantic Web Service Discovery and Composition Framework", "comments": "Accepted to appear in IEEE Transactions on Services Computing 2015", "journal-ref": null, "doi": "10.1109/TSC.2015.2402679", "report-no": null, "categories": "cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a theoretical analysis of graph-based service\ncomposition in terms of its dependency with service discovery. Driven by this\nanalysis we define a composition framework by means of integration with\nfine-grained I/O service discovery that enables the generation of a graph-based\ncomposition which contains the set of services that are semantically relevant\nfor an input-output request. The proposed framework also includes an optimal\ncomposition search algorithm to extract the best composition from the graph\nminimising the length and the number of services, and different graph\noptimisations to improve the scalability of the system. A practical\nimplementation used for the empirical analysis is also provided. This analysis\nproves the scalability and flexibility of our proposal and provides insights on\nhow integrated composition systems can be designed in order to achieve good\nperformance in real scenarios for the Web.\n", "versions": [{"version": "v1", "created": "Tue, 10 Feb 2015 10:25:33 GMT"}], "update_date": "2015-02-11", "authors_parsed": [["Rodriguez-Mier", "Pablo", ""], ["Pedrinaci", "Carlos", ""], ["Lama", "Manuel", ""], ["Mucientes", "Manuel", ""]]}, {"id": "1502.03248", "submitter": "Anna Harutyunyan", "authors": "Anna Harutyunyan and Tim Brys and Peter Vrancx and Ann Nowe", "title": "Off-Policy Reward Shaping with Ensembles", "comments": "To be presented at ALA-15. Short version to appear at AAMAS-15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Potential-based reward shaping (PBRS) is an effective and popular technique\nto speed up reinforcement learning by leveraging domain knowledge. While PBRS\nis proven to always preserve optimal policies, its effect on learning speed is\ndetermined by the quality of its potential function, which, in turn, depends on\nboth the underlying heuristic and the scale. Knowing which heuristic will prove\neffective requires testing the options beforehand, and determining the\nappropriate scale requires tuning, both of which introduce additional sample\ncomplexity. We formulate a PBRS framework that reduces learning speed, but does\nnot incur extra sample complexity. For this, we propose to simultaneously learn\nan ensemble of policies, shaped w.r.t. many heuristics and on a range of\nscales. The target policy is then obtained by voting. The ensemble needs to be\nable to efficiently and reliably learn off-policy: requirements fulfilled by\nthe recent Horde architecture, which we take as our basis. We demonstrate\nempirically that (1) our ensemble policy outperforms both the base policy, and\nits single-heuristic components, and (2) an ensemble over a general range of\nscales performs at least as well as one with optimally tuned components.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 10:27:15 GMT"}, {"version": "v2", "created": "Mon, 23 Mar 2015 13:35:59 GMT"}], "update_date": "2015-03-24", "authors_parsed": [["Harutyunyan", "Anna", ""], ["Brys", "Tim", ""], ["Vrancx", "Peter", ""], ["Nowe", "Ann", ""]]}, {"id": "1502.03322", "submitter": "Yongfeng  Zhang", "authors": "Yongfeng Zhang, Min Zhang, Yiqun Liu, and Shaoping Ma", "title": "Boost Phrase-level Polarity Labelling with Review-level Sentiment\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment analysis on user reviews helps to keep track of user reactions\ntowards products, and make advices to users about what to buy. State-of-the-art\nreview-level sentiment classification techniques could give pretty good\nprecisions of above 90%. However, current phrase-level sentiment analysis\napproaches might only give sentiment polarity labelling precisions of around\n70%~80%, which is far from satisfaction and restricts its application in many\npractical tasks. In this paper, we focus on the problem of phrase-level\nsentiment polarity labelling and attempt to bridge the gap between phrase-level\nand review-level sentiment analysis. We investigate the inconsistency between\nthe numerical star ratings and the sentiment orientation of textual user\nreviews. Although they have long been treated as identical, which serves as a\nbasic assumption in previous work, we find that this assumption is not\nnecessarily true. We further propose to leverage the results of review-level\nsentiment classification to boost the performance of phrase-level polarity\nlabelling using a novel constrained convex optimization framework. Besides, the\nframework is capable of integrating various kinds of information sources and\nheuristics, while giving the global optimal solution due to its convexity.\nExperimental results on both English and Chinese reviews show that our\nframework achieves high labelling precisions of up to 89%, which is a\nsignificant improvement from current approaches.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 14:45:41 GMT"}], "update_date": "2015-02-12", "authors_parsed": [["Zhang", "Yongfeng", ""], ["Zhang", "Min", ""], ["Liu", "Yiqun", ""], ["Ma", "Shaoping", ""]]}, {"id": "1502.03473", "submitter": "Shuai Li", "authors": "Shuai Li and Alexandros Karatzoglou and Claudio Gentile", "title": "Collaborative Filtering Bandits", "comments": "The 39th SIGIR (SIGIR 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classical collaborative filtering, and content-based filtering methods try to\nlearn a static recommendation model given training data. These approaches are\nfar from ideal in highly dynamic recommendation domains such as news\nrecommendation and computational advertisement, where the set of items and\nusers is very fluid. In this work, we investigate an adaptive clustering\ntechnique for content recommendation based on exploration-exploitation\nstrategies in contextual multi-armed bandit settings. Our algorithm takes into\naccount the collaborative effects that arise due to the interaction of the\nusers with the items, by dynamically grouping users based on the items under\nconsideration and, at the same time, grouping items based on the similarity of\nthe clusterings induced over the users. The resulting algorithm thus takes\nadvantage of preference patterns in the data in a way akin to collaborative\nfiltering methods. We provide an empirical analysis on medium-size real-world\ndatasets, showing scalability and increased prediction performance (as measured\nby click-through rate) over state-of-the-art methods for clustering bandits. We\nalso provide a regret analysis within a standard linear stochastic noise\nsetting.\n", "versions": [{"version": "v1", "created": "Wed, 11 Feb 2015 22:28:14 GMT"}, {"version": "v2", "created": "Tue, 17 Mar 2015 17:51:41 GMT"}, {"version": "v3", "created": "Thu, 7 May 2015 17:03:39 GMT"}, {"version": "v4", "created": "Thu, 24 Dec 2015 17:24:07 GMT"}, {"version": "v5", "created": "Wed, 30 Mar 2016 10:29:12 GMT"}, {"version": "v6", "created": "Wed, 11 May 2016 15:17:30 GMT"}, {"version": "v7", "created": "Tue, 31 May 2016 18:47:03 GMT"}], "update_date": "2016-06-01", "authors_parsed": [["Li", "Shuai", ""], ["Karatzoglou", "Alexandros", ""], ["Gentile", "Claudio", ""]]}, {"id": "1502.03536", "submitter": "Vamsi Ithapu", "authors": "Chris Hinrichs, Vamsi K Ithapu, Qinyuan Sun, Sterling C Johnson, Vikas\n  Singh", "title": "Speeding up Permutation Testing in Neuroimaging", "comments": "NIPS 13", "journal-ref": "Advances in neural information processing systems (2013), pp.\n  890-898", "doi": null, "report-no": null, "categories": "stat.CO cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple hypothesis testing is a significant problem in nearly all\nneuroimaging studies. In order to correct for this phenomena, we require a\nreliable estimate of the Family-Wise Error Rate (FWER). The well known\nBonferroni correction method, while simple to implement, is quite conservative,\nand can substantially under-power a study because it ignores dependencies\nbetween test statistics. Permutation testing, on the other hand, is an exact,\nnon-parametric method of estimating the FWER for a given $\\alpha$-threshold,\nbut for acceptably low thresholds the computational burden can be prohibitive.\nIn this paper, we show that permutation testing in fact amounts to populating\nthe columns of a very large matrix ${\\bf P}$. By analyzing the spectrum of this\nmatrix, under certain conditions, we see that ${\\bf P}$ has a low-rank plus a\nlow-variance residual decomposition which makes it suitable for highly\nsub--sampled --- on the order of $0.5\\%$ --- matrix completion methods. Based\non this observation, we propose a novel permutation testing methodology which\noffers a large speedup, without sacrificing the fidelity of the estimated FWER.\nOur evaluations on four different neuroimaging datasets show that a\ncomputational speedup factor of roughly $50\\times$ can be achieved while\nrecovering the FWER distribution up to very high accuracy. Further, we show\nthat the estimated $\\alpha$-threshold is also recovered faithfully, and is\nstable.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 04:30:06 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Hinrichs", "Chris", ""], ["Ithapu", "Vamsi K", ""], ["Sun", "Qinyuan", ""], ["Johnson", "Sterling C", ""], ["Singh", "Vikas", ""]]}, {"id": "1502.03552", "submitter": "Selma Dilek", "authors": "Selma Dilek, H\\\"useyin \\c{C}ak{\\i}r, Mustafa Ayd{\\i}n", "title": "Applications of Artificial Intelligence Techniques to Combating Cyber\n  Crimes: A Review", "comments": "19 pages, a survey, in International Journal of Artificial\n  Intelligence & Applications (IJAIA), Vol. 6, No. 1, January 2015", "journal-ref": null, "doi": "10.5121/ijaia.2015.6102", "report-no": null, "categories": "cs.AI cs.CR cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advances in information technology (IT) criminals are using\ncyberspace to commit numerous cyber crimes. Cyber infrastructures are highly\nvulnerable to intrusions and other threats. Physical devices and human\nintervention are not sufficient for monitoring and protection of these\ninfrastructures; hence, there is a need for more sophisticated cyber defense\nsystems that need to be flexible, adaptable and robust, and able to detect a\nwide variety of threats and make intelligent real-time decisions. Numerous\nbio-inspired computing methods of Artificial Intelligence have been\nincreasingly playing an important role in cyber crime detection and prevention.\nThe purpose of this study is to present advances made so far in the field of\napplying AI techniques for combating cyber crimes, to demonstrate how these\ntechniques can be an effective tool for detection and prevention of cyber\nattacks, as well as to give the scope for future work.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 06:50:17 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["Dilek", "Selma", ""], ["\u00c7ak\u0131r", "H\u00fcseyin", ""], ["Ayd\u0131n", "Mustafa", ""]]}, {"id": "1502.03556", "submitter": "Md. Hanif Seddiqui", "authors": "Md. Hanif Seddiqui, Rudra Pratap Deb Nath, Masaki Aono", "title": "An Efficient Metric of Automatic Weight Generation for Properties in\n  Instance Matching Technique", "comments": "17 pages, 5 figures, 3 tables, pp. 1-17, publication year 2015,\n  journal publication, vol. 6 number 1", "journal-ref": "Journal of Web and Semantic Technology (IJWeST), vol.6 no.1, pp.\n  1-17 (2015)", "doi": "10.5121/ijwest.2015.6101", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The proliferation of heterogeneous data sources of semantic knowledge base\nintensifies the need of an automatic instance matching technique. However, the\nefficiency of instance matching is often influenced by the weight of a property\nassociated to instances. Automatic weight generation is a non-trivial, however\nan important task in instance matching technique. Therefore, identifying an\nappropriate metric for generating weight for a property automatically is\nnevertheless a formidable task. In this paper, we investigate an approach of\ngenerating weights automatically by considering hypotheses: (1) the weight of a\nproperty is directly proportional to the ratio of the number of its distinct\nvalues to the number of instances contain the property, and (2) the weight is\nalso proportional to the ratio of the number of distinct values of a property\nto the number of instances in a training dataset. The basic intuition behind\nthe use of our approach is the classical theory of information content that\ninfrequent words are more informative than frequent ones. Our mathematical\nmodel derives a metric for generating property weights automatically, which is\napplied in instance matching system to produce re-conciliated instances\nefficiently. Our experiments and evaluations show the effectiveness of our\nproposed metric of automatic weight generation for properties in an instance\nmatching technique.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 07:51:39 GMT"}], "update_date": "2015-02-13", "authors_parsed": [["Seddiqui", "Md. Hanif", ""], ["Nath", "Rudra Pratap Deb", ""], ["Aono", "Masaki", ""]]}, {"id": "1502.03683", "submitter": "Paolo Turrini", "authors": "Paolo Turrini", "title": "Computing rational decisions in extensive games with limited foresight", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a class of extensive form games where players might not be able\nto foresee the possible consequences of their decisions and form a model of\ntheir opponents which they exploit to achieve a more profitable outcome. We\nimprove upon existing models of games with limited foresight, endowing players\nwith the ability of higher-order reasoning and proposing a novel solution\nconcept to address intuitions coming from real game play. We analyse the\nresulting equilibria, devising an effective procedure to compute them.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 14:45:22 GMT"}, {"version": "v2", "created": "Tue, 8 Sep 2015 10:49:09 GMT"}, {"version": "v3", "created": "Tue, 17 Nov 2015 10:22:56 GMT"}, {"version": "v4", "created": "Wed, 16 Dec 2015 11:17:17 GMT"}, {"version": "v5", "created": "Thu, 21 Apr 2016 10:32:27 GMT"}, {"version": "v6", "created": "Sun, 29 May 2016 17:21:44 GMT"}], "update_date": "2016-05-31", "authors_parsed": [["Turrini", "Paolo", ""]]}, {"id": "1502.03796", "submitter": "Stanislav Zivny", "authors": "David A. Cohen, Martin C. Cooper, Guillaume Escamocher, Stanislav\n  Zivny", "title": "Variable and value elimination in binary constraint satisfaction via\n  forbidden patterns", "comments": "A full version of an IJCAI'13 paper to appear in Journal of Computer\n  and System Sciences (JCSS)", "journal-ref": "Journal of Computer and System Sciences 81(7) 1127-1143 (2015)", "doi": "10.1016/j.jcss.2015.02.001", "report-no": null, "categories": "cs.CC cs.AI cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variable or value elimination in a constraint satisfaction problem (CSP) can\nbe used in preprocessing or during search to reduce search space size. A\nvariable elimination rule (value elimination rule) allows the polynomial-time\nidentification of certain variables (domain elements) whose elimination,\nwithout the introduction of extra compensatory constraints, does not affect the\nsatisfiability of an instance. We show that there are essentially just four\nvariable elimination rules and three value elimination rules defined by\nforbidding generic sub-instances, known as irreducible existential patterns, in\narc-consistent CSP instances. One of the variable elimination rules is the\nalready-known Broken Triangle Property, whereas the other three are novel. The\nthree value elimination rules can all be seen as strict generalisations of\nneighbourhood substitution.\n", "versions": [{"version": "v1", "created": "Thu, 12 Feb 2015 20:22:44 GMT"}], "update_date": "2015-06-11", "authors_parsed": [["Cohen", "David A.", ""], ["Cooper", "Martin C.", ""], ["Escamocher", "Guillaume", ""], ["Zivny", "Stanislav", ""]]}, {"id": "1502.03890", "submitter": "Song-Ju Kim Dr.", "authors": "Song-Ju Kim and Masashi Aono", "title": "Decision Maker using Coupled Incompressible-Fluid Cylinders", "comments": "5 pages, 5 figures, Waseda AICS Symposium and the 14th Slovenia-Japan\n  Seminar, Waseda University, Tokyo, 24-26 October 2014. in Special Issue of\n  ASTE: Advances in Science, Technology and Environmentology (2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multi-armed bandit problem (MBP) is the problem of finding, as accurately\nand quickly as possible, the most profitable option from a set of options that\ngives stochastic rewards by referring to past experiences. Inspired by\nfluctuated movements of a rigid body in a tug-of-war game, we formulated a\nunique search algorithm that we call the `tug-of-war (TOW) dynamics' for\nsolving the MBP efficiently. The cognitive medium access, which refers to\nmulti-user channel allocations in cognitive radio, can be interpreted as the\ncompetitive multi-armed bandit problem (CMBP); the problem is to determine the\noptimal strategy for allocating channels to users which yields maximum total\nrewards gained by all users. Here we show that it is possible to construct a\nphysical device for solving the CMBP, which we call the `TOW Bombe', by\nexploiting the TOW dynamics existed in coupled incompressible-fluid cylinders.\nThis analog computing device achieves the `socially-maximum' resource\nallocation that maximizes the total rewards in cognitive medium access without\npaying a huge computational cost that grows exponentially as a function of the\nproblem size.\n", "versions": [{"version": "v1", "created": "Fri, 13 Feb 2015 05:31:13 GMT"}], "update_date": "2015-02-16", "authors_parsed": [["Kim", "Song-Ju", ""], ["Aono", "Masashi", ""]]}, {"id": "1502.03919", "submitter": "Aviv Tamar", "authors": "Aviv Tamar, Yinlam Chow, Mohammad Ghavamzadeh, Shie Mannor", "title": "Policy Gradient for Coherent Risk Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several authors have recently developed risk-sensitive policy gradient\nmethods that augment the standard expected cost minimization problem with a\nmeasure of variability in cost. These studies have focused on specific\nrisk-measures, such as the variance or conditional value at risk (CVaR). In\nthis work, we extend the policy gradient method to the whole class of coherent\nrisk measures, which is widely accepted in finance and operations research,\namong other fields. We consider both static and time-consistent dynamic risk\nmeasures. For static risk measures, our approach is in the spirit of policy\ngradient algorithms and combines a standard sampling approach with convex\nprogramming. For dynamic risk measures, our approach is actor-critic style and\ninvolves explicit approximation of value function. Most importantly, our\ncontribution presents a unified approach to risk-sensitive reinforcement\nlearning that generalizes and extends previous results.\n", "versions": [{"version": "v1", "created": "Fri, 13 Feb 2015 09:16:24 GMT"}, {"version": "v2", "created": "Mon, 8 Jun 2015 06:31:42 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Tamar", "Aviv", ""], ["Chow", "Yinlam", ""], ["Ghavamzadeh", "Mohammad", ""], ["Mannor", "Shie", ""]]}, {"id": "1502.03945", "submitter": "Alexandros A. Voudouris", "authors": "Ioannis Caragiannis, Xenophon Chatzigeorgiou, Panagiotis\n  Kanellopoulos, George A. Krimpas, Nikos Protopapas, Alexandros A. Voudouris", "title": "Efficiency and complexity of price competition among single-product\n  vendors", "comments": "23 pages, 6 tables, 2 figures, accepted to Artificial Intelligence\n  Journal, a preliminary version appeared in Proceedings of the 24th\n  International Joint Conference on Artificial Intelligence (IJCAI), pp. 25-31,\n  2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by recent progress on pricing in the AI literature, we study\nmarketplaces that contain multiple vendors offering identical or similar\nproducts and unit-demand buyers with different valuations on these vendors. The\nobjective of each vendor is to set the price of its product to a fixed value so\nthat its profit is maximized. The profit depends on the vendor's price itself\nand the total volume of buyers that find the particular price more attractive\nthan the price of the vendor's competitors. We model the behaviour of buyers\nand vendors as a two-stage full-information game and study a series of\nquestions related to the existence, efficiency (price of anarchy) and\ncomputational complexity of equilibria in this game. To overcome situations\nwhere equilibria do not exist or exist but are highly inefficient, we consider\nthe scenario where some of the vendors are subsidized in order to keep prices\nlow and buyers highly satisfied.\n", "versions": [{"version": "v1", "created": "Fri, 13 Feb 2015 11:18:00 GMT"}, {"version": "v2", "created": "Mon, 6 Mar 2017 09:13:31 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Caragiannis", "Ioannis", ""], ["Chatzigeorgiou", "Xenophon", ""], ["Kanellopoulos", "Panagiotis", ""], ["Krimpas", "George A.", ""], ["Protopapas", "Nikos", ""], ["Voudouris", "Alexandros A.", ""]]}, {"id": "1502.03986", "submitter": "Roberto Amadini", "authors": "Roberto Amadini, Maurizio Gabbrielli, Jacopo Mauro", "title": "A Multicore Tool for Constraint Solving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  *** To appear in IJCAI 2015 proceedings *** In Constraint Programming (CP), a\nportfolio solver uses a variety of different solvers for solving a given\nConstraint Satisfaction / Optimization Problem. In this paper we introduce\nsunny-cp2: the first parallel CP portfolio solver that enables a dynamic,\ncooperative, and simultaneous execution of its solvers in a multicore setting.\nIt incorporates state-of-the-art solvers, providing also a usable and\nconfigurable framework. Empirical results are very promising. sunny-cp2 can\neven outperform the performance of the oracle solver which always selects the\nbest solver of the portfolio for a given problem.\n", "versions": [{"version": "v1", "created": "Fri, 13 Feb 2015 13:45:54 GMT"}, {"version": "v2", "created": "Tue, 21 Apr 2015 17:28:26 GMT"}, {"version": "v3", "created": "Thu, 30 Apr 2015 10:57:43 GMT"}], "update_date": "2015-05-01", "authors_parsed": [["Amadini", "Roberto", ""], ["Gabbrielli", "Maurizio", ""], ["Mauro", "Jacopo", ""]]}, {"id": "1502.04013", "submitter": "Konstantin Selyunin", "authors": "Konstantin Selyunin, Denise Ratasich, Ezio Bartocci and Radu Grosu", "title": "Deep Neural Programs for Adaptive Control in Cyber-Physical Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Deep Neural Programs (DNP), a novel programming paradigm for\nwriting adaptive controllers for cy-ber-physical systems (CPS). DNP replace if\nand while statements, whose discontinuity is responsible for undecidability in\nCPS analysis, intractability in CPS design, and frailness in CPS\nimplementation, with their smooth, neural nif and nwhile counterparts. This not\nonly makes CPS analysis decidable and CPS design tractable, but also allows to\nwrite robust and adaptive CPS code. In DNP the connection between the sigmoidal\nguards of the nif and nwhile statements has to be given as a Gaussian Bayesian\nnetwork, which reflects the partial knowledge, the CPS program has about its\nenvironment. To the best of our knowledge, DNP are the first approach linking\nneural networks to programs, in a way that makes explicit the meaning of the\nnetwork. In order to prove and validate the usefulness of DNP, we use them to\nwrite and learn an adaptive CPS controller for the parallel parking of the\nPioneer rovers available in our CPS lab.\n", "versions": [{"version": "v1", "created": "Fri, 13 Feb 2015 14:50:22 GMT"}], "update_date": "2015-02-16", "authors_parsed": [["Selyunin", "Konstantin", ""], ["Ratasich", "Denise", ""], ["Bartocci", "Ezio", ""], ["Grosu", "Radu", ""]]}, {"id": "1502.04049", "submitter": "Preethi Raghavan", "authors": "Preethi Raghavan, James L. Chen, Eric Fosler-Lussier, Albert M. Lai", "title": "How essential are unstructured clinical narratives and information\n  fusion to clinical trial recruitment?", "comments": "AMIA TBI 2014, 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.CL", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Electronic health records capture patient information using structured\ncontrolled vocabularies and unstructured narrative text. While structured data\ntypically encodes lab values, encounters and medication lists, unstructured\ndata captures the physician's interpretation of the patient's condition,\nprognosis, and response to therapeutic intervention. In this paper, we\ndemonstrate that information extraction from unstructured clinical narratives\nis essential to most clinical applications. We perform an empirical study to\nvalidate the argument and show that structured data alone is insufficient in\nresolving eligibility criteria for recruiting patients onto clinical trials for\nchronic lymphocytic leukemia (CLL) and prostate cancer. Unstructured data is\nessential to solving 59% of the CLL trial criteria and 77% of the prostate\ncancer trial criteria. More specifically, for resolving eligibility criteria\nwith temporal constraints, we show the need for temporal reasoning and\ninformation integration with medical events within and across unstructured\nclinical narratives and structured data.\n", "versions": [{"version": "v1", "created": "Fri, 13 Feb 2015 16:28:40 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Raghavan", "Preethi", ""], ["Chen", "James L.", ""], ["Fosler-Lussier", "Eric", ""], ["Lai", "Albert M.", ""]]}, {"id": "1502.04120", "submitter": "Ohad Asor", "authors": "Ohad Asor", "title": "About Tau-Chain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Tau-chain is a decentralized peer-to-peer network having three unified faces:\nRules, Proofs, and Computer Programs, allowing a generalization of virtually\nany centralized or decentralized P2P network, together with many new abilities,\nas we present on this note.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 17:01:40 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Asor", "Ohad", ""]]}, {"id": "1502.04149", "submitter": "Po-Sen Huang", "authors": "Po-Sen Huang, Minje Kim, Mark Hasegawa-Johnson, Paris Smaragdis", "title": "Joint Optimization of Masks and Deep Recurrent Neural Networks for\n  Monaural Source Separation", "comments": null, "journal-ref": "IEEE/ACM Transactions on Audio, Speech, and Language Processing,\n  vol.23, no.12, pp.2136-2147, Dec. 2015", "doi": "10.1109/TASLP.2015.2468583", "report-no": null, "categories": "cs.SD cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monaural source separation is important for many real world applications. It\nis challenging because, with only a single channel of information available,\nwithout any constraints, an infinite number of solutions are possible. In this\npaper, we explore joint optimization of masking functions and deep recurrent\nneural networks for monaural source separation tasks, including monaural speech\nseparation, monaural singing voice separation, and speech denoising. The joint\noptimization of the deep recurrent neural networks with an extra masking layer\nenforces a reconstruction constraint. Moreover, we explore a discriminative\ncriterion for training neural networks to further enhance the separation\nperformance. We evaluate the proposed system on the TSP, MIR-1K, and TIMIT\ndatasets for speech separation, singing voice separation, and speech denoising\ntasks, respectively. Our approaches achieve 2.30--4.98 dB SDR gain compared to\nNMF models in the speech separation task, 2.30--2.48 dB GNSDR gain and\n4.32--5.42 dB GSIR gain compared to existing models in the singing voice\nseparation task, and outperform NMF and DNN baselines in the speech denoising\ntask.\n", "versions": [{"version": "v1", "created": "Fri, 13 Feb 2015 23:22:16 GMT"}, {"version": "v2", "created": "Tue, 2 Jun 2015 04:22:20 GMT"}, {"version": "v3", "created": "Thu, 13 Aug 2015 04:20:33 GMT"}, {"version": "v4", "created": "Thu, 1 Oct 2015 02:58:01 GMT"}], "update_date": "2015-10-02", "authors_parsed": [["Huang", "Po-Sen", ""], ["Kim", "Minje", ""], ["Hasegawa-Johnson", "Mark", ""], ["Smaragdis", "Paris", ""]]}, {"id": "1502.04266", "submitter": "Masoud Abbaszadeh", "authors": "Masoud Abbaszadeh, Reza Solgi", "title": "Constrained Nonlinear Model Predictive Control of an MMA Polymerization\n  Process via Evolutionary Optimization", "comments": "12 pages, 9 figures, 28 references", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, a nonlinear model predictive controller is developed for a\nbatch polymerization process. The physical model of the process is\nparameterized along a desired trajectory resulting in a trajectory linearized\npiecewise model (a multiple linear model bank) and the parameters are\nidentified for an experimental polymerization reactor. Then, a multiple model\nadaptive predictive controller is designed for thermal trajectory tracking of\nthe MMA polymerization. The input control signal to the process is constrained\nby the maximum thermal power provided by the heaters. The constrained\noptimization in the model predictive controller is solved via genetic\nalgorithms to minimize a DMC cost function in each sampling interval.\n", "versions": [{"version": "v1", "created": "Sun, 15 Feb 2015 01:11:51 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Abbaszadeh", "Masoud", ""], ["Solgi", "Reza", ""]]}, {"id": "1502.04495", "submitter": "Vasile Patrascu", "authors": "Vasile Patrascu", "title": "A Generalization of Gustafson-Kessel Algorithm Using a New Constraint\n  Parameter", "comments": "Proceedings of the Joint 4th Conference of the European Society for\n  Fuzzy Logic and Technology and the 11th Rencontres Francophones sur la\n  Logique Floue et ses Applications, pp. 1250-1255, Barcelona, Spain, September\n  7-9, 2005", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper one presents a new fuzzy clustering algorithm based on a\ndissimilarity function determined by three parameters. This algorithm can be\nconsidered a generalization of the Gustafson-Kessel algorithm for fuzzy\nclustering.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 11:09:52 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Patrascu", "Vasile", ""]]}, {"id": "1502.04593", "submitter": "Nicolas Maudet", "authors": "K. Belahcene, C. Labreuche, N. Maudet, V. Mousseau, W. Ouerdane", "title": "Explaining robust additive utility models by sequences of preference\n  swaps", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multicriteria decision analysis aims at supporting a person facing a decision\nproblem involving conflicting criteria. We consider an additive utility model\nwhich provides robust conclusions based on preferences elicited from the\ndecision maker. The recommendations based on these robust conclusions are even\nmore convincing if they are complemented by explanations. We propose a general\nscheme, based on sequence of preference swaps, in which explanations can be\ncomputed. We show first that the length of explanations can be unbounded in the\ngeneral case. However, in the case of binary reference scales, this length is\nbounded and we provide an algorithm to compute the corresponding explanation.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 16:11:44 GMT"}], "update_date": "2015-02-17", "authors_parsed": [["Belahcene", "K.", ""], ["Labreuche", "C.", ""], ["Maudet", "N.", ""], ["Mousseau", "V.", ""], ["Ouerdane", "W.", ""]]}, {"id": "1502.04665", "submitter": "Michele Stawowy", "authors": "Michele Stawowy", "title": "Optimizations for Decision Making and Planning in Description Logic\n  Dynamic Knowledge Bases", "comments": "16 pages, extended version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artifact-centric models for business processes recently raised a lot of\nattention, as they manage to combine structural (i.e. data related) with\ndynamical (i.e. process related) aspects in a seamless way. Many frameworks\ndeveloped under this approach, although, are not built explicitly for planning,\none of the most prominent operations related to business processes. In this\npaper, we try to overcome this by proposing a framework named Dynamic Knowledge\nBases, aimed at describing rich business domains through Description\nLogic-based ontologies, and where a set of actions allows the system to evolve\nby modifying such ontologies. This framework, by offering action rewriting and\nknowledge partialization, represents a viable and formal environment to develop\ndecision making and planning techniques for DL-based artifact-centric business\ndomains.\n", "versions": [{"version": "v1", "created": "Mon, 16 Feb 2015 19:06:25 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2015 22:17:37 GMT"}, {"version": "v3", "created": "Thu, 12 Mar 2015 16:44:12 GMT"}, {"version": "v4", "created": "Thu, 19 Mar 2015 17:24:55 GMT"}, {"version": "v5", "created": "Fri, 3 Apr 2015 11:52:17 GMT"}, {"version": "v6", "created": "Wed, 29 Apr 2015 16:56:58 GMT"}, {"version": "v7", "created": "Mon, 4 May 2015 16:05:29 GMT"}], "update_date": "2015-05-05", "authors_parsed": [["Stawowy", "Michele", ""]]}, {"id": "1502.04780", "submitter": "Qiong Wu", "authors": "Qiong Wu", "title": "Computational Curiosity (A Book Draft)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This book discusses computational curiosity, from the psychology of curiosity\nto the computational models of curiosity, and then showcases several\ninteresting applications of computational curiosity. A brief overview of the\nbook is given as follows. Chapter 1 discusses the underpinnings of curiosity in\nhuman beings, including the major categories of curiosity, curiosity-related\nemotions and behaviors, and the benefits of curiosity. Chapter 2 reviews the\narousal theories of curiosity in psychology and summarizes a general two-step\nprocess model for computational curiosity. Base on the perspective of the\ntwo-step process model, Chapter 3 reviews and analyzes some of the traditional\ncomputational models of curiosity. Chapter 4 introduces a novel generic\ncomputational model of curiosity, which is developed based on the arousal\ntheories of curiosity. After the discussion of computational models of\ncuriosity, we outline the important applications where computational curiosity\nmay bring significant impacts in Chapter 5. Chapter 6 discusses the application\nof the generic computational model of curiosity in a machine learning\nframework. Chapter 7 discusses the application of the generic computational\nmodel of curiosity in a recommender system. In Chapter 8 and Chapter 9, the\ngeneric computational model of curiosity is studied in two types of pedagogical\nagents. In Chapter 8, a curious peer learner is studied. It is a non-player\ncharacter that aims to provide a believable virtual learning environment for\nusers. In Chapter 9, a curious learning companion is studied. It aims to\nenhance users' learning experience through providing meaningful interactions\nwith them. Chapter 10 discusses open questions in the research field of\ncomputation curiosity.\n", "versions": [{"version": "v1", "created": "Tue, 17 Feb 2015 02:42:36 GMT"}], "update_date": "2015-02-18", "authors_parsed": [["Wu", "Qiong", ""]]}, {"id": "1502.04791", "submitter": "Emanuel Diamant", "authors": "Emanuel Diamant", "title": "Advances in Artificial Intelligence: Are you sure, we are on the right\n  track?", "comments": "The paper was submitted to IJCAI-15 conference, but was prudently\n  rejected. Thus, replenishing the collection of my repudiated papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past decade, AI has made a remarkable progress. It is agreed that\nthis is due to the recently revived Deep Learning technology. Deep Learning\nenables to process large amounts of data using simplified neuron networks that\nsimulate the way in which the brain works. However, there is a different point\nof view, which posits that the brain is processing information, not data. This\nunresolved duality hampered AI progress for years. In this paper, I propose a\nnotion of Integrated information that hopefully will resolve the problem. I\nconsider integrated information as a coupling between two separate entities -\nphysical information (that implies data processing) and semantic information\n(that provides physical information interpretation). In this regard,\nintelligence becomes a product of information processing. Extending further\nthis line of thinking, it can be said that information processing does not\nrequire more a human brain for its implementation. Indeed, bacteria and amoebas\nexhibit intelligent behavior without any sign of a brain. That dramatically\nremoves the need for AI systems to emulate the human brain complexity! The\npaper tries to explore this shift in AI systems design philosophy.\n", "versions": [{"version": "v1", "created": "Tue, 17 Feb 2015 03:40:31 GMT"}], "update_date": "2015-02-19", "authors_parsed": [["Diamant", "Emanuel", ""]]}, {"id": "1502.04956", "submitter": "Wolfgang Gatterbauer", "authors": "Wolfgang Gatterbauer", "title": "The Linearization of Belief Propagation on Pairwise Markov Networks", "comments": "Full version of AAAI 2017 paper with same title (23 pages, 9 figures)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Belief Propagation (BP) is a widely used approximation for exact\nprobabilistic inference in graphical models, such as Markov Random Fields\n(MRFs). In graphs with cycles, however, no exact convergence guarantees for BP\nare known, in general. For the case when all edges in the MRF carry the same\nsymmetric, doubly stochastic potential, recent works have proposed to\napproximate BP by linearizing the update equations around default values, which\nwas shown to work well for the problem of node classification. The present\npaper generalizes all prior work and derives an approach that approximates\nloopy BP on any pairwise MRF with the problem of solving a linear equation\nsystem. This approach combines exact convergence guarantees and a fast matrix\nimplementation with the ability to model heterogenous networks. Experiments on\nsynthetic graphs with planted edge potentials show that the linearization has\ncomparable labeling accuracy as BP for graphs with weak potentials, while\nspeeding-up inference by orders of magnitude.\n", "versions": [{"version": "v1", "created": "Tue, 17 Feb 2015 16:49:23 GMT"}, {"version": "v2", "created": "Tue, 27 Dec 2016 15:01:40 GMT"}], "update_date": "2016-12-28", "authors_parsed": [["Gatterbauer", "Wolfgang", ""]]}, {"id": "1502.05021", "submitter": "Olegs Verhodubs", "authors": "Olegs Verhodubs", "title": "Inductive Learning for Rule Generation from Ontology", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an idea of inductive learning use for rule generation\nfrom ontologies. The main purpose of the paper is to evaluate the possibility\nof inductive learning use in rule generation from ontologies and to develop the\nway how this can be done. Generated rules are necessary to supplement or even\nto develop the Semantic Web Expert System (SWES) knowledge base. The SWES\nemerges as the result of evolution of expert system concept toward the Web, and\nthe SWES is based on the Semantic Web technologies. Available publications show\nthat the problem of rule generation from ontologies based on inductive learning\nis not investigated deeply enough.\n", "versions": [{"version": "v1", "created": "Tue, 17 Feb 2015 20:17:19 GMT"}], "update_date": "2015-02-18", "authors_parsed": [["Verhodubs", "Olegs", ""]]}, {"id": "1502.05040", "submitter": "Tarek Sobh", "authors": "Tamer M. Abo Neama, Ismail A. Ismail, Tarek S. Sobh, M. Zaki", "title": "Design of a Framework to Facilitate Decisions Using Information Fusion", "comments": "17 pages, 5 figures, Journal of Al Azhar University Engineering\n  Sector, Vol. 8, No. 28, July 2013, 1237-1250. arXiv admin note: text overlap\n  with arXiv:cs/0409007 by other authors", "journal-ref": "Journal of Al Azhar University Engineering Sector, Vol. 8, No. 28,\n  July 2013, 1237-1250", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information fusion is an advanced research area which can assist decision\nmakers in enhancing their decisions. This paper aims at designing a new\nmulti-layer framework that can support the process of performing decisions from\nthe obtained beliefs using information fusion. Since it is not an easy task to\ncross the gap between computed beliefs of certain hypothesis and decisions, the\nproposed framework consists of the following layers in order to provide a\nsuitable architecture (ordered bottom up): 1. A layer for combination of basic\nbelief assignments using an information fusion approach. Such approach exploits\nDezert-Smarandache Theory, DSmT, and proportional conflict redistribution to\nprovide more realistic final beliefs. 2. A layer for computation of pignistic\nprobability of the underlying propositions from the corresponding final\nbeliefs. 3. A layer for performing probabilistic reasoning using a Bayesian\nnetwork that can obtain the probable reason of a proposition from its pignistic\nprobability. 4. Ranking the system decisions is ultimately used to support\ndecision making. A case study has been accomplished at various operational\nconditions in order to prove the concept, in addition it pointed out that: 1.\nThe use of DSmT for information fusion yields not only more realistic beliefs\nbut also reliable pignistic probabilities for the underlying propositions. 2.\nExploiting the pignistic probability for the integration of the information\nfusion with the Bayesian network provides probabilistic inference and enable\ndecision making on the basis of both belief based probabilities for the\nunderlying propositions and Bayesian based probabilities for the corresponding\nreasons. A comparative study of the proposed framework with respect to other\ninformation fusion systems confirms its superiority to support decision making.\n", "versions": [{"version": "v1", "created": "Tue, 17 Feb 2015 12:24:58 GMT"}, {"version": "v2", "created": "Sat, 21 Feb 2015 13:12:25 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Neama", "Tamer M. Abo", ""], ["Ismail", "Ismail A.", ""], ["Sobh", "Tarek S.", ""], ["Zaki", "M.", ""]]}, {"id": "1502.05443", "submitter": "Frans A. Oliehoek", "authors": "Frans A. Oliehoek and Matthijs T.J. Spaan and Stefan Witwicki", "title": "Influence-Optimistic Local Values for Multiagent Planning --- Extended\n  Version", "comments": "Long version of IJCAI 2015 paper (and extended abstract at AAMAS\n  2015)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen the development of methods for multiagent planning\nunder uncertainty that scale to tens or even hundreds of agents. However, most\nof these methods either make restrictive assumptions on the problem domain, or\nprovide approximate solutions without any guarantees on quality. Methods in the\nformer category typically build on heuristic search using upper bounds on the\nvalue function. Unfortunately, no techniques exist to compute such upper bounds\nfor problems with non-factored value functions. To allow for meaningful\nbenchmarking through measurable quality guarantees on a very general class of\nproblems, this paper introduces a family of influence-optimistic upper bounds\nfor factored decentralized partially observable Markov decision processes\n(Dec-POMDPs) that do not have factored value functions. Intuitively, we derive\nbounds on very large multiagent planning problems by subdividing them in\nsub-problems, and at each of these sub-problems making optimistic assumptions\nwith respect to the influence that will be exerted by the rest of the system.\nWe numerically compare the different upper bounds and demonstrate how we can\nachieve a non-trivial guarantee that a heuristic solution for problems with\nhundreds of agents is close to optimal. Furthermore, we provide evidence that\nthe upper bounds may improve the effectiveness of heuristic influence search,\nand discuss further potential applications to multiagent planning.\n", "versions": [{"version": "v1", "created": "Wed, 18 Feb 2015 23:42:24 GMT"}, {"version": "v2", "created": "Mon, 20 Jul 2015 08:58:50 GMT"}], "update_date": "2015-07-21", "authors_parsed": [["Oliehoek", "Frans A.", ""], ["Spaan", "Matthijs T. J.", ""], ["Witwicki", "Stefan", ""]]}, {"id": "1502.05450", "submitter": "Jean-Marc Alliot", "authors": "Jean-Marc Alliot", "title": "The (Final) countdown", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Countdown game is one of the oldest TV show running in the world. It\nstarted broadcasting in 1972 on the french television and in 1982 on British\nchannel 4, and it has been running since in both countries. The game, while\nextremely popular, never received any serious scientific attention, probably\nbecause it seems too simple at first sight. We present in this article an\nin-depth analysis of the numbers round of the countdown game. This includes a\ncomplexity analysis of the game, an analysis of existing algorithms, the\npresentation of a new algorithm that increases resolution speed by a factor of\n20. It also includes some leads on how to turn the game into a more difficult\none, both for a human player and for a computer, and even to transform it into\na probably undecidable problem.\n", "versions": [{"version": "v1", "created": "Thu, 19 Feb 2015 00:41:56 GMT"}], "update_date": "2015-02-20", "authors_parsed": [["Alliot", "Jean-Marc", ""]]}, {"id": "1502.05562", "submitter": "Vasile Patrascu", "authors": "Vasile Patrascu", "title": "A New Penta-valued Logic Based Knowledge Representation", "comments": "The 12th International Conference Information Processing and\n  Management of Uncertainty in Knowledge-Based Systems, June 22-27, 2008,\n  Malaga, Spain", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper a knowledge representation model are proposed, FP5, which\ncombine the ideas from fuzzy sets and penta-valued logic. FP5 represents\nimprecise properties whose accomplished degree is undefined, contradictory or\nindeterminate for some objects. Basic operations of conjunction, disjunction\nand negation are introduced. Relations to other representation models like\nfuzzy sets, intuitionistic, paraconsistent and bipolar fuzzy sets are\ndiscussed.\n", "versions": [{"version": "v1", "created": "Thu, 19 Feb 2015 13:23:06 GMT"}], "update_date": "2015-02-20", "authors_parsed": [["Patrascu", "Vasile", ""]]}, {"id": "1502.05615", "submitter": "C\\`esar Ferri", "authors": "Fernando Mart\\'inez-Plumed, C\\`esar Ferri, Jos\\'e Hern\\'andez-Orallo,\n  Mar\\'ia Jos\\'e Ram\\'irez-Quintana", "title": "Forgetting and consolidation for incremental and cumulative knowledge\n  acquisition systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The application of cognitive mechanisms to support knowledge acquisition is,\nfrom our point of view, crucial for making the resulting models coherent,\nefficient, credible, easy to use and understandable. In particular, there are\ntwo characteristic features of intelligence that are essential for knowledge\ndevelopment: forgetting and consolidation. Both plays an important role in\nknowledge bases and learning systems to avoid possible information overflow and\nredundancy, and in order to preserve and strengthen important or frequently\nused rules and remove (or forget) useless ones. We present an incremental,\nlong-life view of knowledge acquisition which tries to improve task after task\nby determining what to keep, what to consolidate and what to forget, overcoming\nThe Stability-Plasticity dilemma. In order to do that, we rate rules by\nintroducing several metrics through the first adaptation, to our knowledge, of\nthe Minimum Message Length (MML) principle to a coverage graph, a hierarchical\nassessment structure which treats evidence and rules in a unified way. The\nmetrics are not only used to forget some of the worst rules, but also to set a\nconsolidation process to promote those selected rules to the knowledge base,\nwhich is also mirrored by a demotion system. We evaluate the framework with a\nseries of tasks in a chess rule learning domain.\n", "versions": [{"version": "v1", "created": "Thu, 19 Feb 2015 16:25:49 GMT"}], "update_date": "2015-02-20", "authors_parsed": [["Mart\u00ednez-Plumed", "Fernando", ""], ["Ferri", "C\u00e8sar", ""], ["Hern\u00e1ndez-Orallo", "Jos\u00e9", ""], ["Ram\u00edrez-Quintana", "Mar\u00eda Jos\u00e9", ""]]}, {"id": "1502.05696", "submitter": "Nihar Shah", "authors": "Nihar B. Shah, Dengyong Zhou, Yuval Peres", "title": "Approval Voting and Incentives in Crowdsourcing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing need for labeled training data has made crowdsourcing an\nimportant part of machine learning. The quality of crowdsourced labels is,\nhowever, adversely affected by three factors: (1) the workers are not experts;\n(2) the incentives of the workers are not aligned with those of the requesters;\nand (3) the interface does not allow workers to convey their knowledge\naccurately, by forcing them to make a single choice among a set of options. In\nthis paper, we address these issues by introducing approval voting to utilize\nthe expertise of workers who have partial knowledge of the true answer, and\ncoupling it with a (\"strictly proper\") incentive-compatible compensation\nmechanism. We show rigorous theoretical guarantees of optimality of our\nmechanism together with a simple axiomatic characterization. We also conduct\npreliminary empirical studies on Amazon Mechanical Turk which validate our\napproach.\n", "versions": [{"version": "v1", "created": "Thu, 19 Feb 2015 20:42:55 GMT"}, {"version": "v2", "created": "Tue, 19 May 2015 09:12:50 GMT"}, {"version": "v3", "created": "Mon, 7 Sep 2015 05:21:06 GMT"}], "update_date": "2015-09-08", "authors_parsed": [["Shah", "Nihar B.", ""], ["Zhou", "Dengyong", ""], ["Peres", "Yuval", ""]]}, {"id": "1502.05698", "submitter": "Jason  Weston", "authors": "Jason Weston, Antoine Bordes, Sumit Chopra, Alexander M. Rush, Bart\n  van Merri\\\"enboer, Armand Joulin, Tomas Mikolov", "title": "Towards AI-Complete Question Answering: A Set of Prerequisite Toy Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One long-term goal of machine learning research is to produce methods that\nare applicable to reasoning and natural language, in particular building an\nintelligent dialogue agent. To measure progress towards that goal, we argue for\nthe usefulness of a set of proxy tasks that evaluate reading comprehension via\nquestion answering. Our tasks measure understanding in several ways: whether a\nsystem is able to answer questions via chaining facts, simple induction,\ndeduction and many more. The tasks are designed to be prerequisites for any\nsystem that aims to be capable of conversing with a human. We believe many\nexisting learning systems can currently not solve them, and hence our aim is to\nclassify these tasks into skill sets, so that researchers can identify (and\nthen rectify) the failings of their systems. We also extend and improve the\nrecently introduced Memory Networks model, and show it is able to solve some,\nbut not all, of the tasks.\n", "versions": [{"version": "v1", "created": "Thu, 19 Feb 2015 20:46:10 GMT"}, {"version": "v10", "created": "Thu, 31 Dec 2015 13:08:14 GMT"}, {"version": "v2", "created": "Tue, 24 Feb 2015 18:35:12 GMT"}, {"version": "v3", "created": "Wed, 25 Feb 2015 17:50:21 GMT"}, {"version": "v4", "created": "Sat, 14 Mar 2015 14:03:33 GMT"}, {"version": "v5", "created": "Fri, 10 Apr 2015 14:51:46 GMT"}, {"version": "v6", "created": "Tue, 2 Jun 2015 21:58:20 GMT"}, {"version": "v7", "created": "Wed, 7 Oct 2015 19:36:24 GMT"}, {"version": "v8", "created": "Sat, 21 Nov 2015 23:23:02 GMT"}, {"version": "v9", "created": "Sun, 29 Nov 2015 06:24:27 GMT"}], "update_date": "2016-01-01", "authors_parsed": [["Weston", "Jason", ""], ["Bordes", "Antoine", ""], ["Chopra", "Sumit", ""], ["Rush", "Alexander M.", ""], ["van Merri\u00ebnboer", "Bart", ""], ["Joulin", "Armand", ""], ["Mikolov", "Tomas", ""]]}, {"id": "1502.05774", "submitter": "Bo Waggoner", "authors": "Jacob Abernethy, Yiling Chen, Chien-Ju Ho, Bo Waggoner", "title": "Low-Cost Learning via Active Data Procurement", "comments": "Full version of EC 2015 paper. Color recommended for figures but\n  nonessential. 36 pages, of which 12 appendix", "journal-ref": null, "doi": "10.1145/2764468.2764519", "report-no": null, "categories": "cs.GT cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We design mechanisms for online procurement of data held by strategic agents\nfor machine learning tasks. The challenge is to use past data to actively price\nfuture data and give learning guarantees even when an agent's cost for\nrevealing her data may depend arbitrarily on the data itself. We achieve this\ngoal by showing how to convert a large class of no-regret algorithms into\nonline posted-price and learning mechanisms. Our results in a sense parallel\nclassic sample complexity guarantees, but with the key resource being money\nrather than quantity of data: With a budget constraint $B$, we give robust risk\n(predictive error) bounds on the order of $1/\\sqrt{B}$. Because we use an\nactive approach, we can often guarantee to do significantly better by\nleveraging correlations between costs and data.\n  Our algorithms and analysis go through a model of no-regret learning with $T$\narriving pairs (cost, data) and a budget constraint of $B$. Our regret bounds\nfor this model are on the order of $T/\\sqrt{B}$ and we give lower bounds on the\nsame order.\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2015 05:11:44 GMT"}, {"version": "v2", "created": "Sat, 6 Jun 2015 03:24:36 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Abernethy", "Jacob", ""], ["Chen", "Yiling", ""], ["Ho", "Chien-Ju", ""], ["Waggoner", "Bo", ""]]}, {"id": "1502.05838", "submitter": "Claudia Schon", "authors": "Ulrich Furbach, Claudia Schon, Frieder Stolzenburg", "title": "Automated Reasoning for Robot Ethics", "comments": "arXiv admin note: substantial text overlap with arXiv:1411.4823", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deontic logic is a very well researched branch of mathematical logic and\nphilosophy. Various kinds of deontic logics are considered for different\napplication domains like argumentation theory, legal reasoning, and acts in\nmulti-agent systems. In this paper, we show how standard deontic logic can be\nused to model ethical codes for multi-agent systems. Furthermore we show how\nHyper, a high performance theorem prover, can be used to prove properties of\nthese ethical codes.\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2015 11:38:58 GMT"}], "update_date": "2015-02-23", "authors_parsed": [["Furbach", "Ulrich", ""], ["Schon", "Claudia", ""], ["Stolzenburg", "Frieder", ""]]}, {"id": "1502.05844", "submitter": "Mona Dadjoo", "authors": "Mona Dadjoo, Esmaeil Kheirkhah", "title": "An Approach For Transforming of Relational Databases to OWL Ontology", "comments": "10 pages in International Journal of Web & Semantic Technology\n  (IJWesT) Vol.6, No.1, January 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rapid growth of documents, web pages, and other types of text content is a\nhuge challenge for the modern content management systems. One of the problems\nin the areas of information storage and retrieval is the lacking of semantic\ndata. Ontologies can present knowledge in sharable and repeatedly usable manner\nand provide an effective way to reduce the data volume overhead by encoding the\nstructure of a particular domain. Metadata in relational databases can be used\nto extract ontology from database in a special domain. According to solve the\nproblem of sharing and reusing of data, approaches based on transforming\nrelational database to ontology are proposed. In this paper we propose a method\nfor automatic ontology construction based on relational database. Mining and\nobtaining further components from relational database leads to obtain knowledge\nwith high semantic power and more expressiveness. Triggers are one of the\ndatabase components which could be transformed to the ontology model and\nincrease the amount of power and expressiveness of knowledge by presenting part\nof the knowledge dynamically\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2015 11:59:51 GMT"}], "update_date": "2015-02-23", "authors_parsed": [["Dadjoo", "Mona", ""], ["Kheirkhah", "Esmaeil", ""]]}, {"id": "1502.05864", "submitter": "Sukanta Nayak", "authors": "Sukanta Nayak and Snehashish Chakraverty", "title": "Pseudo Fuzzy Set", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here a novel idea to handle imprecise or vague set viz. Pseudo fuzzy set has\nbeen proposed. Pseudo fuzzy set is a triplet of element and its two membership\nfunctions. Both the membership functions may or may not be dependent. The\nhypothesis is that every positive sense has some negative sense. So, one\nmembership function has been considered as positive and another as negative.\nConsidering this concept, here the development of Pseudo fuzzy set and its\nproperty along with Pseudo fuzzy numbers has been discussed.\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2015 13:16:05 GMT"}], "update_date": "2015-02-23", "authors_parsed": [["Nayak", "Sukanta", ""], ["Chakraverty", "Snehashish", ""]]}, {"id": "1502.05888", "submitter": "Marija Slavkovik", "authors": "Jer\\^ome Lang and Gabriella Pigozzi and Marija Slavkovik and Leendert\n  van der Torre and Srdjan Vesic", "title": "A partial taxonomy of judgment aggregation rules, and their properties", "comments": null, "journal-ref": null, "doi": "10.1007/s00355-016-1006-8", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The literature on judgment aggregation is moving from studying impossibility\nresults regarding aggregation rules towards studying specific judgment\naggregation rules. Here we give a structured list of most rules that have been\nproposed and studied recently in the literature, together with various\nproperties of such rules. We first focus on the majority-preservation property,\nwhich generalizes Condorcet-consistency, and identify which of the rules\nsatisfy it. We study the inclusion relationships that hold between the rules.\nFinally, we consider two forms of unanimity, monotonicity, homogeneity, and\nreinforcement, and we identify which of the rules satisfy these properties.\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2015 14:50:53 GMT"}, {"version": "v2", "created": "Thu, 18 Feb 2016 15:49:39 GMT"}, {"version": "v3", "created": "Tue, 27 Sep 2016 15:55:41 GMT"}], "update_date": "2016-12-07", "authors_parsed": [["Lang", "Jer\u00f4me", ""], ["Pigozzi", "Gabriella", ""], ["Slavkovik", "Marija", ""], ["van der Torre", "Leendert", ""], ["Vesic", "Srdjan", ""]]}, {"id": "1502.05974", "submitter": "Brian Thomas", "authors": "Brian Thomas", "title": "Development of a VO Registry Subject Ontology using Automated Methods", "comments": null, "journal-ref": "Astronomical Data Analysis Software and Systems XX. ASP Conference\n  Proceedings, Vol. 442, 2011, p.599", "doi": null, "report-no": null, "categories": "astro-ph.IM cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We report on our initial work to automate the generation of a domain ontology\nusing subject fields of resources held in the Virtual Observatory registry.\nPreliminary results are comparable to more generalized ontology learning\nsoftware currently in use. We expect to be able to refine our solution to\nimprove both the depth and breadth of the generated ontology.\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2015 18:56:26 GMT"}], "update_date": "2015-02-23", "authors_parsed": [["Thomas", "Brian", ""]]}, {"id": "1502.05988", "submitter": "Jesse Read", "authors": "Jesse Read, Fernando Perez-Cruz", "title": "Deep Learning for Multi-label Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In multi-label classification, the main focus has been to develop ways of\nlearning the underlying dependencies between labels, and to take advantage of\nthis at classification time. Developing better feature-space representations\nhas been predominantly employed to reduce complexity, e.g., by eliminating\nnon-helpful feature attributes from the input space prior to (or during)\ntraining. This is an important task, since many multi-label methods typically\ncreate many different copies or views of the same input data as they transform\nit, and considerable memory can be saved by taking advantage of redundancy. In\nthis paper, we show that a proper development of the feature space can make\nlabels less interdependent and easier to model and predict at inference time.\nFor this task we use a deep learning approach with restricted Boltzmann\nmachines. We present a deep network that, in an empirical evaluation,\noutperforms a number of competitive methods from the literature\n", "versions": [{"version": "v1", "created": "Wed, 17 Dec 2014 12:06:47 GMT"}], "update_date": "2015-02-23", "authors_parsed": [["Read", "Jesse", ""], ["Perez-Cruz", "Fernando", ""]]}, {"id": "1502.06025", "submitter": "Benjamin Good", "authors": "Benjamin M. Good, Gavin Ha, Chi K. Ho, Mark D. Wilkinson", "title": "OntoLoki: an automatic, instance-based method for the evaluation of\n  biological ontologies on the Semantic Web", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.QM cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  The delineation of logical definitions for each class in an ontology and the\nconsistent application of these definitions to the assignment of instances to\nclasses are important criteria for ontology evaluation. If ontologies are\nspecified with property-based restrictions on class membership, then such\nconsistency can be checked automatically. If no such logical restrictions are\napplied, as is the case with many biological ontologies, there are currently no\nautomated methods for measuring the semantic consistency of instance assignment\non an ontology-wide scale, nor for inferring the patterns of properties that\nmight define a particular class. We constructed a program that takes as its\ninput an OWL/RDF knowledge base containing an ontology, instances associated\nwith each of the classes in the ontology, and properties of those instances.\nFor each class, it outputs: 1) a rule for determining class membership based on\nthe properties of the instances and 2) a quantitative score for the class that\nreflects the ability of the identified rule to correctly predict class\nmembership for the instances in the knowledge base. We evaluated this program\nusing both artificial knowledge bases of known quality and real, widely used\nontologies. The results indicate that the suggested method can be used to\nconduct objective, automatic, data-driven evaluations of biological ontologies\nwithout formal class definitions in regards to the property-based consistency\nof instance-assignment. This inductive method complements existing, purely\ndeductive approaches to automatic consistency checking, offering not just the\npotential to help in the ontology engineering process but also in the knowledge\ndiscovery process.\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2015 22:34:10 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Good", "Benjamin M.", ""], ["Ha", "Gavin", ""], ["Ho", "Chi K.", ""], ["Wilkinson", "Mark D.", ""]]}, {"id": "1502.06030", "submitter": "Shayegan Omidshafiei", "authors": "Shayegan Omidshafiei, Ali-akbar Agha-mohammadi, Christopher Amato,\n  Jonathan P. How", "title": "Decentralized Control of Partially Observable Markov Decision Processes\n  using Belief Space Macro-actions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The focus of this paper is on solving multi-robot planning problems in\ncontinuous spaces with partial observability. Decentralized partially\nobservable Markov decision processes (Dec-POMDPs) are general models for\nmulti-robot coordination problems, but representing and solving Dec-POMDPs is\noften intractable for large problems. To allow for a high-level representation\nthat is natural for multi-robot problems and scalable to large discrete and\ncontinuous problems, this paper extends the Dec-POMDP model to the\ndecentralized partially observable semi-Markov decision process (Dec-POSMDP).\nThe Dec-POSMDP formulation allows asynchronous decision-making by the robots,\nwhich is crucial in multi-robot domains. We also present an algorithm for\nsolving this Dec-POSMDP which is much more scalable than previous methods since\nit can incorporate closed-loop belief space macro-actions in planning. These\nmacro-actions are automatically constructed to produce robust solutions. The\nproposed method's performance is evaluated on a complex multi-robot package\ndelivery problem under uncertainty, showing that our approach can naturally\nrepresent multi-robot problems and provide high-quality solutions for\nlarge-scale problems.\n", "versions": [{"version": "v1", "created": "Fri, 20 Feb 2015 22:56:00 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Omidshafiei", "Shayegan", ""], ["Agha-mohammadi", "Ali-akbar", ""], ["Amato", "Christopher", ""], ["How", "Jonathan P.", ""]]}, {"id": "1502.06124", "submitter": "Dmytro Filatov", "authors": "Dmytro Filatov, Taras Filatov", "title": "Unified vector space mapping for knowledge representation systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most significant problems which inhibits further developments in\nthe areas of Knowledge Representation and Artificial Intelligence is a problem\nof semantic alignment or knowledge mapping. The progress in its solution will\nbe greatly beneficial for further advances of information retrieval, ontology\nalignment, relevance calculation, text mining, natural language processing etc.\nIn the paper the concept of multidimensional global knowledge map, elaborated\nthrough unsupervised extraction of dependencies from large documents corpus, is\nproposed. In addition, the problem of direct Human - Knowledge Representation\nSystem interface is addressed and a concept of adaptive decoder proposed for\nthe purpose of interaction with previously described unified mapping model. In\ncombination these two approaches are suggested as basis for a development of a\nnew generation of knowledge representation systems.\n", "versions": [{"version": "v1", "created": "Sat, 21 Feb 2015 18:29:57 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Filatov", "Dmytro", ""], ["Filatov", "Taras", ""]]}, {"id": "1502.06132", "submitter": "Dan Guralnik", "authors": "Dan P. Guralnik and Daniel E. Koditschek", "title": "Universal Memory Architectures for Autonomous Machines", "comments": "Technical report, 31 pages, 1 table, 14 figures, 2 appendices", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.RO math.MG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a self-organizing memory architecture for perceptual experience,\ncapable of supporting autonomous learning and goal-directed problem solving in\nthe absence of any prior information about the agent's environment. The\narchitecture is simple enough to ensure (1) a quadratic bound (in the number of\navailable sensors) on space requirements, and (2) a quadratic bound on the\ntime-complexity of the update-execute cycle. At the same time, it is\nsufficiently complex to provide the agent with an internal representation which\nis (3) minimal among all representations of its class which account for every\nsensory equivalence class subject to the agent's belief state; (4) capable, in\nprinciple, of recovering the homotopy type of the system's state space; (5)\nlearnable with arbitrary precision through a random application of the\navailable actions. The provable properties of an effectively trained memory\nstructure exploit a duality between weak poc sets -- a symbolic (discrete)\nrepresentation of subset nesting relations -- and non-positively curved cubical\ncomplexes, whose rich convexity theory underlies the planning cycle of the\nproposed architecture.\n", "versions": [{"version": "v1", "created": "Sat, 21 Feb 2015 19:11:23 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Guralnik", "Dan P.", ""], ["Koditschek", "Daniel E.", ""]]}, {"id": "1502.06512", "submitter": "Roman Yampolskiy", "authors": "Roman V. Yampolskiy", "title": "From Seed AI to Technological Singularity via Recursively Self-Improving\n  Software", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Software capable of improving itself has been a dream of computer scientists\nsince the inception of the field. In this work we provide definitions for\nRecursively Self-Improving software, survey different types of self-improving\nsoftware, review the relevant literature, analyze limits on computation\nrestricting recursive self-improvement and introduce RSI Convergence Theory\nwhich aims to predict general behavior of RSI systems. Finally, we address\nsecurity implications from self-improving intelligent software.\n", "versions": [{"version": "v1", "created": "Mon, 23 Feb 2015 17:08:30 GMT"}], "update_date": "2015-02-24", "authors_parsed": [["Yampolskiy", "Roman V.", ""]]}, {"id": "1502.06626", "submitter": "Malik Magdon-Ismail", "authors": "Malik Magdon-Ismail, Christos Boutsidis", "title": "Optimal Sparse Linear Auto-Encoders and Sparse PCA", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Principal components analysis (PCA) is the optimal linear auto-encoder of\ndata, and it is often used to construct features. Enforcing sparsity on the\nprincipal components can promote better generalization, while improving the\ninterpretability of the features. We study the problem of constructing optimal\nsparse linear auto-encoders. Two natural questions in such a setting are: i)\nGiven a level of sparsity, what is the best approximation to PCA that can be\nachieved? ii) Are there low-order polynomial-time algorithms which can\nasymptotically achieve this optimal tradeoff between the sparsity and the\napproximation quality?\n  In this work, we answer both questions by giving efficient low-order\npolynomial-time algorithms for constructing asymptotically \\emph{optimal}\nlinear auto-encoders (in particular, sparse features with near-PCA\nreconstruction error) and demonstrate the performance of our algorithms on real\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 23 Feb 2015 21:06:39 GMT"}], "update_date": "2015-02-25", "authors_parsed": [["Magdon-Ismail", "Malik", ""], ["Boutsidis", "Christos", ""]]}, {"id": "1502.06657", "submitter": "Sahin Geyik", "authors": "Sahin Cem Geyik, Abhishek Saxena, Ali Dasdan", "title": "Multi-Touch Attribution Based Budget Allocation in Online Advertising", "comments": "This paper has been published in ADKDD 2014, August 24, New York\n  City, New York, U.S.A", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Budget allocation in online advertising deals with distributing the campaign\n(insertion order) level budgets to different sub-campaigns which employ\ndifferent targeting criteria and may perform differently in terms of\nreturn-on-investment (ROI). In this paper, we present the efforts at Turn on\nhow to best allocate campaign budget so that the advertiser or campaign-level\nROI is maximized. To do this, it is crucial to be able to correctly determine\nthe performance of sub-campaigns. This determination is highly related to the\naction-attribution problem, i.e. to be able to find out the set of ads, and\nhence the sub-campaigns that provided them to a user, that an action should be\nattributed to. For this purpose, we employ both last-touch (last ad gets all\ncredit) and multi-touch (many ads share the credit) attribution methodologies.\nWe present the algorithms deployed at Turn for the attribution problem, as well\nas their parallel implementation on the large advertiser performance datasets.\nWe conclude the paper with our empirical comparison of last-touch and\nmulti-touch attribution-based budget allocation in a real online advertising\nsetting.\n", "versions": [{"version": "v1", "created": "Tue, 24 Feb 2015 00:09:05 GMT"}], "update_date": "2015-02-25", "authors_parsed": [["Geyik", "Sahin Cem", ""], ["Saxena", "Abhishek", ""], ["Dasdan", "Ali", ""]]}, {"id": "1502.06818", "submitter": "Ben Usman", "authors": "Ben Usman, Ivan Oseledets", "title": "Tensor SimRank for Heterogeneous Information Networks", "comments": "Submited on KDD'15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a generalization of SimRank similarity measure for heterogeneous\ninformation networks. Given the information network, the intraclass similarity\nscore s(a, b) is high if the set of objects that are related with a and the set\nof objects that are related with b are pair-wise similar according to all\nimposed relations.\n", "versions": [{"version": "v1", "created": "Tue, 24 Feb 2015 14:10:04 GMT"}], "update_date": "2015-02-25", "authors_parsed": [["Usman", "Ben", ""], ["Oseledets", "Ivan", ""]]}, {"id": "1502.06956", "submitter": "Xinyang Deng", "authors": "Xinyang Deng and Yong Deng", "title": "Transformation of basic probability assignments to probabilities based\n  on a new entropy measure", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dempster-Shafer evidence theory is an efficient mathematical tool to deal\nwith uncertain information. In that theory, basic probability assignment (BPA)\nis the basic element for the expression and inference of uncertainty.\nDecision-making based on BPA is still an open issue in Dempster-Shafer evidence\ntheory. In this paper, a novel approach of transforming basic probability\nassignments to probabilities is proposed based on Deng entropy which is a new\nmeasure for the uncertainty of BPA. The principle of the proposed method is to\nminimize the difference of uncertainties involving in the given BPA and\nobtained probability distribution. Numerical examples are given to show the\nproposed approach.\n", "versions": [{"version": "v1", "created": "Tue, 24 Feb 2015 04:02:00 GMT"}], "update_date": "2015-02-26", "authors_parsed": [["Deng", "Xinyang", ""], ["Deng", "Yong", ""]]}, {"id": "1502.07019", "submitter": "Shreyansh Daftry", "authors": "Shreyansh Daftry, Christof Hoppe and Horst Bischof", "title": "Building with Drones: Accurate 3D Facade Reconstruction using MAVs", "comments": "8 Pages, 2015 IEEE International Conference on Robotics and\n  Automation (ICRA '15), Seattle, WA, USA", "journal-ref": null, "doi": "10.1109/ICRA.2015.7139681", "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic reconstruction of 3D models from images using multi-view\nStructure-from-Motion methods has been one of the most fruitful outcomes of\ncomputer vision. These advances combined with the growing popularity of Micro\nAerial Vehicles as an autonomous imaging platform, have made 3D vision tools\nubiquitous for large number of Architecture, Engineering and Construction\napplications among audiences, mostly unskilled in computer vision. However, to\nobtain high-resolution and accurate reconstructions from a large-scale object\nusing SfM, there are many critical constraints on the quality of image data,\nwhich often become sources of inaccuracy as the current 3D reconstruction\npipelines do not facilitate the users to determine the fidelity of input data\nduring the image acquisition. In this paper, we present and advocate a\nclosed-loop interactive approach that performs incremental reconstruction in\nreal-time and gives users an online feedback about the quality parameters like\nGround Sampling Distance (GSD), image redundancy, etc on a surface mesh. We\nalso propose a novel multi-scale camera network design to prevent scene drift\ncaused by incremental map building, and release the first multi-scale image\nsequence dataset as a benchmark. Further, we evaluate our system on real\noutdoor scenes, and show that our interactive pipeline combined with a\nmulti-scale camera network approach provides compelling accuracy in multi-view\nreconstruction tasks when compared against the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 25 Feb 2015 00:52:11 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Daftry", "Shreyansh", ""], ["Hoppe", "Christof", ""], ["Bischof", "Horst", ""]]}, {"id": "1502.07314", "submitter": "David Tolpin", "authors": "David Tolpin, Brooks Paige, Jan Willem van de Meent, Frank Wood", "title": "Path Finding under Uncertainty through Probabilistic Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new approach to solving path-finding problems under\nuncertainty by representing them as probabilistic models and applying\ndomain-independent inference algorithms to the models. This approach separates\nproblem representation from the inference algorithm and provides a framework\nfor efficient learning of path-finding policies. We evaluate the new approach\non the Canadian Traveler Problem, which we formulate as a probabilistic model,\nand show how probabilistic inference allows high performance stochastic\npolicies to be obtained for this problem.\n", "versions": [{"version": "v1", "created": "Wed, 25 Feb 2015 19:21:04 GMT"}, {"version": "v2", "created": "Sat, 2 May 2015 21:53:39 GMT"}, {"version": "v3", "created": "Mon, 8 Jun 2015 05:02:53 GMT"}], "update_date": "2015-06-09", "authors_parsed": [["Tolpin", "David", ""], ["Paige", "Brooks", ""], ["van de Meent", "Jan Willem", ""], ["Wood", "Frank", ""]]}, {"id": "1502.07428", "submitter": "Elad Liebman", "authors": "Elad Liebman, Benny Chor and Peter Stone", "title": "Representative Selection in Non Metric Datasets", "comments": null, "journal-ref": null, "doi": "10.1080/08839514.2015.1071092", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the problem of representative selection: choosing a\nsubset of data points from a dataset that best represents its overall set of\nelements. This subset needs to inherently reflect the type of information\ncontained in the entire set, while minimizing redundancy. For such purposes,\nclustering may seem like a natural approach. However, existing clustering\nmethods are not ideally suited for representative selection, especially when\ndealing with non-metric data, where only a pairwise similarity measure exists.\nIn this paper we propose $\\delta$-medoids, a novel approach that can be viewed\nas an extension to the $k$-medoids algorithm and is specifically suited for\nsample representative selection from non-metric data. We empirically validate\n$\\delta$-medoids in two domains, namely music analysis and motion analysis. We\nalso show some theoretical bounds on the performance of $\\delta$-medoids and\nthe hardness of representative selection in general.\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2015 04:16:31 GMT"}, {"version": "v2", "created": "Fri, 19 Jun 2015 22:44:29 GMT"}], "update_date": "2015-09-29", "authors_parsed": [["Liebman", "Elad", ""], ["Chor", "Benny", ""], ["Stone", "Peter", ""]]}, {"id": "1502.07571", "submitter": "Martin Aleksandrov D", "authors": "Martin Aleksandrov, Haris Aziz, Serge Gaspers, Toby Walsh", "title": "Online Fair Division: analysing a Food Bank problem", "comments": "7 pages, 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study an online model of fair division designed to capture features of a\nreal world charity problem. We consider two simple mechanisms for this model in\nwhich agents simply declare what items they like. We analyse several axiomatic\nproperties of these mechanisms like strategy-proofness and envy-freeness.\nFinally, we perform a competitive analysis and compute the price of anarchy.\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2015 14:39:02 GMT"}, {"version": "v2", "created": "Sat, 28 Feb 2015 08:05:32 GMT"}], "update_date": "2015-03-03", "authors_parsed": [["Aleksandrov", "Martin", ""], ["Aziz", "Haris", ""], ["Gaspers", "Serge", ""], ["Walsh", "Toby", ""]]}, {"id": "1502.07628", "submitter": "Jamal Atif", "authors": "Marc Aiguier, Jamal Atif, Isabelle Bloch and C\\'eline Hudelot", "title": "Relaxation-based revision operators in description logics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As ontologies and description logics (DLs) reach out to a broader audience,\nseveral reasoning services are developed in this context. Belief revision is\none of them, of prime importance when knowledge is prone to change and\ninconsistency. In this paper we address both the generalization of the\nwell-known AGM postulates, and the definition of concrete and well-founded\nrevision operators in different DL families. We introduce a model-theoretic\nversion of the AGM postulates with a general definition of inconsistency, hence\nenlarging their scope to a wide family of non-classical logics, in particular\nnegation-free DL families. We propose a general framework for defining revision\noperators based on the notion of relaxation, introduced recently for defining\ndissimilarity measures between DL concepts. A revision operator in this\nframework amounts to relax the set of models of the old belief until it reaches\nthe sets of models of the new piece of knowledge. We demonstrate that such a\nrelaxation-based revision operator defines a faithful assignment and satisfies\nthe generalized AGM postulates. Another important contribution concerns the\ndefinition of several concrete relaxation operators suited to the syntax of\nsome DLs (ALC and its fragments EL and ELU).\n", "versions": [{"version": "v1", "created": "Thu, 26 Feb 2015 16:41:13 GMT"}], "update_date": "2015-02-27", "authors_parsed": [["Aiguier", "Marc", ""], ["Atif", "Jamal", ""], ["Bloch", "Isabelle", ""], ["Hudelot", "C\u00e9line", ""]]}, {"id": "1502.08029", "submitter": "Li Yao", "authors": "Li Yao, Atousa Torabi, Kyunghyun Cho, Nicolas Ballas, Christopher Pal,\n  Hugo Larochelle, Aaron Courville", "title": "Describing Videos by Exploiting Temporal Structure", "comments": "Accepted to ICCV15. This version comes with code release and\n  supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CL cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent progress in using recurrent neural networks (RNNs) for image\ndescription has motivated the exploration of their application for video\ndescription. However, while images are static, working with videos requires\nmodeling their dynamic temporal structure and then properly integrating that\ninformation into a natural language description. In this context, we propose an\napproach that successfully takes into account both the local and global\ntemporal structure of videos to produce descriptions. First, our approach\nincorporates a spatial temporal 3-D convolutional neural network (3-D CNN)\nrepresentation of the short temporal dynamics. The 3-D CNN representation is\ntrained on video action recognition tasks, so as to produce a representation\nthat is tuned to human motion and behavior. Second we propose a temporal\nattention mechanism that allows to go beyond local temporal modeling and learns\nto automatically select the most relevant temporal segments given the\ntext-generating RNN. Our approach exceeds the current state-of-art for both\nBLEU and METEOR metrics on the Youtube2Text dataset. We also present results on\na new, larger and more challenging dataset of paired video and natural language\ndescriptions.\n", "versions": [{"version": "v1", "created": "Fri, 27 Feb 2015 19:30:40 GMT"}, {"version": "v2", "created": "Wed, 4 Mar 2015 17:24:47 GMT"}, {"version": "v3", "created": "Tue, 10 Mar 2015 15:27:08 GMT"}, {"version": "v4", "created": "Sat, 25 Apr 2015 20:32:27 GMT"}, {"version": "v5", "created": "Thu, 1 Oct 2015 00:12:46 GMT"}], "update_date": "2015-10-02", "authors_parsed": [["Yao", "Li", ""], ["Torabi", "Atousa", ""], ["Cho", "Kyunghyun", ""], ["Ballas", "Nicolas", ""], ["Pal", "Christopher", ""], ["Larochelle", "Hugo", ""], ["Courville", "Aaron", ""]]}, {"id": "1502.08039", "submitter": "Jihun Hamm", "authors": "Jihun Hamm, Mikhail Belkin", "title": "Probabilistic Zero-shot Classification with Semantic Rankings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose a non-metric ranking-based representation of\nsemantic similarity that allows natural aggregation of semantic information\nfrom multiple heterogeneous sources. We apply the ranking-based representation\nto zero-shot learning problems, and present deterministic and probabilistic\nzero-shot classifiers which can be built from pre-trained classifiers without\nretraining. We demonstrate their the advantages on two large real-world image\ndatasets. In particular, we show that aggregating different sources of semantic\ninformation, including crowd-sourcing, leads to more accurate classification.\n", "versions": [{"version": "v1", "created": "Fri, 27 Feb 2015 20:00:53 GMT"}], "update_date": "2015-03-02", "authors_parsed": [["Hamm", "Jihun", ""], ["Belkin", "Mikhail", ""]]}]