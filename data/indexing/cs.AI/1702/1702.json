[{"id": "1702.00020", "submitter": "Marwin Segler", "authors": "Marwin Segler, Mike Preu{\\ss}, Mark P. Waller", "title": "Towards \"AlphaChem\": Chemical Synthesis Planning with Tree Search and\n  Deep Neural Network Policies", "comments": "4 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG physics.chem-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Retrosynthesis is a technique to plan the chemical synthesis of organic\nmolecules, for example drugs, agro- and fine chemicals. In retrosynthesis, a\nsearch tree is built by analysing molecules recursively and dissecting them\ninto simpler molecular building blocks until one obtains a set of known\nbuilding blocks. The search space is intractably large, and it is difficult to\ndetermine the value of retrosynthetic positions. Here, we propose to model\nretrosynthesis as a Markov Decision Process. In combination with a Deep Neural\nNetwork policy learned from essentially the complete published knowledge of\nchemistry, Monte Carlo Tree Search (MCTS) can be used to evaluate positions. In\nexploratory studies, we demonstrate that MCTS with neural network policies\noutperforms the traditionally used best-first search with hand-coded\nheuristics.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 19:07:43 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Segler", "Marwin", ""], ["Preu\u00df", "Mike", ""], ["Waller", "Mark P.", ""]]}, {"id": "1702.00137", "submitter": "Eric Eaton", "authors": "Eric Eaton, Sven Koenig, Claudia Schulz, Francesco Maurelli, John Lee,\n  Joshua Eckroth, Mark Crowley, Richard G. Freedman, Rogelio E. Cardona-Rivera,\n  Tiago Machado, Tom Williams", "title": "Blue Sky Ideas in Artificial Intelligence Education from the EAAI 2017\n  New and Future AI Educator Program", "comments": "Working paper in the 7th Symposium on Educational Advances in\n  Artificial Intelligence (EAAI-17)", "journal-ref": "AI Matters 3(4):23-31, Winter 2018", "doi": "10.1145/3175502.3175509", "report-no": null, "categories": "cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The 7th Symposium on Educational Advances in Artificial Intelligence\n(EAAI'17, co-chaired by Sven Koenig and Eric Eaton) launched the EAAI New and\nFuture AI Educator Program to support the training of early-career university\nfaculty, secondary school faculty, and future educators (PhD candidates or\npostdocs who intend a career in academia). As part of the program, awardees\nwere asked to address one of the following \"blue sky\" questions:\n  * How could/should Artificial Intelligence (AI) courses incorporate ethics\ninto the curriculum?\n  * How could we teach AI topics at an early undergraduate or a secondary\nschool level?\n  * AI has the potential for broad impact to numerous disciplines. How could we\nmake AI education more interdisciplinary, specifically to benefit\nnon-engineering fields?\n  This paper is a collection of their responses, intended to help motivate\ndiscussion around these issues in AI education.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 05:16:55 GMT"}], "update_date": "2019-04-26", "authors_parsed": [["Eaton", "Eric", ""], ["Koenig", "Sven", ""], ["Schulz", "Claudia", ""], ["Maurelli", "Francesco", ""], ["Lee", "John", ""], ["Eckroth", "Joshua", ""], ["Crowley", "Mark", ""], ["Freedman", "Richard G.", ""], ["Cardona-Rivera", "Rogelio E.", ""], ["Machado", "Tiago", ""], ["Williams", "Tom", ""]]}, {"id": "1702.00159", "submitter": "Wei Du", "authors": "Wei Du, Yang Tang, Sunney Yung Sun Leung, Le Tong, Athanasios V.\n  Vasilakos, Feng Qian", "title": "Robust Order Scheduling in the Fashion Industry: A Multi-Objective\n  Optimization Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the fashion industry, order scheduling focuses on the assignment of\nproduction orders to appropriate production lines. In reality, before a new\norder can be put into production, a series of activities known as\npre-production events need to be completed. In addition, in real production\nprocess, owing to various uncertainties, the daily production quantity of each\norder is not always as expected. In this research, by considering the\npre-production events and the uncertainties in the daily production quantity,\nrobust order scheduling problems in the fashion industry are investigated with\nthe aid of a multi-objective evolutionary algorithm (MOEA) called nondominated\nsorting adaptive differential evolution (NSJADE). The experimental results\nillustrate that it is of paramount importance to consider pre-production events\nin order scheduling problems in the fashion industry. We also unveil that the\nexistence of the uncertainties in the daily production quantity heavily affects\nthe order scheduling.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 08:33:21 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Du", "Wei", ""], ["Tang", "Yang", ""], ["Leung", "Sunney Yung Sun", ""], ["Tong", "Le", ""], ["Vasilakos", "Athanasios V.", ""], ["Qian", "Feng", ""]]}, {"id": "1702.00318", "submitter": "Christian Blum", "authors": "Christian Blum and Maria J. Blesa", "title": "A Hybrid Evolutionary Algorithm Based on Solution Merging for the\n  Longest Arc-Preserving Common Subsequence Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The longest arc-preserving common subsequence problem is an NP-hard\ncombinatorial optimization problem from the field of computational biology.\nThis problem finds applications, in particular, in the comparison of\narc-annotated Ribonucleic acid (RNA) sequences. In this work we propose a\nsimple, hybrid evolutionary algorithm to tackle this problem. The most\nimportant feature of this algorithm concerns a crossover operator based on\nsolution merging. In solution merging, two or more solutions to the problem are\nmerged, and an exact technique is used to find the best solution within this\nunion. It is experimentally shown that the proposed algorithm outperforms a\nheuristic from the literature.\n", "versions": [{"version": "v1", "created": "Wed, 1 Feb 2017 15:34:27 GMT"}], "update_date": "2017-02-02", "authors_parsed": [["Blum", "Christian", ""], ["Blesa", "Maria J.", ""]]}, {"id": "1702.00539", "submitter": "Adam Summerville", "authors": "Adam Summerville, Sam Snodgrass, Matthew Guzdial, Christoffer\n  Holmg{\\aa}rd, Amy K. Hoover, Aaron Isaksen, Andy Nealen, Julian Togelius", "title": "Procedural Content Generation via Machine Learning (PCGML)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This survey explores Procedural Content Generation via Machine Learning\n(PCGML), defined as the generation of game content using machine learning\nmodels trained on existing content. As the importance of PCG for game\ndevelopment increases, researchers explore new avenues for generating\nhigh-quality content with or without human involvement; this paper addresses\nthe relatively new paradigm of using machine learning (in contrast with\nsearch-based, solver-based, and constructive methods). We focus on what is most\noften considered functional game content such as platformer levels, game maps,\ninteractive fiction stories, and cards in collectible card games, as opposed to\ncosmetic content such as sprites and sound effects. In addition to using PCG\nfor autonomous generation, co-creativity, mixed-initiative design, and\ncompression, PCGML is suited for repair, critique, and content analysis because\nof its focus on modeling existing content. We discuss various data sources and\nrepresentations that affect the resulting generated content. Multiple PCGML\nmethods are covered, including neural networks, long short-term memory (LSTM)\nnetworks, autoencoders, and deep convolutional networks; Markov models,\n$n$-grams, and multi-dimensional Markov chains; clustering; and matrix\nfactorization. Finally, we discuss open problems in the application of PCGML,\nincluding learning from small datasets, lack of training data, multi-layered\nlearning, style-transfer, parameter tuning, and PCG as a game mechanic.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 04:49:22 GMT"}, {"version": "v2", "created": "Wed, 16 Aug 2017 11:50:28 GMT"}, {"version": "v3", "created": "Mon, 7 May 2018 17:30:42 GMT"}], "update_date": "2018-05-08", "authors_parsed": [["Summerville", "Adam", ""], ["Snodgrass", "Sam", ""], ["Guzdial", "Matthew", ""], ["Holmg\u00e5rd", "Christoffer", ""], ["Hoover", "Amy K.", ""], ["Isaksen", "Aaron", ""], ["Nealen", "Andy", ""], ["Togelius", "Julian", ""]]}, {"id": "1702.00700", "submitter": "Rodrigo Agerri", "authors": "Egoitz Laparra and Rodrigo Agerri and Itziar Aldabe and German Rigau", "title": "Multilingual and Cross-lingual Timeline Extraction", "comments": "20 pages, 7 tables, 7 figures; submitted to Knowledge Based Systems\n  (Elsevier), January, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  In this paper we present an approach to extract ordered timelines of events,\ntheir participants, locations and times from a set of multilingual and\ncross-lingual data sources. Based on the assumption that event-related\ninformation can be recovered from different documents written in different\nlanguages, we extend the Cross-document Event Ordering task presented at\nSemEval 2015 by specifying two new tasks for, respectively, Multilingual and\nCross-lingual Timeline Extraction. We then develop three deterministic\nalgorithms for timeline extraction based on two main ideas. First, we address\nimplicit temporal relations at document level since explicit time-anchors are\ntoo scarce to build a wide coverage timeline extraction system. Second, we\nleverage several multilingual resources to obtain a single, inter-operable,\nsemantic representation of events across documents and across languages. The\nresult is a highly competitive system that strongly outperforms the current\nstate-of-the-art. Nonetheless, further analysis of the results reveals that\nlinking the event mentions with their target entities and time-anchors remains\na difficult challenge. The systems, resources and scorers are freely available\nto facilitate its use and guarantee the reproducibility of results.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 14:44:17 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Laparra", "Egoitz", ""], ["Agerri", "Rodrigo", ""], ["Aldabe", "Itziar", ""], ["Rigau", "German", ""]]}, {"id": "1702.00780", "submitter": "Andrea Cohen", "authors": "Zimi Li, Andrea Cohen, Simon Parsons", "title": "Two forms of minimality in ASPIC+", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many systems of structured argumentation explicitly require that the facts\nand rules that make up the argument for a conclusion be the minimal set\nrequired to derive the conclusion. ASPIC+ does not place such a requirement on\narguments, instead requiring that every rule and fact that are part of an\nargument be used in its construction. Thus ASPIC+ arguments are minimal in the\nsense that removing any element of the argument would lead to a structure that\nis not an argument. In this brief note we discuss these two types of minimality\nand show how the first kind of minimality can, if desired, be recovered in\nASPIC+.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 18:45:38 GMT"}], "update_date": "2017-02-03", "authors_parsed": [["Li", "Zimi", ""], ["Cohen", "Andrea", ""], ["Parsons", "Simon", ""]]}, {"id": "1702.00858", "submitter": "Zachary Sunberg", "authors": "Zachary Sunberg, Christopher Ho, and Mykel Kochenderfer", "title": "The Value of Inferring the Internal State of Traffic Participants for\n  Autonomous Freeway Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Safe interaction with human drivers is one of the primary challenges for\nautonomous vehicles. In order to plan driving maneuvers effectively, the\nvehicle's control system must infer and predict how humans will behave based on\ntheir latent internal state (e.g., intentions and aggressiveness). This\nresearch uses a simple model for human behavior with unknown parameters that\nmake up the internal states of the traffic participants and presents a method\nfor quantifying the value of estimating these states and planning with their\nuncertainty explicitly modeled. An upper performance bound is established by an\nomniscient Monte Carlo Tree Search (MCTS) planner that has perfect knowledge of\nthe internal states. A baseline lower bound is established by planning with\nMCTS assuming that all drivers have the same internal state. MCTS variants are\nthen used to solve a partially observable Markov decision process (POMDP) that\nmodels the internal state uncertainty to determine whether inferring the\ninternal state offers an advantage over the baseline. Applying this method to a\nfreeway lane changing scenario reveals that there is a significant performance\ngap between the upper bound and baseline. POMDP planning techniques come close\nto closing this gap, especially when important hidden model parameters are\ncorrelated with measurable parameters.\n", "versions": [{"version": "v1", "created": "Thu, 2 Feb 2017 22:38:10 GMT"}], "update_date": "2017-02-06", "authors_parsed": [["Sunberg", "Zachary", ""], ["Ho", "Christopher", ""], ["Kochenderfer", "Mykel", ""]]}, {"id": "1702.00953", "submitter": "Zhaowei Cai", "authors": "Zhaowei Cai, Xiaodong He, Jian Sun, Nuno Vasconcelos", "title": "Deep Learning with Low Precision by Half-wave Gaussian Quantization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of quantizing the activations of a deep neural network is\nconsidered. An examination of the popular binary quantization approach shows\nthat this consists of approximating a classical non-linearity, the hyperbolic\ntangent, by two functions: a piecewise constant sign function, which is used in\nfeedforward network computations, and a piecewise linear hard tanh function,\nused in the backpropagation step during network learning. The problem of\napproximating the ReLU non-linearity, widely used in the recent deep learning\nliterature, is then considered. An half-wave Gaussian quantizer (HWGQ) is\nproposed for forward approximation and shown to have efficient implementation,\nby exploiting the statistics of of network activations and batch normalization\noperations commonly used in the literature. To overcome the problem of gradient\nmismatch, due to the use of different forward and backward approximations,\nseveral piece-wise backward approximators are then investigated. The\nimplementation of the resulting quantized network, denoted as HWGQ-Net, is\nshown to achieve much closer performance to full precision networks, such as\nAlexNet, ResNet, GoogLeNet and VGG-Net, than previously available low-precision\nnetworks, with 1-bit binary weights and 2-bit quantized activations.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 10:11:40 GMT"}], "update_date": "2017-02-06", "authors_parsed": [["Cai", "Zhaowei", ""], ["He", "Xiaodong", ""], ["Sun", "Jian", ""], ["Vasconcelos", "Nuno", ""]]}, {"id": "1702.01018", "submitter": "Joydeep Banerjee", "authors": "Joydeep Banerjee, Chenyang Zhou, and Arunabha Sen", "title": "On Robustness in Multilayer Interdependent Network", "comments": "CRITIS 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Critical Infrastructures like power and communication networks are highly\ninterdependent on each other for their full functionality. Many significant\nresearch have been pursued to model the interdependency and failure analysis of\nthese interdependent networks. However, most of these models fail to capture\nthe complex interdependencies that might actually exist between the\ninfrastructures. The \\emph{Implicative Interdependency Model} that utilizes\nBoolean Logic to capture complex interdependencies was recently proposed which\novercome the limitations of the existing models. A number of problems were\nstudies based on this model. In this paper we study the \\textit{Robustness}\nproblem in Interdependent Power and Communication Network. The robustness is\ndefined with respect to two parameters $K \\in I^{+} \\cup \\{0\\}$ and $\\rho \\in\n(0,1]$. We utilized the \\emph{Implicative Interdependency Model} model to\ncapture the complex interdependency between the two networks. The model\nclassifies the interdependency relations into four cases. Computational\ncomplexity of the problem is analyzed for each of these cases. A polynomial\ntime algorithm is designed for the first case that outputs the optimal\nsolution. All the other cases are proved to be NP-complete. An\nin-approximability bound is provided for the third case. For the general case\nwe formulate an Integer Linear Program to get the optimal solution and a\npolynomial time heuristic. The applicability of the heuristic is evaluated\nusing power and communication network data of Maricopa County, Arizona. The\nexperimental results showed that the heuristic almost always produced near\noptimal value of parameter $K$ for $\\rho < 0.42$.\n", "versions": [{"version": "v1", "created": "Tue, 24 Jan 2017 23:01:34 GMT"}], "update_date": "2017-02-06", "authors_parsed": [["Banerjee", "Joydeep", ""], ["Zhou", "Chenyang", ""], ["Sen", "Arunabha", ""]]}, {"id": "1702.01135", "submitter": "Guy Katz", "authors": "Guy Katz, Clark Barrett, David Dill, Kyle Julian, Mykel Kochenderfer", "title": "Reluplex: An Efficient SMT Solver for Verifying Deep Neural Networks", "comments": "This is the extended version of a paper with the same title that\n  appeared at CAV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have emerged as a widely used and effective means for\ntackling complex, real-world problems. However, a major obstacle in applying\nthem to safety-critical systems is the great difficulty in providing formal\nguarantees about their behavior. We present a novel, scalable, and efficient\ntechnique for verifying properties of deep neural networks (or providing\ncounter-examples). The technique is based on the simplex method, extended to\nhandle the non-convex Rectified Linear Unit (ReLU) activation function, which\nis a crucial ingredient in many modern neural networks. The verification\nprocedure tackles neural networks as a whole, without making any simplifying\nassumptions. We evaluated our technique on a prototype deep neural network\nimplementation of the next-generation airborne collision avoidance system for\nunmanned aircraft (ACAS Xu). Results show that our technique can successfully\nprove properties of networks that are an order of magnitude larger than the\nlargest networks verified using existing methods.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 19:26:01 GMT"}, {"version": "v2", "created": "Fri, 19 May 2017 04:50:29 GMT"}], "update_date": "2017-05-22", "authors_parsed": [["Katz", "Guy", ""], ["Barrett", "Clark", ""], ["Dill", "David", ""], ["Julian", "Kyle", ""], ["Kochenderfer", "Mykel", ""]]}, {"id": "1702.01205", "submitter": "Shumeet Baluja", "authors": "Shumeet Baluja, Michele Covell, Rahul Sukthankar", "title": "Traffic Lights with Auction-Based Controllers: Algorithms and Real-World\n  Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time optimization of traffic flow addresses important practical\nproblems: reducing a driver's wasted time, improving city-wide efficiency,\nreducing gas emissions and improving air quality. Much of the current research\nin traffic-light optimization relies on extending the capabilities of traffic\nlights to either communicate with each other or communicate with vehicles.\nHowever, before such capabilities become ubiquitous, opportunities exist to\nimprove traffic lights by being more responsive to current traffic situations\nwithin the current, already deployed, infrastructure. In this paper, we\nintroduce a traffic light controller that employs bidding within micro-auctions\nto efficiently incorporate traffic sensor information; no other outside sources\nof information are assumed. We train and test traffic light controllers on\nlarge-scale data collected from opted-in Android cell-phone users over a period\nof several months in Mountain View, California and the River North neighborhood\nof Chicago, Illinois. The learned auction-based controllers surpass (in both\nthe relevant metrics of road-capacity and mean travel time) the currently\ndeployed lights, optimized static-program lights, and longer-term planning\napproaches, in both cities, measured using real user driving data.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 23:44:02 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Baluja", "Shumeet", ""], ["Covell", "Michele", ""], ["Sukthankar", "Rahul", ""]]}, {"id": "1702.01208", "submitter": "Arya Mazumdar", "authors": "Arya Mazumdar, Barna Saha", "title": "A Theoretical Analysis of First Heuristics of Crowdsourced Entity\n  Resolution", "comments": "Appears in AAAI-17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity resolution (ER) is the task of identifying all records in a database\nthat refer to the same underlying entity, and are therefore duplicates of each\nother. Due to inherent ambiguity of data representation and poor data quality,\nER is a challenging task for any automated process. As a remedy, human-powered\nER via crowdsourcing has become popular in recent years. Using crowd to answer\nqueries is costly and time consuming. Furthermore, crowd-answers can often be\nfaulty. Therefore, crowd-based ER methods aim to minimize human participation\nwithout sacrificing the quality and use a computer generated similarity matrix\nactively. While, some of these methods perform well in practice, no theoretical\nanalysis exists for them, and further their worst case performances do not\nreflect the experimental findings. This creates a disparity in the\nunderstanding of the popular heuristics for this problem. In this paper, we\nmake the first attempt to close this gap. We provide a thorough analysis of the\nprominent heuristic algorithms for crowd-based ER. We justify experimental\nobservations with our analysis and information theoretic lower bounds.\n", "versions": [{"version": "v1", "created": "Fri, 3 Feb 2017 23:56:58 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Mazumdar", "Arya", ""], ["Saha", "Barna", ""]]}, {"id": "1702.01313", "submitter": "Bas van Stein", "authors": "Bas van Stein, Hao Wang, Wojtek Kowalczyk, Michael Emmerich, Thomas\n  B\\\"ack", "title": "Cluster-based Kriging Approximation Algorithms for Complexity Reduction", "comments": "Submitted to IEEE Computational Intelligence Magazine for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Kriging or Gaussian Process Regression is applied in many fields as a\nnon-linear regression model as well as a surrogate model in the field of\nevolutionary computation. However, the computational and space complexity of\nKriging, that is cubic and quadratic in the number of data points respectively,\nbecomes a major bottleneck with more and more data available nowadays. In this\npaper, we propose a general methodology for the complexity reduction, called\ncluster Kriging, where the whole data set is partitioned into smaller clusters\nand multiple Kriging models are built on top of them. In addition, four Kriging\napproximation algorithms are proposed as candidate algorithms within the new\nframework. Each of these algorithms can be applied to much larger data sets\nwhile maintaining the advantages and power of Kriging. The proposed algorithms\nare explained in detail and compared empirically against a broad set of\nexisting state-of-the-art Kriging approximation methods on a well-defined\ntesting framework. According to the empirical study, the proposed algorithms\nconsistently outperform the existing algorithms. Moreover, some practical\nsuggestions are provided for using the proposed algorithms.\n", "versions": [{"version": "v1", "created": "Sat, 4 Feb 2017 17:54:59 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["van Stein", "Bas", ""], ["Wang", "Hao", ""], ["Kowalczyk", "Wojtek", ""], ["Emmerich", "Michael", ""], ["B\u00e4ck", "Thomas", ""]]}, {"id": "1702.01332", "submitter": "Michael Huth", "authors": "Andrea Callia D'Iddio and Michael Huth", "title": "Manyopt: An Extensible Tool for Mixed, Non-Linear Optimization Through\n  SMT Solving", "comments": "17 pages, 3 figures, link to open research data and code available", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimization of Mixed-Integer Non-Linear Programming (MINLP) supports\nimportant decisions in applications such as Chemical Process Engineering. But\ncurrent solvers have limited ability for deductive reasoning or the use of\ndomain-specific theories, and the management of integrality constraints does\nnot yet exploit automated reasoning tools such as SMT solvers. This seems to\nlimit both scalability and reach of such tools in practice. We therefore\npresent a tool, ManyOpt, for MINLP optimization that enables experimentation\nwith reduction techniques which transform a MINLP problem to feasibility\nchecking realized by an SMT solver. ManyOpt is similar to the SAT solver\nManySAT in that it runs a specified number of such reduction techniques in\nparallel to get the strongest result on a given MINLP problem. The tool is\nimplemented in layers, which we may see as features and where reduction\ntechniques are feature vectors. Some of these features are inspired by known\nMINLP techniques whereas others are novel and specific to SMT. Our experimental\nresults on standard benchmarks demonstrate the benefits of this approach. The\ntool supports a variety of SMT solvers and is easily extensible with new\nfeatures, courtesy of its layered structure. For example, logical formulas for\ndeductive reasoning are easily added to constrain further the optimization of a\nMINLP problem of interest.\n", "versions": [{"version": "v1", "created": "Sat, 4 Feb 2017 19:49:06 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["D'Iddio", "Andrea Callia", ""], ["Huth", "Michael", ""]]}, {"id": "1702.01510", "submitter": "Yong Wang", "authors": "Zi Jian Yang, Yong Wang", "title": "Survey of modern Fault Diagnosis methods in networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of modern computer networks, fault diagnosis has been a focus\nof research activity. This paper reviews the history of fault diagnosis in\nnetworks and discusses the main methods in information gathering section,\ninformation analyzing section and diagnosing and revolving section of fault\ndiagnosis in networks. Emphasis will be placed upon knowledge-based methods\nwith discussing the advantages and shortcomings of the different methods. The\nsurvey is concluded with a description of some open problems.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 06:43:16 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Yang", "Zi Jian", ""], ["Wang", "Yong", ""]]}, {"id": "1702.01601", "submitter": "David Fern\\'andez-Duque", "authors": "Philippe Balbiani and David Fern\\'andez-Duque and Emiliano Lorini", "title": "Exploring the bidimensional space: A dynamic logic point of view", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a family of logics for reasoning about agents' positions and\nmotion in the plane which have several potential applications in the area of\nmulti-agent systems (MAS), such as multi-agent planning and robotics. The most\ngeneral logic includes (i) atomic formulas for representing the truth of a\ngiven fact or the presence of a given agent at a certain position of the plane,\n(ii) atomic programs corresponding to the four basic orientations in the plane\n(up, down, left, right) as well as the four program constructs of propositional\ndynamic logic (sequential composition, nondeterministic composition, iteration\nand test). As this logic is not computably enumerable, we study some\ninteresting decidable and axiomatizable fragments of it. We also present a\ndecidable extension of the iteration-free fragment of the logic by special\nprograms representing motion of agents in the plane.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 13:05:58 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Balbiani", "Philippe", ""], ["Fern\u00e1ndez-Duque", "David", ""], ["Lorini", "Emiliano", ""]]}, {"id": "1702.01721", "submitter": "Afshin Dehghan", "authors": "Afshin Dehghan, Syed Zain Masood, Guang Shu, Enrique. G. Ortiz", "title": "View Independent Vehicle Make, Model and Color Recognition Using\n  Convolutional Neural Network", "comments": "7 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the details of Sighthound's fully automated vehicle\nmake, model and color recognition system. The backbone of our system is a deep\nconvolutional neural network that is not only computationally inexpensive, but\nalso provides state-of-the-art results on several competitive benchmarks.\nAdditionally, our deep network is trained on a large dataset of several million\nimages which are labeled through a semi-automated process. Finally we test our\nsystem on several public datasets as well as our own internal test dataset. Our\nresults show that we outperform other methods on all benchmarks by significant\nmargins. Our model is available to developers through the Sighthound Cloud API\nat https://www.sighthound.com/products/cloud\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 17:47:08 GMT"}], "update_date": "2017-02-07", "authors_parsed": [["Dehghan", "Afshin", ""], ["Masood", "Syed Zain", ""], ["Shu", "Guang", ""], ["Ortiz", "Enrique. G.", ""]]}, {"id": "1702.01795", "submitter": "Peter Patel-Schneider", "authors": "Peter F. Patel-Schneider", "title": "ASHACL: Alternative Shapes Constraint Language", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  ASHACL, a variant of the W3C Shapes Constraint Language, is designed to\ndetermine whether an RDF graph meets some conditions. These conditions are\ngrouped into shapes, which validate whether particular RDF terms each meet the\nconstraints of the shape. Shapes are themselves expressed as RDF triples in an\nRDF graph, called a shapes graph.\n", "versions": [{"version": "v1", "created": "Mon, 6 Feb 2017 21:13:43 GMT"}, {"version": "v2", "created": "Thu, 9 Mar 2017 13:10:02 GMT"}], "update_date": "2017-03-10", "authors_parsed": [["Patel-Schneider", "Peter F.", ""]]}, {"id": "1702.01886", "submitter": "Sara Bernardini", "authors": "Sara Bernardini, Fabio Fagnani, David E. Smith", "title": "Extracting Lifted Mutual Exclusion Invariants from Temporal Planning\n  Domains", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a technique for automatically extracting mutual exclusion\ninvariants from temporal planning instances. It first identifies a set of\ninvariant templates by inspecting the lifted representation of the domain and\nthen checks these templates against properties that assure invariance. Our\ntechnique builds on other approaches to invariant synthesis presented in the\nliterature, but departs from their limited focus on instantaneous actions by\naddressing temporal domains. To deal with time, we formulate invariance\nconditions that account for the entire structure of the actions and the\npossible concurrent interactions between them. As a result, we construct a\nsignificantly more comprehensive technique than previous methods, which is able\nto find not only invariants for temporal domains, but also a broader set of\ninvariants for non-temporal domains. The experimental results reported in this\npaper provide evidence that identifying a broader set of invariants results in\nthe generation of fewer multi-valued state variables with larger domains. We\nshow that, in turn, this reduction in the number of variables reflects\npositively on the performance of a number of temporal planners that use a\nvariable/value representation by significantly reducing their running time.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 06:02:50 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Bernardini", "Sara", ""], ["Fagnani", "Fabio", ""], ["Smith", "David E.", ""]]}, {"id": "1702.01975", "submitter": "Vladimir Dzyuba", "authors": "Vladimir Dzyuba, Matthijs van Leeuwen", "title": "Learning what matters - Sampling interesting patterns", "comments": "PAKDD 2017, extended version", "journal-ref": "Advances in Knowledge Discovery and Data Mining. PAKDD 2017.\n  Lecture Notes in Computer Science, vol.10234, 2017, pp.534-546", "doi": "10.1007/978-3-319-57454-7_42", "report-no": null, "categories": "stat.ML cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the field of exploratory data mining, local structure in data can be\ndescribed by patterns and discovered by mining algorithms. Although many\nsolutions have been proposed to address the redundancy problems in pattern\nmining, most of them either provide succinct pattern sets or take the interests\nof the user into account-but not both. Consequently, the analyst has to invest\nsubstantial effort in identifying those patterns that are relevant to her\nspecific interests and goals. To address this problem, we propose a novel\napproach that combines pattern sampling with interactive data mining. In\nparticular, we introduce the LetSIP algorithm, which builds upon recent\nadvances in 1) weighted sampling in SAT and 2) learning to rank in interactive\npattern mining. Specifically, it exploits user feedback to directly learn the\nparameters of the sampling distribution that represents the user's interests.\nWe compare the performance of the proposed algorithm to the state-of-the-art in\ninteractive pattern mining by emulating the interests of a user. The resulting\nsystem allows efficient and interleaved learning and sampling, thus\nuser-specific anytime data exploration. Finally, LetSIP demonstrates favourable\ntrade-offs concerning both quality-diversity and exploitation-exploration when\ncompared to existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 12:01:08 GMT"}, {"version": "v2", "created": "Fri, 10 Feb 2017 16:22:18 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Dzyuba", "Vladimir", ""], ["van Leeuwen", "Matthijs", ""]]}, {"id": "1702.01991", "submitter": "Grzegorz Chrupa{\\l}a", "authors": "Grzegorz Chrupa{\\l}a, Lieke Gelderloos, Afra Alishahi", "title": "Representations of language in a model of visually grounded speech\n  signal", "comments": "Accepted at ACL 2017", "journal-ref": null, "doi": "10.18653/v1/P17-1057", "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We present a visually grounded model of speech perception which projects\nspoken utterances and images to a joint semantic space. We use a multi-layer\nrecurrent highway network to model the temporal nature of spoken speech, and\nshow that it learns to extract both form and meaning-based linguistic knowledge\nfrom the input signal. We carry out an in-depth analysis of the representations\nused by different components of the trained model and show that encoding of\nsemantic aspects tends to become richer as we go up the hierarchy of layers,\nwhereas encoding of form-related aspects of the language input tends to\ninitially increase and then plateau or decrease.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 13:02:09 GMT"}, {"version": "v2", "created": "Thu, 1 Jun 2017 12:57:36 GMT"}, {"version": "v3", "created": "Fri, 30 Jun 2017 07:34:55 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Chrupa\u0142a", "Grzegorz", ""], ["Gelderloos", "Lieke", ""], ["Alishahi", "Afra", ""]]}, {"id": "1702.02032", "submitter": "Ji\\v{r}\\'i Vomlel", "authors": "Ji\\v{r}\\'i Vomlel", "title": "Solving the Brachistochrone Problem by an Influence Diagram", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Influence diagrams are a decision-theoretic extension of probabilistic\ngraphical models. In this paper we show how they can be used to solve the\nBrachistochrone problem. We present results of numerical experiments on this\nproblem, compare the solution provided by the influence diagram with the\noptimal solution. The R code used for the experiments is presented in the\nAppendix.\n", "versions": [{"version": "v1", "created": "Sat, 4 Feb 2017 12:35:37 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Vomlel", "Ji\u0159\u00ed", ""]]}, {"id": "1702.02258", "submitter": "Ehsan Jahangiri", "authors": "Ehsan Jahangiri, Alan L. Yuille", "title": "Generating Multiple Diverse Hypotheses for Human 3D Pose Consistent with\n  2D Joint Detections", "comments": "accepted to ICCV 2017 (PeopleCap)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We propose a method to generate multiple diverse and valid human pose\nhypotheses in 3D all consistent with the 2D detection of joints in a monocular\nRGB image. We use a novel generative model uniform (unbiased) in the space of\nanatomically plausible 3D poses. Our model is compositional (produces a pose by\ncombining parts) and since it is restricted only by anatomical constraints it\ncan generalize to every plausible human 3D pose. Removing the model bias\nintrinsically helps to generate more diverse 3D pose hypotheses. We argue that\ngenerating multiple pose hypotheses is more reasonable than generating only a\nsingle 3D pose based on the 2D joint detection given the depth ambiguity and\nthe uncertainty due to occlusion and imperfect 2D joint detection. We hope that\nthe idea of generating multiple consistent pose hypotheses can give rise to a\nnew line of future work that has not received much attention in the literature.\nWe used the Human3.6M dataset for empirical evaluation.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 02:54:25 GMT"}, {"version": "v2", "created": "Sun, 20 Aug 2017 19:39:19 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Jahangiri", "Ehsan", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1702.02277", "submitter": "Frank Z. Xing", "authors": "Frank Z. Xing", "title": "A Historical Review of Forty Years of Research on CMAC", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Cerebellar Model Articulation Controller (CMAC) is an influential\nbrain-inspired computing model in many relevant fields. Since its inception in\nthe 1970s, the model has been intensively studied and many variants of the\nprototype, such as Kernel-CMAC, Self-Organizing Map CMAC, and Linguistic CMAC,\nhave been proposed. This review article focus on how the CMAC model is\ngradually developed and refined to meet the demand of fast, adaptive, and\nrobust control. Two perspective, CMAC as a neural network and CMAC as a table\nlook-up technique are presented. Three aspects of the model: the architecture,\nlearning algorithms and applications are discussed. In the end, some potential\nfuture research directions on this model are suggested.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 04:27:11 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["Xing", "Frank Z.", ""]]}, {"id": "1702.02302", "submitter": "Hyunmin Chae", "authors": "Hyunmin Chae, Chang Mook Kang, ByeoungDo Kim, Jaekyum Kim, Chung Choo\n  Chung and Jun Won Choi", "title": "Autonomous Braking System via Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a new autonomous braking system based on deep\nreinforcement learning. The proposed autonomous braking system automatically\ndecides whether to apply the brake at each time step when confronting the risk\nof collision using the information on the obstacle obtained by the sensors. The\nproblem of designing brake control is formulated as searching for the optimal\npolicy in Markov decision process (MDP) model where the state is given by the\nrelative position of the obstacle and the vehicle's speed, and the action space\nis defined as whether brake is stepped or not. The policy used for brake\ncontrol is learned through computer simulations using the deep reinforcement\nlearning method called deep Q-network (DQN). In order to derive desirable\nbraking policy, we propose the reward function which balances the damage\nimposed to the obstacle in case of accident and the reward achieved when the\nvehicle runs out of risk as soon as possible. DQN is trained for the scenario\nwhere a vehicle is encountered with a pedestrian crossing the urban road.\nExperiments show that the control agent exhibits desirable control behavior and\navoids collision without any mistake in various uncertain environments.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 06:51:33 GMT"}, {"version": "v2", "created": "Mon, 24 Apr 2017 12:43:36 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Chae", "Hyunmin", ""], ["Kang", "Chang Mook", ""], ["Kim", "ByeoungDo", ""], ["Kim", "Jaekyum", ""], ["Chung", "Chung Choo", ""], ["Choi", "Jun Won", ""]]}, {"id": "1702.02470", "submitter": "Emmanuel Hebrard", "authors": "Cl\\'ement Carbonnel (LAAS-ROC), Emmanuel H\\'ebrard (LAAS-ROC)", "title": "Propagation via Kernelization: The Vertex Cover Constraint", "comments": null, "journal-ref": "Michel Rueher. The 22nd International Conference on Principles and\n  Practice of Constraint Programming, Sep 2016, Toulouse, France. Lecture Notes\n  in Computer Science, 9892, pp.147 - 156, 2016, Principles and Practice of\n  Constraint Programming", "doi": "10.1007/978-3-319-44953-1_10", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The technique of kernelization consists in extracting, from an instance of a\nproblem, an essentially equivalent instance whose size is bounded in a\nparameter k. Besides being the basis for efficient param-eterized algorithms,\nthis method also provides a wealth of information to reason about in the\ncontext of constraint programming. We study the use of kernelization for\ndesigning propagators through the example of the Vertex Cover constraint. Since\nthe classic kernelization rules often correspond to dominance rather than\nconsistency, we introduce the notion of \"loss-less\" kernel. While our\npreliminary experimental results show the potential of the approach, they also\nshow some of its limits. In particular, this method is more effective for\nvertex covers of large and sparse graphs, as they tend to have, relatively,\nsmaller kernels.\n", "versions": [{"version": "v1", "created": "Tue, 7 Feb 2017 15:45:39 GMT"}], "update_date": "2017-02-09", "authors_parsed": [["Carbonnel", "Cl\u00e9ment", "", "LAAS-ROC"], ["H\u00e9brard", "Emmanuel", "", "LAAS-ROC"]]}, {"id": "1702.02519", "submitter": "Adrian Benton", "authors": "Adrian Benton, Huda Khayrallah, Biman Gujral, Dee Ann Reisinger, Sheng\n  Zhang, Raman Arora", "title": "Deep Generalized Canonical Correlation Analysis", "comments": "14 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Deep Generalized Canonical Correlation Analysis (DGCCA) -- a\nmethod for learning nonlinear transformations of arbitrarily many views of\ndata, such that the resulting transformations are maximally informative of each\nother. While methods for nonlinear two-view representation learning (Deep CCA,\n(Andrew et al., 2013)) and linear many-view representation learning\n(Generalized CCA (Horst, 1961)) exist, DGCCA is the first CCA-style multiview\nrepresentation learning technique that combines the flexibility of nonlinear\n(deep) representation learning with the statistical power of incorporating\ninformation from many independent sources, or views. We present the DGCCA\nformulation as well as an efficient stochastic optimization algorithm for\nsolving it. We learn DGCCA representations on two distinct datasets for three\ndownstream tasks: phonetic transcription from acoustic and articulatory\nmeasurements, and recommending hashtags and friends on a dataset of Twitter\nusers. We find that DGCCA representations soundly beat existing methods at\nphonetic transcription and hashtag recommendation, and in general perform no\nworse than standard linear many-view techniques.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 16:57:48 GMT"}, {"version": "v2", "created": "Thu, 15 Jun 2017 00:06:08 GMT"}], "update_date": "2017-06-16", "authors_parsed": [["Benton", "Adrian", ""], ["Khayrallah", "Huda", ""], ["Gujral", "Biman", ""], ["Reisinger", "Dee Ann", ""], ["Zhang", "Sheng", ""], ["Arora", "Raman", ""]]}, {"id": "1702.02540", "submitter": "William Murdoch", "authors": "W. James Murdoch and Arthur Szlam", "title": "Automatic Rule Extraction from Long Short Term Memory Networks", "comments": "ICLR 2017 accepted paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although deep learning models have proven effective at solving problems in\nnatural language processing, the mechanism by which they come to their\nconclusions is often unclear. As a result, these models are generally treated\nas black boxes, yielding no insight of the underlying learned patterns. In this\npaper we consider Long Short Term Memory networks (LSTMs) and demonstrate a new\napproach for tracking the importance of a given input to the LSTM for a given\noutput. By identifying consistently important patterns of words, we are able to\ndistill state of the art LSTMs on sentiment analysis and question answering\ninto a set of representative phrases. This representation is then\nquantitatively validated by using the extracted phrases to construct a simple,\nrule-based classifier which approximates the output of the LSTM.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 17:46:37 GMT"}, {"version": "v2", "created": "Fri, 24 Feb 2017 22:20:25 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Murdoch", "W. James", ""], ["Szlam", "Arthur", ""]]}, {"id": "1702.02604", "submitter": "Mohammad Taha Bahadori", "authors": "Mohammad Taha Bahadori, Krzysztof Chalupka, Edward Choi, Robert Chen,\n  Walter F. Stewart, Jimeng Sun", "title": "Causal Regularization", "comments": "Adding theoretical analysis, revising the text", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In application domains such as healthcare, we want accurate predictive models\nthat are also causally interpretable. In pursuit of such models, we propose a\ncausal regularizer to steer predictive models towards causally-interpretable\nsolutions and theoretically study its properties. In a large-scale analysis of\nElectronic Health Records (EHR), our causally-regularized model outperforms its\nL1-regularized counterpart in causal accuracy and is competitive in predictive\nperformance. We perform non-linear causality analysis by causally regularizing\na special neural network architecture. We also show that the proposed causal\nregularizer can be used together with neural representation learning algorithms\nto yield up to 20% improvement over multilayer perceptron in detecting\nmultivariate causation, a situation common in healthcare, where many causal\nfactors should occur simultaneously to have an effect on the target variable.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 20:23:59 GMT"}, {"version": "v2", "created": "Thu, 23 Feb 2017 18:52:58 GMT"}], "update_date": "2017-02-24", "authors_parsed": [["Bahadori", "Mohammad Taha", ""], ["Chalupka", "Krzysztof", ""], ["Choi", "Edward", ""], ["Chen", "Robert", ""], ["Stewart", "Walter F.", ""], ["Sun", "Jimeng", ""]]}, {"id": "1702.02628", "submitter": "Amin Ghafouri", "authors": "Amin Ghafouri, Aron Laszka, Abhishek Dubey, and Xenofon Koutsoukos", "title": "Optimal Detection of Faulty Traffic Sensors Used in Route Planning", "comments": "Proceedings of The 2nd Workshop on Science of Smart City Operations\n  and Platforms Engineering (SCOPE 2017), Pittsburgh, PA USA, April 2017, 6\n  pages", "journal-ref": null, "doi": "10.1145/3063386.3063767", "report-no": null, "categories": "cs.AI cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a smart city, real-time traffic sensors may be deployed for various\napplications, such as route planning. Unfortunately, sensors are prone to\nfailures, which result in erroneous traffic data. Erroneous data can adversely\naffect applications such as route planning, and can cause increased travel\ntime. To minimize the impact of sensor failures, we must detect them promptly\nand accurately. However, typical detection algorithms may lead to a large\nnumber of false positives (i.e., false alarms) and false negatives (i.e.,\nmissed detections), which can result in suboptimal route planning. In this\npaper, we devise an effective detector for identifying faulty traffic sensors\nusing a prediction model based on Gaussian Processes. Further, we present an\napproach for computing the optimal parameters of the detector which minimize\nlosses due to false-positive and false-negative errors. We also characterize\ncritical sensors, whose failure can have high impact on the route planning\napplication. Finally, we implement our method and evaluate it numerically using\na real-world dataset and the route planning platform OpenTripPlanner.\n", "versions": [{"version": "v1", "created": "Wed, 8 Feb 2017 21:49:46 GMT"}, {"version": "v2", "created": "Thu, 16 Mar 2017 16:37:47 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Ghafouri", "Amin", ""], ["Laszka", "Aron", ""], ["Dubey", "Abhishek", ""], ["Koutsoukos", "Xenofon", ""]]}, {"id": "1702.02676", "submitter": "Arman Afrasiyabi", "authors": "Arman Afrasiyabi, Ozan Yildiz, Baris Nasir, Fatos T. Yarman Vural and\n  A. Enis Cetin", "title": "Energy Saving Additive Neural Network", "comments": "8 pages (double column), 2 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years, machine learning techniques based on neural networks for\nmobile computing become increasingly popular. Classical multi-layer neural\nnetworks require matrix multiplications at each stage. Multiplication operation\nis not an energy efficient operation and consequently it drains the battery of\nthe mobile device. In this paper, we propose a new energy efficient neural\nnetwork with the universal approximation property over space of Lebesgue\nintegrable functions. This network, called, additive neural network, is very\nsuitable for mobile computing. The neural structure is based on a novel vector\nproduct definition, called ef-operator, that permits a multiplier-free\nimplementation. In ef-operation, the \"product\" of two real numbers is defined\nas the sum of their absolute values, with the sign determined by the sign of\nthe product of the numbers. This \"product\" is used to construct a vector\nproduct in $R^N$. The vector product induces the $l_1$ norm. The proposed\nadditive neural network successfully solves the XOR problem. The experiments on\nMNIST dataset show that the classification performances of the proposed\nadditive neural networks are very similar to the corresponding multi-layer\nperceptron and convolutional neural networks (LeNet).\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2017 02:02:27 GMT"}], "update_date": "2017-02-10", "authors_parsed": [["Afrasiyabi", "Arman", ""], ["Yildiz", "Ozan", ""], ["Nasir", "Baris", ""], ["Vural", "Fatos T. Yarman", ""], ["Cetin", "A. Enis", ""]]}, {"id": "1702.02817", "submitter": "Immanuel Bayer", "authors": "Immanuel Bayer, Uwe Nagel, Steffen Rendle", "title": "Graph Based Relational Features for Collective Classification", "comments": "Pacific-Asia Conference on Knowledge Discovery and Data Mining", "journal-ref": null, "doi": "10.1007/978-3-319-18032-8_35", "report-no": null, "categories": "cs.IR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Statistical Relational Learning (SRL) methods have shown that classification\naccuracy can be improved by integrating relations between samples. Techniques\nsuch as iterative classification or relaxation labeling achieve this by\npropagating information between related samples during the inference process.\nWhen only a few samples are labeled and connections between samples are sparse,\ncollective inference methods have shown large improvements over standard\nfeature-based ML methods. However, in contrast to feature based ML, collective\ninference methods require complex inference procedures and often depend on the\nstrong assumption of label consistency among related samples. In this paper, we\nintroduce new relational features for standard ML methods by extracting\ninformation from direct and indirect relations. We show empirically on three\nstandard benchmark datasets that our relational features yield results\ncomparable to collective inference methods. Finally we show that our proposal\noutperforms these methods when additional information is available.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2017 12:58:23 GMT"}], "update_date": "2017-02-13", "authors_parsed": [["Bayer", "Immanuel", ""], ["Nagel", "Uwe", ""], ["Rendle", "Steffen", ""]]}, {"id": "1702.02821", "submitter": "Hendrik Schawe", "authors": "Hendrik Schawe, Roman Bleim, Alexander K. Hartmann", "title": "Phase Transitions of the Typical Algorithmic Complexity of the Random\n  Satisfiability Problem Studied with Linear Programming", "comments": "11 pages, 5 figures", "journal-ref": "PLOS ONE 14, 4 (2019)", "doi": "10.1371/journal.pone.0215309", "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Here we study the NP-complete $K$-SAT problem. Although the worst-case\ncomplexity of NP-complete problems is conjectured to be exponential, there\nexist parametrized random ensembles of problems where solutions can typically\nbe found in polynomial time for suitable ranges of the parameter. In fact,\nrandom $K$-SAT, with $\\alpha=M/N $ as control parameter, can be solved quickly\nfor small enough values of $\\alpha$. It shows a phase transition between a\nsatisfiable phase and an unsatisfiable phase. For branch and bound algorithms,\nwhich operate in the space of feasible Boolean configurations, the empirically\nhardest problems are located only close to this phase transition. Here we study\n$K$-SAT ($K=3,4$) and the related optimization problem MAX-SAT by a linear\nprogramming approach, which is widely used for practical problems and allows\nfor polynomial run time. In contrast to branch and bound it operates outside\nthe space of feasible configurations. On the other hand, finding a solution\nwithin polynomial time is not guaranteed. We investigated several variants like\nincluding artificial objective functions, so called cutting-plane approaches,\nand a mapping to the NP-complete vertex-cover problem. We observed several\neasy-hard transitions, from where the problems are typically solvable (in\npolynomial time) using the given algorithms, respectively, to where they are\nnot solvable in polynomial time. For the related vertex-cover problem on random\ngraphs these easy-hard transitions can be identified with structural properties\nof the graphs, like percolation transitions. For the present random $K$-SAT\nproblem we have investigated numerous structural properties also exhibiting\nclear transitions, but they appear not be correlated to the here observed\neasy-hard transitions. This renders the behaviour of random $K$-SAT more\ncomplex than, e.g., the vertex-cover problem.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2017 13:18:08 GMT"}, {"version": "v2", "created": "Tue, 18 Sep 2018 14:59:52 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Schawe", "Hendrik", ""], ["Bleim", "Roman", ""], ["Hartmann", "Alexander K.", ""]]}, {"id": "1702.02890", "submitter": "Johannes Klaus Fichte", "authors": "Johannes Fichte, Markus Hecher, Michael Morak, Stefan Woltran", "title": "Answer Set Solving with Bounded Treewidth Revisited", "comments": "This paper extends and updates a paper that has been presented on the\n  workshop TAASP'16 (arXiv:1612.07601). We provide a higher detail level, full\n  proofs and more examples", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parameterized algorithms are a way to solve hard problems more efficiently,\ngiven that a specific parameter of the input is small. In this paper, we apply\nthis idea to the field of answer set programming (ASP). To this end, we propose\ntwo kinds of graph representations of programs to exploit their treewidth as a\nparameter. Treewidth roughly measures to which extent the internal structure of\na program resembles a tree. Our main contribution is the design of\nparameterized dynamic programming algorithms, which run in linear time if the\ntreewidth and weights of the given program are bounded. Compared to previous\nwork, our algorithms handle the full syntax of ASP. Finally, we report on an\nempirical evaluation that shows good runtime behaviour for benchmark instances\nof low treewidth, especially for counting answer sets.\n", "versions": [{"version": "v1", "created": "Thu, 9 Feb 2017 16:50:23 GMT"}], "update_date": "2017-02-10", "authors_parsed": [["Fichte", "Johannes", ""], ["Hecher", "Markus", ""], ["Morak", "Michael", ""], ["Woltran", "Stefan", ""]]}, {"id": "1702.03037", "submitter": "Marc Lanctot", "authors": "Joel Z. Leibo, Vinicius Zambaldi, Marc Lanctot, Janusz Marecki, Thore\n  Graepel", "title": "Multi-agent Reinforcement Learning in Sequential Social Dilemmas", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.GT cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matrix games like Prisoner's Dilemma have guided research on social dilemmas\nfor decades. However, they necessarily treat the choice to cooperate or defect\nas an atomic action. In real-world social dilemmas these choices are temporally\nextended. Cooperativeness is a property that applies to policies, not\nelementary actions. We introduce sequential social dilemmas that share the\nmixed incentive structure of matrix game social dilemmas but also require\nagents to learn policies that implement their strategic intentions. We analyze\nthe dynamics of policies learned by multiple self-interested independent\nlearning agents, each using its own deep Q-network, on two Markov games we\nintroduce here: 1. a fruit Gathering game and 2. a Wolfpack hunting game. We\ncharacterize how learned behavior in each domain changes as a function of\nenvironmental factors including resource abundance. Our experiments show how\nconflict can emerge from competition over shared resources and shed light on\nhow the sequential nature of real world social dilemmas affects cooperation.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2017 01:48:40 GMT"}], "update_date": "2017-02-13", "authors_parsed": [["Leibo", "Joel Z.", ""], ["Zambaldi", "Vinicius", ""], ["Lanctot", "Marc", ""], ["Marecki", "Janusz", ""], ["Graepel", "Thore", ""]]}, {"id": "1702.03044", "submitter": "Anbang Yao", "authors": "Aojun Zhou, Anbang Yao, Yiwen Guo, Lin Xu, Yurong Chen", "title": "Incremental Network Quantization: Towards Lossless CNNs with\n  Low-Precision Weights", "comments": "Published by ICLR 2017, and the code is available at\n  https://github.com/Zhouaojun/Incremental-Network-Quantization", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents incremental network quantization (INQ), a novel method,\ntargeting to efficiently convert any pre-trained full-precision convolutional\nneural network (CNN) model into a low-precision version whose weights are\nconstrained to be either powers of two or zero. Unlike existing methods which\nare struggled in noticeable accuracy loss, our INQ has the potential to resolve\nthis issue, as benefiting from two innovations. On one hand, we introduce three\ninterdependent operations, namely weight partition, group-wise quantization and\nre-training. A well-proven measure is employed to divide the weights in each\nlayer of a pre-trained CNN model into two disjoint groups. The weights in the\nfirst group are responsible to form a low-precision base, thus they are\nquantized by a variable-length encoding method. The weights in the other group\nare responsible to compensate for the accuracy loss from the quantization, thus\nthey are the ones to be re-trained. On the other hand, these three operations\nare repeated on the latest re-trained group in an iterative manner until all\nthe weights are converted into low-precision ones, acting as an incremental\nnetwork quantization and accuracy enhancement procedure. Extensive experiments\non the ImageNet classification task using almost all known deep CNN\narchitectures including AlexNet, VGG-16, GoogleNet and ResNets well testify the\nefficacy of the proposed method. Specifically, at 5-bit quantization, our\nmodels have improved accuracy than the 32-bit floating-point references. Taking\nResNet-18 as an example, we further show that our quantized models with 4-bit,\n3-bit and 2-bit ternary weights have improved or very similar accuracy against\nits 32-bit floating-point baseline. Besides, impressive results with the\ncombination of network pruning and INQ are also reported. The code is available\nat https://github.com/Zhouaojun/Incremental-Network-Quantization.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2017 02:30:22 GMT"}, {"version": "v2", "created": "Fri, 25 Aug 2017 13:21:18 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Zhou", "Aojun", ""], ["Yao", "Anbang", ""], ["Guo", "Yiwen", ""], ["Xu", "Lin", ""], ["Chen", "Yurong", ""]]}, {"id": "1702.03121", "submitter": "Ashutosh Modi", "authors": "Ashutosh Modi, Ivan Titov, Vera Demberg, Asad Sayeed and Manfred\n  Pinkal", "title": "Modeling Semantic Expectation: Using Script Knowledge for Referent\n  Prediction", "comments": "14 pages, published at TACL, 2017, Volume-5, Pg 31-44, 2017", "journal-ref": "Transactions of ACL, Volume-5, Pg 31-44 (2017)", "doi": null, "report-no": null, "categories": "cs.CL cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research in psycholinguistics has provided increasing evidence that\nhumans predict upcoming content. Prediction also affects perception and might\nbe a key to robustness in human language processing. In this paper, we\ninvestigate the factors that affect human prediction by building a\ncomputational model that can predict upcoming discourse referents based on\nlinguistic knowledge alone vs. linguistic knowledge jointly with common-sense\nknowledge in the form of scripts. We find that script knowledge significantly\nimproves model estimates of human predictions. In a second study, we test the\nhighly controversial hypothesis that predictability influences referring\nexpression type but do not find evidence for such an effect.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2017 10:31:57 GMT"}], "update_date": "2017-02-13", "authors_parsed": [["Modi", "Ashutosh", ""], ["Titov", "Ivan", ""], ["Demberg", "Vera", ""], ["Sayeed", "Asad", ""], ["Pinkal", "Manfred", ""]]}, {"id": "1702.03274", "submitter": "Jason Williams", "authors": "Jason D. Williams, Kavosh Asadi, Geoffrey Zweig", "title": "Hybrid Code Networks: practical and efficient end-to-end dialog control\n  with supervised and reinforcement learning", "comments": "Accepted as a long paper for the 55th Annual Meeting of the\n  Association for Computational Linguistics (ACL 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  End-to-end learning of recurrent neural networks (RNNs) is an attractive\nsolution for dialog systems; however, current techniques are data-intensive and\nrequire thousands of dialogs to learn simple behaviors. We introduce Hybrid\nCode Networks (HCNs), which combine an RNN with domain-specific knowledge\nencoded as software and system action templates. Compared to existing\nend-to-end approaches, HCNs considerably reduce the amount of training data\nrequired, while retaining the key benefit of inferring a latent representation\nof dialog state. In addition, HCNs can be optimized with supervised learning,\nreinforcement learning, or a mixture of both. HCNs attain state-of-the-art\nperformance on the bAbI dialog dataset, and outperform two commercially\ndeployed customer-facing dialog systems.\n", "versions": [{"version": "v1", "created": "Fri, 10 Feb 2017 18:24:13 GMT"}, {"version": "v2", "created": "Mon, 24 Apr 2017 14:39:27 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Williams", "Jason D.", ""], ["Asadi", "Kavosh", ""], ["Zweig", "Geoffrey", ""]]}, {"id": "1702.03401", "submitter": "Aske Plaat", "authors": "Aske Plaat, Jonathan Schaeffer, Wim Pijls, Arie de Bruin", "title": "A Minimax Algorithm Better Than Alpha-beta?: No and Yes", "comments": "Report version of AI Journal article Best-first fixed-depth minimax\n  algorithms 1996. arXiv admin note: text overlap with arXiv:1404.1517", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper has three main contributions to our understanding of fixed-depth\nminimax search: (A) A new formulation for Stockman's SSS* algorithm, based on\nAlpha-Beta, is presented. It solves all the perceived drawbacks of SSS*,\nfinally transforming it into a practical algorithm. In effect, we show that\nSSS* = alpha-beta + ransposition tables. The crucial step is the realization\nthat transposition tables contain so-called solution trees, structures that are\nused in best-first search algorithms like SSS*. Having created a practical\nversion, we present performance measurements with tournament game-playing\nprograms for three different minimax games, yielding results that contradict a\nnumber of publications. (B) Based on the insights gained in our attempts at\nunderstanding SSS*, we present a framework that facilitates the construction of\nseveral best-first fixed- depth game-tree search algorithms, known and new. The\nframework is based on depth-first null-window Alpha-Beta search, enhanced with\nstorage to allow for the refining of previous search results. It focuses\nattention on the essential differences between algorithms. (C) We present a new\ninstance of the framework, MTD(f). It is well-suited for use with iterative\ndeepening, and performs better than algorithms that are currently used in most\nstate-of-the-art game-playing programs. We provide experimental evidence to\nexplain why MTD(f) performs better than the other fixed-depth minimax\nalgorithms.\n", "versions": [{"version": "v1", "created": "Sat, 11 Feb 2017 09:48:12 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Plaat", "Aske", ""], ["Schaeffer", "Jonathan", ""], ["Pijls", "Wim", ""], ["de Bruin", "Arie", ""]]}, {"id": "1702.03443", "submitter": "Wei Wen", "authors": "Yandan Wang, Wei Wen, Beiye Liu, Donald Chiarulli, Hai Li", "title": "Group Scissor: Scaling Neuromorphic Computing Design to Large Neural\n  Networks", "comments": "Accepted in DAC 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Synapse crossbar is an elementary structure in Neuromorphic Computing Systems\n(NCS). However, the limited size of crossbars and heavy routing congestion\nimpedes the NCS implementations of big neural networks. In this paper, we\npropose a two-step framework (namely, group scissor) to scale NCS designs to\nbig neural networks. The first step is rank clipping, which integrates low-rank\napproximation into the training to reduce total crossbar area. The second step\nis group connection deletion, which structurally prunes connections to reduce\nrouting congestion between crossbars. Tested on convolutional neural networks\nof LeNet on MNIST database and ConvNet on CIFAR-10 database, our experiments\nshow significant reduction of crossbar area and routing area in NCS designs.\nWithout accuracy loss, rank clipping reduces total crossbar area to 13.62\\% and\n51.81\\% in the NCS designs of LeNet and ConvNet, respectively. Following rank\nclipping, group connection deletion further reduces the routing area of LeNet\nand ConvNet to 8.1\\% and 52.06\\%, respectively.\n", "versions": [{"version": "v1", "created": "Sat, 11 Feb 2017 17:34:34 GMT"}, {"version": "v2", "created": "Sat, 17 Jun 2017 18:29:37 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Wang", "Yandan", ""], ["Wen", "Wei", ""], ["Liu", "Beiye", ""], ["Chiarulli", "Donald", ""], ["Li", "Hai", ""]]}, {"id": "1702.03488", "submitter": "Karan Goel", "authors": "Karan Goel, Shreya Rajpal and Mausam", "title": "Octopus: A Framework for Cost-Quality-Time Optimization in Crowdsourcing", "comments": "10 pages, to appear in HCOMP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Octopus, an AI agent to jointly balance three conflicting task\nobjectives on a micro-crowdsourcing marketplace - the quality of work, total\ncost incurred, and time to completion. Previous control agents have mostly\nfocused on cost-quality, or cost-time tradeoffs, but not on directly\ncontrolling all three in concert. A naive formulation of three-objective\noptimization is intractable; Octopus takes a hierarchical POMDP approach, with\nthree different components responsible for setting the pay per task, selecting\nthe next task, and controlling task-level quality. We demonstrate that Octopus\nsignificantly outperforms existing state-of-the-art approaches on real\nexperiments. We also deploy Octopus on Amazon Mechanical Turk, showing its\nability to manage tasks in a real-world dynamic setting.\n", "versions": [{"version": "v1", "created": "Sun, 12 Feb 2017 04:53:25 GMT"}, {"version": "v2", "created": "Tue, 15 Aug 2017 09:14:08 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Goel", "Karan", ""], ["Rajpal", "Shreya", ""], ["Mausam", "", ""]]}, {"id": "1702.03584", "submitter": "Qi Lei", "authors": "Qi Lei, Jinfeng Yi, Roman Vaculin, Lingfei Wu, Inderjit S. Dhillon", "title": "Similarity Preserving Representation Learning for Time Series Clustering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A considerable amount of clustering algorithms take instance-feature matrices\nas their inputs. As such, they cannot directly analyze time series data due to\nits temporal nature, usually unequal lengths, and complex properties. This is a\ngreat pity since many of these algorithms are effective, robust, efficient, and\neasy to use. In this paper, we bridge this gap by proposing an efficient\nrepresentation learning framework that is able to convert a set of time series\nwith various lengths to an instance-feature matrix. In particular, we guarantee\nthat the pairwise similarities between time series are well preserved after the\ntransformation, thus the learned feature representation is particularly\nsuitable for the time series clustering task. Given a set of $n$ time series,\nwe first construct an $n\\times n$ partially-observed similarity matrix by\nrandomly sampling $\\mathcal{O}(n \\log n)$ pairs of time series and computing\ntheir pairwise similarities. We then propose an efficient algorithm that solves\na non-convex and NP-hard problem to learn new features based on the\npartially-observed similarity matrix. By conducting extensive empirical\nstudies, we show that the proposed framework is more effective, efficient, and\nflexible, compared to other state-of-the-art time series clustering methods.\n", "versions": [{"version": "v1", "created": "Sun, 12 Feb 2017 22:38:42 GMT"}, {"version": "v2", "created": "Thu, 9 Mar 2017 02:03:48 GMT"}, {"version": "v3", "created": "Sun, 2 Jun 2019 04:15:50 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Lei", "Qi", ""], ["Yi", "Jinfeng", ""], ["Vaculin", "Roman", ""], ["Wu", "Lingfei", ""], ["Dhillon", "Inderjit S.", ""]]}, {"id": "1702.03592", "submitter": "Benedikt B\\\"unz", "authors": "Benedikt B\\\"unz and Matthew Lamm", "title": "Graph Neural Networks and Boolean Satisfiability", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we explore whether or not deep neural architectures can learn\nto classify Boolean satisfiability (SAT). We devote considerable time to\ndiscussing the theoretical properties of SAT. Then, we define a graph\nrepresentation for Boolean formulas in conjunctive normal form, and train\nneural classifiers over general graph structures called Graph Neural Networks,\nor GNNs, to recognize features of satisfiability. To the best of our knowledge\nthis has never been tried before. Our preliminary findings are potentially\nprofound. In a weakly-supervised setting, that is, without problem specific\nfeature engineering, Graph Neural Networks can learn features of\nsatisfiability.\n", "versions": [{"version": "v1", "created": "Sun, 12 Feb 2017 23:12:01 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["B\u00fcnz", "Benedikt", ""], ["Lamm", "Matthew", ""]]}, {"id": "1702.03594", "submitter": "Andr\\'es Herrera-Poyatos", "authors": "Andr\\'es Herrera-Poyatos and Francisco Herrera", "title": "Genetic and Memetic Algorithm with Diversity Equilibrium based on Greedy\n  Diversification", "comments": "27 pages, 5 figures, 11 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The lack of diversity in a genetic algorithm's population may lead to a bad\nperformance of the genetic operators since there is not an equilibrium between\nexploration and exploitation. In those cases, genetic algorithms present a fast\nand unsuitable convergence.\n  In this paper we develop a novel hybrid genetic algorithm which attempts to\nobtain a balance between exploration and exploitation. It confronts the\ndiversity problem using the named greedy diversification operator. Furthermore,\nthe proposed algorithm applies a competition between parent and children so as\nto exploit the high quality visited solutions. These operators are complemented\nby a simple selection mechanism designed to preserve and take advantage of the\npopulation diversity.\n  Additionally, we extend our proposal to the field of memetic algorithms,\nobtaining an improved model with outstanding results in practice.\n  The experimental study shows the validity of the approach as well as how\nimportant is taking into account the exploration and exploitation concepts when\ndesigning an evolutionary algorithm.\n", "versions": [{"version": "v1", "created": "Sun, 12 Feb 2017 23:23:17 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Herrera-Poyatos", "Andr\u00e9s", ""], ["Herrera", "Francisco", ""]]}, {"id": "1702.03724", "submitter": "Mieczys{\\l}aw K{\\l}opotek", "authors": "Mieczys{\\l}aw K{\\l}opotek", "title": "On Seeking Consensus Between Document Similarity Measures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the application of consensus clustering and\nmeta-clustering to the set of all possible partitions of a data set. We show\nthat when using a \"complement\" of Rand Index as a measure of cluster\nsimilarity, the total-separation partition, putting each element in a separate\nset, is chosen.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 11:46:04 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["K\u0142opotek", "Mieczys\u0142aw", ""]]}, {"id": "1702.03767", "submitter": "Patrick O. Glauner", "authors": "Patrick Glauner, Angelo Migliosi, Jorge Meira, Petko Valtchev, Radu\n  State, Franck Bettinger", "title": "Is Big Data Sufficient for a Reliable Detection of Non-Technical Losses?", "comments": "Proceedings of the 19th International Conference on Intelligent\n  System Applications to Power Systems (ISAP 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-technical losses (NTL) occur during the distribution of electricity in\npower grids and include, but are not limited to, electricity theft and faulty\nmeters. In emerging countries, they may range up to 40% of the total\nelectricity distributed. In order to detect NTLs, machine learning methods are\nused that learn irregular consumption patterns from customer data and\ninspection results. The Big Data paradigm followed in modern machine learning\nreflects the desire of deriving better conclusions from simply analyzing more\ndata, without the necessity of looking at theory and models. However, the\nsample of inspected customers may be biased, i.e. it does not represent the\npopulation of all customers. As a consequence, machine learning models trained\non these inspection results are biased as well and therefore lead to unreliable\npredictions of whether customers cause NTL or not. In machine learning, this\nissue is called covariate shift and has not been addressed in the literature on\nNTL detection yet. In this work, we present a novel framework for quantifying\nand visualizing covariate shift. We apply it to a commercial data set from\nBrazil that consists of 3.6M customers and 820K inspection results. We show\nthat some features have a stronger covariate shift than others, making\npredictions less reliable. In particular, previous inspections were focused on\ncertain neighborhoods or customer classes and that they were not sufficiently\nspread among the population of customers. This framework is about to be\ndeployed in a commercial product for NTL detection.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 13:33:47 GMT"}, {"version": "v2", "created": "Tue, 25 Jul 2017 04:35:45 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Glauner", "Patrick", ""], ["Migliosi", "Angelo", ""], ["Meira", "Jorge", ""], ["Valtchev", "Petko", ""], ["State", "Radu", ""], ["Bettinger", "Franck", ""]]}, {"id": "1702.03812", "submitter": "Stefano Nichele", "authors": "Stefano Nichele and Magnus S. Gundersen", "title": "Reservoir Computing Using Non-Uniform Binary Cellular Automata", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.ET cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Reservoir Computing (RC) paradigm utilizes a dynamical system, i.e., a\nreservoir, and a linear classifier, i.e., a read-out layer, to process data\nfrom sequential classification tasks. In this paper the usage of Cellular\nAutomata (CA) as a reservoir is investigated. The use of CA in RC has been\nshowing promising results. In this paper, selected state-of-the-art experiments\nare reproduced. It is shown that some CA-rules perform better than others, and\nthe reservoir performance is improved by increasing the size of the CA\nreservoir itself. In addition, the usage of parallel loosely coupled\nCA-reservoirs, where each reservoir has a different CA-rule, is investigated.\nThe experiments performed on quasi-uniform CA reservoir provide valuable\ninsights in CA reservoir design. The results herein show that some rules do not\nwork well together, while other combinations work remarkably well. This\nsuggests that non-uniform CA could represent a powerful tool for novel CA\nreservoir implementations.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 15:23:06 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Nichele", "Stefano", ""], ["Gundersen", "Magnus S.", ""]]}, {"id": "1702.03814", "submitter": "Zhiguo Wang", "authors": "Zhiguo Wang, Wael Hamza, Radu Florian", "title": "Bilateral Multi-Perspective Matching for Natural Language Sentences", "comments": "To appear in Proceedings of IJCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural language sentence matching is a fundamental technology for a variety\nof tasks. Previous approaches either match sentences from a single direction or\nonly apply single granular (word-by-word or sentence-by-sentence) matching. In\nthis work, we propose a bilateral multi-perspective matching (BiMPM) model\nunder the \"matching-aggregation\" framework. Given two sentences $P$ and $Q$,\nour model first encodes them with a BiLSTM encoder. Next, we match the two\nencoded sentences in two directions $P \\rightarrow Q$ and $P \\leftarrow Q$. In\neach matching direction, each time step of one sentence is matched against all\ntime-steps of the other sentence from multiple perspectives. Then, another\nBiLSTM layer is utilized to aggregate the matching results into a fix-length\nmatching vector. Finally, based on the matching vector, the decision is made\nthrough a fully connected layer. We evaluate our model on three tasks:\nparaphrase identification, natural language inference and answer sentence\nselection. Experimental results on standard benchmark datasets show that our\nmodel achieves the state-of-the-art performance on all tasks.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 15:26:27 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2017 14:03:53 GMT"}, {"version": "v3", "created": "Fri, 14 Jul 2017 19:45:07 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Wang", "Zhiguo", ""], ["Hamza", "Wael", ""], ["Florian", "Radu", ""]]}, {"id": "1702.03859", "submitter": "Samuel L. Smith", "authors": "Samuel L. Smith, David H. P. Turban, Steven Hamblin and Nils Y.\n  Hammerla", "title": "Offline bilingual word vectors, orthogonal transformations and the\n  inverted softmax", "comments": "Accepted to conference track at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Usually bilingual word vectors are trained \"online\". Mikolov et al. showed\nthey can also be found \"offline\", whereby two pre-trained embeddings are\naligned with a linear transformation, using dictionaries compiled from expert\nknowledge. In this work, we prove that the linear transformation between two\nspaces should be orthogonal. This transformation can be obtained using the\nsingular value decomposition. We introduce a novel \"inverted softmax\" for\nidentifying translation pairs, with which we improve the precision @1 of\nMikolov's original mapping from 34% to 43%, when translating a test set\ncomposed of both common and rare English words into Italian. Orthogonal\ntransformations are more robust to noise, enabling us to learn the\ntransformation without expert bilingual signal by constructing a\n\"pseudo-dictionary\" from the identical character strings which appear in both\nlanguages, achieving 40% precision on the same test set. Finally, we extend our\nmethod to retrieve the true translations of English sentences from a corpus of\n200k Italian sentences with a precision @1 of 68%.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 16:31:06 GMT"}], "update_date": "2017-02-14", "authors_parsed": [["Smith", "Samuel L.", ""], ["Turban", "David H. P.", ""], ["Hamblin", "Steven", ""], ["Hammerla", "Nils Y.", ""]]}, {"id": "1702.03920", "submitter": "Saurabh Gupta", "authors": "Saurabh Gupta, Varun Tolani, James Davidson, Sergey Levine, Rahul\n  Sukthankar, Jitendra Malik", "title": "Cognitive Mapping and Planning for Visual Navigation", "comments": "Extended IJCV Version of the original paper at CVPR17. Project\n  website with code, models, simulation environment and videos:\n  https://sites.google.com/view/cognitive-mapping-and-planning/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a neural architecture for navigation in novel environments. Our\nproposed architecture learns to map from first-person views and plans a\nsequence of actions towards goals in the environment. The Cognitive Mapper and\nPlanner (CMP) is based on two key ideas: a) a unified joint architecture for\nmapping and planning, such that the mapping is driven by the needs of the task,\nand b) a spatial memory with the ability to plan given an incomplete set of\nobservations about the world. CMP constructs a top-down belief map of the world\nand applies a differentiable neural net planner to produce the next action at\neach time step. The accumulated belief of the world enables the agent to track\nvisited regions of the environment. We train and test CMP on navigation\nproblems in simulation environments derived from scans of real world buildings.\nOur experiments demonstrate that CMP outperforms alternate learning-based\narchitectures, as well as, classical mapping and path planning approaches in\nmany cases. Furthermore, it naturally extends to semantically specified goals,\nsuch as 'going to a chair'. We also deploy CMP on physical robots in indoor\nenvironments, where it achieves reasonable performance, even though it is\ntrained entirely in simulation.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 18:52:04 GMT"}, {"version": "v2", "created": "Sun, 23 Apr 2017 01:59:30 GMT"}, {"version": "v3", "created": "Thu, 7 Feb 2019 18:54:58 GMT"}], "update_date": "2019-02-08", "authors_parsed": [["Gupta", "Saurabh", ""], ["Tolani", "Varun", ""], ["Davidson", "James", ""], ["Levine", "Sergey", ""], ["Sukthankar", "Rahul", ""], ["Malik", "Jitendra", ""]]}, {"id": "1702.03935", "submitter": "Michael Warren", "authors": "Michael S. Warren, Samuel W. Skillman, Rick Chartrand, Tim Kelton,\n  Ryan Keisler, David Raleigh, Matthew Turk", "title": "Data-Intensive Supercomputing in the Cloud: Global Analytics for\n  Satellite Imagery", "comments": "8 pages, 9 figures. Copyright 2016 IEEE. DataCloud 2016: The Seventh\n  International Workshop on Data-Intensive Computing in the Clouds. In\n  conjunction with SC16. Salt Lake City, Utah", "journal-ref": "Proceedings of the 7th International Workshop on Data-Intensive\n  Computing in the Cloud (DataCloud '16). IEEE Press, Piscataway, NJ, USA,\n  24-31, 2016", "doi": "10.1109/DataCloud.2016.7", "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present our experiences using cloud computing to support data-intensive\nanalytics on satellite imagery for commercial applications. Drawing from our\nbackground in high-performance computing, we draw parallels between the early\ndays of clustered computing systems and the current state of cloud computing\nand its potential to disrupt the HPC market. Using our own virtual file system\nlayer on top of cloud remote object storage, we demonstrate aggregate read\nbandwidth of 230 gigabytes per second using 512 Google Compute Engine (GCE)\nnodes accessing a USA multi-region standard storage bucket. This figure is\ncomparable to the best HPC storage systems in existence. We also present\nseveral of our application results, including the identification of field\nboundaries in Ukraine, and the generation of a global cloud-free base layer\nfrom Landsat imagery.\n", "versions": [{"version": "v1", "created": "Mon, 13 Feb 2017 19:00:04 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Warren", "Michael S.", ""], ["Skillman", "Samuel W.", ""], ["Chartrand", "Rick", ""], ["Kelton", "Tim", ""], ["Keisler", "Ryan", ""], ["Raleigh", "David", ""], ["Turk", "Matthew", ""]]}, {"id": "1702.04047", "submitter": "Marcello Balduccini", "authors": "Marcello Balduccini, Yuliya Lierler", "title": "Constraint Answer Set Solver EZCSP and Why Integration Schemas Matter", "comments": "Under consideration in Theory and Practice of Logic Programming\n  (TPLP)", "journal-ref": "TPLP 17(4) 462-515 (2017)", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers in answer set programming and constraint programming have spent\nsignificant efforts in the development of hybrid languages and solving\nalgorithms combining the strengths of these traditionally separate fields.\nThese efforts resulted in a new research area: constraint answer set\nprogramming. Constraint answer set programming languages and systems proved to\nbe successful at providing declarative, yet efficient solutions to problems\ninvolving hybrid reasoning tasks. One of the main contributions of this paper\nis the first comprehensive account of the constraint answer set language and\nsolver EZCSP, a mainstream representative of this research area that has been\nused in various successful applications. We also develop an extension of the\ntransition systems proposed by Nieuwenhuis et al. in 2006 to capture Boolean\nsatisfiability solvers. We use this extension to describe the EZCSP algorithm\nand prove formal claims about it. The design and algorithmic details behind\nEZCSP clearly demonstrate that the development of the hybrid systems of this\nkind is challenging. Many questions arise when one faces various design choices\nin an attempt to maximize system's benefits. One of the key decisions that a\ndeveloper of a hybrid solver makes is settling on a particular integration\nschema within its implementation. Thus, another important contribution of this\npaper is a thorough case study based on EZCSP, focused on the various\nintegration schemas that it provides.\n  Under consideration in Theory and Practice of Logic Programming (TPLP).\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 02:29:29 GMT"}, {"version": "v2", "created": "Mon, 15 May 2017 22:43:03 GMT"}, {"version": "v3", "created": "Fri, 1 Dec 2017 00:25:53 GMT"}], "update_date": "2017-12-04", "authors_parsed": [["Balduccini", "Marcello", ""], ["Lierler", "Yuliya", ""]]}, {"id": "1702.04267", "submitter": "Jan Hendrik Metzen", "authors": "Jan Hendrik Metzen, Tim Genewein, Volker Fischer, Bastian Bischoff", "title": "On Detecting Adversarial Perturbations", "comments": "Final version for ICLR2017 (see\n  https://openreview.net/forum?id=SJzCSf9xg&noteId=SJzCSf9xg)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning and deep learning in particular has advanced tremendously on\nperceptual tasks in recent years. However, it remains vulnerable against\nadversarial perturbations of the input that have been crafted specifically to\nfool the system while being quasi-imperceptible to a human. In this work, we\npropose to augment deep neural networks with a small \"detector\" subnetwork\nwhich is trained on the binary classification task of distinguishing genuine\ndata from data containing adversarial perturbations. Our method is orthogonal\nto prior work on addressing adversarial perturbations, which has mostly focused\non making the classification network itself more robust. We show empirically\nthat adversarial perturbations can be detected surprisingly well even though\nthey are quasi-imperceptible to humans. Moreover, while the detectors have been\ntrained to detect only a specific adversary, they generalize to similar and\nweaker adversaries. In addition, we propose an adversarial attack that fools\nboth the classifier and the detector and a novel training procedure for the\ndetector that counteracts this attack.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 15:44:26 GMT"}, {"version": "v2", "created": "Tue, 21 Feb 2017 06:53:38 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Metzen", "Jan Hendrik", ""], ["Genewein", "Tim", ""], ["Fischer", "Volker", ""], ["Bischoff", "Bastian", ""]]}, {"id": "1702.04280", "submitter": "Afshin Dehghan", "authors": "Afshin Dehghan and Enrique G. Ortiz and Guang Shu and Syed Zain Masood", "title": "DAGER: Deep Age, Gender and Emotion Recognition Using Convolutional\n  Neural Network", "comments": "10 Pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the details of Sighthound's fully automated age, gender\nand emotion recognition system. The backbone of our system consists of several\ndeep convolutional neural networks that are not only computationally\ninexpensive, but also provide state-of-the-art results on several competitive\nbenchmarks. To power our novel deep networks, we collected large labeled\ndatasets through a semi-supervised pipeline to reduce the annotation\neffort/time. We tested our system on several public benchmarks and report\noutstanding results. Our age, gender and emotion recognition models are\navailable to developers through the Sighthound Cloud API at\nhttps://www.sighthound.com/products/cloud\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 16:34:05 GMT"}, {"version": "v2", "created": "Sat, 4 Mar 2017 01:43:04 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Dehghan", "Afshin", ""], ["Ortiz", "Enrique G.", ""], ["Shu", "Guang", ""], ["Masood", "Syed Zain", ""]]}, {"id": "1702.04282", "submitter": "Yan Karklin", "authors": "Chaitanya Ekanadham, Yan Karklin", "title": "T-SKIRT: Online Estimation of Student Proficiency in an Adaptive\n  Learning System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We develop T-SKIRT: a temporal, structured-knowledge, IRT-based method for\npredicting student responses online. By explicitly accounting for student\nlearning and employing a structured, multidimensional representation of student\nproficiencies, the model outperforms standard IRT-based methods on an online\nresponse prediction task when applied to real responses collected from students\ninteracting with diverse pools of educational content.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 16:42:49 GMT"}], "update_date": "2017-02-15", "authors_parsed": [["Ekanadham", "Chaitanya", ""], ["Karklin", "Yan", ""]]}, {"id": "1702.04389", "submitter": "Norbert B\\'atfai Ph.D.", "authors": "Norbert B\\'atfai and Ren\\'at\\'o Besenczi and Gerg\\H{o} Bogacsovics and\n  Fanny Monori", "title": "Entropy Non-increasing Games for the Improvement of Dataflow Programming", "comments": "15 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we introduce a new conception of a family of esport games\ncalled Samu Entropy to try to improve dataflow program graphs like the ones\nthat are based on Google's TensorFlow. Currently, the Samu Entropy project\nspecifies only requirements for new esport games to be developed with\nparticular attention to the investigation of the relationship between esport\nand artificial intelligence. It is quite obvious that there is a very close and\nnatural relationship between esport games and artificial intelligence.\nFurthermore, the project Samu Entropy focuses not only on using artificial\nintelligence, but on creating AI in a new way. We present a reference game\ncalled Face Battle that implements the Samu Entropy requirements.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 21:18:17 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["B\u00e1tfai", "Norbert", ""], ["Besenczi", "Ren\u00e1t\u00f3", ""], ["Bogacsovics", "Gerg\u0151", ""], ["Monori", "Fanny", ""]]}, {"id": "1702.04423", "submitter": "Han Zhao", "authors": "Han Zhao, Otilia Stretcu, Alex Smola, Geoff Gordon", "title": "Efficient Multitask Feature and Relationship Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a multitask learning problem, in which several predictors are\nlearned jointly. Prior research has shown that learning the relations between\ntasks, and between the input features, together with the predictor, can lead to\nbetter generalization and interpretability, which proved to be useful for\napplications in many domains. In this paper, we consider a formulation of\nmultitask learning that learns the relationships both between tasks and between\nfeatures, represented through a task covariance and a feature covariance\nmatrix, respectively. First, we demonstrate that existing methods proposed for\nthis problem present an issue that may lead to ill-posed optimization. We then\npropose an alternative formulation, as well as an efficient algorithm to\noptimize it. Using ideas from optimization and graph theory, we propose an\nefficient coordinate-wise minimization algorithm that has a closed form\nsolution for each block subproblem. Our experiments show that the proposed\noptimization method is orders of magnitude faster than its competitors. We also\nprovide a nonlinear extension that is able to achieve better generalization\nthan existing methods.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 23:43:32 GMT"}, {"version": "v2", "created": "Sun, 16 Sep 2018 21:35:41 GMT"}, {"version": "v3", "created": "Wed, 10 Jul 2019 06:00:02 GMT"}], "update_date": "2019-07-11", "authors_parsed": [["Zhao", "Han", ""], ["Stretcu", "Otilia", ""], ["Smola", "Alex", ""], ["Gordon", "Geoff", ""]]}, {"id": "1702.04521", "submitter": "Tim Rockt\\\"aschel", "authors": "Micha{\\l} Daniluk, Tim Rockt\\\"aschel, Johannes Welbl, Sebastian Riedel", "title": "Frustratingly Short Attention Spans in Neural Language Modeling", "comments": "Published as a conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural language models predict the next token using a latent representation\nof the immediate token history. Recently, various methods for augmenting neural\nlanguage models with an attention mechanism over a differentiable memory have\nbeen proposed. For predicting the next token, these models query information\nfrom a memory of the recent history which can facilitate learning mid- and\nlong-range dependencies. However, conventional attention mechanisms used in\nmemory-augmented neural language models produce a single output vector per time\nstep. This vector is used both for predicting the next token as well as for the\nkey and value of a differentiable memory of a token history. In this paper, we\npropose a neural language model with a key-value attention mechanism that\noutputs separate representations for the key and value of a differentiable\nmemory, as well as for encoding the next-word distribution. This model\noutperforms existing memory-augmented neural language models on two corpora.\nYet, we found that our method mainly utilizes a memory of the five most recent\noutput representations. This led to the unexpected main finding that a much\nsimpler model based only on the concatenation of recent output representations\nfrom previous time steps is on par with more sophisticated memory-augmented\nneural language models.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 09:45:23 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Daniluk", "Micha\u0142", ""], ["Rockt\u00e4schel", "Tim", ""], ["Welbl", "Johannes", ""], ["Riedel", "Sebastian", ""]]}, {"id": "1702.04577", "submitter": "Mieczys{\\l}aw K{\\l}opotek", "authors": "Robert K{\\l}opotek and Mieczys{\\l}aw K{\\l}opotek", "title": "On the Discrepancy Between Kleinberg's Clustering Axioms and $k$-Means\n  Clustering Algorithm Behavior", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the validity of Kleinberg's axioms for clustering\nfunctions with respect to the quite popular clustering algorithm called\n$k$-means. While Kleinberg's axioms have been discussed heavily in the past, we\nconcentrate here on the case predominantly relevant for $k$-means algorithm,\nthat is behavior embedded in Euclidean space. We point at some contradictions\nand counter intuitiveness aspects of this axiomatic set within $\\mathbb{R}^m$\nthat were evidently not discussed so far. Our results suggest that apparently\nwithout defining clearly what kind of clusters we expect we will not be able to\nconstruct a valid axiomatic system. In particular we look at the shape and the\ngaps between the clusters. Finally we demonstrate that there exist several ways\nto reconcile the formulation of the axioms with their intended meaning and that\nunder this reformulation the axioms stop to be contradictory and the real-world\n$k$-means algorithm conforms to this axiomatic system.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 12:25:28 GMT"}, {"version": "v2", "created": "Mon, 24 Apr 2017 06:48:26 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["K\u0142opotek", "Robert", ""], ["K\u0142opotek", "Mieczys\u0142aw", ""]]}, {"id": "1702.04584", "submitter": "Lina Antonietta Coppola", "authors": "Lina Antonietta Coppola", "title": "Developing an ontology for the access to the contents of an archival\n  fonds: the case of the Catasto Gregoriano", "comments": "in Italian", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The research was proposed to exploit and extend the relational and contextual\nnature of the information assets of the Catasto Gregoriano, kept at the\nArchivio di Stato in Rome. Developed within the MODEUS project (Making Open\nData Effectively Usable), this study originates from the following key ideas of\nMODEUS: to require Open Data to be expressed in terms of an ontology, and to\ninclude such an ontology as a documentation of the data themselves. Thus, Open\nData are naturally linked by means of the ontology, which meets the\nrequirements of the Linked Open Data vision.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 12:58:39 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Coppola", "Lina Antonietta", ""]]}, {"id": "1702.04594", "submitter": "Minghao Yin", "authors": "Yiyuan Wang, Shaowei Cai, Minghao Yin", "title": "Local Search for Minimum Weight Dominating Set with Two-Level\n  Configuration Checking and Frequency Based Scoring Function", "comments": "29 pages, 1 figure", "journal-ref": "JAIR 58 (2017) 267-295", "doi": "10.1613/jair.5205", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Minimum Weight Dominating Set (MWDS) problem is an important\ngeneralization of the Minimum Dominating Set (MDS) problem with extensive\napplications. This paper proposes a new local search algorithm for the MWDS\nproblem, which is based on two new ideas. The first idea is a heuristic called\ntwo-level configuration checking (CC2), which is a new variant of a recent\npowerful configuration checking strategy (CC) for effectively avoiding the\nrecent search paths. The second idea is a novel scoring function based on the\nfrequency of being uncovered of vertices. Our algorithm is called CC2FS,\naccording to the names of the two ideas. The experimental results show that,\nCC2FS performs much better than some state-of-the-art algorithms in terms of\nsolution quality on a broad range of MWDS benchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 13:22:57 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Wang", "Yiyuan", ""], ["Cai", "Shaowei", ""], ["Yin", "Minghao", ""]]}, {"id": "1702.04595", "submitter": "Luisa Zintgraf", "authors": "Luisa M Zintgraf, Taco S Cohen, Tameem Adel, Max Welling", "title": "Visualizing Deep Neural Network Decisions: Prediction Difference\n  Analysis", "comments": "ICLR2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents the prediction difference analysis method for\nvisualizing the response of a deep neural network to a specific input. When\nclassifying images, the method highlights areas in a given input image that\nprovide evidence for or against a certain class. It overcomes several\nshortcoming of previous methods and provides great additional insight into the\ndecision making process of classifiers. Making neural network decisions\ninterpretable through visualization is important both to improve models and to\naccelerate the adoption of black-box classifiers in application areas such as\nmedicine. We illustrate the method in experiments on natural images (ImageNet\ndata), as well as medical images (MRI brain scans).\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 13:25:26 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Zintgraf", "Luisa M", ""], ["Cohen", "Taco S", ""], ["Adel", "Tameem", ""], ["Welling", "Max", ""]]}, {"id": "1702.04638", "submitter": "Mark Burgess", "authors": "Mark Burgess", "title": "A Spacetime Approach to Generalized Cognitive Reasoning in Multi-scale\n  Learning", "comments": "Typos corrected and new examples added to appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In modern machine learning, pattern recognition replaces realtime semantic\nreasoning. The mapping from input to output is learned with fixed semantics by\ntraining outcomes deliberately. This is an expensive and static approach which\ndepends heavily on the availability of a very particular kind of prior raining\ndata to make inferences in a single step. Conventional semantic network\napproaches, on the other hand, base multi-step reasoning on modal logics and\nhandcrafted ontologies, which are ad hoc, expensive to construct, and fragile\nto inconsistency. Both approaches may be enhanced by a hybrid approach, which\ncompletely separates reasoning from pattern recognition. In this report, a\nquasi-linguistic approach to knowledge representation is discussed, motivated\nby spacetime structure. Tokenized patterns from diverse sources are integrated\nto build a lightly constrained and approximately scale-free network. This is\nthen be parsed with very simple recursive algorithms to generate\n`brainstorming' sets of reasoned knowledge.\n", "versions": [{"version": "v1", "created": "Sun, 12 Feb 2017 14:58:45 GMT"}, {"version": "v2", "created": "Tue, 1 Aug 2017 11:32:42 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Burgess", "Mark", ""]]}, {"id": "1702.04767", "submitter": "Han Zhao", "authors": "Han Zhao, Geoff Gordon", "title": "Linear Time Computation of Moments in Sum-Product Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian online algorithms for Sum-Product Networks (SPNs) need to update\ntheir posterior distribution after seeing one single additional instance. To do\nso, they must compute moments of the model parameters under this distribution.\nThe best existing method for computing such moments scales quadratically in the\nsize of the SPN, although it scales linearly for trees. This unfortunate\nscaling makes Bayesian online algorithms prohibitively expensive, except for\nsmall or tree-structured SPNs. We propose an optimal linear-time algorithm that\nworks even when the SPN is a general directed acyclic graph (DAG), which\nsignificantly broadens the applicability of Bayesian online algorithms for\nSPNs. There are three key ingredients in the design and analysis of our\nalgorithm: 1). For each edge in the graph, we construct a linear time reduction\nfrom the moment computation problem to a joint inference problem in SPNs. 2).\nUsing the property that each SPN computes a multilinear polynomial, we give an\nefficient procedure for polynomial evaluation by differentiation without\nexpanding the network that may contain exponentially many monomials. 3). We\npropose a dynamic programming method to further reduce the computation of the\nmoments of all the edges in the graph from quadratic to linear. We demonstrate\nthe usefulness of our linear time algorithm by applying it to develop a linear\ntime assume density filter (ADF) for SPNs.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 20:40:12 GMT"}, {"version": "v2", "created": "Sun, 5 Nov 2017 04:31:23 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Zhao", "Han", ""], ["Gordon", "Geoff", ""]]}, {"id": "1702.04849", "submitter": "Christian Kroer", "authors": "Christian Kroer and Kevin Waugh and Fatma Kilinc-Karzan and Tuomas\n  Sandholm", "title": "Theoretical and Practical Advances on Smoothing for Extensive-Form Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sparse iterative methods, in particular first-order methods, are known to be\namong the most effective in solving large-scale two-player zero-sum\nextensive-form games. The convergence rates of these methods depend heavily on\nthe properties of the distance-generating function that they are based on. We\ninvestigate the acceleration of first-order methods for solving extensive-form\ngames through better design of the dilated entropy function---a class of\ndistance-generating functions related to the domains associated with the\nextensive-form games. By introducing a new weighting scheme for the dilated\nentropy function, we develop the first distance-generating function for the\nstrategy spaces of sequential games that has no dependence on the branching\nfactor of the player. This result improves the convergence rate of several\nfirst-order methods by a factor of $\\Omega(b^dd)$, where $b$ is the branching\nfactor of the player, and $d$ is the depth of the game tree.\n  Thus far, counterfactual regret minimization methods have been faster in\npractice, and more popular, than first-order methods despite their\ntheoretically inferior convergence rates. Using our new weighting scheme and\npractical tuning we show that, for the first time, the excessive gap technique\ncan be made faster than the fastest counterfactual regret minimization\nalgorithm, CFR+, in practice.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2017 03:39:07 GMT"}, {"version": "v2", "created": "Tue, 9 May 2017 02:24:32 GMT"}], "update_date": "2017-05-10", "authors_parsed": [["Kroer", "Christian", ""], ["Waugh", "Kevin", ""], ["Kilinc-Karzan", "Fatma", ""], ["Sandholm", "Tuomas", ""]]}, {"id": "1702.04956", "submitter": "Aaron Gerow", "authors": "Aaron Gerow, Mingyang Zhou, Stan Matwin, Feng Shi", "title": "Reflexive Regular Equivalence for Bipartite Data", "comments": "A condensed version of this paper will appear in Proceedings of the\n  30th Canadian Conference on Artificial Intelligence, Edmonton, Alberta,\n  Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bipartite data is common in data engineering and brings unique challenges,\nparticularly when it comes to clustering tasks that impose on strong structural\nassumptions. This work presents an unsupervised method for assessing similarity\nin bipartite data. Similar to some co-clustering methods, the method is based\non regular equivalence in graphs. The algorithm uses spectral properties of a\nbipartite adjacency matrix to estimate similarity in both dimensions. The\nmethod is reflexive in that similarity in one dimension is used to inform\nsimilarity in the other. Reflexive regular equivalence can also use the\nstructure of transitivities -- in a network sense -- the contribution of which\nis controlled by the algorithm's only free-parameter, $\\alpha$. The method is\ncompletely unsupervised and can be used to validate assumptions of\nco-similarity, which are required but often untested, in co-clustering\nanalyses. Three variants of the method with different normalizations are tested\non synthetic data. The method is found to be robust to noise and well-suited to\nasymmetric co-similar structure, making it particularly informative for cluster\nanalysis and recommendation in bipartite data of unknown structure. In\nexperiments, the convergence and speed of the algorithm are found to be stable\nfor different levels of noise. Real-world data from a network of malaria genes\nare analyzed, where the similarity produced by the reflexive method is shown to\nout-perform other measures' ability to correctly classify genes.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2017 13:29:30 GMT"}], "update_date": "2017-02-17", "authors_parsed": [["Gerow", "Aaron", ""], ["Zhou", "Mingyang", ""], ["Matwin", "Stan", ""], ["Shi", "Feng", ""]]}, {"id": "1702.05112", "submitter": "Evgeny Lipachev K", "authors": "Alexander Elizarov, Alexander Kirillovich, Evgeny Lipachev, and Olga\n  Nevzorova", "title": "OntoMath Digital Ecosystem: Ontologies, Mathematical Knowledge Analytics\n  and Management", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article we consider the basic ideas, approaches and results of\ndeveloping of mathematical knowledge management technologies based on\nontologies. These solutions form the basis of a specialized digital ecosystem\nOntoMath which consists of the ontology of the logical structure of\nmathematical documents Mocassin and ontology of mathematical knowledge\nOntoMathPRO, tools of text analysis, recommender system and other applications\nto manage mathematical knowledge. The studies are in according to the ideas of\ncreating a distributed system of interconnected repositories of digitized\nversions of mathematical documents and project to create a World Digital\nMathematical Library.\n", "versions": [{"version": "v1", "created": "Thu, 16 Feb 2017 19:14:00 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Elizarov", "Alexander", ""], ["Kirillovich", "Alexander", ""], ["Lipachev", "Evgeny", ""], ["Nevzorova", "Olga", ""]]}, {"id": "1702.05222", "submitter": "Morteza Noshad Iranzad", "authors": "Morteza Noshad, Kevin R. Moon, Salimeh Yasaei Sekeh, Alfred O. Hero\n  III", "title": "Direct Estimation of Information Divergence Using Nearest Neighbor\n  Ratios", "comments": "2017 IEEE International Symposium on Information Theory (ISIT)", "journal-ref": "In Information Theory (ISIT), 2017 IEEE International Symposium on\n  (pp. 903-907). IEEE", "doi": "10.1109/ISIT.2017.8006659", "report-no": null, "categories": "cs.IT cs.AI math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a direct estimation method for R\\'{e}nyi and f-divergence measures\nbased on a new graph theoretical interpretation. Suppose that we are given two\nsample sets $X$ and $Y$, respectively with $N$ and $M$ samples, where\n$\\eta:=M/N$ is a constant value. Considering the $k$-nearest neighbor ($k$-NN)\ngraph of $Y$ in the joint data set $(X,Y)$, we show that the average powered\nratio of the number of $X$ points to the number of $Y$ points among all $k$-NN\npoints is proportional to R\\'{e}nyi divergence of $X$ and $Y$ densities. A\nsimilar method can also be used to estimate f-divergence measures. We derive\nbias and variance rates, and show that for the class of $\\gamma$-H\\\"{o}lder\nsmooth functions, the estimator achieves the MSE rate of\n$O(N^{-2\\gamma/(\\gamma+d)})$. Furthermore, by using a weighted ensemble\nestimation technique, for density functions with continuous and bounded\nderivatives of up to the order $d$, and some extra conditions at the support\nset boundary, we derive an ensemble estimator that achieves the parametric MSE\nrate of $O(1/N)$. Our estimators are more computationally tractable than other\ncompeting estimators, which makes them appealing in many practical\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 04:46:24 GMT"}, {"version": "v2", "created": "Mon, 20 Nov 2017 23:37:20 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Noshad", "Morteza", ""], ["Moon", "Kevin R.", ""], ["Sekeh", "Salimeh Yasaei", ""], ["Hero", "Alfred O.", "III"]]}, {"id": "1702.05270", "submitter": "Sandro Pezzelle", "authors": "Sandro Pezzelle, Marco Marelli, Raffaella Bernardi", "title": "Be Precise or Fuzzy: Learning the Meaning of Cardinals and Quantifiers\n  from Vision", "comments": "Accepted at EACL2017. 7 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  People can refer to quantities in a visual scene by using either exact\ncardinals (e.g. one, two, three) or natural language quantifiers (e.g. few,\nmost, all). In humans, these two processes underlie fairly different cognitive\nand neural mechanisms. Inspired by this evidence, the present study proposes\ntwo models for learning the objective meaning of cardinals and quantifiers from\nvisual scenes containing multiple objects. We show that a model capitalizing on\na 'fuzzy' measure of similarity is effective for learning quantifiers, whereas\nthe learning of exact cardinals is better accomplished when information about\nnumber is provided.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 09:26:10 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Pezzelle", "Sandro", ""], ["Marelli", "Marco", ""], ["Bernardi", "Raffaella", ""]]}, {"id": "1702.05376", "submitter": "Dmitry Ignatov", "authors": "Dmitry I. Ignatov and Bruce W. Watson", "title": "Towards a Unified Taxonomy of Biclustering Methods", "comments": "http://ceur-ws.org/Vol-1552/", "journal-ref": "Russian and South African Workshop on Knowledge Discovery\n  Techniques Based on Formal Concept Analysis (RuZA 2015), November 30 -\n  December 5, 2015, Stellenbosch, South Africa, In CEUR Workshop Proceedings,\n  Vol. 1552, p. 23-39", "doi": null, "report-no": null, "categories": "cs.AI cs.DM stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Being an unsupervised machine learning and data mining technique,\nbiclustering and its multimodal extensions are becoming popular tools for\nanalysing object-attribute data in different domains. Apart from conventional\nclustering techniques, biclustering is searching for homogeneous groups of\nobjects while keeping their common description, e.g., in binary setting, their\nshared attributes. In bioinformatics, biclustering is used to find genes, which\nare active in a subset of situations, thus being candidates for biomarkers.\nHowever, the authors of those biclustering techniques that are popular in gene\nexpression analysis, may overlook the existing methods. For instance, BiMax\nalgorithm is aimed at finding biclusters, which are well-known for decades as\nformal concepts. Moreover, even if bioinformatics classify the biclustering\nmethods according to reasonable domain-driven criteria, their classification\ntaxonomies may be different from survey to survey and not full as well. So, in\nthis paper we propose to use concept lattices as a tool for taxonomy building\n(in the biclustering domain) and attribute exploration as means for\ncross-domain taxonomy completion.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 15:12:31 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Ignatov", "Dmitry I.", ""], ["Watson", "Bruce W.", ""]]}, {"id": "1702.05383", "submitter": "Kumar Sankar Ray", "authors": "Kumar S. Ray and Mandrita Mondal", "title": "Theorem Proving Based on Semantics of DNA Strand Graph", "comments": "25 pages,12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Because of several technological limitations of traditional silicon based\ncomputing, for past few years a paradigm shift, from silicon to carbon, is\noccurring in computational world. DNA computing has been considered to be quite\npromising in solving computational and reasoning problems by using DNA strands.\nResolution, an important aspect of automated theorem proving and mathematical\nlogic, is a rule of inference which leads to proof by contradiction technique\nfor sentences in propositional logic and first-order logic. This can also be\ncalled refutation theorem-proving. In this paper we have shown how the theorem\nproving with resolution refutation by DNA computation can be represented by the\nsemantics of process calculus and strand graph.\n", "versions": [{"version": "v1", "created": "Wed, 15 Feb 2017 11:12:34 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Ray", "Kumar S.", ""], ["Mondal", "Mandrita", ""]]}, {"id": "1702.05437", "submitter": "Aws Albarghouthi", "authors": "Aws Albarghouthi and Loris D'Antoni and Samuel Drews and Aditya Nori", "title": "Quantifying Program Bias", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the range and sensitivity of algorithmic decisions expanding at a\nbreak-neck speed, it is imperative that we aggressively investigate whether\nprograms are biased. We propose a novel probabilistic program analysis\ntechnique and apply it to quantifying bias in decision-making programs.\nSpecifically, we (i) present a sound and complete automated verification\ntechnique for proving quantitative properties of probabilistic programs; (ii)\nshow that certain notions of bias, recently proposed in the fairness\nliterature, can be phrased as quantitative correctness properties; and (iii)\npresent FairSquare, the first verification tool for quantifying program bias,\nand evaluate it on a range of decision-making programs.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 17:02:29 GMT"}, {"version": "v2", "created": "Tue, 7 Mar 2017 03:21:07 GMT"}], "update_date": "2017-03-08", "authors_parsed": [["Albarghouthi", "Aws", ""], ["D'Antoni", "Loris", ""], ["Drews", "Samuel", ""], ["Nori", "Aditya", ""]]}, {"id": "1702.05472", "submitter": "Mickael Randour", "authors": "Rapha\\\"el Berthon and Mickael Randour and Jean-Fran\\c{c}ois Raskin", "title": "Threshold Constraints with Guarantees for Parity Objectives in Markov\n  Decision Processes", "comments": "Full version of ICALP 2017 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.FL cs.GT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The beyond worst-case synthesis problem was introduced recently by Bruy\\`ere\net al. [BFRR14]: it aims at building system controllers that provide strict\nworst-case performance guarantees against an antagonistic environment while\nensuring higher expected performance against a stochastic model of the\nenvironment. Our work extends the framework of [BFRR14] and follow-up papers,\nwhich focused on quantitative objectives, by addressing the case of\n$\\omega$-regular conditions encoded as parity objectives, a natural way to\nrepresent functional requirements of systems.\n  We build strategies that satisfy a main parity objective on all plays, while\nensuring a secondary one with sufficient probability. This setting raises new\nchallenges in comparison to quantitative objectives, as one cannot easily mix\ndifferent strategies without endangering the functional properties of the\nsystem. We establish that, for all variants of this problem, deciding the\nexistence of a strategy lies in ${\\sf NP} \\cap {\\sf coNP}$, the same complexity\nclass as classical parity games. Hence, our framework provides additional\nmodeling power while staying in the same complexity class.\n  [BFRR14] V\\'eronique Bruy\\`ere, Emmanuel Filiot, Mickael Randour, and\nJean-Fran\\c{c}ois Raskin. Meet your expectations with guarantees: Beyond\nworst-case synthesis in quantitative games. In Ernst W. Mayr and Natacha\nPortier, editors, 31st International Symposium on Theoretical Aspects of\nComputer Science, STACS 2014, March 5-8, 2014, Lyon, France, volume 25 of\nLIPIcs, pages 199-213. Schloss Dagstuhl - Leibniz - Zentrum fuer Informatik,\n2014.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 18:52:11 GMT"}, {"version": "v2", "created": "Thu, 27 Apr 2017 09:53:04 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Berthon", "Rapha\u00ebl", ""], ["Randour", "Mickael", ""], ["Raskin", "Jean-Fran\u00e7ois", ""]]}, {"id": "1702.05515", "submitter": "Hang Ma", "authors": "Hang Ma, Sven Koenig, Nora Ayanian, Liron Cohen, Wolfgang Hoenig, T.\n  K. Satish Kumar, Tansel Uras, Hong Xu, Craig Tovey, Guni Sharon", "title": "Overview: Generalizations of Multi-Agent Path Finding to Real-World\n  Scenarios", "comments": "In IJCAI-16 Workshop on Multi-Agent Path Finding", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-agent path finding (MAPF) is well-studied in artificial intelligence,\nrobotics, theoretical computer science and operations research. We discuss\nissues that arise when generalizing MAPF methods to real-world scenarios and\nfour research directions that address them. We emphasize the importance of\naddressing these issues as opposed to developing faster methods for the\nstandard formulation of the MAPF problem.\n", "versions": [{"version": "v1", "created": "Fri, 17 Feb 2017 20:39:38 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Ma", "Hang", ""], ["Koenig", "Sven", ""], ["Ayanian", "Nora", ""], ["Cohen", "Liron", ""], ["Hoenig", "Wolfgang", ""], ["Kumar", "T. K. Satish", ""], ["Uras", "Tansel", ""], ["Xu", "Hong", ""], ["Tovey", "Craig", ""], ["Sharon", "Guni", ""]]}, {"id": "1702.05677", "submitter": "Lunjia Hu", "authors": "Lunjia Hu, Ruihan Wu, Tianhong Li, Liwei Wang", "title": "Quadratic Upper Bound for Recursive Teaching Dimension of Finite VC\n  Classes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we study the quantitative relation between the recursive\nteaching dimension (RTD) and the VC dimension (VCD) of concept classes of\nfinite sizes. The RTD of a concept class $\\mathcal C \\subseteq \\{0, 1\\}^n$,\nintroduced by Zilles et al. (2011), is a combinatorial complexity measure\ncharacterized by the worst-case number of examples necessary to identify a\nconcept in $\\mathcal C$ according to the recursive teaching model.\n  For any finite concept class $\\mathcal C \\subseteq \\{0,1\\}^n$ with\n$\\mathrm{VCD}(\\mathcal C)=d$, Simon & Zilles (2015) posed an open problem\n$\\mathrm{RTD}(\\mathcal C) = O(d)$, i.e., is RTD linearly upper bounded by VCD?\nPreviously, the best known result is an exponential upper bound\n$\\mathrm{RTD}(\\mathcal C) = O(d \\cdot 2^d)$, due to Chen et al. (2016). In this\npaper, we show a quadratic upper bound: $\\mathrm{RTD}(\\mathcal C) = O(d^2)$,\nmuch closer to an answer to the open problem. We also discuss the challenges in\nfully solving the problem.\n", "versions": [{"version": "v1", "created": "Sat, 18 Feb 2017 23:46:10 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Hu", "Lunjia", ""], ["Wu", "Ruihan", ""], ["Li", "Tianhong", ""], ["Wang", "Liwei", ""]]}, {"id": "1702.05710", "submitter": "Pallavi Jain", "authors": "Pallavi Jain, Gur Saran, Kamal Srivastava", "title": "Polynomial Time Efficient Construction Heuristics for Vertex Separation\n  Minimization Problem", "comments": "The paper will appear in the proceedings of International Conference\n  on Current Trends in Graph Theory and Computation which will be published in\n  Electronic Notes on Discrete Mathematics (ENDM)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Vertex Separation Minimization Problem (VSMP) consists of finding a layout of\na graph G = (V,E) which minimizes the maximum vertex cut or separation of a\nlayout. It is an NP-complete problem in general for which metaheuristic\ntechniques can be applied to find near optimal solution. VSMP has applications\nin VLSI design, graph drawing and computer language compiler design. VSMP is\npolynomially solvable for grids, trees, permutation graphs and cographs.\nConstruction heuristics play a very important role in the metaheuristic\ntechniques as they are responsible for generating initial solutions which lead\nto fast convergence. In this paper, we have proposed three construction\nheuristics H1, H2 and H3 and performed experiments on Grids, Small graphs,\nTrees and Harwell Boeing graphs, totaling 248 instances of graphs. Experiments\nreveal that H1, H2 and H3 are able to achieve best results for 88.71%, 43.5%\nand 37.1% of the total instances respectively while the best construction\nheuristic in the literature achieves the best solution for 39.9% of the total\ninstances. We have also compared the results with the state-of-the-art\nmetaheuristic GVNS and observed that the proposed construction heuristics\nimproves the results for some of the input instances. It was found that GVNS\nobtained best results for 82.9% instances of all input instances and the\nheuristic H1 obtained best results for 82.3% of all input instances.\n", "versions": [{"version": "v1", "created": "Sun, 19 Feb 2017 07:19:52 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Jain", "Pallavi", ""], ["Saran", "Gur", ""], ["Srivastava", "Kamal", ""]]}, {"id": "1702.05778", "submitter": "Subhash Kak", "authors": "Subhash Kak", "title": "The Absent-Minded Driver Problem Redux", "comments": "9 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reconsiders the problem of the absent-minded driver who must\nchoose between alternatives with different payoff with imperfect recall and\nvarying degrees of knowledge of the system. The classical absent-minded driver\nproblem represents the case with limited information and it has bearing on the\ngeneral area of communication and learning, social choice, mechanism design,\nauctions, theories of knowledge, belief, and rational agency. Within the\nframework of extensive games, this problem has applications to many artificial\nintelligence scenarios. It is obvious that the performance of the agent\nimproves as information available increases. It is shown that a non-uniform\nassignment strategy for successive choices does better than a fixed probability\nstrategy. We consider both classical and quantum approaches to the problem. We\nargue that the superior performance of quantum decisions with access to\nentanglement cannot be fairly compared to a classical algorithm. If the\ncognitive systems of agents are taken to have access to quantum resources, or\nhave a quantum mechanical basis, then that can be leveraged into superior\nperformance.\n", "versions": [{"version": "v1", "created": "Sun, 19 Feb 2017 18:23:47 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Kak", "Subhash", ""]]}, {"id": "1702.05800", "submitter": "Xinghao Pan", "authors": "Xinghao Pan, Jianmin Chen, Rajat Monga, Samy Bengio, Rafal Jozefowicz", "title": "Revisiting Distributed Synchronous SGD", "comments": "This article will be superseded by arXiv:1604.00981", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed training of deep learning models on large-scale training data is\ntypically conducted with asynchronous stochastic optimization to maximize the\nrate of updates, at the cost of additional noise introduced from asynchrony. In\ncontrast, the synchronous approach is often thought to be impractical due to\nidle time wasted on waiting for straggling workers. We revisit these\nconventional beliefs in this paper, and examine the weaknesses of both\napproaches. We demonstrate that a third approach, synchronous optimization with\nbackup workers, can avoid asynchronous noise while mitigating for the worst\nstragglers. Our approach is empirically validated and shown to converge faster\nand to better test accuracies.\n", "versions": [{"version": "v1", "created": "Sun, 19 Feb 2017 21:51:48 GMT"}, {"version": "v2", "created": "Sat, 18 Mar 2017 23:02:17 GMT"}], "update_date": "2017-03-21", "authors_parsed": [["Pan", "Xinghao", ""], ["Chen", "Jianmin", ""], ["Monga", "Rajat", ""], ["Bengio", "Samy", ""], ["Jozefowicz", "Rafal", ""]]}, {"id": "1702.05865", "submitter": "Xinghao Pan", "authors": "Xinghao Pan, Shivaram Venkataraman, Zizheng Tai, Joseph Gonzalez", "title": "Hemingway: Modeling Distributed Optimization Algorithms", "comments": "Presented at ML Systems Workshop at NIPS, Dec 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Distributed optimization algorithms are widely used in many industrial\nmachine learning applications. However choosing the appropriate algorithm and\ncluster size is often difficult for users as the performance and convergence\nrate of optimization algorithms vary with the size of the cluster. In this\npaper we make the case for an ML-optimizer that can select the appropriate\nalgorithm and cluster size to use for a given problem. To do this we propose\nbuilding two models: one that captures the system level characteristics of how\ncomputation, communication change as we increase cluster sizes and another that\ncaptures how convergence rates change with cluster sizes. We present\npreliminary results from our prototype implementation called Hemingway and\ndiscuss some of the challenges involved in developing such a system.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 05:51:18 GMT"}], "update_date": "2017-02-21", "authors_parsed": [["Pan", "Xinghao", ""], ["Venkataraman", "Shivaram", ""], ["Tai", "Zizheng", ""], ["Gonzalez", "Joseph", ""]]}, {"id": "1702.05870", "submitter": "Luo Chunjie", "authors": "Chunjie Luo, Jianfeng Zhan, Lei Wang, Qiang Yang", "title": "Cosine Normalization: Using Cosine Similarity Instead of Dot Product in\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, multi-layer neural networks use dot product between the output\nvector of previous layer and the incoming weight vector as the input to\nactivation function. The result of dot product is unbounded, thus increases the\nrisk of large variance. Large variance of neuron makes the model sensitive to\nthe change of input distribution, thus results in poor generalization, and\naggravates the internal covariate shift which slows down the training. To bound\ndot product and decrease the variance, we propose to use cosine similarity or\ncentered cosine similarity (Pearson Correlation Coefficient) instead of dot\nproduct in neural networks, which we call cosine normalization. We compare\ncosine normalization with batch, weight and layer normalization in\nfully-connected neural networks as well as convolutional networks on the data\nsets of MNIST, 20NEWS GROUP, CIFAR-10/100 and SVHN. Experiments show that\ncosine normalization achieves better performance than other normalization\ntechniques.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 06:17:02 GMT"}, {"version": "v2", "created": "Wed, 22 Feb 2017 06:33:34 GMT"}, {"version": "v3", "created": "Mon, 12 Jun 2017 06:05:03 GMT"}, {"version": "v4", "created": "Tue, 13 Jun 2017 07:22:58 GMT"}, {"version": "v5", "created": "Mon, 23 Oct 2017 03:31:59 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Luo", "Chunjie", ""], ["Zhan", "Jianfeng", ""], ["Wang", "Lei", ""], ["Yang", "Qiang", ""]]}, {"id": "1702.05970", "submitter": "Patrick Christ", "authors": "Patrick Ferdinand Christ, Florian Ettlinger, Felix Gr\\\"un, Mohamed\n  Ezzeldin A. Elshaera, Jana Lipkova, Sebastian Schlecht, Freba Ahmaddy, Sunil\n  Tatavarty, Marc Bickel, Patrick Bilic, Markus Rempfler, Felix Hofmann, Melvin\n  D Anastasi, Seyed-Ahmad Ahmadi, Georgios Kaissis, Julian Holch, Wieland\n  Sommer, Rickmer Braren, Volker Heinemann, Bjoern Menze", "title": "Automatic Liver and Tumor Segmentation of CT and MRI Volumes using\n  Cascaded Fully Convolutional Neural Networks", "comments": "Under Review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic segmentation of the liver and hepatic lesions is an important step\ntowards deriving quantitative biomarkers for accurate clinical diagnosis and\ncomputer-aided decision support systems. This paper presents a method to\nautomatically segment liver and lesions in CT and MRI abdomen images using\ncascaded fully convolutional neural networks (CFCNs) enabling the segmentation\nof a large-scale medical trial or quantitative image analysis. We train and\ncascade two FCNs for a combined segmentation of the liver and its lesions. In\nthe first step, we train a FCN to segment the liver as ROI input for a second\nFCN. The second FCN solely segments lesions within the predicted liver ROIs of\nstep 1. CFCN models were trained on an abdominal CT dataset comprising 100\nhepatic tumor volumes. Validations on further datasets show that CFCN-based\nsemantic liver and lesion segmentation achieves Dice scores over 94% for liver\nwith computation times below 100s per volume. We further experimentally\ndemonstrate the robustness of the proposed method on an 38 MRI liver tumor\nvolumes and the public 3DIRCAD dataset.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 13:52:57 GMT"}, {"version": "v2", "created": "Thu, 23 Feb 2017 15:02:59 GMT"}], "update_date": "2017-02-24", "authors_parsed": [["Christ", "Patrick Ferdinand", ""], ["Ettlinger", "Florian", ""], ["Gr\u00fcn", "Felix", ""], ["Elshaera", "Mohamed Ezzeldin A.", ""], ["Lipkova", "Jana", ""], ["Schlecht", "Sebastian", ""], ["Ahmaddy", "Freba", ""], ["Tatavarty", "Sunil", ""], ["Bickel", "Marc", ""], ["Bilic", "Patrick", ""], ["Rempfler", "Markus", ""], ["Hofmann", "Felix", ""], ["Anastasi", "Melvin D", ""], ["Ahmadi", "Seyed-Ahmad", ""], ["Kaissis", "Georgios", ""], ["Holch", "Julian", ""], ["Sommer", "Wieland", ""], ["Braren", "Rickmer", ""], ["Heinemann", "Volker", ""], ["Menze", "Bjoern", ""]]}, {"id": "1702.06000", "submitter": "Theophanes Raptis Mr", "authors": "T. E. Raptis", "title": "'Viral' Turing Machines, Computation from Noise and Combinatorial\n  Hierarchies", "comments": "24 p., 4 figures", "journal-ref": null, "doi": "10.1016/j.chaos.2017.09.033", "report-no": null, "categories": "cs.AI nlin.CG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The interactive computation paradigm is reviewed and a particular example is\nextended to form the stochastic analog of a computational process via a\ntranscription of a minimal Turing Machine into an equivalent asynchronous\nCellular Automaton with an exponential waiting times distribution of effective\ntransitions. Furthermore, a special toolbox for analytic derivation of\nrecursive relations of important statistical and other quantities is introduced\nin the form of an Inductive Combinatorial Hierarchy.\n", "versions": [{"version": "v1", "created": "Tue, 31 Jan 2017 11:02:37 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Raptis", "T. E.", ""]]}, {"id": "1702.06054", "submitter": "Aravind Srinivas", "authors": "Sahil Sharma, Aravind Srinivas, Balaraman Ravindran", "title": "Learning to Repeat: Fine Grained Action Repetition for Deep\n  Reinforcement Learning", "comments": "24 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement Learning algorithms can learn complex behavioral patterns for\nsequential decision making tasks wherein an agent interacts with an environment\nand acquires feedback in the form of rewards sampled from it. Traditionally,\nsuch algorithms make decisions, i.e., select actions to execute, at every\nsingle time step of the agent-environment interactions. In this paper, we\npropose a novel framework, Fine Grained Action Repetition (FiGAR), which\nenables the agent to decide the action as well as the time scale of repeating\nit. FiGAR can be used for improving any Deep Reinforcement Learning algorithm\nwhich maintains an explicit policy estimate by enabling temporal abstractions\nin the action space. We empirically demonstrate the efficacy of our framework\nby showing performance improvements on top of three policy search algorithms in\ndifferent domains: Asynchronous Advantage Actor Critic in the Atari 2600\ndomain, Trust Region Policy Optimization in Mujoco domain and Deep\nDeterministic Policy Gradients in the TORCS car racing domain.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 16:32:07 GMT"}, {"version": "v2", "created": "Mon, 21 Sep 2020 22:22:25 GMT"}], "update_date": "2020-09-23", "authors_parsed": [["Sharma", "Sahil", ""], ["Srinivas", "Aravind", ""], ["Ravindran", "Balaraman", ""]]}, {"id": "1702.06186", "submitter": "Amit Sahu", "authors": "Amit Sahu", "title": "Survey of reasoning using Neural networks", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reason and inference require process as well as memory skills by humans.\nNeural networks are able to process tasks like image recognition (better than\nhumans) but in memory aspects are still limited (by attention mechanism, size).\nRecurrent Neural Network (RNN) and it's modified version LSTM are able to solve\nsmall memory contexts, but as context becomes larger than a threshold, it is\ndifficult to use them. The Solution is to use large external memory. Still, it\nposes many challenges like, how to train neural networks for discrete memory\nrepresentation, how to describe long term dependencies in sequential data etc.\nMost prominent neural architectures for such tasks are Memory networks:\ninference components combined with long term memory and Neural Turing Machines:\nneural networks using external memory resources. Also, additional techniques\nlike attention mechanism, end to end gradient descent on discrete memory\nrepresentation are needed to support these solutions. Preliminary results of\nabove neural architectures on simple algorithms (sorting, copying) and Question\nAnswering (based on story, dialogs) application are comparable with the state\nof the art. In this paper, I explain these architectures (in general), the\nadditional techniques used and the results of their application.\n", "versions": [{"version": "v1", "created": "Tue, 14 Feb 2017 17:24:04 GMT"}, {"version": "v2", "created": "Thu, 2 Mar 2017 09:36:24 GMT"}], "update_date": "2017-03-03", "authors_parsed": [["Sahu", "Amit", ""]]}, {"id": "1702.06199", "submitter": "Quan Nguyen", "authors": "Quan Nguyen", "title": "The Dialog State Tracking Challenge with Bayesian Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative model has been one of the most common approaches for solving the\nDialog State Tracking Problem with the capabilities to model the dialog\nhypotheses in an explicit manner. The most important task in such Bayesian\nnetworks models is constructing the most reliable user models by learning and\nreflecting the training data into the probability distribution of user actions\nconditional on networks states. This paper provides an overall picture of the\nlearning process in a Bayesian framework with an emphasize on the\nstate-of-the-art theoretical analyses of the Expectation Maximization learning\nalgorithm.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 22:43:54 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Nguyen", "Quan", ""]]}, {"id": "1702.06230", "submitter": "Vlad Firoiu", "authors": "Vlad Firoiu, William F. Whitney, Joshua B. Tenenbaum", "title": "Beating the World's Best at Super Smash Bros. with Deep Reinforcement\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a recent explosion in the capabilities of game-playing\nartificial intelligence. Many classes of RL tasks, from Atari games to motor\ncontrol to board games, are now solvable by fairly generic algorithms, based on\ndeep learning, that learn to play from experience with minimal knowledge of the\nspecific domain of interest. In this work, we will investigate the performance\nof these methods on Super Smash Bros. Melee (SSBM), a popular console fighting\ngame. The SSBM environment has complex dynamics and partial observability,\nmaking it challenging for human and machine alike. The multi-player aspect\nposes an additional challenge, as the vast majority of recent advances in RL\nhave focused on single-agent environments. Nonetheless, we will show that it is\npossible to train agents that are competitive against and even surpass human\nprofessionals, a new result for the multi-player video game setting.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 01:06:11 GMT"}, {"version": "v2", "created": "Thu, 2 Mar 2017 01:54:33 GMT"}, {"version": "v3", "created": "Mon, 8 May 2017 15:03:25 GMT"}], "update_date": "2017-05-09", "authors_parsed": [["Firoiu", "Vlad", ""], ["Whitney", "William F.", ""], ["Tenenbaum", "Joshua B.", ""]]}, {"id": "1702.06238", "submitter": "Karan Goel", "authors": "Karan Goel, Christoph Dann and Emma Brunskill", "title": "Sample Efficient Policy Search for Optimal Stopping Domains", "comments": "To appear in IJCAI-2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Optimal stopping problems consider the question of deciding when to stop an\nobservation-generating process in order to maximize a return. We examine the\nproblem of simultaneously learning and planning in such domains, when data is\ncollected directly from the environment. We propose GFSE, a simple and flexible\nmodel-free policy search method that reuses data for sample efficiency by\nleveraging problem structure. We bound the sample complexity of our approach to\nguarantee uniform convergence of policy value estimates, tightening existing\nPAC bounds to achieve logarithmic dependence on horizon length for our setting.\nWe also examine the benefit of our method against prevalent model-based and\nmodel-free approaches on 3 domains taken from diverse fields.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 02:14:47 GMT"}, {"version": "v2", "created": "Wed, 24 May 2017 06:40:07 GMT"}], "update_date": "2017-05-25", "authors_parsed": [["Goel", "Karan", ""], ["Dann", "Christoph", ""], ["Brunskill", "Emma", ""]]}, {"id": "1702.06253", "submitter": "Zhengxing Chen", "authors": "Zhengxing Chen, Yizhou Sun, Magy Seif El-nasr, Truong-Huy D. Nguyen", "title": "Player Skill Decomposition in Multiplayer Online Battle Arenas", "comments": "2016 Meaningful Play Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.HC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Successful analysis of player skills in video games has important impacts on\nthe process of enhancing player experience without undermining their continuous\nskill development. Moreover, player skill analysis becomes more intriguing in\nteam-based video games because such form of study can help discover useful\nfactors in effective team formation. In this paper, we consider the problem of\nskill decomposition in MOBA (MultiPlayer Online Battle Arena) games, with the\ngoal to understand what player skill factors are essential for the outcome of a\ngame match. To understand the construct of MOBA player skills, we utilize\nvarious skill-based predictive models to decompose player skills into\ninterpretative parts, the impact of which are assessed in statistical terms. We\napply this analysis approach on two widely known MOBAs, namely League of\nLegends (LoL) and Defense of the Ancients 2 (DOTA2). The finding is that base\nskills of in-game avatars, base skills of players, and players'\nchampion-specific skills are three prominent skill components influencing LoL's\nmatch outcomes, while those of DOTA2 are mainly impacted by in-game avatars'\nbase skills but not much by the other two.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 03:41:49 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Chen", "Zhengxing", ""], ["Sun", "Yizhou", ""], ["El-nasr", "Magy Seif", ""], ["Nguyen", "Truong-Huy D.", ""]]}, {"id": "1702.06329", "submitter": "Angel Martinez-Tenor", "authors": "Angel Mart\\'inez-Tenor, Juan Antonio Fern\\'andez-Madrigal, Ana\n  Cruz-Mart\\'in and Javier Gonz\\'alez-Jim\\'enez", "title": "Towards a Common Implementation of Reinforcement Learning for Multiple\n  Robotic Tasks", "comments": "15 pages, 10 figures, 7 tables. To be published in a scientific\n  journal", "journal-ref": null, "doi": "10.1016/j.eswa.2017.11.011", "report-no": null, "categories": "cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile robots are increasingly being employed for performing complex tasks in\ndynamic environments. Reinforcement learning (RL) methods are recognized to be\npromising for specifying such tasks in a relatively simple manner. However, the\nstrong dependency between the learning method and the task to learn is a\nwell-known problem that restricts practical implementations of RL in robotics,\noften requiring major modifications of parameters and adding other techniques\nfor each particular task. In this paper we present a practical core\nimplementation of RL which enables the learning process for multiple robotic\ntasks with minimal per-task tuning or none. Based on value iteration methods,\nthis implementation includes a novel approach for action selection, called\nQ-biased softmax regression (QBIASSR), which avoids poor performance of the\nlearning process when the robot reaches new unexplored states. Our approach\ntakes advantage of the structure of the state space by attending the physical\nvariables involved (e.g., distances to obstacles, X,Y,{\\theta} pose, etc.),\nthus experienced sets of states may favor the decision-making process of\nunexplored or rarely-explored states. This improvement has a relevant role in\nreducing the tuning of the algorithm for particular tasks. Experiments with\nreal and simulated robots, performed with the software framework also\nintroduced here, show that our implementation is effectively able to learn\ndifferent robotic tasks without tuning the learning method. Results also\nsuggest that the combination of true online SARSA({\\lambda}) with QBIASSR can\noutperform the existing RL core algorithms in low-dimensional robotic tasks.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 11:07:27 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Mart\u00ednez-Tenor", "Angel", ""], ["Fern\u00e1ndez-Madrigal", "Juan Antonio", ""], ["Cruz-Mart\u00edn", "Ana", ""], ["Gonz\u00e1lez-Jim\u00e9nez", "Javier", ""]]}, {"id": "1702.06334", "submitter": "Sunbeom So", "authors": "Sunbeom So and Hakjoo Oh", "title": "Synthesizing Imperative Programs from Examples Guided by Static Analysis", "comments": "The paper is accepted in Static Analysis Symposium (SAS) '17. The\n  submission version is somewhat different from the version in arxiv. The final\n  version will be uploaded after the camera-ready version is ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  We present a novel algorithm that synthesizes imperative programs for\nintroductory programming courses. Given a set of input-output examples and a\npartial program, our algorithm generates a complete program that is consistent\nwith every example. Our key idea is to combine enumerative program synthesis\nand static analysis, which aggressively prunes out a large search space while\nguaranteeing to find, if any, a correct solution. We have implemented our\nalgorithm in a tool, called SIMPL, and evaluated it on 30 problems used in\nintroductory programming courses. The results show that SIMPL is able to solve\nthe benchmark problems in 6.6 seconds on average.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 11:29:28 GMT"}, {"version": "v2", "created": "Tue, 13 Jun 2017 09:13:33 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["So", "Sunbeom", ""], ["Oh", "Hakjoo", ""]]}, {"id": "1702.06404", "submitter": "Jacob Whitehill", "authors": "Jacob Whitehill, Kiran Mohan, Daniel Seaton, Yigal Rosen, and Dustin\n  Tingley", "title": "Delving Deeper into MOOC Student Dropout Prediction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to obtain reliable accuracy estimates for automatic MOOC dropout\npredictors, it is important to train and test them in a manner consistent with\nhow they will be used in practice. Yet most prior research on MOOC dropout\nprediction has measured test accuracy on the same course used for training the\nclassifier, which can lead to overly optimistic accuracy estimates. In order to\nunderstand better how accuracy is affected by the training+testing regime, we\ncompared the accuracy of a standard dropout prediction architecture\n(clickstream features + logistic regression) across 4 different training\nparadigms. Results suggest that (1) training and testing on the same course\n(\"post-hoc\") can overestimate accuracy by several percentage points; (2)\ndropout classifiers trained on proxy labels based on students' persistence are\nsurprisingly competitive with post-hoc training (87.33% versus 90.20% AUC\naveraged over 8 weeks of 40 HarvardX MOOCs); and (3) classifier performance\ndoes not vary significantly with the academic discipline. Finally, we also\nresearch new dropout prediction architectures based on deep, fully-connected,\nfeed-forward neural networks and find that (4) networks with as many as 5\nhidden layers can statistically significantly increase test accuracy over that\nof logistic regression.\n", "versions": [{"version": "v1", "created": "Tue, 21 Feb 2017 14:35:55 GMT"}], "update_date": "2017-02-22", "authors_parsed": [["Whitehill", "Jacob", ""], ["Mohan", "Kiran", ""], ["Seaton", "Daniel", ""], ["Rosen", "Yigal", ""], ["Tingley", "Dustin", ""]]}, {"id": "1702.06662", "submitter": "Davoud Mougouei", "authors": "Davoud Mougouei, David M. W. Powers, Asghar Moeini", "title": "An Integer Programming Model for Binary Knapsack Problem with\n  Value-Related Dependencies among Elements", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Binary Knapsack Problem (BKP) is to select a subset of an element (item) set\nwith the highest value while keeping the total weight within the capacity of\nthe knapsack. This paper presents an integer programming model for a variation\nof BKP where the value of each element may depend on selecting or ignoring\nother elements. Strengths of such Value-Related Dependencies are assumed to be\nimprecise and hard to specify. To capture this imprecision, we have proposed\nmodeling value-related dependencies using fuzzy graphs and their algebraic\nstructure.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 03:14:05 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Mougouei", "Davoud", ""], ["Powers", "David M. W.", ""], ["Moeini", "Asghar", ""]]}, {"id": "1702.06674", "submitter": "Yun Cao", "authors": "Yun Cao, Zhiming Zhou, Weinan Zhang, Yong Yu", "title": "Unsupervised Diverse Colorization via Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Colorization of grayscale images has been a hot topic in computer vision.\nPrevious research mainly focuses on producing a colored image to match the\noriginal one. However, since many colors share the same gray value, an input\ngrayscale image could be diversely colored while maintaining its reality. In\nthis paper, we design a novel solution for unsupervised diverse colorization.\nSpecifically, we leverage conditional generative adversarial networks to model\nthe distribution of real-world item colors, in which we develop a fully\nconvolutional generator with multi-layer noise to enhance diversity, with\nmulti-layer condition concatenation to maintain reality, and with stride 1 to\nkeep spatial information. With such a novel network architecture, the model\nyields highly competitive performance on the open LSUN bedroom dataset. The\nTuring test of 80 humans further indicates our generated color schemes are\nhighly convincible.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 04:34:31 GMT"}, {"version": "v2", "created": "Sat, 1 Jul 2017 10:57:03 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Cao", "Yun", ""], ["Zhou", "Zhiming", ""], ["Zhang", "Weinan", ""], ["Yu", "Yong", ""]]}, {"id": "1702.06700", "submitter": "Yuetan Lin", "authors": "Yuetan Lin, Zhangyang Pang, Donghui Wang, Yueting Zhuang", "title": "Task-driven Visual Saliency and Attention-based Visual Question\n  Answering", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Visual question answering (VQA) has witnessed great progress since May, 2015\nas a classic problem unifying visual and textual data into a system. Many\nenlightening VQA works explore deep into the image and question encodings and\nfusing methods, of which attention is the most effective and infusive\nmechanism. Current attention based methods focus on adequate fusion of visual\nand textual features, but lack the attention to where people focus to ask\nquestions about the image. Traditional attention based methods attach a single\nvalue to the feature at each spatial location, which losses many useful\ninformation. To remedy these problems, we propose a general method to perform\nsaliency-like pre-selection on overlapped region features by the interrelation\nof bidirectional LSTM (BiLSTM), and use a novel element-wise multiplication\nbased attention method to capture more competent correlation information\nbetween visual and textual features. We conduct experiments on the large-scale\nCOCO-VQA dataset and analyze the effectiveness of our model demonstrated by\nstrong empirical results.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 08:19:38 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Lin", "Yuetan", ""], ["Pang", "Zhangyang", ""], ["Wang", "Donghui", ""], ["Zhuang", "Yueting", ""]]}, {"id": "1702.06763", "submitter": "Ji Gao", "authors": "Ji Gao, Beilun Wang, Zeming Lin, Weilin Xu, Yanjun Qi", "title": "DeepCloak: Masking Deep Neural Network Models for Robustness Against\n  Adversarial Samples", "comments": "adversarial samples, deep neural network", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have shown that deep neural networks (DNN) are vulnerable to\nadversarial samples: maliciously-perturbed samples crafted to yield incorrect\nmodel outputs. Such attacks can severely undermine DNN systems, particularly in\nsecurity-sensitive settings. It was observed that an adversary could easily\ngenerate adversarial samples by making a small perturbation on irrelevant\nfeature dimensions that are unnecessary for the current classification task. To\novercome this problem, we introduce a defensive mechanism called DeepCloak. By\nidentifying and removing unnecessary features in a DNN model, DeepCloak limits\nthe capacity an attacker can use generating adversarial samples and therefore\nincrease the robustness against such inputs. Comparing with other defensive\napproaches, DeepCloak is easy to implement and computationally efficient.\nExperimental results show that DeepCloak can increase the performance of\nstate-of-the-art DNN models against adversarial samples.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 11:48:35 GMT"}, {"version": "v2", "created": "Wed, 1 Mar 2017 05:55:00 GMT"}, {"version": "v3", "created": "Thu, 2 Mar 2017 16:42:12 GMT"}, {"version": "v4", "created": "Wed, 8 Mar 2017 17:18:19 GMT"}, {"version": "v5", "created": "Fri, 10 Mar 2017 00:02:04 GMT"}, {"version": "v6", "created": "Tue, 21 Mar 2017 16:26:33 GMT"}, {"version": "v7", "created": "Sat, 25 Mar 2017 22:16:05 GMT"}, {"version": "v8", "created": "Mon, 17 Apr 2017 21:54:30 GMT"}], "update_date": "2017-04-28", "authors_parsed": [["Gao", "Ji", ""], ["Wang", "Beilun", ""], ["Lin", "Zeming", ""], ["Xu", "Weilin", ""], ["Qi", "Yanjun", ""]]}, {"id": "1702.06776", "submitter": "Kailash Budhathoki", "authors": "Kailash Budhathoki and Jilles Vreeken", "title": "Causal Inference by Stochastic Complexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The algorithmic Markov condition states that the most likely causal direction\nbetween two random variables X and Y can be identified as that direction with\nthe lowest Kolmogorov complexity. Due to the halting problem, however, this\nnotion is not computable.\n  We hence propose to do causal inference by stochastic complexity. That is, we\npropose to approximate Kolmogorov complexity via the Minimum Description Length\n(MDL) principle, using a score that is mini-max optimal with regard to the\nmodel class under consideration. This means that even in an adversarial\nsetting, such as when the true distribution is not in this class, we still\nobtain the optimal encoding for the data relative to the class.\n  We instantiate this framework, which we call CISC, for pairs of univariate\ndiscrete variables, using the class of multinomial distributions. Experiments\nshow that CISC is highly accurate on synthetic, benchmark, as well as\nreal-world data, outperforming the state of the art by a margin, and scales\nextremely well with regard to sample and domain sizes.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 12:36:21 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Budhathoki", "Kailash", ""], ["Vreeken", "Jilles", ""]]}, {"id": "1702.06820", "submitter": "Zhengxing Chen", "authors": "Zhengxing Chen, Su Xue, John Kolen, Navid Aghdaie, Kazi A. Zaman,\n  Yizhou Sun, Magy Seif El-Nasr", "title": "EOMM: An Engagement Optimized Matchmaking Framework", "comments": "WWW2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Matchmaking connects multiple players to participate in online\nplayer-versus-player games. Current matchmaking systems depend on a single core\nstrategy: create fair games at all times. These systems pair similarly skilled\nplayers on the assumption that a fair game is best player experience. We will\ndemonstrate, however, that this intuitive assumption sometimes fails and that\nmatchmaking based on fairness is not optimal for engagement.\n  In this paper, we propose an Engagement Optimized Matchmaking (EOMM)\nframework that maximizes overall player engagement. We prove that equal-skill\nbased matchmaking is a special case of EOMM on a highly simplified assumption\nthat rarely holds in reality. Our simulation on real data from a popular game\nmade by Electronic Arts, Inc. (EA) supports our theoretical results, showing\nsignificant improvement in enhancing player engagement compared to existing\nmatchmaking methods.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 14:51:52 GMT"}], "update_date": "2018-06-27", "authors_parsed": [["Chen", "Zhengxing", ""], ["Xue", "Su", ""], ["Kolen", "John", ""], ["Aghdaie", "Navid", ""], ["Zaman", "Kazi A.", ""], ["Sun", "Yizhou", ""], ["El-Nasr", "Magy Seif", ""]]}, {"id": "1702.06831", "submitter": "Matej Mihel\\v{c}i\\'c", "authors": "Matej Mihel\\v{c}i\\'c, Goran \\v{S}imi\\'c, Mirjana Babi\\'c Leko, Nada\n  Lavra\\v{c}, Sa\\v{s}o D\\v{z}eroski, Tomislav \\v{S}muc", "title": "Using Redescription Mining to Relate Clinical and Biological\n  Characteristics of Cognitively Impaired and Alzheimer's Disease Patients", "comments": null, "journal-ref": null, "doi": "10.1371/journal.pone.0187364", "report-no": null, "categories": "q-bio.QM cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We used redescription mining to find interpretable rules revealing\nassociations between those determinants that provide insights about the\nAlzheimer's disease (AD). We extended the CLUS-RM redescription mining\nalgorithm to a constraint-based redescription mining (CBRM) setting, which\nenables several modes of targeted exploration of specific, user-constrained\nassociations. Redescription mining enabled finding specific constructs of\nclinical and biological attributes that describe many groups of subjects of\ndifferent size, homogeneity and levels of cognitive impairment. We confirmed\nsome previously known findings. However, in some instances, as with the\nattributes: testosterone, the imaging attribute Spatial Pattern of\nAbnormalities for Recognition of Early AD, as well as the levels of leptin and\nangiopoietin-2 in plasma, we corroborated previously debatable findings or\nprovided additional information about these variables and their association\nwith AD pathogenesis. Applying redescription mining on ADNI data resulted with\nthe discovery of one largely unknown attribute: the Pregnancy-Associated\nProtein-A (PAPP-A), which we found highly associated with cognitive impairment\nin AD. Statistically significant correlations (p <= 0.01) were found between\nPAPP-A and various different clinical tests. The high importance of this\nfinding lies in the fact that PAPP-A is a metalloproteinase, known to cleave\ninsulin-like growth factor binding proteins. Since it also shares similar\nsubstrates with A Disintegrin and the Metalloproteinase family of enzymes that\nact as {\\alpha}-secretase to physiologically cleave amyloid precursor protein\n(APP) in the non-amyloidogenic pathway, it could be directly involved in the\nmetabolism of APP very early during the disease course. Therefore, further\nstudies should investigate the role of PAPP-A in the development of AD more\nthoroughly.\n", "versions": [{"version": "v1", "created": "Mon, 20 Feb 2017 09:56:34 GMT"}, {"version": "v2", "created": "Tue, 14 Nov 2017 18:15:52 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Mihel\u010di\u0107", "Matej", ""], ["\u0160imi\u0107", "Goran", ""], ["Leko", "Mirjana Babi\u0107", ""], ["Lavra\u010d", "Nada", ""], ["D\u017eeroski", "Sa\u0161o", ""], ["\u0160muc", "Tomislav", ""]]}, {"id": "1702.06879", "submitter": "Th\\'eo Trouillon", "authors": "Th\\'eo Trouillon, Christopher R. Dance, Johannes Welbl, Sebastian\n  Riedel, \\'Eric Gaussier, Guillaume Bouchard", "title": "Knowledge Graph Completion via Complex Tensor Factorization", "comments": "38 pages, accepted in JMLR. This is an extended version of the\n  article \"Complex embeddings for simple link prediction\" (ICML 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG math.SP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In statistical relational learning, knowledge graph completion deals with\nautomatically understanding the structure of large knowledge graphs---labeled\ndirected graphs---and predicting missing relationships---labeled edges.\nState-of-the-art embedding models propose different trade-offs between modeling\nexpressiveness, and time and space complexity. We reconcile both expressiveness\nand complexity through the use of complex-valued embeddings and explore the\nlink between such complex-valued embeddings and unitary diagonalization. We\ncorroborate our approach theoretically and show that all real square\nmatrices---thus all possible relation/adjacency matrices---are the real part of\nsome unitarily diagonalizable matrix. This results opens the door to a lot of\nother applications of square matrices factorization. Our approach based on\ncomplex embeddings is arguably simple, as it only involves a Hermitian dot\nproduct, the complex counterpart of the standard dot product between real\nvectors, whereas other methods resort to more and more complicated composition\nfunctions to increase their expressiveness. The proposed complex embeddings are\nscalable to large data sets as it remains linear in both space and time, while\nconsistently outperforming alternative approaches on standard link prediction\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 16:28:11 GMT"}, {"version": "v2", "created": "Sun, 26 Nov 2017 20:39:34 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Trouillon", "Th\u00e9o", ""], ["Dance", "Christopher R.", ""], ["Welbl", "Johannes", ""], ["Riedel", "Sebastian", ""], ["Gaussier", "\u00c9ric", ""], ["Bouchard", "Guillaume", ""]]}, {"id": "1702.06915", "submitter": "Ferdinando Fioretto Ferdinando Fioretto", "authors": "Ferdinando Fioretto and Agostino Dovier and Enrico Pontelli and\n  William Yeoh and Roie Zivan", "title": "Solving DCOPs with Distributed Large Neighborhood Search", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of Distributed Constraint Optimization has gained momentum in\nrecent years, thanks to its ability to address various applications related to\nmulti-agent cooperation. Nevertheless, solving Distributed Constraint\nOptimization Problems (DCOPs) optimally is NP-hard. Therefore, in large-scale,\ncomplex applications, incomplete DCOP algorithms are necessary. Current\nincomplete DCOP algorithms suffer of one or more of the following limitations:\nthey (a) find local minima without providing quality guarantees; (b) provide\nloose quality assessment; or (c) are unable to benefit from the structure of\nthe problem, such as domain-dependent knowledge and hard constraints.\nTherefore, capitalizing on strategies from the centralized constraint solving\ncommunity, we propose a Distributed Large Neighborhood Search (D-LNS) framework\nto solve DCOPs. The proposed framework (with its novel repair phase) provides\nguarantees on solution quality, refining upper and lower bounds during the\niterative process, and can exploit domain-dependent structures. Our\nexperimental results show that D-LNS outperforms other incomplete DCOP\nalgorithms on both structured and unstructured problem instances.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 17:54:23 GMT"}, {"version": "v2", "created": "Thu, 23 Feb 2017 01:21:38 GMT"}], "update_date": "2017-02-24", "authors_parsed": [["Fioretto", "Ferdinando", ""], ["Dovier", "Agostino", ""], ["Pontelli", "Enrico", ""], ["Yeoh", "William", ""], ["Zivan", "Roie", ""]]}, {"id": "1702.06925", "submitter": "Xiang Xiang", "authors": "Feng Wang, Xiang Xiang, Chang Liu, Trac D. Tran, Austin Reiter,\n  Gregory D. Hager, Harry Quon, Jian Cheng, Alan L. Yuille", "title": "Regularizing Face Verification Nets For Pain Intensity Regression", "comments": "5 pages, 3 figure; Camera-ready version to appear at IEEE ICIP 2017", "journal-ref": null, "doi": "10.13140/RG.2.2.20841.49765", "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Limited labeled data are available for the research of estimating facial\nexpression intensities. For instance, the ability to train deep networks for\nautomated pain assessment is limited by small datasets with labels of\npatient-reported pain intensities. Fortunately, fine-tuning from a\ndata-extensive pre-trained domain, such as face verification, can alleviate\nthis problem. In this paper, we propose a network that fine-tunes a\nstate-of-the-art face verification network using a regularized regression loss\nand additional data with expression labels. In this way, the expression\nintensity regression task can benefit from the rich feature representations\ntrained on a huge amount of data for face verification. The proposed\nregularized deep regressor is applied to estimate the pain expression intensity\nand verified on the widely-used UNBC-McMaster Shoulder-Pain dataset, achieving\nthe state-of-the-art performance. A weighted evaluation metric is also proposed\nto address the imbalance issue of different pain intensities.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 18:15:42 GMT"}, {"version": "v2", "created": "Tue, 9 May 2017 17:58:58 GMT"}, {"version": "v3", "created": "Thu, 1 Jun 2017 17:49:56 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Wang", "Feng", ""], ["Xiang", "Xiang", ""], ["Liu", "Chang", ""], ["Tran", "Trac D.", ""], ["Reiter", "Austin", ""], ["Hager", "Gregory D.", ""], ["Quon", "Harry", ""], ["Cheng", "Jian", ""], ["Yuille", "Alan L.", ""]]}, {"id": "1702.06934", "submitter": "Olegs Verhodubs", "authors": "Olegs Verhodubs", "title": "Realization of Ontology Web Search Engine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the realization of the Ontology Web Search Engine. The\nOntology Web Search Engine is realizable as independent project and as a part\nof other projects. The main purpose of this paper is to present the Ontology\nWeb Search Engine realization details as the part of the Semantic Web Expert\nSystem and to present the results of the Ontology Web Search Engine\nfunctioning. It is expected that the Semantic Web Expert System will be able to\nprocess ontologies from the Web, generate rules from these ontologies and\ndevelop its knowledge base.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 18:35:43 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["Verhodubs", "Olegs", ""]]}, {"id": "1702.06970", "submitter": "William Kluegel", "authors": "William Kluegel, Muhammad Aamir Iqbal, Ferdinando Fioretto, William\n  Yeoh, and Enrico Pontelli", "title": "A Realistic Dataset for the Smart Home Device Scheduling Problem for\n  DCOPs", "comments": "15 pages, OPTMAS17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The field of Distributed Constraint Optimization has gained momentum in\nrecent years thanks to its ability to address various applications related to\nmulti-agent cooperation. While techniques to solve Distributed Constraint\nOptimization Problems (DCOPs) are abundant and have matured substantially since\nthe field inception, the number of DCOP realistic applications and benchmark\nused to asses the performance of DCOP algorithms is lagging behind. To contrast\nthis background we (i) introduce the Smart Home Device Scheduling (SHDS)\nproblem, which describe the problem of coordinating smart devices schedules\nacross multiple homes as a multi-agent system, (ii) detail the physical models\nadopted to simulate smart sensors, smart actuators, and homes environments, and\n(iii) introduce a DCOP realistic benchmark for SHDS problems.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 19:10:30 GMT"}], "update_date": "2017-02-24", "authors_parsed": [["Kluegel", "William", ""], ["Iqbal", "Muhammad Aamir", ""], ["Fioretto", "Ferdinando", ""], ["Yeoh", "William", ""], ["Pontelli", "Enrico", ""]]}, {"id": "1702.07001", "submitter": "Doron Zarchy", "authors": "Doron Zarchy", "title": "Theoretical and Experimental Analysis of the Canadian Traveler Problem", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Devising an optimal strategy for navigation in a partially observable\nenvironment is one of the key objectives in AI. One of the problem in this\ncontext is the Canadian Traveler Problem (CTP). CTP is a navigation problem\nwhere an agent is tasked to travel from source to target in a partially\nobservable weighted graph, whose edge might be blocked with a certain\nprobability and observing such blockage occurs only when reaching upon one of\nthe edges end points. The goal is to find a strategy that minimizes the\nexpected travel cost. The problem is known to be P$\\#$ hard. In this work we\nstudy the CTP theoretically and empirically. First, we study the Dep-CTP, a CTP\nvariant we introduce which assumes dependencies between the edges status. We\nshow that Dep-CTP is intractable, and further we analyze two of its subclasses\non disjoint paths graph. Second, we develop a general algorithm Gen-PAO that\noptimally solve the CTP. Gen-PAO is capable of solving two other types of CTP\ncalled Sensing-CTP and Expensive-Edges CTP. Since the CTP is intractable,\nGen-PAO use some pruning methods to reduce the space search for the optimal\nsolution. We also define some variants of Gen-PAO, compare their performance\nand show some benefits of Gen-PAO over existing work.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 20:57:29 GMT"}], "update_date": "2017-02-24", "authors_parsed": [["Zarchy", "Doron", ""]]}, {"id": "1702.07031", "submitter": "Ursula Challita", "authors": "Ursula Challita, Li Dong, and Walid Saad", "title": "Proactive Resource Management for LTE in Unlicensed Spectrum: A Deep\n  Learning Perspective", "comments": "This paper has been accepted for publication at IEEE Transaction on\n  Wireless Communications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AI cs.GT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LTE in unlicensed spectrum using licensed assisted access LTE (LTE-LAA) is a\npromising approach to overcome the wireless spectrum scarcity. However, to reap\nthe benefits of LTE-LAA, a fair coexistence mechanism with other incumbent WiFi\ndeployments is required. In this paper, a novel deep learning approach is\nproposed for modeling the resource allocation problem of LTE-LAA small base\nstations (SBSs). The proposed approach enables multiple SBSs to proactively\nperform dynamic channel selection, carrier aggregation, and fractional spectrum\naccess while guaranteeing fairness with existing WiFi networks and other\nLTE-LAA operators. Adopting a proactive coexistence mechanism enables future\ndelay-tolerant LTE-LAA data demands to be served within a given prediction\nwindow ahead of their actual arrival time thus avoiding the underutilization of\nthe unlicensed spectrum during off-peak hours while maximizing the total served\nLTE-LAA traffic load. To this end, a noncooperative game model is formulated in\nwhich SBSs are modeled as Homo Egualis agents that aim at predicting a sequence\nof future actions and thus achieving long-term equal weighted fairness with\nWLAN and other LTE-LAA operators over a given time horizon. The proposed deep\nlearning algorithm is then shown to reach a mixed-strategy Nash equilibrium\n(NE), when it converges. Simulation results using real data traces show that\nthe proposed scheme can yield up to 28% and 11% gains over a conventional\nreactive approach and a proportional fair coexistence mechanism, respectively.\nThe results also show that the proposed framework prevents WiFi performance\ndegradation for a densely deployed LTE-LAA network.\n", "versions": [{"version": "v1", "created": "Wed, 22 Feb 2017 22:34:57 GMT"}, {"version": "v2", "created": "Sat, 15 Dec 2018 12:38:13 GMT"}], "update_date": "2018-12-18", "authors_parsed": [["Challita", "Ursula", ""], ["Dong", "Li", ""], ["Saad", "Walid", ""]]}, {"id": "1702.07134", "submitter": "Faez Ahmed", "authors": "Faez Ahmed, John P. Dickerson, Mark Fuge", "title": "Diverse Weighted Bipartite b-Matching", "comments": null, "journal-ref": null, "doi": "10.24963/ijcai.2017/6", "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bipartite matching, where agents on one side of a market are matched to\nagents or items on the other, is a classical problem in computer science and\neconomics, with widespread application in healthcare, education, advertising,\nand general resource allocation. A practitioner's goal is typically to maximize\na matching market's economic efficiency, possibly subject to some fairness\nrequirements that promote equal access to resources. A natural balancing act\nexists between fairness and efficiency in matching markets, and has been the\nsubject of much research.\n  In this paper, we study a complementary goal---balancing diversity and\nefficiency---in a generalization of bipartite matching where agents on one side\nof the market can be matched to sets of agents on the other. Adapting a\nclassical definition of the diversity of a set, we propose a quadratic\nprogramming-based approach to solving a supermodular minimization problem that\nbalances diversity and total weight of the solution. We also provide a scalable\ngreedy algorithm with theoretical performance bounds. We then define the price\nof diversity, a measure of the efficiency loss due to enforcing diversity, and\ngive a worst-case theoretical bound. Finally, we demonstrate the efficacy of\nour methods on three real-world datasets, and show that the price of diversity\nis not bad in practice.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 08:35:45 GMT"}, {"version": "v2", "created": "Tue, 15 Aug 2017 16:33:21 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Ahmed", "Faez", ""], ["Dickerson", "John P.", ""], ["Fuge", "Mark", ""]]}, {"id": "1702.07168", "submitter": "Amit Mishra", "authors": "Amit Kumar Mishra", "title": "A DIKW Paradigm to Cognitive Engineering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Though the word cognitive has a wide range of meanings we define cognitive\nengineering as learning from brain to bolster engineering solutions. However,\ngiving an achievable framework to the process towards this has been a difficult\ntask. In this work we take the classic data information knowledge wisdom (DIKW)\nframework to set some achievable goals and sub-goals towards cognitive\nengineering. A layered framework like DIKW aligns nicely with the layered\nstructure of pre-frontal cortex. And breaking the task into sub-tasks based on\nthe layers also makes it easier to start developmental endeavours towards\nachieving the final goal of a brain-inspired system.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 10:51:32 GMT"}], "update_date": "2017-02-24", "authors_parsed": [["Mishra", "Amit Kumar", ""]]}, {"id": "1702.07193", "submitter": "Armando Tacchella", "authors": "Marco Menapace and Armando Tacchella", "title": "Ontologies in System Engineering: a Field Report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years ontologies enjoyed a growing popularity outside specialized\nAI communities. System engineering is no exception to this trend, with\nontologies being proposed as a basis for several tasks in complex industrial\nimplements, including system design, monitoring and diagnosis. In this paper,\nwe consider four different contributions to system engineering wherein\nontologies are instrumental to provide enhancements over traditional ad-hoc\ntechniques. For each application, we briefly report the methodologies, the\ntools and the results obtained with the goal to provide an assessment of merits\nand limits of ontologies in such domains.\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 12:36:14 GMT"}], "update_date": "2017-02-24", "authors_parsed": [["Menapace", "Marco", ""], ["Tacchella", "Armando", ""]]}, {"id": "1702.07281", "submitter": "Yujie Qian", "authors": "Yujie Qian, Jie Tang, Zhilin Yang, Binxuan Huang, Wei Wei, Kathleen M.\n  Carley", "title": "A Probabilistic Framework for Location Inference from Social Media", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the extent to which we can infer users' geographical locations from\nsocial media. Location inference from social media can benefit many\napplications, such as disaster management, targeted advertising, and news\ncontent tailoring. The challenges, however, lie in the limited amount of\nlabeled data and the large scale of social networks. In this paper, we\nformalize the problem of inferring location from social media into a\nsemi-supervised factor graph model (SSFGM). The model provides a probabilistic\nframework in which various sources of information (e.g., content and social\nnetwork) can be combined together. We design a two-layer neural network to\nlearn feature representations, and incorporate the learned latent features into\nSSFGM. To deal with the large-scale problem, we propose a Two-Chain Sampling\n(TCS) algorithm to learn SSFGM. The algorithm achieves a good trade-off between\naccuracy and efficiency. Experiments on Twitter and Weibo show that the\nproposed TCS algorithm for SSFGM can substantially improve the inference\naccuracy over several state-of-the-art methods. More importantly, TCS achieves\nover 100x speedup comparing with traditional propagation-based methods (e.g.,\nloopy belief propagation).\n", "versions": [{"version": "v1", "created": "Thu, 23 Feb 2017 16:34:07 GMT"}, {"version": "v2", "created": "Wed, 1 Mar 2017 16:23:25 GMT"}, {"version": "v3", "created": "Fri, 10 May 2019 18:17:15 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Qian", "Yujie", ""], ["Tang", "Jie", ""], ["Yang", "Zhilin", ""], ["Huang", "Binxuan", ""], ["Wei", "Wei", ""], ["Carley", "Kathleen M.", ""]]}, {"id": "1702.07450", "submitter": "David Balduzzi", "authors": "David Balduzzi", "title": "Strongly-Typed Agents are Guaranteed to Interact Safely", "comments": "ICML 2017, final version", "journal-ref": "PMLR volume 70, 2017", "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As artificial agents proliferate, it is becoming increasingly important to\nensure that their interactions with one another are well-behaved. In this\npaper, we formalize a common-sense notion of when algorithms are well-behaved:\nan algorithm is safe if it does no harm. Motivated by recent progress in deep\nlearning, we focus on the specific case where agents update their actions\naccording to gradient descent. The paper shows that that gradient descent\nconverges to a Nash equilibrium in safe games. The main contribution is to\ndefine strongly-typed agents and show they are guaranteed to interact safely,\nthereby providing sufficient conditions to guarantee safe interactions. A\nseries of examples show that strong-typing generalizes certain key features of\nconvexity, is closely related to blind source separation, and introduces a new\nperspective on classical multilinear games based on tensor decomposition.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 02:30:15 GMT"}, {"version": "v2", "created": "Wed, 6 Jun 2018 12:37:57 GMT"}], "update_date": "2018-06-07", "authors_parsed": [["Balduzzi", "David", ""]]}, {"id": "1702.07475", "submitter": "Fei Han", "authors": "Fei Han, Xue Yang, Yu Zhang, Hao Zhang", "title": "Sequence-based Multimodal Apprenticeship Learning For Robot Perception\n  and Decision Making", "comments": "8 pages, 6 figures, accepted by ICRA'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Apprenticeship learning has recently attracted a wide attention due to its\ncapability of allowing robots to learn physical tasks directly from\ndemonstrations provided by human experts. Most previous techniques assumed that\nthe state space is known a priori or employed simple state representations that\nusually suffer from perceptual aliasing. Different from previous research, we\npropose a novel approach named Sequence-based Multimodal Apprenticeship\nLearning (SMAL), which is capable to simultaneously fusing temporal information\nand multimodal data, and to integrate robot perception with decision making. To\nevaluate the SMAL approach, experiments are performed using both simulations\nand real-world robots in the challenging search and rescue scenarios. The\nempirical study has validated that our SMAL approach can effectively learn\nplans for robots to make decisions using sequence of multimodal observations.\nExperimental results have also showed that SMAL outperforms the baseline\nmethods using individual images.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 06:37:06 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Han", "Fei", ""], ["Yang", "Xue", ""], ["Zhang", "Yu", ""], ["Zhang", "Hao", ""]]}, {"id": "1702.07492", "submitter": "Ahmed Qureshi", "authors": "Ahmed Hussain Qureshi, Yutaka Nakamura, Yuichiro Yoshikawa and Hiroshi\n  Ishiguro", "title": "Robot gains Social Intelligence through Multimodal Deep Reinforcement\n  Learning", "comments": "The paper is published in IEEE-RAS International Conference on\n  Humanoid Robots (Humanoids) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For robots to coexist with humans in a social world like ours, it is crucial\nthat they possess human-like social interaction skills. Programming a robot to\npossess such skills is a challenging task. In this paper, we propose a\nMultimodal Deep Q-Network (MDQN) to enable a robot to learn human-like\ninteraction skills through a trial and error method. This paper aims to develop\na robot that gathers data during its interaction with a human and learns human\ninteraction behaviour from the high-dimensional sensory information using\nend-to-end reinforcement learning. This paper demonstrates that the robot was\nable to learn basic interaction skills successfully, after 14 days of\ninteracting with people.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 08:30:43 GMT"}], "update_date": "2017-02-27", "authors_parsed": [["Qureshi", "Ahmed Hussain", ""], ["Nakamura", "Yutaka", ""], ["Yoshikawa", "Yuichiro", ""], ["Ishiguro", "Hiroshi", ""]]}, {"id": "1702.07543", "submitter": "Mengya Wang", "authors": "Mengya Wang, Hankui Zhuo, Huiling Zhu", "title": "Embedding Knowledge Graphs Based on Transitivity and Antisymmetry of\n  Rules", "comments": "This paper has been withdrawn by the authors due to a crucial sign\n  error in equations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Representation learning of knowledge graphs encodes entities and relation\ntypes into a continuous low-dimensional vector space, learns embeddings of\nentities and relation types. Most existing methods only concentrate on\nknowledge triples, ignoring logic rules which contain rich background\nknowledge. Although there has been some work aiming at leveraging both\nknowledge triples and logic rules, they ignore the transitivity and\nantisymmetry of logic rules. In this paper, we propose a novel approach to\nlearn knowledge representations with entities and ordered relations in\nknowledges and logic rules. The key idea is to integrate knowledge triples and\nlogic rules, and approximately order the relation types in logic rules to\nutilize the transitivity and antisymmetry of logic rules. All entries of the\nembeddings of relation types are constrained to be non-negative. We translate\nthe general constrained optimization problem into an unconstrained optimization\nproblem to solve the non-negative matrix factorization. Experimental results\nshow that our model significantly outperforms other baselines on knowledge\ngraph completion task. It indicates that our model is capable of capturing the\ntransitivity and antisymmetry information, which is significant when learning\nembeddings of knowledge graphs.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 11:28:02 GMT"}, {"version": "v2", "created": "Wed, 19 Apr 2017 07:52:39 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Wang", "Mengya", ""], ["Zhuo", "Hankui", ""], ["Zhu", "Huiling", ""]]}, {"id": "1702.07784", "submitter": "Emiliano De Cristofaro", "authors": "Despoina Chatzakou, Nicolas Kourtellis, Jeremy Blackburn, Emiliano De\n  Cristofaro, Gianluca Stringhini, Athena Vakali", "title": "Measuring #GamerGate: A Tale of Hate, Sexism, and Bullying", "comments": "WWW Cybersafety Workshop 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past few years, online aggression and abusive behaviors have\noccurred in many different forms and on a variety of platforms. In extreme\ncases, these incidents have evolved into hate, discrimination, and bullying,\nand even materialized into real-world threats and attacks against individuals\nor groups. In this paper, we study the Gamergate controversy. Started in August\n2014 in the online gaming world, it quickly spread across various social\nnetworking platforms, ultimately leading to many incidents of cyberbullying and\ncyberaggression. We focus on Twitter, presenting a measurement study of a\ndataset of 340k unique users and 1.6M tweets to study the properties of these\nusers, the content they post, and how they differ from random Twitter users. We\nfind that users involved in this \"Twitter war\" tend to have more friends and\nfollowers, are generally more engaged and post tweets with negative sentiment,\nless joy, and more hate than random users. We also perform preliminary\nmeasurements on how the Twitter suspension mechanism deals with such abusive\nbehaviors. While we focus on Gamergate, our methodology to collect and analyze\ntweets related to aggressive and bullying activities is of independent\ninterest.\n", "versions": [{"version": "v1", "created": "Fri, 24 Feb 2017 22:14:30 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Chatzakou", "Despoina", ""], ["Kourtellis", "Nicolas", ""], ["Blackburn", "Jeremy", ""], ["De Cristofaro", "Emiliano", ""], ["Stringhini", "Gianluca", ""], ["Vakali", "Athena", ""]]}, {"id": "1702.07826", "submitter": "Upol Ehsan", "authors": "Upol Ehsan, Brent Harrison, Larry Chan, Mark O. Riedl", "title": "Rationalization: A Neural Machine Translation Approach to Generating\n  Natural Language Explanations", "comments": "9 pages, 4 figures; added human evaluation section; added author;\n  changed author order-Upol Ehsan and Brent Harrison both contributed equally\n  to this work", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce AI rationalization, an approach for generating explanations of\nautonomous system behavior as if a human had performed the behavior. We\ndescribe a rationalization technique that uses neural machine translation to\ntranslate internal state-action representations of an autonomous agent into\nnatural language. We evaluate our technique in the Frogger game environment,\ntraining an autonomous game playing agent to rationalize its action choices\nusing natural language. A natural language training corpus is collected from\nhuman players thinking out loud as they play the game. We motivate the use of\nrationalization as an approach to explanation generation and show the results\nof two experiments evaluating the effectiveness of rationalization. Results of\nthese evaluations show that neural machine translation is able to accurately\ngenerate rationalizations that describe agent behavior, and that\nrationalizations are more satisfying to humans than other alternative methods\nof explanation.\n", "versions": [{"version": "v1", "created": "Sat, 25 Feb 2017 03:20:49 GMT"}, {"version": "v2", "created": "Tue, 19 Dec 2017 05:20:09 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Ehsan", "Upol", ""], ["Harrison", "Brent", ""], ["Chan", "Larry", ""], ["Riedl", "Mark O.", ""]]}, {"id": "1702.07889", "submitter": "Michael Maher", "authors": "Michael J. Maher", "title": "Contractibility for Open Global Constraints", "comments": "Under consideration in Theory and Practice of Logic Programming\n  (TPLP)", "journal-ref": "Theory and Practice of Logic Programming 17(4), 365--407, 2017", "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open forms of global constraints allow the addition of new variables to an\nargument during the execution of a constraint program. Such forms are needed\nfor difficult constraint programming problems where problem construction and\nproblem solving are interleaved, and fit naturally within constraint logic\nprogramming. However, in general, filtering that is sound for a global\nconstraint can be unsound when the constraint is open. This paper provides a\nsimple characterization, called contractibility, of the constraints where\nfiltering remains sound when the constraint is open. With this characterization\nwe can easily determine whether a constraint has this property or not. In the\nlatter case, we can use it to derive a contractible approximation to the\nconstraint. We demonstrate this work on both hard and soft constraints. In the\nprocess, we formulate two general classes of soft constraints.\n", "versions": [{"version": "v1", "created": "Sat, 25 Feb 2017 13:12:26 GMT"}], "update_date": "2021-02-16", "authors_parsed": [["Maher", "Michael J.", ""]]}, {"id": "1702.07944", "submitter": "Simon Du", "authors": "Simon S. Du, Jianshu Chen, Lihong Li, Lin Xiao, Dengyong Zhou", "title": "Stochastic Variance Reduction Methods for Policy Evaluation", "comments": "Accepted by ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SY math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Policy evaluation is a crucial step in many reinforcement-learning\nprocedures, which estimates a value function that predicts states' long-term\nvalue under a given policy. In this paper, we focus on policy evaluation with\nlinear function approximation over a fixed dataset. We first transform the\nempirical policy evaluation problem into a (quadratic) convex-concave saddle\npoint problem, and then present a primal-dual batch gradient method, as well as\ntwo stochastic variance reduction methods for solving the problem. These\nalgorithms scale linearly in both sample size and feature dimension. Moreover,\nthey achieve linear convergence even when the saddle-point problem has only\nstrong concavity in the dual variables but no strong convexity in the primal\nvariables. Numerical experiments on benchmark problems demonstrate the\neffectiveness of our methods.\n", "versions": [{"version": "v1", "created": "Sat, 25 Feb 2017 20:15:55 GMT"}, {"version": "v2", "created": "Fri, 9 Jun 2017 06:02:47 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Du", "Simon S.", ""], ["Chen", "Jianshu", ""], ["Li", "Lihong", ""], ["Xiao", "Lin", ""], ["Zhou", "Dengyong", ""]]}, {"id": "1702.07983", "submitter": "Yanran Li", "authors": "Tong Che, Yanran Li, Ruixiang Zhang, R Devon Hjelm, Wenjie Li, Yangqiu\n  Song, Yoshua Bengio", "title": "Maximum-Likelihood Augmented Discrete Generative Adversarial Networks", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the successes in capturing continuous distributions, the application\nof generative adversarial networks (GANs) to discrete settings, like natural\nlanguage tasks, is rather restricted. The fundamental reason is the difficulty\nof back-propagation through discrete random variables combined with the\ninherent instability of the GAN training objective. To address these problems,\nwe propose Maximum-Likelihood Augmented Discrete Generative Adversarial\nNetworks. Instead of directly optimizing the GAN objective, we derive a novel\nand low-variance objective using the discriminator's output that follows\ncorresponds to the log-likelihood. Compared with the original, the new\nobjective is proved to be consistent in theory and beneficial in practice. The\nexperimental results on various discrete datasets demonstrate the effectiveness\nof the proposed approach.\n", "versions": [{"version": "v1", "created": "Sun, 26 Feb 2017 03:19:13 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Che", "Tong", ""], ["Li", "Yanran", ""], ["Zhang", "Ruixiang", ""], ["Hjelm", "R Devon", ""], ["Li", "Wenjie", ""], ["Song", "Yangqiu", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1702.08039", "submitter": "Dan Oprisa", "authors": "Dan Oprisa, Peter Toth", "title": "Criticality & Deep Learning I: Generally Weighted Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motivated by the idea that criticality and universality of phase transitions\nmight play a crucial role in achieving and sustaining learning and intelligent\nbehaviour in biological and artificial networks, we analyse a theoretical and a\npragmatic experimental set up for critical phenomena in deep learning. On the\ntheoretical side, we use results from statistical physics to carry out critical\npoint calculations in feed-forward/fully connected networks, while on the\nexperimental side we set out to find traces of criticality in deep neural\nnetworks. This is our first step in a series of upcoming investigations to map\nout the relationship between criticality and learning in deep networks.\n", "versions": [{"version": "v1", "created": "Sun, 26 Feb 2017 14:43:38 GMT"}, {"version": "v2", "created": "Wed, 31 May 2017 09:38:30 GMT"}], "update_date": "2017-06-01", "authors_parsed": [["Oprisa", "Dan", ""], ["Toth", "Peter", ""]]}, {"id": "1702.08165", "submitter": "Tuomas Haarnoja", "authors": "Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, Sergey Levine", "title": "Reinforcement Learning with Deep Energy-Based Policies", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method for learning expressive energy-based policies for\ncontinuous states and actions, which has been feasible only in tabular domains\nbefore. We apply our method to learning maximum entropy policies, resulting\ninto a new algorithm, called soft Q-learning, that expresses the optimal policy\nvia a Boltzmann distribution. We use the recently proposed amortized Stein\nvariational gradient descent to learn a stochastic sampling network that\napproximates samples from this distribution. The benefits of the proposed\nalgorithm include improved exploration and compositionality that allows\ntransferring skills between tasks, which we confirm in simulated experiments\nwith swimming and walking robots. We also draw a connection to actor-critic\nmethods, which can be viewed performing approximate inference on the\ncorresponding energy-based model.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 07:16:41 GMT"}, {"version": "v2", "created": "Fri, 21 Jul 2017 20:25:54 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Haarnoja", "Tuomas", ""], ["Tang", "Haoran", ""], ["Abbeel", "Pieter", ""], ["Levine", "Sergey", ""]]}, {"id": "1702.08192", "submitter": "Christian Wachinger", "authors": "Christian Wachinger, Martin Reuter, Tassilo Klein", "title": "DeepNAT: Deep Convolutional Neural Network for Segmenting Neuroanatomy", "comments": "Accepted for publication in NeuroImage, special issue \"Brain\n  Segmentation and Parcellation\", 2017", "journal-ref": null, "doi": "10.1016/j.neuroimage.2017.02.035", "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce DeepNAT, a 3D Deep convolutional neural network for the\nautomatic segmentation of NeuroAnaTomy in T1-weighted magnetic resonance\nimages. DeepNAT is an end-to-end learning-based approach to brain segmentation\nthat jointly learns an abstract feature representation and a multi-class\nclassification. We propose a 3D patch-based approach, where we do not only\npredict the center voxel of the patch but also neighbors, which is formulated\nas multi-task learning. To address a class imbalance problem, we arrange two\nnetworks hierarchically, where the first one separates foreground from\nbackground, and the second one identifies 25 brain structures on the\nforeground. Since patches lack spatial context, we augment them with\ncoordinates. To this end, we introduce a novel intrinsic parameterization of\nthe brain volume, formed by eigenfunctions of the Laplace-Beltrami operator. As\nnetwork architecture, we use three convolutional layers with pooling, batch\nnormalization, and non-linearities, followed by fully connected layers with\ndropout. The final segmentation is inferred from the probabilistic output of\nthe network with a 3D fully connected conditional random field, which ensures\nlabel agreement between close voxels. The roughly 2.7 million parameters in the\nnetwork are learned with stochastic gradient descent. Our results show that\nDeepNAT compares favorably to state-of-the-art methods. Finally, the purely\nlearning-based method may have a high potential for the adaptation to young,\nold, or diseased brains by fine-tuning the pre-trained network with a small\ntraining sample on the target application, where the availability of larger\ndatasets with manual annotations may boost the overall segmentation accuracy in\nthe future.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 08:53:31 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Wachinger", "Christian", ""], ["Reuter", "Martin", ""], ["Klein", "Tassilo", ""]]}, {"id": "1702.08222", "submitter": "Ewa Andrejczuk Ms.", "authors": "Ewa Andrejczuk, Juan A. Rodriguez-Aguilar, Carme Roig, Carles Sierra", "title": "Synergistic Team Composition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective teams are crucial for organisations, especially in environments\nthat require teams to be constantly created and dismantled, such as software\ndevelopment, scientific experiments, crowd-sourcing, or the classroom. Key\nfactors influencing team performance are competences and personality of team\nmembers. Hence, we present a computational model to compose proficient and\ncongenial teams based on individuals' personalities and their competences to\nperform tasks of different nature. With this purpose, we extend Wilde's\npost-Jungian method for team composition, which solely employs individuals'\npersonalities. The aim of this study is to create a model to partition agents\ninto teams that are balanced in competences, personality and gender. Finally,\nwe present some preliminary empirical results that we obtained when analysing\nstudent performance. Results show the benefits of a more informed team\ncomposition that exploits individuals' competences besides information about\ntheir personalities.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 10:36:36 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Andrejczuk", "Ewa", ""], ["Rodriguez-Aguilar", "Juan A.", ""], ["Roig", "Carme", ""], ["Sierra", "Carles", ""]]}, {"id": "1702.08286", "submitter": "Duncan McElfresh", "authors": "Duncan C. McElfresh and John P. Dickerson", "title": "Balancing Lexicographic Fairness and a Utilitarian Objective with\n  Application to Kidney Exchange", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Balancing fairness and efficiency in resource allocation is a classical\neconomic and computational problem. The price of fairness measures the\nworst-case loss of economic efficiency when using an inefficient but fair\nallocation rule; for indivisible goods in many settings, this price is\nunacceptably high. One such setting is kidney exchange, where needy patients\nswap willing but incompatible kidney donors. In this work, we close an open\nproblem regarding the theoretical price of fairness in modern kidney exchanges.\nWe then propose a general hybrid fairness rule that balances a strict\nlexicographic preference ordering over classes of agents, and a utilitarian\nobjective that maximizes economic efficiency. We develop a utility function for\nthis rule that favors disadvantaged groups lexicographically; but if cost to\noverall efficiency becomes too high, it switches to a utilitarian objective.\nThis rule has only one parameter which is proportional to a bound on the price\nof fairness, and can be adjusted by policymakers. We apply this rule to real\ndata from a large kidney exchange and show that our hybrid rule produces more\nreliable outcomes than other fairness rules.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 13:54:44 GMT"}, {"version": "v2", "created": "Thu, 7 Sep 2017 21:43:49 GMT"}], "update_date": "2017-09-11", "authors_parsed": [["McElfresh", "Duncan C.", ""], ["Dickerson", "John P.", ""]]}, {"id": "1702.08367", "submitter": "Fan Yang", "authors": "Fan Yang, Zhilin Yang, William W. Cohen", "title": "Differentiable Learning of Logical Rules for Knowledge Base Reasoning", "comments": "Accepted at NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of learning probabilistic first-order logical rules for\nknowledge base reasoning. This learning problem is difficult because it\nrequires learning the parameters in a continuous space as well as the structure\nin a discrete space. We propose a framework, Neural Logic Programming, that\ncombines the parameter and structure learning of first-order logical rules in\nan end-to-end differentiable model. This approach is inspired by a\nrecently-developed differentiable logic called TensorLog, where inference tasks\ncan be compiled into sequences of differentiable operations. We design a neural\ncontroller system that learns to compose these operations. Empirically, our\nmethod outperforms prior work on multiple knowledge base benchmark datasets,\nincluding Freebase and WikiMovies.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 16:44:38 GMT"}, {"version": "v2", "created": "Sun, 4 Jun 2017 04:17:58 GMT"}, {"version": "v3", "created": "Mon, 27 Nov 2017 17:50:15 GMT"}], "update_date": "2017-11-28", "authors_parsed": [["Yang", "Fan", ""], ["Yang", "Zhilin", ""], ["Cohen", "William W.", ""]]}, {"id": "1702.08400", "submitter": "Kuniaki Saito Saito Kuniaki", "authors": "Kuniaki Saito, Yoshitaka Ushiku and Tatsuya Harada", "title": "Asymmetric Tri-training for Unsupervised Domain Adaptation", "comments": "TBA on ICML2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep-layered models trained on a large number of labeled samples boost the\naccuracy of many tasks. It is important to apply such models to different\ndomains because collecting many labeled samples in various domains is\nexpensive. In unsupervised domain adaptation, one needs to train a classifier\nthat works well on a target domain when provided with labeled source samples\nand unlabeled target samples. Although many methods aim to match the\ndistributions of source and target samples, simply matching the distribution\ncannot ensure accuracy on the target domain. To learn discriminative\nrepresentations for the target domain, we assume that artificially labeling\ntarget samples can result in a good representation. Tri-training leverages\nthree classifiers equally to give pseudo-labels to unlabeled samples, but the\nmethod does not assume labeling samples generated from a different domain.In\nthis paper, we propose an asymmetric tri-training method for unsupervised\ndomain adaptation, where we assign pseudo-labels to unlabeled samples and train\nneural networks as if they are true labels. In our work, we use three networks\nasymmetrically. By asymmetric, we mean that two networks are used to label\nunlabeled target samples and one network is trained by the samples to obtain\ntarget-discriminative representations. We evaluate our method on digit\nrecognition and sentiment analysis datasets. Our proposed method achieves\nstate-of-the-art performance on the benchmark digit recognition datasets of\ndomain adaptation.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 17:48:17 GMT"}, {"version": "v2", "created": "Thu, 16 Mar 2017 15:11:14 GMT"}, {"version": "v3", "created": "Sat, 13 May 2017 05:44:03 GMT"}], "update_date": "2017-05-16", "authors_parsed": [["Saito", "Kuniaki", ""], ["Ushiku", "Yoshitaka", ""], ["Harada", "Tatsuya", ""]]}, {"id": "1702.08441", "submitter": "Lenz Belzner", "authors": "Lenz Belzner", "title": "Monte Carlo Action Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes Monte Carlo Action Programming, a programming language\nframework for autonomous systems that act in large probabilistic state spaces\nwith high branching factors. It comprises formal syntax and semantics of a\nnondeterministic action programming language. The language is interpreted\nstochastically via Monte Carlo Tree Search. Effectiveness of the approach is\nshown empirically.\n", "versions": [{"version": "v1", "created": "Sat, 25 Feb 2017 11:48:50 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Belzner", "Lenz", ""]]}, {"id": "1702.08484", "submitter": "Aditya Grover", "authors": "Aditya Grover, Stefano Ermon", "title": "Boosted Generative Models", "comments": "AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel approach for using unsupervised boosting to create an\nensemble of generative models, where models are trained in sequence to correct\nearlier mistakes. Our meta-algorithmic framework can leverage any existing base\nlearner that permits likelihood evaluation, including recent deep expressive\nmodels. Further, our approach allows the ensemble to include discriminative\nmodels trained to distinguish real data from model-generated data. We show\ntheoretical conditions under which incorporating a new model in the ensemble\nwill improve the fit and empirically demonstrate the effectiveness of our\nblack-box boosting algorithms on density estimation, classification, and sample\ngeneration on benchmark datasets for a wide range of generative models.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 19:28:40 GMT"}, {"version": "v2", "created": "Fri, 22 Dec 2017 10:13:51 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Grover", "Aditya", ""], ["Ermon", "Stefano", ""]]}, {"id": "1702.08495", "submitter": "Sebastian Benthall", "authors": "Sebastian Benthall", "title": "Don't Fear the Reaper: Refuting Bostrom's Superintelligence Argument", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In recent years prominent intellectuals have raised ethical concerns about\nthe consequences of artificial intelligence. One concern is that an autonomous\nagent might modify itself to become \"superintelligent\" and, in supremely\neffective pursuit of poorly specified goals, destroy all of humanity. This\npaper considers and rejects the possibility of this outcome. We argue that this\nscenario depends on an agent's ability to rapidly improve its ability to\npredict its environment through self-modification. Using a Bayesian model of a\nreasoning agent, we show that there are important limitations to how an agent\nmay improve its predictive ability through self-modification alone. We conclude\nthat concern about this artificial intelligence outcome is misplaced and better\ndirected at policy questions around data access and storage.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 19:57:17 GMT"}, {"version": "v2", "created": "Sat, 4 Mar 2017 20:43:32 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Benthall", "Sebastian", ""]]}, {"id": "1702.08567", "submitter": "AmirEmad Ghassami", "authors": "AmirEmad Ghassami, Saber Salehkaleybar, Negar Kiyavash", "title": "Optimal Experiment Design for Causal Discovery from Fixed Number of\n  Experiments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of causal structure learning over a set of random\nvariables when the experimenter is allowed to perform at most $M$ experiments\nin a non-adaptive manner. We consider the optimal learning strategy in terms of\nminimizing the portions of the structure that remains unknown given the limited\nnumber of experiments in both Bayesian and minimax setting. We characterize the\ntheoretical optimal solution and propose an algorithm, which designs the\nexperiments efficiently in terms of time complexity. We show that for bounded\ndegree graphs, in the minimax case and in the Bayesian case with uniform\npriors, our proposed algorithm is a $\\rho$-approximation algorithm, where\n$\\rho$ is independent of the order of the underlying graph. Simulations on both\nsynthetic and real data show that the performance of our algorithm is very\nclose to the optimal solution.\n", "versions": [{"version": "v1", "created": "Mon, 27 Feb 2017 22:30:43 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Ghassami", "AmirEmad", ""], ["Salehkaleybar", "Saber", ""], ["Kiyavash", "Negar", ""]]}, {"id": "1702.08608", "submitter": "Been Kim", "authors": "Finale Doshi-Velez and Been Kim", "title": "Towards A Rigorous Science of Interpretable Machine Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As machine learning systems become ubiquitous, there has been a surge of\ninterest in interpretable machine learning: systems that provide explanation\nfor their outputs. These explanations are often used to qualitatively assess\nother criteria such as safety or non-discrimination. However, despite the\ninterest in interpretability, there is very little consensus on what\ninterpretable machine learning is and how it should be measured. In this\nposition paper, we first define interpretability and describe when\ninterpretability is needed (and when it is not). Next, we suggest a taxonomy\nfor rigorous evaluation and expose open questions towards a more rigorous\nscience of interpretable machine learning.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 02:19:20 GMT"}, {"version": "v2", "created": "Thu, 2 Mar 2017 19:32:10 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Doshi-Velez", "Finale", ""], ["Kim", "Been", ""]]}, {"id": "1702.08626", "submitter": "Ahmed Qureshi", "authors": "Ahmed Hussain Qureshi, Yutaka Nakamura, Yuichiro Yoshikawa and Hiroshi\n  Ishiguro", "title": "Show, Attend and Interact: Perceivable Human-Robot Social Interaction\n  through Neural Attention Q-Network", "comments": "7 pages, 5 figures, accepted by IEEE-RAS ICRA'17", "journal-ref": null, "doi": "10.1109/ICRA.2017.7989193", "report-no": null, "categories": "cs.RO cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a safe, natural and effective human-robot social interaction, it is\nessential to develop a system that allows a robot to demonstrate the\nperceivable responsive behaviors to complex human behaviors. We introduce the\nMultimodal Deep Attention Recurrent Q-Network using which the robot exhibits\nhuman-like social interaction skills after 14 days of interacting with people\nin an uncontrolled real world. Each and every day during the 14 days, the\nsystem gathered robot interaction experiences with people through a\nhit-and-trial method and then trained the MDARQN on these experiences using\nend-to-end reinforcement learning approach. The results of interaction based\nlearning indicate that the robot has learned to respond to complex human\nbehaviors in a perceivable and socially acceptable manner.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 03:16:40 GMT"}], "update_date": "2018-06-19", "authors_parsed": [["Qureshi", "Ahmed Hussain", ""], ["Nakamura", "Yutaka", ""], ["Yoshikawa", "Yuichiro", ""], ["Ishiguro", "Hiroshi", ""]]}, {"id": "1702.08628", "submitter": "Isaac Sledge", "authors": "Isaac J. Sledge, Jose C. Principe", "title": "Analysis of Agent Expertise in Ms. Pac-Man using\n  Value-of-Information-based Policies", "comments": "IEEE Transactions on Computational Intelligence and Artificial\n  Intelligence in Games", "journal-ref": null, "doi": "10.1109/TG.2018.2808201", "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Conventional reinforcement learning methods for Markov decision processes\nrely on weakly-guided, stochastic searches to drive the learning process. It\ncan therefore be difficult to predict what agent behaviors might emerge. In\nthis paper, we consider an information-theoretic cost function for performing\nconstrained stochastic searches that promote the formation of risk-averse to\nrisk-favoring behaviors. This cost function is the value of information, which\nprovides the optimal trade-off between the expected return of a policy and the\npolicy's complexity; policy complexity is measured by number of bits and\ncontrolled by a single hyperparameter on the cost function. As the policy\ncomplexity is reduced, the agents will increasingly eschew risky actions. This\nreduces the potential for high accrued rewards. As the policy complexity\nincreases, the agents will take actions, regardless of the risk, that can raise\nthe long-term rewards. The obtainable reward depends on a single, tunable\nhyperparameter that regulates the degree of policy complexity.\n  We evaluate the performance of value-of-information-based policies on a\nstochastic version of Ms. Pac-Man. A major component of this paper is the\ndemonstration that ranges of policy complexity values yield different game-play\nstyles and explaining why this occurs. We also show that our\nreinforcement-learning search mechanism is more efficient than the others we\nutilize. This result implies that the value of information theory is\nappropriate for framing the exploitation-exploration trade-off in reinforcement\nlearning.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 03:23:20 GMT"}, {"version": "v2", "created": "Mon, 28 Aug 2017 14:06:44 GMT"}, {"version": "v3", "created": "Sat, 4 Nov 2017 16:15:07 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Sledge", "Isaac J.", ""], ["Principe", "Jose C.", ""]]}, {"id": "1702.08635", "submitter": "Fei Tian", "authors": "Yang Fan and Fei Tian and Tao Qin and Jiang Bian and Tie-Yan Liu", "title": "Learning What Data to Learn", "comments": "A preliminary version will appear in ICLR 2017, workshop track.\n  https://openreview.net/forum?id=SyJNmVqgg&noteId=SyJNmVqgg", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning is essentially the sciences of playing with data. An\nadaptive data selection strategy, enabling to dynamically choose different data\nat various training stages, can reach a more effective model in a more\nefficient way. In this paper, we propose a deep reinforcement learning\nframework, which we call \\emph{\\textbf{N}eural \\textbf{D}ata \\textbf{F}ilter}\n(\\textbf{NDF}), to explore automatic and adaptive data selection in the\ntraining process. In particular, NDF takes advantage of a deep neural network\nto adaptively select and filter important data instances from a sequential\nstream of training data, such that the future accumulative reward (e.g., the\nconvergence speed) is maximized. In contrast to previous studies in data\nselection that is mainly based on heuristic strategies, NDF is quite generic\nand thus can be widely suitable for many machine learning tasks. Taking neural\nnetwork training with stochastic gradient descent (SGD) as an example,\ncomprehensive experiments with respect to various neural network modeling\n(e.g., multi-layer perceptron networks, convolutional neural networks and\nrecurrent neural networks) and several applications (e.g., image classification\nand text understanding) demonstrate that NDF powered SGD can achieve comparable\naccuracy with standard SGD process by using less data and fewer iterations.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 03:52:06 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Fan", "Yang", ""], ["Tian", "Fei", ""], ["Qin", "Tao", ""], ["Bian", "Jiang", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1702.08690", "submitter": "Weifeng Ge", "authors": "Weifeng Ge, Yizhou Yu", "title": "Borrowing Treasures from the Wealthy: Deep Transfer Learning through\n  Selective Joint Fine-tuning", "comments": "To appear in 2017 IEEE Conference on Computer Vision and Pattern\n  Recognition (CVPR 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks require a large amount of labeled training data during\nsupervised learning. However, collecting and labeling so much data might be\ninfeasible in many cases. In this paper, we introduce a source-target selective\njoint fine-tuning scheme for improving the performance of deep learning tasks\nwith insufficient training data. In this scheme, a target learning task with\ninsufficient training data is carried out simultaneously with another source\nlearning task with abundant training data. However, the source learning task\ndoes not use all existing training data. Our core idea is to identify and use a\nsubset of training images from the original source learning task whose\nlow-level characteristics are similar to those from the target learning task,\nand jointly fine-tune shared convolutional layers for both tasks. Specifically,\nwe compute descriptors from linear or nonlinear filter bank responses on\ntraining images from both tasks, and use such descriptors to search for a\ndesired subset of training samples for the source learning task.\n  Experiments demonstrate that our selective joint fine-tuning scheme achieves\nstate-of-the-art performance on multiple visual classification tasks with\ninsufficient training data for deep learning. Such tasks include Caltech 256,\nMIT Indoor 67, Oxford Flowers 102 and Stanford Dogs 120. In comparison to\nfine-tuning without a source domain, the proposed method can improve the\nclassification accuracy by 2% - 10% using a single model.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 08:40:44 GMT"}, {"version": "v2", "created": "Tue, 6 Jun 2017 11:51:03 GMT"}], "update_date": "2018-03-06", "authors_parsed": [["Ge", "Weifeng", ""], ["Yu", "Yizhou", ""]]}, {"id": "1702.08725", "submitter": "Lenz Belzner", "authors": "Lenz Belzner, Thomas Gabor", "title": "Bayesian Verification under Model Uncertainty", "comments": "Accepted at SEsCPS @ ICSE 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning enables systems to build and update domain models based on\nruntime observations. In this paper, we study statistical model checking and\nruntime verification for systems with this ability. Two challenges arise: (1)\nModels built from limited runtime data yield uncertainty to be dealt with. (2)\nThere is no definition of satisfaction w.r.t. uncertain hypotheses. We propose\nsuch a definition of subjective satisfaction based on recently introduced\nsatisfaction functions. We also propose the BV algorithm as a Bayesian solution\nto runtime verification of subjective satisfaction under model uncertainty. BV\nprovides user-definable stochastic bounds for type I and II errors. We discuss\nempirical results from an example application to illustrate our ideas.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 10:14:30 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Belzner", "Lenz", ""], ["Gabor", "Thomas", ""]]}, {"id": "1702.08726", "submitter": "Lenz Belzner", "authors": "Lenz Belzner, Thomas Gabor", "title": "Stacked Thompson Bandits", "comments": "Accepted at SEsCPS @ ICSE 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce Stacked Thompson Bandits (STB) for efficiently generating plans\nthat are likely to satisfy a given bounded temporal logic requirement. STB uses\na simulation for evaluation of plans, and takes a Bayesian approach to using\nthe resulting information to guide its search. In particular, we show that\nstacking multiarmed bandits and using Thompson sampling to guide the action\nselection process for each bandit enables STB to generate plans that satisfy\nrequirements with a high probability while only searching a fraction of the\nsearch space.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 10:19:30 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Belzner", "Lenz", ""], ["Gabor", "Thomas", ""]]}, {"id": "1702.08736", "submitter": "Roxana R\\u{a}dulescu", "authors": "Roxana R\\u{a}dulescu, Peter Vrancx and Ann Now\\'e", "title": "Analysing Congestion Problems in Multi-agent Reinforcement Learning", "comments": "Adaptive Learning Agents (ALA) Workshop at AAMAS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Congestion problems are omnipresent in today's complex networks and represent\na challenge in many research domains. In the context of Multi-agent\nReinforcement Learning (MARL), approaches like difference rewards and resource\nabstraction have shown promising results in tackling such problems. Resource\nabstraction was shown to be an ideal candidate for solving large-scale resource\nallocation problems in a fully decentralized manner. However, its performance\nand applicability strongly depends on some, until now, undocumented\nassumptions. Two of the main congestion benchmark problems considered in the\nliterature are: the Beach Problem Domain and the Traffic Lane Domain. In both\nsettings the highest system utility is achieved when overcrowding one resource\nand keeping the rest at optimum capacity. We analyse how abstract grouping can\npromote this behaviour and how feasible it is to apply this approach in a\nreal-world domain (i.e., what assumptions need to be satisfied and what\nknowledge is necessary). We introduce a new test problem, the Road Network\nDomain (RND), where the resources are no longer independent, but rather part of\na network (e.g., road network), thus choosing one path will also impact the\nload on other paths having common road segments. We demonstrate the application\nof state-of-the-art MARL methods for this new congestion model and analyse\ntheir performance. RND allows us to highlight an important limitation of\nresource abstraction and show that the difference rewards approach manages to\nbetter capture and inform the agents about the dynamics of the environment.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 10:49:36 GMT"}, {"version": "v2", "created": "Thu, 30 Mar 2017 12:10:35 GMT"}], "update_date": "2017-03-31", "authors_parsed": [["R\u0103dulescu", "Roxana", ""], ["Vrancx", "Peter", ""], ["Now\u00e9", "Ann", ""]]}, {"id": "1702.08745", "submitter": "Paulo Adeodato Prof.", "authors": "Paulo J. L. Adeodato, F\\'abio C. Pereira and Rosalvo F. Oliveira Neto", "title": "Optimal Categorical Attribute Transformation for Granularity Change in\n  Relational Databases for Binary Decision Problems in Educational Data Mining", "comments": "5 pages, 2 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an approach for transforming data granularity in\nhierarchical databases for binary decision problems by applying regression to\ncategorical attributes at the lower grain levels. Attributes from a lower\nhierarchy entity in the relational database have their information content\noptimized through regression on the categories histogram trained on a small\nexclusive labelled sample, instead of the usual mode category of the\ndistribution. The paper validates the approach on a binary decision task for\nassessing the quality of secondary schools focusing on how logistic regression\ntransforms the students and teachers attributes into school attributes.\nExperiments were carried out on Brazilian schools public datasets via 10-fold\ncross-validation comparison of the ranking score produced also by logistic\nregression. The proposed approach achieved higher performance than the usual\ndistribution mode transformation and equal to the expert weighing approach\nmeasured by the maximum Kolmogorov-Smirnov distance and the area under the ROC\ncurve at 0.01 significance level.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 11:13:17 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Adeodato", "Paulo J. L.", ""], ["Pereira", "F\u00e1bio C.", ""], ["Neto", "Rosalvo F. Oliveira", ""]]}, {"id": "1702.08791", "submitter": "Matthew Staib", "authors": "Matthew Staib and Stefanie Jegelka", "title": "Robust Budget Allocation via Continuous Submodular Functions", "comments": "ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS cs.SI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The optimal allocation of resources for maximizing influence, spread of\ninformation or coverage, has gained attention in the past years, in particular\nin machine learning and data mining. But in applications, the parameters of the\nproblem are rarely known exactly, and using wrong parameters can lead to\nundesirable outcomes. We hence revisit a continuous version of the Budget\nAllocation or Bipartite Influence Maximization problem introduced by Alon et\nal. (2012) from a robust optimization perspective, where an adversary may\nchoose the least favorable parameters within a confidence set. The resulting\nproblem is a nonconvex-concave saddle point problem (or game). We show that\nthis nonconvex problem can be solved exactly by leveraging connections to\ncontinuous submodular functions, and by solving a constrained submodular\nminimization problem. Although constrained submodular minimization is hard in\ngeneral, here, we establish conditions under which such a problem can be solved\nto arbitrary precision $\\epsilon$.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 14:07:42 GMT"}, {"version": "v2", "created": "Tue, 13 Jun 2017 15:24:28 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Staib", "Matthew", ""], ["Jegelka", "Stefanie", ""]]}, {"id": "1702.08862", "submitter": "Palash Dey", "authors": "Palash Dey, Nimrod Talmon, and Otniel van Handel", "title": "Proportional Representation in Vote Streams", "comments": "Accepted as a full paper in AAMAS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.CC cs.DS cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider elections where the voters come one at a time, in a streaming\nfashion, and devise space-efficient algorithms which identify an approximate\nwinning committee with respect to common multiwinner proportional\nrepresentation voting rules; specifically, we consider the Approval-based and\nthe Borda-based variants of both the Chamberlin-- ourant rule and the Monroe\nrule. We complement our algorithms with lower bounds. Somewhat surprisingly,\nour results imply that, using space which does not depend on the number of\nvoters it is possible to efficiently identify an approximate representative\ncommittee of fixed size over vote streams with huge number of voters.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 16:57:49 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Dey", "Palash", ""], ["Talmon", "Nimrod", ""], ["van Handel", "Otniel", ""]]}, {"id": "1702.08887", "submitter": "Nantas Nardelli", "authors": "Jakob Foerster, Nantas Nardelli, Gregory Farquhar, Triantafyllos\n  Afouras, Philip H. S. Torr, Pushmeet Kohli, Shimon Whiteson", "title": "Stabilising Experience Replay for Deep Multi-Agent Reinforcement\n  Learning", "comments": "Camera-ready version, International Conference of Machine Learning\n  2017; updated to fix print-breaking image", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world problems, such as network packet routing and urban traffic\ncontrol, are naturally modeled as multi-agent reinforcement learning (RL)\nproblems. However, existing multi-agent RL methods typically scale poorly in\nthe problem size. Therefore, a key challenge is to translate the success of\ndeep learning on single-agent RL to the multi-agent setting. A major stumbling\nblock is that independent Q-learning, the most popular multi-agent RL method,\nintroduces nonstationarity that makes it incompatible with the experience\nreplay memory on which deep Q-learning relies. This paper proposes two methods\nthat address this problem: 1) using a multi-agent variant of importance\nsampling to naturally decay obsolete data and 2) conditioning each agent's\nvalue function on a fingerprint that disambiguates the age of the data sampled\nfrom the replay memory. Results on a challenging decentralised variant of\nStarCraft unit micromanagement confirm that these methods enable the successful\ncombination of experience replay with multi-agent RL.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 17:56:41 GMT"}, {"version": "v2", "created": "Mon, 12 Jun 2017 22:00:56 GMT"}, {"version": "v3", "created": "Mon, 21 May 2018 08:24:02 GMT"}], "update_date": "2018-05-22", "authors_parsed": [["Foerster", "Jakob", ""], ["Nardelli", "Nantas", ""], ["Farquhar", "Gregory", ""], ["Afouras", "Triantafyllos", ""], ["Torr", "Philip H. S.", ""], ["Kohli", "Pushmeet", ""], ["Whiteson", "Shimon", ""]]}, {"id": "1702.08892", "submitter": "Ofir Nachum", "authors": "Ofir Nachum, Mohammad Norouzi, Kelvin Xu, Dale Schuurmans", "title": "Bridging the Gap Between Value and Policy Based Reinforcement Learning", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish a new connection between value and policy based reinforcement\nlearning (RL) based on a relationship between softmax temporal value\nconsistency and policy optimality under entropy regularization. Specifically,\nwe show that softmax consistent action values correspond to optimal entropy\nregularized policy probabilities along any action sequence, regardless of\nprovenance. From this observation, we develop a new RL algorithm, Path\nConsistency Learning (PCL), that minimizes a notion of soft consistency error\nalong multi-step action sequences extracted from both on- and off-policy\ntraces. We examine the behavior of PCL in different scenarios and show that PCL\ncan be interpreted as generalizing both actor-critic and Q-learning algorithms.\nWe subsequently deepen the relationship by showing how a single model can be\nused to represent both a policy and the corresponding softmax state values,\neliminating the need for a separate critic. The experimental evaluation\ndemonstrates that PCL significantly outperforms strong actor-critic and\nQ-learning baselines across several benchmarks.\n", "versions": [{"version": "v1", "created": "Tue, 28 Feb 2017 18:06:15 GMT"}, {"version": "v2", "created": "Thu, 8 Jun 2017 19:31:32 GMT"}, {"version": "v3", "created": "Wed, 22 Nov 2017 23:11:20 GMT"}], "update_date": "2017-11-27", "authors_parsed": [["Nachum", "Ofir", ""], ["Norouzi", "Mohammad", ""], ["Xu", "Kelvin", ""], ["Schuurmans", "Dale", ""]]}]