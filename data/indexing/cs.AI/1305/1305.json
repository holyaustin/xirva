[{"id": "1305.0187", "submitter": "Chantal Cherifi", "authors": "Chantal Cherifi, Yvan Rivierre, Jean-Francois Santucci", "title": "A Community Based Algorithm for Large Scale Web Service Composition", "comments": null, "journal-ref": "Cherifi, C., Y. Rivierre, Santucci, J.F.: A Community Based\n  Algorithm for Large Scale Web Service Composition. In Journal of Convergence\n  Information Technology (JCIT), Vol.8 N4 pp. 148-157, (2013)", "doi": null, "report-no": null, "categories": "cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Web service composition is the process of synthesizing a new composite\nservice using a set of available Web services in order to satisfy a client\nrequest that cannot be treated by any available Web services. The Web services\nspace is a dynamic environment characterized by a huge number of elements.\nFurthermore, many Web services are offering similar functionalities. In this\npaper we propose a model for Web service composition designed to address the\nscale effect and the redundancy issue. The Web services space is represented by\na two-layered network architecture. A concrete similarity network layer\norganizes the Web services operations into communities of functionally similar\noperations. An abstract interaction network layer represents the composition\nrelationships between the sets of communities. Composition synthesis is\nperformed by a two-phased graph search algorithm. First, the interaction\nnetwork is mined in order to discover abstract solutions to the request goal.\nThen, the abstract compositions are instantiated with concrete operations\nselected from the similarity network. This strategy allows an efficient\nexploration of the Web services space. Furthermore, operations grouped in a\ncommunity can be easily substituted if necessary during the composition's\nsynthesis's process.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2013 14:44:22 GMT"}], "update_date": "2013-05-02", "authors_parsed": [["Cherifi", "Chantal", ""], ["Rivierre", "Yvan", ""], ["Santucci", "Jean-Francois", ""]]}, {"id": "1305.0191", "submitter": "Chantal Cherifi", "authors": "Chantal Cherifi, Vincent Labatut, Jean-Fran\\c{c}ois Santucci", "title": "Benefits of Semantics on Web Service Composition from a Complex Network\n  Perspective", "comments": null, "journal-ref": "In International Conference on Networked Digital Technologies, pp\n  80-90, Springer CCIS, Czech Republic (2010)", "doi": "10.1007/978-3-642-14306-9_9", "report-no": null, "categories": "cs.SI cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of publicly available Web services (WS) is continuously growing,\nand in parallel, we are witnessing a rapid development in semantic-related web\ntechnologies. The intersection of the semantic web and WS allows the\ndevelopment of semantic WS. In this work, we adopt a complex network\nperspective to perform a comparative analysis of the syntactic and semantic\napproaches used to describe WS. From a collection of publicly available WS\ndescriptions, we extract syntactic and semantic WS interaction networks. We\ntake advantage of tools from the complex network field to analyze them and\ndetermine their properties. We show that WS interaction networks exhibit some\nof the typical characteristics observed in real-world networks, such as short\naverage distance between nodes and community structure. By comparing syntactic\nand semantic networks through their properties, we show the introduction of\nsemantics in WS descriptions should improve the composition process.\n", "versions": [{"version": "v1", "created": "Wed, 1 May 2013 14:56:33 GMT"}], "update_date": "2015-02-04", "authors_parsed": [["Cherifi", "Chantal", ""], ["Labatut", "Vincent", ""], ["Santucci", "Jean-Fran\u00e7ois", ""]]}, {"id": "1305.0423", "submitter": "Somayeh Danafar", "authors": "Somayeh Danafar, Paola M.V. Rancoita, Tobias Glasmachers, Kevin\n  Whittingstall, Juergen Schmidhuber", "title": "Testing Hypotheses by Regularized Maximum Mean Discrepancy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Do two data samples come from different distributions? Recent studies of this\nfundamental problem focused on embedding probability distributions into\nsufficiently rich characteristic Reproducing Kernel Hilbert Spaces (RKHSs), to\ncompare distributions by the distance between their embeddings. We show that\nRegularized Maximum Mean Discrepancy (RMMD), our novel measure for kernel-based\nhypothesis testing, yields substantial improvements even when sample sizes are\nsmall, and excels at hypothesis tests involving multiple comparisons with power\ncontrol. We derive asymptotic distributions under the null and alternative\nhypotheses, and assess power control. Outstanding results are obtained on:\nchallenging EEG data, MNIST, the Berkley Covertype, and the Flare-Solar\ndataset.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2013 13:03:53 GMT"}], "update_date": "2013-05-03", "authors_parsed": [["Danafar", "Somayeh", ""], ["Rancoita", "Paola M. V.", ""], ["Glasmachers", "Tobias", ""], ["Whittingstall", "Kevin", ""], ["Schmidhuber", "Juergen", ""]]}, {"id": "1305.0574", "submitter": "Lakhdar Sais", "authors": "Said Jabbour and Lakhdar Sais and Yakoub Salhi", "title": "Extending Modern SAT Solvers for Enumerating All Models", "comments": "This paper is withdrawn by the authors due to a missing reference.\n  The authors work further on this issue and conduct exhaustive experimental\n  comparison with other related works", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of enumerating all models of a Boolean\nformula in conjunctive normal form (CNF). We propose an extension of CDCL-based\nSAT solvers to deal with this fundamental problem. Then, we provide an\nexperimental evaluation of our proposed SAT model enumeration algorithms on\nboth satisfiable SAT instances taken from the last SAT challenge and on\ninstances from the SAT-based encoding of sequence mining problems.\n", "versions": [{"version": "v1", "created": "Thu, 2 May 2013 20:37:29 GMT"}, {"version": "v2", "created": "Mon, 6 May 2013 20:45:25 GMT"}], "update_date": "2013-05-08", "authors_parsed": [["Jabbour", "Said", ""], ["Sais", "Lakhdar", ""], ["Salhi", "Yakoub", ""]]}, {"id": "1305.0626", "submitter": "Fuqiang Chen", "authors": "Fuqiang Chen", "title": "An Improved EM algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we firstly give a brief introduction of expectation\nmaximization (EM) algorithm, and then discuss the initial value sensitivity of\nexpectation maximization algorithm. Subsequently, we give a short proof of EM's\nconvergence. Then, we implement experiments with the expectation maximization\nalgorithm (We implement all the experiments on Gaussion mixture model (GMM)).\nOur experiment with expectation maximization is performed in the following\nthree cases: initialize randomly; initialize with result of K-means; initialize\nwith result of K-medoids. The experiment result shows that expectation\nmaximization algorithm depend on its initial state or parameters. And we found\nthat EM initialized with K-medoids performed better than both the one\ninitialized with K-means and the one initialized randomly.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2013 06:25:41 GMT"}], "update_date": "2013-05-06", "authors_parsed": [["Chen", "Fuqiang", ""]]}, {"id": "1305.0751", "submitter": "Jose M. Pe\\~na", "authors": "Jose M. Pe\\~na", "title": "Marginal AMP Chain Graphs", "comments": "Changes from v1 to v2: Discussion section got extended. Changes from\n  v2 to v3: New Sections 3 and 5. Changes from v3 to v4: Example 4 added to\n  discussion section. Changes from v4 to v5: None. Changes from v5 to v6: Some\n  minor and major errors have been corrected. The latter include the\n  definitions of descending route and pairwise separation base, and the proofs\n  of Theorems 5 and 6", "journal-ref": "International Journal of Approximate Reasoning, 55 (5), 1185-1206,\n  2014", "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new family of models that is based on graphs that may have\nundirected, directed and bidirected edges. We name these new models marginal\nAMP (MAMP) chain graphs because each of them is Markov equivalent to some AMP\nchain graph under marginalization of some of its nodes. However, MAMP chain\ngraphs do not only subsume AMP chain graphs but also multivariate regression\nchain graphs. We describe global and pairwise Markov properties for MAMP chain\ngraphs and prove their equivalence for compositional graphoids. We also\ncharacterize when two MAMP chain graphs are Markov equivalent.\n  For Gaussian probability distributions, we also show that every MAMP chain\ngraph is Markov equivalent to some directed and acyclic graph with\ndeterministic nodes under marginalization and conditioning on some of its\nnodes. This is important because it implies that the independence model\nrepresented by a MAMP chain graph can be accounted for by some data generating\nprocess that is partially observed and has selection bias. Finally, we modify\nMAMP chain graphs so that they are closed under marginalization for Gaussian\nprobability distributions. This is a desirable feature because it guarantees\nparsimonious models under marginalization.\n", "versions": [{"version": "v1", "created": "Fri, 3 May 2013 15:35:50 GMT"}, {"version": "v2", "created": "Thu, 16 May 2013 13:08:01 GMT"}, {"version": "v3", "created": "Thu, 3 Oct 2013 12:16:24 GMT"}, {"version": "v4", "created": "Wed, 26 Feb 2014 15:50:00 GMT"}, {"version": "v5", "created": "Sat, 8 Mar 2014 12:42:39 GMT"}, {"version": "v6", "created": "Fri, 7 Nov 2014 10:02:12 GMT"}], "update_date": "2014-11-10", "authors_parsed": [["Pe\u00f1a", "Jose M.", ""]]}, {"id": "1305.1060", "submitter": "Gian Luca Pozzato", "authors": "Laura Giordano and Valentina Gliozzi and Nicola Olivetti and Gian Luca\n  Pozzato", "title": "On Rational Closure in Description Logics of Typicality", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We define the notion of rational closure in the context of Description Logics\nextended with a tipicality operator. We start from ALC+T, an extension of ALC\nwith a typicality operator T: intuitively allowing to express concepts of the\nform T(C), meant to select the \"most normal\" instances of a concept C. The\nsemantics we consider is based on rational model. But we further restrict the\nsemantics to minimal models, that is to say, to models that minimise the rank\nof domain elements. We show that this semantics captures exactly a notion of\nrational closure which is a natural extension to Description Logics of Lehmann\nand Magidor's original one. We also extend the notion of rational closure to\nthe Abox component. We provide an ExpTime algorithm for computing the rational\nclosure of an Abox and we show that it is sound and complete with respect to\nthe minimal model semantics.\n", "versions": [{"version": "v1", "created": "Sun, 5 May 2013 22:32:16 GMT"}], "update_date": "2013-05-07", "authors_parsed": [["Giordano", "Laura", ""], ["Gliozzi", "Valentina", ""], ["Olivetti", "Nicola", ""], ["Pozzato", "Gian Luca", ""]]}, {"id": "1305.1169", "submitter": "Marc Schoenauer", "authors": "Mostepha Redouane Khouadjia (INRIA Saclay - Ile de France), Marc\n  Schoenauer (INRIA Saclay - Ile de France, LRI), Vincent Vidal (DCSD), Johann\n  Dr\\'eo (TRT), Pierre Sav\\'eant (TRT)", "title": "Multi-Objective AI Planning: Comparing Aggregation and Pareto Approaches", "comments": null, "journal-ref": "EvoCOP -- 13th European Conference on Evolutionary Computation in\n  Combinatorial Optimisation 7832 (2013) 202-213", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most real-world Planning problems are multi-objective, trying to minimize\nboth the makespan of the solution plan, and some cost of the actions involved\nin the plan. But most, if not all existing approaches are based on\nsingle-objective planners, and use an aggregation of the objectives to remain\nin the single-objective context. Divide and Evolve (DaE) is an evolutionary\nplanner that won the temporal deterministic satisficing track at the last\nInternational Planning Competitions (IPC). Like all Evolutionary Algorithms\n(EA), it can easily be turned into a Pareto-based Multi-Objective EA. It is\nhowever important to validate the resulting algorithm by comparing it with the\naggregation approach: this is the goal of this paper. The comparative\nexperiments on a recently proposed benchmark set that are reported here\ndemonstrate the usefulness of going Pareto-based in AI Planning.\n", "versions": [{"version": "v1", "created": "Mon, 6 May 2013 12:53:25 GMT"}], "update_date": "2013-05-07", "authors_parsed": [["Khouadjia", "Mostepha Redouane", "", "INRIA Saclay - Ile de France"], ["Schoenauer", "Marc", "", "INRIA Saclay - Ile de France, LRI"], ["Vidal", "Vincent", "", "DCSD"], ["Dr\u00e9o", "Johann", "", "TRT"], ["Sav\u00e9ant", "Pierre", "", "TRT"]]}, {"id": "1305.1578", "submitter": "Adi Makmal", "authors": "Julian Mautner, Adi Makmal, Daniel Manzano, Markus Tiersch and Hans J.\n  Briegel", "title": "Projective simulation for classical learning agents: a comprehensive\n  investigation", "comments": "Accepted for publication in New Generation Computing. 23 pages, 23\n  figures", "journal-ref": "New Generation Computing 33(1), 69-114 (2015)", "doi": "10.1007/s00354-015-0102-0", "report-no": null, "categories": "nlin.AO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the model of projective simulation (PS), a novel approach to\nartificial intelligence based on stochastic processing of episodic memory which\nwas recently introduced [H.J. Briegel and G. De las Cuevas. Sci. Rep. 2, 400,\n(2012)]. Here we provide a detailed analysis of the model and examine its\nperformance, including its achievable efficiency, its learning times and the\nway both properties scale with the problems' dimension. In addition, we situate\nthe PS agent in different learning scenarios, and study its learning abilities.\nA variety of new scenarios are being considered, thereby demonstrating the\nmodel's flexibility. Furthermore, to put the PS scheme in context, we compare\nits performance with those of Q-learning and learning classifier systems, two\npopular models in the field of reinforcement learning. It is shown that PS is a\ncompetitive artificial intelligence model of unique properties and strengths.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2013 16:43:59 GMT"}, {"version": "v2", "created": "Mon, 1 Dec 2014 14:47:46 GMT"}], "update_date": "2015-02-12", "authors_parsed": [["Mautner", "Julian", ""], ["Makmal", "Adi", ""], ["Manzano", "Daniel", ""], ["Tiersch", "Markus", ""], ["Briegel", "Hans J.", ""]]}, {"id": "1305.1655", "submitter": "Jose Hernandez-Orallo", "authors": "Jose Hernandez-Orallo", "title": "A short note on estimating intelligence from user profiles in the\n  context of universal psychometrics: prospects and caveats", "comments": "Keywords: intelligence; user profiles; cognitive abilities; social\n  networks; universal psychometrics; games; virtual worlds", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been an increasing interest in inferring some personality traits\nfrom users and players in social networks and games, respectively. This goes\nbeyond classical sentiment analysis, and also much further than customer\nprofiling. The purpose here is to have a characterisation of users in terms of\npersonality traits, such as openness, conscientiousness, extraversion,\nagreeableness, and neuroticism. While this is an incipient area of research, we\nask the question of whether cognitive abilities, and intelligence in\nparticular, are also measurable from user profiles. However, we pose the\nquestion as broadly as possible in terms of subjects, in the context of\nuniversal psychometrics, including humans, machines and hybrids. Namely, in\nthis paper we analyse the following question: is it possible to measure the\nintelligence of humans and (non-human) bots in a social network or a game just\nfrom their user profiles, i.e., by observation, without the use of interactive\ntests, such as IQ tests, the Turing test or other more principled machine\nintelligence tests?\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2013 21:39:57 GMT"}], "update_date": "2013-05-09", "authors_parsed": [["Hernandez-Orallo", "Jose", ""]]}, {"id": "1305.1679", "submitter": "Thiago Christiano Silva", "authors": "Thiago Christiano Silva and Liang Zhao", "title": "High Level Pattern Classification via Tourist Walks in Networks", "comments": "Submitted to the IEEE Transactions on Neural Networks and Learning\n  Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex networks refer to large-scale graphs with nontrivial connection\npatterns. The salient and interesting features that the complex network study\noffer in comparison to graph theory are the emphasis on the dynamical\nproperties of the networks and the ability of inherently uncovering pattern\nformation of the vertices. In this paper, we present a hybrid data\nclassification technique combining a low level and a high level classifier. The\nlow level term can be equipped with any traditional classification techniques,\nwhich realize the classification task considering only physical features (e.g.,\ngeometrical or statistical features) of the input data. On the other hand, the\nhigh level term has the ability of detecting data patterns with semantic\nmeanings. In this way, the classification is realized by means of the\nextraction of the underlying network's features constructed from the input\ndata. As a result, the high level classification process measures the\ncompliance of the test instances with the pattern formation of the training\ndata. Out of various high level perspectives that can be utilized to capture\nsemantic meaning, we utilize the dynamical features that are generated from a\ntourist walker in a networked environment. Specifically, a weighted combination\nof transient and cycle lengths generated by the tourist walk is employed for\nthat end. Interestingly, our study shows that the proposed technique is able to\nfurther improve the already optimized performance of traditional classification\ntechniques.\n", "versions": [{"version": "v1", "created": "Tue, 7 May 2013 23:40:08 GMT"}], "update_date": "2013-05-09", "authors_parsed": [["Silva", "Thiago Christiano", ""], ["Zhao", "Liang", ""]]}, {"id": "1305.1690", "submitter": "Nicholas Downing", "authors": "Nicholas Downing and Thibaut Feydy and Peter J. Stuckey", "title": "Unsatisfiable Cores for Constraint Programming", "comments": "Submitted to CP2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constraint Programming (CP) solvers typically tackle optimization problems by\nrepeatedly finding solutions to a problem while placing tighter and tighter\nbounds on the solution cost. This approach is somewhat naive, especially for\nsoft-constraint optimization problems in which the soft constraints are mostly\nsatisfied. Unsatisfiable-core approaches to solving soft constraint problems in\nBoolean Satisfiability (e.g. MAXSAT) force all soft constraints to hold\ninitially. When solving fails they return an unsatisfiable core, as a set of\nsoft constraints that cannot hold simultaneously. Using this information the\nproblem is relaxed to allow certain soft constraint(s) to be violated and\nsolving continues. Since Lazy Clause Generation (LCG) solvers can also return\nunsatisfiable cores we can adapt the MAXSAT unsatisfiable core approach to CP.\nWe implement the original MAXSAT unsatisfiable core solving algorithms WPM1,\nMSU3 in a state-of-the-art LCG solver and show that there exist problems which\nbenefit from this hybrid approach.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2013 01:50:05 GMT"}], "update_date": "2013-05-09", "authors_parsed": [["Downing", "Nicholas", ""], ["Feydy", "Thibaut", ""], ["Stuckey", "Peter J.", ""]]}, {"id": "1305.1704", "submitter": "Lei Li", "authors": "Yusuf Erol and Lei Li and Bharath Ramsundar and Stuart J. Russell", "title": "The Extended Parameter Filter", "comments": null, "journal-ref": "ICML 2013", "doi": null, "report-no": "UCB/EECS-2013-48", "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The parameters of temporal models, such as dynamic Bayesian networks, may be\nmodelled in a Bayesian context as static or atemporal variables that influence\ntransition probabilities at every time step. Particle filters fail for models\nthat include such variables, while methods that use Gibbs sampling of parameter\nvariables may incur a per-sample cost that grows linearly with the length of\nthe observation sequence. Storvik devised a method for incremental computation\nof exact sufficient statistics that, for some cases, reduces the per-sample\ncost to a constant. In this paper, we demonstrate a connection between\nStorvik's filter and a Kalman filter in parameter space and establish more\ngeneral conditions under which Storvik's filter works. Drawing on an analogy to\nthe extended Kalman filter, we develop and analyze, both theoretically and\nexperimentally, a Taylor approximation to the parameter posterior that allows\nStorvik's method to be applied to a broader class of models. Our experiments on\nboth synthetic examples and real applications show improvement over existing\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2013 03:21:31 GMT"}], "update_date": "2013-05-09", "authors_parsed": [["Erol", "Yusuf", ""], ["Li", "Lei", ""], ["Ramsundar", "Bharath", ""], ["Russell", "Stuart J.", ""]]}, {"id": "1305.1946", "submitter": "Luca Mazzola", "authors": "Elena Camossi, Paola Villa, Luca Mazzola", "title": "Semantic-based Anomalous Pattern Discovery in Moving Object Trajectories", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we investigate a novel semantic approach for pattern discovery\nin trajectories that, relying on ontologies, enhances object movement\ninformation with event semantics. The approach can be applied to the detection\nof movement patterns and behaviors whenever the semantics of events occurring\nalong the trajectory is, explicitly or implicitly, available. In particular, we\ntested it against an exacting case scenario in maritime surveillance, i.e., the\ndiscovery of suspicious container transportations.\n  The methodology we have developed entails the formalization of the\napplication domain through a domain ontology, extending the Moving Object\nOntology (MOO) described in this paper. Afterwards, movement patterns have to\nbe formalized, either as Description Logic (DL) axioms or queries, enabling the\nretrieval of the trajectories that follow the patterns.\n  In our experimental evaluation, we have considered a real world dataset of 18\nMillion of container events describing the deed undertaken in a port to\naccomplish the shipping (e.g., loading on a vessel, export operation).\nLeveraging events, we have reconstructed almost 300 thousand container\ntrajectories referring to 50 thousand containers travelling along three years.\nWe have formalized the anomalous itinerary patterns as DL axioms, testing\ndifferent ontology APIs and DL reasoners to retrieve the suspicious\ntransportations.\n  Our experiments demonstrate that the approach is feasible and efficient. In\nparticular, the joint use of Pellet and SPARQL-DL enables to detect the\ntrajectories following a given pattern in a reasonable time with big size\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2013 20:14:03 GMT"}], "update_date": "2013-05-10", "authors_parsed": [["Camossi", "Elena", ""], ["Villa", "Paola", ""], ["Mazzola", "Luca", ""]]}, {"id": "1305.1958", "submitter": "Carlos Gershenson", "authors": "Tom Froese, Carlos Gershenson, and David A. Rosenblueth", "title": "The Dynamically Extended Mind -- A Minimal Modeling Case Study", "comments": "8 pages, accepted in Congress on Evolutionary Computation IEEE CEC\n  2013, Evolutionary Robotics track", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extended mind hypothesis has stimulated much interest in cognitive\nscience. However, its core claim, i.e. that the process of cognition can extend\nbeyond the brain via the body and into the environment, has been heavily\ncriticized. A prominent critique of this claim holds that when some part of the\nworld is coupled to a cognitive system this does not necessarily entail that\nthe part is also constitutive of that cognitive system. This critique is known\nas the \"coupling-constitution fallacy\". In this paper we respond to this\nreductionist challenge by using an evolutionary robotics approach to create a\nminimal model of two acoustically coupled agents. We demonstrate how the\ninteraction process as a whole has properties that cannot be reduced to the\ncontributions of the isolated agents. We also show that the neural dynamics of\nthe coupled agents has formal properties that are inherently impossible for\nthose neural networks in isolation. By keeping the complexity of the model to\nan absolute minimum, we are able to illustrate how the coupling-constitution\nfallacy is in fact based on an inadequate understanding of the constitutive\nrole of nonlinear interactions in dynamical systems theory.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2013 20:47:18 GMT"}], "update_date": "2013-05-10", "authors_parsed": [["Froese", "Tom", ""], ["Gershenson", "Carlos", ""], ["Rosenblueth", "David A.", ""]]}, {"id": "1305.1961", "submitter": "Jonathan Yedidia Dr.", "authors": "Nate Derbinsky, Jos\\'e Bento, Veit Elser, and Jonathan S. Yedidia", "title": "An Improved Three-Weight Message-Passing Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS math.OC physics.comp-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe how the powerful \"Divide and Concur\" algorithm for constraint\nsatisfaction can be derived as a special case of a message-passing version of\nthe Alternating Direction Method of Multipliers (ADMM) algorithm for convex\noptimization, and introduce an improved message-passing algorithm based on\nADMM/DC by introducing three distinct weights for messages, with \"certain\" and\n\"no opinion\" weights, as well as the standard weight used in ADMM/DC. The\n\"certain\" messages allow our improved algorithm to implement constraint\npropagation as a special case, while the \"no opinion\" messages speed\nconvergence for some problems by making the algorithm focus only on active\nconstraints. We describe how our three-weight version of ADMM/DC can give\ngreatly improved performance for non-convex problems such as circle packing and\nsolving large Sudoku puzzles, while retaining the exact performance of ADMM for\nconvex problems. We also describe the advantages of our algorithm compared to\nother message-passing algorithms based upon belief propagation.\n", "versions": [{"version": "v1", "created": "Wed, 8 May 2013 21:24:14 GMT"}], "update_date": "2013-05-10", "authors_parsed": [["Derbinsky", "Nate", ""], ["Bento", "Jos\u00e9", ""], ["Elser", "Veit", ""], ["Yedidia", "Jonathan S.", ""]]}, {"id": "1305.1991", "submitter": "Jose Hernandez-Orallo", "authors": "David L. Dowe, Jose Hernandez-Orallo", "title": "On the universality of cognitive tests", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The analysis of the adaptive behaviour of many different kinds of systems\nsuch as humans, animals and machines, requires more general ways of assessing\ntheir cognitive abilities. This need is strengthened by increasingly more tasks\nbeing analysed for and completed by a wider diversity of systems, including\nswarms and hybrids. The notion of universal test has recently emerged in the\ncontext of machine intelligence evaluation as a way to define and use the same\ncognitive test for a variety of systems, using some principled tasks and\nadapting the interface to each particular subject. However, how far can\nuniversal tests be taken? This paper analyses this question in terms of\nsubjects, environments, space-time resolution, rewards and interfaces. This\nleads to a number of findings, insights and caveats, according to several\nlevels where universal tests may be progressively more difficult to conceive,\nimplement and administer. One of the most significant contributions is given by\nthe realisation that more universal tests are defined as maximisations of less\nuniversal tests for a variety of configurations. This means that universal\ntests must be necessarily adaptive.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2013 01:46:38 GMT"}], "update_date": "2013-05-10", "authors_parsed": [["Dowe", "David L.", ""], ["Hernandez-Orallo", "Jose", ""]]}, {"id": "1305.2038", "submitter": "Patrick Meyer E.", "authors": "Patrick E. Meyer", "title": "A Rank Minrelation - Majrelation Coefficient", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Improving the detection of relevant variables using a new bivariate measure\ncould importantly impact variable selection and large network inference\nmethods. In this paper, we propose a new statistical coefficient that we call\nthe rank minrelation coefficient. We define a minrelation of X to Y (or\nequivalently a majrelation of Y to X) as a measure that estimate p(Y > X) when\nX and Y are continuous random variables. The approach is similar to Lin's\nconcordance coefficient that rather focuses on estimating p(X = Y). In other\nwords, if a variable X exhibits a minrelation to Y then, as X increases, Y is\nlikely to increases too. However, on the contrary to concordance or\ncorrelation, the minrelation is not symmetric. More explicitly, if X decreases,\nlittle can be said on Y values (except that the uncertainty on Y actually\nincreases). In this paper, we formally define this new kind of bivariate\ndependencies and propose a new statistical coefficient in order to detect those\ndependencies. We show through several key examples that this new coefficient\nhas many interesting properties in order to select relevant variables, in\nparticular when compared to correlation.\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2013 09:09:19 GMT"}], "update_date": "2013-05-10", "authors_parsed": [["Meyer", "Patrick E.", ""]]}, {"id": "1305.2218", "submitter": "Shenghuo Zhu", "authors": "Shenghuo Zhu", "title": "Stochastic gradient descent algorithms for strongly convex functions at\n  O(1/T) convergence rates", "comments": null, "journal-ref": null, "doi": null, "report-no": "2013-TR053", "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With a weighting scheme proportional to t, a traditional stochastic gradient\ndescent (SGD) algorithm achieves a high probability convergence rate of\nO({\\kappa}/T) for strongly convex functions, instead of O({\\kappa} ln(T)/T). We\nalso prove that an accelerated SGD algorithm also achieves a rate of\nO({\\kappa}/T).\n", "versions": [{"version": "v1", "created": "Thu, 9 May 2013 21:31:47 GMT"}], "update_date": "2013-05-13", "authors_parsed": [["Zhu", "Shenghuo", ""]]}, {"id": "1305.2254", "submitter": "William Yang Wang", "authors": "William Yang Wang, Kathryn Mazaitis, William W. Cohen", "title": "Programming with Personalized PageRank: A Locally Groundable First-Order\n  Probabilistic Logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many probabilistic first-order representation systems, inference is\nperformed by \"grounding\"---i.e., mapping it to a propositional representation,\nand then performing propositional inference. With a large database of facts,\ngroundings can be very large, making inference and learning computationally\nexpensive. Here we present a first-order probabilistic language which is\nwell-suited to approximate \"local\" grounding: every query $Q$ can be\napproximately grounded with a small graph. The language is an extension of\nstochastic logic programs where inference is performed by a variant of\npersonalized PageRank. Experimentally, we show that the approach performs well\nwithout weight learning on an entity resolution task; that supervised\nweight-learning improves accuracy; and that grounding time is independent of DB\nsize. We also show that order-of-magnitude speedups are possible by\nparallelizing learning.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2013 04:16:15 GMT"}], "update_date": "2013-05-13", "authors_parsed": [["Wang", "William Yang", ""], ["Mazaitis", "Kathryn", ""], ["Cohen", "William W.", ""]]}, {"id": "1305.2265", "submitter": "Marc Schoenauer", "authors": "Mostepha Redouane Khouadjia (INRIA Saclay - Ile de France), Marc\n  Schoenauer (INRIA Saclay - Ile de France, LRI), Vincent Vidal (DCSD), Johann\n  Dr\\'eo (TRT), Pierre Sav\\'eant (TRT)", "title": "Quality Measures of Parameter Tuning for Aggregated Multi-Objective\n  Temporal Planning", "comments": "arXiv admin note: substantial text overlap with arXiv:1305.1169", "journal-ref": "LION7 - Learning and Intelligent OptimizatioN Conference (2013)", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parameter tuning is recognized today as a crucial ingredient when tackling an\noptimization problem. Several meta-optimization methods have been proposed to\nfind the best parameter set for a given optimization algorithm and (set of)\nproblem instances. When the objective of the optimization is some scalar\nquality of the solution given by the target algorithm, this quality is also\nused as the basis for the quality of parameter sets. But in the case of\nmulti-objective optimization by aggregation, the set of solutions is given by\nseveral single-objective runs with different weights on the objectives, and it\nturns out that the hypervolume of the final population of each single-objective\nrun might be a better indicator of the global performance of the aggregation\nmethod than the best fitness in its population. This paper discusses this issue\non a case study in multi-objective temporal planning using the evolutionary\nplanner DaE-YAHSP and the meta-optimizer ParamILS. The results clearly show how\nParamILS makes a difference between both approaches, and demonstrate that\nindeed, in this context, using the hypervolume indicator as ParamILS target is\nthe best choice. Other issues pertaining to parameter tuning in the proposed\ncontext are also discussed.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2013 06:34:05 GMT"}], "update_date": "2013-05-13", "authors_parsed": [["Khouadjia", "Mostepha Redouane", "", "INRIA Saclay - Ile de France"], ["Schoenauer", "Marc", "", "INRIA Saclay - Ile de France, LRI"], ["Vidal", "Vincent", "", "DCSD"], ["Dr\u00e9o", "Johann", "", "TRT"], ["Sav\u00e9ant", "Pierre", "", "TRT"]]}, {"id": "1305.2299", "submitter": "Michael Otte", "authors": "Joshua Bialkowski and Michael Otte and Emilio Frazzoli", "title": "Fast Collision Checking: From Single Robots to Multi-Robot Teams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine three different algorithms that enable the collision certificate\nmethod from [Bialkowski, et al.] to handle the case of a centralized\nmulti-robot team. By taking advantage of symmetries in the configuration space\nof multi-robot teams, our methods can significantly reduce the number of\ncollision checks vs. both [Bialkowski, et al.] and standard collision checking\nimplementations.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2013 10:06:17 GMT"}], "update_date": "2013-05-13", "authors_parsed": [["Bialkowski", "Joshua", ""], ["Otte", "Michael", ""], ["Frazzoli", "Emilio", ""]]}, {"id": "1305.2415", "submitter": "Djallel Bouneffouf", "authors": "Djallel Bouneffouf", "title": "Exponentiated Gradient LINUCB for Contextual Multi-Armed Bandits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Exponentiated Gradient LINUCB, an algorithm for con-textual\nmulti-armed bandits. This algorithm uses Exponentiated Gradient to find the\noptimal exploration of the LINUCB. Within a deliberately designed offline\nsimulation framework we conduct evaluations with real online event log data.\nThe experimental results demonstrate that our algorithm outperforms surveyed\nalgorithms.\n", "versions": [{"version": "v1", "created": "Fri, 10 May 2013 11:13:14 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["Bouneffouf", "Djallel", ""]]}, {"id": "1305.2498", "submitter": "Jun He", "authors": "Boris Mitavskiy and Jun He", "title": "A Further Generalization of the Finite-Population Geiringer-like Theorem\n  for POMDPs to Allow Recombination Over Arbitrary Set Covers", "comments": "arXiv admin note: text overlap with arXiv:1110.4657", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A popular current research trend deals with expanding the Monte-Carlo tree\nsearch sampling methodologies to the environments with uncertainty and\nincomplete information. Recently a finite population version of Geiringer\ntheorem with nonhomologous recombination has been adopted to the setting of\nMonte-Carlo tree search to cope with randomness and incomplete information by\nexploiting the entrinsic similarities within the state space of the problem.\nThe only limitation of the new theorem is that the similarity relation was\nassumed to be an equivalence relation on the set of states. In the current\npaper we lift this \"curtain of limitation\" by allowing the similarity relation\nto be modeled in terms of an arbitrary set cover of the set of state-action\npairs.\n", "versions": [{"version": "v1", "created": "Sat, 11 May 2013 11:42:09 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["Mitavskiy", "Boris", ""], ["He", "Jun", ""]]}, {"id": "1305.2561", "submitter": "Kartik Talamadupula", "authors": "Kartik Talamadupula and Octavian Udrea and Anton Riabov and Anand\n  Ranganathan", "title": "Strategic Planning for Network Data Analysis", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As network traffic monitoring software for cybersecurity, malware detection,\nand other critical tasks becomes increasingly automated, the rate of alerts and\nsupporting data gathered, as well as the complexity of the underlying model,\nregularly exceed human processing capabilities. Many of these applications\nrequire complex models and constituent rules in order to come up with decisions\nthat influence the operation of entire systems. In this paper, we motivate the\nnovel \"strategic planning\" problem -- one of gathering data from the world and\napplying the underlying model of the domain in order to come up with decisions\nthat will monitor the system in an automated manner. We describe our use of\nautomated planning methods to this problem, including the technique that we\nused to solve it in a manner that would scale to the demands of a real-time,\nreal world scenario. We then present a PDDL model of one such application\nscenario related to network administration and monitoring, followed by a\ndescription of a novel integrated system that was built to accept generated\nplans and to continue the execution process. Finally, we present evaluations of\ntwo different automated planners and their different capabilities with our\nintegrated system, both on a six-month window of network data, and using a\nsimulator.\n", "versions": [{"version": "v1", "created": "Sun, 12 May 2013 05:52:08 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["Talamadupula", "Kartik", ""], ["Udrea", "Octavian", ""], ["Riabov", "Anton", ""], ["Ranganathan", "Anand", ""]]}, {"id": "1305.2724", "submitter": "Said Broumi", "authors": "Said Broumi", "title": "Generalized Neutrosophic Soft Set", "comments": "14 pages, 11 figures", "journal-ref": "International Journal of Computer Science, Engineering and\n  Information Technology (IJCSEIT), Vol.3, No.2,April2013", "doi": "10.5121/ijcseit.2013.3202", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a new concept called generalized neutrosophic soft\nset. This concept incorporates the beneficial properties of both generalized\nneutrosophic set introduced by A.A. Salama [7]and soft set techniques proposed\nby Molodtsov [4]. We also study some properties of this concept. Some\ndefinitions and operations have been introduced on generalized neutrosophic\nsoft set. Finally we present an application of generalized neuutrosophic soft\nset in decision making problem.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2013 09:42:50 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["Broumi", "Said", ""]]}, {"id": "1305.2752", "submitter": "Oumair Naseer", "authors": "Oumair Naseer, Atif Ali Khan", "title": "Hybrid fuzzy logic and pid controller based ph neutralization pilot\n  plant", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Use of Control theory within process control industries has changed rapidly\ndue to the increase complexity of instrumentation, real time requirements,\nminimization of operating costs and highly nonlinear characteristics of\nchemical process. Previously developed process control technologies which are\nmostly based on a single controller are not efficient in terms of signal\ntransmission delays, processing power for computational needs and signal to\nnoise ratio. Hybrid controller with efficient system modelling is essential to\ncope with the current challenges of process control in terms of control\nperformance. This paper presents an optimized mathematical modelling and\nadvance hybrid controller (Fuzzy Logic and PID) design along with practical\nimplementation and validation of pH neutralization pilot plant. This procedure\nis particularly important for control design and automation of Physico-chemical\nsystems for process control industry.\n", "versions": [{"version": "v1", "created": "Mon, 13 May 2013 12:17:55 GMT"}], "update_date": "2013-05-14", "authors_parsed": [["Naseer", "Oumair", ""], ["Khan", "Atif Ali", ""]]}, {"id": "1305.3321", "submitter": "Lakhdar Sais", "authors": "Said Jabbour, Lakhdar Sais, Yakoub Salhi", "title": "A Mining-Based Compression Approach for Constraint Satisfaction Problems", "comments": "arXiv admin note: substantial text overlap with arXiv:1304.4415", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an extension of our Mining for SAT framework to\nConstraint satisfaction Problem (CSP). We consider n-ary extensional\nconstraints (table constraints). Our approach aims to reduce the size of the\nCSP by exploiting the structure of the constraints graph and of its associated\nmicrostructure. More precisely, we apply itemset mining techniques to search\nfor closed frequent itemsets on these two representation. Using Tseitin\nextension, we rewrite the whole CSP to another compressed CSP equivalent with\nrespect to satisfiability. Our approach contrast with previous proposed\napproach by Katsirelos and Walsh, as we do not change the structure of the\nconstraints.\n", "versions": [{"version": "v1", "created": "Tue, 14 May 2013 23:17:49 GMT"}], "update_date": "2013-05-16", "authors_parsed": [["Jabbour", "Said", ""], ["Sais", "Lakhdar", ""], ["Salhi", "Yakoub", ""]]}, {"id": "1305.4130", "submitter": "Andrew Gelfand", "authors": "Andrew Gelfand, Jinwoo Shin, Michael Chertkov", "title": "Belief Propagation for Linear Programming", "comments": "To appear in ISIT 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Belief Propagation (BP) is a popular, distributed heuristic for performing\nMAP computations in Graphical Models. BP can be interpreted, from a variational\nperspective, as minimizing the Bethe Free Energy (BFE). BP can also be used to\nsolve a special class of Linear Programming (LP) problems. For this class of\nproblems, MAP inference can be stated as an integer LP with an LP relaxation\nthat coincides with minimization of the BFE at ``zero temperature\". We\ngeneralize these prior results and establish a tight characterization of the LP\nproblems that can be formulated as an equivalent LP relaxation of MAP\ninference. Moreover, we suggest an efficient, iterative annealing BP algorithm\nfor solving this broader class of LP problems. We demonstrate the algorithm's\nperformance on a set of weighted matching problems by using it as a cutting\nplane method to solve a sequence of LPs tightened by adding ``blossom''\ninequalities.\n", "versions": [{"version": "v1", "created": "Fri, 17 May 2013 16:40:43 GMT"}], "update_date": "2013-05-20", "authors_parsed": [["Gelfand", "Andrew", ""], ["Shin", "Jinwoo", ""], ["Chertkov", "Michael", ""]]}, {"id": "1305.4228", "submitter": "Wei Yu", "authors": "Wei Yu, Junpeng Chen", "title": "The state-of-the-art in web-scale semantic information processing for\n  cloud computing", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on integrated infrastructure of resource sharing and computing in\ndistributed environment, cloud computing involves the provision of dynamically\nscalable and provides virtualized resources as services over the Internet.\nThese applications also bring a large scale heterogeneous and distributed\ninformation which pose a great challenge in terms of the semantic ambiguity. It\nis critical for application services in cloud computing environment to provide\nusers intelligent service and precise information. Semantic information\nprocessing can help users deal with semantic ambiguity and information overload\nefficiently through appropriate semantic models and semantic information\nprocessing technology. The semantic information processing have been\nsuccessfully employed in many fields such as the knowledge representation,\nnatural language understanding, intelligent web search, etc. The purpose of\nthis report is to give an overview of existing technologies for semantic\ninformation processing in cloud computing environment, to propose a research\ndirection for addressing distributed semantic reasoning and parallel semantic\ncomputing by exploiting semantic information newly available in cloud computing\nenvironment.\n", "versions": [{"version": "v1", "created": "Sat, 18 May 2013 05:36:16 GMT"}], "update_date": "2013-05-21", "authors_parsed": [["Yu", "Wei", ""], ["Chen", "Junpeng", ""]]}, {"id": "1305.4455", "submitter": "Mark Wilkinson", "authors": "Ben P Vandervalk, E Luke McCarthy, Mark D Wilkinson", "title": "SHARE: A Web Service Based Framework for Distributed Querying and\n  Reasoning on the Semantic Web", "comments": "Third Asian Semantic Web Conference, ASWC2008 Bangkok, Thailand\n  December 2008, Workshops Proceedings (NEFORS2008), pp69-78", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.AI cs.SE", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Here we describe the SHARE system, a web service based framework for\ndistributed querying and reasoning on the semantic web. The main innovations of\nSHARE are: (1) the extension of a SPARQL query engine to perform on-demand data\nretrieval from web services, and (2) the extension of an OWL reasoner to test\nproperty restrictions by means of web service invocations. In addition to\nenabling queries across distributed datasets, the system allows for a target\ndataset that is significantly larger than is possible under current,\ncentralized approaches. Although the architecture is equally applicable to all\ntypes of data, the SHARE system targets bioinformatics, due to the large number\nof interoperable web services that are already available in this area. SHARE is\nbuilt entirely on semantic web standards, and is the successor of the BioMOBY\nproject.\n", "versions": [{"version": "v1", "created": "Mon, 20 May 2013 07:54:09 GMT"}], "update_date": "2013-05-21", "authors_parsed": [["Vandervalk", "Ben P", ""], ["McCarthy", "E Luke", ""], ["Wilkinson", "Mark D", ""]]}, {"id": "1305.4744", "submitter": "Pietro Galliani Dr", "authors": "Pietro Galliani", "title": "The Doxastic Interpretation of Team Semantics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We advance a doxastic interpretation for many of the logical connectives\nconsidered in Dependence Logic and in its extensions, and we argue that Team\nSemantics is a natural framework for reasoning about beliefs and belief\nupdates.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2013 07:55:49 GMT"}], "update_date": "2013-05-22", "authors_parsed": [["Galliani", "Pietro", ""]]}, {"id": "1305.4859", "submitter": "Jia Xu", "authors": "Jia Xu, Patrick Shironoshita, Ubbo Visser, Nigel John, Mansur Kabuka", "title": "Extract ABox Modules for Efficient Ontology Querying", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The extraction of logically-independent fragments out of an ontology ABox can\nbe useful for solving the tractability problem of querying ontologies with\nlarge ABoxes. In this paper, we propose a formal definition of an ABox module,\nsuch that it guarantees complete preservation of facts about a given set of\nindividuals, and thus can be reasoned independently w.r.t. the ontology TBox.\nWith ABox modules of this type, isolated or distributed (parallel) ABox\nreasoning becomes feasible, and more efficient data retrieval from ontology\nABoxes can be attained. To compute such an ABox module, we present a\ntheoretical approach and also an approximation for $\\mathcal{SHIQ}$ ontologies.\nEvaluation of the module approximation on different types of ontologies shows\nthat, on average, extracted ABox modules are significantly smaller than the\nentire ABox, and the time for ontology reasoning based on ABox modules can be\nimproved significantly.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2013 15:35:03 GMT"}, {"version": "v2", "created": "Wed, 17 Jul 2013 21:16:14 GMT"}, {"version": "v3", "created": "Thu, 21 Nov 2013 15:48:25 GMT"}, {"version": "v4", "created": "Wed, 11 Jun 2014 12:16:53 GMT"}], "update_date": "2014-06-12", "authors_parsed": [["Xu", "Jia", ""], ["Shironoshita", "Patrick", ""], ["Visser", "Ubbo", ""], ["John", "Nigel", ""], ["Kabuka", "Mansur", ""]]}, {"id": "1305.4917", "submitter": "Mark Levin", "authors": "Mark Sh. Levin", "title": "Note on Evaluation of Hierarchical Modular Systems", "comments": "15 pages, 23 figures, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This survey note describes a brief systemic view to approaches for evaluation\nof hierarchical composite (modular) systems. The list of considered issues\ninvolves the following: (i) basic assessment scales (quantitative scale,\nordinal scale, multicriteria description, two kinds of poset-like scales), (ii)\nbasic types of scale transformations problems, (iii) basic types of scale\nintegration methods. Evaluation of the modular systems is considered as\nassessment of system components (and their compatibility) and integration of\nthe obtained local estimates into the total system estimate(s). This process is\nbased on the above-mentioned problems (i.e., scale transformation and\nintegration). Illustrations of the assessment problems and evaluation\napproaches are presented (including numerical examples).\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2013 19:03:02 GMT"}], "update_date": "2013-05-22", "authors_parsed": [["Levin", "Mark Sh.", ""]]}, {"id": "1305.4955", "submitter": "Arthur Carvalho", "authors": "Renato Oliveira and Paulo Adeodato and Arthur Carvalho and Icamaan\n  Viegas and Christian Diego and Tsang Ing-Ren", "title": "A Data Mining Approach to Solve the Goal Scoring Problem", "comments": null, "journal-ref": null, "doi": "10.1109/IJCNN.2009.5178616", "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In soccer, scoring goals is a fundamental objective which depends on many\nconditions and constraints. Considering the RoboCup soccer 2D-simulator, this\npaper presents a data mining-based decision system to identify the best time\nand direction to kick the ball towards the goal to maximize the overall chances\nof scoring during a simulated soccer match. Following the CRISP-DM methodology,\ndata for modeling were extracted from matches of major international\ntournaments (10691 kicks), knowledge about soccer was embedded via\ntransformation of variables and a Multilayer Perceptron was used to estimate\nthe scoring chance. Experimental performance assessment to compare this\napproach against previous LDA-based approach was conducted from 100 matches.\nSeveral statistical metrics were used to analyze the performance of the system\nand the results showed an increase of 7.7% in the number of kicks, producing an\noverall increase of 78% in the number of goals scored.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2013 20:29:02 GMT"}, {"version": "v2", "created": "Wed, 26 Jun 2013 21:59:35 GMT"}], "update_date": "2013-06-28", "authors_parsed": [["Oliveira", "Renato", ""], ["Adeodato", "Paulo", ""], ["Carvalho", "Arthur", ""], ["Viegas", "Icamaan", ""], ["Diego", "Christian", ""], ["Ing-Ren", "Tsang", ""]]}, {"id": "1305.4987", "submitter": "Julie Tibshirani", "authors": "Julie Tibshirani and Christopher D. Manning", "title": "Robust Logistic Regression using Shift Parameters (Long Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Annotation errors can significantly hurt classifier performance, yet datasets\nare only growing noisier with the increased use of Amazon Mechanical Turk and\ntechniques like distant supervision that automatically generate labels. In this\npaper, we present a robust extension of logistic regression that incorporates\nthe possibility of mislabelling directly into the objective. Our model can be\ntrained through nearly the same means as logistic regression, and retains its\nefficiency on high-dimensional datasets. Through named entity recognition\nexperiments, we demonstrate that our approach can provide a significant\nimprovement over the standard model when annotation errors are present.\n", "versions": [{"version": "v1", "created": "Tue, 21 May 2013 23:36:18 GMT"}, {"version": "v2", "created": "Tue, 29 Apr 2014 07:32:58 GMT"}], "update_date": "2014-04-30", "authors_parsed": [["Tibshirani", "Julie", ""], ["Manning", "Christopher D.", ""]]}, {"id": "1305.5030", "submitter": "David Tolpin", "authors": "David Tolpin, Tal Beja, Solomon Eyal Shimony, Ariel Felner, Erez\n  Karpas", "title": "Towards Rational Deployment of Multiple Heuristics in A*", "comments": "7 pages, IJCAI 2013, to appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The obvious way to use several admissible heuristics in A* is to take their\nmaximum. In this paper we aim to reduce the time spent on computing heuristics.\nWe discuss Lazy A*, a variant of A* where heuristics are evaluated lazily: only\nwhen they are essential to a decision to be made in the A* search process. We\npresent a new rational meta-reasoning based scheme, rational lazy A*, which\ndecides whether to compute the more expensive heuristics at all, based on a\nmyopic value of information estimate. Both methods are examined theoretically.\nEmpirical evaluation on several domains supports the theoretical results, and\nshows that lazy A* and rational lazy A* are state-of-the-art heuristic\ncombination methods.\n", "versions": [{"version": "v1", "created": "Wed, 22 May 2013 06:41:00 GMT"}], "update_date": "2013-05-23", "authors_parsed": [["Tolpin", "David", ""], ["Beja", "Tal", ""], ["Shimony", "Solomon Eyal", ""], ["Felner", "Ariel", ""], ["Karpas", "Erez", ""]]}, {"id": "1305.5506", "submitter": "Robert R. Tucci", "authors": "Robert R. Tucci", "title": "Introduction to Judea Pearl's Do-Calculus", "comments": "16 pages (11 files: 1 .tex, 1 .sty, 9 .jpg)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a purely pedagogical paper with no new results. The goal of the paper\nis to give a fairly self-contained introduction to Judea Pearl's do-calculus,\nincluding proofs of his 3 rules.\n", "versions": [{"version": "v1", "created": "Fri, 26 Apr 2013 02:36:43 GMT"}], "update_date": "2013-05-24", "authors_parsed": [["Tucci", "Robert R.", ""]]}, {"id": "1305.5522", "submitter": "Shreyas Balakuntala", "authors": "Shreyas Balakuntala, Sandeep Venkatesh", "title": "An Intelligent System to Detect, Avoid and Maintain Potholes: A Graph\n  Theoretic Approach", "comments": "5 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a conceptual framework where a centralized system,\nclassifies the road based upon the level of damage. The centralized system also\nidentifies the traffic intensity thereby prioritizing the roads that need quick\naction to be taken upon. Moreover, the system helps the driver to detect the\nlevel of damage to the road stretch and route the vehicle from an alternative\npath to its destination. The system sends a feedback to the concerned\nauthorities for a quick response to the condition of the roads. The system we\nuse comprises a laser sensor and pressure sensors in shock absorbers to detect\nand quantify the intensity of the pothole, a centralized server which maintains\na database of locations of all the potholes which can be accessed by another\nunit inside the vehicle. A point to point connection device is also installed\nin vehicles so that, when a vehicle detects a pothole which is not in the\ndatabase, all the vehicles within a range of 20 meters are warned about the\npothole. The system computes a route with least number of potholes which is\nnearest to the desired destination . If the destination is unknown, then the\nsystem will check for potholes in the current road and displays the level of\ndamage. The system is flexible enough that the destination can be added,\nremoved or changed any time during the travel. The best possible route is\nsuggested by the system upon the alteration. We prove that the algorithm\nreturns an efficient path with least number of potholes.\n", "versions": [{"version": "v1", "created": "Thu, 23 May 2013 19:22:06 GMT"}, {"version": "v2", "created": "Fri, 24 May 2013 17:41:10 GMT"}, {"version": "v3", "created": "Wed, 18 Sep 2013 06:36:22 GMT"}], "update_date": "2013-09-19", "authors_parsed": [["Balakuntala", "Shreyas", ""], ["Venkatesh", "Sandeep", ""]]}, {"id": "1305.5610", "submitter": "Tao  Ye", "authors": "Fred Glover and Tao Ye and Abraham P. Punnen and Gary Kochenberger", "title": "Integrating tabu search and VLSN search to develop enhanced algorithms:\n  A case study using bipartite boolean quadratic programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The bipartite boolean quadratic programming problem (BBQP) is a\ngeneralization of the well studied boolean quadratic programming problem. The\nmodel has a variety of real life applications; however, empirical studies of\nthe model are not available in the literature, except in a few isolated\ninstances. In this paper, we develop efficient heuristic algorithms based on\ntabu search, very large scale neighborhood (VLSN) search, and a hybrid\nalgorithm that integrates the two. The computational study establishes that\neffective integration of simple tabu search with VLSN search results in\nsuperior outcomes, and suggests the value of such an integration in other\nsettings. Complexity analysis and implementation details are provided along\nwith conclusions drawn from experimental analysis. In addition, we obtain\nsolutions better than the best previously known for almost all medium and large\nsize benchmark instances.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2013 03:36:00 GMT"}], "update_date": "2013-05-27", "authors_parsed": [["Glover", "Fred", ""], ["Ye", "Tao", ""], ["Punnen", "Abraham P.", ""], ["Kochenberger", "Gary", ""]]}, {"id": "1305.5637", "submitter": "Seppo Ilari Tirri", "authors": "Seppo Ilari Tirri", "title": "Algebraic Net Class Rewriting Systems, Syntax and Semantics for\n  Knowledge Representation and Automated Problem Solving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The intention of the present study is to establish general framework for\nautomated problem solving by approaching the task universal algebraically\nintroducing knowledge as realizations of generalized free algebra based nets,\ngraphs with gluing forms connecting in- and out-edges to nodes. Nets are caused\nto undergo transformations in conceptual level by type wise differentiated\nintervening net rewriting systems dispersing problems to abstract parts,\nmatching being determined by substitution relations. Achieved sets of\nconceptual nets constitute congruent classes. New results are obtained within\nconstruction of problem solving systems where solution algorithms are derived\nparallel with other candidates applied to the same net classes. By applying\nparallel transducer paths consisting of net rewriting systems to net classes\ncongruent quotient algebras are established and the manifested class rewriting\ncomprises all solution candidates whenever produced nets are in anticipated\nlanguages liable to acceptance of net automata.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2013 07:17:59 GMT"}], "update_date": "2013-05-27", "authors_parsed": [["Tirri", "Seppo Ilari", ""]]}, {"id": "1305.5665", "submitter": "Abdelali Boussadi", "authors": "Boussadi Abdelali, Caruba Thibaut, Karras Alexandre, Berdot Sarah,\n  Degoulet Patrice, Durieux Pierre, Sabatier Brigitte", "title": "Validity of a clinical decision rule based alert system for drug dose\n  adjustment in patients with renal failure intended to improve pharmacists'\n  analysis of medication orders in hospitals", "comments": "Word count Body: 3753 Abstract: 280 tables: 5 figures: 1 pages: 26\n  references: 29 This article is the pre print version of an article submitted\n  to the International Journal of Medical Informatics (IJMI, Elsevier) funding:\n  This work was supported by Programme de recherche en qualit\\'e hospitali\\`ere\n  (PREQHOS-PHRQ 1034 SADPM), The French Ministry of Health, grant number 115189", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Objective: The main objective of this study was to assess the diagnostic\nperformances of an alert system integrated into the CPOE/EMR system for renally\ncleared drug dosing control. The generated alerts were compared with the daily\nroutine practice of pharmacists as part of the analysis of medication orders.\nMaterials and Methods: The pharmacists performed their analysis of medication\norders as usual and were not aware of the alert system interventions that were\nnot displayed for the purpose of the study neither to the physician nor to the\npharmacist but kept with associate recommendations in a log file. A senior\npharmacist analyzed the results of medication order analysis with and without\nthe alert system. The unit of analysis was the drug prescription line. The\nprimary study endpoints were the detection of drug-dose prescription errors and\ninter-rater reliability between the alert system and the pharmacists in the\ndetection of drug dose error. Results: The alert system fired alerts in 8.41%\n(421/5006) of cases: 5.65% (283/5006) exceeds max daily dose alerts and 2.76%\n(138/5006) under dose alerts. The alert system and the pharmacists showed a\nrelatively poor concordance: 0.106 (CI 95% [0.068, 0.144]). According to the\nsenior pharmacist review, the alert system fired more appropriate alerts than\npharmacists, and made fewer errors than pharmacists in analyzing drug dose\nprescriptions: 143 for the alert system and 261 for the pharmacists. Unlike the\nalert system, most diagnostic errors made by the pharmacists were false\nnegatives. The pharmacists were not able to analyze a significant number (2097;\n25.42%) of drug prescription lines because understaffing. Conclusion: This\nstudy strongly suggests that an alert system would be complementary to the\npharmacists activity and contribute to drug prescription safety.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2013 09:37:54 GMT"}], "update_date": "2013-05-27", "authors_parsed": [["Abdelali", "Boussadi", ""], ["Thibaut", "Caruba", ""], ["Alexandre", "Karras", ""], ["Sarah", "Berdot", ""], ["Patrice", "Degoulet", ""], ["Pierre", "Durieux", ""], ["Brigitte", "Sabatier", ""]]}, {"id": "1305.5827", "submitter": "Monica Shekhar", "authors": "Monica Shekhar and Saravanaguru RA. K", "title": "Semantic Web Search based on Ontology Modeling using Protege Reasoner", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Semantic Web works on the existing Web which presents the meaning of\ninformation as well-defined vocabularies understood by the people. Semantic\nSearch, at the same time, works on improving the accuracy if a search by\nunderstanding the intent of the search and providing contextually relevant\nresults. This paper describes a semantic approach toward web search through a\nPHP application. The goal was to parse through a user's browsing history and\nreturn semantically relevant web pages for the search query provided.\n", "versions": [{"version": "v1", "created": "Fri, 24 May 2013 19:02:59 GMT"}], "update_date": "2013-05-27", "authors_parsed": [["Shekhar", "Monica", ""], ["K", "Saravanaguru RA.", ""]]}, {"id": "1305.6037", "submitter": "Tshilidzi Marwala", "authors": "Tshilidzi Marwala", "title": "Semi-bounded Rationality: A model for decision making", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the theory of semi-bounded rationality is proposed as an\nextension of the theory of bounded rationality. In particular, it is proposed\nthat a decision making process involves two components and these are the\ncorrelation machine, which estimates missing values, and the causal machine,\nwhich relates the cause to the effect. Rational decision making involves using\ninformation which is almost always imperfect and incomplete as well as some\nintelligent machine which if it is a human being is inconsistent to make\ndecisions. In the theory of bounded rationality this decision is made\nirrespective of the fact that the information to be used is incomplete and\nimperfect and the human brain is inconsistent and thus this decision that is to\nbe made is taken within the bounds of these limitations. In the theory of\nsemi-bounded rationality, signal processing is used to filter noise and\noutliers in the information and the correlation machine is applied to complete\nthe missing information and artificial intelligence is used to make more\nconsistent decisions.\n", "versions": [{"version": "v1", "created": "Sun, 26 May 2013 15:41:48 GMT"}], "update_date": "2013-05-28", "authors_parsed": [["Marwala", "Tshilidzi", ""]]}, {"id": "1305.6129", "submitter": "Kian Hsiang Low", "authors": "Kian Hsiang Low, John M. Dolan, Pradeep Khosla", "title": "Information-Theoretic Approach to Efficient Adaptive Path Planning for\n  Mobile Robotic Environmental Sensing", "comments": "19th International Conference on Automated Planning and Scheduling\n  (ICAPS 2009), Extended version with proofs, 11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MA cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent research in robot exploration and mapping has focused on sampling\nenvironmental hotspot fields. This exploration task is formalized by Low,\nDolan, and Khosla (2008) in a sequential decision-theoretic planning under\nuncertainty framework called MASP. The time complexity of solving MASP\napproximately depends on the map resolution, which limits its use in\nlarge-scale, high-resolution exploration and mapping. To alleviate this\ncomputational difficulty, this paper presents an information-theoretic approach\nto MASP (iMASP) for efficient adaptive path planning; by reformulating the\ncost-minimizing iMASP as a reward-maximizing problem, its time complexity\nbecomes independent of map resolution and is less sensitive to increasing robot\nteam size as demonstrated both theoretically and empirically. Using the\nreward-maximizing dual, we derive a novel adaptive variant of maximum entropy\nsampling, thus improving the induced exploration policy performance. It also\nallows us to establish theoretical bounds quantifying the performance advantage\nof optimal adaptive over non-adaptive policies and the performance quality of\napproximately optimal vs. optimal adaptive policies. We show analytically and\nempirically the superior performance of iMASP-based policies for sampling the\nlog-Gaussian process to that of policies for the widely-used Gaussian process\nin mapping the hotspot field. Lastly, we provide sufficient conditions that,\nwhen met, guarantee adaptivity has no benefit under an assumed environment\nmodel.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2013 07:28:05 GMT"}], "update_date": "2013-05-28", "authors_parsed": [["Low", "Kian Hsiang", ""], ["Dolan", "John M.", ""], ["Khosla", "Pradeep", ""]]}, {"id": "1305.6187", "submitter": "Steven Prestwich", "authors": "S. D. Prestwich", "title": "Improved Branch-and-Bound for Low Autocorrelation Binary Sequences", "comments": "Journal paper in preparation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Low Autocorrelation Binary Sequence problem has applications in\ntelecommunications, is of theoretical interest to physicists, and has inspired\nmany optimisation researchers. Metaheuristics for the problem have progressed\ngreatly in recent years but complete search has not progressed since a\nbranch-and-bound method of 1996. In this paper we find four ways of improving\nbranch-and-bound, leading to a tighter relaxation, faster convergence to\noptimality, and better empirical scalability.\n", "versions": [{"version": "v1", "created": "Mon, 27 May 2013 11:57:40 GMT"}, {"version": "v2", "created": "Tue, 23 Jul 2013 14:42:15 GMT"}], "update_date": "2013-07-24", "authors_parsed": [["Prestwich", "S. D.", ""]]}, {"id": "1305.6537", "submitter": "Arthur Carvalho", "authors": "Arthur Carvalho", "title": "A Cooperative Coevolutionary Genetic Algorithm for Learning Bayesian\n  Network Structures", "comments": null, "journal-ref": null, "doi": "10.1145/2001576.2001729", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a cooperative coevolutionary genetic algorithm for learning\nBayesian network structures from fully observable data sets. Since this problem\ncan be decomposed into two dependent subproblems, that is to find an ordering\nof the nodes and an optimal connectivity matrix, our algorithm uses two\nsubpopulations, each one representing a subtask. We describe the empirical\nresults obtained with simulations of the Alarm and Insurance networks. We show\nthat our algorithm outperforms the deterministic algorithm K2.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2013 15:42:51 GMT"}], "update_date": "2013-05-29", "authors_parsed": [["Carvalho", "Arthur", ""]]}, {"id": "1305.6650", "submitter": "Sheeraz Ahmad", "authors": "Sheeraz Ahmad and Angela J. Yu", "title": "Active Sensing as Bayes-Optimal Sequential Decision Making", "comments": "Scheduled to appear in UAI 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sensory inference under conditions of uncertainty is a major problem in both\nmachine learning and computational neuroscience. An important but poorly\nunderstood aspect of sensory processing is the role of active sensing. Here, we\npresent a Bayes-optimal inference and control framework for active sensing,\nC-DAC (Context-Dependent Active Controller). Unlike previously proposed\nalgorithms that optimize abstract statistical objectives such as information\nmaximization (Infomax) [Butko & Movellan, 2010] or one-step look-ahead accuracy\n[Najemnik & Geisler, 2005], our active sensing model directly minimizes a\ncombination of behavioral costs, such as temporal delay, response error, and\neffort. We simulate these algorithms on a simple visual search task to\nillustrate scenarios in which context-sensitivity is particularly beneficial\nand optimization with respect to generic statistical objectives particularly\ninadequate. Motivated by the geometric properties of the C-DAC policy, we\npresent both parametric and non-parametric approximations, which retain\ncontext-sensitivity while significantly reducing computational complexity.\nThese approximations enable us to investigate the more complex problem\ninvolving peripheral vision, and we notice that the difference between C-DAC\nand statistical policies becomes even more evident in this scenario.\n", "versions": [{"version": "v1", "created": "Tue, 28 May 2013 22:46:35 GMT"}], "update_date": "2013-05-30", "authors_parsed": [["Ahmad", "Sheeraz", ""], ["Yu", "Angela J.", ""]]}, {"id": "1305.7058", "submitter": "Sahar Mokhtar", "authors": "Nora Y. Ibrahim, Sahar A. Mokhtar and Hany M. Harb", "title": "Towards an Ontology based integrated Framework for Semantic Web", "comments": null, "journal-ref": "International Journal of Computer Science and Information Security\n  (IJCSIS) Vol. 10, No. 9, September 2012", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This Ontologies are widely used as a means for solving the information\nheterogeneity problems on the web because of their capability to provide\nexplicit meaning to the information. They become an efficient tool for\nknowledge representation in a structured manner. There is always more than one\nontology for the same domain. Furthermore, there is no standard method for\nbuilding ontologies, and there are many ontology building tools using different\nontology languages. Because of these reasons, interoperability between the\nontologies is very low. Current ontology tools mostly use functions to build,\nedit and inference the ontology. Methods for merging heterogeneous domain\nontologies are not included in most tools. This paper presents ontology merging\nmethodology for building a single global ontology from heterogeneous eXtensible\nMarkup Language (XML) data sources to capture and maintain all the knowledge\nwhich XML data sources can contain\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2013 10:53:07 GMT"}], "update_date": "2013-06-04", "authors_parsed": [["Ibrahim", "Nora Y.", ""], ["Mokhtar", "Sahar A.", ""], ["Harb", "Hany M.", ""]]}, {"id": "1305.7130", "submitter": "Uwe Aickelin", "authors": "William Wilson, Uwe Aickelin", "title": "Memory Implementations - Current Alternatives", "comments": "University of Nottingham, 2005", "journal-ref": null, "doi": null, "report-no": "NOTTCS-TR-2005-4", "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Memory can be defined as the ability to retain and recall information in a\ndiverse range of forms. It is a vital component of the way in which we as human\nbeings operate on a day to day basis. Given a particular situation, decisions\nare made and actions undertaken in response to that situation based on our\nmemory of related prior events and experiences. By utilising our memory we can\nanticipate the outcome of our chosen actions to avoid unexpected or unwanted\nevents. In addition, as we subtly alter our actions and recognise altered\noutcomes we learn and create new memories, enabling us to improve the\nefficiency of our actions over time. However, as this process occurs so\nnaturally in the subconscious its importance is often overlooked.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2013 15:05:10 GMT"}], "update_date": "2013-05-31", "authors_parsed": [["Wilson", "William", ""], ["Aickelin", "Uwe", ""]]}, {"id": "1305.7145", "submitter": "Uwe Aickelin", "authors": "Peer-Olaf Siebers, Uwe Aickelin, David Menachof, Galina Sherman, Peter\n  Zimmerman", "title": "Modelling and Analysing Cargo Screening Processes: A Project Outline", "comments": "Proceedings of the INFORMS Simulation Society Research Workshop, June\n  25-27, 2009, Warwick, UK, 44-47, 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The efficiency of current cargo screening processes at sea and air ports is\nunknown as no benchmarks exists against which they could be measured. Some\nmanufacturer benchmarks exist for individual sensors but we have not found any\nbenchmarks that take a holistic view of the screening procedures assessing a\ncombination of sensors and also taking operator variability into account. Just\nadding up resources and manpower used is not an effective way for assessing\nsystems where human decision-making and operator compliance to rules play a\nvital role. For such systems more advanced assessment methods need to be used,\ntaking into account that the cargo screening process is of a dynamic and\nstochastic nature. Our project aim is to develop a decision support tool\n(cargo-screening system simulator) that will map the right technology and\nmanpower to the right commodity-threat combination in order to maximize\ndetection rates. In this paper we present a project outline and highlight the\nresearch challenges we have identified so far. In addition we introduce our\nfirst case study, where we investigate the cargo screening process at the ferry\nport in Calais.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2013 15:46:43 GMT"}], "update_date": "2013-05-31", "authors_parsed": [["Siebers", "Peer-Olaf", ""], ["Aickelin", "Uwe", ""], ["Menachof", "David", ""], ["Sherman", "Galina", ""], ["Zimmerman", "Peter", ""]]}, {"id": "1305.7185", "submitter": "Philippe Martin", "authors": "Philippe A. Martin", "title": "Collaborative ontology sharing and editing", "comments": "12 pages, 2 figures, journal", "journal-ref": "IJCSIS 6 (2011) 14-29", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article first lists reasons why - in the long term or when creating a\nnew knowledge base (KB) for general knowledge sharing purposes -\ncollaboratively building a well-organized KB does/can provide more\npossibilities, with on the whole no more costs, than the mainstream approach\nwhere knowledge creation and re-use involves searching, merging and creating\n(semi-)independent (relatively small) ontologies or semi-formal documents. The\narticle lists elements required to achieve this and describes the main one: a\nKB editing protocol that keeps the KB free of automatically/manually detected\ninconsistencies while not forcing them to discuss or agree on terminology and\nbeliefs nor requiring a selection committee.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2013 18:06:05 GMT"}], "update_date": "2013-05-31", "authors_parsed": [["Martin", "Philippe A.", ""]]}, {"id": "1305.7196", "submitter": "Philippe Martin", "authors": "Philippe A. Martin", "title": "For a Semantic Web based Peer-reviewing and Publication of Research\n  Results", "comments": "6 pages, conference", "journal-ref": "KGCM (2012) 23-28", "doi": null, "report-no": null, "categories": "cs.DL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article shows why the diffusion and peer-reviewing of research results\nwould be more efficient, precise and relevant if all or at least some parts of\nthe descriptions and peer-reviews of research results took the form of a\nfine-grained semantic network, within articles or knowledge bases, as part of\nthe Semantic Web. This article also shows some ways this can be done and hence\nhow research journal/proceeding publishers could allow this. So far, the World\nWide Web Consortium (W3C) has not proposed simple notations and cooperation\nprotocols - similar to those illustrated or referred to in this article - but\nit now seems likely that Wikipedia/Wikidata, Google or the W3C will propose\nthem sooner or later. Then, research journal/proceeding publishers and\nresearchers may or may not quickly use this approach.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2013 18:34:03 GMT"}], "update_date": "2013-05-31", "authors_parsed": [["Martin", "Philippe A.", ""]]}, {"id": "1305.7200", "submitter": "Philippe Martin", "authors": "Philippe A. Martin", "title": "Organizing Linked Data Quality Related Methods", "comments": "7 pages, 10 tables, conference", "journal-ref": "IKE (2012) 376-382", "doi": null, "report-no": null, "categories": "cs.DL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents the top-level of an ontology categorizing and\ngeneralizing best practices and quality criteria or measures for Linked Data.\nIt permits to compare these techniques and have a synthetic organized view of\nwhat can or should be done for knowledge sharing purposes. This ontology is\npart of a general knowledge base that can be accessed and complemented by any\nWeb user. Thus, it can be seen as a cooperatively built library for the above\ncited elements. Since they permit to evaluate information objects and create\nbetter ones, these elements also permit knowledge-based tools and techniques -\nas well as knowledge providers - to be evaluated and categorized based on their\ninput/output information objects. One top-level distinction permitting to\norganize this ontology is the one between content, medium and containers of\ndescriptions. Various structural, ontological, syntactical and lexical\ndistinctions are then used.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2013 18:40:30 GMT"}], "update_date": "2013-05-31", "authors_parsed": [["Martin", "Philippe A.", ""]]}, {"id": "1305.7254", "submitter": "Imen Ayachi", "authors": "I. Ayachi, R. Kammarti, M.Ksouri, P.Borne LACS, ENIT, Tunis-Belvedere\n  Tunisie LAGIS, ECL, Villeneuve d Ascq, France", "title": "Harmony search to solve the container storage problem with different\n  container types", "comments": "7 pages", "journal-ref": "International Journal of Computer Applications, June 2012", "doi": null, "report-no": "Volume 48-- No.22, June 2012", "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This paper presents an adaptation of the harmony search algorithm to solve\nthe storage allocation problem for inbound and outbound containers. This\nproblem is studied considering multiple container type (regular, open side,\nopen top, tank, empty and refrigerated) which lets the situation more\ncomplicated, as various storage constraints appeared. The objective is to find\nan optimal container arrangement which respects their departure dates, and\nminimize the re-handle operations of containers. The performance of the\nproposed approach is verified comparing to the results generated by genetic\nalgorithm and LIFO algorithm.\n", "versions": [{"version": "v1", "created": "Thu, 30 May 2013 21:13:25 GMT"}], "update_date": "2013-06-03", "authors_parsed": [["Ayachi", "I.", ""], ["Kammarti", "R.", ""], ["Ksouri", "M.", ""], ["LACS", "P. Borne", ""], ["ENIT", "", ""], ["LAGIS", "Tunis-Belvedere Tunisie", ""], ["ECL", "", ""], ["Ascq", "Villeneuve d", ""], ["France", "", ""]]}, {"id": "1305.7345", "submitter": "Diedrich Wolter", "authors": "Frank Dylla, Till Mossakowski, Thomas Schneider and Diedrich Wolter", "title": "Algebraic Properties of Qualitative Spatio-Temporal Calculi", "comments": "COSIT 2013 paper including supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Qualitative spatial and temporal reasoning is based on so-called qualitative\ncalculi. Algebraic properties of these calculi have several implications on\nreasoning algorithms. But what exactly is a qualitative calculus? And to which\nextent do the qualitative calculi proposed meet these demands? The literature\nprovides various answers to the first question but only few facts about the\nsecond. In this paper we identify the minimal requirements to binary\nspatio-temporal calculi and we discuss the relevance of the according axioms\nfor representation and reasoning. We also analyze existing qualitative calculi\nand provide a classification involving different notions of a relation algebra.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2013 10:15:18 GMT"}, {"version": "v2", "created": "Fri, 13 Sep 2013 11:59:17 GMT"}], "update_date": "2013-09-16", "authors_parsed": [["Dylla", "Frank", ""], ["Mossakowski", "Till", ""], ["Schneider", "Thomas", ""], ["Wolter", "Diedrich", ""]]}, {"id": "1305.7437", "submitter": "Uwe Aickelin", "authors": "Tao Zhang, Peer-Olaf Siebers, Uwe Aickelin", "title": "Modelling Electricity Consumption in Office Buildings: An Agent Based\n  Approach", "comments": "Energy and Buildings 43(10), 2882-2892, 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop an agent-based model which integrates four\nimportant elements, i.e. organisational energy management policies/regulations,\nenergy management technologies, electric appliances and equipment, and human\nbehaviour, to simulate the electricity consumption in office buildings. Based\non a case study, we use this model to test the effectiveness of different\nelectricity management strategies, and solve practical office electricity\nconsumption problems. This paper theoretically contributes to an integration of\nthe four elements involved in the complex organisational issue of office\nelectricity consumption, and practically contributes to an application of an\nagent-based approach for office building electricity consumption study.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2013 15:01:01 GMT"}], "update_date": "2013-06-03", "authors_parsed": [["Zhang", "Tao", ""], ["Siebers", "Peer-Olaf", ""], ["Aickelin", "Uwe", ""]]}, {"id": "1305.7458", "submitter": "Uwe Aickelin", "authors": "Chris Roadknight, Uwe Aickelin, Galina Sherman", "title": "Validation of a Microsimulation of the Port of Dover", "comments": "Journal of Computational Science 3 (1-2), 56-66, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.AI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modelling and simulating the traffic of heavily used but secure environments\nsuch as seaports and airports is of increasing importance. Errors made when\nsimulating these environments can have long standing economic, social and\nenvironmental implications. This paper discusses issues and problems that may\narise when designing a simulation strategy. Data for the Port is presented,\nmethods for lightweight vehicle assessment that can be used to calibrate and\nvalidate simulations are also discussed along with a diagnosis of\novercalibration issues. We show that decisions about where the intelligence\nlies in a system has important repercussions for the reliability of system\nstatistics. Finally, conclusions are drawn about how microsimulations can be\nmoved forward as a robust planning tool for the 21st century.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2013 15:40:49 GMT"}], "update_date": "2013-06-03", "authors_parsed": [["Roadknight", "Chris", ""], ["Aickelin", "Uwe", ""], ["Sherman", "Galina", ""]]}, {"id": "1305.7471", "submitter": "Uwe Aickelin", "authors": "Grazziela P. Figueredo, Peer-Olaf Siebers, Uwe Aickelin", "title": "Investigating Mathematical Models of Immuno-Interactions with\n  Early-Stage Cancer under an Agent-Based Modelling Perspective", "comments": null, "journal-ref": "BMC Bioinformatics 14(Suppl 6), S6, 2013", "doi": null, "report-no": null, "categories": "cs.CE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many advances in research regarding immuno-interactions with cancer were\ndeveloped with the help of ordinary differential equation (ODE) models. These\nmodels, however, are not effectively capable of representing problems involving\nindividual localisation, memory and emerging properties, which are common\ncharacteristics of cells and molecules of the immune system. Agent-based\nmodelling and simulation is an alternative paradigm to ODE models that\novercomes these limitations. In this paper we investigate the potential\ncontribution of agent-based modelling and simulation when compared to ODE\nmodelling and simulation. We seek answers to the following questions: Is it\npossible to obtain an equivalent agent-based model from the ODE formulation? Do\nthe outcomes differ? Are there any benefits of using one method compared to the\nother? To answer these questions, we have considered three case studies using\nestablished mathematical models of immune interactions with early-stage cancer.\nThese case studies were re-conceptualised under an agent-based perspective and\nthe simulation results were then compared with those from the ODE models. Our\nresults show that it is possible to obtain equivalent agent-based models (i.e.\nimplementing the same mechanisms); the simulation output of both types of\nmodels however might differ depending on the attributes of the system to be\nmodelled. In some cases, additional insight from using agent-based modelling\nwas obtained. Overall, we can confirm that agent-based modelling is a useful\naddition to the tool set of immunologists, as it has extra features that allow\nfor simulations with characteristics that are closer to the biological\nphenomena.\n", "versions": [{"version": "v1", "created": "Fri, 31 May 2013 16:09:00 GMT"}], "update_date": "2013-06-03", "authors_parsed": [["Figueredo", "Grazziela P.", ""], ["Siebers", "Peer-Olaf", ""], ["Aickelin", "Uwe", ""]]}]