[{"id": "1607.00061", "submitter": "I. Dan Melamed", "authors": "I. Dan Melamed and Nobal B. Niraula", "title": "Towards A Virtual Assistant That Can Be Taught New Tasks In Any Domain\n  By Its End-Users", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The challenge stated in the title can be divided into two main problems. The\nfirst problem is to reliably mimic the way that users interact with user\ninterfaces. The second problem is to build an instructible agent, i.e. one that\ncan be taught to execute tasks expressed as previously unseen natural language\ncommands. This paper proposes a solution to the second problem, a system we\ncall Helpa. End-users can teach Helpa arbitrary new tasks whose level of\ncomplexity is similar to the tasks available from today's most popular virtual\nassistants. Teaching Helpa does not involve any programming. Instead, users\nteach Helpa by providing just one example of a command paired with a\ndemonstration of how to execute that command. Helpa does not rely on any\npre-existing domain-specific knowledge. It is therefore completely\ndomain-independent. Our usability study showed that end-users can teach Helpa\nmany new tasks in less than a minute each, often much less.\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 22:04:26 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Melamed", "I. Dan", ""], ["Niraula", "Nobal B.", ""]]}, {"id": "1607.00087", "submitter": "Miao Cheng", "authors": "Miao Cheng and Ah Chung Tsoi", "title": "Fractal Dimension Pattern Based Multiresolution Analysis for Rough\n  Estimator of Person-Dependent Audio Emotion Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a general means of expression, audio analysis and recognition has\nattracted much attentions for its wide applications in real-life world. Audio\nemotion recognition (AER) attempts to understand emotional states of human with\nthe given utterance signals, and has been studied abroad for its further\ndevelopment on friendly human-machine interfaces. Distinguish from other\nexisting works, the person-dependent patterns of audio emotions are conducted,\nand fractal dimension features are calculated for acoustic feature extraction.\nFurthermore, it is able to efficiently learn intrinsic characteristics of\nauditory emotions, while the utterance features are learned from fractal\ndimensions of each sub-bands. Experimental results show the proposed method is\nable to provide comparative performance for audio emotion recognition.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 00:54:10 GMT"}, {"version": "v2", "created": "Fri, 2 Dec 2016 13:12:35 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Cheng", "Miao", ""], ["Tsoi", "Ah Chung", ""]]}, {"id": "1607.00136", "submitter": "Collins Leke", "authors": "Collins Leke and Tshilidzi Marwala", "title": "Missing Data Estimation in High-Dimensional Datasets: A Swarm\n  Intelligence-Deep Neural Network Approach", "comments": "12 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we examine the problem of missing data in high-dimensional\ndatasets by taking into consideration the Missing Completely at Random and\nMissing at Random mechanisms, as well as theArbitrary missing pattern.\nAdditionally, this paper employs a methodology based on Deep Learning and Swarm\nIntelligence algorithms in order to provide reliable estimates for missing\ndata. The deep learning technique is used to extract features from the input\ndata via an unsupervised learning approach by modeling the data distribution\nbased on the input. This deep learning technique is then used as part of the\nobjective function for the swarm intelligence technique in order to estimate\nthe missing data after a supervised fine-tuning phase by minimizing an error\nfunction based on the interrelationship and correlation between features in the\ndataset. The investigated methodology in this paper therefore has longer\nrunning times, however, the promising potential outcomes justify the trade-off.\nAlso, basic knowledge of statistics is presumed.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 07:34:50 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Leke", "Collins", ""], ["Marwala", "Tshilidzi", ""]]}, {"id": "1607.00148", "submitter": "Pankaj Malhotra Mr.", "authors": "Pankaj Malhotra, Anusha Ramakrishnan, Gaurangi Anand, Lovekesh Vig,\n  Puneet Agarwal, Gautam Shroff", "title": "LSTM-based Encoder-Decoder for Multi-sensor Anomaly Detection", "comments": "Accepted at ICML 2016 Anomaly Detection Workshop, New York, NY, USA,\n  2016. Reference update in this version (v2)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mechanical devices such as engines, vehicles, aircrafts, etc., are typically\ninstrumented with numerous sensors to capture the behavior and health of the\nmachine. However, there are often external factors or variables which are not\ncaptured by sensors leading to time-series which are inherently unpredictable.\nFor instance, manual controls and/or unmonitored environmental conditions or\nload may lead to inherently unpredictable time-series. Detecting anomalies in\nsuch scenarios becomes challenging using standard approaches based on\nmathematical models that rely on stationarity, or prediction models that\nutilize prediction errors to detect anomalies. We propose a Long Short Term\nMemory Networks based Encoder-Decoder scheme for Anomaly Detection (EncDec-AD)\nthat learns to reconstruct 'normal' time-series behavior, and thereafter uses\nreconstruction error to detect anomalies. We experiment with three publicly\navailable quasi predictable time-series datasets: power demand, space shuttle,\nand ECG, and two real-world engine datasets with both predictive and\nunpredictable behavior. We show that EncDec-AD is robust and can detect\nanomalies from predictable, unpredictable, periodic, aperiodic, and\nquasi-periodic time-series. Further, we show that EncDec-AD is able to detect\nanomalies from short time-series (length as small as 30) as well as long\ntime-series (length as large as 500).\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 08:25:48 GMT"}, {"version": "v2", "created": "Mon, 11 Jul 2016 09:33:48 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Malhotra", "Pankaj", ""], ["Ramakrishnan", "Anusha", ""], ["Anand", "Gaurangi", ""], ["Vig", "Lovekesh", ""], ["Agarwal", "Puneet", ""], ["Shroff", "Gautam", ""]]}, {"id": "1607.00186", "submitter": "David Powers", "authors": "David M. W. Powers", "title": "Throwing fuel on the embers: Probability or Dichotomy, Cognitive or\n  Linguistic?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Prof. Robert Berwick's abstract for his forthcoming invited talk at the\nACL2016 workshop on Cognitive Aspects of Computational Language Learning\nrevives an ancient debate. Entitled \"Why take a chance?\", Berwick seems to\nrefer implicitly to Chomsky's critique of the statistical approach of Harris as\nwell as the currently dominant paradigms in CoNLL.\n  Berwick avoids Chomsky's use of \"innate\" but states that \"the debate over the\nexistence of sophisticated mental grammars was settled with Chomsky's Logical\nStructure of Linguistic Theory (1957/1975)\", acknowledging that \"this debate\nhas often been revived\".\n  This paper agrees with the view that this debate has long since been settled,\nbut with the opposite outcome! Given the embers have not yet died away, and the\nquestions remain fundamental, perhaps it is appropriate to refuel the debate,\nso I would like to join Bob in throwing fuel on this fire by reviewing the\nevidence against the Chomskian position!\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 10:01:11 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Powers", "David M. W.", ""]]}, {"id": "1607.00215", "submitter": "Ian Osband", "authors": "Ian Osband, Benjamin Van Roy", "title": "Why is Posterior Sampling Better than Optimism for Reinforcement\n  Learning?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational results demonstrate that posterior sampling for reinforcement\nlearning (PSRL) dramatically outperforms algorithms driven by optimism, such as\nUCRL2. We provide insight into the extent of this performance boost and the\nphenomenon that drives it. We leverage this insight to establish an\n$\\tilde{O}(H\\sqrt{SAT})$ Bayesian expected regret bound for PSRL in\nfinite-horizon episodic Markov decision processes, where $H$ is the horizon,\n$S$ is the number of states, $A$ is the number of actions and $T$ is the time\nelapsed. This improves upon the best previous bound of $\\tilde{O}(H S\n\\sqrt{AT})$ for any reinforcement learning algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 11:58:28 GMT"}, {"version": "v2", "created": "Fri, 22 Jul 2016 22:43:10 GMT"}, {"version": "v3", "created": "Tue, 13 Jun 2017 15:54:51 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Osband", "Ian", ""], ["Van Roy", "Benjamin", ""]]}, {"id": "1607.00234", "submitter": "Florentin Smarandache", "authors": "Florentin Smarandache", "title": "Neutrosophic Overset, Neutrosophic Underset, and Neutrosophic Offset.\n  Similarly for Neutrosophic Over-/Under-/Off- Logic, Probability, and\n  Statistics", "comments": "170 pages", "journal-ref": "Pons Editions, Bruxelles, 2016", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neutrosophic Over-/Under-/Off-Set and -Logic were defined by the author in\n1995 and published for the first time in 2007. We extended the neutrosophic set\nrespectively to Neutrosophic Overset {when some neutrosophic component is over\n1}, Neutrosophic Underset {when some neutrosophic component is below 0}, and to\nNeutrosophic Offset {when some neutrosophic components are off the interval [0,\n1], i.e. some neutrosophic component over 1 and other neutrosophic component\nbelow 0}. This is no surprise with respect to the classical fuzzy set/logic,\nintuitionistic fuzzy set/logic, or classical/imprecise probability, where the\nvalues are not allowed outside the interval [0, 1], since our real-world has\nnumerous examples and applications of over-/under-/off-neutrosophic components.\nFor example, person working overtime deserves a membership degree over 1, while\na person producing more damage than benefit to a company deserves a membership\nbelow 0. Then, similarly, the Neutrosophic Logic/Measure/Probability/Statistics\netc. were extended to respectively Neutrosophic Over-/Under-/Off-Logic,\n-Measure, -Probability, -Statistics etc. [Smarandache, 2007].\n", "versions": [{"version": "v1", "created": "Thu, 30 Jun 2016 02:17:59 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Smarandache", "Florentin", ""]]}, {"id": "1607.00279", "submitter": "Nick Condry", "authors": "Nick Condry", "title": "Meaningful Models: Utilizing Conceptual Structure to Improve Machine\n  Learning Interpretability", "comments": "5 pages, 3 figures, presented at 2016 ICML Workshop on Human\n  Interpretability in Machine Learning (WHI 2016), New York, NY", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The last decade has seen huge progress in the development of advanced machine\nlearning models; however, those models are powerless unless human users can\ninterpret them. Here we show how the mind's construction of concepts and\nmeaning can be used to create more interpretable machine learning models. By\nproposing a novel method of classifying concepts, in terms of 'form' and\n'function', we elucidate the nature of meaning and offer proposals to improve\nmodel understandability. As machine learning begins to permeate daily life,\ninterpretable models may serve as a bridge between domain-expert authors and\nnon-expert users.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 15:07:52 GMT"}], "update_date": "2016-07-04", "authors_parsed": [["Condry", "Nick", ""]]}, {"id": "1607.00410", "submitter": "Yusuke Watanabe Dr.", "authors": "Yusuke Watanabe, Kazuma Hashimoto, Yoshimasa Tsuruoka", "title": "Domain Adaptation for Neural Networks by Parameter Augmentation", "comments": "9 page. To appear in the first ACL Workshop on Representation\n  Learning for NLP", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple domain adaptation method for neural networks in a\nsupervised setting. Supervised domain adaptation is a way of improving the\ngeneralization performance on the target domain by using the source domain\ndataset, assuming that both of the datasets are labeled. Recently, recurrent\nneural networks have been shown to be successful on a variety of NLP tasks such\nas caption generation; however, the existing domain adaptation techniques are\nlimited to (1) tune the model parameters by the target dataset after the\ntraining by the source dataset, or (2) design the network to have dual output,\none for the source domain and the other for the target domain. Reformulating\nthe idea of the domain adaptation technique proposed by Daume (2007), we\npropose a simple domain adaptation method, which can be applied to neural\nnetworks trained with a cross-entropy loss. On captioning datasets, we show\nperformance improvements over other domain adaptation methods.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 21:24:21 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Watanabe", "Yusuke", ""], ["Hashimoto", "Kazuma", ""], ["Tsuruoka", "Yoshimasa", ""]]}, {"id": "1607.00424", "submitter": "Ameet Soni", "authors": "Dileep Viswanathan and Ameet Soni and Jude Shavlik and Sriraam\n  Natarajan", "title": "Learning Relational Dependency Networks for Relation Extraction", "comments": "In Proceedings of Sixth International Workshop on Statistical\n  Relational AI at the 25th International Joint Conference on Artificial\n  Intelligence (IJCAI)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of KBP slot filling -- extracting relation information\nfrom newswire documents for knowledge base construction. We present our\npipeline, which employs Relational Dependency Networks (RDNs) to learn\nlinguistic patterns for relation extraction. Additionally, we demonstrate how\nseveral components such as weak supervision, word2vec features, joint learning\nand the use of human advice, can be incorporated in this relational framework.\nWe evaluate the different components in the benchmark KBP 2015 task and show\nthat RDNs effectively model a diverse set of features and perform competitively\nwith current state-of-the-art relation extraction.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 22:11:38 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Viswanathan", "Dileep", ""], ["Soni", "Ameet", ""], ["Shavlik", "Jude", ""], ["Natarajan", "Sriraam", ""]]}, {"id": "1607.00428", "submitter": "Haley Garrison", "authors": "Haley Garrison and Sonia Chernova", "title": "Situated Structure Learning of a Bayesian Logic Network for Commonsense\n  Reasoning", "comments": "International Joint Conference on Artificial Intelligence (IJCAI),\n  StarAI workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper details the implementation of an algorithm for automatically\ngenerating a high-level knowledge network to perform commonsense reasoning,\nspecifically with the application of robotic task repair. The network is\nrepresented using a Bayesian Logic Network (BLN) (Jain, Waldherr, and Beetz\n2009), which combines a set of directed relations between abstract concepts,\nincluding IsA, AtLocation, HasProperty, and UsedFor, with a corresponding\nprobability distribution that models the uncertainty inherent in these\nrelations. Inference over this network enables reasoning over the abstract\nconcepts in order to perform appropriate object substitution or to locate\nmissing objects in the robot's environment. The structure of the network is\ngenerated by combining information from two existing knowledge sources:\nConceptNet (Speer and Havasi 2012), and WordNet (Miller 1995). This is done in\na \"situated\" manner by only including information relevant a given context.\nResults show that the generated network is able to accurately predict object\ncategories, locations, properties, and affordances in three different household\nscenarios.\n", "versions": [{"version": "v1", "created": "Fri, 1 Jul 2016 22:52:57 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Garrison", "Haley", ""], ["Chernova", "Sonia", ""]]}, {"id": "1607.00446", "submitter": "Adam White", "authors": "Martha White and Adam White", "title": "A Greedy Approach to Adapting the Trace Parameter for Temporal\n  Difference Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the main obstacles to broad application of reinforcement learning\nmethods is the parameter sensitivity of our core learning algorithms. In many\nlarge-scale applications, online computation and function approximation\nrepresent key strategies in scaling up reinforcement learning algorithms. In\nthis setting, we have effective and reasonably well understood algorithms for\nadapting the learning-rate parameter, online during learning. Such\nmeta-learning approaches can improve robustness of learning and enable\nspecialization to current task, improving learning speed. For\ntemporal-difference learning algorithms which we study here, there is yet\nanother parameter, $\\lambda$, that similarly impacts learning speed and\nstability in practice. Unfortunately, unlike the learning-rate parameter,\n$\\lambda$ parametrizes the objective function that temporal-difference methods\noptimize. Different choices of $\\lambda$ produce different fixed-point\nsolutions, and thus adapting $\\lambda$ online and characterizing the\noptimization is substantially more complex than adapting the learning-rate\nparameter. There are no meta-learning method for $\\lambda$ that can achieve (1)\nincremental updating, (2) compatibility with function approximation, and (3)\nmaintain stability of learning under both on and off-policy sampling. In this\npaper we contribute a novel objective function for optimizing $\\lambda$ as a\nfunction of state rather than time. We derive a new incremental, linear\ncomplexity $\\lambda$-adaption algorithm that does not require offline batch\nupdating or access to a model of the world, and present a suite of experiments\nillustrating the practicality of our new algorithm in three different settings.\nTaken together, our contributions represent a concrete step towards black-box\napplication of temporal-difference learning methods in real world problems.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jul 2016 01:33:00 GMT"}, {"version": "v2", "created": "Mon, 24 Oct 2016 19:25:51 GMT"}], "update_date": "2016-10-25", "authors_parsed": [["White", "Martha", ""], ["White", "Adam", ""]]}, {"id": "1607.00474", "submitter": "Shobeir Fakhraei", "authors": "Shobeir Fakhraei, Dhanya Sridhar, Jay Pujara, Lise Getoor", "title": "Adaptive Neighborhood Graph Construction for Inference in\n  Multi-Relational Networks", "comments": "Presented at SIGKDD 12th International Workshop on Mining and\n  Learning with Graphs (MLG'16)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A neighborhood graph, which represents the instances as vertices and their\nrelations as weighted edges, is the basis of many semi-supervised and\nrelational models for node labeling and link prediction. Most methods employ a\nsequential process to construct the neighborhood graph. This process often\nconsists of generating a candidate graph, pruning the candidate graph to make a\nneighborhood graph, and then performing inference on the variables (i.e.,\nnodes) in the neighborhood graph. In this paper, we propose a framework that\ncan dynamically adapt the neighborhood graph based on the states of variables\nfrom intermediate inference results, as well as structural properties of the\nrelations connecting them. A key strength of our framework is its ability to\nhandle multi-relational data and employ varying amounts of relations for each\ninstance based on the intermediate inference results. We formulate the link\nprediction task as inference on neighborhood graphs, and include preliminary\nresults illustrating the effects of different strategies in our proposed\nframework.\n", "versions": [{"version": "v1", "created": "Sat, 2 Jul 2016 07:41:45 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Fakhraei", "Shobeir", ""], ["Sridhar", "Dhanya", ""], ["Pujara", "Jay", ""], ["Getoor", "Lise", ""]]}, {"id": "1607.00623", "submitter": "Kaveh Hassani", "authors": "Kaveh Hassani and Won-Sook Lee", "title": "Visualizing Natural Language Descriptions: A Survey", "comments": "Due to copyright most of the figures only appear in the journal\n  version", "journal-ref": "ACM Computing Surveys, Volume 49 Issue 1, Article No. 17, June\n  2016", "doi": "10.1145/2932710", "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.GR cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A natural language interface exploits the conceptual simplicity and\nnaturalness of the language to create a high-level user-friendly communication\nchannel between humans and machines. One of the promising applications of such\ninterfaces is generating visual interpretations of semantic content of a given\nnatural language that can be then visualized either as a static scene or a\ndynamic animation. This survey discusses requirements and challenges of\ndeveloping such systems and reports 26 graphical systems that exploit natural\nlanguage interfaces and addresses both artificial intelligence and\nvisualization aspects. This work serves as a frame of reference to researchers\nand to enable further advances in the field.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jul 2016 10:30:40 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Hassani", "Kaveh", ""], ["Lee", "Won-Sook", ""]]}, {"id": "1607.00656", "submitter": "Gavin Rens", "authors": "Gavin Rens and Deshendran Moodley", "title": "A Hybrid POMDP-BDI Agent Architecture with Online Stochastic Planning\n  and Plan Caching", "comments": "26 pages, 3 figures, unpublished version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents an agent architecture for controlling an autonomous\nagent in stochastic environments. The architecture combines the partially\nobservable Markov decision process (POMDP) model with the\nbelief-desire-intention (BDI) framework. The Hybrid POMDP-BDI agent\narchitecture takes the best features from the two approaches, that is, the\nonline generation of reward-maximizing courses of action from POMDP theory, and\nsophisticated multiple goal management from BDI theory. We introduce the\nadvances made since the introduction of the basic architecture, including (i)\nthe ability to pursue multiple goals simultaneously and (ii) a plan library for\nstoring pre-written plans and for storing recently generated plans for future\nreuse. A version of the architecture without the plan library is implemented\nand is evaluated using simulations. The results of the simulation experiments\nindicate that the approach is feasible.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jul 2016 17:11:52 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Rens", "Gavin", ""], ["Moodley", "Deshendran", ""]]}, {"id": "1607.00695", "submitter": "Victor Sanchez-Anguix Dr.", "authors": "Victor Sanchez-Anguix and Reyhan Aydogan and Tim Baarslag and\n  Catholijn M. Jonker", "title": "Can we reach Pareto optimal outcomes using bottom-up approaches?", "comments": "2nd Workshop on Conflict Resolution in Decision Making\n  (COREDEMA@ECAI2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditionally, researchers in decision making have focused on attempting to\nreach Pareto Optimality using horizontal approaches, where optimality is\ncalculated taking into account every participant at the same time. Sometimes,\nthis may prove to be a difficult task (e.g., conflict, mistrust, no information\nsharing, etc.). In this paper, we explore the possibility of achieving Pareto\nOptimal outcomes in a group by using a bottom-up approach: discovering Pareto\noptimal outcomes by interacting in subgroups. We analytically show that Pareto\noptimal outcomes in a subgroup are also Pareto optimal in a supergroup of those\nagents in the case of strict, transitive, and complete preferences. Then, we\nempirically analyze the prospective usability and practicality of bottom-up\napproaches in a variety of decision making domains.\n", "versions": [{"version": "v1", "created": "Sun, 3 Jul 2016 22:44:57 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Sanchez-Anguix", "Victor", ""], ["Aydogan", "Reyhan", ""], ["Baarslag", "Tim", ""], ["Jonker", "Catholijn M.", ""]]}, {"id": "1607.00715", "submitter": "Sebastian Sardina", "authors": "Davide Aversa and Sebastian Sardina and Stavros Vassos", "title": "Path planning with Inventory-driven Jump-Point-Search", "comments": null, "journal-ref": "In Proceedings of the AAAI Conference on Artificial Intelligence\n  and Interactive Digital Entertainment (AIIDE), pp. 2-8, 2015", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many navigational domains the traversability of cells is conditioned on\nthe path taken. This is often the case in video-games, in which a character may\nneed to acquire a certain object (i.e., a key or a flying suit) to be able to\ntraverse specific locations (e.g., doors or high walls). In order for\nnon-player characters to handle such scenarios we present invJPS, an\n\"inventory-driven\" pathfinding approach based on the highly successful\ngrid-based Jump-Point-Search (JPS) algorithm. We show, formally and\nexperimentally, that the invJPS preserves JPS's optimality guarantees and its\nsymmetry breaking advantages in inventory-based variants of game maps.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 01:13:32 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Aversa", "Davide", ""], ["Sardina", "Sebastian", ""], ["Vassos", "Stavros", ""]]}, {"id": "1607.00791", "submitter": "Marcin Pietron", "authors": "M. Pietron and M. Wielgosz and K. Wiatr", "title": "Formal analysis of HTM Spatial Pooler performance under predefined\n  operation conditions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces mathematical formalism for Spatial (SP) of Hierarchical\nTemporal Memory (HTM) with a spacial consideration for its hardware\nimplementation. Performance of HTM network and its ability to learn and adjust\nto a problem at hand is governed by a large set of parameters. Most of\nparameters are codependent which makes creating efficient HTM-based solutions\nchallenging. It requires profound knowledge of the settings and their impact on\nthe performance of system. Consequently, this paper introduced a set of\nformulas which are to facilitate the design process by enhancing tedious\ntrial-and-error method with a tool for choosing initial parameters which enable\nquick learning convergence. This is especially important in hardware\nimplementations which are constrained by the limited resources of a platform.\nThe authors focused especially on a formalism of Spatial Pooler and derive at\nthe formulas for quality and convergence of the model. This may be considered\nas recipes for designing efficient HTM models for given input patterns.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 09:20:29 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Pietron", "M.", ""], ["Wielgosz", "M.", ""], ["Wiatr", "K.", ""]]}, {"id": "1607.00819", "submitter": "Sylwia Polberg", "authors": "Sylwia Polberg", "title": "Understanding the Abstract Dialectical Framework (Preliminary Report)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among the most general structures extending the framework by Dung are the\nabstract dialectical frameworks (ADFs). They come equipped with various types\nof semantics, with the most prominent - the labeling-based one - analyzed in\nthe context of computational complexity, signatures, instantiations and\nsoftware support. This makes the abstract dialectical frameworks valuable tools\nfor argumentation. However, there are fewer results available concerning the\nrelation between the ADFs and other argumentation frameworks. In this paper we\nwould like to address this issue by introducing a number of translations from\nvarious formalisms into ADFs. The results of our study show the similarities\nand differences between them, thus promoting the use and understanding of ADFs.\nMoreover, our analysis also proves their capability to model many of the\nexisting frameworks, including those that go beyond the attack relation.\nFinally, translations allow other structures to benefit from the research on\nADFs in general and from the existing software in particular.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 10:52:57 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Polberg", "Sylwia", ""]]}, {"id": "1607.00869", "submitter": "Vinu E V", "authors": "Vinu E.V, Tahani Alsubait, P. Sreenivasa Kumar", "title": "Modeling of Item-Difficulty for Ontology-based MCQs", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multiple choice questions (MCQs) that can be generated from a domain ontology\ncan significantly reduce human effort & time required for authoring &\nadministering assessments in an e-Learning environment. Even though here are\nvarious methods for generating MCQs from ontologies, methods for determining\nthe difficulty-levels of such MCQs are less explored. In this paper, we study\nvarious aspects and factors that are involved in determining the\ndifficulty-score of an MCQ, and propose an ontology-based model for the\nprediction. This model characterizes the difficulty values associated with the\nstem and choice set of the MCQs, and describes a measure which combines both\nthe scores. Further more, the notion of assigning difficultly-scores based on\nthe skill level of the test taker is utilized for predicating difficulty-score\nof a stem. We studied the effectiveness of the predicted difficulty-scores with\nthe help of a psychometric model from the Item Response Theory, by involving\nreal-students and domain experts. Our results show that, the predicated\ndifficulty-levels of the MCQs are having high correlation with their actual\ndifficulty-levels.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 13:05:55 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["E.", "Vinu", "V"], ["Alsubait", "Tahani", ""], ["Kumar", "P. Sreenivasa", ""]]}, {"id": "1607.00872", "submitter": "Patrick O. Glauner", "authors": "Patrick Glauner, Jorge Meira, Lautaro Dolberg, Radu State, Franck\n  Bettinger, Yves Rangoni, Diogo Duarte", "title": "Neighborhood Features Help Detecting Non-Technical Losses in Big Data\n  Sets", "comments": "Proceedings of the 3rd IEEE/ACM International Conference on Big Data\n  Computing Applications and Technologies (BDCAT 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Electricity theft is a major problem around the world in both developed and\ndeveloping countries and may range up to 40% of the total electricity\ndistributed. More generally, electricity theft belongs to non-technical losses\n(NTL), which are losses that occur during the distribution of electricity in\npower grids. In this paper, we build features from the neighborhood of\ncustomers. We first split the area in which the customers are located into\ngrids of different sizes. For each grid cell we then compute the proportion of\ninspected customers and the proportion of NTL found among the inspected\ncustomers. We then analyze the distributions of features generated and show why\nthey are useful to predict NTL. In addition, we compute features from the\nconsumption time series of customers. We also use master data features of\ncustomers, such as their customer class and voltage of their connection. We\ncompute these features for a Big Data base of 31M meter readings, 700K\ncustomers and 400K inspection results. We then use these features to train four\nmachine learning algorithms that are particularly suitable for Big Data sets\nbecause of their parallelizable structure: logistic regression, k-nearest\nneighbors, linear support vector machine and random forest. Using the\nneighborhood features instead of only analyzing the time series has resulted in\nappreciable results for Big Data sets for varying NTL proportions of 1%-90%.\nThis work can therefore be deployed to a wide range of different regions around\nthe world.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 13:08:19 GMT"}, {"version": "v2", "created": "Tue, 25 Jul 2017 04:41:44 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Glauner", "Patrick", ""], ["Meira", "Jorge", ""], ["Dolberg", "Lautaro", ""], ["State", "Radu", ""], ["Bettinger", "Franck", ""], ["Rangoni", "Yves", ""], ["Duarte", "Diogo", ""]]}, {"id": "1607.00888", "submitter": "Alexander Semenov", "authors": "Ilya Otpuschennikov, Alexander Semenov, Irina Gribanova, Oleg Zaikin,\n  Stepan Kochemazov", "title": "Encoding Cryptographic Functions to SAT Using Transalg System", "comments": "Short variant of this paper was accepted to ECAI2016 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we propose the technology for constructing propositional\nencodings of discrete functions. It is aimed at solving inversion problems of\nconsidered functions using state-of-the-art SAT solvers. We implemented this\ntechnology in the form of the software system called Transalg, and used it to\nconstruct SAT encodings for a number of cryptanalysis problems. By applying SAT\nsolvers to these encodings we managed to invert several cryptographic\nfunctions. In particular, we used the SAT encodings produced by Transalg to\nconstruct the family of two-block MD5 collisions in which the first 10 bytes\nare zeros. Also we used Transalg encoding for the widely known A5/1 keystream\ngenerator to solve several dozen of its cryptanalysis instances in a\ndistributed computing environment. In the paper we compare in detail the\nfunctionality of Transalg with that of similar software systems.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 13:57:35 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Otpuschennikov", "Ilya", ""], ["Semenov", "Alexander", ""], ["Gribanova", "Irina", ""], ["Zaikin", "Oleg", ""], ["Kochemazov", "Stepan", ""]]}, {"id": "1607.00913", "submitter": "Iyad Rahwan", "authors": "Manuel Alfonseca, Manuel Cebrian, Antonio Fernandez Anta, Lorenzo\n  Coviello, Andres Abeliuk, Iyad Rahwan", "title": "Superintelligence cannot be contained: Lessons from Computability Theory", "comments": "7 pages, 5 figures", "journal-ref": "Journal of Artificial Intelligence Research (JAIR) 70 (2021) 65-76", "doi": "10.1613/jair.1.12202", "report-no": null, "categories": "cs.CY cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Superintelligence is a hypothetical agent that possesses intelligence far\nsurpassing that of the brightest and most gifted human minds. In light of\nrecent advances in machine intelligence, a number of scientists, philosophers\nand technologists have revived the discussion about the potential catastrophic\nrisks entailed by such an entity. In this article, we trace the origins and\ndevelopment of the neo-fear of superintelligence, and some of the major\nproposals for its containment. We argue that such containment is, in principle,\nimpossible, due to fundamental limits inherent to computing itself. Assuming\nthat a superintelligence will contain a program that includes all the programs\nthat can be executed by a universal Turing machine on input potentially as\ncomplex as the state of the world, strict containment requires simulations of\nsuch a program, something theoretically (and practically) infeasible.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 14:44:21 GMT"}], "update_date": "2021-01-12", "authors_parsed": [["Alfonseca", "Manuel", ""], ["Cebrian", "Manuel", ""], ["Anta", "Antonio Fernandez", ""], ["Coviello", "Lorenzo", ""], ["Abeliuk", "Andres", ""], ["Rahwan", "Iyad", ""]]}, {"id": "1607.00976", "submitter": "Silvio Amir", "authors": "Silvio Amir, Byron C. Wallace, Hao Lyu, Paula Carvalho M\\'ario J.\n  Silva", "title": "Modelling Context with User Embeddings for Sarcasm Detection in Social\n  Media", "comments": "published as a conference paper at CONLL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a deep neural network for automated sarcasm detection. Recent\nwork has emphasized the need for models to capitalize on contextual features,\nbeyond lexical and syntactic cues present in utterances. For example, different\nspeakers will tend to employ sarcasm regarding different subjects and, thus,\nsarcasm detection models ought to encode such speaker information. Current\nmethods have achieved this by way of laborious feature engineering. By\ncontrast, we propose to automatically learn and then exploit user embeddings,\nto be used in concert with lexical signals to recognize sarcasm. Our approach\ndoes not require elaborate feature engineering (and concomitant data scraping);\nfitting user embeddings requires only the text from their previous posts. The\nexperimental results show that our model outperforms a state-of-the-art\napproach leveraging an extensive set of carefully crafted features.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 18:04:18 GMT"}, {"version": "v2", "created": "Tue, 5 Jul 2016 02:27:41 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Amir", "Silvio", ""], ["Wallace", "Byron C.", ""], ["Lyu", "Hao", ""], ["Silva", "Paula Carvalho M\u00e1rio J.", ""]]}, {"id": "1607.00992", "submitter": "Jay Pujara", "authors": "Jay Pujara and Lise Getoor", "title": "Generic Statistical Relational Entity Resolution in Knowledge Graphs", "comments": null, "journal-ref": "In the Sixth International Workshop on Statistical Relational AI,\n  2016", "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity resolution, the problem of identifying the underlying entity of\nreferences found in data, has been researched for many decades in many\ncommunities. A common theme in this research has been the importance of\nincorporating relational features into the resolution process. Relational\nentity resolution is particularly important in knowledge graphs (KGs), which\nhave a regular structure capturing entities and their interrelationships. We\nidentify three major problems in KG entity resolution: (1) intra-KG reference\nambiguity; (2) inter-KG reference ambiguity; and (3) ambiguity when extending\nKGs with new facts. We implement a framework that generalizes across these\nthree settings and exploits this regular structure of KGs. Our framework has\nmany advantages over custom solutions widely deployed in industry, including\ncollective inference, scalability, and interpretability. We apply our framework\nto two real-world KG entity resolution problems, ambiguity in NELL and merging\ndata from Freebase and MusicBrainz, demonstrating the importance of relational\nfeatures.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 19:02:47 GMT"}], "update_date": "2016-07-05", "authors_parsed": [["Pujara", "Jay", ""], ["Getoor", "Lise", ""]]}, {"id": "1607.01036", "submitter": "Jun Han Mr", "authors": "Jun Han, Qiang Liu", "title": "Bootstrap Model Aggregation for Distributed Statistical Learning", "comments": "This paper is about variance reduction on Monte Carol estimation of\n  KL divergence, NIPS, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In distributed, or privacy-preserving learning, we are often given a set of\nprobabilistic models estimated from different local repositories, and asked to\ncombine them into a single model that gives efficient statistical estimation. A\nsimple method is to linearly average the parameters of the local models, which,\nhowever, tends to be degenerate or not applicable on non-convex models, or\nmodels with different parameter dimensions. One more practical strategy is to\ngenerate bootstrap samples from the local models, and then learn a joint model\nbased on the combined bootstrap set. Unfortunately, the bootstrap procedure\nintroduces additional noise and can significantly deteriorate the performance.\nIn this work, we propose two variance reduction methods to correct the\nbootstrap noise, including a weighted M-estimator that is both statistically\nefficient and practically powerful. Both theoretical and empirical analysis is\nprovided to demonstrate our methods.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 20:12:41 GMT"}, {"version": "v2", "created": "Wed, 1 Feb 2017 20:06:40 GMT"}, {"version": "v3", "created": "Sun, 5 Feb 2017 03:07:51 GMT"}, {"version": "v4", "created": "Mon, 27 Feb 2017 21:01:20 GMT"}], "update_date": "2017-03-01", "authors_parsed": [["Han", "Jun", ""], ["Liu", "Qiang", ""]]}, {"id": "1607.01050", "submitter": "Sriraam Natarajan", "authors": "Shuo Yang, Mohammed Korayem, Khalifeh AlJadda, Trey Grainger, Sriraam\n  Natarajan", "title": "Application of Statistical Relational Learning to Hybrid Recommendation\n  Systems", "comments": "Statistical Relational AI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation systems usually involve exploiting the relations among known\nfeatures and content that describe items (content-based filtering) or the\noverlap of similar users who interacted with or rated the target item\n(collaborative filtering). To combine these two filtering approaches, current\nmodel-based hybrid recommendation systems typically require extensive feature\nengineering to construct a user profile. Statistical Relational Learning (SRL)\nprovides a straightforward way to combine the two approaches. However, due to\nthe large scale of the data used in real world recommendation systems, little\nresearch exists on applying SRL models to hybrid recommendation systems, and\nessentially none of that research has been applied on real big-data-scale\nsystems. In this paper, we proposed a way to adapt the state-of-the-art in SRL\nlearning approaches to construct a real hybrid recommendation system.\nFurthermore, in order to satisfy a common requirement in recommendation systems\n(i.e. that false positives are more undesirable and therefore penalized more\nharshly than false negatives), our approach can also allow tuning the trade-off\nbetween the precision and recall of the system in a principled way. Our\nexperimental results demonstrate the efficiency of our proposed approach as\nwell as its improved performance on recommendation precision.\n", "versions": [{"version": "v1", "created": "Mon, 4 Jul 2016 21:21:59 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Yang", "Shuo", ""], ["Korayem", "Mohammed", ""], ["AlJadda", "Khalifeh", ""], ["Grainger", "Trey", ""], ["Natarajan", "Sriraam", ""]]}, {"id": "1607.01115", "submitter": "Suyog Jain", "authors": "Suyog Dutt Jain, Kristen Grauman", "title": "Click Carving: Segmenting Objects in Video with Point Clicks", "comments": "A preliminary version of the material in this document was filed as\n  University of Texas technical report no. UT AI16-01", "journal-ref": null, "doi": null, "report-no": "University of Texas Technical Report UT AI16-01", "categories": "cs.CV cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel form of interactive video object segmentation where a few\nclicks by the user helps the system produce a full spatio-temporal segmentation\nof the object of interest. Whereas conventional interactive pipelines take the\nuser's initialization as a starting point, we show the value in the system\ntaking the lead even in initialization. In particular, for a given video frame,\nthe system precomputes a ranked list of thousands of possible segmentation\nhypotheses (also referred to as object region proposals) using image and motion\ncues. Then, the user looks at the top ranked proposals, and clicks on the\nobject boundary to carve away erroneous ones. This process iterates (typically\n2-3 times), and each time the system revises the top ranked proposal set, until\nthe user is satisfied with a resulting segmentation mask. Finally, the mask is\npropagated across the video to produce a spatio-temporal object tube. On three\nchallenging datasets, we provide extensive comparisons with both existing work\nand simpler alternative methods. In all, the proposed Click Carving approach\nstrikes an excellent balance of accuracy and human effort. It outperforms all\nsimilarly fast methods, and is competitive or better than those requiring 2 to\n12 times the effort.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 05:35:22 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Jain", "Suyog Dutt", ""], ["Grauman", "Kristen", ""]]}, {"id": "1607.01202", "submitter": "Vladislav Nenchev", "authors": "Vladislav Nenchev, Christos G. Cassandras and J\\\"org Raisch", "title": "Optimal control for a robotic exploration, pick-up and delivery problem", "comments": "14 pages, 23 figures", "journal-ref": null, "doi": "10.1016/j.nahs.2018.06.004", "report-no": null, "categories": "cs.SY cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses an optimal control problem for a robot that has to find\nand collect a finite number of objects and move them to a depot in minimum\ntime. The robot has fourth-order dynamics that change instantaneously at any\npick-up or drop-off of an object. The objects are modeled by point masses with\na-priori unknown locations in a bounded two-dimensional space that may contain\nunknown obstacles. For this hybrid system, an Optimal Control Problem (OCP) is\napproximately solved by a receding horizon scheme, where the derived lower\nbound for the cost-to-go is evaluated for the worst and for a probabilistic\ncase, assuming a uniform distribution of the objects. First, a time-driven\napproximate solution based on time and position space discretization and mixed\ninteger programming is presented. Due to the high computational cost of this\nsolution, an alternative event-driven approximate approach based on a suitable\nmotion parameterization and gradient-based optimization is proposed. The\nsolutions are compared in a numerical example, suggesting that the latter\napproach offers a significant computational advantage while yielding similar\nqualitative results compared to the former. The methods are particularly\nrelevant for various robotic applications like automated cleaning, search and\nrescue, harvesting or manufacturing.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 11:53:13 GMT"}], "update_date": "2021-01-27", "authors_parsed": [["Nenchev", "Vladislav", ""], ["Cassandras", "Christos G.", ""], ["Raisch", "J\u00f6rg", ""]]}, {"id": "1607.01254", "submitter": "Jagannath Roy", "authors": "Jagannath Roy, Ananta Ranjan, Animesh Debnath, Samarjit Kar", "title": "An extended MABAC for multi-attribute decision making using trapezoidal\n  interval type-2 fuzzy numbers", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we attempt to extend Multi Attributive Border Approximation\narea Comparison (MABAC) approach for multi-attribute decision making (MADM)\nproblems based on type-2 fuzzy sets (IT2FSs). As a special case of IT2FSs\ninterval type-2 trapezoidal fuzzy numbers (IT2TrFNs) are adopted here to deal\nwith uncertainties present in many practical evaluation and selection problems.\nA systematic description of MABAC based on IT2TrFNs is presented in the current\nstudy. The validity and feasibility of the proposed method are illustrated by a\npractical example of selecting the most suitable candidate for a software\ncompany which is heading to hire a system analysis engineer based on few\nattributes. Finally, a comparison with two other existing MADM methods is\ndescribed.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 14:05:29 GMT"}, {"version": "v2", "created": "Thu, 14 Jul 2016 03:04:48 GMT"}, {"version": "v3", "created": "Mon, 21 Nov 2016 14:53:36 GMT"}, {"version": "v4", "created": "Fri, 2 Dec 2016 08:50:29 GMT"}], "update_date": "2016-12-05", "authors_parsed": [["Roy", "Jagannath", ""], ["Ranjan", "Ananta", ""], ["Debnath", "Animesh", ""], ["Kar", "Samarjit", ""]]}, {"id": "1607.01337", "submitter": "P{\\aa}l Sunds{\\o}y", "authors": "P{\\aa}l Sunds{\\o}y", "title": "Can mobile usage predict illiteracy in a developing country?", "comments": "10 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The present study provides the first evidence that illiteracy can be reliably\npredicted from standard mobile phone logs. By deriving a broad set of mobile\nphone indicators reflecting users financial, social and mobility patterns we\nshow how supervised machine learning can be used to predict individual\nilliteracy in an Asian developing country, externally validated against a\nlarge-scale survey. On average the model performs 10 times better than random\nguessing with a 70% accuracy. Further we show how individual illiteracy can be\naggregated and mapped geographically at cell tower resolution. Geographical\nmapping of illiteracy is crucial to know where the illiterate people are, and\nwhere to put in resources. In underdeveloped countries such mappings are often\nbased on out-dated household surveys with low spatial and temporal resolution.\nOne in five people worldwide struggle with illiteracy, and it is estimated that\nilliteracy costs the global economy more than 1 trillion dollars each year.\nThese results potentially enable costeffective, questionnaire-free\ninvestigation of illiteracy-related questions on an unprecedented scale\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 17:22:18 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["Sunds\u00f8y", "P\u00e5l", ""]]}, {"id": "1607.01381", "submitter": "Yahel David", "authors": "Yahel David, Dotan Di Castro and Zohar Karnin", "title": "One-Shot Session Recommendation Systems with Combinatorial Items", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, content recommendation systems in large websites (or\n\\emph{content providers}) capture an increased focus. While the type of content\nvaries, e.g.\\ movies, articles, music, advertisements, etc., the high level\nproblem remains the same. Based on knowledge obtained so far on the user,\nrecommend the most desired content. In this paper we present a method to handle\nthe well known user-cold-start problem in recommendation systems. In this\nscenario, a recommendation system encounters a new user and the objective is to\npresent items as relevant as possible with the hope of keeping the user's\nsession as long as possible. We formulate an optimization problem aimed to\nmaximize the length of this initial session, as this is believed to be the key\nto have the user come back and perhaps register to the system. In particular,\nour model captures the fact that a single round with low quality recommendation\nis likely to terminate the session. In such a case, we do not proceed to the\nnext round as the user leaves the system, possibly never to seen again. We\ndenote this phenomenon a \\emph{One-Shot Session}. Our optimization problem is\nformulated as an MDP where the action space is of a combinatorial nature as we\nrecommend in each round, multiple items. This huge action space presents a\ncomputational challenge making the straightforward solution intractable. We\nanalyze the structure of the MDP to prove monotone and submodular like\nproperties that allow a computationally efficient solution via a method denoted\nby \\emph{Greedy Value Iteration} (G-VI).\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 19:40:56 GMT"}], "update_date": "2016-07-06", "authors_parsed": [["David", "Yahel", ""], ["Di Castro", "Dotan", ""], ["Karnin", "Zohar", ""]]}, {"id": "1607.01478", "submitter": "Masahiro Ono", "authors": "Masahiro Ono, Mahmoud El Chamie, Marco Pavone, Behcet Acikmese", "title": "Mixed Strategy for Constrained Stochastic Optimal Control", "comments": "11 pages. 9 figures.Preliminary version of a working journal paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Choosing control inputs randomly can result in a reduced expected cost in\noptimal control problems with stochastic constraints, such as stochastic model\npredictive control (SMPC). We consider a controller with initial randomization,\nmeaning that the controller randomly chooses from K+1 control sequences at the\nbeginning (called K-randimization).It is known that, for a finite-state,\nfinite-action Markov Decision Process (MDP) with K constraints, K-randimization\nis sufficient to achieve the minimum cost. We found that the same result holds\nfor stochastic optimal control problems with continuous state and action\nspaces.Furthermore, we show the randomization of control input can result in\nreduced cost when the optimization problem is nonconvex, and the cost reduction\nis equal to the duality gap. We then provide the necessary and sufficient\nconditions for the optimality of a randomized solution, and develop an\nefficient solution method based on dual optimization. Furthermore, in a special\ncase with K=1 such as a joint chance-constrained problem, the dual optimization\ncan be solved even more efficiently by root finding. Finally, we test the\ntheories and demonstrate the solution method on multiple practical problems\nranging from path planning to the planning of entry, descent, and landing (EDL)\nfor future Mars missions.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 04:23:36 GMT"}], "update_date": "2016-07-07", "authors_parsed": [["Ono", "Masahiro", ""], ["Chamie", "Mahmoud El", ""], ["Pavone", "Marco", ""], ["Acikmese", "Behcet", ""]]}, {"id": "1607.01490", "submitter": "Normunds Gruzitis", "authors": "Ren\\=ars Liepi\\c{n}\\v{s}, Uldis Boj\\=ars, Normunds Gr\\=uz\\=itis,\n  K\\=arlis \\v{C}er\\=ans, Edgars Celms", "title": "Towards Self-explanatory Ontology Visualization with Contextual\n  Verbalization", "comments": null, "journal-ref": "Databases and Information Systems, Communications in Computer and\n  Information Science, Vol. 615, Springer, 2016, pp. 3-17", "doi": "10.1007/978-3-319-40180-5_1", "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ontologies are one of the core foundations of the Semantic Web. To\nparticipate in Semantic Web projects, domain experts need to be able to\nunderstand the ontologies involved. Visual notations can provide an overview of\nthe ontology and help users to understand the connections among entities.\nHowever, the users first need to learn the visual notation before they can\ninterpret it correctly. Controlled natural language representation would be\nreadable right away and might be preferred in case of complex axioms, however,\nthe structure of the ontology would remain less apparent. We propose to combine\nontology visualizations with contextual ontology verbalizations of selected\nontology (diagram) elements, displaying controlled natural language (CNL)\nexplanations of OWL axioms corresponding to the selected visual notation\nelements. Thus, the domain experts will benefit from both the high-level\noverview provided by the graphical notation and the detailed textual\nexplanations of particular elements in the diagram.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 06:58:31 GMT"}], "update_date": "2016-07-07", "authors_parsed": [["Liepi\u0146\u0161", "Ren\u0101rs", ""], ["Boj\u0101rs", "Uldis", ""], ["Gr\u016bz\u012btis", "Normunds", ""], ["\u010cer\u0101ns", "K\u0101rlis", ""], ["Celms", "Edgars", ""]]}, {"id": "1607.01634", "submitter": "Sumita Basu", "authors": "Sumita Basu", "title": "Lattice Structure of Variable Precision Rough Sets", "comments": "9 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The main purpose of this paper is to study the lattice structure of variable\nprecision rough sets. The notion of variation in precision of rough sets have\nbeen further extended to variable precision rough set with variable\nclassification error and its algebraic properties are also studied.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 16:27:42 GMT"}], "update_date": "2016-07-07", "authors_parsed": [["Basu", "Sumita", ""]]}, {"id": "1607.01690", "submitter": "Cen Wan", "authors": "Cen Wan and Alex A. Freitas", "title": "A New Hierarchical Redundancy Eliminated Tree Augmented Naive Bayes\n  Classifier for Coping with Gene Ontology-based Features", "comments": "International Conference on Machine Learning (ICML 2016)\n  Computational Biology Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Tree Augmented Naive Bayes classifier is a type of probabilistic\ngraphical model that can represent some feature dependencies. In this work, we\npropose a Hierarchical Redundancy Eliminated Tree Augmented Naive Bayes\n(HRE-TAN) algorithm, which considers removing the hierarchical redundancy\nduring the classifier learning process, when coping with data containing\nhierarchically structured features. The experiments showed that HRE-TAN obtains\nsignificantly better predictive performance than the conventional Tree\nAugmented Naive Bayes classifier, and enhanced the robustness against\nimbalanced class distributions, in aging-related gene datasets with Gene\nOntology terms used as features.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 16:00:43 GMT"}], "update_date": "2016-07-07", "authors_parsed": [["Wan", "Cen", ""], ["Freitas", "Alex A.", ""]]}, {"id": "1607.01719", "submitter": "Baochen Sun", "authors": "Baochen Sun, Kate Saenko", "title": "Deep CORAL: Correlation Alignment for Deep Domain Adaptation", "comments": "Extended Abstract", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are able to learn powerful representations from large\nquantities of labeled input data, however they cannot always generalize well\nacross changes in input distributions. Domain adaptation algorithms have been\nproposed to compensate for the degradation in performance due to domain shift.\nIn this paper, we address the case when the target domain is unlabeled,\nrequiring unsupervised adaptation. CORAL is a \"frustratingly easy\" unsupervised\ndomain adaptation method that aligns the second-order statistics of the source\nand target distributions with a linear transformation. Here, we extend CORAL to\nlearn a nonlinear transformation that aligns correlations of layer activations\nin deep neural networks (Deep CORAL). Experiments on standard benchmark\ndatasets show state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 17:35:55 GMT"}], "update_date": "2016-07-07", "authors_parsed": [["Sun", "Baochen", ""], ["Saenko", "Kate", ""]]}, {"id": "1607.01729", "submitter": "Vikas Shivashankar", "authors": "Vikas Shivashankar, Ron Alford, Mark Roberts and David W. Aha", "title": "Cost-Optimal Algorithms for Planning with Procedural Control Knowledge", "comments": "To appear in the Proc. of ECAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is an impressive body of work on developing heuristics and other\nreasoning algorithms to guide search in optimal and anytime planning algorithms\nfor classical planning. However, very little effort has been directed towards\ndeveloping analogous techniques to guide search towards high-quality solutions\nin hierarchical planning formalisms like HTN planning, which allows using\nadditional domain-specific procedural control knowledge. In lieu of such\ntechniques, this control knowledge often needs to provide the necessary search\nguidance to the planning algorithm, which imposes a substantial burden on the\ndomain author and can yield brittle or error-prone domain models. We address\nthis gap by extending recent work on a new hierarchical goal-based planning\nformalism called Hierarchical Goal Network (HGN) Planning to develop the\nHierarchically-Optimal Goal Decomposition Planner (HOpGDP), an HGN planning\nalgorithm that computes hierarchically-optimal plans. HOpGDP is guided by\n$h_{HL}$, a new HGN planning heuristic that extends existing admissible\nlandmark-based heuristics from classical planning to compute admissible cost\nestimates for HGN planning problems. Our experimental evaluation across three\nbenchmark planning domains shows that HOpGDP compares favorably to both optimal\nclassical planners due to its ability to use domain-specific procedural\nknowledge, and a blind-search version of HOpGDP due to the search guidance\nprovided by $h_{HL}$.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 18:02:33 GMT"}, {"version": "v2", "created": "Thu, 7 Jul 2016 02:07:22 GMT"}], "update_date": "2016-07-08", "authors_parsed": [["Shivashankar", "Vikas", ""], ["Alford", "Ron", ""], ["Roberts", "Mark", ""], ["Aha", "David W.", ""]]}, {"id": "1607.01730", "submitter": "Jialin Liu Ph.D", "authors": "Jialin Liu, Diego P\\'erez-Li\\'ebana and Simon M. Lucas", "title": "Rolling Horizon Coevolutionary Planning for Two-Player Video Games", "comments": "2 figures, 1 table, 6 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a new algorithm for decision making in two-player\nreal-time video games. As with Monte Carlo Tree Search, the algorithm can be\nused without heuristics and has been developed for use in general video game\nAI. The approach is to extend recent work on rolling horizon evolutionary\nplanning, which has been shown to work well for single-player games, to two (or\nin principle many) player games. To select an action the algorithm co-evolves\ntwo (or in the general case N) populations, one for each player, where each\nindividual is a sequence of actions for the respective player. The fitness of\neach individual is evaluated by playing it against a selection of\naction-sequences from the opposing population. When choosing an action to take\nin the game, the first action is chosen from the fittest member of the\npopulation for that player. The new algorithm is compared with a number of\ngeneral video game AI algorithms on three variations of a two-player space\nbattle game, with promising results.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 18:03:18 GMT"}], "update_date": "2016-07-07", "authors_parsed": [["Liu", "Jialin", ""], ["P\u00e9rez-Li\u00e9bana", "Diego", ""], ["Lucas", "Simon M.", ""]]}, {"id": "1607.01869", "submitter": "Mihajlo Grbovic", "authors": "Mihajlo Grbovic, Nemanja Djuric, Vladan Radosavljevic, Fabrizio\n  Silvestri, Ricardo Baeza-Yates, Andrew Feng, Erik Ordentlich, Lee Yang, Gavin\n  Owens", "title": "Scalable Semantic Matching of Queries to Ads in Sponsored Search\n  Advertising", "comments": "10 pages, 4 figures, 39th International ACM SIGIR Conference on\n  Research and Development in Information Retrieval, SIGIR 2016, Pisa, Italy", "journal-ref": "39th International ACM SIGIR Conference on Research and\n  Development in Information Retrieval, SIGIR 2016, Pisa, Italy", "doi": "10.1145/2911451.2911538.", "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sponsored search represents a major source of revenue for web search engines.\nThis popular advertising model brings a unique possibility for advertisers to\ntarget users' immediate intent communicated through a search query, usually by\ndisplaying their ads alongside organic search results for queries deemed\nrelevant to their products or services. However, due to a large number of\nunique queries it is challenging for advertisers to identify all such relevant\nqueries. For this reason search engines often provide a service of advanced\nmatching, which automatically finds additional relevant queries for advertisers\nto bid on. We present a novel advanced matching approach based on the idea of\nsemantic embeddings of queries and ads. The embeddings were learned using a\nlarge data set of user search sessions, consisting of search queries, clicked\nads and search links, while utilizing contextual information such as dwell time\nand skipped ads. To address the large-scale nature of our problem, both in\nterms of data and vocabulary size, we propose a novel distributed algorithm for\ntraining of the embeddings. Finally, we present an approach for overcoming a\ncold-start problem associated with new ads and queries. We report results of\neditorial evaluation and online tests on actual search traffic. The results\nshow that our approach significantly outperforms baselines in terms of\nrelevance, coverage, and incremental revenue. Lastly, we open-source learned\nquery embeddings to be used by researchers in computational advertising and\nrelated fields.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 03:43:12 GMT"}], "update_date": "2016-07-08", "authors_parsed": [["Grbovic", "Mihajlo", ""], ["Djuric", "Nemanja", ""], ["Radosavljevic", "Vladan", ""], ["Silvestri", "Fabrizio", ""], ["Baeza-Yates", "Ricardo", ""], ["Feng", "Andrew", ""], ["Ordentlich", "Erik", ""], ["Yang", "Lee", ""], ["Owens", "Gavin", ""]]}, {"id": "1607.02018", "submitter": "Daniel P. Lupp", "authors": "Daniel P. Lupp and Evgenij Thorstensen", "title": "Mapping Data to Ontologies with Exceptions Using Answer Set Programming", "comments": "8 pages, ONTOLP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In ontology-based data access, databases are connected to an ontology via\nmappings from queries over the database to queries over the ontology. In this\npaper, we consider mappings from relational databases to first-order\nontologies, and define an ASP-based framework for GLAV mappings with queries\nover the ontology in the mapping rule bodies. We show that this type of\nmappings can be used to express constraints and exceptions, as well as being a\npowerful mechanism for succinctly representing OBDA mappings. We give an\nalgorithm for brave reasoning in this setting, and show that this problem has\neither the same data complexity as ASP (NP- complete), or it is at least as\nhard as the complexity of checking entailment for the ontology queries.\nFurthermore, we show that for ontologies with UCQ-rewritable queries there\nexists a natural reduction from mapping programs to \\exists-ASP, an extension\nof ASP with existential variables that itself admits a natural reduction to\nASP.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 14:00:06 GMT"}], "update_date": "2016-07-08", "authors_parsed": [["Lupp", "Daniel P.", ""], ["Thorstensen", "Evgenij", ""]]}, {"id": "1607.02061", "submitter": "Enrico Santus", "authors": "Emmanuele Chersoni, Enrico Santus, Alessandro Lenci, Philippe Blache\n  and Chu-Ren Huang", "title": "Representing Verbs with Rich Contexts: an Evaluation on Verb Similarity", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several studies on sentence processing suggest that the mental lexicon keeps\ntrack of the mutual expectations between words. Current DSMs, however,\nrepresent context words as separate features, thereby loosing important\ninformation for word expectations, such as word interrelations. In this paper,\nwe present a DSM that addresses this issue by defining verb contexts as joint\nsyntactic dependencies. We test our representation in a verb similarity task on\ntwo datasets, showing that joint contexts achieve performances comparable to\nsingle dependencies or even better. Moreover, they are able to overcome the\ndata sparsity problem of joint feature spaces, in spite of the limited size of\nour training corpus.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 16:00:33 GMT"}, {"version": "v2", "created": "Wed, 5 Oct 2016 10:49:58 GMT"}], "update_date": "2016-10-06", "authors_parsed": [["Chersoni", "Emmanuele", ""], ["Santus", "Enrico", ""], ["Lenci", "Alessandro", ""], ["Blache", "Philippe", ""], ["Huang", "Chu-Ren", ""]]}, {"id": "1607.02137", "submitter": "Earl Bellinger", "authors": "Earl P. Bellinger, George C. Angelou, Saskia Hekker, Sarbani Basu,\n  Warrick Ball, Elisabeth Guggenberger", "title": "Fundamental Parameters of Main-Sequence Stars in an Instant with Machine\n  Learning", "comments": "26 pages, 18 figures, accepted for publication in ApJ", "journal-ref": null, "doi": "10.3847/0004-637X/830/1/31", "report-no": null, "categories": "astro-ph.SR astro-ph.IM cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Owing to the remarkable photometric precision of space observatories like\nKepler, stellar and planetary systems beyond our own are now being\ncharacterized en masse for the first time. These characterizations are pivotal\nfor endeavors such as searching for Earth-like planets and solar twins,\nunderstanding the mechanisms that govern stellar evolution, and tracing the\ndynamics of our Galaxy. The volume of data that is becoming available, however,\nbrings with it the need to process this information accurately and rapidly.\nWhile existing methods can constrain fundamental stellar parameters such as\nages, masses, and radii from these observations, they require substantial\ncomputational efforts to do so.\n  We develop a method based on machine learning for rapidly estimating\nfundamental parameters of main-sequence solar-like stars from classical and\nasteroseismic observations. We first demonstrate this method on a\nhare-and-hound exercise and then apply it to the Sun, 16 Cyg A & B, and 34\nplanet-hosting candidates that have been observed by the Kepler spacecraft. We\nfind that our estimates and their associated uncertainties are comparable to\nthe results of other methods, but with the additional benefit of being able to\nexplore many more stellar parameters while using much less computation time. We\nfurthermore use this method to present evidence for an empirical diffusion-mass\nrelation. Our method is open source and freely available for the community to\nuse.\n  The source code for all analyses and for all figures appearing in this\nmanuscript can be found electronically at\nhttps://github.com/earlbellinger/asteroseismology\n", "versions": [{"version": "v1", "created": "Wed, 6 Jul 2016 20:41:25 GMT"}], "update_date": "2017-04-03", "authors_parsed": [["Bellinger", "Earl P.", ""], ["Angelou", "George C.", ""], ["Hekker", "Saskia", ""], ["Basu", "Sarbani", ""], ["Ball", "Warrick", ""], ["Guggenberger", "Elisabeth", ""]]}, {"id": "1607.02171", "submitter": "Eric Nunes", "authors": "Eric Nunes, Paulo Shakarian, Gerardo I. Simari, Andrew Ruef", "title": "Argumentation Models for Cyber Attribution", "comments": "8 pages paper to be presented at International Symposium on\n  Foundations of Open Source Intelligence and Security Informatics (FOSINT-SI)\n  2016 In conjunction with ASONAM 2016 San Francisco, CA, USA, August 19-20,\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge in cyber-threat analysis is combining information from\ndifferent sources to find the person or the group responsible for the\ncyber-attack. It is one of the most important technical and policy challenges\nin cyber-security. The lack of ground truth for an individual responsible for\nan attack has limited previous studies. In this paper, we take a first step\ntowards overcoming this limitation by building a dataset from the\ncapture-the-flag event held at DEFCON, and propose an argumentation model based\non a formal reasoning framework called DeLP (Defeasible Logic Programming)\ndesigned to aid an analyst in attributing a cyber-attack. We build models from\nlatent variables to reduce the search space of culprits (attackers), and show\nthat this reduction significantly improves the performance of\nclassification-based approaches from 37% to 62% in identifying the attacker.\n", "versions": [{"version": "v1", "created": "Thu, 7 Jul 2016 21:01:06 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Nunes", "Eric", ""], ["Shakarian", "Paulo", ""], ["Simari", "Gerardo I.", ""], ["Ruef", "Andrew", ""]]}, {"id": "1607.02306", "submitter": "Huy Phan", "authors": "Huy Phan, Lars Hertel, Marco Maass, Philipp Koch, Alfred Mertins", "title": "CaR-FOREST: Joint Classification-Regression Decision Forests for\n  Overlapping Audio Event Detection", "comments": "Task2 and Task3 technical report for the DCASE2016 challenge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.LG cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This report describes our submissions to Task2 and Task3 of the DCASE 2016\nchallenge. The systems aim at dealing with the detection of overlapping audio\nevents in continuous streams, where the detectors are based on random decision\nforests. The proposed forests are jointly trained for classification and\nregression simultaneously. Initially, the training is classification-oriented\nto encourage the trees to select discriminative features from overlapping\nmixtures to separate positive audio segments from the negative ones. The\nregression phase is then carried out to let the positive audio segments vote\nfor the event onsets and offsets, and therefore model the temporal structure of\naudio events. One random decision forest is specifically trained for each event\ncategory of interest. Experimental results on the development data show that\nour systems significantly outperform the baseline on the Task2 evaluation while\nthey are inferior to the baseline in the Task3 evaluation.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 10:42:43 GMT"}, {"version": "v2", "created": "Mon, 15 Aug 2016 18:02:09 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Phan", "Huy", ""], ["Hertel", "Lars", ""], ["Maass", "Marco", ""], ["Koch", "Philipp", ""], ["Mertins", "Alfred", ""]]}, {"id": "1607.02399", "submitter": "Alexander Hinneburg", "authors": "Frank Rosner, Alexander Hinneburg", "title": "Translating Bayesian Networks into Entity Relationship Models, Extended\n  Version", "comments": "This is an extended version of a short paper published in the\n  Proceedings of the 35th International Conference on Conceptual Modeling, ER\n  2016. In addition to a more detailed discussion of the method, this extended\n  version describes a case study that applies the method as well as first ideas\n  of a conceptual framework for developing big data analytics applications", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big data analytics applications drive the convergence of data management and\nmachine learning. But there is no conceptual language available that is spoken\nin both worlds. The main contribution of the paper is a method to translate\nBayesian networks, a main conceptual language for probabilistic graphical\nmodels, into usable entity relationship models. The transformed representation\nof a Bayesian network leaves out mathematical details about probabilistic\nrelationships but unfolds all information relevant for data management tasks.\nAs a real world example, we present the TopicExplorer system that uses Bayesian\ntopic models as a core component in an interactive, database-supported web\napplication. Last, we sketch a conceptual framework that eases machine learning\nspecific development tasks while building big data analytics applications.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 15:06:46 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Rosner", "Frank", ""], ["Hinneburg", "Alexander", ""]]}, {"id": "1607.02419", "submitter": "Alexander Rubchinsky", "authors": "Alexander Rubchinsky", "title": "Divisive-agglomerative algorithm and complexity of automatic\n  classification problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.EC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An algorithm of solution of the Automatic Classification (AC for brevity)\nproblem is set forth in the paper. In the AC problem, it is required to find\none or several artitions, starting with the given pattern matrix or\ndissimilarity, similarity matrix.\n", "versions": [{"version": "v1", "created": "Tue, 5 Jul 2016 19:25:02 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Rubchinsky", "Alexander", ""]]}, {"id": "1607.02431", "submitter": "Jialin Liu Ph.D", "authors": "Tristan Cazenave, Jialin Liu, Fabien Teytaud, and Olivier Teytaud", "title": "Learning opening books in partially observable games: using random seeds\n  in Phantom Go", "comments": "7 pages, 15 figures. Accepted by CIG2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many artificial intelligences (AIs) are randomized. One can be lucky or\nunlucky with the random seed; we quantify this effect and show that, maybe\ncontrarily to intuition, this is far from being negligible. Then, we apply two\ndifferent existing algorithms for selecting good seeds and good probability\ndistributions over seeds. This mainly leads to learning an opening book. We\napply this to Phantom Go, which, as all phantom games, is hard for opening book\nlearning. We improve the winning rate from 50% to 70% in 5x5 against the same\nAI, and from approximately 0% to 40% in 5x5, 7x7 and 9x9 against a stronger\n(learning) opponent.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 15:58:28 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Cazenave", "Tristan", ""], ["Liu", "Jialin", ""], ["Teytaud", "Fabien", ""], ["Teytaud", "Olivier", ""]]}, {"id": "1607.02436", "submitter": "Rocco Tripodi", "authors": "Rocco Tripodi and Marcello Pelillo", "title": "Document Clustering Games in Static and Dynamic Scenarios", "comments": "This paper will be published in the series Lecture Notes in Computer\n  Science (LNCS) published by Springer, containing the ICPRAM 2016 best papers", "journal-ref": null, "doi": "10.1007/978-3-319-53375-9_2", "report-no": null, "categories": "cs.AI cs.CL cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a game theoretic model for document clustering. Each\ndocument to be clustered is represented as a player and each cluster as a\nstrategy. The players receive a reward interacting with other players that they\ntry to maximize choosing their best strategies. The geometry of the data is\nmodeled with a weighted graph that encodes the pairwise similarity among\ndocuments, so that similar players are constrained to choose similar\nstrategies, updating their strategy preferences at each iteration of the games.\nWe used different approaches to find the prototypical elements of the clusters\nand with this information we divided the players into two disjoint sets, one\ncollecting players with a definite strategy and the other one collecting\nplayers that try to learn from others the correct strategy to play. The latter\nset of players can be considered as new data points that have to be clustered\naccording to previous information. This representation is useful in scenarios\nin which the data are streamed continuously. The evaluation of the system was\nconducted on 13 document datasets using different settings. It shows that the\nproposed method performs well compared to different document clustering\nalgorithms.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 16:17:12 GMT"}], "update_date": "2017-04-07", "authors_parsed": [["Tripodi", "Rocco", ""], ["Pelillo", "Marcello", ""]]}, {"id": "1607.02444", "submitter": "Keunwoo Choi Mr", "authors": "Keunwoo Choi, George Fazekas, Mark Sandler", "title": "Explaining Deep Convolutional Neural Networks on Music Classification", "comments": "7 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.MM cs.SD", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Deep convolutional neural networks (CNNs) have been actively adopted in the\nfield of music information retrieval, e.g. genre classification, mood\ndetection, and chord recognition. However, the process of learning and\nprediction is little understood, particularly when it is applied to\nspectrograms. We introduce auralisation of a CNN to understand its underlying\nmechanism, which is based on a deconvolution procedure introduced in [2].\nAuralisation of a CNN is converting the learned convolutional features that are\nobtained from deconvolution into audio signals. In the experiments and\ndiscussions, we explain trained features of a 5-layer CNN based on the\ndeconvolved spectrograms and auralised signals. The pairwise correlations per\nlayers with varying different musical attributes are also investigated to\nunderstand the evolution of the learnt features. It is shown that in the deep\nlayers, the features are learnt to capture textures, the patterns of continuous\ndistributions, rather than shapes of lines.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 16:40:30 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Choi", "Keunwoo", ""], ["Fazekas", "George", ""], ["Sandler", "Mark", ""]]}, {"id": "1607.02466", "submitter": "J\\\"urgen Koslowski", "authors": "Milan Bankovi\\'c (University of Belgrade)", "title": "Solving finite-domain linear constraints in presence of the\n  $\\texttt{alldifferent}$", "comments": "28 pages, 2 figures", "journal-ref": "Logical Methods in Computer Science, Volume 12, Issue 3 (April 27,\n  2017) lmcs:2016", "doi": "10.2168/LMCS-12(3:5)2016", "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we investigate the possibility of improvement of the\nwidely-used filtering algorithm for the linear constraints in constraint\nsatisfaction problems in the presence of the alldifferent constraints. In many\ncases, the fact that the variables in a linear constraint are also constrained\nby some alldifferent constraints may help us to calculate stronger bounds of\nthe variables, leading to a stronger constraint propagation. We propose an\nimproved filtering algorithm that targets such cases. We provide a detailed\ndescription of the proposed algorithm and prove its correctness. We evaluate\nthe approach on five different problems that involve combinations of the linear\nand the alldifferent constraints. We also compare our algorithm to other\nrelevant approaches. The experimental results show a great potential of the\nproposed improvement.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 17:32:58 GMT"}, {"version": "v2", "created": "Fri, 2 Sep 2016 18:33:50 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Bankovi\u0107", "Milan", "", "University of Belgrade"]]}, {"id": "1607.02467", "submitter": "Marc Dymetman", "authors": "Marc Dymetman, Chunyang Xiao", "title": "Log-Linear RNNs: Towards Recurrent Neural Networks with Flexible Prior\n  Knowledge", "comments": "Updated version of arXiv:1607.02467. Presented at the NIPS-2016 RNN\n  Symposium, Barcelona, December 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce LL-RNNs (Log-Linear RNNs), an extension of Recurrent Neural\nNetworks that replaces the softmax output layer by a log-linear output layer,\nof which the softmax is a special case. This conceptually simple move has two\nmain advantages. First, it allows the learner to combat training data sparsity\nby allowing it to model words (or more generally, output symbols) as complex\ncombinations of attributes without requiring that each combination is directly\nobserved in the training data (as the softmax does). Second, it permits the\ninclusion of flexible prior knowledge in the form of a priori specified modular\nfeatures, where the neural network component learns to dynamically control the\nweights of a log-linear distribution exploiting these features.\n  We conduct experiments in the domain of language modelling of French, that\nexploit morphological prior knowledge and show an important decrease in\nperplexity relative to a baseline RNN.\n  We provide other motivating iillustrations, and finally argue that the\nlog-linear and the neural-network components contribute complementary strengths\nto the LL-RNN: the LL aspect allows the model to incorporate rich prior\nknowledge, while the NN aspect, according to the \"representation learning\"\nparadigm, allows the model to discover novel combination of characteristics.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 17:35:51 GMT"}, {"version": "v2", "created": "Fri, 16 Dec 2016 10:56:22 GMT"}], "update_date": "2016-12-19", "authors_parsed": [["Dymetman", "Marc", ""], ["Xiao", "Chunyang", ""]]}, {"id": "1607.02480", "submitter": "Subutai Ahmad", "authors": "Subutai Ahmad, Scott Purdy", "title": "Real-Time Anomaly Detection for Streaming Analytics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC cs.SY", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Much of the worlds data is streaming, time-series data, where anomalies give\nsignificant information in critical situations. Yet detecting anomalies in\nstreaming data is a difficult task, requiring detectors to process data in\nreal-time, and learn while simultaneously making predictions. We present a\nnovel anomaly detection technique based on an on-line sequence memory algorithm\ncalled Hierarchical Temporal Memory (HTM). We show results from a live\napplication that detects anomalies in financial metrics in real-time. We also\ntest the algorithm on NAB, a published benchmark for real-time anomaly\ndetection, where our algorithm achieves best-in-class results.\n", "versions": [{"version": "v1", "created": "Fri, 8 Jul 2016 18:20:32 GMT"}], "update_date": "2016-07-11", "authors_parsed": [["Ahmad", "Subutai", ""], ["Purdy", "Scott", ""]]}, {"id": "1607.02576", "submitter": "K Paramesha", "authors": "K Paramesha and K C Ravishankar", "title": "Analysis of opinionated text for opinion mining", "comments": "Sentiment Analysis, Features, Feature Engineering, Emotions, Word\n  Sense Disambiguation, Sentiment Lexicons, Meta-Information", "journal-ref": "Machine Learning and Applications: An International Journal\n  (MLAIJ) Vol.3, No.2, June 2016", "doi": "10.5121/mlaij.2016.3204", "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In sentiment analysis, the polarities of the opinions expressed on an\nobject/feature are determined to assess the sentiment of a sentence or document\nwhether it is positive/negative/neutral. Naturally, the object/feature is a\nnoun representation which refers to a product or a component of a product, let\nus say, the \"lens\" in a camera and opinions emanating on it are captured in\nadjectives, verbs, adverbs and noun words themselves. Apart from such words,\nother meta-information and diverse effective features are also going to play an\nimportant role in influencing the sentiment polarity and contribute\nsignificantly to the performance of the system. In this paper, some of the\nassociated information/meta-data are explored and investigated in the sentiment\ntext. Based on the analysis results presented here, there is scope for further\nassessment and utilization of the meta-information as features in text\ncategorization, ranking text document, identification of spam documents and\npolarity classification problems.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jul 2016 07:11:43 GMT"}, {"version": "v2", "created": "Thu, 14 Jul 2016 15:54:29 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Paramesha", "K", ""], ["Ravishankar", "K C", ""]]}, {"id": "1607.02660", "submitter": "Amol Patwardhan", "authors": "Amol Patwardhan and Gerald Knapp", "title": "Augmenting Supervised Emotion Recognition with Rule-Based Decision Model", "comments": "8 pages, 6 figures, 23 tables, IEEE TAC (in review)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this research is development of rule based decision model for\nemotion recognition. This research also proposes using the rules for augmenting\ninter-corporal recognition accuracy in multimodal systems that use supervised\nlearning techniques. The classifiers for such learning based recognition\nsystems are susceptible to over fitting and only perform well on intra-corporal\ndata. To overcome the limitation this research proposes using rule based model\nas an additional modality. The rules were developed using raw feature data from\nvisual channel, based on human annotator agreement and existing studies that\nhave attributed movement and postures to emotions. The outcome of the rule\nevaluations was combined during the decision phase of emotion recognition\nsystem. The results indicate rule based emotion recognition augment recognition\naccuracy of learning based systems and also provide better recognition rate\nacross inter corpus emotion test data.\n", "versions": [{"version": "v1", "created": "Sat, 9 Jul 2016 20:34:48 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Patwardhan", "Amol", ""], ["Knapp", "Gerald", ""]]}, {"id": "1607.02682", "submitter": "Mostafa Milani", "authors": "Mostafa Milani and Leopoldo Bertossi", "title": "Extending Weakly-Sticky Datalog+/-: Query-Answering Tractability and\n  Optimizations", "comments": "Extended version of RR'16 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Weakly-sticky (WS) Datalog+/- is an expressive member of the family of\nDatalog+/- programs that is based on the syntactic notions of stickiness and\nweak-acyclicity. Query answering over the WS programs has been investigated,\nbut there is still much work to do on the design and implementation of\npractical query answering (QA) algorithms and their optimizations. Here, we\nstudy sticky and WS programs from the point of view of the behavior of the\nchase procedure, extending the stickiness property of the chase to that of\ngeneralized stickiness of the chase (gsch-property). With this property we\nspecify the semantic class of GSCh programs, which includes sticky and WS\nprograms, and other syntactic subclasses that we identify. In particular, we\nintroduce joint-weakly-sticky (JWS) programs, that include WS programs. We also\npropose a bottom-up QA algorithm for a range of subclasses of GSCh. The\nalgorithm runs in polynomial time (in data) for JWS programs. Unlike the WS\nclass, JWS is closed under a general magic-sets rewriting procedure for the\noptimization of programs with existential rules. We apply the magic-sets\nrewriting in combination with the proposed QA algorithm for the optimization of\nQA over JWS programs.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jul 2016 02:56:33 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Milani", "Mostafa", ""], ["Bertossi", "Leopoldo", ""]]}, {"id": "1607.02763", "submitter": "Oran Richman", "authors": "Oran Richman, Shie Mannor", "title": "How to Allocate Resources For Features Acquisition?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study classification problems where features are corrupted by noise and\nwhere the magnitude of the noise in each feature is influenced by the resources\nallocated to its acquisition. This is the case, for example, when multiple\nsensors share a common resource (power, bandwidth, attention, etc.). We develop\na method for computing the optimal resource allocation for a variety of\nscenarios and derive theoretical bounds concerning the benefit that may arise\nby non-uniform allocation. We further demonstrate the effectiveness of the\ndeveloped method in simulations.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jul 2016 16:19:00 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Richman", "Oran", ""], ["Mannor", "Shie", ""]]}, {"id": "1607.02784", "submitter": "Duc-Thuan Vo", "authors": "Duc-Thuan Vo and Ebrahim Bagheri", "title": "Open Information Extraction", "comments": "This paper will appear in the Encyclopedia for Semantic Computing", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Open Information Extraction (Open IE) systems aim to obtain relation tuples\nwith highly scalable extraction in portable across domain by identifying a\nvariety of relation phrases and their arguments in arbitrary sentences. The\nfirst generation of Open IE learns linear chain models based on unlexicalized\nfeatures such as Part-of-Speech (POS) or shallow tags to label the intermediate\nwords between pair of potential arguments for identifying extractable\nrelations. Open IE currently is developed in the second generation that is able\nto extract instances of the most frequently observed relation types such as\nVerb, Noun and Prep, Verb and Prep, and Infinitive with deep linguistic\nanalysis. They expose simple yet principled ways in which verbs express\nrelationships in linguistics such as verb phrase-based extraction or\nclause-based extraction. They obtain a significantly higher performance over\nprevious systems in the first generation. In this paper, we describe an\noverview of two Open IE generations including strengths, weaknesses and\napplication areas.\n", "versions": [{"version": "v1", "created": "Sun, 10 Jul 2016 20:39:24 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Vo", "Duc-Thuan", ""], ["Bagheri", "Ebrahim", ""]]}, {"id": "1607.02802", "submitter": "Franck Dernoncourt", "authors": "Franck Dernoncourt", "title": "Mapping distributional to model-theoretic semantic spaces: a baseline", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings have been shown to be useful across state-of-the-art systems\nin many natural language processing tasks, ranging from question answering\nsystems to dependency parsing. (Herbelot and Vecchi, 2015) explored word\nembeddings and their utility for modeling language semantics. In particular,\nthey presented an approach to automatically map a standard distributional\nsemantic space onto a set-theoretic model using partial least squares\nregression. We show in this paper that a simple baseline achieves a +51%\nrelative improvement compared to their model on one of the two datasets they\nused, and yields competitive results on the second dataset.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 01:20:57 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Dernoncourt", "Franck", ""]]}, {"id": "1607.02902", "submitter": "Yewen Pu", "authors": "Yewen Pu, Karthik Narasimhan, Armando Solar-Lezama, Regina Barzilay", "title": "sk_p: a neural program corrector for MOOCs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel technique for automatic program correction in MOOCs,\ncapable of fixing both syntactic and semantic errors without manual, problem\nspecific correction strategies. Given an incorrect student program, it\ngenerates candidate programs from a distribution of likely corrections, and\nchecks each candidate for correctness against a test suite.\n  The key observation is that in MOOCs many programs share similar code\nfragments, and the seq2seq neural network model, used in the natural-language\nprocessing task of machine translation, can be modified and trained to recover\nthese fragments.\n  Experiment shows our scheme can correct 29% of all incorrect submissions and\nout-performs state of the art approach which requires manual, problem specific\ncorrection strategies.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 11:08:00 GMT"}], "update_date": "2016-07-12", "authors_parsed": [["Pu", "Yewen", ""], ["Narasimhan", "Karthik", ""], ["Solar-Lezama", "Armando", ""], ["Barzilay", "Regina", ""]]}, {"id": "1607.03189", "submitter": "Vijay Gadepally", "authors": "Vijay Gadepally and Ashok Krishnamurthy", "title": "A Framework for Estimating Long Term Driver Behavior", "comments": "10 pages", "journal-ref": null, "doi": "10.1155/2017/3080859", "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The authors present a cyber-physical systems study on the estimation of\ndriver behavior in autonomous vehicles and vehicle safety systems. Extending\nupon previous work, the approach described is suitable for the long term\nestimation and tracking of autonomous vehicle behavior. The proposed system\nmakes use of a previously defined Hybrid State System and Hidden Markov Model\n(HSS+HMM) system which has provided good results for driver behavior\nestimation. The HSS+HMM system utilizes the hybrid characteristics of\ndecision-behavior coupling of many systems such as the driver and the vehicle,\nuses Kalman Filter estimates of observable parameters to track the\ninstantaneous continuous state, and estimates the most likely driver state. The\nHSS+HMM system is encompassed in a HSS structure and inter-system connectivity\nis determined by using Signal Processing and Pattern Recognition techniques.\nThe proposed method is suitable for scenarios that involve unknown decisions of\nother individuals, such as lane changes or intersection precedence/access. The\nlong term driver behavior estimation system involves an extended HSS+HMM\nstructure that is capable of including external information in the estimation\nprocess. Through the grafting and pruning of metastates, the HSS+HMM system can\nbe dynamically updated to best represent driver choices given external\ninformation. Three application examples are also provided to elucidate the\ntheoretical system.\n", "versions": [{"version": "v1", "created": "Mon, 11 Jul 2016 22:37:36 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Gadepally", "Vijay", ""], ["Krishnamurthy", "Ashok", ""]]}, {"id": "1607.03290", "submitter": "Chih-Kuan Yeh", "authors": "Chih-Kuan Yeh, Hsuan-Tien Lin", "title": "Automatic Bridge Bidding Using Deep Reinforcement Learning", "comments": "8 pages, 1 figure, 2016 ECAI accepted", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bridge is among the zero-sum games for which artificial intelligence has not\nyet outperformed expert human players. The main difficulty lies in the bidding\nphase of bridge, which requires cooperative decision making under partial\ninformation. Existing artificial intelligence systems for bridge bidding rely\non and are thus restricted by human-designed bidding systems or features. In\nthis work, we propose a pioneering bridge bidding system without the aid of\nhuman domain knowledge. The system is based on a novel deep reinforcement\nlearning model, which extracts sophisticated features and learns to bid\nautomatically based on raw card data. The model includes an\nupper-confidence-bound algorithm and additional techniques to achieve a balance\nbetween exploration and exploitation. Our experiments validate the promising\nperformance of our proposed model. In particular, the model advances from\nhaving no knowledge about bidding to achieving superior performance when\ncompared with a champion-winning computer bridge program that implements a\nhuman-designed bidding system.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 09:58:24 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Yeh", "Chih-Kuan", ""], ["Lin", "Hsuan-Tien", ""]]}, {"id": "1607.03317", "submitter": "Per Kristian Lehre", "authors": "Duc-Cuong Dang and Thomas Jansen and Per Kristian Lehre", "title": "Populations can be essential in tracking dynamic optima", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI q-bio.PE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-world optimisation problems are often dynamic. Previously good solutions\nmust be updated or replaced due to changes in objectives and constraints. It is\noften claimed that evolutionary algorithms are particularly suitable for\ndynamic optimisation because a large population can contain different solutions\nthat may be useful in the future. However, rigorous theoretical demonstrations\nfor how populations in dynamic optimisation can be essential are sparse and\nrestricted to special cases.\n  This paper provides theoretical explanations of how populations can be\nessential in evolutionary dynamic optimisation in a general and natural\nsetting. We describe a natural class of dynamic optimisation problems where a\nsufficiently large population is necessary to keep track of moving optima\nreliably. We establish a relationship between the population-size and the\nprobability that the algorithm loses track of the optimum.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 11:52:48 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Dang", "Duc-Cuong", ""], ["Jansen", "Thomas", ""], ["Lehre", "Per Kristian", ""]]}, {"id": "1607.03354", "submitter": "EPTCS", "authors": "Benjamin Aminof (Technische Universitat Wien, Austria), Vadim Malvone\n  (Universit\\`a degli Studi di Napoli Federico II, Italy), Aniello Murano\n  (Universit\\`a degli Studi di Napoli Federico II, Italy), Sasha Rubin\n  (Universit\\`a degli Studi di Napoli Federico II, Italy)", "title": "Extended Graded Modalities in Strategy Logic", "comments": "In Proceedings SR 2016, arXiv:1607.02694", "journal-ref": "EPTCS 218, 2016, pp. 1-14", "doi": "10.4204/EPTCS.218.1", "report-no": null, "categories": "cs.GT cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Strategy Logic (SL) is a logical formalism for strategic reasoning in\nmulti-agent systems. Its main feature is that it has variables for strategies\nthat are associated to specific agents with a binding operator. We introduce\nGraded Strategy Logic (GradedSL), an extension of SL by graded quantifiers over\ntuples of strategy variables, i.e., \"there exist at least g different tuples\n(x_1,...,x_n) of strategies\" where g is a cardinal from the set N union\n{aleph_0, aleph_1, 2^aleph_0}. We prove that the model-checking problem of\nGradedSL is decidable. We then turn to the complexity of fragments of GradedSL.\nWhen the g's are restricted to finite cardinals, written GradedNSL, the\ncomplexity of model-checking is no harder than for SL, i.e., it is\nnon-elementary in the quantifier rank. We illustrate our formalism by showing\nhow to count the number of different strategy profiles that are Nash equilibria\n(NE), or subgame-perfect equilibria (SPE). By analyzing the structure of the\nspecific formulas involved, we conclude that the important problems of checking\nfor the existence of a unique NE or SPE can both be solved in 2ExpTime, which\nis not harder than merely checking for the existence of such equilibria.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 13:46:52 GMT"}], "update_date": "2016-07-13", "authors_parsed": [["Aminof", "Benjamin", "", "Technische Universitat Wien, Austria"], ["Malvone", "Vadim", "", "Universit\u00e0 degli Studi di Napoli Federico II, Italy"], ["Murano", "Aniello", "", "Universit\u00e0 degli Studi di Napoli Federico II, Italy"], ["Rubin", "Sasha", "", "Universit\u00e0 degli Studi di Napoli Federico II, Italy"]]}, {"id": "1607.03516", "submitter": "Muhammad Ghifary", "authors": "Muhammad Ghifary and W. Bastiaan Kleijn and Mengjie Zhang and David\n  Balduzzi and Wen Li", "title": "Deep Reconstruction-Classification Networks for Unsupervised Domain\n  Adaptation", "comments": "to appear in European Conference on Computer Vision (ECCV) 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel unsupervised domain adaptation algorithm\nbased on deep learning for visual object recognition. Specifically, we design a\nnew model called Deep Reconstruction-Classification Network (DRCN), which\njointly learns a shared encoding representation for two tasks: i) supervised\nclassification of labeled source data, and ii) unsupervised reconstruction of\nunlabeled target data.In this way, the learnt representation not only preserves\ndiscriminability, but also encodes useful information from the target domain.\nOur new DRCN model can be optimized by using backpropagation similarly as the\nstandard neural networks.\n  We evaluate the performance of DRCN on a series of cross-domain object\nrecognition tasks, where DRCN provides a considerable improvement (up to ~8% in\naccuracy) over the prior state-of-the-art algorithms. Interestingly, we also\nobserve that the reconstruction pipeline of DRCN transforms images from the\nsource domain into images whose appearance resembles the target dataset. This\nsuggests that DRCN's performance is due to constructing a single composite\nrepresentation that encodes information about both the structure of target\nimages and the classification of source images. Finally, we provide a formal\nanalysis to justify the algorithm's objective in domain adaptation context.\n", "versions": [{"version": "v1", "created": "Tue, 12 Jul 2016 20:48:58 GMT"}, {"version": "v2", "created": "Mon, 1 Aug 2016 09:58:13 GMT"}], "update_date": "2016-08-03", "authors_parsed": [["Ghifary", "Muhammad", ""], ["Kleijn", "W. Bastiaan", ""], ["Zhang", "Mengjie", ""], ["Balduzzi", "David", ""], ["Li", "Wen", ""]]}, {"id": "1607.03611", "submitter": "Weishan Dong", "authors": "Weishan Dong, Jian Li, Renjie Yao, Changsheng Li, Ting Yuan, Lanjun\n  Wang", "title": "Characterizing Driving Styles with Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Characterizing driving styles of human drivers using vehicle sensor data,\ne.g., GPS, is an interesting research problem and an important real-world\nrequirement from automotive industries. A good representation of driving\nfeatures can be highly valuable for autonomous driving, auto insurance, and\nmany other application scenarios. However, traditional methods mainly rely on\nhandcrafted features, which limit machine learning algorithms to achieve a\nbetter performance. In this paper, we propose a novel deep learning solution to\nthis problem, which could be the first attempt of extending deep learning to\ndriving behavior analysis based on GPS data. The proposed approach can\neffectively extract high level and interpretable features describing complex\ndriving patterns. It also requires significantly less human experience and\nwork. The power of the learned driving style representations are validated\nthrough the driver identification problem using a large real dataset.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 07:15:30 GMT"}, {"version": "v2", "created": "Sat, 8 Oct 2016 05:21:00 GMT"}], "update_date": "2016-10-11", "authors_parsed": [["Dong", "Weishan", ""], ["Li", "Jian", ""], ["Yao", "Renjie", ""], ["Li", "Changsheng", ""], ["Yuan", "Ting", ""], ["Wang", "Lanjun", ""]]}, {"id": "1607.03705", "submitter": "Philippe Leray", "authors": "Maroua Haddad (LINA, LARODEC), Philippe Leray (LINA), Nahla Ben Amor\n  (LARODEC)", "title": "Possibilistic Networks: Parameters Learning from Imprecise Data and\n  Evaluation strategy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been an ever-increasing interest in multidisciplinary research on\nrepresenting and reasoning with imperfect data. Possibilistic networks present\none of the powerful frameworks of interest for representing uncertain and\nimprecise information. This paper covers the problem of their parameters\nlearning from imprecise datasets, i.e., containing multi-valued data. We\npropose in the rst part of this paper a possibilistic networks sampling\nprocess. In the second part, we propose a likelihood function which explores\nthe link between random sets theory and possibility theory. This function is\nthen deployed to parametrize possibilistic networks.\n", "versions": [{"version": "v1", "created": "Wed, 13 Jul 2016 12:45:53 GMT"}], "update_date": "2016-07-14", "authors_parsed": [["Haddad", "Maroua", "", "LINA, LARODEC"], ["Leray", "Philippe", "", "LINA"], ["Amor", "Nahla Ben", "", "LARODEC"]]}, {"id": "1607.03979", "submitter": "Arshia Khaffaf", "authors": "Mona Khaffaf and Arshia Khaffaf", "title": "Resource Planning For Rescue Operations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  After an earthquake, disaster sites pose a multitude of health and safety\nconcerns. A rescue operation of people trapped in the ruins after an earthquake\ndisaster requires a series of intelligent behavior, including planning. For a\nsuccessful rescue operation, given a limited number of available actions and\nregulations, the role of planning in rescue operations is crucial. Fortunately,\nrecent developments in automated planning by artificial intelligence community\ncan help different organization in this crucial task. Due to the number of\nrules and regulations, we believe that a rule based system for planning can be\nhelpful for this specific planning problem. In this research work, we use logic\nrules to represent rescue and related regular regulations, together with a\nlogic based planner to solve this complicated problem. Although this research\nis still in the prototyping and modeling stage, it clearly shows that rule\nbased languages can be a good infrastructure for this computational task. The\nresults of this research can be used by different organizations, such as\nIranian Red Crescent Society and International Institute of Seismology and\nEarthquake Engineering (IISEE).\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 02:21:14 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Khaffaf", "Mona", ""], ["Khaffaf", "Arshia", ""]]}, {"id": "1607.04110", "submitter": "Giulio Petrucci", "authors": "Giulio Petrucci, Chiara Ghidini, Marco Rospocher", "title": "Using Recurrent Neural Network for Learning Expressive Ontologies", "comments": "Technical Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Neural Networks have been proven extremely effective in many\nnatural language processing tasks such as sentiment analysis, question\nanswering, or machine translation. Aiming to exploit such advantages in the\nOntology Learning process, in this technical report we present a detailed\ndescription of a Recurrent Neural Network based system to be used to pursue\nsuch goal.\n", "versions": [{"version": "v1", "created": "Thu, 14 Jul 2016 12:45:07 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Petrucci", "Giulio", ""], ["Ghidini", "Chiara", ""], ["Rospocher", "Marco", ""]]}, {"id": "1607.04186", "submitter": "Mathieu Acher", "authors": "Mathieu Acher (DiverSe), Fran\\c{c}ois Esnault (DiverSe)", "title": "Large-scale Analysis of Chess Games with Chess Engines: A Preliminary\n  Report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The strength of chess engines together with the availability of numerous\nchess games have attracted the attention of chess players, data scientists, and\nresearchers during the last decades. State-of-the-art engines now provide an\nauthoritative judgement that can be used in many applications like cheating\ndetection, intrinsic ratings computation, skill assessment, or the study of\nhuman decision-making. A key issue for the research community is to gather a\nlarge dataset of chess games together with the judgement of chess engines.\nUnfortunately the analysis of each move takes lots of times. In this paper, we\nreport our effort to analyse almost 5 millions chess games with a computing\ngrid. During summer 2015, we processed 270 millions unique played positions\nusing the Stockfish engine with a quite high depth (20). We populated a\ndatabase of 1+ tera-octets of chess evaluations, representing an estimated time\nof 50 years of computation on a single machine. Our effort is a first step\ntowards the replication of research results, the supply of open data and\nprocedures for exploring new directions, and the investigation of software\nengineering/scalability issues when computing billions of moves.\n", "versions": [{"version": "v1", "created": "Thu, 28 Apr 2016 08:37:43 GMT"}], "update_date": "2016-07-15", "authors_parsed": [["Acher", "Mathieu", "", "DiverSe"], ["Esnault", "Fran\u00e7ois", "", "DiverSe"]]}, {"id": "1607.04324", "submitter": "Xin-She Yang", "authors": "Aziz Ouaarab, B. Ahiod, Xin-She Yang", "title": "Random-Key Cuckoo Search for the Travelling Salesman Problem", "comments": "13 pages, 6 figures", "journal-ref": "Soft Computing, 19(4), 1099-1106(2015)", "doi": "10.1007/s00500-014-1322-9", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combinatorial optimization problems are typically NP-hard, and thus very\nchallenging to solve. In this paper, we present the random key cuckoo search\n(RKCS) algorithm for solving the famous Travelling Salesman Problem (TSP). We\nused a simplified random-key encoding scheme to pass from a continuous space\n(real numbers) to a combinatorial space. We also consider the displacement of a\nsolution in both spaces using Levy flights. The performance of the proposed\nRKCS is tested against a set of benchmarks of symmetric TSP from the well-known\nTSPLIB library. The results of the tests show that RKCS is superior to some\nother metaheuristic algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 14 Apr 2016 12:40:50 GMT"}], "update_date": "2016-07-18", "authors_parsed": [["Ouaarab", "Aziz", ""], ["Ahiod", "B.", ""], ["Yang", "Xin-She", ""]]}, {"id": "1607.04373", "submitter": "Ruining He", "authors": "Ruining He, Chen Fang, Zhaowen Wang, Julian McAuley", "title": "Vista: A Visually, Socially, and Temporally-aware Model for Artistic\n  Recommendation", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": "10.1145/2959100.2959152", "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding users' interactions with highly subjective content---like\nartistic images---is challenging due to the complex semantics that guide our\npreferences. On the one hand one has to overcome `standard' recommender systems\nchallenges, such as dealing with large, sparse, and long-tailed datasets. On\nthe other, several new challenges present themselves, such as the need to model\ncontent in terms of its visual appearance, or even social dynamics, such as a\npreference toward a particular artist that is independent of the art they\ncreate.\n  In this paper we build large-scale recommender systems to model the dynamics\nof a vibrant digital art community, Behance, consisting of tens of millions of\ninteractions (clicks and `appreciates') of users toward digital art.\nMethodologically, our main contributions are to model (a) rich content,\nespecially in terms of its visual appearance; (b) temporal dynamics, in terms\nof how users prefer `visually consistent' content within and across sessions;\nand (c) social dynamics, in terms of how users exhibit preferences both towards\ncertain art styles, as well as the artists themselves.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 03:35:56 GMT"}], "update_date": "2016-07-18", "authors_parsed": [["He", "Ruining", ""], ["Fang", "Chen", ""], ["Wang", "Zhaowen", ""], ["McAuley", "Julian", ""]]}, {"id": "1607.04376", "submitter": "Jay Wong", "authors": "Jay Ming Wong and Roderic A. Grupen", "title": "Intrinsically Motivated Multimodal Structure Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a long-term intrinsically motivated structure learning method for\nmodeling transition dynamics during controlled interactions between a robot and\nsemi-permanent structures in the world. In particular, we discuss how\npartially-observable state is represented using distributions over a Markovian\nstate and build models of objects that predict how state distributions change\nin response to interactions with such objects. These structures serve as the\nbasis for a number of possible future tasks defined as Markov Decision\nProcesses (MDPs). The approach is an example of a structure learning technique\napplied to a multimodal affordance representation that yields a population of\nforward models for use in planning. We evaluate the approach using experiments\non a bimanual mobile manipulator (uBot-6) that show the performance of model\nacquisition as the number of transition actions increases.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 04:19:31 GMT"}], "update_date": "2016-07-18", "authors_parsed": [["Wong", "Jay Ming", ""], ["Grupen", "Roderic A.", ""]]}, {"id": "1607.04379", "submitter": "Renzhi Cao", "authors": "Renzhi Cao, Debswapna Bhattacharya, Jie Hou, and Jianlin Cheng", "title": "DeepQA: Improving the estimation of single protein model quality with\n  deep belief networks", "comments": "19 pages, 1 figure, 4 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG q-bio.QM", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Protein quality assessment (QA) by ranking and selecting protein models has\nlong been viewed as one of the major challenges for protein tertiary structure\nprediction. Especially, estimating the quality of a single protein model, which\nis important for selecting a few good models out of a large model pool\nconsisting of mostly low-quality models, is still a largely unsolved problem.\nWe introduce a novel single-model quality assessment method DeepQA based on\ndeep belief network that utilizes a number of selected features describing the\nquality of a model from different perspectives, such as energy, physio-chemical\ncharacteristics, and structural information. The deep belief network is trained\non several large datasets consisting of models from the Critical Assessment of\nProtein Structure Prediction (CASP) experiments, several publicly available\ndatasets, and models generated by our in-house ab initio method. Our experiment\ndemonstrate that deep belief network has better performance compared to Support\nVector Machines and Neural Networks on the protein model quality assessment\nproblem, and our method DeepQA achieves the state-of-the-art performance on\nCASP11 dataset. It also outperformed two well-established methods in selecting\ngood outlier models from a large set of models of mostly low quality generated\nby ab initio modeling methods. DeepQA is a useful tool for protein single model\nquality assessment and protein structure prediction. The source code,\nexecutable, document and training/test datasets of DeepQA for Linux is freely\navailable to non-commercial users at http://cactus.rnet.missouri.edu/DeepQA/.\n", "versions": [{"version": "v1", "created": "Fri, 15 Jul 2016 04:28:55 GMT"}], "update_date": "2016-07-18", "authors_parsed": [["Cao", "Renzhi", ""], ["Bhattacharya", "Debswapna", ""], ["Hou", "Jie", ""], ["Cheng", "Jianlin", ""]]}, {"id": "1607.04583", "submitter": "Matthew Liberatore", "authors": "Matthew J. Liberatore", "title": "A Counterexample to the Forward Recursion in Fuzzy Critical Path\n  Analysis Under Discrete Fuzzy Sets", "comments": "10 pages, 1 figure, 1 table, 22 references", "journal-ref": "International Journal of Fuzzy Logic Systems 6 (2016) 53-62", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fuzzy logic is an alternate approach for quantifying uncertainty relating to\nactivity duration. The fuzzy version of the backward recursion has been shown\nto produce results that incorrectly amplify the level of uncertainty. However,\nthe fuzzy version of the forward recursion has been widely proposed as an\napproach for determining the fuzzy set of critical path lengths. In this paper,\nthe direct application of the extension principle leads to a proposition that\nmust be satisfied in fuzzy critical path analysis. Using a counterexample it is\ndemonstrated that the fuzzy forward recursion when discrete fuzzy sets are used\nto represent activity durations produces results that are not consistent with\nthe theory presented. The problem is shown to be the application of the fuzzy\nmaximum. Several methods presented in the literature are described and shown to\nprovide results that are consistent with the extension principle.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2016 13:35:00 GMT"}], "update_date": "2016-07-18", "authors_parsed": [["Liberatore", "Matthew J.", ""]]}, {"id": "1607.04809", "submitter": "Michael Cochez", "authors": "Michael Cochez, Stefan Decker, Eric Prud'hommeaux", "title": "Knowledge Representation on the Web revisited: Tools for Prototype Based\n  Ontologies", "comments": "Related software available from\n  https://github.com/miselico/knowledgebase/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years RDF and OWL have become the most common knowledge\nrepresentation languages in use on the Web, propelled by the recommendation of\nthe W3C. In this paper we present a practical implementation of a different\nkind of knowledge representation based on Prototypes. In detail, we present a\nconcrete syntax easily and effectively parsable by applications. We also\npresent extensible implementations of a prototype knowledge base, specifically\ndesigned for storage of Prototypes. These implementations are written in Java\nand can be extended by using the implementation as a library. Alternatively,\nthe software can be deployed as such. Further, results of benchmarks for both\nlocal and web deployment are presented. This paper augments a research paper,\nin which we describe the more theoretical aspects of our Prototype system.\n", "versions": [{"version": "v1", "created": "Sat, 16 Jul 2016 23:42:44 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Cochez", "Michael", ""], ["Decker", "Stefan", ""], ["Prud'hommeaux", "Eric", ""]]}, {"id": "1607.04817", "submitter": "Kenji Kawaguchi", "authors": "Kenji Kawaguchi, Yu Maruyama, Xiaoyu Zheng", "title": "Global Continuous Optimization with Error Bound and Fast Convergence", "comments": null, "journal-ref": "Journal of Artificial Intelligence Research, volume 56, pages\n  153-195 (2016)", "doi": "10.1613/jair.4742", "report-no": null, "categories": "math.OC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers global optimization with a black-box unknown objective\nfunction that can be non-convex and non-differentiable. Such a difficult\noptimization problem arises in many real-world applications, such as parameter\ntuning in machine learning, engineering design problem, and planning with a\ncomplex physics simulator. This paper proposes a new global optimization\nalgorithm, called Locally Oriented Global Optimization (LOGO), to aim for both\nfast convergence in practice and finite-time error bound in theory. The\nadvantage and usage of the new algorithm are illustrated via theoretical\nanalysis and an experiment conducted with 11 benchmark test functions. Further,\nwe modify the LOGO algorithm to specifically solve a planning problem via\npolicy search with continuous state/action space and long time horizon while\nmaintaining its finite-time error bound. We apply the proposed planning method\nto accident management of a nuclear power plant. The result of the application\nstudy demonstrates the practical utility of our method.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jul 2016 01:41:16 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Kawaguchi", "Kenji", ""], ["Maruyama", "Yu", ""], ["Zheng", "Xiaoyu", ""]]}, {"id": "1607.04917", "submitter": "Blaine Rister", "authors": "Blaine Rister, Daniel L Rubin", "title": "Piecewise convexity of artificial neural networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Although artificial neural networks have shown great promise in applications\nincluding computer vision and speech recognition, there remains considerable\npractical and theoretical difficulty in optimizing their parameters. The\nseemingly unreasonable success of gradient descent methods in minimizing these\nnon-convex functions remains poorly understood. In this work we offer some\ntheoretical guarantees for networks with piecewise affine activation functions,\nwhich have in recent years become the norm. We prove three main results.\nFirstly, that the network is piecewise convex as a function of the input data.\nSecondly, that the network, considered as a function of the parameters in a\nsingle layer, all others held constant, is again piecewise convex. Finally,\nthat the network as a function of all its parameters is piecewise multi-convex,\na generalization of biconvexity. From here we characterize the local minima and\nstationary points of the training objective, showing that they minimize certain\nsubsets of the parameter space. We then analyze the performance of two\noptimization algorithms on multi-convex problems: gradient descent, and a\nmethod which repeatedly solves a number of convex sub-problems. We prove\nnecessary convergence conditions for the first algorithm and both necessary and\nsufficient conditions for the second, after introducing regularization to the\nobjective. Finally, we remark on the remaining difficulty of the global\noptimization problem. Under the squared error objective, we show that by\nvarying the training data, a single rectifier neuron admits local minima\narbitrarily far apart, both in objective value and parameter space.\n", "versions": [{"version": "v1", "created": "Sun, 17 Jul 2016 21:49:00 GMT"}, {"version": "v2", "created": "Wed, 28 Dec 2016 06:39:01 GMT"}], "update_date": "2016-12-30", "authors_parsed": [["Rister", "Blaine", ""], ["Rubin", "Daniel L", ""]]}, {"id": "1607.05023", "submitter": "Gabriella Panuccio", "authors": "Gabriella Panuccio, Marianna Semprini, Lorenzo Natale, Michela\n  Chiappalone", "title": "Intelligent Biohybrid Neurotechnologies: Are They Really What They\n  Claim?", "comments": "Number of pages: 15 Words in abstract: 49 Words in main text: 3265\n  Number of figures: 5 Number of references: 25 Keywords: artificial\n  intelligence, biohybrid system, closed-loop control, functional brain repair", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the era of intelligent biohybrid neurotechnologies for brain repair, new\nfanciful terms are appearing in the scientific dictionary to define what has so\nfar been unimaginable. As the emerging neurotechnologies are becoming\nincreasingly polyhedral and sophisticated, should we talk about evolution and\nrank the intelligence of these devices?\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 11:28:11 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Panuccio", "Gabriella", ""], ["Semprini", "Marianna", ""], ["Natale", "Lorenzo", ""], ["Chiappalone", "Michela", ""]]}, {"id": "1607.05077", "submitter": "Ionel Hosu", "authors": "Ionel-Alexandru Hosu, Traian Rebedea", "title": "Playing Atari Games with Deep Reinforcement Learning and Human\n  Checkpoint Replay", "comments": "6 pages, 2 figures, EGPAI 2016 - Evaluating General Purpose AI,\n  workshop held in conjunction with ECAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a novel method for learning how to play the most\ndifficult Atari 2600 games from the Arcade Learning Environment using deep\nreinforcement learning. The proposed method, human checkpoint replay, consists\nin using checkpoints sampled from human gameplay as starting points for the\nlearning process. This is meant to compensate for the difficulties of current\nexploration strategies, such as epsilon-greedy, to find successful control\npolicies in games with sparse rewards. Like other deep reinforcement learning\narchitectures, our model uses a convolutional neural network that receives only\nraw pixel inputs to estimate the state value function. We tested our method on\nMontezuma's Revenge and Private Eye, two of the most challenging games from the\nAtari platform. The results we obtained show a substantial improvement compared\nto previous learning approaches, as well as over a random player. We also\npropose a method for training deep reinforcement learning agents using human\ngameplay experience, which we call human experience replay.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 13:55:54 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Hosu", "Ionel-Alexandru", ""], ["Rebedea", "Traian", ""]]}, {"id": "1607.05174", "submitter": "Roger Moore", "authors": "Roger K. Moore", "title": "Is spoken language all-or-nothing? Implications for future speech-based\n  human-machine interaction", "comments": "To appear in K. Jokinen & G. Wilcock (Eds.), Dialogues with Social\n  Robots - Enablements, Analyses, and Evaluation. Springer Lecture Notes in\n  Electrical Engineering (LNEE)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CL cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen significant market penetration for voice-based\npersonal assistants such as Apple's Siri. However, despite this success, user\ntake-up is frustratingly low. This position paper argues that there is a\nhabitability gap caused by the inevitable mismatch between the capabilities and\nexpectations of human users and the features and benefits provided by\ncontemporary technology. Suggestions are made as to how such problems might be\nmitigated, but a more worrisome question emerges: \"is spoken language\nall-or-nothing\"? The answer, based on contemporary views on the special nature\nof (spoken) language, is that there may indeed be a fundamental limit to the\ninteraction that can take place between mismatched interlocutors (such as\nhumans and machines). However, it is concluded that interactions between native\nand non-native speakers, or between adults and children, or even between humans\nand dogs, might provide critical inspiration for the design of future\nspeech-based human-machine interaction.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 16:44:34 GMT"}], "update_date": "2016-07-19", "authors_parsed": [["Moore", "Roger K.", ""]]}, {"id": "1607.05351", "submitter": "Dmitriy Zheleznyakov", "authors": "Evgeny Kharlamov, Yannis Kotidis, Theofilos Mailis, Christian\n  Neuenstadt, Charalampos Nikolaou, \\\"Ozg\\\"ur \\\"Ozcep, Christoforos Svingos,\n  Dmitriy Zheleznyakov, Sebastian Brandt, Ian Horrocks, Yannis Ioannidis,\n  Steffen Lamparter, Ralf M\\\"oller", "title": "Towards Analytics Aware Ontology Based Access to Static and Streaming\n  Data (Extended Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time analytics that requires integration and aggregation of\nheterogeneous and distributed streaming and static data is a typical task in\nmany industrial scenarios such as diagnostics of turbines in Siemens. OBDA\napproach has a great potential to facilitate such tasks; however, it has a\nnumber of limitations in dealing with analytics that restrict its use in\nimportant industrial applications. Based on our experience with Siemens, we\nargue that in order to overcome those limitations OBDA should be extended and\nbecome analytics, source, and cost aware. In this work we propose such an\nextension. In particular, we propose an ontology, mapping, and query language\nfor OBDA, where aggregate and other analytical functions are first class\ncitizens. Moreover, we develop query optimisation techniques that allow to\nefficiently process analytical tasks over static and streaming data. We\nimplement our approach in a system and evaluate our system with Siemens turbine\ndata.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 23:23:21 GMT"}, {"version": "v2", "created": "Mon, 15 Aug 2016 12:06:28 GMT"}], "update_date": "2016-08-16", "authors_parsed": [["Kharlamov", "Evgeny", ""], ["Kotidis", "Yannis", ""], ["Mailis", "Theofilos", ""], ["Neuenstadt", "Christian", ""], ["Nikolaou", "Charalampos", ""], ["\u00d6zcep", "\u00d6zg\u00fcr", ""], ["Svingos", "Christoforos", ""], ["Zheleznyakov", "Dmitriy", ""], ["Brandt", "Sebastian", ""], ["Horrocks", "Ian", ""], ["Ioannidis", "Yannis", ""], ["Lamparter", "Steffen", ""], ["M\u00f6ller", "Ralf", ""]]}, {"id": "1607.05387", "submitter": "Hanock Kwak", "authors": "Hanock Kwak, Byoung-Tak Zhang", "title": "Generating Images Part by Part with Composite Generative Adversarial\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Image generation remains a fundamental problem in artificial intelligence in\ngeneral and deep learning in specific. The generative adversarial network (GAN)\nwas successful in generating high quality samples of natural images. We propose\na model called composite generative adversarial network, that reveals the\ncomplex structure of images with multiple generators in which each generator\ngenerates some part of the image. Those parts are combined by alpha blending\nprocess to create a new single image. It can generate, for example, background\nand face sequentially with two generators, after training on face dataset.\nTraining was done in an unsupervised way without any labels about what each\ngenerator should generate. We found possibilities of learning the structure by\nusing this generative model empirically.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 03:09:31 GMT"}, {"version": "v2", "created": "Mon, 14 Nov 2016 07:32:35 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Kwak", "Hanock", ""], ["Zhang", "Byoung-Tak", ""]]}, {"id": "1607.05540", "submitter": "Michael Crosscombe", "authors": "Michael Crosscombe and Jonathan Lawry", "title": "Exploiting Vagueness for Multi-Agent Consensus", "comments": "Submitted to the second international workshop on Smart Simulation\n  and Modelling for Complex Systems (SSMCS) at IJCAI 2015", "journal-ref": null, "doi": "10.1007/978-981-10-2564-8_5", "report-no": null, "categories": "cs.MA cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  A framework for consensus modelling is introduced using Kleene's three valued\nlogic as a means to express vagueness in agents' beliefs. Explicitly borderline\ncases are inherent to propositions involving vague concepts where sentences of\na propositional language may be absolutely true, absolutely false or\nborderline. By exploiting these intermediate truth values, we can allow agents\nto adopt a more vague interpretation of underlying concepts in order to weaken\ntheir beliefs and reduce the levels of inconsistency, so as to achieve\nconsensus. We consider a consensus combination operation which results in\nagents adopting the borderline truth value as a shared viewpoint if they are in\ndirect conflict. Simulation experiments are presented which show that applying\nthis operator to agents chosen at random (subject to a consistency threshold)\nfrom a population, with initially diverse opinions, results in convergence to a\nsmaller set of more precise shared beliefs. Furthermore, if the choice of\nagents for combination is dependent on the payoff of their beliefs, this acting\nas a proxy for performance or usefulness, then the system converges to beliefs\nwhich, on average, have higher payoff.\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 12:19:35 GMT"}, {"version": "v2", "created": "Tue, 20 Sep 2016 14:26:12 GMT"}], "update_date": "2018-01-15", "authors_parsed": [["Crosscombe", "Michael", ""], ["Lawry", "Jonathan", ""]]}, {"id": "1607.05601", "submitter": "Radoslava Kraleva Dr.", "authors": "Velin Kralev, Radoslava Kraleva, Borislav Yurukov", "title": "An Event Grouping Based Algorithm for University Course Timetabling\n  Problem", "comments": "8 pages, 8 figures, 9 tables, International Journal of Computer\n  Science and Information Security (IJCSIS), PaperID 31051699, Vol. 14, No. 06,\n  June 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents the study of an event grouping based algorithm for a\nuniversity course timetabling problem. Several publications which discuss the\nproblem and some approaches for its solution are analyzed. The grouping of\nevents in groups with an equal number of events in each group is not applicable\nto all input data sets. For this reason, a universal approach to all possible\ngroupings of events in commensurate in size groups is proposed here. Also, an\nimplementation of an algorithm based on this approach is presented. The\nmethodology, conditions and the objectives of the experiment are described. The\nexperimental results are analyzed and the ensuing conclusions are stated. The\nfuture guidelines for further research are formulated.\n", "versions": [{"version": "v1", "created": "Mon, 18 Jul 2016 16:53:14 GMT"}], "update_date": "2016-07-20", "authors_parsed": [["Kralev", "Velin", ""], ["Kraleva", "Radoslava", ""], ["Yurukov", "Borislav", ""]]}, {"id": "1607.05809", "submitter": "Kun Xiong", "authors": "Kun Xiong, Anqi Cui, Zefeng Zhang, Ming Li", "title": "Neural Contextual Conversation Learning with Labeled Question-Answering\n  Pairs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural conversational models tend to produce generic or safe responses in\ndifferent contexts, e.g., reply \\textit{\"Of course\"} to narrative statements or\n\\textit{\"I don't know\"} to questions. In this paper, we propose an end-to-end\napproach to avoid such problem in neural generative models. Additional memory\nmechanisms have been introduced to standard sequence-to-sequence (seq2seq)\nmodels, so that context can be considered while generating sentences. Three\nseq2seq models, which memorize a fix-sized contextual vector from hidden input,\nhidden input/output and a gated contextual attention structure respectively,\nhave been trained and tested on a dataset of labeled question-answering pairs\nin Chinese. The model with contextual attention outperforms others including\nthe state-of-the-art seq2seq models on perplexity test. The novel contextual\nmodel generates diverse and robust responses, and is able to carry out\nconversations on a wide range of topics appropriately.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 03:25:31 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Xiong", "Kun", ""], ["Cui", "Anqi", ""], ["Zhang", "Zefeng", ""], ["Li", "Ming", ""]]}, {"id": "1607.05810", "submitter": "Emanuel Diamant", "authors": "Emanuel Diamant", "title": "You want to survive the data deluge: Be careful, Computational\n  Intelligence will not serve you as a rescue boat", "comments": "Oral presentation at the ICNSC 2016 Conference, Mexico City, April\n  2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We are at the dawn of a new era, where advances in computer power, broadband\ncommunication and digital sensor technologies have led to an unprecedented\nflood of data inundating our surrounding. It is generally believed that means\nsuch as Computational Intelligence will help to outlive these tough times.\nHowever, these hopes are improperly high. Computational Intelligence is a\nsurprising composition of two mutually exclusive and contradicting constituents\nthat could be coupled only if you disregard and neglect their controversies:\n\"Computational\" implies reliance on data processing and \"Intelligence\" implies\nreliance on information processing. Only those who are indifferent to\ndata-information discrepancy can believe that such a combination can be viable.\nWe do not believe in miracles, so we will try to share with you our\nreservations.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 03:47:19 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Diamant", "Emanuel", ""]]}, {"id": "1607.05845", "submitter": "Uwe Aickelin", "authors": "Jenna Reps, Zhaoyang Guo, Haoyue Zhu, Uwe Aickelin", "title": "Identifying Candidate Risk Factors for Prescription Drug Side Effects\n  using Causal Contrast Set Mining", "comments": "Health Information Science (4th International Conference, HIS 2015,\n  Melbourne, Australia, May 28-30), pp. 45-55, Lecture Notes in Computer\n  Science, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big longitudinal observational databases present the opportunity to extract\nnew knowledge in a cost effective manner. Unfortunately, the ability of these\ndatabases to be used for causal inference is limited due to the passive way in\nwhich the data are collected resulting in various forms of bias. In this paper\nwe investigate a method that can overcome these limitations and determine\ncausal contrast set rules efficiently from big data. In particular, we present\na new methodology for the purpose of identifying risk factors that increase a\npatients likelihood of experiencing the known rare side effect of renal failure\nafter ingesting aminosalicylates. The results show that the methodology was\nable to identify previously researched risk factors such as being prescribed\ndiuretics and highlighted that patients with a higher than average risk of\nrenal failure may be even more susceptible to experiencing it as a side effect\nafter ingesting aminosalicylates.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 07:42:52 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Reps", "Jenna", ""], ["Guo", "Zhaoyang", ""], ["Zhu", "Haoyue", ""], ["Aickelin", "Uwe", ""]]}, {"id": "1607.05869", "submitter": "Uwe Aickelin", "authors": "Rodrigo Scarpel, Alexandros Ladas, Uwe Aickelin", "title": "Indebted households profiling: a knowledge discovery from database\n  approach", "comments": "Annals of Data Science, 2 (1), pp. 43-59, 2015", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major challenge in consumer credit risk portfolio management is to classify\nhouseholds according to their risk profile. In order to build such risk\nprofiles it is necessary to employ an approach that analyses data\nsystematically in order to detect important relationships, interactions,\ndependencies and associations amongst the available continuous and categorical\nvariables altogether and accurately generate profiles of most interesting\nhousehold segments according to their credit risk. The objective of this work\nis to employ a knowledge discovery from database process to identify groups of\nindebted households and describe their profiles using a database collected by\nthe Consumer Credit Counselling Service (CCCS) in the UK. Employing a framework\nthat allows the usage of both categorical and continuous data altogether to\nfind hidden structures in unlabelled data it was established the ideal number\nof clusters and such clusters were described in order to identify the\nhouseholds who exhibit a high propensity of excessive debt levels.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 09:03:17 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Scarpel", "Rodrigo", ""], ["Ladas", "Alexandros", ""], ["Aickelin", "Uwe", ""]]}, {"id": "1607.05888", "submitter": "Uwe Aickelin", "authors": "Grazziela P. Figueredo, Peer-Olaf Siebers, Uwe Aickelin, Amanda\n  Whitbrook, Jonathan M. Garibaldi", "title": "Juxtaposition of System Dynamics and Agent-based Simulation for a Case\n  Study in Immunosenescence", "comments": "PLOS One, 10 (3), 2015, ISBN: e0118359", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in healthcare and in the quality of life significantly increase\nhuman life expectancy. With the ageing of populations, new un-faced challenges\nare brought to science. The human body is naturally selected to be\nwell-functioning until the age of reproduction to keep the species alive.\nHowever, as the lifespan extends, unseen problems due to the body deterioration\nemerge. There are several age-related diseases with no appropriate treatment;\ntherefore, the complex ageing phenomena needs further understanding.\nImmunosenescence, the ageing of the immune system, is highly correlated to the\nnegative effects of ageing, such as the increase of auto-inflammatory diseases\nand decrease in responsiveness to new diseases. Besides clinical and\nmathematical tools, we believe there is opportunity to further exploit\nsimulation tools to understand immunosenescence. Compared to real-world\nexperimentation, benefits include time and cost effectiveness due to the\nlaborious, resource-intensiveness of the biological environment and the\npossibility of conducting experiments without ethic restrictions. Contrasted\nwith mathematical models, simulation modelling is more suitable for\nrepresenting complex systems and emergence. In addition, there is the belief\nthat simulation models are easier to communicate in interdisciplinary contexts.\nOur work investigates the usefulness of simulations to understand\nimmunosenescence by employing two different simulation methods, agent-based and\nsystem dynamics simulation, to a case study of immune cells depletion with age.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 09:47:31 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Figueredo", "Grazziela P.", ""], ["Siebers", "Peer-Olaf", ""], ["Aickelin", "Uwe", ""], ["Whitbrook", "Amanda", ""], ["Garibaldi", "Jonathan M.", ""]]}, {"id": "1607.05906", "submitter": "Uwe Aickelin", "authors": "Jenna M. Reps, Uwe Aickelin, Richard B. Hubbard", "title": "Refining adverse drug reaction signals by incorporating interaction\n  variables identified using emergent pattern mining", "comments": "Computers in Biology and Medicine, 69 , pp. 61-70, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Purpose: To develop a framework for identifying and incorporating candidate\nconfounding interaction terms into a regularised cox regression analysis to\nrefine adverse drug reaction signals obtained via longitudinal observational\ndata. Methods: We considered six drug families that are commonly associated\nwith myocardial infarction in observational healthcare data, but where the\ncausal relationship ground truth is known (adverse drug reaction or not). We\napplied emergent pattern mining to find itemsets of drugs and medical events\nthat are associated with the development of myocardial infarction. These are\nthe candidate confounding interaction terms. We then implemented a cohort study\ndesign using regularised cox regression that incorporated and accounted for the\ncandidate confounding interaction terms. Results The methodology was able to\naccount for signals generated due to confounding and a cox regression with\nelastic net regularisation correctly ranked the drug families known to be true\nadverse drug reactions above those.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 10:45:57 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Reps", "Jenna M.", ""], ["Aickelin", "Uwe", ""], ["Hubbard", "Richard B.", ""]]}, {"id": "1607.05909", "submitter": "Uwe Aickelin", "authors": "Jiangang Ma, Le Sun, Hua Wang, Yanchun Zhang, Uwe Aickelin", "title": "Supervised Anomaly Detection in Uncertain Pseudoperiodic Data Streams", "comments": "ACM Transactions on Internet Technology (TOIT), 16 (1 (4)), 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Uncertain data streams have been widely generated in many Web applications.\nThe uncertainty in data streams makes anomaly detection from sensor data\nstreams far more challenging. In this paper, we present a novel framework that\nsupports anomaly detection in uncertain data streams. The proposed framework\nadopts an efficient uncertainty pre-processing procedure to identify and\neliminate uncertainties in data streams. Based on the corrected data streams,\nwe develop effective period pattern recognition and feature extraction\ntechniques to improve the computational efficiency. We use classification\nmethods for anomaly detection in the corrected data stream. We also empirically\nshow that the proposed approach shows a high accuracy of anomaly detection on a\nnumber of real datasets.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 10:52:17 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Ma", "Jiangang", ""], ["Sun", "Le", ""], ["Wang", "Hua", ""], ["Zhang", "Yanchun", ""], ["Aickelin", "Uwe", ""]]}, {"id": "1607.05912", "submitter": "Uwe Aickelin", "authors": "Tao Zhang, Peer-Olaf Siebers, Uwe Aickelin", "title": "Simulating user learning in authoritative technology adoption: An agent\n  based model for council-led smart meter deployment planning in the UK", "comments": "Technological Forecasting and Social Change, 106 , pp. 74-84, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How do technology users effectively transit from having zero knowledge about\na technology to making the best use of it after an authoritative technology\nadoption? This post-adoption user learning has received little research\nattention in technology management literature. In this paper we investigate\nuser learning in authoritative technology adoption by developing an agent-based\nmodel using the case of council-led smart meter deployment in the UK City of\nLeeds. Energy consumers gain experience of using smart meters based on the\nlearning curve in behavioural learning. With the agent-based model we carry out\nexperiments to validate the model and test different energy interventions that\nlocal authorities can use to facilitate energy consumers' learning and maintain\ntheir continuous use of the technology. Our results show that the easier energy\nconsumers become experienced, the more energy-efficient they are and the more\nenergy saving they can achieve; encouraging energy consumers' contacts via\nvarious informational means can facilitate their learning; and developing and\nmaintaining their positive attitude toward smart metering can enable them to\nuse the technology continuously. Contributions and energy policy/intervention\nimplications are discussed in this paper.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 10:57:14 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Zhang", "Tao", ""], ["Siebers", "Peer-Olaf", ""], ["Aickelin", "Uwe", ""]]}, {"id": "1607.05913", "submitter": "Uwe Aickelin", "authors": "Polla Fattah, Uwe Aickelin and Christian Wagner", "title": "Optimising Rule-Based Classification in Temporal Data", "comments": "ZANCO Journal of Pure and Applied Sciences, 28 (2), pp. 135-146,\n  2016, ISSN: 2412-3986", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study optimises manually derived rule-based expert system classification\nof objects according to changes in their properties over time. One of the key\nchallenges that this study tries to address is how to classify objects that\nexhibit changes in their behaviour over time, for example how to classify\ncompanies' share price stability over a period of time or how to classify\nstudents' preferences for subjects while they are progressing through school. A\nspecific case the paper considers is the strategy of players in public goods\ngames (as common in economics) across multiple consecutive games. Initial\nclassification starts from expert definitions specifying class allocation for\nplayers based on aggregated attributes of the temporal data. Based on these\ninitial classifications, the optimisation process tries to find an improved\nclassifier which produces the best possible compact classes of objects\n(players) for every time point in the temporal data. The compactness of the\nclasses is measured by a cost function based on internal cluster indices like\nthe Dunn Index, distance measures like Euclidean distance or statistically\nderived measures like standard deviation. The paper discusses the approach in\nthe context of incorporating changing player strategies in the aforementioned\npublic good games, where common classification approaches so far do not\nconsider such changes in behaviour resulting from learning or in-game\nexperience. By using the proposed process for classifying temporal data and the\nactual players' contribution during the games, we aim to produce a more refined\nclassification which in turn may inform the interpretation of public goods game\ndata.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 11:02:16 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Fattah", "Polla", ""], ["Aickelin", "Uwe", ""], ["Wagner", "Christian", ""]]}, {"id": "1607.05954", "submitter": "Carlos Dafonte", "authors": "C. Dafonte, D. Fustes, M. Manteiga, D. Garabato, M. A. Alvarez, A.\n  Ulla, C. Allende Prieto", "title": "On the estimation of stellar parameters with uncertainty prediction from\n  Generative Artificial Neural Networks: application to Gaia RVS simulated\n  spectra", "comments": null, "journal-ref": "A&A 594, A68 (2016)", "doi": "10.1051/0004-6361/201527045", "report-no": null, "categories": "astro-ph.IM astro-ph.SR cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aims. We present an innovative artificial neural network (ANN) architecture,\ncalled Generative ANN (GANN), that computes the forward model, that is it\nlearns the function that relates the unknown outputs (stellar atmospheric\nparameters, in this case) to the given inputs (spectra). Such a model can be\nintegrated in a Bayesian framework to estimate the posterior distribution of\nthe outputs. Methods. The architecture of the GANN follows the same scheme as a\nnormal ANN, but with the inputs and outputs inverted. We train the network with\nthe set of atmospheric parameters (Teff, logg, [Fe/H] and [alpha/Fe]),\nobtaining the stellar spectra for such inputs. The residuals between the\nspectra in the grid and the estimated spectra are minimized using a validation\ndataset to keep solutions as general as possible. Results. The performance of\nboth conventional ANNs and GANNs to estimate the stellar parameters as a\nfunction of the star brightness is presented and compared for different\nGalactic populations. GANNs provide significantly improved parameterizations\nfor early and intermediate spectral types with rich and intermediate\nmetallicities. The behaviour of both algorithms is very similar for our sample\nof late-type stars, obtaining residuals in the derivation of [Fe/H] and\n[alpha/Fe] below 0.1dex for stars with Gaia magnitude Grvs<12, which accounts\nfor a number in the order of four million stars to be observed by the Radial\nVelocity Spectrograph of the Gaia satellite. Conclusions. Uncertainty\nestimation of computed astrophysical parameters is crucial for the validation\nof the parameterization itself and for the subsequent exploitation by the\nastronomical community. GANNs produce not only the parameters for a given\nspectrum, but a goodness-of-fit between the observed spectrum and the predicted\none for a given set of parameters. Moreover, they allow us to obtain the full\nposterior distribution...\n", "versions": [{"version": "v1", "created": "Tue, 19 Jul 2016 15:16:56 GMT"}], "update_date": "2016-10-19", "authors_parsed": [["Dafonte", "C.", ""], ["Fustes", "D.", ""], ["Manteiga", "M.", ""], ["Garabato", "D.", ""], ["Alvarez", "M. A.", ""], ["Ulla", "A.", ""], ["Prieto", "C. Allende", ""]]}, {"id": "1607.05968", "submitter": "Michael Spranger", "authors": "Michael Spranger and Jakob Suchan and Mehul Bhatt", "title": "Robust Natural Language Processing - Combining Reasoning, Cognitive\n  Semantics and Construction Grammar for Spatial Language", "comments": "in IJCAI'16: Proceedings of the 25th international joint conference\n  on Artificial intelligence, Palo Alto, 2016. AAAI Press", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a system for generating and understanding of dynamic and static\nspatial relations in robotic interaction setups. Robots describe an environment\nof moving blocks using English phrases that include spatial relations such as\n\"across\" and \"in front of\". We evaluate the system in robot-robot interactions\nand show that the system can robustly deal with visual perception errors,\nlanguage omissions and ungrammatical utterances.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 14:15:24 GMT"}], "update_date": "2016-07-21", "authors_parsed": [["Spranger", "Michael", ""], ["Suchan", "Jakob", ""], ["Bhatt", "Mehul", ""]]}, {"id": "1607.06025", "submitter": "Janez Starc", "authors": "Janez Starc and Dunja Mladeni\\'c", "title": "Constructing a Natural Language Inference Dataset using Generative\n  Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural Language Inference is an important task for Natural Language\nUnderstanding. It is concerned with classifying the logical relation between\ntwo sentences. In this paper, we propose several text generative neural\nnetworks for generating text hypothesis, which allows construction of new\nNatural Language Inference datasets. To evaluate the models, we propose a new\nmetric -- the accuracy of the classifier trained on the generated dataset. The\naccuracy obtained by our best generative model is only 2.7% lower than the\naccuracy of the classifier trained on the original, human crafted dataset.\nFurthermore, the best generated dataset combined with the original dataset\nachieves the highest accuracy. The best model learns a mapping embedding for\neach training example. By comparing various metrics we show that datasets that\nobtain higher ROUGE or METEOR scores do not necessarily yield higher\nclassification accuracies. We also provide analysis of what are the\ncharacteristics of a good dataset including the distinguishability of the\ngenerated datasets from the original one.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 16:59:21 GMT"}, {"version": "v2", "created": "Mon, 27 Mar 2017 08:33:27 GMT"}], "update_date": "2017-03-28", "authors_parsed": [["Starc", "Janez", ""], ["Mladeni\u0107", "Dunja", ""]]}, {"id": "1607.06186", "submitter": "Uwe Aickelin", "authors": "Javier Navarro, Christian Wagner, Uwe Aickelin", "title": "Applying Interval Type-2 Fuzzy Rule Based Classifiers Through a\n  Cluster-Based Class Representation", "comments": "2015 IEEE Symposium Series on Computational Intelligence, pp.\n  1816-1823, IEEE, 2015, ISBN: 978-1-4799-7560-0", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Fuzzy Rule-Based Classification Systems (FRBCSs) have the potential to\nprovide so-called interpretable classifiers, i.e. classifiers which can be\nintrospective, understood, validated and augmented by human experts by relying\non fuzzy-set based rules. This paper builds on prior work for interval type-2\nfuzzy set based FRBCs where the fuzzy sets and rules of the classifier are\ngenerated using an initial clustering stage. By introducing Subtractive\nClustering in order to identify multiple cluster prototypes, the proposed\napproach has the potential to deliver improved classification performance while\nmaintaining good interpretability, i.e. without resulting in an excessive\nnumber of rules. The paper provides a detailed overview of the proposed FRBC\nframework, followed by a series of exploratory experiments on both linearly and\nnon-linearly separable datasets, comparing results to existing rule-based and\nSVM approaches. Overall, initial results indicate that the approach enables\ncomparable classification performance to non rule-based classifiers such as\nSVM, while often achieving this with a very small number of rules.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 04:36:23 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Navarro", "Javier", ""], ["Wagner", "Christian", ""], ["Aickelin", "Uwe", ""]]}, {"id": "1607.06187", "submitter": "Uwe Aickelin", "authors": "Javier Navarro, Christian Wagner, Uwe Aickelin, Lynsey Green and\n  Robert Ashford", "title": "Exploring Differences in Interpretation of Words Essential in Medical\n  Expert-Patient Communication", "comments": "IEEE International Conference on Fuzzy Systems (FUZZ-IEEE 2016),\n  24-29 July 2016, Vancouver, Canada, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of cancer treatment and surgery, quality of life assessment is\na crucial part of determining treatment success and viability. In order to\nassess it, patients completed questionnaires which employ words to capture\naspects of patients well-being are the norm. As the results of these\nquestionnaires are often used to assess patient progress and to determine\nfuture treatment options, it is important to establish that the words used are\ninterpreted in the same way by both patients and medical professionals. In this\npaper, we capture and model patients perceptions and associated uncertainty\nabout the words used to describe the level of their physical function used in\nthe highly common (in Sarcoma Services) Toronto Extremity Salvage Score (TESS)\nquestionnaire. The paper provides detail about the interval-valued data capture\nas well as the subsequent modelling of the data using fuzzy sets. Based on an\ninitial sample of participants, we use Jaccard similarity on the resulting\nwords models to show that there may be considerable differences in the\ninterpretation of commonly used questionnaire terms, thus presenting a very\nreal risk of miscommunication between patients and medical professionals as\nwell as within the group of medical professionals.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 04:40:14 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Navarro", "Javier", ""], ["Wagner", "Christian", ""], ["Aickelin", "Uwe", ""], ["Green", "Lynsey", ""], ["Ashford", "Robert", ""]]}, {"id": "1607.06198", "submitter": "Uwe Aickelin", "authors": "Jenna Marie Reps, Jonathan M. Garibaldi, Uwe Aickelin, Jack E. Gibson,\n  Richard B.Hubbard", "title": "Supervised Adverse Drug Reaction Signalling Framework Imitating Bradford\n  Hill's Causality Considerations", "comments": null, "journal-ref": "Journal of Biomedical Informatics, 56 , pp. 356-368, 2015", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Big longitudinal observational medical data potentially hold a wealth of\ninformation and have been recognised as potential sources for gaining new drug\nsafety knowledge. Unfortunately there are many complexities and underlying\nissues when analysing longitudinal observational data. Due to these\ncomplexities, existing methods for large-scale detection of negative side\neffects using observational data all tend to have issues distinguishing between\nassociation and causality. New methods that can better discriminate causal and\nnon-causal relationships need to be developed to fully utilise the data. In\nthis paper we propose using a set of causality considerations developed by the\nepidemiologist Bradford Hill as a basis for engineering features that enable\nthe application of supervised learning for the problem of detecting negative\nside effects. The Bradford Hill considerations look at various perspectives of\na drug and outcome relationship to determine whether it shows causal traits. We\ntaught a classifier to find patterns within these perspectives and it learned\nto discriminate between association and causality. The novelty of this research\nis the combination of supervised learning and Bradford Hill's causality\nconsiderations to automate the Bradford Hill's causality assessment. We\nevaluated the framework on a drug safety gold standard know as the\nobservational medical outcomes partnership's nonspecified association reference\nset. The methodology obtained excellent discriminate ability with area under\nthe curves ranging between 0.792-0.940 (existing method optimal: 0.73) and a\nmean average precision of 0.640 (existing method optimal: 0.141). The proposed\nfeatures can be calculated efficiently and be readily updated, making the\nframework suitable for big observational data.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 05:31:04 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Reps", "Jenna Marie", ""], ["Garibaldi", "Jonathan M.", ""], ["Aickelin", "Uwe", ""], ["Gibson", "Jack E.", ""], ["Hubbard", "Richard B.", ""]]}, {"id": "1607.06264", "submitter": "Alejandro Betancourt", "authors": "Alejandro Betancourt, Pietro Morerio, Emilia Barakova, Lucio\n  Marcenaro, Matthias Rauterberg, Carlo Regazzoni", "title": "Left/Right Hand Segmentation in Egocentric Videos", "comments": null, "journal-ref": null, "doi": "10.1016/j.cviu.2016.09.005", "report-no": null, "categories": "cs.HC cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Wearable cameras allow people to record their daily activities from a\nuser-centered (First Person Vision) perspective. Due to their favorable\nlocation, wearable cameras frequently capture the hands of the user, and may\nthus represent a promising user-machine interaction tool for different\napplications. Existent First Person Vision methods handle hand segmentation as\na background-foreground problem, ignoring two important facts: i) hands are not\na single \"skin-like\" moving element, but a pair of interacting cooperative\nentities, ii) close hand interactions may lead to hand-to-hand occlusions and,\nas a consequence, create a single hand-like segment. These facts complicate a\nproper understanding of hand movements and interactions. Our approach extends\ntraditional background-foreground strategies, by including a\nhand-identification step (left-right) based on a Maxwell distribution of angle\nand position. Hand-to-hand occlusions are addressed by exploiting temporal\nsuperpixels. The experimental results show that, in addition to a reliable\nleft/right hand-segmentation, our approach considerably improves the\ntraditional background-foreground hand-segmentation.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 11:06:05 GMT"}], "update_date": "2016-09-15", "authors_parsed": [["Betancourt", "Alejandro", ""], ["Morerio", "Pietro", ""], ["Barakova", "Emilia", ""], ["Marcenaro", "Lucio", ""], ["Rauterberg", "Matthias", ""], ["Regazzoni", "Carlo", ""]]}, {"id": "1607.06275", "submitter": "Peng Li", "authors": "Peng Li, Wei Li, Zhengyan He, Xuguang Wang, Ying Cao, Jie Zhou, Wei Xu", "title": "Dataset and Neural Recurrent Sequence Labeling Model for Open-Domain\n  Factoid Question Answering", "comments": "10 pages, 3 figures, withdraw experimental results on CNN/Daily Mail\n  datasets", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While question answering (QA) with neural network, i.e. neural QA, has\nachieved promising results in recent years, lacking of large scale real-word QA\ndataset is still a challenge for developing and evaluating neural QA system. To\nalleviate this problem, we propose a large scale human annotated real-world QA\ndataset WebQA with more than 42k questions and 556k evidences. As existing\nneural QA methods resolve QA either as sequence generation or\nclassification/ranking problem, they face challenges of expensive softmax\ncomputation, unseen answers handling or separate candidate answer generation\ncomponent. In this work, we cast neural QA as a sequence labeling problem and\npropose an end-to-end sequence labeling model, which overcomes all the above\nchallenges. Experimental results on WebQA show that our model outperforms the\nbaselines significantly with an F1 score of 74.69% with word-based input, and\nthe performance drops only 3.72 F1 points with more challenging character-based\ninput.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 11:40:50 GMT"}, {"version": "v2", "created": "Thu, 1 Sep 2016 10:56:45 GMT"}], "update_date": "2016-09-02", "authors_parsed": [["Li", "Peng", ""], ["Li", "Wei", ""], ["He", "Zhengyan", ""], ["Wang", "Xuguang", ""], ["Cao", "Ying", ""], ["Zhou", "Jie", ""], ["Xu", "Wei", ""]]}, {"id": "1607.06332", "submitter": "Uwe Aickelin", "authors": "Tao Zhang, Peer-Olaf Siebers, Uwe Aickelin", "title": "Modelling Office Energy Consumption: An Agent Based Approach", "comments": "Proceedings of the 3rd World Congress on Social Simulation\n  (WCSS2010), 5-9 Sep, Kassel, Germany, 2010. arXiv admin note: substantial\n  text overlap with arXiv:1305.7437", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we develop an agent-based model which integrates four\nimportant elements, i.e. organisational energy management policies/regulations,\nenergy management technologies, electric appliances and equipment, and human\nbehaviour, based on a case study, to simulate the energy consumption in office\nbuildings. With the model, we test the effectiveness of different energy\nmanagement strategies, and solve practical office energy consumption problems.\nThis paper theoretically contributes to an integration of four elements\ninvolved in the complex organisational issue of office energy consumption, and\npractically contributes to an application of agent-based approach for office\nbuilding energy consumption study.\n", "versions": [{"version": "v1", "created": "Wed, 20 Jul 2016 07:30:43 GMT"}], "update_date": "2016-07-22", "authors_parsed": [["Zhang", "Tao", ""], ["Siebers", "Peer-Olaf", ""], ["Aickelin", "Uwe", ""]]}, {"id": "1607.06520", "submitter": "Tolga Bolukbasi", "authors": "Tolga Bolukbasi, Kai-Wei Chang, James Zou, Venkatesh Saligrama, Adam\n  Kalai", "title": "Man is to Computer Programmer as Woman is to Homemaker? Debiasing Word\n  Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The blind application of machine learning runs the risk of amplifying biases\npresent in data. Such a danger is facing us with word embedding, a popular\nframework to represent text data as vectors which has been used in many machine\nlearning and natural language processing tasks. We show that even word\nembeddings trained on Google News articles exhibit female/male gender\nstereotypes to a disturbing extent. This raises concerns because their\nwidespread use, as we describe, often tends to amplify these biases.\nGeometrically, gender bias is first shown to be captured by a direction in the\nword embedding. Second, gender neutral words are shown to be linearly separable\nfrom gender definition words in the word embedding. Using these properties, we\nprovide a methodology for modifying an embedding to remove gender stereotypes,\nsuch as the association between between the words receptionist and female,\nwhile maintaining desired associations such as between the words queen and\nfemale. We define metrics to quantify both direct and indirect gender biases in\nembeddings, and develop algorithms to \"debias\" the embedding. Using\ncrowd-worker evaluation as well as standard benchmarks, we empirically\ndemonstrate that our algorithms significantly reduce gender bias in embeddings\nwhile preserving the its useful properties such as the ability to cluster\nrelated concepts and to solve analogy tasks. The resulting embeddings can be\nused in applications without amplifying gender bias.\n", "versions": [{"version": "v1", "created": "Thu, 21 Jul 2016 22:26:20 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Bolukbasi", "Tolga", ""], ["Chang", "Kai-Wei", ""], ["Zou", "James", ""], ["Saligrama", "Venkatesh", ""], ["Kalai", "Adam", ""]]}, {"id": "1607.06532", "submitter": "Kuan-Yu Chen", "authors": "Kuan-Yu Chen, Shih-Hung Liu, Berlin Chen, Hsin-Min Wang, Hsin-Hsi Chen", "title": "Novel Word Embedding and Translation-based Language Modeling for\n  Extractive Speech Summarization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embedding methods revolve around learning continuous distributed vector\nrepresentations of words with neural networks, which can capture semantic\nand/or syntactic cues, and in turn be used to induce similarity measures among\nwords, sentences and documents in context. Celebrated methods can be\ncategorized as prediction-based and count-based methods according to the\ntraining objectives and model architectures. Their pros and cons have been\nextensively analyzed and evaluated in recent studies, but there is relatively\nless work continuing the line of research to develop an enhanced learning\nmethod that brings together the advantages of the two model families. In\naddition, the interpretation of the learned word representations still remains\nsomewhat opaque. Motivated by the observations and considering the pressing\nneed, this paper presents a novel method for learning the word representations,\nwhich not only inherits the advantages of classic word embedding methods but\nalso offers a clearer and more rigorous interpretation of the learned word\nrepresentations. Built upon the proposed word embedding method, we further\nformulate a translation-based language modeling framework for the extractive\nspeech summarization task. A series of empirical evaluations demonstrate the\neffectiveness of the proposed word representation learning and language\nmodeling techniques in extractive speech summarization.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 00:20:09 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Chen", "Kuan-Yu", ""], ["Liu", "Shih-Hung", ""], ["Chen", "Berlin", ""], ["Wang", "Hsin-Min", ""], ["Chen", "Hsin-Hsi", ""]]}, {"id": "1607.06560", "submitter": "Amol Patwardhan", "authors": "Amol S Patwardhan, Jacob Badeaux, Siavash, Gerald M Knapp", "title": "Automated Prediction of Temporal Relations", "comments": "8 pages, 1 figure, Technical report, 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background: There has been growing research interest in automated answering\nof questions or generation of summary of free form text such as news article.\nIn order to implement this task, the computer should be able to identify the\nsequence of events, duration of events, time at which event occurred and the\nrelationship type between event pairs, time pairs or event-time pairs. Specific\nProblem: It is important to accurately identify the relationship type between\ncombinations of event and time before the temporal ordering of events can be\ndefined. The machine learning approach taken in Mani et. al (2006) provides an\naccuracy of only 62.5 on the baseline data from TimeBank. The researchers used\nmaximum entropy classifier in their methodology. TimeML uses the TLINK\nannotation to tag a relationship type between events and time. The time\ncomplexity is quadratic when it comes to tagging documents with TLINK using\nhuman annotation. This research proposes using decision tree and parsing to\nimprove the relationship type tagging. This research attempts to solve the gaps\nin human annotation by automating the task of relationship type tagging in an\nattempt to improve the accuracy of event and time relationship in annotated\ndocuments. Scope information: The documents from the domain of news will be\nused. The tagging will be performed within the same document and not across\ndocuments. The relationship types will be identified only for a pair of event\nand time and not a chain of events. The research focuses on documents tagged\nusing the TimeML specification which contains tags such as EVENT, TLINK, and\nTIMEX. Each tag has attributes such as identifier, relation, POS, time etc.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 05:38:37 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Patwardhan", "Amol S", ""], ["Badeaux", "Jacob", ""], ["Siavash", "", ""], ["Knapp", "Gerald M", ""]]}, {"id": "1607.06583", "submitter": "Saman Sarraf", "authors": "Saman Sarraf, Ghassem Tofighi", "title": "Classification of Alzheimer's Disease Structural MRI Data by Deep\n  Learning Convolutional Neural Networks", "comments": "arXiv admin note: text overlap with arXiv:1603.08631", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, machine learning techniques especially predictive modeling and\npattern recognition in biomedical sciences from drug delivery system to medical\nimaging has become one of the important methods which are assisting researchers\nto have deeper understanding of entire issue and to solve complex medical\nproblems. Deep learning is a powerful machine learning algorithm in\nclassification while extracting low to high-level features. In this paper, we\nused convolutional neural network to classify Alzheimer's brain from normal\nhealthy brain. The importance of classifying this kind of medical data is to\npotentially develop a predict model or system in order to recognize the type\ndisease from normal subjects or to estimate the stage of the disease.\nClassification of clinical data such as Alzheimer's disease has been always\nchallenging and most problematic part has been always selecting the most\ndiscriminative features. Using Convolutional Neural Network (CNN) and the\nfamous architecture LeNet-5, we successfully classified structural MRI data of\nAlzheimer's subjects from normal controls where the accuracy of test data on\ntrained data reached 98.84%. This experiment suggests us the shift and scale\ninvariant features extracted by CNN followed by deep learning classification is\nmost powerful method to distinguish clinical data from healthy data in fMRI.\nThis approach also enables us to expand our methodology to predict more\ncomplicated systems.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 07:48:18 GMT"}, {"version": "v2", "created": "Fri, 19 May 2017 20:41:51 GMT"}], "update_date": "2017-05-23", "authors_parsed": [["Sarraf", "Saman", ""], ["Tofighi", "Ghassem", ""]]}, {"id": "1607.06617", "submitter": "Xuhui Zhang Mr", "authors": "Xuhui Zhang, Kevin B. Korb, Ann E. Nicholson, Steven Mascaro", "title": "Latent Variable Discovery Using Dependency Patterns", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The causal discovery of Bayesian networks is an active and important research\narea, and it is based upon searching the space of causal models for those which\ncan best explain a pattern of probabilistic dependencies shown in the data.\nHowever, some of those dependencies are generated by causal structures\ninvolving variables which have not been measured, i.e., latent variables. Some\nsuch patterns of dependency \"reveal\" themselves, in that no model based solely\nupon the observed variables can explain them as well as a model using a latent\nvariable. That is what latent variable discovery is based upon. Here we did a\nsearch for finding them systematically, so that they may be applied in latent\nvariable discovery in a more rigorous fashion.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 09:48:25 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Zhang", "Xuhui", ""], ["Korb", "Kevin B.", ""], ["Nicholson", "Ann E.", ""], ["Mascaro", "Steven", ""]]}, {"id": "1607.06641", "submitter": "Jialin Liu Ph.D", "authors": "Jialin Liu, Michael Fairbank, Diego P\\'erez-Li\\'ebana, Simon M. Lucas", "title": "Optimal resampling for the noisy OneMax problem", "comments": "8 pages, 1 table, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The OneMax problem is a standard benchmark optimisation problem for a binary\nsearch space. Recent work on applying a Bandit-Based Random Mutation\nHill-Climbing algorithm to the noisy OneMax Problem showed that it is important\nto choose a good value for the resampling number to make a careful trade off\nbetween taking more samples in order to reduce noise, and taking fewer samples\nto reduce the total computational cost. This paper extends that observation, by\nderiving an analytical expression for the running time of the RMHC algorithm\nwith resampling applied to the noisy OneMax problem, and showing both\ntheoretically and empirically that the optimal resampling number increases with\nthe number of dimensions in the search space.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 11:51:49 GMT"}, {"version": "v2", "created": "Sat, 18 Mar 2017 08:41:23 GMT"}, {"version": "v3", "created": "Mon, 12 Jun 2017 15:40:50 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Liu", "Jialin", ""], ["Fairbank", "Michael", ""], ["P\u00e9rez-Li\u00e9bana", "Diego", ""], ["Lucas", "Simon M.", ""]]}, {"id": "1607.06667", "submitter": "Nathanael Perraudin N. P.", "authors": "Nathanael Perraudin, Nicki Holighaus, Piotr Majdak, Peter Balazs", "title": "Inpainting of long audio segments with similarity graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.MM cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel method for the compensation of long duration data loss in\naudio signals, in particular music. The concealment of such signal defects is\nbased on a graph that encodes signal structure in terms of time-persistent\nspectral similarity. A suitable candidate segment for the substitution of the\nlost content is proposed by an intuitive optimization scheme and smoothly\ninserted into the gap, i.e. the lost or distorted signal region. Extensive\nlistening tests show that the proposed algorithm provides highly promising\nresults when applied to a variety of real-world music signals.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 13:12:33 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 19:14:19 GMT"}, {"version": "v3", "created": "Sun, 17 Sep 2017 17:15:08 GMT"}, {"version": "v4", "created": "Fri, 23 Feb 2018 10:15:47 GMT"}], "update_date": "2018-02-26", "authors_parsed": [["Perraudin", "Nathanael", ""], ["Holighaus", "Nicki", ""], ["Majdak", "Piotr", ""], ["Balazs", "Peter", ""]]}, {"id": "1607.06759", "submitter": "Alexander Kott", "authors": "Michael Ownby, Alexander Kott", "title": "Predicting Enemy's Actions Improves Commander Decision-Making", "comments": "A version of this paper was presented at CCRTS'06", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Defense Advanced Research Projects Agency (DARPA) Real-time Adversarial\nIntelligence and Decision-making (RAID) program is investigating the\nfeasibility of \"reading the mind of the enemy\" - to estimate and anticipate, in\nreal-time, the enemy's likely goals, deceptions, actions, movements and\npositions. This program focuses specifically on urban battles at echelons of\nbattalion and below. The RAID program leverages approximate game-theoretic and\ndeception-sensitive algorithms to provide real-time enemy estimates to a\ntactical commander. A key hypothesis of the program is that these predictions\nand recommendations will make the commander more effective, i.e. he should be\nable to achieve his operational goals safer, faster, and more efficiently.\nRealistic experimentation and evaluation drive the development process using\nhuman-in-the-loop wargames to compare humans and the RAID system. Two\nexperiments were conducted in 2005 as part of Phase I to determine if the RAID\nsoftware could make predictions and recommendations as effectively and\naccurately as a 4-person experienced staff. This report discusses the\nintriguing and encouraging results of these first two experiments conducted by\nthe RAID program. It also provides details about the experiment environment and\nmethodology that were used to demonstrate and prove the research goals.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 17:37:24 GMT"}], "update_date": "2016-07-25", "authors_parsed": [["Ownby", "Michael", ""], ["Kott", "Alexander", ""]]}, {"id": "1607.06875", "submitter": "Steve Doubleday", "authors": "Steve Doubleday, Sean Trott, Jerome Feldman", "title": "Processing Natural Language About Ongoing Actions", "comments": "6 pages, 8 figures. Updated with PIPE citations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Actions may not proceed as planned; they may be interrupted, resumed or\noverridden. This is a challenge to handle in a natural language understanding\nsystem. We describe extensions to an existing implementation for the control of\nautonomous systems by natural language, to enable such systems to handle\nincoming language requests regarding actions. Language Communication with\nAutonomous Systems (LCAS) has been extended with support for X-nets,\nparameterized executable schemas representing actions. X-nets enable the system\nto control actions at a desired level of granularity, while providing a\nmechanism for language requests to be processed asynchronously. Standard\nsemantics supported include requests to stop, continue, or override the\nexisting action. The specific domain demonstrated is the control of motion of a\nsimulated robot, but the approach is general, and could be applied to other\ndomains.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jul 2016 01:46:09 GMT"}, {"version": "v2", "created": "Sat, 30 Jul 2016 13:32:01 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Doubleday", "Steve", ""], ["Trott", "Sean", ""], ["Feldman", "Jerome", ""]]}, {"id": "1607.07027", "submitter": "Vinu E V", "authors": "E. V. Vinu, P Sreenivasa Kumar", "title": "Redundancy-free Verbalization of Individuals for Ontology Validation", "comments": "Under review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of verbalizing Web Ontology Language (OWL) axioms\nof domain ontologies in this paper. The existing approaches address the problem\nof fidelity of verbalized OWL texts to OWL semantics by exploring different\nways of expressing the same OWL axiom in various linguistic forms. They also\nperform grouping and aggregating of the natural language (NL) sentences that\nare generated corresponding to each OWL statement into a comprehensible\nstructure. However, no efforts have been taken to try out a semantic reduction\nat logical level to remove redundancies and repetitions, so that the reduced\nset of axioms can be used for generating a more meaningful and\nhuman-understandable (what we call redundancy-free) text. Our experiments show\nthat, formal semantic reduction at logical level is very helpful to generate\nredundancy-free descriptions of ontology entities. In this paper, we\nparticularly focus on generating descriptions of individuals of SHIQ based\nontologies. The details of a case study are provided to support the usefulness\nof the redundancy-free NL descriptions of individuals, in knowledge validation\napplication.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jul 2016 11:22:00 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Vinu", "E. V.", ""], ["Kumar", "P Sreenivasa", ""]]}, {"id": "1607.07043", "submitter": "Amir Shahroudy", "authors": "Jun Liu, Amir Shahroudy, Dong Xu, and Gang Wang", "title": "Spatio-Temporal LSTM with Trust Gates for 3D Human Action Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  3D action recognition - analysis of human actions based on 3D skeleton data -\nbecomes popular recently due to its succinctness, robustness, and\nview-invariant representation. Recent attempts on this problem suggested to\ndevelop RNN-based learning methods to model the contextual dependency in the\ntemporal domain. In this paper, we extend this idea to spatio-temporal domains\nto analyze the hidden sources of action-related information within the input\ndata over both domains concurrently. Inspired by the graphical structure of the\nhuman skeleton, we further propose a more powerful tree-structure based\ntraversal method. To handle the noise and occlusion in 3D skeleton data, we\nintroduce new gating mechanism within LSTM to learn the reliability of the\nsequential input data and accordingly adjust its effect on updating the\nlong-term context information stored in the memory cell. Our method achieves\nstate-of-the-art performance on 4 challenging benchmark datasets for 3D human\naction analysis.\n", "versions": [{"version": "v1", "created": "Sun, 24 Jul 2016 13:39:11 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Liu", "Jun", ""], ["Shahroudy", "Amir", ""], ["Xu", "Dong", ""], ["Wang", "Gang", ""]]}, {"id": "1607.07249", "submitter": "Joern Hees", "authors": "J\\\"orn Hees, Rouven Bauer, Joachim Folz, Damian Borth and Andreas\n  Dengel", "title": "An Evolutionary Algorithm to Learn SPARQL Queries for\n  Source-Target-Pairs: Finding Patterns for Human Associations in DBpedia", "comments": "15 pages, 2 figures, as of 2016-09-13\n  6a19d5d7020770dc0711081ce2c1e52f71bf4b86", "journal-ref": null, "doi": "10.1007/978-3-319-49004-5_22", "report-no": null, "categories": "cs.AI cs.DB cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Efficient usage of the knowledge provided by the Linked Data community is\noften hindered by the need for domain experts to formulate the right SPARQL\nqueries to answer questions. For new questions they have to decide which\ndatasets are suitable and in which terminology and modelling style to phrase\nthe SPARQL query.\n  In this work we present an evolutionary algorithm to help with this\nchallenging task. Given a training list of source-target node-pair examples our\nalgorithm can learn patterns (SPARQL queries) from a SPARQL endpoint. The\nlearned patterns can be visualised to form the basis for further investigation,\nor they can be used to predict target nodes for new source nodes.\n  Amongst others, we apply our algorithm to a dataset of several hundred human\nassociations (such as \"circle - square\") to find patterns for them in DBpedia.\nWe show the scalability of the algorithm by running it against a SPARQL\nendpoint loaded with > 7.9 billion triples. Further, we use the resulting\nSPARQL queries to mimic human associations with a Mean Average Precision (MAP)\nof 39.9 % and a Recall@10 of 63.9 %.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2016 12:47:38 GMT"}, {"version": "v2", "created": "Tue, 26 Jul 2016 12:13:14 GMT"}, {"version": "v3", "created": "Tue, 13 Sep 2016 10:27:06 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Hees", "J\u00f6rn", ""], ["Bauer", "Rouven", ""], ["Folz", "Joachim", ""], ["Borth", "Damian", ""], ["Dengel", "Andreas", ""]]}, {"id": "1607.07288", "submitter": "Alexander Kott", "authors": "Alexander Kott, Wes Milks", "title": "Validation of Information Fusion", "comments": "This is a version of the paper presented at FUSION'09", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We motivate and offer a formal definition of validation as it applies to\ninformation fusion systems. Common definitions of validation compare the actual\nstate of the world with that derived by the fusion process. This definition\nconflates properties of the fusion system with properties of systems that\nintervene between the world and the fusion system. We propose an alternative\ndefinition where validation of an information fusion system references a\nstandard fusion device, such as recognized human experts. We illustrate the\napproach by describing the validation process implemented in RAID, a program\nconducted by DARPA and focused on information fusion in adversarial, deceptive\nenvironments.\n", "versions": [{"version": "v1", "created": "Fri, 22 Jul 2016 17:18:05 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Kott", "Alexander", ""], ["Milks", "Wes", ""]]}, {"id": "1607.07311", "submitter": "Majd Hawasly", "authors": "Majd Hawasly, Florian T. Pokorny and Subramanian Ramamoorthy", "title": "Estimating Activity at Multiple Scales using Spatial Abstractions", "comments": "16 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous robots operating in dynamic environments must maintain beliefs\nover a hypothesis space that is rich enough to represent the activities of\ninterest at different scales. This is important both in order to accommodate\nthe availability of evidence at varying degrees of coarseness, such as when\ninterpreting and assimilating natural instructions, but also in order to make\nsubsequent reactive planning more efficient. We present an algorithm that\ncombines a topology-based trajectory clustering procedure that generates\nhierarchically-structured spatial abstractions with a bank of particle filters\nat each of these abstraction levels so as to produce probability estimates over\nan agent's navigation activity that is kept consistent across the hierarchy. We\nstudy the performance of the proposed method using a synthetic trajectory\ndataset in 2D, as well as a dataset taken from AIS-based tracking of ships in\nan extended harbour area. We show that, in comparison to a baseline which is a\nparticle filter that estimates activity without exploiting such structure, our\nmethod achieves a better normalised error in predicting the trajectory as well\nas better time to convergence to a true class when compared against ground\ntruth.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2016 15:17:06 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Hawasly", "Majd", ""], ["Pokorny", "Florian T.", ""], ["Ramamoorthy", "Subramanian", ""]]}, {"id": "1607.07326", "submitter": "Flavian Vasile", "authors": "Flavian Vasile, Elena Smirnova and Alexis Conneau", "title": "Meta-Prod2Vec - Product Embeddings Using Side-Information for\n  Recommendation", "comments": null, "journal-ref": null, "doi": "10.1145/2959100.2959160", "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose Meta-Prod2vec, a novel method to compute item similarities for\nrecommendation that leverages existing item metadata. Such scenarios are\nfrequently encountered in applications such as content recommendation, ad\ntargeting and web search. Our method leverages past user interactions with\nitems and their attributes to compute low-dimensional embeddings of items.\nSpecifically, the item metadata is in- jected into the model as side\ninformation to regularize the item embeddings. We show that the new item\nrepresenta- tions lead to better performance on recommendation tasks on an open\nmusic dataset.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2016 15:54:07 GMT"}], "update_date": "2016-07-26", "authors_parsed": [["Vasile", "Flavian", ""], ["Smirnova", "Elena", ""], ["Conneau", "Alexis", ""]]}, {"id": "1607.07437", "submitter": "John M. Myers", "authors": "F. Hadi Madjid and John M. Myers", "title": "Symbols of a cosmic order", "comments": "28 pages; accepted for publication in Annals of Physics", "journal-ref": null, "doi": "10.1016/j.aop.2016.07.022", "report-no": null, "categories": "physics.hist-ph cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The world runs on communicated sequences of symbols, e.g. numerals. Examining\nboth engineered and natural communications networks reveals an unsuspected\norder that depends on contact with an unpredictable entity. This order has\nthree roots. The first is a proof within quantum theory that no evidence can\never determine its explanation, so that an agent choosing an explanation must\ndo so unpredictably. The second root is the showing that clocks that step\ncomputers do not \"tell time\" but serve as self-adjusting symbol-handling agents\nthat regulate \"logically synchronized\" motion in response to unpredictable\ndisturbances. Such a clock-agent has a certain independence as well as the\ncapacity to communicate via unpredictable symbols with other clock-agents and\nto adjust its own tick rate in response to that communication. The third root\nis the noticing of unpredictable symbol exchange in natural systems, including\nthe transmission of symbols found in molecular biology. We introduce a\nsymbol-handling agent as a role played in some cases by a person, for example a\nphysicist who chooses an explanation of given experimental outcomes, and in\nother cases by some other biological entity, and in still other cases by an\ninanimate device, such as a computer-based detector used in physical\nmeasurements. While we forbear to try to explain the propensity of agents at\nall levels from cells to civilizations to form and operate networks of\nlogically synchronized symbol-handling agents, we point to this propensity as\nan overlooked cosmic order, an order structured by the unpredictability ensuing\nfrom the proof. Appreciating the cosmic order leads to a conception of agency\nthat replaces volition by unpredictability and re-conceives the notion of\nobjectivity in a way that makes a place for agency in the world as described by\nphysics. Some specific implications for physics are outlined.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 15:27:33 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Madjid", "F. Hadi", ""], ["Myers", "John M.", ""]]}, {"id": "1607.07514", "submitter": "Soroush Vosoughi Dr", "authors": "Soroush Vosoughi, Prashanth Vijayaraghavan and Deb Roy", "title": "Tweet2Vec: Learning Tweet Embeddings Using Character-level CNN-LSTM\n  Encoder-Decoder", "comments": "SIGIR 2016, July 17-21, 2016, Pisa. Proceedings of SIGIR 2016. Pisa,\n  Italy (2016)", "journal-ref": null, "doi": "10.1145/2911451.2914762", "report-no": null, "categories": "cs.CL cs.AI cs.NE cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Tweet2Vec, a novel method for generating general-purpose vector\nrepresentation of tweets. The model learns tweet embeddings using\ncharacter-level CNN-LSTM encoder-decoder. We trained our model on 3 million,\nrandomly selected English-language tweets. The model was evaluated using two\nmethods: tweet semantic similarity and tweet sentiment categorization,\noutperforming the previous state-of-the-art in both tasks. The evaluations\ndemonstrate the power of the tweet embeddings generated by our model for\nvarious tweet categorization tasks. The vector representations generated by our\nmodel are generic, and hence can be applied to a variety of tasks. Though the\nmodel presented in this paper is trained on English-language tweets, the method\npresented can be used to learn tweet embeddings for different languages.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 00:58:14 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Vosoughi", "Soroush", ""], ["Vijayaraghavan", "Prashanth", ""], ["Roy", "Deb", ""]]}, {"id": "1607.07602", "submitter": "Kumar Niraj", "authors": "Niraj Kumar and Premkumar Devanbu", "title": "OntoCat: Automatically categorizing knowledge in API Documentation", "comments": "To be submitted for journal publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most application development happens in the context of complex APIs;\nreference documentation for APIs has grown tremendously in variety, complexity,\nand volume, and can be difficult to navigate. There is a growing need to\ndevelop well-organized ways to access the knowledge latent in the\ndocumentation; several research efforts deal with the organization (ontology)\nof API-related knowledge. Extensive knowledge-engineering work, supported by a\nrigorous qualitative analysis, by Maalej & Robillard [3] has identified a\nuseful taxonomy of API knowledge. Based on this taxonomy, we introduce a domain\nindependent technique to extract the knowledge types from the given API\nreference documentation. Our system, OntoCat, introduces total nine different\nfeatures and their semantic and statistical combinations to classify the\ndifferent knowledge types. We tested OntoCat on python API reference\ndocumentation. Our experimental results show the effectiveness of the system\nand opens the scope of probably related research areas (i.e., user behavior,\ndocumentation quality, etc.).\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 09:19:46 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Kumar", "Niraj", ""], ["Devanbu", "Premkumar", ""]]}, {"id": "1607.07684", "submitter": "Vasilis Syrgkanis", "authors": "Tim Roughgarden, Vasilis Syrgkanis, Eva Tardos", "title": "The Price of Anarchy in Auctions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This survey outlines a general and modular theory for proving approximation\nguarantees for equilibria of auctions in complex settings. This theory\ncomplements traditional economic techniques, which generally focus on exact and\noptimal solutions and are accordingly limited to relatively stylized settings.\n  We highlight three user-friendly analytical tools: smoothness-type\ninequalities, which immediately yield approximation guarantees for many auction\nformats of interest in the special case of complete information and\ndeterministic strategies; extension theorems, which extend such guarantees to\nrandomized strategies, no-regret learning outcomes, and incomplete-information\nsettings; and composition theorems, which extend such guarantees from simpler\nto more complex auctions. Combining these tools yields tight worst-case\napproximation guarantees for the equilibria of many widely-used auction\nformats.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 13:23:20 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Roughgarden", "Tim", ""], ["Syrgkanis", "Vasilis", ""], ["Tardos", "Eva", ""]]}, {"id": "1607.07730", "submitter": "Seth Baum", "authors": "Anthony M. Barrett and Seth D. Baum", "title": "A Model of Pathways to Artificial Superintelligence Catastrophe for Risk\n  and Decision Analysis", "comments": null, "journal-ref": null, "doi": "10.1080/0952813X.2016.1186228", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An artificial superintelligence (ASI) is artificial intelligence that is\nsignificantly more intelligent than humans in all respects. While ASI does not\ncurrently exist, some scholars propose that it could be created sometime in the\nfuture, and furthermore that its creation could cause a severe global\ncatastrophe, possibly even resulting in human extinction. Given the high\nstakes, it is important to analyze ASI risk and factor the risk into decisions\nrelated to ASI research and development. This paper presents a graphical model\nof major pathways to ASI catastrophe, focusing on ASI created via recursive\nself-improvement. The model uses the established risk and decision analysis\nmodeling paradigms of fault trees and influence diagrams in order to depict\ncombinations of events and conditions that could lead to AI catastrophe, as\nwell as intervention options that could decrease risks. The events and\nconditions include select aspects of the ASI itself as well as the human\nprocess of ASI research, development, and management. Model structure is\nderived from published literature on ASI risk. The model offers a foundation\nfor rigorous quantitative evaluation and decision making on the long-term risk\nof ASI catastrophe.\n", "versions": [{"version": "v1", "created": "Mon, 25 Jul 2016 13:04:22 GMT"}], "update_date": "2016-07-27", "authors_parsed": [["Barrett", "Anthony M.", ""], ["Baum", "Seth D.", ""]]}, {"id": "1607.07745", "submitter": "Arin Chaudhuri", "authors": "Deovrat Kakde, Arin Chaudhuri", "title": "Leveraging Unstructured Data to Detect Emerging Reliability Issues", "comments": null, "journal-ref": null, "doi": "10.1109/RAMS.2015.7105093", "report-no": null, "categories": "cs.AI stat.AP stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unstructured data refers to information that does not have a predefined data\nmodel or is not organized in a pre-defined manner. Loosely speaking,\nunstructured data refers to text data that is generated by humans. In\nafter-sales service businesses, there are two main sources of unstructured\ndata: customer complaints, which generally describe symptoms, and technician\ncomments, which outline diagnostics and treatment information. A legitimate\ncustomer complaint can eventually be tracked to a failure or a claim. However,\nthere is a delay between the time of a customer complaint and the time of a\nfailure or a claim. A proactive strategy aimed at analyzing customer complaints\nfor symptoms can help service providers detect reliability problems in advance\nand initiate corrective actions such as recalls. This paper introduces\nessential text mining concepts in the context of reliability analysis and a\nmethod to detect emerging reliability issues. The application of the method is\nillustrated using a case study.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 15:19:12 GMT"}], "update_date": "2016-08-08", "authors_parsed": [["Kakde", "Deovrat", ""], ["Chaudhuri", "Arin", ""]]}, {"id": "1607.07762", "submitter": "Zi Wang", "authors": "Zi Wang, Stefanie Jegelka, Leslie Pack Kaelbling, Tom\\'as\n  Lozano-P\\'erez", "title": "Focused Model-Learning and Planning for Non-Gaussian Continuous\n  State-Action Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.RO stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a framework for model learning and planning in stochastic\ndomains with continuous state and action spaces and non-Gaussian transition\nmodels. It is efficient because (1) local models are estimated only when the\nplanner requires them; (2) the planner focuses on the most relevant states to\nthe current planning problem; and (3) the planner focuses on the most\ninformative and/or high-value actions. Our theoretical analysis shows the\nvalidity and asymptotic optimality of the proposed approach. Empirically, we\ndemonstrate the effectiveness of our algorithm on a simulated multi-modal\npushing problem.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 15:48:03 GMT"}, {"version": "v2", "created": "Thu, 22 Sep 2016 18:08:50 GMT"}, {"version": "v3", "created": "Sun, 2 Oct 2016 05:21:17 GMT"}, {"version": "v4", "created": "Sun, 23 Oct 2016 04:05:34 GMT"}], "update_date": "2018-08-06", "authors_parsed": [["Wang", "Zi", ""], ["Jegelka", "Stefanie", ""], ["Kaelbling", "Leslie Pack", ""], ["Lozano-P\u00e9rez", "Tom\u00e1s", ""]]}, {"id": "1607.07847", "submitter": "Peter Sch\\\"uller", "authors": "Gokhan Avci, Mustafa Mehuljic, Peter Sch\\\"uller", "title": "Technical Report: Giving Hints for Logic Programming Examples without\n  Revealing Solutions", "comments": "7 pages. This is an extended English version of \"Gokhan Avci, Mustafa\n  Mehuljic, and Peter Schuller. Cozumu Aciga Cikarmadan Mantiksal Programlama\n  Orneklerine Ipucu Verme, Sinyal Isleme ve Iletisim Uygulamalari Kurultayi\n  (SIU), pages 513-516, 2016, DOI: 10.1109/SIU.2016.7495790\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  We introduce a framework for supporting learning to program in the paradigm\nof Answer Set Programming (ASP), which is a declarative logic programming\nformalism. Based on the idea of teaching by asking the student to complete\nsmall example ASP programs, we introduce a three-stage method for giving hints\nto the student without revealing the correct solution of an example. We\ncategorize mistakes into (i) syntactic mistakes, (ii) unexpected but\nsyntactically correct input, and (iii) semantic mistakes, describe mathematical\ndefinitions of these mistakes, and show how to compute hints from these\ndefinitions.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 19:17:11 GMT"}], "update_date": "2016-08-12", "authors_parsed": [["Avci", "Gokhan", ""], ["Mehuljic", "Mustafa", ""], ["Sch\u00fcller", "Peter", ""]]}, {"id": "1607.07896", "submitter": "David Miculescu", "authors": "David Miculescu, Sertac Karaman", "title": "Polling-systems-based Autonomous Vehicle Coordination in Traffic\n  Intersections with No Traffic Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The rapid development of autonomous vehicles spurred a careful investigation\nof the potential benefits of all-autonomous transportation networks. Most\nstudies conclude that autonomous systems can enable drastic improvements in\nperformance. A widely studied concept is all-autonomous, collision-free\nintersections, where vehicles arriving in a traffic intersection with no\ntraffic light adjust their speeds to cross safely through the intersection as\nquickly as possible. In this paper, we propose a coordination control algorithm\nfor this problem, assuming stochastic models for the arrival times of the\nvehicles. The proposed algorithm provides provable guarantees on safety and\nperformance. More precisely, it is shown that no collisions occur surely, and\nmoreover a rigorous upper bound is provided for the expected wait time. The\nalgorithm is also demonstrated in simulations. The proposed algorithms are\ninspired by polling systems. In fact, the problem studied in this paper leads\nto a new polling system where customers are subject to differential\nconstraints, which may be interesting in its own right.\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 20:43:02 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Miculescu", "David", ""], ["Karaman", "Sertac", ""]]}, {"id": "1607.07906", "submitter": "Krzysztof Sornat", "authors": "Marek Cygan, {\\L}ukasz Kowalik, Arkadiusz Soca{\\l}a, Krzysztof Sornat", "title": "Approximation and Parameterized Complexity of Minimax Approval Voting", "comments": "14 pages, 3 figures, 2 pseudocodes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.GT cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present three results on the complexity of Minimax Approval Voting. First,\nwe study Minimax Approval Voting parameterized by the Hamming distance $d$ from\nthe solution to the votes. We show Minimax Approval Voting admits no algorithm\nrunning in time $\\mathcal{O}^\\star(2^{o(d\\log d)})$, unless the Exponential\nTime Hypothesis (ETH) fails. This means that the $\\mathcal{O}^\\star(d^{2d})$\nalgorithm of Misra et al. [AAMAS 2015] is essentially optimal. Motivated by\nthis, we then show a parameterized approximation scheme, running in time\n$\\mathcal{O}^\\star(\\left({3}/{\\epsilon}\\right)^{2d})$, which is essentially\ntight assuming ETH. Finally, we get a new polynomial-time randomized\napproximation scheme for Minimax Approval Voting, which runs in time\n$n^{\\mathcal{O}(1/\\epsilon^2 \\cdot \\log(1/\\epsilon))} \\cdot \\mathrm{poly}(m)$,\nalmost matching the running time of the fastest known PTAS for Closest String\ndue to Ma and Sun [SIAM J. Comp. 2009].\n", "versions": [{"version": "v1", "created": "Tue, 26 Jul 2016 22:06:51 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Cygan", "Marek", ""], ["Kowalik", "\u0141ukasz", ""], ["Soca\u0142a", "Arkadiusz", ""], ["Sornat", "Krzysztof", ""]]}, {"id": "1607.07942", "submitter": "Jason Williams", "authors": "Jason L. Williams and Roslyn A. Lau", "title": "Multiple scan data association by convex variational inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IT cs.SY math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data association, the reasoning over correspondence between targets and\nmeasurements, is a problem of fundamental importance in target tracking.\nRecently, belief propagation (BP) has emerged as a promising method for\nestimating the marginal probabilities of measurement to target association,\nproviding fast, accurate estimates. The excellent performance of BP in the\nparticular formulation used may be attributed to the convexity of the\nunderlying free energy which it implicitly optimises. This paper studies\nmultiple scan data association problems, i.e., problems that reason over\ncorrespondence between targets and several sets of measurements, which may\ncorrespond to different sensors or different time steps. We find that the\nmultiple scan extension of the single scan BP formulation is non-convex and\ndemonstrate the undesirable behaviour that can result. A convex free energy is\nconstructed using the recently proposed fractional free energy (FFE). A\nconvergent, BP-like algorithm is provided for the single scan FFE, and employed\nin optimising the multiple scan free energy using primal-dual coordinate\nascent. Finally, based on a variational interpretation of joint probabilistic\ndata association (JPDA), we develop a sequential variant of the algorithm that\nis similar to JPDA, but retains consistency constraints from prior scans. The\nperformance of the proposed methods is demonstrated on a bearings only target\nlocalisation problem.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 02:53:24 GMT"}, {"version": "v2", "created": "Tue, 23 Jan 2018 22:57:39 GMT"}], "update_date": "2018-01-25", "authors_parsed": [["Williams", "Jason L.", ""], ["Lau", "Roslyn A.", ""]]}, {"id": "1607.07956", "submitter": "Yuezhang Li", "authors": "Yuezhang Li, Ronghuo Zheng, Tian Tian, Zhiting Hu, Rahul Iyer, Katia\n  Sycara", "title": "Joint Embedding of Hierarchical Categories and Entities for Concept\n  Categorization and Dataless Classification", "comments": "10 pages, submitted to Coling 2016. arXiv admin note: substantial\n  text overlap with arXiv:1605.03924", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Due to the lack of structured knowledge applied in learning distributed\nrepresentation of cate- gories, existing work cannot incorporate category\nhierarchies into entity information. We propose a framework that embeds\nentities and categories into a semantic space by integrating structured\nknowledge and taxonomy hierarchy from large knowledge bases. The framework\nallows to com- pute meaningful semantic relatedness between entities and\ncategories. Our framework can han- dle both single-word concepts and\nmultiple-word concepts with superior performance on concept categorization and\nyield state of the art results on dataless hierarchical classification.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 04:51:17 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Li", "Yuezhang", ""], ["Zheng", "Ronghuo", ""], ["Tian", "Tian", ""], ["Hu", "Zhiting", ""], ["Iyer", "Rahul", ""], ["Sycara", "Katia", ""]]}, {"id": "1607.08038", "submitter": "Konstantin Yakovlev S", "authors": "Aleksandr I. Panov, Konstantin Yakovlev", "title": "Behavior and path planning for the coalition of cognitive robots in\n  smart relocation tasks", "comments": "As submitted to the 4th International Conference on Robot\n  Intelligence Technology and Applications (RiTA-2015), Bucheon, Korea,\n  December 14-16, 2015", "journal-ref": null, "doi": "10.1007/978-3-319-31293-4_1", "report-no": null, "categories": "cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we outline the approach of solving special type of navigation\ntasks for robotic systems, when a coalition of robots (agents) acts in the 2D\nenvironment, which can be modified by the actions, and share the same goal\nlocation. The latter is originally unreachable for some members of the\ncoalition, but the common task still can be accomplished as the agents can\nassist each other (e.g. by modifying the environment). We call such tasks smart\nrelocation tasks (as the can not be solved by pure path planning methods) and\nstudy spatial and behavior interaction of robots while solving them. We use\ncognitive approach and introduce semiotic knowledge representation - sign world\nmodel which underlines behavioral planning methodology. Planning is viewed as a\nrecursive search process in the hierarchical state-space induced by sings with\npath planning signs reside on the lowest level. Reaching this level triggers\npath planning which is accomplished by state of the art grid-based planners\nfocused on producing smooth paths (e.g. LIAN) and thus indirectly guarantying\nfeasibility of that paths against agent's dynamic constraints.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 11:12:02 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Panov", "Aleksandr I.", ""], ["Yakovlev", "Konstantin", ""]]}, {"id": "1607.08073", "submitter": "Adrian Groza", "authors": "Adrian Groza, Calin Cara, Sergiu Zaporojan, Igor Calmicov", "title": "Assisting Drivers During Overtaking Using Car-2-Car Communication and\n  Multi-Agent Systems", "comments": "preprint ICCP 2016, Cluj-Napoca", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A warning system for assisting drivers during overtaking maneuvers is\nproposed. The system relies on Car-2-Car communication technologies and\nmulti-agent systems. A protocol for safety overtaking is proposed based on ACL\ncommunicative acts. The mathematical model for safety overtaking used Kalman\nfilter to minimize localization error.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 13:08:13 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Groza", "Adrian", ""], ["Cara", "Calin", ""], ["Zaporojan", "Sergiu", ""], ["Calmicov", "Igor", ""]]}, {"id": "1607.08074", "submitter": "Adrian Groza", "authors": "Adrian Groza, Oana Popa", "title": "Mining Arguments from Cancer Documents Using Natural Language Processing\n  and Ontologies", "comments": "ICCP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the medical domain, the continuous stream of scientific research contains\ncontradictory results supported by arguments and counter-arguments. As medical\nexpertise occurs at different levels, part of the human agents have\ndifficulties to face the huge amount of studies, but also to understand the\nreasons and pieces of evidences claimed by the proponents and the opponents of\nthe debated topic. To better understand the supporting arguments for new\nfindings related to current state of the art in the medical domain we need\ntools able to identify arguments in scientific papers. Our work here aims to\nfill the above technological gap.\n  Quite aware of the difficulty of this task, we embark to this road by relying\non the well-known interleaving of domain knowledge with natural language\nprocessing. To formalise the existing medical knowledge, we rely on ontologies.\nTo structure the argumentation model we use also the expressivity and reasoning\ncapabilities of Description Logics. To perform argumentation mining we\nformalise various linguistic patterns in a rule-based language. We tested our\nsolution against a corpus of scientific papers related to breast cancer. The\nrun experiments show a F-measure between 0.71 and 0.86 for identifying\nconclusions of an argument and between 0.65 and 0.86 for identifying premises\nof an argument.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 13:08:41 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Groza", "Adrian", ""], ["Popa", "Oana", ""]]}, {"id": "1607.08075", "submitter": "Adrian Groza", "authors": "Adrian Groza, Madalina Mand Nagy", "title": "Harmonization of conflicting medical opinions using argumentation\n  protocols and textual entailment - a case study on Parkinson disease", "comments": "ICCP 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parkinson's disease is the second most common neurodegenerative disease,\naffecting more than 1.2 million people in Europe. Medications are available for\nthe management of its symptoms, but the exact cause of the disease is unknown\nand there is currently no cure on the market. To better understand the\nrelations between new findings and current medical knowledge, we need tools\nable to analyse published medical papers based on natural language processing\nand tools capable to identify various relationships of new findings with the\ncurrent medical knowledge. Our work aims to fill the above technological gap.\n  To identify conflicting information in medical documents, we enact textual\nentailment technology. To encapsulate existing medical knowledge, we rely on\nontologies. To connect the formal axioms in ontologies with natural text in\nmedical articles, we exploit ontology verbalisation techniques. To assess the\nlevel of disagreement between human agents with respect to a medical issue, we\nrely on fuzzy aggregation. To harmonize this disagreement, we design mediation\nprotocols within a multi-agent framework.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 13:13:41 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Groza", "Adrian", ""], ["Nagy", "Madalina Mand", ""]]}, {"id": "1607.08085", "submitter": "Maxime Bucher", "authors": "Maxime Bucher (Palaiseau), St\\'ephane Herbin (Palaiseau), Fr\\'ed\\'eric\n  Jurie", "title": "Improving Semantic Embedding Consistency by Metric Learning for\n  Zero-Shot Classification", "comments": "in ECCV 2016, Oct 2016, amsterdam, Netherlands. 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the task of zero-shot image classification. The key\ncontribution of the proposed approach is to control the semantic embedding of\nimages -- one of the main ingredients of zero-shot learning -- by formulating\nit as a metric learning problem. The optimized empirical criterion associates\ntwo types of sub-task constraints: metric discriminating capacity and accurate\nattribute prediction. This results in a novel expression of zero-shot learning\nnot requiring the notion of class in the training phase: only pairs of\nimage/attributes, augmented with a consistency indicator, are given as ground\ntruth. At test time, the learned model can predict the consistency of a test\nimage with a given set of attributes , allowing flexible ways to produce\nrecognition inferences. Despite its simplicity, the proposed approach gives\nstate-of-the-art results on four challenging datasets used for zero-shot\nrecognition evaluation.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 13:35:16 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Bucher", "Maxime", "", "Palaiseau"], ["Herbin", "St\u00e9phane", "", "Palaiseau"], ["Jurie", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1607.08098", "submitter": "Carlos Leandro", "authors": "Carlos Leandro and Helder Pita and Lu\\'is Monteiro", "title": "The Actias system: supervised multi-strategy learning paradigm using\n  categorical logic", "comments": "9 pages, 6 figures, conference ICKEDS'04", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most difficult problems in the development of intelligent systems\nis the construction of the underlying knowledge base. As a consequence, the\nrate of progress in the development of this type of system is directly related\nto the speed with which knowledge bases can be assembled, and on its quality.\nWe attempt to solve the knowledge acquisition problem, for a Business\nInformation System, developing a supervised multistrategy learning paradigm.\nThis paradigm is centred on a collaborative data mining strategy, where groups\nof experts collaborate using data-mining process on the supervised acquisition\nof new knowledge extracted from heterogeneous machine learning data models.\n  The Actias system is our approach to this paradigm. It is the result of\napplying the graphic logic based language of sketches to knowledge integration.\nThe system is a data mining collaborative workplace, where the Information\nSystem knowledge base is an algebraic structure. It results from the\nintegration of background knowledge with new insights extracted from data\nmodels, generated for specific data modelling tasks, and represented as rules\nusing the sketches language.\n", "versions": [{"version": "v1", "created": "Fri, 6 May 2016 20:04:40 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Leandro", "Carlos", ""], ["Pita", "Helder", ""], ["Monteiro", "Lu\u00eds", ""]]}, {"id": "1607.08100", "submitter": "Jialin Liu Ph.D", "authors": "David L. St-Pierre, Jean-Baptiste Hoock, Jialin Liu, Fabien Teytaud\n  and Olivier Teytaud", "title": "Automatically Reinforcing a Game AI", "comments": "17 pages, 31 figures, 2 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A recent research trend in Artificial Intelligence (AI) is the combination of\nseveral programs into one single, stronger, program; this is termed portfolio\nmethods. We here investigate the application of such methods to Game Playing\nPrograms (GPPs). In addition, we consider the case in which only one GPP is\navailable - by decomposing this single GPP into several ones through the use of\nparameters or even simply random seeds. These portfolio methods are trained in\na learning phase. We propose two different offline approaches. The simplest\none, BestArm, is a straightforward optimization of seeds or parame- ters; it\nperforms quite well against the original GPP, but performs poorly against an\nopponent which repeats games and learns. The second one, namely Nash-portfolio,\nperforms similarly in a \"one game\" test, and is much more robust against an\nopponent who learns. We also propose an online learning portfolio, which tests\nseveral of the GPP repeatedly and progressively switches to the best one -\nusing a bandit algorithm.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 14:10:28 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["St-Pierre", "David L.", ""], ["Hoock", "Jean-Baptiste", ""], ["Liu", "Jialin", ""], ["Teytaud", "Fabien", ""], ["Teytaud", "Olivier", ""]]}, {"id": "1607.08116", "submitter": "Xinyang Deng", "authors": "Xinyi Zhou and Yong Hu and Yong Deng and Felix T.S. Chan and Alessio\n  Ishizak", "title": "A DEMATEL-Based Completion Method for Incomplete Pairwise Comparison\n  Matrix in AHP", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Pairwise comparison matrix as a crucial component of AHP, presents the\nprefer- ence relations among alternatives. However, in many cases, the pairwise\ncomparison matrix is difficult to complete, which obstructs the subsequent\noperations of the clas- sical AHP. In this paper, based on DEMATEL which has\nability to derive the total relation matrix from direct relation matrix, a new\ncompletion method for incomplete pairwise comparison matrix is proposed. The\nproposed method provides a new per- spective to estimate the missing values\nwith explicit physical meaning. Besides, the proposed method has low\ncomputational cost. This promising method has a wide application in\nmulti-criteria decision-making.\n", "versions": [{"version": "v1", "created": "Sat, 23 Jul 2016 08:43:12 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Zhou", "Xinyi", ""], ["Hu", "Yong", ""], ["Deng", "Yong", ""], ["Chan", "Felix T. S.", ""], ["Ishizak", "Alessio", ""]]}, {"id": "1607.08131", "submitter": "Larisa Safina", "authors": "Alexander Tchitchigin, Max Talanov, Larisa Safina, Manuel Mazzara", "title": "Neuromorphic Robot Dream", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present the next step in our approach to neurobiologically\nplausible implementation of emotional reactions and behaviors for real-time\nautonomous robotic systems. The working metaphor we use is the \"day\" and the\n\"night\" phases of mammalian life. During the \"day phase\" a robotic system\nstores the inbound information and is controlled by a light-weight rule-based\nsystem in real time. In contrast to that, during the \"night phase\" information\nthat has been stored is transferred to a supercomputing system to update the\nrealistic neural network: emotional and behavioral strategies.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 14:54:47 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Tchitchigin", "Alexander", ""], ["Talanov", "Max", ""], ["Safina", "Larisa", ""], ["Mazzara", "Manuel", ""]]}, {"id": "1607.08149", "submitter": "Suleiman Yerima", "authors": "BooJoong Kang, Suleiman Y. Yerima, Kieran McLaughlin, Sakir Sezer", "title": "N-opcode Analysis for Android Malware Classification and Categorization", "comments": "7 pages, 8 figures, conference", "journal-ref": null, "doi": "10.1109/CyberSecPODS.2016.7502343", "report-no": null, "categories": "cs.CR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Malware detection is a growing problem particularly on the Android mobile\nplatform due to its increasing popularity and accessibility to numerous third\nparty app markets. This has also been made worse by the increasingly\nsophisticated detection avoidance techniques employed by emerging malware\nfamilies. This calls for more effective techniques for detection and\nclassification of Android malware. Hence, in this paper we present an n-opcode\nanalysis based approach that utilizes machine learning to classify and\ncategorize Android malware. This approach enables automated feature discovery\nthat eliminates the need for applying expert or domain knowledge to define the\nneeded features. Our experiments on 2520 samples that were performed using up\nto 10-gram opcode features showed that an f-measure of 98% is achievable using\nthis approach.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 15:32:18 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Kang", "BooJoong", ""], ["Yerima", "Suleiman Y.", ""], ["McLaughlin", "Kieran", ""], ["Sezer", "Sakir", ""]]}, {"id": "1607.08181", "submitter": "Konstantin Yakovlev S", "authors": "Aleksandr I. Panov, Konstantin Yakovlev", "title": "Psychologically inspired planning method for smart relocation task", "comments": "As submitted to the 7th International Conference on Biologically\n  Inspired Cognitive Architectures (BICA 2016), New-York, USA, July 16-19 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Behavior planning is known to be one of the basic cognitive functions, which\nis essential for any cognitive architecture of any control system used in\nrobotics. At the same time most of the widespread planning algorithms employed\nin those systems are developed using only approaches and models of Artificial\nIntelligence and don't take into account numerous results of cognitive\nexperiments. As a result, there is a strong need for novel methods of behavior\nplanning suitable for modern cognitive architectures aimed at robot control.\nOne such method is presented in this work and is studied within a special class\nof navigation task called smart relocation task. The method is based on the\nhierarchical two-level model of abstraction and knowledge representation, e.g.\nsymbolic and subsymbolic. On the symbolic level sign world model is used for\nknowledge representation and hierarchical planning algorithm, PMA, is utilized\nfor planning. On the subsymbolic level the task of path planning is considered\nand solved as a graph search problem. Interaction between both planners is\nexamined and inter-level interfaces and feedback loops are described.\nPreliminary experimental results are presented.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 17:08:05 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Panov", "Aleksandr I.", ""], ["Yakovlev", "Konstantin", ""]]}, {"id": "1607.08186", "submitter": "Suleiman Yerima", "authors": "Suleiman Y. Yerima, Sakir Sezer, Igor Muttik", "title": "Android Malware Detection Using Parallel Machine Learning Classifiers", "comments": "8th International Conference on Next Generation Mobile Applications,\n  Services and Technologies, (NGMAST), 10-14 Sept., 2014, Oxford, United\n  Kingdom", "journal-ref": null, "doi": "10.1109/NGMAST.2014.23", "report-no": null, "categories": "cs.CR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mobile malware has continued to grow at an alarming rate despite on-going\nefforts towards mitigating the problem. This has been particularly noticeable\non Android due to its being an open platform that has subsequently overtaken\nother platforms in the share of the mobile smart devices market. Hence,\nincentivizing a new wave of emerging Android malware sophisticated enough to\nevade most common detection methods. This paper proposes and investigates a\nparallel machine learning based classification approach for early detection of\nAndroid malware. Using real malware samples and benign applications, a\ncomposite classification model is developed from parallel combination of\nheterogeneous classifiers. The empirical evaluation of the model under\ndifferent combination schemes demonstrates its efficacy and potential to\nimprove detection accuracy. More importantly, by utilizing several classifiers\nwith diverse characteristics, their strengths can be harnessed not only for\nenhanced Android malware detection but also quicker white box analysis by means\nof the more interpretable constituent classifiers.\n", "versions": [{"version": "v1", "created": "Wed, 27 Jul 2016 17:22:00 GMT"}], "update_date": "2016-07-28", "authors_parsed": [["Yerima", "Suleiman Y.", ""], ["Sezer", "Sakir", ""], ["Muttik", "Igor", ""]]}, {"id": "1607.08289", "submitter": "Gopal P. Sarma", "authors": "Gopal P. Sarma and Nick J. Hay", "title": "Mammalian Value Systems", "comments": "12 pages", "journal-ref": "Informatica Vol. 41 No. 3 (2017)", "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.HC cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Characterizing human values is a topic deeply interwoven with the sciences,\nhumanities, art, and many other human endeavors. In recent years, a number of\nthinkers have argued that accelerating trends in computer science, cognitive\nscience, and related disciplines foreshadow the creation of intelligent\nmachines which meet and ultimately surpass the cognitive abilities of human\nbeings, thereby entangling an understanding of human values with future\ntechnological development. Contemporary research accomplishments suggest\nsophisticated AI systems becoming widespread and responsible for managing many\naspects of the modern world, from preemptively planning users' travel schedules\nand logistics, to fully autonomous vehicles, to domestic robots assisting in\ndaily living. The extrapolation of these trends has been most forcefully\ndescribed in the context of a hypothetical \"intelligence explosion,\" in which\nthe capabilities of an intelligent software agent would rapidly increase due to\nthe presence of feedback loops unavailable to biological organisms. The\npossibility of superintelligent agents, or simply the widespread deployment of\nsophisticated, autonomous AI systems, highlights an important theoretical\nproblem: the need to separate the cognitive and rational capacities of an agent\nfrom the fundamental goal structure, or value system, which constrains and\nguides the agent's actions. The \"value alignment problem\" is to specify a goal\nstructure for autonomous agents compatible with human values. In this brief\narticle, we suggest that recent ideas from affective neuroscience and related\ndisciplines aimed at characterizing neurological and behavioral universals in\nthe mammalian class provide important conceptual foundations relevant to\ndescribing human values. We argue that the notion of \"mammalian value systems\"\npoints to a potential avenue for fundamental research in AI safety and AI\nethics.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 01:22:26 GMT"}, {"version": "v2", "created": "Fri, 26 May 2017 18:17:46 GMT"}, {"version": "v3", "created": "Sun, 31 Dec 2017 18:47:10 GMT"}, {"version": "v4", "created": "Mon, 21 Jan 2019 19:29:30 GMT"}], "update_date": "2019-01-23", "authors_parsed": [["Sarma", "Gopal P.", ""], ["Hay", "Nick J.", ""]]}, {"id": "1607.08316", "submitter": "Ilija Ilievski", "authors": "Ilija Ilievski and Taimoor Akhtar and Jiashi Feng and Christine\n  Annette Shoemaker", "title": "Efficient Hyperparameter Optimization of Deep Learning Algorithms Using\n  Deterministic RBF Surrogates", "comments": "AAAI-17 Camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically searching for optimal hyperparameter configurations is of\ncrucial importance for applying deep learning algorithms in practice. Recently,\nBayesian optimization has been proposed for optimizing hyperparameters of\nvarious machine learning algorithms. Those methods adopt probabilistic\nsurrogate models like Gaussian processes to approximate and minimize the\nvalidation error function of hyperparameter values. However, probabilistic\nsurrogates require accurate estimates of sufficient statistics (e.g.,\ncovariance) of the error distribution and thus need many function evaluations\nwith a sizeable number of hyperparameters. This makes them inefficient for\noptimizing hyperparameters of deep learning algorithms, which are highly\nexpensive to evaluate. In this work, we propose a new deterministic and\nefficient hyperparameter optimization method that employs radial basis\nfunctions as error surrogates. The proposed mixed integer algorithm, called\nHORD, searches the surrogate for the most promising hyperparameter values\nthrough dynamic coordinate search and requires many fewer function evaluations.\nHORD does well in low dimensions but it is exceptionally better in higher\ndimensions. Extensive evaluations on MNIST and CIFAR-10 for four deep neural\nnetworks demonstrate HORD significantly outperforms the well-established\nBayesian optimization methods such as GP, SMAC, and TPE. For instance, on\naverage, HORD is more than 6 times faster than GP-EI in obtaining the best\nconfiguration of 19 hyperparameters.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 05:03:32 GMT"}, {"version": "v2", "created": "Sat, 21 Jan 2017 03:26:06 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Ilievski", "Ilija", ""], ["Akhtar", "Taimoor", ""], ["Feng", "Jiashi", ""], ["Shoemaker", "Christine Annette", ""]]}, {"id": "1607.08325", "submitter": "Gianmarco De Francisci Morales", "authors": "Nicolas Kourtellis and Gianmarco De Francisci Morales and Albert Bifet\n  and Arinto Murdopo", "title": "VHT: Vertical Hoeffding Tree", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  IoT Big Data requires new machine learning methods able to scale to large\nsize of data arriving at high speed. Decision trees are popular machine\nlearning models since they are very effective, yet easy to interpret and\nvisualize. In the literature, we can find distributed algorithms for learning\ndecision trees, and also streaming algorithms, but not algorithms that combine\nboth features. In this paper we present the Vertical Hoeffding Tree (VHT), the\nfirst distributed streaming algorithm for learning decision trees. It features\na novel way of distributing decision trees via vertical parallelism. The\nalgorithm is implemented on top of Apache SAMOA, a platform for mining\ndistributed data streams, and thus able to run on real-world clusters. We run\nseveral experiments to study the accuracy and throughput performance of our new\nVHT algorithm, as well as its ability to scale while keeping its superior\nperformance with respect to non-distributed decision trees.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 06:15:24 GMT"}], "update_date": "2016-07-29", "authors_parsed": [["Kourtellis", "Nicolas", ""], ["Morales", "Gianmarco De Francisci", ""], ["Bifet", "Albert", ""], ["Murdopo", "Arinto", ""]]}, {"id": "1607.08329", "submitter": "Jiongqian Liang", "authors": "Jiongqian Liang and Srinivasan Parthasarathy", "title": "Robust Contextual Outlier Detection: Where Context Meets Sparsity", "comments": "11 pages. Extended version of CIKM'16 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outlier detection is a fundamental data science task with applications\nranging from data cleaning to network security. Given the fundamental nature of\nthe task, this has been the subject of much research. Recently, a new class of\noutlier detection algorithms has emerged, called {\\it contextual outlier\ndetection}, and has shown improved performance when studying anomalous behavior\nin a specific context. However, as we point out in this article, such\napproaches have limited applicability in situations where the context is sparse\n(i.e. lacking a suitable frame of reference). Moreover, approaches developed to\ndate do not scale to large datasets. To address these problems, here we propose\na novel and robust approach alternative to the state-of-the-art called RObust\nContextual Outlier Detection (ROCOD). We utilize a local and global behavioral\nmodel based on the relevant contexts, which is then integrated in a natural and\nrobust fashion. We also present several optimizations to improve the\nscalability of the approach. We run ROCOD on both synthetic and real-world\ndatasets and demonstrate that it outperforms other competitive baselines on the\naxes of efficacy and efficiency (40X speedup compared to modern contextual\noutlier detection methods). We also drill down and perform a fine-grained\nanalysis to shed light on the rationale for the performance gains of ROCOD and\nreveal its effectiveness when handling objects with sparse contexts.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 06:40:30 GMT"}, {"version": "v2", "created": "Mon, 22 Aug 2016 03:47:51 GMT"}, {"version": "v3", "created": "Thu, 22 Dec 2016 21:52:12 GMT"}], "update_date": "2016-12-26", "authors_parsed": [["Liang", "Jiongqian", ""], ["Parthasarathy", "Srinivasan", ""]]}, {"id": "1607.08438", "submitter": "Seong Joon Oh", "authors": "Seong Joon Oh, Rodrigo Benenson, Mario Fritz, Bernt Schiele", "title": "Faceless Person Recognition; Privacy Implications in Social Media", "comments": "Accepted to ECCV'16", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As we shift more of our lives into the virtual domain, the volume of data\nshared on the web keeps increasing and presents a threat to our privacy. This\nworks contributes to the understanding of privacy implications of such data\nsharing by analysing how well people are recognisable in social media data. To\nfacilitate a systematic study we define a number of scenarios considering\nfactors such as how many heads of a person are tagged and if those heads are\nobfuscated or not. We propose a robust person recognition system that can\nhandle large variations in pose and clothing, and can be trained with few\ntraining samples. Our results indicate that a handful of images is enough to\nthreaten users' privacy, even in the presence of obfuscation. We show detailed\nexperimental results, and discuss their implications.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 13:10:27 GMT"}], "update_date": "2016-07-29", "authors_parsed": [["Oh", "Seong Joon", ""], ["Benenson", "Rodrigo", ""], ["Fritz", "Mario", ""], ["Schiele", "Bernt", ""]]}, {"id": "1607.08485", "submitter": "Manuele Leonelli", "authors": "Manuele Leonelli, Eva Riccomagno, Jim Q. Smith", "title": "A symbolic algebra for the computation of expected utilities in\n  multiplicative influence diagrams", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Influence diagrams provide a compact graphical representation of decision\nproblems. Several algorithms for the quick computation of their associated\nexpected utilities are available in the literature. However, often they rely on\na full quantification of both probabilistic uncertainties and utility values.\nFor problems where all random variables and decision spaces are finite and\ndiscrete, here we develop a symbolic way to calculate the expected utilities of\ninfluence diagrams that does not require a full numerical representation.\nWithin this approach expected utilities correspond to families of polynomials.\nAfter characterizing their polynomial structure, we develop an efficient\nsymbolic algorithm for the propagation of expected utilities through the\ndiagram and provide an implementation of this algorithm using a computer\nalgebra system. We then characterize many of the standard manipulations of\ninfluence diagrams as transformations of polynomials. We also generalize the\ndecision analytic framework of these diagrams by defining asymmetries as\noperations over the expected utility polynomials.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 14:47:52 GMT"}, {"version": "v2", "created": "Wed, 18 Jan 2017 09:54:13 GMT"}], "update_date": "2017-01-19", "authors_parsed": [["Leonelli", "Manuele", ""], ["Riccomagno", "Eva", ""], ["Smith", "Jim Q.", ""]]}, {"id": "1607.08580", "submitter": "Elham Shaabani", "authors": "Elham Shaabani, Hamidreza Alvari, Paulo Shakarian, J.E. Kelly Snyder", "title": "MIST: Missing Person Intelligence Synthesis Toolkit", "comments": "10 pages, 12 figures, Accepted in CIKM 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Each day, approximately 500 missing persons cases occur that go\nunsolved/unresolved in the United States. The non-profit organization known as\nthe Find Me Group (FMG), led by former law enforcement professionals, is\ndedicated to solving or resolving these cases. This paper introduces the\nMissing Person Intelligence Synthesis Toolkit (MIST) which leverages a\ndata-driven variant of geospatial abductive inference. This system takes search\nlocations provided by a group of experts and rank-orders them based on the\nprobability assigned to areas based on the prior performance of the experts\ntaken as a group. We evaluate our approach compared to the current practices\nemployed by the Find Me Group and found it significantly reduces the search\narea - leading to a reduction of 31 square miles over 24 cases we examined in\nour experiments. Currently, we are using MIST to aid the Find Me Group in an\nactive missing person case.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 19:13:20 GMT"}, {"version": "v2", "created": "Mon, 29 Aug 2016 18:51:21 GMT"}], "update_date": "2016-08-30", "authors_parsed": [["Shaabani", "Elham", ""], ["Alvari", "Hamidreza", ""], ["Shakarian", "Paulo", ""], ["Snyder", "J. E. Kelly", ""]]}, {"id": "1607.08583", "submitter": "Eric Nunes", "authors": "Eric Nunes, Ahmad Diab, Andrew Gunn, Ericsson Marin, Vineet Mishra,\n  Vivin Paliath, John Robertson, Jana Shakarian, Amanda Thart, Paulo Shakarian", "title": "Darknet and Deepnet Mining for Proactive Cybersecurity Threat\n  Intelligence", "comments": "6 page paper accepted to be presented at IEEE Intelligence and\n  Security Informatics 2016 Tucson, Arizona USA September 27-30, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present an operational system for cyber threat intelligence\ngathering from various social platforms on the Internet particularly sites on\nthe darknet and deepnet. We focus our attention to collecting information from\nhacker forum discussions and marketplaces offering products and services\nfocusing on malicious hacking. We have developed an operational system for\nobtaining information from these sites for the purposes of identifying emerging\ncyber threats. Currently, this system collects on average 305 high-quality\ncyber threat warnings each week. These threat warnings include information on\nnewly developed malware and exploits that have not yet been deployed in a\ncyber-attack. This provides a significant service to cyber-defenders. The\nsystem is significantly augmented through the use of various data mining and\nmachine learning techniques. With the use of machine learning models, we are\nable to recall 92% of products in marketplaces and 80% of discussions on forums\nrelating to malicious hacking with high precision. We perform preliminary\nanalysis on the data collected, demonstrating its application to aid a security\nexpert for better threat analysis.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 19:30:04 GMT"}], "update_date": "2016-08-02", "authors_parsed": [["Nunes", "Eric", ""], ["Diab", "Ahmad", ""], ["Gunn", "Andrew", ""], ["Marin", "Ericsson", ""], ["Mishra", "Vineet", ""], ["Paliath", "Vivin", ""], ["Robertson", "John", ""], ["Shakarian", "Jana", ""], ["Thart", "Amanda", ""], ["Shakarian", "Paulo", ""]]}, {"id": "1607.08592", "submitter": "Erkki Luuk", "authors": "Erkki Luuk", "title": "Modeling selectional restrictions in a relational type system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selectional restrictions are semantic constraints on forming certain complex\ntypes in natural language. The paper gives an overview of modeling selectional\nrestrictions in a relational type system with morphological and syntactic\ntypes. We discuss some foundations of the system and ways of formalizing\nselectional restrictions.\n  Keywords: type theory, selectional restrictions, syntax, morphology\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 19:47:25 GMT"}], "update_date": "2016-07-29", "authors_parsed": [["Luuk", "Erkki", ""]]}, {"id": "1607.08665", "submitter": "Shreyansh Daftry", "authors": "Shreyansh Daftry, Sam Zeng, J. Andrew Bagnell and Martial Hebert", "title": "Introspective Perception: Learning to Predict Failures in Vision Systems", "comments": "IEEE/RSJ International Conference on Intelligent Robots and Systems\n  (IROS 2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As robots aspire for long-term autonomous operations in complex dynamic\nenvironments, the ability to reliably take mission-critical decisions in\nambiguous situations becomes critical. This motivates the need to build systems\nthat have situational awareness to assess how qualified they are at that moment\nto make a decision. We call this self-evaluating capability as introspection.\nIn this paper, we take a small step in this direction and propose a generic\nframework for introspective behavior in perception systems. Our goal is to\nlearn a model to reliably predict failures in a given system, with respect to a\ntask, directly from input sensor data. We present this in the context of\nvision-based autonomous MAV flight in outdoor natural environments, and show\nthat it effectively handles uncertain situations.\n", "versions": [{"version": "v1", "created": "Thu, 28 Jul 2016 23:27:13 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Daftry", "Shreyansh", ""], ["Zeng", "Sam", ""], ["Bagnell", "J. Andrew", ""], ["Hebert", "Martial", ""]]}, {"id": "1607.08695", "submitter": "Kuang Zhou", "authors": "Kuang Zhou (NPU, DRUID), Arnaud Martin (DRUID), Quan Pan (NPU)", "title": "Semi-supervised evidential label propagation algorithm for graph data", "comments": "in The 4th International Conference on Belief Functions, Sep 2016,\n  Prague, Czech Republic", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the task of community detection, there often exists some useful prior\ninformation. In this paper, a Semi-supervised clustering approach using a new\nEvidential Label Propagation strategy (SELP) is proposed to incorporate the\ndomain knowledge into the community detection model. The main advantage of SELP\nis that it can take limited supervised knowledge to guide the detection\nprocess. The prior information of community labels is expressed in the form of\nmass functions initially. Then a new evidential label propagation rule is\nadopted to propagate the labels from labeled data to unlabeled ones. The\noutliers can be identified to be in a special class. The experimental results\ndemonstrate the effectiveness of SELP.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 06:35:14 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Zhou", "Kuang", "", "NPU, DRUID"], ["Martin", "Arnaud", "", "DRUID"], ["Pan", "Quan", "", "NPU"]]}, {"id": "1607.08723", "submitter": "Emmanuel Dupoux", "authors": "Emmanuel Dupoux", "title": "Cognitive Science in the era of Artificial Intelligence: A roadmap for\n  reverse-engineering the infant language-learner", "comments": "27 pages, 5 figures, 3 tables, supplementary materials", "journal-ref": "Dupoux, E. (2018). Cognitive science in the era of artificial\n  intelligence: A roadmap for reverse-engineering the infant language learner.\n  Cognition, 173, 43-59", "doi": "10.1016/j.cognition.2017.11.008", "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During their first years of life, infants learn the language(s) of their\nenvironment at an amazing speed despite large cross cultural variations in\namount and complexity of the available language input. Understanding this\nsimple fact still escapes current cognitive and linguistic theories. Recently,\nspectacular progress in the engineering science, notably, machine learning and\nwearable technology, offer the promise of revolutionizing the study of\ncognitive development. Machine learning offers powerful learning algorithms\nthat can achieve human-like performance on many linguistic tasks. Wearable\nsensors can capture vast amounts of data, which enable the reconstruction of\nthe sensory experience of infants in their natural environment. The project of\n'reverse engineering' language development, i.e., of building an effective\nsystem that mimics infant's achievements appears therefore to be within reach.\nHere, we analyze the conditions under which such a project can contribute to\nour scientific understanding of early language development. We argue that\ninstead of defining a sub-problem or simplifying the data, computational models\nshould address the full complexity of the learning situation, and take as input\nthe raw sensory signals available to infants. This implies that (1) accessible\nbut privacy-preserving repositories of home data be setup and widely shared,\nand (2) models be evaluated at different linguistic levels through a benchmark\nof psycholinguist tests that can be passed by machines and humans alike, (3)\nlinguistically and psychologically plausible learning architectures be scaled\nup to real data using probabilistic/optimization principles from machine\nlearning. We discuss the feasibility of this approach and present preliminary\nresults.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 08:33:10 GMT"}, {"version": "v2", "created": "Wed, 31 Aug 2016 08:07:08 GMT"}, {"version": "v3", "created": "Fri, 28 Oct 2016 12:59:59 GMT"}, {"version": "v4", "created": "Wed, 14 Feb 2018 15:56:51 GMT"}], "update_date": "2018-02-15", "authors_parsed": [["Dupoux", "Emmanuel", ""]]}, {"id": "1607.08864", "submitter": "Christoph Redl", "authors": "Christoph Redl", "title": "The DLVHEX System for Knowledge Representation: Recent Advances (System\n  Description)", "comments": "Paper presented at the 32nd International Conference on Logic\n  Programming (ICLP 2016), New York City, USA, 16-21 October 2016, 15 pages,\n  LaTeX, 3 PDF figures (arXiv:1607.08864)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The DLVHEX system implements the HEX-semantics, which integrates answer set\nprogramming (ASP) with arbitrary external sources. Since its first release ten\nyears ago, significant advancements were achieved. Most importantly, the\nexploitation of properties of external sources led to efficiency improvements\nand flexibility enhancements of the language, and technical improvements on the\nsystem side increased user's convenience. In this paper, we present the current\nstatus of the system and point out the most important recent enhancements over\nearly versions. While existing literature focuses on theoretical aspects and\nspecific components, a bird's eye view of the overall system is missing. In\norder to promote the system for real-world applications, we further present\napplications which were already successfully realized on top of DLVHEX. This\npaper is under consideration for acceptance in Theory and Practice of Logic\nProgramming.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 16:26:54 GMT"}, {"version": "v2", "created": "Tue, 2 Aug 2016 13:23:12 GMT"}], "update_date": "2016-08-04", "authors_parsed": [["Redl", "Christoph", ""]]}, {"id": "1607.08878", "submitter": "Randal Olson", "authors": "Randal S. Olson and Jason H. Moore", "title": "Identifying and Harnessing the Building Blocks of Machine Learning\n  Pipelines for Sensible Initialization of a Data Science Automation Tool", "comments": "13 pages, 5 figures, preprint of chapter to appear in GPTP 2016 book", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As data science continues to grow in popularity, there will be an increasing\nneed to make data science tools more scalable, flexible, and accessible. In\nparticular, automated machine learning (AutoML) systems seek to automate the\nprocess of designing and optimizing machine learning pipelines. In this\nchapter, we present a genetic programming-based AutoML system called TPOT that\noptimizes a series of feature preprocessors and machine learning models with\nthe goal of maximizing classification accuracy on a supervised classification\nproblem. Further, we analyze a large database of pipelines that were previously\nused to solve various supervised classification problems and identify 100 short\nseries of machine learning operations that appear the most frequently, which we\ncall the building blocks of machine learning pipelines. We harness these\nbuilding blocks to initialize TPOT with promising solutions, and find that this\nsensible initialization method significantly improves TPOT's performance on one\nbenchmark at no cost of significantly degrading performance on the others.\nThus, sensible initialization with machine learning pipeline building blocks\nshows promise for GP-based AutoML systems, and should be further refined in\nfuture work.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 18:06:39 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Olson", "Randal S.", ""], ["Moore", "Jason H.", ""]]}, {"id": "1607.08898", "submitter": "Tao Ding", "authors": "Tao Ding and Shimei Pan", "title": "Personalized Emphasis Framing for Persuasive Message Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a study on personalized emphasis framing which can\nbe used to tailor the content of a message to enhance its appeal to different\nindividuals. With this framework, we directly model content selection decisions\nbased on a set of psychologically-motivated domain-independent personal traits\nincluding personality (e.g., extraversion and conscientiousness) and basic\nhuman values (e.g., self-transcendence and hedonism). We also demonstrate how\nthe analysis results can be used in automated personalized content selection\nfor persuasive message generation.\n", "versions": [{"version": "v1", "created": "Fri, 29 Jul 2016 19:16:08 GMT"}], "update_date": "2016-08-01", "authors_parsed": [["Ding", "Tao", ""], ["Pan", "Shimei", ""]]}]