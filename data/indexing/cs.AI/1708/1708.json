[{"id": "1708.00102", "submitter": "Lucas Lehnert", "authors": "Lucas Lehnert, Stefanie Tellex, and Michael L. Littman", "title": "Advantages and Limitations of using Successor Features for Transfer in\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One question central to Reinforcement Learning is how to learn a feature\nrepresentation that supports algorithm scaling and re-use of learned\ninformation from different tasks. Successor Features approach this problem by\nlearning a feature representation that satisfies a temporal constraint. We\npresent an implementation of an approach that decouples the feature\nrepresentation from the reward function, making it suitable for transferring\nknowledge between domains. We then assess the advantages and limitations of\nusing Successor Features for transfer.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 23:36:18 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Lehnert", "Lucas", ""], ["Tellex", "Stefanie", ""], ["Littman", "Michael L.", ""]]}, {"id": "1708.00107", "submitter": "Bryan McCann", "authors": "Bryan McCann, James Bradbury, Caiming Xiong and Richard Socher", "title": "Learned in Translation: Contextualized Word Vectors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computer vision has benefited from initializing multiple deep layers with\nweights pretrained on large supervised training sets like ImageNet. Natural\nlanguage processing (NLP) typically sees initialization of only the lowest\nlayer of deep models with pretrained word vectors. In this paper, we use a deep\nLSTM encoder from an attentional sequence-to-sequence model trained for machine\ntranslation (MT) to contextualize word vectors. We show that adding these\ncontext vectors (CoVe) improves performance over using only unsupervised word\nand character vectors on a wide variety of common NLP tasks: sentiment analysis\n(SST, IMDb), question classification (TREC), entailment (SNLI), and question\nanswering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe\nimproves performance of our baseline models to the state of the art.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 00:05:34 GMT"}, {"version": "v2", "created": "Wed, 20 Jun 2018 13:15:06 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["McCann", "Bryan", ""], ["Bradbury", "James", ""], ["Xiong", "Caiming", ""], ["Socher", "Richard", ""]]}, {"id": "1708.00109", "submitter": "Regis Riveret", "authors": "Regis Riveret, Pietro Baroni, Yang Gao, Guido Governatori, Antonino\n  Rotolo, Giovanni Sartor", "title": "A Labelling Framework for Probabilistic Argumentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The combination of argumentation and probability paves the way to new\naccounts of qualitative and quantitative uncertainty, thereby offering new\ntheoretical and applicative opportunities. Due to a variety of interests,\nprobabilistic argumentation is approached in the literature with different\nframeworks, pertaining to structured and abstract argumentation, and with\nrespect to diverse types of uncertainty, in particular the uncertainty on the\ncredibility of the premises, the uncertainty about which arguments to consider,\nand the uncertainty on the acceptance status of arguments or statements.\nTowards a general framework for probabilistic argumentation, we investigate a\nlabelling-oriented framework encompassing a basic setting for rule-based\nargumentation and its (semi-) abstract account, along with diverse types of\nuncertainty. Our framework provides a systematic treatment of various kinds of\nuncertainty and of their relationships and allows us to back or question\nassertions from the literature.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 00:12:58 GMT"}, {"version": "v2", "created": "Fri, 9 Mar 2018 01:59:56 GMT"}], "update_date": "2018-03-12", "authors_parsed": [["Riveret", "Regis", ""], ["Baroni", "Pietro", ""], ["Gao", "Yang", ""], ["Governatori", "Guido", ""], ["Rotolo", "Antonino", ""], ["Sartor", "Giovanni", ""]]}, {"id": "1708.00133", "submitter": "Karthik Narasimhan", "authors": "Karthik Narasimhan, Regina Barzilay and Tommi Jaakkola", "title": "Grounding Language for Transfer in Deep Reinforcement Learning", "comments": "JAIR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore the utilization of natural language to drive\ntransfer for reinforcement learning (RL). Despite the wide-spread application\nof deep RL techniques, learning generalized policy representations that work\nacross domains remains a challenging problem. We demonstrate that textual\ndescriptions of environments provide a compact intermediate channel to\nfacilitate effective policy transfer. Specifically, by learning to ground the\nmeaning of text to the dynamics of the environment such as transitions and\nrewards, an autonomous agent can effectively bootstrap policy learning on a new\ndomain given its description. We employ a model-based RL approach consisting of\na differentiable planning module, a model-free component and a factorized state\nrepresentation to effectively use entity descriptions. Our model outperforms\nprior work on both transfer and multi-task scenarios in a variety of different\nenvironments. For instance, we achieve up to 14% and 11.5% absolute improvement\nover previously existing models in terms of average and initial rewards,\nrespectively.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 02:20:00 GMT"}, {"version": "v2", "created": "Wed, 5 Dec 2018 22:14:04 GMT"}], "update_date": "2018-12-07", "authors_parsed": [["Narasimhan", "Karthik", ""], ["Barzilay", "Regina", ""], ["Jaakkola", "Tommi", ""]]}, {"id": "1708.00146", "submitter": "Quanming Yao", "authors": "Quanming Yao, James T.Kwok, Taifeng Wang and Tie-Yan Liu", "title": "Large-Scale Low-Rank Matrix Learning with Nonconvex Regularizers", "comments": "Accepted by TPAMI in 2018 (extension of ICDM-2015 conference paper\n  arXiv:1512.00984)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rank modeling has many important applications in computer vision and\nmachine learning. While the matrix rank is often approximated by the convex\nnuclear norm, the use of nonconvex low-rank regularizers has demonstrated\nbetter empirical performance. However, the resulting optimization problem is\nmuch more challenging. Recent state-of-the-art requires an expensive full SVD\nin each iteration. In this paper, we show that for many commonly-used nonconvex\nlow-rank regularizers, a cutoff can be derived to automatically threshold the\nsingular values obtained from the proximal operator. This allows such operator\nbeing efficiently approximated by power method. Based on it, we develop a\nproximal gradient algorithm (and its accelerated variant) with inexact proximal\nsplitting and prove that a convergence rate of O(1/T) where T is the number of\niterations is guaranteed. Furthermore, we show the proposed algorithm can be\nwell parallelized, which achieves nearly linear speedup w.r.t the number of\nthreads. Extensive experiments are performed on matrix completion and robust\nprincipal component analysis, which shows a significant speedup over the\nstate-of-the-art. Moreover, the matrix solution obtained is more accurate and\nhas a lower rank than that of the nuclear norm regularizer.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 03:21:55 GMT"}, {"version": "v2", "created": "Sat, 23 Sep 2017 03:33:27 GMT"}, {"version": "v3", "created": "Mon, 23 Jul 2018 17:10:20 GMT"}], "update_date": "2018-07-25", "authors_parsed": [["Yao", "Quanming", ""], ["Kwok", "James T.", ""], ["Wang", "Taifeng", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1708.00154", "submitter": "Piji Li", "authors": "Piji Li, Zihao Wang, Zhaochun Ren, Lidong Bing, Wai Lam", "title": "Neural Rating Regression with Abstractive Tips Generation for\n  Recommendation", "comments": "SIGIR 2017", "journal-ref": null, "doi": "10.1145/3077136.3080822", "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, some E-commerce sites launch a new interaction box called Tips on\ntheir mobile apps. Users can express their experience and feelings or provide\nsuggestions using short texts typically several words or one sentence. In\nessence, writing some tips and giving a numerical rating are two facets of a\nuser's product assessment action, expressing the user experience and feelings.\nJointly modeling these two facets is helpful for designing a better\nrecommendation system. While some existing models integrate text information\nsuch as item specifications or user reviews into user and item latent factors\nfor improving the rating prediction, no existing works consider tips for\nimproving recommendation quality. We propose a deep learning based framework\nnamed NRT which can simultaneously predict precise ratings and generate\nabstractive tips with good linguistic quality simulating user experience and\nfeelings. For abstractive tips generation, gated recurrent neural networks are\nemployed to \"translate\" user and item latent representations into a concise\nsentence. Extensive experiments on benchmark datasets from different domains\nshow that NRT achieves significant improvements over the state-of-the-art\nmethods. Moreover, the generated tips can vividly predict the user experience\nand feelings.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 04:25:17 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Li", "Piji", ""], ["Wang", "Zihao", ""], ["Ren", "Zhaochun", ""], ["Bing", "Lidong", ""], ["Lam", "Wai", ""]]}, {"id": "1708.00224", "submitter": "Yibing Song", "authors": "Yibing Song, Jiawei Zhang, Linchao Bao, Qingxiong Yang", "title": "Fast Preprocessing for Robust Face Sketch Synthesis", "comments": "IJCAI 2017. Project page:\n  http://www.cs.cityu.edu.hk/~yibisong/ijcai17_sketch/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exemplar-based face sketch synthesis methods usually meet the challenging\nproblem that input photos are captured in different lighting conditions from\ntraining photos. The critical step causing the failure is the search of similar\npatch candidates for an input photo patch. Conventional illumination invariant\npatch distances are adopted rather than directly relying on pixel intensity\ndifference, but they will fail when local contrast within a patch changes. In\nthis paper, we propose a fast preprocessing method named Bidirectional\nLuminance Remapping (BLR), which interactively adjust the lighting of training\nand input photos. Our method can be directly integrated into state-of-the-art\nexemplar-based methods to improve their robustness with ignorable computational\ncost.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 09:46:54 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Song", "Yibing", ""], ["Zhang", "Jiawei", ""], ["Bao", "Linchao", ""], ["Yang", "Qingxiong", ""]]}, {"id": "1708.00225", "submitter": "Yibing Song", "authors": "Yibing Song, Chao Ma, Lijun Gong, Jiawei Zhang, Rynson Lau, Ming-Hsuan\n  Yang", "title": "CREST: Convolutional Residual Learning for Visual Tracking", "comments": "ICCV 2017. Project page:\n  http://www.cs.cityu.edu.hk/~yibisong/iccv17/index.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Discriminative correlation filters (DCFs) have been shown to perform\nsuperiorly in visual tracking. They only need a small set of training samples\nfrom the initial frame to generate an appearance model. However, existing DCFs\nlearn the filters separately from feature extraction, and update these filters\nusing a moving average operation with an empirical weight. These DCF trackers\nhardly benefit from the end-to-end training. In this paper, we propose the\nCREST algorithm to reformulate DCFs as a one-layer convolutional neural\nnetwork. Our method integrates feature extraction, response map generation as\nwell as model update into the neural networks for an end-to-end training. To\nreduce model degradation during online update, we apply residual learning to\ntake appearance changes into account. Extensive experiments on the benchmark\ndatasets demonstrate that our CREST tracker performs favorably against\nstate-of-the-art trackers.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 09:47:20 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Song", "Yibing", ""], ["Ma", "Chao", ""], ["Gong", "Lijun", ""], ["Zhang", "Jiawei", ""], ["Lau", "Rynson", ""], ["Yang", "Ming-Hsuan", ""]]}, {"id": "1708.00339", "submitter": "Yanjun  Qi Dr.", "authors": "Ritambhara Singh, Jack Lanchantin, Arshdeep Sekhon, Yanjun Qi", "title": "Attend and Predict: Understanding Gene Regulation by Selective Attention\n  on Chromatin", "comments": "12 pages; At NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past decade has seen a revolution in genomic technologies that enable a\nflood of genome-wide profiling of chromatin marks. Recent literature tried to\nunderstand gene regulation by predicting gene expression from large-scale\nchromatin measurements. Two fundamental challenges exist for such learning\ntasks: (1) genome-wide chromatin signals are spatially structured,\nhigh-dimensional and highly modular; and (2) the core aim is to understand what\nare the relevant factors and how they work together? Previous studies either\nfailed to model complex dependencies among input signals or relied on separate\nfeature analysis to explain the decisions. This paper presents an\nattention-based deep learning approach; we call AttentiveChrome, that uses a\nunified architecture to model and to interpret dependencies among chromatin\nfactors for controlling gene regulation. AttentiveChrome uses a hierarchy of\nmultiple Long short-term memory (LSTM) modules to encode the input signals and\nto model how various chromatin marks cooperate automatically. AttentiveChrome\ntrains two levels of attention jointly with the target prediction, enabling it\nto attend differentially to relevant marks and to locate important positions\nper mark. We evaluate the model across 56 different cell types (tasks) in\nhuman. Not only is the proposed architecture more accurate, but its attention\nscores also provide a better interpretation than state-of-the-art feature\nvisualization methods such as saliency map.\n  Code and data are shared at www.deepchrome.org\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 14:06:12 GMT"}, {"version": "v2", "created": "Wed, 2 Aug 2017 17:20:13 GMT"}, {"version": "v3", "created": "Tue, 7 Nov 2017 16:40:55 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Singh", "Ritambhara", ""], ["Lanchantin", "Jack", ""], ["Sekhon", "Arshdeep", ""], ["Qi", "Yanjun", ""]]}, {"id": "1708.00376", "submitter": "Svetlin Penkov", "authors": "Svetlin Penkov and Subramanian Ramamoorthy", "title": "Using Program Induction to Interpret Transition System Dynamics", "comments": "Presented at 2017 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2017), Sydney, NSW, Australia. arXiv admin note: substantial\n  text overlap with arXiv:1705.08320", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explaining and reasoning about processes which underlie observed black-box\nphenomena enables the discovery of causal mechanisms, derivation of suitable\nabstract representations and the formulation of more robust predictions. We\npropose to learn high level functional programs in order to represent abstract\nmodels which capture the invariant structure in the observed data. We introduce\nthe $\\pi$-machine (program-induction machine) -- an architecture able to induce\ninterpretable LISP-like programs from observed data traces. We propose an\noptimisation procedure for program learning based on backpropagation, gradient\ndescent and A* search. We apply the proposed method to two problems: system\nidentification of dynamical systems and explaining the behaviour of a DQN\nagent. Our results show that the $\\pi$-machine can efficiently induce\ninterpretable programs from individual data traces.\n", "versions": [{"version": "v1", "created": "Wed, 26 Jul 2017 12:49:04 GMT"}], "update_date": "2017-08-02", "authors_parsed": [["Penkov", "Svetlin", ""], ["Ramamoorthy", "Subramanian", ""]]}, {"id": "1708.00463", "submitter": "Adam Earle", "authors": "Adam C. Earle, Andrew M. Saxe, Benjamin Rosman", "title": "Hierarchical Subtask Discovery With Non-Negative Matrix Factorization", "comments": "7 pages, Accepted at Lifelong Learning: A Reinforcement Learning\n  Approach Workshop, ICML, Sydney, Australia, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchical reinforcement learning methods offer a powerful means of\nplanning flexible behavior in complicated domains. However, learning an\nappropriate hierarchical decomposition of a domain into subtasks remains a\nsubstantial challenge. We present a novel algorithm for subtask discovery,\nbased on the recently introduced multitask linearly-solvable Markov decision\nprocess (MLMDP) framework. The MLMDP can perform never-before-seen tasks by\nrepresenting them as a linear combination of a previously learned basis set of\ntasks. In this setting, the subtask discovery problem can naturally be posed as\nfinding an optimal low-rank approximation of the set of tasks the agent will\nface in a domain. We use non-negative matrix factorization to discover this\nminimal basis set of tasks, and show that the technique learns intuitive\ndecompositions in a variety of domains. Our method has several qualitatively\ndesirable features: it is not limited to learning subtasks with single goal\nstates, instead learning distributed patterns of preferred states; it learns\nqualitatively different hierarchical decompositions in the same domain\ndepending on the ensemble of tasks the agent will face; and it may be\nstraightforwardly iterated to obtain deeper hierarchical decompositions.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 18:19:40 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Earle", "Adam C.", ""], ["Saxe", "Andrew M.", ""], ["Rosman", "Benjamin", ""]]}, {"id": "1708.00495", "submitter": "Brett Israelsen", "authors": "Brett W Israelsen", "title": "\"I can assure you [$\\ldots$] that it's going to be all right\" -- A\n  definition, case for, and survey of algorithmic assurances in human-autonomy\n  trust relationships", "comments": "Copy submitted to area exam committee", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI cs.HC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As technology become more advanced, those who design, use and are otherwise\naffected by it want to know that it will perform correctly, and understand why\nit does what it does, and how to use it appropriately. In essence they want to\nbe able to trust the systems that are being designed. In this survey we present\nassurances that are the method by which users can understand how to trust this\ntechnology. Trust between humans and autonomy is reviewed, and the implications\nfor the design of assurances are highlighted. A survey of research that has\nbeen performed with respect to assurances is presented, and several key ideas\nare extracted in order to refine the definition of assurances. Several\ndirections for future research are identified and discussed.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 20:13:18 GMT"}, {"version": "v2", "created": "Mon, 4 Sep 2017 18:39:22 GMT"}], "update_date": "2017-09-07", "authors_parsed": [["Israelsen", "Brett W", ""]]}, {"id": "1708.00543", "submitter": "Sarath Sreedharan", "authors": "Tathagata Chakraborti, Sarath Sreedharan and Subbarao Kambhampati", "title": "Balancing Explicability and Explanation in Human-Aware Planning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human aware planning requires an agent to be aware of the intentions,\ncapabilities and mental model of the human in the loop during its decision\nprocess. This can involve generating plans that are explicable to a human\nobserver as well as the ability to provide explanations when such plans cannot\nbe generated. This has led to the notion \"multi-model planning\" which aim to\nincorporate effects of human expectation in the deliberative process of a\nplanner - either in the form of explicable task planning or explanations\nproduced thereof. In this paper, we bring these two concepts together and show\nhow a planner can account for both these needs and achieve a trade-off during\nthe plan generation process itself by means of a model-space search method\nMEGA. This in effect provides a comprehensive perspective of what it means for\na decision making agent to be \"human-aware\" by bringing together existing\nprinciples of planning under the umbrella of a single plan generation process.\nWe situate our discussion specifically keeping in mind the recent work on\nexplicable planning and explanation generation, and illustrate these concepts\nin modified versions of two well known planning domains, as well as a\ndemonstration on a robot involved in a typical search and reconnaissance task\nwith an external supervisor.\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 22:47:42 GMT"}, {"version": "v2", "created": "Sat, 3 Feb 2018 19:04:43 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Chakraborti", "Tathagata", ""], ["Sreedharan", "Sarath", ""], ["Kambhampati", "Subbarao", ""]]}, {"id": "1708.00588", "submitter": "Maziar Raissi", "authors": "Maziar Raissi and George Em Karniadakis", "title": "Hidden Physics Models: Machine Learning of Nonlinear Partial\n  Differential Equations", "comments": null, "journal-ref": null, "doi": "10.1016/j.jcp.2017.11.039", "report-no": null, "categories": "cs.AI cs.LG math.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While there is currently a lot of enthusiasm about \"big data\", useful data is\nusually \"small\" and expensive to acquire. In this paper, we present a new\nparadigm of learning partial differential equations from {\\em small} data. In\nparticular, we introduce \\emph{hidden physics models}, which are essentially\ndata-efficient learning machines capable of leveraging the underlying laws of\nphysics, expressed by time dependent and nonlinear partial differential\nequations, to extract patterns from high-dimensional data generated from\nexperiments. The proposed methodology may be applied to the problem of\nlearning, system identification, or data-driven discovery of partial\ndifferential equations. Our framework relies on Gaussian processes, a powerful\ntool for probabilistic inference over functions, that enables us to strike a\nbalance between model complexity and data fitting. The effectiveness of the\nproposed approach is demonstrated through a variety of canonical problems,\nspanning a number of scientific domains, including the Navier-Stokes,\nSchr\\\"odinger, Kuramoto-Sivashinsky, and time dependent linear fractional\nequations. The methodology provides a promising new direction for harnessing\nthe long-standing developments of classical methods in applied mathematics and\nmathematical physics to design learning machines with the ability to operate in\ncomplex domains without requiring large quantities of data.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 03:28:54 GMT"}, {"version": "v2", "created": "Mon, 21 Aug 2017 22:39:46 GMT"}], "update_date": "2018-01-17", "authors_parsed": [["Raissi", "Maziar", ""], ["Karniadakis", "George Em", ""]]}, {"id": "1708.00625", "submitter": "Piji Li", "authors": "Piji Li, Wai Lam, Lidong Bing, Zihao Wang", "title": "Deep Recurrent Generative Decoder for Abstractive Text Summarization", "comments": "10 pages, EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new framework for abstractive text summarization based on a\nsequence-to-sequence oriented encoder-decoder model equipped with a deep\nrecurrent generative decoder (DRGN).\n  Latent structure information implied in the target summaries is learned based\non a recurrent latent random model for improving the summarization quality.\n  Neural variational inference is employed to address the intractable posterior\ninference for the recurrent latent variables.\n  Abstractive summaries are generated based on both the generative latent\nvariables and the discriminative deterministic states.\n  Extensive experiments on some benchmark datasets in different languages show\nthat DRGN achieves improvements over the state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 07:47:14 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Li", "Piji", ""], ["Lam", "Wai", ""], ["Bing", "Lidong", ""], ["Wang", "Zihao", ""]]}, {"id": "1708.00630", "submitter": "Sujith Ravi", "authors": "Sujith Ravi", "title": "ProjectionNet: Learning Efficient On-Device Deep Networks Using Neural\n  Projections", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks have become ubiquitous for applications related to\nvisual recognition and language understanding tasks. However, it is often\nprohibitive to use typical neural networks on devices like mobile phones or\nsmart watches since the model sizes are huge and cannot fit in the limited\nmemory available on such devices. While these devices could make use of machine\nlearning models running on high-performance data centers with CPUs or GPUs,\nthis is not feasible for many applications because data can be privacy\nsensitive and inference needs to be performed directly \"on\" device.\n  We introduce a new architecture for training compact neural networks using a\njoint optimization framework. At its core lies a novel objective that jointly\ntrains using two different types of networks--a full trainer neural network\n(using existing architectures like Feed-forward NNs or LSTM RNNs) combined with\na simpler \"projection\" network that leverages random projections to transform\ninputs or intermediate representations into bits. The simpler network encodes\nlightweight and efficient-to-compute operations in bit space with a low memory\nfootprint. The two networks are trained jointly using backpropagation, where\nthe projection network learns from the full network similar to apprenticeship\nlearning. Once trained, the smaller network can be used directly for inference\nat low memory and computation cost. We demonstrate the effectiveness of the new\napproach at significantly shrinking the memory requirements of different types\nof neural networks while preserving good accuracy on visual recognition and\ntext classification tasks. We also study the question \"how many neural bits are\nrequired to solve a given task?\" using the new framework and show empirical\nresults contrasting model predictive capacity (in bits) versus accuracy on\nseveral datasets.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 07:58:45 GMT"}, {"version": "v2", "created": "Wed, 9 Aug 2017 10:05:09 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Ravi", "Sujith", ""]]}, {"id": "1708.00631", "submitter": "Chengxi Ye", "authors": "Chengxi Ye, Yezhou Yang, Cornelia Fermuller, Yiannis Aloimonos", "title": "On the Importance of Consistency in Training Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explain that the difficulties of training deep neural networks come from a\nsyndrome of three consistency issues. This paper describes our efforts in their\nanalysis and treatment. The first issue is the training speed inconsistency in\ndifferent layers. We propose to address it with an intuitive,\nsimple-to-implement, low footprint second-order method. The second issue is the\nscale inconsistency between the layer inputs and the layer residuals. We\nexplain how second-order information provides favorable convenience in removing\nthis roadblock. The third and most challenging issue is the inconsistency in\nresidual propagation. Based on the fundamental theorem of linear algebra, we\nprovide a mathematical characterization of the famous vanishing gradient\nproblem. Thus, an important design principle for future optimization and neural\nnetwork design is derived. We conclude this paper with the construction of a\nnovel contractive neural network.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 08:05:09 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Ye", "Chengxi", ""], ["Yang", "Yezhou", ""], ["Fermuller", "Cornelia", ""], ["Aloimonos", "Yiannis", ""]]}, {"id": "1708.00667", "submitter": "Takuya Hiraoka", "authors": "Takuya Hiraoka, Masaaki Tsuchida, Yotaro Watanabe", "title": "Deep Reinforcement Learning for Inquiry Dialog Policies with Logical\n  Formula Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is the first attempt to learn the policy of an inquiry dialog\nsystem (IDS) by using deep reinforcement learning (DRL). Most IDS frameworks\nrepresent dialog states and dialog acts with logical formulae. In order to make\nlearning inquiry dialog policies more effective, we introduce a logical formula\nembedding framework based on a recursive neural network. The results of\nexperiments to evaluate the effect of 1) the DRL and 2) the logical formula\nembedding framework show that the combination of the two are as effective or\neven better than existing rule-based methods for inquiry dialog policies.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 09:40:42 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Hiraoka", "Takuya", ""], ["Tsuchida", "Masaaki", ""], ["Watanabe", "Yotaro", ""]]}, {"id": "1708.00730", "submitter": "Tomasz Tajmajer", "authors": "Andrzej Janusz, Maciej \\'Swiechowski, Tomasz Tajmajer", "title": "Helping AI to Play Hearthstone: AAIA'17 Data Mining Challenge", "comments": "Federated Conference on Computer Science and Information Systems,\n  Prague (FedCSIS-2017) (Prague, Czech Republic)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper summarizes the AAIA'17 Data Mining Challenge: Helping AI to Play\nHearthstone which was held between March 23, and May 15, 2017 at the Knowledge\nPit platform. We briefly describe the scope and background of this competition\nin the context of a more general project related to the development of an AI\nengine for video games, called Grail. We also discuss the outcomes of this\nchallenge and demonstrate how predictive models for the assessment of player's\nwinning chances can be utilized in a construction of an intelligent agent for\nplaying Hearthstone. Finally, we show a few selected machine learning\napproaches for modeling state and action values in Hearthstone. We provide\nevaluation for a few promising solutions that may be used to create more\nadvanced types of agents, especially in conjunction with Monte Carlo Tree\nSearch algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 12:58:04 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Janusz", "Andrzej", ""], ["\u015awiechowski", "Maciej", ""], ["Tajmajer", "Tomasz", ""]]}, {"id": "1708.00754", "submitter": "Indre Zliobaite", "authors": "Indre Zliobaite", "title": "Fairness-aware machine learning: a perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Algorithms learned from data are increasingly used for deciding many aspects\nin our life: from movies we see, to prices we pay, or medicine we get. Yet\nthere is growing evidence that decision making by inappropriately trained\nalgorithms may unintentionally discriminate people. For example, in automated\nmatching of candidate CVs with job descriptions, algorithms may capture and\npropagate ethnicity related biases. Several repairs for selected algorithms\nhave already been proposed, but the underlying mechanisms how such\ndiscrimination happens from the computational perspective are not yet\nscientifically understood. We need to develop theoretical understanding how\nalgorithms may become discriminatory, and establish fundamental machine\nlearning principles for prevention. We need to analyze machine learning process\nas a whole to systematically explain the roots of discrimination occurrence,\nwhich will allow to devise global machine learning optimization criteria for\nguaranteed prevention, as opposed to pushing empirical constraints into\nexisting algorithms case-by-case. As a result, the state-of-the-art will\nadvance from heuristic repairing, to proactive and theoretically supported\nprevention. This is needed not only because law requires to protect vulnerable\npeople. Penetration of big data initiatives will only increase, and computer\nscience needs to provide solid explanations and accountability to the public,\nbefore public concerns lead to unnecessarily restrictive regulations against\nmachine learning.\n", "versions": [{"version": "v1", "created": "Wed, 2 Aug 2017 14:14:49 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Zliobaite", "Indre", ""]]}, {"id": "1708.00807", "submitter": "Yanjun  Qi Dr.", "authors": "Andrew P. Norton, Yanjun Qi", "title": "Adversarial-Playground: A Visualization Suite Showing How Adversarial\n  Examples Fool Deep Learning", "comments": "5 pages. {I.2.6}{Artificial Intelligence} ; {K.6.5}{Management of\n  Computing and Information Systems}{Security and Protection}. arXiv admin\n  note: substantial text overlap with arXiv:1706.01763", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent studies have shown that attackers can force deep learning models to\nmisclassify so-called \"adversarial examples\": maliciously generated images\nformed by making imperceptible modifications to pixel values. With growing\ninterest in deep learning for security applications, it is important for\nsecurity experts and users of machine learning to recognize how learning\nsystems may be attacked. Due to the complex nature of deep learning, it is\nchallenging to understand how deep models can be fooled by adversarial\nexamples. Thus, we present a web-based visualization tool,\nAdversarial-Playground, to demonstrate the efficacy of common adversarial\nmethods against a convolutional neural network (CNN) system.\nAdversarial-Playground is educational, modular and interactive. (1) It enables\nnon-experts to compare examples visually and to understand why an adversarial\nexample can fool a CNN-based image classifier. (2) It can help security experts\nexplore more vulnerability of deep learning as a software module. (3) Building\nan interactive visualization is challenging in this domain due to the large\nfeature space of image classification (generating adversarial examples is slow\nin general and visualizing images are costly). Through multiple novel design\nchoices, our tool can provide fast and accurate responses to user requests.\nEmpirically, we find that our client-server division strategy reduced the\nresponse time by an average of 1.5 seconds per sample. Our other innovation, a\nfaster variant of JSMA evasion algorithm, empirically performed twice as fast\nas JSMA and yet maintains a comparable evasion rate.\n  Project source code and data from our experiments available at:\nhttps://github.com/QData/AdversarialDNN-Playground\n", "versions": [{"version": "v1", "created": "Tue, 1 Aug 2017 14:34:35 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Norton", "Andrew P.", ""], ["Qi", "Yanjun", ""]]}, {"id": "1708.01035", "submitter": "Charmgil Hong", "authors": "Charmgil Hong, Siqi Liu, Milos Hauskrecht", "title": "Detection of Abnormal Input-Output Associations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a novel outlier detection problem that aims to identify abnormal\ninput-output associations in data, whose instances consist of multi-dimensional\ninput (context) and output (responses) pairs. We present our approach that\nworks by analyzing data in the conditional (input--output) relation space,\ncaptured by a decomposable probabilistic model. Experimental results\ndemonstrate the ability of our approach in identifying multivariate conditional\noutliers.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 07:41:55 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Hong", "Charmgil", ""], ["Liu", "Siqi", ""], ["Hauskrecht", "Milos", ""]]}, {"id": "1708.01065", "submitter": "Piji Li", "authors": "Piji Li, Lidong Bing, Wai Lam", "title": "Reader-Aware Multi-Document Summarization: An Enhanced Model and The\n  First Dataset", "comments": "EMNLP 2017 Workshop on New Frontiers in Summarization; Dataset:\n  http://www.se.cuhk.edu.hk/~textmine/dataset/ra-mds/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate the problem of reader-aware multi-document summarization\n(RA-MDS) and introduce a new dataset for this problem. To tackle RA-MDS, we\nextend a variational auto-encodes (VAEs) based MDS framework by jointly\nconsidering news documents and reader comments. To conduct evaluation for\nsummarization performance, we prepare a new dataset. We describe the methods\nfor data collection, aspect annotation, and summary writing as well as\nscrutinizing by experts. Experimental results show that reader comments can\nimprove the summarization performance, which also demonstrates the usefulness\nof the proposed dataset. The annotated dataset for RA-MDS is available online.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 09:18:16 GMT"}], "update_date": "2017-08-04", "authors_parsed": [["Li", "Piji", ""], ["Bing", "Lidong", ""], ["Lam", "Wai", ""]]}, {"id": "1708.01104", "submitter": "Andreas Holzinger", "authors": "Andreas Holzinger, Markus Plass, Katharina Holzinger, Gloria Cerasela\n  Crisan, Camelia-M. Pintea, Vasile Palade", "title": "A glass-box interactive machine learning approach for solving NP-hard\n  problems with the human-in-the-loop", "comments": "26 pages, 5 figures", "journal-ref": "CREAT.MATH.INFORM. 28(2) (2019) 121-134", "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of Machine Learning to automatically learn from data, extract\nknowledge and to make decisions without any human intervention. Such automatic\n(aML) approaches show impressive success. Recent results even demonstrate\nintriguingly that deep learning applied for automatic classification of skin\nlesions is on par with the performance of dermatologists, yet outperforms the\naverage. As human perception is inherently limited, such approaches can\ndiscover patterns, e.g. that two objects are similar, in arbitrarily\nhigh-dimensional spaces what no human is able to do. Humans can deal only with\nlimited amounts of data, whilst big data is beneficial for aML; however, in\nhealth informatics, we are often confronted with a small number of data sets,\nwhere aML suffer of insufficient training samples and many problems are\ncomputationally hard. Here, interactive machine learning (iML) may be of help,\nwhere a human-in-the-loop contributes to reduce the complexity of NP-hard\nproblems. A further motivation for iML is that standard black-box approaches\nlack transparency, hence do not foster trust and acceptance of ML among\nend-users. Rising legal and privacy aspects, e.g. with the new European General\nData Protection Regulations, make black-box approaches difficult to use,\nbecause they often are not able to explain why a decision has been made. In\nthis paper, we present some experiments to demonstrate the effectiveness of the\nhuman-in-the-loop approach, particularly in opening the black-box to a\nglass-box and thus enabling a human directly to interact with an learning\nalgorithm. We selected the Ant Colony Optimization framework, and applied it on\nthe Traveling Salesman Problem, which is a good example, due to its relevance\nfor health informatics, e.g. for the study of protein folding. From studies of\nhow humans extract so much from so little data, fundamental ML-research also\nmay benefit.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 11:33:10 GMT"}], "update_date": "2021-03-16", "authors_parsed": [["Holzinger", "Andreas", ""], ["Plass", "Markus", ""], ["Holzinger", "Katharina", ""], ["Crisan", "Gloria Cerasela", ""], ["Pintea", "Camelia-M.", ""], ["Palade", "Vasile", ""]]}, {"id": "1708.01289", "submitter": "Jules Pondard", "authors": "Valentin Thomas, Jules Pondard, Emmanuel Bengio, Marc Sarfati,\n  Philippe Beaudoin, Marie-Jean Meurs, Joelle Pineau, Doina Precup, Yoshua\n  Bengio", "title": "Independently Controllable Factors", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It has been postulated that a good representation is one that disentangles\nthe underlying explanatory factors of variation. However, it remains an open\nquestion what kind of training framework could potentially achieve that.\nWhereas most previous work focuses on the static setting (e.g., with images),\nwe postulate that some of the causal factors could be discovered if the learner\nis allowed to interact with its environment. The agent can experiment with\ndifferent actions and observe their effects. More specifically, we hypothesize\nthat some of these factors correspond to aspects of the environment which are\nindependently controllable, i.e., that there exists a policy and a learnable\nfeature for each such aspect of the environment, such that this policy can\nyield changes in that feature with minimal changes to other features that\nexplain the statistical variations in the observed data. We propose a specific\nobjective function to find such factors and verify experimentally that it can\nindeed disentangle independently controllable aspects of the environment\nwithout any extrinsic reward signal.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 19:32:33 GMT"}, {"version": "v2", "created": "Fri, 25 Aug 2017 22:18:11 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Thomas", "Valentin", ""], ["Pondard", "Jules", ""], ["Bengio", "Emmanuel", ""], ["Sarfati", "Marc", ""], ["Beaudoin", "Philippe", ""], ["Meurs", "Marie-Jean", ""], ["Pineau", "Joelle", ""], ["Precup", "Doina", ""], ["Bengio", "Yoshua", ""]]}, {"id": "1708.01318", "submitter": "Khanh Nguyen", "authors": "Amr Sharaf, Shi Feng, Khanh Nguyen, Kiant\\'e Brantley, Hal Daum\\'e III", "title": "The UMD Neural Machine Translation Systems at WMT17 Bandit Learning Task", "comments": "7 pages, 1 figure, WMT 2017 Bandit Learning Task", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the University of Maryland machine translation systems submitted\nto the WMT17 German-English Bandit Learning Task. The task is to adapt a\ntranslation system to a new domain, using only bandit feedback: the system\nreceives a German sentence to translate, produces an English sentence, and only\ngets a scalar score as feedback. Targeting these two challenges (adaptation and\nbandit learning), we built a standard neural machine translation system and\nextended it in two ways: (1) robust reinforcement learning techniques to learn\neffectively from the bandit feedback, and (2) domain adaptation using data\nselection from a large corpus of parallel data.\n", "versions": [{"version": "v1", "created": "Thu, 3 Aug 2017 21:42:46 GMT"}, {"version": "v2", "created": "Mon, 7 Aug 2017 20:45:50 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Sharaf", "Amr", ""], ["Feng", "Shi", ""], ["Nguyen", "Khanh", ""], ["Brantley", "Kiant\u00e9", ""], ["Daum\u00e9", "Hal", "III"]]}, {"id": "1708.01425", "submitter": "Ivan Habernal", "authors": "Ivan Habernal and Henning Wachsmuth and Iryna Gurevych and Benno Stein", "title": "The Argument Reasoning Comprehension Task: Identification and\n  Reconstruction of Implicit Warrants", "comments": "Accepted as NAACL 2018 Long Paper; see details on the front page", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reasoning is a crucial part of natural language argumentation. To comprehend\nan argument, one must analyze its warrant, which explains why its claim follows\nfrom its premises. As arguments are highly contextualized, warrants are usually\npresupposed and left implicit. Thus, the comprehension does not only require\nlanguage understanding and logic skills, but also depends on common sense. In\nthis paper we develop a methodology for reconstructing warrants systematically.\nWe operationalize it in a scalable crowdsourcing process, resulting in a freely\nlicensed dataset with warrants for 2k authentic arguments from news comments.\nOn this basis, we present a new challenging task, the argument reasoning\ncomprehension task. Given an argument with a claim and a premise, the goal is\nto choose the correct implicit warrant from two options. Both warrants are\nplausible and lexically close, but lead to contradicting claims. A solution to\nthis task will define a substantial step towards automatic warrant\nreconstruction. However, experiments with several neural attention and language\nmodels reveal that current approaches do not suffice.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 08:46:03 GMT"}, {"version": "v2", "created": "Tue, 22 Aug 2017 13:34:24 GMT"}, {"version": "v3", "created": "Mon, 19 Feb 2018 12:34:20 GMT"}, {"version": "v4", "created": "Tue, 27 Feb 2018 12:53:48 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Habernal", "Ivan", ""], ["Wachsmuth", "Henning", ""], ["Gurevych", "Iryna", ""], ["Stein", "Benno", ""]]}, {"id": "1708.01599", "submitter": "Muaz Niazi", "authors": "Muaz A. Niazi, Amir Hussain", "title": "Agent based Tools for Modeling and Simulation of Self-Organization in\n  Peer-to-Peer, Ad-Hoc and other Complex Networks", "comments": "20 pages, 6 figures", "journal-ref": "IEEE Communications Magazine, 47(3), 163 - 173 (2009)", "doi": "10.1109/MCOM.2009.4804403", "report-no": null, "categories": "cs.NI cs.AI cs.MA cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Agent-based modeling and simulation tools provide a mature platform for\ndevelopment of complex simulations. They however, have not been applied much in\nthe domain of mainstream modeling and simulation of computer networks. In this\narticle, we evaluate how and if these tools can offer any value-addition in the\nmodeling & simulation of complex networks such as pervasive computing,\nlarge-scale peer-to-peer systems, and networks involving considerable\nenvironment and human/animal/habitat interaction. Specifically, we demonstrate\nthe effectiveness of NetLogo - a tool that has been widely used in the area of\nagent-based social simulation.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 17:43:01 GMT"}], "update_date": "2017-08-07", "authors_parsed": [["Niazi", "Muaz A.", ""], ["Hussain", "Amir", ""]]}, {"id": "1708.01611", "submitter": "Paul Vitanyi", "authors": "Paul M.B. Vitanyi (CWI and University of Amsterdam) and Nick Chater\n  (Behavioural Science Group, Warwick Business School, University of Warwick,\n  Coventry, UK)", "title": "Identification of Probabilities", "comments": "31 pages LaTeX. arXiv admin note: substantial text overlap with\n  arXiv:1311.7385", "journal-ref": "Journal of Mathematical Psychology 51, 135-163 (2007)", "doi": "10.1016/j.jmp.2006.10.002", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Within psychology, neuroscience and artificial intelligence, there has been\nincreasing interest in the proposal that the brain builds probabilistic models\nof sensory and linguistic input: that is, to infer a probabilistic model from a\nsample. The practical problems of such inference are substantial: the brain has\nlimited data and restricted computational resources. But there is a more\nfundamental question: is the problem of inferring a probabilistic model from a\nsample possible even in principle? We explore this question and find some\nsurprisingly positive and general results. First, for a broad class of\nprobability distributions characterised by computability restrictions, we\nspecify a learning algorithm that will almost surely identify a probability\ndistribution in the limit given a finite i.i.d. sample of sufficient but\nunknown length. This is similarly shown to hold for sequences generated by a\nbroad class of Markov chains, subject to computability assumptions. The\ntechnical tool is the strong law of large numbers. Second, for a large class of\ndependent sequences, we specify an algorithm which identifies in the limit a\ncomputable measure for which the sequence is typical, in the sense of\nMartin-Lof (there may be more than one such measure). The technical tool is the\ntheory of Kolmogorov complexity. We analyse the associated predictions in both\ncases. We also briefly consider special cases, including language learning, and\nwider theoretical implications for psychology.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 16:36:12 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Vitanyi", "Paul M. B.", "", "CWI and University of Amsterdam"], ["Chater", "Nick", "", "Behavioural Science Group, Warwick Business School, University of Warwick,\n  Coventry, UK"]]}, {"id": "1708.01628", "submitter": "Muaz Niazi", "authors": "Faisal Riaz, Muaz A. Niazi", "title": "Validation of Enhanced Emotion Enabled Cognitive Agent Using Virtual\n  Overlay Multi-Agent System Approach", "comments": "35 pages, 21 figures, 19 tables", "journal-ref": "Broad Research in Artificial Intelligence and Neuroscience 8.3\n  (2017): 13-37", "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.GT cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Making roads safer by avoiding road collisions is one of the main reasons for\ninventing Autonomous vehicles (AVs). In this context, designing agent-based\ncollision avoidance components of AVs which truly represent human cognition and\nemotions look is a more feasible approach as agents can replace human drivers.\nHowever, to the best of our knowledge, very few human emotion and\ncognition-inspired agent-based studies have previously been conducted in this\ndomain. Furthermore, these agent-based solutions have not been validated using\nany key validation technique. Keeping in view this lack of validation\npractices, we have selected state-of-the-art Emotion Enabled Cognitive Agent\n(EEC_Agent), which was proposed to avoid lateral collisions between semi-AVs.\nThe architecture of EEC_Agent has been revised using Exploratory Agent Based\nModeling (EABM) level of the Cognitive Agent Based Computing (CABC) framework\nand real-time fear emotion generation mechanism using the Ortony, Clore &\nCollins (OCC) model has also been introduced. Then the proposed fear generation\nmechanism has been validated using the Validated Agent Based Modeling level of\nCABC framework using a Virtual Overlay MultiAgent System (VOMAS). Extensive\nsimulation and practical experiments demonstrate that the Enhanced EEC_Agent\nexhibits the capability to feel different levels of fear, according to\ndifferent traffic situations and also needs a smaller Stopping Sight Distance\n(SSD) and Overtaking Sight Distance (OSD) as compared to human drivers.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 18:11:24 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Riaz", "Faisal", ""], ["Niazi", "Muaz A.", ""]]}, {"id": "1708.01636", "submitter": "Muaz Niazi", "authors": "Aisha D. Farooqui, Muaz A. Niazi", "title": "Game theory models for communication between agents: a review", "comments": "31 pages, 7 figures", "journal-ref": "Complex Adaptive Systems Modeling, 4(1), 13. (2016)", "doi": "10.1186/s40294-016-0026-7", "report-no": null, "categories": "cs.GT cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the real world, agents or entities are in a continuous state of\ninteractions. These inter- actions lead to various types of complexity\ndynamics. One key difficulty in the study of complex agent interactions is the\ndifficulty of modeling agent communication on the basis of rewards. Game theory\noffers a perspective of analysis and modeling these interactions. Previously,\nwhile a large amount of literature is available on game theory, most of it is\nfrom specific domains and does not cater for the concepts from an agent- based\nperspective. Here in this paper, we present a comprehensive multidisciplinary\nstate-of-the-art review and taxonomy of game theory models of complex\ninteractions between agents.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 18:37:27 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Farooqui", "Aisha D.", ""], ["Niazi", "Muaz A.", ""]]}, {"id": "1708.01648", "submitter": "Chuhang Zou", "authors": "Chuhang Zou, Ersin Yumer, Jimei Yang, Duygu Ceylan, Derek Hoiem", "title": "3D-PRNN: Generating Shape Primitives with Recurrent Neural Networks", "comments": "ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The success of various applications including robotics, digital content\ncreation, and visualization demand a structured and abstract representation of\nthe 3D world from limited sensor data. Inspired by the nature of human\nperception of 3D shapes as a collection of simple parts, we explore such an\nabstract shape representation based on primitives. Given a single depth image\nof an object, we present 3D-PRNN, a generative recurrent neural network that\nsynthesizes multiple plausible shapes composed of a set of primitives. Our\ngenerative model encodes symmetry characteristics of common man-made objects,\npreserves long-range structural coherence, and describes objects of varying\ncomplexity with a compact representation. We also propose a method based on\nGaussian Fields to generate a large scale dataset of primitive-based shape\nrepresentations to train our network. We evaluate our approach on a wide range\nof examples and show that it outperforms nearest-neighbor based shape retrieval\nmethods and is on-par with voxel-based generative models while using a\nsignificantly reduced parameter space.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 19:30:13 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Zou", "Chuhang", ""], ["Yumer", "Ersin", ""], ["Yang", "Jimei", ""], ["Ceylan", "Duygu", ""], ["Hoiem", "Derek", ""]]}, {"id": "1708.01666", "submitter": "Jie Cao", "authors": "Yang Jiang, Zeyang Dou, Qun Hao, Jie Cao, Kun Gao, Xi Chen", "title": "An Effective Training Method For Deep Convolutional Neural Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose the nonlinearity generation method to speed up and\nstabilize the training of deep convolutional neural networks. The proposed\nmethod modifies a family of activation functions as nonlinearity generators\n(NGs). NGs make the activation functions linear symmetric for their inputs to\nlower model capacity, and automatically introduce nonlinearity to enhance the\ncapacity of the model during training. The proposed method can be considered an\nunusual form of regularization: the model parameters are obtained by training a\nrelatively low-capacity model, that is relatively easy to optimize at the\nbeginning, with only a few iterations, and these parameters are reused for the\ninitialization of a higher-capacity model. We derive the upper and lower bounds\nof variance of the weight variation, and show that the initial symmetric\nstructure of NGs helps stabilize training. We evaluate the proposed method on\ndifferent frameworks of convolutional neural networks over two object\nrecognition benchmark tasks (CIFAR-10 and CIFAR-100). Experimental results\nshowed that the proposed method allows us to (1) speed up the convergence of\ntraining, (2) allow for less careful weight initialization, (3) improve or at\nleast maintain the performance of the model at negligible extra computational\ncost, and (4) easily train a very deep model.\n", "versions": [{"version": "v1", "created": "Mon, 31 Jul 2017 23:19:03 GMT"}, {"version": "v2", "created": "Sun, 13 Aug 2017 14:41:04 GMT"}, {"version": "v3", "created": "Mon, 21 Aug 2017 15:45:11 GMT"}, {"version": "v4", "created": "Tue, 10 Oct 2017 08:58:03 GMT"}, {"version": "v5", "created": "Tue, 17 Oct 2017 15:53:20 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Jiang", "Yang", ""], ["Dou", "Zeyang", ""], ["Hao", "Qun", ""], ["Cao", "Jie", ""], ["Gao", "Kun", ""], ["Chen", "Xi", ""]]}, {"id": "1708.01729", "submitter": "Zhiming Zhou", "authors": "Zhiming Zhou, Weinan Zhang, Jun Wang", "title": "Inception Score, Label Smoothing, Gradient Vanishing and -log(D(x))\n  Alternative", "comments": "An advanced version is included in arXiv:1703.02000 \"Activation\n  Maximization Generative Adversarial Nets\"", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In this article, we mathematically study several GAN related topics,\nincluding Inception score, label smoothing, gradient vanishing and the\n-log(D(x)) alternative.\n  ---\n  An advanced version is included in arXiv:1703.02000 \"Activation Maximization\nGenerative Adversarial Nets\".\n  Please refer Section 6 in 1703.02000 for detailed analysis on Inception\nScore, and refer its appendix for the discussions on Label Smoothing, Gradient\nVanishing and -log(D(x)) Alternative.\n", "versions": [{"version": "v1", "created": "Sat, 5 Aug 2017 08:15:07 GMT"}, {"version": "v2", "created": "Sat, 3 Mar 2018 02:30:32 GMT"}, {"version": "v3", "created": "Sat, 30 Jun 2018 07:02:11 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Zhou", "Zhiming", ""], ["Zhang", "Weinan", ""], ["Wang", "Jun", ""]]}, {"id": "1708.01733", "submitter": "Francesco Locatello", "authors": "Francesco Locatello, Rajiv Khanna, Joydeep Ghosh, Gunnar R\\\"atsch", "title": "Boosting Variational Inference: an Optimization Perspective", "comments": null, "journal-ref": "AISTATS 2018", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational inference is a popular technique to approximate a possibly\nintractable Bayesian posterior with a more tractable one. Recently, boosting\nvariational inference has been proposed as a new paradigm to approximate the\nposterior by a mixture of densities by greedily adding components to the\nmixture. However, as is the case with many other variational inference\nalgorithms, its theoretical properties have not been studied. In the present\nwork, we study the convergence properties of this approach from a modern\noptimization viewpoint by establishing connections to the classic Frank-Wolfe\nalgorithm. Our analyses yields novel theoretical insights regarding the\nsufficient conditions for convergence, explicit rates, and algorithmic\nsimplifications. Since a lot of focus in previous works for variational\ninference has been on tractability, our work is especially important as a much\nneeded attempt to bridge the gap between probabilistic models and their\ncorresponding theoretical properties.\n", "versions": [{"version": "v1", "created": "Sat, 5 Aug 2017 08:42:11 GMT"}, {"version": "v2", "created": "Wed, 7 Mar 2018 13:04:35 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Locatello", "Francesco", ""], ["Khanna", "Rajiv", ""], ["Ghosh", "Joydeep", ""], ["R\u00e4tsch", "Gunnar", ""]]}, {"id": "1708.01776", "submitter": "Clemens Rosenbaum", "authors": "Clemens Rosenbaum, Tian Gao, Tim Klinger", "title": "e-QRAQ: A Multi-turn Reasoning Dataset and Simulator with Explanations", "comments": "7 pages, 3 figures, presented at 2017 ICML Workshop on Human\n  Interpretability in Machine Learning (WHI 2017), Sydney, NSW, Australia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a new dataset and user simulator e-QRAQ (explainable\nQuery, Reason, and Answer Question) which tests an Agent's ability to read an\nambiguous text; ask questions until it can answer a challenge question; and\nexplain the reasoning behind its questions and answer. The User simulator\nprovides the Agent with a short, ambiguous story and a challenge question about\nthe story. The story is ambiguous because some of the entities have been\nreplaced by variables. At each turn the Agent may ask for the value of a\nvariable or try to answer the challenge question. In response the User\nsimulator provides a natural language explanation of why the Agent's query or\nanswer was useful in narrowing down the set of possible answers, or not. To\ndemonstrate one potential application of the e-QRAQ dataset, we train a new\nneural architecture based on End-to-End Memory Networks to successfully\ngenerate both predictions and partial explanations of its current understanding\nof the problem. We observe a strong correlation between the quality of the\nprediction and explanation.\n", "versions": [{"version": "v1", "created": "Sat, 5 Aug 2017 15:06:56 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Rosenbaum", "Clemens", ""], ["Gao", "Tian", ""], ["Klinger", "Tim", ""]]}, {"id": "1708.01791", "submitter": "Ole-Christoffer Granmo", "authors": "Sondre Glimsdal, Ole-Christoffer Granmo", "title": "Thompson Sampling Guided Stochastic Searching on the Line for Deceptive\n  Environments with Applications to Root-Finding Problems", "comments": "17 pages, 2 figures. A preliminary version of some of the results of\n  this paper appears in the Proceedings of AIAI'15", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The multi-armed bandit problem forms the foundation for solving a wide range\nof on-line stochastic optimization problems through a simple, yet effective\nmechanism. One simply casts the problem as a gambler that repeatedly pulls one\nout of N slot machine arms, eliciting random rewards. Learning of reward\nprobabilities is then combined with reward maximization, by carefully balancing\nreward exploration against reward exploitation. In this paper, we address a\nparticularly intriguing variant of the multi-armed bandit problem, referred to\nas the {\\it Stochastic Point Location (SPL) Problem}. The gambler is here only\ntold whether the optimal arm (point) lies to the \"left\" or to the \"right\" of\nthe arm pulled, with the feedback being erroneous with probability $1-\\pi$.\nThis formulation thus captures optimization in continuous action spaces with\nboth {\\it informative} and {\\it deceptive} feedback. To tackle this class of\nproblems, we formulate a compact and scalable Bayesian representation of the\nsolution space that simultaneously captures both the location of the optimal\narm as well as the probability of receiving correct feedback. We further\nintroduce the accompanying Thompson Sampling guided Stochastic Point Location\n(TS-SPL) scheme for balancing exploration against exploitation. By learning\n$\\pi$, TS-SPL also supports {\\it deceptive} environments that are lying about\nthe direction of the optimal arm. This, in turn, allows us to solve the\nfundamental Stochastic Root Finding (SRF) Problem. Empirical results\ndemonstrate that our scheme deals with both deceptive and informative\nenvironments, significantly outperforming competing algorithms both for SRF and\nSPL.\n", "versions": [{"version": "v1", "created": "Sat, 5 Aug 2017 17:23:01 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Glimsdal", "Sondre", ""], ["Granmo", "Ole-Christoffer", ""]]}, {"id": "1708.01829", "submitter": "Roberto Rossi", "authors": "Roberto Rossi and \\\"Ozg\\\"ur Akg\\\"un and Steven Prestwich and S.\n  Armagan Tarim", "title": "Declarative Statistics", "comments": "The modeling framework and the examples used in this work are\n  available at https://gwr3n.github.io/syat-choco/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we introduce declarative statistics, a suite of declarative\nmodelling tools for statistical analysis. Statistical constraints represent the\nkey building block of declarative statistics. First, we introduce a range of\nrelevant counting and matrix constraints and associated decompositions, some of\nwhich novel, that are instrumental in the design of statistical constraints.\nSecond, we introduce a selection of novel statistical constraints and\nassociated decompositions, which constitute a self-contained toolbox that can\nbe used to tackle a wide range of problems typically encountered by\nstatisticians. Finally, we deploy these statistical constraints to a wide range\nof application areas drawn from classical statistics and we contrast our\nframework against established practices.\n", "versions": [{"version": "v1", "created": "Sun, 6 Aug 2017 01:25:30 GMT"}, {"version": "v2", "created": "Thu, 28 Dec 2017 17:51:49 GMT"}], "update_date": "2017-12-29", "authors_parsed": [["Rossi", "Roberto", ""], ["Akg\u00fcn", "\u00d6zg\u00fcr", ""], ["Prestwich", "Steven", ""], ["Tarim", "S. Armagan", ""]]}, {"id": "1708.01867", "submitter": "Felix Leibfried", "authors": "Felix Leibfried, Jordi Grau-Moya and Haitham Bou-Ammar", "title": "An Information-Theoretic Optimality Principle for Deep Reinforcement\n  Learning", "comments": "Presented at the NIPS Deep Reinforcement Learning Workshop, Montreal,\n  Canada, 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We methodologically address the problem of Q-value overestimation in deep\nreinforcement learning to handle high-dimensional state spaces efficiently. By\nadapting concepts from information theory, we introduce an intrinsic penalty\nsignal encouraging reduced Q-value estimates. The resultant algorithm\nencompasses a wide range of learning outcomes containing deep Q-networks as a\nspecial case. Different learning outcomes can be demonstrated by tuning a\nLagrange multiplier accordingly. We furthermore propose a novel scheduling\nscheme for this Lagrange multiplier to ensure efficient and robust learning. In\nexperiments on Atari, our algorithm outperforms other algorithms (e.g. deep and\ndouble deep Q-networks) in terms of both game-play performance and sample\ncomplexity. These results remain valid under the recently proposed dueling\narchitecture.\n", "versions": [{"version": "v1", "created": "Sun, 6 Aug 2017 09:23:22 GMT"}, {"version": "v2", "created": "Wed, 8 Nov 2017 16:27:50 GMT"}, {"version": "v3", "created": "Thu, 8 Feb 2018 14:07:53 GMT"}, {"version": "v4", "created": "Thu, 6 Sep 2018 09:27:21 GMT"}, {"version": "v5", "created": "Tue, 20 Nov 2018 11:55:21 GMT"}], "update_date": "2018-11-21", "authors_parsed": [["Leibfried", "Felix", ""], ["Grau-Moya", "Jordi", ""], ["Bou-Ammar", "Haitham", ""]]}, {"id": "1708.01911", "submitter": "Thomas Kurbiel", "authors": "Thomas Kurbiel and Shahrzad Khaleghian", "title": "Training of Deep Neural Networks based on Distance Measures using\n  RMSProp", "comments": "6 pages, 14 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vanishing gradient problem was a major obstacle for the success of deep\nlearning. In recent years it was gradually alleviated through multiple\ndifferent techniques. However the problem was not really overcome in a\nfundamental way, since it is inherent to neural networks with activation\nfunctions based on dot products. In a series of papers, we are going to analyze\nalternative neural network structures which are not based on dot products. In\nthis first paper, we revisit neural networks built up of layers based on\ndistance measures and Gaussian activation functions. These kinds of networks\nwere only sparsely used in the past since they are hard to train when using\nplain stochastic gradient descent methods. We show that by using Root Mean\nSquare Propagation (RMSProp) it is possible to efficiently learn multi-layer\nneural networks. Furthermore we show that when appropriately initialized these\nkinds of neural networks suffer much less from the vanishing and exploding\ngradient problem than traditional neural networks even for deep networks.\n", "versions": [{"version": "v1", "created": "Sun, 6 Aug 2017 17:10:38 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Kurbiel", "Thomas", ""], ["Khaleghian", "Shahrzad", ""]]}, {"id": "1708.01925", "submitter": "Muaz Niazi", "authors": "Faisal Riaz, Muaz A. Niazi", "title": "Designing Autonomous Vehicles: Evaluating the Role of Human Emotions and\n  Social Norms", "comments": "42 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.CY cs.RO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans are going to delegate the rights of driving to the autonomous vehicles\nin near future. However, to fulfill this complicated task, there is a need for\na mechanism, which enforces the autonomous vehicles to obey the road and social\nrules that have been practiced by well-behaved drivers. This task can be\nachieved by introducing social norms compliance mechanism in the autonomous\nvehicles. This research paper is proposing an artificial society of autonomous\nvehicles as an analogy of human social society. Each AV has been assigned a\nsocial personality having different social influence. Social norms have been\nintroduced which help the AVs in making the decisions, influenced by emotions,\nregarding road collision avoidance. Furthermore, social norms compliance\nmechanism, by artificial social AVs, has been proposed using prospect based\nemotion i.e. fear, which is conceived from OCC model. Fuzzy logic has been\nemployed to compute the emotions quantitatively. Then, using SimConnect\napproach, fuzzy values of fear has been provided to the Netlogo simulation\nenvironment to simulate artificial society of AVs. Extensive testing has been\nperformed using the behavior space tool to find out the performance of the\nproposed approach in terms of the number of collisions. For comparison, the\nrandom-walk model based artificial society of AVs has been proposed as well. A\ncomparative study with a random walk, prove that proposed approach provides a\nbetter option to tailor the autopilots of future AVS, Which will be more\nsocially acceptable and trustworthy by their riders in terms of safe road\ntravel.\n", "versions": [{"version": "v1", "created": "Sun, 6 Aug 2017 19:24:00 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Riaz", "Faisal", ""], ["Niazi", "Muaz A.", ""]]}, {"id": "1708.01927", "submitter": "Muaz Niazi", "authors": "Faisal Riaz, Muaz A. Niazi", "title": "Emotion Controlled Spectrum Mobility Scheme for Efficient Syntactic\n  Interoperability In Cognitive Radio Based Internet of Vehicles", "comments": "33 pages, 16 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI cs.GT cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Blind spots are one of the causes of road accidents in the hilly and flat\nareas. These blind spot accidents can be decreased by establishing an Internet\nof Vehicles (IoV) using Vehicle-2-Vehicle (V2V) and Vehicle-2-Infrastrtructure\n(V2I) communication systems. But the problem with these IoV is that most of\nthem are using DSRC or single Radio Access Technology (RAT) as a wireless\ntechnology, which has been proven to be failed for efficient communication\nbetween vehicles. Recently, Cognitive Radio (CR) based IoV have to be proven\nbest wireless communication systems for vehicular networks. However, the\nspectrum mobility is a challenging task to keep CR based vehicular networks\ninteroperable and has not been addressed sufficiently in existing research. In\nour previous research work, the Cognitive Radio Site (CR-Site) has been\nproposed as in-vehicle CR-device, which can be utilized to establish efficient\nIoV systems. H In this paper, we have introduced the Emotions Inspired\nCognitive Agent (EIC_Agent) based spectrum mobility mechanism in CR-Site and\nproposed a novel emotions controlled spectrum mobility scheme for efficient\nsyntactic interoperability between vehicles. For this purpose, a probabilistic\ndeterministic finite automaton using fear factor is proposed to perform\nefficient spectrum mobility using fuzzy logic. In addition, the quantitative\ncomputation of different fear intensity levels has been performed with the help\nof fuzzy logic. The system has been tested using active data from different GSM\nservice providers on Mangla-Mirpur road. This is supplemented by extensive\nsimulation experiments which validate the proposed scheme for CR based\nhigh-speed vehicular networks. The qualitative comparison with the\nexisting-state-of the-art has proven the superiority of the proposed emotions\ncontrolled syntactic interoperable spectrum mobility scheme within cognitive\nradio based IoV systems.\n", "versions": [{"version": "v1", "created": "Sun, 6 Aug 2017 19:40:50 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Riaz", "Faisal", ""], ["Niazi", "Muaz A.", ""]]}, {"id": "1708.01930", "submitter": "Muaz Niazi", "authors": "Faisal Riaz, Muaz A. Niazi", "title": "Enhanced Emotion Enabled Cognitive Agent Based Rear End Collision\n  Avoidance Controller for Autonomous Vehicles", "comments": "39 pages, 17 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA cs.RO cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rear end collisions are deadliest in nature and cause most of traffic\ncasualties and injuries. In the existing research, many rear end collision\navoidance solutions have been proposed. However, the problem with these\nproposed solutions is that they are highly dependent on precise mathematical\nmodels. Whereas, the real road driving is influenced by non-linear factors such\nas road surface situations, driver reaction time, pedestrian flow and vehicle\ndynamics, hence obtaining the accurate mathematical model of the vehicle\ncontrol system is challenging. This problem with precise control based rear end\ncollision avoidance schemes has been addressed using fuzzy logic, but the\nexcessive number of fuzzy rules straightforwardly prejudice their efficiency.\nFurthermore, these fuzzy logic based controllers have been proposed without\nusing proper agent based modeling that helps in mimicking the functions of an\nartificial human driver executing these fuzzy rules. Keeping in view these\nlimitations, we have proposed an Enhanced Emotion Enabled Cognitive Agent\n(EEEC_Agent) based controller that helps the Autonomous Vehicles (AVs) to\nperform rear end collision avoidance with less number of rules, designed after\nfear emotion, and high efficiency. To introduce a fear emotion generation\nmechanism in EEEC_Agent, Orton, Clore & Collins (OCC) model has been employed.\nThe fear generation mechanism of EEEC_Agent has been verified using NetLogo\nsimulation. Furthermore, practical validation of EEEC_Agent functions has been\nperformed using specially built prototype AV platform. Eventually, the\nqualitative comparative study with existing state of the art research works\nreflect that proposed model outperforms recent research.\n", "versions": [{"version": "v1", "created": "Sun, 6 Aug 2017 19:54:05 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Riaz", "Faisal", ""], ["Niazi", "Muaz A.", ""]]}, {"id": "1708.01931", "submitter": "Muaz Niazi", "authors": "Faisal Riaz, Muaz A. Niazi", "title": "Towards Social Autonomous Vehicles: Efficient Collision Avoidance Scheme\n  Using Richardson's Arms Race Model", "comments": "48 pages, 21 figures", "journal-ref": "PLoS ONE12(10): e0186103 (2017)", "doi": "10.1371/journal.pone.0186103", "report-no": null, "categories": "cs.AI cs.HC cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Background Road collisions and casualties pose a serious threat to commuters\naround the globe. Autonomous Vehicles (AVs) aim to make the use of technology\nto reduce the road accidents. However, the most of research work in the context\nof collision avoidance has been performed to address, separately, the rear end,\nfront end and lateral collisions in less congested and with high\ninter-vehicular distances. Purpose The goal of this paper is to introduce the\nconcept of a social agent, which interact with other AVs in social manners like\nhumans are social having the capability of predicting intentions, i.e.\nmentalizing and copying the actions of each other, i.e. mirroring. The proposed\nsocial agent is based on a human-brain inspired mentalizing and mirroring\ncapabilities and has been modelled for collision detection and avoidance under\ncongested urban road traffic.\n  Method We designed our social agent having the capabilities of mentalizing\nand mirroring and for this purpose we utilized Exploratory Agent Based Modeling\n(EABM) level of Cognitive Agent Based Computing (CABC) framework proposed by\nNiazi and Hussain.\n  Results Our simulation and practical experiments reveal that by embedding\nRichardson's arms race model within AVs, collisions can be avoided while\ntravelling on congested urban roads in a flock like topologies. The performance\nof the proposed social agent has been compared at two different levels.\n", "versions": [{"version": "v1", "created": "Sun, 6 Aug 2017 20:07:14 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Riaz", "Faisal", ""], ["Niazi", "Muaz A.", ""]]}, {"id": "1708.01967", "submitter": "Kai Shu", "authors": "Kai Shu, Amy Sliva, Suhang Wang, Jiliang Tang, Huan Liu", "title": "Fake News Detection on Social Media: A Data Mining Perspective", "comments": "ACM SIGKDD Explorations Newsletter, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Social media for news consumption is a double-edged sword. On the one hand,\nits low cost, easy access, and rapid dissemination of information lead people\nto seek out and consume news from social media. On the other hand, it enables\nthe wide spread of \"fake news\", i.e., low quality news with intentionally false\ninformation. The extensive spread of fake news has the potential for extremely\nnegative impacts on individuals and society. Therefore, fake news detection on\nsocial media has recently become an emerging research that is attracting\ntremendous attention. Fake news detection on social media presents unique\ncharacteristics and challenges that make existing detection algorithms from\ntraditional news media ineffective or not applicable. First, fake news is\nintentionally written to mislead readers to believe false information, which\nmakes it difficult and nontrivial to detect based on news content; therefore,\nwe need to include auxiliary information, such as user social engagements on\nsocial media, to help make a determination. Second, exploiting this auxiliary\ninformation is challenging in and of itself as users' social engagements with\nfake news produce data that is big, incomplete, unstructured, and noisy.\nBecause the issue of fake news detection on social media is both challenging\nand relevant, we conducted this survey to further facilitate research on the\nproblem. In this survey, we present a comprehensive review of detecting fake\nnews on social media, including fake news characterizations on psychology and\nsocial theories, existing algorithms from a data mining perspective, evaluation\nmetrics and representative datasets. We also discuss related research areas,\nopen problems, and future research directions for fake news detection on social\nmedia.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 02:29:09 GMT"}, {"version": "v2", "created": "Wed, 9 Aug 2017 00:25:03 GMT"}, {"version": "v3", "created": "Sun, 3 Sep 2017 02:40:05 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Shu", "Kai", ""], ["Sliva", "Amy", ""], ["Wang", "Suhang", ""], ["Tang", "Jiliang", ""], ["Liu", "Huan", ""]]}, {"id": "1708.02072", "submitter": "Ronald Kemker", "authors": "Ronald Kemker, Marc McClure, Angelina Abitino, Tyler Hayes, and\n  Christopher Kanan", "title": "Measuring Catastrophic Forgetting in Neural Networks", "comments": "To appear in AAAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are used in many state-of-the-art systems for machine\nperception. Once a network is trained to do a specific task, e.g., bird\nclassification, it cannot easily be trained to do new tasks, e.g.,\nincrementally learning to recognize additional bird species or learning an\nentirely different task such as flower recognition. When new tasks are added,\ntypical deep neural networks are prone to catastrophically forgetting previous\ntasks. Networks that are capable of assimilating new information incrementally,\nmuch like how humans form new memories over time, will be more efficient than\nre-training the model from scratch each time a new task needs to be learned.\nThere have been multiple attempts to develop schemes that mitigate catastrophic\nforgetting, but these methods have not been directly compared, the tests used\nto evaluate them vary considerably, and these methods have only been evaluated\non small-scale problems (e.g., MNIST). In this paper, we introduce new metrics\nand benchmarks for directly comparing five different mechanisms designed to\nmitigate catastrophic forgetting in neural networks: regularization,\nensembling, rehearsal, dual-memory, and sparse-coding. Our experiments on\nreal-world images and sounds show that the mechanism(s) that are critical for\noptimal performance vary based on the incremental training paradigm and type of\ndata being used, but they all demonstrate that the catastrophic forgetting\nproblem has yet to be solved.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 11:18:43 GMT"}, {"version": "v2", "created": "Tue, 8 Aug 2017 09:33:24 GMT"}, {"version": "v3", "created": "Mon, 11 Sep 2017 16:50:39 GMT"}, {"version": "v4", "created": "Thu, 9 Nov 2017 14:53:07 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Kemker", "Ronald", ""], ["McClure", "Marc", ""], ["Abitino", "Angelina", ""], ["Hayes", "Tyler", ""], ["Kanan", "Christopher", ""]]}, {"id": "1708.02139", "submitter": "Zeming Lin", "authors": "Zeming Lin, Jonas Gehring, Vasil Khalidov, Gabriel Synnaeve", "title": "STARDATA: A StarCraft AI Research Dataset", "comments": "To be presented at AIIDE17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We release a dataset of 65646 StarCraft replays that contains 1535 million\nframes and 496 million player actions. We provide full game state data along\nwith the original replays that can be viewed in StarCraft. The game state data\nwas recorded every 3 frames which ensures suitability for a wide variety of\nmachine learning tasks such as strategy classification, inverse reinforcement\nlearning, imitation learning, forward modeling, partial information extraction,\nand others. We use TorchCraft to extract and store the data, which standardizes\nthe data format for both reading from replays and reading directly from the\ngame. Furthermore, the data can be used on different operating systems and\nplatforms. The dataset contains valid, non-corrupted replays only and its\nquality and diversity was ensured by a number of heuristics. We illustrate the\ndiversity of the data with various statistics and provide examples of tasks\nthat benefit from the dataset. We make the dataset available at\nhttps://github.com/TorchCraft/StarData . En Taro Adun!\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 14:47:47 GMT"}], "update_date": "2017-10-03", "authors_parsed": [["Lin", "Zeming", ""], ["Gehring", "Jonas", ""], ["Khalidov", "Vasil", ""], ["Synnaeve", "Gabriel", ""]]}, {"id": "1708.02153", "submitter": "Jakub Sliwinski", "authors": "Jakub Sliwinski, Martin Strobel, Yair Zick", "title": "Axiomatic Characterization of Data-Driven Influence Measures for\n  Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the following problem: given a labeled dataset and a specific\ndatapoint x, how did the i-th feature influence the classification for x? We\nidentify a family of numerical influence measures - functions that, given a\ndatapoint x, assign a numeric value phi_i(x) to every feature i, corresponding\nto how altering i's value would influence the outcome for x. This family, which\nwe term monotone influence measures (MIM), is uniquely derived from a set of\ndesirable properties, or axioms. The MIM family constitutes a provably sound\nmethodology for measuring feature influence in classification domains; the\nvalues generated by MIM are based on the dataset alone, and do not make any\nqueries to the classifier. While this requirement naturally limits the scope of\nour framework, we demonstrate its effectiveness on data.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 15:09:01 GMT"}, {"version": "v2", "created": "Thu, 15 Nov 2018 09:35:51 GMT"}], "update_date": "2018-11-16", "authors_parsed": [["Sliwinski", "Jakub", ""], ["Strobel", "Martin", ""], ["Zick", "Yair", ""]]}, {"id": "1708.02167", "submitter": "Wen Shen", "authors": "Wen Shen, Alanoud Al Khemeiri, Abdulla Almehrezi, Wael Al Enezi, Iyad\n  Rahwan, Jacob W. Crandall", "title": "Regulating Highly Automated Robot Ecologies: Insights from Three User\n  Studies", "comments": "10 pages, 7 figures, to appear in the 5th International Conference on\n  Human Agent Interaction (HAI-2017), Bielefeld, Germany", "journal-ref": "In Proceedings of the 5th International Conference on Human Agent\n  Interaction (HAI 2017). ACM, New York, NY, USA, 111-120", "doi": "10.1145/3125739.3125758", "report-no": null, "categories": "cs.AI cs.CY cs.HC", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Highly automated robot ecologies (HARE), or societies of independent\nautonomous robots or agents, are rapidly becoming an important part of much of\nthe world's critical infrastructure. As with human societies, regulation,\nwherein a governing body designs rules and processes for the society, plays an\nimportant role in ensuring that HARE meet societal objectives. However, to\ndate, a careful study of interactions between a regulator and HARE is lacking.\nIn this paper, we report on three user studies which give insights into how to\ndesign systems that allow people, acting as the regulatory authority, to\neffectively interact with HARE. As in the study of political systems in which\ngovernments regulate human societies, our studies analyze how interactions\nbetween HARE and regulators are impacted by regulatory power and individual\n(robot or agent) autonomy. Our results show that regulator power, decision\nsupport, and adaptive autonomy can each diminish the social welfare of HARE,\nand hint at how these seemingly desirable mechanisms can be designed so that\nthey become part of successful HARE.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 15:28:09 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Shen", "Wen", ""], ["Khemeiri", "Alanoud Al", ""], ["Almehrezi", "Abdulla", ""], ["Enezi", "Wael Al", ""], ["Rahwan", "Iyad", ""], ["Crandall", "Jacob W.", ""]]}, {"id": "1708.02188", "submitter": "Ulrich Finkler", "authors": "Minsik Cho, Ulrich Finkler, Sameer Kumar, David Kung, Vaibhav Saxena,\n  Dheeraj Sreedhar", "title": "PowerAI DDL", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As deep neural networks become more complex and input datasets grow larger,\nit can take days or even weeks to train a deep neural network to the desired\naccuracy. Therefore, distributed Deep Learning at a massive scale is a critical\ncapability, since it offers the potential to reduce the training time from\nweeks to hours. In this paper, we present a software-hardware co-optimized\ndistributed Deep Learning system that can achieve near-linear scaling up to\nhundreds of GPUs. The core algorithm is a multi-ring communication pattern that\nprovides a good tradeoff between latency and bandwidth and adapts to a variety\nof system configurations. The communication algorithm is implemented as a\nlibrary for easy use. This library has been integrated into Tensorflow, Caffe,\nand Torch. We train Resnet-101 on Imagenet 22K with 64 IBM Power8 S822LC\nservers (256 GPUs) in about 7 hours to an accuracy of 33.8 % validation\naccuracy. Microsoft's ADAM and Google's DistBelief results did not reach 30 %\nvalidation accuracy for Imagenet 22K. Compared to Facebook AI Research's recent\npaper on 256 GPU training, we use a different communication algorithm, and our\ncombined software and hardware system offers better communication overhead for\nResnet-50. A PowerAI DDL enabled version of Torch completed 90 epochs of\ntraining on Resnet 50 for 1K classes in 50 minutes using 64 IBM Power8 S822LC\nservers (256 GPUs).\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 16:24:00 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Cho", "Minsik", ""], ["Finkler", "Ulrich", ""], ["Kumar", "Sameer", ""], ["Kung", "David", ""], ["Saxena", "Vaibhav", ""], ["Sreedhar", "Dheeraj", ""]]}, {"id": "1708.02190", "submitter": "S\\'ebastien Forestier", "authors": "S\\'ebastien Forestier, R\\'emy Portelas, Yoan Mollard, Pierre-Yves\n  Oudeyer", "title": "Intrinsically Motivated Goal Exploration Processes with Automatic\n  Curriculum Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intrinsically motivated spontaneous exploration is a key enabler of\nautonomous lifelong learning in human children. It enables the discovery and\nacquisition of large repertoires of skills through self-generation,\nself-selection, self-ordering and self-experimentation of learning goals. We\npresent an algorithmic approach called Intrinsically Motivated Goal Exploration\nProcesses (IMGEP) to enable similar properties of autonomous or self-supervised\nlearning in machines. The IMGEP algorithmic architecture relies on several\nprinciples: 1) self-generation of goals, generalized as fitness functions; 2)\nselection of goals based on intrinsic rewards; 3) exploration with incremental\ngoal-parameterized policy search and exploitation of the gathered data with a\nbatch learning algorithm; 4) systematic reuse of information acquired when\ntargeting a goal for improving towards other goals. We present a particularly\nefficient form of IMGEP, called Modular Population-Based IMGEP, that uses a\npopulation-based policy and an object-centered modularity in goals and\nmutations. We provide several implementations of this architecture and\ndemonstrate their ability to automatically generate a learning curriculum\nwithin several experimental setups including a real humanoid robot that can\nexplore multiple spaces of goals with several hundred continuous dimensions.\nWhile no particular target goal is provided to the system, this curriculum\nallows the discovery of skills that act as stepping stone for learning more\ncomplex skills, e.g. nested tool use. We show that learning diverse spaces of\ngoals with intrinsic motivations is more efficient for learning complex skills\nthan only trying to directly learn these complex skills.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 16:32:39 GMT"}, {"version": "v2", "created": "Fri, 24 Jul 2020 10:12:16 GMT"}], "update_date": "2020-07-27", "authors_parsed": [["Forestier", "S\u00e9bastien", ""], ["Portelas", "R\u00e9my", ""], ["Mollard", "Yoan", ""], ["Oudeyer", "Pierre-Yves", ""]]}, {"id": "1708.02191", "submitter": "Kihyuk Sohn", "authors": "Kihyuk Sohn, Sifei Liu, Guangyu Zhong, Xiang Yu, Ming-Hsuan Yang,\n  Manmohan Chandraker", "title": "Unsupervised Domain Adaptation for Face Recognition in Unlabeled Videos", "comments": "accepted for publication at International Conference on Computer\n  Vision (ICCV) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite rapid advances in face recognition, there remains a clear gap between\nthe performance of still image-based face recognition and video-based face\nrecognition, due to the vast difference in visual quality between the domains\nand the difficulty of curating diverse large-scale video datasets. This paper\naddresses both of those challenges, through an image to video feature-level\ndomain adaptation approach, to learn discriminative video frame\nrepresentations. The framework utilizes large-scale unlabeled video data to\nreduce the gap between different domains while transferring discriminative\nknowledge from large-scale labeled still images. Given a face recognition\nnetwork that is pretrained in the image domain, the adaptation is achieved by\n(i) distilling knowledge from the network to a video adaptation network through\nfeature matching, (ii) performing feature restoration through synthetic data\naugmentation and (iii) learning a domain-invariant feature through a domain\nadversarial discriminator. We further improve performance through a\ndiscriminator-guided feature fusion that boosts high-quality frames while\neliminating those degraded by video domain-specific factors. Experiments on the\nYouTube Faces and IJB-A datasets demonstrate that each module contributes to\nour feature-level domain adaptation framework and substantially improves video\nface recognition performance to achieve state-of-the-art accuracy. We\ndemonstrate qualitatively that the network learns to suppress diverse artifacts\nin videos such as pose, illumination or occlusion without being explicitly\ntrained for them.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 16:36:54 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Sohn", "Kihyuk", ""], ["Liu", "Sifei", ""], ["Zhong", "Guangyu", ""], ["Yu", "Xiang", ""], ["Yang", "Ming-Hsuan", ""], ["Chandraker", "Manmohan", ""]]}, {"id": "1708.02254", "submitter": "Cristian Danescu-Niculescu-Mizil", "authors": "Justine Zhang, Arthur Spirling, Cristian Danescu-Niculescu-Mizil", "title": "Asking Too Much? The Rhetorical Role of Questions in Political Discourse", "comments": "To appear at EMNLP 2017; 15 pages including appendix; 3 figures;\n  parliament data and code available at\n  http://www.cs.cornell.edu/~cristian/Asking_too_much.html", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CY cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Questions play a prominent role in social interactions, performing rhetorical\nfunctions that go beyond that of simple informational exchange. The surface\nform of a question can signal the intention and background of the person asking\nit, as well as the nature of their relation with the interlocutor. While the\ninformational nature of questions has been extensively examined in the context\nof question-answering applications, their rhetorical aspects have been largely\nunderstudied.\n  In this work we introduce an unsupervised methodology for extracting surface\nmotifs that recur in questions, and for grouping them according to their latent\nrhetorical role. By applying this framework to the setting of question sessions\nin the UK parliament, we show that the resulting typology encodes key aspects\nof the political discourse---such as the bifurcation in questioning behavior\nbetween government and opposition parties---and reveals new insights into the\neffects of a legislator's tenure and political career ambitions.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 18:00:32 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Zhang", "Justine", ""], ["Spirling", "Arthur", ""], ["Danescu-Niculescu-Mizil", "Cristian", ""]]}, {"id": "1708.02255", "submitter": "Eita Nakamura", "authors": "Hiroaki Tsushima, Eita Nakamura, Katsutoshi Itoyama, Kazuyoshi Yoshii", "title": "Generative Statistical Models with Self-Emergent Grammar of Chord\n  Sequences", "comments": "22 pages, 14 figures, version accepted to JNMR, minor revision", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative statistical models of chord sequences play crucial roles in music\nprocessing. To capture syntactic similarities among certain chords (e.g. in C\nmajor key, between G and G7 and between F and Dm), we study hidden Markov\nmodels and probabilistic context-free grammar models with latent variables\ndescribing syntactic categories of chord symbols and their unsupervised\nlearning techniques for inducing the latent grammar from data. Surprisingly, we\nfind that these models often outperform conventional Markov models in\npredictive power, and the self-emergent categories often correspond to\ntraditional harmonic functions. This implies the need for chord categories in\nharmony models from the informatics perspective.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 18:00:42 GMT"}, {"version": "v2", "created": "Tue, 15 Aug 2017 01:36:14 GMT"}, {"version": "v3", "created": "Fri, 2 Mar 2018 14:54:25 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Tsushima", "Hiroaki", ""], ["Nakamura", "Eita", ""], ["Itoyama", "Katsutoshi", ""], ["Yoshii", "Kazuyoshi", ""]]}, {"id": "1708.02300", "submitter": "Ramakanth Pasunuru", "authors": "Ramakanth Pasunuru, Mohit Bansal", "title": "Reinforced Video Captioning with Entailment Rewards", "comments": "EMNLP 2017 (9 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sequence-to-sequence models have shown promising improvements on the temporal\ntask of video captioning, but they optimize word-level cross-entropy loss\nduring training. First, using policy gradient and mixed-loss methods for\nreinforcement learning, we directly optimize sentence-level task-based metrics\n(as rewards), achieving significant improvements over the baseline, based on\nboth automatic metrics and human evaluation on multiple datasets. Next, we\npropose a novel entailment-enhanced reward (CIDEnt) that corrects\nphrase-matching based metrics (such as CIDEr) to only allow for\nlogically-implied partial matches and avoid contradictions, achieving further\nsignificant improvements over the CIDEr-reward model. Overall, our\nCIDEnt-reward model achieves the new state-of-the-art on the MSR-VTT dataset.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 20:50:24 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Pasunuru", "Ramakanth", ""], ["Bansal", "Mohit", ""]]}, {"id": "1708.02312", "submitter": "Yixin Nie", "authors": "Yixin Nie, Mohit Bansal", "title": "Shortcut-Stacked Sentence Encoders for Multi-Domain Inference", "comments": "EMNLP 2017 RepEval Multi-NLI Shared Task (6 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a simple sequential sentence encoder for multi-domain natural\nlanguage inference. Our encoder is based on stacked bidirectional LSTM-RNNs\nwith shortcut connections and fine-tuning of word embeddings. The overall\nsupervised model uses the above encoder to encode two input sentences into two\nvectors, and then uses a classifier over the vector combination to label the\nrelationship between these two sentences as that of entailment, contradiction,\nor neural. Our Shortcut-Stacked sentence encoders achieve strong improvements\nover existing encoders on matched and mismatched multi-domain natural language\ninference (top non-ensemble single-model result in the EMNLP RepEval 2017\nShared Task (Nangia et al., 2017)). Moreover, they achieve the new\nstate-of-the-art encoding result on the original SNLI dataset (Bowman et al.,\n2015).\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 21:33:11 GMT"}, {"version": "v2", "created": "Tue, 28 Nov 2017 18:15:47 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Nie", "Yixin", ""], ["Bansal", "Mohit", ""]]}, {"id": "1708.02314", "submitter": "Veeru Talreja", "authors": "Veeru Talreja, Matthew C. Valenti, Nasser M. Nasrabadi", "title": "Multibiometric Secure System Based on Deep Learning", "comments": "To be published in Proc. IEEE Global SIP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a secure multibiometric system that uses deep\nneural networks and error-correction coding. We present a feature-level fusion\nframework to generate a secure multibiometric template from each user's\nmultiple biometrics. Two fusion architectures, fully connected architecture and\nbilinear architecture, are implemented to develop a robust multibiometric\nshared representation. The shared representation is used to generate a\ncancelable biometric template that involves the selection of a different set of\nreliable and discriminative features for each user. This cancelable template is\na binary vector and is passed through an appropriate error-correcting decoder\nto find a closest codeword and this codeword is hashed to generate the final\nsecure template. The efficacy of the proposed approach is shown using a\nmultimodal database where we achieve state-of-the-art matching performance,\nalong with cancelability and security.\n", "versions": [{"version": "v1", "created": "Mon, 7 Aug 2017 21:35:26 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Talreja", "Veeru", ""], ["Valenti", "Matthew C.", ""], ["Nasrabadi", "Nasser M.", ""]]}, {"id": "1708.02357", "submitter": "Muaz Niazi", "authors": "Muaz A. Niazi", "title": "Towards A Novel Unified Framework for Developing Formal, Network and\n  Validated Agent-Based Simulation Models of Complex Adaptive Systems", "comments": "PhD Thesis, University of Stirling, Scotland, UK. (2011), 93 figures,\n  23 tables, 292 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.NI cs.SI nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Literature on the modeling and simulation of complex adaptive systems (cas)\nhas primarily advanced vertically in different scientific domains with\nscientists developing a variety of domain-specific approaches and applications.\nHowever, while cas researchers are inher-ently interested in an\ninterdisciplinary comparison of models, to the best of our knowledge, there is\ncurrently no single unified framework for facilitating the development,\ncomparison, communication and validation of models across different scientific\ndomains. In this thesis, we propose first steps towards such a unified\nframework using a combination of agent-based and complex network-based modeling\napproaches and guidelines formulated in the form of a set of four levels of\nusage, which allow multidisciplinary researchers to adopt a suitable framework\nlevel on the basis of available data types, their research study objectives and\nexpected outcomes, thus allowing them to better plan and conduct their\nrespective re-search case studies.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 02:38:18 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Niazi", "Muaz A.", ""]]}, {"id": "1708.02361", "submitter": "Muaz Niazi", "authors": "Muaz A. Niazi, Amir Hussain, Mario Kolberg", "title": "Verification & Validation of Agent Based Simulations using the VOMAS\n  (Virtual Overlay Multi-agent System) approach", "comments": "7 pages, 5 figures, cite as Muaz Niazi, Amir Hussain and Mario\n  Kolberg , Verification and Validation of Agent-Based Simulation using the\n  VOMAS approach, Proceedings of the Third Workshop on Multi-Agent Systems and\n  Simulation'09 (MASS '09), as part of MALLOW 09, Sep 7-11, 2009, Torino, Italy", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.SE nlin.AO nlin.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Agent Based Models are very popular in a number of different areas. For\nexample, they have been used in a range of domains ranging from modeling of\ntumor growth, immune systems, molecules to models of social networks, crowds\nand computer and mobile self-organizing networks. One reason for their success\nis their intuitiveness and similarity to human cognition. However, with this\npower of abstraction, in spite of being easily applicable to such a wide number\nof domains, it is hard to validate agent-based models. In addition, building\nvalid and credible simulations is not just a challenging task but also a\ncrucial exercise to ensure that what we are modeling is, at some level of\nabstraction, a model of our conceptual system; the system that we have in mind.\nIn this paper, we address this important area of validation of agent based\nmodels by presenting a novel technique which has broad applicability and can be\napplied to all kinds of agent-based models. We present a framework, where a\nvirtual overlay multi-agent system can be used to validate simulation models.\nIn addition, since agent-based models have been typically growing, in parallel,\nin multiple domains, to cater for all of these, we present a new single\nvalidation technique applicable to all agent based models. Our technique, which\nallows for the validation of agent based simulations uses VOMAS: a Virtual\nOverlay Multi-agent System. This overlay multi-agent system can comprise\nvarious types of agents, which form an overlay on top of the agent based\nsimulation model that needs to be validated. Other than being able to watch and\nlog, each of these agents contains clearly defined constraints, which, if\nviolated, can be logged in real time. To demonstrate its effectiveness, we show\nits broad applicability in a wide variety of simulation models ranging from\nsocial sciences to computer networks in spatial and non-spatial conceptual\nmodels.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 03:07:41 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Niazi", "Muaz A.", ""], ["Hussain", "Amir", ""], ["Kolberg", "Mario", ""]]}, {"id": "1708.02363", "submitter": "Ilias Flaounas", "authors": "Ilias Flaounas", "title": "Beyond the technical challenges for deploying Machine Learning solutions\n  in a software company", "comments": "Human in the Loop Machine Learning Workshop, International Conference\n  on Machine Learning, Sydney, Australia, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.SE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently software development companies started to embrace Machine Learning\n(ML) techniques for introducing a series of advanced functionality in their\nproducts such as personalisation of the user experience, improved search,\ncontent recommendation and automation. The technical challenges for tackling\nthese problems are heavily researched in literature. A less studied area is a\npragmatic approach to the role of humans in a complex modern industrial\nenvironment where ML based systems are developed. Key stakeholders affect the\nsystem from inception and up to operation and maintenance. Product managers\nwant to embed \"smart\" experiences for their users and drive the decisions on\nwhat should be built next; software engineers are challenged to build or\nutilise ML software tools that require skills that are well outside of their\ncomfort zone; legal and risk departments may influence design choices and data\naccess; operations teams are requested to maintain ML systems which are\nnon-stationary in their nature and change behaviour over time; and finally ML\npractitioners should communicate with all these stakeholders to successfully\nbuild a reliable system. This paper discusses some of the challenges we faced\nin Atlassian as we started investing more in the ML space.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 03:59:09 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Flaounas", "Ilias", ""]]}, {"id": "1708.02378", "submitter": "David Von Dollen", "authors": "David Von Dollen", "title": "Investigating Reinforcement Learning Agents for Continuous State Space\n  Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given an environment with continuous state spaces and discrete actions, we\ninvestigate using a Double Deep Q-learning Reinforcement Agent to find optimal\npolicies using the LunarLander-v2 OpenAI gym environment.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 05:44:12 GMT"}, {"version": "v2", "created": "Mon, 28 Aug 2017 22:16:08 GMT"}, {"version": "v3", "created": "Mon, 11 Mar 2019 20:17:09 GMT"}], "update_date": "2019-03-13", "authors_parsed": [["Von Dollen", "David", ""]]}, {"id": "1708.02383", "submitter": "Meng Fang", "authors": "Meng Fang, Yuan Li and Trevor Cohn", "title": "Learning how to Active Learn: A Deep Reinforcement Learning Approach", "comments": "To appear in EMNLP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active learning aims to select a small subset of data for annotation such\nthat a classifier learned on the data is highly accurate. This is usually done\nusing heuristic selection methods, however the effectiveness of such methods is\nlimited and moreover, the performance of heuristics varies between datasets. To\naddress these shortcomings, we introduce a novel formulation by reframing the\nactive learning as a reinforcement learning problem and explicitly learning a\ndata selection policy, where the policy takes the role of the active learning\nheuristic. Importantly, our method allows the selection policy learned using\nsimulation on one language to be transferred to other languages. We demonstrate\nour method using cross-lingual named entity recognition, observing uniform\nimprovements over traditional active learning.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 07:06:48 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Fang", "Meng", ""], ["Li", "Yuan", ""], ["Cohn", "Trevor", ""]]}, {"id": "1708.02531", "submitter": "Yuming Shen", "authors": "Yuming Shen, Li Liu, Ling Shao, Jingkuan Song", "title": "Deep Binaries: Encoding Semantic-Rich Cues for Efficient Textual-Visual\n  Cross Retrieval", "comments": "Accepted by ICCV 2017 as a conference paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cross-modal hashing is usually regarded as an effective technique for\nlarge-scale textual-visual cross retrieval, where data from different\nmodalities are mapped into a shared Hamming space for matching. Most of the\ntraditional textual-visual binary encoding methods only consider holistic image\nrepresentations and fail to model descriptive sentences. This renders existing\nmethods inappropriate to handle the rich semantics of informative cross-modal\ndata for quality textual-visual search tasks. To address the problem of hashing\ncross-modal data with semantic-rich cues, in this paper, a novel integrated\ndeep architecture is developed to effectively encode the detailed semantics of\ninformative images and long descriptive sentences, named as Textual-Visual Deep\nBinaries (TVDB). In particular, region-based convolutional networks with long\nshort-term memory units are introduced to fully explore image regional details\nwhile semantic cues of sentences are modeled by a text convolutional network.\nAdditionally, we propose a stochastic batch-wise training routine, where\nhigh-quality binary codes and deep encoding functions are efficiently optimized\nin an alternating manner. Experiments are conducted on three multimedia\ndatasets, i.e. Microsoft COCO, IAPR TC-12, and INRIA Web Queries, where the\nproposed TVDB model significantly outperforms state-of-the-art binary coding\nmethods in the task of cross-modal retrieval.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 15:46:16 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Shen", "Yuming", ""], ["Liu", "Li", ""], ["Shao", "Ling", ""], ["Song", "Jingkuan", ""]]}, {"id": "1708.02536", "submitter": "Sudeepa Roy", "authors": "Sudeepa Roy and Babak Salimi", "title": "A Framework for Inferring Causality from Multi-Relational Observational\n  Data using Conditional Independence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The study of causality or causal inference - how much a given treatment\ncausally affects a given outcome in a population - goes way beyond correlation\nor association analysis of variables, and is critical in making sound data\ndriven decisions and policies in a multitude of applications. The gold standard\nin causal inference is performing \"controlled experiments\", which often is not\npossible due to logistical or ethical reasons. As an alternative, inferring\ncausality on \"observational data\" based on the \"Neyman-Rubin potential outcome\nmodel\" has been extensively used in statistics, economics, and social sciences\nover several decades. In this paper, we present a formal framework for sound\ncausal analysis on observational datasets that are given as multiple relations\nand where the population under study is obtained by joining these base\nrelations. We study a crucial condition for inferring causality from\nobservational data, called the \"strong ignorability assumption\" (the treatment\nand outcome variables should be independent in the joined relation given the\nobserved covariates), using known conditional independences that hold in the\nbase relations. We also discuss how the structure of the conditional\nindependences in base relations given as graphical models help infer new\nconditional independences in the joined relation. The proposed framework\ncombines concepts from databases, statistics, and graphical models, and aims to\ninitiate new research directions spanning these fields to facilitate powerful\ndata-driven decisions in today's big data world.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 15:56:18 GMT"}], "update_date": "2017-08-09", "authors_parsed": [["Roy", "Sudeepa", ""], ["Salimi", "Babak", ""]]}, {"id": "1708.02544", "submitter": "Farnood Salehi", "authors": "Farnood Salehi, L. Elisa Celis and Patrick Thiran", "title": "Stochastic Optimization with Bandit Sampling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many stochastic optimization algorithms work by estimating the gradient of\nthe cost function on the fly by sampling datapoints uniformly at random from a\ntraining set. However, the estimator might have a large variance, which\ninadvertently slows down the convergence rate of the algorithms. One way to\nreduce this variance is to sample the datapoints from a carefully selected\nnon-uniform distribution. In this work, we propose a novel non-uniform sampling\napproach that uses the multi-armed bandit framework. Theoretically, we show\nthat our algorithm asymptotically approximates the optimal variance within a\nfactor of 3. Empirically, we show that using this datapoint-selection technique\nresults in a significant reduction in the convergence time and variance of\nseveral stochastic optimization algorithms such as SGD, SVRG and SAGA. This\napproach for sampling datapoints is general, and can be used in conjunction\nwith any algorithm that uses an unbiased gradient estimation -- we expect it to\nhave broad applicability beyond the specific examples explored in this work.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 16:15:26 GMT"}, {"version": "v2", "created": "Wed, 9 Aug 2017 13:20:18 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Salehi", "Farnood", ""], ["Celis", "L. Elisa", ""], ["Thiran", "Patrick", ""]]}, {"id": "1708.02553", "submitter": "Gopal P. Sarma", "authors": "Gopal P. Sarma and Nick J. Hay", "title": "Robust Computer Algebra, Theorem Proving, and Oracle AI", "comments": "15 pages, 3 figures", "journal-ref": "Informatica Vol. 41 No. 3 (2017)", "doi": null, "report-no": null, "categories": "cs.AI cs.SC", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In the context of superintelligent AI systems, the term \"oracle\" has two\nmeanings. One refers to modular systems queried for domain-specific tasks.\nAnother usage, referring to a class of systems which may be useful for\naddressing the value alignment and AI control problems, is a superintelligent\nAI system that only answers questions. The aim of this manuscript is to survey\ncontemporary research problems related to oracles which align with long-term\nresearch goals of AI safety. We examine existing question answering systems and\nargue that their high degree of architectural heterogeneity makes them poor\ncandidates for rigorous analysis as oracles. On the other hand, we identify\ncomputer algebra systems (CASs) as being primitive examples of domain-specific\noracles for mathematics and argue that efforts to integrate computer algebra\nsystems with theorem provers, systems which have largely been developed\nindependent of one another, provide a concrete set of problems related to the\nnotion of provable safety that has emerged in the AI safety community. We\nreview approaches to interfacing CASs with theorem provers, describe\nwell-defined architectural deficiencies that have been identified with CASs,\nand suggest possible lines of research and practical software projects for\nscientists interested in AI safety.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 16:35:40 GMT"}, {"version": "v2", "created": "Sun, 31 Dec 2017 18:43:40 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Sarma", "Gopal P.", ""], ["Hay", "Nick J.", ""]]}, {"id": "1708.02556", "submitter": "Tu Dinh Nguyen", "authors": "Quan Hoang, Tu Dinh Nguyen, Trung Le and Dinh Phung", "title": "Multi-Generator Generative Adversarial Nets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach to train the Generative Adversarial Nets (GANs)\nwith a mixture of generators to overcome the mode collapsing problem. The main\nintuition is to employ multiple generators, instead of using a single one as in\nthe original GAN. The idea is simple, yet proven to be extremely effective at\ncovering diverse data modes, easily overcoming the mode collapse and delivering\nstate-of-the-art results. A minimax formulation is able to establish among a\nclassifier, a discriminator, and a set of generators in a similar spirit with\nGAN. Generators create samples that are intended to come from the same\ndistribution as the training data, whilst the discriminator determines whether\nsamples are true data or generated by generators, and the classifier specifies\nwhich generator a sample comes from. The distinguishing feature is that\ninternal samples are created from multiple generators, and then one of them\nwill be randomly selected as final output similar to the mechanism of a\nprobabilistic mixture model. We term our method Mixture GAN (MGAN). We develop\ntheoretical analysis to prove that, at the equilibrium, the Jensen-Shannon\ndivergence (JSD) between the mixture of generators' distributions and the\nempirical data distribution is minimal, whilst the JSD among generators'\ndistributions is maximal, hence effectively avoiding the mode collapse. By\nutilizing parameter sharing, our proposed model adds minimal computational cost\nto the standard GAN, and thus can also efficiently scale to large-scale\ndatasets. We conduct extensive experiments on synthetic 2D data and natural\nimage databases (CIFAR-10, STL-10 and ImageNet) to demonstrate the superior\nperformance of our MGAN in achieving state-of-the-art Inception scores over\nlatest baselines, generating diverse and appealing recognizable objects at\ndifferent resolutions, and specializing in capturing different types of objects\nby generators.\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 16:48:35 GMT"}, {"version": "v2", "created": "Wed, 9 Aug 2017 01:00:49 GMT"}, {"version": "v3", "created": "Tue, 19 Sep 2017 03:24:03 GMT"}, {"version": "v4", "created": "Fri, 27 Oct 2017 23:54:26 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Hoang", "Quan", ""], ["Nguyen", "Tu Dinh", ""], ["Le", "Trung", ""], ["Phung", "Dinh", ""]]}, {"id": "1708.02596", "submitter": "Anusha Nagabandi", "authors": "Anusha Nagabandi, Gregory Kahn, Ronald S. Fearing, Sergey Levine", "title": "Neural Network Dynamics for Model-Based Deep Reinforcement Learning with\n  Model-Free Fine-Tuning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model-free deep reinforcement learning algorithms have been shown to be\ncapable of learning a wide range of robotic skills, but typically require a\nvery large number of samples to achieve good performance. Model-based\nalgorithms, in principle, can provide for much more efficient learning, but\nhave proven difficult to extend to expressive, high-capacity models such as\ndeep neural networks. In this work, we demonstrate that medium-sized neural\nnetwork models can in fact be combined with model predictive control (MPC) to\nachieve excellent sample complexity in a model-based reinforcement learning\nalgorithm, producing stable and plausible gaits to accomplish various complex\nlocomotion tasks. We also propose using deep neural network dynamics models to\ninitialize a model-free learner, in order to combine the sample efficiency of\nmodel-based approaches with the high task-specific performance of model-free\nmethods. We empirically demonstrate on MuJoCo locomotion tasks that our pure\nmodel-based approach trained on just random action data can follow arbitrary\ntrajectories with excellent sample efficiency, and that our hybrid algorithm\ncan accelerate model-free learning on high-speed benchmark tasks, achieving\nsample efficiency gains of 3-5x on swimmer, cheetah, hopper, and ant agents.\nVideos can be found at https://sites.google.com/view/mbmf\n", "versions": [{"version": "v1", "created": "Tue, 8 Aug 2017 18:13:03 GMT"}, {"version": "v2", "created": "Sat, 2 Dec 2017 02:04:21 GMT"}], "update_date": "2017-12-05", "authors_parsed": [["Nagabandi", "Anusha", ""], ["Kahn", "Gregory", ""], ["Fearing", "Ronald S.", ""], ["Levine", "Sergey", ""]]}, {"id": "1708.02654", "submitter": "EPTCS", "authors": "Hans van Ditmarsch, Michael Ian Hartley, Barteld Kooi, Jonathan\n  Welton, Joseph B.W. Yeo", "title": "Cheryl's Birthday", "comments": "In Proceedings TARK 2017, arXiv:1707.08250", "journal-ref": "EPTCS 251, 2017, pp. 1-9", "doi": "10.4204/EPTCS.251.1", "report-no": null, "categories": "cs.AI cs.GL cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present four logic puzzles and after that their solutions. Joseph Yeo\ndesigned 'Cheryl's Birthday'. Mike Hartley came up with a novel solution for\n'One Hundred Prisoners and a Light Bulb'. Jonathan Welton designed 'A Blind\nGuess' and 'Abby's Birthday'. Hans van Ditmarsch and Barteld Kooi authored the\npuzzlebook 'One Hundred Prisoners and a Light Bulb' that contains other\nknowledge puzzles, and that can also be found on the webpage\nhttp://personal.us.es/hvd/lightbulb.html dedicated to the book.\n", "versions": [{"version": "v1", "created": "Thu, 27 Jul 2017 07:44:49 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["van Ditmarsch", "Hans", ""], ["Hartley", "Michael Ian", ""], ["Kooi", "Barteld", ""], ["Welton", "Jonathan", ""], ["Yeo", "Joseph B. W.", ""]]}, {"id": "1708.02747", "submitter": "Na Li", "authors": "Na Li (CSTJF, DRUID), Arnaud Martin (DRUID), R\\'emi Estival (CSTJF)", "title": "An automatic water detection approach based on Dempster-Shafer theory\n  for multi spectral images", "comments": "20th International Conference on Information Fusion, Jul 2017, XI'AN,\n  China", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detection of surface water in natural environment via multi-spectral imagery\nhas been widely utilized in many fields, such land cover identification.\nHowever, due to the similarity of the spectra of water bodies, built-up areas,\napproaches based on high-resolution satellites sometimes confuse these\nfeatures. A popular direction to detect water is spectral index, often\nrequiring the ground truth to find appropriate thresholds manually. As for\ntraditional machine learning methods, they identify water merely via\ndifferences of spectra of various land covers, without taking specific\nproperties of spectral reflection into account. In this paper, we propose an\nautomatic approach to detect water bodies based on Dempster-Shafer theory,\ncombining supervised learning with specific property of water in spectral band\nin a fully unsupervised context. The benefits of our approach are twofold. On\nthe one hand, it performs well in mapping principle water bodies, including\nlittle streams and branches. On the other hand, it labels all objects usually\nconfused with water as `ignorance', including half-dry watery areas, built-up\nareas and semi-transparent clouds and shadows. `Ignorance' indicates not only\nlimitations of the spectral properties of water and supervised learning itself\nbut insufficiency of information from multi-spectral bands as well, providing\nvaluable information for further land cover classification.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 07:59:39 GMT"}, {"version": "v2", "created": "Mon, 4 Sep 2017 08:04:27 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Li", "Na", "", "CSTJF, DRUID"], ["Martin", "Arnaud", "", "DRUID"], ["Estival", "R\u00e9mi", "", "CSTJF"]]}, {"id": "1708.02838", "submitter": "Pieter Van Molle", "authors": "Pieter Van Molle, Tim Verbelen, Steven Bohez, Sam Leroux, Pieter\n  Simoens, Bart Dhoedt", "title": "Decoupled Learning of Environment Characteristics for Safe Exploration", "comments": "4 pages, 4 figures, ICML 2017 workshop on Reliable Machine Learning\n  in the Wild", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning is a proven technique for an agent to learn a task.\nHowever, when learning a task using reinforcement learning, the agent cannot\ndistinguish the characteristics of the environment from those of the task. This\nmakes it harder to transfer skills between tasks in the same environment.\nFurthermore, this does not reduce risk when training for a new task. In this\npaper, we introduce an approach to decouple the environment characteristics\nfrom the task-specific ones, allowing an agent to develop a sense of survival.\nWe evaluate our approach in an environment where an agent must learn a sequence\nof collection tasks, and show that decoupled learning allows for a safer\nutilization of prior knowledge.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 13:51:47 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Van Molle", "Pieter", ""], ["Verbelen", "Tim", ""], ["Bohez", "Steven", ""], ["Leroux", "Sam", ""], ["Simoens", "Pieter", ""], ["Dhoedt", "Bart", ""]]}, {"id": "1708.02851", "submitter": "Anthony Hunter", "authors": "Anthony Hunter", "title": "Measuring Inconsistency in Argument Graphs", "comments": "29 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There have been a number of developments in measuring inconsistency in\nlogic-based representations of knowledge. In contrast, the development of\ninconsistency measures for computational models of argument has been limited.\nTo address this shortcoming, this paper provides a general framework for\nmeasuring inconsistency in abstract argumentation, together with some proposals\nfor specific measures, and a consideration of measuring inconsistency in\nlogic-based instantiations of argument graphs, including a review of some\nexisting proposals and a consideration of how existing logic-based measures of\ninconsistency can be applied.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 14:02:51 GMT"}], "update_date": "2017-08-10", "authors_parsed": [["Hunter", "Anthony", ""]]}, {"id": "1708.02918", "submitter": "Volker Tresp", "authors": "Volker Tresp and Yunpu Ma", "title": "The Tensor Memory Hypothesis", "comments": "Presented at MLINI-2016 workshop, 2016 (arXiv:1701.01437) Report-no:\n  MLINI/2016/06", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss memory models which are based on tensor decompositions using\nlatent representations of entities and events. We show how episodic memory and\nsemantic memory can be realized and discuss how new memory traces can be\ngenerated from sensory input: Existing memories are the basis for perception\nand new memories are generated via perception. We relate our mathematical\napproach to the hippocampal memory indexing theory. We describe the first\ndetailed mathematical models for the complete processing pipeline from sensory\ninput and its semantic decoding, i.e., perception, to the formation of episodic\nand semantic memories and their declarative semantic decodings. Our main\nhypothesis is that perception includes an active semantic decoding process,\nwhich relies on latent representations of entities and predicates, and that\nepisodic and semantic memories depend on the same decoding process. We\ncontribute to the debate between the leading memory consolidation theories,\ni.e., the standard consolidation theory (SCT) and the multiple trace theory\n(MTT). The latter is closely related to the complementary learning systems\n(CLS) framework. In particular, we show explicitly how episodic memory can\nteach the neocortex to form a semantic memory, which is a core issue in MTT and\nCLS.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 17:22:18 GMT"}, {"version": "v2", "created": "Mon, 28 Aug 2017 14:20:57 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Tresp", "Volker", ""], ["Ma", "Yunpu", ""]]}, {"id": "1708.02940", "submitter": "Pooja Thakar", "authors": "Pooja Thakar, Anil Mehta, Manisha", "title": "Role of Secondary Attributes to Boost the Prediction Accuracy of\n  Students Employability Via Data Mining", "comments": "7 pages", "journal-ref": "(IJACSA) International Journal of Advanced Computer Science and\n  Applications, Vol. 6, No. 11, 2015", "doi": null, "report-no": null, "categories": "cs.CY cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data Mining is best-known for its analytical and prediction capabilities. It\nis used in several areas such as fraud detection, predicting client behavior,\nmoney market behavior, bankruptcy prediction. It can also help in establishing\nan educational ecosystem, which discovers useful knowledge, and assist\neducators to take proactive decisions to boost student performance and\nemployability. This paper presents an empirical study that compares varied\nclassification algorithms on two datasets of MCA (Masters in Computer\nApplications) students collected from various affiliated colleges of a reputed\nstate university in India. One dataset includes only primary attributes,\nwhereas other dataset is feeded with secondary psychometric attributes in it.\nThe results showcase that solely primary academic attributes do not lead to\nsmart prediction accuracy of students employability, once they square measure\nwithin the initial year of their education. The study analyzes and stresses the\nrole of secondary psychometric attributes for better prediction accuracy and\nanalysis of students performance. Timely prediction and analysis of students\nperformance can help Management, Teachers and Students to work on their gray\nareas for better results and employment opportunities.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 09:14:36 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Thakar", "Pooja", ""], ["Mehta", "Anil", ""], ["Manisha", "", ""]]}, {"id": "1708.02977", "submitter": "Licheng Yu", "authors": "Licheng Yu and Mohit Bansal and Tamara L. Berg", "title": "Hierarchically-Attentive RNN for Album Summarization and Storytelling", "comments": "To appear at EMNLP-2017 (7 pages)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of end-to-end visual storytelling. Given a photo\nalbum, our model first selects the most representative (summary) photos, and\nthen composes a natural language story for the album. For this task, we make\nuse of the Visual Storytelling dataset and a model composed of three\nhierarchically-attentive Recurrent Neural Nets (RNNs) to: encode the album\nphotos, select representative (summary) photos, and compose the story.\nAutomatic and human evaluations show our model achieves better performance on\nselection, generation, and retrieval than baselines.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 19:26:47 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Yu", "Licheng", ""], ["Bansal", "Mohit", ""], ["Berg", "Tamara L.", ""]]}, {"id": "1708.03019", "submitter": "Lavindra de Silva", "authors": "Lavindra de Silva and Sebastian Sardina and Lin Padgham", "title": "Addendum to: Summary Information for Reasoning About Hierarchical Plans", "comments": "This paper is a more detailed version of the following publication:\n  Lavindra de Silva, Sebastian Sardina, Lin Padgham: Summary Information for\n  Reasoning About Hierarchical Plans. ECAI 2016: 1300-1308", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hierarchically structured agent plans are important for efficient planning\nand acting, and they also serve (among other things) to produce \"richer\"\nclassical plans, composed not just of a sequence of primitive actions, but also\n\"abstract\" ones representing the supplied hierarchies. A crucial step for this\nand other approaches is deriving precondition and effect \"summaries\" from a\ngiven plan hierarchy. This paper provides mechanisms to do this for more\npragmatic and conventional hierarchies than in the past. To this end, we\nformally define the notion of a precondition and an effect for a hierarchical\nplan; we present data structures and algorithms for automatically deriving this\ninformation; and we analyse the properties of the presented algorithms. We\nconclude the paper by detailing how our algorithms may be used together with a\nclassical planner in order to obtain abstract plans.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 21:27:29 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["de Silva", "Lavindra", ""], ["Sardina", "Sebastian", ""], ["Padgham", "Lin", ""]]}, {"id": "1708.03044", "submitter": "Ting-Hao Huang", "authors": "Ting-Hao Kenneth Huang and Walter S. Lasecki and Amos Azaria and\n  Jeffrey P. Bigham", "title": "\"Is there anything else I can help you with?\": Challenges in Deploying\n  an On-Demand Crowd-Powered Conversational Agent", "comments": "10 pages. In Proceedings of Conference on Human Computation &\n  Crowdsourcing (HCOMP 2016), 2016, Austin, TX, USA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent conversational assistants, such as Apple's Siri, Microsoft's\nCortana, and Amazon's Echo, have quickly become a part of our digital life.\nHowever, these assistants have major limitations, which prevents users from\nconversing with them as they would with human dialog partners. This limits our\nability to observe how users really want to interact with the underlying\nsystem. To address this problem, we developed a crowd-powered conversational\nassistant, Chorus, and deployed it to see how users and workers would interact\ntogether when mediated by the system. Chorus sophisticatedly converses with end\nusers over time by recruiting workers on demand, which in turn decide what\nmight be the best response for each user sentence. Up to the first month of our\ndeployment, 59 users have held conversations with Chorus during 320\nconversational sessions. In this paper, we present an account of Chorus'\ndeployment, with a focus on four challenges: (i) identifying when conversations\nare over, (ii) malicious users and workers, (iii) on-demand recruiting, and\n(iv) settings in which consensus is not enough. Our observations could assist\nthe deployment of crowd-powered conversation systems and crowd-powered systems\nin general.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 01:40:49 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Huang", "Ting-Hao Kenneth", ""], ["Lasecki", "Walter S.", ""], ["Azaria", "Amos", ""], ["Bigham", "Jeffrey P.", ""]]}, {"id": "1708.03080", "submitter": "Wonho Kang", "authors": "Wonho Kang and Youngnam Han", "title": "A Simple and Realistic Pedestrian Model for Crowd Simulation and\n  Application", "comments": "https://scholar.google.co.kr/citations?user=HHP3UrYAAAAJ", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI nlin.CG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The simulation of pedestrian crowd that reflects reality is a major challenge\nfor researches. Several crowd simulation models have been proposed such as\ncellular automata model, agent-based model, fluid dynamic model, etc. It is\nimportant to note that agent-based model is able, over others approaches, to\nprovide a natural description of the system and then to capture complex human\nbehaviors. In this paper, we propose a multi-agent simulation model in which\npedestrian positions are updated at discrete time intervals. It takes into\naccount the major normal conditions of a simple pedestrian situated in a crowd\nsuch as preferences, realistic perception of environment, etc. Our objective is\nto simulate the pedestrian crowd realistically towards a simulation of\nbelievable pedestrian behaviors. Typical pedestrian phenomena, including the\nunidirectional and bidirectional movement in a corridor as well as the flow\nthrough bottleneck, are simulated. The conducted simulations show that our\nmodel is able to produce realistic pedestrian behaviors. The obtained\nfundamental diagram and flow rate at bottleneck agree very well with classic\nconclusions and empirical study results. It is hoped that the idea of this\nstudy may be helpful in promoting the modeling and simulation of pedestrian\ncrowd in a simple way.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 05:46:38 GMT"}, {"version": "v2", "created": "Sun, 13 Aug 2017 23:52:53 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Kang", "Wonho", ""], ["Han", "Youngnam", ""]]}, {"id": "1708.03151", "submitter": "Michael Saint-Guillain", "authors": "Michael Saint-Guillain, Christine Solnon and Yves Deville", "title": "The Static and Stochastic VRPTW with both random Customers and Reveal\n  Times: algorithms and recourse strategies", "comments": "Preprint version submitted to Transportation Research Part E", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unlike its deterministic counterpart, static and stochastic vehicle routing\nproblems (SS-VRP) aim at modeling and solving real-life operational problems by\nconsidering uncertainty on data. We consider the SS-VRPTW-CR introduced in\nSaint-Guillain et al. (2017). Like the SS-VRP introduced by Bertsimas (1992),\nwe search for optimal first stage routes for a fleet of vehicles to handle a\nset of stochastic customer demands, i.e., demands are uncertain and we only\nknow their probabilities. In addition to capacity constraints, customer demands\nare also constrained by time windows. Unlike all SS-VRP variants, the\nSS-VRPTW-CR does not make any assumption on the time at which a stochastic\ndemand is revealed, i.e., the reveal time is stochastic as well. To handle this\nnew problem, we introduce waiting locations: Each vehicle is assigned a\nsequence of waiting locations from which it may serve some associated demands,\nand the objective is to minimize the expected number of demands that cannot be\nsatisfied in time. In this paper, we propose two new recourse strategies for\nthe SS-VRPTW-CR, together with their closed-form expressions for efficiently\ncomputing their expectations: The first one allows us to take vehicle\ncapacities into account; The second one allows us to optimize routes by\navoiding some useless trips. We propose two algorithms for searching for routes\nwith optimal expected costs: The first one is an extended branch-and-cut\nalgorithm, based on a stochastic integer formulation, and the second one is a\nlocal search based heuristic method. We also introduce a new public benchmark\nfor the SS-VRPTW-CR, based on real-world data coming from the city of Lyon. We\nevaluate our two algorithms on this benchmark and empirically demonstrate the\nexpected superiority of the SS-VRPTW-CR anticipative actions over a basic\n\"wait-and-serve\" policy.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 10:20:01 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Saint-Guillain", "Michael", ""], ["Solnon", "Christine", ""], ["Deville", "Yves", ""]]}, {"id": "1708.03209", "submitter": "Thomas C King", "authors": "Thomas C. King, Ak{\\i}n G\\\"unay, Amit K. Chopra, Munindar P. Singh", "title": "Tosca: Operationalizing Commitments Over Information Protocols", "comments": null, "journal-ref": null, "doi": "10.24963/ijcai.2017/37", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of commitment is widely studied as a high-level abstraction for\nmodeling multiagent interaction. An important challenge is supporting flexible\ndecentralized enactments of commitment specifications. In this paper, we\ncombine recent advances on specifying commitments and information protocols.\nSpecifically, we contribute Tosca, a technique for automatically synthesizing\ninformation protocols from commitment specifications. Our main result is that\nthe synthesized protocols support commitment alignment, which is the idea that\nagents must make compatible inferences about their commitments despite\ndecentralization.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 13:39:59 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["King", "Thomas C.", ""], ["G\u00fcnay", "Ak\u0131n", ""], ["Chopra", "Amit K.", ""], ["Singh", "Munindar P.", ""]]}, {"id": "1708.03229", "submitter": "Yanshuai Cao", "authors": "Yanshuai Cao, Luyu Wang", "title": "Automatic Selection of t-SNE Perplexity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  t-Distributed Stochastic Neighbor Embedding (t-SNE) is one of the most widely\nused dimensionality reduction methods for data visualization, but it has a\nperplexity hyperparameter that requires manual selection. In practice, proper\ntuning of t-SNE perplexity requires users to understand the inner working of\nthe method as well as to have hands-on experience. We propose a model selection\nobjective for t-SNE perplexity that requires negligible extra computation\nbeyond that of the t-SNE itself. We empirically validate that the perplexity\nsettings found by our approach are consistent with preferences elicited from\nhuman experts across a number of datasets. The similarities of our approach to\nBayesian information criteria (BIC) and minimum description length (MDL) are\nalso analyzed.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 14:19:20 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Cao", "Yanshuai", ""], ["Wang", "Luyu", ""]]}, {"id": "1708.03246", "submitter": "Dasha Bogdanova", "authors": "Dasha Bogdanova, Majid Yazdani", "title": "SESA: Supervised Explicit Semantic Analysis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years supervised representation learning has provided state of the\nart or close to the state of the art results in semantic analysis tasks\nincluding ranking and information retrieval. The core idea is to learn how to\nembed items into a latent space such that they optimize a supervised objective\nin that latent space. The dimensions of the latent space have no clear\nsemantics, and this reduces the interpretability of the system. For example, in\npersonalization models, it is hard to explain why a particular item is ranked\nhigh for a given user profile. We propose a novel model of representation\nlearning called Supervised Explicit Semantic Analysis (SESA) that is trained in\na supervised fashion to embed items to a set of dimensions with explicit\nsemantics. The model learns to compare two objects by representing them in this\nexplicit space, where each dimension corresponds to a concept from a knowledge\nbase. This work extends Explicit Semantic Analysis (ESA) with a supervised\nmodel for ranking problems. We apply this model to the task of Job-Profile\nrelevance in LinkedIn in which a set of skills defines our explicit dimensions\nof the space. Every profile and job are encoded to this set of skills their\nsimilarity is calculated in this space. We use RNNs to embed text input into\nthis space. In addition to interpretability, our model makes use of the\nweb-scale collaborative skills data that is provided by users for each LinkedIn\nprofile. Our model provides state of the art result while it remains\ninterpretable.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 15:03:12 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Bogdanova", "Dasha", ""], ["Yazdani", "Majid", ""]]}, {"id": "1708.03259", "submitter": "Yiru Zhang", "authors": "Yiru Zhang (1), Tassadit Bouadi (1), Arnaud Martin (1) ((1) DRUID,\n  UR1)", "title": "Preference fusion and Condorcet's Paradox under uncertainty", "comments": "International Conference on Information Fusion, Jul 2017, Xi'an,\n  China", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Facing an unknown situation, a person may not be able to firmly elicit\nhis/her preferences over different alternatives, so he/she tends to express\nuncertain preferences. Given a community of different persons expressing their\npreferences over certain alternatives under uncertainty, to get a collective\nrepresentative opinion of the whole community, a preference fusion process is\nrequired. The aim of this work is to propose a preference fusion method that\ncopes with uncertainty and escape from the Condorcet paradox. To model\npreferences under uncertainty, we propose to develop a model of preferences\nbased on belief function theory that accurately describes and captures the\nuncertainty associated with individual or collective preferences. This work\nimproves and extends the previous results. This work improves and extends the\ncontribution presented in a previous work. The benefits of our contribution are\ntwofold. On the one hand, we propose a qualitative and expressive preference\nmodeling strategy based on belief-function theory which scales better with the\nnumber of sources. On the other hand, we propose an incremental distance-based\nalgorithm (using Jousselme distance) for the construction of the collective\npreference order to avoid the Condorcet Paradox.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 12:25:36 GMT"}], "update_date": "2017-08-11", "authors_parsed": [["Zhang", "Yiru", ""], ["Bouadi", "Tassadit", ""], ["Martin", "Arnaud", ""]]}, {"id": "1708.03309", "submitter": "Tommaso Dreossi", "authors": "Tommaso Dreossi, Shromona Ghosh, Alberto Sangiovanni-Vincentelli,\n  Sanjit A. Seshia", "title": "Systematic Testing of Convolutional Neural Networks for Autonomous\n  Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a framework to systematically analyze convolutional neural\nnetworks (CNNs) used in classification of cars in autonomous vehicles. Our\nanalysis procedure comprises an image generator that produces synthetic\npictures by sampling in a lower dimension image modification subspace and a\nsuite of visualization tools. The image generator produces images which can be\nused to test the CNN and hence expose its vulnerabilities. The presented\nframework can be used to extract insights of the CNN classifier, compare across\nclassification models, or generate training and validation datasets.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 17:33:52 GMT"}, {"version": "v2", "created": "Fri, 11 Aug 2017 17:34:23 GMT"}], "update_date": "2017-08-14", "authors_parsed": [["Dreossi", "Tommaso", ""], ["Ghosh", "Shromona", ""], ["Sangiovanni-Vincentelli", "Alberto", ""], ["Seshia", "Sanjit A.", ""]]}, {"id": "1708.03310", "submitter": "Sudip Mittal", "authors": "Sudip Mittal, Anupam Joshi, Tim Finin", "title": "Thinking, Fast and Slow: Combining Vector Spaces and Knowledge Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graphs and vector space models are robust knowledge representation\ntechniques with individual strengths and weaknesses. Vector space models excel\nat determining similarity between concepts, but are severely constrained when\nevaluating complex dependency relations and other logic-based operations that\nare a strength of knowledge graphs. We describe the VKG structure that helps\nunify knowledge graphs and vector representation of entities, and enables\npowerful inference methods and search capabilities that combine their\ncomplementary strengths. We analogize this to thinking `fast' in vector space\nalong with thinking 'slow' and `deeply' by reasoning over the knowledge graph.\nWe have created a query processing engine that takes complex queries and\ndecomposes them into subqueries optimized to run on the respective knowledge\ngraph or vector view of a VKG. We show that the VKG structure can process\nspecific queries that are not efficiently handled by vector spaces or knowledge\ngraphs alone. We also demonstrate and evaluate the VKG structure and the query\nprocessing engine by developing a system called Cyber-All-Intel for knowledge\nextraction, representation and querying in an end-to-end pipeline grounded in\nthe cybersecurity informatics domain.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 17:39:55 GMT"}, {"version": "v2", "created": "Mon, 21 Aug 2017 01:49:05 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Mittal", "Sudip", ""], ["Joshi", "Anupam", ""], ["Finin", "Tim", ""]]}, {"id": "1708.03341", "submitter": "Muaz Niazi", "authors": "Muaz A. Niazi", "title": "Technical Problems With \"Programmable self-assembly in a thousand-robot\n  swarm\"", "comments": "5 pages, eLetter response to article in Science 345(6198), pp.\n  795-799, 2014", "journal-ref": null, "doi": "10.6084/m9.figshare.1185186.v1", "report-no": null, "categories": "cs.AI cs.MA cs.RO nlin.AO nlin.PS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Rubenstein et al. present an interesting system of programmable\nself-assembled structure formation using 1000 Kilobot robots. The paper claims\nto advance work in artificial swarms similar to capabilities of natural systems\nbesides being highly robust. However, the system lacks in terms of matching\nmotility and complex shapes with holes, thereby limiting practical similarity\nto self-assembly in living systems.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 18:08:49 GMT"}], "update_date": "2017-08-14", "authors_parsed": [["Niazi", "Muaz A.", ""]]}, {"id": "1708.03366", "submitter": "Sangdon Park", "authors": "Sangdon Park, James Weimer and Insup Lee", "title": "Resilient Linear Classification: An Approach to Deal with Attacks on\n  Training Data", "comments": "Accepted as a conference paper at ICCPS17", "journal-ref": null, "doi": "10.1145/3055004.3055006", "report-no": null, "categories": "cs.LG cs.AI cs.CR cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data-driven techniques are used in cyber-physical systems (CPS) for\ncontrolling autonomous vehicles, handling demand responses for energy\nmanagement, and modeling human physiology for medical devices. These\ndata-driven techniques extract models from training data, where their\nperformance is often analyzed with respect to random errors in the training\ndata. However, if the training data is maliciously altered by attackers, the\neffect of these attacks on the learning algorithms underpinning data-driven CPS\nhave yet to be considered. In this paper, we analyze the resilience of\nclassification algorithms to training data attacks. Specifically, a generic\nmetric is proposed that is tailored to measure resilience of classification\nalgorithms with respect to worst-case tampering of the training data. Using the\nmetric, we show that traditional linear classification algorithms are resilient\nunder restricted conditions. To overcome these limitations, we propose a linear\nclassification algorithm with a majority constraint and prove that it is\nstrictly more resilient than the traditional algorithms. Evaluations on both\nsynthetic data and a real-world retrospective arrhythmia medical case-study\nshow that the traditional algorithms are vulnerable to tampered training data,\nwhereas the proposed algorithm is more resilient (as measured by worst-case\ntampering).\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 19:54:58 GMT"}, {"version": "v2", "created": "Tue, 15 Aug 2017 15:25:16 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Park", "Sangdon", ""], ["Weimer", "James", ""], ["Lee", "Insup", ""]]}, {"id": "1708.03395", "submitter": "Jean Barbier Dr.", "authors": "Jean Barbier, Florent Krzakala, Nicolas Macris, L\\'eo Miolane, Lenka\n  Zdeborov\\'a", "title": "Optimal Errors and Phase Transitions in High-Dimensional Generalized\n  Linear Models", "comments": "101 pages, 5 figures", "journal-ref": "Proceedings of the National Academy of Sciences 116. 12 (2019):\n  5451-5460", "doi": "10.1073/pnas.1802705116", "report-no": null, "categories": "cs.IT cond-mat.dis-nn cs.AI cs.LG math-ph math.IT math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalized linear models (GLMs) arise in high-dimensional machine learning,\nstatistics, communications and signal processing. In this paper we analyze GLMs\nwhen the data matrix is random, as relevant in problems such as compressed\nsensing, error-correcting codes or benchmark models in neural networks. We\nevaluate the mutual information (or \"free entropy\") from which we deduce the\nBayes-optimal estimation and generalization errors. Our analysis applies to the\nhigh-dimensional limit where both the number of samples and the dimension are\nlarge and their ratio is fixed. Non-rigorous predictions for the optimal errors\nexisted for special cases of GLMs, e.g. for the perceptron, in the field of\nstatistical physics based on the so-called replica method. Our present paper\nrigorously establishes those decades old conjectures and brings forward their\nalgorithmic interpretation in terms of performance of the generalized\napproximate message-passing algorithm. Furthermore, we tightly characterize,\nfor many learning problems, regions of parameters for which this algorithm\nachieves the optimal performance, and locate the associated sharp phase\ntransitions separating learnable and non-learnable regions. We believe that\nthis random version of GLMs can serve as a challenging benchmark for\nmulti-purpose algorithms. This paper is divided in two parts that can be read\nindependently: The first part (main part) presents the model and main results,\ndiscusses some applications and sketches the main ideas of the proof. The\nsecond part (supplementary informations) is much more detailed and provides\nmore examples as well as all the proofs.\n", "versions": [{"version": "v1", "created": "Thu, 10 Aug 2017 21:53:40 GMT"}, {"version": "v2", "created": "Fri, 3 Nov 2017 19:17:17 GMT"}, {"version": "v3", "created": "Thu, 1 Nov 2018 12:05:50 GMT"}], "update_date": "2019-04-01", "authors_parsed": [["Barbier", "Jean", ""], ["Krzakala", "Florent", ""], ["Macris", "Nicolas", ""], ["Miolane", "L\u00e9o", ""], ["Zdeborov\u00e1", "Lenka", ""]]}, {"id": "1708.03417", "submitter": "Seungkyun Hong", "authors": "Seungkyun Hong, Seongchan Kim, Minsu Joh, Sa-kwang Song", "title": "GlobeNet: Convolutional Neural Networks for Typhoon Eye Tracking from\n  Remote Sensing Imagery", "comments": "Under review as a workshop paper at CI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in remote sensing technologies have made it possible to use\nhigh-resolution visual data for weather observation and forecasting tasks. We\npropose the use of multi-layer neural networks for understanding complex\natmospheric dynamics based on multichannel satellite images. The capability of\nour model was evaluated by using a linear regression task for single typhoon\ncoordinates prediction. A specific combination of models and different\nactivation policies enabled us to obtain an interesting prediction result in\nthe northeastern hemisphere (ENH).\n", "versions": [{"version": "v1", "created": "Fri, 11 Aug 2017 00:41:56 GMT"}], "update_date": "2017-11-30", "authors_parsed": [["Hong", "Seungkyun", ""], ["Kim", "Seongchan", ""], ["Joh", "Minsu", ""], ["Song", "Sa-kwang", ""]]}, {"id": "1708.03418", "submitter": "Mostafa Dehghani", "authors": "Mostafa Dehghani, Sascha Rothe, Enrique Alfonseca, Pascal Fleury", "title": "Learning to Attend, Copy, and Generate for Session-Based Query\n  Suggestion", "comments": "Accepted to be published at The 26th ACM International Conference on\n  Information and Knowledge Management (CIKM2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Users try to articulate their complex information needs during search\nsessions by reformulating their queries. To make this process more effective,\nsearch engines provide related queries to help users in specifying the\ninformation need in their search process. In this paper, we propose a\ncustomized sequence-to-sequence model for session-based query suggestion. In\nour model, we employ a query-aware attention mechanism to capture the structure\nof the session context. is enables us to control the scope of the session from\nwhich we infer the suggested next query, which helps not only handle the noisy\ndata but also automatically detect session boundaries. Furthermore, we observe\nthat, based on the user query reformulation behavior, within a single session a\nlarge portion of query terms is retained from the previously submitted queries\nand consists of mostly infrequent or unseen terms that are usually not included\nin the vocabulary. We therefore empower the decoder of our model to access the\nsource words from the session context during decoding by incorporating a copy\nmechanism. Moreover, we propose evaluation metrics to assess the quality of the\ngenerative models for query suggestion. We conduct an extensive set of\nexperiments and analysis. e results suggest that our model outperforms the\nbaselines both in terms of the generating queries and scoring candidate queries\nfor the task of query suggestion.\n", "versions": [{"version": "v1", "created": "Fri, 11 Aug 2017 00:55:57 GMT"}, {"version": "v2", "created": "Thu, 7 Sep 2017 12:02:09 GMT"}, {"version": "v3", "created": "Tue, 19 Sep 2017 10:43:53 GMT"}, {"version": "v4", "created": "Mon, 13 Nov 2017 11:29:43 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Dehghani", "Mostafa", ""], ["Rothe", "Sascha", ""], ["Alfonseca", "Enrique", ""], ["Fleury", "Pascal", ""]]}, {"id": "1708.03800", "submitter": "Michel Fliess", "authors": "Hassane Aboua\\\"issa, Ola Alhaj Hasan, C\\'edric Join, Michel Fliess,\n  Didier Defer", "title": "Energy saving for building heating via a simple and efficient model-free\n  control design: First steps with computer simulations", "comments": "21st International Conference on System Theory, Control and\n  Computing, October 2017, Sinaia, Romania", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The model-based control of building heating systems for energy saving\nencounters severe physical, mathematical and calibration difficulties in the\nnumerous attempts that has been published until now. This topic is addressed\nhere via a new model-free control setting, where the need of any mathematical\ndescription disappears. Several convincing computer simulations are presented.\nComparisons with classic PI controllers and flatness-based predictive control\nare provided.\n", "versions": [{"version": "v1", "created": "Sat, 12 Aug 2017 17:35:52 GMT"}, {"version": "v2", "created": "Wed, 6 Sep 2017 20:21:03 GMT"}], "update_date": "2017-09-08", "authors_parsed": [["Aboua\u00efssa", "Hassane", ""], ["Hasan", "Ola Alhaj", ""], ["Join", "C\u00e9dric", ""], ["Fliess", "Michel", ""], ["Defer", "Didier", ""]]}, {"id": "1708.03901", "submitter": "Mohsen Malmir", "authors": "Mohsen Malmir and Garrison W. Cottrell", "title": "Belief Tree Search for Active Object Recognition", "comments": "IROS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active Object Recognition (AOR) has been approached as an unsupervised\nlearning problem, in which optimal trajectories for object inspection are not\nknown and are to be discovered by reducing label uncertainty measures or\ntraining with reinforcement learning. Such approaches have no guarantees of the\nquality of their solution. In this paper, we treat AOR as a Partially\nObservable Markov Decision Process (POMDP) and find near-optimal policies on\ntraining data using Belief Tree Search (BTS) on the corresponding belief Markov\nDecision Process (MDP). AOR then reduces to the problem of knowledge transfer\nfrom near-optimal policies on training set to the test set. We train a Long\nShort Term Memory (LSTM) network to predict the best next action on the\ntraining set rollouts. We sho that the proposed AOR method generalizes well to\nnovel views of familiar objects and also to novel objects. We compare this\nsupervised scheme against guided policy search, and find that the LSTM network\nreaches higher recognition accuracy compared to the guided policy method. We\nfurther look into optimizing the observation function to increase the total\ncollected reward of optimal policy. In AOR, the observation function is known\nonly approximately. We propose a gradient-based method update to this\napproximate observation function to increase the total reward of any policy. We\nshow that by optimizing the observation function and retraining the supervised\nLSTM network, the AOR performance on the test set improves significantly.\n", "versions": [{"version": "v1", "created": "Sun, 13 Aug 2017 13:24:28 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Malmir", "Mohsen", ""], ["Cottrell", "Garrison W.", ""]]}, {"id": "1708.03910", "submitter": "Mario Giulianelli", "authors": "Mario Giulianelli", "title": "Semi-supervised emotion lexicon expansion with label propagation and\n  specialized word embeddings", "comments": null, "journal-ref": "Computational Linguistics in the Netherlands Journal, 8, 99-121\n  (2018). Retrieved from https://clinjournal.org/clinj/article/view/82", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There exist two main approaches to automatically extract affective\norientation: lexicon-based and corpus-based. In this work, we argue that these\ntwo methods are compatible and show that combining them can improve the\naccuracy of emotion classifiers. In particular, we introduce a novel variant of\nthe Label Propagation algorithm that is tailored to distributed word\nrepresentations, we apply batch gradient descent to accelerate the optimization\nof label propagation and to make the optimization feasible for large graphs,\nand we propose a reproducible method for emotion lexicon expansion. We conclude\nthat label propagation can expand an emotion lexicon in a meaningful way and\nthat the expanded emotion lexicon can be leveraged to improve the accuracy of\nan emotion classifier.\n", "versions": [{"version": "v1", "created": "Sun, 13 Aug 2017 14:09:22 GMT"}], "update_date": "2020-04-30", "authors_parsed": [["Giulianelli", "Mario", ""]]}, {"id": "1708.03951", "submitter": "Anirudh Kamath", "authors": "Anirudh Kamath, Aditya Singh, Raj Ramnani, Ayush Vyas, Jay Shenoy", "title": "Optimization of Ensemble Supervised Learning Algorithms for Increased\n  Sensitivity, Specificity, and AUC of Population-Based Colorectal Cancer\n  Screenings", "comments": "7 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI q-bio.QM", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Over 150,000 new people in the United States are diagnosed with colorectal\ncancer each year. Nearly a third die from it (American Cancer Society). The\nonly approved noninvasive diagnosis tools currently involve fecal blood count\ntests (FOBTs) or stool DNA tests. Fecal blood count tests take only five\nminutes and are available over the counter for as low as \\$15. They are highly\nspecific, yet not nearly as sensitive, yielding a high percentage (25%) of\nfalse negatives (Colon Cancer Alliance). Moreover, FOBT results are far too\ngeneralized, meaning that a positive result could mean much more than just\ncolorectal cancer, and could just as easily mean hemorrhoids, anal fissure,\nproctitis, Crohn's disease, diverticulosis, ulcerative colitis, rectal ulcer,\nrectal prolapse, ischemic colitis, angiodysplasia, rectal trauma, proctitis\nfrom radiation therapy, and others. Stool DNA tests, the modern benchmark for\nCRC screening, have a much higher sensitivity and specificity, but also cost\n\\$600, take two weeks to process, and are not for high-risk individuals or\npeople with a history of polyps. To yield a cheap and effective CRC screening\nalternative, a unique ensemble-based classification algorithm is put in place\nthat considers the FIT result, BMI, smoking history, and diabetic status of\npatients. This method is tested under ten-fold cross validation to have a .95\nAUC, 92% specificity, 89% sensitivity, .88 F1, and 90% precision. Once\nclinically validated, this test promises to be cheaper, faster, and potentially\nmore accurate when compared to a stool DNA test.\n", "versions": [{"version": "v1", "created": "Sun, 13 Aug 2017 18:48:58 GMT"}, {"version": "v2", "created": "Tue, 15 Aug 2017 02:49:02 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Kamath", "Anirudh", ""], ["Singh", "Aditya", ""], ["Ramnani", "Raj", ""], ["Vyas", "Ayush", ""], ["Shenoy", "Jay", ""]]}, {"id": "1708.03993", "submitter": "Yan Yan", "authors": "Yan Yan, Wentao Guo, Meng Zhao, Jinghe Hu and Weipeng P. Yan", "title": "Optimizing Gross Merchandise Volume via DNN-MAB Dynamic Ranking Paradigm", "comments": "7 pages, 7 figures, accepted by 'IJCAI-17 Workshop AI Applications in\n  E-Commerce'", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the transition from people's traditional `brick-and-mortar' shopping to\nonline mobile shopping patterns in web 2.0 $\\mathit{era}$, the recommender\nsystem plays a critical role in E-Commerce and E-Retails. This is especially\ntrue when designing this system for more than $\\mathbf{236~million}$ daily\nactive users. Ranking strategy, the key module of the recommender system, needs\nto be precise, accurate, and responsive for estimating customers' intents. We\npropose a dynamic ranking paradigm, named as DNN-MAB, that is composed of a\npairwise deep neural network (DNN) $\\mathit{pre}$-ranker connecting a revised\nmulti-armed bandit (MAB) dynamic $\\mathit{post}$-ranker. By taking into account\nof explicit and implicit user feedbacks such as impressions, clicks,\nconversions, etc. DNN-MAB is able to adjust DNN $\\mathit{pre}$-ranking scores\nto assist customers locating items they are interested in most so that they can\nconverge quickly and frequently. To the best of our knowledge, frameworks like\nDNN-MAB have not been discussed in the previous literature to either E-Commerce\nor machine learning audiences. In practice, DNN-MAB has been deployed to\nproduction and it easily outperforms against other state-of-the-art models by\nsignificantly lifting the gross merchandise volume (GMV) which is the objective\nmetrics at JD.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 02:33:53 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Yan", "Yan", ""], ["Guo", "Wentao", ""], ["Zhao", "Meng", ""], ["Hu", "Jinghe", ""], ["Yan", "Weipeng P.", ""]]}, {"id": "1708.03995", "submitter": "Prathusha Kameswara Sarma", "authors": "Prathusha Kameswara Sarma, Bill Sethares", "title": "Sentiment Analysis by Joint Learning of Word Embeddings and Classifier", "comments": "10 pages. Under submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings are representations of individual words of a text document in\na vector space and they are often use- ful for performing natural language pro-\ncessing tasks. Current state of the art al- gorithms for learning word\nembeddings learn vector representations from large corpora of text documents in\nan unsu- pervised fashion. This paper introduces SWESA (Supervised Word\nEmbeddings for Sentiment Analysis), an algorithm for sentiment analysis via\nword embeddings. SWESA leverages document label infor- mation to learn vector\nrepresentations of words from a modest corpus of text doc- uments by solving an\noptimization prob- lem that minimizes a cost function with respect to both word\nembeddings as well as classification accuracy. Analysis re- veals that SWESA\nprovides an efficient way of estimating the dimension of the word embeddings\nthat are to be learned. Experiments on several real world data sets show that\nSWESA has superior per- formance when compared to previously suggested\napproaches to word embeddings and sentiment analysis tasks.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 02:40:20 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Sarma", "Prathusha Kameswara", ""], ["Sethares", "Bill", ""]]}, {"id": "1708.04028", "submitter": "Lucas Carvalho Cordeiro", "authors": "Rodrigo F. Ara\\'ujo, Alexandre Ribeiro, Iury V. Bessa, Lucas C.\n  Cordeiro, Jo\\~ao E. C. Filho", "title": "Counterexample Guided Inductive Optimization Applied to Mobile Robots\n  Path Planning (Extended Version)", "comments": "7 pages, 14rd Latin American Robotics Symposium (LARS'2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe and evaluate a novel optimization-based off-line path planning\nalgorithm for mobile robots based on the Counterexample-Guided Inductive\nOptimization (CEGIO) technique. CEGIO iteratively employs counterexamples\ngenerated from Boolean Satisfiability (SAT) and Satisfiability Modulo Theories\n(SMT) solvers, in order to guide the optimization process and to ensure global\noptimization. This paper marks the first application of CEGIO for planning\nmobile robot path. In particular, CEGIO has been successfully applied to obtain\noptimal two-dimensional paths for autonomous mobile robots using off-the-shelf\nSAT and SMT solvers.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 08:04:43 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Ara\u00fajo", "Rodrigo F.", ""], ["Ribeiro", "Alexandre", ""], ["Bessa", "Iury V.", ""], ["Cordeiro", "Lucas C.", ""], ["Filho", "Jo\u00e3o E. C.", ""]]}, {"id": "1708.04033", "submitter": "Tadanobu Inoue", "authors": "Tadanobu Inoue, Giovanni De Magistris, Asim Munawar, Tsuyoshi Yokoya,\n  Ryuki Tachibana", "title": "Deep Reinforcement Learning for High Precision Assembly Tasks", "comments": "Conference: Accepted to IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS), Vancouver, Canada, September 24-28,\n  2017. Video: https://youtu.be/b2pC78rBGH4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  High precision assembly of mechanical parts requires accuracy exceeding the\nrobot precision. Conventional part mating methods used in the current\nmanufacturing requires tedious tuning of numerous parameters before deployment.\nWe show how the robot can successfully perform a tight clearance peg-in-hole\ntask through training a recurrent neural network with reinforcement learning.\nIn addition to saving the manual effort, the proposed technique also shows\nrobustness against position and angle errors for the peg-in-hole task. The\nneural network learns to take the optimal action by observing the robot sensors\nto estimate the system state. The advantages of our proposed method is\nvalidated experimentally on a 7-axis articulated robot arm.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 08:32:30 GMT"}, {"version": "v2", "created": "Fri, 22 Sep 2017 01:34:42 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Inoue", "Tadanobu", ""], ["De Magistris", "Giovanni", ""], ["Munawar", "Asim", ""], ["Yokoya", "Tsuyoshi", ""], ["Tachibana", "Ryuki", ""]]}, {"id": "1708.04134", "submitter": "Biplav Srivastava", "authors": "Q Vera Liao, Biplav Srivastava, Pavan Kapanipathi", "title": "A Measure for Dialog Complexity and its Application in Streamlining\n  Service Operations", "comments": "11 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dialog is a natural modality for interaction between customers and businesses\nin the service industry. As customers call up the service provider, their\ninteractions may be routine or extraordinary. We believe that these\ninteractions, when seen as dialogs, can be analyzed to obtain a better\nunderstanding of customer needs and how to efficiently address them. We\nintroduce the idea of a dialog complexity measure to characterize multi-party\ninteractions, propose a general data-driven method to calculate it, use it to\ndiscover insights in public and enterprise dialog datasets, and demonstrate its\nbeneficial usage in facilitating better handling of customer requests and\nevaluating service agents.\n", "versions": [{"version": "v1", "created": "Fri, 4 Aug 2017 03:44:35 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Liao", "Q Vera", ""], ["Srivastava", "Biplav", ""], ["Kapanipathi", "Pavan", ""]]}, {"id": "1708.04196", "submitter": "Kiana Roshan Zamir", "authors": "Kiana Roshan Zamir, Ali Shafahi, Ali Haghani", "title": "Understanding and Visualizing the District of Columbia Capital Bikeshare\n  System Using Data Analysis for Balancing Purposes", "comments": "Submitted to TRB2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bike sharing systems' popularity has consistently been rising during the past\nyears. Managing and maintaining these emerging systems are indispensable parts\nof these systems. Visualizing the current operations can assist in getting a\nbetter grasp on the performance of the system. In this paper, a data mining\napproach is used to identify and visualize some important factors related to\nbike-share operations and management. To consolidate the data, we cluster\nstations that have a similar pickup and drop-off profiles during weekdays and\nweekends. We provide the temporal profile of the center of each cluster which\ncan be used as a simple and practical approach for approximating the number of\npickups and drop-offs of the stations. We also define two indices based on\nstations' shortages and surpluses that reflect the degree of balancing aid a\nstation needs. These indices can help stakeholders improve the quality of the\nbike-share user experience in at-least two ways. It can act as a complement to\nbalancing optimization efforts, and it can identify stations that need\nexpansion. We mine the District of Columbia's regional bike-share data and\ndiscuss the findings of this data set. We examine the bike-share system during\ndifferent quarters of the year and during both peak and non-peak hours.\nFindings reflect that on weekdays most of the pickups and drop-offs happen\nduring the morning and evening peaks whereas on weekends pickups and drop-offs\nare spread out throughout the day. We also show that throughout the day, more\nthan 40% of the stations are relatively self-balanced. Not worrying about these\nstations during ordinary days can allow the balancing efforts to focus on a\nfewer stations and therefore potentially improve the efficiency of the\nbalancing optimization models.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 16:16:24 GMT"}], "update_date": "2017-08-15", "authors_parsed": [["Zamir", "Kiana Roshan", ""], ["Shafahi", "Ali", ""], ["Haghani", "Ali", ""]]}, {"id": "1708.04198", "submitter": "Saber Moradi", "authors": "Saber Moradi, Ning Qiao, Fabio Stefanini and Giacomo Indiveri", "title": "A scalable multi-core architecture with heterogeneous memory structures\n  for Dynamic Neuromorphic Asynchronous Processors (DYNAPs)", "comments": "17 pages, 14 figures", "journal-ref": null, "doi": "10.1109/TBCAS.2017.2759700", "report-no": null, "categories": "cs.AR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neuromorphic computing systems comprise networks of neurons that use\nasynchronous events for both computation and communication. This type of\nrepresentation offers several advantages in terms of bandwidth and power\nconsumption in neuromorphic electronic systems. However, managing the traffic\nof asynchronous events in large scale systems is a daunting task, both in terms\nof circuit complexity and memory requirements. Here we present a novel routing\nmethodology that employs both hierarchical and mesh routing strategies and\ncombines heterogeneous memory structures for minimizing both memory\nrequirements and latency, while maximizing programming flexibility to support a\nwide range of event-based neural network architectures, through parameter\nconfiguration. We validated the proposed scheme in a prototype multi-core\nneuromorphic processor chip that employs hybrid analog/digital circuits for\nemulating synapse and neuron dynamics together with asynchronous digital\ncircuits for managing the address-event traffic. We present a theoretical\nanalysis of the proposed connectivity scheme, describe the methods and circuits\nused to implement such scheme, and characterize the prototype chip. Finally, we\ndemonstrate the use of the neuromorphic processor with a convolutional neural\nnetwork for the real-time classification of visual symbols being flashed to a\ndynamic vision sensor (DVS) at high speed.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 16:28:02 GMT"}, {"version": "v2", "created": "Wed, 16 Aug 2017 17:50:21 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Moradi", "Saber", ""], ["Qiao", "Ning", ""], ["Stefanini", "Fabio", ""], ["Indiveri", "Giacomo", ""]]}, {"id": "1708.04202", "submitter": "Marwin Segler", "authors": "Marwin H.S. Segler, Mike Preuss, Mark P. Waller", "title": "Learning to Plan Chemical Syntheses", "comments": null, "journal-ref": "Nature 555 (2018), 604-610", "doi": "10.1038/nature25978", "report-no": null, "categories": "cs.AI cs.LG physics.chem-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  From medicines to materials, small organic molecules are indispensable for\nhuman well-being. To plan their syntheses, chemists employ a problem solving\ntechnique called retrosynthesis. In retrosynthesis, target molecules are\nrecursively transformed into increasingly simpler precursor compounds until a\nset of readily available starting materials is obtained. Computer-aided\nretrosynthesis would be a highly valuable tool, however, past approaches were\nslow and provided results of unsatisfactory quality. Here, we employ Monte\nCarlo Tree Search (MCTS) to efficiently discover retrosynthetic routes. MCTS\nwas combined with an expansion policy network that guides the search, and an\n\"in-scope\" filter network to pre-select the most promising retrosynthetic\nsteps. These deep neural networks were trained on 12 million reactions, which\nrepresents essentially all reactions ever published in organic chemistry. Our\nsystem solves almost twice as many molecules and is 30 times faster in\ncomparison to the traditional search method based on extracted rules and\nhand-coded heuristics. Finally after a 60 year history of computer-aided\nsynthesis planning, chemists can no longer distinguish between routes generated\nby a computer system and real routes taken from the scientific literature. We\nanticipate that our method will accelerate drug and materials discovery by\nassisting chemists to plan better syntheses faster, and by enabling fully\nautomated robot synthesis.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 16:46:08 GMT"}], "update_date": "2018-04-17", "authors_parsed": [["Segler", "Marwin H. S.", ""], ["Preuss", "Mike", ""], ["Waller", "Mark P.", ""]]}, {"id": "1708.04225", "submitter": "Coline Devin", "authors": "Coline Devin, Pieter Abbeel, Trevor Darrell, Sergey Levine", "title": "Deep Object-Centric Representations for Generalizable Robot Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robotic manipulation in complex open-world scenarios requires both reliable\nphysical manipulation skills and effective and generalizable perception. In\nthis paper, we propose a method where general purpose pretrained visual models\nserve as an object-centric prior for the perception system of a learned policy.\nWe devise an object-level attentional mechanism that can be used to determine\nrelevant objects from a few trajectories or demonstrations, and then\nimmediately incorporate those objects into a learned policy. A task-independent\nmeta-attention locates possible objects in the scene, and a task-specific\nattention identifies which objects are predictive of the trajectories. The\nscope of the task-specific attention is easily adjusted by showing\ndemonstrations with distractor objects or with diverse relevant objects. Our\nresults indicate that this approach exhibits good generalization across object\ninstances using very few samples, and can be used to learn a variety of\nmanipulation tasks using reinforcement learning.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 17:42:59 GMT"}, {"version": "v2", "created": "Fri, 25 Aug 2017 00:14:15 GMT"}, {"version": "v3", "created": "Tue, 26 Sep 2017 17:06:36 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Devin", "Coline", ""], ["Abbeel", "Pieter", ""], ["Darrell", "Trevor", ""], ["Levine", "Sergey", ""]]}, {"id": "1708.04236", "submitter": "Leonore Winterer", "authors": "Leonore Winterer, Sebastian Junges, Ralf Wimmer, Nils Jansen, Ufuk\n  Topcu, Joost-Pieter Katoen, and Bernd Becker", "title": "Strategy Synthesis in POMDPs via Game-Based Abstractions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study synthesis problems with constraints in partially observable Markov\ndecision processes (POMDPs), where the objective is to compute a strategy for\nan agent that is guaranteed to satisfy certain safety and performance\nspecifications. Verification and strategy synthesis for POMDPs are, however,\ncomputationally intractable in general. We alleviate this difficulty by\nfocusing on planning applications and exploiting typical structural properties\nof such scenarios; for instance, we assume that the agent has the ability to\nobserve its own position inside an environment. We propose an abstraction\nrefinement framework which turns such a POMDP model into a (fully observable)\nprobabilistic two-player game (PG). For the obtained PGs, efficient\nverification and synthesis tools allow to determine strategies with optimal\nsafety and performance measures, which approximate optimal schedulers on the\nPOMDP. If the approximation is too coarse to satisfy the given specifications,\nan refinement scheme improves the computed strategies. As a running example, we\nuse planning problems where an agent moves inside an environment with randomly\nmoving obstacles and restricted observability. We demonstrate that the proposed\nmethod advances the state of the art by solving problems several\norders-of-magnitude larger than those that can be handled by existing POMDP\nsolvers. Furthermore, this method gives guarantees on safety constraints, which\nis not supported by the majority of the existing solvers.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 15:49:21 GMT"}, {"version": "v2", "created": "Mon, 27 May 2019 10:19:03 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Winterer", "Leonore", ""], ["Junges", "Sebastian", ""], ["Wimmer", "Ralf", ""], ["Jansen", "Nils", ""], ["Topcu", "Ufuk", ""], ["Katoen", "Joost-Pieter", ""], ["Becker", "Bernd", ""]]}, {"id": "1708.04321", "submitter": "Surya Prasath", "authors": "V. B. Surya Prasath, Haneen Arafat Abu Alfeilat, Ahmad B. A. Hassanat,\n  Omar Lasassmeh, Ahmad S. Tarawneh, Mahmoud Bashir Alhasanat, Hamzeh S. Eyal\n  Salman", "title": "Distance and Similarity Measures Effect on the Performance of K-Nearest\n  Neighbor Classifier -- A Review", "comments": "39 pages, 6 figures, 17 tables, revised text and added extra\n  experiments", "journal-ref": null, "doi": "10.1089/big.2018.0175", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The K-nearest neighbor (KNN) classifier is one of the simplest and most\ncommon classifiers, yet its performance competes with the most complex\nclassifiers in the literature. The core of this classifier depends mainly on\nmeasuring the distance or similarity between the tested examples and the\ntraining examples. This raises a major question about which distance measures\nto be used for the KNN classifier among a large number of distance and\nsimilarity measures available? This review attempts to answer this question\nthrough evaluating the performance (measured by accuracy, precision and recall)\nof the KNN using a large number of distance measures, tested on a number of\nreal-world datasets, with and without adding different levels of noise. The\nexperimental results show that the performance of KNN classifier depends\nsignificantly on the distance used, and the results showed large gaps between\nthe performances of different distances. We found that a recently proposed\nnon-convex distance performed the best when applied on most datasets comparing\nto the other tested distances. In addition, the performance of the KNN with\nthis top performing distance degraded only about $20\\%$ while the noise level\nreaches $90\\%$, this is true for most of the distances used as well. This means\nthat the KNN classifier using any of the top $10$ distances tolerate noise to a\ncertain degree. Moreover, the results show that some distances are less\naffected by the added noise comparing to other distances.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 20:52:35 GMT"}, {"version": "v2", "created": "Tue, 18 Jun 2019 19:58:50 GMT"}, {"version": "v3", "created": "Sun, 29 Sep 2019 16:27:25 GMT"}], "update_date": "2019-10-01", "authors_parsed": [["Prasath", "V. B. Surya", ""], ["Alfeilat", "Haneen Arafat Abu", ""], ["Hassanat", "Ahmad B. A.", ""], ["Lasassmeh", "Omar", ""], ["Tarawneh", "Ahmad S.", ""], ["Alhasanat", "Mahmoud Bashir", ""], ["Salman", "Hamzeh S. Eyal", ""]]}, {"id": "1708.04352", "submitter": "Peter Henderson", "authors": "Peter Henderson, Wei-Di Chang, Florian Shkurti, Johanna Hansen, David\n  Meger, Gregory Dudek", "title": "Benchmark Environments for Multitask Learning in Continuous Domains", "comments": "Accepted at Lifelong Learning: A Reinforcement Learning Approach\n  Workshop @ ICML, Sydney, Australia, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As demand drives systems to generalize to various domains and problems, the\nstudy of multitask, transfer and lifelong learning has become an increasingly\nimportant pursuit. In discrete domains, performance on the Atari game suite has\nemerged as the de facto benchmark for assessing multitask learning. However, in\ncontinuous domains there is a lack of agreement on standard multitask\nevaluation environments which makes it difficult to compare different\napproaches fairly. In this work, we describe a benchmark set of tasks that we\nhave developed in an extendable framework based on OpenAI Gym. We run a simple\nbaseline using Trust Region Policy Optimization and release the framework\npublicly to be expanded and used for the systematic comparison of multitask,\ntransfer, and lifelong learning in continuous domains.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 22:55:03 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Henderson", "Peter", ""], ["Chang", "Wei-Di", ""], ["Shkurti", "Florian", ""], ["Hansen", "Johanna", ""], ["Meger", "David", ""], ["Dudek", "Gregory", ""]]}, {"id": "1708.04357", "submitter": "Trang Pham", "authors": "Trang Pham, Truyen Tran, Hoa Dam, Svetha Venkatesh", "title": "Graph Classification via Deep Learning with Virtual Nodes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning representation for graph classification turns a variable-size graph\ninto a fixed-size vector (or matrix). Such a representation works nicely with\nalgebraic manipulations. Here we introduce a simple method to augment an\nattributed graph with a virtual node that is bidirectionally connected to all\nexisting nodes. The virtual node represents the latent aspects of the graph,\nwhich are not immediately available from the attributes and local connectivity\nstructures. The expanded graph is then put through any node representation\nmethod. The representation of the virtual node is then the representation of\nthe entire graph. In this paper, we use the recently introduced Column Network\nfor the expanded graph, resulting in a new end-to-end graph classification\nmodel dubbed Virtual Column Network (VCN). The model is validated on two tasks:\n(i) predicting bio-activity of chemical compounds, and (ii) finding software\nvulnerability from source code. Results demonstrate that VCN is competitive\nagainst well-established rivals.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 23:47:02 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Pham", "Trang", ""], ["Tran", "Truyen", ""], ["Dam", "Hoa", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1708.04391", "submitter": "Nicholas Guttenberg", "authors": "Nicholas Guttenberg, Martin Biehl, Ryota Kanai", "title": "Learning body-affordances to simplify action spaces", "comments": "4 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Controlling embodied agents with many actuated degrees of freedom is a\nchallenging task. We propose a method that can discover and interpolate between\ncontext dependent high-level actions or body-affordances. These provide an\nabstract, low-dimensional interface indexing high-dimensional and time-\nextended action policies. Our method is related to recent ap- proaches in the\nmachine learning literature but is conceptually simpler and easier to\nimplement. More specifically our method requires the choice of a n-dimensional\ntarget sensor space that is endowed with a distance metric. The method then\nlearns an also n-dimensional embedding of possibly reactive body-affordances\nthat spread as far as possible throughout the target sensor space.\n", "versions": [{"version": "v1", "created": "Tue, 15 Aug 2017 04:07:57 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Guttenberg", "Nicholas", ""], ["Biehl", "Martin", ""], ["Kanai", "Ryota", ""]]}, {"id": "1708.04403", "submitter": "Zhi-Hua Zhou", "authors": "Wei Wang and Zhi-Hua Zhou", "title": "Theoretical Foundation of Co-Training and Disagreement-Based Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Disagreement-based approaches generate multiple classifiers and exploit the\ndisagreement among them with unlabeled data to improve learning performance.\nCo-training is a representative paradigm of them, which trains two classifiers\nseparately on two sufficient and redundant views; while for the applications\nwhere there is only one view, several successful variants of co-training with\ntwo different classifiers on single-view data instead of two views have been\nproposed. For these disagreement-based approaches, there are several important\nissues which still are unsolved, in this article we present theoretical\nanalyses to address these issues, which provides a theoretical foundation of\nco-training and disagreement-based approaches.\n", "versions": [{"version": "v1", "created": "Tue, 15 Aug 2017 06:00:33 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Wang", "Wei", ""], ["Zhou", "Zhi-Hua", ""]]}, {"id": "1708.04529", "submitter": "Yuya Yoshikawa", "authors": "Yuya Yoshikawa", "title": "Learning from Noisy Label Distributions", "comments": "Accepted in ICANN2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider a novel machine learning problem, that is,\nlearning a classifier from noisy label distributions. In this problem, each\ninstance with a feature vector belongs to at least one group. Then, instead of\nthe true label of each instance, we observe the label distribution of the\ninstances associated with a group, where the label distribution is distorted by\nan unknown noise. Our goals are to (1) estimate the true label of each\ninstance, and (2) learn a classifier that predicts the true label of a new\ninstance. We propose a probabilistic model that considers true label\ndistributions of groups and parameters that represent the noise as hidden\nvariables. The model can be learned based on a variational Bayesian method. In\nnumerical experiments, we show that the proposed model outperforms existing\nmethods in terms of the estimation of the true labels of instances.\n", "versions": [{"version": "v1", "created": "Fri, 11 Aug 2017 03:25:46 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Yoshikawa", "Yuya", ""]]}, {"id": "1708.04571", "submitter": "Jiaqi Li", "authors": "Jiaqi Li, Zhifeng Zhao, Rongpeng Li", "title": "A Machine Learning Based Intrusion Detection System for Software Defined\n  5G Network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.NI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  As an inevitable trend of future 5G networks, Software Defined architecture\nhas many advantages in providing central- ized control and flexible resource\nmanagement. But it is also confronted with various security challenges and\npotential threats with emerging services and technologies. As the focus of\nnetwork security, Intrusion Detection Systems (IDS) are usually deployed\nseparately without collaboration. They are also unable to detect novel attacks\nwith limited intelligent abilities, which are hard to meet the needs of\nsoftware defined 5G. In this paper, we propose an intelligent intrusion system\ntaking the advances of software defined technology and artificial intelligence\nbased on Software Defined 5G architecture. It flexibly combines security\nfunction mod- ules which are adaptively invoked under centralized management\nand control with a globle view. It can also deal with unknown intrusions by\nusing machine learning algorithms. Evaluation results prove that the\nintelligent intrusion detection system achieves a better performance.\n", "versions": [{"version": "v1", "created": "Mon, 10 Jul 2017 05:48:30 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Li", "Jiaqi", ""], ["Zhao", "Zhifeng", ""], ["Li", "Rongpeng", ""]]}, {"id": "1708.04587", "submitter": "Nattapong Sanchan", "authors": "Nattapong Sanchan, Ahmet Aker and Kalina Bontcheva", "title": "Automatic Summarization of Online Debates", "comments": "Accepted and to be published in Natural Language Processing and\n  Information Retrieval workshop, Recent Advances in Natural Language\n  Processing 2017 (RANLP 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Debate summarization is one of the novel and challenging research areas in\nautomatic text summarization which has been largely unexplored. In this paper,\nwe develop a debate summarization pipeline to summarize key topics which are\ndiscussed or argued in the two opposing sides of online debates. We view that\nthe generation of debate summaries can be achieved by clustering, cluster\nlabeling, and visualization. In our work, we investigate two different\nclustering approaches for the generation of the summaries. In the first\napproach, we generate the summaries by applying purely term-based clustering\nand cluster labeling. The second approach makes use of X-means for clustering\nand Mutual Information for labeling the clusters. Both approaches are driven by\nontologies. We visualize the results using bar charts. We think that our\nresults are a smooth entry for users aiming to receive the first impression\nabout what is discussed within a debate topic containing waste number of\nargumentations.\n", "versions": [{"version": "v1", "created": "Tue, 15 Aug 2017 16:44:28 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Sanchan", "Nattapong", ""], ["Aker", "Ahmet", ""], ["Bontcheva", "Kalina", ""]]}, {"id": "1708.04592", "submitter": "Nattapong Sanchan", "authors": "Nattapong Sanchan, Ahmet Aker and Kalina Bontcheva", "title": "Gold Standard Online Debates Summaries and First Experiments Towards\n  Automatic Summarization of Online Debate Data", "comments": "accepted and presented at the CICLING 2017 - 18th International\n  Conference on Intelligent Text Processing and Computational Linguistics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Usage of online textual media is steadily increasing. Daily, more and more\nnews stories, blog posts and scientific articles are added to the online\nvolumes. These are all freely accessible and have been employed extensively in\nmultiple research areas, e.g. automatic text summarization, information\nretrieval, information extraction, etc. Meanwhile, online debate forums have\nrecently become popular, but have remained largely unexplored. For this reason,\nthere are no sufficient resources of annotated debate data available for\nconducting research in this genre. In this paper, we collected and annotated\ndebate data for an automatic summarization task. Similar to extractive gold\nstandard summary generation our data contains sentences worthy to include into\na summary. Five human annotators performed this task. Inter-annotator\nagreement, based on semantic similarity, is 36% for Cohen's kappa and 48% for\nKrippendorff's alpha. Moreover, we also implement an extractive summarization\nsystem for online debates and discuss prominent features for the task of\nsummarizing online debate data automatically.\n", "versions": [{"version": "v1", "created": "Tue, 15 Aug 2017 16:52:22 GMT"}], "update_date": "2017-08-16", "authors_parsed": [["Sanchan", "Nattapong", ""], ["Aker", "Ahmet", ""], ["Bontcheva", "Kalina", ""]]}, {"id": "1708.04670", "submitter": "Dianbo Liu Mr", "authors": "Dianbo Liu, Fengjiao Peng, Andrew Shea, Ognjen (Oggi) Rudovic,\n  Rosalind Picard", "title": "DeepFaceLIFT: Interpretable Personalized Models for Automatic Estimation\n  of Self-Reported Pain", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Previous research on automatic pain estimation from facial expressions has\nfocused primarily on \"one-size-fits-all\" metrics (such as PSPI). In this work,\nwe focus on directly estimating each individual's self-reported visual-analog\nscale (VAS) pain metric, as this is considered the gold standard for pain\nmeasurement. The VAS pain score is highly subjective and context-dependent, and\nits range can vary significantly among different persons. To tackle these\nissues, we propose a novel two-stage personalized model, named DeepFaceLIFT,\nfor automatic estimation of VAS. This model is based on (1) Neural Network and\n(2) Gaussian process regression models, and is used to personalize the\nestimation of self-reported pain via a set of hand-crafted personal features\nand multi-task learning. We show on the benchmark dataset for pain analysis\n(The UNBC-McMaster Shoulder Pain Expression Archive) that the proposed\npersonalized model largely outperforms the traditional, unpersonalized models:\nthe intra-class correlation improves from a baseline performance of 19\\% to a\npersonalized performance of 35\\% while also providing confidence in the\nmodel\\textquotesingle s estimates -- in contrast to existing models for the\ntarget task. Additionally, DeepFaceLIFT automatically discovers the\npain-relevant facial regions for each person, allowing for an easy\ninterpretation of the pain-related facial cues.\n", "versions": [{"version": "v1", "created": "Wed, 9 Aug 2017 12:07:45 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Liu", "Dianbo", "", "Oggi"], ["Peng", "Fengjiao", "", "Oggi"], ["Shea", "Andrew", "", "Oggi"], ["Ognjen", "", "", "Oggi"], ["Rudovic", "", ""], ["Picard", "Rosalind", ""]]}, {"id": "1708.04733", "submitter": "Minh Trung Le", "authors": "Trung Le, Hung Vu, Tu Dinh Nguyen, Dinh Phung", "title": "Geometric Enclosing Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training model to generate data has increasingly attracted research attention\nand become important in modern world applications. We propose in this paper a\nnew geometry-based optimization approach to address this problem. Orthogonal to\ncurrent state-of-the-art density-based approaches, most notably VAE and GAN, we\npresent a fresh new idea that borrows the principle of minimal enclosing ball\nto train a generator G\\left(\\bz\\right) in such a way that both training and\ngenerated data, after being mapped to the feature space, are enclosed in the\nsame sphere. We develop theory to guarantee that the mapping is bijective so\nthat its inverse from feature space to data space results in expressive\nnonlinear contours to describe the data manifold, hence ensuring data generated\nare also lying on the data manifold learned from training data. Our model\nenjoys a nice geometric interpretation, hence termed Geometric Enclosing\nNetworks (GEN), and possesses some key advantages over its rivals, namely\nsimple and easy-to-control optimization formulation, avoidance of mode\ncollapsing and efficiently learn data manifold representation in a completely\nunsupervised manner. We conducted extensive experiments on synthesis and\nreal-world datasets to illustrate the behaviors, strength and weakness of our\nproposed GEN, in particular its ability to handle multi-modal data and quality\nof generated data.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 01:10:49 GMT"}, {"version": "v2", "created": "Thu, 17 Aug 2017 04:58:35 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Le", "Trung", ""], ["Vu", "Hung", ""], ["Nguyen", "Tu Dinh", ""], ["Phung", "Dinh", ""]]}, {"id": "1708.04757", "submitter": "Hossein Soleimani", "authors": "Hossein Soleimani, James Hensman, Suchi Saria", "title": "Scalable Joint Models for Reliable Uncertainty-Aware Event Prediction", "comments": "To appear in IEEE Transaction on Pattern Analysis and Machine\n  Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Missing data and noisy observations pose significant challenges for reliably\npredicting events from irregularly sampled multivariate time series\n(longitudinal) data. Imputation methods, which are typically used for\ncompleting the data prior to event prediction, lack a principled mechanism to\naccount for the uncertainty due to missingness. Alternatively, state-of-the-art\njoint modeling techniques can be used for jointly modeling the longitudinal and\nevent data and compute event probabilities conditioned on the longitudinal\nobservations. These approaches, however, make strong parametric assumptions and\ndo not easily scale to multivariate signals with many observations. Our\nproposed approach consists of several key innovations. First, we develop a\nflexible and scalable joint model based upon sparse multiple-output Gaussian\nprocesses. Unlike state-of-the-art joint models, the proposed model can explain\nhighly challenging structure including non-Gaussian noise while scaling to\nlarge data. Second, we derive an optimal policy for predicting events using the\ndistribution of the event occurrence estimated by the joint model. The derived\npolicy trades-off the cost of a delayed detection versus incorrect assessments\nand abstains from making decisions when the estimated event probability does\nnot satisfy the derived confidence criteria. Experiments on a large dataset\nshow that the proposed framework significantly outperforms state-of-the-art\ntechniques in event prediction.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 03:27:25 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Soleimani", "Hossein", ""], ["Hensman", "James", ""], ["Saria", "Suchi", ""]]}, {"id": "1708.04782", "submitter": "Timothy Lillicrap", "authors": "Oriol Vinyals, Timo Ewalds, Sergey Bartunov, Petko Georgiev, Alexander\n  Sasha Vezhnevets, Michelle Yeo, Alireza Makhzani, Heinrich K\\\"uttler, John\n  Agapiou, Julian Schrittwieser, John Quan, Stephen Gaffney, Stig Petersen,\n  Karen Simonyan, Tom Schaul, Hado van Hasselt, David Silver, Timothy\n  Lillicrap, Kevin Calderone, Paul Keet, Anthony Brunasso, David Lawrence,\n  Anders Ekermo, Jacob Repp, Rodney Tsing", "title": "StarCraft II: A New Challenge for Reinforcement Learning", "comments": "Collaboration between DeepMind & Blizzard. 20 pages, 9 figures, 2\n  tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces SC2LE (StarCraft II Learning Environment), a\nreinforcement learning environment based on the StarCraft II game. This domain\nposes a new grand challenge for reinforcement learning, representing a more\ndifficult class of problems than considered in most prior work. It is a\nmulti-agent problem with multiple players interacting; there is imperfect\ninformation due to a partially observed map; it has a large action space\ninvolving the selection and control of hundreds of units; it has a large state\nspace that must be observed solely from raw input feature planes; and it has\ndelayed credit assignment requiring long-term strategies over thousands of\nsteps. We describe the observation, action, and reward specification for the\nStarCraft II domain and provide an open source Python-based interface for\ncommunicating with the game engine. In addition to the main game maps, we\nprovide a suite of mini-games focusing on different elements of StarCraft II\ngameplay. For the main game maps, we also provide an accompanying dataset of\ngame replay data from human expert players. We give initial baseline results\nfor neural networks trained from this data to predict game outcomes and player\nactions. Finally, we present initial baseline results for canonical deep\nreinforcement learning agents applied to the StarCraft II domain. On the\nmini-games, these agents learn to achieve a level of play that is comparable to\na novice player. However, when trained on the main game, these agents are\nunable to make significant progress. Thus, SC2LE offers a new and challenging\nenvironment for exploring deep reinforcement learning algorithms and\narchitectures.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 06:20:52 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Vinyals", "Oriol", ""], ["Ewalds", "Timo", ""], ["Bartunov", "Sergey", ""], ["Georgiev", "Petko", ""], ["Vezhnevets", "Alexander Sasha", ""], ["Yeo", "Michelle", ""], ["Makhzani", "Alireza", ""], ["K\u00fcttler", "Heinrich", ""], ["Agapiou", "John", ""], ["Schrittwieser", "Julian", ""], ["Quan", "John", ""], ["Gaffney", "Stephen", ""], ["Petersen", "Stig", ""], ["Simonyan", "Karen", ""], ["Schaul", "Tom", ""], ["van Hasselt", "Hado", ""], ["Silver", "David", ""], ["Lillicrap", "Timothy", ""], ["Calderone", "Kevin", ""], ["Keet", "Paul", ""], ["Brunasso", "Anthony", ""], ["Lawrence", "David", ""], ["Ekermo", "Anders", ""], ["Repp", "Jacob", ""], ["Tsing", "Rodney", ""]]}, {"id": "1708.04801", "submitter": "Daning Cheng", "authors": "Cheng Daning, Li Shigang and Zhang Yunquan", "title": "Weighted parallel SGD for distributed unbalanced-workload training\n  system", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stochastic gradient descent (SGD) is a popular stochastic optimization method\nin machine learning. Traditional parallel SGD algorithms, e.g., SimuParallel\nSGD, often require all nodes to have the same performance or to consume equal\nquantities of data. However, these requirements are difficult to satisfy when\nthe parallel SGD algorithms run in a heterogeneous computing environment;\nlow-performance nodes will exert a negative influence on the final result. In\nthis paper, we propose an algorithm called weighted parallel SGD (WP-SGD).\nWP-SGD combines weighted model parameters from different nodes in the system to\nproduce the final output. WP-SGD makes use of the reduction in standard\ndeviation to compensate for the loss from the inconsistency in performance of\nnodes in the cluster, which means that WP-SGD does not require that all nodes\nconsume equal quantities of data. We also analyze the theoretical feasibility\nof running two other parallel SGD algorithms combined with WP-SGD in a\nheterogeneous environment. The experimental results show that WP-SGD\nsignificantly outperforms the traditional parallel SGD algorithms on\ndistributed training systems with an unbalanced workload.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 08:29:23 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Daning", "Cheng", ""], ["Shigang", "Li", ""], ["Yunquan", "Zhang", ""]]}, {"id": "1708.04806", "submitter": "Kieran Greer Dr", "authors": "Kieran Greer", "title": "New Ideas for Brain Modelling 4", "comments": null, "journal-ref": "BRAIN. Broad Research in Artificial Intelligence and Neuroscience,\n  Vol. 9, No. 2, pp. 155-167. ISSN 2067-3957", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper continues the research that considers a new cognitive model based\nstrongly on the human brain. In particular, it considers the neural binding\nstructure of an earlier paper. It also describes some new methods in the areas\nof image processing and behaviour simulation. The work is all based on earlier\nresearch by the author and the new additions are intended to fit in with the\noverall design. For image processing, a grid-like structure is used with 'full\nlinking'. Each cell in the classifier grid stores a list of all other cells it\ngets associated with and this is used as the learned image that new input is\ncompared to. For the behaviour metric, a new prediction equation is suggested,\nas part of a simulation, that uses feedback and history to dynamically\ndetermine its course of action. While the new methods are from widely different\ntopics, both can be compared with the binary-analog type of interface that is\nthe main focus of the paper. It is suggested that the simplest of linking\nbetween a tree and ensemble can explain neural binding and variable signal\nstrengths.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 08:32:03 GMT"}, {"version": "v2", "created": "Wed, 27 Sep 2017 13:41:41 GMT"}, {"version": "v3", "created": "Wed, 28 Feb 2018 21:19:44 GMT"}, {"version": "v4", "created": "Mon, 12 Mar 2018 15:51:06 GMT"}], "update_date": "2018-12-11", "authors_parsed": [["Greer", "Kieran", ""]]}, {"id": "1708.04828", "submitter": "Yi Tay", "authors": "Yi Tay, Luu Anh Tuan, Minh C. Phan, Siu Cheung Hui", "title": "Multi-task Neural Network for Non-discrete Attribute Prediction in\n  Knowledge Graphs", "comments": "Accepted at CIKM 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many popular knowledge graphs such as Freebase, YAGO or DBPedia maintain a\nlist of non-discrete attributes for each entity. Intuitively, these attributes\nsuch as height, price or population count are able to richly characterize\nentities in knowledge graphs. This additional source of information may help to\nalleviate the inherent sparsity and incompleteness problem that are prevalent\nin knowledge graphs. Unfortunately, many state-of-the-art relational learning\nmodels ignore this information due to the challenging nature of dealing with\nnon-discrete data types in the inherently binary-natured knowledge graphs. In\nthis paper, we propose a novel multi-task neural network approach for both\nencoding and prediction of non-discrete attribute information in a relational\nsetting. Specifically, we train a neural network for triplet prediction along\nwith a separate network for attribute value regression. Via multi-task\nlearning, we are able to learn representations of entities, relations and\nattributes that encode information about both tasks. Moreover, such attributes\nare not only central to many predictive tasks as an information source but also\nas a prediction target. Therefore, models that are able to encode, incorporate\nand predict such information in a relational learning context are highly\nattractive as well. We show that our approach outperforms many state-of-the-art\nmethods for the tasks of relational triplet classification and attribute value\nprediction.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 09:55:15 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Tay", "Yi", ""], ["Tuan", "Luu Anh", ""], ["Phan", "Minh C.", ""], ["Hui", "Siu Cheung", ""]]}, {"id": "1708.04846", "submitter": "Jun Mei", "authors": "Jun Mei, Yong Jiang, Kewei Tu", "title": "Maximum A Posteriori Inference in Sum-Product Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sum-product networks (SPNs) are a class of probabilistic graphical models\nthat allow tractable marginal inference. However, the maximum a posteriori\n(MAP) inference in SPNs is NP-hard. We investigate MAP inference in SPNs from\nboth theoretical and algorithmic perspectives. For the theoretical part, we\nreduce general MAP inference to its special case without evidence and hidden\nvariables; we also show that it is NP-hard to approximate the MAP problem to\n$2^{n^\\epsilon}$ for fixed $0 \\leq \\epsilon < 1$, where $n$ is the input size.\nFor the algorithmic part, we first present an exact MAP solver that runs\nreasonably fast and could handle SPNs with up to 1k variables and 150k arcs in\nour experiments. We then present a new approximate MAP solver with a good\nbalance between speed and accuracy, and our comprehensive experiments on\nreal-world datasets show that it has better overall performance than existing\napproximate solvers.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 11:05:48 GMT"}, {"version": "v2", "created": "Fri, 18 Aug 2017 07:07:01 GMT"}, {"version": "v3", "created": "Mon, 20 Nov 2017 03:08:16 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Mei", "Jun", ""], ["Jiang", "Yong", ""], ["Tu", "Kewei", ""]]}, {"id": "1708.04927", "submitter": "Mark Stalzer", "authors": "Mark A. Stalzer and Chao Ju", "title": "TheoSea: Marching Theory to Light", "comments": "8 pages, 3 figures. arXiv admin note: text overlap with\n  arXiv:1706.06975", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is sufficient information in the far-field of a radiating dipole\nantenna to rediscover the Maxwell Equations and the wave equations of light,\nincluding the speed of light $c.$ TheoSea is a Julia program that does this in\nabout a second, and the key insight is that the compactness of theories drives\nthe search. The program is a computational embodiment of the scientific method:\nobservation, consideration of candidate theories, and validation.\n", "versions": [{"version": "v1", "created": "Mon, 14 Aug 2017 22:06:49 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Stalzer", "Mark A.", ""], ["Ju", "Chao", ""]]}, {"id": "1708.04983", "submitter": "Andrey Boytsov", "authors": "Andrey Boytsov, Francois Fouquet, Thomas Hartmann, and Yves LeTraon", "title": "Visualizing and Exploring Dynamic High-Dimensional Datasets with\n  LION-tSNE", "comments": "44 pages, 24 figures, 7 tables, planned for submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  T-distributed stochastic neighbor embedding (tSNE) is a popular and\nprize-winning approach for dimensionality reduction and visualizing\nhigh-dimensional data. However, tSNE is non-parametric: once visualization is\nbuilt, tSNE is not designed to incorporate additional data into existing\nrepresentation. It highly limits the applicability of tSNE to the scenarios\nwhere data are added or updated over time (like dashboards or series of data\nsnapshots).\n  In this paper we propose, analyze and evaluate LION-tSNE (Local Interpolation\nwith Outlier coNtrol) - a novel approach for incorporating new data into tSNE\nrepresentation. LION-tSNE is based on local interpolation in the vicinity of\ntraining data, outlier detection and a special outlier mapping algorithm. We\nshow that LION-tSNE method is robust both to outliers and to new samples from\nexisting clusters. We also discuss multiple possible improvements for special\ncases.\n  We compare LION-tSNE to a comprehensive list of possible benchmark approaches\nthat include multiple interpolation techniques, gradient descent for new data,\nand neural network approximation.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 17:17:56 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Boytsov", "Andrey", ""], ["Fouquet", "Francois", ""], ["Hartmann", "Thomas", ""], ["LeTraon", "Yves", ""]]}, {"id": "1708.04988", "submitter": "Assya Trofimov", "authors": "Trofimov Assya, Lemieux Sebastien, Perreault Claude", "title": "Warp: a method for neural network interpretability applied to gene\n  expression profiles", "comments": "5 pages, 3 figures, NIPS2016, Machine Learning in Computational\n  Biology workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.GN cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show a proof of principle for warping, a method to interpret the inner\nworking of neural networks in the context of gene expression analysis. Warping\nis an efficient way to gain insight to the inner workings of neural nets and\nmake them more interpretable. We demonstrate the ability of warping to recover\nmeaningful information for a given class on a samplespecific individual basis.\nWe found warping works well in both linearly and nonlinearly separable\ndatasets. These encouraging results show that warping has a potential to be the\nanswer to neural networks interpretability in computational biology.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 17:27:40 GMT"}], "update_date": "2017-08-17", "authors_parsed": [["Assya", "Trofimov", ""], ["Sebastien", "Lemieux", ""], ["Claude", "Perreault", ""]]}, {"id": "1708.05045", "submitter": "Wei Hu", "authors": "Zequn Sun, Wei Hu, Chengkai Li", "title": "Cross-lingual Entity Alignment via Joint Attribute-Preserving Embedding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity alignment is the task of finding entities in two knowledge bases (KBs)\nthat represent the same real-world object. When facing KBs in different natural\nlanguages, conventional cross-lingual entity alignment methods rely on machine\ntranslation to eliminate the language barriers. These approaches often suffer\nfrom the uneven quality of translations between languages. While recent\nembedding-based techniques encode entities and relationships in KBs and do not\nneed machine translation for cross-lingual entity alignment, a significant\nnumber of attributes remain largely unexplored. In this paper, we propose a\njoint attribute-preserving embedding model for cross-lingual entity alignment.\nIt jointly embeds the structures of two KBs into a unified vector space and\nfurther refines it by leveraging attribute correlations in the KBs. Our\nexperimental results on real-world datasets show that this approach\nsignificantly outperforms the state-of-the-art embedding approaches for\ncross-lingual entity alignment and could be complemented with methods based on\nmachine translation.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 19:30:17 GMT"}, {"version": "v2", "created": "Tue, 26 Sep 2017 02:06:08 GMT"}], "update_date": "2017-09-27", "authors_parsed": [["Sun", "Zequn", ""], ["Hu", "Wei", ""], ["Li", "Chengkai", ""]]}, {"id": "1708.05069", "submitter": "Joshua Brul\\'e", "authors": "Joshua Brul\\'e", "title": "A causation coefficient and taxonomy of correlation/causation\n  relationships", "comments": "31 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a causation coefficient which is defined in terms of\nprobabilistic causal models. This coefficient is suggested as the natural\ncausal analogue of the Pearson correlation coefficient and permits comparing\ncausation and correlation to each other in a simple, yet rigorous manner.\nTogether, these coefficients provide a natural way to classify the possible\ncorrelation/causation relationships that can occur in practice and examples of\neach relationship are provided. In addition, the typical relationship between\ncorrelation and causation is analyzed to provide insight into why correlation\nand causation are often conflated. Finally, example calculations of the\ncausation coefficient are shown on a real data set.\n", "versions": [{"version": "v1", "created": "Sat, 5 Aug 2017 05:47:33 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Brul\u00e9", "Joshua", ""]]}, {"id": "1708.05106", "submitter": "Arin Chaudhuri", "authors": "Arin Chaudhuri, Deovrat Kakde, Carol Sadek, Laura Gonzalez, Seunghyun\n  Kong", "title": "The Mean and Median Criterion for Automatic Kernel Bandwidth Selection\n  for Support Vector Data Description", "comments": null, "journal-ref": null, "doi": "10.1109/ICDMW.2017.116", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Support vector data description (SVDD) is a popular technique for detecting\nanomalies. The SVDD classifier partitions the whole space into an inlier\nregion, which consists of the region near the training data, and an outlier\nregion, which consists of points away from the training data. The computation\nof the SVDD classifier requires a kernel function, and the Gaussian kernel is a\ncommon choice for the kernel function. The Gaussian kernel has a bandwidth\nparameter, whose value is important for good results. A small bandwidth leads\nto overfitting, and the resulting SVDD classifier overestimates the number of\nanomalies. A large bandwidth leads to underfitting, and the classifier fails to\ndetect many anomalies. In this paper we present a new automatic, unsupervised\nmethod for selecting the Gaussian kernel bandwidth. The selected value can be\ncomputed quickly, and it is competitive with existing bandwidth selection\nmethods.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 23:38:35 GMT"}, {"version": "v2", "created": "Mon, 21 Aug 2017 23:12:34 GMT"}], "update_date": "2018-07-23", "authors_parsed": [["Chaudhuri", "Arin", ""], ["Kakde", "Deovrat", ""], ["Sadek", "Carol", ""], ["Gonzalez", "Laura", ""], ["Kong", "Seunghyun", ""]]}, {"id": "1708.05122", "submitter": "Prithvijit Chattopadhyay Chattopadhyay", "authors": "Prithvijit Chattopadhyay, Deshraj Yadav, Viraj Prabhu, Arjun\n  Chandrasekaran, Abhishek Das, Stefan Lee, Dhruv Batra, Devi Parikh", "title": "Evaluating Visual Conversational Agents via Cooperative Human-AI Games", "comments": "HCOMP 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As AI continues to advance, human-AI teams are inevitable. However, progress\nin AI is routinely measured in isolation, without a human in the loop. It is\ncrucial to benchmark progress in AI, not just in isolation, but also in terms\nof how it translates to helping humans perform certain tasks, i.e., the\nperformance of human-AI teams.\n  In this work, we design a cooperative game - GuessWhich - to measure human-AI\nteam performance in the specific context of the AI being a visual\nconversational agent. GuessWhich involves live interaction between the human\nand the AI. The AI, which we call ALICE, is provided an image which is unseen\nby the human. Following a brief description of the image, the human questions\nALICE about this secret image to identify it from a fixed pool of images.\n  We measure performance of the human-ALICE team by the number of guesses it\ntakes the human to correctly identify the secret image after a fixed number of\ndialog rounds with ALICE. We compare performance of the human-ALICE teams for\ntwo versions of ALICE. Our human studies suggest a counterintuitive trend -\nthat while AI literature shows that one version outperforms the other when\npaired with an AI questioner bot, we find that this improvement in AI-AI\nperformance does not translate to improved human-AI performance. This suggests\na mismatch between benchmarking of AI in isolation and in the context of\nhuman-AI teams.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 03:27:53 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Chattopadhyay", "Prithvijit", ""], ["Yadav", "Deshraj", ""], ["Prabhu", "Viraj", ""], ["Chandrasekaran", "Arjun", ""], ["Das", "Abhishek", ""], ["Lee", "Stefan", ""], ["Batra", "Dhruv", ""], ["Parikh", "Devi", ""]]}, {"id": "1708.05263", "submitter": "Lucas Bechberger", "authors": "Lucas Bechberger", "title": "The Size of a Hyperball in a Conceptual Space", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The cognitive framework of conceptual spaces [3] provides geometric means for\nrepresenting knowledge. A conceptual space is a high-dimensional space whose\ndimensions are partitioned into so-called domains. Within each domain, the\nEuclidean metric is used to compute distances. Distances in the overall space\nare computed by applying the Manhattan metric to the intra-domain distances.\nInstances are represented as points in this space and concepts are represented\nby regions. In this paper, we derive a formula for the size of a hyperball\nunder the combined metric of a conceptual space. One can think of such a\nhyperball as the set of all points having a certain minimal similarity to the\nhyperball's center.\n", "versions": [{"version": "v1", "created": "Tue, 4 Jul 2017 10:13:51 GMT"}, {"version": "v2", "created": "Tue, 5 Sep 2017 09:10:06 GMT"}, {"version": "v3", "created": "Mon, 18 Sep 2017 11:37:45 GMT"}, {"version": "v4", "created": "Fri, 22 Sep 2017 12:44:03 GMT"}], "update_date": "2017-09-25", "authors_parsed": [["Bechberger", "Lucas", ""]]}, {"id": "1708.05296", "submitter": "Alex Fukunaga", "authors": "Alex Fukunaga, Adi Botea, Yuu Jinnai and Akihiro Kishimoto", "title": "A Survey of Parallel A*", "comments": "arXiv admin note: text overlap with arXiv:1201.3204", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A* is a best-first search algorithm for finding optimal-cost paths in graphs.\nA* benefits significantly from parallelism because in many applications, A* is\nlimited by memory usage, so distributed memory implementations of A* that use\nall of the aggregate memory on the cluster enable problems that can not be\nsolved by serial, single-machine implementations to be solved. We survey\napproaches to parallel A*, focusing on decentralized approaches to A* which\npartition the state space among processors. We also survey approaches to\nparallel, limited-memory variants of A* such as parallel IDA*.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 01:45:40 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Fukunaga", "Alex", ""], ["Botea", "Adi", ""], ["Jinnai", "Yuu", ""], ["Kishimoto", "Akihiro", ""]]}, {"id": "1708.05325", "submitter": "Stefan Lattner", "authors": "Stefan Lattner, Maarten Grachten, Gerhard Widmer", "title": "Learning Musical Relations using Gated Autoencoders", "comments": "In Proceedings of the 2nd Conference on Computer Simulation of\n  Musical Creativity (CSMC 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Music is usually highly structured and it is still an open question how to\ndesign models which can successfully learn to recognize and represent musical\nstructure. A fundamental problem is that structurally related patterns can have\nvery distinct appearances, because the structural relationships are often based\non transformations of musical material, like chromatic or diatonic\ntransposition, inversion, retrograde, or rhythm change. In this preliminary\nwork, we study the potential of two unsupervised learning techniques -\nRestricted Boltzmann Machines (RBMs) and Gated Autoencoders (GAEs) - to capture\npre-defined transformations from constructed data pairs. We evaluate the models\nby using the learned representations as inputs in a discriminative task where\nfor a given type of transformation (e.g. diatonic transposition), the specific\nrelation between two musical patterns must be recognized (e.g. an upward\ntransposition of diatonic steps). Furthermore, we measure the reconstruction\nerror of models when reconstructing musical transformed patterns. Lastly, we\ntest the models in an analogy-making task. We find that it is difficult to\nlearn musical transformations with the RBM and that the GAE is much more\nadequate for this task, since it is able to learn representations of specific\ntransformations that are largely content-invariant. We believe these results\nshow that models such as GAEs may provide the basis for more encompassing music\nanalysis systems, by endowing them with a better understanding of the\nstructures underlying music.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 15:04:37 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Lattner", "Stefan", ""], ["Grachten", "Maarten", ""], ["Widmer", "Gerhard", ""]]}, {"id": "1708.05346", "submitter": "Jan Feyereisl", "authors": "Jan Feyereisl, Matej Nikl, Martin Poliak, Martin Stransky, Michal\n  Vlasak", "title": "General AI Challenge - Round One: Gradual Learning", "comments": "Presented as keynote talk at IJCAI Workshop on Evaluating\n  General-Purpose AI (EGPAI 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The General AI Challenge is an initiative to encourage the wider artificial\nintelligence community to focus on important problems in building intelligent\nmachines with more general scope than is currently possible. The challenge\ncomprises of multiple rounds, with the first round focusing on gradual\nlearning, i.e. the ability to re-use already learned knowledge for efficiently\nlearning to solve subsequent problems. In this article, we will present details\nof the first round of the challenge, its inspiration and aims. We also outline\na more formal description of the challenge and present a preliminary analysis\nof its curriculum, based on ideas from computational mechanics. We believe,\nthat such formalism will allow for a more principled approach towards\ninvestigating tasks in the challenge, building new curricula and for\npotentially improving consequent challenge rounds.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 16:10:58 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Feyereisl", "Jan", ""], ["Nikl", "Matej", ""], ["Poliak", "Martin", ""], ["Stransky", "Martin", ""], ["Vlasak", "Michal", ""]]}, {"id": "1708.05448", "submitter": "Philip Thomas", "authors": "Philip S. Thomas, Bruno Castro da Silva, Andrew G. Barto, and Emma\n  Brunskill", "title": "On Ensuring that Intelligent Machines Are Well-Behaved", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine learning algorithms are everywhere, ranging from simple data analysis\nand pattern recognition tools used across the sciences to complex systems that\nachieve super-human performance on various tasks. Ensuring that they are\nwell-behaved---that they do not, for example, cause harm to humans or act in a\nracist or sexist way---is therefore not a hypothetical problem to be dealt with\nin the future, but a pressing one that we address here. We propose a new\nframework for designing machine learning algorithms that simplifies the problem\nof specifying and regulating undesirable behaviors. To show the viability of\nthis new framework, we use it to create new machine learning algorithms that\npreclude the sexist and harmful behaviors exhibited by standard machine\nlearning algorithms in our experiments. Our framework for designing machine\nlearning algorithms simplifies the safe and responsible application of machine\nlearning.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 21:53:47 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Thomas", "Philip S.", ""], ["da Silva", "Bruno Castro", ""], ["Barto", "Andrew G.", ""], ["Brunskill", "Emma", ""]]}, {"id": "1708.05522", "submitter": "Shufeng Kong", "authors": "Shufeng Kong, Sanjiang Li, Michael Sioutis", "title": "Exploring Directional Path-Consistency for Solving Constraint Networks", "comments": null, "journal-ref": null, "doi": "10.1093/comjnl/bxx122", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Among the local consistency techniques used for solving constraint networks,\npath-consistency (PC) has received a great deal of attention. However,\nenforcing PC is computationally expensive and sometimes even unnecessary.\nDirectional path-consistency (DPC) is a weaker notion of PC that considers a\ngiven variable ordering and can thus be enforced more efficiently than PC. This\npaper shows that DPC (the DPC enforcing algorithm of Dechter and Pearl) decides\nthe constraint satisfaction problem (CSP) of a constraint language if it is\ncomplete and has the variable elimination property (VEP). However, we also show\nthat no complete VEP constraint language can have a domain with more than 2\nvalues. We then present a simple variant of the DPC algorithm, called DPC*, and\nshow that the CSP of a constraint language can be decided by DPC* if it is\nclosed under a majority operation. In fact, DPC* is sufficient for guaranteeing\nbacktrack-free search for such constraint networks. Examples of majority-closed\nconstraint classes include the classes of connected row-convex (CRC)\nconstraints and tree-preserving constraints, which have found applications in\nvarious domains, such as scene labeling, temporal reasoning, geometric\nreasoning, and logical filtering. Our experimental evaluations show that DPC*\nsignificantly outperforms the state-of-the-art algorithms for solving\nmajority-closed constraints.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 07:06:23 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Kong", "Shufeng", ""], ["Li", "Sanjiang", ""], ["Sioutis", "Michael", ""]]}, {"id": "1708.05563", "submitter": "Pedro Almagro-Blanco", "authors": "Pedro Almagro-Blanco, Fernando Sancho-Caparrini", "title": "Induction of Decision Trees based on Generalized Graph Queries", "comments": "Multi-lingual Paper. Main language: English. Additional Language:\n  Spanish. 7 Figures. Engish: 16 pages. Spanish: 20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Usually, decision tree induction algorithms are limited to work with non\nrelational data. Given a record, they do not take into account other objects\nattributes even though they can provide valuable information for the learning\ntask. In this paper we present GGQ-ID3, a multi-relational decision tree\nlearning algorithm that uses Generalized Graph Queries (GGQ) as predicates in\nthe decision nodes. GGQs allow to express complex patterns (including cycles)\nand they can be refined step-by-step. Also, they can evaluate structures (not\nonly single records) and perform Regular Pattern Matching. GGQ are built\ndynamically (pattern mining) during the GGQ-ID3 tree construction process. We\nwill show how to use GGQ-ID3 to perform multi-relational machine learning\nkeeping complexity under control. Finally, some real examples of automatically\nobtained classification trees and semantic patterns are shown.\n  -----\n  Normalmente, los algoritmos de inducci\\'on de \\'arboles de decisi\\'on\ntrabajan con datos no relacionales. Dado un registro, no tienen en cuenta los\natributos de otros objetos a pesar de que \\'estos pueden proporcionar\ninformaci\\'on \\'util para la tarea de aprendizaje. En este art\\'iculo\npresentamos GGQ-ID3, un algoritmo de aprendizaje de \\'arboles de decisiones\nmulti-relacional que utiliza Generalized Graph Queries (GGQ) como predicados en\nlos nodos de decisi\\'on. Los GGQs permiten expresar patrones complejos\n(incluyendo ciclos) y pueden ser refinados paso a paso. Adem\\'as, pueden\nevaluar estructuras (no solo registros) y llevar a cabo Regular Pattern\nMatching. En GGQ-ID3, los GGQ son construidos din\\'amicamente (pattern mining)\ndurante el proceso de construcci\\'on del \\'arbol. Adem\\'as, se muestran algunos\nejemplos reales de \\'arboles de clasificaci\\'on multi-relacionales y patrones\nsem\\'anticos obtenidos autom\\'aticamente.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 11:19:01 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Almagro-Blanco", "Pedro", ""], ["Sancho-Caparrini", "Fernando", ""]]}, {"id": "1708.05565", "submitter": "Yu Wang", "authors": "Yu Wang, Jiayi Liu, Yuxiang Liu, Jun Hao, Yang He, Jinghe Hu, Weipeng\n  P. Yan, Mantian Li", "title": "LADDER: A Human-Level Bidding Agent for Large-Scale Real-Time Online\n  Auctions", "comments": "8 pages, 12 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present LADDER, the first deep reinforcement learning agent that can\nsuccessfully learn control policies for large-scale real-world problems\ndirectly from raw inputs composed of high-level semantic information. The agent\nis based on an asynchronous stochastic variant of DQN (Deep Q Network) named\nDASQN. The inputs of the agent are plain-text descriptions of states of a game\nof incomplete information, i.e. real-time large scale online auctions, and the\nrewards are auction profits of very large scale. We apply the agent to an\nessential portion of JD's online RTB (real-time bidding) advertising business\nand find that it easily beats the former state-of-the-art bidding policy that\nhad been carefully engineered and calibrated by human experts: during JD.com's\nJune 18th anniversary sale, the agent increased the company's ads revenue from\nthe portion by more than 50%, while the advertisers' ROI (return on investment)\nalso improved significantly.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 11:25:30 GMT"}, {"version": "v2", "created": "Fri, 1 Sep 2017 14:05:09 GMT"}], "update_date": "2017-09-04", "authors_parsed": [["Wang", "Yu", ""], ["Liu", "Jiayi", ""], ["Liu", "Yuxiang", ""], ["Hao", "Jun", ""], ["He", "Yang", ""], ["Hu", "Jinghe", ""], ["Yan", "Weipeng P.", ""], ["Li", "Mantian", ""]]}, {"id": "1708.05629", "submitter": "Ying Wei", "authors": "Ying Wei, Yu Zhang, Qiang Yang", "title": "Learning to Transfer", "comments": "12 pages, 8 figures, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer learning borrows knowledge from a source domain to facilitate\nlearning in a target domain. Two primary issues to be addressed in transfer\nlearning are what and how to transfer. For a pair of domains, adopting\ndifferent transfer learning algorithms results in different knowledge\ntransferred between them. To discover the optimal transfer learning algorithm\nthat maximally improves the learning performance in the target domain,\nresearchers have to exhaustively explore all existing transfer learning\nalgorithms, which is computationally intractable. As a trade-off, a sub-optimal\nalgorithm is selected, which requires considerable expertise in an ad-hoc way.\nMeanwhile, it is widely accepted in educational psychology that human beings\nimprove transfer learning skills of deciding what to transfer through\nmeta-cognitive reflection on inductive transfer learning practices. Motivated\nby this, we propose a novel transfer learning framework known as Learning to\nTransfer (L2T) to automatically determine what and how to transfer are the best\nby leveraging previous transfer learning experiences. We establish the L2T\nframework in two stages: 1) we first learn a reflection function encrypting\ntransfer learning skills from experiences; and 2) we infer what and how to\ntransfer for a newly arrived pair of domains by optimizing the reflection\nfunction. Extensive experiments demonstrate the L2T's superiority over several\nstate-of-the-art transfer learning algorithms and its effectiveness on\ndiscovering more transferable knowledge.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 14:36:29 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Wei", "Ying", ""], ["Zhang", "Yu", ""], ["Yang", "Qiang", ""]]}, {"id": "1708.05682", "submitter": "Lu Huang", "authors": "Lu Huang, Jiasong Sun, Ji Xu and Yi Yang", "title": "An Improved Residual LSTM Architecture for Acoustic Modeling", "comments": "5 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Long Short-Term Memory (LSTM) is the primary recurrent neural networks\narchitecture for acoustic modeling in automatic speech recognition systems.\nResidual learning is an efficient method to help neural networks converge\neasier and faster. In this paper, we propose several types of residual LSTM\nmethods for our acoustic modeling. Our experiments indicate that, compared with\nclassic LSTM, our architecture shows more than 8% relative reduction in Phone\nError Rate (PER) on TIMIT tasks. At the same time, our residual fast LSTM\napproach shows 4% relative reduction in PER on the same task. Besides, we find\nthat all this architecture could have good results on THCHS-30, Librispeech and\nSwitchboard corpora.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 01:37:21 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Huang", "Lu", ""], ["Sun", "Jiasong", ""], ["Xu", "Ji", ""], ["Yang", "Yi", ""]]}, {"id": "1708.05688", "submitter": "Kevin Jasberg", "authors": "Kevin Jasberg and Sergej Sizov", "title": "Human Uncertainty and Ranking Error -- The Secret of Successful\n  Evaluation in Predictive Data Mining", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the most crucial issues in data mining is to model human behaviour in\norder to provide personalisation, adaptation and recommendation. This usually\ninvolves implicit or explicit knowledge, either by observing user interactions,\nor by asking users directly. But these sources of information are always\nsubject to the volatility of human decisions, making utilised data uncertain to\na particular extent. In this contribution, we elaborate on the impact of this\nhuman uncertainty when it comes to comparative assessments of different data\nmining approaches. In particular, we reveal two problems: (1) biasing effects\non various metrics of model-based prediction and (2) the propagation of\nuncertainty and its thus induced error probabilities for algorithm rankings.\nFor this purpose, we introduce a probabilistic view and prove the existence of\nthose problems mathematically, as well as provide possible solution strategies.\nWe exemplify our theory mainly in the context of recommender systems along with\nthe metric RMSE as a prominent example of precision quality measures.\n", "versions": [{"version": "v1", "created": "Thu, 17 Aug 2017 12:44:08 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Jasberg", "Kevin", ""], ["Sizov", "Sergej", ""]]}, {"id": "1708.05714", "submitter": "Mark Inman Ph.D.", "authors": "Mark Inman", "title": "A Stronger Foundation for Computer Science and P=NP", "comments": "13 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article describes a Turing machine which can solve for $\\beta^{'}$ which\nis RE-complete. RE-complete problems are proven to be undecidable by Turing's\naccepted proof on the Entscheidungsproblem. Thus, constructing a machine which\ndecides over $\\beta^{'}$ implies inconsistency in ZFC. We then discover that\nunrestricted use of the axiom of substitution can lead to hidden assumptions in\na certain class of proofs by contradiction. These hidden assumptions create an\nimplied axiom of incompleteness for ZFC. Later, we offer a restriction on the\naxiom of substitution by introducing a new axiom which prevents impredicative\ntautologies from producing theorems. Our discovery in regards to these\nfoundational arguments, disproves the SPACE hierarchy theorem which allows us\nto solve the P vs NP problem using a TIME-SPACE equivalence oracle.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 22:36:07 GMT"}, {"version": "v2", "created": "Mon, 23 Apr 2018 12:40:03 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Inman", "Mark", ""]]}, {"id": "1708.05732", "submitter": "Raja Naeem Akram", "authors": "Raja Naeem Akram, Konstantinos Markantonakis, Keith Mayes, Oussama\n  Habachi, Damien Sauveron, Andreas Steyven and Serge Chaumette", "title": "Security, Privacy and Safety Evaluation of Dynamic and Static Fleets of\n  Drones", "comments": "12 Pages, 7 Figures, Conference, The 36th IEEE/AIAA Digital Avionics\n  Systems Conference (DASC'17)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.NE cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inter-connected objects, either via public or private networks are the near\nfuture of modern societies. Such inter-connected objects are referred to as\nInternet-of-Things (IoT) and/or Cyber-Physical Systems (CPS). One example of\nsuch a system is based on Unmanned Aerial Vehicles (UAVs). The fleet of such\nvehicles are prophesied to take on multiple roles involving mundane to\nhigh-sensitive, such as, prompt pizza or shopping deliveries to your homes to\nbattlefield deployment for reconnaissance and combat missions. Drones, as we\nrefer to UAVs in this paper, either can operate individually (solo missions) or\npart of a fleet (group missions), with and without constant connection with the\nbase station. The base station acts as the command centre to manage the\nactivities of the drones. However, an independent, localised and effective\nfleet control is required, potentially based on swarm intelligence, for the\nreasons: 1) increase in the number of drone fleets, 2) number of drones in a\nfleet might be multiple of tens, 3) time-criticality in making decisions by\nsuch fleets in the wild, 4) potential communication congestions/lag, and 5) in\nsome cases working in challenging terrains that hinders or mandates-limited\ncommunication with control centre (i.e., operations spanning long period of\ntimes or military usage of such fleets in enemy territory). This self-ware,\nmission-focused and independent fleet of drones that potential utilises swarm\nintelligence for a) air-traffic and/or flight control management, b) obstacle\navoidance, c) self-preservation while maintaining the mission criteria, d)\ncollaboration with other fleets in the wild (autonomously) and e) assuring the\nsecurity, privacy and safety of physical (drones itself) and virtual (data,\nsoftware) assets. In this paper, we investigate the challenges faced by fleet\nof drones and propose a potential course of action on how to overcome them.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 18:41:38 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Akram", "Raja Naeem", ""], ["Markantonakis", "Konstantinos", ""], ["Mayes", "Keith", ""], ["Habachi", "Oussama", ""], ["Sauveron", "Damien", ""], ["Steyven", "Andreas", ""], ["Chaumette", "Serge", ""]]}, {"id": "1708.05824", "submitter": "Yu Zhao", "authors": "Yu Zhao, Rennong Yang, Guillaume Chevalier, Rajiv Shah, Rob Romijnders", "title": "Applying Deep Bidirectional LSTM and Mixture Density Network for\n  Basketball Trajectory Prediction", "comments": null, "journal-ref": null, "doi": "10.1016/j.ijleo.2017.12.038", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Data analytics helps basketball teams to create tactics. However, manual data\ncollection and analytics are costly and ineffective. Therefore, we applied a\ndeep bidirectional long short-term memory (BLSTM) and mixture density network\n(MDN) approach. This model is not only capable of predicting a basketball\ntrajectory based on real data, but it also can generate new trajectory samples.\nIt is an excellent application to help coaches and players decide when and\nwhere to shoot. Its structure is particularly suitable for dealing with time\nseries problems. BLSTM receives forward and backward information at the same\ntime, while stacking multiple BLSTMs further increases the learning ability of\nthe model. Combined with BLSTMs, MDN is used to generate a multi-modal\ndistribution of outputs. Thus, the proposed model can, in principle, represent\narbitrary conditional probability distributions of output variables. We tested\nour model with two experiments on three-pointer datasets from NBA SportVu data.\nIn the hit-or-miss classification experiment, the proposed model outperformed\nother models in terms of the convergence speed and accuracy. In the trajectory\ngeneration experiment, eight model-generated trajectories at a given time\nclosely matched real trajectories.\n", "versions": [{"version": "v1", "created": "Sat, 19 Aug 2017 08:36:12 GMT"}], "update_date": "2018-02-14", "authors_parsed": [["Zhao", "Yu", ""], ["Yang", "Rennong", ""], ["Chevalier", "Guillaume", ""], ["Shah", "Rajiv", ""], ["Romijnders", "Rob", ""]]}, {"id": "1708.05840", "submitter": "Disha Shrivastava", "authors": "Disha Shrivastava, Santanu Chaudhury and Dr. Jayadeva", "title": "A Data and Model-Parallel, Distributed and Scalable Framework for\n  Training of Deep Networks in Apache Spark", "comments": "12 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep networks is expensive and time-consuming with the training\nperiod increasing with data size and growth in model parameters. In this paper,\nwe provide a framework for distributed training of deep networks over a cluster\nof CPUs in Apache Spark. The framework implements both Data Parallelism and\nModel Parallelism making it suitable to use for deep networks which require\nhuge training data and model parameters which are too big to fit into the\nmemory of a single machine. It can be scaled easily over a cluster of cheap\ncommodity hardware to attain significant speedup and obtain better results\nmaking it quite economical as compared to farm of GPUs and supercomputers. We\nhave proposed a new algorithm for training of deep networks for the case when\nthe network is partitioned across the machines (Model Parallelism) along with\ndetailed cost analysis and proof of convergence of the same. We have developed\nimplementations for Fully-Connected Feedforward Networks, Convolutional Neural\nNetworks, Recurrent Neural Networks and Long Short-Term Memory architectures.\nWe present the results of extensive simulations demonstrating the speedup and\naccuracy obtained by our framework for different sizes of the data and model\nparameters with variation in the number of worker cores/partitions; thereby\nshowing that our proposed framework can achieve significant speedup (upto 11X\nfor CNN) and is also quite scalable.\n", "versions": [{"version": "v1", "created": "Sat, 19 Aug 2017 13:17:58 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Shrivastava", "Disha", ""], ["Chaudhury", "Santanu", ""], ["Jayadeva", "Dr.", ""]]}, {"id": "1708.05866", "submitter": "Kai Arulkumaran", "authors": "Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, Anil Anthony\n  Bharath", "title": "A Brief Survey of Deep Reinforcement Learning", "comments": "IEEE Signal Processing Magazine, Special Issue on Deep Learning for\n  Image Understanding (arXiv extended version)", "journal-ref": null, "doi": "10.1109/MSP.2017.2743240", "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning is poised to revolutionise the field of AI and\nrepresents a step towards building autonomous systems with a higher level\nunderstanding of the visual world. Currently, deep learning is enabling\nreinforcement learning to scale to problems that were previously intractable,\nsuch as learning to play video games directly from pixels. Deep reinforcement\nlearning algorithms are also applied to robotics, allowing control policies for\nrobots to be learned directly from camera inputs in the real world. In this\nsurvey, we begin with an introduction to the general field of reinforcement\nlearning, then progress to the main streams of value-based and policy-based\nmethods. Our survey will cover central algorithms in deep reinforcement\nlearning, including the deep $Q$-network, trust region policy optimisation, and\nasynchronous advantage actor-critic. In parallel, we highlight the unique\nadvantages of deep neural networks, focusing on visual understanding via\nreinforcement learning. To conclude, we describe several current areas of\nresearch within the field.\n", "versions": [{"version": "v1", "created": "Sat, 19 Aug 2017 15:55:31 GMT"}, {"version": "v2", "created": "Thu, 28 Sep 2017 21:51:43 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Arulkumaran", "Kai", ""], ["Deisenroth", "Marc Peter", ""], ["Brundage", "Miles", ""], ["Bharath", "Anil Anthony", ""]]}, {"id": "1708.05872", "submitter": "Muaz Niazi", "authors": "Muaz A. Niazi, Amir Hussain", "title": "Agent-based computing from multi-agent systems to agent-based Models: a\n  visual survey", "comments": "30 pages, 11 figures, 9 tables", "journal-ref": "Scientometrics 89.2 (2011): 479", "doi": "10.1007/s11192-011-0468-9", "report-no": null, "categories": "cs.SI cs.AI cs.DL cs.MA nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Agent-Based Computing is a diverse research domain concerned with the\nbuilding of intelligent software based on the concept of \"agents\". In this\npaper, we use Scientometric analysis to analyze all sub-domains of agent-based\ncomputing. Our data consists of 1,064 journal articles indexed in the ISI web\nof knowledge published during a twenty year period: 1990-2010. These were\nretrieved using a topic search with various keywords commonly used in\nsub-domains of agent-based computing. In our proposed approach, we have\nemployed a combination of two applications for analysis, namely Network\nWorkbench and CiteSpace - wherein Network Workbench allowed for the analysis of\ncomplex network aspects of the domain, detailed visualization-based analysis of\nthe bibliographic data was performed using CiteSpace. Our results include the\nidentification of the largest cluster based on keywords, the timeline of\npublication of index terms, the core journals and key subject categories. We\nalso identify the core authors, top countries of origin of the manuscripts\nalong with core research institutes. Finally, our results have interestingly\nrevealed the strong presence of agent-based computing in a number of\nnon-computing related scientific domains including Life Sciences, Ecological\nSciences and Social Sciences.\n", "versions": [{"version": "v1", "created": "Sat, 19 Aug 2017 16:35:04 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Niazi", "Muaz A.", ""], ["Hussain", "Amir", ""]]}, {"id": "1708.05875", "submitter": "Muaz Niazi", "authors": "Muaz A. Niazi, Amir Hussain", "title": "A novel agent-based simulation framework for sensing in complex adaptive\n  environments", "comments": "8 pages", "journal-ref": "IEEE Sensors Journal 11.2 (2011): 404-412", "doi": "10.1109/JSEN.2010.2068044", "report-no": null, "categories": "cs.NI cs.AI cs.MA cs.SE nlin.AO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a novel Formal Agent-Based Simulation framework\n(FABS). FABS uses formal specification as a means of clear description of\nwireless sensor networks (WSN) sensing a Complex Adaptive Environment. This\nspecification model is then used to develop an agent-based model of both the\nwireless sensor network as well as the environment. As proof of concept, we\ndemonstrate the application of FABS to a boids model of self-organized flocking\nof animals monitored by a random deployment of proximity sensors.\n", "versions": [{"version": "v1", "created": "Sat, 19 Aug 2017 16:48:53 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Niazi", "Muaz A.", ""], ["Hussain", "Amir", ""]]}, {"id": "1708.05878", "submitter": "Sibo Zhang", "authors": "Sibo Zhang, Yuan Cheng, Deyuan Ke", "title": "Event-Radar: Real-time Local Event Detection System for Geo-Tagged Tweet\n  Streams", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The local event detection is to use posting messages with geotags on social\nnetworks to reveal the related ongoing events and their locations. Recent\nstudies have demonstrated that the geo-tagged tweet stream serves as an\nunprecedentedly valuable source for local event detection. Nevertheless, how to\neffectively extract local events from large geo-tagged tweet streams in real\ntime remains challenging. A robust and efficient cloud-based real-time local\nevent detection software system would benefit various aspects in the real-life\nsociety, from shopping recommendation for customer service providers to\ndisaster alarming for emergency departments. We use the preliminary research\nGeoBurst as a starting point, which proposed a novel method to detect local\nevents. GeoBurst+ leverages a novel cross-modal authority measure to identify\nseveral pivots in the query window. Such pivots reveal different geo-topical\nactivities and naturally attract related tweets to form candidate events. It\nfurther summarises the continuous stream and compares the candidates against\nthe historical summaries to pinpoint truly interesting local events. We mainly\nimplement a website demonstration system Event-Radar with an improved algorithm\nto show the real-time local events online for public interests. Better still,\nas the query window shifts, our method can update the event list with little\ntime cost, thus achieving continuous monitoring of the stream.\n", "versions": [{"version": "v1", "created": "Sat, 19 Aug 2017 17:32:24 GMT"}, {"version": "v2", "created": "Thu, 5 Oct 2017 14:07:23 GMT"}], "update_date": "2020-04-07", "authors_parsed": [["Zhang", "Sibo", ""], ["Cheng", "Yuan", ""], ["Ke", "Deyuan", ""]]}, {"id": "1708.05930", "submitter": "Longfei Wang", "authors": "Haoyuan Hu, Xiaodong Zhang, Xiaowei Yan, Longfei Wang, Yinghui Xu", "title": "Solving a New 3D Bin Packing Problem with Deep Reinforcement Learning\n  Method", "comments": "7 pages, 1 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, a new type of 3D bin packing problem (BPP) is proposed, in\nwhich a number of cuboid-shaped items must be put into a bin one by one\northogonally. The objective is to find a way to place these items that can\nminimize the surface area of the bin. This problem is based on the fact that\nthere is no fixed-sized bin in many real business scenarios and the cost of a\nbin is proportional to its surface area. Our research shows that this problem\nis NP-hard. Based on previous research on 3D BPP, the surface area is\ndetermined by the sequence, spatial locations and orientations of items. Among\nthese factors, the sequence of items plays a key role in minimizing the surface\narea. Inspired by recent achievements of deep reinforcement learning (DRL)\ntechniques, especially Pointer Network, on combinatorial optimization problems\nsuch as TSP, a DRL-based method is applied to optimize the sequence of items to\nbe packed into the bin. Numerical results show that the method proposed in this\npaper achieve about 5% improvement than heuristic method.\n", "versions": [{"version": "v1", "created": "Sun, 20 Aug 2017 03:53:04 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Hu", "Haoyuan", ""], ["Zhang", "Xiaodong", ""], ["Yan", "Xiaowei", ""], ["Wang", "Longfei", ""], ["Xu", "Yinghui", ""]]}, {"id": "1708.05935", "submitter": "Ali Al-Bayaty", "authors": "Ali Al-Bayaty", "title": "Software-Defined Robotics -- Idea & Approach", "comments": "4 pages, 4 figures, robotics and automation, Software-Defined\n  Robotics", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The methodology of Software-Defined Robotics hierarchical-based and\nstand-alone framework can be designed and implemented to program and control\ndifferent sets of robots, regardless of their manufacturers' parameters and\nspecifications, with unified commands and communications. This framework\napproach will increase the capability of (re)programming a specific group of\nrobots during the runtime without affecting the others as desired in the\ncritical missions and industrial operations, expand the shared bandwidth,\nenhance the reusability of code, leverage the computational processing power,\ndecrease the unnecessary analyses of vast supplemental electrical components\nfor each robot, as well as get advantages of the most state-of-the-art\nindustrial trends in the cloud-based computing, Virtual Machines (VM), and\nRobot-as-a-Service (RaaS) technologies.\n", "versions": [{"version": "v1", "created": "Sun, 20 Aug 2017 06:25:57 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Al-Bayaty", "Ali", ""]]}, {"id": "1708.05997", "submitter": "Youssef Oualil", "authors": "Youssef Oualil, Dietrich Klakow", "title": "A Batch Noise Contrastive Estimation Approach for Training Large\n  Vocabulary Language Models", "comments": "Accepted for publication at INTERSPEECH'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training large vocabulary Neural Network Language Models (NNLMs) is a\ndifficult task due to the explicit requirement of the output layer\nnormalization, which typically involves the evaluation of the full softmax\nfunction over the complete vocabulary. This paper proposes a Batch Noise\nContrastive Estimation (B-NCE) approach to alleviate this problem. This is\nachieved by reducing the vocabulary, at each time step, to the target words in\nthe batch and then replacing the softmax by the noise contrastive estimation\napproach, where these words play the role of targets and noise samples at the\nsame time. In doing so, the proposed approach can be fully formulated and\nimplemented using optimal dense matrix operations. Applying B-NCE to train\ndifferent NNLMs on the Large Text Compression Benchmark (LTCB) and the One\nBillion Word Benchmark (OBWB) shows a significant reduction of the training\ntime with no noticeable degradation of the models performance. This paper also\npresents a new baseline comparative study of different standard NNLMs on the\nlarge OBWB on a single Titan-X GPU.\n", "versions": [{"version": "v1", "created": "Sun, 20 Aug 2017 17:48:35 GMT"}, {"version": "v2", "created": "Tue, 22 Aug 2017 09:15:38 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Oualil", "Youssef", ""], ["Klakow", "Dietrich", ""]]}, {"id": "1708.06000", "submitter": "Wei Wei", "authors": "Wei Wei, Kennth Joseph, Kathleen Carley", "title": "Efficient Online Inference for Infinite Evolutionary Cluster models with\n  Applications to Latent Social Event Discovery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.SI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The Recurrent Chinese Restaurant Process (RCRP) is a powerful statistical\nmethod for modeling evolving clusters in large scale social media data. With\nthe RCRP, one can allow both the number of clusters and the cluster parameters\nin a model to change over time. However, application of the RCRP has largely\nbeen limited due to the non-conjugacy between the cluster evolutionary priors\nand the Multinomial likelihood. This non-conjugacy makes inference di cult and\nrestricts the scalability of models which use the RCRP, leading to the RCRP\nbeing applied only in simple problems, such as those that can be approximated\nby a single Gaussian emission. In this paper, we provide a novel solution for\nthe non-conjugacy issues for the RCRP and an example of how to leverage our\nsolution for one speci c problem - the social event discovery problem. By\nutilizing Sequential Monte Carlo methods in inference, our approach can be\nmassively paralleled and is highly scalable, to the extent it can work on tens\nof millions of documents. We are able to generate high quality topical and\nlocation distributions of the clusters that can be directly interpreted as real\nsocial events, and our experimental results suggest that the approaches\nproposed achieve much better predictive performance than techniques reported in\nprior work. We also demonstrate how the techniques we develop can be used in a\nmuch more general ways toward similar problems.\n", "versions": [{"version": "v1", "created": "Sun, 20 Aug 2017 18:17:27 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Wei", "Wei", ""], ["Joseph", "Kennth", ""], ["Carley", "Kathleen", ""]]}, {"id": "1708.06039", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Delia Fernandez, Alejandro Woodward, Victor Campos, Xavier\n  Giro-i-Nieto, Brendan Jou and Shih-Fu Chang", "title": "More cat than cute? Interpretable Prediction of Adjective-Noun Pairs", "comments": "Oral paper at ACM Multimedia 2017 Workshop on Multimodal\n  Understanding of Social, Affective and Subjective Attributes (MUSA2)", "journal-ref": null, "doi": "10.1145/3132515.3132520", "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The increasing availability of affect-rich multimedia resources has bolstered\ninterest in understanding sentiment and emotions in and from visual content.\nAdjective-noun pairs (ANP) are a popular mid-level semantic construct for\ncapturing affect via visually detectable concepts such as \"cute dog\" or\n\"beautiful landscape\". Current state-of-the-art methods approach ANP prediction\nby considering each of these compound concepts as individual tokens, ignoring\nthe underlying relationships in ANPs. This work aims at disentangling the\ncontributions of the `adjectives' and `nouns' in the visual prediction of ANPs.\nTwo specialised classifiers, one trained for detecting adjectives and another\nfor nouns, are fused to predict 553 different ANPs. The resulting ANP\nprediction model is more interpretable as it allows us to study contributions\nof the adjective and noun components. Source code and models are available at\nhttps://imatge-upc.github.io/affective-2017-musa2/ .\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 00:33:05 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Fernandez", "Delia", ""], ["Woodward", "Alejandro", ""], ["Campos", "Victor", ""], ["Giro-i-Nieto", "Xavier", ""], ["Jou", "Brendan", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1708.06040", "submitter": "Tongzhou Wang", "authors": "Tongzhou Wang, Yi Wu, David A. Moore, Stuart J. Russell", "title": "Meta-Learning MCMC Proposals", "comments": "32nd Conference on Neural Information Processing Systems (NeurIPS\n  2018), Montreal, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Effective implementations of sampling-based probabilistic inference often\nrequire manually constructed, model-specific proposals. Inspired by recent\nprogresses in meta-learning for training learning agents that can generalize to\nunseen environments, we propose a meta-learning approach to building effective\nand generalizable MCMC proposals. We parametrize the proposal as a neural\nnetwork to provide fast approximations to block Gibbs conditionals. The learned\nneural proposals generalize to occurrences of common structural motifs across\ndifferent models, allowing for the construction of a library of learned\ninference primitives that can accelerate inference on unseen models with no\nmodel-specific training required. We explore several applications including\nopen-universe Gaussian mixture models, in which our learned proposals\noutperform a hand-tuned sampler, and a real-world named entity recognition\ntask, in which our sampler yields higher final F1 scores than classical\nsingle-site Gibbs sampling.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 00:44:32 GMT"}, {"version": "v2", "created": "Sun, 3 Dec 2017 18:47:50 GMT"}, {"version": "v3", "created": "Thu, 14 Dec 2017 04:32:39 GMT"}, {"version": "v4", "created": "Tue, 27 Nov 2018 12:09:11 GMT"}, {"version": "v5", "created": "Tue, 1 Jan 2019 06:47:06 GMT"}], "update_date": "2019-01-03", "authors_parsed": [["Wang", "Tongzhou", ""], ["Wu", "Yi", ""], ["Moore", "David A.", ""], ["Russell", "Stuart J.", ""]]}, {"id": "1708.06068", "submitter": "Barathi Ganesh H B", "authors": "Barathi Ganesh HB and Anand Kumar M and Soman KP", "title": "Vector Space Model as Cognitive Space for Text Classification", "comments": "6 pages, 6 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this era of digitization, knowing the user's sociolect aspects have become\nessential features to build the user specific recommendation systems. These\nsociolect aspects could be found by mining the user's language sharing in the\nform of text in social media and reviews. This paper describes about the\nexperiment that was performed in PAN Author Profiling 2017 shared task. The\nobjective of the task is to find the sociolect aspects of the users from their\ntweets. The sociolect aspects considered in this experiment are user's gender\nand native language information. Here user's tweets written in a different\nlanguage from their native language are represented as Document - Term Matrix\nwith document frequency as the constraint. Further classification is done using\nthe Support Vector Machine by taking gender and native language as target\nclasses. This experiment attains the average accuracy of 73.42% in gender\nprediction and 76.26% in the native language identification task.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 03:06:07 GMT"}], "update_date": "2018-04-13", "authors_parsed": [["HB", "Barathi Ganesh", ""], ["M", "Anand Kumar", ""], ["KP", "Soman", ""]]}, {"id": "1708.06233", "submitter": "Co-Pierre Georg", "authors": "Christoph Aymanns and Jakob Foerster and Co-Pierre Georg", "title": "Fake News in Social Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA cs.SI physics.soc-ph q-fin.EC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We model the spread of news as a social learning game on a network. Agents\ncan either endorse or oppose a claim made in a piece of news, which itself may\nbe either true or false. Agents base their decision on a private signal and\ntheir neighbors' past actions. Given these inputs, agents follow strategies\nderived via multi-agent deep reinforcement learning and receive utility from\nacting in accordance with the veracity of claims. Our framework yields\nstrategies with agent utility close to a theoretical, Bayes optimal benchmark,\nwhile remaining flexible to model re-specification. Optimized strategies allow\nagents to correctly identify most false claims, when all agents receive\nunbiased private signals. However, an adversary's attempt to spread fake news\nby targeting a subset of agents with a biased private signal can be successful.\nEven more so when the adversary has information about agents' network position\nor private signal. When agents are aware of the presence of an adversary they\nre-optimize their strategies in the training stage and the adversary's attack\nis less effective. Hence, exposing agents to the possibility of fake news can\nbe an effective way to curtail the spread of fake news in social networks. Our\nresults also highlight that information about the users' private beliefs and\ntheir social network structure can be extremely valuable to adversaries and\nshould be well protected.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 14:09:31 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Aymanns", "Christoph", ""], ["Foerster", "Jakob", ""], ["Georg", "Co-Pierre", ""]]}, {"id": "1708.06246", "submitter": "Garima Gupta", "authors": "Karamjit Singh, Garima Gupta, Vartika Tewari and Gautam Shroff", "title": "Comparative Benchmarking of Causal Discovery Techniques", "comments": "arXiv admin note: text overlap with arXiv:1506.07669,\n  arXiv:1611.03977 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present a comprehensive view of prominent causal discovery\nalgorithms, categorized into two main categories (1) assuming acyclic and no\nlatent variables, and (2) allowing both cycles and latent variables, along with\nexperimental results comparing them from three perspectives: (a) structural\naccuracy, (b) standard predictive accuracy, and (c) accuracy of counterfactual\ninference. For (b) and (c) we train causal Bayesian networks with structures as\npredicted by each causal discovery technique to carry out counterfactual or\nstandard predictive inference. We compare causal algorithms on two pub- licly\navailable and one simulated datasets having different sample sizes: small,\nmedium and large. Experiments show that structural accuracy of a technique does\nnot necessarily correlate with higher accuracy of inferencing tasks. Fur- ther,\nsurveyed structure learning algorithms do not perform well in terms of\nstructural accuracy in case of datasets having large number of variables.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 04:18:30 GMT"}, {"version": "v2", "created": "Tue, 12 Sep 2017 04:02:48 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Singh", "Karamjit", ""], ["Gupta", "Garima", ""], ["Tewari", "Vartika", ""], ["Shroff", "Gautam", ""]]}, {"id": "1708.06257", "submitter": "Zhen Li", "authors": "Zhen Li, Zuoqiang Shi", "title": "A Flow Model of Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Based on a natural connection between ResNet and transport equation or its\ncharacteristic equation, we propose a continuous flow model for both ResNet and\nplain net. Through this continuous model, a ResNet can be explicitly\nconstructed as a refinement of a plain net. The flow model provides an\nalternative perspective to understand phenomena in deep neural networks, such\nas why it is necessary and sufficient to use 2-layer blocks in ResNets, why\ndeeper is better, and why ResNets are even deeper, and so on. It also opens a\ngate to bring in more tools from the huge area of differential equations.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 14:30:49 GMT"}, {"version": "v2", "created": "Mon, 11 Dec 2017 11:26:00 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Li", "Zhen", ""], ["Shi", "Zuoqiang", ""]]}, {"id": "1708.06266", "submitter": "Zied Bouraoui", "authors": "Zied Bouraoui, Shoaib Jameel, Steven Schockaert", "title": "Probabilistic Relation Induction in Vector Space Embeddings", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Word embeddings have been found to capture a surprisingly rich amount of\nsyntactic and semantic knowledge. However, it is not yet sufficiently\nwell-understood how the relational knowledge that is implicitly encoded in word\nembeddings can be extracted in a reliable way. In this paper, we propose two\nprobabilistic models to address this issue. The first model is based on the\ncommon relations-as-translations view, but is cast in a probabilistic setting.\nOur second model is based on the much weaker assumption that there is a linear\nrelationship between the vector representations of related words. Compared to\nexisting approaches, our models lead to more accurate predictions, and they are\nmore explicit about what can and cannot be extracted from the word embedding.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 14:52:10 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Bouraoui", "Zied", ""], ["Jameel", "Shoaib", ""], ["Schockaert", "Steven", ""]]}, {"id": "1708.06276", "submitter": "Alessandro Saffiotti", "authors": "Barbara Bruno, Nak Young Chong, Hiroko Kamide, Sanjeev Kanoria,\n  Jaeryoung Lee, Yuto Lim, Amit Kumar Pandey, Chris Papadopoulos, Irena\n  Papadopoulos, Federico Pecora, Alessandro Saffiotti, Antonio Sgorbissa", "title": "The CARESSES EU-Japan project: making assistive robots culturally\n  competent", "comments": "Paper presented at: Ambient Assisted Living, Italian Forum. Genova,\n  Italy, June 12--15, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CY cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The nursing literature shows that cultural competence is an important\nrequirement for effective healthcare. We claim that personal assistive robots\nshould likewise be culturally competent, that is, they should be aware of\ngeneral cultural characteristics and of the different forms they take in\ndifferent individuals, and take these into account while perceiving, reasoning,\nand acting. The CARESSES project is an Europe-Japan collaborative effort that\naims at designing, developing and evaluating culturally competent assistive\nrobots. These robots will be able to adapt the way they behave, speak and\ninteract to the cultural identity of the person they assist. This paper\ndescribes the approach taken in the CARESSES project, its initial steps, and\nits future plans.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 15:12:54 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Bruno", "Barbara", ""], ["Chong", "Nak Young", ""], ["Kamide", "Hiroko", ""], ["Kanoria", "Sanjeev", ""], ["Lee", "Jaeryoung", ""], ["Lim", "Yuto", ""], ["Pandey", "Amit Kumar", ""], ["Papadopoulos", "Chris", ""], ["Papadopoulos", "Irena", ""], ["Pecora", "Federico", ""], ["Saffiotti", "Alessandro", ""], ["Sgorbissa", "Antonio", ""]]}, {"id": "1708.06303", "submitter": "Ivan Brugere", "authors": "Ivan Brugere and Chris Kanich and Tanya Y. Berger-Wolf", "title": "Network Model Selection for Task-Focused Attributed Network Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Networks are models representing relationships between entities. Often these\nrelationships are explicitly given, or we must learn a representation which\ngeneralizes and predicts observed behavior in underlying individual data (e.g.\nattributes or labels). Whether given or inferred, choosing the best\nrepresentation affects subsequent tasks and questions on the network. This work\nfocuses on model selection to evaluate network representations from data,\nfocusing on fundamental predictive tasks on networks. We present a modular\nmethodology using general, interpretable network models, task neighborhood\nfunctions found across domains, and several criteria for robust model\nselection. We demonstrate our methodology on three online user activity\ndatasets and show that network model selection for the appropriate network task\nvs. an alternate task increases performance by an order of magnitude in our\nexperiments.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 16:04:17 GMT"}, {"version": "v2", "created": "Sat, 16 Sep 2017 04:08:22 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Brugere", "Ivan", ""], ["Kanich", "Chris", ""], ["Berger-Wolf", "Tanya Y.", ""]]}, {"id": "1708.06374", "submitter": "Shai Shalev-Shwartz", "authors": "Shai Shalev-Shwartz, Shaked Shammah, Amnon Shashua", "title": "On a Formal Model of Safe and Scalable Self-driving Cars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, car makers and tech companies have been racing towards self\ndriving cars. It seems that the main parameter in this race is who will have\nthe first car on the road. The goal of this paper is to add to the equation two\nadditional crucial parameters. The first is standardization of safety assurance\n--- what are the minimal requirements that every self-driving car must satisfy,\nand how can we verify these requirements. The second parameter is scalability\n--- engineering solutions that lead to unleashed costs will not scale to\nmillions of cars, which will push interest in this field into a niche academic\ncorner, and drive the entire field into a \"winter of autonomous driving\". In\nthe first part of the paper we propose a white-box, interpretable, mathematical\nmodel for safety assurance, which we call Responsibility-Sensitive Safety\n(RSS). In the second part we describe a design of a system that adheres to our\nsafety assurance requirements and is scalable to millions of cars.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 18:22:19 GMT"}, {"version": "v2", "created": "Sun, 8 Oct 2017 04:06:22 GMT"}, {"version": "v3", "created": "Tue, 12 Dec 2017 17:15:39 GMT"}, {"version": "v4", "created": "Mon, 18 Dec 2017 07:10:24 GMT"}, {"version": "v5", "created": "Thu, 15 Mar 2018 04:39:41 GMT"}, {"version": "v6", "created": "Sat, 27 Oct 2018 09:10:37 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Shalev-Shwartz", "Shai", ""], ["Shammah", "Shaked", ""], ["Shashua", "Amnon", ""]]}, {"id": "1708.06425", "submitter": "Mustafa Kocak", "authors": "Mustafa A. Kocak, David Ramirez, Elza Erkip, and Dennis E. Shasha", "title": "SafePredict: A Meta-Algorithm for Machine Learning That Uses Refusals to\n  Guarantee Correctness", "comments": "Submitted to IEEE Transactions on Pattern Analysis and Machine\n  Intelligence, August 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  SafePredict is a novel meta-algorithm that works with any base prediction\nalgorithm for online data to guarantee an arbitrarily chosen correctness rate,\n$1-\\epsilon$, by allowing refusals. Allowing refusals means that the\nmeta-algorithm may refuse to emit a prediction produced by the base algorithm\non occasion so that the error rate on non-refused predictions does not exceed\n$\\epsilon$. The SafePredict error bound does not rely on any assumptions on the\ndata distribution or the base predictor. When the base predictor happens not to\nexceed the target error rate $\\epsilon$, SafePredict refuses only a finite\nnumber of times. When the error rate of the base predictor changes through time\nSafePredict makes use of a weight-shifting heuristic that adapts to these\nchanges without knowing when the changes occur yet still maintains the\ncorrectness guarantee. Empirical results show that (i) SafePredict compares\nfavorably with state-of-the art confidence based refusal mechanisms which fail\nto offer robust error guarantees; and (ii) combining SafePredict with such\nrefusal mechanisms can in many cases further reduce the number of refusals. Our\nsoftware (currently in Python) is included in the supplementary material.\n", "versions": [{"version": "v1", "created": "Mon, 21 Aug 2017 21:23:42 GMT"}, {"version": "v2", "created": "Thu, 9 Nov 2017 03:35:00 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Kocak", "Mustafa A.", ""], ["Ramirez", "David", ""], ["Erkip", "Elza", ""], ["Shasha", "Dennis E.", ""]]}, {"id": "1708.06519", "submitter": "Zhuang Liu", "authors": "Zhuang Liu and Jianguo Li and Zhiqiang Shen and Gao Huang and Shoumeng\n  Yan and Changshui Zhang", "title": "Learning Efficient Convolutional Networks through Network Slimming", "comments": "Accepted by ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The deployment of deep convolutional neural networks (CNNs) in many real\nworld applications is largely hindered by their high computational cost. In\nthis paper, we propose a novel learning scheme for CNNs to simultaneously 1)\nreduce the model size; 2) decrease the run-time memory footprint; and 3) lower\nthe number of computing operations, without compromising accuracy. This is\nachieved by enforcing channel-level sparsity in the network in a simple but\neffective way. Different from many existing approaches, the proposed method\ndirectly applies to modern CNN architectures, introduces minimum overhead to\nthe training process, and requires no special software/hardware accelerators\nfor the resulting models. We call our approach network slimming, which takes\nwide and large networks as input models, but during training insignificant\nchannels are automatically identified and pruned afterwards, yielding thin and\ncompact models with comparable accuracy. We empirically demonstrate the\neffectiveness of our approach with several state-of-the-art CNN models,\nincluding VGGNet, ResNet and DenseNet, on various image classification\ndatasets. For VGGNet, a multi-pass version of network slimming gives a 20x\nreduction in model size and a 5x reduction in computing operations.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 07:35:26 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Liu", "Zhuang", ""], ["Li", "Jianguo", ""], ["Shen", "Zhiqiang", ""], ["Huang", "Gao", ""], ["Yan", "Shoumeng", ""], ["Zhang", "Changshui", ""]]}, {"id": "1708.06551", "submitter": "Denis Steckelmacher", "authors": "Denis Steckelmacher, Diederik M. Roijers, Anna Harutyunyan, Peter\n  Vrancx, H\\'el\\`ene Plisnier, Ann Now\\'e", "title": "Reinforcement Learning in POMDPs with Memoryless Options and\n  Option-Observation Initiation Sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real-world reinforcement learning problems have a hierarchical nature,\nand often exhibit some degree of partial observability. While hierarchy and\npartial observability are usually tackled separately (for instance by combining\nrecurrent neural networks and options), we show that addressing both problems\nsimultaneously is simpler and more efficient in many cases. More specifically,\nwe make the initiation set of options conditional on the previously-executed\noption, and show that options with such Option-Observation Initiation Sets\n(OOIs) are at least as expressive as Finite State Controllers (FSCs), a\nstate-of-the-art approach for learning in POMDPs. OOIs are easy to design based\non an intuitive description of the task, lead to explainable policies and keep\nthe top-level and option policies memoryless. Our experiments show that OOIs\nallow agents to learn optimal policies in challenging POMDPs, while being much\nmore sample-efficient than a recurrent neural network over options.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 09:51:18 GMT"}, {"version": "v2", "created": "Tue, 12 Sep 2017 08:34:04 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Steckelmacher", "Denis", ""], ["Roijers", "Diederik M.", ""], ["Harutyunyan", "Anna", ""], ["Vrancx", "Peter", ""], ["Plisnier", "H\u00e9l\u00e8ne", ""], ["Now\u00e9", "Ann", ""]]}, {"id": "1708.06564", "submitter": "Benjamin Paassen", "authors": "Benjamin Paa{\\ss}en and Barbara Hammer and Thomas William Price and\n  Tiffany Barnes and Sebastian Gross and Niels Pinkwart", "title": "The Continuous Hint Factory - Providing Hints in Vast and Sparsely\n  Populated Edit Distance Spaces", "comments": null, "journal-ref": "Journal of Educational Data Mining, 10 (2018) 1-35. Retrieved from\n  https://jedm.educationaldatamining.org/index.php/JEDM/article/view/158", "doi": null, "report-no": null, "categories": "cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intelligent tutoring systems can support students in solving multi-step tasks\nby providing hints regarding what to do next. However, engineering such\nnext-step hints manually or via an expert model becomes infeasible if the space\nof possible states is too large. Therefore, several approaches have emerged to\ninfer next-step hints automatically, relying on past students' data. In\nparticular, the Hint Factory (Barnes & Stamper, 2008) recommends edits that are\nmost likely to guide students from their current state towards a correct\nsolution, based on what successful students in the past have done in the same\nsituation. Still, the Hint Factory relies on student data being available for\nany state a student might visit while solving the task, which is not the case\nfor some learning tasks, such as open-ended programming tasks. In this\ncontribution we provide a mathematical framework for edit-based hint policies\nand, based on this theory, propose a novel hint policy to provide edit hints in\nvast and sparsely populated state spaces. In particular, we extend the Hint\nFactory by considering data of past students in all states which are similar to\nthe student's current state and creating hints approximating the weighted\naverage of all these reference states. Because the space of possible weighted\naverages is continuous, we call this approach the Continuous Hint Factory. In\nour experimental evaluation, we demonstrate that the Continuous Hint Factory\ncan predict more accurately what capable students would do compared to existing\nprediction schemes on two learning tasks, especially in an open-ended\nprogramming task, and that the Continuous Hint Factory is comparable to\nexisting hint policies at reproducing tutor hints on a simple UML diagram task.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 11:33:46 GMT"}, {"version": "v2", "created": "Sat, 30 Jun 2018 23:00:11 GMT"}], "update_date": "2018-07-03", "authors_parsed": [["Paa\u00dfen", "Benjamin", ""], ["Hammer", "Barbara", ""], ["Price", "Thomas William", ""], ["Barnes", "Tiffany", ""], ["Gross", "Sebastian", ""], ["Pinkwart", "Niels", ""]]}, {"id": "1708.06665", "submitter": "J. G. Wolff", "authors": "J Gerard Wolff", "title": "Software engineering and the SP Theory of Intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a novel approach to software engineering derived from\nthe \"SP Theory of Intelligence\" and its realisation in the \"SP Computer Model\".\nDespite superficial appearances, it is shown that many of the key ideas in\nsoftware engineering have counterparts in the structure and workings of the SP\nsystem. Potential benefits of this new approach to software engineering\ninclude: the automation or semi-automation of software development, with\nsupport for programming of the SP system where necessary; allowing programmers\nto concentrate on 'world-oriented' parallelism, without worries about\nparallelism to speed up processing; support for the long-term goal of\nprogramming the SP system via written or spoken natural language; reducing or\neliminating the distinction between 'design' and 'implementation'; reducing or\neliminating operations like compiling or interpretation; reducing or\neliminating the need for verification of software; reducing the need for\nvalidation of software; no formal distinction between program and database; the\npotential for substantial reductions in the number of types of data file and\nthe number of computer languages; benefits for version control; and reducing\ntechnical debt.\n", "versions": [{"version": "v1", "created": "Fri, 18 Aug 2017 19:27:49 GMT"}, {"version": "v2", "created": "Sun, 5 Aug 2018 08:44:16 GMT"}], "update_date": "2018-08-07", "authors_parsed": [["Wolff", "J Gerard", ""]]}, {"id": "1708.06716", "submitter": "Larissa Albantakis", "authors": "Larissa Albantakis, William Marshall, Erik Hoel, Giulio Tononi", "title": "What caused what? A quantitative account of actual causation using\n  dynamical causal networks", "comments": "43 pages, 16 figures, supplementary discussion, supplementary\n  methods, supplementary proofs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Actual causation is concerned with the question \"what caused what?\" Consider\na transition between two states within a system of interacting elements, such\nas an artificial neural network, or a biological brain circuit. Which\ncombination of synapses caused the neuron to fire? Which image features caused\nthe classifier to misinterpret the picture? Even detailed knowledge of the\nsystem's causal network, its elements, their states, connectivity, and dynamics\ndoes not automatically provide a straightforward answer to the \"what caused\nwhat?\" question. Counterfactual accounts of actual causation based on graphical\nmodels, paired with system interventions, have demonstrated initial success in\naddressing specific problem cases in line with intuitive causal judgments.\nHere, we start from a set of basic requirements for causation (realization,\ncomposition, information, integration, and exclusion) and develop a rigorous,\nquantitative account of actual causation that is generally applicable to\ndiscrete dynamical systems. We present a formal framework to evaluate these\ncausal requirements that is based on system interventions and partitions, and\nconsiders all counterfactuals of a state transition. This framework is used to\nprovide a complete causal account of the transition by identifying and\nquantifying the strength of all actual causes and effects linking the two\nconsecutive system states. Finally, we examine several exemplary cases and\nparadoxes of causation and show that they can be illuminated by the proposed\nframework for quantifying actual causation.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 16:51:45 GMT"}, {"version": "v2", "created": "Wed, 9 Jan 2019 20:53:07 GMT"}], "update_date": "2019-01-11", "authors_parsed": [["Albantakis", "Larissa", ""], ["Marshall", "William", ""], ["Hoel", "Erik", ""], ["Tononi", "Giulio", ""]]}, {"id": "1708.06794", "submitter": "Jonti Talukdar", "authors": "Jonti Talukdar and Bhavana Mehta", "title": "Human Action Recognition System using Good Features and Multilayer\n  Perceptron Network", "comments": "6 pages, 7 Figures, IEEE International Conference on Communication\n  and Signal Processing 2017 (ICCSP 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human action recognition involves the characterization of human actions\nthrough the automated analysis of video data and is integral in the development\nof smart computer vision systems. However, several challenges like dynamic\nbackgrounds, camera stabilization, complex actions, occlusions etc. make action\nrecognition in a real time and robust fashion difficult. Several complex\napproaches exist but are computationally intensive. This paper presents a novel\napproach of using a combination of good features along with iterative optical\nflow algorithm to compute feature vectors which are classified using a\nmultilayer perceptron (MLP) network. The use of multiple features for motion\ndescriptors enhances the quality of tracking. Resilient backpropagation\nalgorithm is used for training the feedforward neural network reducing the\nlearning time. The overall system accuracy is improved by optimizing the\nvarious parameters of the multilayer perceptron network.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 19:39:45 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Talukdar", "Jonti", ""], ["Mehta", "Bhavana", ""]]}, {"id": "1708.06816", "submitter": "Bhushan Kotnis", "authors": "Bhushan Kotnis and Vivi Nastase", "title": "Analysis of the Impact of Negative Sampling on Link Prediction in\n  Knowledge Graphs", "comments": "14 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge graphs are large, useful, but incomplete knowledge repositories.\nThey encode knowledge through entities and relations which define each other\nthrough the connective structure of the graph. This has inspired methods for\nthe joint embedding of entities and relations in continuous low-dimensional\nvector spaces, that can be used to induce new edges in the graph, i.e., link\nprediction in knowledge graphs. Learning these representations relies on\ncontrasting positive instances with negative ones. Knowledge graphs include\nonly positive relation instances, leaving the door open for a variety of\nmethods for selecting negative examples. In this paper we present an empirical\nstudy on the impact of negative sampling on the learned embeddings, assessed\nthrough the task of link prediction. We use state-of-the-art knowledge graph\nembeddings -- \\rescal , TransE, DistMult and ComplEX -- and evaluate on\nbenchmark datasets -- FB15k and WN18. We compare well known methods for\nnegative sampling and additionally propose embedding based sampling methods. We\nnote a marked difference in the impact of these sampling methods on the two\ndatasets, with the \"traditional\" corrupting positives method leading to best\nresults on WN18, while embedding based methods benefiting the task on FB15k.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 20:53:29 GMT"}, {"version": "v2", "created": "Fri, 2 Mar 2018 12:27:10 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Kotnis", "Bhushan", ""], ["Nastase", "Vivi", ""]]}, {"id": "1708.06828", "submitter": "Bonggun Shin", "authors": "Bonggun Shin, Falgun H. Chokshi, Timothy Lee and Jinho D. Choi", "title": "Classification of Radiology Reports Using Neural Attention Models", "comments": null, "journal-ref": "In Proceedings of the International Joint Conference on Neural\n  Networks, of IJCNN'17, pages 4363--4370, Anchorage, AK, 2017", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  The electronic health record (EHR) contains a large amount of\nmulti-dimensional and unstructured clinical data of significant operational and\nresearch value. Distinguished from previous studies, our approach embraces a\ndouble-annotated dataset and strays away from obscure \"black-box\" models to\ncomprehensive deep learning models. In this paper, we present a novel neural\nattention mechanism that not only classifies clinically important findings.\nSpecifically, convolutional neural networks (CNN) with attention analysis are\nused to classify radiology head computed tomography reports based on five\ncategories that radiologists would account for in assessing acute and\ncommunicable findings in daily practice. The experiments show that our CNN\nattention models outperform non-neural models, especially when trained on a\nlarger dataset. Our attention analysis demonstrates the intuition behind the\nclassifier's decision by generating a heatmap that highlights attended terms\nused by the CNN model; this is valuable when potential downstream medical\ndecisions are to be performed by human experts or the classifier information is\nto be used in cohort construction such as for epidemiological studies.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 21:30:23 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Shin", "Bonggun", ""], ["Chokshi", "Falgun H.", ""], ["Lee", "Timothy", ""], ["Choi", "Jinho D.", ""]]}, {"id": "1708.06832", "submitter": "Hanzhang Hu", "authors": "Hanzhang Hu, Debadeepta Dey, Martial Hebert, J. Andrew Bagnell", "title": "Learning Anytime Predictions in Neural Networks via Adaptive Loss\n  Balancing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work considers the trade-off between accuracy and test-time\ncomputational cost of deep neural networks (DNNs) via \\emph{anytime}\npredictions from auxiliary predictions. Specifically, we optimize auxiliary\nlosses jointly in an \\emph{adaptive} weighted sum, where the weights are\ninversely proportional to average of each loss. Intuitively, this balances the\nlosses to have the same scale. We demonstrate theoretical considerations that\nmotivate this approach from multiple viewpoints, including connecting it to\noptimizing the geometric mean of the expectation of each loss, an objective\nthat ignores the scale of losses. Experimentally, the adaptive weights induce\nmore competitive anytime predictions on multiple recognition data-sets and\nmodels than non-adaptive approaches including weighing all losses equally. In\nparticular, anytime neural networks (ANNs) can achieve the same accuracy faster\nusing adaptive weights on a small network than using static constant weights on\na large one. For problems with high performance saturation, we also show a\nsequence of exponentially deepening ANNscan achieve near-optimal anytime\nresults at any budget, at the cost of a const fraction of extra computation.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 21:42:15 GMT"}, {"version": "v2", "created": "Mon, 30 Oct 2017 21:25:38 GMT"}, {"version": "v3", "created": "Fri, 25 May 2018 05:18:33 GMT"}], "update_date": "2018-05-28", "authors_parsed": [["Hu", "Hanzhang", ""], ["Dey", "Debadeepta", ""], ["Hebert", "Martial", ""], ["Bagnell", "J. Andrew", ""]]}, {"id": "1708.06834", "submitter": "Xavier Gir\\'o-i-Nieto", "authors": "Victor Campos, Brendan Jou, Xavier Giro-i-Nieto, Jordi Torres and\n  Shih-Fu Chang", "title": "Skip RNN: Learning to Skip State Updates in Recurrent Neural Networks", "comments": "Accepted as conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent Neural Networks (RNNs) continue to show outstanding performance in\nsequence modeling tasks. However, training RNNs on long sequences often face\nchallenges like slow inference, vanishing gradients and difficulty in capturing\nlong term dependencies. In backpropagation through time settings, these issues\nare tightly coupled with the large, sequential computational graph resulting\nfrom unfolding the RNN in time. We introduce the Skip RNN model which extends\nexisting RNN models by learning to skip state updates and shortens the\neffective size of the computational graph. This model can also be encouraged to\nperform fewer state updates through a budget constraint. We evaluate the\nproposed model on various tasks and show how it can reduce the number of\nrequired RNN updates while preserving, and sometimes even improving, the\nperformance of the baseline RNN models. Source code is publicly available at\nhttps://imatge-upc.github.io/skiprnn-2017-telecombcn/ .\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 21:53:34 GMT"}, {"version": "v2", "created": "Thu, 24 Aug 2017 00:54:45 GMT"}, {"version": "v3", "created": "Mon, 5 Feb 2018 17:14:12 GMT"}], "update_date": "2018-02-06", "authors_parsed": [["Campos", "Victor", ""], ["Jou", "Brendan", ""], ["Giro-i-Nieto", "Xavier", ""], ["Torres", "Jordi", ""], ["Chang", "Shih-Fu", ""]]}, {"id": "1708.06846", "submitter": "Arthur Choi", "authors": "Arthur Choi and Adnan Darwiche", "title": "On Relaxing Determinism in Arithmetic Circuits", "comments": "In Proceedings of the Thirty-fourth International Conference on\n  Machine Learning (ICML)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past decade has seen a significant interest in learning tractable\nprobabilistic representations. Arithmetic circuits (ACs) were among the first\nproposed tractable representations, with some subsequent representations being\ninstances of ACs with weaker or stronger properties. In this paper, we provide\na formal basis under which variants on ACs can be compared, and where the\nprecise roles and semantics of their various properties can be made more\ntransparent. This allows us to place some recent developments on ACs in a\nclearer perspective and to also derive new results for ACs. This includes an\nexponential separation between ACs with and without determinism; completeness\nand incompleteness results; and tractability results (or lack thereof) when\ncomputing most probable explanations (MPEs).\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 23:02:11 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Choi", "Arthur", ""], ["Darwiche", "Adnan", ""]]}, {"id": "1708.06850", "submitter": "Enoch Yeung Ph.D.", "authors": "Enoch Yeung, Soumya Kundu, Nathan Hodas", "title": "Learning Deep Neural Network Representations for Koopman Operators of\n  Nonlinear Dynamical Systems", "comments": "16 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Koopman operator has recently garnered much attention for its value in\ndynamical systems analysis and data-driven model discovery. However, its\napplication has been hindered by the computational complexity of extended\ndynamic mode decomposition; this requires a combinatorially large basis set to\nadequately describe many nonlinear systems of interest, e.g. cyber-physical\ninfrastructure systems, biological networks, social systems, and fluid\ndynamics. Often the dictionaries generated for these problems are manually\ncurated, requiring domain-specific knowledge and painstaking tuning. In this\npaper we introduce a deep learning framework for learning Koopman operators of\nnonlinear dynamical systems. We show that this novel method automatically\nselects efficient deep dictionaries, outperforming state-of-the-art methods. We\nbenchmark this method on partially observed nonlinear systems, including the\nglycolytic oscillator and show it is able to predict quantitatively 100 steps\ninto the future, using only a single timepoint, and qualitative oscillatory\nbehavior 400 steps into the future.\n", "versions": [{"version": "v1", "created": "Tue, 22 Aug 2017 23:32:19 GMT"}, {"version": "v2", "created": "Fri, 17 Nov 2017 19:36:19 GMT"}], "update_date": "2017-12-11", "authors_parsed": [["Yeung", "Enoch", ""], ["Kundu", "Soumya", ""], ["Hodas", "Nathan", ""]]}, {"id": "1708.06877", "submitter": "Reginaldo Filho R.I.S.F", "authors": "Reginaldo I. Silva Filho, Ricardo L. Azevedo da Rocha, Camila Leite\n  Silva, Ricardo H. Gracini Guiraldelli", "title": "The Reachability of Computer Programs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AI math.IT", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Would it be possible to explain the emergence of new computational ideas\nusing the computation itself? Would it be feasible to describe the discovery\nprocess of new algorithmic solutions using only mathematics? This study is the\nfirst effort to analyze the nature of such inquiry from the viewpoint of effort\nto find a new algorithmic solution to a given problem. We define program\nreachability as a probability function whose argument is a form of the\nenergetic cost (algorithmic entropy) of the problem.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 03:32:56 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Filho", "Reginaldo I. Silva", ""], ["da Rocha", "Ricardo L. Azevedo", ""], ["Silva", "Camila Leite", ""], ["Guiraldelli", "Ricardo H. Gracini", ""]]}, {"id": "1708.06975", "submitter": "Maxime Bucher", "authors": "Maxime Bucher (1), St\\'ephane Herbin (1), Fr\\'ed\\'eric Jurie ((1)\n  Palaiseau)", "title": "Generating Visual Representations for Zero-Shot Classification", "comments": null, "journal-ref": "International Conference on Computer Vision (ICCV) Workshops :\n  TASK-CV: Transferring and Adapting Source Knowledge in Computer Vision, Oct\n  2017, venise, Italy. International Conference on Computer Vision (ICCV)\n  Workshops, 2017", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the task of learning an image clas-sifier when some\ncategories are defined by semantic descriptions only (e.g. visual attributes)\nwhile the others are defined by exemplar images as well. This task is often\nreferred to as the Zero-Shot classification task (ZSC). Most of the previous\nmethods rely on learning a common embedding space allowing to compare visual\nfeatures of unknown categories with semantic descriptions. This paper argues\nthat these approaches are limited as i) efficient discrimi-native classifiers\ncan't be used ii) classification tasks with seen and unseen categories\n(Generalized Zero-Shot Classification or GZSC) can't be addressed efficiently.\nIn contrast , this paper suggests to address ZSC and GZSC by i) learning a\nconditional generator using seen classes ii) generate artificial training\nexamples for the categories without exemplars. ZSC is then turned into a\nstandard supervised learning problem. Experiments with 4 generative models and\n5 datasets experimentally validate the approach, giving state-of-the-art\nresults on both ZSC and GZSC.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 12:23:51 GMT"}, {"version": "v2", "created": "Mon, 28 Aug 2017 12:56:18 GMT"}, {"version": "v3", "created": "Mon, 11 Dec 2017 16:16:01 GMT"}], "update_date": "2017-12-12", "authors_parsed": [["Bucher", "Maxime", ""], ["Herbin", "St\u00e9phane", ""], ["Jurie", "Fr\u00e9d\u00e9ric", ""]]}, {"id": "1708.06988", "submitter": "Alessia Knauss", "authors": "Alessia Knauss, Jan Schr\\\"oder, Christian Berger, and Henrik Eriksson", "title": "Paving the Roadway for Safety of Automated Vehicles: An Empirical Study\n  on Testing Challenges", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The technology in the area of automated vehicles is gaining speed and\npromises many advantages. However, with the recent introduction of\nconditionally automated driving, we have also seen accidents. Test protocols\nfor both, conditionally automated (e.g., on highways) and automated vehicles do\nnot exist yet and leave researchers and practitioners with different\nchallenges. For instance, current test procedures do not suffice for fully\nautomated vehicles, which are supposed to be completely in charge for the\ndriving task and have no driver as a back up. This paper presents current\nchallenges of testing the functionality and safety of automated vehicles\nderived from conducting focus groups and interviews with 26 participants from\nfive countries having a background related to testing automotive safety-related\ntopics.We provide an overview of the state-of-practice of testing active safety\nfeatures as well as challenges that needs to be addressed in the future to\nensure safety for automated vehicles. The major challenges identified through\nthe interviews and focus groups, enriched by literature on this topic are\nrelated to 1) virtual testing and simulation, 2) safety, reliability, and\nquality, 3) sensors and sensor models, 4) required scenario complexity and\namount of test cases, and 5) handover of responsibility between the driver and\nthe vehicle.\n", "versions": [{"version": "v1", "created": "Tue, 9 May 2017 10:09:18 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Knauss", "Alessia", ""], ["Schr\u00f6der", "Jan", ""], ["Berger", "Christian", ""], ["Eriksson", "Henrik", ""]]}, {"id": "1708.06989", "submitter": "Youssef Oualil", "authors": "Youssef Oualil and Dietrich Klakow", "title": "A Neural Network Approach for Mixing Language Models", "comments": "Published at IEEE International Conference on Acoustics, Speech and\n  Signal Processing (ICASSP) 2017. arXiv admin note: text overlap with\n  arXiv:1703.08068", "journal-ref": "IEEE International Conference on Acoustics, Speech and Signal\n  Processing (ICASSP), New Orleans, LA, 2017, pp. 5710-5714", "doi": "10.1109/ICASSP.2017.7953250", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The performance of Neural Network (NN)-based language models is steadily\nimproving due to the emergence of new architectures, which are able to learn\ndifferent natural language characteristics. This paper presents a novel\nframework, which shows that a significant improvement can be achieved by\ncombining different existing heterogeneous models in a single architecture.\nThis is done through 1) a feature layer, which separately learns different\nNN-based models and 2) a mixture layer, which merges the resulting model\nfeatures. In doing so, this architecture benefits from the learning\ncapabilities of each model with no noticeable increase in the number of model\nparameters or the training time. Extensive experiments conducted on the Penn\nTreebank (PTB) and the Large Text Compression Benchmark (LTCB) corpus showed a\nsignificant reduction of the perplexity when compared to state-of-the-art\nfeedforward as well as recurrent neural network architectures.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 13:27:16 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Oualil", "Youssef", ""], ["Klakow", "Dietrich", ""]]}, {"id": "1708.07038", "submitter": "Georgios Zoumpourlis", "authors": "Georgios Zoumpourlis, Alexandros Doumanoglou, Nicholas Vretos, Petros\n  Daras", "title": "Non-linear Convolution Filters for CNN-based Learning", "comments": "9 pages, 5 figures, code link, ICCV 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  During the last years, Convolutional Neural Networks (CNNs) have achieved\nstate-of-the-art performance in image classification. Their architectures have\nlargely drawn inspiration by models of the primate visual system. However,\nwhile recent research results of neuroscience prove the existence of non-linear\noperations in the response of complex visual cells, little effort has been\ndevoted to extend the convolution technique to non-linear forms. Typical\nconvolutional layers are linear systems, hence their expressiveness is limited.\nTo overcome this, various non-linearities have been used as activation\nfunctions inside CNNs, while also many pooling strategies have been applied. We\naddress the issue of developing a convolution method in the context of a\ncomputational model of the visual cortex, exploring quadratic forms through the\nVolterra kernels. Such forms, constituting a more rich function space, are used\nas approximations of the response profile of visual cells. Our proposed\nsecond-order convolution is tested on CIFAR-10 and CIFAR-100. We show that a\nnetwork which combines linear and non-linear filters in its convolutional\nlayers, can outperform networks that use standard linear filters with the same\narchitecture, yielding results competitive with the state-of-the-art on these\ndatasets.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 15:07:35 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Zoumpourlis", "Georgios", ""], ["Doumanoglou", "Alexandros", ""], ["Vretos", "Nicholas", ""], ["Daras", "Petros", ""]]}, {"id": "1708.07050", "submitter": "Soheil Khorram", "authors": "Soheil Khorram, Zakaria Aldeneh, Dimitrios Dimitriadis, Melvin\n  McInnis, Emily Mower Provost", "title": "Capturing Long-term Temporal Dependencies with Convolutional Networks\n  for Continuous Emotion Recognition", "comments": "5 pages, 5 figures, 2 tables, Interspeech 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal of continuous emotion recognition is to assign an emotion value to\nevery frame in a sequence of acoustic features. We show that incorporating\nlong-term temporal dependencies is critical for continuous emotion recognition\ntasks. To this end, we first investigate architectures that use dilated\nconvolutions. We show that even though such architectures outperform previously\nreported systems, the output signals produced from such architectures undergo\nerratic changes between consecutive time steps. This is inconsistent with the\nslow moving ground-truth emotion labels that are obtained from human\nannotators. To deal with this problem, we model a downsampled version of the\ninput signal and then generate the output signal through upsampling. Not only\ndoes the resulting downsampling/upsampling network achieve good performance, it\nalso generates smooth output trajectories. Our method yields the best known\naudio-only performance on the RECOLA dataset.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 15:27:00 GMT"}], "update_date": "2017-08-24", "authors_parsed": [["Khorram", "Soheil", ""], ["Aldeneh", "Zakaria", ""], ["Dimitriadis", "Dimitrios", ""], ["McInnis", "Melvin", ""], ["Provost", "Emily Mower", ""]]}, {"id": "1708.07129", "submitter": "Siamak Yousefi mr", "authors": "Siamak Yousefi, Hirokazu Narui, Sankalp Dayal, Stefano Ermon, Shahrokh\n  Valaee", "title": "A Survey of Human Activity Recognition Using WiFi CSI", "comments": "4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we present a survey of recent advances in passive human\nbehaviour recognition in indoor areas using the channel state information (CSI)\nof commercial WiFi systems. Movement of human body causes a change in the\nwireless signal reflections, which results in variations in the CSI. By\nanalyzing the data streams of CSIs for different activities and comparing them\nagainst stored models, human behaviour can be recognized. This is done by\nextracting features from CSI data streams and using machine learning techniques\nto build models and classifiers. The techniques from the literature that are\npresented herein have great performances, however, instead of the machine\nlearning techniques employed in these works, we propose to use deep learning\ntechniques such as long-short term memory (LSTM) recurrent neural network\n(RNN), and show the improved performance. We also discuss about different\nchallenges such as environment change, frame rate selection, and multi-user\nscenario, and suggest possible directions for future work.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 18:00:05 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Yousefi", "Siamak", ""], ["Narui", "Hirokazu", ""], ["Dayal", "Sankalp", ""], ["Ermon", "Stefano", ""], ["Valaee", "Shahrokh", ""]]}, {"id": "1708.07149", "submitter": "Ryan Lowe T.", "authors": "Ryan Lowe, Michael Noseworthy, Iulian V. Serban, Nicolas\n  Angelard-Gontier, Yoshua Bengio, Joelle Pineau", "title": "Towards an Automatic Turing Test: Learning to Evaluate Dialogue\n  Responses", "comments": "ACL 2017", "journal-ref": "Proceedings of the 55th annual meeting on Association for\n  Computational Linguistics (2017), pp. 1116-1126", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatically evaluating the quality of dialogue responses for unstructured\ndomains is a challenging problem. Unfortunately, existing automatic evaluation\nmetrics are biased and correlate very poorly with human judgements of response\nquality. Yet having an accurate automatic evaluation procedure is crucial for\ndialogue research, as it allows rapid prototyping and testing of new models\nwith fewer expensive human evaluations. In response to this challenge, we\nformulate automatic dialogue evaluation as a learning problem. We present an\nevaluation model (ADEM) that learns to predict human-like scores to input\nresponses, using a new dataset of human response scores. We show that the ADEM\nmodel's predictions correlate significantly, and at a level much higher than\nword-overlap metrics such as BLEU, with human judgements at both the utterance\nand system-level. We also show that ADEM can generalize to evaluating dialogue\nmodels unseen during training, an important step for automatic dialogue\nevaluation.\n", "versions": [{"version": "v1", "created": "Wed, 23 Aug 2017 18:56:00 GMT"}, {"version": "v2", "created": "Tue, 16 Jan 2018 23:29:14 GMT"}], "update_date": "2018-01-18", "authors_parsed": [["Lowe", "Ryan", ""], ["Noseworthy", "Michael", ""], ["Serban", "Iulian V.", ""], ["Angelard-Gontier", "Nicolas", ""], ["Bengio", "Yoshua", ""], ["Pineau", "Joelle", ""]]}, {"id": "1708.07239", "submitter": "Prashant Shiralkar", "authors": "Prashant Shiralkar, Alessandro Flammini, Filippo Menczer, Giovanni\n  Luca Ciampaglia", "title": "Finding Streams in Knowledge Graphs to Support Fact Checking", "comments": "Extended version of the paper in proceedings of ICDM 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The volume and velocity of information that gets generated online limits\ncurrent journalistic practices to fact-check claims at the same rate.\nComputational approaches for fact checking may be the key to help mitigate the\nrisks of massive misinformation spread. Such approaches can be designed to not\nonly be scalable and effective at assessing veracity of dubious claims, but\nalso to boost a human fact checker's productivity by surfacing relevant facts\nand patterns to aid their analysis. To this end, we present a novel,\nunsupervised network-flow based approach to determine the truthfulness of a\nstatement of fact expressed in the form of a (subject, predicate, object)\ntriple. We view a knowledge graph of background information about real-world\nentities as a flow network, and knowledge as a fluid, abstract commodity. We\nshow that computational fact checking of such a triple then amounts to finding\na \"knowledge stream\" that emanates from the subject node and flows toward the\nobject node through paths connecting them. Evaluation on a range of real-world\nand hand-crafted datasets of facts related to entertainment, business, sports,\ngeography and more reveals that this network-flow model can be very effective\nin discerning true statements from false ones, outperforming existing\nalgorithms on many test cases. Moreover, the model is expressive in its ability\nto automatically discover several useful path patterns and surface relevant\nfacts that may help a human fact checker corroborate or refute a claim.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 01:07:21 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Shiralkar", "Prashant", ""], ["Flammini", "Alessandro", ""], ["Menczer", "Filippo", ""], ["Ciampaglia", "Giovanni Luca", ""]]}, {"id": "1708.07244", "submitter": "Senjian An Dr.", "authors": "Senjian An, Mohammed Bennamoun and Farid Boussaid", "title": "On the Compressive Power of Deep Rectifier Networks for High Resolution\n  Representation of Class Boundaries", "comments": "18 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper provides a theoretical justification of the superior\nclassification performance of deep rectifier networks over shallow rectifier\nnetworks from the geometrical perspective of piecewise linear (PWL) classifier\nboundaries. We show that, for a given threshold on the approximation error, the\nrequired number of boundary facets to approximate a general smooth boundary\ngrows exponentially with the dimension of the data, and thus the number of\nboundary facets, referred to as boundary resolution, of a PWL classifier is an\nimportant quality measure that can be used to estimate a lower bound on the\nclassification errors. However, learning naively an exponentially large number\nof boundary facets requires the determination of an exponentially large number\nof parameters and also requires an exponentially large number of training\npatterns. To overcome this issue of \"curse of dimensionality\", compressive\nrepresentations of high resolution classifier boundaries are required. To show\nthe superior compressive power of deep rectifier networks over shallow\nrectifier networks, we prove that the maximum boundary resolution of a single\nhidden layer rectifier network classifier grows exponentially with the number\nof units when this number is smaller than the dimension of the patterns. When\nthe number of units is larger than the dimension of the patterns, the growth\nrate is reduced to a polynomial order. Consequently, the capacity of generating\na high resolution boundary will increase if the same large number of units are\narranged in multiple layers instead of a single hidden layer. Taking high\ndimensional spherical boundaries as examples, we show how deep rectifier\nnetworks can utilize geometric symmetries to approximate a boundary with the\nsame accuracy but with a significantly fewer number of parameters than single\nhidden layer nets.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 01:37:36 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["An", "Senjian", ""], ["Bennamoun", "Mohammed", ""], ["Boussaid", "Farid", ""]]}, {"id": "1708.07252", "submitter": "Dengliang Shi", "authors": "Dengliang Shi", "title": "A Study on Neural Network Language Modeling", "comments": "20 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An exhaustive study on neural network language modeling (NNLM) is performed\nin this paper. Different architectures of basic neural network language models\nare described and examined. A number of different improvements over basic\nneural network language models, including importance sampling, word classes,\ncaching and bidirectional recurrent neural network (BiRNN), are studied\nseparately, and the advantages and disadvantages of every technique are\nevaluated. Then, the limits of neural network language modeling are explored\nfrom the aspects of model architecture and knowledge representation. Part of\nthe statistical information from a word sequence will loss when it is processed\nword by word in a certain order, and the mechanism of training neural network\nby updating weight matrixes and vectors imposes severe restrictions on any\nsignificant enhancement of NNLM. For knowledge representation, the knowledge\nrepresented by neural network language models is the approximate probabilistic\ndistribution of word sequences from a certain training data set rather than the\nknowledge of a language itself or the information conveyed by word sequences in\na natural language. Finally, some directions for improving neural network\nlanguage modeling further is discussed.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 02:14:50 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Shi", "Dengliang", ""]]}, {"id": "1708.07280", "submitter": "Edward Groshev", "authors": "Edward Groshev, Maxwell Goldstein, Aviv Tamar, Siddharth Srivastava,\n  Pieter Abbeel", "title": "Learning Generalized Reactive Policies using Deep Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach to learning for planning, where knowledge acquired\nwhile solving a given set of planning problems is used to plan faster in\nrelated, but new problem instances. We show that a deep neural network can be\nused to learn and represent a \\emph{generalized reactive policy} (GRP) that\nmaps a problem instance and a state to an action, and that the learned GRPs\nefficiently solve large classes of challenging problem instances. In contrast\nto prior efforts in this direction, our approach significantly reduces the\ndependence of learning on handcrafted domain knowledge or feature selection.\nInstead, the GRP is trained from scratch using a set of successful execution\ntraces. We show that our approach can also be used to automatically learn a\nheuristic function that can be used in directed search algorithms. We evaluate\nour approach using an extensive suite of experiments on two challenging\nplanning problem domains and show that our approach facilitates learning\ncomplex decision making policies and powerful heuristic functions with minimal\nhuman input. Videos of our results are available at goo.gl/Hpy4e3.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 05:24:36 GMT"}, {"version": "v2", "created": "Sun, 29 Apr 2018 08:30:18 GMT"}, {"version": "v3", "created": "Wed, 25 Jul 2018 01:54:26 GMT"}], "update_date": "2018-07-26", "authors_parsed": [["Groshev", "Edward", ""], ["Goldstein", "Maxwell", ""], ["Tamar", "Aviv", ""], ["Srivastava", "Siddharth", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1708.07285", "submitter": "Pavel Surynek", "authors": "Marika Ivanov\\'a, Pavel Surynek", "title": "Area Protection in Adversarial Path-Finding Scenarios with Multiple\n  Mobile Agents on Graphs: a theoretical and experimental study of\n  target-allocation strategies for defense coordination", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address a problem of area protection in graph-based scenarios with\nmultiple agents. The problem consists of two adversarial teams of agents that\nmove in an undirected graph shared by both teams. Agents are placed in vertices\nof the graph; at most one agent can occupy a vertex; and they can move into\nadjacent vertices in a conflict free way. Teams have asymmetric goals: the aim\nof one team - attackers - is to invade into given area while the aim of the\nopponent team - defenders - is to protect the area from being entered by\nattackers by occupying selected vertices. We study strategies for allocating\nvertices to be occupied by the team of defenders to block attacking agents. We\nshow that the decision version of the problem of area protection is PSPACE-hard\nunder the assumption that agents can allocate their target vertices multiple\ntimes. Further we develop various on-line vertex-allocation strategies for the\ndefender team in a simplified variant of the problem with single stage vertex\nallocation and evaluated their performance in multiple benchmarks. The success\nof a strategy is heavily dependent on the type of the instance, and so one of\nthe contributions of this work is that we identify suitable vertex-allocation\nstrategies for diverse instance types. In particular, we introduce a\nsimulation-based method that identifies and tries to capture bottlenecks in the\ngraph, that are frequently used by the attackers. Our experimental evaluation\nsuggests that this method often allows a successful defense even in instances\nwhere the attackers significantly outnumber the defenders.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 05:58:28 GMT"}], "update_date": "2017-08-25", "authors_parsed": [["Ivanov\u00e1", "Marika", ""], ["Surynek", "Pavel", ""]]}, {"id": "1708.07303", "submitter": "Xinchen Yan", "authors": "Xinchen Yan, Jasmine Hsu, Mohi Khansari, Yunfei Bai, Arkanath Pathak,\n  Abhinav Gupta, James Davidson, Honglak Lee", "title": "Learning 6-DOF Grasping Interaction via Deep Geometry-aware 3D\n  Representations", "comments": "Published at ICRA 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on the problem of learning 6-DOF grasping with a parallel\njaw gripper in simulation. We propose the notion of a geometry-aware\nrepresentation in grasping based on the assumption that knowledge of 3D\ngeometry is at the heart of interaction. Our key idea is constraining and\nregularizing grasping interaction learning through 3D geometry prediction.\nSpecifically, we formulate the learning of deep geometry-aware grasping model\nin two steps: First, we learn to build mental geometry-aware representation by\nreconstructing the scene (i.e., 3D occupancy grid) from RGBD input via\ngenerative 3D shape modeling. Second, we learn to predict grasping outcome with\nits internal geometry-aware representation. The learned outcome prediction\nmodel is used to sequentially propose grasping solutions via\nanalysis-by-synthesis optimization. Our contributions are fourfold: (1) To best\nof our knowledge, we are presenting for the first time a method to learn a\n6-DOF grasping net from RGBD input; (2) We build a grasping dataset from\ndemonstrations in virtual reality with rich sensory and interaction\nannotations. This dataset includes 101 everyday objects spread across 7\ncategories, additionally, we propose a data augmentation strategy for effective\nlearning; (3) We demonstrate that the learned geometry-aware representation\nleads to about 10 percent relative performance improvement over the baseline\nCNN on grasping objects from our dataset. (4) We further demonstrate that the\nmodel generalizes to novel viewpoints and object instances.\n", "versions": [{"version": "v1", "created": "Thu, 24 Aug 2017 08:09:04 GMT"}, {"version": "v2", "created": "Fri, 25 Aug 2017 02:50:28 GMT"}, {"version": "v3", "created": "Mon, 4 Dec 2017 18:57:26 GMT"}, {"version": "v4", "created": "Fri, 15 Jun 2018 03:40:53 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Yan", "Xinchen", ""], ["Hsu", "Jasmine", ""], ["Khansari", "Mohi", ""], ["Bai", "Yunfei", ""], ["Pathak", "Arkanath", ""], ["Gupta", "Abhinav", ""], ["Davidson", "James", ""], ["Lee", "Honglak", ""]]}, {"id": "1708.07579", "submitter": "Nikola Trkulja", "authors": "Milo\\v{s} Stojakovi\\'c and Nikola Trkulja", "title": "Hamiltonian Maker-Breaker games on small graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.AI cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We look at the unbiased Maker-Breaker Hamiltonicity game played on the edge\nset of a complete graph $K_n$, where Maker's goal is to claim a Hamiltonian\ncycle. First, we prove that, independent of who starts, Maker can win the game\nfor $n = 8$ and $n = 9$. Then we use an inductive argument to show that,\nindependent of who starts, Maker can win the game if and only if $n \\geq 8$.\nThis, in particular, resolves in the affirmative the long-standing conjecture\nof Papaioannou.\n  We also study two standard positional games related to Hamiltonicity game.\nFor Hamiltonian Path game, we show that Maker can claim a Hamiltonian path if\nand only if $n \\geq 5$, independent of who starts. Next, we look at Fixed\nHamiltonian Path game, where the goal of Maker is to claim a Hamiltonian path\nbetween two predetermined vertices. We prove that if Maker starts the game, he\nwins if and only if $n \\geq 7$, and if Breaker starts, Maker wins if and only\nif $n \\geq 8$. Using this result, we are able to improve the previously best\nupper bound on the smallest number of edges a graph on $n$ vertices can have,\nknowing that Maker can win the Maker-Breaker Hamiltonicity game played on its\nedges.\n  To resolve the outcomes of the mentioned games on small (finite) boards, we\ndevise algorithms for efficiently searching game trees and then obtain our\nresults with the help of a computer.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 00:09:20 GMT"}, {"version": "v2", "created": "Mon, 11 Sep 2017 14:28:09 GMT"}, {"version": "v3", "created": "Tue, 7 Aug 2018 01:25:04 GMT"}], "update_date": "2018-08-08", "authors_parsed": [["Stojakovi\u0107", "Milo\u0161", ""], ["Trkulja", "Nikola", ""]]}, {"id": "1708.07580", "submitter": "Haris Aziz", "authors": "Haris Aziz and Barton Lee", "title": "The Expanding Approvals Rule: Improving Proportional Representation and\n  Monotonicity", "comments": "Change in title plus several edits over the previous version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Proportional representation (PR) is often discussed in voting settings as a\nmajor desideratum. For the past century or so, it is common both in practice\nand in the academic literature to jump to single transferable vote (STV) as the\nsolution for achieving PR. Some of the most prominent electoral reform\nmovements around the globe are pushing for the adoption of STV. It has been\ntermed a major open problem to design a voting rule that satisfies the same PR\nproperties as STV and better monotonicity properties. In this paper, we first\npresent a taxonomy of proportional representation axioms for general weak order\npreferences, some of which generalise and strengthen previously introduced\nconcepts. We then present a rule called Expanding Approvals Rule (EAR) that\nsatisfies properties stronger than the central PR axiom satisfied by STV, can\nhandle indifferences in a convenient and computationally efficient manner, and\nalso satisfies better candidate monotonicity properties. In view of this, our\nproposed rule seems to be a compelling solution for achieving proportional\nrepresentation in voting settings.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 00:10:43 GMT"}, {"version": "v2", "created": "Mon, 4 Jun 2018 13:16:25 GMT"}], "update_date": "2018-06-05", "authors_parsed": [["Aziz", "Haris", ""], ["Lee", "Barton", ""]]}, {"id": "1708.07607", "submitter": "Qingpeng Cai", "authors": "Qingpeng Cai, Aris Filos-Ratsikas, Pingzhong Tang, Yiwei Zhang", "title": "Reinforcement Mechanism Design for e-commerce", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of allocating impressions to sellers in e-commerce\nwebsites, such as Amazon, eBay or Taobao, aiming to maximize the total revenue\ngenerated by the platform. We employ a general framework of reinforcement\nmechanism design, which uses deep reinforcement learning to design efficient\nalgorithms, taking the strategic behaviour of the sellers into account.\nSpecifically, we model the impression allocation problem as a Markov decision\nprocess, where the states encode the history of impressions, prices,\ntransactions and generated revenue and the actions are the possible impression\nallocations in each round. To tackle the problem of continuity and\nhigh-dimensionality of states and actions, we adopt the ideas of the DDPG\nalgorithm to design an actor-critic policy gradient algorithm which takes\nadvantage of the problem domain in order to achieve convergence and stability.\nWe evaluate our proposed algorithm, coined IA(GRU), by comparing it against\nDDPG, as well as several natural heuristics, under different rationality models\nfor the sellers - we assume that sellers follow well-known no-regret type\nstrategies which may vary in their degree of sophistication. We find that\nIA(GRU) outperforms all algorithms in terms of the total revenue.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 03:55:21 GMT"}, {"version": "v2", "created": "Tue, 24 Oct 2017 13:15:33 GMT"}, {"version": "v3", "created": "Tue, 27 Feb 2018 06:17:29 GMT"}], "update_date": "2018-02-28", "authors_parsed": [["Cai", "Qingpeng", ""], ["Filos-Ratsikas", "Aris", ""], ["Tang", "Pingzhong", ""], ["Zhang", "Yiwei", ""]]}, {"id": "1708.07689", "submitter": "Sebastian Lapuschkin", "authors": "Sebastian Lapuschkin, Alexander Binder, Klaus-Robert M\\\"uller,\n  Wojciech Samek", "title": "Understanding and Comparing Deep Neural Networks for Age and Gender\n  Classification", "comments": "8 pages, 5 figures, 5 tables. Presented at ICCV 2017 Workshop: 7th\n  IEEE International Workshop on Analysis and Modeling of Faces and Gestures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, deep neural networks have demonstrated excellent performances in\nrecognizing the age and gender on human face images. However, these models were\napplied in a black-box manner with no information provided about which facial\nfeatures are actually used for prediction and how these features depend on\nimage preprocessing, model initialization and architecture choice. We present a\nstudy investigating these different effects.\n  In detail, our work compares four popular neural network architectures,\nstudies the effect of pretraining, evaluates the robustness of the considered\nalignment preprocessings via cross-method test set swapping and intuitively\nvisualizes the model's prediction strategies in given preprocessing conditions\nusing the recent Layer-wise Relevance Propagation (LRP) algorithm. Our\nevaluations on the challenging Adience benchmark show that suitable parameter\ninitialization leads to a holistic perception of the input, compensating\nartefactual data representations. With a combination of simple preprocessing\nsteps, we reach state of the art performance in gender recognition.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 11:08:38 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Lapuschkin", "Sebastian", ""], ["Binder", "Alexander", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Samek", "Wojciech", ""]]}, {"id": "1708.07732", "submitter": "Raghuram Bharadwaj Diddigi", "authors": "Raghuram Bharadwaj Diddigi, D. Sai Koti Reddy and Shalabh Bhatnagar", "title": "Multi-Agent Q-Learning for Minimizing Demand-Supply Power Deficit in\n  Microgrids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SY cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of minimizing the difference in the demand and the\nsupply of power using microgrids. We setup multiple microgrids, that provide\nelectricity to a village. They have access to the batteries that can store\nrenewable power and also the electrical lines from the main grid. During each\ntime period, these microgrids need to take decision on the amount of renewable\npower to be used from the batteries as well as the amount of power needed from\nthe main grid. We formulate this problem in the framework of Markov Decision\nProcess (MDP), similar to the one discussed in [1]. The power allotment to the\nvillage from the main grid is fixed and bounded, whereas the renewable energy\ngeneration is uncertain in nature. Therefore we adapt a distributed version of\nthe popular Reinforcement learning technique, Multi-Agent Q-Learning to the\nproblem. Finally, we also consider a variant of this problem where the cost of\npower production at the main site is taken into consideration. In this scenario\nthe microgrids need to minimize the demand-supply deficit, while maintaining\nthe desired average cost of the power production.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 13:36:41 GMT"}, {"version": "v2", "created": "Mon, 28 Aug 2017 04:25:20 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Diddigi", "Raghuram Bharadwaj", ""], ["Reddy", "D. Sai Koti", ""], ["Bhatnagar", "Shalabh", ""]]}, {"id": "1708.07767", "submitter": "Florent Capelli", "authors": "Andrea Cal\\`i, Florent Capelli and Igor Razgon", "title": "Non-FPT lower bounds for structural restrictions of decision DNNF", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a non-FPT lower bound on the size of structured decision DNNF and\nOBDD with decomposable AND-nodes representing CNF-formulas of bounded incidence\ntreewidth. Both models are known to be of FPT size for CNFs of bounded primal\ntreewidth. To the best of our knowledge this is the first parameterized\nseparation of primal treewidth and incidence treewidth for knowledge\ncompilation models.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 15:05:22 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Cal\u00ec", "Andrea", ""], ["Capelli", "Florent", ""], ["Razgon", "Igor", ""]]}, {"id": "1708.07775", "submitter": "Jing Wang", "authors": "Jing Wang", "title": "Subspace Approximation for Approximate Nearest Neighbor Search in NLP", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most natural language processing tasks can be formulated as the approximated\nnearest neighbor search problem, such as word analogy, document similarity,\nmachine translation. Take the question-answering task as an example, given a\nquestion as the query, the goal is to search its nearest neighbor in the\ntraining dataset as the answer. However, existing methods for approximate\nnearest neighbor search problem may not perform well owing to the following\npractical challenges: 1) there are noise in the data; 2) the large scale\ndataset yields a huge retrieval space and high search time complexity.\n  In order to solve these problems, we propose a novel approximate nearest\nneighbor search framework which i) projects the data to a subspace based\nspectral analysis which eliminates the influence of noise; ii) partitions the\ntraining dataset to different groups in order to reduce the search space.\nSpecifically, the retrieval space is reduced from $O(n)$ to $O(\\log n)$ (where\n$n$ is the number of data points in the training dataset). We prove that the\nretrieved nearest neighbor in the projected subspace is the same as the one in\nthe original feature space. We demonstrate the outstanding performance of our\nframework on real-world natural language processing tasks.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 15:26:15 GMT"}], "update_date": "2017-08-28", "authors_parsed": [["Wang", "Jing", ""]]}, {"id": "1708.07863", "submitter": "Zhiguo Wang", "authors": "Zhiguo Wang, Wael Hamza, Linfeng Song", "title": "$k$-Nearest Neighbor Augmented Neural Networks for Text Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, many deep-learning based models are proposed for text\nclassification. This kind of models well fits the training set from the\nstatistical point of view. However, it lacks the capacity of utilizing\ninstance-level information from individual instances in the training set. In\nthis work, we propose to enhance neural network models by allowing them to\nleverage information from $k$-nearest neighbor (kNN) of the input text. Our\nmodel employs a neural network that encodes texts into text embeddings.\nMoreover, we also utilize $k$-nearest neighbor of the input text as an external\nmemory, and utilize it to capture instance-level information from the training\nset. The final prediction is made based on features from both the neural\nnetwork encoder and the kNN memory. Experimental results on several standard\nbenchmark datasets show that our model outperforms the baseline model on all\nthe datasets, and it even beats a very deep neural network model (with 29\nlayers) in several datasets. Our model also shows superior performance when\ntraining instances are scarce, and when the training set is severely\nunbalanced. Our model also leverages techniques such as semi-supervised\ntraining and transfer learning quite well.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 19:04:25 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Wang", "Zhiguo", ""], ["Hamza", "Wael", ""], ["Song", "Linfeng", ""]]}, {"id": "1708.07867", "submitter": "Chen Luo", "authors": "Chen Luo, Zhengzhang Chen, Lu-An Tang, Anshumali Shrivastava, Zhichun\n  Li", "title": "Accelerating Dependency Graph Learning from Heterogeneous Categorical\n  Event Streams via Knowledge Transfer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Dependency graph, as a heterogeneous graph representing the intrinsic\nrelationships between different pairs of system entities, is essential to many\ndata analysis applications, such as root cause diagnosis, intrusion detection,\netc. Given a well-trained dependency graph from a source domain and an immature\ndependency graph from a target domain, how can we extract the entity and\ndependency knowledge from the source to enhance the target? One way is to\ndirectly apply a mature dependency graph learned from a source domain to the\ntarget domain. But due to the domain variety problem, directly using the source\ndependency graph often can not achieve good performance. Traditional transfer\nlearning methods mainly focus on numerical data and are not applicable.\n  In this paper, we propose ACRET, a knowledge transfer based model for\naccelerating dependency graph learning from heterogeneous categorical event\nstreams. In particular, we first propose an entity estimation model to filter\nout irrelevant entities from the source domain based on entity embedding and\nmanifold learning. Only the entities with statistically high correlations are\ntransferred to the target domain. On the surviving entities, we propose a\ndependency construction model for constructing the unbiased dependency\nrelationships by solving a two-constraint optimization problem. The\nexperimental results on synthetic and real-world datasets demonstrate the\neffectiveness and efficiency of ACRET. We also apply ACRET to a real enterprise\nsecurity system for intrusion detection. Our method is able to achieve superior\ndetection performance at least 20 days lead lag time in advance with more than\n70% accuracy.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 19:24:27 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Luo", "Chen", ""], ["Chen", "Zhengzhang", ""], ["Tang", "Lu-An", ""], ["Shrivastava", "Anshumali", ""], ["Li", "Zhichun", ""]]}, {"id": "1708.07902", "submitter": "Niels Justesen", "authors": "Niels Justesen, Philip Bontrager, Julian Togelius, Sebastian Risi", "title": "Deep Learning for Video Game Playing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we review recent Deep Learning advances in the context of\nhow they have been applied to play different types of video games such as\nfirst-person shooters, arcade games, and real-time strategy games. We analyze\nthe unique requirements that different game genres pose to a deep learning\nsystem and highlight important open challenges in the context of applying these\nmachine learning methods to video games, such as general game playing, dealing\nwith extremely large decision spaces and sparse rewards.\n", "versions": [{"version": "v1", "created": "Fri, 25 Aug 2017 22:01:09 GMT"}, {"version": "v2", "created": "Mon, 30 Oct 2017 20:46:01 GMT"}, {"version": "v3", "created": "Mon, 18 Feb 2019 15:43:22 GMT"}], "update_date": "2019-02-19", "authors_parsed": [["Justesen", "Niels", ""], ["Bontrager", "Philip", ""], ["Togelius", "Julian", ""], ["Risi", "Sebastian", ""]]}, {"id": "1708.07918", "submitter": "Mo Yu", "authors": "Mo Yu, Xiaoxiao Guo, Jinfeng Yi, Shiyu Chang, Saloni Potdar, Gerald\n  Tesauro, Haoyu Wang, Bowen Zhou", "title": "Robust Task Clustering for Deep Many-Task Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate task clustering for deep-learning based multi-task and\nfew-shot learning in a many-task setting. We propose a new method to measure\ntask similarities with cross-task transfer performance matrix for the deep\nlearning scenario. Although this matrix provides us critical information\nregarding similarity between tasks, its asymmetric property and unreliable\nperformance scores can affect conventional clustering methods adversely.\nAdditionally, the uncertain task-pairs, i.e., the ones with extremely\nasymmetric transfer scores, may collectively mislead clustering algorithms to\noutput an inaccurate task-partition. To overcome these limitations, we propose\na novel task-clustering algorithm by using the matrix completion technique. The\nproposed algorithm constructs a partially-observed similarity matrix based on\nthe certainty of cluster membership of the task-pairs. We then use a matrix\ncompletion algorithm to complete the similarity matrix. Our theoretical\nanalysis shows that under mild constraints, the proposed algorithm will\nperfectly recover the underlying \"true\" similarity matrix with a high\nprobability. Our results show that the new task clustering method can discover\ntask clusters for training flexible and superior neural network models in a\nmulti-task learning setup for sentiment classification and dialog intent\nclassification tasks. Our task clustering approach also extends metric-based\nfew-shot learning methods to adapt multiple metrics, which demonstrates\nempirical advantages when the tasks are diverse.\n", "versions": [{"version": "v1", "created": "Sat, 26 Aug 2017 02:29:50 GMT"}, {"version": "v2", "created": "Fri, 18 May 2018 00:53:48 GMT"}], "update_date": "2018-05-21", "authors_parsed": [["Yu", "Mo", ""], ["Guo", "Xiaoxiao", ""], ["Yi", "Jinfeng", ""], ["Chang", "Shiyu", ""], ["Potdar", "Saloni", ""], ["Tesauro", "Gerald", ""], ["Wang", "Haoyu", ""], ["Zhou", "Bowen", ""]]}, {"id": "1708.07938", "submitter": "Kui Zhao", "authors": "Kui Zhao, Xia Hu, Jiajun Bu, Can Wang", "title": "Deep Style Match for Complementary Recommendation", "comments": "Workshops at the Thirty-First AAAI Conference on Artificial\n  Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans develop a common sense of style compatibility between items based on\ntheir attributes. We seek to automatically answer questions like \"Does this\nshirt go well with that pair of jeans?\" In order to answer these kinds of\nquestions, we attempt to model human sense of style compatibility in this\npaper. The basic assumption of our approach is that most of the important\nattributes for a product in an online store are included in its title\ndescription. Therefore it is feasible to learn style compatibility from these\ndescriptions. We design a Siamese Convolutional Neural Network architecture and\nfeed it with title pairs of items, which are either compatible or incompatible.\nThose pairs will be mapped from the original space of symbolic words into some\nembedded style space. Our approach takes only words as the input with few\npreprocessing and there is no laborious and expensive feature engineering.\n", "versions": [{"version": "v1", "created": "Sat, 26 Aug 2017 06:09:53 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Zhao", "Kui", ""], ["Hu", "Xia", ""], ["Bu", "Jiajun", ""], ["Wang", "Can", ""]]}, {"id": "1708.07940", "submitter": "Kui Zhao", "authors": "Kui Zhao, Bangpeng Li, Zilun Peng, Jiajun Bu, Can Wang", "title": "Navigation Objects Extraction for Better Content Structure Understanding", "comments": "2017 IEEE/WIC/ACM International Conference on Web Intelligence (WI)", "journal-ref": null, "doi": "10.1145/3106426.3106437", "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing works for extracting navigation objects from webpages focus on\nnavigation menus, so as to reveal the information architecture of the site.\nHowever, web 2.0 sites such as social networks, e-commerce portals etc. are\nmaking the understanding of the content structure in a web site increasingly\ndifficult. Dynamic and personalized elements such as top stories, recommended\nlist in a webpage are vital to the understanding of the dynamic nature of web\n2.0 sites. To better understand the content structure in web 2.0 sites, in this\npaper we propose a new extraction method for navigation objects in a webpage.\nOur method will extract not only the static navigation menus, but also the\ndynamic and personalized page-specific navigation lists. Since the navigation\nobjects in a webpage naturally come in blocks, we first cluster hyperlinks into\ndifferent blocks by exploiting spatial locations of hyperlinks, the\nhierarchical structure of the DOM-tree and the hyperlink density. Then we\nidentify navigation objects from those blocks using the SVM classifier with\nnovel features such as anchor text lengths etc. Experiments on real-world data\nsets with webpages from various domains and styles verified the effectiveness\nof our method.\n", "versions": [{"version": "v1", "created": "Sat, 26 Aug 2017 06:59:24 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Zhao", "Kui", ""], ["Li", "Bangpeng", ""], ["Peng", "Zilun", ""], ["Bu", "Jiajun", ""], ["Wang", "Can", ""]]}, {"id": "1708.07969", "submitter": "Bo Yang", "authors": "Bo Yang, Hongkai Wen, Sen Wang, Ronald Clark, Andrew Markham, Niki\n  Trigoni", "title": "3D Object Reconstruction from a Single Depth View with Adversarial\n  Learning", "comments": "ICCV Workshops 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose a novel 3D-RecGAN approach, which reconstructs the\ncomplete 3D structure of a given object from a single arbitrary depth view\nusing generative adversarial networks. Unlike the existing work which typically\nrequires multiple views of the same object or class labels to recover the full\n3D geometry, the proposed 3D-RecGAN only takes the voxel grid representation of\na depth view of the object as input, and is able to generate the complete 3D\noccupancy grid by filling in the occluded/missing regions. The key idea is to\ncombine the generative capabilities of autoencoders and the conditional\nGenerative Adversarial Networks (GAN) framework, to infer accurate and\nfine-grained 3D structures of objects in high-dimensional voxel space.\nExtensive experiments on large synthetic datasets show that the proposed\n3D-RecGAN significantly outperforms the state of the art in single view 3D\nobject reconstruction, and is able to reconstruct unseen types of objects. Our\ncode and data are available at: https://github.com/Yang7879/3D-RecGAN.\n", "versions": [{"version": "v1", "created": "Sat, 26 Aug 2017 13:46:21 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Yang", "Bo", ""], ["Wen", "Hongkai", ""], ["Wang", "Sen", ""], ["Clark", "Ronald", ""], ["Markham", "Andrew", ""], ["Trigoni", "Niki", ""]]}, {"id": "1708.08035", "submitter": "Bin Shi Mr", "authors": "Bin Shi", "title": "A Conservation Law Method in Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "math.OC cs.AI cs.NA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose some algorithms to find local minima in nonconvex optimization and\nto obtain global minima in some degree from the Newton Second Law without\nfriction. With the key observation of the velocity observable and controllable\nin the motion, the algorithms simulate the Newton Second Law without friction\nbased on symplectic Euler scheme. From the intuitive analysis of analytical\nsolution, we give a theoretical analysis for the high-speed convergence in the\nalgorithm proposed. Finally, we propose the experiments for strongly convex\nfunction, non-strongly convex function and nonconvex function in\nhigh-dimension.\n", "versions": [{"version": "v1", "created": "Sun, 27 Aug 2017 01:55:49 GMT"}, {"version": "v2", "created": "Thu, 31 Aug 2017 04:43:48 GMT"}, {"version": "v3", "created": "Sat, 14 Oct 2017 22:26:51 GMT"}], "update_date": "2017-10-17", "authors_parsed": [["Shi", "Bin", ""]]}, {"id": "1708.08079", "submitter": "Richard Oentaryo", "authors": "Truc Viet Le, Richard J. Oentaryo, Siyuan Liu, Hoong Chuin Lau", "title": "Local Gaussian Processes for Efficient Fine-Grained Traffic Speed\n  Prediction", "comments": null, "journal-ref": "IEEE Transactions on Big Data, vol. 3, no. 2, pp. 194-207, 2017", "doi": "10.1109/TBDATA.2016.2620488", "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traffic speed is a key indicator for the efficiency of an urban\ntransportation system. Accurate modeling of the spatiotemporally varying\ntraffic speed thus plays a crucial role in urban planning and development. This\npaper addresses the problem of efficient fine-grained traffic speed prediction\nusing big traffic data obtained from static sensors. Gaussian processes (GPs)\nhave been previously used to model various traffic phenomena, including flow\nand speed. However, GPs do not scale with big traffic data due to their cubic\ntime complexity. In this work, we address their efficiency issues by proposing\nlocal GPs to learn from and make predictions for correlated subsets of data.\nThe main idea is to quickly group speed variables in both spatial and temporal\ndimensions into a finite number of clusters, so that future and unobserved\ntraffic speed queries can be heuristically mapped to one of such clusters. A\nlocal GP corresponding to that cluster can then be trained on the fly to make\npredictions in real-time. We call this method localization. We use non-negative\nmatrix factorization for localization and propose simple heuristics for cluster\nmapping. We additionally leverage on the expressiveness of GP kernel functions\nto model road network topology and incorporate side information. Extensive\nexperiments using real-world traffic data collected in the two U.S. cities of\nPittsburgh and Washington, D.C., show that our proposed local GPs significantly\nimprove both runtime performances and prediction accuracies compared to the\nbaseline global and local GPs.\n", "versions": [{"version": "v1", "created": "Sun, 27 Aug 2017 11:41:16 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Le", "Truc Viet", ""], ["Oentaryo", "Richard J.", ""], ["Liu", "Siyuan", ""], ["Lau", "Hoong Chuin", ""]]}, {"id": "1708.08113", "submitter": "Raghuram Bharadwaj Diddigi", "authors": "Raghuram Bharadwaj Diddigi, Prabuchandran K.J. and Shalabh Bhatnagar", "title": "Novel Sensor Scheduling Scheme for Intruder Tracking in Energy Efficient\n  Sensor Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of tracking an intruder using a network of wireless\nsensors. For tracking the intruder at each instant, the optimal number and the\nright configuration of sensors has to be powered. As powering the sensors\nconsumes energy, there is a trade off between accurately tracking the position\nof the intruder at each instant and the energy consumption of sensors. This\nproblem has been formulated in the framework of Partially Observable Markov\nDecision Process (POMDP). Even for the state-of-the-art algorithm in the\nliterature, the curse of dimensionality renders the problem intractable. In\nthis paper, we formulate the Intrusion Detection (ID) problem with a suitable\nstate-action space in the framework of POMDP and develop a Reinforcement\nLearning (RL) algorithm utilizing the Upper Confidence Tree Search (UCT) method\nto solve the ID problem. Through simulations, we show that our algorithm\nperforms and scales well with the increasing state and action spaces.\n", "versions": [{"version": "v1", "created": "Sun, 27 Aug 2017 17:19:17 GMT"}, {"version": "v2", "created": "Thu, 4 Jan 2018 16:06:27 GMT"}, {"version": "v3", "created": "Sat, 24 Feb 2018 06:48:33 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Diddigi", "Raghuram Bharadwaj", ""], ["J.", "Prabuchandran K.", ""], ["Bhatnagar", "Shalabh", ""]]}, {"id": "1708.08127", "submitter": "Jianfeng Chen", "authors": "Jianfeng Chen, Tim Menzies", "title": "RIOT: a Stochastic-based Method for Workflow Scheduling in the Cloud", "comments": "8 pages, 4 figures, 3 tables. In Proceedings of IEEE international\n  conference on Cloud Computing'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cloud computing provides engineers or scientists a place to run complex\ncomputing tasks. Finding a workflow's deployment configuration in a cloud\nenvironment is not easy. Traditional workflow scheduling algorithms were based\non some heuristics, e.g. reliability greedy, cost greedy, cost-time balancing,\netc., or more recently, the meta-heuristic methods, such as genetic algorithms.\nThese methods are very slow and not suitable for rescheduling in the dynamic\ncloud environment. This paper introduces RIOT (Randomized Instance Order\nTypes), a stochastic based method for workflow scheduling. RIOT groups the\ntasks in the workflow into virtual machines via a probability model and then\nuses an effective surrogate-based method to assess a large amount of potential\nscheduling. Experiments in dozens of study cases showed that RIOT executes tens\nof times faster than traditional methods while generating comparable results to\nother methods.\n", "versions": [{"version": "v1", "created": "Sun, 27 Aug 2017 19:45:02 GMT"}, {"version": "v2", "created": "Sun, 22 Apr 2018 21:35:01 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Chen", "Jianfeng", ""], ["Menzies", "Tim", ""]]}, {"id": "1708.08133", "submitter": "Aaron Voelker", "authors": "Aaron R. Voelker and Chris Eliasmith", "title": "Methods for applying the Neural Engineering Framework to neuromorphic\n  hardware", "comments": "11 pages, no figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.SY math.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We review our current software tools and theoretical methods for applying the\nNeural Engineering Framework to state-of-the-art neuromorphic hardware. These\nmethods can be used to implement linear and nonlinear dynamical systems that\nexploit axonal transmission time-delays, and to fully account for nonideal\nmixed-analog-digital synapses that exhibit higher-order dynamics with\nheterogeneous time-constants. This summarizes earlier versions of these methods\nthat have been discussed in a more biological context (Voelker & Eliasmith,\n2017) or regarding a specific neuromorphic architecture (Voelker et al., 2017).\n", "versions": [{"version": "v1", "created": "Sun, 27 Aug 2017 20:27:01 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Voelker", "Aaron R.", ""], ["Eliasmith", "Chris", ""]]}, {"id": "1708.08227", "submitter": "Mostapha Benhenda", "authors": "Mostapha Benhenda (LAGA)", "title": "ChemGAN challenge for drug discovery: can AI reproduce natural chemical\n  diversity?", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generating molecules with desired chemical properties is important for drug\ndiscovery. The use of generative neural networks is promising for this task.\nHowever, from visual inspection, it often appears that generated samples lack\ndiversity. In this paper, we quantify this internal chemical diversity, and we\nraise the following challenge: can a nontrivial AI model reproduce natural\nchemical diversity for desired molecules? To illustrate this question, we\nconsider two generative models: a Reinforcement Learning model and the recently\nintroduced ORGAN. Both fail at this challenge. We hope this challenge will\nstimulate research in this direction.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 08:02:55 GMT"}, {"version": "v2", "created": "Wed, 30 Aug 2017 09:03:57 GMT"}, {"version": "v3", "created": "Thu, 31 Aug 2017 14:14:29 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Benhenda", "Mostapha", "", "LAGA"]]}, {"id": "1708.08289", "submitter": "Dar\\'io Garigliotti", "authors": "Dar\\'io Garigliotti and Krisztian Balog", "title": "Generating Query Suggestions to Support Task-Based Search", "comments": "Proceedings of the 40th International ACM SIGIR Conference on\n  Research and Development in Information Retrieval (SIGIR '17), 2017", "journal-ref": null, "doi": "10.1145/3077136.3080745", "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We address the problem of generating query suggestions to support users in\ncompleting their underlying tasks (which motivated them to search in the first\nplace). Given an initial query, these query suggestions should provide a\ncoverage of possible subtasks the user might be looking for. We propose a\nprobabilistic modeling framework that obtains keyphrases from multiple sources\nand generates query suggestions from these keyphrases. Using the test suites of\nthe TREC Tasks track, we evaluate and analyze each component of our model.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 12:44:14 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Garigliotti", "Dar\u00edo", ""], ["Balog", "Krisztian", ""]]}, {"id": "1708.08291", "submitter": "Dar\\'io Garigliotti", "authors": "Dar\\'io Garigliotti and Krisztian Balog", "title": "On Type-Aware Entity Retrieval", "comments": "Proceedings of the 3rd ACM International Conference on the Theory of\n  Information Retrieval (ICTIR '17), 2017", "journal-ref": null, "doi": "10.1145/3121050.3121054", "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, the practice of returning entities from a knowledge base in response\nto search queries has become widespread. One of the distinctive characteristics\nof entities is that they are typed, i.e., assigned to some hierarchically\norganized type system (type taxonomy). The primary objective of this paper is\nto gain a better understanding of how entity type information can be utilized\nin entity retrieval. We perform this investigation in an idealized \"oracle\"\nsetting, assuming that we know the distribution of target types of the relevant\nentities for a given query. We perform a thorough analysis of three main\naspects: (i) the choice of type taxonomy, (ii) the representation of\nhierarchical type information, and (iii) the combination of type-based and\nterm-based similarity in the retrieval model. Using a standard entity search\ntest collection based on DBpedia, we find that type information proves most\nuseful when using large type taxonomies that provide very specific types. We\nprovide further insights on the extensional coverage of entities and on the\nutility of target types.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 12:47:04 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Garigliotti", "Dar\u00edo", ""], ["Balog", "Krisztian", ""]]}, {"id": "1708.08296", "submitter": "Wojciech Samek", "authors": "Wojciech Samek, Thomas Wiegand, Klaus-Robert M\\\"uller", "title": "Explainable Artificial Intelligence: Understanding, Visualizing and\n  Interpreting Deep Learning Models", "comments": "8 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the availability of large databases and recent improvements in deep\nlearning methodology, the performance of AI systems is reaching or even\nexceeding the human level on an increasing number of complex tasks. Impressive\nexamples of this development can be found in domains such as image\nclassification, sentiment analysis, speech understanding or strategic game\nplaying. However, because of their nested non-linear structure, these highly\nsuccessful machine learning and artificial intelligence models are usually\napplied in a black box manner, i.e., no information is provided about what\nexactly makes them arrive at their predictions. Since this lack of transparency\ncan be a major drawback, e.g., in medical applications, the development of\nmethods for visualizing, explaining and interpreting deep learning models has\nrecently attracted increasing attention. This paper summarizes recent\ndevelopments in this field and makes a plea for more interpretability in\nartificial intelligence. Furthermore, it presents two approaches to explaining\npredictions of deep learning models, one method which computes the sensitivity\nof the prediction with respect to changes in the input and one approach which\nmeaningfully decomposes the decision in terms of the input variables. These\nmethods are evaluated on three classification tasks.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 12:53:49 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Samek", "Wojciech", ""], ["Wiegand", "Thomas", ""], ["M\u00fcller", "Klaus-Robert", ""]]}, {"id": "1708.08299", "submitter": "Wojciech Samek", "authors": "Wojciech Samek, Slawomir Stanczak, Thomas Wiegand", "title": "The Convergence of Machine Learning and Communications", "comments": "8 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NI cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The areas of machine learning and communication technology are converging.\nToday's communications systems generate a huge amount of traffic data, which\ncan help to significantly enhance the design and management of networks and\ncommunication components when combined with advanced machine learning methods.\nFurthermore, recently developed end-to-end training procedures offer new ways\nto jointly optimize the components of a communication system. Also in many\nemerging application fields of communication technology, e.g., smart cities or\ninternet of things, machine learning methods are of central importance. This\npaper gives an overview over the use of machine learning in different areas of\ncommunications and discusses two exemplar applications in wireless networking.\nFurthermore, it identifies promising future research topics and discusses their\npotential impact.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 13:02:56 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Samek", "Wojciech", ""], ["Stanczak", "Slawomir", ""], ["Wiegand", "Thomas", ""]]}, {"id": "1708.08377", "submitter": "Shunichi Matsubara", "authors": "Shunichi Matsubara", "title": "Two-Dimensional Indirect Binary Search for the Positive One-in-Three\n  Satisfiability Problem", "comments": "This version added a subsection for describing the idea of the\n  algorithm", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.CC cs.DM math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose an algorithm for the positive one-in-three\nsatisfiability problem (Pos1in3SAT). The proposed algorithm can efficiently\ndecide the existence of a satisfying assignment in all assignments for a given\nformula by using a 2-dimensional binary search method without constructing an\nexponential number of assignments.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 15:34:04 GMT"}, {"version": "v2", "created": "Sun, 17 Sep 2017 11:50:50 GMT"}], "update_date": "2017-09-19", "authors_parsed": [["Matsubara", "Shunichi", ""]]}, {"id": "1708.08430", "submitter": "J.T. Turner", "authors": "JT Turner, Adam Page, Tinoosh Mohsenin and Tim Oates", "title": "Deep Belief Networks used on High Resolution Multichannel\n  Electroencephalography Data for Seizure Detection", "comments": "Old draft of AAAI paper, AAAI Spring Symposium Series. 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ubiquitous bio-sensing for personalized health monitoring is slowly becoming\na reality with the increasing availability of small, diverse, robust, high\nfidelity sensors. This oncoming flood of data begs the question of how we will\nextract useful information from it. In this paper we explore the use of a\nvariety of representations and machine learning algorithms applied to the task\nof seizure detection in high resolution, multichannel EEG data. We explore\nclassification accuracy, computational complexity and memory requirements with\na view toward understanding which approaches are most suitable for such tasks\nas the number of people involved and the amount of data they produce grows to\nbe quite large. In particular, we show that layered learning approaches such as\nDeep Belief Networks excel along these dimensions.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 17:28:48 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Turner", "JT", ""], ["Page", "Adam", ""], ["Mohsenin", "Tinoosh", ""], ["Oates", "Tim", ""]]}, {"id": "1708.08508", "submitter": "Azin Asgarian", "authors": "Azin Asgarian, Ahmed Bilal Ashraf, David Fleet, and Babak Taati", "title": "Subspace Selection to Suppress Confounding Source Domain Information in\n  AAM Transfer Learning", "comments": "Copyright IEEE. To be published in the proceedings of International\n  Joint Conference on Biometrics (IJCB) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Active appearance models (AAMs) are a class of generative models that have\nseen tremendous success in face analysis. However, model learning depends on\nthe availability of detailed annotation of canonical landmark points. As a\nresult, when accurate AAM fitting is required on a different set of variations\n(expression, pose, identity), a new dataset is collected and annotated. To\novercome the need for time consuming data collection and annotation, transfer\nlearning approaches have received recent attention. The goal is to transfer\nknowledge from previously available datasets (source) to a new dataset\n(target). We propose a subspace transfer learning method, in which we select a\nsubspace from the source that best describes the target space. We propose a\nmetric to compute the directional similarity between the source eigenvectors\nand the target subspace. We show an equivalence between this metric and the\nvariance of target data when projected onto source eigenvectors. Using this\nequivalence, we select a subset of source principal directions that capture the\nvariance in target data. To define our model, we augment the selected source\nsubspace with the target subspace learned from a handful of target examples. In\nexperiments done on six publicly available datasets, we show that our approach\noutperforms the state of the art in terms of the RMS fitting error as well as\nthe percentage of test examples for which AAM fitting converges to the ground\ntruth.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 20:21:21 GMT"}, {"version": "v2", "created": "Tue, 3 Oct 2017 18:26:16 GMT"}], "update_date": "2017-10-05", "authors_parsed": [["Asgarian", "Azin", ""], ["Ashraf", "Ahmed Bilal", ""], ["Fleet", "David", ""], ["Taati", "Babak", ""]]}, {"id": "1708.08551", "submitter": "Mohammad Amin Nabian", "authors": "Mohammad Amin Nabian, Hadi Meidani", "title": "Deep Learning for Accelerated Reliability Analysis of Infrastructure\n  Networks", "comments": null, "journal-ref": "Nabian, M. A. and Meidani, H. (2018), Deep Learning for\n  Accelerated Seismic Reliability Analysis of Transportation Networks. Computer\n  Aided Civil and Infrastructure Engineering, 33: 443-458", "doi": "10.1111/mice.12359", "report-no": null, "categories": "cs.CE cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Natural disasters can have catastrophic impacts on the functionality of\ninfrastructure systems and cause severe physical and socio-economic losses.\nGiven budget constraints, it is crucial to optimize decisions regarding\nmitigation, preparedness, response, and recovery practices for these systems.\nThis requires accurate and efficient means to evaluate the infrastructure\nsystem reliability. While numerous research efforts have addressed and\nquantified the impact of natural disasters on infrastructure systems, typically\nusing the Monte Carlo approach, they still suffer from high computational cost\nand, thus, are of limited applicability to large systems. This paper presents a\ndeep learning framework for accelerating infrastructure system reliability\nanalysis. In particular, two distinct deep neural network surrogates are\nconstructed and studied: (1) A classifier surrogate which speeds up the\nconnectivity determination of networks, and (2) An end-to-end surrogate that\nreplaces a number of components such as roadway status realization,\nconnectivity determination, and connectivity averaging. The proposed approach\nis applied to a simulation-based study of the two-terminal connectivity of a\nCalifornia transportation network subject to extreme probabilistic earthquake\nevents. Numerical results highlight the effectiveness of the proposed approach\nin accelerating the transportation system two-terminal reliability analysis\nwith extremely high prediction accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 22:41:11 GMT"}], "update_date": "2018-06-11", "authors_parsed": [["Nabian", "Mohammad Amin", ""], ["Meidani", "Hadi", ""]]}, {"id": "1708.08559", "submitter": "Baishakhi Ray", "authors": "Yuchi Tian, Kexin Pei, Suman Jana, Baishakhi Ray", "title": "DeepTest: Automated Testing of Deep-Neural-Network-driven Autonomous\n  Cars", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent advances in Deep Neural Networks (DNNs) have led to the development of\nDNN-driven autonomous cars that, using sensors like camera, LiDAR, etc., can\ndrive without any human intervention. Most major manufacturers including Tesla,\nGM, Ford, BMW, and Waymo/Google are working on building and testing different\ntypes of autonomous vehicles. The lawmakers of several US states including\nCalifornia, Texas, and New York have passed new legislation to fast-track the\nprocess of testing and deployment of autonomous vehicles on their roads.\n  However, despite their spectacular progress, DNNs, just like traditional\nsoftware, often demonstrate incorrect or unexpected corner case behaviors that\ncan lead to potentially fatal collisions. Several such real-world accidents\ninvolving autonomous cars have already happened including one which resulted in\na fatality. Most existing testing techniques for DNN-driven vehicles are\nheavily dependent on the manual collection of test data under different driving\nconditions which become prohibitively expensive as the number of test\nconditions increases.\n  In this paper, we design, implement and evaluate DeepTest, a systematic\ntesting tool for automatically detecting erroneous behaviors of DNN-driven\nvehicles that can potentially lead to fatal crashes. First, our tool is\ndesigned to automatically generated test cases leveraging real-world changes in\ndriving conditions like rain, fog, lighting conditions, etc. DeepTest\nsystematically explores different parts of the DNN logic by generating test\ninputs that maximize the numbers of activated neurons. DeepTest found thousands\nof erroneous behaviors under different realistic driving conditions (e.g.,\nblurring, rain, fog, etc.) many of which lead to potentially fatal crashes in\nthree top performing DNNs in the Udacity self-driving car challenge.\n", "versions": [{"version": "v1", "created": "Mon, 28 Aug 2017 23:26:14 GMT"}, {"version": "v2", "created": "Tue, 20 Mar 2018 06:10:24 GMT"}], "update_date": "2018-03-21", "authors_parsed": [["Tian", "Yuchi", ""], ["Pei", "Kexin", ""], ["Jana", "Suman", ""], ["Ray", "Baishakhi", ""]]}, {"id": "1708.08611", "submitter": "Mohammed Alshiekh", "authors": "Mohammed Alshiekh, Roderick Bloem, Ruediger Ehlers, Bettina\n  K\\\"onighofer, Scott Niekum, Ufuk Topcu", "title": "Safe Reinforcement Learning via Shielding", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning algorithms discover policies that maximize reward, but\ndo not necessarily guarantee safety during learning or execution phases. We\nintroduce a new approach to learn optimal policies while enforcing properties\nexpressed in temporal logic. To this end, given the temporal logic\nspecification that is to be obeyed by the learning system, we propose to\nsynthesize a reactive system called a shield. The shield is introduced in the\ntraditional learning process in two alternative ways, depending on the location\nat which the shield is implemented. In the first one, the shield acts each time\nthe learning agent is about to make a decision and provides a list of safe\nactions. In the second way, the shield is introduced after the learning agent.\nThe shield monitors the actions from the learner and corrects them only if the\nchosen action causes a violation of the specification. We discuss which\nrequirements a shield must meet to preserve the convergence guarantees of the\nlearner. Finally, we demonstrate the versatility of our approach on several\nchallenging reinforcement learning scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 07:16:54 GMT"}, {"version": "v2", "created": "Sun, 3 Sep 2017 20:35:33 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Alshiekh", "Mohammed", ""], ["Bloem", "Roderick", ""], ["Ehlers", "Ruediger", ""], ["K\u00f6nighofer", "Bettina", ""], ["Niekum", "Scott", ""], ["Topcu", "Ufuk", ""]]}, {"id": "1708.08722", "submitter": "Jose M. Pe\\~na", "authors": "Jose M. Pe\\~na", "title": "Unifying DAGs and UGs", "comments": "v2: A factorization property has been added in Section 5. v3: Minor\n  errors corrected. v4: Additional example added. v5: Learning algorithm added.\n  v6: Section 6 added. v7: Appendix B added", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new class of graphical models that generalizes\nLauritzen-Wermuth-Frydenberg chain graphs by relaxing the semi-directed\nacyclity constraint so that only directed cycles are forbidden. Moreover, up to\ntwo edges are allowed between any pair of nodes. Specifically, we present\nlocal, pairwise and global Markov properties for the new graphical models and\nprove their equivalence. We also present an equivalent factorization property.\nFinally, we present a causal interpretation of the new models.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 12:17:59 GMT"}, {"version": "v2", "created": "Tue, 12 Sep 2017 17:46:13 GMT"}, {"version": "v3", "created": "Tue, 17 Oct 2017 11:43:48 GMT"}, {"version": "v4", "created": "Thu, 14 Dec 2017 15:40:53 GMT"}, {"version": "v5", "created": "Wed, 17 Jan 2018 13:42:29 GMT"}, {"version": "v6", "created": "Tue, 23 Jan 2018 20:57:19 GMT"}, {"version": "v7", "created": "Mon, 19 Feb 2018 09:47:52 GMT"}, {"version": "v8", "created": "Thu, 1 Mar 2018 17:02:21 GMT"}], "update_date": "2018-03-02", "authors_parsed": [["Pe\u00f1a", "Jose M.", ""]]}, {"id": "1708.08917", "submitter": "Caiwen Ding Kevin Ding", "authors": "Caiwen Ding, Siyu Liao, Yanzhi Wang, Zhe Li, Ning Liu, Youwei Zhuo,\n  Chao Wang, Xuehai Qian, Yu Bai, Geng Yuan, Xiaolong Ma, Yipeng Zhang, Jian\n  Tang, Qinru Qiu, Xue Lin, Bo Yuan", "title": "CirCNN: Accelerating and Compressing Deep Neural Networks Using\n  Block-CirculantWeight Matrices", "comments": "14 pages, 15 Figures, conference", "journal-ref": null, "doi": "10.1145/3123939.3124552", "report-no": null, "categories": "cs.CV cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Large-scale deep neural networks (DNNs) are both compute and memory\nintensive. As the size of DNNs continues to grow, it is critical to improve the\nenergy efficiency and performance while maintaining accuracy. For DNNs, the\nmodel size is an important factor affecting performance, scalability and energy\nefficiency. Weight pruning achieves good compression ratios but suffers from\nthree drawbacks: 1) the irregular network structure after pruning; 2) the\nincreased training complexity; and 3) the lack of rigorous guarantee of\ncompression ratio and inference accuracy. To overcome these limitations, this\npaper proposes CirCNN, a principled approach to represent weights and process\nneural networks using block-circulant matrices. CirCNN utilizes the Fast\nFourier Transform (FFT)-based fast multiplication, simultaneously reducing the\ncomputational complexity (both in inference and training) from O(n2) to\nO(nlogn) and the storage complexity from O(n2) to O(n), with negligible\naccuracy loss. Compared to other approaches, CirCNN is distinct due to its\nmathematical rigor: it can converge to the same effectiveness as DNNs without\ncompression. The CirCNN architecture, a universal DNN inference engine that can\nbe implemented on various hardware/software platforms with configurable network\narchitecture. To demonstrate the performance and energy efficiency, we test\nCirCNN in FPGA, ASIC and embedded processors. Our results show that CirCNN\narchitecture achieves very high energy efficiency and performance with a small\nhardware footprint. Based on the FPGA implementation and ASIC synthesis\nresults, CirCNN achieves 6-102X energy efficiency improvements compared with\nthe best state-of-the-art results.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 04:18:57 GMT"}], "update_date": "2017-09-11", "authors_parsed": [["Ding", "Caiwen", ""], ["Liao", "Siyu", ""], ["Wang", "Yanzhi", ""], ["Li", "Zhe", ""], ["Liu", "Ning", ""], ["Zhuo", "Youwei", ""], ["Wang", "Chao", ""], ["Qian", "Xuehai", ""], ["Bai", "Yu", ""], ["Yuan", "Geng", ""], ["Ma", "Xiaolong", ""], ["Zhang", "Yipeng", ""], ["Tang", "Jian", ""], ["Qiu", "Qinru", ""], ["Lin", "Xue", ""], ["Yuan", "Bo", ""]]}, {"id": "1708.08985", "submitter": "Giovanni De Magistris", "authors": "Asim Munawar, Phongtharin Vinayavekhin and Giovanni De Magistris", "title": "Limiting the Reconstruction Capability of Generative Neural Network\n  using Negative Learning", "comments": "Conference: IEEE International Workshop on Machine Learning for\n  Signal Processing (MLSP), Roppongi, Tokyo, Japan, September 25-28, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative models are widely used for unsupervised learning with various\napplications, including data compression and signal restoration. Training\nmethods for such systems focus on the generality of the network given limited\namount of training data. A less researched type of techniques concerns\ngeneration of only a single type of input. This is useful for applications such\nas constraint handling, noise reduction and anomaly detection. In this paper we\npresent a technique to limit the generative capability of the network using\nnegative learning. The proposed method searches the solution in the gradient\ndirection for the desired input and in the opposite direction for the undesired\ninput. One of the application can be anomaly detection where the undesired\ninputs are the anomalous data. In the results section we demonstrate the\nfeatures of the algorithm using MNIST handwritten digit dataset and latter\napply the technique to a real-world obstacle detection problem. The results\nclearly show that the proposed learning technique can significantly improve the\nperformance for anomaly detection.\n", "versions": [{"version": "v1", "created": "Wed, 16 Aug 2017 01:16:14 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Munawar", "Asim", ""], ["Vinayavekhin", "Phongtharin", ""], ["De Magistris", "Giovanni", ""]]}, {"id": "1708.09020", "submitter": "Abbas Kazerouni", "authors": "Abbas Kazerouni and Benjamin Van Roy", "title": "Learning to Price with Reference Effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a firm varies the price of a product, consumers exhibit reference effects,\nmaking purchase decisions based not only on the prevailing price but also the\nproduct's price history. We consider the problem of learning such behavioral\npatterns as a monopolist releases, markets, and prices products. This context\ncalls for pricing decisions that intelligently trade off between maximizing\nrevenue generated by a current product and probing to gain information for\nfuture benefit. Due to dependence on price history, realized demand can reflect\ndelayed consequences of earlier pricing decisions. As such, inference entails\nattribution of outcomes to prior decisions and effective exploration requires\nplanning price sequences that yield informative future outcomes. Despite the\nconsiderable complexity of this problem, we offer a tractable systematic\napproach. In particular, we frame the problem as one of reinforcement learning\nand leverage Thompson sampling. We also establish a regret bound that provides\ngraceful guarantees on how performance improves as data is gathered and how\nthis depends on the complexity of the demand model. We illustrate merits of the\napproach through simulations.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 20:40:10 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Kazerouni", "Abbas", ""], ["Van Roy", "Benjamin", ""]]}, {"id": "1708.09032", "submitter": "Andrew MacFie", "authors": "Andrew MacFie", "title": "Plausibility and probability in deductive reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of rational uncertainty about unproven mathematical\nstatements, remarked on by G\\\"odel and others. Using Bayesian-inspired\narguments we build a normative model of fair bets under deductive uncertainty\nwhich draws from both probability and the theory of algorithms. We comment on\nconnections to Zeilberger's notion of \"semi-rigorous proofs\", particularly that\ninherent subjectivity would be present. We also discuss a financial view with\nmodels of arbitrage where traders have limited computational resources.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 21:29:05 GMT"}, {"version": "v2", "created": "Fri, 22 Sep 2017 17:03:21 GMT"}, {"version": "v3", "created": "Wed, 6 Feb 2019 04:48:57 GMT"}, {"version": "v4", "created": "Mon, 4 Mar 2019 16:09:35 GMT"}, {"version": "v5", "created": "Sun, 24 Mar 2019 23:00:49 GMT"}, {"version": "v6", "created": "Mon, 16 Dec 2019 17:27:47 GMT"}], "update_date": "2019-12-17", "authors_parsed": [["MacFie", "Andrew", ""]]}, {"id": "1708.09040", "submitter": "Elahe Rahimtoroghi", "authors": "Elahe Rahimtoroghi, Jiaqi Wu, Ruimin Wang, Pranav Anand, Marilyn A\n  Walker", "title": "Modelling Protagonist Goals and Desires in First-Person Narrative", "comments": "10 pages, 18th Annual SIGdial Meeting on Discourse and Dialogue\n  (SIGDIAL 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many genres of natural language text are narratively structured, a testament\nto our predilection for organizing our experiences as narratives. There is\nbroad consensus that understanding a narrative requires identifying and\ntracking the goals and desires of the characters and their narrative outcomes.\nHowever, to date, there has been limited work on computational models for this\nproblem. We introduce a new dataset, DesireDB, which includes gold-standard\nlabels for identifying statements of desire, textual evidence for desire\nfulfillment, and annotations for whether the stated desire is fulfilled given\nthe evidence in the narrative context. We report experiments on tracking desire\nfulfillment using different methods, and show that LSTM Skip-Thought model\nachieves F-measure of 0.7 on our corpus.\n", "versions": [{"version": "v1", "created": "Tue, 29 Aug 2017 21:40:22 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Rahimtoroghi", "Elahe", ""], ["Wu", "Jiaqi", ""], ["Wang", "Ruimin", ""], ["Anand", "Pranav", ""], ["Walker", "Marilyn A", ""]]}, {"id": "1708.09086", "submitter": "Caleb Robinson", "authors": "Caleb Robinson, Fred Hohman, Bistra Dilkina", "title": "A Deep Learning Approach for Population Estimation from Satellite\n  Imagery", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowing where people live is a fundamental component of many decision making\nprocesses such as urban development, infectious disease containment, evacuation\nplanning, risk management, conservation planning, and more. While bottom-up,\nsurvey driven censuses can provide a comprehensive view into the population\nlandscape of a country, they are expensive to realize, are infrequently\nperformed, and only provide population counts over broad areas. Population\ndisaggregation techniques and population projection methods individually\naddress these shortcomings, but also have shortcomings of their own. To jointly\nanswer the questions of \"where do people live\" and \"how many people live\nthere,\" we propose a deep learning model for creating high-resolution\npopulation estimations from satellite imagery. Specifically, we train\nconvolutional neural networks to predict population in the USA at a\n$0.01^{\\circ} \\times 0.01^{\\circ}$ resolution grid from 1-year composite\nLandsat imagery. We validate these models in two ways: quantitatively, by\ncomparing our model's grid cell estimates aggregated at a county-level to\nseveral US Census county-level population projections, and qualitatively, by\ndirectly interpreting the model's predictions in terms of the satellite image\ninputs. We find that aggregating our model's estimates gives comparable results\nto the Census county-level population projections and that the predictions made\nby our model can be directly interpreted, which give it advantages over\ntraditional population disaggregation methods. In general, our model is an\nexample of how machine learning techniques can be an effective tool for\nextracting information from inherently unstructured, remotely sensed data to\nprovide effective solutions to social problems.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 02:05:16 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Robinson", "Caleb", ""], ["Hohman", "Fred", ""], ["Dilkina", "Bistra", ""]]}, {"id": "1708.09175", "submitter": "Saverio De Vito", "authors": "S. De Vito, E. Esposito, M. Salvato, O. Popoola, F. Formisano, R.\n  Jones, G. Di Francia", "title": "Calibrating chemical multisensory devices for real world applications:\n  An in-depth comparison of quantitative Machine Learning approaches", "comments": null, "journal-ref": "Sensors and Actuators B: Chemical, Volume 255, Part 2, 2018, Pages\n  1191-1210, ISSN 0925-4005", "doi": "10.1016/j.snb.2017.07.155", "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Chemical multisensor devices need calibration algorithms to estimate gas\nconcentrations. Their possible adoption as indicative air quality measurements\ndevices poses new challenges due to the need to operate in continuous\nmonitoring modes in uncontrolled environments. Several issues, including slow\ndynamics, continue to affect their real world performances. At the same time,\nthe need for estimating pollutant concentrations on board the devices, espe-\ncially for wearables and IoT deployments, is becoming highly desirable. In this\nframework, several calibration approaches have been proposed and tested on a\nvariety of proprietary devices and datasets; still, no thorough comparison is\navailable to researchers. This work attempts a benchmarking of the most\npromising calibration algorithms according to recent literature with a focus on\nmachine learning approaches. We test the techniques against absolute and\ndynamic performances, generalization capabilities and computational/storage\nneeds using three different datasets sharing continuous monitoring operation\nmethodology. Our results can guide researchers and engineers in the choice of\noptimal strategy. They show that non-linear multivariate techniques yield\nreproducible results, outperforming lin- ear approaches. Specifically, the\nSupport Vector Regression method consistently shows good performances in all\nthe considered scenarios. We highlight the enhanced suitability of shallow\nneural networks in a trade-off between performance and computational/storage\nneeds. We confirm, on a much wider basis, the advantages of dynamic approaches\nwith respect to static ones that only rely on instantaneous sensor array\nresponse. The latter have been shown to be best choice whenever prompt and\nprecise response is needed.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 08:53:15 GMT"}], "update_date": "2020-02-14", "authors_parsed": [["De Vito", "S.", ""], ["Esposito", "E.", ""], ["Salvato", "M.", ""], ["Popoola", "O.", ""], ["Formisano", "F.", ""], ["Jones", "R.", ""], ["Di Francia", "G.", ""]]}, {"id": "1708.09251", "submitter": "Antoine Cully", "authors": "Antoine Cully and Yiannis Demiris", "title": "Quality and Diversity Optimization: A Unifying Modular Framework", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The optimization of functions to find the best solution according to one or\nseveral objectives has a central role in many engineering and research fields.\nRecently, a new family of optimization algorithms, named Quality-Diversity\noptimization, has been introduced, and contrasts with classic algorithms.\nInstead of searching for a single solution, Quality-Diversity algorithms are\nsearching for a large collection of both diverse and high-performing solutions.\nThe role of this collection is to cover the range of possible solution types as\nmuch as possible, and to contain the best solution for each type. The\ncontribution of this paper is threefold. Firstly, we present a unifying\nframework of Quality-Diversity optimization algorithms that covers the two main\nalgorithms of this family (Multi-dimensional Archive of Phenotypic Elites and\nthe Novelty Search with Local Competition), and that highlights the large\nvariety of variants that can be investigated within this family. Secondly, we\npropose algorithms with a new selection mechanism for Quality-Diversity\nalgorithms that outperforms all the algorithms tested in this paper. Lastly, we\npresent a new collection management that overcomes the erosion issues observed\nwhen using unstructured collections. These three contributions are supported by\nextensive experimental comparisons of Quality-Diversity algorithms on three\ndifferent experimental scenarios.\n", "versions": [{"version": "v1", "created": "Fri, 12 May 2017 13:38:08 GMT"}], "update_date": "2017-08-31", "authors_parsed": [["Cully", "Antoine", ""], ["Demiris", "Yiannis", ""]]}, {"id": "1708.09401", "submitter": "Huitao Shen", "authors": "Pengfei Zhang, Huitao Shen, Hui Zhai", "title": "Machine Learning Topological Invariants with Neural Networks", "comments": "6 pages, 4 figures and 1 table + 2 pages of supplemental material", "journal-ref": "Phys. Rev. Lett. 120, 066401 (2018)", "doi": "10.1103/PhysRevLett.120.066401", "report-no": null, "categories": "cond-mat.mes-hall cond-mat.dis-nn cond-mat.str-el cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this Letter we supervisedly train neural networks to distinguish different\ntopological phases in the context of topological band insulators. After\ntraining with Hamiltonians of one-dimensional insulators with chiral symmetry,\nthe neural network can predict their topological winding numbers with nearly\n100% accuracy, even for Hamiltonians with larger winding numbers that are not\nincluded in the training data. These results show a remarkable success that the\nneural network can capture the global and nonlinear topological features of\nquantum phases from local inputs. By opening up the neural network, we confirm\nthat the network does learn the discrete version of the winding number formula.\nWe also make a couple of remarks regarding the role of the symmetry and the\nopposite effect of regularization techniques when applying machine learning to\nphysical systems.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 18:00:51 GMT"}, {"version": "v2", "created": "Thu, 7 Sep 2017 01:07:25 GMT"}, {"version": "v3", "created": "Fri, 19 Jan 2018 23:13:36 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Zhang", "Pengfei", ""], ["Shen", "Huitao", ""], ["Zhai", "Hui", ""]]}, {"id": "1708.09427", "submitter": "Li Shen", "authors": "Li Shen, Laurie R. Margolies, Joseph H. Rothstein, Eugene Fluder,\n  Russell B. McBride, Weiva Sieh", "title": "Deep Learning to Improve Breast Cancer Early Detection on Screening\n  Mammography", "comments": "Major modification with an additional figure and new results", "journal-ref": "Scientific Reports, volume 9, Article number: 12495 (2019)", "doi": "10.1038/s41598-019-48995-4", "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  The rapid development of deep learning, a family of machine learning\ntechniques, has spurred much interest in its application to medical imaging\nproblems. Here, we develop a deep learning algorithm that can accurately detect\nbreast cancer on screening mammograms using an \"end-to-end\" training approach\nthat efficiently leverages training datasets with either complete clinical\nannotation or only the cancer status (label) of the whole image. In this\napproach, lesion annotations are required only in the initial training stage,\nand subsequent stages require only image-level labels, eliminating the reliance\non rarely available lesion annotations. Our all convolutional network method\nfor classifying screening mammograms attained excellent performance in\ncomparison with previous methods. On an independent test set of digitized film\nmammograms from Digital Database for Screening Mammography (DDSM), the best\nsingle model achieved a per-image AUC of 0.88, and four-model averaging\nimproved the AUC to 0.91 (sensitivity: 86.1%, specificity: 80.1%). On a\nvalidation set of full-field digital mammography (FFDM) images from the\nINbreast database, the best single model achieved a per-image AUC of 0.95, and\nfour-model averaging improved the AUC to 0.98 (sensitivity: 86.7%, specificity:\n96.1%). We also demonstrate that a whole image classifier trained using our\nend-to-end approach on the DDSM digitized film mammograms can be transferred to\nINbreast FFDM images using only a subset of the INbreast data for fine-tuning\nand without further reliance on the availability of lesion annotations. These\nfindings show that automatic deep learning methods can be readily trained to\nattain high accuracy on heterogeneous mammography platforms, and hold\ntremendous promise for improving clinical tools to reduce false positive and\nfalse negative screening mammography results.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 18:46:16 GMT"}, {"version": "v2", "created": "Fri, 6 Oct 2017 14:40:06 GMT"}, {"version": "v3", "created": "Thu, 19 Oct 2017 02:03:40 GMT"}, {"version": "v4", "created": "Sat, 22 Sep 2018 16:10:10 GMT"}, {"version": "v5", "created": "Mon, 31 Dec 2018 23:23:07 GMT"}], "update_date": "2019-10-08", "authors_parsed": [["Shen", "Li", ""], ["Margolies", "Laurie R.", ""], ["Rothstein", "Joseph H.", ""], ["Fluder", "Eugene", ""], ["McBride", "Russell B.", ""], ["Sieh", "Weiva", ""]]}, {"id": "1708.09441", "submitter": "Shubhomoy Das", "authors": "Shubhomoy Das, Weng-Keen Wong, Alan Fern, Thomas G. Dietterich, Md\n  Amran Siddiqui", "title": "Incorporating Feedback into Tree-based Anomaly Detection", "comments": "8 Pages, KDD 2017 Workshop on Interactive Data Exploration and\n  Analytics (IDEA'17), August 14th, 2017, Halifax, Nova Scotia, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Anomaly detectors are often used to produce a ranked list of statistical\nanomalies, which are examined by human analysts in order to extract the actual\nanomalies of interest. Unfortunately, in realworld applications, this process\ncan be exceedingly difficult for the analyst since a large fraction of\nhigh-ranking anomalies are false positives and not interesting from the\napplication perspective. In this paper, we aim to make the analyst's job easier\nby allowing for analyst feedback during the investigation process. Ideally, the\nfeedback influences the ranking of the anomaly detector in a way that reduces\nthe number of false positives that must be examined before discovering the\nanomalies of interest. In particular, we introduce a novel technique for\nincorporating simple binary feedback into tree-based anomaly detectors. We\nfocus on the Isolation Forest algorithm as a representative tree-based anomaly\ndetector, and show that we can significantly improve its performance by\nincorporating feedback, when compared with the baseline algorithm that does not\nincorporate feedback. Our technique is simple and scales well as the size of\nthe data increases, which makes it suitable for interactive discovery of\nanomalies in large datasets.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 19:36:21 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Das", "Shubhomoy", ""], ["Wong", "Weng-Keen", ""], ["Fern", "Alan", ""], ["Dietterich", "Thomas G.", ""], ["Siddiqui", "Md Amran", ""]]}, {"id": "1708.09450", "submitter": "Elahe Rahimtoroghi", "authors": "Elahe Rahimtoroghi, Ernesto Hernandez, Marilyn A Walker", "title": "Learning Fine-Grained Knowledge about Contingent Relations between\n  Everyday Events", "comments": "SIGDIAL 2016", "journal-ref": "17th Annual SIGdial Meeting on Discourse and Dialogue (SIGDIAL\n  2016)", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of the user-generated content on social media is provided by ordinary\npeople telling stories about their daily lives. We develop and test a novel\nmethod for learning fine-grained common-sense knowledge from these stories\nabout contingent (causal and conditional) relationships between everyday\nevents. This type of knowledge is useful for text and story understanding,\ninformation extraction, question answering, and text summarization. We test and\ncompare different methods for learning contingency relation, and compare what\nis learned from topic-sorted story collections vs. general-domain stories. Our\nexperiments show that using topic-specific datasets enables learning\nfiner-grained knowledge about events and results in significant improvement\nover the baselines. An evaluation on Amazon Mechanical Turk shows 82% of the\nrelations between events that we learn from topic-sorted stories are judged as\ncontingent.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 20:01:34 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Rahimtoroghi", "Elahe", ""], ["Hernandez", "Ernesto", ""], ["Walker", "Marilyn A", ""]]}, {"id": "1708.09453", "submitter": "Elahe Rahimtoroghi", "authors": "Zhichao Hu, Elahe Rahimtoroghi, Marilyn A Walker", "title": "Inference of Fine-Grained Event Causality from Blogs and Films", "comments": "Events and Stories in the News Workshop, ACL 2017", "journal-ref": "Events and Stories in the News Workshop, ACL 2017", "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human understanding of narrative is mainly driven by reasoning about causal\nrelations between events and thus recognizing them is a key capability for\ncomputational models of language understanding. Computational work in this area\nhas approached this via two different routes: by focusing on acquiring a\nknowledge base of common causal relations between events, or by attempting to\nunderstand a particular story or macro-event, along with its storyline. In this\nposition paper, we focus on knowledge acquisition approach and claim that\nnewswire is a relatively poor source for learning fine-grained causal relations\nbetween everyday events. We describe experiments using an unsupervised method\nto learn causal relations between events in the narrative genres of\nfirst-person narratives and film scene descriptions. We show that our method\nlearns fine-grained causal relations, judged by humans as likely to be causal\nover 80% of the time. We also demonstrate that the learned event pairs do not\nexist in publicly available event-pair datasets extracted from newswire.\n", "versions": [{"version": "v1", "created": "Wed, 30 Aug 2017 20:12:01 GMT"}], "update_date": "2017-09-01", "authors_parsed": [["Hu", "Zhichao", ""], ["Rahimtoroghi", "Elahe", ""], ["Walker", "Marilyn A", ""]]}]