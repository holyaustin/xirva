[{"id": "1009.0077", "submitter": "Emanuel Diamant", "authors": "Emanuel Diamant", "title": "Not only a lack of right definitions: Arguments for a shift in\n  information-processing paradigm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Consciousness and Machine Intelligence are not simply new buzzwords\nthat occupy our imagination. Over the last decades, we witness an unprecedented\nrise in attempts to create machines with human-like features and capabilities.\nHowever, despite widespread sympathy and abundant funding, progress in these\nenterprises is far from being satisfactory. The reasons for this are twofold:\nFirst, the notions of cognition and intelligence (usually borrowed from human\nbehavior studies) are notoriously blurred and ill-defined, and second, the\nbasic concepts underpinning the whole discourse are by themselves either\nundefined or defined very vaguely. That leads to improper and inadequate\nresearch goals determination, which I will illustrate with some examples drawn\nfrom recent documents issued by DARPA and the European Commission. On the other\nhand, I would like to propose some remedies that, I hope, would improve the\ncurrent state-of-the-art disgrace.\n", "versions": [{"version": "v1", "created": "Wed, 1 Sep 2010 02:37:54 GMT"}], "update_date": "2010-09-03", "authors_parsed": [["Diamant", "Emanuel", ""]]}, {"id": "1009.0108", "submitter": "Ke Chen", "authors": "Arslan Shaukat and Ke Chen", "title": "Emotional State Categorization from Speech: Machine vs. Human", "comments": "14 pages, 15 figures, 12 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents our investigations on emotional state categorization from\nspeech signals with a psychologically inspired computational model against\nhuman performance under the same experimental setup. Based on psychological\nstudies, we propose a multistage categorization strategy which allows\nestablishing an automatic categorization model flexibly for a given emotional\nspeech categorization task. We apply the strategy to the Serbian Emotional\nSpeech Corpus (GEES) and the Danish Emotional Speech Corpus (DES), where human\nperformance was reported in previous psychological studies. Our work is the\nfirst attempt to apply machine learning to the GEES corpus where the human\nrecognition rates were only available prior to our study. Unlike the previous\nwork on the DES corpus, our work focuses on a comparison to human performance\nunder the same experimental settings. Our studies suggest that\npsychology-inspired systems yield behaviours that, to a great extent, resemble\nwhat humans perceived and their performance is close to that of humans under\nthe same experimental setup. Furthermore, our work also uncovers some\ndifferences between machine and humans in terms of emotional state recognition\nfrom speech.\n", "versions": [{"version": "v1", "created": "Wed, 1 Sep 2010 07:51:07 GMT"}], "update_date": "2010-09-02", "authors_parsed": [["Shaukat", "Arslan", ""], ["Chen", "Ke", ""]]}, {"id": "1009.0347", "submitter": "Peter J. Stuckey", "authors": "Andreas Schutt, Thibaut Feydy, Peter J. Stuckey, Mark G. Wallace", "title": "Solving the Resource Constrained Project Scheduling Problem with\n  Generalized Precedences by Lazy Clause Generation", "comments": "37 pages, 3 figures, 16 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The technical report presents a generic exact solution approach for\nminimizing the project duration of the resource-constrained project scheduling\nproblem with generalized precedences (Rcpsp/max). The approach uses lazy clause\ngeneration, i.e., a hybrid of finite domain and Boolean satisfiability solving,\nin order to apply nogood learning and conflict-driven search on the solution\ngeneration. Our experiments show the benefit of lazy clause generation for\nfinding an optimal solutions and proving its optimality in comparison to other\nstate-of-the-art exact and non-exact methods. The method is highly robust: it\nmatched or bettered the best known results on all of the 2340 instances we\nexamined except 3, according to the currently available data on the PSPLib. Of\nthe 631 open instances in this set it closed 573 and improved the bounds of 51\nof the remaining 58 instances.\n", "versions": [{"version": "v1", "created": "Thu, 2 Sep 2010 08:03:47 GMT"}], "update_date": "2010-09-03", "authors_parsed": [["Schutt", "Andreas", ""], ["Feydy", "Thibaut", ""], ["Stuckey", "Peter J.", ""], ["Wallace", "Mark G.", ""]]}, {"id": "1009.0407", "submitter": "Thanasis Balafoutis", "authors": "Thanasis Balafoutis, Anastasia Paparrizou and Kostas Stergiou", "title": "Experimental Evaluation of Branching Schemes for the CSP", "comments": "To appear in the 3rd workshop on techniques for implementing\n  constraint programming systems (TRICS workshop at the 16th CP Conference),\n  St. Andrews, Scotland 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The search strategy of a CP solver is determined by the variable and value\nordering heuristics it employs and by the branching scheme it follows. Although\nthe effects of variable and value ordering heuristics on search effort have\nbeen widely studied, the effects of different branching schemes have received\nless attention. In this paper we study this effect through an experimental\nevaluation that includes standard branching schemes such as 2-way, d-way, and\ndichotomic domain splitting, as well as variations of set branching where\nbranching is performed on sets of values. We also propose and evaluate a\ngeneric approach to set branching where the partition of a domain into sets is\ncreated using the scores assigned to values by a value ordering heuristic, and\na clustering algorithm from machine learning. Experimental results demonstrate\nthat although exponential differences between branching schemes, as predicted\nin theory between 2-way and d-way branching, are not very common, still the\nchoice of branching scheme can make quite a difference on certain classes of\nproblems. Set branching methods are very competitive with 2-way branching and\noutperform it on some problem classes. A statistical analysis of the results\nreveals that our generic clustering-based set branching method is the best\namong the methods compared.\n", "versions": [{"version": "v1", "created": "Thu, 2 Sep 2010 12:24:32 GMT"}], "update_date": "2010-09-03", "authors_parsed": [["Balafoutis", "Thanasis", ""], ["Paparrizou", "Anastasia", ""], ["Stergiou", "Kostas", ""]]}, {"id": "1009.0451", "submitter": "Fabien Tence", "authors": "Fabien Tenc\\'e (LISYC), C\\'edric Buche (LISYC), Pierre De Loor\n  (LISYC), Olivier Marc (LISYC)", "title": "The Challenge of Believability in Video Games: Definitions, Agents\n  Models and Imitation Learning", "comments": null, "journal-ref": "GAMEON-ASIA'2010, France (2010)", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we address the problem of creating believable agents (virtual\ncharacters) in video games. We consider only one meaning of believability,\n``giving the feeling of being controlled by a player'', and outline the problem\nof its evaluation. We present several models for agents in games which can\nproduce believable behaviours, both from industry and research. For high level\nof believability, learning and especially imitation learning seems to be the\nway to go. We make a quick overview of different approaches to make video\ngames' agents learn from players. To conclude we propose a two-step method to\ndevelop new models for believable agents. First we must find the criteria for\nbelievability for our application and define an evaluation method. Then the\nmodel and the learning algorithm can be designed.\n", "versions": [{"version": "v1", "created": "Thu, 2 Sep 2010 15:25:06 GMT"}], "update_date": "2010-09-03", "authors_parsed": [["Tenc\u00e9", "Fabien", "", "LISYC"], ["Buche", "C\u00e9dric", "", "LISYC"], ["De Loor", "Pierre", "", "LISYC"], ["Marc", "Olivier", "", "LISYC"]]}, {"id": "1009.0501", "submitter": "Fabien Tence", "authors": "Fabien Tenc\\'e (LISYC), C\\'edric Buche (LISYC)", "title": "Automatable Evaluation Method Oriented toward Behaviour Believability\n  for Video Games", "comments": "GAME-ON 2008, France (2008)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Classic evaluation methods of believable agents are time-consuming because\nthey involve many human to judge agents. They are well suited to validate work\non new believable behaviours models. However, during the implementation,\nnumerous experiments can help to improve agents' believability. We propose a\nmethod which aim at assessing how much an agent's behaviour looks like humans'\nbehaviours. By representing behaviours with vectors, we can store data computed\nfor humans and then evaluate as many agents as needed without further need of\nhumans. We present a test experiment which shows that even a simple evaluation\nfollowing our method can reveal differences between quite believable agents and\nhumans. This method seems promising although, as shown in our experiment,\nresults' analysis can be difficult.\n", "versions": [{"version": "v1", "created": "Thu, 2 Sep 2010 18:36:44 GMT"}], "update_date": "2010-09-03", "authors_parsed": [["Tenc\u00e9", "Fabien", "", "LISYC"], ["Buche", "C\u00e9dric", "", "LISYC"]]}, {"id": "1009.0550", "submitter": "Omid David-Tabibi", "authors": "Omid David-Tabibi, Moshe Koppel, Nathan S. Netanyahu", "title": "Optimizing Selective Search in Chess", "comments": null, "journal-ref": "Proceedings of the International Conference on Machine Learning\n  (ICML) Workshop on Machine Learning and Games, Haifa, Israel, June 2010", "doi": null, "report-no": null, "categories": "cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a novel method for automatically tuning the search\nparameters of a chess program using genetic algorithms. Our results show that a\nlarge set of parameter values can be learned automatically, such that the\nresulting performance is comparable with that of manually tuned parameters of\ntop tournament-playing chess programs.\n", "versions": [{"version": "v1", "created": "Thu, 2 Sep 2010 21:41:45 GMT"}], "update_date": "2010-09-06", "authors_parsed": [["David-Tabibi", "Omid", ""], ["Koppel", "Moshe", ""], ["Netanyahu", "Nathan S.", ""]]}, {"id": "1009.0605", "submitter": "Louis Dorard", "authors": "Louis Dorard and John Shawe-Taylor", "title": "Gaussian Process Bandits for Tree Search: Theory and Application to\n  Planning in Discounted MDPs", "comments": "Second draft. Tried to follow the JMLR formatting guidelines. Made\n  corrections to the section on planning in MDPs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We motivate and analyse a new Tree Search algorithm, GPTS, based on recent\ntheoretical advances in the use of Gaussian Processes for Bandit problems. We\nconsider tree paths as arms and we assume the target/reward function is drawn\nfrom a GP distribution. The posterior mean and variance, after observing data,\nare used to define confidence intervals for the function values, and we\nsequentially play arms with highest upper confidence bounds. We give an\nefficient implementation of GPTS and we adapt previous regret bounds by\ndetermining the decay rate of the eigenvalues of the kernel matrix on the whole\nset of tree paths. We consider two kernels in the feature space of binary\nvectors indexed by the nodes of the tree: linear and Gaussian. The regret grows\nin square root of the number of iterations T, up to a logarithmic factor, with\na constant that improves with bigger Gaussian kernel widths. We focus on\npractical values of T, smaller than the number of arms. Finally, we apply GPTS\nto Open Loop Planning in discounted Markov Decision Processes by modelling the\nreward as a discounted sum of independent Gaussian Processes. We report similar\nregret bounds to those of the OLOP algorithm.\n", "versions": [{"version": "v1", "created": "Fri, 3 Sep 2010 08:36:07 GMT"}, {"version": "v2", "created": "Sat, 15 Jan 2011 15:34:21 GMT"}], "update_date": "2011-01-18", "authors_parsed": [["Dorard", "Louis", ""], ["Shawe-Taylor", "John", ""]]}, {"id": "1009.0861", "submitter": "Ameet Talwalkar", "authors": "Mehryar Mohri, Ameet Talwalkar", "title": "On the Estimation of Coherence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Low-rank matrix approximations are often used to help scale standard machine\nlearning algorithms to large-scale problems. Recently, matrix coherence has\nbeen used to characterize the ability to extract global information from a\nsubset of matrix entries in the context of these low-rank approximations and\nother sampling-based algorithms, e.g., matrix com- pletion, robust PCA. Since\ncoherence is defined in terms of the singular vectors of a matrix and is\nexpensive to compute, the practical significance of these results largely\nhinges on the following question: Can we efficiently and accurately estimate\nthe coherence of a matrix? In this paper we address this question. We propose a\nnovel algorithm for estimating coherence from a small number of columns,\nformally analyze its behavior, and derive a new coherence-based matrix\napproximation bound based on this analysis. We then present extensive\nexperimental results on synthetic and real datasets that corroborate our\nworst-case theoretical analysis, yet provide strong support for the use of our\nproposed algorithm whenever low-rank approximation is being considered. Our\nalgorithm efficiently and accurately estimates matrix coherence across a wide\nrange of datasets, and these coherence estimates are excellent predictors of\nthe effectiveness of sampling-based matrix approximation on a case-by-case\nbasis.\n", "versions": [{"version": "v1", "created": "Sat, 4 Sep 2010 19:18:54 GMT"}], "update_date": "2010-09-07", "authors_parsed": [["Mohri", "Mehryar", ""], ["Talwalkar", "Ameet", ""]]}, {"id": "1009.0896", "submitter": "Farnood Merrikh-Bayat", "authors": "Farnood Merrikh-Bayat and Saeed Bagheri Shouraki", "title": "Memristor Crossbar-based Hardware Implementation of Fuzzy Membership\n  Functions", "comments": "5 pages, 5 figures, Submitted to ICCAE 2011 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.AR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In May 1, 2008, researchers at Hewlett Packard (HP) announced the first\nphysical realization of a fundamental circuit element called memristor that\nattracted so much interest worldwide. This newly found element can easily be\ncombined with crossbar interconnect technology which this new structure has\nopened a new field in designing configurable or programmable electronic\nsystems. These systems in return can have applications in signal processing and\nartificial intelligence. In this paper, based on the simple memristor crossbar\nstructure, we propose new and simple circuits for hardware implementation of\nfuzzy membership functions. In our proposed circuits, these fuzzy membership\nfunctions can have any shapes and resolutions. In addition, these circuits can\nbe used as a basis in the construction of evolutionary systems.\n", "versions": [{"version": "v1", "created": "Sun, 5 Sep 2010 07:46:47 GMT"}], "update_date": "2010-09-07", "authors_parsed": [["Merrikh-Bayat", "Farnood", ""], ["Shouraki", "Saeed Bagheri", ""]]}, {"id": "1009.1174", "submitter": "Toby Walsh", "authors": "Toby Walsh", "title": "Parameterized Complexity Results in Symmetry Breaking", "comments": "Invited talk at IPEC 2010, 5th International Symposium on\n  Parameterized and Exact Computation, December 13-15, 2010, Chennai, India\n  http://www.imsc.res.in/ipec", "journal-ref": null, "doi": "10.1007/978-3-642-17493-3_3", "report-no": null, "categories": "cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Symmetry is a common feature of many combinatorial problems. Unfortunately\neliminating all symmetry from a problem is often computationally intractable.\nThis paper argues that recent parameterized complexity results provide insight\ninto that intractability and help identify special cases in which symmetry can\nbe dealt with more tractably\n", "versions": [{"version": "v1", "created": "Mon, 6 Sep 2010 22:54:46 GMT"}], "update_date": "2015-05-19", "authors_parsed": [["Walsh", "Toby", ""]]}, {"id": "1009.1446", "submitter": "Sanmay Das", "authors": "Aseem Brahma, Sanmay Das and Malik Magdon-Ismail", "title": "Comparing Prediction Market Structures, With an Application to Market\n  Making", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.TR cs.AI cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ensuring sufficient liquidity is one of the key challenges for designers of\nprediction markets. Various market making algorithms have been proposed in the\nliterature and deployed in practice, but there has been little effort to\nevaluate their benefits and disadvantages in a systematic manner. We introduce\na novel experimental design for comparing market structures in live trading\nthat ensures fair comparison between two different microstructures with the\nsame trading population. Participants trade on outcomes related to a\ntwo-dimensional random walk that they observe on their computer screens. They\ncan simultaneously trade in two markets, corresponding to the independent\nhorizontal and vertical random walks. We use this experimental design to\ncompare the popular inventory-based logarithmic market scoring rule (LMSR)\nmarket maker and a new information based Bayesian market maker (BMM). Our\nexperiments reveal that BMM can offer significant benefits in terms of price\nstability and expected loss when controlling for liquidity; the caveat is that,\nunlike LMSR, BMM does not guarantee bounded loss. Our investigation also\nelucidates some general properties of market makers in prediction markets. In\nparticular, there is an inherent tradeoff between adaptability to market shocks\nand convergence during market equilibrium.\n", "versions": [{"version": "v1", "created": "Wed, 8 Sep 2010 03:26:27 GMT"}], "update_date": "2010-09-09", "authors_parsed": [["Brahma", "Aseem", ""], ["Das", "Sanmay", ""], ["Magdon-Ismail", "Malik", ""]]}, {"id": "1009.1720", "submitter": "Dominik Janzing", "authors": "Dominik Janzing", "title": "Is there a physically universal cellular automaton or Hamiltonian?", "comments": "27 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "quant-ph cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is known that both quantum and classical cellular automata (CA) exist that\nare computationally universal in the sense that they can simulate, after\nappropriate initialization, any quantum or classical computation, respectively.\nHere we introduce a different notion of universality: a CA is called physically\nuniversal if every transformation on any finite region can be (approximately)\nimplemented by the autonomous time evolution of the system after the complement\nof the region has been initialized in an appropriate way. We pose the question\nof whether physically universal CAs exist. Such CAs would provide a model of\nthe world where the boundary between a physical system and its controller can\nbe consistently shifted, in analogy to the Heisenberg cut for the quantum\nmeasurement problem. We propose to study the thermodynamic cost of computation\nand control within such a model because implementing a cyclic process on a\nmicrosystem may require a non-cyclic process for its controller, whereas\nimplementing a cyclic process on system and controller may require the\nimplementation of a non-cyclic process on a \"meta\"-controller, and so on.\nPhysically universal CAs avoid this infinite hierarchy of controllers and the\ncost of implementing cycles on a subsystem can be described by mixing\nproperties of the CA dynamics. We define a physical prior on the CA\nconfigurations by applying the dynamics to an initial state where half of the\nCA is in the maximum entropy state and half of it is in the all-zero state\n(thus reflecting the fact that life requires non-equilibrium states like the\nboundary between a hold and a cold reservoir). As opposed to Solomonoff's\nprior, our prior does not only account for the Kolmogorov complexity but also\nfor the cost of isolating the system during the state preparation if the\npreparation process is not robust.\n", "versions": [{"version": "v1", "created": "Thu, 9 Sep 2010 09:40:55 GMT"}], "update_date": "2010-09-10", "authors_parsed": [["Janzing", "Dominik", ""]]}, {"id": "1009.1990", "submitter": "Michael Thomas", "authors": "Michael Thomas and Heribert Vollmer", "title": "Complexity of Non-Monotonic Logics", "comments": "To appear in Bulletin of the EATCS", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CC cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the past few decades, non-monotonic reasoning has developed to be one of\nthe most important topics in computational logic and artificial intelligence.\nDifferent ways to introduce non-monotonic aspects to classical logic have been\nconsidered, e.g., extension with default rules, extension with modal belief\noperators, or modification of the semantics. In this survey we consider a\nlogical formalism from each of the above possibilities, namely Reiter's default\nlogic, Moore's autoepistemic logic and McCarthy's circumscription.\nAdditionally, we consider abduction, where one is not interested in inferences\nfrom a given knowledge base but in computing possible explanations for an\nobservation with respect to a given knowledge base.\n  Complexity results for different reasoning tasks for propositional variants\nof these logics have been studied already in the nineties. In recent years,\nhowever, a renewed interest in complexity issues can be observed. One current\nfocal approach is to consider parameterized problems and identify reasonable\nparameters that allow for FPT algorithms. In another approach, the emphasis\nlies on identifying fragments, i.e., restriction of the logical language, that\nallow more efficient algorithms for the most important reasoning tasks. In this\nsurvey we focus on this second aspect. We describe complexity results for\nfragments of logical languages obtained by either restricting the allowed set\nof operators (e.g., forbidding negations one might consider only monotone\nformulae) or by considering only formulae in conjunctive normal form but with\ngeneralized clause types.\n  The algorithmic problems we consider are suitable variants of satisfiability\nand implication in each of the logics, but also counting problems, where one is\nnot only interested in the existence of certain objects (e.g., models of a\nformula) but asks for their number.\n", "versions": [{"version": "v1", "created": "Fri, 10 Sep 2010 12:03:44 GMT"}], "update_date": "2010-09-13", "authors_parsed": [["Thomas", "Michael", ""], ["Vollmer", "Heribert", ""]]}, {"id": "1009.2003", "submitter": "Zeeshan Ahmed Mr.", "authors": "Zeeshan Ahmed", "title": "AI 3D Cybug Gaming", "comments": "In the proceedings of 9th National Research Conference on Management\n  and Computer Sciences, SZABIST Institute of Science and Technology, Pakistan", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this short paper I briefly discuss 3D war Game based on artificial\nintelligence concepts called AI WAR. Going in to the details, I present the\nimportance of CAICL language and how this language is used in AI WAR. Moreover\nI also present a designed and implemented 3D War Cybug for AI WAR using CAICL\nand discus the implemented strategy to defeat its enemies during the game life.\n", "versions": [{"version": "v1", "created": "Fri, 10 Sep 2010 12:58:30 GMT"}], "update_date": "2010-09-13", "authors_parsed": [["Ahmed", "Zeeshan", ""]]}, {"id": "1009.2009", "submitter": "Truyen Tran", "authors": "Tran The Truyen, Dinh Q. Phung, Hung H. Bui, and Svetha Venkatesh", "title": "Hierarchical Semi-Markov Conditional Random Fields for Recursive\n  Sequential Data", "comments": "56 pages, short version presented at NIPS'08", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the hierarchical hidden Markov models (HHMM), we present the\nhierarchical semi-Markov conditional random field (HSCRF), a generalisation of\nembedded undirectedMarkov chains tomodel complex hierarchical, nestedMarkov\nprocesses. It is parameterised in a discriminative framework and has polynomial\ntime algorithms for learning and inference. Importantly, we consider\npartiallysupervised learning and propose algorithms for generalised\npartially-supervised learning and constrained inference. We demonstrate the\nHSCRF in two applications: (i) recognising human activities of daily living\n(ADLs) from indoor surveillance cameras, and (ii) noun-phrase chunking. We show\nthat the HSCRF is capable of learning rich hierarchical models with reasonable\naccuracy in both fully and partially observed data cases.\n", "versions": [{"version": "v1", "created": "Fri, 10 Sep 2010 13:25:05 GMT"}], "update_date": "2010-09-13", "authors_parsed": [["Truyen", "Tran The", ""], ["Phung", "Dinh Q.", ""], ["Bui", "Hung H.", ""], ["Venkatesh", "Svetha", ""]]}, {"id": "1009.2021", "submitter": "Alexandra Meliou", "authors": "Alexandra Meliou and Wolfgang Gatterbauer and Katherine F. Moore and\n  Dan Suciu", "title": "The Complexity of Causality and Responsibility for Query Answers and\n  non-Answers", "comments": "15 pages, 12 figures, PVLDB 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An answer to a query has a well-defined lineage expression (alternatively\ncalled how-provenance) that explains how the answer was derived. Recent work\nhas also shown how to compute the lineage of a non-answer to a query. However,\nthe cause of an answer or non-answer is a more subtle notion and consists, in\ngeneral, of only a fragment of the lineage. In this paper, we adapt Halpern,\nPearl, and Chockler's recent definitions of causality and responsibility to\ndefine the causes of answers and non-answers to queries, and their degree of\nresponsibility. Responsibility captures the notion of degree of causality and\nserves to rank potentially many causes by their relative contributions to the\neffect. Then, we study the complexity of computing causes and responsibilities\nfor conjunctive queries. It is known that computing causes is NP-complete in\ngeneral. Our first main result shows that all causes to conjunctive queries can\nbe computed by a relational query which may involve negation. Thus, causality\ncan be computed in PTIME, and very efficiently so. Next, we study computing\nresponsibility. Here, we prove that the complexity depends on the conjunctive\nquery and demonstrate a dichotomy between PTIME and NP-complete cases. For the\nPTIME cases, we give a non-trivial algorithm, consisting of a reduction to the\nmax-flow computation problem. Finally, we prove that, even when it is in PTIME,\nresponsibility is complete for LOGSPACE, implying that, unlike causality, it\ncannot be computed by a relational query.\n", "versions": [{"version": "v1", "created": "Fri, 10 Sep 2010 14:35:30 GMT"}, {"version": "v2", "created": "Fri, 30 Sep 2011 00:18:41 GMT"}], "update_date": "2011-10-03", "authors_parsed": [["Meliou", "Alexandra", ""], ["Gatterbauer", "Wolfgang", ""], ["Moore", "Katherine F.", ""], ["Suciu", "Dan", ""]]}, {"id": "1009.2041", "submitter": "Vaishak Belle", "authors": "Vaishak Belle and Gerhard Lakemeyer", "title": "Multi-Agent Only-Knowing Revisited", "comments": "Appears in Principles of Knowledge Representation and Reasoning 2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Levesque introduced the notion of only-knowing to precisely capture the\nbeliefs of a knowledge base. He also showed how only-knowing can be used to\nformalize non-monotonic behavior within a monotonic logic. Despite its appeal,\nall attempts to extend only-knowing to the many agent case have undesirable\nproperties. A belief model by Halpern and Lakemeyer, for instance, appeals to\nproof-theoretic constructs in the semantics and needs to axiomatize validity as\npart of the logic. It is also not clear how to generalize their ideas to a\nfirst-order case. In this paper, we propose a new account of multi-agent\nonly-knowing which, for the first time, has a natural possible-world semantics\nfor a quantified language with equality. We then provide, for the propositional\nfragment, a sound and complete axiomatization that faithfully lifts Levesque's\nproof theory to the many agent case. We also discuss comparisons to the earlier\napproach by Halpern and Lakemeyer.\n", "versions": [{"version": "v1", "created": "Fri, 10 Sep 2010 16:10:26 GMT"}], "update_date": "2010-10-01", "authors_parsed": [["Belle", "Vaishak", ""], ["Lakemeyer", "Gerhard", ""]]}, {"id": "1009.2054", "submitter": "Bo Yang", "authors": "Bo Yang, Jiming Liu", "title": "Multiplex Structures: Patterns of Complexity in Real-World Networks", "comments": "48 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex network theory aims to model and analyze complex systems that consist\nof multiple and interdependent components. Among all studies on complex\nnetworks, topological structure analysis is of the most fundamental importance,\nas it represents a natural route to understand the dynamics, as well as to\nsynthesize or optimize the functions, of networks. A broad spectrum of network\nstructural patterns have been respectively reported in the past decade, such as\ncommunities, multipartites, hubs, authorities, outliers, bow ties, and others.\nHere, we show that most individual real-world networks demonstrate multiplex\nstructures. That is, a multitude of known or even unknown (hidden) patterns can\nsimultaneously situate in the same network, and moreover they may be overlapped\nand nested with each other to collaboratively form a heterogeneous, nested or\nhierarchical organization, in which different connective phenomena can be\nobserved at different granular levels. In addition, we show that the multiplex\nstructures hidden in exploratory networks can be well defined as well as\neffectively recognized within an unified framework consisting of a set of\nproposed concepts, models, and algorithms. Our findings provide a strong\nevidence that most real-world complex systems are driven by a combination of\nheterogeneous mechanisms that may collaboratively shape their ubiquitous\nmultiplex structures as we observe currently. This work also contributes a\nmathematical tool for analyzing different sources of networks from a new\nperspective of unveiling multiplex structures, which will be beneficial to\nmultiple disciplines including sociology, economics and computer science.\n", "versions": [{"version": "v1", "created": "Fri, 10 Sep 2010 17:07:56 GMT"}, {"version": "v2", "created": "Mon, 13 Sep 2010 09:12:27 GMT"}, {"version": "v3", "created": "Tue, 14 Sep 2010 01:44:54 GMT"}], "update_date": "2010-09-15", "authors_parsed": [["Yang", "Bo", ""], ["Liu", "Jiming", ""]]}, {"id": "1009.2084", "submitter": "Massimiliano Dal Mas", "authors": "Massimiliano Dal Mas", "title": "Ontology Temporal Evolution for Multi-Entity Bayesian Networks under\n  Exogenous and Endogenous Semantic Updating", "comments": "20 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is a challenge for any Knowledge Base reasoning to manage ubiquitous\nuncertain ontology as well as uncertain updating times, while achieving\nacceptable service levels at minimum computational cost. This paper proposes an\napplication-independent merging ontologies for any open interaction system. A\nsolution that uses Multi-Entity Bayesan Networks with SWRL rules, and a Java\nprogram is presented to dynamically monitor Exogenous and Endogenous temporal\nevolution on updating merging ontologies on a probabilistic framework for the\nSemantic Web.\n", "versions": [{"version": "v1", "created": "Fri, 10 Sep 2010 19:44:07 GMT"}], "update_date": "2010-09-13", "authors_parsed": [["Mas", "Massimiliano Dal", ""]]}, {"id": "1009.4586", "submitter": "S. M. Kamruzzaman", "authors": "Md. Hijbul Alam, Abdul Kadar Muhammad Masum, Mohammad Mahadi Hassan,\n  and S. M. Kamruzzaman", "title": "Optimal Bangla Keyboard Layout using Association Rule of Data Mining", "comments": "3 Pages, International Conference", "journal-ref": "Proc. 7th International Conference on Computer and Information\n  Technology (ICCIT 2004), Dhaka, Bangladesh, pp. 679-681, Dec. 2004", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an optimal Bangla Keyboard Layout, which distributes\nthe load equally on both hands so that maximizing the ease and minimizing the\neffort. Bangla alphabet has a large number of letters, for this it is difficult\nto type faster using Bangla keyboard. Our proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Here we use the\nassociation rule of data mining to distribute the Bangla characters in the\nkeyboard. First, we analyze the frequencies of data consisting of monograph,\ndigraph and trigraph, which are derived from data wire-house, and then used\nassociation rule of data mining to distribute the Bangla characters in the\nlayout. Finally, we propose a Bangla Keyboard Layout. Experimental results on\nseveral keyboard layout shows the effectiveness of the proposed approach with\nbetter performance.\n", "versions": [{"version": "v1", "created": "Thu, 23 Sep 2010 11:42:41 GMT"}], "update_date": "2010-09-27", "authors_parsed": [["Alam", "Md. Hijbul", ""], ["Masum", "Abdul Kadar Muhammad", ""], ["Hassan", "Mohammad Mahadi", ""], ["Kamruzzaman", "S. M.", ""]]}, {"id": "1009.4982", "submitter": "S. M. Kamruzzaman", "authors": "S. M. Kamruzzaman, Md. Hijbul Alam, Abdul Kadar Muhammad Masum, and\n  Md. Mahadi Hassan", "title": "Optimal Bangla Keyboard Layout using Data Mining Technique", "comments": "9 Pages, International Conference", "journal-ref": "Proc. International Conference on Information and Communication\n  Technology in Management (ICTM 2005), Multimedia University, Malaysia, May\n  2005", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an optimal Bangla Keyboard Layout, which distributes the\nload equally on both hands so that maximizing the ease and minimizing the\neffort. Bangla alphabet has a large number of letters, for this it is difficult\nto type faster using Bangla keyboard. Our proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Here we use the\nassociation rule of data mining to distribute the Bangla characters in the\nkeyboard. First, we analyze the frequencies of data consisting of monograph,\ndigraph and trigraph, which are derived from data wire-house, and then used\nassociation rule of data mining to distribute the Bangla characters in the\nlayout. Experimental results on several data show the effectiveness of the\nproposed approach with better performance.\n", "versions": [{"version": "v1", "created": "Sat, 25 Sep 2010 06:55:27 GMT"}], "update_date": "2010-09-28", "authors_parsed": [["Kamruzzaman", "S. M.", ""], ["Alam", "Md. Hijbul", ""], ["Masum", "Abdul Kadar Muhammad", ""], ["Hassan", "Md. Mahadi", ""]]}, {"id": "1009.5048", "submitter": "S. M. Kamruzzaman", "authors": "Abdul Kadar Muhammad Masum, Mohammad Mahadi Hassan, and S. M.\n  Kamruzzaman", "title": "The Most Advantageous Bangla Keyboard Layout Using Data Mining Technique", "comments": "10 Pages, International Journal", "journal-ref": "Journal of Computer Science, IBAIS University, Dkhaka, Bangladesh,\n  Vol. 1, No. 2, Dec. 2007", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bangla alphabet has a large number of letters, for this it is complicated to\ntype faster using Bangla keyboard. The proposed keyboard will maximize the\nspeed of operator as they can type with both hands parallel. Association rule\nof data mining to distribute the Bangla characters in the keyboard is used\nhere. The frequencies of data consisting of monograph, digraph and trigraph are\nanalyzed, which are derived from data wire-house, and then used association\nrule of data mining to distribute the Bangla characters in the layout.\nExperimental results on several data show the effectiveness of the proposed\napproach with better performance. This paper presents an optimal Bangla\nKeyboard Layout, which distributes the load equally on both hands so that\nmaximizing the ease and minimizing the effort.\n", "versions": [{"version": "v1", "created": "Sun, 26 Sep 2010 02:09:41 GMT"}], "update_date": "2010-09-28", "authors_parsed": [["Masum", "Abdul Kadar Muhammad", ""], ["Hassan", "Mohammad Mahadi", ""], ["Kamruzzaman", "S. M.", ""]]}, {"id": "1009.5268", "submitter": "Forrest Sheng Bao", "authors": "Xin Liu, Ying Ding, Forrest Sheng Bao", "title": "General Scaled Support Vector Machines", "comments": "5 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Support Vector Machines (SVMs) are popular tools for data mining tasks such\nas classification, regression, and density estimation. However, original SVM\n(C-SVM) only considers local information of data points on or over the margin.\nTherefore, C-SVM loses robustness. To solve this problem, one approach is to\ntranslate (i.e., to move without rotation or change of shape) the hyperplane\naccording to the distribution of the entire data. But existing work can only be\napplied for 1-D case. In this paper, we propose a simple and efficient method\ncalled General Scaled SVM (GS-SVM) to extend the existing approach to\nmulti-dimensional case. Our method translates the hyperplane according to the\ndistribution of data projected on the normal vector of the hyperplane. Compared\nwith C-SVM, GS-SVM has better performance on several data sets.\n", "versions": [{"version": "v1", "created": "Mon, 27 Sep 2010 14:27:49 GMT"}], "update_date": "2010-09-28", "authors_parsed": [["Liu", "Xin", ""], ["Ding", "Ying", ""], ["Bao", "Forrest Sheng", ""]]}, {"id": "1009.5290", "submitter": "Mladen Nikolic", "authors": "Mladen Nikolic", "title": "Measuring Similarity of Graphs and their Nodes by Neighbor Matching", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of measuring similarity of graphs and their nodes is important in\na range of practical problems. There is a number of proposed measures, some of\nthem being based on iterative calculation of similarity between two graphs and\nthe principle that two nodes are as similar as their neighbors are. In our\nwork, we propose one novel method of that sort, with a refined concept of\nsimilarity of two nodes that involves matching of their neighbors. We prove\nconvergence of the proposed method and show that it has some additional\ndesirable properties that, to our knowledge, the existing methods lack. We\nillustrate the method on two specific problems and empirically compare it to\nother methods.\n", "versions": [{"version": "v1", "created": "Mon, 27 Sep 2010 15:31:54 GMT"}], "update_date": "2010-09-28", "authors_parsed": [["Nikolic", "Mladen", ""]]}, {"id": "1009.5346", "submitter": "Murugesan Kuttikrishnan", "authors": "Murugesan Kuttikrishnan", "title": "A Novel Approach for Cardiac Disease Prediction and Classification Using\n  Intelligent Agents", "comments": "8 pages 2 figures and 7 tables", "journal-ref": "(IJCSIS) International Journal of Computer Science and Information\n  Security, Vol. 8, No. 5, August 2010", "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The goal is to develop a novel approach for cardiac disease prediction and\ndiagnosis using intelligent agents. Initially the symptoms are preprocessed\nusing filter and wrapper based agents. The filter removes the missing or\nirrelevant symptoms. Wrapper is used to extract the data in the data set\naccording to the threshold limits. Dependency of each symptom is identified\nusing dependency checker agent. The classification is based on the prior and\nposterior probability of the symptoms with the evidence value. Finally the\nsymptoms are classified in to five classes namely absence, starting, mild,\nmoderate and serious. Using the cooperative approach the cardiac problem is\nsolved and verified.\n", "versions": [{"version": "v1", "created": "Mon, 27 Sep 2010 18:20:56 GMT"}], "update_date": "2010-09-28", "authors_parsed": [["Kuttikrishnan", "Murugesan", ""]]}, {"id": "1009.5473", "submitter": "Paul Merolla", "authors": "Paul Merolla, Tristan Ursell, John Arthur", "title": "The thermodynamic temperature of a rhythmic spiking network", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial neural networks built from two-state neurons are powerful\ncomputational substrates, whose computational ability is well understood by\nanalogy with statistical mechanics. In this work, we introduce similar\nanalogies in the context of spiking neurons in a fixed time window, where\nexcitatory and inhibitory inputs drawn from a Poisson distribution play the\nrole of temperature. For single neurons with a \"bandgap\" between their inputs\nand the spike threshold, this temperature allows for stochastic spiking. By\nimposing a global inhibitory rhythm over the fixed time windows, we connect\nneurons into a network that exhibits synchronous, clock-like updating akin to\nneural networks. We implement a single-layer Boltzmann machine without learning\nto demonstrate our model.\n", "versions": [{"version": "v1", "created": "Tue, 28 Sep 2010 07:22:08 GMT"}], "update_date": "2010-09-29", "authors_parsed": [["Merolla", "Paul", ""], ["Ursell", "Tristan", ""], ["Arthur", "John", ""]]}, {"id": "1009.6119", "submitter": "Clifton Phua", "authors": "Clifton Phua, Vincent Lee, Kate Smith and Ross Gayler", "title": "A Comprehensive Survey of Data Mining-based Fraud Detection Research", "comments": "14 pages", "journal-ref": null, "doi": "10.1016/j.chb.2012.01.002", "report-no": null, "categories": "cs.AI cs.CE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This survey paper categorises, compares, and summarises from almost all\npublished technical and review articles in automated fraud detection within the\nlast 10 years. It defines the professional fraudster, formalises the main types\nand subtypes of known fraud, and presents the nature of data evidence collected\nwithin affected industries. Within the business context of mining the data to\nachieve higher cost savings, this research presents methods and techniques\ntogether with their problems. Compared to all related reviews on fraud\ndetection, this survey covers much more technical articles and is the only one,\nto the best of our knowledge, which proposes alternative data and solutions\nfrom related domains.\n", "versions": [{"version": "v1", "created": "Thu, 30 Sep 2010 13:06:12 GMT"}], "update_date": "2019-04-03", "authors_parsed": [["Phua", "Clifton", ""], ["Lee", "Vincent", ""], ["Smith", "Kate", ""], ["Gayler", "Ross", ""]]}, {"id": "1009.6127", "submitter": "Hong Jiang", "authors": "Hong Jiang", "title": "Efficient Knowledge Base Management in DCSP", "comments": "11 pages", "journal-ref": "International Journal of Ad hoc, Sensor & Ubiquitous Computing\n  (IJASUC) Vol.1, No.3, September 2010, 21-31", "doi": "10.5121/ijasuc.2010.1302", "report-no": null, "categories": "cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  DCSP (Distributed Constraint Satisfaction Problem) has been a very important\nresearch area in AI (Artificial Intelligence). There are many application\nproblems in distributed AI that can be formalized as DSCPs. With the increasing\ncomplexity and problem size of the application problems in AI, the required\nstorage place in searching and the average searching time are increasing too.\nThus, to use a limited storage place efficiently in solving DCSP becomes a very\nimportant problem, and it can help to reduce searching time as well. This paper\nprovides an efficient knowledge base management approach based on general usage\nof hyper-resolution-rule in consistence algorithm. The approach minimizes the\nincreasing of the knowledge base by eliminate sufficient constraint and false\nnogood. These eliminations do not change the completeness of the original\nknowledge base increased. The proofs are given as well. The example shows that\nthis approach decrease both the new nogoods generated and the knowledge base\ngreatly. Thus it decreases the required storage place and simplify the\nsearching process.\n", "versions": [{"version": "v1", "created": "Thu, 30 Sep 2010 13:45:55 GMT"}], "update_date": "2010-10-01", "authors_parsed": [["Jiang", "Hong", ""]]}]