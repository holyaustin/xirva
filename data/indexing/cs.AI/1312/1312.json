[{"id": "1312.0032", "submitter": "Gerardo Simari", "authors": "Thomas Lukasiewicz, Maria Vanina Martinez, Cristian Molinaro, Livia\n  Predoiu, and Gerardo I. Simari", "title": "Top-k Query Answering in Datalog+/- Ontologies under Subjective Reports\n  (Technical Report)", "comments": "arXiv admin note: text overlap with arXiv:1106.3767 by other authors", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of preferences in query answering, both in traditional databases and\nin ontology-based data access, has recently received much attention, due to its\nmany real-world applications. In this paper, we tackle the problem of top-k\nquery answering in Datalog+/- ontologies subject to the querying user's\npreferences and a collection of (subjective) reports of other users. Here, each\nreport consists of scores for a list of features, its author's preferences\namong the features, as well as other information. Theses pieces of information\nof every report are then combined, along with the querying user's preferences\nand his/her trust into each report, to rank the query results. We present two\nalternative such rankings, along with algorithms for top-k (atomic) query\nanswering under these rankings. We also show that, under suitable assumptions,\nthese algorithms run in polynomial time in the data complexity. We finally\npresent more general reports, which are associated with sets of atoms rather\nthan single atoms.\n", "versions": [{"version": "v1", "created": "Fri, 29 Nov 2013 22:06:09 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["Lukasiewicz", "Thomas", ""], ["Martinez", "Maria Vanina", ""], ["Molinaro", "Cristian", ""], ["Predoiu", "Livia", ""], ["Simari", "Gerardo I.", ""]]}, {"id": "1312.0049", "submitter": "Shehroz Khan", "authors": "Shehroz S.Khan, Michael G.Madden", "title": "One-Class Classification: Taxonomy of Study and Review of Techniques", "comments": "24 pages + 11 pages of references, 8 figures", "journal-ref": "The Knowledge Engineering Review, pp 1-30, 2014", "doi": "10.1017/S026988891300043X", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One-class classification (OCC) algorithms aim to build classification models\nwhen the negative class is either absent, poorly sampled or not well defined.\nThis unique situation constrains the learning of efficient classifiers by\ndefining class boundary just with the knowledge of positive class. The OCC\nproblem has been considered and applied under many research themes, such as\noutlier/novelty detection and concept learning. In this paper we present a\nunified view of the general problem of OCC by presenting a taxonomy of study\nfor OCC problems, which is based on the availability of training data,\nalgorithms used and the application domains applied. We further delve into each\nof the categories of the proposed taxonomy and present a comprehensive\nliterature review of the OCC algorithms, techniques and methodologies with a\nfocus on their significance, limitations and applications. We conclude our\npaper by discussing some open research problems in the field of OCC and present\nour vision for future research.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2013 01:52:36 GMT"}], "update_date": "2018-02-05", "authors_parsed": [["Khan", "Shehroz S.", ""], ["Madden", "Michael G.", ""]]}, {"id": "1312.0127", "submitter": "Kim Bauters", "authors": "Kim Bauters, Steven Schockaert, Martine De Cock, Dirk Vermeir", "title": "Characterizing and Extending Answer Set Semantics using Possibility\n  Theory", "comments": "39 pages and 16 pages appendix with proofs. This article has been\n  accepted for publication in Theory and Practice of Logic Programming,\n  Copyright Cambridge University Press", "journal-ref": "Theory and Practice of Logic Programming 15 (2015) 79-116", "doi": "10.1017/S147106841300063X", "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answer Set Programming (ASP) is a popular framework for modeling\ncombinatorial problems. However, ASP cannot easily be used for reasoning about\nuncertain information. Possibilistic ASP (PASP) is an extension of ASP that\ncombines possibilistic logic and ASP. In PASP a weight is associated with each\nrule, where this weight is interpreted as the certainty with which the\nconclusion can be established when the body is known to hold. As such, it\nallows us to model and reason about uncertain information in an intuitive way.\nIn this paper we present new semantics for PASP, in which rules are interpreted\nas constraints on possibility distributions. Special models of these\nconstraints are then identified as possibilistic answer sets. In addition,\nsince ASP is a special case of PASP in which all the rules are entirely\ncertain, we obtain a new characterization of ASP in terms of constraints on\npossibility distributions. This allows us to uncover a new form of disjunction,\ncalled weak disjunction, that has not been previously considered in the\nliterature. In addition to introducing and motivating the semantics of weak\ndisjunction, we also pinpoint its computational complexity. In particular,\nwhile the complexity of most reasoning tasks coincides with standard\ndisjunctive ASP, we find that brave reasoning for programs with weak\ndisjunctions is easier.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2013 17:12:48 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Bauters", "Kim", ""], ["Schockaert", "Steven", ""], ["De Cock", "Martine", ""], ["Vermeir", "Dirk", ""]]}, {"id": "1312.0144", "submitter": "Yanjing Wang", "authors": "Jie Fan, Yanjing Wang, Hans van Ditmarsch", "title": "Knowing Whether", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowing whether a proposition is true means knowing that it is true or\nknowing that it is false. In this paper, we study logics with a modal operator\nKw for knowing whether but without a modal operator K for knowing that. This\nlogic is not a normal modal logic, because we do not have Kw (phi -> psi) ->\n(Kw phi -> Kw psi). Knowing whether logic cannot define many common frame\nproperties, and its expressive power less than that of basic modal logic over\nclasses of models without reflexivity. These features make axiomatizing knowing\nwhether logics non-trivial. We axiomatize knowing whether logic over various\nframe classes. We also present an extension of knowing whether logic with\npublic announcement operators and we give corresponding reduction axioms for\nthat. We compare our work in detail to two recent similar proposals.\n", "versions": [{"version": "v1", "created": "Sat, 30 Nov 2013 19:18:49 GMT"}, {"version": "v2", "created": "Wed, 11 Dec 2013 13:19:33 GMT"}, {"version": "v3", "created": "Thu, 12 Dec 2013 10:02:26 GMT"}], "update_date": "2013-12-13", "authors_parsed": [["Fan", "Jie", ""], ["Wang", "Yanjing", ""], ["van Ditmarsch", "Hans", ""]]}, {"id": "1312.0200", "submitter": "S\\'ebastien Bardin", "authors": "S\\'ebastien Bardin and Arnaud Gotlieb", "title": "A Combined Approach for Constraints over Finite Domains and Arrays", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Arrays are ubiquitous in the context of software verification. However,\neffective reasoning over arrays is still rare in CP, as local reasoning is\ndramatically ill-conditioned for constraints over arrays. In this paper, we\npropose an approach combining both global symbolic reasoning and local\nconsistency filtering in order to solve constraint systems involving arrays\n(with accesses, updates and size constraints) and finite-domain constraints\nover their elements and indexes. Our approach, named FDCC, is based on a\ncombination of a congruence closure algorithm for the standard theory of arrays\nand a CP solver over finite domains. The tricky part of the work lies in the\nbi-directional communication mechanism between both solvers. We identify the\nsignificant information to share, and design ways to master the communication\noverhead. Experiments on random instances show that FDCC solves more formulas\nthan any portfolio combination of the two solvers taken in isolation, while\noverhead is kept reasonable.\n", "versions": [{"version": "v1", "created": "Sun, 1 Dec 2013 11:06:10 GMT"}], "update_date": "2013-12-03", "authors_parsed": [["Bardin", "S\u00e9bastien", ""], ["Gotlieb", "Arnaud", ""]]}, {"id": "1312.0735", "submitter": "Jean-Baptiste Lamy", "authors": "Jean-Baptiste Lamy (LIM\\&BIO), Anis Ellini (LIM\\&BIO), Vahid\n  Ebrahiminia, Jean-Daniel Zucker (UMMISCO), Hector Falcoff (SFTG), Alain Venot\n  (LIM\\&BIO)", "title": "Use of the C4.5 machine learning algorithm to test a clinical\n  guideline-based decision support system", "comments": null, "journal-ref": "Studies in Health Technology and Informatics 136 (2008) 223-8", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Well-designed medical decision support system (DSS) have been shown to\nimprove health care quality. However, before they can be used in real clinical\nsituations, these systems must be extensively tested, to ensure that they\nconform to the clinical guidelines (CG) on which they are based. Existing\nmethods cannot be used for the systematic testing of all possible test cases.\nWe describe here a new exhaustive dynamic verification method. In this method,\nthe DSS is considered to be a black box, and the Quinlan C4.5 algorithm is used\nto build a decision tree from an exhaustive set of DSS input vectors and\noutputs. This method was successfully used for the testing of a medical DSS\nrelating to chronic diseases: the ASTI critiquing module for type 2 diabetes.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2013 08:59:02 GMT"}], "update_date": "2013-12-04", "authors_parsed": [["Lamy", "Jean-Baptiste", "", "LIM\\&BIO"], ["Ellini", "Anis", "", "LIM\\&BIO"], ["Ebrahiminia", "Vahid", "", "UMMISCO"], ["Zucker", "Jean-Daniel", "", "UMMISCO"], ["Falcoff", "Hector", "", "SFTG"], ["Venot", "Alain", "", "LIM\\&BIO"]]}, {"id": "1312.0736", "submitter": "Jean-Baptiste Lamy", "authors": "Jean-Baptiste Lamy (LIM\\&BIO), Vahid Ebrahiminia, Brigitte Seroussi\n  (LIM\\&BIO), Jacques Bouaud, Christian Simon (LIM\\&BIO), Madeleine Favre\n  (SFTG), Hector Falcoff (SFTG), Alain Venot (LIM\\&BIO)", "title": "A generic system for critiquing physicians' prescriptions: usability,\n  satisfaction and lessons learnt", "comments": null, "journal-ref": "Studies in Health Technology and Informatics 169 (2011) 125-9", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Clinical decision support systems have been developed to help physicians to\ntake clinical guidelines into account during consultations. The ASTI critiquing\nmodule is one such systems; it provides the physician with automatic criticisms\nwhen a drug prescription does not follow the guidelines. It was initially\ndeveloped for hypertension and type 2 diabetes, but is designed to be generic\nenough for application to all chronic diseases. We present here the results of\nusability and satisfaction evaluations for the ASTI critiquing module, obtained\nwith GPs for a newly implemented guideline concerning dyslipaemia, and we\ndiscuss the lessons learnt and the difficulties encountered when building a\ngeneric DSS for critiquing physicians' prescriptions.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2013 08:59:47 GMT"}], "update_date": "2013-12-04", "authors_parsed": [["Lamy", "Jean-Baptiste", "", "LIM\\&BIO"], ["Ebrahiminia", "Vahid", "", "LIM\\&BIO"], ["Seroussi", "Brigitte", "", "LIM\\&BIO"], ["Bouaud", "Jacques", "", "LIM\\&BIO"], ["Simon", "Christian", "", "LIM\\&BIO"], ["Favre", "Madeleine", "", "SFTG"], ["Falcoff", "Hector", "", "SFTG"], ["Venot", "Alain", "", "LIM\\&BIO"]]}, {"id": "1312.0750", "submitter": "Jean-Baptiste Lamy", "authors": "Jean-Baptiste Lamy (LIM\\&BIO), Rosy Tsopra (LIM\\&BIO), Alain Venot\n  (LIM\\&BIO), Catherine Duclos (LIM\\&BIO)", "title": "A semi-automatic semantic method for mapping SNOMED CT concepts to VCM\n  Icons", "comments": null, "journal-ref": "Studies in Health Technology and Informatics 192 (2013) 42-6", "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  VCM (Visualization of Concept in Medicine) is an iconic language for\nrepresenting key medical concepts by icons. However, the use of this language\nwith reference terminologies, such as SNOMED CT, will require the mapping of\nits icons to the terms of these terminologies. Here, we present and evaluate a\nsemi-automatic semantic method for the mapping of SNOMED CT concepts to VCM\nicons. Both SNOMED CT and VCM are compositional in nature; SNOMED CT is\nexpressed in description logic and VCM semantics are formalized in an OWL\nontology. The proposed method involves the manual mapping of a limited number\nof underlying concepts from the VCM ontology, followed by automatic generation\nof the rest of the mapping. We applied this method to the clinical findings of\nthe SNOMED CT CORE subset, and 100 randomly-selected mappings were evaluated by\nthree experts. The results obtained were promising, with 82 of the SNOMED CT\nconcepts correctly linked to VCM icons according to the experts. Most of the\nerrors were easy to fix.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2013 09:33:45 GMT"}], "update_date": "2013-12-04", "authors_parsed": [["Lamy", "Jean-Baptiste", "", "LIM\\&BIO"], ["Tsopra", "Rosy", "", "LIM\\&BIO"], ["Venot", "Alain", "", "LIM\\&BIO"], ["Duclos", "Catherine", "", "LIM\\&BIO"]]}, {"id": "1312.0790", "submitter": "Sneha Chaudhari", "authors": "Sneha Chaudhari, Pankaj Dayama, Vinayaka Pandit, Indrajit Bhattacharya", "title": "Test Set Selection using Active Information Acquisition for Predictive\n  Models", "comments": "The paper has been withdrawn by the authors. The current version is\n  incomplete and the work is still on going. The algorithm gives poor results\n  for a particular setting and we are working on it. However, we are not\n  planning to submit a revision of the paper. This work is going to take some\n  time and we want to withdraw the current version since it is not in a good\n  shape and needs a lot more work to be in publishable condition", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we consider active information acquisition when the prediction\nmodel is meant to be applied on a targeted subset of the population. The goal\nis to label a pre-specified fraction of customers in the target or test set by\niteratively querying for information from the non-target or training set. The\nnumber of queries is limited by an overall budget. Arising in the context of\ntwo rather disparate applications- banking and medical diagnosis, we pose the\nactive information acquisition problem as a constrained optimization problem.\nWe propose two greedy iterative algorithms for solving the above problem. We\nconduct experiments with synthetic data and compare results of our proposed\nalgorithms with few other baseline approaches. The experimental results show\nthat our proposed approaches perform better than the baseline schemes.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2013 12:12:23 GMT"}, {"version": "v2", "created": "Fri, 14 Mar 2014 16:36:36 GMT"}], "update_date": "2014-03-17", "authors_parsed": [["Chaudhari", "Sneha", ""], ["Dayama", "Pankaj", ""], ["Pandit", "Vinayaka", ""], ["Bhattacharya", "Indrajit", ""]]}, {"id": "1312.0841", "submitter": "Ben Ruijl", "authors": "Ben Ruijl and Jos Vermaseren and Aske Plaat and Jaap van den Herik", "title": "Combining Simulated Annealing and Monte Carlo Tree Search for Expression\n  Simplification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many applications of computer algebra large expressions must be simplified\nto make repeated numerical evaluations tractable. Previous works presented\nheuristically guided improvements, e.g., for Horner schemes. The remaining\nexpression is then further reduced by common subexpression elimination. A\nrecent approach successfully applied a relatively new algorithm, Monte Carlo\nTree Search (MCTS) with UCT as the selection criterion, to find better variable\norderings. Yet, this approach is fit for further improvements since it is\nsensitive to the so-called exploration-exploitation constant $C_p$ and the\nnumber of tree updates $N$. In this paper we propose a new selection criterion\ncalled Simulated Annealing UCT (SA-UCT) that has a dynamic\nexploration-exploitation parameter, which decreases with the iteration number\n$i$ and thus reduces the importance of exploration over time. First, we provide\nan intuitive explanation in terms of the exploration-exploitation behavior of\nthe algorithm. Then, we test our algorithm on three large expressions of\ndifferent origins. We observe that SA-UCT widens the interval of good initial\nvalues $C_p$ where best results are achieved. The improvement is large (more\nthan a tenfold) and facilitates the selection of an appropriate $C_p$.\n", "versions": [{"version": "v1", "created": "Tue, 3 Dec 2013 14:56:28 GMT"}], "update_date": "2013-12-04", "authors_parsed": [["Ruijl", "Ben", ""], ["Vermaseren", "Jos", ""], ["Plaat", "Aske", ""], ["Herik", "Jaap van den", ""]]}, {"id": "1312.1003", "submitter": "Upul Senanayake Mr", "authors": "Upul Senanayake, Rahal Prabuddha, Roshan Ragel", "title": "High Throughput Virtual Screening with Data Level Parallelism in\n  Multi-core Processors", "comments": "Information and Automation for Sustainability (ICIAfS), 2012 IEEE 6th\n  International Conference on", "journal-ref": null, "doi": "10.1109/ICIAFS.2012.6419885", "report-no": null, "categories": "cs.AI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Improving the throughput of molecular docking, a computationally intensive\nphase of the virtual screening process, is a highly sought area of research\nsince it has a significant weight in the drug designing process. With such\nimprovements, the world might find cures for incurable diseases like HIV\ndisease and Cancer sooner. Our approach presented in this paper is to utilize a\nmulti-core environment to introduce Data Level Parallelism (DLP) to the\nAutodock Vina software, which is a widely used for molecular docking software.\nAutodock Vina already exploits Instruction Level Parallelism (ILP) in\nmulti-core environments and therefore optimized for such environments. However,\nwith the results we have obtained, it can be clearly seen that our approach has\nenhanced the throughput of the already optimized software by more than six\ntimes. This will dramatically reduce the time consumed for the lead\nidentification phase in drug designing along with the shift in the processor\ntechnology from multi-core to many-core of the current era. Therefore, we\nbelieve that the contribution of this project will effectively make it possible\nto expand the number of small molecules docked against a drug target and\nimproving the chances to design drugs for incurable diseases.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2013 01:53:33 GMT"}], "update_date": "2013-12-05", "authors_parsed": [["Senanayake", "Upul", ""], ["Prabuddha", "Rahal", ""], ["Ragel", "Roshan", ""]]}, {"id": "1312.1146", "submitter": "Anna Roub\\'i\\v{c}kov\\'a", "authors": "Anna Roub\\'i\\v{c}kov\\'a and Ivan Serina", "title": "Case-Based Merging Techniques in OAKPLAN", "comments": "preliminary version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Case-based planning can take advantage of former problem-solving experiences\nby storing in a plan library previously generated plans that can be reused to\nsolve similar planning problems in the future. Although comparative worst-case\ncomplexity analyses of plan generation and reuse techniques reveal that it is\nnot possible to achieve provable efficiency gain of reuse over generation, we\nshow that the case-based planning approach can be an effective alternative to\nplan generation when similar reuse candidates can be chosen.\n", "versions": [{"version": "v1", "created": "Wed, 4 Dec 2013 13:10:45 GMT"}], "update_date": "2013-12-05", "authors_parsed": [["Roub\u00ed\u010dkov\u00e1", "Anna", ""], ["Serina", "Ivan", ""]]}, {"id": "1312.1423", "submitter": "Muhammad Marwan  Muhammad Fuad", "authors": "Muhammad Marwan Muhammad Fuad", "title": "ABC-SG: A New Artificial Bee Colony Algorithm-Based Distance of\n  Sequential Data Using Sigma Grams", "comments": "The Tenth Australasian Data Mining Conference - AusDM 2012, Sydney,\n  Australia, 5-7 December, 2012", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of similarity search is one of the main problems in computer\nscience. This problem has many applications in text-retrieval, web search,\ncomputational biology, bioinformatics and others. Similarity between two data\nobjects can be depicted using a similarity measure or a distance metric. There\nare numerous distance metrics in the literature, some are used for a particular\ndata type, and others are more general. In this paper we present a new distance\nmetric for sequential data which is based on the sum of n-grams. The novelty of\nour distance is that these n-grams are weighted using artificial bee colony; a\nrecent optimization algorithm based on the collective intelligence of a swarm\nof bees on their search for nectar. This algorithm has been used in optimizing\na large number of numerical problems. We validate the new distance\nexperimentally.\n", "versions": [{"version": "v1", "created": "Thu, 5 Dec 2013 03:19:51 GMT"}], "update_date": "2013-12-06", "authors_parsed": [["Fuad", "Muhammad Marwan Muhammad", ""]]}, {"id": "1312.1752", "submitter": "Muhammad Marwan Muhammad Fuad", "authors": "Muhammad Marwan Muhammad Fuad", "title": "Particle Swarm Optimization of Information-Content Weighting of Symbolic\n  Aggregate Approximation", "comments": "The 8th International Conference on Advanced Data Mining and\n  Applications (ADMA 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bio-inspired optimization algorithms have been gaining more popularity\nrecently. One of the most important of these algorithms is particle swarm\noptimization (PSO). PSO is based on the collective intelligence of a swam of\nparticles. Each particle explores a part of the search space looking for the\noptimal position and adjusts its position according to two factors; the first\nis its own experience and the second is the collective experience of the whole\nswarm. PSO has been successfully used to solve many optimization problems. In\nthis work we use PSO to improve the performance of a well-known representation\nmethod of time series data which is the symbolic aggregate approximation (SAX).\nAs with other time series representation methods, SAX results in loss of\ninformation when applied to represent time series. In this paper we use PSO to\npropose a new minimum distance WMD for SAX to remedy this problem. Unlike the\noriginal minimum distance, the new distance sets different weights to different\nsegments of the time series according to their information content. This\nweighted minimum distance enhances the performance of SAX as we show through\nexperiments using different time series datasets.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2013 02:22:59 GMT"}], "update_date": "2013-12-09", "authors_parsed": [["Fuad", "Muhammad Marwan Muhammad", ""]]}, {"id": "1312.1760", "submitter": "Muhammad Marwan Muhammad Fuad", "authors": "Muhammad Marwan Muhammad Fuad", "title": "Towards Normalizing the Edit Distance Using a Genetic Algorithms Based\n  Scheme", "comments": "The 8th International Conference on Advanced Data Mining and\n  Applications (ADMA 2012)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The normalized edit distance is one of the distances derived from the edit\ndistance. It is useful in some applications because it takes into account the\nlengths of the two strings compared. The normalized edit distance is not\ndefined in terms of edit operations but rather in terms of the edit path. In\nthis paper we propose a new derivative of the edit distance that also takes\ninto consideration the lengths of the two strings, but the new distance is\nrelated directly to the edit distance. The particularity of the new distance is\nthat it uses the genetic algorithms to set the values of the parameters it\nuses. We conduct experiments to test the new distance and we obtain promising\nresults.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2013 02:50:42 GMT"}], "update_date": "2013-12-09", "authors_parsed": [["Fuad", "Muhammad Marwan Muhammad", ""]]}, {"id": "1312.1887", "submitter": "Julio Lemos", "authors": "Julio Lemos", "title": "Constraints on the search space of argumentation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drawing from research on computational models of argumentation (particularly\nthe Carneades Argumentation System), we explore the graphical representation of\narguments in a dispute; then, comparing two different traditions on the limits\nof the justification of decisions, and devising an intermediate, semi-formal,\nmodel, we also show that it can shed light on the theory of dispute resolution.\n  We conclude our paper with an observation on the usefulness of highly\nconstrained reasoning for Online Dispute Resolution systems. Restricting the\nsearch space of arguments exclusively to reasons proposed by the parties\n(vetoing the introduction of new arguments by the human or artificial\narbitrator) is the only way to introduce some kind of decidability -- together\nwith foreseeability -- in the argumentation system.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2013 15:21:10 GMT"}], "update_date": "2013-12-09", "authors_parsed": [["Lemos", "Julio", ""]]}, {"id": "1312.1971", "submitter": "Sarwat Nizamani", "authors": "Sarwat Nizamani, Nasrullah Memon, Uffe Kock Wiil, Panagiotis\n  Karampelas", "title": "Modeling Suspicious Email Detection using Enhanced Feature Selection", "comments": null, "journal-ref": "IJMO 2012 Vol.2(4): 371-377 ISSN: 2010-3697", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  The paper presents a suspicious email detection model which incorporates\nenhanced feature selection. In the paper we proposed the use of feature\nselection strategies along with classification technique for terrorists email\ndetection. The presented model focuses on the evaluation of machine learning\nalgorithms such as decision tree (ID3), logistic regression, Na\\\"ive Bayes\n(NB), and Support Vector Machine (SVM) for detecting emails containing\nsuspicious content. In the literature, various algorithms achieved good\naccuracy for the desired task. However, the results achieved by those\nalgorithms can be further improved by using appropriate feature selection\nmechanisms. We have identified the use of a specific feature selection scheme\nthat improves the performance of the existing algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 6 Dec 2013 19:25:33 GMT"}], "update_date": "2013-12-09", "authors_parsed": [["Nizamani", "Sarwat", ""], ["Memon", "Nasrullah", ""], ["Wiil", "Uffe Kock", ""], ["Karampelas", "Panagiotis", ""]]}, {"id": "1312.2062", "submitter": "Debajit Sensarma", "authors": "Debajit Sensarma and Koushik Majumder", "title": "A Novel Hierarchical Ant based QoS aware Intelligent Routing Scheme for\n  MANETS", "comments": "15 pages, 7 figures. arXiv admin note: text overlap with\n  arXiv:1308.2762", "journal-ref": "International Journal of Computer Networks & Communications\n  (IJCNC) Vol.5, No.6, November 2013", "doi": "10.5121/ijcnc.2013.5614", "report-no": null, "categories": "cs.NI cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MANET is a collection of mobile devices with no centralized control and no\npre-existing infrastructures. Due to the nodal mobility, supporting QoS during\nrouting in this type of networks is a very challenging task. To tackle this\ntype of overhead many routing algorithms with clustering approach have been\nproposed. Clustering is an effective method for resource management regarding\nnetwork performance, routing protocol design, QoS etc. Most of the flat network\narchitecture contains homogeneous capacity of nodes but in real time nodes are\nwith heterogeneous capacity and transmission power. Hierarchical routing\nprovides routing through this kind of heterogeneous nodes. Here, routes can be\nrecorded hierarchically, across clusters to increase routing flexibility.\nBesides this, it increases scalability and robustness of routes. In this paper,\na novel ant based QoS aware routing is proposed on a three level hierarchical\ncluster based topology in MANET which will be more scalable and efficient\ncompared to flat architecture and will give better throughput.\n", "versions": [{"version": "v1", "created": "Sat, 7 Dec 2013 05:56:18 GMT"}], "update_date": "2013-12-10", "authors_parsed": [["Sensarma", "Debajit", ""], ["Majumder", "Koushik", ""]]}, {"id": "1312.2242", "submitter": "Nikolaos Mavridis", "authors": "N. Mavridis, S. Konstantopoulos, I. Vetsikas, I. Heldal, P.\n  Karampiperis, G. Mathiason, S. Thill, K. Stathis, V. Karkaletsis", "title": "CLIC: A Framework for Distributed, On-Demand, Human-Machine Cognitive\n  Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional Artificial Cognitive Systems (for example, intelligent robots)\nshare a number of limitations. First, they are usually made up only of machine\ncomponents; humans are only playing the role of user or supervisor. And yet,\nthere are tasks in which the current state of the art of AI has much worse\nperformance or is more expensive than humans: thus, it would be highly\nbeneficial to have a systematic way of creating systems with both human and\nmachine components, possibly with remote non-expert humans providing\nshort-duration real-time services. Second, their components are often dedicated\nto only one system, and underutilized for a big part of their lifetime. Third,\nthere is no inherent support for robust operation, and if a new better\ncomponent becomes available, one cannot easily replace the old component.\nFourth, they are viewed as a resource to be developed and owned, not as a\nutility. Thus, we are presenting CLIC: a framework for constructing cognitive\nsystems that overcome the above limitations. The architecture of CLIC provides\nspecific mechanisms for creating and operating cognitive systems that fulfill a\nset of desiderata: First, that are distributed yet situated, interacting with\nthe physical world though sensing and actuation services, and that are also\ncombining human as well as machine services. Second, that are made up of\ncomponents that are time-shared and re-usable. Third, that provide increased\nrobustness through self-repair. Fourth, that are constructed and reconstructed\non the fly, with components that dynamically enter and exit the system during\noperation, on the basis of availability, pricing, and need. Importantly, fifth,\nthe cognitive systems created and operated by CLIC do not need to be owned and\ncan be provided on demand, as a utility; thus transforming human-machine\nsituated intelligence to a service, and opening up many interesting\nopportunities.\n", "versions": [{"version": "v1", "created": "Sun, 8 Dec 2013 18:53:58 GMT"}], "update_date": "2013-12-10", "authors_parsed": [["Mavridis", "N.", ""], ["Konstantopoulos", "S.", ""], ["Vetsikas", "I.", ""], ["Heldal", "I.", ""], ["Karampiperis", "P.", ""], ["Mathiason", "G.", ""], ["Thill", "S.", ""], ["Stathis", "K.", ""], ["Karkaletsis", "V.", ""]]}, {"id": "1312.2506", "submitter": "Daniela Inclezan", "authors": "Daniela Inclezan", "title": "An Application of Answer Set Programming to the Field of Second Language\n  Acquisition", "comments": "17 pages, 3 tables, to appear in Theory and Practice of Logic\n  Programming (TPLP)", "journal-ref": "Theory and Practice of Logic Programming 15 (2015) 1-17", "doi": "10.1017/S1471068413000653", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper explores the contributions of Answer Set Programming (ASP) to the\nstudy of an established theory from the field of Second Language Acquisition:\nInput Processing. The theory describes default strategies that learners of a\nsecond language use in extracting meaning out of a text, based on their\nknowledge of the second language and their background knowledge about the\nworld. We formalized this theory in ASP, and as a result we were able to\ndetermine opportunities for refining its natural language description, as well\nas directions for future theory development. We applied our model to automating\nthe prediction of how learners of English would interpret sentences containing\nthe passive voice. We present a system, PIas, that uses these predictions to\nassist language instructors in designing teaching materials. To appear in\nTheory and Practice of Logic Programming (TPLP).\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2013 16:37:22 GMT"}], "update_date": "2020-02-19", "authors_parsed": [["Inclezan", "Daniela", ""]]}, {"id": "1312.2551", "submitter": "Dmitry Lesnik", "authors": "Dmitry Lesnik, Tobias Schaefer", "title": "A state vector algebra for algorithmic implementation of second-order\n  logic", "comments": "This paper has been withdrawn by the author due to numerous errors\n  found", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a mathematical framework for mapping second-order logic relations\nonto a simple state vector algebra. Using this algebra, basic theorems of set\ntheory can be proven in an algorithmic way, hence by an expert system. We\nillustrate the use of the algebra with simple examples and show that, in\nprinciple, all theorems of basic set theory can be recovered in an elementary\nway. The developed technique can be used for an automated theorem proving in\nthe 1st and 2nd order logic.\n", "versions": [{"version": "v1", "created": "Mon, 9 Dec 2013 19:25:18 GMT"}, {"version": "v2", "created": "Tue, 17 Nov 2015 17:43:49 GMT"}, {"version": "v3", "created": "Wed, 18 Nov 2015 01:34:11 GMT"}], "update_date": "2015-11-19", "authors_parsed": [["Lesnik", "Dmitry", ""], ["Schaefer", "Tobias", ""]]}, {"id": "1312.2709", "submitter": "Anugrah Kumar", "authors": "Anugrah Kumar, Sanjiban Shekar Roy, Sarvesh SS Rawat, Sanklan Saxena", "title": "Phishing Detection by determining reliability factor using rough set\n  theory", "comments": "The International Conference on Machine Intelligence Research and\n  Advancement, ICMIRA-2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Phishing is a common online weapon, used against users, by Phishers for\nacquiring a confidential information through deception. Since the inception of\ninternet, nearly everything, ranging from money transaction to sharing\ninformation, is done online in most parts of the world. This has also given\nrise to malicious activities such as Phishing. Detecting Phishing is an\nintricate process due to complexity, ambiguity and copious amount of\npossibilities of factors responsible for phishing . Rough sets can be a\npowerful tool, when working on such kind of Applications containing vague or\nimprecise data. This paper proposes an approach towards Phishing Detection\nUsing Rough Set Theory. The Thirteen basic factors, directly responsible\ntowards Phishing, are grouped into four Strata. Reliability Factor is\ndetermined on the basis of the outcome of these strata, using Rough Set Theory\n. Reliability Factor determines the possibility of a suspected site to be Valid\nor Fake. Using Rough set Theory most and the least influential factors towards\nPhishing are also determined.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2013 08:10:38 GMT"}], "update_date": "2013-12-11", "authors_parsed": [["Kumar", "Anugrah", ""], ["Roy", "Sanjiban Shekar", ""], ["Rawat", "Sarvesh SS", ""], ["Saxena", "Sanklan", ""]]}, {"id": "1312.2710", "submitter": "Anugrah Kumar", "authors": "Sarvesh SS Rawat, Dheeraj Dilip Mor, Anugrah Kumar, Sanjiban Shekar\n  Roy, Rohit kumar", "title": "Improving circuit miniaturization and its efficiency using Rough Set\n  Theory", "comments": "The International Conference on Machine Intelligence Research and\n  Advancement,ICMIRA-2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  High-speed, accuracy, meticulousness and quick response are notion of the\nvital necessities for modern digital world. An efficient electronic circuit\nunswervingly affects the maneuver of the whole system. Different tools are\nrequired to unravel different types of engineering tribulations. Improving the\nefficiency, accuracy and low power consumption in an electronic circuit is\nalways been a bottle neck problem. So the need of circuit miniaturization is\nalways there. It saves a lot of time and power that is wasted in switching of\ngates, the wiring-crises is reduced, cross-sectional area of chip is reduced,\nthe number of transistors that can implemented in chip is multiplied many\nfolds. Therefore to trounce with this problem we have proposed an Artificial\nintelligence (AI) based approach that make use of Rough Set Theory for its\nimplementation. Theory of rough set has been proposed by Z Pawlak in the year\n1982. Rough set theory is a new mathematical tool which deals with uncertainty\nand vagueness. Decisions can be generated using rough set theory by reducing\nthe unwanted and superfluous data. We have condensed the number of gates\nwithout upsetting the productivity of the given circuit. This paper proposes an\napproach with the help of rough set theory which basically lessens the number\nof gates in the circuit, based on decision rules.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2013 08:11:14 GMT"}], "update_date": "2013-12-11", "authors_parsed": [["Rawat", "Sarvesh SS", ""], ["Mor", "Dheeraj Dilip", ""], ["Kumar", "Anugrah", ""], ["Roy", "Sanjiban Shekar", ""], ["kumar", "Rohit", ""]]}, {"id": "1312.2798", "submitter": "Fennie Liang", "authors": "Shao Fen Liang, Donia Scott, Robert Stevens and Alan Rector", "title": "OntoVerbal: a Generic Tool and Practical Application to SNOMED CT", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ontology development is a non-trivial task requiring expertise in the chosen\nontological language. We propose a method for making the content of ontologies\nmore transparent by presenting, through the use of natural language generation,\nnaturalistic descriptions of ontology classes as textual paragraphs. The method\nhas been implemented in a proof-of- concept system, OntoVerbal, that\nautomatically generates paragraph-sized textual descriptions of ontological\nclasses expressed in OWL. OntoVerbal has been applied to ontologies that can be\nloaded into Prot\\'eg\\'e and been evaluated with SNOMED CT, showing that it\nprovides coherent, well-structured and accurate textual descriptions of\nontology classes.\n", "versions": [{"version": "v1", "created": "Tue, 10 Dec 2013 13:55:30 GMT"}], "update_date": "2013-12-11", "authors_parsed": [["Liang", "Shao Fen", ""], ["Scott", "Donia", ""], ["Stevens", "Robert", ""], ["Rector", "Alan", ""]]}, {"id": "1312.3020", "submitter": "Huasha Zhao Mr", "authors": "Huasha Zhao, John Canny", "title": "Sparse Allreduce: Efficient Scalable Communication for Power-Law Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DC cs.AI cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many large datasets exhibit power-law statistics: The web graph, social\nnetworks, text data, click through data etc. Their adjacency graphs are termed\nnatural graphs, and are known to be difficult to partition. As a consequence\nmost distributed algorithms on these graphs are communication intensive. Many\nalgorithms on natural graphs involve an Allreduce: a sum or average of\npartitioned data which is then shared back to the cluster nodes. Examples\ninclude PageRank, spectral partitioning, and many machine learning algorithms\nincluding regression, factor (topic) models, and clustering. In this paper we\ndescribe an efficient and scalable Allreduce primitive for power-law data. We\npoint out scaling problems with existing butterfly and round-robin networks for\nSparse Allreduce, and show that a hybrid approach improves on both.\nFurthermore, we show that Sparse Allreduce stages should be nested instead of\ncascaded (as in the dense case). And that the optimum throughput Allreduce\nnetwork should be a butterfly of heterogeneous degree where degree decreases\nwith depth into the network. Finally, a simple replication scheme is introduced\nto deal with node failures. We present experiments showing significant\nimprovements over existing systems such as PowerGraph and Hadoop.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2013 02:33:45 GMT"}], "update_date": "2013-12-12", "authors_parsed": [["Zhao", "Huasha", ""], ["Canny", "John", ""]]}, {"id": "1312.3060", "submitter": "Istiadi", "authors": "Istiadi, Emma Budi Sulistiarini", "title": "Representing Knowledge Base into Database for WAP and Web-based Expert\n  System", "comments": null, "journal-ref": "International Conference on Information Systems for Business\n  Competitiveness (ICISBC 2013), Semarang, Indonesia, December 5, 2013", "doi": null, "report-no": null, "categories": "cs.AI cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Expert System is developed as consulting service for users spread or public\nrequires affordable access. The Internet has become a medium for such services,\nbut presence of mobile devices make the access becomes more widespread by\nutilizing mobile web and WAP (Wireless Application Protocol). Applying expert\nsystems applications over the web and WAP requires a knowledge base\nrepresentation that can be accessed simultaneously. This paper proposes single\ndatabase to accommodate the knowledge representation with decision tree mapping\napproach. Because of the database exist, consulting application through both\nweb and WAP can access it to provide expert system services options for more\naffordable for public.\n", "versions": [{"version": "v1", "created": "Wed, 11 Dec 2013 07:49:00 GMT"}], "update_date": "2013-12-23", "authors_parsed": [["Istiadi", "", ""], ["Sulistiarini", "Emma Budi", ""]]}, {"id": "1312.3613", "submitter": "Jean-Baptiste  Tristan", "authors": "Jean-Baptiste Tristan, Daniel Huang, Joseph Tassarotti, Adam Pocock,\n  Stephen J. Green, Guy L. Steele Jr", "title": "Augur: a Modeling Language for Data-Parallel Probabilistic Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.DC cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is time-consuming and error-prone to implement inference procedures for\neach new probabilistic model. Probabilistic programming addresses this problem\nby allowing a user to specify the model and having a compiler automatically\ngenerate an inference procedure for it. For this approach to be practical, it\nis important to generate inference code that has reasonable performance. In\nthis paper, we present a probabilistic programming language and compiler for\nBayesian networks designed to make effective use of data-parallel architectures\nsuch as GPUs. Our language is fully integrated within the Scala programming\nlanguage and benefits from tools such as IDE support, type-checking, and code\ncompletion. We show that the compiler can generate data-parallel inference code\nscalable to thousands of GPU cores by making use of the conditional\nindependence relationships in the Bayesian network.\n", "versions": [{"version": "v1", "created": "Thu, 12 Dec 2013 20:23:20 GMT"}, {"version": "v2", "created": "Tue, 10 Jun 2014 19:53:09 GMT"}], "update_date": "2014-06-11", "authors_parsed": [["Tristan", "Jean-Baptiste", ""], ["Huang", "Daniel", ""], ["Tassarotti", "Joseph", ""], ["Pocock", "Adam", ""], ["Green", "Stephen J.", ""], ["Steele", "Guy L.", "Jr"]]}, {"id": "1312.3825", "submitter": "Claas Ahlrichs", "authors": "Claas Ahlrichs and Michael Lawo", "title": "Parkinson's Disease Motor Symptoms in Machine Learning: A Review", "comments": "Health Informatics: An International Journal (HIIJ), November 2013,\n  Volume 2, Number 4", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper reviews related work and state-of-the-art publications for\nrecognizing motor symptoms of Parkinson's Disease (PD). It presents research\nefforts that were undertaken to inform on how well traditional machine learning\nalgorithms can handle this task. In particular, four PD related motor symptoms\nare highlighted (i.e. tremor, bradykinesia, freezing of gait and dyskinesia)\nand their details summarized. Thus the primary objective of this research is to\nprovide a literary foundation for development and improvement of algorithms for\ndetecting PD related motor symptoms.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2013 14:58:39 GMT"}], "update_date": "2013-12-16", "authors_parsed": [["Ahlrichs", "Claas", ""], ["Lawo", "Michael", ""]]}, {"id": "1312.3903", "submitter": "Marlos C. Machado", "authors": "Marlos C. Machado", "title": "A Methodology for Player Modeling based on Machine Learning", "comments": "Thesis presented by Marlos C. Machado as part of the requirements for\n  the degree or Master of Science in Computer Science granted by the\n  Universidade Federal de Minas Gerais. February, 18th, 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  AI is gradually receiving more attention as a fundamental feature to increase\nthe immersion in digital games. Among the several AI approaches, player\nmodeling is becoming an important one. The main idea is to understand and model\nthe player characteristics and behaviors in order to develop a better AI. In\nthis work, we discuss several aspects of this new field. We proposed a taxonomy\nto organize the area, discussing several facets of this topic, ranging from\nimplementation decisions up to what a model attempts to describe. We then\nclassify, in our taxonomy, some of the most important works in this field. We\nalso presented a generic approach to deal with player modeling using ML, and we\ninstantiated this approach to model players' preferences in the game\nCivilization IV. The instantiation of this approach has several steps. We first\ndiscuss a generic representation, regardless of what is being modeled, and\nevaluate it performing experiments with the strategy game Civilization IV.\nContinuing the instantiation of the proposed approach we evaluated the\napplicability of using game score information to distinguish different\npreferences. We presented a characterization of virtual agents in the game,\ncomparing their behavior with their stated preferences. Once we have\ncharacterized these agents, we were able to observe that different preferences\ngenerate different behaviors, measured by several game indicators. We then\ntackled the preference modeling problem as a binary classification task, with a\nsupervised learning approach. We compared four different methods, based on\ndifferent paradigms (SVM, AdaBoost, NaiveBayes and JRip), evaluating them on a\nset of matches played by different virtual agents. We conclude our work using\nthe learned models to infer human players' preferences. Using some of the\nevaluated classifiers we obtained accuracies over 60% for most of the inferred\npreferences.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2013 18:32:51 GMT"}], "update_date": "2013-12-16", "authors_parsed": [["Machado", "Marlos C.", ""]]}, {"id": "1312.3971", "submitter": "Tommaso Urli", "authors": "Tommaso Urli", "title": "Balancing bike sharing systems (BBSS): instance generation from the\n  CitiBike NYC data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bike sharing systems are a very popular means to provide bikes to citizens in\na simple and cheap way. The idea is to install bike stations at various points\nin the city, from which a registered user can easily loan a bike by removing it\nfrom a specialized rack. After the ride, the user may return the bike at any\nstation (if there is a free rack). Services of this kind are mainly public or\nsemi-public, often aimed at increasing the attractiveness of non-motorized\nmeans of transportation, and are usually free, or almost free, of charge for\nthe users. Depending on their location, bike stations have specific patterns\nregarding when they are empty or full. For instance, in cities where most jobs\nare located near the city centre, the commuters cause certain peaks in the\nmorning: the central bike stations are filled, while the stations in the\noutskirts are emptied. Furthermore, stations located on top of a hill are more\nlikely to be empty, since users are less keen on cycling uphill to return the\nbike, and often leave their bike at a more reachable station. These issues\nresult in substantial user dissatisfaction which may eventually cause the users\nto abandon the service. This is why nowadays most bike sharing system providers\ntake measures to rebalance them. Over the last few years, balancing bike\nsharing systems (BBSS) has become increasingly studied in optimization. As\nsuch, generating meaningful instance to serve as a benchmark for the proposed\napproaches is an important task. In this technical report we describe the\nprocedure we used to generate BBSS problem instances from data of the CitiBike\nNYC bike sharing system.\n", "versions": [{"version": "v1", "created": "Fri, 13 Dec 2013 22:10:54 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["Urli", "Tommaso", ""]]}, {"id": "1312.4026", "submitter": "Piotr Skowron", "authors": "Piotr Skowron, Piotr Faliszewski, Arkadii Slinko", "title": "Achieving Fully Proportional Representation: Approximability Results", "comments": "arXiv admin note: substantial text overlap with arXiv:1208.1661,\n  arXiv:1301.6400", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the complexity of (approximate) winner determination under the\nMonroe and Chamberlin--Courant multiwinner voting rules, which determine the\nset of representatives by optimizing the total (dis)satisfaction of the voters\nwith their representatives. The total (dis)satisfaction is calculated either as\nthe sum of individual (dis)satisfactions (the utilitarian case) or as the\n(dis)satisfaction of the worst off voter (the egalitarian case). We provide\ngood approximation algorithms for the satisfaction-based utilitarian versions\nof the Monroe and Chamberlin--Courant rules, and inapproximability results for\nthe dissatisfaction-based utilitarian versions of them and also for all\negalitarian cases. Our algorithms are applicable and particularly appealing\nwhen voters submit truncated ballots. We provide experimental evaluation of the\nalgorithms both on real-life preference-aggregation data and on synthetic data.\nThese experiments show that our simple and fast algorithms can in many cases\nfind near-perfect solutions.\n", "versions": [{"version": "v1", "created": "Sat, 14 Dec 2013 10:50:57 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["Skowron", "Piotr", ""], ["Faliszewski", "Piotr", ""], ["Slinko", "Arkadii", ""]]}, {"id": "1312.4231", "submitter": "Aiping  Huang", "authors": "Aiping Huang and William Zhu", "title": "Dependence space of matroids and its application to attribute reduction", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Attribute reduction is a basic issue in knowledge representation and data\nmining. Rough sets provide a theoretical foundation for the issue. Matroids\ngeneralized from matrices have been widely used in many fields, particularly\ngreedy algorithm design, which plays an important role in attribute reduction.\nTherefore, it is meaningful to combine matroids with rough sets to solve the\noptimization problems. In this paper, we introduce an existing algebraic\nstructure called dependence space to study the reduction problem in terms of\nmatroids. First, a dependence space of matroids is constructed. Second, the\ncharacterizations for the space such as consistent sets and reducts are studied\nthrough matroids. Finally, we investigate matroids by the means of the space\nand present two expressions for their bases. In a word, this paper provides new\napproaches to study attribute reduction.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2013 02:27:15 GMT"}, {"version": "v2", "created": "Thu, 5 Mar 2015 02:45:33 GMT"}], "update_date": "2015-03-06", "authors_parsed": [["Huang", "Aiping", ""], ["Zhu", "William", ""]]}, {"id": "1312.4232", "submitter": "Aiping  Huang", "authors": "Aiping Huang and William Zhu", "title": "Geometric lattice structure of covering and its application to attribute\n  reduction through matroids", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The reduction of covering decision systems is an important problem in data\nmining, and covering-based rough sets serve as an efficient technique to\nprocess the problem. Geometric lattices have been widely used in many fields,\nespecially greedy algorithm design which plays an important role in the\nreduction problems. Therefore, it is meaningful to combine coverings with\ngeometric lattices to solve the optimization problems. In this paper, we obtain\ngeometric lattices from coverings through matroids and then apply them to the\nissue of attribute reduction. First, a geometric lattice structure of a\ncovering is constructed through transversal matroids. Then its atoms are\nstudied and used to describe the lattice. Second, considering that all the\nclosed sets of a finite matroid form a geometric lattice, we propose a\ndependence space through matroids and study the attribute reduction issues of\nthe space, which realizes the application of geometric lattices to attribute\nreduction. Furthermore, a special type of information system is taken as an\nexample to illustrate the application. In a word, this work points out an\ninteresting view, namely, geometric lattice to study the attribute reduction\nissues of information systems.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2013 02:30:07 GMT"}, {"version": "v2", "created": "Sat, 4 Jan 2014 11:55:51 GMT"}], "update_date": "2014-01-07", "authors_parsed": [["Huang", "Aiping", ""], ["Zhu", "William", ""]]}, {"id": "1312.4234", "submitter": "Aiping  Huang", "authors": "Aiping Huang and William Zhu", "title": "Connectedness of graphs and its application to connected matroids\n  through covering-based rough sets", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Graph theoretical ideas are highly utilized by computer science fields\nespecially data mining. In this field, a data structure can be designed in the\nform of tree. Covering is a widely used form of data representation in data\nmining and covering-based rough sets provide a systematic approach to this type\nof representation. In this paper, we study the connectedness of graphs through\ncovering-based rough sets and apply it to connected matroids. First, we present\nan approach to inducing a covering by a graph, and then study the connectedness\nof the graph from the viewpoint of the covering approximation operators.\nSecond, we construct a graph from a matroid, and find the matroid and the graph\nhave the same connectedness, which makes us to use covering-based rough sets to\nstudy connected matroids. In summary, this paper provides a new approach to\nstudying graph theory and matroid theory.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2013 02:32:43 GMT"}, {"version": "v2", "created": "Sat, 4 Jan 2014 12:00:52 GMT"}, {"version": "v3", "created": "Wed, 4 Mar 2015 08:23:03 GMT"}], "update_date": "2015-03-05", "authors_parsed": [["Huang", "Aiping", ""], ["Zhu", "William", ""]]}, {"id": "1312.4287", "submitter": "Guido Governatori", "authors": "Guido Governatori, Francesco Olivieri, Simone Scannapieco, Antonino\n  Rotolo, Matteo Cristani", "title": "Strategic Argumentation is NP-Complete", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we study the complexity of strategic argumentation for dialogue\ngames. A dialogue game is a 2-player game where the parties play arguments. We\nshow how to model dialogue games in a skeptical, non-monotonic formalism, and\nwe show that the problem of deciding what move (set of rules) to play at each\nturn is an NP-complete problem.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2013 10:09:06 GMT"}], "update_date": "2013-12-17", "authors_parsed": [["Governatori", "Guido", ""], ["Olivieri", "Francesco", ""], ["Scannapieco", "Simone", ""], ["Rotolo", "Antonino", ""], ["Cristani", "Matteo", ""]]}, {"id": "1312.4353", "submitter": "Tim Genewein", "authors": "Tim Genewein, Daniel A. Braun", "title": "Abstraction in decision-makers with limited information processing\n  capabilities", "comments": "Presented at the NIPS 2013 Workshop on Planning with Information\n  Constraints", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A distinctive property of human and animal intelligence is the ability to\nform abstractions by neglecting irrelevant information which allows to separate\nstructure from noise. From an information theoretic point of view abstractions\nare desirable because they allow for very efficient information processing. In\nartificial systems abstractions are often implemented through computationally\ncostly formations of groups or clusters. In this work we establish the relation\nbetween the free-energy framework for decision making and rate-distortion\ntheory and demonstrate how the application of rate-distortion for\ndecision-making leads to the emergence of abstractions. We argue that\nabstractions are induced due to a limit in information processing capacity.\n", "versions": [{"version": "v1", "created": "Mon, 16 Dec 2013 13:38:10 GMT"}, {"version": "v2", "created": "Thu, 19 Dec 2013 17:30:03 GMT"}], "update_date": "2013-12-20", "authors_parsed": [["Genewein", "Tim", ""], ["Braun", "Daniel A.", ""]]}, {"id": "1312.4637", "submitter": "Zhen Zhang", "authors": "Zhen Zhang, Qinfeng Shi, Yanning Zhang, Chunhua Shen, Anton van den\n  Hengel", "title": "Constraint Reduction using Marginal Polytope Diagrams for MAP LP\n  Relaxations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LP relaxation-based message passing algorithms provide an effective tool for\nMAP inference over Probabilistic Graphical Models. However, different LP\nrelaxations often have different objective functions and variables of differing\ndimensions, which presents a barrier to effective comparison and analysis. In\naddition, the computational complexity of LP relaxation-based methods grows\nquickly with the number of constraints. Reducing the number of constraints\nwithout sacrificing the quality of the solutions is thus desirable.\n  We propose a unified formulation under which existing MAP LP relaxations may\nbe compared and analysed. Furthermore, we propose a new tool called Marginal\nPolytope Diagrams. Some properties of Marginal Polytope Diagrams are exploited\nsuch as node redundancy and edge equivalence. We show that using Marginal\nPolytope Diagrams allows the number of constraints to be reduced without\nloosening the LP relaxations. Then, using Marginal Polytope Diagrams and\nconstraint reduction, we develop three novel message passing algorithms, and\ndemonstrate that two of these show a significant improvement in speed over\nstate-of-art algorithms while delivering a competitive, and sometimes higher,\nquality of solution.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2013 04:44:04 GMT"}, {"version": "v2", "created": "Mon, 21 Apr 2014 09:30:20 GMT"}], "update_date": "2014-04-22", "authors_parsed": [["Zhang", "Zhen", ""], ["Shi", "Qinfeng", ""], ["Zhang", "Yanning", ""], ["Shen", "Chunhua", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1312.4640", "submitter": "Sarajane Marques Peres Dr.", "authors": "Renata Cristina Barros Madeo, Priscilla Koch Wagner, Sarajane Marques\n  Peres", "title": "A Review of Temporal Aspects of Hand Gesture Analysis Applied to\n  Discourse Analysis and Natural Conversation", "comments": "20 pages, International Journal of Computer Science & Information\n  Technology (IJCSIT) Vol 5, No 4, August 2013", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lately, there has been an increasing interest in hand gesture analysis\nsystems. Recent works have employed pattern recognition techniques and have\nfocused on the development of systems with more natural user interfaces. These\nsystems may use gestures to control interfaces or recognize sign language\ngestures, which can provide systems with multimodal interaction; or consist in\nmultimodal tools to help psycholinguists to understand new aspects of discourse\nanalysis and to automate laborious tasks. Gestures are characterized by several\naspects, mainly by movements and sequence of postures. Since data referring to\nmovements or sequences carry temporal information, this paper presents a\nliterature review about temporal aspects of hand gesture analysis, focusing on\napplications related to natural conversation and psycholinguistic analysis,\nusing Systematic Literature Review methodology. In our results, we organized\nworks according to type of analysis, methods, highlighting the use of Machine\nLearning techniques, and applications.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2013 05:00:23 GMT"}], "update_date": "2013-12-18", "authors_parsed": [["Madeo", "Renata Cristina Barros", ""], ["Wagner", "Priscilla Koch", ""], ["Peres", "Sarajane Marques", ""]]}, {"id": "1312.4704", "submitter": "Alex Stolz", "authors": "Alex Stolz, Bene Rodriguez-Castro, Martin Hepp", "title": "RDF Translator: A RESTful Multi-Format Data Converter for the Semantic\n  Web", "comments": "Technical report, 15 pages", "journal-ref": null, "doi": null, "report-no": "TR-2013-1", "categories": "cs.DL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The interdisciplinary nature of the Semantic Web and the many projects put\nforward by the community led to a large number of widely accepted serialization\nformats for RDF. Most of these RDF syntaxes have been developed out of a\nnecessity to serve specific purposes better than existing ones, e.g. RDFa was\nproposed as an extension to HTML for embedding non-intrusive RDF statements in\nhuman-readable documents. Nonetheless, the RDF serialization formats are\ngenerally transducible among themselves given that they are commonly based on\nthe RDF model. In this paper, we present (1) a RESTful Web service based on the\nHTTP protocol that translates between different serializations. In addition to\nits core functionality, our proposed solution provides (2) features to\naccommodate frequent needs of Semantic Web developers, namely a straightforward\nuser interface with copy-to-clipboard functionality, syntax highlighting,\npersistent URI links for easy sharing, cool URI patterns, and content\nnegotiation using respective HTTP headers. We demonstrate the benefit of our\nconverter by presenting two use cases.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2013 10:14:15 GMT"}], "update_date": "2013-12-18", "authors_parsed": [["Stolz", "Alex", ""], ["Rodriguez-Castro", "Bene", ""], ["Hepp", "Martin", ""]]}, {"id": "1312.4794", "submitter": "Thabet Slimani", "authors": "Thabet Slimani", "title": "Semantic Annotation: The Mainstay of Semantic Web", "comments": "8 pages, 3 figures", "journal-ref": "International Journal of Computer Applications Technology and\n  Research, Volume 2, Issue 6, 763-770, 2013", "doi": "10.7753/IJCATR0206.1025", "report-no": null, "categories": "cs.DL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Given that semantic Web realization is based on the critical mass of metadata\naccessibility and the representation of data with formal knowledge, it needs to\ngenerate metadata that is specific, easy to understand and well-defined.\nHowever, semantic annotation of the web documents is the successful way to make\nthe Semantic Web vision a reality. This paper introduces the Semantic Web and\nits vision (stack layers) with regard to some concept definitions that helps\nthe understanding of semantic annotation. Additionally, this paper introduces\nthe semantic annotation categories, tools, domains and models.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2013 14:12:51 GMT"}], "update_date": "2013-12-18", "authors_parsed": [["Slimani", "Thabet", ""]]}, {"id": "1312.4814", "submitter": "Hugo Daniel Macedo", "authors": "Hugo Daniel Macedo (INRIA Paris-Rocquencourt), Tayssir Touili (LIAFA)", "title": "Mining Malware Specifications through Static Reachability Analysis", "comments": "Lecture notes in computer science (2013)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of malicious software (malware) is growing out of control.\nSyntactic signature based detection cannot cope with such growth and manual\nconstruction of malware signature databases needs to be replaced by computer\nlearning based approaches. Currently, a single modern signature capturing the\nsemantics of a malicious behavior can be used to replace an arbitrarily large\nnumber of old-fashioned syntactical signatures. However teaching computers to\nlearn such behaviors is a challenge. Existing work relies on dynamic analysis\nto extract malicious behaviors, but such technique does not guarantee the\ncoverage of all behaviors. To sidestep this limitation we show how to learn\nmalware signatures using static reachability analysis. The idea is to model\nbinary programs using pushdown systems (that can be used to model the stack\noperations occurring during the binary code execution), use reachability\nanalysis to extract behaviors in the form of trees, and use subtrees that are\ncommon among the trees extracted from a training set of malware files as\nsignatures. To detect malware we propose to use a tree automaton to compactly\nstore malicious behavior trees and check if any of the subtrees extracted from\nthe file under analysis is malicious. Experimental data shows that our approach\ncan be used to learn signatures from a training set of malware files and use\nthem to detect a test set of malware that is 5 times the size of the training\nset.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2013 15:08:39 GMT"}], "update_date": "2013-12-18", "authors_parsed": [["Macedo", "Hugo Daniel", "", "INRIA Paris-Rocquencourt"], ["Touili", "Tayssir", "", "LIAFA"]]}, {"id": "1312.4828", "submitter": "Federico Cerutti", "authors": "Federico Cerutti, Alice Toniolo, Nir Oren, Timothy J. Norman", "title": "Subjective Logic Operators in Trust Assessment: an Empirical Study", "comments": "Submitted to Information Systems Frontiers Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Computational trust mechanisms aim to produce trust ratings from both direct\nand indirect information about agents' behaviour. Subjective Logic (SL) has\nbeen widely adopted as the core of such systems via its fusion and discount\noperators. In recent research we revisited the semantics of these operators to\nexplore an alternative, geometric interpretation. In this paper we present a\nprincipled desiderata for discounting and fusion operators in SL. Building upon\nthis we present operators that satisfy these desirable properties, including a\nfamily of discount operators. We then show, through a rigorous empirical study,\nthat specific, geometrically interpreted operators significantly outperform\nstandard SL operators in estimating ground truth. These novel operators offer\nreal advantages for computational models of trust and reputation, in which they\nmay be employed without modifying other aspects of an existing system.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2013 09:22:21 GMT"}], "update_date": "2013-12-18", "authors_parsed": [["Cerutti", "Federico", ""], ["Toniolo", "Alice", ""], ["Oren", "Nir", ""], ["Norman", "Timothy J.", ""]]}, {"id": "1312.4839", "submitter": "Federico Cerutti", "authors": "Chatschik Bisdikian, Federico Cerutti, Yuqing Tang, Nir Oren", "title": "Reasoning about the Impacts of Information Sharing", "comments": "Submitted to Information Systems Frontiers Journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we describe a decision process framework allowing an agent to\ndecide what information it should reveal to its neighbours within a\ncommunication graph in order to maximise its utility. We assume that these\nneighbours can pass information onto others within the graph. The inferences\nmade by agents receiving the messages can have a positive or negative impact on\nthe information providing agent, and our decision process seeks to identify how\na message should be modified in order to be most beneficial to the information\nproducer. Our decision process is based on the provider's subjective beliefs\nabout others in the system, and therefore makes extensive use of the notion of\ntrust. Our core contributions are therefore the construction of a model of\ninformation propagation; the description of the agent's decision procedure; and\nan analysis of some of its properties.\n", "versions": [{"version": "v1", "created": "Tue, 19 Nov 2013 09:30:23 GMT"}], "update_date": "2013-12-18", "authors_parsed": [["Bisdikian", "Chatschik", ""], ["Cerutti", "Federico", ""], ["Tang", "Yuqing", ""], ["Oren", "Nir", ""]]}, {"id": "1312.4851", "submitter": "Serge Stinckwich", "authors": "Le Nguyen Tuan Thanh, Chihab Hanachi, Serge Stinckwich and Ho Tuong\n  Vinh", "title": "Representing, Simulating and Analysing Ho Chi Minh City Tsunami Plan by\n  Means of Process Models", "comments": "7 pages. ISCRAM (Information Systems for Crisis Response and\n  Management) Vietnam 2013 Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers the textual plan (guidelines) proposed by People's\nCommittee of Ho Chi Minh City (Vietnam) to manage earthquake and tsunami, and\ntry to represent it in a more formal way, in order to provide means to\nsimulate, analyse and adapt it. We first present a state of the art about\ncoordination models for disaster management with a focus on process oriented\napproaches. We give an overview of the different dimensions of the textual\ntsunami plan of Ho Chi Minh City and then the graphical representation of its\nprocess with BPMN (Business Process Model and Notation). We finally show how to\nexploit this process with workflow tools to simulate (YAWL tool) and analyse it\n(ProM tool).\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2013 16:29:52 GMT"}, {"version": "v2", "created": "Sat, 21 Dec 2013 13:10:04 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Thanh", "Le Nguyen Tuan", ""], ["Hanachi", "Chihab", ""], ["Stinckwich", "Serge", ""], ["Vinh", "Ho Tuong", ""]]}, {"id": "1312.5097", "submitter": "Alexander Darer", "authors": "Alexander Darer and Peter Lewis", "title": "A Cellular Automaton Based Controller for a Ms. Pac-Man Agent", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Video games can be used as an excellent test bed for Artificial Intelligence\n(AI) techniques. They are challenging and non-deterministic, this makes it very\ndifficult to write strong AI players. An example of such a video game is Ms.\nPac-Man. In this paper we will outline some of the previous techniques used to\nbuild AI controllers for Ms. Pac-Man as well as presenting a new and novel\nsolution. My technique utilises a Cellular Automaton (CA) to build a\nrepresentation of the environment at each time step of the game. The basis of\nthe representation is a 2-D grid of cells. Each cell has a state and a set of\nrules which determine whether or not that cell will dominate (i.e. pass its\nstate value onto) adjacent cells at each update. Once a certain number of\nupdate iterations have been completed, the CA represents the state of the\nenvironment in the game, the goals and the dangers. At this point, Ms. Pac-Man\nwill decide her next move based only on her adjacent cells, that is to say, she\nhas no knowledge of the state of the environment as a whole, she will simply\nfollow the strongest path. This technique shows promise and allows the\ncontroller to achieve high scores in a live game, the behaviour it exhibits is\ninteresting and complex. Moreover, this behaviour is produced by using very\nsimple rules which are applied many times to each cell in the grid. Simple\nlocal interactions with complex global results are truly achieved.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2013 11:08:05 GMT"}], "update_date": "2013-12-19", "authors_parsed": [["Darer", "Alexander", ""], ["Lewis", "Peter", ""]]}, {"id": "1312.5162", "submitter": "Leon Abdillah", "authors": "Ardina Ariani, Leon Andretti Abdillah, Firamon Syakti", "title": "Sistem pendukung keputusan kelayakan TKI ke luar negeri menggunakan\n  FMADM", "comments": "Jurnal Sistem Informasi (SISFO)", "journal-ref": "Jurnal Sistem Informasi (SISFO), vol. 4, pp. 336-343, September\n  2013", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  BP3TKI Palembang is the government agencies that coordinate, execute and\nselection of prospective migrants registration and placement. To simplify the\nexisting procedures and improve decision-making is necessary to build a\ndecision support system (DSS) to determine eligibility for employment abroad by\napplying Fuzzy Multiple Attribute Decision Making (FMADM), using the linear\nsequential systems development methods. The system is built using Microsoft\nVisual Basic. Net 2010 and SQL Server 2008 database. The design of the system\nusing use case diagrams and class diagrams to identify the needs of users and\nsystems as well as systems implementation guidelines. This Decision Support\nSystem able to rank and produce the prospective migrants, making it easier for\nparties to take decision BP3TKI the workers who will be working out of the\ncountry.\n", "versions": [{"version": "v1", "created": "Tue, 17 Dec 2013 16:59:16 GMT"}], "update_date": "2013-12-19", "authors_parsed": [["Ariani", "Ardina", ""], ["Abdillah", "Leon Andretti", ""], ["Syakti", "Firamon", ""]]}, {"id": "1312.5198", "submitter": "Ashutosh Modi", "authors": "Ashutosh Modi and Ivan Titov", "title": "Learning Semantic Script Knowledge with Event Embeddings", "comments": "4 Pages, 1 figure, ICLR Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Induction of common sense knowledge about prototypical sequences of events\nhas recently received much attention. Instead of inducing this knowledge in the\nform of graphs, as in much of the previous work, in our method, distributed\nrepresentations of event realizations are computed based on distributed\nrepresentations of predicates and their arguments, and then these\nrepresentations are used to predict prototypical event orderings. The\nparameters of the compositional process for computing the event representations\nand the ranking component of the model are jointly estimated from texts. We\nshow that this approach results in a substantial boost in ordering performance\nwith respect to previous methods.\n", "versions": [{"version": "v1", "created": "Wed, 18 Dec 2013 16:13:08 GMT"}, {"version": "v2", "created": "Mon, 20 Jan 2014 10:42:35 GMT"}, {"version": "v3", "created": "Mon, 17 Feb 2014 17:08:34 GMT"}, {"version": "v4", "created": "Fri, 25 Apr 2014 13:31:53 GMT"}], "update_date": "2017-02-13", "authors_parsed": [["Modi", "Ashutosh", ""], ["Titov", "Ivan", ""]]}, {"id": "1312.5378", "submitter": "Guy Van den Broeck", "authors": "Guy Van den Broeck, Wannes Meert, Adnan Darwiche", "title": "Skolemization for Weighted First-Order Model Counting", "comments": "To appear in Proceedings of the 14th International Conference on\n  Principles of Knowledge Representation and Reasoning (KR), Vienna, Austria,\n  July 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  First-order model counting emerged recently as a novel reasoning task, at the\ncore of efficient algorithms for probabilistic logics. We present a\nSkolemization algorithm for model counting problems that eliminates existential\nquantifiers from a first-order logic theory without changing its weighted model\ncount. For certain subsets of first-order logic, lifted model counters were\nshown to run in time polynomial in the number of objects in the domain of\ndiscourse, where propositional model counters require exponential time.\nHowever, these guarantees apply only to Skolem normal form theories (i.e., no\nexistential quantifiers) as the presence of existential quantifiers reduces\nlifted model counters to propositional ones. Since textbook Skolemization is\nnot sound for model counting, these restrictions precluded efficient model\ncounting for directed models, such as probabilistic logic programs, which rely\non existential quantification. Our Skolemization procedure extends the\napplicability of first-order model counters to these representations. Moreover,\nit simplifies the design of lifted model counting algorithms.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 00:40:56 GMT"}, {"version": "v2", "created": "Wed, 5 Mar 2014 13:50:15 GMT"}], "update_date": "2014-03-06", "authors_parsed": [["Broeck", "Guy Van den", ""], ["Meert", "Wannes", ""], ["Darwiche", "Adnan", ""]]}, {"id": "1312.5515", "submitter": "Marek Kurdej", "authors": "Marek Kurdej (HEUDIASYC), V\\'eronique Cherfaoui (HEUDIASYC)", "title": "Conservative, Proportional and Optimistic Contextual Discounting in the\n  Belief Functions Theory", "comments": "7 pages", "journal-ref": "16th International Conference on Information Fusion, Istanbul :\n  Turkey (2013)", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Information discounting plays an important role in the theory of belief\nfunctions and, generally, in information fusion. Nevertheless, neither\nclassical uniform discounting nor contextual cannot model certain use cases,\nnotably temporal discounting. In this article, new contextual discounting\nschemes, conservative, proportional and optimistic, are proposed. Some\nproperties of these discounting operations are examined. Classical discounting\nis shown to be a special case of these schemes. Two motivating cases are\ndiscussed: modelling of source reliability and application to temporal\ndiscounting.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 12:40:25 GMT"}], "update_date": "2013-12-20", "authors_parsed": [["Kurdej", "Marek", "", "HEUDIASYC"], ["Cherfaoui", "V\u00e9ronique", "", "HEUDIASYC"]]}, {"id": "1312.5713", "submitter": "Dimiter Dobrev", "authors": "Dimiter Dobrev", "title": "Giving the AI definition a form suitable for the engineer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial Intelligence - what is this? That is the question! In earlier\npapers we already gave a formal definition for AI, but if one desires to build\nan actual AI implementation, the following issues require attention and are\ntreated here: the data format to be used, the idea of Undef and Nothing\nsymbols, various ways for defining the \"meaning of life\", and finally, a new\nnotion of \"incorrect move\". These questions are of minor importance in the\ntheoretical discussion, but we already know the answer of the question \"Does AI\nexist?\" Now we want to make the next step and to create this program.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 19:28:18 GMT"}, {"version": "v2", "created": "Tue, 31 Mar 2015 10:26:48 GMT"}], "update_date": "2015-04-01", "authors_parsed": [["Dobrev", "Dimiter", ""]]}, {"id": "1312.5714", "submitter": "Patrick Connor", "authors": "Patrick C. Connor and Thomas P. Trappenberg", "title": "Avoiding Confusion between Predictors and Inhibitors in Value Function\n  Approximation", "comments": "14 pages, 3 figures, 23 references, Workshop paper in ICLR 2014\n  (updated based on reviewer comments)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In reinforcement learning, the goal is to seek rewards and avoid punishments.\nA simple scalar captures the value of a state or of taking an action, where\nexpected future rewards increase and punishments decrease this quantity.\nNaturally an agent should learn to predict this quantity to take beneficial\nactions, and many value function approximators exist for this purpose. In the\npresent work, however, we show how value function approximators can cause\nconfusion between predictors of an outcome of one valence (e.g., a signal of\nreward) and the inhibitor of the opposite valence (e.g., a signal canceling\nexpectation of punishment). We show this to be a problem for both linear and\nnon-linear value function approximators, especially when the amount of data (or\nexperience) is limited. We propose and evaluate a simple resolution: to instead\npredict reward and punishment values separately, and rectify and add them to\nget the value needed for decision making. We evaluate several function\napproximators in this slightly different value function approximation\narchitecture and show that this approach is able to circumvent the confusion\nand thereby achieve lower value-prediction errors.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 19:52:52 GMT"}, {"version": "v2", "created": "Wed, 18 Feb 2015 15:35:56 GMT"}], "update_date": "2015-02-19", "authors_parsed": [["Connor", "Patrick C.", ""], ["Trappenberg", "Thomas P.", ""]]}, {"id": "1312.6096", "submitter": "Michael Fink", "authors": "Mario Alviano and Wolfgang Faber", "title": "Properties of Answer Set Programming with Convex Generalized Atoms", "comments": "Proceedings of Answer Set Programming and Other Computing Paradigms\n  (ASPOCP 2013), 6th International Workshop, August 25, 2013, Istanbul, Turkey", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, Answer Set Programming (ASP), logic programming under the\nstable model or answer set semantics, has seen several extensions by\ngeneralizing the notion of an atom in these programs: be it aggregate atoms,\nHEX atoms, generalized quantifiers, or abstract constraints, the idea is to\nhave more complicated satisfaction patterns in the lattice of Herbrand\ninterpretations than traditional, simple atoms. In this paper we refer to any\nof these constructs as generalized atoms. Several semantics with differing\ncharacteristics have been proposed for these extensions, rendering the big\npicture somewhat blurry. In this paper, we analyze the class of programs that\nhave convex generalized atoms (originally proposed by Liu and Truszczynski in\n[10]) in rule bodies and show that for this class many of the proposed\nsemantics coincide. This is an interesting result, since recently it has been\nshown that this class is the precise complexity boundary for the FLP semantics.\nWe investigate whether similar results also hold for other semantics, and\ndiscuss the implications of our findings.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 20:18:04 GMT"}], "update_date": "2013-12-23", "authors_parsed": [["Alviano", "Mario", ""], ["Faber", "Wolfgang", ""]]}, {"id": "1312.6105", "submitter": "Michael Fink", "authors": "Marcello Balduccini and Yulia Lierler", "title": "Hybrid Automated Reasoning Tools: from Black-box to Clear-box\n  Integration", "comments": "Proceedings of Answer Set Programming and Other Computing Paradigms\n  (ASPOCP 2013), 6th International Workshop, August 25, 2013, Istanbul, Turkey", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, researchers in answer set programming and constraint programming\nspent significant efforts in the development of hybrid languages and solving\nalgorithms combining the strengths of these traditionally separate fields.\nThese efforts resulted in a new research area: constraint answer set\nprogramming (CASP). CASP languages and systems proved to be largely successful\nat providing efficient solutions to problems involving hybrid reasoning tasks,\nsuch as scheduling problems with elements of planning. Yet, the development of\nCASP systems is difficult, requiring non-trivial expertise in multiple areas.\nThis suggests a need for a study identifying general development principles of\nhybrid systems. Once these principles and their implications are well\nunderstood, the development of hybrid languages and systems may become a\nwell-established and well-understood routine process. As a step in this\ndirection, in this paper we conduct a case study aimed at evaluating various\nintegration schemas of CASP methods.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 20:44:58 GMT"}], "update_date": "2013-12-23", "authors_parsed": [["Balduccini", "Marcello", ""], ["Lierler", "Yulia", ""]]}, {"id": "1312.6113", "submitter": "Michael Fink", "authors": "Mutsunori Banbara, Martin Gebser, Katsumi Inoue, Torsten Schaub,\n  Takehide Soh, Naoyuki Tamura and Matthias Weise", "title": "Aspartame: Solving Constraint Satisfaction Problems with Answer Set\n  Programming", "comments": "Proceedings of Answer Set Programming and Other Computing Paradigms\n  (ASPOCP 2013), 6th International Workshop, August 25, 2013, Istanbul, Turkey", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Encoding finite linear CSPs as Boolean formulas and solving them by using\nmodern SAT solvers has proven to be highly effective, as exemplified by the\naward-winning sugar system. We here develop an alternative approach based on\nASP. This allows us to use first-order encodings providing us with a high\ndegree of flexibility for easy experimentation with different implementations.\nThe resulting system aspartame re-uses parts of sugar for parsing and\nnormalizing CSPs. The obtained set of facts is then combined with an ASP\nencoding that can be grounded and solved by off-the-shelf ASP systems. We\nestablish the competitiveness of our approach by empirically contrasting\naspartame and sugar.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 20:57:28 GMT"}], "update_date": "2013-12-23", "authors_parsed": [["Banbara", "Mutsunori", ""], ["Gebser", "Martin", ""], ["Inoue", "Katsumi", ""], ["Schaub", "Torsten", ""], ["Soh", "Takehide", ""], ["Tamura", "Naoyuki", ""], ["Weise", "Matthias", ""]]}, {"id": "1312.6130", "submitter": "Michael Fink", "authors": "Michael Bartholomew and Joohyung Lee", "title": "A Functional View of Strong Negation in Answer Set Programming", "comments": "Proceedings of Answer Set Programming and Other Computing Paradigms\n  (ASPOCP 2013), 6th International Workshop, August 25, 2013, Istanbul, Turkey", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The distinction between strong negation and default negation has been useful\nin answer set programming. We present an alternative account of strong\nnegation, which lets us view strong negation in terms of the functional stable\nmodel semantics by Bartholomew and Lee. More specifically, we show that, under\ncomplete interpretations, minimizing both positive and negative literals in the\ntraditional answer set semantics is essentially the same as ensuring the\nuniqueness of Boolean function values under the functional stable model\nsemantics. The same account lets us view Lifschitz's two-valued logic programs\nas a special case of the functional stable model semantics. In addition, we\nshow how non-Boolean intensional functions can be eliminated in favor of\nBoolean intensional functions, and furthermore can be represented using strong\nnegation, which provides a way to compute the functional stable model semantics\nusing existing ASP solvers. We also note that similar results hold with the\nfunctional stable model semantics by Cabalar.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 21:01:32 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Bartholomew", "Michael", ""], ["Lee", "Joohyung", ""]]}, {"id": "1312.6134", "submitter": "Michael Fink", "authors": "Pedro Cabalar and Jorge Fandinno", "title": "An Algebra of Causal Chains", "comments": "Proceedings of Answer Set Programming and Other Computing Paradigms\n  (ASPOCP 2013), 6th International Workshop, August 25, 2013, Istanbul, Turkey", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we propose a multi-valued extension of logic programs under the\nstable models semantics where each true atom in a model is associated with a\nset of justifications, in a similar spirit than a set of proof trees. The main\ncontribution of this paper is that we capture justifications into an algebra of\ntruth values with three internal operations: an addition '+' representing\nalternative justifications for a formula, a commutative product '*'\nrepresenting joint interaction of causes and a non-commutative product '.'\nacting as a concatenation or proof constructor. Using this multi-valued\nsemantics, we obtain a one-to-one correspondence between the syntactic proof\ntree of a standard (non-causal) logic program and the interpretation of each\ntrue atom in a model. Furthermore, thanks to this algebraic characterization we\ncan detect semantic properties like redundancy and relevance of the obtained\njustifications. We also identify a lattice-based characterization of this\nalgebra, defining a direct consequences operator, proving its continuity and\nthat its least fix point can be computed after a finite number of iterations.\nFinally, we define the concept of causal stable model by introducing an\nanalogous transformation to Gelfond and Lifschitz's program reduct.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 21:07:19 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Cabalar", "Pedro", ""], ["Fandinno", "Jorge", ""]]}, {"id": "1312.6138", "submitter": "Michael Fink", "authors": "Vinay K. Chaudhri, Stijn Heymans, Michael Wessel and Tran Cao Son", "title": "Query Answering in Object Oriented Knowledge Bases in Logic Programming:\n  Description and Challenge for ASP", "comments": "Proceedings of Answer Set Programming and Other Computing Paradigms\n  (ASPOCP 2013), 6th International Workshop, August 25, 2013, Istanbul, Turkey", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research on developing efficient and scalable ASP solvers can substantially\nbenefit by the availability of data sets to experiment with. KB_Bio_101\ncontains knowledge from a biology textbook, has been developed as part of\nProject Halo, and has recently become available for research use. KB_Bio_101 is\none of the largest KBs available in ASP and the reasoning with it is\nundecidable in general. We give a description of this KB and ASP programs for a\nsuite of queries that have been of practical interest. We explain why these\nqueries pose significant practical challenges for the current ASP solvers.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 21:13:10 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Chaudhri", "Vinay K.", ""], ["Heymans", "Stijn", ""], ["Wessel", "Michael", ""], ["Son", "Tran Cao", ""]]}, {"id": "1312.6140", "submitter": "Michael Fink", "authors": "Stefan Ellmauthaler and Hannes Strass", "title": "The DIAMOND System for Argumentation: Preliminary Report", "comments": "Proceedings of Answer Set Programming and Other Computing Paradigms\n  (ASPOCP 2013), 6th International Workshop, August 25, 2013, Istanbul, Turkey", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Abstract dialectical frameworks (ADFs) are a powerful generalisation of\nDung's abstract argumentation frameworks. In this paper we present an answer\nset programming based software system, called DIAMOND (DIAlectical MOdels\neNcoDing). It translates ADFs into answer set programs whose stable models\ncorrespond to models of the ADF with respect to several semantics (i.e.\nadmissible, complete, stable, grounded).\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 21:17:03 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Ellmauthaler", "Stefan", ""], ["Strass", "Hannes", ""]]}, {"id": "1312.6143", "submitter": "Michael Fink", "authors": "Martin Gebser, Philipp Obermeier and Torsten Schaub", "title": "A System for Interactive Query Answering with Answer Set Programming", "comments": "Proceedings of Answer Set Programming and Other Computing Paradigms\n  (ASPOCP 2013), 6th International Workshop, August 25, 2013, Istanbul, Turkey", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reactive answer set programming has paved the way for incorporating online\ninformation into operative solving processes. Although this technology was\noriginally devised for dealing with data streams in dynamic environments, like\nassisted living and cognitive robotics, it can likewise be used to incorporate\nfacts, rules, or queries provided by a user. As a result, we present the design\nand implementation of a system for interactive query answering with reactive\nanswer set programming. Our system quontroller is based on the reactive solver\noclingo and implemented as a dedicated front-end. We describe its functionality\nand implementation, and we illustrate its features by some selected use cases.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 21:24:30 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Gebser", "Martin", ""], ["Obermeier", "Philipp", ""], ["Schaub", "Torsten", ""]]}, {"id": "1312.6146", "submitter": "Michael Fink", "authors": "Canan G\\\"uni\\c{c}en, Esra Erdem and H\\\"usn\\\"u Yenig\\\"un", "title": "Generating Shortest Synchronizing Sequences using Answer Set Programming", "comments": "Proceedings of Answer Set Programming and Other Computing Paradigms\n  (ASPOCP 2013), 6th International Workshop, August 25, 2013, Istanbul, Turkey", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For a finite state automaton, a synchronizing sequence is an input sequence\nthat takes all the states to the same state. Checking the existence of a\nsynchronizing sequence and finding a synchronizing sequence, if one exists, can\nbe performed in polynomial time. However, the problem of finding a shortest\nsynchronizing sequence is known to be NP-hard. In this work, the usefulness of\nAnswer Set Programming to solve this optimization problem is investigated, in\ncomparison with brute-force algorithms and SAT-based approaches.\n  Keywords: finite automata, shortest synchronizing sequence, ASP\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 21:30:10 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["G\u00fcni\u00e7en", "Canan", ""], ["Erdem", "Esra", ""], ["Yenig\u00fcn", "H\u00fcsn\u00fc", ""]]}, {"id": "1312.6149", "submitter": "Michael Fink", "authors": "Amelia Harrison, Vladimir Lifschitz and Fangkai Yang", "title": "On the Semantics of Gringo", "comments": "Proceedings of Answer Set Programming and Other Computing Paradigms\n  (ASPOCP 2013), 6th International Workshop, August 25, 2013, Istanbul, Turkey", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Input languages of answer set solvers are based on the mathematically simple\nconcept of a stable model. But many useful constructs available in these\nlanguages, including local variables, conditional literals, and aggregates,\ncannot be easily explained in terms of stable models in the sense of the\noriginal definition of this concept and its straightforward generalizations.\nManuals written by designers of answer set solvers usually explain such\nconstructs using examples and informal comments that appeal to the user's\nintuition, without references to any precise semantics. We propose to approach\nthe problem of defining the semantics of gringo programs by translating them\ninto the language of infinitary propositional formulas. This semantics allows\nus to study equivalent transformations of gringo programs using natural\ndeduction in infinitary propositional logic.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 21:33:55 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Harrison", "Amelia", ""], ["Lifschitz", "Vladimir", ""], ["Yang", "Fangkai", ""]]}, {"id": "1312.6151", "submitter": "Michael Fink", "authors": "Yuliya Lierler and Miroslaw Truszczynski", "title": "Abstract Modular Systems and Solvers", "comments": "Proceedings of Answer Set Programming and Other Computing Paradigms\n  (ASPOCP 2013), 6th International Workshop, August 25, 2013, Istanbul, Turkey", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Integrating diverse formalisms into modular knowledge representation systems\noffers increased expressivity, modeling convenience and computational benefits.\nWe introduce concepts of abstract modules and abstract modular systems to study\ngeneral principles behind the design and analysis of model-finding programs, or\nsolvers, for integrated heterogeneous multi-logic systems. We show how abstract\nmodules and abstract modular systems give rise to transition systems, which are\na natural and convenient representation of solvers pioneered by the SAT\ncommunity. We illustrate our approach by showing how it applies to answer set\nprogramming and propositional logic, and to multi-logic systems based on these\ntwo formalisms.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 21:37:56 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Lierler", "Yuliya", ""], ["Truszczynski", "Miroslaw", ""]]}, {"id": "1312.6156", "submitter": "Michael Fink", "authors": "Joost Vennekens", "title": "Negation in the Head of CP-logic Rules", "comments": "Proceedings of Answer Set Programming and Other Computing Paradigms\n  (ASPOCP 2013), 6th International Workshop, August 25, 2013, Istanbul, Turkey", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CP-logic is a probabilistic extension of the logic FO(ID). Unlike ASP, both\nof these logics adhere to a Tarskian informal semantics, in which\ninterpretations represent objective states-of-affairs. In other words, these\nlogics lack the epistemic component of ASP, in which interpretations represent\nthe beliefs or knowledge of a rational agent. Consequently, neither CP-logic\nnor FO(ID) have the need for two kinds of negations: there is only one\nnegation, and its meaning is that of objective falsehood. Nevertheless, the\nformal semantics of this objective negation is mathematically more similar to\nASP's negation-as-failure than to its classical negation. The reason is that\nboth CP-logic and FO(ID) have a constructive semantics in which all atoms start\nout as false, and may only become true as the result of a rule application.\nThis paper investigates the possibility of adding the well-known ASP feature of\nallowing negation in the head of rules to CP-logic. Because CP-logic only has\none kind of negation, it is of necessity this ''negation-as-failure like''\nnegation that will be allowed in the head. We investigate the intuitive meaning\nof such a construct and the benefits that arise from it.\n", "versions": [{"version": "v1", "created": "Fri, 20 Dec 2013 21:41:20 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Vennekens", "Joost", ""]]}, {"id": "1312.6214", "submitter": "Elad Hazan", "authors": "Elad Hazan and Zohar Karnin and Raghu Mehka", "title": "Volumetric Spanners: an Efficient Exploration Basis for Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Numerous machine learning problems require an exploration basis - a mechanism\nto explore the action space. We define a novel geometric notion of exploration\nbasis with low variance, called volumetric spanners, and give efficient\nalgorithms to construct such a basis.\n  We show how efficient volumetric spanners give rise to the first efficient\nand optimal regret algorithm for bandit linear optimization over general convex\nsets. Previously such results were known only for specific convex sets, or\nunder special conditions such as the existence of an efficient self-concordant\nbarrier for the underlying set.\n", "versions": [{"version": "v1", "created": "Sat, 21 Dec 2013 06:51:50 GMT"}, {"version": "v2", "created": "Sun, 26 Jan 2014 12:16:59 GMT"}, {"version": "v3", "created": "Sun, 25 May 2014 11:57:08 GMT"}], "update_date": "2014-05-27", "authors_parsed": [["Hazan", "Elad", ""], ["Karnin", "Zohar", ""], ["Mehka", "Raghu", ""]]}, {"id": "1312.6546", "submitter": "Haris Aziz", "authors": "Haris Aziz and Serge Gaspers and Simon Mackenzie and Toby Walsh", "title": "Fair assignment of indivisible objects under ordinal preferences", "comments": "extended version of a paper presented at AAMAS 2014", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the discrete assignment problem in which agents express ordinal\npreferences over objects and these objects are allocated to the agents in a\nfair manner. We use the stochastic dominance relation between fractional or\nrandomized allocations to systematically define varying notions of\nproportionality and envy-freeness for discrete assignments. The computational\ncomplexity of checking whether a fair assignment exists is studied for these\nfairness notions. We also characterize the conditions under which a fair\nassignment is guaranteed to exist. For a number of fairness concepts,\npolynomial-time algorithms are presented to check whether a fair assignment\nexists. Our algorithmic results also extend to the case of unequal entitlements\nof agents. Our NP-hardness result, which holds for several variants of\nenvy-freeness, answers an open question posed by Bouveret, Endriss, and Lang\n(ECAI 2010). We also propose fairness concepts that always suggest a non-empty\nset of assignments with meaningful fairness properties. Among these concepts,\noptimal proportionality and optimal weak proportionality appear to be desirable\nfairness concepts.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2013 13:37:19 GMT"}, {"version": "v2", "created": "Tue, 16 Sep 2014 03:57:49 GMT"}, {"version": "v3", "created": "Sat, 6 Dec 2014 04:30:06 GMT"}, {"version": "v4", "created": "Wed, 17 Jun 2015 04:25:04 GMT"}], "update_date": "2015-06-18", "authors_parsed": [["Aziz", "Haris", ""], ["Gaspers", "Serge", ""], ["Mackenzie", "Simon", ""], ["Walsh", "Toby", ""]]}, {"id": "1312.6558", "submitter": "Indre Zliobaite", "authors": "Indre Zliobaite, Mykola Pechenizkiy", "title": "Predictive User Modeling with Actionable Attributes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Different machine learning techniques have been proposed and used for\nmodeling individual and group user needs, interests and preferences. In the\ntraditional predictive modeling instances are described by observable\nvariables, called attributes. The goal is to learn a model for predicting the\ntarget variable for unseen instances. For example, for marketing purposes a\ncompany consider profiling a new user based on her observed web browsing\nbehavior, referral keywords or other relevant information. In many real world\napplications the values of some attributes are not only observable, but can be\nactively decided by a decision maker. Furthermore, in some of such applications\nthe decision maker is interested not only to generate accurate predictions, but\nto maximize the probability of the desired outcome. For example, a direct\nmarketing manager can choose which type of a special offer to send to a client\n(actionable attribute), hoping that the right choice will result in a positive\nresponse with a higher probability. We study how to learn to choose the value\nof an actionable attribute in order to maximize the probability of a desired\noutcome in predictive modeling. We emphasize that not all instances are equally\nsensitive to changes in actions. Accurate choice of an action is critical for\nthose instances, which are on the borderline (e.g. users who do not have a\nstrong opinion one way or the other). We formulate three supervised learning\napproaches for learning to select the value of an actionable attribute at an\ninstance level. We also introduce a focused training procedure which puts more\nemphasis on the situations where varying the action is the most likely to take\nthe effect. The proof of concept experimental validation on two real-world case\nstudies in web analytics and e-learning domains highlights the potential of the\nproposed approaches.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2013 14:37:44 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Zliobaite", "Indre", ""], ["Pechenizkiy", "Mykola", ""]]}, {"id": "1312.6599", "submitter": "Shatrughan Modi", "authors": "Shatrughan Modi and Dr. Seema Bawa", "title": "Image Processing based Systems and Techniques for the Recognition of\n  Ancient and Modern Coins", "comments": "5 pages, 1 table, Published with International Journal of Computer\n  Applications (IJCA)", "journal-ref": "International Journal of Computer Applications 47(10):1-5, June\n  2012", "doi": "10.5120/7221-0041", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coins are frequently used in everyday life at various places like in banks,\ngrocery stores, supermarkets, automated weighing machines, vending machines\netc. So, there is a basic need to automate the counting and sorting of coins.\nFor this machines need to recognize the coins very fast and accurately, as\nfurther transaction processing depends on this recognition. Three types of\nsystems are available in the market: Mechanical method based systems,\nElectromagnetic method based systems and Image processing based systems. This\npaper presents an overview of available systems and techniques based on image\nprocessing to recognize ancient and modern coins.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2013 17:00:02 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Modi", "Shatrughan", ""], ["Bawa", "Dr. Seema", ""]]}, {"id": "1312.6615", "submitter": "Shatrughan Modi", "authors": "Shatrughan Modi and Dr. Seema Bawa", "title": "Automated Coin Recognition System using ANN", "comments": "6 pages, 11 figures, 1 table, Published with International Journal of\n  Computer Applications (IJCA)", "journal-ref": "International Journal of Computer Applications 26(4):13-18, July\n  2011", "doi": "10.5120/3093-4244", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coins are integral part of our day to day life. We use coins everywhere like\ngrocery store, banks, buses, trains etc. So it becomes a basic need that coins\ncan be sorted and counted automatically. For this it is necessary that coins\ncan be recognized automatically. In this paper we have developed an ANN\n(Artificial Neural Network) based Automated Coin Recognition System for the\nrecognition of Indian Coins of denomination Rs. 1, 2, 5 and 10 with rotation\ninvariance. We have taken images from both sides of coin. So this system is\ncapable of recognizing coins from both sides. Features are extracted from\nimages using techniques of Hough Transformation, Pattern Averaging etc. Then,\nthe extracted features are passed as input to a trained Neural Network. 97.74%\nrecognition rate has been achieved during the experiments i.e. only 2.26% miss\nrecognition, which is quite encouraging.\n", "versions": [{"version": "v1", "created": "Mon, 23 Dec 2013 17:40:08 GMT"}], "update_date": "2013-12-24", "authors_parsed": [["Modi", "Shatrughan", ""], ["Bawa", "Dr. Seema", ""]]}, {"id": "1312.6726", "submitter": "Jordi Grau-Moya", "authors": "Jordi Grau-Moya and Daniel A. Braun", "title": "Bounded Rational Decision-Making in Changing Environments", "comments": "9 pages, 2 figures, NIPS 2013 Workshop on Planning with Information\n  Constraints", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A perfectly rational decision-maker chooses the best action with the highest\nutility gain from a set of possible actions. The optimality principles that\ndescribe such decision processes do not take into account the computational\ncosts of finding the optimal action. Bounded rational decision-making addresses\nthis problem by specifically trading off information-processing costs and\nexpected utility. Interestingly, a similar trade-off between energy and entropy\narises when describing changes in thermodynamic systems. This similarity has\nbeen recently used to describe bounded rational agents. Crucially, this\nframework assumes that the environment does not change while the decision-maker\nis computing the optimal policy. When this requirement is not fulfilled, the\ndecision-maker will suffer inefficiencies in utility, that arise because the\ncurrent policy is optimal for an environment in the past. Here we borrow\nconcepts from non-equilibrium thermodynamics to quantify these inefficiencies\nand illustrate with simulations its relationship with computational resources.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2013 00:22:44 GMT"}], "update_date": "2013-12-25", "authors_parsed": [["Grau-Moya", "Jordi", ""], ["Braun", "Daniel A.", ""]]}, {"id": "1312.6764", "submitter": "Eric Nivel", "authors": "E. Nivel, K. R. Th\\'orisson, B. R. Steunebrink, H. Dindo, G. Pezzulo,\n  M. Rodriguez, C. Hernandez, D. Ognibene, J. Schmidhuber, R. Sanz, H. P.\n  Helgason, A. Chella and G. K. Jonsson", "title": "Bounded Recursive Self-Improvement", "comments": null, "journal-ref": null, "doi": null, "report-no": "RUTR-SCS13006", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We have designed a machine that becomes increasingly better at behaving in\nunderspecified circumstances, in a goal-directed way, on the job, by modeling\nitself and its environment as experience accumulates. Based on principles of\nautocatalysis, endogeny, and reflectivity, the work provides an architectural\nblueprint for constructing systems with high levels of operational autonomy in\nunderspecified circumstances, starting from a small seed. Through value-driven\ndynamic priority scheduling controlling the parallel execution of a vast number\nof reasoning threads, the system achieves recursive self-improvement after it\nleaves the lab, within the boundaries imposed by its designers. A prototype\nsystem has been implemented and demonstrated to learn a complex real-world\ntask, real-time multimodal dialogue with humans, by on-line observation. Our\nwork presents solutions to several challenges that must be solved for achieving\nartificial general intelligence.\n", "versions": [{"version": "v1", "created": "Tue, 24 Dec 2013 06:17:55 GMT"}], "update_date": "2013-12-25", "authors_parsed": [["Nivel", "E.", ""], ["Th\u00f3risson", "K. R.", ""], ["Steunebrink", "B. R.", ""], ["Dindo", "H.", ""], ["Pezzulo", "G.", ""], ["Rodriguez", "M.", ""], ["Hernandez", "C.", ""], ["Ognibene", "D.", ""], ["Schmidhuber", "J.", ""], ["Sanz", "R.", ""], ["Helgason", "H. P.", ""], ["Chella", "A.", ""], ["Jonsson", "G. K.", ""]]}, {"id": "1312.6832", "submitter": "Eugene Feinberg", "authors": "Eugene A. Feinberg and Jefferson Huang", "title": "The Value Iteration Algorithm is Not Strongly Polynomial for Discounted\n  Dynamic Programming", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This note provides a simple example demonstrating that, if exact computations\nare allowed, the number of iterations required for the value iteration\nalgorithm to find an optimal policy for discounted dynamic programming problems\nmay grow arbitrarily quickly with the size of the problem. In particular, the\nnumber of iterations can be exponential in the number of actions. Thus, unlike\npolicy iterations, the value iteration algorithm is not strongly polynomial for\ndiscounted dynamic programming.\n", "versions": [{"version": "v1", "created": "Thu, 19 Dec 2013 23:54:50 GMT"}], "update_date": "2013-12-25", "authors_parsed": [["Feinberg", "Eugene A.", ""], ["Huang", "Jefferson", ""]]}, {"id": "1312.6947", "submitter": "Sourish Dasgupta", "authors": "Sourish Dasgupta, Ankur Padia, Kushal Shah, Prasenjit Majumder", "title": "Formal Ontology Learning on Factual IS-A Corpus in English using\n  Description Logics", "comments": "This paper has been withdrawn by the author due to requirement of\n  re-evaluation of results", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Ontology Learning (OL) is the computational task of generating a knowledge\nbase in the form of an ontology given an unstructured corpus whose content is\nin natural language (NL). Several works can be found in this area most of which\nare limited to statistical and lexico-syntactic pattern matching based\ntechniques Light-Weight OL. These techniques do not lead to very accurate\nlearning mostly because of several linguistic nuances in NL. Formal OL is an\nalternative (less explored) methodology were deep linguistics analysis is made\nusing theory and tools found in computational linguistics to generate formal\naxioms and definitions instead simply inducing a taxonomy. In this paper we\npropose \"Description Logic (DL)\" based formal OL framework for learning factual\nIS-A type sentences in English. We claim that semantic construction of IS-A\nsentences is non trivial. Hence, we also claim that such sentences requires\nspecial studies in the context of OL before any truly formal OL can be\nproposed. We introduce a learner tool, called DLOL_IS-A, that generated such\nontologies in the owl format. We have adopted \"Gold Standard\" based OL\nevaluation on IS-A rich WCL v.1.1 dataset and our own Community representative\nIS-A dataset. We observed significant improvement of DLOL_IS-A when compared to\nthe light-weight OL tool Text2Onto and formal OL tool FRED.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 09:17:28 GMT"}, {"version": "v2", "created": "Tue, 8 Mar 2016 05:20:47 GMT"}], "update_date": "2016-03-09", "authors_parsed": [["Dasgupta", "Sourish", ""], ["Padia", "Ankur", ""], ["Shah", "Kushal", ""], ["Majumder", "Prasenjit", ""]]}, {"id": "1312.6948", "submitter": "Sourish Dasgupta", "authors": "Sourish Dasgupta, Rupali KaPatel, Ankur Padia, Kushal Shah", "title": "Description Logics based Formalization of Wh-Queries", "comments": "Natural Language Query Processing, Representation", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The problem of Natural Language Query Formalization (NLQF) is to translate a\ngiven user query in natural language (NL) into a formal language so that the\nsemantic interpretation has equivalence with the NL interpretation.\nFormalization of NL queries enables logic based reasoning during information\nretrieval, database query, question-answering, etc. Formalization also helps in\nWeb query normalization and indexing, query intent analysis, etc. In this paper\nwe are proposing a Description Logics based formal methodology for wh-query\nintent (also called desire) identification and corresponding formal\ntranslation. We evaluated the scalability of our proposed formalism using\nMicrosoft Encarta 98 query dataset and OWL-S TC v.4.0 dataset.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 09:23:49 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Dasgupta", "Sourish", ""], ["KaPatel", "Rupali", ""], ["Padia", "Ankur", ""], ["Shah", "Kushal", ""]]}, {"id": "1312.6995", "submitter": "Sourav Bhattacharya", "authors": "Sourav Bhattacharya and Petteri Nurmi and Nils Hammerla and Thomas\n  Pl\\\"otz", "title": "Towards Using Unlabeled Data in a Sparse-coding Framework for Human\n  Activity Recognition", "comments": "18 pages, 12 figures, Pervasive and Mobile Computing, 2014", "journal-ref": null, "doi": "10.1016/j.pmcj.2014.05.006", "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a sparse-coding framework for activity recognition in ubiquitous\nand mobile computing that alleviates two fundamental problems of current\nsupervised learning approaches. (i) It automatically derives a compact, sparse\nand meaningful feature representation of sensor data that does not rely on\nprior expert knowledge and generalizes extremely well across domain boundaries.\n(ii) It exploits unlabeled sample data for bootstrapping effective activity\nrecognizers, i.e., substantially reduces the amount of ground truth annotation\nrequired for model estimation. Such unlabeled data is trivial to obtain, e.g.,\nthrough contemporary smartphones carried by users as they go about their\neveryday activities.\n  Based on the self-taught learning paradigm we automatically derive an\nover-complete set of basis vectors from unlabeled data that captures inherent\npatterns present within activity data. Through projecting raw sensor data onto\nthe feature space defined by such over-complete sets of basis vectors effective\nfeature extraction is pursued. Given these learned feature representations,\nclassification backends are then trained using small amounts of labeled\ntraining data.\n  We study the new approach in detail using two datasets which differ in terms\nof the recognition tasks and sensor modalities. Primarily we focus on\ntransportation mode analysis task, a popular task in mobile-phone based\nsensing. The sparse-coding framework significantly outperforms the\nstate-of-the-art in supervised learning approaches. Furthermore, we demonstrate\nthe great practical potential of the new approach by successfully evaluating\nits generalization capabilities across both domain and sensor modalities by\nconsidering the popular Opportunity dataset. Our feature learning approach\noutperforms state-of-the-art approaches to analyzing activities in daily\nliving.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 18:08:44 GMT"}, {"version": "v2", "created": "Sat, 5 Jul 2014 10:32:32 GMT"}, {"version": "v3", "created": "Wed, 23 Jul 2014 13:39:53 GMT"}], "update_date": "2014-07-24", "authors_parsed": [["Bhattacharya", "Sourav", ""], ["Nurmi", "Petteri", ""], ["Hammerla", "Nils", ""], ["Pl\u00f6tz", "Thomas", ""]]}, {"id": "1312.6996", "submitter": "Muhammad Rezaul Karim", "authors": "Muhammad Rezaul Karim", "title": "A New Approach to Constraint Weight Learning for Variable Ordering in\n  CSPs", "comments": null, "journal-ref": "Proceedings of the IEEE Congress on Evolutionary Computation (CEC\n  2014), pp. 2716-2723, Beijing, China, July 6-11, 2014", "doi": "10.1109/CEC.2014.6900262", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A Constraint Satisfaction Problem (CSP) is a framework used for modeling and\nsolving constrained problems. Tree-search algorithms like backtracking try to\nconstruct a solution to a CSP by selecting the variables of the problem one\nafter another. The order in which these algorithm select the variables\npotentially have significant impact on the search performance. Various\nheuristics have been proposed for choosing good variable ordering. Many\npowerful variable ordering heuristics weigh the constraints first and then\nutilize the weights for selecting good order of the variables. Constraint\nweighting are basically employed to identify global bottlenecks in a CSP.\n  In this paper, we propose a new approach for learning weights for the\nconstraints using competitive coevolutionary Genetic Algorithm (GA). Weights\nlearned by the coevolutionary GA later help to make better choices for the\nfirst few variables in a search. In the competitive coevolutionary GA,\nconstraints and candidate solutions for a CSP evolve together through an\ninverse fitness interaction process. We have conducted experiments on several\nrandom, quasi-random and patterned instances to measure the efficiency of the\nproposed approach. The results and analysis show that the proposed approach is\ngood at learning weights to distinguish the hard constraints for quasi-random\ninstances and forced satisfiable random instances generated with the Model RB.\nFor other type of instances, RNDI still seems to be the best approach as our\nexperiments show.\n", "versions": [{"version": "v1", "created": "Wed, 25 Dec 2013 18:15:14 GMT"}], "update_date": "2014-10-06", "authors_parsed": [["Karim", "Muhammad Rezaul", ""]]}, {"id": "1312.7326", "submitter": "Hiqmet Kamberaj Dr.", "authors": "Hiqmet Kamberaj", "title": "Replica Exchange using q-Gaussian Swarm Quantum Particle Intelligence\n  Method", "comments": "10 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a newly developed Replica Exchange algorithm using q -Gaussian\nSwarm Quantum Particle Optimization (REX@q-GSQPO) method for solving the\nproblem of finding the global optimum. The basis of the algorithm is to run\nmultiple copies of independent swarms at different values of q parameter. Based\non an energy criterion, chosen to satisfy the detailed balance, we are swapping\nthe particle coordinates of neighboring swarms at regular iteration intervals.\nThe swarm replicas with high q values are characterized by high diversity of\nparticles allowing escaping local minima faster, while the low q replicas,\ncharacterized by low diversity of particles, are used to sample more\nefficiently the local basins. We compare the new algorithm with the standard\nGaussian Swarm Quantum Particle Optimization (GSQPO) and q-Gaussian Swarm\nQuantum Particle Optimization (q-GSQPO) algorithms, and we found that the new\nalgorithm is more robust in terms of the number of fitness function calls, and\nmore efficient in terms ability convergence to the global minimum. In\nadditional, we also provide a method of optimally allocating the swarm replicas\namong different q values. Our algorithm is tested for three benchmark\nfunctions, which are known to be multimodal problems, at different\ndimensionalities. In addition, we considered a polyalanine peptide of 12\nresidues modeled using a G\\=o coarse-graining potential energy function.\n", "versions": [{"version": "v1", "created": "Sun, 17 Nov 2013 12:49:15 GMT"}], "update_date": "2013-12-30", "authors_parsed": [["Kamberaj", "Hiqmet", ""]]}, {"id": "1312.7422", "submitter": "Michael Fink", "authors": "Michael Fink and Yuliya Lierler", "title": "Proceedings of Answer Set Programming and Other Computing Paradigms\n  (ASPOCP 2013), 6th International Workshop, August 25, 2013, Istanbul, Turkey", "comments": "Proceedings of Answer Set Programming and Other Computing Paradigms\n  (ASPOCP 2013), 6th International Workshop, August 25, 2013, Istanbul, Turkey", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the papers presented at the sixth workshop on Answer Set\nProgramming and Other Computing Paradigms (ASPOCP 2013) held on August 25th,\n2013 in Istanbul, co-located with the 29th International Conference on Logic\nProgramming (ICLP 2013). It thus continues a series of previous events\nco-located with ICLP, aiming at facilitating the discussion about crossing the\nboundaries of current ASP techniques in theory, solving, and applications, in\ncombination with or inspired by other computing paradigms.\n", "versions": [{"version": "v1", "created": "Sat, 28 Dec 2013 11:08:35 GMT"}], "update_date": "2013-12-31", "authors_parsed": [["Fink", "Michael", ""], ["Lierler", "Yuliya", ""]]}, {"id": "1312.7485", "submitter": "Elias Bareinboim", "authors": "Elias Bareinboim and Judea Pearl", "title": "A General Algorithm for Deciding Transportability of Experimental\n  Results", "comments": null, "journal-ref": "Journal of Causal Inference, 2013; 1(1): 107-134", "doi": "10.1515/jci-2012-0004", "report-no": null, "categories": "cs.AI stat.ME stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generalizing empirical findings to new environments, settings, or populations\nis essential in most scientific explorations. This article treats a particular\nproblem of generalizability, called \"transportability\", defined as a license to\ntransfer information learned in experimental studies to a different population,\non which only observational studies can be conducted. Given a set of\nassumptions concerning commonalities and differences between the two\npopulations, Pearl and Bareinboim (2011) derived sufficient conditions that\npermit such transfer to take place. This article summarizes their findings and\nsupplements them with an effective procedure for deciding when and how\ntransportability is feasible. It establishes a necessary and sufficient\ncondition for deciding when causal effects in the target population are\nestimable from both the statistical information available and the causal\ninformation transferred from the experiments. The article further provides a\ncomplete algorithm for computing the transport formula, that is, a way of\ncombining observational and experimental information to synthesize bias-free\nestimate of the desired causal relation. Finally, the article examines the\ndifferences between transportability and other variants of generalizability.\n", "versions": [{"version": "v1", "created": "Sun, 29 Dec 2013 00:54:47 GMT"}], "update_date": "2014-01-03", "authors_parsed": [["Bareinboim", "Elias", ""], ["Pearl", "Judea", ""]]}, {"id": "1312.7606", "submitter": "Sergio Valcarcel Macua", "authors": "Sergio Valcarcel Macua, Jianshu Chen, Santiago Zazo, Ali H. Sayed", "title": "Distributed Policy Evaluation Under Multiple Behavior Strategies", "comments": "36 pages, 4 figures, accepted for publication on IEEE Transactions on\n  Automatic Control", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.DC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We apply diffusion strategies to develop a fully-distributed cooperative\nreinforcement learning algorithm in which agents in a network communicate only\nwith their immediate neighbors to improve predictions about their environment.\nThe algorithm can also be applied to off-policy learning, meaning that the\nagents can predict the response to a behavior different from the actual\npolicies they are following. The proposed distributed strategy is efficient,\nwith linear complexity in both computation time and memory footprint. We\nprovide a mean-square-error performance analysis and establish convergence\nunder constant step-size updates, which endow the network with continuous\nlearning capabilities. The results show a clear gain from cooperation: when the\nindividual agents can estimate the solution, cooperation increases stability\nand reduces bias and variance of the prediction error; but, more importantly,\nthe network is able to approach the optimal solution even when none of the\nindividual agents can (e.g., when the individual behavior policies restrict\neach agent to sample a small portion of the state space).\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2013 00:16:34 GMT"}, {"version": "v2", "created": "Wed, 5 Nov 2014 19:50:03 GMT"}], "update_date": "2014-11-06", "authors_parsed": [["Macua", "Sergio Valcarcel", ""], ["Chen", "Jianshu", ""], ["Zazo", "Santiago", ""], ["Sayed", "Ali H.", ""]]}, {"id": "1312.7740", "submitter": "Reza Mortezapour", "authors": "Reza Mortezapour, Mehdi Afzali", "title": "Assessment of Customer Credit through Combined Clustering of Artificial\n  Neural Networks, Genetics Algorithm and Bayesian Probabilities", "comments": "5 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Today, with respect to the increasing growth of demand to get credit from the\ncustomers of banks and finance and credit institutions, using an effective and\nefficient method to decrease the risk of non-repayment of credit given is very\nnecessary. Assessment of customers' credit is one of the most important and the\nmost essential duties of banks and institutions, and if an error occurs in this\nfield, it would leads to the great losses for banks and institutions. Thus,\nusing the predicting computer systems has been significantly progressed in\nrecent decades. The data that are provided to the credit institutions' managers\nhelp them to make a straight decision for giving the credit or not-giving it.\nIn this paper, we will assess the customer credit through a combined\nclassification using artificial neural networks, genetics algorithm and\nBayesian probabilities simultaneously, and the results obtained from three\nmethods mentioned above would be used to achieve an appropriate and final\nresult. We use the K_folds cross validation test in order to assess the method\nand finally, we compare the proposed method with the methods such as\nClustering-Launched Classification (CLC), Support Vector Machine (SVM) as well\nas GA+SVM where the genetics algorithm has been used to improve them.\n", "versions": [{"version": "v1", "created": "Mon, 30 Dec 2013 15:31:25 GMT"}], "update_date": "2013-12-31", "authors_parsed": [["Mortezapour", "Reza", ""], ["Afzali", "Mehdi", ""]]}]