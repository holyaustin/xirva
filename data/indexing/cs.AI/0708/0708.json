[{"id": "0708.0505", "submitter": "Luca Di Gaspero PhD", "authors": "Luca Di Gaspero, Andrea Roli", "title": "A preliminary analysis on metaheuristics methods applied to the\n  Haplotype Inference Problem", "comments": "22 pages, 4 figures Technical Report: DEIS - Alma Mater Studiorum,\n  University of Bologna no. DEIS-LIA-006-07", "journal-ref": null, "doi": null, "report-no": "DEIS-LIA-006-07", "categories": "cs.AI cs.CE cs.DM q-bio.QM", "license": null, "abstract": "  Haplotype Inference is a challenging problem in bioinformatics that consists\nin inferring the basic genetic constitution of diploid organisms on the basis\nof their genotype. This information allows researchers to perform association\nstudies for the genetic variants involved in diseases and the individual\nresponses to therapeutic agents.\n  A notable approach to the problem is to encode it as a combinatorial problem\n(under certain hypotheses, such as the pure parsimony criterion) and to solve\nit using off-the-shelf combinatorial optimization techniques. The main methods\napplied to Haplotype Inference are either simple greedy heuristic or exact\nmethods (Integer Linear Programming, Semidefinite Programming, SAT encoding)\nthat, at present, are adequate only for moderate size instances.\n  We believe that metaheuristic and hybrid approaches could provide a better\nscalability. Moreover, metaheuristics can be very easily combined with problem\nspecific heuristics and they can also be integrated with tree-based search\ntechniques, thus providing a promising framework for hybrid systems in which a\ngood trade-off between effectiveness and efficiency can be reached.\n  In this paper we illustrate a feasibility study of the approach and discuss\nsome relevant design issues, such as modeling and design of approximate solvers\nthat combine constructive heuristics, local search-based improvement strategies\nand learning mechanisms. Besides the relevance of the Haplotype Inference\nproblem itself, this preliminary analysis is also an interesting case study\nbecause the formulation of the problem poses some challenges in modeling and\nhybrid metaheuristic solver design that can be generalized to other problems.\n", "versions": [{"version": "v1", "created": "Fri, 3 Aug 2007 12:49:21 GMT"}], "update_date": "2007-08-06", "authors_parsed": [["Di Gaspero", "Luca", ""], ["Roli", "Andrea", ""]]}, {"id": "0708.0927", "submitter": "Emanuel Diamant", "authors": "Emanuel Diamant", "title": "Modeling Visual Information Processing in Brain: A Computer Vision Point\n  of View and Approach", "comments": "That is a journal version of a paper that in 2007 has been submitted\n  to 15 computer vision conferences and was discarded by 11 of them", "journal-ref": "Signal Processing: Image Communication, vol. 22, issue 6, pp.\n  583-590, July 2007", "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": null, "abstract": "  We live in the Information Age, and information has become a critically\nimportant component of our life. The success of the Internet made huge amounts\nof it easily available and accessible to everyone. To keep the flow of this\ninformation manageable, means for its faultless circulation and effective\nhandling have become urgently required. Considerable research efforts are\ndedicated today to address this necessity, but they are seriously hampered by\nthe lack of a common agreement about \"What is information?\" In particular, what\nis \"visual information\" - human's primary input from the surrounding world. The\nproblem is further aggravated by a long-lasting stance borrowed from the\nbiological vision research that assumes human-like information processing as an\nenigmatic mix of perceptual and cognitive vision faculties. I am trying to find\na remedy for this bizarre situation. Relying on a new definition of\n\"information\", which can be derived from Kolmogorov's compexity theory and\nChaitin's notion of algorithmic information, I propose a unifying framework for\nvisual information processing, which explicitly accounts for the perceptual and\ncognitive image processing peculiarities. I believe that this framework will be\nuseful to overcome the difficulties that are impeding our attempts to develop\nthe right model of human-like intelligent image processing.\n", "versions": [{"version": "v1", "created": "Tue, 7 Aug 2007 11:16:15 GMT"}], "update_date": "2007-08-08", "authors_parsed": [["Diamant", "Emanuel", ""]]}, {"id": "0708.1150", "submitter": "Marko Antonio Rodriguez", "authors": "Marko A. Rodriguez, Johah Bollen, Herbert Van de Sompel", "title": "A Practical Ontology for the Large-Scale Modeling of Scholarly Artifacts\n  and their Usage", "comments": null, "journal-ref": "Proceedings of the IEEE/ACM Joint Conference on Digital Libraries\n  (JCDL'07), pp. 278-287, 2007", "doi": "10.1145/1255175.1255229", "report-no": null, "categories": "cs.DL cs.AI", "license": null, "abstract": "  The large-scale analysis of scholarly artifact usage is constrained primarily\nby current practices in usage data archiving, privacy issues concerned with the\ndissemination of usage data, and the lack of a practical ontology for modeling\nthe usage domain. As a remedy to the third constraint, this article presents a\nscholarly ontology that was engineered to represent those classes for which\nlarge-scale bibliographic and usage data exists, supports usage research, and\nwhose instantiation is scalable to the order of 50 million articles along with\ntheir associated artifacts (e.g. authors and journals) and an accompanying 1\nbillion usage events. The real world instantiation of the presented abstract\nontology is a semantic network model of the scholarly community which lends the\nscholarly process to statistical analysis and computational support. We present\nthe ontology, discuss its instantiation, and provide some example inference\nrules for calculating various scholarly artifact metrics.\n", "versions": [{"version": "v1", "created": "Wed, 8 Aug 2007 17:06:55 GMT"}], "update_date": "2007-08-09", "authors_parsed": [["Rodriguez", "Marko A.", ""], ["Bollen", "Johah", ""], ["Van de Sompel", "Herbert", ""]]}, {"id": "0708.1527", "submitter": "Stasinos Konstantopoulos", "authors": "Stasinos Konstantopoulos", "title": "A Data-Parallel Version of Aleph", "comments": "Proceedings of Parallel and Distributed Computing for Machine\n  Learning workshop, held in conjunction with the 14th European Conference on\n  Machine Learning. Cavtat, Croatia, 2003", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DC", "license": null, "abstract": "  This is to present work on modifying the Aleph ILP system so that it\nevaluates the hypothesised clauses in parallel by distributing the data-set\namong the nodes of a parallel or distributed machine. The paper briefly\ndiscusses MPI, the interface used to access message- passing libraries for\nparallel computers and clusters. It then proceeds to describe an extension of\nYAP Prolog with an MPI interface and an implementation of data-parallel clause\nevaluation for Aleph through this interface. The paper concludes by testing the\ndata-parallel Aleph on artificially constructed data-sets.\n", "versions": [{"version": "v1", "created": "Fri, 10 Aug 2007 23:32:16 GMT"}], "update_date": "2007-08-14", "authors_parsed": [["Konstantopoulos", "Stasinos", ""]]}, {"id": "0708.1964", "submitter": "Mihai Oltean", "authors": "Mihai Oltean, Oana Muntean", "title": "Solving the subset-sum problem with a light-based device", "comments": "14 pages, 6 figures, Natural Computing, 2007", "journal-ref": "Natural Computing, Springer-Verlag, Vol 8, Issue 2, pp. 321-331,\n  2009", "doi": "10.1007/s11047-007-9059-3", "report-no": null, "categories": "cs.AR cs.AI cs.DC", "license": null, "abstract": "  We propose a special computational device which uses light rays for solving\nthe subset-sum problem. The device has a graph-like representation and the\nlight is traversing it by following the routes given by the connections between\nnodes. The nodes are connected by arcs in a special way which lets us to\ngenerate all possible subsets of the given set. To each arc we assign either a\nnumber from the given set or a predefined constant. When the light is passing\nthrough an arc it is delayed by the amount of time indicated by the number\nplaced in that arc. At the destination node we will check if there is a ray\nwhose total delay is equal to the target value of the subset sum problem (plus\nsome constants).\n", "versions": [{"version": "v1", "created": "Tue, 14 Aug 2007 21:46:32 GMT"}], "update_date": "2015-09-11", "authors_parsed": [["Oltean", "Mihai", ""], ["Muntean", "Oana", ""]]}, {"id": "0708.2303", "submitter": "W Saba", "authors": "Walid S. Saba", "title": "Compositional Semantics Grounded in Commonsense Metaphysics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": null, "abstract": "  We argue for a compositional semantics grounded in a strongly typed ontology\nthat reflects our commonsense view of the world and the way we talk about it in\nordinary language. Assuming the existence of such a structure, we show that the\nsemantics of various natural language phenomena may become nearly trivial.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2007 01:15:11 GMT"}, {"version": "v2", "created": "Fri, 31 Aug 2007 17:48:14 GMT"}], "update_date": "2009-09-29", "authors_parsed": [["Saba", "Walid S.", ""]]}, {"id": "0708.2432", "submitter": "Oliver Knill", "authors": "Oliver Knill and Jose Ramirez-Herran", "title": "A structure from motion inequality", "comments": "15 pages, 22 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": null, "abstract": "  We state an elementary inequality for the structure from motion problem for m\ncameras and n points. This structure from motion inequality relates space\ndimension, camera parameter dimension, the number of cameras and number points\nand global symmetry properties and provides a rigorous criterion for which\nreconstruction is not possible with probability 1. Mathematically the\ninequality is based on Frobenius theorem which is a geometric incarnation of\nthe fundamental theorem of linear algebra. The paper also provides a general\nmathematical formalism for the structure from motion problem. It includes the\nsituation the points can move while the camera takes the pictures.\n", "versions": [{"version": "v1", "created": "Sat, 18 Aug 2007 14:36:28 GMT"}], "update_date": "2007-08-21", "authors_parsed": [["Knill", "Oliver", ""], ["Ramirez-Herran", "Jose", ""]]}, {"id": "0708.2438", "submitter": "Oliver Knill", "authors": "Oliver Knill and Jose Ramirez-Herran", "title": "On Ullman's theorem in computer vision", "comments": "16 pages, 13 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": null, "abstract": "  Both in the plane and in space, we invert the nonlinear Ullman transformation\nfor 3 points and 3 orthographic cameras. While Ullman's theorem assures a\nunique reconstruction modulo a reflection for 3 cameras and 4 points, we find a\nlocally unique reconstruction for 3 cameras and 3 points. Explicit\nreconstruction formulas allow to decide whether picture data of three cameras\nseeing three points can be realized as a point-camera configuration.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2007 21:36:08 GMT"}], "update_date": "2007-08-21", "authors_parsed": [["Knill", "Oliver", ""], ["Ramirez-Herran", "Jose", ""]]}, {"id": "0708.2442", "submitter": "Oliver Knill", "authors": "Oliver Knill and Jose Ramirez-Herran", "title": "Space and camera path reconstruction for omni-directional vision", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": null, "abstract": "  In this paper, we address the inverse problem of reconstructing a scene as\nwell as the camera motion from the image sequence taken by an omni-directional\ncamera. Our structure from motion results give sharp conditions under which the\nreconstruction is unique. For example, if there are three points in general\nposition and three omni-directional cameras in general position, a unique\nreconstruction is possible up to a similarity. We then look at the\nreconstruction problem with m cameras and n points, where n and m can be large\nand the over-determined system is solved by least square methods. The\nreconstruction is robust and generalizes to the case of a dynamic environment\nwhere landmarks can move during the movie capture. Possible applications of the\nresult are computer assisted scene reconstruction, 3D scanning, autonomous\nrobot navigation, medical tomography and city reconstructions.\n", "versions": [{"version": "v1", "created": "Fri, 17 Aug 2007 21:53:41 GMT"}], "update_date": "2007-08-21", "authors_parsed": [["Knill", "Oliver", ""], ["Ramirez-Herran", "Jose", ""]]}, {"id": "0708.4170", "submitter": "Paolo Liberatore", "authors": "Paolo Liberatore", "title": "Raising a Hardness Result", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC cs.LO", "license": null, "abstract": "  This article presents a technique for proving problems hard for classes of\nthe polynomial hierarchy or for PSPACE. The rationale of this technique is that\nsome problem restrictions are able to simulate existential or universal\nquantifiers. If this is the case, reductions from Quantified Boolean Formulae\n(QBF) to these restrictions can be transformed into reductions from QBFs having\none more quantifier in the front. This means that a proof of hardness of a\nproblem at level n in the polynomial hierarchy can be split into n separate\nproofs, which may be simpler than a proof directly showing a reduction from a\nclass of QBFs to the considered problem.\n", "versions": [{"version": "v1", "created": "Thu, 30 Aug 2007 14:42:50 GMT"}], "update_date": "2007-08-31", "authors_parsed": [["Liberatore", "Paolo", ""]]}, {"id": "0708.4311", "submitter": "Juergen Schmidhuber", "authors": "Juergen Schmidhuber", "title": "2006: Celebrating 75 years of AI - History and Outlook: the Next 25\n  Years", "comments": "14 pages; preprint of invited contribution to the Proceedings of the\n  ``50th Anniversary Summit of Artificial Intelligence'' at Monte Verita,\n  Ascona, Switzerland, 9-14 July 2006", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": null, "abstract": "  When Kurt Goedel layed the foundations of theoretical computer science in\n1931, he also introduced essential concepts of the theory of Artificial\nIntelligence (AI). Although much of subsequent AI research has focused on\nheuristics, which still play a major role in many practical AI applications, in\nthe new millennium AI theory has finally become a full-fledged formal science,\nwith important optimality results for embodied agents living in unknown\nenvironments, obtained through a combination of theory a la Goedel and\nprobability theory. Here we look back at important milestones of AI history,\nmention essential recent results, and speculate about what we may expect from\nthe next 25 years, emphasizing the significance of the ongoing dramatic\nhardware speedups, and discussing Goedel-inspired, self-referential,\nself-improving universal problem solvers.\n", "versions": [{"version": "v1", "created": "Fri, 31 Aug 2007 11:12:26 GMT"}], "update_date": "2007-09-03", "authors_parsed": [["Schmidhuber", "Juergen", ""]]}]