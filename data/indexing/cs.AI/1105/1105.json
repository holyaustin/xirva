[{"id": "1105.0167", "submitter": "Gang Niu", "authors": "Gang Niu, Bo Dai, Makoto Yamada and Masashi Sugiyama", "title": "SERAPH: Semi-supervised Metric Learning Paradigm with Hyper Sparsity", "comments": "The same paper has been submitted to arXiv by ICML 2012. See\n  http://arxiv.org/abs/1206.4614", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a general information-theoretic approach called Seraph\n(SEmi-supervised metRic leArning Paradigm with Hyper-sparsity) for metric\nlearning that does not rely upon the manifold assumption. Given the probability\nparameterized by a Mahalanobis distance, we maximize the entropy of that\nprobability on labeled data and minimize it on unlabeled data following entropy\nregularization, which allows the supervised and unsupervised parts to be\nintegrated in a natural and meaningful way. Furthermore, Seraph is regularized\nby encouraging a low-rank projection induced from the metric. The optimization\nof Seraph is solved efficiently and stably by an EM-like scheme with the\nanalytical E-Step and convex M-Step. Experiments demonstrate that Seraph\ncompares favorably with many well-known global and local metric learning\nmethods.\n", "versions": [{"version": "v1", "created": "Sun, 1 May 2011 13:07:51 GMT"}, {"version": "v2", "created": "Mon, 3 Oct 2011 02:26:05 GMT"}, {"version": "v3", "created": "Thu, 15 Nov 2012 07:05:14 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Niu", "Gang", ""], ["Dai", "Bo", ""], ["Yamada", "Makoto", ""], ["Sugiyama", "Masashi", ""]]}, {"id": "1105.0288", "submitter": "Martin Slota", "authors": "Martin Slota and Jo\\~ao Leite and Terrance Swift", "title": "Splitting and Updating Hybrid Knowledge Bases (Extended Version)", "comments": "64 pages; extended version of the paper accepted for ICLP 2011", "journal-ref": "Theory and Practice of Logic Programming, 11(4-5), 801-819, 2011", "doi": "10.1017/S1471068411000317", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the years, nonmonotonic rules have proven to be a very expressive and\nuseful knowledge representation paradigm. They have recently been used to\ncomplement the expressive power of Description Logics (DLs), leading to the\nstudy of integrative formal frameworks, generally referred to as hybrid\nknowledge bases, where both DL axioms and rules can be used to represent\nknowledge. The need to use these hybrid knowledge bases in dynamic domains has\ncalled for the development of update operators, which, given the substantially\ndifferent way Description Logics and rules are usually updated, has turned out\nto be an extremely difficult task.\n  In [SL10], a first step towards addressing this problem was taken, and an\nupdate operator for hybrid knowledge bases was proposed. Despite its\nsignificance -- not only for being the first update operator for hybrid\nknowledge bases in the literature, but also because it has some applications -\nthis operator was defined for a restricted class of problems where only the\nABox was allowed to change, which considerably diminished its applicability.\nMany applications that use hybrid knowledge bases in dynamic scenarios require\nboth DL axioms and rules to be updated.\n  In this paper, motivated by real world applications, we introduce an update\noperator for a large class of hybrid knowledge bases where both the DL\ncomponent as well as the rule component are allowed to dynamically change. We\nintroduce splitting sequences and splitting theorem for hybrid knowledge bases,\nuse them to define a modular update semantics, investigate its basic\nproperties, and illustrate its use on a realistic example about cargo imports.\n", "versions": [{"version": "v1", "created": "Mon, 2 May 2011 10:12:59 GMT"}], "update_date": "2011-07-27", "authors_parsed": [["Slota", "Martin", ""], ["Leite", "Jo\u00e3o", ""], ["Swift", "Terrance", ""]]}, {"id": "1105.0650", "submitter": "Miroslaw Truszczynski", "authors": "Yuliya Lierler and Miroslaw Truszczynski", "title": "Transition Systems for Model Generators - A Unifying Approach", "comments": "30 pages; Accepted for presentation at ICLP 2011 and for publication\n  in Theory and Practice of Logic Programming; contains the appendix with\n  proofs", "journal-ref": "Theory and Practice of Logic Programming, volume 11, issue 4-5, pp\n  629 - 646, 2011", "doi": "10.1017/S1471068411000214", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A fundamental task for propositional logic is to compute models of\npropositional formulas. Programs developed for this task are called\nsatisfiability solvers. We show that transition systems introduced by\nNieuwenhuis, Oliveras, and Tinelli to model and analyze satisfiability solvers\ncan be adapted for solvers developed for two other propositional formalisms:\nlogic programming under the answer-set semantics, and the logic PC(ID). We show\nthat in each case the task of computing models can be seen as \"satisfiability\nmodulo answer-set programming,\" where the goal is to find a model of a theory\nthat also is an answer set of a certain program. The unifying perspective we\ndevelop shows, in particular, that solvers CLASP and MINISATID are closely\nrelated despite being developed for different formalisms, one for answer-set\nprogramming and the latter for the logic PC(ID).\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2011 18:29:58 GMT"}], "update_date": "2014-10-08", "authors_parsed": [["Lierler", "Yuliya", ""], ["Truszczynski", "Miroslaw", ""]]}, {"id": "1105.0707", "submitter": "Rajesh Chitnis", "authors": "Rajesh Chitnis, MohammadTaghi Hajiaghayi, Vahid Liaghat", "title": "Parameterized Complexity of Problems in Coalitional Resource Games", "comments": "This is the full version of a paper that will appear in the\n  proceedings of AAAI 2011", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Coalition formation is a key topic in multi-agent systems. Coalitions enable\nagents to achieve goals that they may not have been able to achieve on their\nown. Previous work has shown problems in coalitional games to be\ncomputationally hard. Wooldridge and Dunne (Artificial Intelligence 2006)\nstudied the classical computational complexity of several natural decision\nproblems in Coalitional Resource Games (CRG) - games in which each agent is\nendowed with a set of resources and coalitions can bring about a set of goals\nif they are collectively endowed with the necessary amount of resources. The\ninput of coalitional resource games bundles together several elements, e.g.,\nthe agent set Ag, the goal set G, the resource set R, etc. Shrot, Aumann and\nKraus (AAMAS 2009) examine coalition formation problems in the CRG model using\nthe theory of Parameterized Complexity. Their refined analysis shows that not\nall parts of input act equal - some instances of the problem are indeed\ntractable while others still remain intractable.\n  We answer an important question left open by Shrot, Aumann and Kraus by\nshowing that the SC Problem (checking whether a Coalition is Successful) is\nW[1]-hard when parameterized by the size of the coalition. Then via a single\ntheme of reduction from SC, we are able to show that various problems related\nto resources, resource bounds and resource conflicts introduced by Wooldridge\net al are 1. W[1]-hard or co-W[1]-hard when parameterized by the size of the\ncoalition. 2. para-NP-hard or co-para-NP-hard when parameterized by |R|. 3. FPT\nwhen parameterized by either |G| or |Ag|+|R|.\n", "versions": [{"version": "v1", "created": "Tue, 3 May 2011 23:03:52 GMT"}], "update_date": "2011-05-05", "authors_parsed": [["Chitnis", "Rajesh", ""], ["Hajiaghayi", "MohammadTaghi", ""], ["Liaghat", "Vahid", ""]]}, {"id": "1105.0728", "submitter": "Zhiwei Qin", "authors": "Zhiwei Qin and Donald Goldfarb", "title": "Structured Sparsity via Alternating Direction Methods", "comments": null, "journal-ref": "Journal of Machine Learning Research 13 (2012) 1435-1468", "doi": null, "report-no": null, "categories": "math.OC cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a class of sparse learning problems in high dimensional feature\nspace regularized by a structured sparsity-inducing norm which incorporates\nprior knowledge of the group structure of the features. Such problems often\npose a considerable challenge to optimization algorithms due to the\nnon-smoothness and non-separability of the regularization term. In this paper,\nwe focus on two commonly adopted sparsity-inducing regularization terms, the\noverlapping Group Lasso penalty $l_1/l_2$-norm and the $l_1/l_\\infty$-norm. We\npropose a unified framework based on the augmented Lagrangian method, under\nwhich problems with both types of regularization and their variants can be\nefficiently solved. As the core building-block of this framework, we develop\nnew algorithms using an alternating partial-linearization/splitting technique,\nand we prove that the accelerated versions of these algorithms require\n$O(\\frac{1}{\\sqrt{\\epsilon}})$ iterations to obtain an $\\epsilon$-optimal\nsolution. To demonstrate the efficiency and relevance of our algorithms, we\ntest them on a collection of data sets and apply them to two real-world\nproblems to compare the relative merits of the two norms.\n", "versions": [{"version": "v1", "created": "Wed, 4 May 2011 03:02:19 GMT"}, {"version": "v2", "created": "Thu, 15 Dec 2011 03:29:46 GMT"}], "update_date": "2014-02-11", "authors_parsed": [["Qin", "Zhiwei", ""], ["Goldfarb", "Donald", ""]]}, {"id": "1105.0972", "submitter": "Zhixiang Eddie Xu", "authors": "Zhixiang Eddie Xu, Kilian Q. Weinberger, Fei Sha", "title": "Rapid Feature Learning with Stacked Linear Denoisers", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We investigate unsupervised pre-training of deep architectures as feature\ngenerators for \"shallow\" classifiers. Stacked Denoising Autoencoders (SdA),\nwhen used as feature pre-processing tools for SVM classification, can lead to\nsignificant improvements in accuracy - however, at the price of a substantial\nincrease in computational cost. In this paper we create a simple algorithm\nwhich mimics the layer by layer training of SdAs. However, in contrast to SdAs,\nour algorithm requires no training through gradient descent as the parameters\ncan be computed in closed-form. It can be implemented in less than 20 lines of\nMATLABTMand reduces the computation time from several hours to mere seconds. We\nshow that our feature transformation reliably improves the results of SVM\nclassification significantly on all our data sets - often outperforming SdAs\nand even deep neural networks in three out of four deep learning benchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2011 04:02:35 GMT"}], "update_date": "2012-08-17", "authors_parsed": [["Xu", "Zhixiang Eddie", ""], ["Weinberger", "Kilian Q.", ""], ["Sha", "Fei", ""]]}, {"id": "1105.0974", "submitter": "Seyed Salim Tabatabaei", "authors": "Seyed Salim Tabatabaei and Mark Coates and Michael Rabbat", "title": "GANC: Greedy Agglomerative Normalized Cut", "comments": "Submitted to Pattern Recognition. 27 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes a graph clustering algorithm that aims to minimize the\nnormalized cut criterion and has a model order selection procedure. The\nperformance of the proposed algorithm is comparable to spectral approaches in\nterms of minimizing normalized cut. However, unlike spectral approaches, the\nproposed algorithm scales to graphs with millions of nodes and edges. The\nalgorithm consists of three components that are processed sequentially: a\ngreedy agglomerative hierarchical clustering procedure, model order selection,\nand a local refinement.\n  For a graph of n nodes and O(n) edges, the computational complexity of the\nalgorithm is O(n log^2 n), a major improvement over the O(n^3) complexity of\nspectral methods. Experiments are performed on real and synthetic networks to\ndemonstrate the scalability of the proposed approach, the effectiveness of the\nmodel order selection procedure, and the performance of the proposed algorithm\nin terms of minimizing the normalized cut metric.\n", "versions": [{"version": "v1", "created": "Thu, 5 May 2011 04:55:53 GMT"}], "update_date": "2011-05-06", "authors_parsed": [["Tabatabaei", "Seyed Salim", ""], ["Coates", "Mark", ""], ["Rabbat", "Michael", ""]]}, {"id": "1105.1247", "submitter": "Manojit Chattopadhyay Mr.", "authors": "Manojit Chattopadhyay (Pailan College of Management & Technology),\n  Surajit Chattopadhyay (Pailan College of Management & Technology), Pranab K.\n  Dan (West Bengal University of Technology)", "title": "Machine-Part cell formation through visual decipherable clustering of\n  Self Organizing Map", "comments": "18 pages,3 table, 4 figures", "journal-ref": "The International Journal of Advanced Manufacturing Technology,\n  2011, Volume 52, Numbers 9-12, 1019-1030", "doi": "10.1007/s00170-010-2802-4", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine-part cell formation is used in cellular manufacturing in order to\nprocess a large variety, quality, lower work in process levels, reducing\nmanufacturing lead-time and customer response time while retaining flexibility\nfor new products. This paper presents a new and novel approach for obtaining\nmachine cells and part families. In the cellular manufacturing the fundamental\nproblem is the formation of part families and machine cells. The present paper\ndeals with the Self Organising Map (SOM) method an unsupervised learning\nalgorithm in Artificial Intelligence, and has been used as a visually\ndecipherable clustering tool of machine-part cell formation. The objective of\nthe paper is to cluster the binary machine-part matrix through visually\ndecipherable cluster of SOM color-coding and labelling via the SOM map nodes in\nsuch a way that the part families are processed in that machine cells. The\nUmatrix, component plane, principal component projection, scatter plot and\nhistogram of SOM have been reported in the present work for the successful\nvisualization of the machine-part cell formation. Computational result with the\nproposed algorithm on a set of group technology problems available in the\nliterature is also presented. The proposed SOM approach produced solutions with\na grouping efficacy that is at least as good as any results earlier reported in\nthe literature and improved the grouping efficacy for 70% of the problems and\nfound immensely useful to both industry practitioners and researchers.\n", "versions": [{"version": "v1", "created": "Fri, 6 May 2011 09:27:49 GMT"}], "update_date": "2011-05-09", "authors_parsed": [["Chattopadhyay", "Manojit", "", "Pailan College of Management & Technology"], ["Chattopadhyay", "Surajit", "", "Pailan College of Management & Technology"], ["Dan", "Pranab K.", "", "West Bengal University of Technology"]]}, {"id": "1105.1386", "submitter": "Marc Timme", "authors": "Silke Steingrube, Marc Timme, Florentin Woergoetter and Poramate\n  Manoonpong", "title": "Self-organized adaptation of a simple neural circuit enables complex\n  robot behaviour", "comments": "16 pages, non-final version, for final see Nature Physics homepage", "journal-ref": "Nature Phys. 6:224 (2010)", "doi": "10.1038/nphys1860", "report-no": null, "categories": "cond-mat.dis-nn cs.AI cs.RO nlin.CD q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Controlling sensori-motor systems in higher animals or complex robots is a\nchallenging combinatorial problem, because many sensory signals need to be\nsimultaneously coordinated into a broad behavioural spectrum. To rapidly\ninteract with the environment, this control needs to be fast and adaptive.\nCurrent robotic solutions operate with limited autonomy and are mostly\nrestricted to few behavioural patterns. Here we introduce chaos control as a\nnew strategy to generate complex behaviour of an autonomous robot. In the\npresented system, 18 sensors drive 18 motors via a simple neural control\ncircuit, thereby generating 11 basic behavioural patterns (e.g., orienting,\ntaxis, self-protection, various gaits) and their combinations. The control\nsignal quickly and reversibly adapts to new situations and additionally enables\nlearning and synaptic long-term storage of behaviourally useful motor\nresponses. Thus, such neural control provides a powerful yet simple way to\nself-organize versatile behaviours in autonomous agents with many degrees of\nfreedom.\n", "versions": [{"version": "v1", "created": "Fri, 6 May 2011 21:11:49 GMT"}], "update_date": "2011-05-10", "authors_parsed": [["Steingrube", "Silke", ""], ["Timme", "Marc", ""], ["Woergoetter", "Florentin", ""], ["Manoonpong", "Poramate", ""]]}, {"id": "1105.1436", "submitter": "Jingchao Chen", "authors": "Jingchao Chen", "title": "Solving Rubik's Cube Using SAT Solvers", "comments": "13 pages", "journal-ref": "SPA 2011: SAT for Practical Applications", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Rubik's Cube is an easily-understood puzzle, which is originally called the\n\"magic cube\". It is a well-known planning problem, which has been studied for a\nlong time. Yet many simple properties remain unknown. This paper studies\nwhether modern SAT solvers are applicable to this puzzle. To our best\nknowledge, we are the first to translate Rubik's Cube to a SAT problem. To\nreduce the number of variables and clauses needed for the encoding, we replace\na naive approach of 6 Boolean variables to represent each color on each facelet\nwith a new approach of 3 or 2 Boolean variables. In order to be able to solve\nquickly Rubik's Cube, we replace the direct encoding of 18 turns with the layer\nencoding of 18-subtype turns based on 6-type turns. To speed up the solving\nfurther, we encode some properties of two-phase algorithm as an additional\nconstraint, and restrict some move sequences by adding some constraint clauses.\nUsing only efficient encoding cannot solve this puzzle. For this reason, we\nimprove the existing SAT solvers, and develop a new SAT solver based on\nPrecoSAT, though it is suited only for Rubik's Cube. The new SAT solver\nreplaces the lookahead solving strategy with an ALO (\\emph{at-least-one})\nsolving strategy, and decomposes the original problem into sub-problems. Each\nsub-problem is solved by PrecoSAT. The empirical results demonstrate both our\nSAT translation and new solving technique are efficient. Without the efficient\nSAT encoding and the new solving technique, Rubik's Cube will not be able to be\nsolved still by any SAT solver. Using the improved SAT solver, we can find\nalways a solution of length 20 in a reasonable time. Although our solver is\nslower than Kociemba's algorithm using lookup tables, but does not require a\nhuge lookup table.\n", "versions": [{"version": "v1", "created": "Sat, 7 May 2011 12:07:49 GMT"}], "update_date": "2011-05-10", "authors_parsed": [["Chen", "Jingchao", ""]]}, {"id": "1105.1749", "submitter": "Todd Hester", "authors": "Todd Hester, Michael Quinlan, Peter Stone", "title": "A Real-Time Model-Based Reinforcement Learning Architecture for Robot\n  Control", "comments": "Added a reference Presents a real-time parallel architecture for\n  model-based reinforcement learning methods", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.RO cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement Learning (RL) is a method for learning decision-making tasks\nthat could enable robots to learn and adapt to their situation on-line. For an\nRL algorithm to be practical for robotic control tasks, it must learn in very\nfew actions, while continually taking those actions in real-time. Existing\nmodel-based RL methods learn in relatively few actions, but typically take too\nmuch time between each action for practical on-line learning. In this paper, we\npresent a novel parallel architecture for model-based RL that runs in real-time\nby 1) taking advantage of sample-based approximate planning methods and 2)\nparallelizing the acting, model learning, and planning processes such that the\nacting process is sufficiently fast for typical robot control cycles. We\ndemonstrate that algorithms using this architecture perform nearly as well as\nmethods using the typical sequential architecture when both are given unlimited\ntime, and greatly out-perform these methods on tasks that require real-time\nactions such as controlling an autonomous vehicle.\n", "versions": [{"version": "v1", "created": "Mon, 9 May 2011 18:17:20 GMT"}, {"version": "v2", "created": "Sat, 21 May 2011 14:15:28 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Hester", "Todd", ""], ["Quinlan", "Michael", ""], ["Stone", "Peter", ""]]}, {"id": "1105.1853", "submitter": "Ying Liu", "authors": "Ying Liu, Venkat Chandrasekaran, Animashree Anandkumar, Alan S.\n  Willsky", "title": "Feedback Message Passing for Inference in Gaussian Graphical Models", "comments": "30 pages", "journal-ref": null, "doi": "10.1109/TSP.2012.2195656", "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While loopy belief propagation (LBP) performs reasonably well for inference\nin some Gaussian graphical models with cycles, its performance is\nunsatisfactory for many others. In particular for some models LBP does not\nconverge, and in general when it does converge, the computed variances are\nincorrect (except for cycle-free graphs for which belief propagation (BP) is\nnon-iterative and exact). In this paper we propose {\\em feedback message\npassing} (FMP), a message-passing algorithm that makes use of a special set of\nvertices (called a {\\em feedback vertex set} or {\\em FVS}) whose removal\nresults in a cycle-free graph. In FMP, standard BP is employed several times on\nthe cycle-free subgraph excluding the FVS while a special message-passing\nscheme is used for the nodes in the FVS. The computational complexity of exact\ninference is $O(k^2n)$, where $k$ is the number of feedback nodes, and $n$ is\nthe total number of nodes. When the size of the FVS is very large, FMP is\nintractable. Hence we propose {\\em approximate FMP}, where a pseudo-FVS is used\ninstead of an FVS, and where inference in the non-cycle-free graph obtained by\nremoving the pseudo-FVS is carried out approximately using LBP. We show that,\nwhen approximate FMP converges, it yields exact means and variances on the\npseudo-FVS and exact means throughout the remainder of the graph. We also\nprovide theoretical results on the convergence and accuracy of approximate FMP.\nIn particular, we prove error bounds on variance computation. Based on these\ntheoretical results, we design efficient algorithms to select a pseudo-FVS of\nbounded size. The choice of the pseudo-FVS allows us to explicitly trade off\nbetween efficiency and accuracy. Experimental results show that using a\npseudo-FVS of size no larger than $\\log(n)$, this procedure converges much more\noften, more quickly, and provides more accurate results than LBP on the entire\ngraph.\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2011 04:22:00 GMT"}], "update_date": "2015-05-28", "authors_parsed": [["Liu", "Ying", ""], ["Chandrasekaran", "Venkat", ""], ["Anandkumar", "Animashree", ""], ["Willsky", "Alan S.", ""]]}, {"id": "1105.1929", "submitter": "Fabian Suchanek", "authors": "Fabian Suchanek (INRIA Saclay - Ile de France), Aparna Varde, Richi\n  Nayak (QUT), Pierre Senellart", "title": "The Hidden Web, XML and Semantic Web: A Scientific Data Management\n  Perspective", "comments": "EDBT - Tutorial (2011)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The World Wide Web no longer consists just of HTML pages. Our work sheds\nlight on a number of trends on the Internet that go beyond simple Web pages.\nThe hidden Web provides a wealth of data in semi-structured form, accessible\nthrough Web forms and Web services. These services, as well as numerous other\napplications on the Web, commonly use XML, the eXtensible Markup Language. XML\nhas become the lingua franca of the Internet that allows customized markups to\nbe defined for specific domains. On top of XML, the Semantic Web grows as a\ncommon structured data source. In this work, we first explain each of these\ndevelopments in detail. Using real-world examples from scientific domains of\ngreat interest today, we then demonstrate how these new developments can assist\nthe managing, harvesting, and organization of data on the Web. On the way, we\nalso illustrate the current research avenues in these domains. We believe that\nthis effort would help bridge multiple database tracks, thereby attracting\nresearchers with a view to extend database technology.\n", "versions": [{"version": "v1", "created": "Tue, 10 May 2011 12:33:41 GMT"}], "update_date": "2011-05-11", "authors_parsed": [["Suchanek", "Fabian", "", "INRIA Saclay - Ile de France"], ["Varde", "Aparna", "", "QUT"], ["Nayak", "Richi", "", "QUT"], ["Senellart", "Pierre", ""]]}, {"id": "1105.2790", "submitter": "Alberto Bernacchia Ph.D.", "authors": "Adriano Barra, Alberto Bernacchia, Enrica Santucci, Pierluigi Contucci", "title": "On the equivalence of Hopfield Networks and Boltzmann Machines", "comments": "15 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cond-mat.dis-nn cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A specific type of neural network, the Restricted Boltzmann Machine (RBM), is\nimplemented for classification and feature detection in machine learning. RBM\nis characterized by separate layers of visible and hidden units, which are able\nto learn efficiently a generative model of the observed data. We study a\n\"hybrid\" version of RBM's, in which hidden units are analog and visible units\nare binary, and we show that thermodynamics of visible units are equivalent to\nthose of a Hopfield network, in which the N visible units are the neurons and\nthe P hidden units are the learned patterns. We apply the method of stochastic\nstability to derive the thermodynamics of the model, by considering a formal\nextension of this technique to the case of multiple sets of stored patterns,\nwhich may act as a benchmark for the study of correlated sets. Our results\nimply that simulating the dynamics of a Hopfield network, requiring the update\nof N neurons and the storage of N(N-1)/2 synapses, can be accomplished by a\nhybrid Boltzmann Machine, requiring the update of N+P neurons but the storage\nof only NP synapses. In addition, the well known glass transition of the\nHopfield network has a counterpart in the Boltzmann Machine: It corresponds to\nan optimum criterion for selecting the relative sizes of the hidden and visible\nlayers, resolving the trade-off between flexibility and generality of the\nmodel. The low storage phase of the Hopfield model corresponds to few hidden\nunits and hence a overly constrained RBM, while the spin-glass phase (too many\nhidden units) corresponds to unconstrained RBM prone to overfitting of the\nobserved data.\n", "versions": [{"version": "v1", "created": "Fri, 13 May 2011 17:51:45 GMT"}, {"version": "v2", "created": "Mon, 16 May 2011 14:23:07 GMT"}, {"version": "v3", "created": "Tue, 10 Jan 2012 14:01:17 GMT"}], "update_date": "2012-01-11", "authors_parsed": [["Barra", "Adriano", ""], ["Bernacchia", "Alberto", ""], ["Santucci", "Enrica", ""], ["Contucci", "Pierluigi", ""]]}, {"id": "1105.2813", "submitter": "Wolfgang Gatterbauer", "authors": "Wolfgang Gatterbauer, Dan Suciu", "title": "Optimal Upper and Lower Bounds for Boolean Expressions by Dissociation", "comments": "7 pages, 2 figures; for details see the project page:\n  http://LaPushDB.com/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper develops upper and lower bounds for the probability of Boolean\nexpressions by treating multiple occurrences of variables as independent and\nassigning them new individual probabilities. Our technique generalizes and\nextends the underlying idea of a number of recent approaches which are\nvaryingly called node splitting, variable renaming, variable splitting, or\ndissociation for probabilistic databases. We prove that the probabilities we\nassign to new variables are the best possible in some sense.\n", "versions": [{"version": "v1", "created": "Fri, 13 May 2011 19:41:45 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["Gatterbauer", "Wolfgang", ""], ["Suciu", "Dan", ""]]}, {"id": "1105.2868", "submitter": "Vincent Etter", "authors": "Etter Vincent", "title": "Semantic Vector Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We first present our work in machine translation, during which we used\naligned sentences to train a neural network to embed n-grams of different\nlanguages into an $d$-dimensional space, such that n-grams that are the\ntranslation of each other are close with respect to some metric. Good n-grams\nto n-grams translation results were achieved, but full sentences translation is\nstill problematic. We realized that learning semantics of sentences and\ndocuments was the key for solving a lot of natural language processing\nproblems, and thus moved to the second part of our work: sentence compression.\nWe introduce a flexible neural network architecture for learning embeddings of\nwords and sentences that extract their semantics, propose an efficient\nimplementation in the Torch framework and present embedding results comparable\nto the ones obtained with classical neural language models, while being more\npowerful.\n", "versions": [{"version": "v1", "created": "Sat, 14 May 2011 07:13:25 GMT"}], "update_date": "2011-05-17", "authors_parsed": [["Vincent", "Etter", ""]]}, {"id": "1105.2902", "submitter": "Zahra Forootan Jahromi", "authors": "Zahra Forootan Jahromi, Amir Rajabzadeh and Ali Reza Manashty", "title": "A Multi-Purpose Scenario-based Simulator for Smart House Environments", "comments": null, "journal-ref": "(IJCSIS) International Journal of Computer Science and Information\n  Security, Vol. 9, No. 1, January 2011", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Developing smart house systems has been a great challenge for researchers and\nengineers in this area because of the high cost of implementation and\nevaluation process of these systems, while being very time consuming. Testing a\ndesigned smart house before actually building it is considered as an obstacle\ntowards an efficient smart house project. This is because of the variety of\nsensors, home appliances and devices available for a real smart environment. In\nthis paper, we present the design and implementation of a multi-purpose smart\nhouse simulation system for designing and simulating all aspects of a smart\nhouse environment. This simulator provides the ability to design the house plan\nand different virtual sensors and appliances in a two dimensional model of the\nvirtual house environment. This simulator can connect to any external smart\nhouse remote controlling system, providing evaluation capabilities to their\nsystem much easier than before. It also supports detailed adding of new\nemerging sensors and devices to help maintain its compatibility with future\nsimulation needs. Scenarios can also be defined for testing various possible\ncombinations of device states; so different criteria and variables can be\nsimply evaluated without the need of experimenting on a real environment.\n", "versions": [{"version": "v1", "created": "Sat, 14 May 2011 15:11:00 GMT"}], "update_date": "2011-05-17", "authors_parsed": [["Jahromi", "Zahra Forootan", ""], ["Rajabzadeh", "Amir", ""], ["Manashty", "Ali Reza", ""]]}, {"id": "1105.2943", "submitter": "Rui Wang", "authors": "Rui Wang, Ke Tang", "title": "Feature Selection for MAUC-Oriented Classification Systems", "comments": "A journal length paper", "journal-ref": null, "doi": "10.1016/j.neucom.2012.01.013", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature selection is an important pre-processing step for many pattern\nclassification tasks. Traditionally, feature selection methods are designed to\nobtain a feature subset that can lead to high classification accuracy. However,\nclassification accuracy has recently been shown to be an inappropriate\nperformance metric of classification systems in many cases. Instead, the Area\nUnder the receiver operating characteristic Curve (AUC) and its multi-class\nextension, MAUC, have been proved to be better alternatives. Hence, the target\nof classification system design is gradually shifting from seeking a system\nwith the maximum classification accuracy to obtaining a system with the maximum\nAUC/MAUC. Previous investigations have shown that traditional feature selection\nmethods need to be modified to cope with this new objective. These methods most\noften are restricted to binary classification problems only. In this study, a\nfilter feature selection method, namely MAUC Decomposition based Feature\nSelection (MDFS), is proposed for multi-class classification problems. To the\nbest of our knowledge, MDFS is the first method specifically designed to select\nfeatures for building classification systems with maximum MAUC. Extensive\nempirical results demonstrate the advantage of MDFS over several compared\nfeature selection methods.\n", "versions": [{"version": "v1", "created": "Sun, 15 May 2011 12:35:56 GMT"}], "update_date": "2012-05-03", "authors_parsed": [["Wang", "Rui", ""], ["Tang", "Ke", ""]]}, {"id": "1105.3486", "submitter": "Ladislau B\\\"ol\\\"oni", "authors": "Ladislau B\\\"ol\\\"oni", "title": "Xapagy: a cognitive architecture for narrative reasoning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the Xapagy cognitive architecture: a software system designed to\nperform narrative reasoning. The architecture has been designed from scratch to\nmodel and mimic the activities performed by humans when witnessing, reading,\nrecalling, narrating and talking about stories.\n", "versions": [{"version": "v1", "created": "Tue, 17 May 2011 20:28:31 GMT"}], "update_date": "2015-03-19", "authors_parsed": [["B\u00f6l\u00f6ni", "Ladislau", ""]]}, {"id": "1105.3635", "submitter": "M. C. Garrido", "authors": "M. C. Garrido, P. E. Lopez-de-Teruel, A. Ruiz", "title": "Probabilistic Inference from Arbitrary Uncertainty using Mixtures of\n  Factorized Generalized Gaussians", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 9, pages\n  167-217, 1998", "doi": "10.1613/jair.533", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a general and efficient framework for probabilistic\ninference and learning from arbitrary uncertain information. It exploits the\ncalculation properties of finite mixture models, conjugate families and\nfactorization. Both the joint probability density of the variables and the\nlikelihood function of the (objective or subjective) observation are\napproximated by a special mixture model, in such a way that any desired\nconditional distribution can be directly obtained without numerical\nintegration. We have developed an extended version of the expectation\nmaximization (EM) algorithm to estimate the parameters of mixture models from\nuncertain training examples (indirect observations). As a consequence, any\npiece of exact or uncertain information about both input and output values is\nconsistently handled in the inference and learning stages. This ability,\nextremely useful in certain situations, is not found in most alternative\nmethods. The proposed framework is formally justified from standard\nprobabilistic principles and illustrative examples are provided in the fields\nof nonparametric pattern classification, nonlinear regression and pattern\ncompletion. Finally, experiments on a real application and comparative results\nover standard databases provide empirical evidence of the utility of the method\nin a wide range of applications.\n", "versions": [{"version": "v1", "created": "Wed, 18 May 2011 14:06:49 GMT"}], "update_date": "2011-05-19", "authors_parsed": [["Garrido", "M. C.", ""], ["Lopez-de-Teruel", "P. E.", ""], ["Ruiz", "A.", ""]]}, {"id": "1105.3821", "submitter": "Peter de Blanc", "authors": "Peter de Blanc", "title": "Ontological Crises in Artificial Agents' Value Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Decision-theoretic agents predict and evaluate the results of their actions\nusing a model, or ontology, of their environment. An agent's goal, or utility\nfunction, may also be specified in terms of the states of, or entities within,\nits ontology. If the agent may upgrade or replace its ontology, it faces a\ncrisis: the agent's original goal may not be well-defined with respect to its\nnew ontology. This crisis must be resolved before the agent can make plans\ntowards achieving its goals.\n  We discuss in this paper which sorts of agents will undergo ontological\ncrises and why we may want to create such agents. We present some concrete\nexamples, and argue that a well-defined procedure for resolving ontological\ncrises is needed. We point to some possible approaches to solving this problem,\nand evaluate these methods on our examples.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2011 09:32:46 GMT"}], "update_date": "2011-05-20", "authors_parsed": [["de Blanc", "Peter", ""]]}, {"id": "1105.3833", "submitter": "Eliezer Lozinskii", "authors": "Eliezer L. Lozinskii", "title": "Typical models: minimizing false beliefs", "comments": null, "journal-ref": "Journal of Experimental & Theoretical Artificial Intelligence,\n  vol. 22, no.4, December 2010, 321-340", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A knowledge system S describing a part of real world does in general not\ncontain complete information. Reasoning with incomplete information is prone to\nerrors since any belief derived from S may be false in the present state of the\nworld. A false belief may suggest wrong decisions and lead to harmful actions.\nSo an important goal is to make false beliefs as unlikely as possible. This\nwork introduces the notions of \"typical atoms\" and \"typical models\", and shows\nthat reasoning with typical models minimizes the expected number of false\nbeliefs over all ways of using incomplete information. Various properties of\ntypical models are studied, in particular, correctness and stability of beliefs\nsuggested by typical models, and their connection to oblivious reasoning.\n", "versions": [{"version": "v1", "created": "Thu, 19 May 2011 10:00:39 GMT"}], "update_date": "2011-05-20", "authors_parsed": [["Lozinskii", "Eliezer L.", ""]]}, {"id": "1105.4224", "submitter": "Sanjiang Li", "authors": "Weiming Liu and Sanjiang Li", "title": "On A Semi-Automatic Method for Generating Composition Tables", "comments": "15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Originating from Allen's Interval Algebra, composition-based reasoning has\nbeen widely acknowledged as the most popular reasoning technique in qualitative\nspatial and temporal reasoning. Given a qualitative calculus (i.e. a relation\nmodel), the first thing we should do is to establish its composition table\n(CT). In the past three decades, such work is usually done manually. This is\nundesirable and error-prone, given that the calculus may contain tens or\nhundreds of basic relations. Computing the correct CT has been identified by\nTony Cohn as a challenge for computer scientists in 1995. This paper addresses\nthis problem and introduces a semi-automatic method to compute the CT by\nrandomly generating triples of elements. For several important qualitative\ncalculi, our method can establish the correct CT in a reasonable short time.\nThis is illustrated by applications to the Interval Algebra, the Region\nConnection Calculus RCC-8, the INDU calculus, and the Oriented Point Relation\nAlgebras. Our method can also be used to generate CTs for customised\nqualitative calculi defined on restricted domains.\n", "versions": [{"version": "v1", "created": "Sat, 21 May 2011 07:37:47 GMT"}], "update_date": "2011-05-24", "authors_parsed": [["Liu", "Weiming", ""], ["Li", "Sanjiang", ""]]}, {"id": "1105.4318", "submitter": "Diptesh Chatterjee", "authors": "Diptesh Chatterhee", "title": "Correction of Noisy Sentences using a Monolingual Corpus", "comments": "67 pages, 2 figures, 4 tables, 2 algorithms", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Correction of Noisy Natural Language Text is an important and well studied\nproblem in Natural Language Processing. It has a number of applications in\ndomains like Statistical Machine Translation, Second Language Learning and\nNatural Language Generation. In this work, we consider some statistical\ntechniques for Text Correction. We define the classes of errors commonly found\nin text and describe algorithms to correct them. The data has been taken from a\npoorly trained Machine Translation system. The algorithms use only a language\nmodel in the target language in order to correct the sentences. We use phrase\nbased correction methods in both the algorithms. The phrases are replaced and\ncombined to give us the final corrected sentence. We also present the methods\nto model different kinds of errors, in addition to results of the working of\nthe algorithms on the test set. We show that one of the approaches fail to\nachieve the desired goal, whereas the other succeeds well. In the end, we\nanalyze the possible reasons for such a trend in performance.\n", "versions": [{"version": "v1", "created": "Sun, 22 May 2011 09:01:38 GMT"}], "update_date": "2016-11-25", "authors_parsed": [["Chatterhee", "Diptesh", ""]]}, {"id": "1105.4394", "submitter": "EPTCS", "authors": "Harsh Raju Chamarthi (Northeastern University), Peter C. Dillinger\n  (Northeastern University), Matt Kaufmann (University of Texas at Austin),\n  Panagiotis Manolios (Northeastern University)", "title": "Integrating Testing and Interactive Theorem Proving", "comments": "In Proceedings ACL2 2011, arXiv:1110.4473", "journal-ref": "EPTCS 70, 2011, pp. 4-19", "doi": "10.4204/EPTCS.70.1", "report-no": null, "categories": "cs.SE cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using an interactive theorem prover to reason about programs involves a\nsequence of interactions where the user challenges the theorem prover with\nconjectures. Invariably, many of the conjectures posed are in fact false, and\nusers often spend considerable effort examining the theorem prover's output\nbefore realizing this. We present a synergistic integration of testing with\ntheorem proving, implemented in the ACL2 Sedan (ACL2s), for automatically\ngenerating concrete counterexamples. Our method uses the full power of the\ntheorem prover and associated libraries to simplify conjectures; this\nsimplification can transform conjectures for which finding counterexamples is\nhard into conjectures where finding counterexamples is trivial. In fact, our\napproach even leads to better theorem proving, e.g. if testing shows that a\ngeneralization step leads to a false conjecture, we force the theorem prover to\nbacktrack, allowing it to pursue more fruitful options that may yield a proof.\nThe focus of the paper is on the engineering of a synergistic integration of\ntesting with interactive theorem proving; this includes extending ACL2 with new\nfunctionality that we expect to be of general interest. We also discuss our\nexperience in using ACL2s to teach freshman students how to reason about their\nprograms.\n", "versions": [{"version": "v1", "created": "Mon, 23 May 2011 03:05:44 GMT"}, {"version": "v2", "created": "Fri, 21 Oct 2011 02:09:20 GMT"}], "update_date": "2011-10-24", "authors_parsed": [["Chamarthi", "Harsh Raju", "", "Northeastern University"], ["Dillinger", "Peter C.", "", "Northeastern University"], ["Kaufmann", "Matt", "", "University of Texas at Austin"], ["Manolios", "Panagiotis", "", "Northeastern University"]]}, {"id": "1105.5129", "submitter": "Nathan Keller", "authors": "Ehud Friedgut, Gil Kalai, Nathan Keller, and Noam Nisan", "title": "A Quantitative Version of the Gibbard-Satterthwaite Theorem for Three\n  Alternatives", "comments": "27 pages, extended version of a FOCS'08 paper, to appear in SICOMP", "journal-ref": null, "doi": null, "report-no": null, "categories": "math.CO cs.AI math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Gibbard-Satterthwaite theorem states that every non-dictatorial election\nrule among at least three alternatives can be strategically manipulated. We\nprove a quantitative version of the Gibbard-Satterthwaite theorem: a random\nmanipulation by a single random voter will succeed with a non-negligible\nprobability for any election rule among three alternatives that is far from\nbeing a dictatorship and from having only two alternatives in its range.\n", "versions": [{"version": "v1", "created": "Wed, 25 May 2011 19:51:50 GMT"}], "update_date": "2011-05-26", "authors_parsed": [["Friedgut", "Ehud", ""], ["Kalai", "Gil", ""], ["Keller", "Nathan", ""], ["Nisan", "Noam", ""]]}, {"id": "1105.5440", "submitter": "J. M. Ahuactzin", "authors": "J. M. Ahuactzin, P. Bessiere, E. Mazer", "title": "The Ariadne's Clew Algorithm", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 9, pages\n  295-316, 1998", "doi": "10.1613/jair.468", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new approach to path planning, called the \"Ariadne's clew\nalgorithm\". It is designed to find paths in high-dimensional continuous spaces\nand applies to robots with many degrees of freedom in static, as well as\ndynamic environments - ones where obstacles may move. The Ariadne's clew\nalgorithm comprises two sub-algorithms, called Search and Explore, applied in\nan interleaved manner. Explore builds a representation of the accessible space\nwhile Search looks for the target. Both are posed as optimization problems. We\ndescribe a real implementation of the algorithm to plan paths for a six degrees\nof freedom arm in a dynamic environment where another six degrees of freedom\narm is used as a moving obstacle. Experimental results show that a path is\nfound in about one second without any pre-processing.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2011 01:44:34 GMT"}], "update_date": "2011-05-30", "authors_parsed": [["Ahuactzin", "J. M.", ""], ["Bessiere", "P.", ""], ["Mazer", "E.", ""]]}, {"id": "1105.5441", "submitter": "C. Backstrom", "authors": "C. Backstrom", "title": "Computational Aspects of Reordering Plans", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 9, pages\n  99-137, 1998", "doi": "10.1613/jair.477", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article studies the problem of modifying the action ordering of a plan\nin order to optimise the plan according to various criteria. One of these\ncriteria is to make a plan less constrained and the other is to minimize its\nparallel execution time. Three candidate definitions are proposed for the first\nof these criteria, constituting a sequence of increasing optimality guarantees.\nTwo of these are based on deordering plans, which means that ordering relations\nmay only be removed, not added, while the third one uses reordering, where\narbitrary modifications to the ordering are allowed. It is shown that only the\nweakest one of the three criteria is tractable to achieve, the other two being\nNP-hard and even difficult to approximate. Similarly, optimising the parallel\nexecution time of a plan is studied both for deordering and reordering of\nplans. In the general case, both of these computations are NP-hard. However, it\nis shown that optimal deorderings can be computed in polynomial time for a\nclass of planning languages based on the notions of producers, consumers and\nthreats, which includes most of the commonly used planning languages. Computing\noptimal reorderings can potentially lead to even faster parallel executions,\nbut this problem remains NP-hard and difficult to approximate even under quite\nsevere restrictions.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2011 01:44:57 GMT"}], "update_date": "2011-05-30", "authors_parsed": [["Backstrom", "C.", ""]]}, {"id": "1105.5442", "submitter": "O. Ledeniov", "authors": "O. Ledeniov, S. Markovitch", "title": "The Divide-and-Conquer Subgoal-Ordering Algorithm for Speeding up Logic\n  Inference", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 9, pages\n  37-97, 1998", "doi": "10.1613/jair.509", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is common to view programs as a combination of logic and control: the\nlogic part defines what the program must do, the control part -- how to do it.\nThe Logic Programming paradigm was developed with the intention of separating\nthe logic from the control. Recently, extensive research has been conducted on\nautomatic generation of control for logic programs. Only a few of these works\nconsidered the issue of automatic generation of control for improving the\nefficiency of logic programs. In this paper we present a novel algorithm for\nautomatic finding of lowest-cost subgoal orderings. The algorithm works using\nthe divide-and-conquer strategy. The given set of subgoals is partitioned into\nsmaller sets, based on co-occurrence of free variables. The subsets are ordered\nrecursively and merged, yielding a provably optimal order. We experimentally\ndemonstrate the utility of the algorithm by testing it in several domains, and\ndiscuss the possibilities of its cooperation with other existing methods.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2011 01:45:23 GMT"}], "update_date": "2011-05-30", "authors_parsed": [["Ledeniov", "O.", ""], ["Markovitch", "S.", ""]]}, {"id": "1105.5443", "submitter": "J. Culberson", "authors": "J. Culberson, B. Vandegriend", "title": "The Gn,m Phase Transition is Not Hard for the Hamiltonian Cycle Problem", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 9, pages\n  219-245, 1998", "doi": "10.1613/jair.512", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using an improved backtrack algorithm with sophisticated pruning techniques,\nwe revise previous observations correlating a high frequency of hard to solve\nHamiltonian Cycle instances with the Gn,m phase transition between\nHamiltonicity and non-Hamiltonicity. Instead all tested graphs of 100 to 1500\nvertices are easily solved. When we artificially restrict the degree sequence\nwith a bounded maximum degree, although there is some increase in difficulty,\nthe frequency of hard graphs is still low. When we consider more regular graphs\nbased on a generalization of knight's tours, we observe frequent instances of\nreally hard graphs, but on these the average degree is bounded by a constant.\nWe design a set of graphs with a feature our algorithm is unable to detect and\nso are very hard for our algorithm, but in these we can vary the average degree\nfrom O(1) to O(n). We have so far found no class of graphs correlated with the\nGn,m phase transition which asymptotically produces a high frequency of hard\ninstances.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2011 01:45:52 GMT"}], "update_date": "2011-05-30", "authors_parsed": [["Culberson", "J.", ""], ["Vandegriend", "B.", ""]]}, {"id": "1105.5444", "submitter": "P. Resnik", "authors": "P. Resnik", "title": "Semantic Similarity in a Taxonomy: An Information-Based Measure and its\n  Application to Problems of Ambiguity in Natural Language", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 11, pages\n  95-130, 1999", "doi": "10.1613/jair.514", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a measure of semantic similarity in an IS-A taxonomy\nbased on the notion of shared information content. Experimental evaluation\nagainst a benchmark set of human similarity judgments demonstrates that the\nmeasure performs better than the traditional edge-counting approach. The\narticle presents algorithms that take advantage of taxonomic similarity in\nresolving syntactic and semantic ambiguity, along with experimental results\ndemonstrating their effectiveness.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2011 01:46:05 GMT"}], "update_date": "2011-05-30", "authors_parsed": [["Resnik", "P.", ""]]}, {"id": "1105.5446", "submitter": "A. Artale", "authors": "A. Artale, E. Franconi", "title": "A Temporal Description Logic for Reasoning about Actions and Plans", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 9, pages\n  463-506, 1998", "doi": "10.1613/jair.516", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A class of interval-based temporal languages for uniformly representing and\nreasoning about actions and plans is presented. Actions are represented by\ndescribing what is true while the action itself is occurring, and plans are\nconstructed by temporally relating actions and world states. The temporal\nlanguages are members of the family of Description Logics, which are\ncharacterized by high expressivity combined with good computational properties.\nThe subsumption problem for a class of temporal Description Logics is\ninvestigated and sound and complete decision procedures are given. The basic\nlanguage TL-F is considered first: it is the composition of a temporal logic TL\n-- able to express interval temporal networks -- together with the non-temporal\nlogic F -- a Feature Description Logic. It is proven that subsumption in this\nlanguage is an NP-complete problem. Then it is shown how to reason with the\nmore expressive languages TLU-FU and TL-ALCF. The former adds disjunction both\nat the temporal and non-temporal sides of the language, the latter extends the\nnon-temporal side with set-valued features (i.e., roles) and a propositionally\ncomplete language.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2011 01:46:39 GMT"}], "update_date": "2011-05-30", "authors_parsed": [["Artale", "A.", ""], ["Franconi", "E.", ""]]}, {"id": "1105.5447", "submitter": "D. J. Cook", "authors": "D. J. Cook, R. C. Varnell", "title": "Adaptive Parallel Iterative Deepening Search", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 9, pages\n  139-165, 1998", "doi": "10.1613/jair.518", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many of the artificial intelligence techniques developed to date rely on\nheuristic search through large spaces. Unfortunately, the size of these spaces\nand the corresponding computational effort reduce the applicability of\notherwise novel and effective algorithms. A number of parallel and distributed\napproaches to search have considerably improved the performance of the search\nprocess. Our goal is to develop an architecture that automatically selects\nparallel search strategies for optimal performance on a variety of search\nproblems. In this paper we describe one such architecture realized in the\nEureka system, which combines the benefits of many different approaches to\nparallel heuristic search. Through empirical and theoretical analyses we\nobserve that features of the problem space directly affect the choice of\noptimal parallel search strategy. We then employ machine learning techniques to\nselect the optimal parallel search strategy for a given problem space. When a\nnew search task is input to the system, Eureka uses features describing the\nsearch space and the chosen architecture to automatically select the\nappropriate search strategy. Eureka has been tested on a MIMD parallel\nprocessor, a distributed network of workstations, and a single workstation\nusing multithreading. Results generated from fifteen puzzle problems, robot arm\nmotion problems, artificial search spaces, and planning problems indicate that\nEureka outperforms any of the tested strategies used exclusively for all\nproblem instances and is able to greatly reduce the search time for these\napplications.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2011 01:47:18 GMT"}], "update_date": "2011-05-30", "authors_parsed": [["Cook", "D. J.", ""], ["Varnell", "R. C.", ""]]}, {"id": "1105.5448", "submitter": "E. Davis", "authors": "E. Davis", "title": "Order of Magnitude Comparisons of Distance", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 10, pages\n  1-38, 1999", "doi": "10.1613/jair.520", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Order of magnitude reasoning - reasoning by rough comparisons of the sizes of\nquantities - is often called 'back of the envelope calculation', with the\nimplication that the calculations are quick though approximate. This paper\nexhibits an interesting class of constraint sets in which order of magnitude\nreasoning is demonstrably fast. Specifically, we present a polynomial-time\nalgorithm that can solve a set of constraints of the form 'Points a and b are\nmuch closer together than points c and d.' We prove that this algorithm can be\napplied if `much closer together' is interpreted either as referring to an\ninfinite difference in scale or as referring to a finite difference in scale,\nas long as the difference in scale is greater than the number of variables in\nthe constraint set. We also prove that the first-order theory over such\nconstraints is decidable.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2011 01:47:48 GMT"}], "update_date": "2011-05-30", "authors_parsed": [["Davis", "E.", ""]]}, {"id": "1105.5449", "submitter": "G. Di Caro", "authors": "G. Di Caro, M. Dorigo", "title": "AntNet: Distributed Stigmergetic Control for Communications Networks", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 9, pages\n  317-365, 1998", "doi": "10.1613/jair.530", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces AntNet, a novel approach to the adaptive learning of\nrouting tables in communications networks. AntNet is a distributed, mobile\nagents based Monte Carlo system that was inspired by recent work on the ant\ncolony metaphor for solving optimization problems. AntNet's agents concurrently\nexplore the network and exchange collected information. The communication among\nthe agents is indirect and asynchronous, mediated by the network itself. This\nform of communication is typical of social insects and is called stigmergy. We\ncompare our algorithm with six state-of-the-art routing algorithms coming from\nthe telecommunications and machine learning fields. The algorithms' performance\nis evaluated over a set of realistic testbeds. We run many experiments over\nreal and artificial IP datagram networks with increasing number of nodes and\nunder several paradigmatic spatial and temporal traffic distributions. Results\nare very encouraging. AntNet showed superior performance under all the\nexperimental conditions with respect to its competitors. We analyze the main\ncharacteristics of the algorithm and try to explain the reasons for its\nsuperiority.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2011 01:48:39 GMT"}], "update_date": "2011-05-30", "authors_parsed": [["Di Caro", "G.", ""], ["Dorigo", "M.", ""]]}, {"id": "1105.5450", "submitter": "J. Y. Halpern", "authors": "J. Y. Halpern", "title": "A Counter Example to Theorems of Cox and Fine", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 10, pages\n  67-85, 1999", "doi": "10.1613/jair.536", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Cox's well-known theorem justifying the use of probability is shown not to\nhold in finite domains. The counterexample also suggests that Cox's assumptions\nare insufficient to prove the result even in infinite domains. The same\ncounterexample is used to disprove a result of Fine on comparative conditional\nprobability.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2011 01:49:04 GMT"}], "update_date": "2011-05-30", "authors_parsed": [["Halpern", "J. Y.", ""]]}, {"id": "1105.5451", "submitter": "M. Fox", "authors": "M. Fox, D. Long", "title": "The Automatic Inference of State Invariants in TIM", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 9, pages\n  367-421, 1998", "doi": "10.1613/jair.544", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As planning is applied to larger and richer domains the effort involved in\nconstructing domain descriptions increases and becomes a significant burden on\nthe human application designer. If general planners are to be applied\nsuccessfully to large and complex domains it is necessary to provide the domain\ndesigner with some assistance in building correctly encoded domains. One way of\ndoing this is to provide domain-independent techniques for extracting, from a\ndomain description, knowledge that is implicit in that description and that can\nassist domain designers in debugging domain descriptions. This knowledge can\nalso be exploited to improve the performance of planners: several researchers\nhave explored the potential of state invariants in speeding up the performance\nof domain-independent planners. In this paper we describe a process by which\nstate invariants can be extracted from the automatically inferred type\nstructure of a domain. These techniques are being developed for exploitation by\nSTAN, a Graphplan based planner that employs state analysis techniques to\nenhance its performance.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2011 01:49:44 GMT"}], "update_date": "2011-05-30", "authors_parsed": [["Fox", "M.", ""], ["Long", "D.", ""]]}, {"id": "1105.5452", "submitter": "D. Calvanese", "authors": "D. Calvanese, M. Lenzerini, D. Nardi", "title": "Unifying Class-Based Representation Formalisms", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 11, pages\n  199-240, 1999", "doi": "10.1613/jair.548", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The notion of class is ubiquitous in computer science and is central in many\nformalisms for the representation of structured knowledge used both in\nknowledge representation and in databases. In this paper we study the basic\nissues underlying such representation formalisms and single out both their\ncommon characteristics and their distinguishing features. Such investigation\nleads us to propose a unifying framework in which we are able to capture the\nfundamental aspects of several representation languages used in different\ncontexts. The proposed formalism is expressed in the style of description\nlogics, which have been introduced in knowledge representation as a means to\nprovide a semantically well-founded basis for the structural aspects of\nknowledge representation systems. The description logic considered in this\npaper is a subset of first order logic with nice computational characteristics.\nIt is quite expressive and features a novel combination of constructs that has\nnot been studied before. The distinguishing constructs are number restrictions,\nwhich generalize existence and functional dependencies, inverse roles, which\nallow one to refer to the inverse of a relationship, and possibly cyclic\nassertions, which are necessary for capturing real world domains. We are able\nto show that it is precisely such combination of constructs that makes our\nlogic powerful enough to model the essential set of features for defining class\nstructures that are common to frame systems, object-oriented database\nlanguages, and semantic data models. As a consequence of the established\ncorrespondences, several significant extensions of each of the above formalisms\nbecome available. The high expressiveness of the logic we propose and the need\nfor capturing the reasoning in different contexts forces us to distinguish\nbetween unrestricted and finite model reasoning. A notable feature of our\nproposal is that reasoning in both cases is decidable. We argue that, by virtue\nof the high expressive power and of the associated reasoning capabilities on\nboth unrestricted and finite models, our logic provides a common core for\nclass-based representation formalisms.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2011 01:49:59 GMT"}], "update_date": "2011-05-30", "authors_parsed": [["Calvanese", "D.", ""], ["Lenzerini", "M.", ""], ["Nardi", "D.", ""]]}, {"id": "1105.5453", "submitter": "J. Rintanen", "authors": "J. Rintanen", "title": "Complexity of Prioritized Default Logics", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 9, pages\n  423-461, 1998", "doi": "10.1613/jair.554", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In default reasoning, usually not all possible ways of resolving conflicts\nbetween default rules are acceptable. Criteria expressing acceptable ways of\nresolving the conflicts may be hardwired in the inference mechanism, for\nexample specificity in inheritance reasoning can be handled this way, or they\nmay be given abstractly as an ordering on the default rules. In this article we\ninvestigate formalizations of the latter approach in Reiter's default logic.\nOur goal is to analyze and compare the computational properties of three such\nformalizations in terms of their computational complexity: the prioritized\ndefault logics of Baader and Hollunder, and Brewka, and a prioritized default\nlogic that is based on lexicographic comparison. The analysis locates the\npropositional variants of these logics on the second and third levels of the\npolynomial hierarchy, and identifies the boundary between tractable and\nintractable inference for restricted classes of prioritized default theories.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2011 01:50:12 GMT"}], "update_date": "2011-05-30", "authors_parsed": [["Rintanen", "J.", ""]]}, {"id": "1105.5454", "submitter": "D. P. Clements", "authors": "D. P. Clements, D. E. Joslin", "title": "Squeaky Wheel Optimization", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 10, pages\n  353-373, 1999", "doi": "10.1613/jair.561", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a general approach to optimization which we term `Squeaky Wheel'\nOptimization (SWO). In SWO, a greedy algorithm is used to construct a solution\nwhich is then analyzed to find the trouble spots, i.e., those elements, that,\nif improved, are likely to improve the objective function score. The results of\nthe analysis are used to generate new priorities that determine the order in\nwhich the greedy algorithm constructs the next solution. This\nConstruct/Analyze/Prioritize cycle continues until some limit is reached, or an\nacceptable solution is found. SWO can be viewed as operating on two search\nspaces: solutions and prioritizations. Successive solutions are only indirectly\nrelated, via the re-prioritization that results from analyzing the prior\nsolution. Similarly, successive prioritizations are generated by constructing\nand analyzing solutions. This `coupled search' has some interesting properties,\nwhich we discuss. We report encouraging experimental results on two domains,\nscheduling problems that arise in fiber-optic cable manufacturing, and graph\ncoloring problems. The fact that these domains are very different supports our\nclaim that SWO is a general technique for optimization.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2011 01:51:22 GMT"}], "update_date": "2011-05-30", "authors_parsed": [["Clements", "D. P.", ""], ["Joslin", "D. E.", ""]]}, {"id": "1105.5455", "submitter": "D. Barber", "authors": "D. Barber, P. de van Laar", "title": "Variational Cumulant Expansions for Intractable Distributions", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 10, pages\n  435-455, 1999", "doi": "10.1613/jair.567", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Intractable distributions present a common difficulty in inference within the\nprobabilistic knowledge representation framework and variational methods have\nrecently been popular in providing an approximate solution. In this article, we\ndescribe a perturbational approach in the form of a cumulant expansion which,\nto lowest order, recovers the standard Kullback-Leibler variational bound.\nHigher-order terms describe corrections on the variational approach without\nincurring much further computational cost. The relationship to other\nperturbational approaches such as TAP is also elucidated. We demonstrate the\nmethod on a particular class of undirected graphical models, Boltzmann\nmachines, for which our simulation results confirm improved accuracy and\nenhanced stability during learning.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2011 01:51:46 GMT"}], "update_date": "2011-05-30", "authors_parsed": [["Barber", "D.", ""], ["de van Laar", "P.", ""]]}, {"id": "1105.5457", "submitter": "M. Fox", "authors": "M. Fox, D. Long", "title": "Efficient Implementation of the Plan Graph in STAN", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 10, pages\n  87-115, 1999", "doi": "10.1613/jair.570", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  STAN is a Graphplan-based planner, so-called because it uses a variety of\nSTate ANalysis techniques to enhance its performance. STAN competed in the\nAIPS-98 planning competition where it compared well with the other competitors\nin terms of speed, finding solutions fastest to many of the problems posed.\nAlthough the domain analysis techniques STAN exploits are an important factor\nin its overall performance, we believe that the speed at which STAN solved the\ncompetition problems is largely due to the implementation of its plan graph.\nThe implementation is based on two insights: that many of the graph\nconstruction operations can be implemented as bit-level logical operations on\nbit vectors, and that the graph should not be explicitly constructed beyond the\nfix point. This paper describes the implementation of STAN's plan graph and\nprovides experimental results which demonstrate the circumstances under which\nadvantages can be obtained from using this implementation.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2011 01:52:09 GMT"}], "update_date": "2011-05-30", "authors_parsed": [["Fox", "M.", ""], ["Long", "D.", ""]]}, {"id": "1105.5458", "submitter": "M. Fuchs", "authors": "M. Fuchs, D. Fuchs", "title": "Cooperation between Top-Down and Bottom-Up Theorem Provers", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 10, pages\n  169-198, 1999", "doi": "10.1613/jair.573", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Top-down and bottom-up theorem proving approaches each have specific\nadvantages and disadvantages. Bottom-up provers profit from strong redundancy\ncontrol but suffer from the lack of goal-orientation, whereas top-down provers\nare goal-oriented but often have weak calculi when their proof lengths are\nconsidered. In order to integrate both approaches, we try to achieve\ncooperation between a top-down and a bottom-up prover in two different ways:\nThe first technique aims at supporting a bottom-up with a top-down prover. A\ntop-down prover generates subgoal clauses, they are then processed by a\nbottom-up prover. The second technique deals with the use of bottom-up\ngenerated lemmas in a top-down prover. We apply our concept to the areas of\nmodel elimination and superposition. We discuss the ability of our techniques\nto shorten proofs as well as to reorder the search space in an appropriate\nmanner. Furthermore, in order to identify subgoal clauses and lemmas which are\nactually relevant for the proof task, we develop methods for a relevancy-based\nfiltering. Experiments with the provers SETHEO and SPASS performed in the\nproblem library TPTP reveal the high potential of our cooperation approaches.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2011 01:52:28 GMT"}], "update_date": "2011-05-30", "authors_parsed": [["Fuchs", "M.", ""], ["Fuchs", "D.", ""]]}, {"id": "1105.5459", "submitter": "T. Hogg", "authors": "T. Hogg", "title": "Solving Highly Constrained Search Problems with Quantum Computers", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 10, pages\n  39-66, 1999", "doi": "10.1613/jair.574", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A previously developed quantum search algorithm for solving 1-SAT problems in\na single step is generalized to apply to a range of highly constrained k-SAT\nproblems. We identify a bound on the number of clauses in satisfiability\nproblems for which the generalized algorithm can find a solution in a constant\nnumber of steps as the number of variables increases. This performance\ncontrasts with the linear growth in the number of steps required by the best\nclassical algorithms, and the exponential number required by classical and\nquantum methods that ignore the problem structure. In some cases, the algorithm\ncan also guarantee that insoluble problems in fact have no solutions, unlike\npreviously proposed quantum search algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2011 01:52:46 GMT"}], "update_date": "2011-05-30", "authors_parsed": [["Hogg", "T.", ""]]}, {"id": "1105.5460", "submitter": "C. Boutilier", "authors": "C. Boutilier, T. Dean, S. Hanks", "title": "Decision-Theoretic Planning: Structural Assumptions and Computational\n  Leverage", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 11, pages\n  1-94, 1999", "doi": "10.1613/jair.575", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Planning under uncertainty is a central problem in the study of automated\nsequential decision making, and has been addressed by researchers in many\ndifferent fields, including AI planning, decision analysis, operations\nresearch, control theory and economics. While the assumptions and perspectives\nadopted in these areas often differ in substantial ways, many planning problems\nof interest to researchers in these fields can be modeled as Markov decision\nprocesses (MDPs) and analyzed using the techniques of decision theory. This\npaper presents an overview and synthesis of MDP-related methods, showing how\nthey provide a unifying framework for modeling many classes of planning\nproblems studied in AI. It also describes structural properties of MDPs that,\nwhen exhibited by particular classes of problems, can be exploited in the\nconstruction of optimal or approximately optimal policies or plans. Planning\nproblems commonly possess structure in the reward and value functions used to\ndescribe performance criteria, in the functions used to describe state\ntransitions and observations, and in the relationships among features used to\ndescribe states, actions, rewards, and observations. Specialized\nrepresentations, and algorithms employing these representations, can achieve\ncomputational leverage by exploiting these various forms of structure. Certain\nAI techniques -- in particular those based on the use of structured,\nintensional representations -- can be viewed in this way. This paper surveys\nseveral types of representations for both classical and decision-theoretic\nplanning problems, and planning algorithms that exploit these representations\nin a number of different ways to ease the computational burden of constructing\npolicies or plans. It focuses primarily on abstraction, aggregation and\ndecomposition techniques based on AI-style representations.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2011 01:53:02 GMT"}], "update_date": "2011-05-30", "authors_parsed": [["Boutilier", "C.", ""], ["Dean", "T.", ""], ["Hanks", "S.", ""]]}, {"id": "1105.5461", "submitter": "T. Lukasiewicz", "authors": "T. Lukasiewicz", "title": "Probabilistic Deduction with Conditional Constraints over Basic Events", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 10, pages\n  199-241, 1999", "doi": "10.1613/jair.577", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of probabilistic deduction with conditional constraints\nover basic events. We show that globally complete probabilistic deduction with\nconditional constraints over basic events is NP-hard. We then concentrate on\nthe special case of probabilistic deduction in conditional constraint trees. We\nelaborate very efficient techniques for globally complete probabilistic\ndeduction. In detail, for conditional constraint trees with point\nprobabilities, we present a local approach to globally complete probabilistic\ndeduction, which runs in linear time in the size of the conditional constraint\ntrees. For conditional constraint trees with interval probabilities, we show\nthat globally complete probabilistic deduction can be done in a global approach\nby solving nonlinear programs. We show how these nonlinear programs can be\ntransformed into equivalent linear programs, which are solvable in polynomial\ntime in the size of the conditional constraint trees.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2011 01:53:20 GMT"}], "update_date": "2011-05-30", "authors_parsed": [["Lukasiewicz", "T.", ""]]}, {"id": "1105.5462", "submitter": "T. S. Jaakkola", "authors": "T. S. Jaakkola, M. I. Jordan", "title": "Variational Probabilistic Inference and the QMR-DT Network", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 10, pages\n  291-322, 1999", "doi": "10.1613/jair.583", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a variational approximation method for efficient inference in\nlarge-scale probabilistic models. Variational methods are deterministic\nprocedures that provide approximations to marginal and conditional\nprobabilities of interest. They provide alternatives to approximate inference\nmethods based on stochastic sampling or search. We describe a variational\napproach to the problem of diagnostic inference in the `Quick Medical\nReference' (QMR) network. The QMR network is a large-scale probabilistic\ngraphical model built on statistical and expert knowledge. Exact probabilistic\ninference is infeasible in this model for all but a small set of cases. We\nevaluate our variational inference algorithm on a large set of diagnostic test\ncases, comparing the algorithm to a state-of-the-art stochastic sampling\nmethod.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2011 01:53:36 GMT"}], "update_date": "2011-05-30", "authors_parsed": [["Jaakkola", "T. S.", ""], ["Jordan", "M. I.", ""]]}, {"id": "1105.5463", "submitter": "A. Borgida", "authors": "A. Borgida", "title": "Extensible Knowledge Representation: the Case of Description Reasoners", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 10, pages\n  399-434, 1999", "doi": "10.1613/jair.584", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper offers an approach to extensible knowledge representation and\nreasoning for a family of formalisms known as Description Logics. The approach\nis based on the notion of adding new concept constructors, and includes a\nheuristic methodology for specifying the desired extensions, as well as a\nmodularized software architecture that supports implementing extensions. The\narchitecture detailed here falls in the normalize-compared paradigm, and\nsupports both intentional reasoning (subsumption) involving concepts, and\nextensional reasoning involving individuals after incremental updates to the\nknowledge base. The resulting approach can be used to extend the reasoner with\nspecialized notions that are motivated by specific problems or application\nareas, such as reasoning about dates, plans, etc. In addition, it provides an\nopportunity to implement constructors that are not currently yet sufficiently\nwell understood theoretically, but are needed in practice. Also, for\nconstructors that are provably hard to reason with (e.g., ones whose presence\nwould lead to undecidability), it allows the implementation of incomplete\nreasoners where the incompleteness is tailored to be acceptable for the\napplication at hand.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2011 01:53:50 GMT"}], "update_date": "2011-05-30", "authors_parsed": [["Borgida", "A.", ""]]}, {"id": "1105.5464", "submitter": "W. W. Cohen", "authors": "W. W. Cohen, R. E. Schapire, Y. Singer", "title": "Learning to Order Things", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 10, pages\n  243-270, 1999", "doi": "10.1613/jair.587", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There are many applications in which it is desirable to order rather than\nclassify instances. Here we consider the problem of learning how to order\ninstances given feedback in the form of preference judgments, i.e., statements\nto the effect that one instance should be ranked ahead of another. We outline a\ntwo-stage approach in which one first learns by conventional means a binary\npreference function indicating whether it is advisable to rank one instance\nbefore another. Here we consider an on-line algorithm for learning preference\nfunctions that is based on Freund and Schapire's 'Hedge' algorithm. In the\nsecond stage, new instances are ordered so as to maximize agreement with the\nlearned preference function. We show that the problem of finding the ordering\nthat agrees best with a learned preference function is NP-complete.\nNevertheless, we describe simple greedy algorithms that are guaranteed to find\na good approximation. Finally, we show how metasearch can be formulated as an\nordering problem, and present experimental results on learning a combination of\n'search experts', each of which is a domain-specific query expansion strategy\nfor a web search engine.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2011 01:54:11 GMT"}], "update_date": "2011-05-30", "authors_parsed": [["Cohen", "W. W.", ""], ["Schapire", "R. E.", ""], ["Singer", "Y.", ""]]}, {"id": "1105.5465", "submitter": "J. Rintanen", "authors": "J. Rintanen", "title": "Constructing Conditional Plans by a Theorem-Prover", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 10, pages\n  323-352, 1999", "doi": "10.1613/jair.591", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The research on conditional planning rejects the assumptions that there is no\nuncertainty or incompleteness of knowledge with respect to the state and\nchanges of the system the plans operate on. Without these assumptions the\nsequences of operations that achieve the goals depend on the initial state and\nthe outcomes of nondeterministic changes in the system. This setting raises the\nquestions of how to represent the plans and how to perform plan search. The\nanswers are quite different from those in the simpler classical framework. In\nthis paper, we approach conditional planning from a new viewpoint that is\nmotivated by the use of satisfiability algorithms in classical planning.\nTranslating conditional planning to formulae in the propositional logic is not\nfeasible because of inherent computational limitations. Instead, we translate\nconditional planning to quantified Boolean formulae. We discuss three\nformalizations of conditional planning as quantified Boolean formulae, and\npresent experimental results obtained with a theorem-prover.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2011 01:54:30 GMT"}], "update_date": "2011-05-30", "authors_parsed": [["Rintanen", "J.", ""]]}, {"id": "1105.5466", "submitter": "K. M. Ting", "authors": "K. M. Ting, I. H. Witten", "title": "Issues in Stacked Generalization", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 10, pages\n  271-289, 1999", "doi": "10.1613/jair.594", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Stacked generalization is a general method of using a high-level model to\ncombine lower-level models to achieve greater predictive accuracy. In this\npaper we address two crucial issues which have been considered to be a `black\nart' in classification tasks ever since the introduction of stacked\ngeneralization in 1992 by Wolpert: the type of generalizer that is suitable to\nderive the higher-level model, and the kind of attributes that should be used\nas its input. We find that best results are obtained when the higher-level\nmodel combines the confidence (and not just the predictions) of the lower-level\nones. We demonstrate the effectiveness of stacked generalization for combining\nthree different types of learning algorithms for classification tasks. We also\ncompare the performance of stacked generalization with majority vote and\npublished results of arcing and bagging.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2011 01:54:47 GMT"}], "update_date": "2011-05-30", "authors_parsed": [["Ting", "K. M.", ""], ["Witten", "I. H.", ""]]}, {"id": "1105.5516", "submitter": "Fabian Suchanek", "authors": "Fabian Suchanek (INRIA Saclay - Ile de France), Serge Abiteboul (INRIA\n  Saclay - Ile de France), Pierre Senellart", "title": "Ontology Alignment at the Instance and Schema Level", "comments": "Technical Report at INRIA RT-0408", "journal-ref": "N&deg; RT-0408 (2011)", "doi": null, "report-no": "RT-0408", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present PARIS, an approach for the automatic alignment of ontologies.\nPARIS aligns not only instances, but also relations and classes. Alignments at\nthe instance-level cross-fertilize with alignments at the schema-level.\nThereby, our system provides a truly holistic solution to the problem of\nontology alignment. The heart of the approach is probabilistic. This allows\nPARIS to run without any parameter tuning. We demonstrate the efficiency of the\nalgorithm and its precision through extensive experiments. In particular, we\nobtain a precision of around 90% in experiments with two of the world's largest\nontologies.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2011 10:18:08 GMT"}, {"version": "v2", "created": "Wed, 1 Jun 2011 12:38:05 GMT"}, {"version": "v3", "created": "Thu, 18 Aug 2011 13:00:27 GMT"}], "update_date": "2011-08-19", "authors_parsed": [["Suchanek", "Fabian", "", "INRIA Saclay - Ile de France"], ["Abiteboul", "Serge", "", "INRIA\n  Saclay - Ile de France"], ["Senellart", "Pierre", ""]]}, {"id": "1105.5667", "submitter": "Nina Narodytska", "authors": "Jessica Davies, George Katsirelos, Nina Narodytska, Toby Walsh", "title": "Complexity of and Algorithms for Borda Manipulation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We prove that it is NP-hard for a coalition of two manipulators to compute\nhow to manipulate the Borda voting rule. This resolves one of the last open\nproblems in the computational complexity of manipulating common voting rules.\nBecause of this NP-hardness, we treat computing a manipulation as an\napproximation problem where we try to minimize the number of manipulators.\nBased on ideas from bin packing and multiprocessor scheduling, we propose two\nnew approximation methods to compute manipulations of the Borda rule.\nExperiments show that these methods significantly outperform the previous best\nknown %existing approximation method. We are able to find optimal manipulations\nin almost all the randomly generated elections tested. Our results suggest\nthat, whilst computing a manipulation of the Borda rule by a coalition is\nNP-hard, computational complexity may provide only a weak barrier against\nmanipulation in practice.\n", "versions": [{"version": "v1", "created": "Fri, 27 May 2011 23:11:40 GMT"}], "update_date": "2011-05-31", "authors_parsed": [["Davies", "Jessica", ""], ["Katsirelos", "George", ""], ["Narodytska", "Nina", ""], ["Walsh", "Toby", ""]]}, {"id": "1105.6124", "submitter": "F. Barber", "authors": "F. Barber", "title": "Reasoning on Interval and Point-based Disjunctive Metric Constraints in\n  Temporal Contexts", "comments": null, "journal-ref": "Journal Of Artificial Intelligence Research, Volume 12, pages\n  35-86, 2000", "doi": "10.1613/jair.693", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a temporal model for reasoning on disjunctive metric constraints\non intervals and time points in temporal contexts. This temporal model is\ncomposed of a labeled temporal algebra and its reasoning algorithms. The\nlabeled temporal algebra defines labeled disjunctive metric point-based\nconstraints, where each disjunct in each input disjunctive constraint is\nunivocally associated to a label. Reasoning algorithms manage labeled\nconstraints, associated label lists, and sets of mutually inconsistent\ndisjuncts. These algorithms guarantee consistency and obtain a minimal network.\nAdditionally, constraints can be organized in a hierarchy of alternative\ntemporal contexts. Therefore, we can reason on context-dependent disjunctive\nmetric constraints on intervals and points. Moreover, the model is able to\nrepresent non-binary constraints, such that logical dependencies on disjuncts\nin constraints can be handled. The computational cost of reasoning algorithms\nis exponential in accordance with the underlying problem complexity, although\nsome improvements are proposed.\n", "versions": [{"version": "v1", "created": "Mon, 30 May 2011 22:09:11 GMT"}], "update_date": "2011-06-01", "authors_parsed": [["Barber", "F.", ""]]}, {"id": "1105.6148", "submitter": "Mohammed El-Dosuky", "authors": "M. A. El-Dosuky, T. T. Hamza, M. Z. Rashad and A. H. Naguib", "title": "Overcoming Misleads In Logic Programs by Redefining Negation", "comments": "8 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/3.0/", "abstract": "  Negation as failure and incomplete information in logic programs have been\nstudied by many researchers In order to explains HOW a negated conclusion was\nreached, we introduce and proof a different way for negating facts to\novercoming misleads in logic programs. Negating facts can be achieved by asking\nthe user for constants that do not appear elsewhere in the knowledge base.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2011 02:19:21 GMT"}, {"version": "v2", "created": "Mon, 4 Mar 2013 23:40:11 GMT"}], "update_date": "2013-03-06", "authors_parsed": [["El-Dosuky", "M. A.", ""], ["Hamza", "T. T.", ""], ["Rashad", "M. Z.", ""], ["Naguib", "A. H.", ""]]}, {"id": "1105.6314", "submitter": "Laurent  Michel D", "authors": "L. Michel and P. Van Hentenryck", "title": "Activity-Based Search for Black-Box Contraint-Programming Solvers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.MS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robust search procedures are a central component in the design of black-box\nconstraint-programming solvers. This paper proposes activity-based search, the\nidea of using the activity of variables during propagation to guide the search.\nActivity-based search was compared experimentally to impact-based search and\nthe WDEG heuristics. Experimental results on a variety of benchmarks show that\nactivity-based search is more robust than other heuristics and may produce\nsignificant improvements in performance.\n", "versions": [{"version": "v1", "created": "Tue, 31 May 2011 15:25:33 GMT"}], "update_date": "2011-06-01", "authors_parsed": [["Michel", "L.", ""], ["Van Hentenryck", "P.", ""]]}]