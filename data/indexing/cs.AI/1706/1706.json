[{"id": "1706.00007", "submitter": "Bo Wu", "authors": "Bo Wu and Bin Hu and Hai Lin", "title": "A Learning Based Optimal Human Robot Collaboration with Linear Temporal\n  Logic Constraints", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper considers an optimal task allocation problem for human robot\ncollaboration in human robot systems with persistent tasks. Such human robot\nsystems consist of human operators and intelligent robots collaborating with\neach other to accomplish complex tasks that cannot be done by either part\nalone. The system objective is to maximize the probability of successfully\nexecuting persistent tasks that are formulated as linear temporal logic\nspecifications and minimize the average cost between consecutive visits of a\nparticular proposition. This paper proposes to model the human robot\ncollaboration under a framework with the composition of multiple Markov\nDecision Process (MDP) with possibly unknown transition probabilities, which\ncharacterizes how human cognitive states, such as human trust and fatigue,\nstochastically change with the robot performance. Under the unknown MDP models,\nan algorithm is developed to learn the model and obtain an optimal task\nallocation policy that minimizes the expected average cost for each task cycle\nand maximizes the probability of satisfying linear temporal logic constraints.\nMoreover, this paper shows that the difference between the optimal policy based\non the learned model and that based on the underlying ground truth model can be\nbounded by arbitrarily small constant and large confidence level with\nsufficient samples. The case study of an assembly process demonstrates the\neffectiveness and benefits of our proposed learning based human robot\ncollaboration.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 17:52:48 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Wu", "Bo", ""], ["Hu", "Bin", ""], ["Lin", "Hai", ""]]}, {"id": "1706.00037", "submitter": "Mark Lewis", "authors": "Mark W. Lewis", "title": "A Diversified Multi-Start Algorithm for Unconstrained Binary Quadratic\n  Problems Leveraging the Graphics Processor Unit", "comments": "Quality solutions quickly obtained for xQx using the GPU to perform\n  matrix multiplication, however improvements to solution intensification are\n  needed", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-start algorithms are a common and effective tool for metaheuristic\nsearches. In this paper we amplify multi-start capabilities by employing the\nparallel processing power of the graphics processer unit (GPU) to quickly\ngenerate a diverse starting set of solutions for the Unconstrained Binary\nQuadratic Optimization Problem which are evaluated and used to implement\nscreening methods to select solutions for further optimization. This method is\nimplemented as an initial high quality solution generation phase prior to a\nsecondary steepest ascent search and a comparison of results to best known\napproaches on benchmark unconstrained binary quadratic problems demonstrates\nthat GPU-enabled diversified multi-start with screening quickly yields very\ngood results.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 18:15:51 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Lewis", "Mark W.", ""]]}, {"id": "1706.00061", "submitter": "Reinhard Heckel", "authors": "Reinhard Heckel and Kannan Ramchandran", "title": "The Sample Complexity of Online One-Class Collaborative Filtering", "comments": "ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the online one-class collaborative filtering (CF) problem that\nconsists of recommending items to users over time in an online fashion based on\npositive ratings only. This problem arises when users respond only occasionally\nto a recommendation with a positive rating, and never with a negative one. We\nstudy the impact of the probability of a user responding to a recommendation,\np_f, on the sample complexity, i.e., the number of ratings required to make\n`good' recommendations, and ask whether receiving positive and negative\nratings, instead of positive ratings only, improves the sample complexity. Both\nquestions arise in the design of recommender systems. We introduce a simple\nprobabilistic user model, and analyze the performance of an online user-based\nCF algorithm. We prove that after an initial cold start phase, where\nrecommendations are invested in exploring the user's preferences, this\nalgorithm makes---up to a fraction of the recommendations required for updating\nthe user's preferences---perfect recommendations. The number of ratings\nrequired for the cold start phase is nearly proportional to 1/p_f, and that for\nupdating the user's preferences is essentially independent of p_f. As a\nconsequence we find that, receiving positive and negative ratings instead of\nonly positive ones improves the number of ratings required for initial\nexploration by a factor of 1/p_f, which can be significant.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 19:37:12 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Heckel", "Reinhard", ""], ["Ramchandran", "Kannan", ""]]}, {"id": "1706.00066", "submitter": "Chuyu Xiong", "authors": "Chuyu Xiong", "title": "Descriptions of Objectives and Processes of Mechanical Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  In [1], we introduced mechanical learning and proposed 2 approaches to\nmechanical learning. Here, we follow one such approach to well describe the\nobjects and the processes of learning. We discuss 2 kinds of patterns:\nobjective and subjective pattern. Subjective pattern is crucial for learning\nmachine. We prove that for any objective pattern we can find a proper\nsubjective pattern based upon least base patterns to express the objective\npattern well. X-form is algebraic expression for subjective pattern. Collection\nof X-forms form internal representation space, which is center of learning\nmachine. We discuss learning by teaching and without teaching. We define data\nsufficiency by X-form. We then discussed some learning strategies. We show, in\neach strategy, with sufficient data, and with certain capabilities, learning\nmachine indeed can learn any pattern (universal learning machine). In appendix,\nwith knowledge of learning machine, we try to view deep learning from a\ndifferent angle, i.e. its internal representation space and its learning\ndynamics.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 19:42:41 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Xiong", "Chuyu", ""]]}, {"id": "1706.00074", "submitter": "Daniel Crawford", "authors": "Anna Levit, Daniel Crawford, Navid Ghadermarzy, Jaspreet S. Oberoi,\n  Ehsan Zahedinejad, Pooya Ronagh", "title": "Free energy-based reinforcement learning using a quantum processor", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE math.OC quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent theoretical and experimental results suggest the possibility of using\ncurrent and near-future quantum hardware in challenging sampling tasks. In this\npaper, we introduce free energy-based reinforcement learning (FERL) as an\napplication of quantum hardware. We propose a method for processing a quantum\nannealer's measured qubit spin configurations in approximating the free energy\nof a quantum Boltzmann machine (QBM). We then apply this method to perform\nreinforcement learning on the grid-world problem using the D-Wave 2000Q quantum\nannealer. The experimental results show that our technique is a promising\nmethod for harnessing the power of quantum sampling in reinforcement learning\ntasks.\n", "versions": [{"version": "v1", "created": "Mon, 29 May 2017 18:57:42 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Levit", "Anna", ""], ["Crawford", "Daniel", ""], ["Ghadermarzy", "Navid", ""], ["Oberoi", "Jaspreet S.", ""], ["Zahedinejad", "Ehsan", ""], ["Ronagh", "Pooya", ""]]}, {"id": "1706.00123", "submitter": "Junping Zhou", "authors": "Junping Zhou, Huanyao Sun, Feifei Ma, Jian Gao, Ke Xu, and Minghao Yin", "title": "Diversified Top-k Partial MaxSAT Solving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a diversified top-k partial MaxSAT problem, a combination of\npartial MaxSAT problem and enumeration problem. Given a partial MaxSAT formula\nF and a positive integer k, the diversified top-k partial MaxSAT is to find k\nmaximal solutions for F such that the k maximal solutions satisfy the maximum\nnumber of soft clauses of F. This problem can be widely used in many\napplications including community detection, sensor place, motif discovery, and\ncombinatorial testing. We prove the problem is NP-hard and propose an approach\nfor solving the problem. The concrete idea of the approach is to design an\nencoding EE which reduces diversified top-k partial MaxSAT problem into partial\nMaxSAT problem, and then solve the resulting problem with state-of-art solvers.\nIn addition, we present an algorithm MEMKC exactly solving the diversified\ntop-k partial MaxSAT. Through several experiments we show that our approach can\nbe successfully applied to the interesting problem.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 23:37:18 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Zhou", "Junping", ""], ["Sun", "Huanyao", ""], ["Ma", "Feifei", ""], ["Gao", "Jian", ""], ["Xu", "Ke", ""], ["Yin", "Minghao", ""]]}, {"id": "1706.00130", "submitter": "Huan Ling", "authors": "Huan Ling, Sanja Fidler", "title": "Teaching Machines to Describe Images via Natural Language Feedback", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Robots will eventually be part of every household. It is thus critical to\nenable algorithms to learn from and be guided by non-expert users. In this\npaper, we bring a human in the loop, and enable a human teacher to give\nfeedback to a learning agent in the form of natural language. We argue that a\ndescriptive sentence can provide a much stronger learning signal than a numeric\nreward in that it can easily point to where the mistakes are and how to correct\nthem. We focus on the problem of image captioning in which the quality of the\noutput can easily be judged by non-experts. We propose a hierarchical\nphrase-based captioning model trained with policy gradients, and design a\nfeedback network that provides reward to the learner by conditioning on the\nhuman-provided feedback. We show that by exploiting descriptive feedback our\nmodel learns to perform better than when given independently written human\ncaptions.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 00:24:55 GMT"}, {"version": "v2", "created": "Mon, 5 Jun 2017 16:47:40 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Ling", "Huan", ""], ["Fidler", "Sanja", ""]]}, {"id": "1706.00327", "submitter": "Thanh Lam Hoang", "authors": "Hoang Thanh Lam, Johann-Michael Thiebaut, Mathieu Sinn, Bei Chen, Tiep\n  Mai and Oznur Alkan", "title": "One button machine for automating feature engineering in relational\n  databases", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature engineering is one of the most important and time consuming tasks in\npredictive analytics projects. It involves understanding domain knowledge and\ndata exploration to discover relevant hand-crafted features from raw data. In\nthis paper, we introduce a system called One Button Machine, or OneBM for\nshort, which automates feature discovery in relational databases. OneBM\nautomatically performs a key activity of data scientists, namely, joining of\ndatabase tables and applying advanced data transformations to extract useful\nfeatures from data. We validated OneBM in Kaggle competitions in which OneBM\nachieved performance as good as top 16% to 24% data scientists in three Kaggle\ncompetitions. More importantly, OneBM outperformed the state-of-the-art system\nin a Kaggle competition in terms of prediction accuracy and ranking on Kaggle\nleaderboard. The results show that OneBM can be useful for both data scientists\nand non-experts. It helps data scientists reduce data exploration time allowing\nthem to try and error many ideas in short time. On the other hand, it enables\nnon-experts, who are not familiar with data science, to quickly extract value\nfrom their data with a little effort, time and cost.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 14:44:34 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Lam", "Hoang Thanh", ""], ["Thiebaut", "Johann-Michael", ""], ["Sinn", "Mathieu", ""], ["Chen", "Bei", ""], ["Mai", "Tiep", ""], ["Alkan", "Oznur", ""]]}, {"id": "1706.00342", "submitter": "Francois Malgouyres", "authors": "Francois Malgouyres (IMT)", "title": "On the stable recovery of deep structured linear networks under sparsity\n  constraints", "comments": null, "journal-ref": "Mathematical and Scientific Machine Learning, Jul 2020, Princeton,\n  United States", "doi": null, "report-no": null, "categories": "math.OC cs.AI cs.LG math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider a deep structured linear network under sparsity constraints. We\nstudy sharp conditions guaranteeing the stability of the optimal parameters\ndefining the network. More precisely, we provide sharp conditions on the\nnetwork architecture and the sample under which the error on the parameters\ndefining the network scales linearly with the reconstruction error (i.e. the\nrisk). Therefore, under these conditions, the weights obtained with a\nsuccessful algorithms are well defined and only depend on the architecture of\nthe network and the sample. The features in the latent spaces are stably\ndefined. The stability property is required in order to interpret the features\ndefined in the latent spaces. It can also lead to a guarantee on the\nstatistical risk. This is what motivates this study. The analysis is based on\nthe recently proposed Tensorial Lifting. The particularity of this paper is to\nconsider a sparsity prior. This leads to a better stability constant. As an\nillustration, we detail the analysis and provide sharp stability guarantees for\nconvolutional linear network under sparsity prior. In this analysis, we\ndistinguish the role of the network architecture and the sample input. This\nhighlights the requirements on the data in connection to parameter stability.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 09:49:34 GMT"}, {"version": "v2", "created": "Tue, 20 Feb 2018 10:04:01 GMT"}, {"version": "v3", "created": "Wed, 20 May 2020 06:16:22 GMT"}], "update_date": "2020-05-21", "authors_parsed": [["Malgouyres", "Francois", "", "IMT"]]}, {"id": "1706.00355", "submitter": "Yordan Hristov", "authors": "Yordan Hristov, Svetlin Penkov, Alex Lascarides and Subramanian\n  Ramamoorthy", "title": "Grounding Symbols in Multi-Modal Instructions", "comments": "9 pages, 8 figures, To appear in the Proceedings of the ACL workshop\n  Language Grounding for Robotics, Vancouver, Canada", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As robots begin to cohabit with humans in semi-structured environments, the\nneed arises to understand instructions involving rich variability---for\ninstance, learning to ground symbols in the physical world. Realistically, this\ntask must cope with small datasets consisting of a particular users' contextual\nassignment of meaning to terms. We present a method for processing a raw stream\nof cross-modal input---i.e., linguistic instructions, visual perception of a\nscene and a concurrent trace of 3D eye tracking fixations---to produce the\nsegmentation of objects with a correspondent association to high-level\nconcepts. To test our framework we present experiments in a table-top object\nmanipulation scenario. Our results show our model learns the user's notion of\ncolour and shape from a small number of physical demonstrations, generalising\nto identifying physical referents for novel combinations of the words.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 15:42:50 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Hristov", "Yordan", ""], ["Penkov", "Svetlin", ""], ["Lascarides", "Alex", ""], ["Ramamoorthy", "Subramanian", ""]]}, {"id": "1706.00356", "submitter": "Riccardo De Masellis", "authors": "Riccardo De Masellis and Chiara Di Francescomarino and Chiara Ghidini\n  and Sergio Tessaris", "title": "Enhancing workflow-nets with data for trace completion", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The growing adoption of IT-systems for modeling and executing (business)\nprocesses or services has thrust the scientific investigation towards\ntechniques and tools which support more complex forms of process analysis. Many\nof them, such as conformance checking, process alignment, mining and\nenhancement, rely on complete observation of past (tracked and logged)\nexecutions. In many real cases, however, the lack of human or IT-support on all\nthe steps of process execution, as well as information hiding and abstraction\nof model and data, result in incomplete log information of both data and\nactivities. This paper tackles the issue of automatically repairing traces with\nmissing information by notably considering not only activities but also data\nmanipulated by them. Our technique recasts such a problem in a reachability\nproblem and provides an encoding in an action language which allows to\nvirtually use any state-of-the-art planning to return solutions.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 15:46:47 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["De Masellis", "Riccardo", ""], ["Di Francescomarino", "Chiara", ""], ["Ghidini", "Chiara", ""], ["Tessaris", "Sergio", ""]]}, {"id": "1706.00359", "submitter": "Yishu Miao", "authors": "Yishu Miao, Edward Grefenstette, Phil Blunsom", "title": "Discovering Discrete Latent Topics with Neural Variational Inference", "comments": "ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Topic models have been widely explored as probabilistic generative models of\ndocuments. Traditional inference methods have sought closed-form derivations\nfor updating the models, however as the expressiveness of these models grows,\nso does the difficulty of performing fast and accurate inference over their\nparameters. This paper presents alternative neural approaches to topic\nmodelling by providing parameterisable distributions over topics which permit\ntraining by backpropagation in the framework of neural variational inference.\nIn addition, with the help of a stick-breaking construction, we propose a\nrecurrent network that is able to discover a notionally unbounded number of\ntopics, analogous to Bayesian non-parametric topic models. Experimental results\non the MXM Song Lyrics, 20NewsGroups and Reuters News datasets demonstrate the\neffectiveness and efficiency of these neural topic models.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 15:55:42 GMT"}, {"version": "v2", "created": "Mon, 21 May 2018 19:00:21 GMT"}], "update_date": "2018-05-23", "authors_parsed": [["Miao", "Yishu", ""], ["Grefenstette", "Edward", ""], ["Blunsom", "Phil", ""]]}, {"id": "1706.00374", "submitter": "Nikola Mrk\\v{s}i\\'c", "authors": "Nikola Mrk\\v{s}i\\'c, Ivan Vuli\\'c, Diarmuid \\'O S\\'eaghdha, Ira\n  Leviant, Roi Reichart, Milica Ga\\v{s}i\\'c, Anna Korhonen and Steve Young", "title": "Semantic Specialisation of Distributional Word Vector Spaces using\n  Monolingual and Cross-Lingual Constraints", "comments": "Accepted for publication at TACL (to be presented at EMNLP 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present Attract-Repel, an algorithm for improving the semantic quality of\nword vectors by injecting constraints extracted from lexical resources.\nAttract-Repel facilitates the use of constraints from mono- and cross-lingual\nresources, yielding semantically specialised cross-lingual vector spaces. Our\nevaluation shows that the method can make use of existing cross-lingual\nlexicons to construct high-quality vector spaces for a plethora of different\nlanguages, facilitating semantic transfer from high- to lower-resource ones.\nThe effectiveness of our approach is demonstrated with state-of-the-art results\non semantic similarity datasets in six languages. We next show that\nAttract-Repel-specialised vectors boost performance in the downstream task of\ndialogue state tracking (DST) across multiple languages. Finally, we show that\ncross-lingual vector spaces produced by our algorithm facilitate the training\nof multilingual DST models, which brings further performance improvements.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 16:29:47 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Mrk\u0161i\u0107", "Nikola", ""], ["Vuli\u0107", "Ivan", ""], ["S\u00e9aghdha", "Diarmuid \u00d3", ""], ["Leviant", "Ira", ""], ["Reichart", "Roi", ""], ["Ga\u0161i\u0107", "Milica", ""], ["Korhonen", "Anna", ""], ["Young", "Steve", ""]]}, {"id": "1706.00387", "submitter": "Shixiang Gu", "authors": "Shixiang Gu and Timothy Lillicrap and Zoubin Ghahramani and Richard E.\n  Turner and Bernhard Sch\\\"olkopf and Sergey Levine", "title": "Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient\n  Estimation for Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Off-policy model-free deep reinforcement learning methods using previously\ncollected data can improve sample efficiency over on-policy policy gradient\ntechniques. On the other hand, on-policy algorithms are often more stable and\neasier to use. This paper examines, both theoretically and empirically,\napproaches to merging on- and off-policy updates for deep reinforcement\nlearning. Theoretical results show that off-policy updates with a value\nfunction estimator can be interpolated with on-policy policy gradient updates\nwhilst still satisfying performance bounds. Our analysis uses control variate\nmethods to produce a family of policy gradient algorithms, with several\nrecently proposed algorithms being special cases of this family. We then\nprovide an empirical comparison of these techniques with the remaining\nalgorithmic details fixed, and show how different mixing of off-policy gradient\nestimates with on-policy samples contribute to improvements in empirical\nperformance. The final algorithm provides a generalization and unification of\nexisting deep policy gradient techniques, has theoretical guarantees on the\nbias introduced by off-policy updates, and improves on the state-of-the-art\nmodel-free deep RL methods on a number of OpenAI Gym continuous control\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 17:00:52 GMT"}], "update_date": "2017-06-02", "authors_parsed": [["Gu", "Shixiang", ""], ["Lillicrap", "Timothy", ""], ["Ghahramani", "Zoubin", ""], ["Turner", "Richard E.", ""], ["Sch\u00f6lkopf", "Bernhard", ""], ["Levine", "Sergey", ""]]}, {"id": "1706.00400", "submitter": "Narayanaswamy Siddharth", "authors": "N. Siddharth, Brooks Paige, Jan-Willem van de Meent, Alban Desmaison,\n  Noah D. Goodman, Pushmeet Kohli, Frank Wood, Philip H.S. Torr", "title": "Learning Disentangled Representations with Semi-Supervised Deep\n  Generative Models", "comments": "Accepted for publication at NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Variational autoencoders (VAEs) learn representations of data by jointly\ntraining a probabilistic encoder and decoder network. Typically these models\nencode all features of the data into a single variable. Here we are interested\nin learning disentangled representations that encode distinct aspects of the\ndata into separate variables. We propose to learn such representations using\nmodel architectures that generalise from standard VAEs, employing a general\ngraphical model structure in the encoder and decoder. This allows us to train\npartially-specified models that make relatively strong assumptions about a\nsubset of interpretable variables and rely on the flexibility of neural\nnetworks to learn representations for the remaining variables. We further\ndefine a general objective for semi-supervised learning in this model class,\nwhich can be approximated using an importance sampling procedure. We evaluate\nour framework's ability to learn disentangled representations, both by\nqualitative exploration of its generative capacity, and quantitative evaluation\nof its discriminative ability on a variety of models and datasets.\n", "versions": [{"version": "v1", "created": "Thu, 1 Jun 2017 17:23:07 GMT"}, {"version": "v2", "created": "Mon, 13 Nov 2017 17:55:15 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Siddharth", "N.", ""], ["Paige", "Brooks", ""], ["van de Meent", "Jan-Willem", ""], ["Desmaison", "Alban", ""], ["Goodman", "Noah D.", ""], ["Kohli", "Pushmeet", ""], ["Wood", "Frank", ""], ["Torr", "Philip H. S.", ""]]}, {"id": "1706.00526", "submitter": "Evan Patterson", "authors": "Evan Patterson", "title": "Knowledge Representation in Bicategories of Relations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO math.CT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the relational ontology log, or relational olog, a knowledge\nrepresentation system based on the category of sets and relations. It is\ninspired by Spivak and Kent's olog, a recent categorical framework for\nknowledge representation. Relational ologs interpolate between ologs and\ndescription logic, the dominant formalism for knowledge representation today.\nIn this paper, we investigate relational ologs both for their own sake and to\ngain insight into the relationship between the algebraic and logical approaches\nto knowledge representation. On a practical level, we show by example that\nrelational ologs have a friendly and intuitive--yet fully precise--graphical\nsyntax, derived from the string diagrams of monoidal categories. We explain\nseveral other useful features of relational ologs not possessed by most\ndescription logics, such as a type system and a rich, flexible notion of\ninstance data. In a more theoretical vein, we draw on categorical logic to show\nhow relational ologs can be translated to and from logical theories in a\nfragment of first-order logic. Although we make extensive use of categorical\nlanguage, this paper is designed to be self-contained and has considerable\nexpository content. The only prerequisites are knowledge of first-order logic\nand the rudiments of category theory.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 00:42:32 GMT"}, {"version": "v2", "created": "Wed, 1 Nov 2017 15:46:36 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Patterson", "Evan", ""]]}, {"id": "1706.00536", "submitter": "Christopher Grimm", "authors": "Christopher Grimm, Dilip Arumugam, Siddharth Karamcheti, David Abel,\n  Lawson L.S. Wong, Michael L. Littman", "title": "Modeling Latent Attention Within Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep neural networks are able to solve tasks across a variety of domains and\nmodalities of data. Despite many empirical successes, we lack the ability to\nclearly understand and interpret the learned internal mechanisms that\ncontribute to such effective behaviors or, more critically, failure modes. In\nthis work, we present a general method for visualizing an arbitrary neural\nnetwork's inner mechanisms and their power and limitations. Our dataset-centric\nmethod produces visualizations of how a trained network attends to components\nof its inputs. The computed \"attention masks\" support improved interpretability\nby highlighting which input attributes are critical in determining output. We\ndemonstrate the effectiveness of our framework on a variety of deep neural\nnetwork architectures in domains from computer vision, natural language\nprocessing, and reinforcement learning. The primary contribution of our\napproach is an interpretable visualization of attention that provides unique\ninsights into the network's underlying decision-making process irrespective of\nthe data modality.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 02:10:39 GMT"}, {"version": "v2", "created": "Sat, 30 Dec 2017 08:08:50 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Grimm", "Christopher", ""], ["Arumugam", "Dilip", ""], ["Karamcheti", "Siddharth", ""], ["Abel", "David", ""], ["Wong", "Lawson L. S.", ""], ["Littman", "Michael L.", ""]]}, {"id": "1706.00585", "submitter": "Joao Leite", "authors": "Martin Slota and Joao Leite", "title": "Exception-Based Knowledge Updates", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing methods for dealing with knowledge updates differ greatly depending\non the underlying knowledge representation formalism. When Classical Logic is\nused, updates are typically performed by manipulating the knowledge base on the\nmodel-theoretic level. On the opposite side of the spectrum stand the semantics\nfor updating Answer-Set Programs that need to rely on rule syntax. Yet, a\nunifying perspective that could embrace both these branches of research is of\ngreat importance as it enables a deeper understanding of all involved methods\nand principles and creates room for their cross-fertilisation, ripening and\nfurther development.\n  This paper bridges the seemingly irreconcilable approaches to updates. It\nintroduces a novel monotonic characterisation of rules, dubbed RE-models, and\nshows it to be a more suitable semantic foundation for rule updates than\nSE-models. Then it proposes a generic scheme for specifying semantic rule\nupdate operators, based on the idea of viewing a program as the set of sets of\nRE-models of its rules; updates are performed by introducing additional\ninterpretations - exceptions - to the sets of RE-models of rules in the\noriginal program. The introduced scheme is used to define rule update operators\nthat are closely related to both classical update principles and traditional\napproaches to rules updates, and serve as a basis for a solution to the\nlong-standing problem of state condensing, showing how they can be equivalently\ndefined as binary operators on some class of logic programs.\n  Finally, the essence of these ideas is extracted to define an abstract\nframework for exception-based update operators, viewing a knowledge base as the\nset of sets of models of its elements, which can capture a wide range of both\nmodel- and formula-based classical update operators, and thus serves as the\nfirst firm formal ground connecting classical and rule updates.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 08:31:10 GMT"}], "update_date": "2017-06-05", "authors_parsed": [["Slota", "Martin", ""], ["Leite", "Joao", ""]]}, {"id": "1706.00637", "submitter": "Prachi Jain", "authors": "Prachi Jain, Shikhar Murty, Mausam, Soumen Chakrabarti", "title": "Joint Matrix-Tensor Factorization for Knowledge Base Inference", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While several matrix factorization (MF) and tensor factorization (TF) models\nhave been proposed for knowledge base (KB) inference, they have rarely been\ncompared across various datasets. Is there a single model that performs well\nacross datasets? If not, what characteristics of a dataset determine the\nperformance of MF and TF models? Is there a joint TF+MF model that performs\nrobustly on all datasets? We perform an extensive evaluation to compare popular\nKB inference models across popular datasets in the literature. In addition to\nanswering the questions above, we remove a limitation in the standard\nevaluation protocol for MF models, propose an extension to MF models so that\nthey can better handle out-of-vocabulary (OOV) entity pairs, and develop a\nnovel combination of TF and MF models. We also analyze and explain the results\nbased on models and dataset characteristics. Our best model is robust, and\nobtains strong results across all datasets.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 11:34:37 GMT"}], "update_date": "2017-06-05", "authors_parsed": [["Jain", "Prachi", ""], ["Murty", "Shikhar", ""], ["Mausam", "", ""], ["Chakrabarti", "Soumen", ""]]}, {"id": "1706.00638", "submitter": "Amit Mishra", "authors": "Amit Kumar Mishra", "title": "ICABiDAS: Intuition Centred Architecture for Big Data Analysis and\n  Synthesis", "comments": "This paper is presented in the Biologically Inspired Cognitive\n  Architecture Conference 2017 and published by their proceedings", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans are expert in the amount of sensory data they deal with each moment.\nHuman brain not only analyses these data but also starts synthesizing new\ninformation from the existing data. The current age Big-data systems are needed\nnot just to analyze data but also to come up new interpretation. We believe\nthat the pivotal ability in human brain which enables us to do this is what is\nknown as \"intuition\". Here, we present an intuition based architecture for big\ndata analysis and synthesis.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 11:35:52 GMT"}], "update_date": "2017-06-05", "authors_parsed": [["Mishra", "Amit Kumar", ""]]}, {"id": "1706.00764", "submitter": "Yang Yuan", "authors": "Elad Hazan, Adam Klivans, Yang Yuan", "title": "Hyperparameter Optimization: A Spectral Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We give a simple, fast algorithm for hyperparameter optimization inspired by\ntechniques from the analysis of Boolean functions. We focus on the\nhigh-dimensional regime where the canonical example is training a neural\nnetwork with a large number of hyperparameters. The algorithm --- an iterative\napplication of compressed sensing techniques for orthogonal polynomials ---\nrequires only uniform sampling of the hyperparameters and is thus easily\nparallelizable.\n  Experiments for training deep neural networks on Cifar-10 show that compared\nto state-of-the-art tools (e.g., Hyperband and Spearmint), our algorithm finds\nsignificantly improved solutions, in some cases better than what is attainable\nby hand-tuning. In terms of overall running time (i.e., time required to sample\nvarious settings of hyperparameters plus additional computation time), we are\nat least an order of magnitude faster than Hyperband and Bayesian Optimization.\nWe also outperform Random Search 8x.\n  Additionally, our method comes with provable guarantees and yields the first\nimprovements on the sample complexity of learning decision trees in over two\ndecades. In particular, we obtain the first quasi-polynomial time algorithm for\nlearning noisy decision trees with polynomial sample complexity.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 17:25:58 GMT"}, {"version": "v2", "created": "Wed, 7 Jun 2017 16:51:58 GMT"}, {"version": "v3", "created": "Fri, 27 Oct 2017 02:54:31 GMT"}, {"version": "v4", "created": "Sat, 20 Jan 2018 03:49:23 GMT"}], "update_date": "2018-01-23", "authors_parsed": [["Hazan", "Elad", ""], ["Klivans", "Adam", ""], ["Yuan", "Yang", ""]]}, {"id": "1706.00868", "submitter": "Alexey Melnikov", "authors": "Alexey A. Melnikov, Hendrik Poulsen Nautrup, Mario Krenn, Vedran\n  Dunjko, Markus Tiersch, Anton Zeilinger, Hans J. Briegel", "title": "Active learning machine learns to create new quantum experiments", "comments": "11 pages, 6 figures, 1 table; A. A. Melnikov and H. Poulsen Nautrup\n  contributed equally to this work", "journal-ref": "PNAS 115(6), 1221-1226 (2018)", "doi": "10.1073/pnas.1714936115", "report-no": null, "categories": "quant-ph cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  How useful can machine learning be in a quantum laboratory? Here we raise the\nquestion of the potential of intelligent machines in the context of scientific\nresearch. A major motivation for the present work is the unknown reachability\nof various entanglement classes in quantum experiments. We investigate this\nquestion by using the projective simulation model, a physics-oriented approach\nto artificial intelligence. In our approach, the projective simulation system\nis challenged to design complex photonic quantum experiments that produce\nhigh-dimensional entangled multiphoton states, which are of high interest in\nmodern quantum experiments. The artificial intelligence system learns to create\na variety of entangled states, and improves the efficiency of their\nrealization. In the process, the system autonomously (re)discovers experimental\ntechniques which are only now becoming standard in modern quantum optical\nexperiments - a trait which was not explicitly demanded from the system but\nemerged through the process of learning. Such features highlight the\npossibility that machines could have a significantly more creative role in\nfuture research.\n", "versions": [{"version": "v1", "created": "Fri, 2 Jun 2017 22:35:41 GMT"}, {"version": "v2", "created": "Mon, 12 Jun 2017 17:27:05 GMT"}, {"version": "v3", "created": "Thu, 8 Feb 2018 15:40:01 GMT"}], "update_date": "2018-02-09", "authors_parsed": [["Melnikov", "Alexey A.", ""], ["Nautrup", "Hendrik Poulsen", ""], ["Krenn", "Mario", ""], ["Dunjko", "Vedran", ""], ["Tiersch", "Markus", ""], ["Zeilinger", "Anton", ""], ["Briegel", "Hans J.", ""]]}, {"id": "1706.00989", "submitter": "Seyed Reza Ahmadzadeh", "authors": "S. Reza Ahmadzadeh, Fulvio Mastrogiovanni, Petar Kormushev", "title": "Visuospatial Skill Learning for Robots", "comments": "24 pages, 36 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A novel skill learning approach is proposed that allows a robot to acquire\nhuman-like visuospatial skills for object manipulation tasks. Visuospatial\nskills are attained by observing spatial relationships among objects through\ndemonstrations. The proposed Visuospatial Skill Learning (VSL) is a goal-based\napproach that focuses on achieving a desired goal configuration of objects\nrelative to one another while maintaining the sequence of operations. VSL is\ncapable of learning and generalizing multi-operation skills from a single\ndemonstration, while requiring minimum prior knowledge about the objects and\nthe environment. In contrast to many existing approaches, VSL offers\nsimplicity, efficiency and user-friendly human-robot interaction. We also show\nthat VSL can be easily extended towards 3D object manipulation tasks, simply by\nemploying point cloud processing techniques. In addition, a robot learning\nframework, VSL-SP, is proposed by integrating VSL, Imitation Learning, and a\nconventional planning method. In VSL-SP, the sequence of performed actions are\nlearned using VSL, while the sensorimotor skills are learned using a\nconventional trajectory-based learning approach. such integration easily\nextends robot capabilities to novel situations, even by users without\nprogramming ability. In VSL-SP the internal planner of VSL is integrated with\nan existing action-level symbolic planner. Using the underlying constraints of\nthe task and extracted symbolic predicates, identified by VSL, symbolic\nrepresentation of the task is updated. Therefore the planner maintains a\ngeneralized representation of each skill as a reusable action, which can be\nused in planning and performed independently during the learning phase. The\nproposed approach is validated through several real-world experiments.\n", "versions": [{"version": "v1", "created": "Sat, 3 Jun 2017 18:45:34 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Ahmadzadeh", "S. Reza", ""], ["Mastrogiovanni", "Fulvio", ""], ["Kormushev", "Petar", ""]]}, {"id": "1706.01077", "submitter": "Tomoki Nishi", "authors": "Tomoki Nishi and Prashant Doshi and Michael R. James and Danil\n  Prokhorov", "title": "Actor-Critic for Linearly-Solvable Continuous MDP with Partially Known\n  Dynamics", "comments": "10 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In many robotic applications, some aspects of the system dynamics can be\nmodeled accurately while others are difficult to obtain or model. We present a\nnovel reinforcement learning (RL) method for continuous state and action spaces\nthat learns with partial knowledge of the system and without active\nexploration. It solves linearly-solvable Markov decision processes (L-MDPs),\nwhich are well suited for continuous state and action spaces, based on an\nactor-critic architecture. Compared to previous RL methods for L-MDPs and path\nintegral methods which are model based, the actor-critic learning does not need\na model of the uncontrolled dynamics and, importantly, transition noise levels;\nhowever, it requires knowing the control dynamics for the problem. We evaluate\nour method on two synthetic test problems, and one real-world problem in\nsimulation and using real traffic data. Our experiments demonstrate improved\nlearning and policy performance.\n", "versions": [{"version": "v1", "created": "Sun, 4 Jun 2017 14:02:01 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Nishi", "Tomoki", ""], ["Doshi", "Prashant", ""], ["James", "Michael R.", ""], ["Prokhorov", "Danil", ""]]}, {"id": "1706.01284", "submitter": "Xinyun Chen", "authors": "Xinyun Chen, Chang Liu, Dawn Song", "title": "Towards Synthesizing Complex Programs from Input-Output Examples", "comments": "Published as a conference paper at ICLR 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, deep learning techniques have been developed to improve the\nperformance of program synthesis from input-output examples. Albeit its\nsignificant progress, the programs that can be synthesized by state-of-the-art\napproaches are still simple in terms of their complexity. In this work, we move\na significant step forward along this direction by proposing a new class of\nchallenging tasks in the domain of program synthesis from input-output\nexamples: learning a context-free parser from pairs of input programs and their\nparse trees. We show that this class of tasks are much more challenging than\npreviously studied tasks, and the test accuracy of existing approaches is\nalmost 0%.\n  We tackle the challenges by developing three novel techniques inspired by\nthree novel observations, which reveal the key ingredients of using deep\nlearning to synthesize a complex program. First, the use of a\nnon-differentiable machine is the key to effectively restrict the search space.\nThus our proposed approach learns a neural program operating a domain-specific\nnon-differentiable machine. Second, recursion is the key to achieve\ngeneralizability. Thus, we bake-in the notion of recursion in the design of our\nnon-differentiable machine. Third, reinforcement learning is the key to learn\nhow to operate the non-differentiable machine, but it is also hard to train the\nmodel effectively with existing reinforcement learning algorithms from a cold\nboot. We develop a novel two-phase reinforcement learning-based search\nalgorithm to overcome this issue. In our evaluation, we show that using our\nnovel approach, neural parsing programs can be learned to achieve 100% test\naccuracy on test inputs that are 500x longer than the training samples.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 11:44:35 GMT"}, {"version": "v2", "created": "Tue, 30 Jan 2018 04:54:32 GMT"}, {"version": "v3", "created": "Sun, 11 Feb 2018 04:33:30 GMT"}, {"version": "v4", "created": "Thu, 8 Mar 2018 00:22:59 GMT"}], "update_date": "2018-03-09", "authors_parsed": [["Chen", "Xinyun", ""], ["Liu", "Chang", ""], ["Song", "Dawn", ""]]}, {"id": "1706.01303", "submitter": "Roman Yampolskiy", "authors": "Roman V. Yampolskiy", "title": "The Singularity May Be Near", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Toby Walsh in 'The Singularity May Never Be Near' gives six arguments to\nsupport his point of view that technological singularity may happen but that it\nis unlikely. In this paper, we provide analysis of each one of his arguments\nand arrive at similar conclusions, but with more weight given to the 'likely to\nhappen' probability.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 19:42:06 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Yampolskiy", "Roman V.", ""]]}, {"id": "1706.01320", "submitter": "Diptangshu Pandit", "authors": "Diptangshu Pandit", "title": "3D Pathfinding and Collision Avoidance Using Uneven Search-space\n  Quantization and Visual Cone Search", "comments": "major problems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pathfinding is a very popular area in computer game development. While\ntwo-dimensional (2D) pathfinding is widely applied in most of the popular game\nengines, little implementation of real three-dimensional (3D) pathfinding can\nbe found. This research presents a dynamic search space optimization algorithm\nwhich can be applied to tessellate 3D search space unevenly, significantly\nreducing the total number of resulting nodes. The algorithm can be used with\npopular pathfinding algorithms in 3D game engines. Furthermore, a simplified\nstandalone 3D pathfinding algorithm is proposed in this paper. The proposed\nalgorithm relies on ray-casting or line vision to generate a feasible path\nduring runtime without requiring division of the search space into a 3D grid.\nBoth of the proposed algorithms are simulated on Unreal Engine to show\ninnerworkings and resultant path comparison with A*. The advantages and\nshortcomings of the proposed algorithms are also discussed along with future\ndirections.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 13:49:49 GMT"}, {"version": "v2", "created": "Tue, 10 Apr 2018 23:47:51 GMT"}, {"version": "v3", "created": "Tue, 19 Jun 2018 16:01:28 GMT"}], "update_date": "2018-06-20", "authors_parsed": [["Pandit", "Diptangshu", ""]]}, {"id": "1706.01322", "submitter": "Alexander Kuhnle", "authors": "Alexander Kuhnle and Ann Copestake", "title": "Deep learning evaluation using deep linguistic processing", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We discuss problems with the standard approaches to evaluation for tasks like\nvisual question answering, and argue that artificial data can be used to\naddress these as a complement to current practice. We demonstrate that with the\nhelp of existing 'deep' linguistic processing technology we are able to create\nchallenging abstract datasets, which enable us to investigate the language\nunderstanding abilities of multimodal deep learning models in detail, as\ncompared to a single performance value on a static and monolithic dataset.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 13:53:56 GMT"}, {"version": "v2", "created": "Sat, 12 May 2018 10:37:02 GMT"}], "update_date": "2018-05-15", "authors_parsed": [["Kuhnle", "Alexander", ""], ["Copestake", "Ann", ""]]}, {"id": "1706.01331", "submitter": "Mark Riedl", "authors": "Lara J. Martin, Prithviraj Ammanabrolu, Xinyu Wang, William Hancock,\n  Shruti Singh, Brent Harrison, Mark O. Riedl", "title": "Event Representations for Automated Story Generation with Deep Neural\n  Nets", "comments": "Submitted to AAAI'18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automated story generation is the problem of automatically selecting a\nsequence of events, actions, or words that can be told as a story. We seek to\ndevelop a system that can generate stories by learning everything it needs to\nknow from textual story corpora. To date, recurrent neural networks that learn\nlanguage models at character, word, or sentence levels have had little success\ngenerating coherent stories. We explore the question of event representations\nthat provide a mid-level of abstraction between words and sentences in order to\nretain the semantic information of the original data while minimizing event\nsparsity. We present a technique for preprocessing textual story data into\nevent sequences. We then present a technique for automated story generation\nwhereby we decompose the problem into the generation of successive events\n(event2event) and the generation of natural language sentences from events\n(event2sentence). We give empirical results comparing different event\nrepresentations and their effects on event successor generation and the\ntranslation of events to natural language.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 14:04:48 GMT"}, {"version": "v2", "created": "Mon, 14 Aug 2017 18:14:02 GMT"}, {"version": "v3", "created": "Tue, 12 Sep 2017 14:45:22 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Martin", "Lara J.", ""], ["Ammanabrolu", "Prithviraj", ""], ["Wang", "Xinyu", ""], ["Hancock", "William", ""], ["Singh", "Shruti", ""], ["Harrison", "Brent", ""], ["Riedl", "Mark O.", ""]]}, {"id": "1706.01350", "submitter": "Alessandro Achille", "authors": "Alessandro Achille and Stefano Soatto", "title": "Emergence of Invariance and Disentanglement in Deep Representations", "comments": "Deep learning, neural network, representation, flat minima,\n  information bottleneck, overfitting, generalization, sufficiency, minimality,\n  sensitivity, information complexity, stochastic gradient descent,\n  regularization, total correlation, PAC-Bayes", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Using established principles from Statistics and Information Theory, we show\nthat invariance to nuisance factors in a deep neural network is equivalent to\ninformation minimality of the learned representation, and that stacking layers\nand injecting noise during training naturally bias the network towards learning\ninvariant representations. We then decompose the cross-entropy loss used during\ntraining and highlight the presence of an inherent overfitting term. We propose\nregularizing the loss by bounding such a term in two equivalent ways: One with\na Kullbach-Leibler term, which relates to a PAC-Bayes perspective; the other\nusing the information in the weights as a measure of complexity of a learned\nmodel, yielding a novel Information Bottleneck for the weights. Finally, we\nshow that invariance and independence of the components of the representation\nlearned by the network are bounded above and below by the information in the\nweights, and therefore are implicitly optimized during training. The theory\nenables us to quantify and predict sharp phase transitions between underfitting\nand overfitting of random labels when using our regularized loss, which we\nverify in experiments, and sheds light on the relation between the geometry of\nthe loss function, invariance properties of the learned representation, and\ngeneralization error.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 14:31:03 GMT"}, {"version": "v2", "created": "Mon, 16 Oct 2017 01:21:49 GMT"}, {"version": "v3", "created": "Thu, 28 Jun 2018 17:50:54 GMT"}], "update_date": "2018-06-29", "authors_parsed": [["Achille", "Alessandro", ""], ["Soatto", "Stefano", ""]]}, {"id": "1706.01417", "submitter": "Leonardo Anjoletto Ferreira", "authors": "Leonardo A. Ferreira, Reinaldo A. C. Bianchi, Paulo E. Santos, Ramon\n  Lopez de Mantaras", "title": "A method for the online construction of the set of states of a Markov\n  Decision Process using Answer Set Programming", "comments": "Submitted to IJCAI 17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Non-stationary domains, that change in unpredicted ways, are a challenge for\nagents searching for optimal policies in sequential decision-making problems.\nThis paper presents a combination of Markov Decision Processes (MDP) with\nAnswer Set Programming (ASP), named {\\em Online ASP for MDP} (oASP(MDP)), which\nis a method capable of constructing the set of domain states while the agent\ninteracts with a changing environment. oASP(MDP) updates previously obtained\npolicies, learnt by means of Reinforcement Learning (RL), using rules that\nrepresent the domain changes observed by the agent. These rules represent a set\nof domain constraints that are processed as ASP programs reducing the search\nspace. Results show that oASP(MDP) is capable of finding solutions for problems\nin non-stationary domains without interfering with the action-value function\napproximation process.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 16:48:23 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Ferreira", "Leonardo A.", ""], ["Bianchi", "Reinaldo A. C.", ""], ["Santos", "Paulo E.", ""], ["de Mantaras", "Ramon Lopez", ""]]}, {"id": "1706.01443", "submitter": "Camilo Miguel Signorelli", "authors": "Camilo Miguel Signorelli", "title": "Types of Cognition and its Implications for future High-Level Cognitive\n  Machines", "comments": "2017 AAAI Spring Symposium Series, Science of Intelligence:\n  Computational Principles of Natural and Artificial Intelligence", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work summarizes part of current knowledge on High-level Cognitive\nprocess and its relation with biological hardware. Thus, it is possible to\nidentify some paradoxes which could impact the development of future\ntechnologies and artificial intelligence: we may make a High-level Cognitive\nMachine, sacrificing the principal attribute of a machine, its accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 17:47:58 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Signorelli", "Camilo Miguel", ""]]}, {"id": "1706.01450", "submitter": "Tong Wang", "authors": "Tong Wang and Xingdi Yuan and Adam Trischler", "title": "A Joint Model for Question Answering and Question Generation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a generative machine comprehension model that learns jointly to\nask and answer questions based on documents. The proposed model uses a\nsequence-to-sequence framework that encodes the document and generates a\nquestion (answer) given an answer (question). Significant improvement in model\nperformance is observed empirically on the SQuAD corpus, confirming our\nhypothesis that the model benefits from jointly learning to perform both tasks.\nWe believe the joint model's novelty offers a new perspective on machine\ncomprehension beyond architectural engineering, and serves as a first step\ntowards autonomous information seeking.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 17:58:52 GMT"}], "update_date": "2017-06-06", "authors_parsed": [["Wang", "Tong", ""], ["Yuan", "Xingdi", ""], ["Trischler", "Adam", ""]]}, {"id": "1706.01554", "submitter": "Jiasen Lu", "authors": "Jiasen Lu, Anitha Kannan, Jianwei Yang, Devi Parikh, Dhruv Batra", "title": "Best of Both Worlds: Transferring Knowledge from Discriminative Learning\n  to a Generative Visual Dialog Model", "comments": "11 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel training framework for neural sequence models,\nparticularly for grounded dialog generation. The standard training paradigm for\nthese models is maximum likelihood estimation (MLE), or minimizing the\ncross-entropy of the human responses. Across a variety of domains, a recurring\nproblem with MLE trained generative neural dialog models (G) is that they tend\nto produce 'safe' and generic responses (\"I don't know\", \"I can't tell\"). In\ncontrast, discriminative dialog models (D) that are trained to rank a list of\ncandidate human responses outperform their generative counterparts; in terms of\nautomatic metrics, diversity, and informativeness of the responses. However, D\nis not useful in practice since it cannot be deployed to have real\nconversations with users.\n  Our work aims to achieve the best of both worlds -- the practical usefulness\nof G and the strong performance of D -- via knowledge transfer from D to G. Our\nprimary contribution is an end-to-end trainable generative visual dialog model,\nwhere G receives gradients from D as a perceptual (not adversarial) loss of the\nsequence sampled from G. We leverage the recently proposed Gumbel-Softmax (GS)\napproximation to the discrete distribution -- specifically, an RNN augmented\nwith a sequence of GS samplers, coupled with the straight-through gradient\nestimator to enable end-to-end differentiability. We also introduce a stronger\nencoder for visual dialog, and employ a self-attention mechanism for answer\nencoding along with a metric learning loss to aid D in better capturing\nsemantic similarities in answer responses. Overall, our proposed model\noutperforms state-of-the-art on the VisDial dataset by a significant margin\n(2.67% on recall@10). The source code can be downloaded from\nhttps://github.com/jiasenlu/visDial.pytorch.\n", "versions": [{"version": "v1", "created": "Mon, 5 Jun 2017 22:50:37 GMT"}, {"version": "v2", "created": "Fri, 27 Oct 2017 20:27:07 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Lu", "Jiasen", ""], ["Kannan", "Anitha", ""], ["Yang", "Jianwei", ""], ["Parikh", "Devi", ""], ["Batra", "Dhruv", ""]]}, {"id": "1706.01574", "submitter": "Rishabh Mehrotra", "authors": "Rishabh Mehrotra and Emine Yilmaz", "title": "Extracting Hierarchies of Search Tasks & Subtasks via a Bayesian\n  Nonparametric Approach", "comments": "10 pages. Accepted at SIGIR 2017 as a full paper", "journal-ref": null, "doi": "10.1145/3077136.3080823", "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A significant amount of search queries originate from some real world\ninformation need or tasks. In order to improve the search experience of the end\nusers, it is important to have accurate representations of tasks. As a result,\nsignificant amount of research has been devoted to extracting proper\nrepresentations of tasks in order to enable search systems to help users\ncomplete their tasks, as well as providing the end user with better query\nsuggestions, for better recommendations, for satisfaction prediction, and for\nimproved personalization in terms of tasks. Most existing task extraction\nmethodologies focus on representing tasks as flat structures. However, tasks\noften tend to have multiple subtasks associated with them and a more\nnaturalistic representation of tasks would be in terms of a hierarchy, where\neach task can be composed of multiple (sub)tasks. To this end, we propose an\nefficient Bayesian nonparametric model for extracting hierarchies of such tasks\n\\& subtasks. We evaluate our method based on real world query log data both\nthrough quantitative and crowdsourced experiments and highlight the importance\nof considering task/subtask hierarchies.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 01:10:51 GMT"}, {"version": "v2", "created": "Wed, 7 Jun 2017 01:47:30 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Mehrotra", "Rishabh", ""], ["Yilmaz", "Emine", ""]]}, {"id": "1706.01758", "submitter": "Ming Li", "authors": "Ming Li, Peilun Xiao, Ju Zhang", "title": "A WL-SPPIM Semantic Model for Document Classification", "comments": "7pages, 5figures, Keywords: LDA, SPPIM, word embedding, low\n  frequency, document classification", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we explore SPPIM-based text classification method, and the\nexperiment reveals that the SPPIM method is equal to or even superior than SGNS\nmethod in text classification task on three international and standard text\ndatasets, namely 20newsgroups, Reuters52 and WebKB. Comparing to SGNS, although\nSPPMI provides a better solution, it is not necessarily better than SGNS in\ntext classification tasks. Based on our analysis, SGNS takes into the\nconsideration of weight calculation during decomposition process, so it has\nbetter performance than SPPIM in some standard datasets. Inspired by this, we\npropose a WL-SPPIM semantic model based on SPPIM model, and experiment shows\nthat WL-SPPIM approach has better classification and higher scalability in the\ntext classification task compared with LDA, SGNS and SPPIM approaches.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 08:03:10 GMT"}], "update_date": "2017-06-07", "authors_parsed": [["Li", "Ming", ""], ["Xiao", "Peilun", ""], ["Zhang", "Ju", ""]]}, {"id": "1706.01763", "submitter": "Yanjun  Qi Dr.", "authors": "Andrew Norton and Yanjun Qi", "title": "Adversarial-Playground: A Visualization Suite for Adversarial Sample\n  Generation", "comments": "8 pages; 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With growing interest in adversarial machine learning, it is important for\nmachine learning practitioners and users to understand how their models may be\nattacked. We propose a web-based visualization tool, Adversarial-Playground, to\ndemonstrate the efficacy of common adversarial methods against a deep neural\nnetwork (DNN) model, built on top of the TensorFlow library.\nAdversarial-Playground provides users an efficient and effective experience in\nexploring techniques generating adversarial examples, which are inputs crafted\nby an adversary to fool a machine learning system. To enable\nAdversarial-Playground to generate quick and accurate responses for users, we\nuse two primary tactics: (1) We propose a faster variant of the\nstate-of-the-art Jacobian saliency map approach that maintains a comparable\nevasion rate. (2) Our visualization does not transmit the generated adversarial\nimages to the client, but rather only the matrix describing the sample and the\nvector representing classification likelihoods.\n  The source code along with the data from all of our experiments are available\nat \\url{https://github.com/QData/AdversarialDNN-Playground}.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 13:43:11 GMT"}, {"version": "v2", "created": "Fri, 16 Jun 2017 16:38:09 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["Norton", "Andrew", ""], ["Qi", "Yanjun", ""]]}, {"id": "1706.01863", "submitter": "Peter Sch\\\"uller", "authors": "Peter Sch\\\"uller and K\\\"ubra C{\\i}ng{\\i}ll{\\i} and Ferit Tun\\c{c}er\n  and Bar{\\i}\\c{s} G\\\"un S\\\"urmeli and Ay\\c{s}eg\\\"ul Pekel and Ay\\c{s}e Hande\n  Karatay and Hacer Ezgi Karaka\\c{s}", "title": "Marmara Turkish Coreference Corpus and Coreference Resolution Baseline", "comments": "Submitted to Natural Language Engineering", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe the Marmara Turkish Coreference Corpus, which is an annotation of\nthe whole METU-Sabanci Turkish Treebank with mentions and coreference chains.\nCollecting eight or more independent annotations for each document allowed for\nfully automatic adjudication. We provide a baseline system for Turkish mention\ndetection and coreference resolution and evaluate it on the corpus.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 17:25:36 GMT"}, {"version": "v2", "created": "Tue, 31 Jul 2018 15:15:05 GMT"}], "update_date": "2018-08-01", "authors_parsed": [["Sch\u00fcller", "Peter", ""], ["C\u0131ng\u0131ll\u0131", "K\u00fcbra", ""], ["Tun\u00e7er", "Ferit", ""], ["S\u00fcrmeli", "Bar\u0131\u015f G\u00fcn", ""], ["Pekel", "Ay\u015feg\u00fcl", ""], ["Karatay", "Ay\u015fe Hande", ""], ["Karaka\u015f", "Hacer Ezgi", ""]]}, {"id": "1706.01905", "submitter": "Matthias Plappert", "authors": "Matthias Plappert, Rein Houthooft, Prafulla Dhariwal, Szymon Sidor,\n  Richard Y. Chen, Xi Chen, Tamim Asfour, Pieter Abbeel, Marcin Andrychowicz", "title": "Parameter Space Noise for Exploration", "comments": "Updated to camera-ready ICLR submission", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning (RL) methods generally engage in exploratory\nbehavior through noise injection in the action space. An alternative is to add\nnoise directly to the agent's parameters, which can lead to more consistent\nexploration and a richer set of behaviors. Methods such as evolutionary\nstrategies use parameter perturbations, but discard all temporal structure in\nthe process and require significantly more samples. Combining parameter noise\nwith traditional RL methods allows to combine the best of both worlds. We\ndemonstrate that both off- and on-policy methods benefit from this approach\nthrough experimental comparison of DQN, DDPG, and TRPO on high-dimensional\ndiscrete action environments as well as continuous control tasks. Our results\nshow that RL with parameter noise learns more efficiently than traditional RL\nwith action space noise and evolutionary strategies individually.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 18:09:29 GMT"}, {"version": "v2", "created": "Wed, 31 Jan 2018 09:05:10 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Plappert", "Matthias", ""], ["Houthooft", "Rein", ""], ["Dhariwal", "Prafulla", ""], ["Sidor", "Szymon", ""], ["Chen", "Richard Y.", ""], ["Chen", "Xi", ""], ["Asfour", "Tamim", ""], ["Abbeel", "Pieter", ""], ["Andrychowicz", "Marcin", ""]]}, {"id": "1706.01991", "submitter": "Son Tran", "authors": "Son N. Tran", "title": "Unsupervised Neural-Symbolic Integration", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Symbolic has been long considered as a language of human intelligence while\nneural networks have advantages of robust computation and dealing with noisy\ndata. The integration of neural-symbolic can offer better learning and\nreasoning while providing a means for interpretability through the\nrepresentation of symbolic knowledge. Although previous works focus intensively\non supervised feedforward neural networks, little has been done for the\nunsupervised counterparts. In this paper we show how to integrate symbolic\nknowledge into unsupervised neural networks. We exemplify our approach with\nknowledge in different forms, including propositional logic for DNA promoter\nprediction and first-order logic for understanding family relationship.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 21:58:50 GMT"}, {"version": "v2", "created": "Thu, 22 Jun 2017 04:11:21 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Tran", "Son N.", ""]]}, {"id": "1706.02048", "submitter": "Yifeng Ding", "authors": "Yifeng Ding", "title": "Epistemic Logic with Functional Dependency Operator", "comments": null, "journal-ref": "Studies in Logic, Vol. 9, No. 4 (2016): 55-84", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Epistemic logic with non-standard knowledge operators, especially the\n\"knowing-value\" operator, has recently gathered much attention. With the\n\"knowing-value\" operator, we can express knowledge of individual variables, but\nnot of the relations between them in general. In this paper, we propose a new\noperator Kf to express knowledge of the functional dependencies between\nvariables. The semantics of this Kf operator uses a function domain which\nimposes a constraint on what counts as a functional dependency relation. By\nadjusting this function domain, different interesting logics arise, and in this\npaper we axiomatize three such logics in a single agent setting. Then we show\nhow these three logics can be unified by allowing the function domain to vary\nrelative to different agents and possible worlds. A multiagent axiomatization\nis given in this case.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 05:16:54 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Ding", "Yifeng", ""]]}, {"id": "1706.02109", "submitter": "Maikel Van Eck", "authors": "Maikel L. van Eck, Natalia Sidorova, Wil M.P. van der Aalst", "title": "Guided Interaction Exploration in Artifact-centric Process Models", "comments": "10 pages, 4 figures, to be published in proceedings of the 19th IEEE\n  Conference on Business Informatics, CBI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artifact-centric process models aim to describe complex processes as a\ncollection of interacting artifacts. Recent development in process mining allow\nfor the discovery of such models. However, the focus is often on the\nrepresentation of the individual artifacts rather than their interactions.\nBased on event data we can automatically discover composite state machines\nrepresenting artifact-centric processes. Moreover, we provide ways of\nvisualizing and quantifying interactions among different artifacts. For\nexample, we are able to highlight strongly correlated behaviours in different\nartifacts. The approach has been fully implemented as a ProM plug-in; the CSM\nMiner provides an interactive artifact-centric process discovery tool focussing\non interactions. The approach has been evaluated using real life data sets,\nincluding the personal loan and overdraft process of a Dutch financial\ninstitution.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 09:55:48 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["van Eck", "Maikel L.", ""], ["Sidorova", "Natalia", ""], ["van der Aalst", "Wil M. P.", ""]]}, {"id": "1706.02141", "submitter": "Carlos G\\'omez-Rodr\\'iguez", "authors": "Carlos G\\'omez-Rodr\\'iguez, Iago Alonso-Alonso, David Vilares", "title": "How Important is Syntactic Parsing Accuracy? An Empirical Evaluation on\n  Rule-Based Sentiment Analysis", "comments": "19 pages. Accepted for publication in Artificial Intelligence Review.\n  This update only adds the DOI link to comply with journal's terms", "journal-ref": null, "doi": "10.1007/s10462-017-9584-0", "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Syntactic parsing, the process of obtaining the internal structure of\nsentences in natural languages, is a crucial task for artificial intelligence\napplications that need to extract meaning from natural language text or speech.\nSentiment analysis is one example of application for which parsing has recently\nproven useful.\n  In recent years, there have been significant advances in the accuracy of\nparsing algorithms. In this article, we perform an empirical, task-oriented\nevaluation to determine how parsing accuracy influences the performance of a\nstate-of-the-art rule-based sentiment analysis system that determines the\npolarity of sentences from their parse trees. In particular, we evaluate the\nsystem using four well-known dependency parsers, including both current models\nwith state-of-the-art accuracy and more innacurate models which, however,\nrequire less computational resources.\n  The experiments show that all of the parsers produce similarly good results\nin the sentiment analysis task, without their accuracy having any relevant\ninfluence on the results. Since parsing is currently a task with a relatively\nhigh computational cost that varies strongly between algorithms, this suggests\nthat sentiment analysis researchers and users should prioritize speed over\naccuracy when choosing a parser; and parsing researchers should investigate\nmodels that improve speed further, even at some cost to accuracy.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 12:03:07 GMT"}, {"version": "v2", "created": "Mon, 2 Oct 2017 09:17:39 GMT"}, {"version": "v3", "created": "Tue, 24 Oct 2017 08:13:38 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["G\u00f3mez-Rodr\u00edguez", "Carlos", ""], ["Alonso-Alonso", "Iago", ""], ["Vilares", "David", ""]]}, {"id": "1706.02179", "submitter": "S\\'ebastien Ehrhardt", "authors": "S\\'ebastien Ehrhardt, Aron Monszpart, Andrea Vedaldi, Niloy Mitra", "title": "Learning to Represent Mechanics via Long-term Extrapolation and\n  Interpolation", "comments": "arXiv admin note: text overlap with arXiv:1703.00247", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  While the basic laws of Newtonian mechanics are well understood, explaining a\nphysical scenario still requires manually modeling the problem with suitable\nequations and associated parameters. In order to adopt such models for\nartificial intelligence, researchers have handcrafted the relevant states, and\nthen used neural networks to learn the state transitions using simulation runs\nas training data. Unfortunately, such approaches can be unsuitable for modeling\ncomplex real-world scenarios, where manually authoring relevant state spaces\ntend to be challenging. In this work, we investigate if neural networks can\nimplicitly learn physical states of real-world mechanical processes only based\non visual data, and thus enable long-term physical extrapolation. We develop a\nrecurrent neural network architecture for this task and also characterize\nresultant uncertainties in the form of evolving variance estimates. We evaluate\nour setup to extrapolate motion of a rolling ball on bowl of varying shape and\norientation using only images as input, and report competitive results with\napproaches that assume access to internal physics models and parameters.\n", "versions": [{"version": "v1", "created": "Tue, 6 Jun 2017 15:45:48 GMT"}, {"version": "v2", "created": "Thu, 8 Jun 2017 09:31:22 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Ehrhardt", "S\u00e9bastien", ""], ["Monszpart", "Aron", ""], ["Vedaldi", "Andrea", ""], ["Mitra", "Niloy", ""]]}, {"id": "1706.02209", "submitter": "Gauthier Picard", "authors": "Jes\\'us Cerquides (IIIA / CSIC), R\\'emi Emonet (LHC), Gauthier Picard\n  (LHC, ISCOD-ENSMSE), Juan A. Rodr\\'iguez-Aguilar (IIIA / CSIC)", "title": "Improving Max-Sum through Decimation to Solve Loopy Distributed\n  Constraint Optimization Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the context of solving large distributed constraint optimization problems\n(DCOP), belief-propagation and approximate inference algorithms are candidates\nof choice. However, in general, when the factor graph is very loopy (i.e.\ncyclic), these solution methods suffer from bad performance, due to\nnon-convergence and many exchanged messages. As to improve performances of the\nMax-Sum inference algorithm when solving loopy constraint optimization\nproblems, we propose here to take inspiration from the\nbelief-propagation-guided dec-imation used to solve sparse random graphs\n(k-satisfiability). We propose the novel DeciMaxSum method, which is\nparameterized in terms of policies to decide when to trigger decimation, which\nvariables to decimate, and which values to assign to decimated variables. Based\non an empirical evaluation on a classical BP benchmark (the Ising model), some\nof these combinations of policies exhibit better performance than\nstate-of-the-art competitors.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 14:29:23 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Cerquides", "Jes\u00fas", "", "IIIA / CSIC"], ["Emonet", "R\u00e9mi", "", "LHC"], ["Picard", "Gauthier", "", "LHC, ISCOD-ENSMSE"], ["Rodr\u00edguez-Aguilar", "Juan A.", "", "IIIA / CSIC"]]}, {"id": "1706.02240", "submitter": "Martin Schrimpf", "authors": "Hanlin Tang, Martin Schrimpf, Bill Lotter, Charlotte Moerman, Ana\n  Paredes, Josue Ortega Caro, Walter Hardesty, David Cox, Gabriel Kreiman", "title": "Recurrent computations for visual pattern completion", "comments": null, "journal-ref": null, "doi": "10.1073/pnas.1719397115", "report-no": null, "categories": "q-bio.NC cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Making inferences from partial information constitutes a critical aspect of\ncognition. During visual perception, pattern completion enables recognition of\npoorly visible or occluded objects. We combined psychophysics, physiology and\ncomputational models to test the hypothesis that pattern completion is\nimplemented by recurrent computations and present three pieces of evidence that\nare consistent with this hypothesis. First, subjects robustly recognized\nobjects even when rendered <15% visible, but recognition was largely impaired\nwhen processing was interrupted by backward masking. Second, invasive\nphysiological responses along the human ventral cortex exhibited visually\nselective responses to partially visible objects that were delayed compared to\nwhole objects, suggesting the need for additional computations. These\nphysiological delays were correlated with the effects of backward masking.\nThird, state-of-the-art feed-forward computational architectures were not\nrobust to partial visibility. However, recognition performance was recovered\nwhen the model was augmented with attractor-based recurrent connectivity. These\nresults provide a strong argument of plausibility for the role of recurrent\ncomputations in making visual inferences from partial information.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 16:23:28 GMT"}, {"version": "v2", "created": "Fri, 6 Apr 2018 12:29:22 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Tang", "Hanlin", ""], ["Schrimpf", "Martin", ""], ["Lotter", "Bill", ""], ["Moerman", "Charlotte", ""], ["Paredes", "Ana", ""], ["Caro", "Josue Ortega", ""], ["Hardesty", "Walter", ""], ["Cox", "David", ""], ["Kreiman", "Gabriel", ""]]}, {"id": "1706.02246", "submitter": "Jonatan Gomez", "authors": "Jonatan Gomez", "title": "Stochastic Global Optimization Algorithms: A Systematic Formal Approach", "comments": "30 pages, 0 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As we know, some global optimization problems cannot be solved using analytic\nmethods, so numeric/algorithmic approaches are used to find near to the optimal\nsolutions for them. A stochastic global optimization algorithm (SGoal) is an\niterative algorithm that generates a new population (a set of candidate\nsolutions) from a previous population using stochastic operations. Although\nsome research works have formalized SGoals using Markov kernels, such\nformalization is not general and sometimes is blurred. In this paper, we\npropose a comprehensive and systematic formal approach for studying SGoals.\nFirst, we present the required theory of probability (\\sigma-algebras,\nmeasurable functions, kernel, markov chain, products, convergence and so on)\nand prove that some algorithmic functions like swapping and projection can be\nrepresented by kernels. Then, we introduce the notion of join-kernel as a way\nof characterizing the combination of stochastic methods. Next, we define the\noptimization space, a formal structure (a set with a \\sigma-algebra that\ncontains strict \\epsilon-optimal states) for studying SGoals, and we develop\nkernels, like sort and permutation, on such structure. Finally, we present some\npopular SGoals in terms of the developed theory, we introduce sufficient\nconditions for convergence of a SGoal, and we prove convergence of some popular\nSGoals.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 16:38:03 GMT"}], "update_date": "2017-06-08", "authors_parsed": [["Gomez", "Jonatan", ""]]}, {"id": "1706.02257", "submitter": "Oluwatobi Olabiyi", "authors": "Oluwatobi Olabiyi, Eric Martinson, Vijay Chintalapudi, Rui Guo", "title": "Driver Action Prediction Using Deep (Bidirectional) Recurrent Neural\n  Network", "comments": "ITSC'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advanced driver assistance systems (ADAS) can be significantly improved with\neffective driver action prediction (DAP). Predicting driver actions early and\naccurately can help mitigate the effects of potentially unsafe driving\nbehaviors and avoid possible accidents. In this paper, we formulate driver\naction prediction as a timeseries anomaly prediction problem. While the anomaly\n(driver actions of interest) detection might be trivial in this context,\nfinding patterns that consistently precede an anomaly requires searching for or\nextracting features across multi-modal sensory inputs. We present such a driver\naction prediction system, including a real-time data acquisition, processing\nand learning framework for predicting future or impending driver action. The\nproposed system incorporates camera-based knowledge of the driving environment\nand the driver themselves, in addition to traditional vehicle dynamics. It then\nuses a deep bidirectional recurrent neural network (DBRNN) to learn the\ncorrelation between sensory inputs and impending driver behavior achieving\naccurate and high horizon action prediction. The proposed system performs\nbetter than other existing systems on driver action prediction tasks and can\naccurately predict key driver actions including acceleration, braking, lane\nchange and turning at durations of 5sec before the action is executed by the\ndriver.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 17:00:08 GMT"}], "update_date": "2018-06-01", "authors_parsed": [["Olabiyi", "Oluwatobi", ""], ["Martinson", "Eric", ""], ["Chintalapudi", "Vijay", ""], ["Guo", "Rui", ""]]}, {"id": "1706.02262", "submitter": "Shengjia Zhao", "authors": "Shengjia Zhao, Jiaming Song, Stefano Ermon", "title": "InfoVAE: Information Maximizing Variational Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key advance in learning generative models is the use of amortized inference\ndistributions that are jointly trained with the models. We find that existing\ntraining objectives for variational autoencoders can lead to inaccurate\namortized inference distributions and, in some cases, improving the objective\nprovably degrades the inference quality. In addition, it has been observed that\nvariational autoencoders tend to ignore the latent variables when combined with\na decoding distribution that is too flexible. We again identify the cause in\nexisting training criteria and propose a new class of objectives (InfoVAE) that\nmitigate these problems. We show that our model can significantly improve the\nquality of the variational posterior and can make effective use of the latent\nfeatures regardless of the flexibility of the decoding distribution. Through\nextensive qualitative and quantitative analyses, we demonstrate that our models\noutperform competing approaches on multiple performance metrics.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 17:05:01 GMT"}, {"version": "v2", "created": "Fri, 17 Nov 2017 18:35:30 GMT"}, {"version": "v3", "created": "Wed, 30 May 2018 17:28:31 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Zhao", "Shengjia", ""], ["Song", "Jiaming", ""], ["Ermon", "Stefano", ""]]}, {"id": "1706.02274", "submitter": "Camilo Miguel Signorelli", "authors": "Camilo Miguel Signorelli", "title": "Can Computers overcome Humans? Consciousness interaction and its\n  implications", "comments": "16th IEEE Cognitive Informatics and Cognitive Computing preprint, 8\n  pages; Added references and short discussion for section 6", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Can computers overcome human capabilities? This is a paradoxical and\ncontroversial question, particularly because there are many hidden assumptions.\nThis article focuses on that issue putting on evidence some misconception\nrelated with future generations of machines and the understanding of the brain.\nIt will be discussed to what extent computers might reach human capabilities,\nand how it could be possible only if the computer is a conscious machine.\nHowever, it will be shown that if the computer is conscious, an interference\nprocess due to consciousness would affect the information processing of the\nsystem. Therefore, it might be possible to make conscious machines to overcome\nhuman capabilities, which will have limitations as well as humans. In other\nwords, trying to overcome human capabilities with computers implies the\nparadoxical conclusion that a computer will never overcome human capabilities\nat all, or if the computer does, it should not be considered as a computer\nanymore.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 17:34:49 GMT"}, {"version": "v2", "created": "Mon, 26 Jun 2017 06:50:02 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Signorelli", "Camilo Miguel", ""]]}, {"id": "1706.02275", "submitter": "Ryan Lowe T.", "authors": "Ryan Lowe, Yi Wu, Aviv Tamar, Jean Harb, Pieter Abbeel, Igor Mordatch", "title": "Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We explore deep reinforcement learning methods for multi-agent domains. We\nbegin by analyzing the difficulty of traditional algorithms in the multi-agent\ncase: Q-learning is challenged by an inherent non-stationarity of the\nenvironment, while policy gradient suffers from a variance that increases as\nthe number of agents grows. We then present an adaptation of actor-critic\nmethods that considers action policies of other agents and is able to\nsuccessfully learn policies that require complex multi-agent coordination.\nAdditionally, we introduce a training regimen utilizing an ensemble of policies\nfor each agent that leads to more robust multi-agent policies. We show the\nstrength of our approach compared to existing methods in cooperative as well as\ncompetitive scenarios, where agent populations are able to discover various\nphysical and informational coordination strategies.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 17:35:00 GMT"}, {"version": "v2", "created": "Wed, 21 Jun 2017 22:18:54 GMT"}, {"version": "v3", "created": "Tue, 16 Jan 2018 23:37:25 GMT"}, {"version": "v4", "created": "Sat, 14 Mar 2020 20:33:00 GMT"}], "update_date": "2020-03-17", "authors_parsed": [["Lowe", "Ryan", ""], ["Wu", "Yi", ""], ["Tamar", "Aviv", ""], ["Harb", "Jean", ""], ["Abbeel", "Pieter", ""], ["Mordatch", "Igor", ""]]}, {"id": "1706.02416", "submitter": "Sufeng Niu", "authors": "Sufeng Niu, Siheng Chen, Hanyu Guo, Colin Targonski, Melissa C. Smith,\n  Jelena Kova\\v{c}evi\\'c", "title": "Generalized Value Iteration Networks: Life Beyond Lattices", "comments": "14 pages, conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce a generalized value iteration network (GVIN),\nwhich is an end-to-end neural network planning module. GVIN emulates the value\niteration algorithm by using a novel graph convolution operator, which enables\nGVIN to learn and plan on irregular spatial graphs. We propose three novel\ndifferentiable kernels as graph convolution operators and show that the\nembedding based kernel achieves the best performance. We further propose\nepisodic Q-learning, an improvement upon traditional n-step Q-learning that\nstabilizes training for networks that contain a planning module. Lastly, we\nevaluate GVIN on planning problems in 2D mazes, irregular graphs, and\nreal-world street networks, showing that GVIN generalizes well for both\narbitrary graphs and unseen graphs of larger scale and outperforms a naive\ngeneralization of VIN (discretizing a spatial graph into a 2D image).\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 00:04:05 GMT"}, {"version": "v2", "created": "Thu, 26 Oct 2017 15:23:18 GMT"}], "update_date": "2017-10-27", "authors_parsed": [["Niu", "Sufeng", ""], ["Chen", "Siheng", ""], ["Guo", "Hanyu", ""], ["Targonski", "Colin", ""], ["Smith", "Melissa C.", ""], ["Kova\u010devi\u0107", "Jelena", ""]]}, {"id": "1706.02423", "submitter": "Jungsik Hwang", "authors": "Jungsik Hwang and Jun Tani", "title": "Seamless Integration and Coordination of Cognitive Skills in Humanoid\n  Robots: A Deep Learning Approach", "comments": "Accepted in the IEEE Transactions on Cognitive and Developmental\n  Systems (TCDS), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study investigates how adequate coordination among the different\ncognitive processes of a humanoid robot can be developed through end-to-end\nlearning of direct perception of visuomotor stream. We propose a deep dynamic\nneural network model built on a dynamic vision network, a motor generation\nnetwork, and a higher-level network. The proposed model was designed to process\nand to integrate direct perception of dynamic visuomotor patterns in a\nhierarchical model characterized by different spatial and temporal constraints\nimposed on each level. We conducted synthetic robotic experiments in which a\nrobot learned to read human's intention through observing the gestures and then\nto generate the corresponding goal-directed actions. Results verify that the\nproposed model is able to learn the tutored skills and to generalize them to\nnovel situations. The model showed synergic coordination of perception, action\nand decision making, and it integrated and coordinated a set of cognitive\nskills including visual perception, intention reading, attention switching,\nworking memory, action preparation and execution in a seamless manner. Analysis\nreveals that coherent internal representations emerged at each level of the\nhierarchy. Higher-level representation reflecting actional intention developed\nby means of continuous integration of the lower-level visuo-proprioceptive\nstream.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 01:15:00 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Hwang", "Jungsik", ""], ["Tani", "Jun", ""]]}, {"id": "1706.02444", "submitter": "Jungsik Hwang", "authors": "Jungsik Hwang, Jinhyung Kim, Ahmadreza Ahmadi, Minkyu Choi, Jun Tani", "title": "Predictive Coding-based Deep Dynamic Neural Network for Visuomotor\n  Learning", "comments": "Accepted at the 7th Joint IEEE International Conference of\n  Developmental Learning and Epigenetic Robotics (ICDL-EpiRob 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.RO q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This study presents a dynamic neural network model based on the predictive\ncoding framework for perceiving and predicting the dynamic visuo-proprioceptive\npatterns. In our previous study [1], we have shown that the deep dynamic neural\nnetwork model was able to coordinate visual perception and action generation in\na seamless manner. In the current study, we extended the previous model under\nthe predictive coding framework to endow the model with a capability of\nperceiving and predicting dynamic visuo-proprioceptive patterns as well as a\ncapability of inferring intention behind the perceived visuomotor information\nthrough minimizing prediction error. A set of synthetic experiments were\nconducted in which a robot learned to imitate the gestures of another robot in\na simulation environment. The experimental results showed that with given\nintention states, the model was able to mentally simulate the possible incoming\ndynamic visuo-proprioceptive patterns in a top-down process without the inputs\nfrom the external environment. Moreover, the results highlighted the role of\nminimizing prediction error in inferring underlying intention of the perceived\nvisuo-proprioceptive patterns, supporting the predictive coding account of the\nmirror neuron systems. The results also revealed that minimizing prediction\nerror in one modality induced the recall of the corresponding representation of\nanother modality acquired during the consolidative learning of raw-level\nvisuo-proprioceptive patterns.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 03:29:39 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Hwang", "Jungsik", ""], ["Kim", "Jinhyung", ""], ["Ahmadi", "Ahmadreza", ""], ["Choi", "Minkyu", ""], ["Tani", "Jun", ""]]}, {"id": "1706.02462", "submitter": "Marek Szyku{\\l}a", "authors": "Jakub Kowalski, Maksymilian Mika, Jakub Sutowicz, Marek Szyku{\\l}a", "title": "Regular Boardgames", "comments": "AAAI 2019", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new General Game Playing (GGP) language called Regular\nBoardgames (RBG), which is based on the theory of regular languages. The\nobjective of RBG is to join key properties as expressiveness, efficiency, and\nnaturalness of the description in one GGP formalism, compensating certain\ndrawbacks of the existing languages. This often makes RBG more suitable for\nvarious research and practical developments in GGP. While dedicated mostly for\ndescribing board games, RBG is universal for the class of all finite\ndeterministic turn-based games with perfect information. We establish\nfoundations of RBG, and analyze it theoretically and experimentally, focusing\non the efficiency of reasoning. Regular Boardgames is the first GGP language\nthat allows efficient encoding and playing games with complex rules and with\nlarge branching factor (e.g.\\ amazons, arimaa, large chess variants, go,\ninternational checkers, paper soccer).\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 07:22:21 GMT"}, {"version": "v2", "created": "Tue, 13 Nov 2018 14:50:36 GMT"}], "update_date": "2018-11-14", "authors_parsed": [["Kowalski", "Jakub", ""], ["Mika", "Maksymilian", ""], ["Sutowicz", "Jakub", ""], ["Szyku\u0142a", "Marek", ""]]}, {"id": "1706.02490", "submitter": "Matej Hoffmann", "authors": "Karla Stepanova and Matej Hoffmann and Zdenek Straka and Frederico B.\n  Klein and Angelo Cangelosi and Michal Vavrecka", "title": "Where is my forearm? Clustering of body parts from simultaneous tactile\n  and linguistic input using sequential mapping", "comments": "pp. 155-162", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CL cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans and animals are constantly exposed to a continuous stream of sensory\ninformation from different modalities. At the same time, they form more\ncompressed representations like concepts or symbols. In species that use\nlanguage, this process is further structured by this interaction, where a\nmapping between the sensorimotor concepts and linguistic elements needs to be\nestablished. There is evidence that children might be learning language by\nsimply disambiguating potential meanings based on multiple exposures to\nutterances in different contexts (cross-situational learning). In existing\nmodels, the mapping between modalities is usually found in a single step by\ndirectly using frequencies of referent and meaning co-occurrences. In this\npaper, we present an extension of this one-step mapping and introduce a newly\nproposed sequential mapping algorithm together with a publicly available Matlab\nimplementation. For demonstration, we have chosen a less typical scenario:\ninstead of learning to associate objects with their names, we focus on body\nrepresentations. A humanoid robot is receiving tactile stimulations on its\nbody, while at the same time listening to utterances of the body part names\n(e.g., hand, forearm and torso). With the goal at arriving at the correct \"body\ncategories\", we demonstrate how a sequential mapping algorithm outperforms\none-step mapping. In addition, the effect of data set size and noise in the\nlinguistic input are studied.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 09:31:42 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Stepanova", "Karla", ""], ["Hoffmann", "Matej", ""], ["Straka", "Zdenek", ""], ["Klein", "Frederico B.", ""], ["Cangelosi", "Angelo", ""], ["Vavrecka", "Michal", ""]]}, {"id": "1706.02513", "submitter": "Virginia Dignum", "authors": "Virginia Dignum", "title": "Responsible Autonomy", "comments": "IJCAI2017 (International Joint Conference on Artificial Intelligence)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As intelligent systems are increasingly making decisions that directly affect\nsociety, perhaps the most important upcoming research direction in AI is to\nrethink the ethical implications of their actions. Means are needed to\nintegrate moral, societal and legal values with technological developments in\nAI, both during the design process as well as part of the deliberation\nalgorithms employed by these systems. In this paper, we describe leading ethics\ntheories and propose alternative ways to ensure ethical behavior by artificial\nsystems. Given that ethics are dependent on the socio-cultural context and are\noften only implicit in deliberation processes, methodologies are needed to\nelicit the values held by designers and stakeholders, and to make these\nexplicit leading to better understanding and trust on artificial autonomous\nsystems.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 11:06:52 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Dignum", "Virginia", ""]]}, {"id": "1706.02591", "submitter": "Serkan Ayvaz", "authors": "Serkan Ayvaz and Mehmet Aydar", "title": "Dynamic Discovery of Type Classes and Relations in Semantic Web Data", "comments": null, "journal-ref": "J Data Semant (2019) 8: 57", "doi": "10.1007/s13740-019-00102-6", "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The continuing development of Semantic Web technologies and the increasing\nuser adoption in the recent years have accelerated the progress incorporating\nexplicit semantics with data on the Web. With the rapidly growing RDF (Resource\nDescription Framework) data on the Semantic Web, processing large semantic\ngraph data have become more challenging. Constructing a summary graph structure\nfrom the raw RDF can help obtain semantic type relations and reduce the\ncomputational complexity for graph processing purposes. In this paper, we\naddressed the problem of graph summarization in RDF graphs, and we proposed an\napproach for building summary graph structures automatically from RDF graph\ndata. Moreover, we introduced a measure to help discover optimum class\ndissimilarity thresholds and an effective method to discover the type classes\nautomatically. In future work, we plan to investigate further improvement\noptions on the scalability of the proposed method.\n", "versions": [{"version": "v1", "created": "Wed, 31 May 2017 11:58:31 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Ayvaz", "Serkan", ""], ["Aydar", "Mehmet", ""]]}, {"id": "1706.02596", "submitter": "Dirk Weissenborn", "authors": "Dirk Weissenborn, Tom\\'a\\v{s} Ko\\v{c}isk\\'y, Chris Dyer", "title": "Dynamic Integration of Background Knowledge in Neural NLU Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Common-sense and background knowledge is required to understand natural\nlanguage, but in most neural natural language understanding (NLU) systems, this\nknowledge must be acquired from training corpora during learning, and then it\nis static at test time. We introduce a new architecture for the dynamic\nintegration of explicit background knowledge in NLU models. A general-purpose\nreading module reads background knowledge in the form of free-text statements\n(together with task-specific text inputs) and yields refined word\nrepresentations to a task-specific NLU architecture that reprocesses the task\ninputs with these representations. Experiments on document question answering\n(DQA) and recognizing textual entailment (RTE) demonstrate the effectiveness\nand flexibility of the approach. Analysis shows that our model learns to\nexploit knowledge in a semantically appropriate way.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 14:10:22 GMT"}, {"version": "v2", "created": "Wed, 25 Oct 2017 14:54:53 GMT"}, {"version": "v3", "created": "Tue, 21 Aug 2018 08:57:43 GMT"}], "update_date": "2018-08-22", "authors_parsed": [["Weissenborn", "Dirk", ""], ["Ko\u010disk\u00fd", "Tom\u00e1\u0161", ""], ["Dyer", "Chris", ""]]}, {"id": "1706.02621", "submitter": "Sandeep Kumar", "authors": "Rajani Kumari, Vivek Kumar Sharma, Sandeep Kumar", "title": "Design and Implementation of Modified Fuzzy based CPU Scheduling\n  Algorithm", "comments": "6 Pages", "journal-ref": "International Journal of Computer Applications, Volume 77, No 17,\n  September 2013", "doi": "10.5120/13612-1323 10.5120/13612-1323 10.5120/13612-1323\n  10.5120/13612-1323", "report-no": null, "categories": "cs.OS cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  CPU Scheduling is the base of multiprogramming. Scheduling is a process which\ndecides order of task from a set of multiple tasks that are ready to execute.\nThere are number of CPU scheduling algorithms available, but it is very\ndifficult task to decide which one is better. This paper discusses the design\nand implementation of modified fuzzy based CPU scheduling algorithm. This paper\npresent a new set of fuzzy rules. It demonstrates that scheduling done with new\npriority improves average waiting time and average turnaround time.\n", "versions": [{"version": "v1", "created": "Fri, 26 May 2017 16:39:55 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Kumari", "Rajani", ""], ["Sharma", "Vivek Kumar", ""], ["Kumar", "Sandeep", ""]]}, {"id": "1706.02686", "submitter": "Mieczys{\\l}aw K{\\l}opotek", "authors": "Andrzej Matuszewski, Mieczys{\\l}aw A. K{\\l}opotek", "title": "What Does a Belief Function Believe In ?", "comments": "13 pages", "journal-ref": null, "doi": null, "report-no": "IPI-PAN report 758, 1994", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The conditioning in the Dempster-Shafer Theory of Evidence has been defined\n(by Shafer \\cite{Shafer:90} as combination of a belief function and of an\n\"event\" via Dempster rule.\n  On the other hand Shafer \\cite{Shafer:90} gives a \"probabilistic\"\ninterpretation of a belief function (hence indirectly its derivation from a\nsample). Given the fact that conditional probability distribution of a\nsample-derived probability distribution is a probability distribution derived\nfrom a subsample (selected on the grounds of a conditioning event), the paper\ninvestigates the empirical nature of the Dempster- rule of combination.\n  It is demonstrated that the so-called \"conditional\" belief function is not a\nbelief function given an event but rather a belief function given manipulation\nof original empirical data.\\\\ Given this, an interpretation of belief function\ndifferent from that of Shafer is proposed. Algorithms for construction of\nbelief networks from data are derived for this interpretation.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 17:17:23 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Matuszewski", "Andrzej", ""], ["K\u0142opotek", "Mieczys\u0142aw A.", ""]]}, {"id": "1706.02780", "submitter": "Victor Silva", "authors": "Marcelo Souza Nery, Roque Anderson Teixeira, Victor do Nascimento\n  Silva, Adriano Alonso Veloso", "title": "Setting Players' Behaviors in World of Warcraft through Semi-Supervised\n  Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Digital games are one of the major and most important fields on the\nentertainment domain, which also involves cinema and music. Numerous attempts\nhave been done to improve the quality of the games including more realistic\nartistic production and computer science. Assessing the player's behavior, a\ntask known as player modeling, is currently the need of the hour which leads to\npossible improvements in terms of: (i) better game interaction experience, (ii)\nbetter exploitation of the relationship between players, and (iii)\nincreasing/maintaining the number of players interested in the game. In this\npaper we model players using the basic four behaviors proposed in\n\\cite{BartleArtigo}, namely: achiever, explorer, socializer and killer. Our\nanalysis is carried out using data obtained from the game \"World of Warcraft\"\nover 3 years (2006 $-$ 2009). We employ a semi-supervised learning technique in\norder to find out characteristics that possibly impact player's behavior.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 21:48:46 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Nery", "Marcelo Souza", ""], ["Teixeira", "Roque Anderson", ""], ["Silva", "Victor do Nascimento", ""], ["Veloso", "Adriano Alonso", ""]]}, {"id": "1706.02789", "submitter": "Victor Silva", "authors": "Victor do Nascimento Silva and Luiz Chaimowicz", "title": "On the Development of Intelligent Agents for MOBA Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Multiplayer Online Battle Arena (MOBA) is one of the most played game genres\nnowadays. With the increasing growth of this genre, it becomes necessary to\ndevelop effective intelligent agents to play alongside or against human\nplayers. In this paper we address the problem of agent development for MOBA\ngames. We implement a two-layered architecture agent that handles both\nnavigation and game mechanics. This architecture relies on the use of Influence\nMaps, a widely used approach for tactical analysis. Several experiments were\nperformed using {\\em League of Legends} as a testbed, and show promising\nresults in this highly dynamic real-time context.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 23:20:34 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Silva", "Victor do Nascimento", ""], ["Chaimowicz", "Luiz", ""]]}, {"id": "1706.02792", "submitter": "Liron Cohen", "authors": "Liron Cohen, Tansel Uras, Shiva Jahangiri, Aliyah Arunasalam, Sven\n  Koenig, T.K. Satish Kumar", "title": "The FastMap Algorithm for Shortest Path Computations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a new preprocessing algorithm for embedding the nodes of a given\nedge-weighted undirected graph into a Euclidean space. The Euclidean distance\nbetween any two nodes in this space approximates the length of the shortest\npath between them in the given graph. Later, at runtime, a shortest path\nbetween any two nodes can be computed with A* search using the Euclidean\ndistances as heuristic. Our preprocessing algorithm, called FastMap, is\ninspired by the data mining algorithm of the same name and runs in near-linear\ntime. Hence, FastMap is orders of magnitude faster than competing approaches\nthat produce a Euclidean embedding using Semidefinite Programming. FastMap also\nproduces admissible and consistent heuristics and therefore guarantees the\ngeneration of shortest paths. Moreover, FastMap applies to general undirected\ngraphs for which many traditional heuristics, such as the Manhattan Distance\nheuristic, are not well defined. Empirically, we demonstrate that A* search\nusing the FastMap heuristic is competitive with A* search using other\nstate-of-the-art heuristics, such as the Differential heuristic.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 23:29:05 GMT"}, {"version": "v2", "created": "Sat, 21 Oct 2017 19:11:06 GMT"}, {"version": "v3", "created": "Thu, 21 Dec 2017 19:57:53 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Cohen", "Liron", ""], ["Uras", "Tansel", ""], ["Jahangiri", "Shiva", ""], ["Arunasalam", "Aliyah", ""], ["Koenig", "Sven", ""], ["Kumar", "T. K. Satish", ""]]}, {"id": "1706.02794", "submitter": "Liron Cohen", "authors": "Liron Cohen, Glenn Wagner, T.K. Satish Kumar, Howie Choset and Sven\n  Koenig", "title": "Rapid Randomized Restarts for Multi-Agent Path Finding Solvers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-Agent Path Finding (MAPF) is an NP-hard problem well studied in\nartificial intelligence and robotics. It has many real-world applications for\nwhich existing MAPF solvers use various heuristics. However, these solvers are\ndeterministic and perform poorly on \"hard\" instances typically characterized by\nmany agents interfering with each other in a small region. In this paper, we\nenhance MAPF solvers with randomization and observe that they exhibit\nheavy-tailed distributions of runtimes on hard instances. This leads us to\ndevelop simple rapid randomized restart (RRR) strategies with the intuition\nthat, given a hard instance, multiple short runs have a better chance of\nsolving it compared to one long run. We validate this intuition through\nexperiments and show that our RRR strategies indeed boost the performance of\nstate-of-the-art MAPF solvers such as iECBS and M*.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 23:31:01 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Cohen", "Liron", ""], ["Wagner", "Glenn", ""], ["Kumar", "T. K. Satish", ""], ["Choset", "Howie", ""], ["Koenig", "Sven", ""]]}, {"id": "1706.02796", "submitter": "Victor Silva", "authors": "Mirna Paula Silva, Victor do Nascimento Silva and Luiz Chaimowicz", "title": "Dynamic Difficulty Adjustment on MOBA Games", "comments": "103-123", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  This paper addresses the dynamic difficulty adjustment on MOBA games as a way\nto improve the player's entertainment. Although MOBA is currently one of the\nmost played genres around the world, it is known as a game that offer less\nautonomy, more challenges and consequently more frustration. Due to these\ncharacteristics, the use of a mechanism that performs the difficulty balance\ndynamically seems to be an interesting alternative to minimize and/or avoid\nthat players experience such frustrations. In this sense, this paper presents a\ndynamic difficulty adjustment mechanism for MOBA games. The main idea is to\ncreate a computer controlled opponent that adapts dynamically to the player\nperformance, trying to offer to the player a better game experience. This is\ndone by evaluating the performance of the player using a metric based on some\ngame features and switching the difficulty of the opponent's artificial\nintelligence behavior accordingly. Quantitative and qualitative experiments\nwere performed and the results showed that the system is capable of adapting\ndynamically to the opponent's skills. In spite of that, the qualitative\nexperiments with users showed that the player's expertise has a greater\ninfluence on the perception of the difficulty level and dynamic adaptation.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 23:45:07 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Silva", "Mirna Paula", ""], ["Silva", "Victor do Nascimento", ""], ["Chaimowicz", "Luiz", ""]]}, {"id": "1706.02832", "submitter": "Victor Silva", "authors": "Victor do Nascimento Silva and Luiz Chaimowicz", "title": "A Tutor Agent for MOBA Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Digital games have become a key player in the entertainment industry,\nattracting millions of new players each year. In spite of that, novice players\nmay have a hard time when playing certain types of games, such as MOBAs and\nMMORPGs, due to their steep learning curves and not so friendly online\ncommunities. In this paper, we present an approach to help novice players in\nMOBA games overcome these problems. An artificial intelligence agent plays\nalongside the player analyzing his/her performance and giving tips about the\ngame. Experiments performed with the game {\\em League of Legends} show the\npotential of this approach.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 04:33:58 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Silva", "Victor do Nascimento", ""], ["Chaimowicz", "Luiz", ""]]}, {"id": "1706.02897", "submitter": "Djallel Bouneffouf", "authors": "Djallel Bouneffouf, Irina Rish, Guillermo A. Cecchi", "title": "Bandit Models of Human Behavior: Reward Processing in Mental Disorders", "comments": "Conference on Artificial General Intelligence, AGI-17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Drawing an inspiration from behavioral studies of human decision making, we\npropose here a general parametric framework for multi-armed bandit problem,\nwhich extends the standard Thompson Sampling approach to incorporate reward\nprocessing biases associated with several neurological and psychiatric\nconditions, including Parkinson's and Alzheimer's diseases,\nattention-deficit/hyperactivity disorder (ADHD), addiction, and chronic pain.\nWe demonstrate empirically that the proposed parametric approach can often\noutperform the baseline Thompson Sampling on a variety of datasets. Moreover,\nfrom the behavioral modeling perspective, our parametric framework can be\nviewed as a first step towards a unifying computational model capturing reward\nprocessing abnormalities across multiple mental conditions.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 18:36:12 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Bouneffouf", "Djallel", ""], ["Rish", "Irina", ""], ["Cecchi", "Guillermo A.", ""]]}, {"id": "1706.02929", "submitter": "Mieczys{\\l}aw K{\\l}opotek", "authors": "Mieczys{\\l}aw A. K{\\l}opotek and Andrzej Matuszewski", "title": "Evidence Against Evidence Theory (?!)", "comments": "30 pages. arXiv admin note: substantial text overlap with\n  arXiv:1704.04000", "journal-ref": null, "doi": null, "report-no": "IPI PAN report 759, 1994", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper is concerned with the apparent greatest weakness of the\nMathematical Theory of Evidence (MTE) of Shafer \\cite{Shafer:76}, which has\nbeen strongly criticized by Wasserman \\cite{Wasserman:92ijar} - the\nrelationship to frequencies.\n  Weaknesses of various proposals of probabilistic interpretation of MTE belief\nfunctions are demonstrated.\n  A new frequency-based interpretation is presented overcoming various\ndrawbacks of earlier interpretations.\n", "versions": [{"version": "v1", "created": "Thu, 8 Jun 2017 17:23:34 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["K\u0142opotek", "Mieczys\u0142aw A.", ""], ["Matuszewski", "Andrzej", ""]]}, {"id": "1706.02952", "submitter": "Amit Dhurandhar", "authors": "Amit Dhurandhar, Vijay Iyengar, Ronny Luss and Karthikeyan Shanmugam", "title": "TIP: Typifying the Interpretability of Procedures", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.AP stat.CO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We provide a novel notion of what it means to be interpretable, looking past\nthe usual association with human understanding. Our key insight is that\ninterpretability is not an absolute concept and so we define it relative to a\ntarget model, which may or may not be a human. We define a framework that\nallows for comparing interpretable procedures by linking them to important\npractical aspects such as accuracy and robustness. We characterize many of the\ncurrent state-of-the-art interpretable methods in our framework portraying its\ngeneral applicability. Finally, principled interpretable strategies are\nproposed and empirically evaluated on synthetic data, as well as on the largest\npublic olfaction dataset that was made recently available \\cite{olfs}. We also\nexperiment on MNIST with a simple target model and different oracle models of\nvarying complexity. This leads to the insight that the improvement in the\ntarget model is not only a function of the oracle model's performance, but also\nits relative complexity with respect to the target model. Further experiments\non CIFAR-10, a real manufacturing dataset and FICO dataset showcase the benefit\nof our methods over Knowledge Distillation when the target models are simple\nand the complex model is a neural network.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 13:55:18 GMT"}, {"version": "v2", "created": "Tue, 26 Dec 2017 16:12:02 GMT"}, {"version": "v3", "created": "Mon, 29 Oct 2018 15:49:37 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Dhurandhar", "Amit", ""], ["Iyengar", "Vijay", ""], ["Luss", "Ronny", ""], ["Shanmugam", "Karthikeyan", ""]]}, {"id": "1706.02985", "submitter": "Ratthachat Chatpatanasiri", "authors": "Haizhen Wang, Ratthachat Chatpatanasiri, Pairote Sattayatham", "title": "Stock Trading Using PE ratio: A Dynamic Bayesian Network Modeling on\n  Behavioral Finance and Fundamental Investment", "comments": "34 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CE cs.AI cs.LG q-fin.GN", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  On a daily investment decision in a security market, the price earnings (PE)\nratio is one of the most widely applied methods being used as a firm valuation\ntool by investment experts. Unfortunately, recent academic developments in\nfinancial econometrics and machine learning rarely look at this tool. In\npractice, fundamental PE ratios are often estimated only by subjective expert\nopinions. The purpose of this research is to formalize a process of fundamental\nPE estimation by employing advanced dynamic Bayesian network (DBN) methodology.\nThe estimated PE ratio from our model can be used either as a information\nsupport for an expert to make investment decisions, or as an automatic trading\nsystem illustrated in experiments. Forward-backward inference and EM parameter\nestimation algorithms are derived with respect to the proposed DBN structure.\nUnlike existing works in literatures, the economic interpretation of our DBN\nmodel is well-justified by behavioral finance evidences of volatility. A simple\nbut practical trading strategy is invented based on the result of Bayesian\ninference. Extensive experiments show that our trading strategy equipped with\nthe inferenced PE ratios consistently outperforms standard investment\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Thu, 25 May 2017 13:24:22 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Wang", "Haizhen", ""], ["Chatpatanasiri", "Ratthachat", ""], ["Sattayatham", "Pairote", ""]]}, {"id": "1706.02999", "submitter": "Theja Tulabandhula", "authors": "Anuj Mahajan and Theja Tulabandhula", "title": "Symmetry Learning for Function Approximation in Reinforcement Learning", "comments": "12 pages, 3 figures. A preliminary version appears in AAMAS 2017.\n  Also presented at the 3rd Multidisciplinary Conference on Reinforcement\n  Learning and Decision Making", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we explore methods to exploit symmetries for ensuring sample\nefficiency in reinforcement learning (RL), this problem deserves ever\nincreasing attention with the recent advances in the use of deep networks for\ncomplex RL tasks which require large amount of training data. We introduce a\nnovel method to detect symmetries using reward trails observed during episodic\nexperience and prove its completeness. We also provide a framework to\nincorporate the discovered symmetries for functional approximation. Finally we\nshow that the use of potential based reward shaping is especially effective for\nour symmetry exploitation mechanism. Experiments on various classical problems\nshow that our method improves the learning performance significantly by\nutilizing symmetry information.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 15:30:32 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Mahajan", "Anuj", ""], ["Tulabandhula", "Theja", ""]]}, {"id": "1706.03021", "submitter": "Utku Kose", "authors": "Alice Pavaloiu, Utku Kose", "title": "Ethical Artificial Intelligence - An Open Question", "comments": "13 pages, 3 figures", "journal-ref": "Journal of Multidisciplinary Developments, 2(2), 2017, 15-27", "doi": null, "report-no": null, "categories": "cs.AI cs.CY", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Artificial Intelligence (AI) is an effective science which employs strong\nenough approaches, methods, and techniques to solve unsolvable real world based\nproblems. Because of its unstoppable rise towards the future, there are also\nsome discussions about its ethics and safety. Shaping an AI friendly\nenvironment for people and a people friendly environment for AI can be a\npossible answer for finding a shared context of values for both humans and\nrobots. In this context, objective of this paper is to address the ethical\nissues of AI and explore the moral dilemmas that arise from ethical algorithms,\nfrom pre set or acquired values. In addition, the paper will also focus on the\nsubject of AI safety. As general, the paper will briefly analyze the concerns\nand potential solutions to solving the ethical issues presented and increase\nreaders awareness on AI safety as another related research interest.\n", "versions": [{"version": "v1", "created": "Tue, 16 May 2017 20:57:36 GMT"}], "update_date": "2017-06-12", "authors_parsed": [["Pavaloiu", "Alice", ""], ["Kose", "Utku", ""]]}, {"id": "1706.03065", "submitter": "Mark Levin", "authors": "Mark Sh. Levin", "title": "Towards balanced clustering - part 1 (preliminaries)", "comments": "21 pages, 17 figures, 14 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The article contains a preliminary glance at balanced clustering problems.\nBasic balanced structures and combinatorial balanced problems are briefly\ndescribed. A special attention is targeted to various balance/unbalance indices\n(including some new versions of the indices): by cluster cardinality, by\ncluster weights, by inter-cluster edge/arc weights, by cluster element\nstructure (for element multi-type clustering). Further, versions of\noptimization clustering problems are suggested (including multicriteria problem\nformulations). Illustrative numerical examples describe calculation of balance\nindices and element multi-type balance clustering problems (including example\nfor design of student teams).\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 19:34:58 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Levin", "Mark Sh.", ""]]}, {"id": "1706.03100", "submitter": "Philip Thomas", "authors": "Philip S. Thomas and Christoph Dann and Emma Brunskill", "title": "Decoupling Learning Rules from Representations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the artificial intelligence field, learning often corresponds to changing\nthe parameters of a parameterized function. A learning rule is an algorithm or\nmathematical expression that specifies precisely how the parameters should be\nchanged. When creating an artificial intelligence system, we must make two\ndecisions: what representation should be used (i.e., what parameterized\nfunction should be used) and what learning rule should be used to search\nthrough the resulting set of representable functions. Using most learning\nrules, these two decisions are coupled in a subtle (and often unintentional)\nway. That is, using the same learning rule with two different representations\nthat can represent the same sets of functions can result in two different\noutcomes. After arguing that this coupling is undesirable, particularly when\nusing artificial neural networks, we present a method for partially decoupling\nthese two decisions for a broad class of learning rules that span unsupervised\nlearning, reinforcement learning, and supervised learning.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 19:34:03 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Thomas", "Philip S.", ""], ["Dann", "Christoph", ""], ["Brunskill", "Emma", ""]]}, {"id": "1706.03122", "submitter": "Michael Cook", "authors": "Michael Cook, Adam Summerville and Simon Colton", "title": "Off The Beaten Lane: AI Challenges In MOBAs Beyond Player Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  MOBAs represent a huge segment of online gaming and are growing as both an\neSport and a casual genre. The natural starting point for AI researchers\ninterested in MOBAs is to develop an AI to play the game better than a human -\nbut MOBAs have many more challenges besides adversarial AI. In this paper we\nintroduce the reader to the wider context of MOBA culture, propose a range of\nchallenges faced by the community today, and posit concrete AI projects that\ncan be undertaken to begin solving them.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 20:57:18 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Cook", "Michael", ""], ["Summerville", "Adam", ""], ["Colton", "Simon", ""]]}, {"id": "1706.03144", "submitter": "Pei Cao", "authors": "Pei Cao, Zhaoyan Fan, Robert X. Gao, Jiong Tang", "title": "A Focal Any-Angle Path-finding Algorithm Based on A* on Visibility\n  Graphs", "comments": null, "journal-ref": null, "doi": "10.1115/1.4040320", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this research, we investigate the subject of path-finding. A pruned\nversion of visibility graph based on Candidate Vertices is formulated, followed\nby a new visibility check technique. Such combination enables us to quickly\nidentify the useful vertices and thus find the optimal path more efficiently.\nThe algorithm proposed is demonstrated on various path-finding cases. The\nperformance of the new technique on visibility graphs is compared to the\ntraditional A* on Grids, Theta* and A* on Visibility Graphs in terms of path\nlength, number of nodes evaluated, as well as computational time. The key\nalgorithmic contribution is that the new approach combines the merits of\ngrid-based method and visibility graph-based method and thus yields better\noverall performance.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 22:19:12 GMT"}], "update_date": "2018-10-30", "authors_parsed": [["Cao", "Pei", ""], ["Fan", "Zhaoyan", ""], ["Gao", "Robert X.", ""], ["Tang", "Jiong", ""]]}, {"id": "1706.03146", "submitter": "Shuai Tang", "authors": "Shuai Tang, Hailin Jin, Chen Fang, Zhaowen Wang, Virginia R. de Sa", "title": "Rethinking Skip-thought: A Neighborhood based Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the skip-thought model with neighborhood information as weak\nsupervision. More specifically, we propose a skip-thought neighbor model to\nconsider the adjacent sentences as a neighborhood. We train our skip-thought\nneighbor model on a large corpus with continuous sentences, and then evaluate\nthe trained model on 7 tasks, which include semantic relatedness, paraphrase\ndetection, and classification benchmarks. Both quantitative comparison and\nqualitative investigation are conducted. We empirically show that, our\nskip-thought neighbor model performs as well as the skip-thought model on\nevaluation tasks. In addition, we found that, incorporating an autoencoder path\nin our model didn't aid our model to perform better, while it hurts the\nperformance of the skip-thought model.\n", "versions": [{"version": "v1", "created": "Fri, 9 Jun 2017 22:39:31 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Tang", "Shuai", ""], ["Jin", "Hailin", ""], ["Fang", "Chen", ""], ["Wang", "Zhaowen", ""], ["de Sa", "Virginia R.", ""]]}, {"id": "1706.03205", "submitter": "Xiang Wang", "authors": "Xiang Wang, Xiangnan He, Liqiang Nie, Tat-Seng Chua", "title": "Item Silk Road: Recommending Items from Information Domains to Social\n  Users", "comments": "10 pages, 7 figures, SIGIR 2017", "journal-ref": null, "doi": "10.1145/3077136.3080771", "report-no": null, "categories": "cs.IR cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Online platforms can be divided into information-oriented and social-oriented\ndomains. The former refers to forums or E-commerce sites that emphasize\nuser-item interactions, like Trip.com and Amazon; whereas the latter refers to\nsocial networking services (SNSs) that have rich user-user connections, such as\nFacebook and Twitter. Despite their heterogeneity, these two domains can be\nbridged by a few overlapping users, dubbed as bridge users. In this work, we\naddress the problem of cross-domain social recommendation, i.e., recommending\nrelevant items of information domains to potential users of social networks. To\nour knowledge, this is a new problem that has rarely been studied before.\n  Existing cross-domain recommender systems are unsuitable for this task since\nthey have either focused on homogeneous information domains or assumed that\nusers are fully overlapped. Towards this end, we present a novel Neural Social\nCollaborative Ranking (NSCR) approach, which seamlessly sews up the user-item\ninteractions in information domains and user-user connections in SNSs. In the\ninformation domain part, the attributes of users and items are leveraged to\nstrengthen the embedding learning of users and items. In the SNS part, the\nembeddings of bridge users are propagated to learn the embeddings of other\nnon-bridge users. Extensive experiments on two real-world datasets demonstrate\nthe effectiveness and rationality of our NSCR method.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jun 2017 08:58:02 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Wang", "Xiang", ""], ["He", "Xiangnan", ""], ["Nie", "Liqiang", ""], ["Chua", "Tat-Seng", ""]]}, {"id": "1706.03207", "submitter": "Rafael Pe\\~naloza", "authors": "Rafael Pe\\~naloza and Nico Potyka", "title": "Towards Statistical Reasoning in Description Logics over Finite Domains\n  (Full Version)", "comments": "16 pages. Extended version of \"Towards Statistical Reasoning in\n  Description Logics over Finite Domains\" published at the 11th International\n  Conference on Scalable Uncertainty Management (SUM 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a probabilistic extension of the description logic $\\mathcal{ALC}$\nfor reasoning about statistical knowledge. We consider conditional statements\nover proportions of the domain and are interested in the probabilistic-logical\nconsequences of these proportions. After introducing some general reasoning\nproblems and analyzing their properties, we present first algorithms and\ncomplexity results for reasoning in some fragments of Statistical\n$\\mathcal{ALC}$.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jun 2017 09:22:32 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Pe\u00f1aloza", "Rafael", ""], ["Potyka", "Nico", ""]]}, {"id": "1706.03235", "submitter": "Hangyu Mao", "authors": "Hangyu Mao, Zhibo Gong, Yan Ni and Zhen Xiao", "title": "ACCNet: Actor-Coordinator-Critic Net for \"Learning-to-Communicate\" with\n  Deep Multi-agent Reinforcement Learning", "comments": "V3 of original submission. Actor-Critic Method for Multi-agent\n  Learning-to-Communicate based on Deep Reinforcement Learning, It is suitable\n  for both continuous and discrete action space environments", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Communication is a critical factor for the big multi-agent world to stay\norganized and productive. Typically, most previous multi-agent\n\"learning-to-communicate\" studies try to predefine the communication protocols\nor use technologies such as tabular reinforcement learning and evolutionary\nalgorithm, which can not generalize to changing environment or large collection\nof agents.\n  In this paper, we propose an Actor-Coordinator-Critic Net (ACCNet) framework\nfor solving \"learning-to-communicate\" problem. The ACCNet naturally combines\nthe powerful actor-critic reinforcement learning technology with deep learning\ntechnology. It can efficiently learn the communication protocols even from\nscratch under partially observable environment. We demonstrate that the ACCNet\ncan achieve better results than several baselines under both continuous and\ndiscrete action space environments. We also analyse the learned protocols and\ndiscuss some design considerations.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jun 2017 13:50:23 GMT"}, {"version": "v2", "created": "Tue, 13 Jun 2017 02:00:14 GMT"}, {"version": "v3", "created": "Sun, 29 Oct 2017 05:09:39 GMT"}], "update_date": "2017-10-31", "authors_parsed": [["Mao", "Hangyu", ""], ["Gong", "Zhibo", ""], ["Ni", "Yan", ""], ["Xiao", "Zhen", ""]]}, {"id": "1706.03254", "submitter": "Yuu Jinnai", "authors": "Yuu Jinnai, Alex Fukunaga", "title": "On Hash-Based Work Distribution Methods for Parallel Best-First Search", "comments": "Source code of domain-specific solvers in multicore environment:\n  https://github.com/jinnaiyuu/Parallel-Best-First-Searches Source code of\n  classical planning in distributed environment:\n  https://github.com/jinnaiyuu/distributed-fast-downward", "journal-ref": "Yuu Jinnai and Alex Fukunaga. (2017). On Hash-Based Work\n  Distribution Methods for Parallel Best-First Search. Journal of Artificial\n  Intelligence Research (JAIR), 60, 491-548", "doi": null, "report-no": null, "categories": "cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Parallel best-first search algorithms such as Hash Distributed A* (HDA*)\ndistribute work among the processes using a global hash function. We analyze\nthe search and communication overheads of state-of-the-art hash-based parallel\nbest-first search algorithms, and show that although Zobrist hashing, the\nstandard hash function used by HDA*, achieves good load balance for many\ndomains, it incurs significant communication overhead since almost all\ngenerated nodes are transferred to a different processor than their parents. We\npropose Abstract Zobrist hashing, a new work distribution method for parallel\nsearch which, instead of computing a hash value based on the raw features of a\nstate, uses a feature projection function to generate a set of abstract\nfeatures which results in a higher locality, resulting in reduced\ncommunications overhead. We show that Abstract Zobrist hashing outperforms\nprevious methods on search domains using hand-coded, domain specific feature\nprojection functions. We then propose GRAZHDA*, a graph-partitioning based\napproach to automatically generating feature projection functions. GRAZHDA*\nseeks to approximate the partitioning of the actual search space graph by\npartitioning the domain transition graph, an abstraction of the state space\ngraph. We show that GRAZHDA* outperforms previous methods on domain-independent\nplanning.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jun 2017 17:05:46 GMT"}, {"version": "v2", "created": "Thu, 9 Nov 2017 23:27:48 GMT"}], "update_date": "2017-11-13", "authors_parsed": [["Jinnai", "Yuu", ""], ["Fukunaga", "Alex", ""]]}, {"id": "1706.03304", "submitter": "Neil Newman", "authors": "Neil Newman and Alexandre Fr\\'echette and Kevin Leyton-Brown", "title": "Deep Optimization for Spectrum Repacking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over 13 months in 2016-17 the FCC conducted an \"incentive auction\" to\nrepurpose radio spectrum from broadcast television to wireless internet. In the\nend, the auction yielded $19.8 billion, $10.05 billion of which was paid to 175\nbroadcasters for voluntarily relinquishing their licenses across 14 UHF\nchannels. Stations that continued broadcasting were assigned potentially new\nchannels to fit as densely as possible into the channels that remained. The\ngovernment netted more than $7 billion (used to pay down the national debt)\nafter covering costs. A crucial element of the auction design was the\nconstruction of a solver, dubbed SATFC, that determined whether sets of\nstations could be \"repacked\" in this way; it needed to run every time a station\nwas given a price quote. This paper describes the process by which we built\nSATFC. We adopted an approach we dub \"deep optimization\", taking a data-driven,\nhighly parametric, and computationally intensive approach to solver design.\nMore specifically, to build SATFC we designed software that could pair both\ncomplete and local-search SAT-encoded feasibility checking with a wide range of\ndomain-specific techniques. We then used automatic algorithm configuration\ntechniques to construct a portfolio of eight complementary algorithms to be run\nin parallel, aiming to achieve good performance on instances that arose in\nproprietary auction simulations. To evaluate the impact of our solver in this\npaper, we built an open-source reverse auction simulator. We found that within\nthe short time budget required in practice, SATFC solved more than 95% of the\nproblems it encountered. Furthermore, the incentive auction paired with SATFC\nproduced nearly optimal allocations in a restricted setting and substantially\noutperformed other alternatives at national scale.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jun 2017 03:15:20 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Newman", "Neil", ""], ["Fr\u00e9chette", "Alexandre", ""], ["Leyton-Brown", "Kevin", ""]]}, {"id": "1706.03416", "submitter": "Kaiyu Zheng", "authors": "Kaiyu Zheng", "title": "Learning Large-Scale Topological Maps Using Sum-Product Networks", "comments": "26 pages, 14 figures, senior thesis for departmental honors at the\n  Allen School of Computer Science and Engineering at the University of\n  Washington", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  In order to perform complex actions in human environments, an autonomous\nrobot needs the ability to understand the environment, that is, to gather and\nmaintain spatial knowledge. Topological map is commonly used for representing\nlarge scale, global maps such as floor plans. Although much work has been done\nin topological map extraction, we have found little previous work on the\nproblem of learning the topological map using a probabilistic model. Learning a\ntopological map means learning the structure of the large-scale space and\ndependency between places, for example, how the evidence of a group of places\ninfluence the attributes of other places. This is an important step towards\nplanning complex actions in the environment. In this thesis, we consider the\nproblem of using probabilistic deep learning model to learn the topological\nmap, which is essentially a sparse undirected graph where nodes represent\nplaces annotated with their semantic attributes (e.g. place category). We\npropose to use a novel probabilistic deep model, Sum-Product Networks (SPNs),\ndue to their unique properties. We present two methods for learning topological\nmaps using SPNs: the place grid method and the template-based method. We\ncontribute an algorithm that builds SPNs for graphs using template models. Our\nexperiments evaluate the ability of our models to enable robots to infer\nsemantic attributes and detect maps with novel semantic attribute arrangements.\nOur results demonstrate their understanding of the topological map structure\nand spatial relations between places.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jun 2017 21:52:56 GMT"}, {"version": "v2", "created": "Mon, 10 Jul 2017 06:16:23 GMT"}], "update_date": "2017-07-11", "authors_parsed": [["Zheng", "Kaiyu", ""]]}, {"id": "1706.03459", "submitter": "Zhe Feng", "authors": "Paul D\\\"utting and Zhe Feng and Harikrishna Narasimhan and David C.\n  Parkes and Sai Srivatsa Ravindranath", "title": "Optimal Auctions through Deep Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing an incentive compatible auction that maximizes expected revenue is\nan intricate task. The single-item case was resolved in a seminal piece of work\nby Myerson in 1981. Even after 30-40 years of intense research the problem\nremains unsolved for settings with two or more items. In this work, we initiate\nthe exploration of the use of tools from deep learning for the automated design\nof optimal auctions. We model an auction as a multi-layer neural network, frame\noptimal auction design as a constrained learning problem, and show how it can\nbe solved using standard machine learning pipelines. We prove generalization\nbounds and present extensive experiments, recovering essentially all known\nanalytical solutions for multi-item settings, and obtaining novel mechanisms\nfor settings in which the optimal mechanism is unknown.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 04:03:15 GMT"}, {"version": "v2", "created": "Mon, 19 Mar 2018 17:56:32 GMT"}, {"version": "v3", "created": "Mon, 1 Apr 2019 00:33:16 GMT"}, {"version": "v4", "created": "Sat, 29 Feb 2020 23:54:43 GMT"}, {"version": "v5", "created": "Fri, 21 Aug 2020 15:04:03 GMT"}], "update_date": "2020-08-24", "authors_parsed": [["D\u00fctting", "Paul", ""], ["Feng", "Zhe", ""], ["Narasimhan", "Harikrishna", ""], ["Parkes", "David C.", ""], ["Ravindranath", "Sai Srivatsa", ""]]}, {"id": "1706.03469", "submitter": "Josiah Hanna", "authors": "Josiah P. Hanna, Philip S. Thomas, Peter Stone, Scott Niekum", "title": "Data-Efficient Policy Evaluation Through Behavior Policy Search", "comments": "Accepted to ICML 2017; Extended version; 15 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the task of evaluating a policy for a Markov decision process\n(MDP). The standard unbiased technique for evaluating a policy is to deploy the\npolicy and observe its performance. We show that the data collected from\ndeploying a different policy, commonly called the behavior policy, can be used\nto produce unbiased estimates with lower mean squared error than this standard\ntechnique. We derive an analytic expression for the optimal behavior policy ---\nthe behavior policy that minimizes the mean squared error of the resulting\nestimates. Because this expression depends on terms that are unknown in\npractice, we propose a novel policy evaluation sub-problem, behavior policy\nsearch: searching for a behavior policy that reduces mean squared error. We\npresent a behavior policy search algorithm and empirically demonstrate its\neffectiveness in lowering the mean squared error of policy performance\nestimates.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 05:19:47 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Hanna", "Josiah P.", ""], ["Thomas", "Philip S.", ""], ["Stone", "Peter", ""], ["Niekum", "Scott", ""]]}, {"id": "1706.03471", "submitter": "Jian Zhang", "authors": "Jian Zhang, Ioannis Mitliagkas", "title": "YellowFin and the Art of Momentum Tuning", "comments": "Updated to reflect improved stability discussion and work for SysML\n  presentation", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hyperparameter tuning is one of the most time-consuming workloads in deep\nlearning. State-of-the-art optimizers, such as AdaGrad, RMSProp and Adam,\nreduce this labor by adaptively tuning an individual learning rate for each\nvariable. Recently researchers have shown renewed interest in simpler methods\nlike momentum SGD as they may yield better test metrics. Motivated by this\ntrend, we ask: can simple adaptive methods based on SGD perform as well or\nbetter? We revisit the momentum SGD algorithm and show that hand-tuning a\nsingle learning rate and momentum makes it competitive with Adam. We then\nanalyze its robustness to learning rate misspecification and objective\ncurvature variation. Based on these insights, we design YellowFin, an automatic\ntuner for momentum and learning rate in SGD. YellowFin optionally uses a\nnegative-feedback loop to compensate for the momentum dynamics in asynchronous\nsettings on the fly. We empirically show that YellowFin can converge in fewer\niterations than Adam on ResNets and LSTMs for image recognition, language\nmodeling and constituency parsing, with a speedup of up to 3.28x in synchronous\nand up to 2.69x in asynchronous settings.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 05:43:56 GMT"}, {"version": "v2", "created": "Wed, 14 Feb 2018 20:46:23 GMT"}], "update_date": "2018-02-16", "authors_parsed": [["Zhang", "Jian", ""], ["Mitliagkas", "Ioannis", ""]]}, {"id": "1706.03576", "submitter": "Martin  Biehl", "authors": "Martin Biehl, Daniel Polani", "title": "Action and perception for spatiotemporal patterns", "comments": "8 pages, 2 figures, accepted at the European Conference on Artificial\n  Life 2017, Lyon, France", "journal-ref": "Proceedings of The Fourteenth European Conference on Artificial\n  Life (September 2017) p.68-75", "doi": "10.7551/ecal_a_015", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This is a contribution to the formalization of the concept of agents in\nmultivariate Markov chains. Agents are commonly defined as entities that act,\nperceive, and are goal-directed. In a multivariate Markov chain (e.g. a\ncellular automaton) the transition matrix completely determines the dynamics.\nThis seems to contradict the possibility of acting entities within such a\nsystem. Here we present definitions of actions and perceptions within\nmultivariate Markov chains based on entity-sets. Entity-sets represent a\nlargely independent choice of a set of spatiotemporal patterns that are\nconsidered as all the entities within the Markov chain. For example, the\nentity-set can be chosen according to operational closure conditions or\ncomplete specific integration. Importantly, the perception-action loop also\ninduces an entity-set and is a multivariate Markov chain. We then show that our\ndefinition of actions leads to non-heteronomy and that of perceptions\nspecialize to the usual concept of perception in the perception-action loop.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 11:44:24 GMT"}], "update_date": "2018-08-13", "authors_parsed": [["Biehl", "Martin", ""], ["Polani", "Daniel", ""]]}, {"id": "1706.03610", "submitter": "Georg Wiese", "authors": "Georg Wiese, Dirk Weissenborn, Mariana Neves", "title": "Neural Domain Adaptation for Biomedical Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Factoid question answering (QA) has recently benefited from the development\nof deep learning (DL) systems. Neural network models outperform traditional\napproaches in domains where large datasets exist, such as SQuAD (ca. 100,000\nquestions) for Wikipedia articles. However, these systems have not yet been\napplied to QA in more specific domains, such as biomedicine, because datasets\nare generally too small to train a DL system from scratch. For example, the\nBioASQ dataset for biomedical QA comprises less then 900 factoid (single\nanswer) and list (multiple answers) QA instances. In this work, we adapt a\nneural QA system trained on a large open-domain dataset (SQuAD, source) to a\nbiomedical dataset (BioASQ, target) by employing various transfer learning\ntechniques. Our network architecture is based on a state-of-the-art QA system,\nextended with biomedical word embeddings and a novel mechanism to answer list\nquestions. In contrast to existing biomedical QA systems, our system does not\nrely on domain-specific ontologies, parsers or entity taggers, which are\nexpensive to create. Despite this fact, our systems achieve state-of-the-art\nresults on factoid questions and competitive results on list questions.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 13:08:21 GMT"}, {"version": "v2", "created": "Thu, 15 Jun 2017 15:16:18 GMT"}], "update_date": "2017-06-16", "authors_parsed": [["Wiese", "Georg", ""], ["Weissenborn", "Dirk", ""], ["Neves", "Mariana", ""]]}, {"id": "1706.03661", "submitter": "Cl\u00c3\u00a9ment Moulin-Frier", "authors": "Cl\\'ement Moulin-Frier, Tobias Fischer, Maxime Petit, Gr\\'egoire\n  Pointeau, Jordi-Ysard Puigbo, Ugo Pattacini, Sock Ching Low, Daniel\n  Camilleri, Phuong Nguyen, Matej Hoffmann, Hyung Jin Chang, Martina Zambelli,\n  Anne-Laure Mealier, Andreas Damianou, Giorgio Metta, Tony J. Prescott,\n  Yiannis Demiris, Peter Ford Dominey, Paul F. M. J. Verschure", "title": "DAC-h3: A Proactive Robot Cognitive Architecture to Acquire and Express\n  Knowledge About the World and the Self", "comments": "Preprint version; final version available at\n  http://ieeexplore.ieee.org/ IEEE Transactions on Cognitive and Developmental\n  Systems (Accepted) DOI: 10.1109/TCDS.2017.2754143", "journal-ref": "IEEE Transactions on Cognitive and Developmental Systems 10 (4),\n  1005-1022, 2018", "doi": "10.1109/TCDS.2017.2754143", "report-no": null, "categories": "cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a cognitive architecture for a humanoid robot to engage\nin a proactive, mixed-initiative exploration and manipulation of its\nenvironment, where the initiative can originate from both the human and the\nrobot. The framework, based on a biologically-grounded theory of the brain and\nmind, integrates a reactive interaction engine, a number of state-of-the-art\nperceptual and motor learning algorithms, as well as planning abilities and an\nautobiographical memory. The architecture as a whole drives the robot behavior\nto solve the symbol grounding problem, acquire language capabilities, execute\ngoal-oriented behavior, and express a verbal narrative of its own experience in\nthe world. We validate our approach in human-robot interaction experiments with\nthe iCub humanoid robot, showing that the proposed cognitive architecture can\nbe applied in real time within a realistic scenario and that it can be used\nwith naive users.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 14:39:43 GMT"}, {"version": "v2", "created": "Mon, 18 Sep 2017 22:22:15 GMT"}], "update_date": "2021-01-25", "authors_parsed": [["Moulin-Frier", "Cl\u00e9ment", ""], ["Fischer", "Tobias", ""], ["Petit", "Maxime", ""], ["Pointeau", "Gr\u00e9goire", ""], ["Puigbo", "Jordi-Ysard", ""], ["Pattacini", "Ugo", ""], ["Low", "Sock Ching", ""], ["Camilleri", "Daniel", ""], ["Nguyen", "Phuong", ""], ["Hoffmann", "Matej", ""], ["Chang", "Hyung Jin", ""], ["Zambelli", "Martina", ""], ["Mealier", "Anne-Laure", ""], ["Damianou", "Andreas", ""], ["Metta", "Giorgio", ""], ["Prescott", "Tony J.", ""], ["Demiris", "Yiannis", ""], ["Dominey", "Peter Ford", ""], ["Verschure", "Paul F. M. J.", ""]]}, {"id": "1706.03741", "submitter": "Paul Christiano", "authors": "Paul Christiano, Jan Leike, Tom B. Brown, Miljan Martic, Shane Legg,\n  Dario Amodei", "title": "Deep reinforcement learning from human preferences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For sophisticated reinforcement learning (RL) systems to interact usefully\nwith real-world environments, we need to communicate complex goals to these\nsystems. In this work, we explore goals defined in terms of (non-expert) human\npreferences between pairs of trajectory segments. We show that this approach\ncan effectively solve complex RL tasks without access to the reward function,\nincluding Atari games and simulated robot locomotion, while providing feedback\non less than one percent of our agent's interactions with the environment. This\nreduces the cost of human oversight far enough that it can be practically\napplied to state-of-the-art RL systems. To demonstrate the flexibility of our\napproach, we show that we can successfully train complex novel behaviors with\nabout an hour of human time. These behaviors and environments are considerably\nmore complex than any that have been previously learned from human feedback.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 17:23:59 GMT"}, {"version": "v2", "created": "Sun, 2 Jul 2017 20:25:56 GMT"}, {"version": "v3", "created": "Thu, 13 Jul 2017 20:18:41 GMT"}], "update_date": "2017-10-25", "authors_parsed": [["Christiano", "Paul", ""], ["Leike", "Jan", ""], ["Brown", "Tom B.", ""], ["Martic", "Miljan", ""], ["Legg", "Shane", ""], ["Amodei", "Dario", ""]]}, {"id": "1706.03757", "submitter": "Christophe Van Gysel", "authors": "Christophe Van Gysel, Maarten de Rijke, Evangelos Kanoulas", "title": "Semantic Entity Retrieval Toolkit", "comments": "SIGIR 2017 Workshop on Neural Information Retrieval (Neu-IR'17). 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Unsupervised learning of low-dimensional, semantic representations of words\nand entities has recently gained attention. In this paper we describe the\nSemantic Entity Retrieval Toolkit (SERT) that provides implementations of our\npreviously published entity representation models. The toolkit provides a\nunified interface to different representation learning algorithms, fine-grained\nparsing configuration and can be used transparently with GPUs. In addition,\nusers can easily modify existing models or implement their own models in the\nframework. After model training, SERT can be used to rank entities according to\na textual query and extract the learned entity/word representation for use in\ndownstream algorithms, such as clustering or recommendation.\n", "versions": [{"version": "v1", "created": "Mon, 12 Jun 2017 17:51:05 GMT"}, {"version": "v2", "created": "Mon, 17 Jul 2017 14:30:49 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Van Gysel", "Christophe", ""], ["de Rijke", "Maarten", ""], ["Kanoulas", "Evangelos", ""]]}, {"id": "1706.03768", "submitter": "Kun Zhang", "authors": "Kun Zhang, Mingming Gong, Joseph Ramsey, Kayhan Batmanghelich, Peter\n  Spirtes, Clark Glymour", "title": "Causal Discovery in the Presence of Measurement Error: Identifiability\n  Conditions", "comments": "15 pages, 5 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  Measurement error in the observed values of the variables can greatly change\nthe output of various causal discovery methods. This problem has received much\nattention in multiple fields, but it is not clear to what extent the causal\nmodel for the measurement-error-free variables can be identified in the\npresence of measurement error with unknown variance. In this paper, we study\nprecise sufficient identifiability conditions for the measurement-error-free\ncausal model and show what information of the causal model can be recovered\nfrom observed data. In particular, we present two different sets of\nidentifiability conditions, based on the second-order statistics and\nhigher-order statistics of the data, respectively. The former was inspired by\nthe relationship between the generating model of the\nmeasurement-error-contaminated data and the factor analysis model, and the\nlatter makes use of the identifiability result of the over-complete independent\ncomponent analysis problem.\n", "versions": [{"version": "v1", "created": "Sat, 10 Jun 2017 17:22:26 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Zhang", "Kun", ""], ["Gong", "Mingming", ""], ["Ramsey", "Joseph", ""], ["Batmanghelich", "Kayhan", ""], ["Spirtes", "Peter", ""], ["Glymour", "Clark", ""]]}, {"id": "1706.03906", "submitter": "Cunjing Ge", "authors": "Cunjing Ge, Feifei Ma, Tian Liu, Jian Zhang", "title": "A New Probabilistic Algorithm for Approximate Model Counting", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constrained counting is important in domains ranging from artificial\nintelligence to software analysis. There are already a few approaches for\ncounting models over various types of constraints. Recently, hashing-based\napproaches achieve both theoretical guarantees and scalability, but still rely\non solution enumeration. In this paper, a new probabilistic polynomial time\napproximate model counter is proposed, which is also a hashing-based universal\nframework, but with only satisfiability queries. A variant with a dynamic\nstopping criterion is also presented. Empirical evaluation over benchmarks on\npropositional logic formulas and SMT(BV) formulas shows that the approach is\npromising.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 05:26:02 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Ge", "Cunjing", ""], ["Ma", "Feifei", ""], ["Liu", "Tian", ""], ["Zhang", "Jian", ""]]}, {"id": "1706.03930", "submitter": "Chi Hong", "authors": "Chi Hong", "title": "Generative Models for Learning from Crowds", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose generative probabilistic models for label\naggregation. We use Gibbs sampling and a novel variational inference algorithm\nto perform the posterior inference. Empirical results show that our methods\nconsistently outperform state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 07:28:26 GMT"}, {"version": "v2", "created": "Wed, 5 Jul 2017 07:28:14 GMT"}, {"version": "v3", "created": "Tue, 3 Oct 2017 09:18:04 GMT"}], "update_date": "2017-10-04", "authors_parsed": [["Hong", "Chi", ""]]}, {"id": "1706.03940", "submitter": "Julia Sidorova", "authors": "S. Podapati, L. Lundberg, L. Skold, O. Rosander, J. Sidorova", "title": "Fuzzy Recommendations in Marketing Campaigns", "comments": "conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The population in Sweden is growing rapidly due to immigration. In this\nlight, the issue of infrastructure upgrades to provide telecommunication\nservices is of importance. New antennas can be installed at hot spots of user\ndemand, which will require an investment, and/or the clientele expansion can be\ncarried out in a planned manner to promote the exploitation of the\ninfrastructure in the less loaded geographical zones. In this paper, we explore\nthe second alternative. Informally speaking, the term Infrastructure-Stressing\ndescribes a user who stays in the zones of high demand, which are prone to\nproduce service failures, if further loaded. We have studied the\nInfrastructure-Stressing population in the light of their correlation with\ngeo-demographic segments. This is motivated by the fact that specific\ngeo-demographic segments can be targeted via marketing campaigns. Fuzzy logic\nis applied to create an interface between big data, numeric methods for\nprocessing big data and a manager.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 07:56:18 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Podapati", "S.", ""], ["Lundberg", "L.", ""], ["Skold", "L.", ""], ["Rosander", "O.", ""], ["Sidorova", "J.", ""]]}, {"id": "1706.03944", "submitter": "Julia Sidorova", "authors": "J. Sidorova, L. Skold, O. Rosander, L. Lundberg", "title": "Recommendations for Marketing Campaigns in Telecommunication Business\n  based on the footprint analysis", "comments": "conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A major investment made by a telecom operator goes into the infrastructure\nand its maintenance, while business revenues are proportional to how big and\ngood the customer base is. We present a data-driven analytic strategy based on\ncombinatorial optimization and analysis of historical data. The data cover\nhistorical mobility of the users in one region of Sweden during a week.\nApplying the proposed method to the case study, we have identified the optimal\nproportion of geo-demographic segments in the customer base, developed a\nfunctionality to assess the potential of a planned marketing campaign, and\nexplored the problem of an optimal number and types of the geo-demographic\nsegments to target through marketing campaigns. With the help of fuzzy logic,\nthe conclusions of data analysis are automatically translated into\ncomprehensible recommendations in a natural language.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 08:08:11 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Sidorova", "J.", ""], ["Skold", "L.", ""], ["Rosander", "O.", ""], ["Lundberg", "L.", ""]]}, {"id": "1706.03946", "submitter": "Isabelle Augenstein", "authors": "Ed Collins and Isabelle Augenstein and Sebastian Riedel", "title": "A Supervised Approach to Extractive Summarisation of Scientific Papers", "comments": "11 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Automatic summarisation is a popular approach to reduce a document to its\nmain arguments. Recent research in the area has focused on neural approaches to\nsummarisation, which can be very data-hungry. However, few large datasets exist\nand none for the traditionally popular domain of scientific publications, which\nopens up challenging research avenues centered on encoding large, complex\ndocuments. In this paper, we introduce a new dataset for summarisation of\ncomputer science publications by exploiting a large resource of author provided\nsummaries and show straightforward ways of extending it further. We develop\nmodels on the dataset making use of both neural sentence encoding and\ntraditionally used summarisation features and show that models which encode\nsentences as well as their local and global context perform best, significantly\noutperforming well-established baseline methods.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 08:15:25 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Collins", "Ed", ""], ["Augenstein", "Isabelle", ""], ["Riedel", "Sebastian", ""]]}, {"id": "1706.03993", "submitter": "Joan Serr\\`a", "authors": "Joan Serr\\`a and Alexandros Karatzoglou", "title": "Getting deep recommenders fit: Bloom embeddings for sparse binary\n  input/output networks", "comments": "Accepted for publication at ACM RecSys 2017; previous version\n  submitted to ICLR 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recommendation algorithms that incorporate techniques from deep learning are\nbecoming increasingly popular. Due to the structure of the data coming from\nrecommendation domains (i.e., one-hot-encoded vectors of item preferences),\nthese algorithms tend to have large input and output dimensionalities that\ndominate their overall size. This makes them difficult to train, due to the\nlimited memory of graphical processing units, and difficult to deploy on mobile\ndevices with limited hardware. To address these difficulties, we propose Bloom\nembeddings, a compression technique that can be applied to the input and output\nof neural network models dealing with sparse high-dimensional binary-coded\ninstances. Bloom embeddings are computationally efficient, and do not seriously\ncompromise the accuracy of the model up to 1/5 compression ratios. In some\ncases, they even improve over the original accuracy, with relative increases up\nto 12%. We evaluate Bloom embeddings on 7 data sets and compare it against 4\nalternative methods, obtaining favorable results. We also discuss a number of\nfurther advantages of Bloom embeddings, such as 'on-the-fly' constant-time\noperation, zero or marginal space requirements, training time speedups, or the\nfact that they do not require any change to the core model architecture or\ntraining configuration.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 10:50:25 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Serr\u00e0", "Joan", ""], ["Karatzoglou", "Alexandros", ""]]}, {"id": "1706.04033", "submitter": "Federico Cerutti", "authors": "Federico Cerutti and Alice Toniolo and Timothy J. Norman", "title": "On Natural Language Generation of Formal Argumentation", "comments": "17 pages, 4 figures, technical report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we provide a first analysis of the research questions that\narise when dealing with the problem of communicating pieces of formal\nargumentation through natural language interfaces. It is a generally held\nopinion that formal models of argumentation naturally capture human argument,\nand some preliminary studies have focused on justifying this view.\nUnfortunately, the results are not only inconclusive, but seem to suggest that\nexplaining formal argumentation to humans is a rather articulated task.\nGraphical models for expressing argumentation-based reasoning are appealing,\nbut often humans require significant training to use these tools effectively.\nWe claim that natural language interfaces to formal argumentation systems offer\na real alternative, and may be the way forward for systems that capture human\nargument.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 13:01:53 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Cerutti", "Federico", ""], ["Toniolo", "Alice", ""], ["Norman", "Timothy J.", ""]]}, {"id": "1706.04038", "submitter": "Ahmad Al Sallab Dr", "authors": "Ahmad El Sallab, Mahmoud Saeed, Omar Abdel Tawab, Mohammed Abdou", "title": "Meta learning Framework for Automated Driving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The success of automated driving deployment is highly depending on the\nability to develop an efficient and safe driving policy. The problem is well\nformulated under the framework of optimal control as a cost optimization\nproblem. Model based solutions using traditional planning are efficient, but\nrequire the knowledge of the environment model. On the other hand, model free\nsolutions suffer sample inefficiency and require too many interactions with the\nenvironment, which is infeasible in practice. Methods under the Reinforcement\nLearning framework usually require the notion of a reward function, which is\nnot available in the real world. Imitation learning helps in improving sample\nefficiency by introducing prior knowledge obtained from the demonstrated\nbehavior, on the risk of exact behavior cloning without generalizing to unseen\nenvironments. In this paper we propose a Meta learning framework, based on data\nset aggregation, to improve generalization of imitation learning algorithms.\nUnder the proposed framework, we propose MetaDAgger, a novel algorithm which\ntackles the generalization issues in traditional imitation learning. We use The\nOpen Race Car Simulator (TORCS) to test our algorithm. Results on unseen test\ntracks show significant improvement over traditional imitation learning\nalgorithms, improving the learning time and sample efficiency in the same time.\nThe results are also supported by visualization of the learnt features to prove\ngeneralization of the captured details.\n", "versions": [{"version": "v1", "created": "Sun, 11 Jun 2017 12:32:30 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Sallab", "Ahmad El", ""], ["Saeed", "Mahmoud", ""], ["Tawab", "Omar Abdel", ""], ["Abdou", "Mohammed", ""]]}, {"id": "1706.04052", "submitter": "Jinzhuo Wang", "authors": "Jinzhuo Wang, Wenmin Wang, Ronggang Wang, Wen Gao", "title": "Beyond Monte Carlo Tree Search: Playing Go with Deep Alternative Neural\n  Network and Long-Term Evaluation", "comments": "AAAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Monte Carlo tree search (MCTS) is extremely popular in computer Go which\ndetermines each action by enormous simulations in a broad and deep search tree.\nHowever, human experts select most actions by pattern analysis and careful\nevaluation rather than brute search of millions of future nteractions. In this\npaper, we propose a computer Go system that follows experts way of thinking and\nplaying. Our system consists of two parts. The first part is a novel deep\nalternative neural network (DANN) used to generate candidates of next move.\nCompared with existing deep convolutional neural network (DCNN), DANN inserts\nrecurrent layer after each convolutional layer and stacks them in an\nalternative manner. We show such setting can preserve more contexts of local\nfeatures and its evolutions which are beneficial for move prediction. The\nsecond part is a long-term evaluation (LTE) module used to provide a reliable\nevaluation of candidates rather than a single probability from move predictor.\nThis is consistent with human experts nature of playing since they can foresee\ntens of steps to give an accurate estimation of candidates. In our system, for\neach candidate, LTE calculates a cumulative reward after several future\ninteractions when local variations are settled. Combining criteria from the two\nparts, our system determines the optimal choice of next move. For more\ncomprehensive experiments, we introduce a new professional Go dataset (PGD),\nconsisting of 253233 professional records. Experiments on GoGoD and PGD\ndatasets show the DANN can substantially improve performance of move prediction\nover pure DCNN. When combining LTE, our system outperforms most relevant\napproaches and open engines based on MCTS.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 13:30:04 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Wang", "Jinzhuo", ""], ["Wang", "Wenmin", ""], ["Wang", "Ronggang", ""], ["Gao", "Wen", ""]]}, {"id": "1706.04109", "submitter": "Fran Casino", "authors": "Fran Casino, Constantinos Patsakis, Antoni Martinez-Balleste, Frederic\n  Borras, Edgar Batista", "title": "Technical Report: Implementation and Validation of a Smart Health\n  Application", "comments": "4-page Tech Report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we explain in detail the internal structures and databases\nof a smart health application. Moreover, we describe how to generate a\nstatistically sound synthetic dataset using real-world medical data.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 14:58:41 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Casino", "Fran", ""], ["Patsakis", "Constantinos", ""], ["Martinez-Balleste", "Antoni", ""], ["Borras", "Frederic", ""], ["Batista", "Edgar", ""]]}, {"id": "1706.04115", "submitter": "Omer Levy", "authors": "Omer Levy, Minjoon Seo, Eunsol Choi, Luke Zettlemoyer", "title": "Zero-Shot Relation Extraction via Reading Comprehension", "comments": "CoNLL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show that relation extraction can be reduced to answering simple reading\ncomprehension questions, by associating one or more natural-language questions\nwith each relation slot. This reduction has several advantages: we can (1)\nlearn relation-extraction models by extending recent neural\nreading-comprehension techniques, (2) build very large training sets for those\nmodels by combining relation-specific crowd-sourced questions with distant\nsupervision, and even (3) do zero-shot learning by extracting new relation\ntypes that are only specified at test-time, for which we have no labeled\ntraining examples. Experiments on a Wikipedia slot-filling task demonstrate\nthat the approach can generalize to new questions for known relation types with\nhigh accuracy, and that zero-shot generalization to unseen relation types is\npossible, at lower accuracy levels, setting the bar for future work on this\ntask.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 15:17:42 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Levy", "Omer", ""], ["Seo", "Minjoon", ""], ["Choi", "Eunsol", ""], ["Zettlemoyer", "Luke", ""]]}, {"id": "1706.04156", "submitter": "Vaishnavh Nagarajan", "authors": "Vaishnavh Nagarajan, J. Zico Kolter", "title": "Gradient descent GAN optimization is locally stable", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite the growing prominence of generative adversarial networks (GANs),\noptimization in GANs is still a poorly understood topic. In this paper, we\nanalyze the \"gradient descent\" form of GAN optimization i.e., the natural\nsetting where we simultaneously take small gradient steps in both generator and\ndiscriminator parameters. We show that even though GAN optimization does not\ncorrespond to a convex-concave game (even for simple parameterizations), under\nproper conditions, equilibrium points of this optimization procedure are still\n\\emph{locally asymptotically stable} for the traditional GAN formulation. On\nthe other hand, we show that the recently proposed Wasserstein GAN can have\nnon-convergent limit cycles near equilibrium. Motivated by this stability\nanalysis, we propose an additional regularization term for gradient descent GAN\nupdates, which \\emph{is} able to guarantee local stability for both the WGAN\nand the traditional GAN, and also shows practical promise in speeding up\nconvergence and addressing mode collapse.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 16:49:13 GMT"}, {"version": "v2", "created": "Wed, 15 Nov 2017 03:29:12 GMT"}, {"version": "v3", "created": "Sat, 13 Jan 2018 18:39:22 GMT"}], "update_date": "2018-01-16", "authors_parsed": [["Nagarajan", "Vaishnavh", ""], ["Kolter", "J. Zico", ""]]}, {"id": "1706.04189", "submitter": "Somaiyeh Mahmoud Zadeh", "authors": "Somaiyeh Mahmoud.Zadeh", "title": "Autonomous Reactive Mission Scheduling and Task-Path Planning\n  Architecture for Autonomous Underwater Vehicle", "comments": "Thesis of PhD completed at Flinders University of South Australia,\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An Autonomous Underwater Vehicle (AUV) should carry out complex tasks in a\nlimited time interval. Since existing AUVs have limited battery capacity and\nrestricted endurance, they should autonomously manage mission time and the\nresources to perform effective persistent deployment in longer missions. Task\nassignment requires making decisions subject to resource constraints, while\ntasks are assigned with costs and/or values that are budgeted in advance. Tasks\nare distributed in a particular operation zone and mapped by a waypoint covered\nnetwork. Thus, design an efficient routing-task priority assign framework\nconsidering vehicle's availabilities and properties is essential for increasing\nmission productivity and on-time mission completion. This depends strongly on\nthe order and priority of the tasks that are located between node-like\nwaypoints in an operation network. On the other hand, autonomous operation of\nAUVs in an unfamiliar dynamic underwater and performing quick response to\nsudden environmental changes is a complicated process. Water current\ninstabilities can deflect the vehicle to an undesired direction and perturb\nAUVs safety. The vehicle's robustness to strong environmental variations is\nextremely crucial for its safe and optimum operations in an uncertain and\ndynamic environment. To this end, the AUV needs to have a general overview of\nthe environment in top level to perform an autonomous action selection (task\nselection) and a lower level local motion planner to operate successfully in\ndealing with continuously changing situations. This research deals with\ndeveloping a novel reactive control architecture to provide a higher level of\ndecision autonomy for the AUV operation that enables a single vehicle to\naccomplish multiple tasks in a single mission in the face of periodic\ndisturbances in a turbulent and highly uncertain environment.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 00:15:48 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Zadeh", "Somaiyeh Mahmoud.", ""]]}, {"id": "1706.04215", "submitter": "Ashwinkumar Ganesan", "authors": "Mandar Haldekar, Ashwinkumar Ganesan, Tim Oates", "title": "Identifying Spatial Relations in Images using Convolutional Neural\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional approaches to building a large scale knowledge graph have usually\nrelied on extracting information (entities, their properties, and relations\nbetween them) from unstructured text (e.g. Dbpedia). Recent advances in\nConvolutional Neural Networks (CNN) allow us to shift our focus to learning\nentities and relations from images, as they build robust models that require\nlittle or no pre-processing of the images. In this paper, we present an\napproach to identify and extract spatial relations (e.g., The girl is standing\nbehind the table) from images using CNNs. Our research addresses two specific\nchallenges: providing insight into how spatial relations are learned by the\nnetwork and which parts of the image are used to predict these relations. We\nuse the pre-trained network VGGNet to extract features from an image and train\na Multi-layer Perceptron (MLP) on a set of synthetic images and the sun09\ndataset to extract spatial relations. The MLP predicts spatial relations\nwithout a bounding box around the objects or the space in the image depicting\nthe relation. To understand how the spatial relations are represented in the\nnetwork, a heatmap is overlayed on the image to show the regions that are\ndeemed important by the network. Also, we analyze the MLP to show the\nrelationship between the activation of consistent groups of nodes and the\nprediction of a spatial relation. We show how the loss of these groups affects\nthe networks ability to identify relations.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 18:24:11 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Haldekar", "Mandar", ""], ["Ganesan", "Ashwinkumar", ""], ["Oates", "Tim", ""]]}, {"id": "1706.04262", "submitter": "Abolfazl Ramezanpour", "authors": "A. Ramezanpour", "title": "Optimization by a quantum reinforcement algorithm", "comments": "14 pages, 5 figures", "journal-ref": "Phys. Rev. A 96, 052307 (2017)", "doi": "10.1103/PhysRevA.96.052307", "report-no": null, "categories": "cond-mat.dis-nn cond-mat.stat-mech cs.AI cs.LG quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A reinforcement algorithm solves a classical optimization problem by\nintroducing a feedback to the system which slowly changes the energy landscape\nand converges the algorithm to an optimal solution in the configuration space.\nHere, we use this strategy to concentrate (localize) preferentially the wave\nfunction of a quantum particle, which explores the configuration space of the\nproblem, on an optimal configuration. We examine the method by solving\nnumerically the equations governing the evolution of the system, which are\nsimilar to the nonlinear Schr\\\"odinger equations, for small problem sizes. In\nparticular, we observe that reinforcement increases the minimal energy gap of\nthe system in a quantum annealing algorithm. Our numerical simulations and the\nlatter observation show that such kind of quantum feedbacks might be helpful in\nsolving a computationally hard optimization problem by a quantum reinforcement\nalgorithm.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 21:32:54 GMT"}, {"version": "v2", "created": "Fri, 4 Aug 2017 12:00:47 GMT"}, {"version": "v3", "created": "Fri, 3 Nov 2017 17:19:47 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Ramezanpour", "A.", ""]]}, {"id": "1706.04317", "submitter": "Ken Kansky", "authors": "Ken Kansky, Tom Silver, David A. M\\'ely, Mohamed Eldawy, Miguel\n  L\\'azaro-Gredilla, Xinghua Lou, Nimrod Dorfman, Szymon Sidor, Scott Phoenix,\n  Dileep George", "title": "Schema Networks: Zero-shot Transfer with a Generative Causal Model of\n  Intuitive Physics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The recent adaptation of deep neural network-based methods to reinforcement\nlearning and planning domains has yielded remarkable progress on individual\ntasks. Nonetheless, progress on task-to-task transfer remains limited. In\npursuit of efficient and robust generalization, we introduce the Schema\nNetwork, an object-oriented generative physics simulator capable of\ndisentangling multiple causes of events and reasoning backward through causes\nto achieve goals. The richly structured architecture of the Schema Network can\nlearn the dynamics of an environment directly from data. We compare Schema\nNetworks with Asynchronous Advantage Actor-Critic and Progressive Networks on a\nsuite of Breakout variations, reporting results on training efficiency and\nzero-shot generalization, consistently demonstrating faster, more robust\nlearning and better transfer. We argue that generalizing from limited data and\nlearning causal relationships are essential abilities on the path toward\ngenerally intelligent systems.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 05:11:08 GMT"}, {"version": "v2", "created": "Thu, 17 Aug 2017 23:37:54 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Kansky", "Ken", ""], ["Silver", "Tom", ""], ["M\u00e9ly", "David A.", ""], ["Eldawy", "Mohamed", ""], ["L\u00e1zaro-Gredilla", "Miguel", ""], ["Lou", "Xinghua", ""], ["Dorfman", "Nimrod", ""], ["Sidor", "Szymon", ""], ["Phoenix", "Scott", ""], ["George", "Dileep", ""]]}, {"id": "1706.04399", "submitter": "Manh Duong Phung", "authors": "Manh Duong Phung, Cong Hoang Quach, Tran Hiep Dinh, Quang Ha", "title": "Enhanced discrete particle swarm optimization path planning for UAV\n  vision-based surface inspection", "comments": null, "journal-ref": "Automation in Construction, Vol.81, pp.25-33 (2017)", "doi": "10.1016/j.autcon.2017.04.013", "report-no": null, "categories": "cs.RO cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In built infrastructure monitoring, an efficient path planning algorithm is\nessential for robotic inspection of large surfaces using computer vision. In\nthis work, we first formulate the inspection path planning problem as an\nextended travelling salesman problem (TSP) in which both the coverage and\nobstacle avoidance were taken into account. An enhanced discrete particle swarm\noptimization (DPSO) algorithm is then proposed to solve the TSP, with\nperformance improvement by using deterministic initialization, random mutation,\nand edge exchange. Finally, we take advantage of parallel computing to\nimplement the DPSO in a GPU-based framework so that the computation time can be\nsignificantly reduced while keeping the hardware requirement unchanged. To show\nthe effectiveness of the proposed algorithm, experimental results are included\nfor datasets obtained from UAV inspection of an office building and a bridge.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 10:40:19 GMT"}], "update_date": "2017-06-15", "authors_parsed": [["Phung", "Manh Duong", ""], ["Quach", "Cong Hoang", ""], ["Dinh", "Tran Hiep", ""], ["Ha", "Quang", ""]]}, {"id": "1706.04463", "submitter": "Zutao Jiang", "authors": "Zutao Jiang, Jihua Zhu, Yaochen Li, Zhongyu Li, Huimin Lu", "title": "Simultaneous merging multiple grid maps using the robust motion\n  averaging", "comments": null, "journal-ref": null, "doi": "10.1007/s10846-018-0895-4", "report-no": null, "categories": "cs.AI cs.RO", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Mapping in the GPS-denied environment is an important and challenging task in\nthe field of robotics. In the large environment, mapping can be significantly\naccelerated by multiple robots exploring different parts of the environment.\nAccordingly, a key problem is how to integrate these local maps built by\ndifferent robots into a single global map. In this paper, we propose an\napproach for simultaneous merging of multiple grid maps by the robust motion\naveraging. The main idea of this approach is to recover all global motions for\nmap merging from a set of relative motions. Therefore, it firstly adopts the\npair-wise map merging method to estimate relative motions for grid map pairs.\nTo obtain as many reliable relative motions as possible, a graph-based sampling\nscheme is utilized to efficiently remove unreliable relative motions obtained\nfrom the pair-wise map merging. Subsequently, the accurate global motions can\nbe recovered from the set of reliable relative motions by the motion averaging.\nExperimental results carried on real robot data sets demonstrate that proposed\napproach can achieve simultaneous merging of multiple grid maps with good\nperformances.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 13:03:04 GMT"}], "update_date": "2018-09-05", "authors_parsed": [["Jiang", "Zutao", ""], ["Zhu", "Jihua", ""], ["Li", "Yaochen", ""], ["Li", "Zhongyu", ""], ["Lu", "Huimin", ""]]}, {"id": "1706.04486", "submitter": "Mason Bretan", "authors": "Mason Bretan, Sageev Oore, Doug Eck, Larry Heck", "title": "Learning and Evaluating Musical Features with Deep Autoencoders", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work we describe and evaluate methods to learn musical embeddings.\nEach embedding is a vector that represents four contiguous beats of music and\nis derived from a symbolic representation. We consider autoencoding-based\nmethods including denoising autoencoders, and context reconstruction, and\nevaluate the resulting embeddings on a forward prediction and a classification\ntask.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 13:35:46 GMT"}, {"version": "v2", "created": "Thu, 15 Jun 2017 20:09:52 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["Bretan", "Mason", ""], ["Oore", "Sageev", ""], ["Eck", "Doug", ""], ["Heck", "Larry", ""]]}, {"id": "1706.04560", "submitter": "Tong Wang", "authors": "Sandeep Subramanian, Tong Wang, Xingdi Yuan, Saizheng Zhang, Yoshua\n  Bengio, Adam Trischler", "title": "Neural Models for Key Phrase Detection and Question Generation", "comments": "Machine Reading for Question Answering workshop at ACL 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a two-stage neural model to tackle question generation from\ndocuments. First, our model estimates the probability that word sequences in a\ndocument are ones that a human would pick when selecting candidate answers by\ntraining a neural key-phrase extractor on the answers in a question-answering\ncorpus. Predicted key phrases then act as target answers and condition a\nsequence-to-sequence question-generation model with a copy mechanism.\nEmpirically, our key-phrase extraction model significantly outperforms an\nentity-tagging baseline and existing rule-based approaches. We further\ndemonstrate that our question generation system formulates fluent, answerable\nquestions from key phrases. This two-stage system could be used to augment or\ngenerate reading comprehension datasets, which may be leveraged to improve\nmachine reading systems or in educational settings.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 16:06:18 GMT"}, {"version": "v2", "created": "Thu, 14 Sep 2017 17:43:48 GMT"}, {"version": "v3", "created": "Wed, 30 May 2018 17:57:41 GMT"}], "update_date": "2018-05-31", "authors_parsed": [["Subramanian", "Sandeep", ""], ["Wang", "Tong", ""], ["Yuan", "Xingdi", ""], ["Zhang", "Saizheng", ""], ["Bengio", "Yoshua", ""], ["Trischler", "Adam", ""]]}, {"id": "1706.04582", "submitter": "David Narv\\'aez", "authors": "Lane A. Hemaspaandra and David E. Narv\\'aez", "title": "Existence versus Exploitation: The Opacity of Backbones and Backdoors\n  Under a Weak Assumption", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Backdoors and backbones of Boolean formulas are hidden structural properties.\nA natural goal, already in part realized, is that solver algorithms seek to\nobtain substantially better performance by exploiting these structures.\n  However, the present paper is not intended to improve the performance of SAT\nsolvers, but rather is a cautionary paper. In particular, the theme of this\npaper is that there is a potential chasm between the existence of such\nstructures in the Boolean formula and being able to effectively exploit them.\nThis does not mean that these structures are not useful to solvers. It does\nmean that one must be very careful not to assume that it is computationally\neasy to go from the existence of a structure to being able to get one's hands\non it and/or being able to exploit the structure.\n  For example, in this paper we show that, under the assumption that P $\\neq$\nNP, there are easily recognizable families of Boolean formulas with strong\nbackdoors that are easy to find, yet for which it is hard (in fact,\nNP-complete) to determine whether the formulas are satisfiable. We also show\nthat, also under the assumption P $\\neq$ NP, there are easily recognizable sets\nof Boolean formulas for which it is hard (in fact, NP-complete) to determine\nwhether they have a large backbone.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 16:46:01 GMT"}, {"version": "v2", "created": "Wed, 6 Sep 2017 21:45:55 GMT"}, {"version": "v3", "created": "Fri, 13 Oct 2017 17:54:53 GMT"}, {"version": "v4", "created": "Tue, 24 Oct 2017 15:51:57 GMT"}, {"version": "v5", "created": "Sat, 10 Mar 2018 16:44:58 GMT"}, {"version": "v6", "created": "Tue, 24 Apr 2018 00:47:04 GMT"}, {"version": "v7", "created": "Tue, 3 Jul 2018 18:44:09 GMT"}, {"version": "v8", "created": "Thu, 1 Nov 2018 19:06:35 GMT"}], "update_date": "2018-11-05", "authors_parsed": [["Hemaspaandra", "Lane A.", ""], ["Narv\u00e1ez", "David E.", ""]]}, {"id": "1706.04652", "submitter": "Ulrich Viereck", "authors": "Ulrich Viereck, Andreas ten Pas, Kate Saenko, Robert Platt", "title": "Learning a visuomotor controller for real world robotic grasping using\n  simulated depth images", "comments": "1st Conference on Robot Learning (CoRL), 13-15 November 2017,\n  Mountain View, CA", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We want to build robots that are useful in unstructured real world\napplications, such as doing work in the household. Grasping in particular is an\nimportant skill in this domain, yet it remains a challenge. One of the key\nhurdles is handling unexpected changes or motion in the objects being grasped\nand kinematic noise or other errors in the robot. This paper proposes an\napproach to learning a closed-loop controller for robotic grasping that\ndynamically guides the gripper to the object. We use a wrist-mounted sensor to\nacquire depth images in front of the gripper and train a convolutional neural\nnetwork to learn a distance function to true grasps for grasp configurations\nover an image. The training sensor data is generated in simulation, a major\nadvantage over previous work that uses real robot experience, which is costly\nto obtain. Despite being trained in simulation, our approach works well on real\nnoisy sensor images. We compare our controller in simulated and real robot\nexperiments to a strong baseline for grasp pose detection, and find that our\napproach significantly outperforms the baseline in the presence of kinematic\nnoise, perceptual errors and disturbances of the object during grasping.\n", "versions": [{"version": "v1", "created": "Wed, 14 Jun 2017 19:50:09 GMT"}, {"version": "v2", "created": "Fri, 30 Jun 2017 21:18:20 GMT"}, {"version": "v3", "created": "Fri, 17 Nov 2017 20:09:12 GMT"}], "update_date": "2017-11-21", "authors_parsed": [["Viereck", "Ulrich", ""], ["Pas", "Andreas ten", ""], ["Saenko", "Kate", ""], ["Platt", "Robert", ""]]}, {"id": "1706.04721", "submitter": "Shannon Fenn", "authors": "Shannon Fenn, Pablo Moscato", "title": "Target Curricula via Selection of Minimum Feature Sets: a Case Study in\n  Boolean Networks", "comments": "Accepted for publication in JMLR issue 18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the effect of introducing a curriculum of targets when training\nBoolean models on supervised Multi Label Classification (MLC) problems. In\nparticular, we consider how to order targets in the absence of prior knowledge,\nand how such a curriculum may be enforced when using meta-heuristics to train\ndiscrete non-linear models.\n  We show that hierarchical dependencies between targets can be exploited by\nenforcing an appropriate curriculum using hierarchical loss functions. On\nseveral multi output circuit-inference problems with known target difficulties,\nFeedforward Boolean Networks (FBNs) trained with such a loss function achieve\nsignificantly lower out-of-sample error, up to $10\\%$ in some cases. This\nimprovement increases as the loss places more emphasis on target order and is\nstrongly correlated with an easy-to-hard curricula. We also demonstrate the\nsame improvements on three real-world models and two Gene Regulatory Network\n(GRN) inference problems.\n  We posit a simple a-priori method for identifying an appropriate target order\nand estimating the strength of target relationships in Boolean MLCs. These\nmethods use intrinsic dimension as a proxy for target difficulty, which is\nestimated using optimal solutions to a combinatorial optimisation problem known\nas the Minimum-Feature-Set (minFS) problem. We also demonstrate that the same\ngeneralisation gains can be achieved without providing any knowledge of target\ndifficulty.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 02:08:54 GMT"}, {"version": "v2", "created": "Wed, 1 Nov 2017 02:57:13 GMT"}], "update_date": "2017-11-02", "authors_parsed": [["Fenn", "Shannon", ""], ["Moscato", "Pablo", ""]]}, {"id": "1706.04825", "submitter": "Lucas Bechberger", "authors": "Lucas Bechberger and Kai-Uwe K\\\"uhnberger", "title": "Towards Grounding Conceptual Spaces in Neural Representations", "comments": "accepted at NeSy 2017; The final version of this paper is available\n  at http://ceur-ws.org/Vol-2003/", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The highly influential framework of conceptual spaces provides a geometric\nway of representing knowledge. It aims at bridging the gap between symbolic and\nsubsymbolic processing. Instances are represented by points in a\nhigh-dimensional space and concepts are represented by convex regions in this\nspace. In this paper, we present our approach towards grounding the dimensions\nof a conceptual space in latent spaces learned by an InfoGAN from unlabeled\ndata.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 11:59:06 GMT"}, {"version": "v2", "created": "Tue, 21 Nov 2017 07:27:49 GMT"}], "update_date": "2017-11-22", "authors_parsed": [["Bechberger", "Lucas", ""], ["K\u00fchnberger", "Kai-Uwe", ""]]}, {"id": "1706.04972", "submitter": "Hieu Pham", "authors": "Azalia Mirhoseini and Hieu Pham and Quoc V. Le and Benoit Steiner and\n  Rasmus Larsen and Yuefeng Zhou and Naveen Kumar and Mohammad Norouzi and Samy\n  Bengio and Jeff Dean", "title": "Device Placement Optimization with Reinforcement Learning", "comments": "To appear at ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The past few years have witnessed a growth in size and computational\nrequirements for training and inference with neural networks. Currently, a\ncommon approach to address these requirements is to use a heterogeneous\ndistributed environment with a mixture of hardware devices such as CPUs and\nGPUs. Importantly, the decision of placing parts of the neural models on\ndevices is often made by human experts based on simple heuristics and\nintuitions. In this paper, we propose a method which learns to optimize device\nplacement for TensorFlow computational graphs. Key to our method is the use of\na sequence-to-sequence model to predict which subsets of operations in a\nTensorFlow graph should run on which of the available devices. The execution\ntime of the predicted placements is then used as the reward signal to optimize\nthe parameters of the sequence-to-sequence model. Our main result is that on\nInception-V3 for ImageNet classification, and on RNN LSTM, for language\nmodeling and neural machine translation, our model finds non-trivial device\nplacements that outperform hand-crafted heuristics and traditional algorithmic\nmethods.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 16:26:40 GMT"}, {"version": "v2", "created": "Sun, 25 Jun 2017 23:55:21 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Mirhoseini", "Azalia", ""], ["Pham", "Hieu", ""], ["Le", "Quoc V.", ""], ["Steiner", "Benoit", ""], ["Larsen", "Rasmus", ""], ["Zhou", "Yuefeng", ""], ["Kumar", "Naveen", ""], ["Norouzi", "Mohammad", ""], ["Bengio", "Samy", ""], ["Dean", "Jeff", ""]]}, {"id": "1706.05059", "submitter": "Victor Dalmau", "authors": "Victor Dalmau", "title": "Conjunctions of Among Constraints", "comments": "15 pages plus appendix", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many existing global constraints can be encoded as a conjunction of among\nconstraints. An among constraint holds if the number of the variables in its\nscope whose value belongs to a prespecified set, which we call its range, is\nwithin some given bounds. It is known that domain filtering algorithms can\nbenefit from reasoning about the interaction of among constraints so that\nvalues can be filtered out taking into consideration several among constraints\nsimultaneously. The present pa- per embarks into a systematic investigation on\nthe circumstances under which it is possible to obtain efficient and complete\ndomain filtering algorithms for conjunctions of among constraints. We start by\nobserving that restrictions on both the scope and the range of the among\nconstraints are necessary to obtain meaningful results. Then, we derive a\ndomain flow-based filtering algorithm and present several applications. In\nparticular, it is shown that the algorithm unifies and generalizes several\nprevious existing results.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 19:51:52 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["Dalmau", "Victor", ""]]}, {"id": "1706.05064", "submitter": "Junhyuk Oh", "authors": "Junhyuk Oh, Satinder Singh, Honglak Lee, Pushmeet Kohli", "title": "Zero-Shot Task Generalization with Multi-Task Deep Reinforcement\n  Learning", "comments": "ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a step towards developing zero-shot task generalization capabilities in\nreinforcement learning (RL), we introduce a new RL problem where the agent\nshould learn to execute sequences of instructions after learning useful skills\nthat solve subtasks. In this problem, we consider two types of generalizations:\nto previously unseen instructions and to longer sequences of instructions. For\ngeneralization over unseen instructions, we propose a new objective which\nencourages learning correspondences between similar subtasks by making\nanalogies. For generalization over sequential instructions, we present a\nhierarchical architecture where a meta controller learns to use the acquired\nskills for executing the instructions. To deal with delayed reward, we propose\na new neural architecture in the meta controller that learns when to update the\nsubtask, which makes learning more efficient. Experimental results on a\nstochastic 3D domain show that the proposed ideas are crucial for\ngeneralization to longer instructions as well as unseen instructions.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 20:04:35 GMT"}, {"version": "v2", "created": "Tue, 7 Nov 2017 00:37:51 GMT"}], "update_date": "2017-11-08", "authors_parsed": [["Oh", "Junhyuk", ""], ["Singh", "Satinder", ""], ["Lee", "Honglak", ""], ["Kohli", "Pushmeet", ""]]}, {"id": "1706.05075", "submitter": "Peng Zhou", "authors": "Suncong Zheng, Feng Wang, Hongyun Bao, Yuexing Hao, Peng Zhou, Bo Xu", "title": "Joint Extraction of Entities and Relations Based on a Novel Tagging\n  Scheme", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Joint extraction of entities and relations is an important task in\ninformation extraction. To tackle this problem, we firstly propose a novel\ntagging scheme that can convert the joint extraction task to a tagging problem.\nThen, based on our tagging scheme, we study different end-to-end models to\nextract entities and their relations directly, without identifying entities and\nrelations separately. We conduct experiments on a public dataset produced by\ndistant supervision method and the experimental results show that the tagging\nbased methods are better than most of the existing pipelined and joint learning\nmethods. What's more, the end-to-end model proposed in this paper, achieves the\nbest results on the public dataset.\n", "versions": [{"version": "v1", "created": "Wed, 7 Jun 2017 03:14:23 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["Zheng", "Suncong", ""], ["Wang", "Feng", ""], ["Bao", "Hongyun", ""], ["Hao", "Yuexing", ""], ["Zhou", "Peng", ""], ["Xu", "Bo", ""]]}, {"id": "1706.05086", "submitter": "Jialin Liu Ph.D", "authors": "Simon M. Lucas, Jialin Liu, Diego P\\'erez-Li\\'ebana", "title": "Evaluating Noisy Optimisation Algorithms: First Hitting Time is\n  Problematic", "comments": "4 pages, 4 figurs, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A key part of any evolutionary algorithm is fitness evaluation. When fitness\nevaluations are corrupted by noise, as happens in many real-world problems as a\nconsequence of various types of uncertainty, a strategy is needed in order to\ncope with this. Resampling is one of the most common strategies, whereby each\nsolution is evaluated many times in order to reduce the variance of the fitness\nestimates. When evaluating the performance of a noisy optimisation algorithm, a\nkey consideration is the stopping condition for the algorithm. A frequently\nused stopping condition in runtime analysis, known as \"First Hitting Time\", is\nto stop the algorithm as soon as it encounters the optimal solution. However,\nthis is unrealistic for real-world problems, as if the optimal solution were\nalready known, there would be no need to search for it. This paper argues that\nthe use of First Hitting Time, despite being a commonly used approach, is\nsignificantly flawed and overestimates the quality of many algorithms in\nreal-world cases, where the optimum is not known in advance and has to be\ngenuinely searched for. A better alternative is to measure the quality of the\nsolution an algorithm returns after a fixed evaluation budget, i.e., to focus\non final solution quality. This paper argues that focussing on final solution\nquality is more realistic and demonstrates cases where the results produced by\neach algorithm evaluation method lead to very different conclusions regarding\nthe quality of each noisy optimisation algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 09:44:34 GMT"}, {"version": "v2", "created": "Wed, 12 Jul 2017 11:20:05 GMT"}], "update_date": "2017-07-13", "authors_parsed": [["Lucas", "Simon M.", ""], ["Liu", "Jialin", ""], ["P\u00e9rez-Li\u00e9bana", "Diego", ""]]}, {"id": "1706.05098", "submitter": "Sebastian Ruder", "authors": "Sebastian Ruder", "title": "An Overview of Multi-Task Learning in Deep Neural Networks", "comments": "14 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-task learning (MTL) has led to successes in many applications of\nmachine learning, from natural language processing and speech recognition to\ncomputer vision and drug discovery. This article aims to give a general\noverview of MTL, particularly in deep neural networks. It introduces the two\nmost common methods for MTL in Deep Learning, gives an overview of the\nliterature, and discusses recent advances. In particular, it seeks to help ML\npractitioners apply MTL by shedding light on how MTL works and providing\nguidelines for choosing appropriate auxiliary tasks.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 21:38:12 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["Ruder", "Sebastian", ""]]}, {"id": "1706.05122", "submitter": "Takuma Yoneda", "authors": "Takuma Yoneda and Koki Mori and Makoto Miwa and Yutaka Sasaki", "title": "Bib2vec: An Embedding-based Search System for Bibliographic Information", "comments": "EACL2017 extended version. The demonstration is available at\n  http://tti-coin.jp/demo/bib2vec/", "journal-ref": "Proceedings of the EACL 2017 Software Demonstrations, Valencia,\n  Spain, April 3-7 2017, pages 112-115", "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel embedding model that represents relationships among\nseveral elements in bibliographic information with high representation ability\nand flexibility. Based on this model, we present a novel search system that\nshows the relationships among the elements in the ACL Anthology Reference\nCorpus. The evaluation results show that our model can achieve a high\nprediction ability and produce reasonable search results.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 00:53:28 GMT"}, {"version": "v2", "created": "Tue, 17 Oct 2017 16:33:20 GMT"}, {"version": "v3", "created": "Thu, 5 Apr 2018 09:19:57 GMT"}], "update_date": "2018-04-06", "authors_parsed": [["Yoneda", "Takuma", ""], ["Mori", "Koki", ""], ["Miwa", "Makoto", ""], ["Sasaki", "Yutaka", ""]]}, {"id": "1706.05125", "submitter": "Yann Dauphin", "authors": "Mike Lewis, Denis Yarats, Yann N. Dauphin, Devi Parikh and Dhruv Batra", "title": "Deal or No Deal? End-to-End Learning for Negotiation Dialogues", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Much of human dialogue occurs in semi-cooperative settings, where agents with\ndifferent goals attempt to agree on common decisions. Negotiations require\ncomplex communication and reasoning skills, but success is easy to measure,\nmaking this an interesting task for AI. We gather a large dataset of\nhuman-human negotiations on a multi-issue bargaining task, where agents who\ncannot observe each other's reward functions must reach an agreement (or a\ndeal) via natural language dialogue. For the first time, we show it is possible\nto train end-to-end models for negotiation, which must learn both linguistic\nand reasoning skills with no annotated dialogue states. We also introduce\ndialogue rollouts, in which the model plans ahead by simulating possible\ncomplete continuations of the conversation, and find that this technique\ndramatically improves performance. Our code and dataset are publicly available\n(https://github.com/facebookresearch/end-to-end-negotiator).\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 01:26:09 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["Lewis", "Mike", ""], ["Yarats", "Denis", ""], ["Dauphin", "Yann N.", ""], ["Parikh", "Devi", ""], ["Batra", "Dhruv", ""]]}, {"id": "1706.05143", "submitter": "Terrence Adams", "authors": "Terrence Adams", "title": "AI-Powered Social Bots", "comments": "2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.CY cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper gives an overview of impersonation bots that generate output in\none, or possibly, multiple modalities. We also discuss rapidly advancing areas\nof machine learning and artificial intelligence that could lead to\nfrighteningly powerful new multi-modal social bots. Our main conclusion is that\nmost commonly known bots are one dimensional (i.e., chatterbot), and far from\ndeceiving serious interrogators. However, using recent advances in machine\nlearning, it is possible to unleash incredibly powerful, human-like armies of\nsocial bots, in potentially well coordinated campaigns of deception and\ninfluence.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 04:54:47 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["Adams", "Terrence", ""]]}, {"id": "1706.05171", "submitter": "Peter Sch\\\"uller", "authors": "Mishal Kazmi and Peter Sch\\\"uller and Y\\\"ucel Sayg{\\i}n", "title": "Improving Scalability of Inductive Logic Programming via Pruning and\n  Best-Effort Optimisation", "comments": "24 pages, preprint of article accepted at Expert Systems With\n  Applications", "journal-ref": "Expert Systems With Applications 87, pages 291-303, 2017", "doi": "10.1016/j.eswa.2017.06.013", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inductive Logic Programming (ILP) combines rule-based and statistical\nartificial intelligence methods, by learning a hypothesis comprising a set of\nrules given background knowledge and constraints for the search space. We focus\non extending the XHAIL algorithm for ILP which is based on Answer Set\nProgramming and we evaluate our extensions using the Natural Language\nProcessing application of sentence chunking. With respect to processing natural\nlanguage, ILP can cater for the constant change in how we use language on a\ndaily basis. At the same time, ILP does not require huge amounts of training\nexamples such as other statistical methods and produces interpretable results,\nthat means a set of rules, which can be analysed and tweaked if necessary. As\ncontributions we extend XHAIL with (i) a pruning mechanism within the\nhypothesis generalisation algorithm which enables learning from larger\ndatasets, (ii) a better usage of modern solver technology using recently\ndeveloped optimisation methods, and (iii) a time budget that permits the usage\nof suboptimal results. We evaluate these improvements on the task of sentence\nchunking using three datasets from a recent SemEval competition. Results show\nthat our improvements allow for learning on bigger datasets with results that\nare of similar quality to state-of-the-art systems on the same task. Moreover,\nwe compare the hypotheses obtained on datasets to gain insights on the\nstructure of each dataset.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 08:02:55 GMT"}], "update_date": "2018-02-01", "authors_parsed": [["Kazmi", "Mishal", ""], ["Sch\u00fcller", "Peter", ""], ["Sayg\u0131n", "Y\u00fccel", ""]]}, {"id": "1706.05198", "submitter": "Ruitong Huang", "authors": "Ruitong Huang, Mohammad M. Ajallooeian, Csaba Szepesv\\'ari, Martin\n  M\\\"uller", "title": "Structured Best Arm Identification with Fixed Confidence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of identifying the best action among a set of possible\noptions when the value of each action is given by a mapping from a number of\nnoisy micro-observables in the so-called fixed confidence setting. Our main\nmotivation is the application to the minimax game search, which has been a\nmajor topic of interest in artificial intelligence. In this paper we introduce\nan abstract setting to clearly describe the essential properties of the\nproblem. While previous work only considered a two-move game tree search\nproblem, our abstract setting can be applied to the general minimax games where\nthe depth can be non-uniform and arbitrary, and transpositions are allowed. We\nintroduce a new algorithm (LUCB-micro) for the abstract setting, and give its\nlower and upper sample complexity results. Our bounds recover some previous\nresults, which were only available in more limited settings, while they also\nshed further light on how the structure of minimax problems influence sample\ncomplexity.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 09:51:36 GMT"}, {"version": "v2", "created": "Mon, 19 Jun 2017 05:47:31 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Huang", "Ruitong", ""], ["Ajallooeian", "Mohammad M.", ""], ["Szepesv\u00e1ri", "Csaba", ""], ["M\u00fcller", "Martin", ""]]}, {"id": "1706.05254", "submitter": "Margaretha Gansterer", "authors": "Margaretha Gansterer and Richard F. Hartl", "title": "Collaborative vehicle routing: a survey", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.CY math.OC physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In horizontal collaborations, carriers form coalitions in order to perform\nparts of their logistics operations jointly. By exchanging transportation\nrequests among each other, they can operate more efficiently and in a more\nsustainable way. Collaborative vehicle routing has been extensively discussed\nin the literature. We identify three major streams of research: (i) centralized\ncollaborative planning, (ii) decentralized planning without auctions, and (ii)\nauction-based decentralized planning. For each of them we give a structured\noverview on the state of knowledge and discuss future research directions.\n", "versions": [{"version": "v1", "created": "Tue, 13 Jun 2017 20:21:00 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["Gansterer", "Margaretha", ""], ["Hartl", "Richard F.", ""]]}, {"id": "1706.05261", "submitter": "Kevin Van Horn", "authors": "Kevin S. Van Horn", "title": "From Propositional Logic to Plausible Reasoning: A Uniqueness Theorem", "comments": "Submitted to Int'l Journal of Approximate Reasoning", "journal-ref": "International Journal of Approximate Reasoning 88 (2017), 309--332", "doi": "10.1016/j.ijar.2017.06.003", "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the question of extending propositional logic to a logic of\nplausible reasoning, and posit four requirements that any such extension should\nsatisfy. Each is a requirement that some property of classical propositional\nlogic be preserved in the extended logic; as such, the requirements are simpler\nand less problematic than those used in Cox's Theorem and its variants. As with\nCox's Theorem, our requirements imply that the extended logic must be\nisomorphic to (finite-set) probability theory. We also obtain specific\nnumerical values for the probabilities, recovering the classical definition of\nprobability as a theorem, with truth assignments that satisfy the premise\nplaying the role of the \"possible cases.\"\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 13:13:45 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Van Horn", "Kevin S.", ""]]}, {"id": "1706.05296", "submitter": "Peter Sunehag", "authors": "Peter Sunehag, Guy Lever, Audrunas Gruslys, Wojciech Marian Czarnecki,\n  Vinicius Zambaldi, Max Jaderberg, Marc Lanctot, Nicolas Sonnerat, Joel Z.\n  Leibo, Karl Tuyls, Thore Graepel", "title": "Value-Decomposition Networks For Cooperative Multi-Agent Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the problem of cooperative multi-agent reinforcement learning with a\nsingle joint reward signal. This class of learning problems is difficult\nbecause of the often large combined action and observation spaces. In the fully\ncentralized and decentralized approaches, we find the problem of spurious\nrewards and a phenomenon we call the \"lazy agent\" problem, which arises due to\npartial observability. We address these problems by training individual agents\nwith a novel value decomposition network architecture, which learns to\ndecompose the team value function into agent-wise value functions. We perform\nan experimental evaluation across a range of partially-observable multi-agent\ndomains and show that learning such value-decompositions leads to superior\nresults, in particular when combined with weight sharing, role information and\ninformation channels.\n", "versions": [{"version": "v1", "created": "Fri, 16 Jun 2017 14:47:21 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["Sunehag", "Peter", ""], ["Lever", "Guy", ""], ["Gruslys", "Audrunas", ""], ["Czarnecki", "Wojciech Marian", ""], ["Zambaldi", "Vinicius", ""], ["Jaderberg", "Max", ""], ["Lanctot", "Marc", ""], ["Sonnerat", "Nicolas", ""], ["Leibo", "Joel Z.", ""], ["Tuyls", "Karl", ""], ["Graepel", "Thore", ""]]}, {"id": "1706.05477", "submitter": "Ehsan Abbasnejad M", "authors": "M. Ehsan Abbasnejad, Qinfeng Shi, Iman Abbasnejad, Anton van den\n  Hengel, Anthony Dick", "title": "Bayesian Conditional Generative Adverserial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Traditional GANs use a deterministic generator function (typically a neural\nnetwork) to transform a random noise input $z$ to a sample $\\mathbf{x}$ that\nthe discriminator seeks to distinguish. We propose a new GAN called Bayesian\nConditional Generative Adversarial Networks (BC-GANs) that use a random\ngenerator function to transform a deterministic input $y'$ to a sample\n$\\mathbf{x}$. Our BC-GANs extend traditional GANs to a Bayesian framework, and\nnaturally handle unsupervised learning, supervised learning, and\nsemi-supervised learning problems. Experiments show that the proposed BC-GANs\noutperforms the state-of-the-arts.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jun 2017 05:29:13 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Abbasnejad", "M. Ehsan", ""], ["Shi", "Qinfeng", ""], ["Abbasnejad", "Iman", ""], ["Hengel", "Anton van den", ""], ["Dick", "Anthony", ""]]}, {"id": "1706.05507", "submitter": "Mahesh Chandra Mukkamala", "authors": "Mahesh Chandra Mukkamala, Matthias Hein", "title": "Variants of RMSProp and Adagrad with Logarithmic Regret Bounds", "comments": "ICML 2017, 16 pages, 23 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive gradient methods have become recently very popular, in particular as\nthey have been shown to be useful in the training of deep neural networks. In\nthis paper we have analyzed RMSProp, originally proposed for the training of\ndeep neural networks, in the context of online convex optimization and show\n$\\sqrt{T}$-type regret bounds. Moreover, we propose two variants SC-Adagrad and\nSC-RMSProp for which we show logarithmic regret bounds for strongly convex\nfunctions. Finally, we demonstrate in the experiments that these new variants\noutperform other adaptive gradient techniques or stochastic gradient descent in\nthe optimization of strongly convex functions as well as in training of deep\nneural networks.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jun 2017 09:48:55 GMT"}, {"version": "v2", "created": "Tue, 28 Nov 2017 18:47:37 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Mukkamala", "Mahesh Chandra", ""], ["Hein", "Matthias", ""]]}, {"id": "1706.05518", "submitter": "Jes\\'us Ib\\'a\\~nez Ruiz", "authors": "Jes\\'us Ib\\'a\\~nez-Ruiz, Laura Sebasti\\'a, Eva Onaindia", "title": "Evaluating the quality of tourist agendas customized to different travel\n  styles", "comments": "Twenty-seventh Workshop on Constraint Satisfaction Techniques for\n  Planning and Scheduling Problems (COPLAS'17)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many tourist applications provide a personalized tourist agenda with the list\nof recommended activities to the user. These applications must undoubtedly deal\nwith the constraints and preferences that define the user interests. Among\nthese preferences, we can find those that define the travel style of the user,\nsuch as the rhythm of the trip, the number of visits to include in the tour or\nthe priority to visits of special interest for the user. In this paper, we deal\nwith the task of creating a customized tourist agenda as a planning and\nscheduling application capable of conveniently scheduling the most appropriate\ngoals (visits) so as to maximize the user satisfaction with the tourist route.\nThis paper makes an analysis of the meaning of the travel style preferences and\ncompares the quality of the solutions obtained by two different solvers, a\nPDDL-based planner and a Constraint Satisfaction Problem solver. We also define\nseveral quality metrics and perform extensive experiments in order to evaluate\nthe results obtained with both solvers.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jun 2017 11:59:40 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Ib\u00e1\u00f1ez-Ruiz", "Jes\u00fas", ""], ["Sebasti\u00e1", "Laura", ""], ["Onaindia", "Eva", ""]]}, {"id": "1706.05585", "submitter": "Tom Hope", "authors": "Tom Hope, Joel Chan, Aniket Kittur, Dafna Shahaf", "title": "Accelerating Innovation Through Analogy Mining", "comments": "KDD 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The availability of large idea repositories (e.g., the U.S. patent database)\ncould significantly accelerate innovation and discovery by providing people\nwith inspiration from solutions to analogous problems. However, finding useful\nanalogies in these large, messy, real-world repositories remains a persistent\nchallenge for either human or automated methods. Previous approaches include\ncostly hand-created databases that have high relational structure (e.g.,\npredicate calculus representations) but are very sparse. Simpler\nmachine-learning/information-retrieval similarity metrics can scale to large,\nnatural-language datasets, but struggle to account for structural similarity,\nwhich is central to analogy. In this paper we explore the viability and value\nof learning simpler structural representations, specifically, \"problem\nschemas\", which specify the purpose of a product and the mechanisms by which it\nachieves that purpose. Our approach combines crowdsourcing and recurrent neural\nnetworks to extract purpose and mechanism vector representations from product\ndescriptions. We demonstrate that these learned vectors allow us to find\nanalogies with higher precision and recall than traditional\ninformation-retrieval methods. In an ideation experiment, analogies retrieved\nby our models significantly increased people's likelihood of generating\ncreative ideas compared to analogies retrieved by traditional methods. Our\nresults suggest a promising approach to enabling computational analogy at scale\nis to learn and leverage weaker structural representations.\n", "versions": [{"version": "v1", "created": "Sat, 17 Jun 2017 22:29:37 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Hope", "Tom", ""], ["Chan", "Joel", ""], ["Kittur", "Aniket", ""], ["Shahaf", "Dafna", ""]]}, {"id": "1706.05637", "submitter": "Ofer Strichman", "authors": "Dor Cohen and Ofer Strichman", "title": "The impact of Entropy and Solution Density on selected SAT heuristics", "comments": null, "journal-ref": null, "doi": "10.3390/e20090713", "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In a recent article [Oh'15], Oh examined the impact of various key heuristics\n(e.g., deletion strategy, restart policy, decay factor, database reduction) in\ncompetitive SAT solvers. His key findings are that their expected success\ndepends on whether the input formula is satisfiable or not. To further\ninvestigate these findings, we focused on two properties of satisfiable\nformulas: the entropy of the formula, which approximates the freedom we have in\nassigning the variables, and the solution density, which is the number of\nsolutions divided by the search space. We found that both predict better the\neffect of these heuristics, and that satisfiable formulas with small entropy\n`behave' similarly to unsatisfiable formulas.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jun 2017 12:03:33 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Cohen", "Dor", ""], ["Strichman", "Ofer", ""]]}, {"id": "1706.05643", "submitter": "Vasile Patrascu", "authors": "Vasile Patrascu", "title": "Entropy, neutro-entropy and anti-entropy for neutrosophic information", "comments": "14 pages", "journal-ref": null, "doi": "10.13140/RG.2.2.29001.95848", "report-no": "Technical Report TI.1.6.2017", "categories": "cs.AI cs.IT math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This approach presents a multi-valued representation of the neutrosophic\ninformation. It highlights the link between the bifuzzy information and\nneutrosophic one. The constructed deca-valued structure shows the neutrosophic\ninformation complexity. This deca-valued structure led to construction of two\nnew concepts for the neutrosophic information: neutro-entropy and anti-entropy.\nThese two concepts are added to the two existing: entropy and non-entropy.\nThus, we obtained the following triad: entropy, neutro-entropy and\nanti-entropy.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jun 2017 13:01:59 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Patrascu", "Vasile", ""]]}, {"id": "1706.05733", "submitter": "Georgios Feretzakis", "authors": "Dimitris Kalles, Vassilios S. Verykios, Georgios Feretzakis,\n  Athanasios Papagelis", "title": "Data set operations to hide decision tree rules", "comments": "7 pages, 4 figures and 2 tables. ECAI 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper focuses on preserving the privacy of sensitive patterns when\ninducing decision trees. We adopt a record augmentation approach for hiding\nsensitive classification rules in binary datasets. Such a hiding methodology is\npreferred over other heuristic solutions like output perturbation or\ncryptographic techniques - which restrict the usability of the data - since the\nraw data itself is readily available for public use. We show some key lemmas\nwhich are related to the hiding process and we also demonstrate the methodology\nwith an example and an indicative experiment using a prototype hiding tool.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jun 2017 21:57:36 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Kalles", "Dimitris", ""], ["Verykios", "Vassilios S.", ""], ["Feretzakis", "Georgios", ""], ["Papagelis", "Athanasios", ""]]}, {"id": "1706.05744", "submitter": "Danijar Hafner", "authors": "Danijar Hafner, Alex Irpan, James Davidson, Nicolas Heess", "title": "Learning Hierarchical Information Flow with Recurrent Neural Modules", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose ThalNet, a deep learning model inspired by neocortical\ncommunication via the thalamus. Our model consists of recurrent neural modules\nthat send features through a routing center, endowing the modules with the\nflexibility to share features over multiple time steps. We show that our model\nlearns to route information hierarchically, processing input data by a chain of\nmodules. We observe common architectures, such as feed forward neural networks\nand skip connections, emerging as special cases of our architecture, while\nnovel connectivity patterns are learned for the text8 compression task. Our\nmodel outperforms standard recurrent neural networks on several sequential\nbenchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 18 Jun 2017 23:20:12 GMT"}, {"version": "v2", "created": "Sat, 4 Nov 2017 01:20:06 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Hafner", "Danijar", ""], ["Irpan", "Alex", ""], ["Davidson", "James", ""], ["Heess", "Nicolas", ""]]}, {"id": "1706.05749", "submitter": "Nick Erickson", "authors": "Nick Erickson and Qi Zhao", "title": "Dex: Incremental Learning for Complex Environments in Deep Reinforcement\n  Learning", "comments": "NIPS 2017 submission, 10 pages, 26 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces Dex, a reinforcement learning environment toolkit\nspecialized for training and evaluation of continual learning methods as well\nas general reinforcement learning problems. We also present the novel continual\nlearning method of incremental learning, where a challenging environment is\nsolved using optimal weight initialization learned from first solving a similar\neasier environment. We show that incremental learning can produce vastly\nsuperior results than standard methods by providing a strong baseline method\nacross ten Dex environments. We finally develop a saliency method for\nqualitative analysis of reinforcement learning, which shows the impact\nincremental learning has on network attention.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 00:16:24 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Erickson", "Nick", ""], ["Zhao", "Qi", ""]]}, {"id": "1706.05826", "submitter": "Di Wang", "authors": "Di Wang, Kimon Fountoulakis, Monika Henzinger, Michael W. Mahoney,\n  Satish Rao", "title": "Capacity Releasing Diffusion for Speed and Locality", "comments": "Appeared in ICML 2017. Current version added reference and discussion\n  of work on generalized Cheeger's inequalities", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Diffusions and related random walk procedures are of central importance in\nmany areas of machine learning, data analysis, and applied mathematics. Because\nthey spread mass agnostically at each step in an iterative manner, they can\nsometimes spread mass \"too aggressively,\" thereby failing to find the \"right\"\nclusters. We introduce a novel Capacity Releasing Diffusion (CRD) Process,\nwhich is both faster and stays more local than the classical spectral diffusion\nprocess. As an application, we use our CRD Process to develop an improved local\nalgorithm for graph clustering. Our local graph clustering method can find\nlocal clusters in a model of clustering where one begins the CRD Process in a\ncluster whose vertices are connected better internally than externally by an\n$O(\\log^2 n)$ factor, where $n$ is the number of nodes in the cluster. Thus,\nour CRD Process is the first local graph clustering algorithm that is not\nsubject to the well-known quadratic Cheeger barrier. Our result requires a\ncertain smoothness condition, which we expect to be an artifact of our\nanalysis. Our empirical evaluation demonstrates improved results, in particular\nfor realistic social graphs where there are moderately good---but not very\ngood---clusters.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 08:18:04 GMT"}, {"version": "v2", "created": "Sun, 10 Jun 2018 08:58:07 GMT"}], "update_date": "2018-06-12", "authors_parsed": [["Wang", "Di", ""], ["Fountoulakis", "Kimon", ""], ["Henzinger", "Monika", ""], ["Mahoney", "Michael W.", ""], ["Rao", "Satish", ""]]}, {"id": "1706.05928", "submitter": "Carlos M. Ala\\'iz", "authors": "Carlos M. Ala\\'iz and Johan A. K. Suykens", "title": "Modified Frank-Wolfe Algorithm for Enhanced Sparsity in Support Vector\n  Machine Classifiers", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work proposes a new algorithm for training a re-weighted L2 Support\nVector Machine (SVM), inspired on the re-weighted Lasso algorithm of Cand\\`es\net al. and on the equivalence between Lasso and SVM shown recently by Jaggi. In\nparticular, the margin required for each training vector is set independently,\ndefining a new weighted SVM model. These weights are selected to be binary, and\nthey are automatically adapted during the training of the model, resulting in a\nvariation of the Frank-Wolfe optimization algorithm with essentially the same\ncomputational complexity as the original algorithm. As shown experimentally,\nthis algorithm is computationally cheaper to apply since it requires less\niterations to converge, and it produces models with a sparser representation in\nterms of support vectors and which are more stable with respect to the\nselection of the regularization hyper-parameter.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 13:31:09 GMT"}, {"version": "v2", "created": "Fri, 13 Apr 2018 13:28:15 GMT"}], "update_date": "2018-04-16", "authors_parsed": [["Ala\u00edz", "Carlos M.", ""], ["Suykens", "Johan A. K.", ""]]}, {"id": "1706.06051", "submitter": "Hanan Rosemarin", "authors": "Hanan Rosemarin and John P. Dickerson and Sarit Kraus", "title": "Learning to Schedule Deadline- and Operator-Sensitive Tasks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of semi-autonomous and autonomous robotic assistants to aid in care\nof the elderly is expected to ease the burden on human caretakers, with\nsmall-stage testing already occurring in a variety of countries. Yet, it is\nlikely that these robots will need to request human assistance via\nteleoperation when domain expertise is needed for a specific task. As\ndeployment of robotic assistants moves to scale, mapping these requests for\nhuman aid to the teleoperators themselves will be a difficult online\noptimization problem. In this paper, we design a system that allocates requests\nto a limited number of teleoperators, each with different specialities, in an\nonline fashion. We generalize a recent model of online job scheduling with a\nworst-case competitive-ratio bound to our setting. Next, we design a scalable\nmachine-learning-based teleoperator-aware task scheduling algorithm and show,\nexperimentally, that it performs well when compared to an omniscient optimal\nscheduling algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 16:42:23 GMT"}], "update_date": "2017-06-20", "authors_parsed": [["Rosemarin", "Hanan", ""], ["Dickerson", "John P.", ""], ["Kraus", "Sarit", ""]]}, {"id": "1706.06060", "submitter": "Scott Lundberg", "authors": "Scott M. Lundberg and Su-In Lee", "title": "Consistent feature attribution for tree ensembles", "comments": "presented at 2017 ICML Workshop on Human Interpretability in Machine\n  Learning (WHI 2017), Sydney, NSW, Australia", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Note that a newer expanded version of this paper is now available at:\narXiv:1802.03888\n  It is critical in many applications to understand what features are important\nfor a model, and why individual predictions were made. For tree ensemble\nmethods these questions are usually answered by attributing importance values\nto input features, either globally or for a single prediction. Here we show\nthat current feature attribution methods are inconsistent, which means changing\nthe model to rely more on a given feature can actually decrease the importance\nassigned to that feature. To address this problem we develop fast exact\nsolutions for SHAP (SHapley Additive exPlanation) values, which were recently\nshown to be the unique additive feature attribution method based on conditional\nexpectations that is both consistent and locally accurate. We integrate these\nimprovements into the latest version of XGBoost, demonstrate the\ninconsistencies of current methods, and show how using SHAP values results in\nsignificantly improved supervised clustering performance. Feature importance\nvalues are a key part of understanding widely used models such as gradient\nboosting trees and random forests, so improvements to them have broad practical\nimplications.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 17:03:46 GMT"}, {"version": "v2", "created": "Mon, 31 Jul 2017 19:28:56 GMT"}, {"version": "v3", "created": "Thu, 3 Aug 2017 21:33:01 GMT"}, {"version": "v4", "created": "Tue, 8 Aug 2017 01:18:43 GMT"}, {"version": "v5", "created": "Sun, 11 Feb 2018 21:44:49 GMT"}, {"version": "v6", "created": "Sat, 17 Feb 2018 01:11:50 GMT"}], "update_date": "2018-02-20", "authors_parsed": [["Lundberg", "Scott M.", ""], ["Lee", "Su-In", ""]]}, {"id": "1706.06120", "submitter": "Junming Yin", "authors": "Xuan Wei, Daniel Dajun Zeng, Junming Yin", "title": "Multi-Label Annotation Aggregation in Crowdsourcing", "comments": "The paper needs more refinement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  As a means of human-based computation, crowdsourcing has been widely used to\nannotate large-scale unlabeled datasets. One of the obvious challenges is how\nto aggregate these possibly noisy labels provided by a set of heterogeneous\nannotators. Another challenge stems from the difficulty in evaluating the\nannotator reliability without even knowing the ground truth, which can be used\nto build incentive mechanisms in crowdsourcing platforms. When each instance is\nassociated with many possible labels simultaneously, the problem becomes even\nharder because of its combinatorial nature. In this paper, we present new\nflexible Bayesian models and efficient inference algorithms for multi-label\nannotation aggregation by taking both annotator reliability and label\ndependency into account. Extensive experiments on real-world datasets confirm\nthat the proposed methods outperform other competitive alternatives, and the\nmodel can recover the type of the annotators with high accuracy.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 18:03:15 GMT"}, {"version": "v2", "created": "Sun, 18 Oct 2020 03:14:53 GMT"}], "update_date": "2020-10-20", "authors_parsed": [["Wei", "Xuan", ""], ["Zeng", "Daniel Dajun", ""], ["Yin", "Junming", ""]]}, {"id": "1706.06122", "submitter": "Yedid Hoshen", "authors": "Yedid Hoshen", "title": "VAIN: Attentional Multi-agent Predictive Modeling", "comments": "NIPS 2017 Wrong sign fixed in Eqs:3-5", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi-agent predictive modeling is an essential step for understanding\nphysical, social and team-play systems. Recently, Interaction Networks (INs)\nwere proposed for the task of modeling multi-agent physical systems, INs scale\nwith the number of interactions in the system (typically quadratic or higher\norder in the number of agents). In this paper we introduce VAIN, a novel\nattentional architecture for multi-agent predictive modeling that scales\nlinearly with the number of agents. We show that VAIN is effective for\nmulti-agent predictive modeling. Our method is evaluated on tasks from\nchallenging multi-agent prediction domains: chess and soccer, and outperforms\ncompeting multi-agent approaches.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 18:09:25 GMT"}, {"version": "v2", "created": "Fri, 28 Sep 2018 12:15:44 GMT"}], "update_date": "2018-10-01", "authors_parsed": [["Hoshen", "Yedid", ""]]}, {"id": "1706.06133", "submitter": "Nicholas Cheney", "authors": "Nick Cheney, Josh Bongard, Vytas SunSpiral, Hod Lipson", "title": "Scalable Co-Optimization of Morphology and Control in Embodied Machines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Evolution sculpts both the body plans and nervous systems of agents together\nover time. In contrast, in AI and robotics, a robot's body plan is usually\ndesigned by hand, and control policies are then optimized for that fixed\ndesign. The task of simultaneously co-optimizing the morphology and controller\nof an embodied robot has remained a challenge. In psychology, the theory of\nembodied cognition posits that behavior arises from a close coupling between\nbody plan and sensorimotor control, which suggests why co-optimizing these two\nsubsystems is so difficult: most evolutionary changes to morphology tend to\nadversely impact sensorimotor control, leading to an overall decrease in\nbehavioral performance. Here, we further examine this hypothesis and\ndemonstrate a technique for \"morphological innovation protection\", which\ntemporarily reduces selection pressure on recently morphologically-changed\nindividuals, thus enabling evolution some time to \"readapt\" to the new\nmorphology with subsequent control policy mutations. We show the potential for\nthis method to avoid local optima and converge to similar highly fit\nmorphologies across widely varying initial conditions, while sustaining fitness\nimprovements further into optimization. While this technique is admittedly only\nthe first of many steps that must be taken to achieve scalable optimization of\nembodied machines, we hope that theoretical insight into the cause of\nevolutionary stagnation in current methods will help to enable the automation\nof robot design and behavioral training -- while simultaneously providing a\ntestbed to investigate the theory of embodied cognition.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 18:47:57 GMT"}, {"version": "v2", "created": "Tue, 12 Dec 2017 20:10:09 GMT"}], "update_date": "2017-12-14", "authors_parsed": [["Cheney", "Nick", ""], ["Bongard", "Josh", ""], ["SunSpiral", "Vytas", ""], ["Lipson", "Hod", ""]]}, {"id": "1706.06160", "submitter": "Arjun Bhardwaj", "authors": "Arjun Bhardwaj, Alexander Rudnicky", "title": "User Intent Classification using Memory Networks: A Comparative Analysis\n  for a Limited Data Scenario", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this report, we provide a comparative analysis of different techniques for\nuser intent classification towards the task of app recommendation. We analyse\nthe performance of different models and architectures for multi-label\nclassification over a dataset with a relative large number of classes and only\na handful examples of each class. We focus, in particular, on memory network\narchitectures, and compare how well the different versions perform under the\ntask constraints. Since the classifier is meant to serve as a module in a\npractical dialog system, it needs to be able to work with limited training data\nand incorporate new data on the fly. We devise a 1-shot learning task to test\nthe models under the above constraint. We conclude that relatively simple\nversions of memory networks perform better than other approaches. Although, for\ntasks with very limited data, simple non-parametric methods perform comparably,\nwithout needing the extra training data.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 20:12:07 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Bhardwaj", "Arjun", ""], ["Rudnicky", "Alexander", ""]]}, {"id": "1706.06197", "submitter": "Xu Sun", "authors": "Xu Sun, Xuancheng Ren, Shuming Ma, Houfeng Wang", "title": "meProp: Sparsified Back Propagation for Accelerated Deep Learning with\n  Reduced Overfitting", "comments": "Accepted by the 34th International Conference on Machine Learning\n  (ICML 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple yet effective technique for neural network learning. The\nforward propagation is computed as usual. In back propagation, only a small\nsubset of the full gradient is computed to update the model parameters. The\ngradient vectors are sparsified in such a way that only the top-$k$ elements\n(in terms of magnitude) are kept. As a result, only $k$ rows or columns\n(depending on the layout) of the weight matrix are modified, leading to a\nlinear reduction ($k$ divided by the vector dimension) in the computational\ncost. Surprisingly, experimental results demonstrate that we can update only\n1-4% of the weights at each back propagation pass. This does not result in a\nlarger number of training iterations. More interestingly, the accuracy of the\nresulting models is actually improved rather than degraded, and a detailed\nanalysis is given. The code is available at https://github.com/lancopku/meProp\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 22:36:33 GMT"}, {"version": "v2", "created": "Wed, 5 Jul 2017 01:34:50 GMT"}, {"version": "v3", "created": "Mon, 30 Oct 2017 09:48:41 GMT"}, {"version": "v4", "created": "Tue, 31 Oct 2017 02:04:52 GMT"}, {"version": "v5", "created": "Mon, 11 Mar 2019 02:57:03 GMT"}], "update_date": "2019-03-12", "authors_parsed": [["Sun", "Xu", ""], ["Ren", "Xuancheng", ""], ["Ma", "Shuming", ""], ["Wang", "Houfeng", ""]]}, {"id": "1706.06210", "submitter": "Pawe{\\l} Budzianowski", "authors": "Pawe{\\l} Budzianowski, Stefan Ultes, Pei-Hao Su, Nikola Mrk\\v{s}i\\'c,\n  Tsung-Hsien Wen, I\\~nigo Casanueva, Lina Rojas-Barahona, Milica Ga\\v{s}i\\'c", "title": "Sub-domain Modelling for Dialogue Management with Hierarchical\n  Reinforcement Learning", "comments": "Update of the section 4 and the bibliography", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Human conversation is inherently complex, often spanning many different\ntopics/domains. This makes policy learning for dialogue systems very\nchallenging. Standard flat reinforcement learning methods do not provide an\nefficient framework for modelling such dialogues. In this paper, we focus on\nthe under-explored problem of multi-domain dialogue management. First, we\npropose a new method for hierarchical reinforcement learning using the option\nframework. Next, we show that the proposed architecture learns faster and\narrives at a better policy than the existing flat ones do. Moreover, we show\nhow pretrained policies can be adapted to more complex systems with an\nadditional set of new actions. In doing that, we show that our approach has the\npotential to facilitate policy optimisation for more sophisticated multi-domain\ndialogue systems.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 23:15:22 GMT"}, {"version": "v2", "created": "Mon, 17 Jul 2017 13:01:09 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Budzianowski", "Pawe\u0142", ""], ["Ultes", "Stefan", ""], ["Su", "Pei-Hao", ""], ["Mrk\u0161i\u0107", "Nikola", ""], ["Wen", "Tsung-Hsien", ""], ["Casanueva", "I\u00f1igo", ""], ["Rojas-Barahona", "Lina", ""], ["Ga\u0161i\u0107", "Milica", ""]]}, {"id": "1706.06216", "submitter": "Yujia Li", "authors": "Yujia Li, Alexander Schwing, Kuan-Chieh Wang, Richard Zemel", "title": "Dualing GANs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial nets (GANs) are a promising technique for modeling a\ndistribution from samples. It is however well known that GAN training suffers\nfrom instability due to the nature of its maximin formulation. In this paper,\nwe explore ways to tackle the instability problem by dualizing the\ndiscriminator. We start from linear discriminators in which case conjugate\nduality provides a mechanism to reformulate the saddle point objective into a\nmaximization problem, such that both the generator and the discriminator of\nthis 'dualing GAN' act in concert. We then demonstrate how to extend this\nintuition to non-linear formulations. For GANs with linear discriminators our\napproach is able to remove the instability in training, while for GANs with\nnonlinear discriminators our approach provides an alternative to the commonly\nused GAN training algorithm.\n", "versions": [{"version": "v1", "created": "Mon, 19 Jun 2017 23:28:49 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Li", "Yujia", ""], ["Schwing", "Alexander", ""], ["Wang", "Kuan-Chieh", ""], ["Zemel", "Richard", ""]]}, {"id": "1706.06243", "submitter": "Luke Miles", "authors": "Cory Siler, Luke Harold Miles, Judy Goldsmith", "title": "The Complexity of Campaigning", "comments": "Will be presented at the 2017 Algorithmic Decision Theory Conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In \"The Logic of Campaigning\", Dean and Parikh consider a candidate making\ncampaign statements to appeal to the voters. They model these statements as\nBoolean formulas over variables that represent stances on the issues, and study\noptimal candidate strategies under three proposed models of voter preferences\nbased on the assignments that satisfy these formulas. We prove that voter\nutility evaluation is computationally hard under these preference models (in\none case, #P-hard), along with certain problems related to candidate strategic\nreasoning. Our results raise questions about the desirable characteristics of a\nvoter preference model and to what extent a polynomial-time-evaluable function\ncan capture them.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 02:28:04 GMT"}, {"version": "v2", "created": "Mon, 17 Jul 2017 21:07:09 GMT"}], "update_date": "2017-07-19", "authors_parsed": [["Siler", "Cory", ""], ["Miles", "Luke Harold", ""], ["Goldsmith", "Judy", ""]]}, {"id": "1706.06328", "submitter": "Reuth Mirsky", "authors": "Reuth Mirsky, Ya'akov Gal, David Tolpin", "title": "Session Analysis using Plan Recognition", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents preliminary results of our work with a major financial\ncompany, where we try to use methods of plan recognition in order to\ninvestigate the interactions of a costumer with the company's online interface.\nIn this paper, we present the first steps of integrating a plan recognition\nalgorithm in a real-world application for detecting and analyzing the\ninteractions of a costumer. It uses a novel approach for plan recognition from\nbare-bone UI data, which reasons about the plan library at the lowest\nrecognition level in order to define the relevancy of actions in our domain,\nand then uses it to perform plan recognition.\n  We present preliminary results of inference on three different use-cases\nmodeled by domain experts from the company, and show that this approach manages\nto decrease the overload of information required from an analyst to evaluate a\ncostumer's session - whether this is a malicious or benign session, whether the\nintended tasks were completed, and if not - what actions are expected next.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 09:03:53 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Mirsky", "Reuth", ""], ["Gal", "Ya'akov", ""], ["Tolpin", "David", ""]]}, {"id": "1706.06366", "submitter": "Lucas Bechberger", "authors": "Lucas Bechberger and Kai-Uwe K\\\"uhnberger", "title": "A Thorough Formalization of Conceptual Spaces", "comments": "accepted at KI 2017 (http://ki2017.tu-dortmund.de/), final\n  publication is available at Springer via\n  http://dx.doi.org/10.1007/978-3-319-67190-1_5", "journal-ref": null, "doi": "10.1007/978-3-319-67190-1_5", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The highly influential framework of conceptual spaces provides a geometric\nway of representing knowledge. Instances are represented by points in a\nhigh-dimensional space and concepts are represented by convex regions in this\nspace. After pointing out a problem with the convexity requirement, we propose\na formalization of conceptual spaces based on fuzzy star-shaped sets. Our\nformalization uses a parametric definition of concepts and extends the original\nframework by adding means to represent correlations between different domains\nin a geometric way. Moreover, we define computationally efficient operations on\nconcepts (intersection, union, and projection onto a subspace) and show that\nthese operations can support both learning and reasoning processes.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 11:19:28 GMT"}, {"version": "v2", "created": "Thu, 21 Sep 2017 07:48:49 GMT"}], "update_date": "2017-09-22", "authors_parsed": [["Bechberger", "Lucas", ""], ["K\u00fchnberger", "Kai-Uwe", ""]]}, {"id": "1706.06383", "submitter": "Misha Denil", "authors": "Misha Denil, Sergio G\\'omez Colmenarejo, Serkan Cabi, David Saxton,\n  Nando de Freitas", "title": "Programmable Agents", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We build deep RL agents that execute declarative programs expressed in formal\nlanguage. The agents learn to ground the terms in this language in their\nenvironment, and can generalize their behavior at test time to execute new\nprograms that refer to objects that were not referenced during training. The\nagents develop disentangled interpretable representations that allow them to\ngeneralize to a wide variety of zero-shot semantic tasks.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 12:19:34 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Denil", "Misha", ""], ["Colmenarejo", "Sergio G\u00f3mez", ""], ["Cabi", "Serkan", ""], ["Saxton", "David", ""], ["de Freitas", "Nando", ""]]}, {"id": "1706.06462", "submitter": "Kohei Suenaga", "authors": "Taro Sekiyama, Akifumi Imanishi, and Kohei Suenaga", "title": "Towards Proof Synthesis Guided by Neural Machine Translation for\n  Intuitionistic Propositional Logic", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.PL cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inspired by the recent evolution of deep neural networks (DNNs) in machine\nlearning, we explore their application to PL-related topics. This paper is the\nfirst step towards this goal; we propose a proof-synthesis method for the\nnegation-free propositional logic in which we use a DNN to obtain a guide of\nproof search. The idea is to view the proof-synthesis problem as a translation\nfrom a proposition to its proof. We train seq2seq, which is a popular network\nin neural machine translation, so that it generates a proof encoded as a\n$\\lambda$-term of a given proposition. We implement the whole framework and\nempirically observe that a generated proof term is close to a correct proof in\nterms of the tree edit distance of AST. This observation justifies using the\noutput from a trained seq2seq model as a guide for proof search.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 14:22:29 GMT"}], "update_date": "2017-06-21", "authors_parsed": [["Sekiyama", "Taro", ""], ["Imanishi", "Akifumi", ""], ["Suenaga", "Kohei", ""]]}, {"id": "1706.06511", "submitter": "Nathaniel Rodriguez", "authors": "Nathaniel Rodriguez, Eduardo Izquierdo, Yong-Yeol Ahn", "title": "Optimal modularity and memory capacity of neural reservoirs", "comments": null, "journal-ref": null, "doi": "10.1162/netn_a_00082", "report-no": null, "categories": "cs.NE cs.AI physics.bio-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The neural network is a powerful computing framework that has been exploited\nby biological evolution and by humans for solving diverse problems. Although\nthe computational capabilities of neural networks are determined by their\nstructure, the current understanding of the relationships between a neural\nnetwork's architecture and function is still primitive. Here we reveal that\nneural network's modular architecture plays a vital role in determining the\nneural dynamics and memory performance of the network of threshold neurons. In\nparticular, we demonstrate that there exists an optimal modularity for memory\nperformance, where a balance between local cohesion and global connectivity is\nestablished, allowing optimally modular networks to remember longer. Our\nresults suggest that insights from dynamical analysis of neural networks and\ninformation spreading processes can be leveraged to better design neural\nnetworks and may shed light on the brain's modular organization.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 15:17:37 GMT"}, {"version": "v2", "created": "Mon, 2 Oct 2017 22:45:53 GMT"}, {"version": "v3", "created": "Tue, 26 Mar 2019 01:57:17 GMT"}], "update_date": "2019-03-27", "authors_parsed": [["Rodriguez", "Nathaniel", ""], ["Izquierdo", "Eduardo", ""], ["Ahn", "Yong-Yeol", ""]]}, {"id": "1706.06525", "submitter": "Hamid Eghbal-Zadeh", "authors": "Hamid Eghbal-zadeh, Bernhard Lehner, Matthias Dorfer, Gerhard Widmer", "title": "A Hybrid Approach with Multi-channel I-Vectors and Convolutional Neural\n  Networks for Acoustic Scene Classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Acoustic Scene Classification (ASC) two major approaches have been\nfollowed . While one utilizes engineered features such as\nmel-frequency-cepstral-coefficients (MFCCs), the other uses learned features\nthat are the outcome of an optimization algorithm. I-vectors are the result of\na modeling technique that usually takes engineered features as input. It has\nbeen shown that standard MFCCs extracted from monaural audio signals lead to\ni-vectors that exhibit poor performance, especially on indoor acoustic scenes.\nAt the same time, Convolutional Neural Networks (CNNs) are well known for their\nability to learn features by optimizing their filters. They have been applied\non ASC and have shown promising results. In this paper, we first propose a\nnovel multi-channel i-vector extraction and scoring scheme for ASC, improving\ntheir performance on indoor and outdoor scenes. Second, we propose a CNN\narchitecture that achieves promising ASC results. Further, we show that\ni-vectors and CNNs capture complementary information from acoustic scenes.\nFinally, we propose a hybrid system for ASC using multi-channel i-vectors and\nCNNs by utilizing a score fusion technique. Using our method, we participated\nin the ASC task of the DCASE-2016 challenge. Our hybrid approach achieved 1 st\nrank among 49 submissions, substantially improving the previous state of the\nart.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 15:44:09 GMT"}], "update_date": "2017-11-15", "authors_parsed": [["Eghbal-zadeh", "Hamid", ""], ["Lehner", "Bernhard", ""], ["Dorfer", "Matthias", ""], ["Widmer", "Gerhard", ""]]}, {"id": "1706.06544", "submitter": "Taylor Killian", "authors": "Taylor Killian, Samuel Daulton, George Konidaris, Finale Doshi-Velez", "title": "Robust and Efficient Transfer Learning with Hidden-Parameter Markov\n  Decision Processes", "comments": "To appear at NIPS 2017, selected for an oral presentation. 17 pages\n  (incl references and appendix). Example code can be found at\n  http://github.com/dtak/hip-mdp-public", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new formulation of the Hidden Parameter Markov Decision\nProcess (HiP-MDP), a framework for modeling families of related tasks using\nlow-dimensional latent embeddings. Our new framework correctly models the joint\nuncertainty in the latent parameters and the state space. We also replace the\noriginal Gaussian Process-based model with a Bayesian Neural Network, enabling\nmore scalable inference. Thus, we expand the scope of the HiP-MDP to\napplications with higher dimensions and more complex dynamics.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 16:51:46 GMT"}, {"version": "v2", "created": "Tue, 24 Oct 2017 01:35:08 GMT"}, {"version": "v3", "created": "Tue, 31 Oct 2017 02:50:56 GMT"}], "update_date": "2017-11-01", "authors_parsed": [["Killian", "Taylor", ""], ["Daulton", "Samuel", ""], ["Konidaris", "George", ""], ["Doshi-Velez", "Finale", ""]]}, {"id": "1706.06617", "submitter": "Olivier Pietquin", "authors": "Diana Borsa, Bilal Piot, R\\'emi Munos and Olivier Pietquin", "title": "Observational Learning by Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Observational learning is a type of learning that occurs as a function of\nobserving, retaining and possibly replicating or imitating the behaviour of\nanother agent. It is a core mechanism appearing in various instances of social\nlearning and has been found to be employed in several intelligent species,\nincluding humans. In this paper, we investigate to what extent the explicit\nmodelling of other agents is necessary to achieve observational learning\nthrough machine learning. Especially, we argue that observational learning can\nemerge from pure Reinforcement Learning (RL), potentially coupled with memory.\nThrough simple scenarios, we demonstrate that an RL agent can leverage the\ninformation provided by the observations of an other agent performing a task in\na shared environment. The other agent is only observed through the effect of\nits actions on the environment and never explicitly modeled. Two key aspects\nare borrowed from observational learning: i) the observer behaviour needs to\nchange as a result of viewing a 'teacher' (another agent) and ii) the observer\nneeds to be motivated somehow to engage in making use of the other agent's\nbehaviour. The later is naturally modeled by RL, by correlating the learning\nagent's reward with the teacher agent's behaviour.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 18:44:49 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Borsa", "Diana", ""], ["Piot", "Bilal", ""], ["Munos", "R\u00e9mi", ""], ["Pietquin", "Olivier", ""]]}, {"id": "1706.06636", "submitter": "Chenyan Xiong", "authors": "Chenyan Xiong, Jamie Callan, and Tie-Yan Liu", "title": "Word-Entity Duet Representations for Document Ranking", "comments": null, "journal-ref": "SIGIR 2017", "doi": "10.1145/3077136.3080768", "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a word-entity duet framework for utilizing knowledge\nbases in ad-hoc retrieval. In this work, the query and documents are modeled by\nword-based representations and entity-based representations. Ranking features\nare generated by the interactions between the two representations,\nincorporating information from the word space, the entity space, and the\ncross-space connections through the knowledge graph. To handle the\nuncertainties from the automatically constructed entity representations, an\nattention-based ranking model AttR-Duet is developed. With back-propagation\nfrom ranking labels, the model learns simultaneously how to demote noisy\nentities and how to rank documents with the word-entity duet. Evaluation\nresults on TREC Web Track ad-hoc task demonstrate that all of the four-way\ninteractions in the duet are useful, the attention mechanism successfully\nsteers the model away from noisy entities, and together they significantly\noutperform both word-based and entity-based learning to rank systems.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 19:23:07 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Xiong", "Chenyan", ""], ["Callan", "Jamie", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1706.06643", "submitter": "Philip Thomas", "authors": "Philip S. Thomas and Emma Brunskill", "title": "Policy Gradient Methods for Reinforcement Learning with Function\n  Approximation and Action-Dependent Baselines", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how an action-dependent baseline can be used by the policy gradient\ntheorem using function approximation, originally presented with\naction-independent baselines by (Sutton et al. 2000).\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 19:47:44 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Thomas", "Philip S.", ""], ["Brunskill", "Emma", ""]]}, {"id": "1706.06689", "submitter": "Garrett Goh", "authors": "Garrett B. Goh, Charles Siegel, Abhinav Vishnu, Nathan O. Hodas,\n  Nathan Baker", "title": "Chemception: A Deep Neural Network with Minimal Chemistry Knowledge\n  Matches the Performance of Expert-developed QSAR/QSPR Models", "comments": "Submitted to a chemistry peer-reviewed journal", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CE cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the last few years, we have seen the transformative impact of deep\nlearning in many applications, particularly in speech recognition and computer\nvision. Inspired by Google's Inception-ResNet deep convolutional neural network\n(CNN) for image classification, we have developed \"Chemception\", a deep CNN for\nthe prediction of chemical properties, using just the images of 2D drawings of\nmolecules. We develop Chemception without providing any additional explicit\nchemistry knowledge, such as basic concepts like periodicity, or advanced\nfeatures like molecular descriptors and fingerprints. We then show how\nChemception can serve as a general-purpose neural network architecture for\npredicting toxicity, activity, and solvation properties when trained on a\nmodest database of 600 to 40,000 compounds. When compared to multi-layer\nperceptron (MLP) deep neural networks trained with ECFP fingerprints,\nChemception slightly outperforms in activity and solvation prediction and\nslightly underperforms in toxicity prediction. Having matched the performance\nof expert-developed QSAR/QSPR deep learning models, our work demonstrates the\nplausibility of using deep neural networks to assist in computational chemistry\nresearch, where the feature engineering process is performed primarily by a\ndeep learning algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 22:25:57 GMT"}], "update_date": "2018-08-15", "authors_parsed": [["Goh", "Garrett B.", ""], ["Siegel", "Charles", ""], ["Vishnu", "Abhinav", ""], ["Hodas", "Nathan O.", ""], ["Baker", "Nathan", ""]]}, {"id": "1706.06695", "submitter": "Kenzo Lobos", "authors": "Kenzo Lobos-Tsunekawa, David L. Leottau and Javier Ruiz-del-Solar", "title": "Toward Real-Time Decentralized Reinforcement Learning using Finite\n  Support Basis Functions", "comments": "Accepted in the RoboCup Symposium 2017. Final version will be\n  published at Springer", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the design and implementation of complex Reinforcement\nLearning (RL) behaviors where multi-dimensional action spaces are involved, as\nwell as the need to execute the behaviors in real-time using robotic platforms\nwith limited computational resources and training times. For this purpose, we\npropose the use of decentralized RL, in combination with finite support basis\nfunctions as alternatives to Gaussian RBF, in order to alleviate the effects of\nthe curse of dimensionality on the action and state spaces respectively, and to\nreduce the computation time. As testbed, a RL based controller for the in-walk\nkick in NAO robots, a challenging and critical problem for soccer robotics, is\nused. The reported experiments show empirically that our solution saves up to\n99.94% of execution time and 98.82% of memory consumption during execution,\nwithout diminishing performance compared to classical approaches.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 22:52:10 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Lobos-Tsunekawa", "Kenzo", ""], ["Leottau", "David L.", ""], ["Ruiz-del-Solar", "Javier", ""]]}, {"id": "1706.06783", "submitter": "Sina Sajadmanesh", "authors": "Sina Sajadmanesh, Jiawei Zhang, Hamid R. Rabiee", "title": "NPGLM: A Non-Parametric Method for Temporal Link Prediction", "comments": "7 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we try to solve the problem of temporal link prediction in\ninformation networks. This implies predicting the time it takes for a link to\nappear in the future, given its features that have been extracted at the\ncurrent network snapshot. To this end, we introduce a probabilistic\nnon-parametric approach, called \"Non-Parametric Generalized Linear Model\"\n(NP-GLM), which infers the hidden underlying probability distribution of the\nlink advent time given its features. We then present a learning algorithm for\nNP-GLM and an inference method to answer time-related queries. Extensive\nexperiments conducted on both synthetic data and real-world Sina Weibo social\nnetwork demonstrate the effectiveness of NP-GLM in solving temporal link\nprediction problem vis-a-vis competitive baselines.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 08:16:47 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Sajadmanesh", "Sina", ""], ["Zhang", "Jiawei", ""], ["Rabiee", "Hamid R.", ""]]}, {"id": "1706.06827", "submitter": "Ari Weinstein", "authors": "Ari Weinstein and Matthew M. Botvinick", "title": "Structure Learning in Motor Control:A Deep Reinforcement Learning Model", "comments": "39th Annual Meeting of the Cognitive Science Society, to appear", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Motor adaptation displays a structure-learning effect: adaptation to a new\nperturbation occurs more quickly when the subject has prior exposure to\nperturbations with related structure. Although this `learning-to-learn' effect\nis well documented, its underlying computational mechanisms are poorly\nunderstood. We present a new model of motor structure learning, approaching it\nfrom the point of view of deep reinforcement learning. Previous work outside of\nmotor control has shown how recurrent neural networks can account for\nlearning-to-learn effects. We leverage this insight to address motor learning,\nby importing it into the setting of model-based reinforcement learning. We\napply the resulting processing architecture to empirical findings from a\nlandmark study of structure learning in target-directed reaching (Braun et al.,\n2009), and discuss its implications for a wider range of learning-to-learn\nphenomena.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 11:20:43 GMT"}, {"version": "v2", "created": "Thu, 13 Jul 2017 14:31:27 GMT"}], "update_date": "2017-07-14", "authors_parsed": [["Weinstein", "Ari", ""], ["Botvinick", "Matthew M.", ""]]}, {"id": "1706.06906", "submitter": "Toby Walsh", "authors": "Toby Walsh", "title": "Expert and Non-Expert Opinion about Technological Unemployment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is significant concern that technological advances, especially in\nRobotics and Artificial Intelligence (AI), could lead to high levels of\nunemployment in the coming decades. Studies have estimated that around half of\nall current jobs are at risk of automation. To look into this issue in more\ndepth, we surveyed experts in Robotics and AI about the risk, and compared\ntheir views with those of non-experts. Whilst the experts predicted a\nsignificant number of occupations were at risk of automation in the next two\ndecades, they were more cautious than people outside the field in predicting\noccupations at risk. Their predictions were consistent with their estimates for\nwhen computers might be expected to reach human level performance across a wide\nrange of skills. These estimates were typically decades later than those of the\nnon-experts. Technological barriers may therefore provide society with more\ntime to prepare for an automated future than the public fear. In addition,\npublic expectations may need to be dampened about the speed of progress to be\nexpected in Robotics and AI.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 13:51:57 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Walsh", "Toby", ""]]}, {"id": "1706.06927", "submitter": "Jonathan Ferrer-Mestres", "authors": "Jonathan Ferrer-Mestres, Guillem Franc\\`es and Hector Geffner", "title": "Combined Task and Motion Planning as Classical AI Planning", "comments": "10 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Planning in robotics is often split into task and motion planning. The\nhigh-level, symbolic task planner decides what needs to be done, while the\nmotion planner checks feasibility and fills up geometric detail. It is known\nhowever that such a decomposition is not effective in general as the symbolic\nand geometrical components are not independent. In this work, we show that it\nis possible to compile task and motion planning problems into classical AI\nplanning problems; i.e., planning problems over finite and discrete state\nspaces with a known initial state, deterministic actions, and goal states to be\nreached. The compilation is sound, meaning that classical plans are valid robot\nplans, and probabilistically complete, meaning that valid robot plans are\nclassical plans when a sufficient number of configurations is sampled. In this\napproach, motion planners and collision checkers are used for the compilation,\nbut not at planning time. The key elements that make the approach effective are\n1) expressive classical AI planning languages for representing the compiled\nproblems in compact form, that unlike PDDL make use of functions and state\nconstraints, and 2) general width-based search algorithms capable of finding\nplans over huge combinatorial spaces using weak heuristics only. Empirical\nresults are presented for a PR2 robot manipulating tens of objects, for which\nlong plans are required.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 14:24:54 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Ferrer-Mestres", "Jonathan", ""], ["Franc\u00e8s", "Guillem", ""], ["Geffner", "Hector", ""]]}, {"id": "1706.06952", "submitter": "Philip Rodgers", "authors": "Philip Rodgers, John Levine", "title": "Ensemble Framework for Real-time Decision Making", "comments": "7 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a new framework for real-time decision making in video\ngames. An Ensemble agent is a compound agent composed of multiple agents, each\nwith its own tasks or goals to achieve. Usually when dealing with real-time\ndecision making, reactive agents are used; that is agents that return a\ndecision based on the current state. While reactive agents are very fast, most\ngames require more than just a rule-based agent to achieve good results.\nDeliberative agents---agents that use a forward model to search future\nstates---are very useful in games with no hard time limit, such as Go or\nBackgammon, but generally take too long for real-time games. The Ensemble\nframework addresses this issue by allowing the agent to be both deliberative\nand reactive at the same time. This is achieved by breaking up the game-play\ninto logical roles and having highly focused components for each role, with\neach component disregarding anything outwith its own role. Reactive agents can\nbe used where a reactive agent is suited to the role, and where a deliberative\napproach is required, branching is kept to a minimum by the removal of all\nextraneous factors, enabling an informed decision to be made within a much\nsmaller time-frame. An Arbiter is used to combine the component results,\nallowing high performing agents to be created from simple, efficient\ncomponents.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 15:17:57 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Rodgers", "Philip", ""], ["Levine", "John", ""]]}, {"id": "1706.06954", "submitter": "Claudia Schulz", "authors": "Christos Rodosthenous and Loizos Michael", "title": "Web-STAR: Towards a Visual Web-Based IDE for a Story Comprehension\n  System", "comments": "Proceedings of the 2nd International Workshop on User-Oriented Logic\n  Paradigms (IULP 2017), Editors: Claudia Schulz and Stefan Ellmauthaler", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we present Web-STAR, an online platform for story understanding\nbuilt on top of the STAR (STory comprehension through ARgumentation) reasoning\nengine. This platform includes a web-based IDE, integration with the STAR\nsystem and a web service infrastructure to support integration with other\nsystems that rely on story understanding functionality to complete their tasks.\nThe platform also delivers a number of \"social\" features like public story\nsharing with a built-in commenting system, a public repository for sharing\nstories with the community and collaboration tools that can be used from both\nproject team members for development and educators for teaching. Moreover, we\ndiscuss the ongoing work on adding new features and functionality to this\nplatform.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 10:02:13 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Rodosthenous", "Christos", ""], ["Michael", "Loizos", ""]]}, {"id": "1706.06975", "submitter": "Mark Stalzer", "authors": "Mark A. Stalzer", "title": "On the enumeration of sentences by compactness", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Presented is a Julia meta-program that discovers compact theories from data\nif they exist. It writes candidate theories in Julia and then validates:\ntossing the bad theories and keeping the good theories. Compactness is measured\nby a metric: such as the number of space-time derivatives. The underlying\nalgorithm is applicable to a wide variety of combinatorics problems and\ncompactness serves to cut down the search space.\n", "versions": [{"version": "v1", "created": "Thu, 15 Jun 2017 22:57:06 GMT"}], "update_date": "2017-06-22", "authors_parsed": [["Stalzer", "Mark A.", ""]]}, {"id": "1706.07068", "submitter": "Ahmed Elgammal", "authors": "Ahmed Elgammal, Bingchen Liu, Mohamed Elhoseiny, Marian Mazzone", "title": "CAN: Creative Adversarial Networks, Generating \"Art\" by Learning About\n  Styles and Deviating from Style Norms", "comments": "This paper is an extended version of a paper published on the eighth\n  International Conference on Computational Creativity (ICCC), held in Atlanta,\n  GA, June 20th-June 22nd, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new system for generating art. The system generates art by\nlooking at art and learning about style; and becomes creative by increasing the\narousal potential of the generated art by deviating from the learned styles. We\nbuild over Generative Adversarial Networks (GAN), which have shown the ability\nto learn to generate novel images simulating a given distribution. We argue\nthat such networks are limited in their ability to generate creative products\nin their original design. We propose modifications to its objective to make it\ncapable of generating creative art by maximizing deviation from established\nstyles and minimizing deviation from art distribution. We conducted experiments\nto compare the response of human subjects to the generated art with their\nresponse to art created by artists. The results show that human subjects could\nnot distinguish art generated by the proposed system from art generated by\ncontemporary artists and shown in top art fairs. Human subjects even rated the\ngenerated images higher on various scales.\n", "versions": [{"version": "v1", "created": "Wed, 21 Jun 2017 18:05:13 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Elgammal", "Ahmed", ""], ["Liu", "Bingchen", ""], ["Elhoseiny", "Mohamed", ""], ["Mazzone", "Marian", ""]]}, {"id": "1706.07147", "submitter": "Kevin Feigelis", "authors": "Kevin T. Feigelis and Daniel L. K. Yamins", "title": "A Useful Motif for Flexible Task Learning in an Embodied Two-Dimensional\n  Visual Environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Animals (especially humans) have an amazing ability to learn new tasks\nquickly, and switch between them flexibly. How brains support this ability is\nlargely unknown, both neuroscientifically and algorithmically. One reasonable\nsupposition is that modules drawing on an underlying general-purpose sensory\nrepresentation are dynamically allocated on a per-task basis. Recent results\nfrom neuroscience and artificial intelligence suggest the role of the general\npurpose visual representation may be played by a deep convolutional neural\nnetwork, and give some clues how task modules based on such a representation\nmight be discovered and constructed. In this work, we investigate module\narchitectures in an embodied two-dimensional touchscreen environment, in which\nan agent's learning must occur via interactions with an environment that emits\nimages and rewards, and accepts touches as input. This environment is designed\nto capture the physical structure of the task environments that are commonly\ndeployed in visual neuroscience and psychophysics. We show that in this\ncontext, very simple changes in the nonlinear activations used by such a module\ncan significantly influence how fast it is at learning visual tasks and how\nsuitable it is for switching to new tasks.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 02:07:14 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Feigelis", "Kevin T.", ""], ["Yamins", "Daniel L. K.", ""]]}, {"id": "1706.07160", "submitter": "Nikaash Puri", "authors": "Nikaash Puri, Piyush Gupta, Pratiksha Agarwal, Sukriti Verma, and\n  Balaji Krishnamurthy", "title": "MAGIX: Model Agnostic Globally Interpretable Explanations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Explaining the behavior of a black box machine learning model at the instance\nlevel is useful for building trust. However, it is also important to understand\nhow the model behaves globally. Such an understanding provides insight into\nboth the data on which the model was trained and the patterns that it learned.\nWe present here an approach that learns if-then rules to globally explain the\nbehavior of black box machine learning models that have been used to solve\nclassification problems. The approach works by first extracting conditions that\nwere important at the instance level and then evolving rules through a genetic\nalgorithm with an appropriate fitness function. Collectively, these rules\nrepresent the patterns followed by the model for decisioning and are useful for\nunderstanding its behavior. We demonstrate the validity and usefulness of the\napproach by interpreting black box models created using publicly available data\nsets as well as a private digital marketing data set.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 03:55:28 GMT"}, {"version": "v2", "created": "Tue, 24 Oct 2017 04:45:15 GMT"}, {"version": "v3", "created": "Fri, 15 Jun 2018 10:46:29 GMT"}], "update_date": "2018-06-18", "authors_parsed": [["Puri", "Nikaash", ""], ["Gupta", "Piyush", ""], ["Agarwal", "Pratiksha", ""], ["Verma", "Sukriti", ""], ["Krishnamurthy", "Balaji", ""]]}, {"id": "1706.07206", "submitter": "Wojciech Samek", "authors": "Leila Arras, Gr\\'egoire Montavon, Klaus-Robert M\\\"uller, Wojciech\n  Samek", "title": "Explaining Recurrent Neural Network Predictions in Sentiment Analysis", "comments": "9 pages, 4 figures, accepted for EMNLP'17 Workshop on Computational\n  Approaches to Subjectivity, Sentiment & Social Media Analysis (WASSA)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, a technique called Layer-wise Relevance Propagation (LRP) was shown\nto deliver insightful explanations in the form of input space relevances for\nunderstanding feed-forward neural network classification decisions. In the\npresent work, we extend the usage of LRP to recurrent neural networks. We\npropose a specific propagation rule applicable to multiplicative connections as\nthey arise in recurrent network architectures such as LSTMs and GRUs. We apply\nour technique to a word-based bi-directional LSTM model on a five-class\nsentiment prediction task, and evaluate the resulting LRP relevances both\nqualitatively and quantitatively, obtaining better results than a\ngradient-based related method which was used in previous work.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 08:24:59 GMT"}, {"version": "v2", "created": "Fri, 4 Aug 2017 20:01:33 GMT"}], "update_date": "2017-08-08", "authors_parsed": [["Arras", "Leila", ""], ["Montavon", "Gr\u00e9goire", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Samek", "Wojciech", ""]]}, {"id": "1706.07230", "submitter": "Devendra Singh Chaplot", "authors": "Devendra Singh Chaplot, Kanthashree Mysore Sathyendra, Rama Kumar\n  Pasumarthi, Dheeraj Rajagopal, Ruslan Salakhutdinov", "title": "Gated-Attention Architectures for Task-Oriented Language Grounding", "comments": "To appear in AAAI-18", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To perform tasks specified by natural language instructions, autonomous\nagents need to extract semantically meaningful representations of language and\nmap it to visual elements and actions in the environment. This problem is\ncalled task-oriented language grounding. We propose an end-to-end trainable\nneural architecture for task-oriented language grounding in 3D environments\nwhich assumes no prior linguistic or perceptual knowledge and requires only raw\npixels from the environment and the natural language instruction as input. The\nproposed model combines the image and text representations using a\nGated-Attention mechanism and learns a policy to execute the natural language\ninstruction using standard reinforcement and imitation learning methods. We\nshow the effectiveness of the proposed model on unseen instructions as well as\nunseen maps, both quantitatively and qualitatively. We also introduce a novel\nenvironment based on a 3D game engine to simulate the challenges of\ntask-oriented language grounding over a rich set of instructions and\nenvironment states.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 09:39:17 GMT"}, {"version": "v2", "created": "Tue, 9 Jan 2018 03:24:06 GMT"}], "update_date": "2018-01-10", "authors_parsed": [["Chaplot", "Devendra Singh", ""], ["Sathyendra", "Kanthashree Mysore", ""], ["Pasumarthi", "Rama Kumar", ""], ["Rajagopal", "Dheeraj", ""], ["Salakhutdinov", "Ruslan", ""]]}, {"id": "1706.07269", "submitter": "Tim Miller", "authors": "Tim Miller", "title": "Explanation in Artificial Intelligence: Insights from the Social\n  Sciences", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There has been a recent resurgence in the area of explainable artificial\nintelligence as researchers and practitioners seek to make their algorithms\nmore understandable. Much of this research is focused on explicitly explaining\ndecisions or actions to a human observer, and it should not be controversial to\nsay that looking at how humans explain to each other can serve as a useful\nstarting point for explanation in artificial intelligence. However, it is fair\nto say that most work in explainable artificial intelligence uses only the\nresearchers' intuition of what constitutes a `good' explanation. There exists\nvast and valuable bodies of research in philosophy, psychology, and cognitive\nscience of how people define, generate, select, evaluate, and present\nexplanations, which argues that people employ certain cognitive biases and\nsocial expectations towards the explanation process. This paper argues that the\nfield of explainable artificial intelligence should build on this existing\nresearch, and reviews relevant papers from philosophy, cognitive\npsychology/science, and social psychology, which study these topics. It draws\nout some important findings, and discusses ways that these can be infused with\nwork on explainable artificial intelligence.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 11:46:11 GMT"}, {"version": "v2", "created": "Thu, 24 May 2018 02:43:30 GMT"}, {"version": "v3", "created": "Wed, 15 Aug 2018 00:50:00 GMT"}], "update_date": "2018-08-16", "authors_parsed": [["Miller", "Tim", ""]]}, {"id": "1706.07294", "submitter": "Adeyinka K. Akanbi MR", "authors": "A. K. Akanbi, M. Masinde", "title": "A Framework for Accurate Drought Forecasting System Using\n  Semantics-Based Data Integration Middleware", "comments": "5 pages, 1 figure. arXiv admin note: substantial text overlap with\n  arXiv:1601.01920", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Technological advancement in Wireless Sensor Networks (WSN) has made it\nbecome an invaluable component of a reliable environmental monitoring system;\nthey form the digital skin' through which to 'sense' and collect the context of\nthe surroundings and provides information on the process leading to complex\nevents such as drought. However, these environmental properties are measured by\nvarious heterogeneous sensors of different modalities in distributed locations\nmaking up the WSN, using different abstruse terms and vocabulary in most cases\nto denote the same observed property, causing data heterogeneity. Adding\nsemantics and understanding the relationships that exist between the observed\nproperties, and augmenting it with local indigenous knowledge is necessary for\nan accurate drought forecasting system. In this paper, we propose the framework\nfor the semantic representation of sensor data and integration with indigenous\nknowledge on drought using a middleware for an efficient drought forecasting\nsystem.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 13:21:50 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Akanbi", "A. K.", ""], ["Masinde", "M.", ""]]}, {"id": "1706.07351", "submitter": "Lalit Maganti", "authors": "Alessio Lomuscio, Lalit Maganti", "title": "An approach to reachability analysis for feed-forward ReLU neural\n  networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the reachability problem for systems implemented as feed-forward\nneural networks whose activation function is implemented via ReLU functions. We\ndraw a correspondence between establishing whether some arbitrary output can\never be outputed by a neural system and linear problems characterising a neural\nsystem of interest. We present a methodology to solve cases of practical\ninterest by means of a state-of-the-art linear programs solver. We evaluate the\ntechnique presented by discussing the experimental results obtained by\nanalysing reachability properties for a number of benchmarks in the literature.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 14:59:49 GMT"}], "update_date": "2017-06-23", "authors_parsed": [["Lomuscio", "Alessio", ""], ["Maganti", "Lalit", ""]]}, {"id": "1706.07412", "submitter": "Raine Ronnholm", "authors": "Valentin Goranko, Antti Kuusisto, Raine R\\\"onnholm", "title": "Rational coordination with no communication or conventions", "comments": "Preprint of a paper published in Journal of Logic and Computation,\n  30(6): 1183-1211, 2020", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.GT cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study pure coordination games where in every outcome, all players have\nidentical payoffs, 'win' or 'lose'. We identify and discuss a range of 'purely\nrational principles' guiding the reasoning of rational players in such games\nand analyze which classes of coordination games can be solved by such players\nwith no preplay communication or conventions. We observe that it is highly\nnontrivial to delineate a boundary between purely rational principles and other\ndecision methods, such as conventions, for solving such coordination games.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 17:36:08 GMT"}, {"version": "v2", "created": "Thu, 7 Sep 2017 13:07:31 GMT"}, {"version": "v3", "created": "Wed, 17 Mar 2021 16:07:07 GMT"}], "update_date": "2021-03-18", "authors_parsed": [["Goranko", "Valentin", ""], ["Kuusisto", "Antti", ""], ["R\u00f6nnholm", "Raine", ""]]}, {"id": "1706.07506", "submitter": "Massimiliano Ruocco", "authors": "Massimiliano Ruocco, Ole Steinar Lillest{\\o}l Skrede, Helge Langseth", "title": "Inter-Session Modeling for Session-Based Recommendation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years, research has been done on applying Recurrent Neural Networks\n(RNNs) as recommender systems. Results have been promising, especially in the\nsession-based setting where RNNs have been shown to outperform state-of-the-art\nmodels. In many of these experiments, the RNN could potentially improve the\nrecommendations by utilizing information about the user's past sessions, in\naddition to its own interactions in the current session. A problem for\nsession-based recommendation, is how to produce accurate recommendations at the\nstart of a session, before the system has learned much about the user's current\ninterests. We propose a novel approach that extends a RNN recommender to be\nable to process the user's recent sessions, in order to improve\nrecommendations. This is done by using a second RNN to learn from recent\nsessions, and predict the user's interest in the current session. By feeding\nthis information to the original RNN, it is able to improve its\nrecommendations. Our experiments on two different datasets show that the\nproposed approach can significantly improve recommendations throughout the\nsessions, compared to a single RNN working only on the current session. The\nproposed model especially improves recommendations at the start of sessions,\nand is therefore able to deal with the cold start problem within sessions.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 22:17:00 GMT"}], "update_date": "2017-06-26", "authors_parsed": [["Ruocco", "Massimiliano", ""], ["Skrede", "Ole Steinar Lillest\u00f8l", ""], ["Langseth", "Helge", ""]]}, {"id": "1706.07515", "submitter": "Denis Parra", "authors": "Vicente Dominguez and Pablo Messina and Denis Parra and Domingo Mery\n  and Christoph Trattner and Alvaro Soto", "title": "Comparing Neural and Attractiveness-based Visual Features for Artwork\n  Recommendation", "comments": "DLRS 2017 workshop, co-located at RecSys 2017", "journal-ref": null, "doi": "10.1145/3125486.3125495", "report-no": null, "categories": "cs.IR cs.AI cs.CV cs.DL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Advances in image processing and computer vision in the latest years have\nbrought about the use of visual features in artwork recommendation. Recent\nworks have shown that visual features obtained from pre-trained deep neural\nnetworks (DNNs) perform very well for recommending digital art. Other recent\nworks have shown that explicit visual features (EVF) based on attractiveness\ncan perform well in preference prediction tasks, but no previous work has\ncompared DNN features versus specific attractiveness-based visual features\n(e.g. brightness, texture) in terms of recommendation performance. In this\nwork, we study and compare the performance of DNN and EVF features for the\npurpose of physical artwork recommendation using transactional data from\nUGallery, an online store of physical paintings. In addition, we perform an\nexploratory analysis to understand if DNN embedded features have some relation\nwith certain EVF. Our results show that DNN features outperform EVF, that\ncertain EVF features are more suited for physical artwork recommendation and,\nfinally, we show evidence that certain neurons in the DNN might be partially\nencoding visual features such as brightness, providing an opportunity for\nexplaining recommendations based on visual neural models.\n", "versions": [{"version": "v1", "created": "Thu, 22 Jun 2017 22:48:48 GMT"}, {"version": "v2", "created": "Fri, 21 Jul 2017 22:17:48 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Dominguez", "Vicente", ""], ["Messina", "Pablo", ""], ["Parra", "Denis", ""], ["Mery", "Domingo", ""], ["Trattner", "Christoph", ""], ["Soto", "Alvaro", ""]]}, {"id": "1706.07527", "submitter": "Hemanth Venkateswara", "authors": "Hemanth Venkateswara, Shayok Chakraborty, Troy McDaniel, Sethuraman\n  Panchanathan", "title": "Model Selection with Nonlinear Embedding for Unsupervised Domain\n  Adaptation", "comments": "AAAI Workshops 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Domain adaptation deals with adapting classifiers trained on data from a\nsource distribution, to work effectively on data from a target distribution. In\nthis paper, we introduce the Nonlinear Embedding Transform (NET) for\nunsupervised domain adaptation. The NET reduces cross-domain disparity through\nnonlinear domain alignment. It also embeds the domain-aligned data such that\nsimilar data points are clustered together. This results in enhanced\nclassification. To determine the parameters in the NET model (and in other\nunsupervised domain adaptation models), we introduce a validation procedure by\nsampling source data points that are similar in distribution to the target\ndata. We test the NET and the validation procedure using popular image datasets\nand compare the classification results across competitive procedures for\nunsupervised domain adaptation.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 00:04:38 GMT"}], "update_date": "2017-06-26", "authors_parsed": [["Venkateswara", "Hemanth", ""], ["Chakraborty", "Shayok", ""], ["McDaniel", "Troy", ""], ["Panchanathan", "Sethuraman", ""]]}, {"id": "1706.07561", "submitter": "Jiaming Song", "authors": "Jiaming Song and Shengjia Zhao and Stefano Ermon", "title": "A-NICE-MC: Adversarial Training for MCMC", "comments": "NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Existing Markov Chain Monte Carlo (MCMC) methods are either based on\ngeneral-purpose and domain-agnostic schemes which can lead to slow convergence,\nor hand-crafting of problem-specific proposals by an expert. We propose\nA-NICE-MC, a novel method to train flexible parametric Markov chain kernels to\nproduce samples with desired properties. First, we propose an efficient\nlikelihood-free adversarial training method to train a Markov chain and mimic a\ngiven data distribution. Then, we leverage flexible volume preserving flows to\nobtain parametric kernels for MCMC. Using a bootstrap approach, we show how to\ntrain efficient Markov chains to sample from a prescribed posterior\ndistribution by iteratively improving the quality of both the model and the\nsamples. A-NICE-MC provides the first framework to automatically design\nefficient domain-specific MCMC proposals. Empirical results demonstrate that\nA-NICE-MC combines the strong guarantees of MCMC with the expressiveness of\ndeep neural networks, and is able to significantly outperform competing methods\nsuch as Hamiltonian Monte Carlo.\n", "versions": [{"version": "v1", "created": "Fri, 23 Jun 2017 04:19:04 GMT"}, {"version": "v2", "created": "Tue, 14 Nov 2017 18:20:40 GMT"}, {"version": "v3", "created": "Wed, 14 Mar 2018 19:23:42 GMT"}], "update_date": "2018-03-16", "authors_parsed": [["Song", "Jiaming", ""], ["Zhao", "Shengjia", ""], ["Ermon", "Stefano", ""]]}, {"id": "1706.07946", "submitter": "Thom Fruehwirth", "authors": "Thom Fruehwirth", "title": "Justifications in Constraint Handling Rules for Logical Retraction in\n  Dynamic Algorithms", "comments": "Pre-proceedings paper presented at the 27th International Symposium\n  on Logic-Based Program Synthesis and Transformation (LOPSTR 2017), Namur,\n  Belgium, 10-12 October 2017 (arXiv:1708.07854)", "journal-ref": null, "doi": null, "report-no": "LOPSTR/2017/23", "categories": "cs.AI cs.LO cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a straightforward source-to-source transformation that introduces\njustifications for user-defined constraints into the CHR programming language.\nThen a scheme of two rules suffices to allow for logical retraction (deletion,\nremoval) of constraints during computation. Without the need to recompute from\nscratch, these rules remove not only the constraint but also undo all\nconsequences of the rule applications that involved the constraint. We prove a\nconfluence result concerning the rule scheme and show its correctness. When\nalgorithms are written in CHR, constraints represent both data and operations.\nCHR is already incremental by nature, i.e. constraints can be added at runtime.\nLogical retraction adds decrementality. Hence any algorithm written in CHR with\njustifications will become fully dynamic. Operations can be undone and data can\nbe removed at any point in the computation without compromising the correctness\nof the result. We present two classical examples of dynamic algorithms, written\nin our prototype implementation of CHR with justifications that is available\nonline: maintaining the minimum of a changing set of numbers and shortest paths\nin a graph whose edges change.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jun 2017 12:43:50 GMT"}, {"version": "v2", "created": "Mon, 11 Sep 2017 13:07:20 GMT"}], "update_date": "2017-09-12", "authors_parsed": [["Fruehwirth", "Thom", ""]]}, {"id": "1706.08001", "submitter": "Zizhuang (Prince K) Wang Mr", "authors": "Zizhuang Wang", "title": "Temporal-related Convolutional-Restricted-Boltzmann-Machine capable of\n  learning relational order via reinforcement learning procedure?", "comments": "Keywords: Convolutional-Restricted-Boltzmann-Machine, Reinforcement\n  learning", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article, we extend the conventional framework of\nconvolutional-Restricted-Boltzmann-Machine to learn highly abstract features\namong abitrary number of time related input maps by constructing a layer of\nmultiplicative units, which capture the relations among inputs. In many cases,\nmore than two maps are strongly related, so it is wise to make multiplicative\nunit learn relations among more input maps, in other words, to find the optimal\nrelational-order of each unit. In order to enable our machine to learn\nrelational order, we developed a reinforcement-learning method whose optimality\nis proven to train the network.\n", "versions": [{"version": "v1", "created": "Sat, 24 Jun 2017 20:56:27 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Wang", "Zizhuang", ""]]}, {"id": "1706.08089", "submitter": "Christophe Guyeux", "authors": "R\\'egis Garnier, Christophe Guyeux, and St\\'ephane Chr\\'etien", "title": "Finding optimal finite biological sequences over finite alphabets: the\n  OptiFin toolbox", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we present a toolbox for a specific optimization problem that\nfrequently arises in bioinformatics or genomics. In this specific optimisation\nproblem, the state space is a set of words of specified length over a finite\nalphabet. To each word is associated a score. The overall objective is to find\nthe words which have the lowest possible score. This type of general\noptimization problem is encountered in e.g 3D conformation optimisation for\nprotein structure prediction, or largest core genes subset discovery based on\nbest supported phylogenetic tree for a set of species. In order to solve this\nproblem, we propose a toolbox that can be easily launched using MPI and embeds\n3 well-known metaheuristics. The toolbox is fully parametrized and well\ndocumented. It has been specifically designed to be easy modified and possibly\nimproved by the user depending on the application, and does not require to be a\ncomputer scientist. We show that the toolbox performs very well on two\ndifficult practical problems.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jun 2017 12:38:55 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Garnier", "R\u00e9gis", ""], ["Guyeux", "Christophe", ""], ["Chr\u00e9tien", "St\u00e9phane", ""]]}, {"id": "1706.08090", "submitter": "Jarryd Martin", "authors": "Jarryd Martin, Suraj Narayanan Sasikumar, Tom Everitt, Marcus Hutter", "title": "Count-Based Exploration in Feature Space for Reinforcement Learning", "comments": "Conference: Twenty-sixth International Joint Conference on Artificial\n  Intelligence (IJCAI-17), 8 pages, 1 figure", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce a new count-based optimistic exploration algorithm for\nReinforcement Learning (RL) that is feasible in environments with\nhigh-dimensional state-action spaces. The success of RL algorithms in these\ndomains depends crucially on generalisation from limited training experience.\nFunction approximation techniques enable RL agents to generalise in order to\nestimate the value of unvisited states, but at present few methods enable\ngeneralisation regarding uncertainty. This has prevented the combination of\nscalable RL algorithms with efficient exploration strategies that drive the\nagent to reduce its uncertainty. We present a new method for computing a\ngeneralised state visit-count, which allows the agent to estimate the\nuncertainty associated with any state. Our \\phi-pseudocount achieves\ngeneralisation by exploiting same feature representation of the state space\nthat is used for value function approximation. States that have less frequently\nobserved features are deemed more uncertain. The \\phi-Exploration-Bonus\nalgorithm rewards the agent for exploring in feature space rather than in the\nuntransformed state space. The method is simpler and less computationally\nexpensive than some previous proposals, and achieves near state-of-the-art\nresults on high-dimensional RL benchmarks.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jun 2017 12:39:44 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Martin", "Jarryd", ""], ["Sasikumar", "Suraj Narayanan", ""], ["Everitt", "Tom", ""], ["Hutter", "Marcus", ""]]}, {"id": "1706.08100", "submitter": "Fabio Patrizi", "authors": "Ronen Brafman, Giuseppe De Giacomo, Fabio Patrizi", "title": "Specifying Non-Markovian Rewards in MDPs Using LDL on Finite Traces\n  (Preliminary Version)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Markov Decision Processes (MDPs), the reward obtained in a state depends\non the properties of the last state and action. This state dependency makes it\ndifficult to reward more interesting long-term behaviors, such as always\nclosing a door after it has been opened, or providing coffee only following a\nrequest. Extending MDPs to handle such non-Markovian reward function was the\nsubject of two previous lines of work, both using variants of LTL to specify\nthe reward function and then compiling the new model back into a Markovian\nmodel. Building upon recent progress in the theories of temporal logics over\nfinite traces, we adopt LDLf for specifying non-Markovian rewards and provide\nan elegant automata construction for building a Markovian model, which extends\nthat of previous work and offers strong minimality and compositionality\nguarantees.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jun 2017 13:37:00 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Brafman", "Ronen", ""], ["De Giacomo", "Giuseppe", ""], ["Patrizi", "Fabio", ""]]}, {"id": "1706.08106", "submitter": "Christophe Guyeux", "authors": "Wiem Elghazel, Kamal Medjaher, Nourredine Zerhouni, Jacques Bahi,\n  Ahamd Farhat, Christophe Guyeux, and Mourad Hakem", "title": "Random Forests for Industrial Device Functioning Diagnostics Using\n  Wireless Sensor Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, random forests are proposed for operating devices diagnostics\nin the presence of a variable number of features. In various contexts, like\nlarge or difficult-to-access monitored areas, wired sensor networks providing\nfeatures to achieve diagnostics are either very costly to use or totally\nimpossible to spread out. Using a wireless sensor network can solve this\nproblem, but this latter is more subjected to flaws. Furthermore, the networks'\ntopology often changes, leading to a variability in quality of coverage in the\ntargeted area. Diagnostics at the sink level must take into consideration that\nboth the number and the quality of the provided features are not constant, and\nthat some politics like scheduling or data aggregation may be developed across\nthe network. The aim of this article is ($1$) to show that random forests are\nrelevant in this context, due to their flexibility and robustness, and ($2$) to\nprovide first examples of use of this method for diagnostics based on data\nprovided by a wireless sensor network.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jun 2017 13:54:33 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Elghazel", "Wiem", ""], ["Medjaher", "Kamal", ""], ["Zerhouni", "Nourredine", ""], ["Bahi", "Jacques", ""], ["Farhat", "Ahamd", ""], ["Guyeux", "Christophe", ""], ["Hakem", "Mourad", ""]]}, {"id": "1706.08146", "submitter": "Vatsal Sharan", "authors": "Vatsal Sharan, Kai Sheng Tai, Peter Bailis, Gregory Valiant", "title": "Compressed Factorization: Fast and Accurate Low-Rank Factorization of\n  Compressively-Sensed Data", "comments": "Updates for ICML'19 camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  What learning algorithms can be run directly on compressively-sensed data? In\nthis work, we consider the question of accurately and efficiently computing\nlow-rank matrix or tensor factorizations given data compressed via random\nprojections. We examine the approach of first performing factorization in the\ncompressed domain, and then reconstructing the original high-dimensional\nfactors from the recovered (compressed) factors. In both the matrix and tensor\nsettings, we establish conditions under which this natural approach will\nprovably recover the original factors. While it is well-known that random\nprojections preserve a number of geometric properties of a dataset, our work\ncan be viewed as showing that they can also preserve certain solutions of\nnon-convex, NP-Hard problems like non-negative matrix factorization. We support\nthese theoretical results with experiments on synthetic data and demonstrate\nthe practical applicability of compressed factorization on real-world gene\nexpression and EEG time series datasets.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jun 2017 17:47:16 GMT"}, {"version": "v2", "created": "Wed, 30 May 2018 09:13:19 GMT"}, {"version": "v3", "created": "Mon, 27 May 2019 08:27:04 GMT"}], "update_date": "2019-05-28", "authors_parsed": [["Sharan", "Vatsal", ""], ["Tai", "Kai Sheng", ""], ["Bailis", "Peter", ""], ["Valiant", "Gregory", ""]]}, {"id": "1706.08317", "submitter": "Eliseo Marzal", "authors": "Eliseo Marzal, Mohannad Babli, Eva Onaindia, Laura Sebastia", "title": "Handling PDDL3.0 State Trajectory Constraints with Temporal Landmarks", "comments": "Workshop on Constraint Satisfaction Techniques for Planning and\n  Scheduling (COPLAS), (2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Temporal landmarks have been proved to be a helpful mechanism to deal with\ntemporal planning problems, specifically to improve planners performance and\nhandle problems with deadline constraints. In this paper, we show the strength\nof using temporal landmarks to handle the state trajectory constraints of\nPDDL3.0. We analyze the formalism of TempLM, a temporal planner particularly\naimed at solving planning problems with deadlines, and we present a detailed\nstudy that exploits the underlying temporal landmark-based mechanism of TempLM\nfor representing and reasoning with trajectory constraints.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 10:56:57 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Marzal", "Eliseo", ""], ["Babli", "Mohannad", ""], ["Onaindia", "Eva", ""], ["Sebastia", "Laura", ""]]}, {"id": "1706.08325", "submitter": "Matti Karppa", "authors": "Tommi Junttila (1), Matti Karppa (1), Petteri Kaski (1), Jukka Kohonen\n  (1) ((1) Aalto University, Department of Computer Science)", "title": "An adaptive prefix-assignment technique for symmetry reduction", "comments": "Updated manuscript submitted for review", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a technique for symmetry reduction that adaptively\nassigns a prefix of variables in a system of constraints so that the generated\nprefix-assignments are pairwise nonisomorphic under the action of the symmetry\ngroup of the system. The technique is based on McKay's canonical extension\nframework [J.~Algorithms 26 (1998), no.~2, 306--324]. Among key features of the\ntechnique are (i) adaptability---the prefix sequence can be user-prescribed and\ntruncated for compatibility with the group of symmetries; (ii)\nparallelizability---prefix-assignments can be processed in parallel\nindependently of each other; (iii) versatility---the method is applicable\nwhenever the group of symmetries can be concisely represented as the\nautomorphism group of a vertex-colored graph; and (iv) implementability---the\nmethod can be implemented relying on a canonical labeling map for\nvertex-colored graphs as the only nontrivial subroutine. To demonstrate the\npractical applicability of our technique, we have prepared an experimental\nopen-source implementation of the technique and carry out a set of experiments\nthat demonstrate ability to reduce symmetry on hard instances. Furthermore, we\ndemonstrate that the implementation effectively parallelizes to compute\nclusters with multiple nodes via a message-passing interface.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 11:18:27 GMT"}, {"version": "v2", "created": "Sat, 8 Sep 2018 10:58:31 GMT"}], "update_date": "2018-09-11", "authors_parsed": [["Junttila", "Tommi", "", "Aalto University, Department of Computer Science"], ["Karppa", "Matti", "", "Aalto University, Department of Computer Science"], ["Kaski", "Petteri", "", "Aalto University, Department of Computer Science"], ["Kohonen", "Jukka", "", "Aalto University, Department of Computer Science"]]}, {"id": "1706.08329", "submitter": "Christoph Wernhard", "authors": "Christoph Wernhard", "title": "The Boolean Solution Problem from the Perspective of Predicate Logic -\n  Extended Version", "comments": null, "journal-ref": null, "doi": null, "report-no": "KRR 17-01", "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Finding solution values for unknowns in Boolean equations was a principal\nreasoning mode in the Algebra of Logic of the 19th century. Schr\\\"oder\ninvestigated it as \"Aufl\\\"osungsproblem\" (\"solution problem\"). It is closely\nrelated to the modern notion of Boolean unification. Today it is commonly\npresented in an algebraic setting, but seems potentially useful also in\nknowledge representation based on predicate logic. We show that it can be\nmodeled on the basis of first-order logic extended by second-order\nquantification. A wealth of classical results transfers, foundations for\nalgorithms unfold, and connections with second-order quantifier elimination and\nCraig interpolation show up. Although for first-order inputs the set of\nsolutions is recursively enumerable, the development of constructive methods\nremains a challenge. We identify some cases that allow constructions, most of\nthem based on Craig interpolation, and show a method to take vocabulary\nrestrictions on solution components into account.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 11:30:06 GMT"}, {"version": "v2", "created": "Fri, 27 Oct 2017 09:19:48 GMT"}, {"version": "v3", "created": "Mon, 18 Dec 2017 22:29:44 GMT"}], "update_date": "2017-12-20", "authors_parsed": [["Wernhard", "Christoph", ""]]}, {"id": "1706.08439", "submitter": "Marina Sapir", "authors": "Marina Sapir", "title": "Optimal choice: new machine learning problem and its solution", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The task of learning to pick a single preferred example out a finite set of\nexamples, an \"optimal choice problem\", is a supervised machine learning problem\nwith complex, structured input. Problems of optimal choice emerge often in\nvarious practical applications. We formalize the problem, show that it does not\nsatisfy the assumptions of statistical learning theory, yet it can be solved\nefficiently in some cases. We propose two approaches to solve the problem. Both\nof them reach good solutions on real life data from a signal processing\napplication.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 15:32:33 GMT"}, {"version": "v2", "created": "Thu, 6 Jul 2017 17:28:23 GMT"}], "update_date": "2017-07-07", "authors_parsed": [["Sapir", "Marina", ""]]}, {"id": "1706.08476", "submitter": "Tiancheng Zhao", "authors": "Tiancheng Zhao, Allen Lu, Kyusong Lee and Maxine Eskenazi", "title": "Generative Encoder-Decoder Models for Task-Oriented Spoken Dialog\n  Systems with Chatting Capability", "comments": "Accepted as a long paper in SIGIDIAL 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative encoder-decoder models offer great promise in developing\ndomain-general dialog systems. However, they have mainly been applied to\nopen-domain conversations. This paper presents a practical and novel framework\nfor building task-oriented dialog systems based on encoder-decoder models. This\nframework enables encoder-decoder models to accomplish slot-value independent\ndecision-making and interact with external databases. Moreover, this paper\nshows the flexibility of the proposed method by interleaving chatting\ncapability with a slot-filling system for better out-of-domain recovery. The\nmodels were trained on both real-user data from a bus information system and\nhuman-human chat data. Results show that the proposed framework achieves good\nperformance in both offline evaluation metrics and in task success rate with\nhuman users.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 16:52:42 GMT"}], "update_date": "2017-06-27", "authors_parsed": [["Zhao", "Tiancheng", ""], ["Lu", "Allen", ""], ["Lee", "Kyusong", ""], ["Eskenazi", "Maxine", ""]]}, {"id": "1706.08493", "submitter": "Filipe Assun\\c{c}\\~ao", "authors": "Filipe Assun\\c{c}\\~ao, Nuno Louren\\c{c}o, Penousal Machado, Bernardete\n  Ribeiro", "title": "Towards the Evolution of Multi-Layered Neural Networks: A Dynamic\n  Structured Grammatical Evolution Approach", "comments": null, "journal-ref": null, "doi": "10.1145/3071178.3071286", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current grammar-based NeuroEvolution approaches have several shortcomings. On\nthe one hand, they do not allow the generation of Artificial Neural Networks\n(ANNs composed of more than one hidden-layer. On the other, there is no way to\nevolve networks with more than one output neuron. To properly evolve ANNs with\nmore than one hidden-layer and multiple output nodes there is the need to know\nthe number of neurons available in previous layers. In this paper we introduce\nDynamic Structured Grammatical Evolution (DSGE): a new genotypic representation\nthat overcomes the aforementioned limitations. By enabling the creation of\ndynamic rules that specify the connection possibilities of each neuron, the\nmethodology enables the evolution of multi-layered ANNs with more than one\noutput neuron. Results in different classification problems show that DSGE\nevolves effective single and multi-layered ANNs, with a varying number of\noutput neurons.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 17:34:14 GMT"}], "update_date": "2018-01-08", "authors_parsed": [["Assun\u00e7\u00e3o", "Filipe", ""], ["Louren\u00e7o", "Nuno", ""], ["Machado", "Penousal", ""], ["Ribeiro", "Bernardete", ""]]}, {"id": "1706.08501", "submitter": "Luke Miles", "authors": "Luke Harold Miles", "title": "A Simulator for Hedonic Games", "comments": "Will be presented at the 2017 Algorithmic Decision Theory Doctoral\n  Consortium. 6 pages and 4 pictures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Hedonic games are meant to model how coalitions of people form and break\napart in the real world. However, it is difficult to run simulations when\neverything must be done by hand on paper. We present an online software that\nallows fast and visual simulation of several types of hedonic games.\nhttp://lukemiles.org/hedonic-games/\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 17:47:32 GMT"}, {"version": "v2", "created": "Tue, 25 Jul 2017 09:58:24 GMT"}], "update_date": "2017-07-26", "authors_parsed": [["Miles", "Luke Harold", ""]]}, {"id": "1706.08502", "submitter": "Satwik Kottur", "authors": "Satwik Kottur, Jos\\'e M.F. Moura, Stefan Lee, Dhruv Batra", "title": "Natural Language Does Not Emerge 'Naturally' in Multi-Agent Dialog", "comments": "9 pages, 7 figures, 2 tables, accepted at EMNLP 2017 as short paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A number of recent works have proposed techniques for end-to-end learning of\ncommunication protocols among cooperative multi-agent populations, and have\nsimultaneously found the emergence of grounded human-interpretable language in\nthe protocols developed by the agents, all learned without any human\nsupervision!\n  In this paper, using a Task and Tell reference game between two agents as a\ntestbed, we present a sequence of 'negative' results culminating in a\n'positive' one -- showing that while most agent-invented languages are\neffective (i.e. achieve near-perfect task rewards), they are decidedly not\ninterpretable or compositional.\n  In essence, we find that natural language does not emerge 'naturally',\ndespite the semblance of ease of natural-language-emergence that one may gather\nfrom recent literature. We discuss how it is possible to coax the invented\nlanguages to become more and more human-like and compositional by increasing\nrestrictions on how two agents may communicate.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 17:47:46 GMT"}, {"version": "v2", "created": "Thu, 3 Aug 2017 03:37:00 GMT"}, {"version": "v3", "created": "Sun, 20 Aug 2017 04:41:15 GMT"}], "update_date": "2017-08-22", "authors_parsed": [["Kottur", "Satwik", ""], ["Moura", "Jos\u00e9 M. F.", ""], ["Lee", "Stefan", ""], ["Batra", "Dhruv", ""]]}, {"id": "1706.08514", "submitter": "Christophe Guyeux", "authors": "Reem Alsrraj, Bassam AlKindy, Christophe Guyeux, Laurent Philippe,\n  Jean-Fran\\c{c}ois Couchot", "title": "Well-supported phylogenies using largest subsets of core-genes by\n  discrete particle swarm optimization", "comments": "arXiv admin note: substantial text overlap with arXiv:1608.08749", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.PE cs.AI q-bio.QM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The number of complete chloroplastic genomes increases day after day, making\nit possible to rethink plants phylogeny at the biomolecular era. Given a set of\nclose plants sharing in the order of one hundred of core chloroplastic genes,\nthis article focuses on how to extract the largest subset of sequences in order\nto obtain the most supported species tree. Due to computational complexity, a\ndiscrete and distributed Particle Swarm Optimization (DPSO) is proposed. It is\nfinally applied to the core genes of Rosales order.\n", "versions": [{"version": "v1", "created": "Sun, 25 Jun 2017 15:02:01 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Alsrraj", "Reem", ""], ["AlKindy", "Bassam", ""], ["Guyeux", "Christophe", ""], ["Philippe", "Laurent", ""], ["Couchot", "Jean-Fran\u00e7ois", ""]]}, {"id": "1706.08568", "submitter": "Georg Wiese", "authors": "Georg Wiese, Dirk Weissenborn, Mariana Neves", "title": "Neural Question Answering at BioASQ 5B", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes our submission to the 2017 BioASQ challenge. We\nparticipated in Task B, Phase B which is concerned with biomedical question\nanswering (QA). We focus on factoid and list question, using an extractive QA\nmodel, that is, we restrict our system to output substrings of the provided\ntext snippets. At the core of our system, we use FastQA, a state-of-the-art\nneural QA system. We extended it with biomedical word embeddings and changed\nits answer layer to be able to answer list questions in addition to factoid\nquestions. We pre-trained the model on a large-scale open-domain QA dataset,\nSQuAD, and then fine-tuned the parameters on the BioASQ training set. With our\napproach, we achieve state-of-the-art results on factoid questions and\ncompetitive results on list questions.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 19:14:10 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Wiese", "Georg", ""], ["Weissenborn", "Dirk", ""], ["Neves", "Mariana", ""]]}, {"id": "1706.08605", "submitter": "Daniel Selsam", "authors": "Daniel Selsam, Percy Liang, David L. Dill", "title": "Developing Bug-Free Machine Learning Systems With Formal Mathematics", "comments": "To appear at the Thirty-fourth International Conference on Machine\n  Learning (ICML) 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Noisy data, non-convex objectives, model misspecification, and numerical\ninstability can all cause undesired behaviors in machine learning systems. As a\nresult, detecting actual implementation errors can be extremely difficult. We\ndemonstrate a methodology in which developers use an interactive proof\nassistant to both implement their system and to state a formal theorem defining\nwhat it means for their system to be correct. The process of proving this\ntheorem interactively in the proof assistant exposes all implementation errors\nsince any error in the program would cause the proof to fail. As a case study,\nwe implement a new system, Certigrad, for optimizing over stochastic\ncomputation graphs, and we generate a formal (i.e. machine-checkable) proof\nthat the gradients sampled by the system are unbiased estimates of the true\nmathematical gradients. We train a variational autoencoder using Certigrad and\nfind the performance comparable to training the same model in TensorFlow.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 21:30:02 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Selsam", "Daniel", ""], ["Liang", "Percy", ""], ["Dill", "David L.", ""]]}, {"id": "1706.08611", "submitter": "Edward Zulkoski", "authors": "Edward Zulkoski, Ruben Martins, Christoph Wintersteiger, Robert\n  Robere, Jia Liang, Krzysztof Czarnecki, Vijay Ganesh", "title": "Relating Complexity-theoretic Parameters with SAT Solver Performance", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the years complexity theorists have proposed many structural parameters\nto explain the surprising efficiency of conflict-driven clause-learning (CDCL)\nSAT solvers on a wide variety of large industrial Boolean instances. While some\nof these parameters have been studied empirically, until now there has not been\na unified comparative study of their explanatory power on a comprehensive\nbenchmark. We correct this state of affairs by conducting a large-scale\nempirical evaluation of CDCL SAT solver performance on nearly 7000 industrial\nand crafted formulas against several structural parameters such as backdoors,\ntreewidth, backbones, and community structure.\n  Our study led us to several results. First, we show that while such\nparameters only weakly correlate with CDCL solving time, certain combinations\nof them yield much better regression models. Second, we show how some\nparameters can be used as a \"lens\" to better understand the efficiency of\ndifferent solving heuristics. Finally, we propose a new complexity-theoretic\nparameter, which we call learning-sensitive with restarts (LSR) backdoors, that\nextends the notion of learning-sensitive (LS) backdoors to incorporate restarts\nand discuss algorithms to compute them. We mathematically prove that for\ncertain class of instances minimal LSR-backdoors are exponentially smaller than\nminimal-LS backdoors.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 21:40:30 GMT"}], "update_date": "2017-06-28", "authors_parsed": [["Zulkoski", "Edward", ""], ["Martins", "Ruben", ""], ["Wintersteiger", "Christoph", ""], ["Robere", "Robert", ""], ["Liang", "Jia", ""], ["Czarnecki", "Krzysztof", ""], ["Ganesh", "Vijay", ""]]}, {"id": "1706.08627", "submitter": "Roberto Amadini", "authors": "Roberto Amadini, Maurizio Gabbrielli, Jacopo Mauro", "title": "SUNNY-CP and the MiniZinc Challenge", "comments": "Under consideration in Theory and Practice of Logic Programming\n  (TPLP)", "journal-ref": "Theory and Practice of Logic Programming, Volume 18, Issue 1,\n  January 2018 , pp. 81-96", "doi": "10.1017/S1471068417000205", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In Constraint Programming (CP) a portfolio solver combines a variety of\ndifferent constraint solvers for solving a given problem. This fairly recent\napproach enables to significantly boost the performance of single solvers,\nespecially when multicore architectures are exploited. In this work we give a\nbrief overview of the portfolio solver sunny-cp, and we discuss its performance\nin the MiniZinc Challenge---the annual international competition for CP\nsolvers---where it won two gold medals in 2015 and 2016. Under consideration in\nTheory and Practice of Logic Programming (TPLP)\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 23:48:14 GMT"}, {"version": "v2", "created": "Wed, 28 Jun 2017 00:23:05 GMT"}, {"version": "v3", "created": "Wed, 5 Jul 2017 23:49:34 GMT"}], "update_date": "2019-09-26", "authors_parsed": [["Amadini", "Roberto", ""], ["Gabbrielli", "Maurizio", ""], ["Mauro", "Jacopo", ""]]}, {"id": "1706.08840", "submitter": "David Lopez-Paz", "authors": "David Lopez-Paz and Marc'Aurelio Ranzato", "title": "Gradient Episodic Memory for Continual Learning", "comments": "Published at NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One major obstacle towards AI is the poor ability of models to solve new\nproblems quicker, and without forgetting previously acquired knowledge. To\nbetter understand this issue, we study the problem of continual learning, where\nthe model observes, once and one by one, examples concerning a sequence of\ntasks. First, we propose a set of metrics to evaluate models learning over a\ncontinuum of data. These metrics characterize models not only by their test\naccuracy, but also in terms of their ability to transfer knowledge across\ntasks. Second, we propose a model for continual learning, called Gradient\nEpisodic Memory (GEM) that alleviates forgetting, while allowing beneficial\ntransfer of knowledge to previous tasks. Our experiments on variants of the\nMNIST and CIFAR-100 datasets demonstrate the strong performance of GEM when\ncompared to the state-of-the-art.\n", "versions": [{"version": "v1", "created": "Mon, 26 Jun 2017 14:53:34 GMT"}, {"version": "v2", "created": "Wed, 28 Jun 2017 16:19:12 GMT"}, {"version": "v3", "created": "Wed, 12 Jul 2017 17:14:59 GMT"}, {"version": "v4", "created": "Wed, 2 Aug 2017 20:37:51 GMT"}, {"version": "v5", "created": "Sat, 4 Nov 2017 13:11:18 GMT"}], "update_date": "2017-11-07", "authors_parsed": [["Lopez-Paz", "David", ""], ["Ranzato", "Marc'Aurelio", ""]]}, {"id": "1706.08948", "submitter": "Sambhav R. Jain", "authors": "Sambhav R. Jain, Kye Okabe", "title": "Training a Fully Convolutional Neural Network to Route Integrated\n  Circuits", "comments": "Code released. 8 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a deep, fully convolutional neural network that learns to route a\ncircuit layout net with appropriate choice of metal tracks and wire class\ncombinations. Inputs to the network are the encoded layouts containing spatial\nlocation of pins to be routed. After 15 fully convolutional stages followed by\na score comparator, the network outputs 8 layout layers (corresponding to 4\nroute layers, 3 via layers and an identity-mapped pin layer) which are then\ndecoded to obtain the routed layouts. We formulate this as a binary\nsegmentation problem on a per-pixel per-layer basis, where the network is\ntrained to correctly classify pixels in each layout layer to be 'on' or 'off'.\nTo demonstrate learnability of layout design rules, we train the network on a\ndataset of 50,000 train and 10,000 validation samples that we generate based on\ncertain pre-defined layout constraints. Precision, recall and $F_1$ score\nmetrics are used to track the training progress. Our network achieves\n$F_1\\approx97\\%$ on the train set and $F_1\\approx92\\%$ on the validation set.\nWe use PyTorch for implementing our model. Code is made publicly available at\nhttps://github.com/sjain-stanford/deep-route .\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 17:20:21 GMT"}, {"version": "v2", "created": "Mon, 11 Sep 2017 18:37:04 GMT"}], "update_date": "2017-09-13", "authors_parsed": [["Jain", "Sambhav R.", ""], ["Okabe", "Kye", ""]]}, {"id": "1706.09007", "submitter": "Gianpiero Monaco", "authors": "Michele Flammini, Gianpiero Monaco, Qiang Zhang", "title": "Strategyproof Mechanisms for Additively Separable Hedonic Games and\n  Fractional Hedonic Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Additively separable hedonic games and fractional hedonic games have received\nconsiderable attention. They are coalition forming games of selfish agents\nbased on their mutual preferences. Most of the work in the literature\ncharacterizes the existence and structure of stable outcomes (i.e., partitions\nin coalitions), assuming that preferences are given. However, there is little\ndiscussion on this assumption. In fact, agents receive different utilities if\nthey belong to different partitions, and thus it is natural for them to declare\ntheir preferences strategically in order to maximize their benefit. In this\npaper we consider strategyproof mechanisms for additively separable hedonic\ngames and fractional hedonic games, that is, partitioning methods without\npayments such that utility maximizing agents have no incentive to lie about\ntheir true preferences. We focus on social welfare maximization and provide\nseveral lower and upper bounds on the performance achievable by strategyproof\nmechanisms for general and specific additive functions. In most of the cases we\nprovide tight or asymptotically tight results. All our mechanisms are simple\nand can be computed in polynomial time. Moreover, all the lower bounds are\nunconditional, that is, they do not rely on any computational or complexity\nassumptions.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 18:47:49 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Flammini", "Michele", ""], ["Monaco", "Gianpiero", ""], ["Zhang", "Qiang", ""]]}, {"id": "1706.09076", "submitter": "Jo\\~ao Miguel Cunha", "authors": "Jo\\~ao M. Cunha, Jo\\~ao Gon\\c{c}alves, Pedro Martins, Penousal Machado\n  and Am\\'ilcar Cardoso", "title": "A Pig, an Angel and a Cactus Walk Into a Blender: A Descriptive Approach\n  to Visual Blending", "comments": "9 pages, 8 figures, Proceedings of the Eighth International\n  Conference on Computational Creativity (ICCC-2017). Atlanta, GA, June\n  20th-June 22nd, 2017", "journal-ref": "Proceedings of the Eighth International Conference on\n  Computational Creativity (ICCC-2017)", "doi": null, "report-no": null, "categories": "cs.AI cs.GR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A descriptive approach for automatic generation of visual blends is\npresented. The implemented system, the Blender, is composed of two components:\nthe Mapper and the Visual Blender. The approach uses structured visual\nrepresentations along with sets of visual relations which describe how the\nelements (in which the visual representation can be decomposed) relate among\neach other. Our system is a hybrid blender, as the blending process starts at\nthe Mapper (conceptual level) and ends at the Visual Blender (visual\nrepresentation level). The experimental results show that the Blender is able\nto create analogies from input mental spaces and produce well-composed blends,\nwhich follow the rules imposed by its base-analogy and its relations. The\nresulting blends are visually interesting and some can be considered as\nunexpected.\n", "versions": [{"version": "v1", "created": "Tue, 27 Jun 2017 23:37:22 GMT"}, {"version": "v2", "created": "Thu, 30 Nov 2017 15:59:01 GMT"}, {"version": "v3", "created": "Tue, 19 Feb 2019 16:24:27 GMT"}], "update_date": "2019-02-20", "authors_parsed": [["Cunha", "Jo\u00e3o M.", ""], ["Gon\u00e7alves", "Jo\u00e3o", ""], ["Martins", "Pedro", ""], ["Machado", "Penousal", ""], ["Cardoso", "Am\u00edlcar", ""]]}, {"id": "1706.09152", "submitter": "Wenhu Chen", "authors": "Wenhu Chen, Guanlin Li, Shuo Ren, Shujie Liu, Zhirui Zhang, Mu Li,\n  Ming Zhou", "title": "Generative Bridging Network in Neural Sequence Prediction", "comments": "Accepted to NAACL 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to alleviate data sparsity and overfitting problems in maximum\nlikelihood estimation (MLE) for sequence prediction tasks, we propose the\nGenerative Bridging Network (GBN), in which a novel bridge module is introduced\nto assist the training of the sequence prediction model (the generator\nnetwork). Unlike MLE directly maximizing the conditional likelihood, the bridge\nextends the point-wise ground truth to a bridge distribution conditioned on it,\nand the generator is optimized to minimize their KL-divergence. Three different\nGBNs, namely uniform GBN, language-model GBN and coaching GBN, are proposed to\npenalize confidence, enhance language smoothness and relieve learning burden.\nExperiments conducted on two recognized sequence prediction tasks (machine\ntranslation and abstractive text summarization) show that our proposed GBNs can\nyield significant improvements over strong baselines. Furthermore, by analyzing\nsamples drawn from different bridges, expected influences on the generator are\nverified.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 07:44:17 GMT"}, {"version": "v2", "created": "Sun, 13 Aug 2017 16:24:41 GMT"}, {"version": "v3", "created": "Sun, 20 Aug 2017 11:17:13 GMT"}, {"version": "v4", "created": "Tue, 31 Oct 2017 17:49:11 GMT"}, {"version": "v5", "created": "Sat, 17 Mar 2018 22:03:58 GMT"}, {"version": "v6", "created": "Thu, 29 Nov 2018 22:29:53 GMT"}], "update_date": "2018-12-03", "authors_parsed": [["Chen", "Wenhu", ""], ["Li", "Guanlin", ""], ["Ren", "Shuo", ""], ["Liu", "Shujie", ""], ["Zhang", "Zhirui", ""], ["Li", "Mu", ""], ["Zhou", "Ming", ""]]}, {"id": "1706.09248", "submitter": "Claudia Schulz", "authors": "Timothy Yuen, Maritz Reyes and Yuanlin Zhang", "title": "Logic Programming for an Introductory Computer Science Course for High\n  School Students", "comments": "Proceedings of the 2nd International Workshop on User-Oriented Logic\n  Paradigms (IULP 2017), Editors: Claudia Schulz and Stefan Ellmauthaler", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates how high school students approach computing through\nan introductory computer science course situated in the Logic Programming (LP)\nparadigm. This study shows how novice students operate within the LP paradigm\nwhile engaging in foundational computing concepts and skills, and presents a\ncase for LP as a viable paradigm choice for introductory CS courses.\n", "versions": [{"version": "v1", "created": "Tue, 20 Jun 2017 12:18:33 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Yuen", "Timothy", ""], ["Reyes", "Maritz", ""], ["Zhang", "Yuanlin", ""]]}, {"id": "1706.09262", "submitter": "Adam Kosiorek", "authors": "Adam R. Kosiorek, Alex Bewley, Ingmar Posner", "title": "Hierarchical Attentive Recurrent Tracking", "comments": "Published as a conference paper at NIPS 2017. Code is available at\n  https://github.com/akosiorek/hart and qualitative results are available at\n  https://youtu.be/Vvkjm0FRGSs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Class-agnostic object tracking is particularly difficult in cluttered\nenvironments as target specific discriminative models cannot be learned a\npriori. Inspired by how the human visual cortex employs spatial attention and\nseparate \"where\" and \"what\" processing pathways to actively suppress irrelevant\nvisual features, this work develops a hierarchical attentive recurrent model\nfor single object tracking in videos. The first layer of attention discards the\nmajority of background by selecting a region containing the object of interest,\nwhile the subsequent layers tune in on visual features particular to the\ntracked object. This framework is fully differentiable and can be trained in a\npurely data driven fashion by gradient methods. To improve training\nconvergence, we augment the loss function with terms for a number of auxiliary\ntasks relevant for tracking. Evaluation of the proposed model is performed on\ntwo datasets: pedestrian tracking on the KTH activity recognition dataset and\nthe more difficult KITTI object tracking dataset.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 13:00:14 GMT"}, {"version": "v2", "created": "Tue, 5 Sep 2017 14:35:08 GMT"}], "update_date": "2017-09-06", "authors_parsed": [["Kosiorek", "Adam R.", ""], ["Bewley", "Alex", ""], ["Posner", "Ingmar", ""]]}, {"id": "1706.09278", "submitter": "Bhushan Kotnis", "authors": "Bhushan Kotnis and Vivi Nastase", "title": "Learning Knowledge Graph Embeddings with Type Regularizer", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning relations based on evidence from knowledge bases relies on\nprocessing the available relation instances. Many relations, however, have\nclear domain and range, which we hypothesize could help learn a better, more\ngeneralizing, model. We include such information in the RESCAL model in the\nform of a regularization factor added to the loss function that takes into\naccount the types (categories) of the entities that appear as arguments to\nrelations in the knowledge base. We note increased performance compared to the\nbaseline model in terms of mean reciprocal rank and hits@N, N = 1, 3, 10.\nFurthermore, we discover scenarios that significantly impact the effectiveness\nof the type regularizer.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 13:24:55 GMT"}, {"version": "v2", "created": "Fri, 2 Mar 2018 12:41:59 GMT"}], "update_date": "2018-03-05", "authors_parsed": [["Kotnis", "Bhushan", ""], ["Nastase", "Vivi", ""]]}, {"id": "1706.09347", "submitter": "Marius Merschformann", "authors": "Marius Merschformann, Lin Xie, Daniel Erdmann", "title": "Path planning for Robotic Mobile Fulfillment Systems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a collection of path planning algorithms for real-time\nmovement of multiple robots across a Robotic Mobile Fulfillment System (RMFS).\nRobots are assigned to move storage units to pickers at working stations\ninstead of requiring pickers to go to the storage area. Path planning\nalgorithms aim to find paths for the robots to fulfill the requests without\ncollisions or deadlocks. The state-of-the-art path planning algorithms,\nincluding WHCA*, FAR, BCP, OD&ID and CBS, were adapted to suit path planning in\nRMFS and integrated within a simulation tool to guide the robots from their\nstarting points to their destinations during the storage and retrieval\nprocesses. Ten different layouts with a variety of numbers of robots, floors,\npods, stations and the sizes of storage areas were considered in the simulation\nstudy. Performance metrics of throughput, path length and search time were\nmonitored. Simulation results demonstrate the best algorithm based on each\nperformance metric.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 16:18:12 GMT"}, {"version": "v2", "created": "Mon, 19 Nov 2018 08:19:56 GMT"}], "update_date": "2018-11-20", "authors_parsed": [["Merschformann", "Marius", ""], ["Xie", "Lin", ""], ["Erdmann", "Daniel", ""]]}, {"id": "1706.09370", "submitter": "Johannes Klaus Fichte", "authors": "Johannes K. Fichte, Markus Hecher, Michael Morak, Stefan Woltran", "title": "DynASP2.5: Dynamic Programming on Tree Decompositions in Action", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.CC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A vibrant theoretical research area are efficient exact parameterized\nalgorithms. Very recent solving competitions such as the PACE challenge show\nthat there is also increasing practical interest in the parameterized\nalgorithms community. An important research question is whether dedicated\nparameterized exact algorithms exhibit certain practical relevance and one can\neven beat well-established problem solvers. We consider the logic-based\ndeclarative modeling language and problem solving framework Answer Set\nProgramming (ASP). State-of-the-art ASP solvers rely considerably on Sat-based\nalgorithms. An ASP solver (DynASP2), which is based on a classical dynamic\nprogramming on tree decompositions, has been published very recently.\nUnfortunately, DynASP2 can outperform modern ASP solvers on programs of small\ntreewidth only if the question of interest is to count the number of solutions.\nIn this paper, we describe underlying concepts of our new implementation\n(DynASP2.5) that shows competitive behavior to state-of-the-art ASP solvers\neven for finding just one solution when solving problems as the Steiner tree\nproblem that have been modeled in ASP on graphs with low treewidth. Our\nimplementation is based on a novel approach that we call multi-pass dynamic\nprogramming (M-DPSINC).\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 17:20:24 GMT"}], "update_date": "2017-06-29", "authors_parsed": [["Fichte", "Johannes K.", ""], ["Hecher", "Markus", ""], ["Morak", "Michael", ""], ["Woltran", "Stefan", ""]]}, {"id": "1706.09393", "submitter": "Markus Hecher", "authors": "Johannes K. Fichte, Markus Hecher, Irina Schindler", "title": "Default Logic and Bounded Treewidth", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CC cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we study Reiter's propositional default logic when the\ntreewidth of a certain graph representation (semi-primal graph) of the input\ntheory is bounded. We establish a dynamic programming algorithm on tree\ndecompositions that decides whether a theory has a consistent stable extension\n(Ext). Our algorithm can even be used to enumerate all generating defaults\n(ExtEnum) that lead to stable extensions.\n  We show that our algorithm decides Ext in linear time in the input theory and\ntriple exponential time in the treewidth (so-called fixed-parameter linear\nalgorithm).\n  Further, our algorithm solves ExtEnum with a pre-computation step that is\nlinear in the input theory and triple exponential in the treewidth followed by\na linear delay to output solutions.\n", "versions": [{"version": "v1", "created": "Wed, 28 Jun 2017 17:57:02 GMT"}, {"version": "v2", "created": "Sat, 30 Dec 2017 11:03:22 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Fichte", "Johannes K.", ""], ["Hecher", "Markus", ""], ["Schindler", "Irina", ""]]}, {"id": "1706.09520", "submitter": "Jingwei Zhang", "authors": "Jingwei Zhang, Lei Tai, Ming Liu, Joschka Boedecker, Wolfram Burgard", "title": "Neural SLAM: Learning to Explore with External Memory", "comments": "A video of our experiments can be found at: https://goo.gl/G2Vu5y", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach for agents to learn representations of a global map\nfrom sensor data, to aid their exploration in new environments. To achieve\nthis, we embed procedures mimicking that of traditional Simultaneous\nLocalization and Mapping (SLAM) into the soft attention based addressing of\nexternal memory architectures, in which the external memory acts as an internal\nrepresentation of the environment. This structure encourages the evolution of\nSLAM-like behaviors inside a completely differentiable deep neural network. We\nshow that this approach can help reinforcement learning agents to successfully\nexplore new environments where long-term memory is essential. We validate our\napproach in both challenging grid-world environments and preliminary Gazebo\nexperiments. A video of our experiments can be found at: https://goo.gl/G2Vu5y.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 00:14:15 GMT"}, {"version": "v2", "created": "Mon, 3 Jul 2017 09:07:06 GMT"}, {"version": "v3", "created": "Tue, 4 Jul 2017 08:55:44 GMT"}, {"version": "v4", "created": "Wed, 5 Jul 2017 09:23:05 GMT"}, {"version": "v5", "created": "Mon, 20 Nov 2017 10:16:03 GMT"}, {"version": "v6", "created": "Wed, 29 Nov 2017 11:40:51 GMT"}, {"version": "v7", "created": "Wed, 30 Dec 2020 13:57:28 GMT"}], "update_date": "2021-01-01", "authors_parsed": [["Zhang", "Jingwei", ""], ["Tai", "Lei", ""], ["Liu", "Ming", ""], ["Boedecker", "Joschka", ""], ["Burgard", "Wolfram", ""]]}, {"id": "1706.09597", "submitter": "Masashi Okada Dr", "authors": "Masashi Okada, Luca Rigazio, Takenobu Aoshima", "title": "Path Integral Networks: End-to-End Differentiable Optimal Control", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we introduce Path Integral Networks (PI-Net), a recurrent\nnetwork representation of the Path Integral optimal control algorithm. The\nnetwork includes both system dynamics and cost models, used for optimal control\nbased planning. PI-Net is fully differentiable, learning both dynamics and cost\nmodels end-to-end by back-propagation and stochastic gradient descent. Because\nof this, PI-Net can learn to plan. PI-Net has several advantages: it can\ngeneralize to unseen states thanks to planning, it can be applied to continuous\ncontrol tasks, and it allows for a wide variety learning schemes, including\nimitation and reinforcement learning. Preliminary experiment results show that\nPI-Net, trained by imitation learning, can mimic control demonstrations for two\nsimulated problems; a linear system and a pendulum swing-up problem. We also\nshow that PI-Net is able to learn dynamics and cost models latent in the\ndemonstrations.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 07:13:15 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Okada", "Masashi", ""], ["Rigazio", "Luca", ""], ["Aoshima", "Takenobu", ""]]}, {"id": "1706.09737", "submitter": "Yohanes Khosiawan", "authors": "Yohanes Khosiawan and Izabela Nielsen", "title": "Indoor UAV scheduling with Restful Task Assignment Algorithm", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Research in UAV scheduling has obtained an emerging interest from scientists\nin the optimization field. When the scheduling itself has established a strong\nroot since the 19th century, works on UAV scheduling in indoor environment has\ncome forth in the latest decade. Several works on scheduling UAV operations in\nindoor (two and three dimensional) and outdoor environments are reported. In\nthis paper, a further study on UAV scheduling in three dimensional indoor\nenvironment is investigated. Dealing with indoor environment\\textemdash where\nhumans, UAVs, and other elements or infrastructures are likely to coexist in\nthe same space\\textemdash draws attention towards the safety of the operations.\nIn relation to the battery level, a preserved battery level leads to safer\noperations, promoting the UAV to have a decent remaining power level. A\nmethodology which consists of a heuristic approach based on Restful Task\nAssignment Algorithm, incorporated with Particle Swarm Optimization Algorithm,\nis proposed. The motivation is to preserve the battery level throughout the\noperations, which promotes less possibility in having failed UAVs on duty. This\nmethodology is tested with 54 benchmark datasets stressing on 4 different\naspects: geographical distance, number of tasks, number of predecessors, and\nslack time. The test results and their characteristics in regard to the\nproposed methodology are discussed and presented.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 13:11:39 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Khosiawan", "Yohanes", ""], ["Nielsen", "Izabela", ""]]}, {"id": "1706.09767", "submitter": "Ismail Shahin", "authors": "Ismail Shahin", "title": "Speaker Identification in each of the Neutral and Shouted Talking\n  Environments based on Gender-Dependent Approach Using SPHMMs", "comments": null, "journal-ref": null, "doi": "10.2316/Journal.202.2011.1.202-3019", "report-no": null, "categories": "cs.AI cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is well known that speaker identification performs extremely well in the\nneutral talking environments; however, the identification performance is\ndeclined sharply in the shouted talking environments. This work aims at\nproposing, implementing and testing a new approach to enhance the declined\nperformance in the shouted talking environments. The new proposed approach is\nbased on gender-dependent speaker identification using Suprasegmental Hidden\nMarkov Models (SPHMMs) as classifiers. This proposed approach has been tested\non two different and separate speech databases: our collected database and the\nSpeech Under Simulated and Actual Stress (SUSAS) database. The results of this\nwork show that gender-dependent speaker identification based on SPHMMs\noutperforms gender-independent speaker identification based on the same models\nand gender-dependent speaker identification based on Hidden Markov Models\n(HMMs) by about 6% and 8%, respectively. The results obtained based on the\nproposed approach are close to those obtained in subjective evaluation by human\njudges.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 14:05:48 GMT"}], "update_date": "2017-06-30", "authors_parsed": [["Shahin", "Ismail", ""]]}, {"id": "1706.09838", "submitter": "Sirui Yao", "authors": "Sirui Yao, Bert Huang", "title": "New Fairness Metrics for Recommendation that Embrace Differences", "comments": "Presented as a poster at the 2017 Workshop on Fairness,\n  Accountability, and Transparency in Machine Learning (FAT/ML 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CY cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study fairness in collaborative-filtering recommender systems, which are\nsensitive to discrimination that exists in historical data. Biased data can\nlead collaborative filtering methods to make unfair predictions against\nminority groups of users. We identify the insufficiency of existing fairness\nmetrics and propose four new metrics that address different forms of\nunfairness. These fairness metrics can be optimized by adding fairness terms to\nthe learning objective. Experiments on synthetic and real data show that our\nnew metrics can better measure fairness than the baseline, and that the\nfairness objectives effectively help reduce unfairness.\n", "versions": [{"version": "v1", "created": "Thu, 29 Jun 2017 16:32:12 GMT"}, {"version": "v2", "created": "Wed, 13 Dec 2017 19:00:37 GMT"}], "update_date": "2017-12-15", "authors_parsed": [["Yao", "Sirui", ""], ["Huang", "Bert", ""]]}, {"id": "1706.10036", "submitter": "Xingjun Ma", "authors": "Xingjun Ma, Sudanthi Wijewickrema, Yun Zhou, Shuo Zhou, Stephen\n  O'Leary, James Bailey", "title": "Providing Effective Real-time Feedback in Simulation-based Surgical\n  Training", "comments": "To appear in Proceedings of the 20th International Conference on\n  Medical Image Computing and Computer Assisted Intervention (MICCAI), Quebec\n  City, Canada, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Virtual reality simulation is becoming popular as a training platform in\nsurgical education. However, one important aspect of simulation-based surgical\ntraining that has not received much attention is the provision of automated\nreal-time performance feedback to support the learning process. Performance\nfeedback is actionable advice that improves novice behaviour. In simulation,\nautomated feedback is typically extracted from prediction models trained using\ndata mining techniques. Existing techniques suffer from either low\neffectiveness or low efficiency resulting in their inability to be used in\nreal-time. In this paper, we propose a random forest based method that finds a\nbalance between effectiveness and efficiency. Experimental results in a\ntemporal bone surgery simulation show that the proposed method is able to\nextract highly effective feedback at a high level of efficiency.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 06:36:14 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Ma", "Xingjun", ""], ["Wijewickrema", "Sudanthi", ""], ["Zhou", "Yun", ""], ["Zhou", "Shuo", ""], ["O'Leary", "Stephen", ""], ["Bailey", "James", ""]]}, {"id": "1706.10059", "submitter": "Jinjun Liang", "authors": "Zhengyao Jiang, Dixing Xu, Jinjun Liang", "title": "A Deep Reinforcement Learning Framework for the Financial Portfolio\n  Management Problem", "comments": "30 pages, 5 figures, submitting to JMLR", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.CP cs.AI q-fin.PM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Financial portfolio management is the process of constant redistribution of a\nfund into different financial products. This paper presents a\nfinancial-model-free Reinforcement Learning framework to provide a deep machine\nlearning solution to the portfolio management problem. The framework consists\nof the Ensemble of Identical Independent Evaluators (EIIE) topology, a\nPortfolio-Vector Memory (PVM), an Online Stochastic Batch Learning (OSBL)\nscheme, and a fully exploiting and explicit reward function. This framework is\nrealized in three instants in this work with a Convolutional Neural Network\n(CNN), a basic Recurrent Neural Network (RNN), and a Long Short-Term Memory\n(LSTM). They are, along with a number of recently reviewed or published\nportfolio-selection strategies, examined in three back-test experiments with a\ntrading period of 30 minutes in a cryptocurrency market. Cryptocurrencies are\nelectronic and decentralized alternatives to government-issued money, with\nBitcoin as the best-known example of a cryptocurrency. All three instances of\nthe framework monopolize the top three positions in all experiments,\noutdistancing other compared trading algorithms. Although with a high\ncommission rate of 0.25% in the backtests, the framework is able to achieve at\nleast 4-fold returns in 50 days.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 08:31:28 GMT"}, {"version": "v2", "created": "Sun, 16 Jul 2017 10:29:38 GMT"}], "update_date": "2017-07-18", "authors_parsed": [["Jiang", "Zhengyao", ""], ["Xu", "Dixing", ""], ["Liang", "Jinjun", ""]]}, {"id": "1706.10102", "submitter": "Peter Baumgartner", "authors": "Peter Baumgartner, Sylvie Thi\\'ebaux, Felipe Trevizan", "title": "Tableaux for Policy Synthesis for MDPs with PCTL* Constraints", "comments": "This is a long version of a conference paper published at TABLEAUX\n  2017. It contains proofs of the main results and fixes a bug. See the\n  footnote on page 1 for details", "journal-ref": "Proceedings of TABLEAUX 2017, pp. 175--192, Springer 2017", "doi": "10.1007/978-3-319-66902-1_11", "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov decision processes (MDPs) are the standard formalism for modelling\nsequential decision making in stochastic environments. Policy synthesis\naddresses the problem of how to control or limit the decisions an agent makes\nso that a given specification is met. In this paper we consider PCTL*, the\nprobabilistic counterpart of CTL*, as the specification language. Because in\ngeneral the policy synthesis problem for PCTL* is undecidable, we restrict to\npolicies whose execution history memory is finitely bounded a priori.\n  Surprisingly, no algorithm for policy synthesis for this natural and\nexpressive framework has been developed so far. We close this gap and describe\na tableau-based algorithm that, given an MDP and a PCTL* specification, derives\nin a non-deterministic way a system of (possibly nonlinear) equalities and\ninequalities. The solutions of this system, if any, describe the desired\n(stochastic) policies.\n  Our main result in this paper is the correctness of our method, i.e.,\nsoundness, completeness and termination.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 10:13:26 GMT"}, {"version": "v2", "created": "Sat, 22 Jul 2017 14:02:17 GMT"}, {"version": "v3", "created": "Fri, 6 Oct 2017 01:04:47 GMT"}], "update_date": "2017-10-09", "authors_parsed": [["Baumgartner", "Peter", ""], ["Thi\u00e9baux", "Sylvie", ""], ["Trevizan", "Felipe", ""]]}, {"id": "1706.10117", "submitter": "Mieczys{\\l}aw K{\\l}opotek", "authors": "Mieczys{\\l}aw A. K{\\l}opotek", "title": "Restricted Causal Inference Algorithm", "comments": "M.A. K{\\l}opotek: Restricted Causal Inference Algorithm. [in:] B.\n  Pehrson, I. Simon Eds.: Proc. World Computer Congress of IFIP . Hamburg 28\n  August - 2 September 1994, Vol.1, Elsevier Scientific Publishers\n  (North-Holland), Amsterdam, pp. 342-347", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a new algorithm for recovery of belief network structure\nfrom data handling hidden variables. It consists essentially in an extension of\nthe CI algorithm of Spirtes et al. by restricting the number of conditional\ndependencies checked up to k variables and in an extension of the original CI\nby additional steps transforming so called partial including path graph into a\nbelief network. Its correctness is demonstrated.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 10:57:53 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["K\u0142opotek", "Mieczys\u0142aw A.", ""]]}, {"id": "1706.10151", "submitter": "Luca Buoncompagni", "authors": "Luca Buoncompagni, Alessio Capitanelli, Fulvio Mastrogiovanni", "title": "A ROS multi-ontology references services: OWL reasoners and application\n  prototyping issues", "comments": "Presented to the IEEE RO-MAN 2017 Workshop on \"Autonomous Robot\n  Ontology\", Lisbon Portugal, August 28, 2017. Published in: Proceedings of the\n  5th Italian Workshop on Artificial Intelligence and Robotics (AIRO) A\n  workshop of the XVII International Conference of the Italian Association for\n  Artificial Intelligence (AIXIA), Trento, Italy (2018). CEUR-WS, Vol-2352,\n  pages 36-41", "journal-ref": "In proceedings of AIRO@AIXIA 2018, Trento, Italy\n  (http://ceur-ws.org/Vol-2352/short7.pdf)", "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper introduces a ROS Multi Ontology References (ARMOR) service, a\ngeneral-purpose and scalable interface between robot architectures and OWL\nreasoners. ARMOR addresses synchronisation and communication issues among\nheterogeneous and distributed software components. As a guiding scenario, we\nconsider a prototyping approach for the use of symbolic reasoning in\nhuman-robot interaction applications.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 11:47:39 GMT"}, {"version": "v2", "created": "Mon, 13 May 2019 12:30:44 GMT"}], "update_date": "2019-05-14", "authors_parsed": [["Buoncompagni", "Luca", ""], ["Capitanelli", "Alessio", ""], ["Mastrogiovanni", "Fulvio", ""]]}, {"id": "1706.10177", "submitter": "Alessia Amelio Dr.", "authors": "Darko Brodi\\'c, Alessia Amelio, Ivo R. Draganov", "title": "Statistical Analysis of Dice CAPTCHA Usability", "comments": "9 pages, 5 figures, 52nd International Scientific Conference on\n  Information, Communication and Energy Systems and Technologies (ICEST 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.HC cs.AI cs.CR cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper the elements of the CAPTCHA usability are analyzed. CAPTCHA, as\na time progressive element in computer science, has been under constant\ninterest of ordinary, professional as well as the scientific users of the\nInternet. The analysis is given based on the usability elements of CAPTCHA\nwhich are abbreviated as user-centric approach to the CAPTCHA. To demonstrate\nit, the specific type of Dice CAPTCHA is used in the experiment. The experiment\nis conducted on 190 Internet users with different demographic characteristics\non laptop and tablet computers. The obtained results are statistically\nprocessed. At the end, the results are compared and conclusion of their use is\ndrawn.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 13:00:25 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Brodi\u0107", "Darko", ""], ["Amelio", "Alessia", ""], ["Draganov", "Ivo R.", ""]]}, {"id": "1706.10188", "submitter": "Siwar Jendoubi", "authors": "Siwar Jendoubi and Arnaud Martin", "title": "A reliability-based approach for influence maximization using the\n  evidence theory", "comments": "14 pages, 8 figures, DaWak 2017 conference", "journal-ref": null, "doi": "10.1007/978-3-319-64283-3_23", "report-no": null, "categories": "cs.SI cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  The influence maximization is the problem of finding a set of social network\nusers, called influencers, that can trigger a large cascade of propagation.\nInfluencers are very beneficial to make a marketing campaign goes viral through\nsocial networks for example. In this paper, we propose an influence measure\nthat combines many influence indicators. Besides, we consider the reliability\nof each influence indicator and we present a distance-based process that allows\nto estimate the reliability of each indicator. The proposed measure is defined\nunder the framework of the theory of belief functions. Furthermore, the\nreliability-based influence measure is used with an influence maximization\nmodel to select a set of users that are able to maximize the influence in the\nnetwork. Finally, we present a set of experiments on a dataset collected from\nTwitter. These experiments show the performance of the proposed solution in\ndetecting social influencers with good quality.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 13:23:56 GMT"}], "update_date": "2017-08-29", "authors_parsed": [["Jendoubi", "Siwar", ""], ["Martin", "Arnaud", ""]]}, {"id": "1706.10234", "submitter": "Paul Rubenstein", "authors": "Paul K. Rubenstein, Ilya Tolstikhin, Philipp Hennig, Bernhard\n  Schoelkopf", "title": "Probabilistic Active Learning of Functions in Structural Causal Models", "comments": "9 pages main text + 4 pages supplement", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning the functions computing children from\nparents in a Structural Causal Model once the underlying causal graph has been\nidentified. This is in some sense the second step after causal discovery.\nTaking a probabilistic approach to estimating these functions, we derive a\nnatural myopic active learning scheme that identifies the intervention which is\noptimally informative about all of the unknown functions jointly, given\npreviously observed data. We test the derived algorithms on simple examples, to\ndemonstrate that they produce a structured exploration policy that\nsignificantly improves on unstructured base-lines.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 15:06:57 GMT"}], "update_date": "2017-07-03", "authors_parsed": [["Rubenstein", "Paul K.", ""], ["Tolstikhin", "Ilya", ""], ["Hennig", "Philipp", ""], ["Schoelkopf", "Bernhard", ""]]}, {"id": "1706.10239", "submitter": "Lei Wu", "authors": "Lei Wu, Zhanxing Zhu, Weinan E", "title": "Towards Understanding Generalization of Deep Learning: Perspective of\n  Loss Landscapes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  It is widely observed that deep learning models with learned parameters\ngeneralize well, even with much more model parameters than the number of\ntraining samples. We systematically investigate the underlying reasons why deep\nneural networks often generalize well, and reveal the difference between the\nminima (with the same training error) that generalize well and those they\ndon't. We show that it is the characteristics the landscape of the loss\nfunction that explains the good generalization capability. For the landscape of\nloss function for deep networks, the volume of basin of attraction of good\nminima dominates over that of poor minima, which guarantees optimization\nmethods with random initialization to converge to good minima. We theoretically\njustify our findings through analyzing 2-layer neural networks; and show that\nthe low-complexity solutions have a small norm of Hessian matrix with respect\nto model parameters. For deeper networks, extensive numerical evidence helps to\nsupport our arguments.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 15:30:21 GMT"}, {"version": "v2", "created": "Tue, 28 Nov 2017 02:40:04 GMT"}], "update_date": "2017-11-29", "authors_parsed": [["Wu", "Lei", ""], ["Zhu", "Zhanxing", ""], ["E", "Weinan", ""]]}, {"id": "1706.10240", "submitter": "Ahmadreza Ahmadi", "authors": "Ahmadreza Ahmadi and Jun Tani", "title": "Bridging the Gap between Probabilistic and Deterministic Models: A\n  Simulation Study on a Variational Bayes Predictive Coding Recurrent Neural\n  Network Model", "comments": "This paper is accepted the 24th International Conference On Neural\n  Information Processing (ICONIP 2017). The previous submission to arXiv is\n  replaced by this version because there was an error in Equation 8", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current paper proposes a novel variational Bayes predictive coding RNN\nmodel, which can learn to generate fluctuated temporal patterns from exemplars.\nThe model learns to maximize the lower bound of the weighted sum of the\nregularization and reconstruction error terms. We examined how this weighting\ncan affect development of different types of information processing while\nlearning fluctuated temporal patterns. Simulation results show that strong\nweighting of the reconstruction term causes the development of deterministic\nchaos for imitating the randomness observed in target sequences, while strong\nweighting of the regularization term causes the development of stochastic\ndynamics imitating probabilistic processes observed in targets. Moreover,\nresults indicate that the most generalized learning emerges between these two\nextremes. The paper concludes with implications in terms of the underlying\nneuronal mechanisms for autism spectrum disorder and for free action.\n", "versions": [{"version": "v1", "created": "Fri, 30 Jun 2017 15:31:17 GMT"}, {"version": "v2", "created": "Fri, 15 Sep 2017 14:35:31 GMT"}], "update_date": "2017-09-18", "authors_parsed": [["Ahmadi", "Ahmadreza", ""], ["Tani", "Jun", ""]]}]