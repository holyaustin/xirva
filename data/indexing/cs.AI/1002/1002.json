[{"id": "1002.0102", "submitter": "Florentin Smarandache", "authors": "Florentin Smarandache", "title": "$\\alpha$-Discounting Multi-Criteria Decision Making ($\\alpha$-D MCDM)", "comments": "62 pages", "journal-ref": "Proceedings of Fusion 2010 International Conference, Edinburgh,\n  Scotland, 26-29 July, 2010", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this book we introduce a new procedure called \\alpha-Discounting Method\nfor Multi-Criteria Decision Making (\\alpha-D MCDM), which is as an alternative\nand extension of Saaty Analytical Hierarchy Process (AHP). It works for any\nnumber of preferences that can be transformed into a system of homogeneous\nlinear equations. A degree of consistency (and implicitly a degree of\ninconsistency) of a decision-making problem are defined. \\alpha-D MCDM is\nafterwards generalized to a set of preferences that can be transformed into a\nsystem of linear and or non-linear homogeneous and or non-homogeneous equations\nand or inequalities. The general idea of \\alpha-D MCDM is to assign non-null\npositive parameters \\alpha_1, \\alpha_2, and so on \\alpha_p to the coefficients\nin the right-hand side of each preference that diminish or increase them in\norder to transform the above linear homogeneous system of equations which has\nonly the null-solution, into a system having a particular non-null solution.\nAfter finding the general solution of this system, the principles used to\nassign particular values to all parameters \\alpha is the second important part\nof \\alpha-D, yet to be deeper investigated in the future. In the current book\nwe propose the Fairness Principle, i.e. each coefficient should be discounted\nwith the same percentage (we think this is fair: not making any favoritism or\nunfairness to any coefficient), but the reader can propose other principles.\nFor consistent decision-making problems with pairwise comparisons,\n\\alpha-Discounting Method together with the Fairness Principle give the same\nresult as AHP. But for weak inconsistent decision-making problem,\n\\alpha-Discounting together with the Fairness Principle give a different result\nfrom AHP. Many consistent, weak inconsistent, and strong inconsistent examples\nare given in this book.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2010 02:38:07 GMT"}, {"version": "v2", "created": "Wed, 3 Feb 2010 03:44:43 GMT"}, {"version": "v3", "created": "Wed, 10 Feb 2010 18:58:01 GMT"}, {"version": "v4", "created": "Fri, 2 Dec 2011 20:08:40 GMT"}, {"version": "v5", "created": "Fri, 2 Oct 2015 19:09:38 GMT"}], "update_date": "2015-10-05", "authors_parsed": [["Smarandache", "Florentin", ""]]}, {"id": "1002.0108", "submitter": "Petr Kub\\'anek", "authors": "Petr Kubanek", "title": "Genetic algorithm for robotic telescope scheduling", "comments": "38 pages; master thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI astro-ph.IM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work was inspired by author experiences with a telescope scheduling.\nAuthor long time goal is to develop and further extend software for an\nautonomous observatory. The software shall provide users with all the\nfacilities they need to take scientific images of the night sky, cooperate with\nother autonomous observatories, and possibly more. This works shows how genetic\nalgorithm can be used for scheduling of a single observatory, as well as\nnetwork of observatories.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2010 05:19:58 GMT"}], "update_date": "2010-02-02", "authors_parsed": [["Kubanek", "Petr", ""]]}, {"id": "1002.0134", "submitter": "Lars Kotthoff", "authors": "Lars Kotthoff", "title": "Constraint solvers: An empirical evaluation of design decisions", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PF", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents an evaluation of the design decisions made in four\nstate-of-the-art constraint solvers; Choco, ECLiPSe, Gecode, and Minion. To\nassess the impact of design decisions, instances of the five problem classes\nn-Queens, Golomb Ruler, Magic Square, Social Golfers, and Balanced Incomplete\nBlock Design are modelled and solved with each solver. The results of the\nexperiments are not meant to give an indication of the performance of a solver,\nbut rather investigate what influence the choice of algorithms and data\nstructures has.\n  The analysis of the impact of the design decisions focuses on the different\nways of memory management, behaviour with increasing problem size, and\nspecialised algorithms for specific types of variables. It also briefly\nconsiders other, less significant decisions.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2010 15:36:36 GMT"}], "update_date": "2010-02-02", "authors_parsed": [["Kotthoff", "Lars", ""]]}, {"id": "1002.0136", "submitter": "Lars Kotthoff", "authors": "Lars Kotthoff", "title": "Dominion -- A constraint solver generator", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a design for a system to generate constraint solvers that\nare specialised for specific problem models. It describes the design in detail\nand gives preliminary experimental results showing the feasibility and\neffectiveness of the approach.\n", "versions": [{"version": "v1", "created": "Sun, 31 Jan 2010 15:46:56 GMT"}], "update_date": "2010-02-02", "authors_parsed": [["Kotthoff", "Lars", ""]]}, {"id": "1002.0177", "submitter": "Chinmayananda Padhy Mr", "authors": "C.N. Padhy, R.R. Panda", "title": "Logical Evaluation of Consciousness: For Incorporating Consciousness\n  into Machine Architecture", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Machine Consciousness is the study of consciousness in a biological,\nphilosophical, mathematical and physical perspective and designing a model that\ncan fit into a programmable system architecture. Prime objective of the study\nis to make the system architecture behave consciously like a biological model\ndoes. Present work has developed a feasible definition of consciousness, that\ncharacterizes consciousness with four parameters i.e., parasitic, symbiotic,\nself referral and reproduction. Present work has also developed a biologically\ninspired consciousness architecture that has following layers: quantum layer,\ncellular layer, organ layer and behavioral layer and traced the characteristics\nof consciousness at each layer. Finally, the work has estimated physical and\nalgorithmic architecture to devise a system that can behave consciously.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2010 04:07:34 GMT"}], "update_date": "2010-02-02", "authors_parsed": [["Padhy", "C. N.", ""], ["Panda", "R. R.", ""]]}, {"id": "1002.0184", "submitter": "Emanuel Diamant", "authors": "Emanuel Diamant", "title": "Some considerations on how the human brain must be arranged in order to\n  make its replication in a thinking machine possible", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI q-bio.NC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  For the most of my life, I have earned my living as a computer vision\nprofessional busy with image processing tasks and problems. In the computer\nvision community there is a widespread belief that artificial vision systems\nfaithfully replicate human vision abilities or at least very closely mimic\nthem. It was a great surprise to me when one day I have realized that computer\nand human vision have next to nothing in common. The former is occupied with\nextensive data processing, carrying out massive pixel-based calculations, while\nthe latter is busy with meaningful information processing, concerned with smart\nobjects-based manipulations. And the gap between the two is insurmountable. To\nresolve this confusion, I had had to return and revaluate first the vision\nphenomenon itself, define more carefully what visual information is and how to\ntreat it properly. In this work I have not been, as it is usually accepted,\nbiologically inspired . On the contrary, I have drawn my inspirations from a\npure mathematical theory, the Kolmogorov s complexity theory. The results of my\nwork have been already published elsewhere. So the objective of this paper is\nto try and apply the insights gained in course of this my enterprise to a more\ngeneral case of information processing in human brain and the challenging issue\nof human intelligence.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2010 06:05:12 GMT"}], "update_date": "2010-02-02", "authors_parsed": [["Diamant", "Emanuel", ""]]}, {"id": "1002.0276", "submitter": "Uwe Aickelin", "authors": "Julie Greensmith and Uwe Aickelin", "title": "Dendritic Cells for SYN Scan Detection", "comments": "8 Pages, 9 Figures, Genetic and Evolutionary Computation Conference\n  (GECCO 2007)", "journal-ref": "Proceedings of the Genetic and Evolutionary Computation Conference\n  (GECCO 2007)", "doi": null, "report-no": null, "categories": "cs.AI cs.CR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Artificial immune systems have previously been applied to the problem of\nintrusion detection. The aim of this research is to develop an intrusion\ndetection system based on the function of Dendritic Cells (DCs). DCs are\nantigen presenting cells and key to activation of the human immune system,\nbehaviour which has been abstracted to form the Dendritic Cell Algorithm (DCA).\nIn algorithmic terms, individual DCs perform multi-sensor data fusion,\nasynchronously correlating the the fused data signals with a secondary data\nstream. Aggregate output of a population of cells, is analysed and forms the\nbasis of an anomaly detection system. In this paper the DCA is applied to the\ndetection of outgoing port scans using TCP SYN packets. Results show that\ndetection can be achieved with the DCA, yet some false positives can be\nencountered when simultaneously scanning and using other network services.\nSuggestions are made for using adaptive signals to alleviate this uncovered\nproblem.\n", "versions": [{"version": "v1", "created": "Mon, 1 Feb 2010 15:53:04 GMT"}], "update_date": "2010-07-05", "authors_parsed": [["Greensmith", "Julie", ""], ["Aickelin", "Uwe", ""]]}, {"id": "1002.0378", "submitter": "Jinzhong Niu", "authors": "Jinzhong Niu, Kai Cai, and Simon Parsons", "title": "A Grey-Box Approach to Automated Mechanism Design", "comments": "18 pages, 2 figures, 2 tables, and 1 algorithm. Extended abstract to\n  appear in the proceedings of AAMAS'2010", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Auctions play an important role in electronic commerce, and have been used to\nsolve problems in distributed computing. Automated approaches to designing\neffective auction mechanisms are helpful in reducing the burden of traditional\ngame theoretic, analytic approaches and in searching through the large space of\npossible auction mechanisms. This paper presents an approach to automated\nmechanism design (AMD) in the domain of double auctions. We describe a novel\nparametrized space of double auctions, and then introduce an evolutionary\nsearch method that searches this space of parameters. The approach evaluates\nauction mechanisms using the framework of the TAC Market Design Game and\nrelates the performance of the markets in that game to their constituent parts\nusing reinforcement learning. Experiments show that the strongest mechanisms we\nfound using this approach not only win the Market Design Game against known,\nstrong opponents, but also exhibit desirable economic properties when they run\nin isolation.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2010 01:53:55 GMT"}, {"version": "v2", "created": "Mon, 8 Feb 2010 15:50:13 GMT"}], "update_date": "2010-02-08", "authors_parsed": [["Niu", "Jinzhong", ""], ["Cai", "Kai", ""], ["Parsons", "Simon", ""]]}, {"id": "1002.0382", "submitter": "Dakshina Ranjan Kisku", "authors": "Dakshina Ranjan Kisku, Massimo Tistarelli, Jamuna Kanta Sing, Phalguni\n  Gupta", "title": "Face Recognition by Fusion of Local and Global Matching Scores using DS\n  Theory: An Evaluation with Uni-classifier and Multi-classifier Paradigm", "comments": "7 pages, 6 figures, IEEE Computer Vision and Pattern Recognition\n  Workshop on Biometrics", "journal-ref": null, "doi": "10.1109/CVPRW.2009.5204298", "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Faces are highly deformable objects which may easily change their appearance\nover time. Not all face areas are subject to the same variability. Therefore\ndecoupling the information from independent areas of the face is of paramount\nimportance to improve the robustness of any face recognition technique. This\npaper presents a robust face recognition technique based on the extraction and\nmatching of SIFT features related to independent face areas. Both a global and\nlocal (as recognition from parts) matching strategy is proposed. The local\nstrategy is based on matching individual salient facial SIFT features as\nconnected to facial landmarks such as the eyes and the mouth. As for the global\nmatching strategy, all SIFT features are combined together to form a single\nfeature. In order to reduce the identification errors, the Dempster-Shafer\ndecision theory is applied to fuse the two matching techniques. The proposed\nalgorithms are evaluated with the ORL and the IITK face databases. The\nexperimental results demonstrate the effectiveness and potential of the\nproposed face recognition techniques also in the case of partially occluded\nfaces or with missing information.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2010 02:22:58 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Kisku", "Dakshina Ranjan", ""], ["Tistarelli", "Massimo", ""], ["Sing", "Jamuna Kanta", ""], ["Gupta", "Phalguni", ""]]}, {"id": "1002.0411", "submitter": "Dakshina Ranjan Kisku", "authors": "Dakshina Ranjan Kisku, Ajita Rattani, Enrico Grosso, Massimo\n  Tistarelli", "title": "Face Identification by SIFT-based Complete Graph Topology", "comments": "6 pages, 7 figures, AutoId 2007", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This paper presents a new face identification system based on Graph Matching\nTechnique on SIFT features extracted from face images. Although SIFT features\nhave been successfully used for general object detection and recognition, only\nrecently they were applied to face recognition. This paper further investigates\nthe performance of identification techniques based on Graph matching topology\ndrawn on SIFT features which are invariant to rotation, scaling and\ntranslation. Face projections on images, represented by a graph, can be matched\nonto new images by maximizing a similarity function taking into account spatial\ndistortions and the similarities of the local features. Two graph based\nmatching techniques have been investigated to deal with false pair assignment\nand reducing the number of features to find the optimal feature set between\ndatabase and query face SIFT features. The experimental results, performed on\nthe BANCA database, demonstrate the effectiveness of the proposed system for\nautomatic face identification.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2010 08:00:33 GMT"}], "update_date": "2010-02-03", "authors_parsed": [["Kisku", "Dakshina Ranjan", ""], ["Rattani", "Ajita", ""], ["Grosso", "Enrico", ""], ["Tistarelli", "Massimo", ""]]}, {"id": "1002.0412", "submitter": "Dakshina Ranjan Kisku", "authors": "Dakshina Ranjan Kisku, Hunny Mehrotra, Phalguni Gupta, and Jamuna\n  Kanta Sing", "title": "SIFT-based Ear Recognition by Fusion of Detected Keypoints from Color\n  Similarity Slice Regions", "comments": "6 pages, 4 figures, ACTEA 2009", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Ear biometric is considered as one of the most reliable and invariant\nbiometrics characteristics in line with iris and fingerprint characteristics.\nIn many cases, ear biometrics can be compared with face biometrics regarding\nmany physiological and texture characteristics. In this paper, a robust and\nefficient ear recognition system is presented, which uses Scale Invariant\nFeature Transform (SIFT) as feature descriptor for structural representation of\near images. In order to make it more robust to user authentication, only the\nregions having color probabilities in a certain ranges are considered for\ninvariant SIFT feature extraction, where the K-L divergence is used for keeping\ncolor consistency. Ear skin color model is formed by Gaussian mixture model and\nclustering the ear color pattern using vector quantization. Finally, K-L\ndivergence is applied to the GMM framework for recording the color similarity\nin the specified ranges by comparing color similarity between a pair of\nreference model and probe ear images. After segmentation of ear images in some\ncolor slice regions, SIFT keypoints are extracted and an augmented vector of\nextracted SIFT features are created for matching, which is accomplished between\na pair of reference model and probe ear images. The proposed technique has been\ntested on the IITK Ear database and the experimental results show improvements\nin recognition accuracy while invariant features are extracted from color slice\nregions to maintain the robustness of the system.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2010 08:06:04 GMT"}], "update_date": "2010-02-03", "authors_parsed": [["Kisku", "Dakshina Ranjan", ""], ["Mehrotra", "Hunny", ""], ["Gupta", "Phalguni", ""], ["Sing", "Jamuna Kanta", ""]]}, {"id": "1002.0414", "submitter": "Dakshina Ranjan Kisku", "authors": "Dakshina Ranjan Kisku, Phalguni Gupta, Jamuna Kanta Sing", "title": "Feature Level Fusion of Biometrics Cues: Human Identification with\n  Doddingtons Caricature", "comments": "8 pages, 3 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  This paper presents a multimodal biometric system of fingerprint and ear\nbiometrics. Scale Invariant Feature Transform (SIFT) descriptor based feature\nsets extracted from fingerprint and ear are fused. The fused set is encoded by\nK-medoids partitioning approach with less number of feature points in the set.\nK-medoids partition the whole dataset into clusters to minimize the error\nbetween data points belonging to the clusters and its center. Reduced feature\nset is used to match between two biometric sets. Matching scores are generated\nusing wolf-lamb user-dependent feature weighting scheme introduced by\nDoddington. The technique is tested to exhibit its robust performance.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2010 08:12:23 GMT"}], "update_date": "2010-02-03", "authors_parsed": [["Kisku", "Dakshina Ranjan", ""], ["Gupta", "Phalguni", ""], ["Sing", "Jamuna Kanta", ""]]}, {"id": "1002.0432", "submitter": "Uwe Aickelin", "authors": "William O. Wilson, Jan Feyereisl, Uwe Aickelin", "title": "Detecting Motifs in System Call Sequences", "comments": "16 pages, 3 tables, 1 figure, 8th International Workshop on\n  Information Security Applications (WISA2007), Lecture Notes in Computer\n  Science, Jeju, Korea", "journal-ref": "Proceedings of the 8th International Workshop on Information\n  Security Applications (WISA2007), Lecture Notes in Computer Science, Jeju,\n  Korea", "doi": null, "report-no": null, "categories": "cs.AI cs.CR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The search for patterns or motifs in data represents an area of key interest\nto many researchers. In this paper we present the Motif Tracking Algorithm, a\nnovel immune inspired pattern identification tool that is able to identify\nunknown motifs which repeat within time series data. The power of the algorithm\nis derived from its use of a small number of parameters with minimal\nassumptions. The algorithm searches from a completely neutral perspective that\nis independent of the data being analysed, and the underlying motifs. In this\npaper the motif tracking algorithm is applied to the search for patterns within\nsequences of low level system calls between the Linux kernel and the operating\nsystem's user space. The MTA is able to compress data found in large system\ncall data sets to a limited number of motifs which summarise that data. The\nmotifs provide a resource from which a profile of executed processes can be\nbuilt. The potential for these profiles and new implications for security\nresearch are highlighted. A higher level call system language for measuring\nsimilarity between patterns of such calls is also suggested.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2010 09:30:31 GMT"}], "update_date": "2010-07-05", "authors_parsed": [["Wilson", "William O.", ""], ["Feyereisl", "Jan", ""], ["Aickelin", "Uwe", ""]]}, {"id": "1002.0449", "submitter": "Ping Zhu", "authors": "Ping Zhu and Qiaoyan Wen", "title": "Some improved results on communication between information systems", "comments": "12 pages", "journal-ref": "Information Sciences, 180(18): 3521-3531, 2010", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To study the communication between information systems, Wang et al. [C. Wang,\nC. Wu, D. Chen, Q. Hu, and C. Wu, Communicating between information systems,\nInformation Sciences 178 (2008) 3228-3239] proposed two concepts of type-1 and\ntype-2 consistent functions. Some properties of such functions and induced\nrelation mappings have been investigated there. In this paper, we provide an\nimprovement of the aforementioned work by disclosing the symmetric relationship\nbetween type-1 and type-2 consistent functions. We present more properties of\nconsistent functions and induced relation mappings and improve upon several\ndeficient assertions in the original work. In particular, we unify and extend\ntype-1 and type-2 consistent functions into the so-called\nneighborhood-consistent functions. This provides a convenient means for\nstudying the communication between information systems based on various\nneighborhoods.\n", "versions": [{"version": "v1", "created": "Tue, 2 Feb 2010 10:54:30 GMT"}, {"version": "v2", "created": "Thu, 8 Jul 2010 09:34:19 GMT"}], "update_date": "2015-03-13", "authors_parsed": [["Zhu", "Ping", ""], ["Wen", "Qiaoyan", ""]]}, {"id": "1002.0696", "submitter": "Uwe Aickelin", "authors": "Julie Greensmith, Uwe Aickelin, Jamie Twycross", "title": "Detecting Danger: Applying a Novel Immunological Concept to Intrusion\n  Detection Systems", "comments": "3 pages, The 6th International Conference in Adaptive Computing in\n  Design and Manufacture (ACDM2004), Bristol, UK", "journal-ref": "Proceedings of The 6th International Conference in Adaptive\n  Computing in Design and Manufacture (ACDM2004), Bristol, UK", "doi": null, "report-no": null, "categories": "cs.AI cs.CR cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years computer systems have become increasingly complex and\nconsequently the challenge of protecting these systems has become increasingly\ndifficult. Various techniques have been implemented to counteract the misuse of\ncomputer systems in the form of firewalls, anti-virus software and intrusion\ndetection systems. The complexity of networks and dynamic nature of computer\nsystems leaves current methods with significant room for improvement. Computer\nscientists have recently drawn inspiration from mechanisms found in biological\nsystems and, in the context of computer security, have focused on the human\nimmune system (HIS). The human immune system provides a high level of\nprotection from constant attacks. By examining the precise mechanisms of the\nhuman immune system, it is hoped the paradigm will improve the performance of\nreal intrusion detection systems. This paper presents an introduction to recent\ndevelopments in the field of immunology. It discusses the incorporation of a\nnovel immunological paradigm, Danger Theory, and how this concept is inspiring\nartificial immune systems (AIS). Applications within the context of computer\nsecurity are outlined drawing direct reference to the underlying principles of\nDanger Theory and finally, the current state of intrusion detection systems is\ndiscussed and improvements suggested.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2010 10:35:03 GMT"}], "update_date": "2010-07-05", "authors_parsed": [["Greensmith", "Julie", ""], ["Aickelin", "Uwe", ""], ["Twycross", "Jamie", ""]]}, {"id": "1002.0745", "submitter": "Mahamed Omran PhD", "authors": "Mahamed G. H. Omran and Faisal al-Adwani", "title": "Using CODEQ to Train Feed-forward Neural Networks", "comments": "8 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CODEQ is a new, population-based meta-heuristic algorithm that is a hybrid of\nconcepts from chaotic search, opposition-based learning, differential evolution\nand quantum mechanics. CODEQ has successfully been used to solve different\ntypes of problems (e.g. constrained, integer-programming, engineering) with\nexcellent results. In this paper, CODEQ is used to train feed-forward neural\nnetworks. The proposed method is compared with particle swarm optimization and\ndifferential evolution algorithms on three data sets with encouraging results.\n", "versions": [{"version": "v1", "created": "Wed, 3 Feb 2010 14:02:39 GMT"}], "update_date": "2010-02-04", "authors_parsed": [["Omran", "Mahamed G. H.", ""], ["al-Adwani", "Faisal", ""]]}, {"id": "1002.0908", "submitter": "Ping Zhu", "authors": "Ping Zhu and Qiaoyan Wen", "title": "Homomorphisms between fuzzy information systems revisited", "comments": "10 pages", "journal-ref": "Applied Mathematics Letters, 24(9): 1548-1553, 2011", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, Wang et al. discussed the properties of fuzzy information systems\nunder homomorphisms in the paper [C. Wang, D. Chen, L. Zhu, Homomorphisms\nbetween fuzzy information systems, Applied Mathematics Letters 22 (2009)\n1045-1050], where homomorphisms are based upon the concepts of consistent\nfunctions and fuzzy relation mappings. In this paper, we classify consistent\nfunctions as predecessor-consistent and successor-consistent, and then proceed\nto present more properties of consistent functions. In addition, we improve\nsome characterizations of fuzzy relation mappings provided by Wang et al.\n", "versions": [{"version": "v1", "created": "Thu, 4 Feb 2010 07:09:46 GMT"}], "update_date": "2011-11-08", "authors_parsed": [["Zhu", "Ping", ""], ["Wen", "Qiaoyan", ""]]}, {"id": "1002.1157", "submitter": "Vishal Goyal", "authors": "K. Soorya Prakash, S. S. Mohamed Nazirudeen, M. Joseph Malvin Raj", "title": "Establishment of Relationships between Material Design and Product\n  Design Domains by Hybrid FEM-ANN Technique", "comments": "International Journal of Computer Science Issues, IJCSI, Vol. 7,\n  Issue 1, No. 1, January 2010,\n  http://ijcsi.org/articles/Establishment-of-Relationships-between-Material-Design-and-Product-Design-Domains-by-Hybrid-FEM-ANN-Technique.php", "journal-ref": "International Journal of Computer Science Issues, IJCSI, Vol. 7,\n  Issue 1, No. 1, January 2010,\n  http://ijcsi.org/articles/Establishment-of-Relationships-between-Material-Design-and-Product-Design-Domains-by-Hybrid-FEM-ANN-Technique.php", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, research on AI based modeling technique to optimize\ndevelopment of new alloys with necessitated improvements in properties and\nchemical mixture over existing alloys as per functional requirements of product\nis done. The current research work novels AI in lieu of predictions to\nestablish association between material and product customary. Advanced\ncomputational simulation techniques like CFD, FEA interrogations are made\nviable to authenticate product dynamics in context to experimental\ninvestigations. Accordingly, the current research is focused towards binding\nrelationships between material design and product design domains. The input to\nfeed forward back propagation prediction network model constitutes of material\ndesign features. Parameters relevant to product design strategies are furnished\nas target outputs. The outcomes of ANN shows good sign of correlation between\nmaterial and product design domains. The study enriches a new path to\nillustrate material factors at the time of new product development.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2010 09:02:54 GMT"}], "update_date": "2010-02-08", "authors_parsed": [["Prakash", "K. Soorya", ""], ["Nazirudeen", "S. S. Mohamed", ""], ["Raj", "M. Joseph Malvin", ""]]}, {"id": "1002.1200", "submitter": "Uwe Aickelin", "authors": "Yousof Al-Hammadi, Uwe Aickelin", "title": "Detecting Bots Based on Keylogging Activities", "comments": "7 pages, 7 figures, 3rd International Conference on Availability,\n  Reliability and Security (ARES2008)", "journal-ref": "Proceedings of the 3rd International Conference on Availability,\n  Reliability and Security (ARES2008)", "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A bot is a piece of software that is usually installed on an infected machine\nwithout the user's knowledge. A bot is controlled remotely by the attacker\nunder a Command and Control structure. Recent statistics show that bots\nrepresent one of the fastest growing threats to our network by performing\nmalicious activities such as email spamming or keylogging. However, few bot\ndetection techniques have been developed to date. In this paper, we investigate\na behavioural algorithm to detect a single bot that uses keylogging activity.\nOur approach involves the use of function calls analysis for the detection of\nthe bot with a keylogging component. Correlation of the frequency of a\nspecified time-window is performed to enhance he detection scheme. We perform a\nrange of experiments with the spybot. Our results show that there is a high\ncorrelation between some function calls executed by this bot which indicates\nabnormal activity in our system.\n", "versions": [{"version": "v1", "created": "Fri, 5 Feb 2010 11:09:46 GMT"}], "update_date": "2010-07-05", "authors_parsed": [["Al-Hammadi", "Yousof", ""], ["Aickelin", "Uwe", ""]]}, {"id": "1002.1480", "submitter": "Pedro Alejandro Ortega", "authors": "Pedro A. Ortega, Daniel A. Braun", "title": "A Minimum Relative Entropy Controller for Undiscounted Markov Decision\n  Processes", "comments": "8 pages, 3 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Adaptive control problems are notoriously difficult to solve even in the\npresence of plant-specific controllers. One way to by-pass the intractable\ncomputation of the optimal policy is to restate the adaptive control as the\nminimization of the relative entropy of a controller that ignores the true\nplant dynamics from an informed controller. The solution is given by the\nBayesian control rule-a set of equations characterizing a stochastic adaptive\ncontroller for the class of possible plant dynamics. Here, the Bayesian control\nrule is applied to derive BCR-MDP, a controller to solve undiscounted Markov\ndecision processes with finite state and action spaces and unknown dynamics. In\nparticular, we derive a non-parametric conjugate prior distribution over the\npolicy space that encapsulates the agent's whole relevant history and we\npresent a Gibbs sampler to draw random policies from this distribution.\nPreliminary results show that BCR-MDP successfully avoids sub-optimal limit\ncycles due to its built-in mechanism to balance exploration versus\nexploitation.\n", "versions": [{"version": "v1", "created": "Sun, 7 Feb 2010 19:58:46 GMT"}], "update_date": "2010-02-09", "authors_parsed": [["Ortega", "Pedro A.", ""], ["Braun", "Daniel A.", ""]]}, {"id": "1002.2034", "submitter": "Christophe Roche", "authors": "Christophe Roche (LISTIC)", "title": "Dire n'est pas concevoir", "comments": "12 pages", "journal-ref": "Ing\\'enierie des Connaissances, Grenoble : France (2007)", "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The conceptual modelling built from text is rarely an ontology. As a matter\nof fact, such a conceptualization is corpus-dependent and does not offer the\nmain properties we expect from ontology. Furthermore, ontology extracted from\ntext in general does not match ontology defined by expert using a formal\nlanguage. It is not surprising since ontology is an extra-linguistic\nconceptualization whereas knowledge extracted from text is the concern of\ntextual linguistics. Incompleteness of text and using rhetorical figures, like\nellipsis, modify the perception of the conceptualization we may have.\nOntological knowledge, which is necessary for text understanding, is not in\ngeneral embedded into documents.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2010 07:45:57 GMT"}], "update_date": "2010-02-11", "authors_parsed": [["Roche", "Christophe", "", "LISTIC"]]}, {"id": "1002.2202", "submitter": "Rdv Ijcsis", "authors": "Ramesh Kumar Gopala Pillai, Dr. Ramakanth Kumar .P", "title": "Modeling of Human Criminal Behavior using Probabilistic Networks", "comments": "IEEE format, International Journal of Computer Science and\n  Information Security, IJCSIS January 2010, ISSN 1947 5500,\n  http://sites.google.com/site/ijcsis/", "journal-ref": "International Journal of Computer Science and Information\n  Security, IJCSIS, Vol. 7, No. 1, pp. 216-219, January 2010, USA", "doi": null, "report-no": "Journal of Computer Science, ISSN 19475500", "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Currently, criminals profile (CP) is obtained from investigators or forensic\npsychologists interpretation, linking crime scene characteristics and an\noffenders behavior to his or her characteristics and psychological profile.\nThis paper seeks an efficient and systematic discovery of nonobvious and\nvaluable patterns between variables from a large database of solved cases via a\nprobabilistic network (PN) modeling approach. The PN structure can be used to\nextract behavioral patterns and to gain insight into what factors influence\nthese behaviors. Thus, when a new case is being investigated and the profile\nvariables are unknown because the offender has yet to be identified, the\nobserved crime scene variables are used to infer the unknown variables based on\ntheir connections in the structure and the corresponding numerical\n(probabilistic) weights. The objective is to produce a more systematic and\nempirical approach to profiling, and to use the resulting PN model as a\ndecision tool.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2010 20:21:52 GMT"}], "update_date": "2010-02-11", "authors_parsed": [["Pillai", "Ramesh Kumar Gopala", ""], ["P", "Dr. Ramakanth Kumar .", ""]]}, {"id": "1002.2240", "submitter": "Joe Suzuki", "authors": "Joe Suzuki", "title": "A Generalization of the Chow-Liu Algorithm and its Application to\n  Statistical Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AI cs.LG math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We extend the Chow-Liu algorithm for general random variables while the\nprevious versions only considered finite cases. In particular, this paper\napplies the generalization to Suzuki's learning algorithm that generates from\ndata forests rather than trees based on the minimum description length by\nbalancing the fitness of the data to the forest and the simplicity of the\nforest. As a result, we successfully obtain an algorithm when both of the\nGaussian and finite random variables are present.\n", "versions": [{"version": "v1", "created": "Wed, 10 Feb 2010 23:19:56 GMT"}], "update_date": "2010-02-12", "authors_parsed": [["Suzuki", "Joe", ""]]}, {"id": "1002.2523", "submitter": "Dakshina Ranjan Kisku", "authors": "Ajita Rattani, Dakshina Ranjan Kisku, Manuele Bicego, Massimo\n  Tistarelli", "title": "Feature Level Fusion of Face and Fingerprint Biometrics", "comments": "6 pages, 7 figures, conference", "journal-ref": "BTAS 2007", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The aim of this paper is to study the fusion at feature extraction level for\nface and fingerprint biometrics. The proposed approach is based on the fusion\nof the two traits by extracting independent feature pointsets from the two\nmodalities, and making the two pointsets compatible for concatenation.\nMoreover, to handle the problem of curse of dimensionality, the feature\npointsets are properly reduced in dimension. Different feature reduction\ntechniques are implemented, prior and after the feature pointsets fusion, and\nthe results are duly recorded. The fused feature pointset for the database and\nthe query face and fingerprint images are matched using techniques based on\neither the point pattern matching, or the Delaunay triangulation. Comparative\nexperiments are conducted on chimeric and real databases, to assess the actual\nadvantage of the fusion performed at the feature extraction level, in\ncomparison to the matching score level.\n", "versions": [{"version": "v1", "created": "Fri, 12 Feb 2010 11:14:57 GMT"}], "update_date": "2010-02-15", "authors_parsed": [["Rattani", "Ajita", ""], ["Kisku", "Dakshina Ranjan", ""], ["Bicego", "Manuele", ""], ["Tistarelli", "Massimo", ""]]}, {"id": "1002.2755", "submitter": "Dakshina Ranjan Kisku", "authors": "Dakshina Ranjan Kisku, Jamuna Kanta Sing, Phalguni Gupta", "title": "Multibiometrics Belief Fusion", "comments": "4 pages, 3 figures", "journal-ref": "ICMV 2009", "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a multimodal biometric system through Gaussian Mixture\nModel (GMM) for face and ear biometrics with belief fusion of the estimated\nscores characterized by Gabor responses and the proposed fusion is accomplished\nby Dempster-Shafer (DS) decision theory. Face and ear images are convolved with\nGabor wavelet filters to extracts spatially enhanced Gabor facial features and\nGabor ear features. Further, GMM is applied to the high-dimensional Gabor face\nand Gabor ear responses separately for quantitive measurements. Expectation\nMaximization (EM) algorithm is used to estimate density parameters in GMM. This\nproduces two sets of feature vectors which are then fused using Dempster-Shafer\ntheory. Experiments are conducted on multimodal database containing face and\near images of 400 individuals. It is found that use of Gabor wavelet filters\nalong with GMM and DS theory can provide robust and efficient multimodal fusion\nstrategy.\n", "versions": [{"version": "v1", "created": "Sun, 14 Feb 2010 07:38:45 GMT"}], "update_date": "2010-02-16", "authors_parsed": [["Kisku", "Dakshina Ranjan", ""], ["Sing", "Jamuna Kanta", ""], ["Gupta", "Phalguni", ""]]}, {"id": "1002.2897", "submitter": "Raphael Chenouard", "authors": "Raphael Chenouard (LINA), Laurent Granvilliers (LINA), Ricardo Soto\n  (LINA)", "title": "Model-Driven Constraint Programming", "comments": null, "journal-ref": "International Conference on Principles and Practice of Declarative\n  Programming, Valence : Spain (2008)", "doi": "10.1145/1389449.1389479", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constraint programming can definitely be seen as a model-driven paradigm. The\nusers write programs for modeling problems. These programs are mapped to\nexecutable models to calculate the solutions. This paper focuses on efficient\nmodel management (definition and transformation). From this point of view, we\npropose to revisit the design of constraint-programming systems. A model-driven\narchitecture is introduced to map solving-independent constraint models to\nsolving-dependent decision models. Several important questions are examined,\nsuch as the need for a visual highlevel modeling language, and the quality of\nmetamodeling techniques to implement the transformations. A main result is the\ns-COMMA platform that efficiently implements the chain from modeling to solving\nconstraint problems\n", "versions": [{"version": "v1", "created": "Mon, 15 Feb 2010 15:47:29 GMT"}], "update_date": "2010-02-16", "authors_parsed": [["Chenouard", "Raphael", "", "LINA"], ["Granvilliers", "Laurent", "", "LINA"], ["Soto", "Ricardo", "", "LINA"]]}, {"id": "1002.3023", "submitter": "Raphael Chenouard", "authors": "Raphael Chenouard (LINA), Laurent Granvilliers (LINA), Ricardo Soto\n  (LINA)", "title": "Rewriting Constraint Models with Metamodels", "comments": null, "journal-ref": "The eight symposium on abstraction, reformulation, and\n  approximation, Lake Arrowhead : United States (2009)", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important challenge in constraint programming is to rewrite constraint\nmodels into executable programs calculat- ing the solutions. This phase of\nconstraint processing may require translations between constraint programming\nlan- guages, transformations of constraint representations, model\noptimizations, and tuning of solving strategies. In this paper, we introduce a\npivot metamodel describing the common fea- tures of constraint models including\ndifferent kinds of con- straints, statements like conditionals and loops, and\nother first-class elements like object classes and predicates. This metamodel\nis general enough to cope with the constructions of many languages, from\nobject-oriented modeling languages to logic languages, but it is independent\nfrom them. The rewriting operations manipulate metamodel instances apart from\nlanguages. As a consequence, the rewriting operations apply whatever languages\nare selected and they are able to manage model semantic information. A bridge\nis created between the metamodel space and languages using parsing techniques.\nTools from the software engineering world can be useful to implement this\nframework.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2010 07:26:48 GMT"}], "update_date": "2010-02-17", "authors_parsed": [["Chenouard", "Raphael", "", "LINA"], ["Granvilliers", "Laurent", "", "LINA"], ["Soto", "Ricardo", "", "LINA"]]}, {"id": "1002.3078", "submitter": "Raphael Chenouard", "authors": "Raphael Chenouard (LINA), Laurent Granvilliers (LINA), Ricardo Soto\n  (LINA)", "title": "Using ATL to define advanced and flexible constraint model\n  transformations", "comments": null, "journal-ref": "MtATL2009, Nantes : France (2009)", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transforming constraint models is an important task in re- cent constraint\nprogramming systems. User-understandable models are defined during the modeling\nphase but rewriting or tuning them is manda- tory to get solving-efficient\nmodels. We propose a new architecture al- lowing to define bridges between any\n(modeling or solver) languages and to implement model optimizations. This\narchitecture follows a model- driven approach where the constraint modeling\nprocess is seen as a set of model transformations. Among others, an interesting\nfeature is the def- inition of transformations as concept-oriented rules, i.e.\nbased on types of model elements where the types are organized into a hierarchy\ncalled a metamodel.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2010 13:09:07 GMT"}], "update_date": "2010-02-17", "authors_parsed": [["Chenouard", "Raphael", "", "LINA"], ["Granvilliers", "Laurent", "", "LINA"], ["Soto", "Ricardo", "", "LINA"]]}, {"id": "1002.3086", "submitter": "Pedro Alejandro Ortega", "authors": "Pedro A. Ortega, Daniel A. Braun", "title": "Convergence of Bayesian Control Rule", "comments": "8 pages, 7 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recently, new approaches to adaptive control have sought to reformulate the\nproblem as a minimization of a relative entropy criterion to obtain tractable\nsolutions. In particular, it has been shown that minimizing the expected\ndeviation from the causal input-output dependencies of the true plant leads to\na new promising stochastic control rule called the Bayesian control rule. This\nwork proves the convergence of the Bayesian control rule under two sufficient\nassumptions: boundedness, which is an ergodicity condition; and consistency,\nwhich is an instantiation of the sure-thing principle.\n", "versions": [{"version": "v1", "created": "Tue, 16 Feb 2010 14:14:59 GMT"}], "update_date": "2010-02-17", "authors_parsed": [["Ortega", "Pedro A.", ""], ["Braun", "Daniel A.", ""]]}, {"id": "1002.3174", "submitter": "Mohsen Toorani", "authors": "M. C. Amirani, M. Toorani, A. A. Beheshti", "title": "A new approach to content-based file type detection", "comments": "6 Pages, 5 Figure, 2 Tables", "journal-ref": "Proceedings of the 13th IEEE Symposium on Computers and\n  Communications (ISCC'08), pp.1103-1108, July 2008", "doi": "10.1109/ISCC.2008.4625611", "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  File type identification and file type clustering may be difficult tasks that\nhave an increasingly importance in the field of computer and network security.\nClassical methods of file type detection including considering file extensions\nand magic bytes can be easily spoofed. Content-based file type detection is a\nnewer way that is taken into account recently. In this paper, a new\ncontent-based method for the purpose of file type detection and file type\nclustering is proposed that is based on the PCA and neural networks. The\nproposed method has a good accuracy and is fast enough.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2010 10:18:07 GMT"}, {"version": "v2", "created": "Mon, 11 Jul 2011 14:02:13 GMT"}, {"version": "v3", "created": "Fri, 16 Mar 2012 21:31:17 GMT"}], "update_date": "2012-03-20", "authors_parsed": [["Amirani", "M. C.", ""], ["Toorani", "M.", ""], ["Beheshti", "A. A.", ""]]}, {"id": "1002.3195", "submitter": "Mahmud Hossain", "authors": "M. Shahriar Hossain, Michael Narayan and Naren Ramakrishnan", "title": "Efficiently Discovering Hammock Paths from Induced Similarity Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Similarity networks are important abstractions in many information management\napplications such as recommender systems, corpora analysis, and medical\ninformatics. For instance, by inducing similarity networks between movies rated\nsimilarly by users, or between documents containing common terms, and or\nbetween clinical trials involving the same themes, we can aim to find the\nglobal structure of connectivities underlying the data, and use the network as\na basis to make connections between seemingly disparate entities. In the above\napplications, composing similarities between objects of interest finds uses in\nserendipitous recommendation, in storytelling, and in clinical diagnosis,\nrespectively. We present an algorithmic framework for traversing similarity\npaths using the notion of `hammock' paths which are generalization of\ntraditional paths. Our framework is exploratory in nature so that, given\nstarting and ending objects of interest, it explores candidate objects for path\nfollowing, and heuristics to admissibly estimate the potential for paths to\nlead to a desired destination. We present three diverse applications: exploring\nmovie similarities in the Netflix dataset, exploring abstract similarities\nacross the PubMed corpus, and exploring description similarities in a database\nof clinical trials. Experimental results demonstrate the potential of our\napproach for unstructured knowledge discovery in similarity networks.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2010 04:07:06 GMT"}], "update_date": "2010-02-18", "authors_parsed": [["Hossain", "M. Shahriar", ""], ["Narayan", "Michael", ""], ["Ramakrishnan", "Naren", ""]]}, {"id": "1002.3239", "submitter": "Nicholas Ruozzi", "authors": "Nicholas Ruozzi, Sekhar Tatikonda", "title": "Message-Passing Algorithms: Reparameterizations and Splittings", "comments": "A complete rework and expansion of the previous versions", "journal-ref": "Information Theory, IEEE Transactions on , vol.59, no.9,\n  pp.5860,5881, Sept. 2013", "doi": "10.1109/TIT.2013.2259576", "report-no": null, "categories": "cs.IT cs.AI math.IT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The max-product algorithm, a local message-passing scheme that attempts to\ncompute the most probable assignment (MAP) of a given probability distribution,\nhas been successfully employed as a method of approximate inference for\napplications arising in coding theory, computer vision, and machine learning.\nHowever, the max-product algorithm is not guaranteed to converge to the MAP\nassignment, and if it does, is not guaranteed to recover the MAP assignment.\n  Alternative convergent message-passing schemes have been proposed to overcome\nthese difficulties. This work provides a systematic study of such\nmessage-passing algorithms that extends the known results by exhibiting new\nsufficient conditions for convergence to local and/or global optima, providing\na combinatorial characterization of these optima based on graph covers, and\ndescribing a new convergent and correct message-passing algorithm whose\nderivation unifies many of the known convergent message-passing algorithms.\n  While convergent and correct message-passing algorithms represent a step\nforward in the analysis of max-product style message-passing algorithms, the\nconditions needed to guarantee convergence to a global optimum can be too\nrestrictive in both theory and practice. This limitation of convergent and\ncorrect message-passing schemes is characterized by graph covers and\nillustrated by example.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2010 18:53:23 GMT"}, {"version": "v2", "created": "Fri, 7 May 2010 17:54:00 GMT"}, {"version": "v3", "created": "Sat, 1 Dec 2012 23:57:50 GMT"}], "update_date": "2014-01-07", "authors_parsed": [["Ruozzi", "Nicholas", ""], ["Tatikonda", "Sekhar", ""]]}, {"id": "1002.3307", "submitter": "Yusuke Watanabe", "authors": "Yusuke Watanabe and Kenji Fukumizu", "title": "Graph Zeta Function in the Bethe Free Energy and Loopy Belief\n  Propagation", "comments": "19 pages, Annual Conference on Neural Information Processing Systems\n  (NIPS 2009), together with the supplementary material", "journal-ref": "Advances in Neural Information Processing Systems 22, pages\n  2017-2025", "doi": null, "report-no": null, "categories": "cs.AI cs.DM math-ph math.MP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new approach to the analysis of Loopy Belief Propagation (LBP)\nby establishing a formula that connects the Hessian of the Bethe free energy\nwith the edge zeta function. The formula has a number of theoretical\nimplications on LBP. It is applied to give a sufficient condition that the\nHessian of the Bethe free energy is positive definite, which shows\nnon-convexity for graphs with multiple cycles. The formula clarifies the\nrelation between the local stability of a fixed point of LBP and local minima\nof the Bethe free energy. We also propose a new approach to the uniqueness of\nLBP fixed point, and show various conditions of uniqueness.\n", "versions": [{"version": "v1", "created": "Wed, 17 Feb 2010 17:55:58 GMT"}], "update_date": "2010-02-18", "authors_parsed": [["Watanabe", "Yusuke", ""], ["Fukumizu", "Kenji", ""]]}, {"id": "1002.4014", "submitter": "Ciro Russo", "authors": "Salvatore Rampone, Ciro Russo", "title": "A fuzzified BRAIN algorithm for learning DNF from incomplete data", "comments": null, "journal-ref": "Electronic Journal of Applied Statistical Analysis, Vol. 5, Issue\n  2, 256-270, 2012", "doi": "10.1285/i20705948v5n2p256", "report-no": null, "categories": "cs.IT cs.AI math.IT math.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Aim of this paper is to address the problem of learning Boolean functions\nfrom training data with missing values. We present an extension of the BRAIN\nalgorithm, called U-BRAIN (Uncertainty-managing Batch Relevance-based\nArtificial INtelligence), conceived for learning DNF Boolean formulas from\npartial truth tables, possibly with uncertain values or missing bits.\n  Such an algorithm is obtained from BRAIN by introducing fuzzy sets in order\nto manage uncertainty. In the case where no missing bits are present, the\nalgorithm reduces to the original BRAIN.\n", "versions": [{"version": "v1", "created": "Sun, 21 Feb 2010 20:40:50 GMT"}, {"version": "v2", "created": "Fri, 6 Aug 2010 17:31:53 GMT"}, {"version": "v3", "created": "Thu, 16 Jun 2011 13:39:12 GMT"}], "update_date": "2015-08-31", "authors_parsed": [["Rampone", "Salvatore", ""], ["Russo", "Ciro", ""]]}, {"id": "1002.4286", "submitter": "Jos\\'e L Balc\\'azar", "authors": "Jose L. Balcazar", "title": "Redundancy, Deduction Schemes, and Minimum-Size Bases for Association\n  Rules", "comments": "LMCS accepted paper", "journal-ref": "Logical Methods in Computer Science, Volume 6, Issue 2 (June 27,\n  2010) lmcs:812", "doi": "10.2168/LMCS-6(2:4)2010", "report-no": null, "categories": "cs.LO cs.AI", "license": "http://creativecommons.org/licenses/by/3.0/", "abstract": "  Association rules are among the most widely employed data analysis methods in\nthe field of Data Mining. An association rule is a form of partial implication\nbetween two sets of binary variables. In the most common approach, association\nrules are parameterized by a lower bound on their confidence, which is the\nempirical conditional probability of their consequent given the antecedent,\nand/or by some other parameter bounds such as \"support\" or deviation from\nindependence. We study here notions of redundancy among association rules from\na fundamental perspective. We see each transaction in a dataset as an\ninterpretation (or model) in the propositional logic sense, and consider\nexisting notions of redundancy, that is, of logical entailment, among\nassociation rules, of the form \"any dataset in which this first rule holds must\nobey also that second rule, therefore the second is redundant\". We discuss\nseveral existing alternative definitions of redundancy between association\nrules and provide new characterizations and relationships among them. We show\nthat the main alternatives we discuss correspond actually to just two variants,\nwhich differ in the treatment of full-confidence implications. For each of\nthese two notions of redundancy, we provide a sound and complete deduction\ncalculus, and we show how to construct complete bases (that is,\naxiomatizations) of absolutely minimum size in terms of the number of rules. We\nexplore finally an approach to redundancy with respect to several association\nrules, and fully characterize its simplest case of two partial premises.\n", "versions": [{"version": "v1", "created": "Tue, 23 Feb 2010 10:02:24 GMT"}, {"version": "v2", "created": "Sat, 26 Jun 2010 22:44:45 GMT"}], "update_date": "2019-03-14", "authors_parsed": [["Balcazar", "Jose L.", ""]]}, {"id": "1002.4453", "submitter": "Joe Suzuki", "authors": "Joe Suzuki", "title": "Nonparametric Estimation and On-Line Prediction for General Stationary\n  Ergodic Sources", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IT cs.AI math.IT math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We proposed a learning algorithm for nonparametric estimation and on-line\nprediction for general stationary ergodic sources. We prepare histograms each\nof which estimates the probability as a finite distribution, and mixture them\nwith weights to construct an estimator. The whole analysis is based on measure\ntheory. The estimator works whether the source is discrete or continuous. If it\nis stationary ergodic, then the measure theoretically given Kullback-Leibler\ninformation divided by the sequence length $n$ converges to zero as $n$ goes to\ninfinity. In particular, for continuous sources, the method does not require\nexistence of a probability density function.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2010 02:13:49 GMT"}, {"version": "v2", "created": "Thu, 4 Mar 2010 02:19:38 GMT"}, {"version": "v3", "created": "Sat, 26 Jun 2010 08:48:19 GMT"}], "update_date": "2010-06-29", "authors_parsed": [["Suzuki", "Joe", ""]]}, {"id": "1002.4522", "submitter": "Vitaly Schetinin", "authors": "L. Jakaite, V. Schetinin, and C. Maple", "title": "Feature Importance in Bayesian Assessment of Newborn Brain Maturity from\n  EEG", "comments": "Proceedings of the 9th WSEAS International Conference on Artificial\n  Intelligence, Knowledge Engineering and Data Bases (AIKED), University of\n  Cambridge, UK, 2010, edited by L. A. Zadeh et al, pp 191 - 195", "journal-ref": "Proceedings of the 9th WSEAS International Conference on\n  Artificial Intelligence, Knowledge Engineering and Data Bases (AIKED),\n  University of Cambridge, UK, 2010, edited by L. A. Zadeh et al, pp 191 - 195", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The methodology of Bayesian Model Averaging (BMA) is applied for assessment\nof newborn brain maturity from sleep EEG. In theory this methodology provides\nthe most accurate assessments of uncertainty in decisions. However, the\nexisting BMA techniques have been shown providing biased assessments in the\nabsence of some prior information enabling to explore model parameter space in\ndetails within a reasonable time. The lack in details leads to disproportional\nsampling from the posterior distribution. In case of the EEG assessment of\nbrain maturity, BMA results can be biased because of the absence of information\nabout EEG feature importance. In this paper we explore how the posterior\ninformation about EEG features can be used in order to reduce a negative impact\nof disproportional sampling on BMA performance. We use EEG data recorded from\nsleeping newborns to test the efficiency of the proposed BMA technique.\n", "versions": [{"version": "v1", "created": "Wed, 24 Feb 2010 11:11:52 GMT"}], "update_date": "2010-02-25", "authors_parsed": [["Jakaite", "L.", ""], ["Schetinin", "V.", ""], ["Maple", "C.", ""]]}, {"id": "1002.4665", "submitter": "Jordan Boyd-Graber", "authors": "Jordan Boyd-Graber, David M. Blei", "title": "Syntactic Topic Models", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The syntactic topic model (STM) is a Bayesian nonparametric model of language\nthat discovers latent distributions of words (topics) that are both\nsemantically and syntactically coherent. The STM models dependency parsed\ncorpora where sentences are grouped into documents. It assumes that each word\nis drawn from a latent topic chosen by combining document-level features and\nthe local syntactic context. Each document has a distribution over latent\ntopics, as in topic models, which provides the semantic consistency. Each\nelement in the dependency parse tree also has a distribution over the topics of\nits children, as in latent-state syntax models, which provides the syntactic\nconsistency. These distributions are convolved so that the topic of each word\nis likely under both its document and syntactic context. We derive a fast\nposterior inference algorithm based on variational methods. We report\nqualitative and quantitative studies on both synthetic data and hand-parsed\ndocuments. We show that the STM is a more predictive model of language than\ncurrent models based only on syntax or only on topics.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2010 00:00:47 GMT"}], "update_date": "2010-03-04", "authors_parsed": [["Boyd-Graber", "Jordan", ""], ["Blei", "David M.", ""]]}, {"id": "1002.4862", "submitter": "Matthew Streeter", "authors": "Matthew Streeter and H. Brendan McMahan", "title": "Less Regret via Online Conditioning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze and evaluate an online gradient descent algorithm with adaptive\nper-coordinate adjustment of learning rates. Our algorithm can be thought of as\nan online version of batch gradient descent with a diagonal preconditioner.\nThis approach leads to regret bounds that are stronger than those of standard\nonline gradient descent for general online convex optimization problems.\nExperimentally, we show that our algorithm is competitive with state-of-the-art\nalgorithms for large scale machine learning problems.\n", "versions": [{"version": "v1", "created": "Thu, 25 Feb 2010 20:31:05 GMT"}], "update_date": "2010-02-26", "authors_parsed": [["Streeter", "Matthew", ""], ["McMahan", "H. Brendan", ""]]}]