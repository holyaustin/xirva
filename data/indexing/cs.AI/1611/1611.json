[{"id": "1611.00020", "submitter": "Chen Liang", "authors": "Chen Liang, Jonathan Berant, Quoc Le, Kenneth D. Forbus, Ni Lao", "title": "Neural Symbolic Machines: Learning Semantic Parsers on Freebase with\n  Weak Supervision", "comments": "ACL 2017 camera ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Harnessing the statistical power of neural networks to perform language\nunderstanding and symbolic reasoning is difficult, when it requires executing\nefficient discrete operations against a large knowledge-base. In this work, we\nintroduce a Neural Symbolic Machine, which contains (a) a neural \"programmer\",\ni.e., a sequence-to-sequence model that maps language utterances to programs\nand utilizes a key-variable memory to handle compositionality (b) a symbolic\n\"computer\", i.e., a Lisp interpreter that performs program execution, and helps\nfind good programs by pruning the search space. We apply REINFORCE to directly\noptimize the task reward of this structured prediction problem. To train with\nweak supervision and improve the stability of REINFORCE, we augment it with an\niterative maximum-likelihood training process. NSM outperforms the\nstate-of-the-art on the WebQuestionsSP dataset when trained from\nquestion-answer pairs only, without requiring any feature engineering or\ndomain-specific knowledge.\n", "versions": [{"version": "v1", "created": "Mon, 31 Oct 2016 20:07:23 GMT"}, {"version": "v2", "created": "Wed, 2 Nov 2016 05:25:19 GMT"}, {"version": "v3", "created": "Thu, 3 Nov 2016 16:24:24 GMT"}, {"version": "v4", "created": "Sun, 23 Apr 2017 07:16:13 GMT"}], "update_date": "2017-04-25", "authors_parsed": [["Liang", "Chen", ""], ["Berant", "Jonathan", ""], ["Le", "Quoc", ""], ["Forbus", "Kenneth D.", ""], ["Lao", "Ni", ""]]}, {"id": "1611.00094", "submitter": "Eyrun Eyjolfsdottir", "authors": "Eyrun Eyjolfsdottir, Kristin Branson, Yisong Yue and Pietro Perona", "title": "Learning recurrent representations for hierarchical behavior modeling", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a framework for detecting action patterns from motion sequences\nand modeling the sensory-motor relationship of animals, using a generative\nrecurrent neural network. The network has a discriminative part (classifying\nactions) and a generative part (predicting motion), whose recurrent cells are\nlaterally connected, allowing higher levels of the network to represent high\nlevel phenomena. We test our framework on two types of data, fruit fly behavior\nand online handwriting. Our results show that 1) taking advantage of unlabeled\nsequences, by predicting future motion, significantly improves action detection\nperformance when training labels are scarce, 2) the network learns to represent\nhigh level phenomena such as writer identity and fly gender, without\nsupervision, and 3) simulated motion trajectories, generated by treating motion\nprediction as input to the network, look realistic and may be used to\nqualitatively evaluate whether the model has learnt generative control rules.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2016 01:03:53 GMT"}, {"version": "v2", "created": "Thu, 3 Nov 2016 23:39:43 GMT"}, {"version": "v3", "created": "Tue, 15 Nov 2016 18:06:10 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Eyjolfsdottir", "Eyrun", ""], ["Branson", "Kristin", ""], ["Yue", "Yisong", ""], ["Perona", "Pietro", ""]]}, {"id": "1611.00175", "submitter": "Moontae Lee", "authors": "Moontae Lee, David Bindel, David Mimno", "title": "Robust Spectral Inference for Joint Stochastic Matrix Factorization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Spectral inference provides fast algorithms and provable optimality for\nlatent topic analysis. But for real data these algorithms require additional\nad-hoc heuristics, and even then often produce unusable results. We explain\nthis poor performance by casting the problem of topic inference in the\nframework of Joint Stochastic Matrix Factorization (JSMF) and showing that\nprevious methods violate the theoretical conditions necessary for a good\nsolution to exist. We then propose a novel rectification method that learns\nhigh quality topics and their interactions even on small, noisy data. This\nmethod achieves results comparable to probabilistic techniques in several\ndomains while maintaining scalability and provable optimality.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2016 10:06:57 GMT"}], "update_date": "2016-11-02", "authors_parsed": [["Lee", "Moontae", ""], ["Bindel", "David", ""], ["Mimno", "David", ""]]}, {"id": "1611.00183", "submitter": "Bas Van Stein", "authors": "Bas van Stein, Matthijs van Leeuwen and Thomas B\\\"ack", "title": "Local Subspace-Based Outlier Detection using Global Neighbourhoods", "comments": "Short version accepted at IEEE BigData 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Outlier detection in high-dimensional data is a challenging yet important\ntask, as it has applications in, e.g., fraud detection and quality control.\nState-of-the-art density-based algorithms perform well because they 1) take the\nlocal neighbourhoods of data points into account and 2) consider feature\nsubspaces. In highly complex and high-dimensional data, however, existing\nmethods are likely to overlook important outliers because they do not\nexplicitly take into account that the data is often a mixture distribution of\nmultiple components.\n  We therefore introduce GLOSS, an algorithm that performs local subspace\noutlier detection using global neighbourhoods. Experiments on synthetic data\ndemonstrate that GLOSS more accurately detects local outliers in mixed data\nthan its competitors. Moreover, experiments on real-world data show that our\napproach identifies relevant outliers overlooked by existing methods,\nconfirming that one should keep an eye on the global perspective even when\ndoing local outlier detection.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2016 11:22:26 GMT"}], "update_date": "2016-11-02", "authors_parsed": [["van Stein", "Bas", ""], ["van Leeuwen", "Matthijs", ""], ["B\u00e4ck", "Thomas", ""]]}, {"id": "1611.00201", "submitter": "Jay Wong", "authors": "Jay M. Wong", "title": "Towards Lifelong Self-Supervision: A Deep Learning Direction for\n  Robotics", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Despite outstanding success in vision amongst other domains, many of the\nrecent deep learning approaches have evident drawbacks for robots. This\nmanuscript surveys recent work in the literature that pertain to applying deep\nlearning systems to the robotics domain, either as means of estimation or as a\ntool to resolve motor commands directly from raw percepts. These recent\nadvances are only a piece to the puzzle. We suggest that deep learning as a\ntool alone is insufficient in building a unified framework to acquire general\nintelligence. For this reason, we complement our survey with insights from\ncognitive development and refer to ideas from classical control theory,\nproducing an integrated direction for a lifelong learning architecture.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2016 12:47:50 GMT"}], "update_date": "2016-11-02", "authors_parsed": [["Wong", "Jay M.", ""]]}, {"id": "1611.00230", "submitter": "Michele Colledanchise", "authors": "Michele Colledanchise, Diogo Almeida, and Petter \\\"Ogren", "title": "Towards Blended Reactive Planning and Acting using Behavior Trees", "comments": null, "journal-ref": "2019 International Conference on Robotics and Automation (ICRA)", "doi": "10.1109/ICRA.2019.8794128", "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we show how a planning algorithm can be used to automatically\ncreate and update a Behavior Tree (BT), controlling a robot in a dynamic\nenvironment. The planning part of the algorithm is based on the idea of back\nchaining. Starting from a goal condition we iteratively select actions to\nachieve that goal, and if those actions have unmet preconditions, they are\nextended with actions to achieve them in the same way. The fact that BTs are\ninherently modular and reactive makes the proposed solution blend acting and\nplanning in a way that enables the robot to efficiently react to external\ndisturbances. If an external agent undoes an action the robot reexecutes it\nwithout re-planning, and if an external agent helps the robot, it skips the\ncorresponding actions, again without replanning. We illustrate our approach in\ntwo different robotics scenarios.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2016 13:56:55 GMT"}, {"version": "v2", "created": "Fri, 14 Sep 2018 16:34:35 GMT"}], "update_date": "2020-07-16", "authors_parsed": [["Colledanchise", "Michele", ""], ["Almeida", "Diogo", ""], ["\u00d6gren", "Petter", ""]]}, {"id": "1611.00274", "submitter": "Wolfram Schenck", "authors": "Wolfram Schenck, Hendrik Hasenbein, Ralf M\\\"oller", "title": "Detecting Affordances by Visuomotor Simulation", "comments": "26 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The term \"affordance\" denotes the behavioral meaning of objects. We propose a\ncognitive architecture for the detection of affordances in the visual modality.\nThis model is based on the internal simulation of movement sequences. For each\nmovement step, the resulting sensory state is predicted by a forward model,\nwhich in turn triggers the generation of a new (simulated) motor command by an\ninverse model. Thus, a series of mental images in the sensory and in the motor\ndomain is evoked. Starting from a real sensory state, a large number of such\nsequences is simulated in parallel. Final affordance detection is based on the\ngenerated motor commands. We apply this model to a real-world mobile robot\nwhich is faced with obstacle arrangements some of which are passable (corridor)\nand some of which are not (dead ends). The robot's task is to detect the right\naffordance (\"pass-through-able\" or \"non-pass-through-able\"). The required\ninternal models are acquired in a hierarchical training process. Afterwards,\nthe robotic agent is able to distinguish reliably between corridors and dead\nends. This real-world result enhances the validity of the proposed mental\nsimulation approach. In addition, we compare several key factors in the\nsimulation process regarding performance and efficiency.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2016 15:35:53 GMT"}], "update_date": "2016-11-02", "authors_parsed": [["Schenck", "Wolfram", ""], ["Hasenbein", "Hendrik", ""], ["M\u00f6ller", "Ralf", ""]]}, {"id": "1611.00447", "submitter": "Peter Krafft", "authors": "Peter M Krafft, Michael Macy, Alex Pentland", "title": "Bots as Virtual Confederates: Design and Ethics", "comments": "Forthcoming in CSCW 2017", "journal-ref": "The 20th ACM Conference on Computer-Supported Cooperative Work and\n  Social Computing (CSCW) (2016)", "doi": "10.1145/2998181.2998354", "report-no": null, "categories": "cs.CY cs.AI cs.HC cs.SI physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The use of bots as virtual confederates in online field experiments holds\nextreme promise as a new methodological tool in computational social science.\nHowever, this potential tool comes with inherent ethical challenges. Informed\nconsent can be difficult to obtain in many cases, and the use of confederates\nnecessarily implies the use of deception. In this work we outline a design\nspace for bots as virtual confederates, and we propose a set of guidelines for\nmeeting the status quo for ethical experimentation. We draw upon examples from\nprior work in the CSCW community and the broader social science literature for\nillustration. While a handful of prior researchers have used bots in online\nexperimentation, our work is meant to inspire future work in this area and\nraise awareness of the associated ethical issues.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 02:31:18 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Krafft", "Peter M", ""], ["Macy", "Michael", ""], ["Pentland", "Alex", ""]]}, {"id": "1611.00448", "submitter": "Hao Wang", "authors": "Hao Wang, Xingjian Shi, Dit-Yan Yeung", "title": "Natural-Parameter Networks: A Class of Probabilistic Neural Networks", "comments": "To appear at NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks (NN) have achieved state-of-the-art performance in various\napplications. Unfortunately in applications where training data is\ninsufficient, they are often prone to overfitting. One effective way to\nalleviate this problem is to exploit the Bayesian approach by using Bayesian\nneural networks (BNN). Another shortcoming of NN is the lack of flexibility to\ncustomize different distributions for the weights and neurons according to the\ndata, as is often done in probabilistic graphical models. To address these\nproblems, we propose a class of probabilistic neural networks, dubbed\nnatural-parameter networks (NPN), as a novel and lightweight Bayesian treatment\nof NN. NPN allows the usage of arbitrary exponential-family distributions to\nmodel the weights and neurons. Different from traditional NN and BNN, NPN takes\ndistributions as input and goes through layers of transformation before\nproducing distributions to match the target output distributions. As a Bayesian\ntreatment, efficient backpropagation (BP) is performed to learn the natural\nparameters for the distributions over both the weights and neurons. The output\ndistributions of each layer, as byproducts, may be used as second-order\nrepresentations for the associated tasks such as link prediction. Experiments\non real-world datasets show that NPN can achieve state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 02:32:05 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Wang", "Hao", ""], ["Shi", "Xingjian", ""], ["Yeung", "Dit-Yan", ""]]}, {"id": "1611.00454", "submitter": "Hao Wang", "authors": "Hao Wang, Xingjian Shi, Dit-Yan Yeung", "title": "Collaborative Recurrent Autoencoder: Recommend while Learning to Fill in\n  the Blanks", "comments": "To appear at NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CL cs.CV stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Hybrid methods that utilize both content and rating information are commonly\nused in many recommender systems. However, most of them use either handcrafted\nfeatures or the bag-of-words representation as a surrogate for the content\ninformation but they are neither effective nor natural enough. To address this\nproblem, we develop a collaborative recurrent autoencoder (CRAE) which is a\ndenoising recurrent autoencoder (DRAE) that models the generation of content\nsequences in the collaborative filtering (CF) setting. The model generalizes\nrecent advances in recurrent deep learning from i.i.d. input to non-i.i.d.\n(CF-based) input and provides a new denoising scheme along with a novel\nlearnable pooling scheme for the recurrent autoencoder. To do this, we first\ndevelop a hierarchical Bayesian model for the DRAE and then generalize it to\nthe CF setting. The synergy between denoising and CF enables CRAE to make\naccurate recommendations while learning to fill in the blanks in sequences.\nExperiments on real-world datasets from different domains (CiteULike and\nNetflix) show that, by jointly modeling the order-aware generation of sequences\nfor the content information and performing CF for the ratings, CRAE is able to\nsignificantly outperform the state of the art on both the recommendation task\nbased on ratings and the sequence generation task based on content information.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 02:49:44 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Wang", "Hao", ""], ["Shi", "Xingjian", ""], ["Yeung", "Dit-Yan", ""]]}, {"id": "1611.00538", "submitter": "L\\'aszl\\'o Csat\\'o", "authors": "S\\'andor Boz\\'oki, L\\'aszl\\'o Csat\\'o, J\\'ozsef Temesi", "title": "An application of incomplete pairwise comparison matrices for ranking\n  top tennis players", "comments": "14 pages, 2 figures", "journal-ref": "European Journal of Operational Research, 248(1): 211-218, 2016", "doi": "10.1016/j.ejor.2015.06.069", "report-no": null, "categories": "cs.AI cs.GT stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Pairwise comparison is an important tool in multi-attribute decision making.\nPairwise comparison matrices (PCM) have been applied for ranking criteria and\nfor scoring alternatives according to a given criterion. Our paper presents a\nspecial application of incomplete PCMs: ranking of professional tennis players\nbased on their results against each other. The selected 25 players have been on\nthe top of the ATP rankings for a shorter or longer period in the last 40\nyears. Some of them have never met on the court. One of the aims of the paper\nis to provide ranking of the selected players, however, the analysis of\nincomplete pairwise comparison matrices is also in the focus. The eigenvector\nmethod and the logarithmic least squares method were used to calculate weights\nfrom incomplete PCMs. In our results the top three players of four decades were\nNadal, Federer and Sampras. Some questions have been raised on the properties\nof incomplete PCMs and remains open for further investigation.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 10:36:11 GMT"}], "update_date": "2019-06-20", "authors_parsed": [["Boz\u00f3ki", "S\u00e1ndor", ""], ["Csat\u00f3", "L\u00e1szl\u00f3", ""], ["Temesi", "J\u00f3zsef", ""]]}, {"id": "1611.00547", "submitter": "Dario Garcia-Gasulla", "authors": "Dario Garcia-Gasulla, Eduard Ayguad\\'e, Jes\\'us Labarta, Ulises\n  Cort\\'es", "title": "Limitations and Alternatives for the Evaluation of Large-scale Link\n  Prediction", "comments": "Submitted to New Generation Computing, 15 pages, 4 tables, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI cs.DB", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Link prediction, the problem of identifying missing links among a set of\ninter-related data entities, is a popular field of research due to its\napplication to graph-like domains. Producing consistent evaluations of the\nperformance of the many link prediction algorithms being proposed can be\nchallenging due to variable graph properties, such as size and density. In this\npaper we first discuss traditional data mining solutions which are applicable\nto link prediction evaluation, arguing about their capacity for producing\nfaithful and useful evaluations. We also introduce an innovative modification\nto a traditional evaluation methodology with the goal of adapting it to the\nproblem of evaluating link prediction algorithms when applied to large graphs,\nby tackling the problem of class imbalance. We empirically evaluate the\nproposed methodology and, building on these findings, make a case for its\nimportance on the evaluation of large-scale graph processing.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 11:07:51 GMT"}, {"version": "v2", "created": "Fri, 25 Nov 2016 08:52:02 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Garcia-Gasulla", "Dario", ""], ["Ayguad\u00e9", "Eduard", ""], ["Labarta", "Jes\u00fas", ""], ["Cort\u00e9s", "Ulises", ""]]}, {"id": "1611.00549", "submitter": "Oliver Cliff", "authors": "Oliver M. Cliff and Mikhail Prokopenko and Robert Fitch", "title": "Inferring Coupling of Distributed Dynamical Systems via Transfer Entropy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we are interested in structure learning for a set of spatially\ndistributed dynamical systems, where individual subsystems are coupled via\nlatent variables and observed through a filter. We represent this model as a\ndirected acyclic graph (DAG) that characterises the unidirectional coupling\nbetween subsystems. Standard approaches to structure learning are not\napplicable in this framework due to the hidden variables, however we can\nexploit the properties of certain dynamical systems to formulate exact methods\nbased on state space reconstruction. We approach the problem by using\nreconstruction theorems to analytically derive a tractable expression for the\nKL-divergence of a candidate DAG from the observed dataset. We show this\nmeasure can be decomposed as a function of two information-theoretic measures,\ntransfer entropy and stochastic interaction. We then present two mathematically\nrobust scoring functions based on transfer entropy and statistical independence\ntests. These results support the previously held conjecture that transfer\nentropy can be used to infer effective connectivity in complex networks.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 11:23:54 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Cliff", "Oliver M.", ""], ["Prokopenko", "Mikhail", ""], ["Fitch", "Robert", ""]]}, {"id": "1611.00576", "submitter": "Florentin Smarandache", "authors": "W. B. Vasantha Kandasamy, Ilanthenral K, Florentin Smarandache", "title": "Strong Neutrosophic Graphs and Subgraph Topological Subspaces", "comments": "226 pages, many graphs, Europa Belgique, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this book authors for the first time introduce the notion of strong\nneutrosophic graphs. They are very different from the usual graphs and\nneutrosophic graphs. Using these new structures special subgraph topological\nspaces are defined. Further special lattice graph of subgraphs of these graphs\nare defined and described. Several interesting properties using subgraphs of a\nstrong neutrosophic graph are obtained. Several open conjectures are proposed.\nThese new class of strong neutrosophic graphs will certainly find applications\nin Neutrosophic Cognitive Maps (NCM), Neutrosophic Relational Maps (NRM) and\nNeutrosophic Relational Equations (NRE) with appropriate modifications.\n", "versions": [{"version": "v1", "created": "Sun, 30 Oct 2016 15:10:55 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Kandasamy", "W. B. Vasantha", ""], ["K", "Ilanthenral", ""], ["Smarandache", "Florentin", ""]]}, {"id": "1611.00577", "submitter": "Elham Shadkam", "authors": "Zeinab Borhanifar and Elham Shadkam", "title": "The new hybrid COAW method for solving multi-objective problems", "comments": null, "journal-ref": null, "doi": "10.5121/ijfcst.2015.5602", "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this article using Cuckoo Optimization Algorithm and simple additive\nweighting method the hybrid COAW algorithm is presented to solve\nmulti-objective problems. Cuckoo algorithm is an efficient and structured\nmethod for solving nonlinear continuous problems. The created Pareto frontiers\nof the COAW proposed algorithm are exact and have good dispersion. This method\nhas a high speed in finding the Pareto frontiers and identifies the beginning\nand end points of Pareto frontiers properly. In order to validation the\nproposed algorithm, several experimental problems were analyzed. The results of\nwhich indicate the proper effectiveness of COAW algorithm for solving\nmulti-objective problems.\n", "versions": [{"version": "v1", "created": "Wed, 6 Jan 2016 16:32:47 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Borhanifar", "Zeinab", ""], ["Shadkam", "Elham", ""]]}, {"id": "1611.00625", "submitter": "Gabriel Synnaeve", "authors": "Gabriel Synnaeve, Nantas Nardelli, Alex Auvolat, Soumith Chintala,\n  Timoth\\'ee Lacroix, Zeming Lin, Florian Richoux, Nicolas Usunier", "title": "TorchCraft: a Library for Machine Learning Research on Real-Time\n  Strategy Games", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present TorchCraft, a library that enables deep learning research on\nReal-Time Strategy (RTS) games such as StarCraft: Brood War, by making it\neasier to control these games from a machine learning framework, here Torch.\nThis white paper argues for using RTS games as a benchmark for AI research, and\ndescribes the design and components of TorchCraft.\n", "versions": [{"version": "v1", "created": "Tue, 1 Nov 2016 05:01:24 GMT"}, {"version": "v2", "created": "Thu, 3 Nov 2016 21:54:28 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Synnaeve", "Gabriel", ""], ["Nardelli", "Nantas", ""], ["Auvolat", "Alex", ""], ["Chintala", "Soumith", ""], ["Lacroix", "Timoth\u00e9e", ""], ["Lin", "Zeming", ""], ["Richoux", "Florian", ""], ["Usunier", "Nicolas", ""]]}, {"id": "1611.00685", "submitter": "Jan Feyereisl", "authors": "Marek Rosa, Jan Feyereisl and The GoodAI Collective", "title": "A Framework for Searching for General Artificial Intelligence", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There is a significant lack of unified approaches to building generally\nintelligent machines. The majority of current artificial intelligence research\noperates within a very narrow field of focus, frequently without considering\nthe importance of the 'big picture'. In this document, we seek to describe and\nunify principles that guide the basis of our development of general artificial\nintelligence. These principles revolve around the idea that intelligence is a\ntool for searching for general solutions to problems. We define intelligence as\nthe ability to acquire skills that narrow this search, diversify it and help\nsteer it to more promising areas. We also provide suggestions for studying,\nmeasuring, and testing the various skills and abilities that a human-level\nintelligent machine needs to acquire. The document aims to be both\nimplementation agnostic, and to provide an analytic, systematic, and scalable\nway to generate hypotheses that we believe are needed to meet the necessary\nconditions in the search for general artificial intelligence. We believe that\nsuch a framework is an important stepping stone for bringing together\ndefinitions, highlighting open problems, connecting researchers willing to\ncollaborate, and for unifying the arguably most significant search of this\ncentury.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 17:02:14 GMT"}], "update_date": "2016-11-03", "authors_parsed": [["Rosa", "Marek", ""], ["Feyereisl", "Jan", ""], ["Collective", "The GoodAI", ""]]}, {"id": "1611.00736", "submitter": "Wojciech Zaremba", "authors": "Eric Price, Wojciech Zaremba, Ilya Sutskever", "title": "Extensions and Limitations of the Neural GPU", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Neural GPU is a recent model that can learn algorithms such as\nmulti-digit binary addition and binary multiplication in a way that generalizes\nto inputs of arbitrary length. We show that there are two simple ways of\nimproving the performance of the Neural GPU: by carefully designing a\ncurriculum, and by increasing model size. The latter requires a memory\nefficient implementation, as a naive implementation of the Neural GPU is memory\nintensive. We find that these techniques increase the set of algorithmic\nproblems that can be solved by the Neural GPU: we have been able to learn to\nperform all the arithmetic operations (and generalize to arbitrarily long\nnumbers) when the arguments are given in the decimal representation (which,\nsurprisingly, has not been possible before). We have also been able to train\nthe Neural GPU to evaluate long arithmetic expressions with multiple operands\nthat require respecting the precedence order of the operands, although these\nhave succeeded only in their binary representation, and not with perfect\naccuracy.\n  In addition, we gain insight into the Neural GPU by investigating its failure\nmodes. We find that Neural GPUs that correctly generalize to arbitrarily long\nnumbers still fail to compute the correct answer on highly-symmetric, atypical\ninputs: for example, a Neural GPU that achieves near-perfect generalization on\ndecimal multiplication of up to 100-digit long numbers can fail on\n$000000\\dots002 \\times 000000\\dots002$ while succeeding at $2 \\times 2$. These\nfailure modes are reminiscent of adversarial examples.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 19:18:17 GMT"}, {"version": "v2", "created": "Fri, 4 Nov 2016 20:46:40 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Price", "Eric", ""], ["Zaremba", "Wojciech", ""], ["Sutskever", "Ilya", ""]]}, {"id": "1611.00791", "submitter": "Hyrum Anderson", "authors": "Jonathan Woodbridge, Hyrum S. Anderson, Anjum Ahuja and Daniel Grant", "title": "Predicting Domain Generation Algorithms with Long Short-Term Memory\n  Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Various families of malware use domain generation algorithms (DGAs) to\ngenerate a large number of pseudo-random domain names to connect to a command\nand control (C&C) server. In order to block DGA C&C traffic, security\norganizations must first discover the algorithm by reverse engineering malware\nsamples, then generating a list of domains for a given seed. The domains are\nthen either preregistered or published in a DNS blacklist. This process is not\nonly tedious, but can be readily circumvented by malware authors using a large\nnumber of seeds in algorithms with multivariate recurrence properties (e.g.,\nbanjori) or by using a dynamic list of seeds (e.g., bedep). Another technique\nto stop malware from using DGAs is to intercept DNS queries on a network and\npredict whether domains are DGA generated. Such a technique will alert network\nadministrators to the presence of malware on their networks. In addition, if\nthe predictor can also accurately predict the family of DGAs, then network\nadministrators can also be alerted to the type of malware that is on their\nnetworks. This paper presents a DGA classifier that leverages long short-term\nmemory (LSTM) networks to predict DGAs and their respective families without\nthe need for a priori feature extraction. Results are significantly better than\nstate-of-the-art techniques, providing 0.9993 area under the receiver operating\ncharacteristic curve for binary classification and a micro-averaged F1 score of\n0.9906. In other terms, the LSTM technique can provide a 90% detection rate\nwith a 1:10000 false positive (FP) rate---a twenty times FP improvement over\ncomparable methods. Experiments in this paper are run on open datasets and code\nsnippets are provided to reproduce the results.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 20:34:56 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Woodbridge", "Jonathan", ""], ["Anderson", "Hyrum S.", ""], ["Ahuja", "Anjum", ""], ["Grant", "Daniel", ""]]}, {"id": "1611.00862", "submitter": "Paul Weng", "authors": "Hugo Gilbert and Paul Weng", "title": "Quantile Reinforcement Learning", "comments": "AWRL 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In reinforcement learning, the standard criterion to evaluate policies in a\nstate is the expectation of (discounted) sum of rewards. However, this\ncriterion may not always be suitable, we consider an alternative criterion\nbased on the notion of quantiles. In the case of episodic reinforcement\nlearning problems, we propose an algorithm based on stochastic approximation\nwith two timescales. We evaluate our proposition on a simple model of the TV\nshow, Who wants to be a millionaire.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 02:28:53 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Gilbert", "Hugo", ""], ["Weng", "Paul", ""]]}, {"id": "1611.00873", "submitter": "Qiang Lyu", "authors": "Qiang Lyu, Yixin Chen, Zhaorong Li, Zhicheng Cui, Ling Chen, Xing\n  Zhang, Haihua Shen", "title": "Extracting Actionability from Machine Learning Models by Sub-optimal\n  Deterministic Planning", "comments": "16 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A main focus of machine learning research has been improving the\ngeneralization accuracy and efficiency of prediction models. Many models such\nas SVM, random forest, and deep neural nets have been proposed and achieved\ngreat success. However, what emerges as missing in many applications is\nactionability, i.e., the ability to turn prediction results into actions. For\nexample, in applications such as customer relationship management, clinical\nprediction, and advertisement, the users need not only accurate prediction, but\nalso actionable instructions which can transfer an input to a desirable goal\n(e.g., higher profit repays, lower morbidity rates, higher ads hit rates).\nExisting effort in deriving such actionable knowledge is few and limited to\nsimple action models which restricted to only change one attribute for each\naction. The dilemma is that in many real applications those action models are\noften more complex and harder to extract an optimal solution.\n  In this paper, we propose a novel approach that achieves actionability by\ncombining learning with planning, two core areas of AI. In particular, we\npropose a framework to extract actionable knowledge from random forest, one of\nthe most widely used and best off-the-shelf classifiers. We formulate the\nactionability problem to a sub-optimal action planning (SOAP) problem, which is\nto find a plan to alter certain features of a given input so that the random\nforest would yield a desirable output, while minimizing the total costs of\nactions. Technically, the SOAP problem is formulated in the SAS+ planning\nformalism, and solved using a Max-SAT based approach. Our experimental results\ndemonstrate the effectiveness and efficiency of the proposed approach on a\npersonal credit dataset and other benchmarks. Our work represents a new\napplication of automated planning on an emerging and challenging machine\nlearning paradigm.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 03:53:41 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Lyu", "Qiang", ""], ["Chen", "Yixin", ""], ["Li", "Zhaorong", ""], ["Cui", "Zhicheng", ""], ["Chen", "Ling", ""], ["Zhang", "Xing", ""], ["Shen", "Haihua", ""]]}, {"id": "1611.00890", "submitter": "Jeremy Every Mr", "authors": "Jeremy Every, Li Li, Youguang G. Guo, David G. Dorrell", "title": "Maximizing Investment Value of Small-Scale PV in a Smart Grid\n  Environment", "comments": "To appear the proceedings of the 5th International Conference for\n  Renewable Energy Research and Applications (ICRERA2016), Birmingham, United\n  Kingdom. 6 pages. 3 figures", "journal-ref": "2016 IEEE International Conference on Renewable Energy Research\n  and Applications (ICRERA), 2016, pp. 385-390", "doi": "10.1109/ICRERA.2016.7884366", "report-no": null, "categories": "math.OC cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Determining the optimal size and orientation of small-scale residential based\nPV arrays will become increasingly complex in the future smart grid environment\nwith the introduction of smart meters and dynamic tariffs. However consumers\ncan leverage the availability of smart meter data to conduct a more detailed\nexploration of PV investment options for their particular circumstances. In\nthis paper, an optimization method for PV orientation and sizing is proposed\nwhereby maximizing the PV investment value is set as the defining objective.\nSolar insolation and PV array models are described to form the basis of the PV\narray optimization strategy. A constrained particle swarm optimization\nalgorithm is selected due to its strong performance in non-linear applications.\nThe optimization algorithm is applied to real-world metered data to quantify\nthe possible investment value of a PV installation under different energy\nretailers and tariff structures. The arrangement with the highest value is\ndetermined to enable prospective small-scale PV investors to select the most\ncost-effective system.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 05:57:01 GMT"}], "update_date": "2017-03-29", "authors_parsed": [["Every", "Jeremy", ""], ["Li", "Li", ""], ["Guo", "Youguang G.", ""], ["Dorrell", "David G.", ""]]}, {"id": "1611.01080", "submitter": "Giuliano Armano", "authors": "Giuliano Armano", "title": "Probabilistic Modeling of Progressive Filtering", "comments": "The article entitled Modeling Progressive Filtering, published on\n  Fundamenta Informaticae (Vol. 138, Issue 3, pp. 285-320, July 2015), has been\n  derived from this extended report", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Progressive filtering is a simple way to perform hierarchical classification,\ninspired by the behavior that most humans put into practice while attempting to\ncategorize an item according to an underlying taxonomy. Each node of the\ntaxonomy being associated with a different category, one may visualize the\ncategorization process by looking at the item going downwards through all the\nnodes that accept it as belonging to the corresponding category. This paper is\naimed at modeling the progressive filtering technique from a probabilistic\nperspective, in a hierarchical text categorization setting. As a result, the\ndesigner of a system based on progressive filtering should be facilitated in\nthe task of devising, training, and testing it.\n", "versions": [{"version": "v1", "created": "Thu, 3 Nov 2016 16:31:32 GMT"}], "update_date": "2016-11-04", "authors_parsed": [["Armano", "Giuliano", ""]]}, {"id": "1611.01423", "submitter": "Miltiadis Allamanis", "authors": "Miltiadis Allamanis, Pankajan Chanthirasegaran, Pushmeet Kohli,\n  Charles Sutton", "title": "Learning Continuous Semantic Representations of Symbolic Expressions", "comments": "Accepted to ICML 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Combining abstract, symbolic reasoning with continuous neural reasoning is a\ngrand challenge of representation learning. As a step in this direction, we\npropose a new architecture, called neural equivalence networks, for the problem\nof learning continuous semantic representations of algebraic and logical\nexpressions. These networks are trained to represent semantic equivalence, even\nof expressions that are syntactically very different. The challenge is that\nsemantic representations must be computed in a syntax-directed manner, because\nsemantics is compositional, but at the same time, small changes in syntax can\nlead to very large changes in semantics, which can be difficult for continuous\nneural architectures. We perform an exhaustive evaluation on the task of\nchecking equivalence on a highly diverse class of symbolic algebraic and\nboolean expression types, showing that our model significantly outperforms\nexisting architectures.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 15:30:43 GMT"}, {"version": "v2", "created": "Sat, 10 Jun 2017 19:18:55 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Allamanis", "Miltiadis", ""], ["Chanthirasegaran", "Pankajan", ""], ["Kohli", "Pushmeet", ""], ["Sutton", "Charles", ""]]}, {"id": "1611.01455", "submitter": "Hanock Kwak", "authors": "Hanock Kwak and Byoung-Tak Zhang", "title": "Ways of Conditioning Generative Adversarial Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The GANs are generative models whose random samples realistically reflect\nnatural images. It also can generate samples with specific attributes by\nconcatenating a condition vector into the input, yet research on this field is\nnot well studied. We propose novel methods of conditioning generative\nadversarial networks (GANs) that achieve state-of-the-art results on MNIST and\nCIFAR-10. We mainly introduce two models: an information retrieving model that\nextracts conditional information from the samples, and a spatial bilinear\npooling model that forms bilinear features derived from the spatial cross\nproduct of an image and a condition vector. These methods significantly enhance\nlog-likelihood of test data under the conditional distributions compared to the\nmethods of concatenation.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 17:08:54 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Kwak", "Hanock", ""], ["Zhang", "Byoung-Tak", ""]]}, {"id": "1611.01491", "submitter": "Anirbit Mukherjee", "authors": "Raman Arora, Amitabh Basu, Poorya Mianjy and Anirbit Mukherjee", "title": "Understanding Deep Neural Networks with Rectified Linear Units", "comments": "The poly(data) exact training algorithm has been improved to now be\n  applicable to any single hidden layer R^n-> R ReLU DNN and there is a cleaner\n  pseudocode for it given on page 8. Also now on page 7 there is a more precise\n  description about when and how the Zonotope construction improves on the\n  Theorem 4 of this paper, arXiv:1402.1869", "journal-ref": "ICLR 2028", "doi": null, "report-no": null, "categories": "cs.LG cond-mat.dis-nn cs.AI cs.CC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we investigate the family of functions representable by deep\nneural networks (DNN) with rectified linear units (ReLU). We give an algorithm\nto train a ReLU DNN with one hidden layer to *global optimality* with runtime\npolynomial in the data size albeit exponential in the input dimension. Further,\nwe improve on the known lower bounds on size (from exponential to super\nexponential) for approximating a ReLU deep net function by a shallower ReLU\nnet. Our gap theorems hold for smoothly parametrized families of \"hard\"\nfunctions, contrary to countable, discrete families known in the literature. An\nexample consequence of our gap theorems is the following: for every natural\nnumber $k$ there exists a function representable by a ReLU DNN with $k^2$\nhidden layers and total size $k^3$, such that any ReLU DNN with at most $k$\nhidden layers will require at least $\\frac{1}{2}k^{k+1}-1$ total nodes.\nFinally, for the family of $\\mathbb{R}^n\\to \\mathbb{R}$ DNNs with ReLU\nactivations, we show a new lowerbound on the number of affine pieces, which is\nlarger than previous constructions in certain regimes of the network\narchitecture and most distinctively our lowerbound is demonstrated by an\nexplicit construction of a *smoothly parameterized* family of functions\nattaining this scaling. Our construction utilizes the theory of zonotopes from\npolyhedral theory.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 18:54:50 GMT"}, {"version": "v2", "created": "Fri, 11 Nov 2016 20:25:56 GMT"}, {"version": "v3", "created": "Sat, 26 Nov 2016 17:38:11 GMT"}, {"version": "v4", "created": "Mon, 29 May 2017 20:06:50 GMT"}, {"version": "v5", "created": "Tue, 18 Jul 2017 17:17:14 GMT"}, {"version": "v6", "created": "Wed, 28 Feb 2018 02:23:47 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Arora", "Raman", ""], ["Basu", "Amitabh", ""], ["Mianjy", "Poorya", ""], ["Mukherjee", "Anirbit", ""]]}, {"id": "1611.01504", "submitter": "Krzysztof Chalupka", "authors": "Krzysztof Chalupka, Frederick Eberhardt and Pietro Perona", "title": "Estimating Causal Direction and Confounding of Two Discrete Variables", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to classify the causal relationship between two discrete\nvariables given only the joint distribution of the variables, acknowledging\nthat the method is subject to an inherent baseline error. We assume that the\ncausal system is acyclicity, but we do allow for hidden common causes. Our\nalgorithm presupposes that the probability distributions $P(C)$ of a cause $C$\nis independent from the probability distribution $P(E\\mid C)$ of the\ncause-effect mechanism. While our classifier is trained with a Bayesian\nassumption of flat hyperpriors, we do not make this assumption about our test\ndata. This work connects to recent developments on the identifiability of\ncausal models over continuous variables under the assumption of \"independent\nmechanisms\". Carefully-commented Python notebooks that reproduce all our\nexperiments are available online at\nhttp://vision.caltech.edu/~kchalupk/code.html.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 19:33:35 GMT"}], "update_date": "2016-11-07", "authors_parsed": [["Chalupka", "Krzysztof", ""], ["Eberhardt", "Frederick", ""], ["Perona", "Pietro", ""]]}, {"id": "1611.01553", "submitter": "Vedad Hadzic", "authors": "Roderick Bloem, Nicolas Braud-Santoni and Vedad Hadzic", "title": "QBF Solving by Counterexample-guided Expansion", "comments": "This is a **very** old version of the paper arXiv:1807.08964 and\n  should be taken down. I did not know you could just replace papers, and did\n  not know whether I could change authors and similar, so that is why I made a\n  different submission. Please take it down", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://creativecommons.org/publicdomain/zero/1.0/", "abstract": "  We introduce a novel generalization of Counterexample-Guided Inductive\nSynthesis (CEGIS) and instantiate it to yield a novel, competitive algorithm\nfor solving Quantified Boolean Formulas (QBF). Current QBF solvers based on\ncounterexample-guided expansion use a recursive approach which scales poorly\nwith the number of quantifier alternations. Our generalization of CEGIS removes\nthe need for this recursive approach, and we instantiate it to yield a simple\nand efficient algorithm for QBF solving. Lastly, this research is supported by\na competitive, though straightforward, implementation of the algorithm, making\nit possible to study the practical impact of our algorithm design decisions,\nalong with various optimizations.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 22:08:40 GMT"}, {"version": "v2", "created": "Tue, 8 Nov 2016 15:26:12 GMT"}, {"version": "v3", "created": "Fri, 25 Nov 2016 17:48:36 GMT"}, {"version": "v4", "created": "Fri, 27 Jul 2018 10:16:43 GMT"}], "update_date": "2018-07-30", "authors_parsed": [["Bloem", "Roderick", ""], ["Braud-Santoni", "Nicolas", ""], ["Hadzic", "Vedad", ""]]}, {"id": "1611.01576", "submitter": "James Bradbury", "authors": "James Bradbury, Stephen Merity, Caiming Xiong, Richard Socher", "title": "Quasi-Recurrent Neural Networks", "comments": "Submitted to conference track at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks are a powerful tool for modeling sequential data,\nbut the dependence of each timestep's computation on the previous timestep's\noutput limits parallelism and makes RNNs unwieldy for very long sequences. We\nintroduce quasi-recurrent neural networks (QRNNs), an approach to neural\nsequence modeling that alternates convolutional layers, which apply in parallel\nacross timesteps, and a minimalist recurrent pooling function that applies in\nparallel across channels. Despite lacking trainable recurrent layers, stacked\nQRNNs have better predictive accuracy than stacked LSTMs of the same hidden\nsize. Due to their increased parallelism, they are up to 16 times faster at\ntrain and test time. Experiments on language modeling, sentiment\nclassification, and character-level neural machine translation demonstrate\nthese advantages and underline the viability of QRNNs as a basic building block\nfor a variety of sequence tasks.\n", "versions": [{"version": "v1", "created": "Sat, 5 Nov 2016 00:31:25 GMT"}, {"version": "v2", "created": "Mon, 21 Nov 2016 20:52:34 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Bradbury", "James", ""], ["Merity", "Stephen", ""], ["Xiong", "Caiming", ""], ["Socher", "Richard", ""]]}, {"id": "1611.01578", "submitter": "Quoc Le", "authors": "Barret Zoph and Quoc V. Le", "title": "Neural Architecture Search with Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neural networks are powerful and flexible models that work well for many\ndifficult learning tasks in image, speech and natural language understanding.\nDespite their success, neural networks are still hard to design. In this paper,\nwe use a recurrent network to generate the model descriptions of neural\nnetworks and train this RNN with reinforcement learning to maximize the\nexpected accuracy of the generated architectures on a validation set. On the\nCIFAR-10 dataset, our method, starting from scratch, can design a novel network\narchitecture that rivals the best human-invented architecture in terms of test\nset accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is\n0.09 percent better and 1.05x faster than the previous state-of-the-art model\nthat used a similar architectural scheme. On the Penn Treebank dataset, our\nmodel can compose a novel recurrent cell that outperforms the widely-used LSTM\ncell, and other state-of-the-art baselines. Our cell achieves a test set\nperplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than\nthe previous state-of-the-art model. The cell can also be transferred to the\ncharacter language modeling task on PTB and achieves a state-of-the-art\nperplexity of 1.214.\n", "versions": [{"version": "v1", "created": "Sat, 5 Nov 2016 00:41:37 GMT"}, {"version": "v2", "created": "Wed, 15 Feb 2017 05:28:05 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Zoph", "Barret", ""], ["Le", "Quoc V.", ""]]}, {"id": "1611.01587", "submitter": "Kazuma Hashimoto", "authors": "Kazuma Hashimoto, Caiming Xiong, Yoshimasa Tsuruoka, Richard Socher", "title": "A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks", "comments": "Accepted as a full paper at the 2017 Conference on Empirical Methods\n  in Natural Language Processing (EMNLP 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Transfer and multi-task learning have traditionally focused on either a\nsingle source-target pair or very few, similar tasks. Ideally, the linguistic\nlevels of morphology, syntax and semantics would benefit each other by being\ntrained in a single model. We introduce a joint many-task model together with a\nstrategy for successively growing its depth to solve increasingly complex\ntasks. Higher layers include shortcut connections to lower-level task\npredictions to reflect linguistic hierarchies. We use a simple regularization\nterm to allow for optimizing all model weights to improve one task's loss\nwithout exhibiting catastrophic interference of the other tasks. Our single\nend-to-end model obtains state-of-the-art or competitive results on five\ndifferent tasks from tagging, parsing, relatedness, and entailment tasks.\n", "versions": [{"version": "v1", "created": "Sat, 5 Nov 2016 01:59:29 GMT"}, {"version": "v2", "created": "Fri, 11 Nov 2016 01:16:07 GMT"}, {"version": "v3", "created": "Sat, 19 Nov 2016 00:20:12 GMT"}, {"version": "v4", "created": "Sun, 16 Apr 2017 22:38:21 GMT"}, {"version": "v5", "created": "Mon, 24 Jul 2017 14:41:16 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Hashimoto", "Kazuma", ""], ["Xiong", "Caiming", ""], ["Tsuruoka", "Yoshimasa", ""], ["Socher", "Richard", ""]]}, {"id": "1611.01604", "submitter": "Victor Zhong", "authors": "Caiming Xiong, Victor Zhong, Richard Socher", "title": "Dynamic Coattention Networks For Question Answering", "comments": "14 pages, 7 figures, International Conference on Learning\n  Representations 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Several deep learning models have been proposed for question answering.\nHowever, due to their single-pass nature, they have no way to recover from\nlocal maxima corresponding to incorrect answers. To address this problem, we\nintroduce the Dynamic Coattention Network (DCN) for question answering. The DCN\nfirst fuses co-dependent representations of the question and the document in\norder to focus on relevant parts of both. Then a dynamic pointing decoder\niterates over potential answer spans. This iterative procedure enables the\nmodel to recover from initial local maxima corresponding to incorrect answers.\nOn the Stanford question answering dataset, a single DCN model improves the\nprevious state of the art from 71.0% F1 to 75.9%, while a DCN ensemble obtains\n80.4% F1.\n", "versions": [{"version": "v1", "created": "Sat, 5 Nov 2016 04:53:40 GMT"}, {"version": "v2", "created": "Thu, 17 Nov 2016 19:58:22 GMT"}, {"version": "v3", "created": "Mon, 13 Feb 2017 23:00:32 GMT"}, {"version": "v4", "created": "Tue, 6 Mar 2018 22:45:53 GMT"}], "update_date": "2018-03-08", "authors_parsed": [["Xiong", "Caiming", ""], ["Zhong", "Victor", ""], ["Socher", "Richard", ""]]}, {"id": "1611.01626", "submitter": "Brendan O'Donoghue", "authors": "Brendan O'Donoghue, Remi Munos, Koray Kavukcuoglu and Volodymyr Mnih", "title": "Combining policy gradient and Q-learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Policy gradient is an efficient technique for improving a policy in a\nreinforcement learning setting. However, vanilla online variants are on-policy\nonly and not able to take advantage of off-policy data. In this paper we\ndescribe a new technique that combines policy gradient with off-policy\nQ-learning, drawing experience from a replay buffer. This is motivated by\nmaking a connection between the fixed points of the regularized policy gradient\nalgorithm and the Q-values. This connection allows us to estimate the Q-values\nfrom the action preferences of the policy, to which we apply Q-learning\nupdates. We refer to the new technique as 'PGQL', for policy gradient and\nQ-learning. We also establish an equivalency between action-value fitting\ntechniques and actor-critic algorithms, showing that regularized policy\ngradient techniques can be interpreted as advantage function learning\nalgorithms. We conclude with some numerical examples that demonstrate improved\ndata efficiency and stability of PGQL. In particular, we tested PGQL on the\nfull suite of Atari games and achieved performance exceeding that of both\nasynchronous advantage actor-critic (A3C) and Q-learning.\n", "versions": [{"version": "v1", "created": "Sat, 5 Nov 2016 10:49:37 GMT"}, {"version": "v2", "created": "Mon, 6 Mar 2017 12:38:42 GMT"}, {"version": "v3", "created": "Fri, 7 Apr 2017 15:20:05 GMT"}], "update_date": "2017-04-10", "authors_parsed": [["O'Donoghue", "Brendan", ""], ["Munos", "Remi", ""], ["Kavukcuoglu", "Koray", ""], ["Mnih", "Volodymyr", ""]]}, {"id": "1611.01652", "submitter": "Jonas Degrave", "authors": "Jonas Degrave, Michiel Hermans, Joni Dambre, Francis wyffels", "title": "A Differentiable Physics Engine for Deep Learning in Robotics", "comments": "Submitted for International Conference on Learning Representations\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important field in robotics is the optimization of controllers. Currently,\nrobots are often treated as a black box in this optimization process, which is\nthe reason why derivative-free optimization methods such as evolutionary\nalgorithms or reinforcement learning are omnipresent. When gradient-based\nmethods are used, models are kept small or rely on finite difference\napproximations for the Jacobian. This method quickly grows expensive with\nincreasing numbers of parameters, such as found in deep learning. We propose\nthe implementation of a modern physics engine, which can differentiate control\nparameters. This engine is implemented for both CPU and GPU. Firstly, this\npaper shows how such an engine speeds up the optimization process, even for\nsmall problems. Furthermore, it explains why this is an alternative approach to\ndeep Q-learning, for using deep learning in robotics. Finally, we argue that\nthis is a big step for deep learning in robotics, as it opens up new\npossibilities to optimize robots, both in hardware and software.\n", "versions": [{"version": "v1", "created": "Sat, 5 Nov 2016 13:34:58 GMT"}, {"version": "v2", "created": "Sat, 24 Nov 2018 14:41:48 GMT"}], "update_date": "2018-11-27", "authors_parsed": [["Degrave", "Jonas", ""], ["Hermans", "Michiel", ""], ["Dambre", "Joni", ""], ["wyffels", "Francis", ""]]}, {"id": "1611.01702", "submitter": "Adji Bousso Dieng", "authors": "Adji B. Dieng, Chong Wang, Jianfeng Gao, John Paisley", "title": "TopicRNN: A Recurrent Neural Network with Long-Range Semantic Dependency", "comments": "International Conference on Learning Representations", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose TopicRNN, a recurrent neural network (RNN)-based\nlanguage model designed to directly capture the global semantic meaning\nrelating words in a document via latent topics. Because of their sequential\nnature, RNNs are good at capturing the local structure of a word sequence -\nboth semantic and syntactic - but might face difficulty remembering long-range\ndependencies. Intuitively, these long-range dependencies are of semantic\nnature. In contrast, latent topic models are able to capture the global\nunderlying semantic structure of a document but do not account for word\nordering. The proposed TopicRNN model integrates the merits of RNNs and latent\ntopic models: it captures local (syntactic) dependencies using an RNN and\nglobal (semantic) dependencies using latent topics. Unlike previous work on\ncontextual RNN language modeling, our model is learned end-to-end. Empirical\nresults on word prediction show that TopicRNN outperforms existing contextual\nRNN baselines. In addition, TopicRNN can be used as an unsupervised feature\nextractor for documents. We do this for sentiment analysis on the IMDB movie\nreview dataset and report an error rate of $6.28\\%$. This is comparable to the\nstate-of-the-art $5.91\\%$ resulting from a semi-supervised approach. Finally,\nTopicRNN also yields sensible topics, making it a useful alternative to\ndocument models such as latent Dirichlet allocation.\n", "versions": [{"version": "v1", "created": "Sat, 5 Nov 2016 21:25:07 GMT"}, {"version": "v2", "created": "Mon, 27 Feb 2017 03:03:38 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Dieng", "Adji B.", ""], ["Wang", "Chong", ""], ["Gao", "Jianfeng", ""], ["Paisley", "John", ""]]}, {"id": "1611.01708", "submitter": "Feras Saad", "authors": "Feras Saad, Vikash Mansinghka", "title": "Detecting Dependencies in Sparse, Multivariate Databases Using\n  Probabilistic Programming and Non-parametric Bayes", "comments": null, "journal-ref": "Proceedings of the 20th International Conference on Artificial\n  Intelligence and Statistics, PMLR 54:632-641, 2017", "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Datasets with hundreds of variables and many missing values are commonplace.\nIn this setting, it is both statistically and computationally challenging to\ndetect true predictive relationships between variables and also to suppress\nfalse positives. This paper proposes an approach that combines probabilistic\nprogramming, information theory, and non-parametric Bayes. It shows how to use\nBayesian non-parametric modeling to (i) build an ensemble of joint probability\nmodels for all the variables; (ii) efficiently detect marginal independencies;\nand (iii) estimate the conditional mutual information between arbitrary subsets\nof variables, subject to a broad class of constraints. Users can access these\ncapabilities using BayesDB, a probabilistic programming platform for\nprobabilistic data analysis, by writing queries in a simple, SQL-like language.\nThis paper demonstrates empirically that the method can (i) detect\ncontext-specific (in)dependencies on challenging synthetic problems and (ii)\nyield improved sensitivity and specificity over baselines from statistics and\nmachine learning, on a real-world database of over 300 sparsely observed\nindicators of macroeconomic development and public health.\n", "versions": [{"version": "v1", "created": "Sat, 5 Nov 2016 23:02:25 GMT"}, {"version": "v2", "created": "Mon, 27 Mar 2017 03:07:49 GMT"}], "update_date": "2018-04-03", "authors_parsed": [["Saad", "Feras", ""], ["Mansinghka", "Vikash", ""]]}, {"id": "1611.01711", "submitter": "Leopoldo Bertossi", "authors": "Leopoldo Bertossi and Babak Salimi", "title": "Causes for Query Answers from Databases: Datalog Abduction,\n  View-Updates, and Integrity Constraints", "comments": "To appear in International Journal of Approximate Reasoning. Extended\n  version of \"Flairs'16\" and \"UAI'15 WS on Causality\" papers", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causality has been recently introduced in databases, to model, characterize,\nand possibly compute causes for query answers. Connections between QA-causality\nand consistency-based diagnosis and database repairs (wrt. integrity constraint\nviolations) have already been established. In this work we establish precise\nconnections between QA-causality and both abductive diagnosis and the\nview-update problem in databases, allowing us to obtain new algorithmic and\ncomplexity results for QA-causality. We also obtain new results on the\ncomplexity of view-conditioned causality, and investigate the notion of\nQA-causality in the presence of integrity constraints, obtaining complexity\nresults from a connection with view-conditioned causality. The abduction\nconnection under integrity constraints allows us to obtain algorithmic tools\nfor QA-causality.\n", "versions": [{"version": "v1", "created": "Sun, 6 Nov 2016 00:35:09 GMT"}, {"version": "v2", "created": "Fri, 14 Jul 2017 23:02:08 GMT"}, {"version": "v3", "created": "Mon, 31 Jul 2017 17:58:01 GMT"}], "update_date": "2017-08-01", "authors_parsed": [["Bertossi", "Leopoldo", ""], ["Salimi", "Babak", ""]]}, {"id": "1611.01747", "submitter": "Shuohang Wang", "authors": "Shuohang Wang and Jing Jiang", "title": "A Compare-Aggregate Model for Matching Text Sequences", "comments": "11 pages, 2 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many NLP tasks including machine comprehension, answer selection and text\nentailment require the comparison between sequences. Matching the important\nunits between sequences is a key to solve these problems. In this paper, we\npresent a general \"compare-aggregate\" framework that performs word-level\nmatching followed by aggregation using Convolutional Neural Networks. We\nparticularly focus on the different comparison functions we can use to match\ntwo vectors. We use four different datasets to evaluate the model. We find that\nsome simple comparison functions based on element-wise operations can work\nbetter than standard neural network and neural tensor network.\n", "versions": [{"version": "v1", "created": "Sun, 6 Nov 2016 09:50:24 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Wang", "Shuohang", ""], ["Jiang", "Jing", ""]]}, {"id": "1611.01779", "submitter": "Alexey Dosovitskiy", "authors": "Alexey Dosovitskiy and Vladlen Koltun", "title": "Learning to Act by Predicting the Future", "comments": "Published as a conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an approach to sensorimotor control in immersive environments. Our\napproach utilizes a high-dimensional sensory stream and a lower-dimensional\nmeasurement stream. The cotemporal structure of these streams provides a rich\nsupervisory signal, which enables training a sensorimotor control model by\ninteracting with the environment. The model is trained using supervised\nlearning techniques, but without extraneous supervision. It learns to act based\non raw sensory input from a complex three-dimensional environment. The\npresented formulation enables learning without a fixed goal at training time,\nand pursuing dynamically changing goals at test time. We conduct extensive\nexperiments in three-dimensional simulations based on the classical\nfirst-person game Doom. The results demonstrate that the presented approach\noutperforms sophisticated prior formulations, particularly on challenging\ntasks. The results also show that trained models successfully generalize across\nenvironments and goals. A model trained using the presented approach won the\nFull Deathmatch track of the Visual Doom AI Competition, which was held in\npreviously unseen environments.\n", "versions": [{"version": "v1", "created": "Sun, 6 Nov 2016 13:45:00 GMT"}, {"version": "v2", "created": "Tue, 14 Feb 2017 19:47:46 GMT"}], "update_date": "2017-02-16", "authors_parsed": [["Dosovitskiy", "Alexey", ""], ["Koltun", "Vladlen", ""]]}, {"id": "1611.01802", "submitter": "Ricardo Usbeck", "authors": "Ricardo Usbeck, Jonathan Huthmann, Nico Duldhardt, Axel-Cyrille Ngonga\n  Ngomo", "title": "Self-Wiring Question Answering Systems", "comments": "6 pages, 1 figure, pre-print in lncs", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Question answering (QA) has been the subject of a resurgence over the past\nyears. The said resurgence has led to a multitude of question answering (QA)\nsystems being developed both by companies and research facilities. While a few\ncomponents of QA systems get reused across implementations, most systems do not\nleverage the full potential of component reuse. Hence, the development of QA\nsystems is currently still a tedious and time-consuming process. We address the\nchallenge of accelerating the creation of novel or tailored QA systems by\npresenting a concept for a self-wiring approach to composing QA systems. Our\napproach will allow the reuse of existing, web-based QA systems or modules\nwhile developing new QA platforms. To this end, it will rely on QA modules\nbeing described using the Web Ontology Language. Based on these descriptions,\nour approach will be able to automatically compose QA systems using a\ndata-driven approach automatically.\n", "versions": [{"version": "v1", "created": "Sun, 6 Nov 2016 16:08:21 GMT"}, {"version": "v2", "created": "Tue, 8 Nov 2016 17:27:39 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Usbeck", "Ricardo", ""], ["Huthmann", "Jonathan", ""], ["Duldhardt", "Nico", ""], ["Ngomo", "Axel-Cyrille Ngonga", ""]]}, {"id": "1611.01843", "submitter": "Misha Denil", "authors": "Misha Denil, Pulkit Agrawal, Tejas D Kulkarni, Tom Erez, Peter\n  Battaglia, Nando de Freitas", "title": "Learning to Perform Physics Experiments via Deep Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CV cs.LG cs.NE physics.soc-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When encountering novel objects, humans are able to infer a wide range of\nphysical properties such as mass, friction and deformability by interacting\nwith them in a goal driven way. This process of active interaction is in the\nsame spirit as a scientist performing experiments to discover hidden facts.\nRecent advances in artificial intelligence have yielded machines that can\nachieve superhuman performance in Go, Atari, natural language processing, and\ncomplex control problems; however, it is not clear that these systems can rival\nthe scientific intuition of even a young child. In this work we introduce a\nbasic set of tasks that require agents to estimate properties such as mass and\ncohesion of objects in an interactive simulated environment where they can\nmanipulate the objects and observe the consequences. We found that state of art\ndeep reinforcement learning methods can learn to perform the experiments\nnecessary to discover such hidden properties. By systematically manipulating\nthe problem difficulty and the cost incurred by the agent for performing\nexperiments, we found that agents learn different strategies that balance the\ncost of gathering information against the cost of making mistakes in different\nsituations.\n", "versions": [{"version": "v1", "created": "Sun, 6 Nov 2016 20:55:19 GMT"}, {"version": "v2", "created": "Mon, 14 Nov 2016 16:40:58 GMT"}, {"version": "v3", "created": "Thu, 17 Aug 2017 19:51:29 GMT"}], "update_date": "2017-08-21", "authors_parsed": [["Denil", "Misha", ""], ["Agrawal", "Pulkit", ""], ["Kulkarni", "Tejas D", ""], ["Erez", "Tom", ""], ["Battaglia", "Peter", ""], ["de Freitas", "Nando", ""]]}, {"id": "1611.01855", "submitter": "Rishabh Singh", "authors": "Emilio Parisotto, Abdel-rahman Mohamed, Rishabh Singh, Lihong Li,\n  Dengyong Zhou, Pushmeet Kohli", "title": "Neuro-Symbolic Program Synthesis", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.PL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have seen the proposal of a number of neural architectures for\nthe problem of Program Induction. Given a set of input-output examples, these\narchitectures are able to learn mappings that generalize to new test inputs.\nWhile achieving impressive results, these approaches have a number of important\nlimitations: (a) they are computationally expensive and hard to train, (b) a\nmodel has to be trained for each task (program) separately, and (c) it is hard\nto interpret or verify the correctness of the learnt mapping (as it is defined\nby a neural network). In this paper, we propose a novel technique,\nNeuro-Symbolic Program Synthesis, to overcome the above-mentioned problems.\nOnce trained, our approach can automatically construct computer programs in a\ndomain-specific language that are consistent with a set of input-output\nexamples provided at test time. Our method is based on two novel neural\nmodules. The first module, called the cross correlation I/O network, given a\nset of input-output examples, produces a continuous representation of the set\nof I/O examples. The second module, the Recursive-Reverse-Recursive Neural\nNetwork (R3NN), given the continuous representation of the examples,\nsynthesizes a program by incrementally expanding partial programs. We\ndemonstrate the effectiveness of our approach by applying it to the rich and\ncomplex domain of regular expression based string transformations. Experiments\nshow that the R3NN model is not only able to construct programs from new\ninput-output examples, but it is also able to construct new programs for tasks\nthat it had never observed before during training.\n", "versions": [{"version": "v1", "created": "Sun, 6 Nov 2016 22:23:56 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Parisotto", "Emilio", ""], ["Mohamed", "Abdel-rahman", ""], ["Singh", "Rishabh", ""], ["Li", "Lihong", ""], ["Zhou", "Dengyong", ""], ["Kohli", "Pushmeet", ""]]}, {"id": "1611.01886", "submitter": "Wentao Huang", "authors": "Wentao Huang and Kechen Zhang", "title": "An Information-Theoretic Framework for Fast and Robust Unsupervised\n  Learning via Neural Population Infomax", "comments": "25 pages, 7 figures, 5th International Conference on Learning\n  Representations (ICLR 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.IT math.IT q-bio.NC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A framework is presented for unsupervised learning of representations based\non infomax principle for large-scale neural populations. We use an asymptotic\napproximation to the Shannon's mutual information for a large neural population\nto demonstrate that a good initial approximation to the global\ninformation-theoretic optimum can be obtained by a hierarchical infomax method.\nStarting from the initial solution, an efficient algorithm based on gradient\ndescent of the final objective function is proposed to learn representations\nfrom the input datasets, and the method works for complete, overcomplete, and\nundercomplete bases. As confirmed by numerical experiments, our method is\nrobust and highly efficient for extracting salient features from input\ndatasets. Compared with the main existing methods, our algorithm has a distinct\nadvantage in both the training speed and the robustness of unsupervised\nrepresentation learning. Furthermore, the proposed method is easily extended to\nthe supervised or unsupervised model for training deep structure networks.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 04:17:28 GMT"}, {"version": "v2", "created": "Thu, 19 Jan 2017 17:53:31 GMT"}, {"version": "v3", "created": "Mon, 6 Feb 2017 17:11:34 GMT"}, {"version": "v4", "created": "Fri, 10 Mar 2017 16:41:16 GMT"}], "update_date": "2017-03-13", "authors_parsed": [["Huang", "Wentao", ""], ["Zhang", "Kechen", ""]]}, {"id": "1611.01929", "submitter": "Oron Anschel", "authors": "Oron Anschel, Nir Baram, Nahum Shimkin", "title": "Averaged-DQN: Variance Reduction and Stabilization for Deep\n  Reinforcement Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Instability and variability of Deep Reinforcement Learning (DRL) algorithms\ntend to adversely affect their performance. Averaged-DQN is a simple extension\nto the DQN algorithm, based on averaging previously learned Q-values estimates,\nwhich leads to a more stable training procedure and improved performance by\nreducing approximation error variance in the target values. To understand the\neffect of the algorithm, we examine the source of value function estimation\nerrors and provide an analytical comparison within a simplified model. We\nfurther present experiments on the Arcade Learning Environment benchmark that\ndemonstrate significantly improved stability and performance due to the\nproposed extension.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 08:12:53 GMT"}, {"version": "v2", "created": "Tue, 8 Nov 2016 08:40:02 GMT"}, {"version": "v3", "created": "Wed, 8 Mar 2017 13:50:38 GMT"}, {"version": "v4", "created": "Fri, 10 Mar 2017 09:52:52 GMT"}], "update_date": "2017-03-13", "authors_parsed": [["Anschel", "Oron", ""], ["Baram", "Nir", ""], ["Shimkin", "Nahum", ""]]}, {"id": "1611.02047", "submitter": "Andrey Filchenkov", "authors": "Ivan Smetannikov, Ilya Isaev, Andrey Filchenkov", "title": "Reinforcement Learning Approach for Parallelization in Filters\n  Aggregation Based Feature Selection Algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  One of the classical problems in machine learning and data mining is feature\nselection. A feature selection algorithm is expected to be quick, and at the\nsame time it should show high performance. MeLiF algorithm effectively solves\nthis problem using ensembles of ranking filters. This article describes two\ndifferent ways to improve MeLiF algorithm performance with parallelization.\nExperiments show that proposed schemes significantly improves algorithm\nperformance and increase feature selection quality.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 13:43:38 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Smetannikov", "Ivan", ""], ["Isaev", "Ilya", ""], ["Filchenkov", "Andrey", ""]]}, {"id": "1611.02053", "submitter": "Andrey Filchenkov", "authors": "Valeria Efimova, Andrey Filchenkov, Anatoly Shalyto", "title": "Reinforcement-based Simultaneous Algorithm and its Hyperparameters\n  Selection", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many algorithms for data analysis exist, especially for classification\nproblems. To solve a data analysis problem, a proper algorithm should be\nchosen, and also its hyperparameters should be selected. In this paper, we\npresent a new method for the simultaneous selection of an algorithm and its\nhyperparameters. In order to do so, we reduced this problem to the multi-armed\nbandit problem. We consider an algorithm as an arm and algorithm\nhyperparameters search during a fixed time as the corresponding arm play. We\nalso suggest a problem-specific reward function. We performed the experiments\non 10 real datasets and compare the suggested method with the existing one\nimplemented in Auto-WEKA. The results show that our method is significantly\nbetter in most of the cases and never worse than the Auto-WEKA.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 13:55:00 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Efimova", "Valeria", ""], ["Filchenkov", "Andrey", ""], ["Shalyto", "Anatoly", ""]]}, {"id": "1611.02126", "submitter": "David Heckerman", "authors": "Dan Geiger and David Heckerman", "title": "Dependence and Relevance: A probabilistic view", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI math.CO math.PR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We examine three probabilistic concepts related to the sentence \"two\nvariables have no bearing on each other\". We explore the relationships between\nthese three concepts and establish their relevance to the process of\nconstructing similarity networks---a tool for acquiring probabilistic knowledge\nfrom human experts. We also establish a precise relationship between\nconnectedness in Bayesian networks and relevance in probability.\n", "versions": [{"version": "v1", "created": "Thu, 27 Oct 2016 15:16:21 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Geiger", "Dan", ""], ["Heckerman", "David", ""]]}, {"id": "1611.02154", "submitter": "Meisam Hejazi Nia", "authors": "Meisam Hejazi Nia, Brian Ratchford", "title": "Bayesian Non-parametric model to Target Gamification Notifications Using\n  Big Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  I suggest an approach that helps the online marketers to target their\nGamification elements to users by modifying the order of the list of tasks that\nthey send to users. It is more realistic and flexible as it allows the model to\nlearn more parameters when the online marketers collect more data. The\ntargeting approach is scalable and quick, and it can be used over streaming\ndata.\n", "versions": [{"version": "v1", "created": "Fri, 4 Nov 2016 04:40:23 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Nia", "Meisam Hejazi", ""], ["Ratchford", "Brian", ""]]}, {"id": "1611.02205", "submitter": "Nadav Bhonker", "authors": "Nadav Bhonker, Shai Rozenberg and Itay Hubara", "title": "Playing SNES in the Retro Learning Environment", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mastering a video game requires skill, tactics and strategy. While these\nattributes may be acquired naturally by human players, teaching them to a\ncomputer program is a far more challenging task. In recent years, extensive\nresearch was carried out in the field of reinforcement learning and numerous\nalgorithms were introduced, aiming to learn how to perform human tasks such as\nplaying video games. As a result, the Arcade Learning Environment (ALE)\n(Bellemare et al., 2013) has become a commonly used benchmark environment\nallowing algorithms to train on various Atari 2600 games. In many games the\nstate-of-the-art algorithms outperform humans. In this paper we introduce a new\nlearning environment, the Retro Learning Environment --- RLE, that can run\ngames on the Super Nintendo Entertainment System (SNES), Sega Genesis and\nseveral other gaming consoles. The environment is expandable, allowing for more\nvideo games and consoles to be easily added to the environment, while\nmaintaining the same interface as ALE. Moreover, RLE is compatible with Python\nand Torch. SNES games pose a significant challenge to current algorithms due to\ntheir higher level of complexity and versatility.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 18:33:38 GMT"}, {"version": "v2", "created": "Tue, 7 Feb 2017 18:50:50 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Bhonker", "Nadav", ""], ["Rozenberg", "Shai", ""], ["Hubara", "Itay", ""]]}, {"id": "1611.02252", "submitter": "Miguel L\\'azaro-Gredilla", "authors": "Miguel L\\'azaro-Gredilla, Yi Liu, D. Scott Phoenix, Dileep George", "title": "Hierarchical compositional feature learning", "comments": "Removed the \"under review\" header from every page, no changes to\n  content", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the hierarchical compositional network (HCN), a directed\ngenerative model able to discover and disentangle, without supervision, the\nbuilding blocks of a set of binary images. The building blocks are binary\nfeatures defined hierarchically as a composition of some of the features in the\nlayer immediately below, arranged in a particular manner. At a high level, HCN\nis similar to a sigmoid belief network with pooling. Inference and learning in\nHCN are very challenging and existing variational approximations do not work\nsatisfactorily. A main contribution of this work is to show that both can be\naddressed using max-product message passing (MPMP) with a particular schedule\n(no EM required). Also, using MPMP as an inference engine for HCN makes new\ntasks simple: adding supervision information, classifying images, or performing\ninpainting all correspond to clamping some variables of the model to their\nknown values and running MPMP on the rest. When used for classification, fast\ninference with HCN has exactly the same functional form as a convolutional\nneural network (CNN) with linear activations and binary weights. However, HCN's\nfeatures are qualitatively very different.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 20:25:08 GMT"}, {"version": "v2", "created": "Thu, 26 Oct 2017 01:23:40 GMT"}], "update_date": "2017-10-27", "authors_parsed": [["L\u00e1zaro-Gredilla", "Miguel", ""], ["Liu", "Yi", ""], ["Phoenix", "D. Scott", ""], ["George", "Dileep", ""]]}, {"id": "1611.02266", "submitter": "Ryota Tomioka", "authors": "Liwen Zhang and John Winn and Ryota Tomioka", "title": "Gaussian Attention Model and Its Application to Knowledge Base Embedding\n  and Question Answering", "comments": "16 pages, 4 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose the Gaussian attention model for content-based neural memory\naccess. With the proposed attention model, a neural network has the additional\ndegree of freedom to control the focus of its attention from a laser sharp\nattention to a broad attention. It is applicable whenever we can assume that\nthe distance in the latent space reflects some notion of semantics. We use the\nproposed attention model as a scoring function for the embedding of a knowledge\nbase into a continuous vector space and then train a model that performs\nquestion answering about the entities in the knowledge base. The proposed\nattention model can handle both the propagation of uncertainty when following a\nseries of relations and also the conjunction of conditions in a natural way. On\na dataset of soccer players who participated in the FIFA World Cup 2014, we\ndemonstrate that our model can handle both path queries and conjunctive queries\nwell.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 20:57:24 GMT"}, {"version": "v2", "created": "Wed, 30 Nov 2016 16:44:17 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Zhang", "Liwen", ""], ["Winn", "John", ""], ["Tomioka", "Ryota", ""]]}, {"id": "1611.02268", "submitter": "Akshay Balsubramani", "authors": "Akshay Balsubramani", "title": "Optimal Binary Autoencoding with Pairwise Correlations", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formulate learning of a binary autoencoder as a biconvex optimization\nproblem which learns from the pairwise correlations between encoded and decoded\nbits. Among all possible algorithms that use this information, ours finds the\nautoencoder that reconstructs its inputs with worst-case optimal loss. The\noptimal decoder is a single layer of artificial neurons, emerging entirely from\nthe minimax loss minimization, and with weights learned by convex optimization.\nAll this is reflected in competitive experimental results, demonstrating that\nbinary autoencoding can be done efficiently by conveying information in\npairwise correlations in an optimal fashion.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 20:58:58 GMT"}], "update_date": "2016-11-08", "authors_parsed": [["Balsubramani", "Akshay", ""]]}, {"id": "1611.02304", "submitter": "Mevlana Gemici", "authors": "Mevlana C. Gemici, Danilo Rezende, Shakir Mohamed", "title": "Normalizing Flows on Riemannian Manifolds", "comments": "3 pages, 2 figures, Submitted to Workshop on Bayesian Deep Learning\n  at NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of density estimation on Riemannian manifolds.\nDensity estimation on manifolds has many applications in fluid-mechanics,\noptics and plasma physics and it appears often when dealing with angular\nvariables (such as used in protein folding, robot limbs, gene-expression) and\nin general directional statistics. In spite of the multitude of algorithms\navailable for density estimation in the Euclidean spaces $\\mathbf{R}^n$ that\nscale to large n (e.g. normalizing flows, kernel methods and variational\napproximations), most of these methods are not immediately suitable for density\nestimation in more general Riemannian manifolds. We revisit techniques related\nto homeomorphisms from differential geometry for projecting densities to\nsub-manifolds and use it to generalize the idea of normalizing flows to more\ngeneral Riemannian manifolds. The resulting algorithm is scalable, simple to\nimplement and suitable for use with automatic differentiation. We demonstrate\nconcrete examples of this method on the n-sphere $\\mathbf{S}^n$.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 21:21:34 GMT"}, {"version": "v2", "created": "Wed, 9 Nov 2016 07:16:26 GMT"}], "update_date": "2016-11-10", "authors_parsed": [["Gemici", "Mevlana C.", ""], ["Rezende", "Danilo", ""], ["Mohamed", "Shakir", ""]]}, {"id": "1611.02315", "submitter": "Jacob Steinhardt", "authors": "Moses Charikar and Jacob Steinhardt and Gregory Valiant", "title": "Learning from Untrusted Data", "comments": "Updated based on STOC camera-ready", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CC cs.CR math.ST stat.TH", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The vast majority of theoretical results in machine learning and statistics\nassume that the available training data is a reasonably reliable reflection of\nthe phenomena to be learned or estimated. Similarly, the majority of machine\nlearning and statistical techniques used in practice are brittle to the\npresence of large amounts of biased or malicious data. In this work we consider\ntwo frameworks in which to study estimation, learning, and optimization in the\npresence of significant fractions of arbitrary data.\n  The first framework, list-decodable learning, asks whether it is possible to\nreturn a list of answers, with the guarantee that at least one of them is\naccurate. For example, given a dataset of $n$ points for which an unknown\nsubset of $\\alpha n$ points are drawn from a distribution of interest, and no\nassumptions are made about the remaining $(1-\\alpha)n$ points, is it possible\nto return a list of $\\operatorname{poly}(1/\\alpha)$ answers, one of which is\ncorrect? The second framework, which we term the semi-verified learning model,\nconsiders the extent to which a small dataset of trusted data (drawn from the\ndistribution in question) can be leveraged to enable the accurate extraction of\ninformation from a much larger but untrusted dataset (of which only an\n$\\alpha$-fraction is drawn from the distribution).\n  We show strong positive results in both settings, and provide an algorithm\nfor robust learning in a very general stochastic optimization setting. This\ngeneral result has immediate implications for robust estimation in a number of\nsettings, including for robustly estimating the mean of distributions with\nbounded second moments, robustly learning mixtures of such distributions, and\nrobustly finding planted partitions in random graphs in which significant\nportions of the graph have been perturbed by an adversary.\n", "versions": [{"version": "v1", "created": "Mon, 7 Nov 2016 21:43:39 GMT"}, {"version": "v2", "created": "Sun, 11 Jun 2017 17:48:31 GMT"}], "update_date": "2017-06-13", "authors_parsed": [["Charikar", "Moses", ""], ["Steinhardt", "Jacob", ""], ["Valiant", "Gregory", ""]]}, {"id": "1611.02385", "submitter": "Alexander Peysakhovich", "authors": "Alexander Peysakhovich, Akos Lada", "title": "Combining observational and experimental data to find heterogeneous\n  treatment effects", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Every design choice will have different effects on different units. However\ntraditional A/B tests are often underpowered to identify these heterogeneous\neffects. This is especially true when the set of unit-level attributes is\nhigh-dimensional and our priors are weak about which particular covariates are\nimportant. However, there are often observational data sets available that are\norders of magnitude larger. We propose a method to combine these two data\nsources to estimate heterogeneous treatment effects. First, we use\nobservational time series data to estimate a mapping from covariates to\nunit-level effects. These estimates are likely biased but under some conditions\nthe bias preserves unit-level relative rank orderings. If these conditions\nhold, we only need sufficient experimental data to identify a monotonic,\none-dimensional transformation from observationally predicted treatment effects\nto real treatment effects. This reduces power demands greatly and makes the\ndetection of heterogeneous effects much easier. As an application, we show how\nour method can be used to improve Facebook page recommendations.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 04:40:27 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Peysakhovich", "Alexander", ""], ["Lada", "Akos", ""]]}, {"id": "1611.02439", "submitter": "Sarah Alice Gaggl", "authors": "Sarah Alice Gaggl, Juan Carlos Nieves, Hannes Strass", "title": "Proceedings of the First International Workshop on Argumentation in\n  Logic Programming and Non-Monotonic Reasoning (Arg-LPNMR 2016)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This volume contains the papers presented at Arg-LPNMR 2016: First\nInternational Workshop on Argumentation in Logic Programming and Nonmonotonic\nReasoning held on July 8-10, 2016 in New York City, NY.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 09:17:08 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Gaggl", "Sarah Alice", ""], ["Nieves", "Juan Carlos", ""], ["Strass", "Hannes", ""]]}, {"id": "1611.02453", "submitter": "Thorsten Wissmann", "authors": "Carsten Lutz and Frank Wolter", "title": "The Data Complexity of Description Logic Ontologies", "comments": null, "journal-ref": "Logical Methods in Computer Science, Volume 13, Issue 4 (November\n  13, 2017) lmcs:4060", "doi": "10.23638/LMCS-13(4:7)2017", "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We analyze the data complexity of ontology-mediated querying where the\nontologies are formulated in a description logic (DL) of the ALC family and\nqueries are conjunctive queries, positive existential queries, or acyclic\nconjunctive queries. Our approach is non-uniform in the sense that we aim to\nunderstand the complexity of each single ontology instead of for all ontologies\nformulated in a certain language. While doing so, we quantify over the queries\nand are interested, for example, in the question whether all queries can be\nevaluated in polynomial time w.r.t. a given ontology. Our results include a\nPTime/coNP-dichotomy for ontologies of depth one in the description logic\nALCFI, the same dichotomy for ALC- and ALCI-ontologies of unrestricted depth,\nand the non-existence of such a dichotomy for ALCF-ontologies. For the latter\nDL, we additionally show that it is undecidable whether a given ontology admits\nPTime query evaluation. We also consider the connection between PTime query\nevaluation and rewritability into (monadic) Datalog.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 09:52:54 GMT"}, {"version": "v2", "created": "Tue, 24 Oct 2017 09:19:25 GMT"}, {"version": "v3", "created": "Fri, 10 Nov 2017 09:38:00 GMT"}], "update_date": "2018-01-03", "authors_parsed": [["Lutz", "Carsten", ""], ["Wolter", "Frank", ""]]}, {"id": "1611.02512", "submitter": "Wen-Chieh Fang", "authors": "Wen-Chieh Fang and Yi-ting Chiang", "title": "Cognitive Discriminative Mappings for Rapid Learning", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans can learn concepts or recognize items from just a handful of examples,\nwhile machines require many more samples to perform the same task. In this\npaper, we build a computational model to investigate the possibility of this\nkind of rapid learning. The proposed method aims to improve the learning task\nof input from sensory memory by leveraging the information retrieved from\nlong-term memory. We present a simple and intuitive technique called cognitive\ndiscriminative mappings (CDM) to explore the cognitive problem. First, CDM\nseparates and clusters the data instances retrieved from long-term memory into\ndistinct classes with a discrimination method in working memory when a sensory\ninput triggers the algorithm. CDM then maps each sensory data instance to be as\nclose as possible to the median point of the data group with the same class.\nThe experimental results demonstrate that the CDM approach is effective for\nlearning the discriminative features of supervised classifications with few\ntraining sensory input instances.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 13:26:32 GMT"}], "update_date": "2016-11-09", "authors_parsed": [["Fang", "Wen-Chieh", ""], ["Chiang", "Yi-ting", ""]]}, {"id": "1611.02554", "submitter": "Lei Yu", "authors": "Lei Yu, Phil Blunsom, Chris Dyer, Edward Grefenstette, Tomas Kocisky", "title": "The Neural Noisy Channel", "comments": "ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We formulate sequence to sequence transduction as a noisy channel decoding\nproblem and use recurrent neural networks to parameterise the source and\nchannel models. Unlike direct models which can suffer from explaining-away\neffects during training, noisy channel models must produce outputs that explain\ntheir inputs, and their component models can be trained with not only paired\ntraining samples but also unpaired samples from the marginal output\ndistribution. Using a latent variable to control how much of the conditioning\nsequence the channel model needs to read in order to generate a subsequent\nsymbol, we obtain a tractable and effective beam search decoder. Experimental\nresults on abstractive sentence summarisation, morphological inflection, and\nmachine translation show that noisy channel models outperform direct models,\nand that they significantly benefit from increased amounts of unpaired output\ndata that direct models cannot easily use.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 15:18:44 GMT"}, {"version": "v2", "created": "Mon, 6 Mar 2017 12:37:12 GMT"}], "update_date": "2017-03-07", "authors_parsed": [["Yu", "Lei", ""], ["Blunsom", "Phil", ""], ["Dyer", "Chris", ""], ["Grefenstette", "Edward", ""], ["Kocisky", "Tomas", ""]]}, {"id": "1611.02646", "submitter": "Tatiana Makhalova", "authors": "Sergei O. Kuznetsov, Tatiana Makhalova", "title": "On interestingness measures of formal concepts", "comments": "20 pages, 5 figures, 3 tables", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Formal concepts and closed itemsets proved to be of big importance for\nknowledge discovery, both as a tool for concise representation of association\nrules and a tool for clustering and constructing domain taxonomies and\nontologies. Exponential explosion makes it difficult to consider the whole\nconcept lattice arising from data, one needs to select most useful and\ninteresting concepts. In this paper interestingness measures of concepts are\nconsidered and compared with respect to various aspects, such as efficiency of\ncomputation and applicability to noisy data and performing ranking correlation.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 18:26:24 GMT"}, {"version": "v2", "created": "Wed, 19 Apr 2017 18:19:22 GMT"}], "update_date": "2017-04-21", "authors_parsed": [["Kuznetsov", "Sergei O.", ""], ["Makhalova", "Tatiana", ""]]}, {"id": "1611.02654", "submitter": "Lajanugen Logeswaran", "authors": "Lajanugen Logeswaran, Honglak Lee, Dragomir Radev", "title": "Sentence Ordering and Coherence Modeling using Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Modeling the structure of coherent texts is a key NLP problem. The task of\ncoherently organizing a given set of sentences has been commonly used to build\nand evaluate models that understand such structure. We propose an end-to-end\nunsupervised deep learning approach based on the set-to-sequence framework to\naddress this problem. Our model strongly outperforms prior methods in the order\ndiscrimination task and a novel task of ordering abstracts from scientific\narticles. Furthermore, our work shows that useful text representations can be\nobtained by learning to order sentences. Visualizing the learned sentence\nrepresentations shows that the model captures high-level logical structure in\nparagraphs. Our representations perform comparably to state-of-the-art\npre-training methods on sentence similarity and paraphrase detection tasks.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 19:04:09 GMT"}, {"version": "v2", "created": "Fri, 22 Dec 2017 02:36:08 GMT"}], "update_date": "2017-12-25", "authors_parsed": [["Logeswaran", "Lajanugen", ""], ["Lee", "Honglak", ""], ["Radev", "Dragomir", ""]]}, {"id": "1611.02755", "submitter": "Abram Friesen", "authors": "Abram L. Friesen and Pedro Domingos", "title": "Recursive Decomposition for Nonconvex Optimization", "comments": "11 pages, 7 figures, pdflatex", "journal-ref": "Proceedings of the 24th International Joint Conference on\n  Artificial Intelligence (2015), pp. 253-259", "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Continuous optimization is an important problem in many areas of AI,\nincluding vision, robotics, probabilistic inference, and machine learning.\nUnfortunately, most real-world optimization problems are nonconvex, causing\nstandard convex techniques to find only local optima, even with extensions like\nrandom restarts and simulated annealing. We observe that, in many cases, the\nlocal modes of the objective function have combinatorial structure, and thus\nideas from combinatorial optimization can be brought to bear. Based on this, we\npropose a problem-decomposition approach to nonconvex optimization. Similarly\nto DPLL-style SAT solvers and recursive conditioning in probabilistic\ninference, our algorithm, RDIS, recursively sets variables so as to simplify\nand decompose the objective function into approximately independent\nsub-functions, until the remaining functions are simple enough to be optimized\nby standard techniques like gradient descent. The variables to set are chosen\nby graph partitioning, ensuring decomposition whenever possible. We show\nanalytically that RDIS can solve a broad class of nonconvex optimization\nproblems exponentially faster than gradient descent with random restarts.\nExperimentally, RDIS outperforms standard techniques on problems like structure\nfrom motion and protein folding.\n", "versions": [{"version": "v1", "created": "Tue, 8 Nov 2016 22:52:08 GMT"}], "update_date": "2016-11-10", "authors_parsed": [["Friesen", "Abram L.", ""], ["Domingos", "Pedro", ""]]}, {"id": "1611.02779", "submitter": "Yan Duan", "authors": "Yan Duan, John Schulman, Xi Chen, Peter L. Bartlett, Ilya Sutskever,\n  Pieter Abbeel", "title": "RL$^2$: Fast Reinforcement Learning via Slow Reinforcement Learning", "comments": "14 pages. Under review as a conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG cs.NE stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep reinforcement learning (deep RL) has been successful in learning\nsophisticated behaviors automatically; however, the learning process requires a\nhuge number of trials. In contrast, animals can learn new tasks in just a few\ntrials, benefiting from their prior knowledge about the world. This paper seeks\nto bridge this gap. Rather than designing a \"fast\" reinforcement learning\nalgorithm, we propose to represent it as a recurrent neural network (RNN) and\nlearn it from data. In our proposed method, RL$^2$, the algorithm is encoded in\nthe weights of the RNN, which are learned slowly through a general-purpose\n(\"slow\") RL algorithm. The RNN receives all information a typical RL algorithm\nwould receive, including observations, actions, rewards, and termination flags;\nand it retains its state across episodes in a given Markov Decision Process\n(MDP). The activations of the RNN store the state of the \"fast\" RL algorithm on\nthe current (previously unseen) MDP. We evaluate RL$^2$ experimentally on both\nsmall-scale and large-scale problems. On the small-scale side, we train it to\nsolve randomly generated multi-arm bandit problems and finite MDPs. After\nRL$^2$ is trained, its performance on new MDPs is close to human-designed\nalgorithms with optimality guarantees. On the large-scale side, we test RL$^2$\non a vision-based navigation task and show that it scales up to\nhigh-dimensional problems.\n", "versions": [{"version": "v1", "created": "Wed, 9 Nov 2016 00:13:29 GMT"}, {"version": "v2", "created": "Thu, 10 Nov 2016 01:17:36 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Duan", "Yan", ""], ["Schulman", "John", ""], ["Chen", "Xi", ""], ["Bartlett", "Peter L.", ""], ["Sutskever", "Ilya", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1611.02796", "submitter": "Natasha Jaques", "authors": "Natasha Jaques, Shixiang Gu, Dzmitry Bahdanau, Jos\\'e Miguel\n  Hern\\'andez-Lobato, Richard E. Turner, Douglas Eck", "title": "Sequence Tutor: Conservative Fine-Tuning of Sequence Generation Models\n  with KL-control", "comments": "Add supplementary material", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper proposes a general method for improving the structure and quality\nof sequences generated by a recurrent neural network (RNN), while maintaining\ninformation originally learned from data, as well as sample diversity. An RNN\nis first pre-trained on data using maximum likelihood estimation (MLE), and the\nprobability distribution over the next token in the sequence learned by this\nmodel is treated as a prior policy. Another RNN is then trained using\nreinforcement learning (RL) to generate higher-quality outputs that account for\ndomain-specific incentives while retaining proximity to the prior policy of the\nMLE RNN. To formalize this objective, we derive novel off-policy RL methods for\nRNNs from KL-control. The effectiveness of the approach is demonstrated on two\napplications; 1) generating novel musical melodies, and 2) computational\nmolecular generation. For both problems, we show that the proposed method\nimproves the desired properties and structure of the generated sequences, while\nmaintaining information learned from data.\n", "versions": [{"version": "v1", "created": "Wed, 9 Nov 2016 01:46:32 GMT"}, {"version": "v2", "created": "Thu, 10 Nov 2016 18:54:17 GMT"}, {"version": "v3", "created": "Wed, 7 Dec 2016 14:42:30 GMT"}, {"version": "v4", "created": "Thu, 12 Jan 2017 02:18:20 GMT"}, {"version": "v5", "created": "Mon, 27 Feb 2017 20:38:06 GMT"}, {"version": "v6", "created": "Sat, 4 Mar 2017 19:38:01 GMT"}, {"version": "v7", "created": "Thu, 6 Apr 2017 15:02:04 GMT"}, {"version": "v8", "created": "Thu, 4 May 2017 17:11:45 GMT"}, {"version": "v9", "created": "Mon, 16 Oct 2017 21:31:31 GMT"}], "update_date": "2017-10-18", "authors_parsed": [["Jaques", "Natasha", ""], ["Gu", "Shixiang", ""], ["Bahdanau", "Dzmitry", ""], ["Hern\u00e1ndez-Lobato", "Jos\u00e9 Miguel", ""], ["Turner", "Richard E.", ""], ["Eck", "Douglas", ""]]}, {"id": "1611.02885", "submitter": "Martin Diller", "authors": "Martin Diller, Anthony Hunter", "title": "Encoding monotonic multi-set preferences using CI-nets: preliminary\n  report", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  CP-nets and their variants constitute one of the main AI approaches for\nspecifying and reasoning about preferences. CI-nets, in particular, are a\nCP-inspired formalism for representing ordinal preferences over sets of goods,\nwhich are typically required to be monotonic.\n  Considering also that goods often come in multi-sets rather than sets, a\nnatural question is whether CI-nets can be used more or less directly to encode\npreferences over multi-sets. We here provide some initial ideas on how to\nachieve this, in the sense that at least a restricted form of reasoning on our\nframework, which we call \"confined reasoning\", can be efficiently reduced to\nreasoning on CI-nets. Our framework nevertheless allows for encoding\npreferences over multi-sets with unbounded multiplicities. We also show the\nextent to which it can be used to represent preferences where multiplicites of\nthe goods are not stated explicitly (\"purely qualitative preferences\") as well\nas a potential use of our generalization of CI-nets as a component of a recent\nsystem for evidence aggregation.\n", "versions": [{"version": "v1", "created": "Wed, 9 Nov 2016 10:56:42 GMT"}], "update_date": "2016-11-10", "authors_parsed": [["Diller", "Martin", ""], ["Hunter", "Anthony", ""]]}, {"id": "1611.03130", "submitter": "Lukas Cavigelli", "authors": "Lukas Cavigelli, Dominic Bernath, Michele Magno, Luca Benini", "title": "Computationally Efficient Target Classification in Multispectral Image\n  Data with Deep Neural Networks", "comments": "Presented at SPIE Security + Defence 2016 Proc. SPIE 9997, Target and\n  Background Signatures II", "journal-ref": null, "doi": "10.1117/12.2241383", "report-no": null, "categories": "cs.CV cs.AI cs.NE eess.IV eess.SP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Detecting and classifying targets in video streams from surveillance cameras\nis a cumbersome, error-prone and expensive task. Often, the incurred costs are\nprohibitive for real-time monitoring. This leads to data being stored locally\nor transmitted to a central storage site for post-incident examination. The\nrequired communication links and archiving of the video data are still\nexpensive and this setup excludes preemptive actions to respond to imminent\nthreats. An effective way to overcome these limitations is to build a smart\ncamera that transmits alerts when relevant video sequences are detected. Deep\nneural networks (DNNs) have come to outperform humans in visual classifications\ntasks. The concept of DNNs and Convolutional Networks (ConvNets) can easily be\nextended to make use of higher-dimensional input data such as multispectral\ndata. We explore this opportunity in terms of achievable accuracy and required\ncomputational effort. To analyze the precision of DNNs for scene labeling in an\nurban surveillance scenario we have created a dataset with 8 classes obtained\nin a field experiment. We combine an RGB camera with a 25-channel VIS-NIR\nsnapshot sensor to assess the potential of multispectral image data for target\nclassification. We evaluate several new DNNs, showing that the spectral\ninformation fused together with the RGB frames can be used to improve the\naccuracy of the system or to achieve similar accuracy with a 3x smaller\ncomputation effort. We achieve a very high per-pixel accuracy of 99.1%. Even\nfor scarcely occurring, but particularly interesting classes, such as cars, 75%\nof the pixels are labeled correctly with errors occurring only around the\nborder of the objects. This high accuracy was obtained with a training set of\nonly 30 labeled images, paving the way for fast adaptation to various\napplication scenarios.\n", "versions": [{"version": "v1", "created": "Wed, 9 Nov 2016 23:13:18 GMT"}], "update_date": "2017-11-10", "authors_parsed": [["Cavigelli", "Lukas", ""], ["Bernath", "Dominic", ""], ["Magno", "Michele", ""], ["Benini", "Luca", ""]]}, {"id": "1611.03218", "submitter": "Emilio Jorge", "authors": "Emilio Jorge, Mikael K{\\aa}geb\\\"ack, Fredrik D. Johansson, Emil\n  Gustavsson", "title": "Learning to Play Guess Who? and Inventing a Grounded Language as a\n  Consequence", "comments": "Previous version was accepted to Deep Reinforcement Learning Workshop\n  at NIPS 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Acquiring your first language is an incredible feat and not easily\nduplicated. Learning to communicate using nothing but a few pictureless books,\na corpus, would likely be impossible even for humans. Nevertheless, this is the\ndominating approach in most natural language processing today. As an\nalternative, we propose the use of situated interactions between agents as a\ndriving force for communication, and the framework of Deep Recurrent Q-Networks\nfor evolving a shared language grounded in the provided environment. We task\nthe agents with interactive image search in the form of the game Guess Who?.\nThe images from the game provide a non trivial environment for the agents to\ndiscuss and a natural grounding for the concepts they decide to encode in their\ncommunication. Our experiments show that the agents learn not only to encode\nphysical concepts in their words, i.e. grounding, but also that the agents\nlearn to hold a multi-step dialogue remembering the state of the dialogue from\nstep to step.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 08:44:52 GMT"}, {"version": "v2", "created": "Sun, 27 Nov 2016 14:50:50 GMT"}, {"version": "v3", "created": "Tue, 3 Jan 2017 18:28:43 GMT"}, {"version": "v4", "created": "Wed, 15 Mar 2017 11:24:49 GMT"}], "update_date": "2017-03-16", "authors_parsed": [["Jorge", "Emilio", ""], ["K\u00e5geb\u00e4ck", "Mikael", ""], ["Johansson", "Fredrik D.", ""], ["Gustavsson", "Emil", ""]]}, {"id": "1611.03372", "submitter": "Hongyang Qu", "authors": "Paolo Izzo, Hongyang Qu and Sandor M. Veres", "title": "A stochastically verifiable autonomous control architecture with\n  reasoning", "comments": "Accepted at IEEE Conf. Decision and Control, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI cs.SE cs.SY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A new agent architecture called Limited Instruction Set Agent (LISA) is\nintroduced for autonomous control. The new architecture is based on previous\nimplementations of AgentSpeak and it is structurally simpler than its\npredecessors with the aim of facilitating design-time and run-time verification\nmethods. The process of abstracting the LISA system to two different types of\ndiscrete probabilistic models (DTMC and MDP) is investigated and illustrated.\nThe LISA system provides a tool for complete modelling of the agent and the\nenvironment for probabilistic verification. The agent program can be\nautomatically compiled into a DTMC or a MDP model for verification with Prism.\nThe automatically generated Prism model can be used for both design-time and\nrun-time verification. The run-time verification is investigated and\nillustrated in the LISA system as an internal modelling mechanism for\nprediction of future outcomes.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 16:06:31 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Izzo", "Paolo", ""], ["Qu", "Hongyang", ""], ["Veres", "Sandor M.", ""]]}, {"id": "1611.03398", "submitter": "Christophe Lecoutre", "authors": "Frederic Boussemart and Christophe Lecoutre and Gilles Audemard and\n  C\\'edric Piette", "title": "XCSP3: An Integrated Format for Benchmarking Combinatorial Constrained\n  Problems", "comments": "238 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a major revision of the format XCSP 2.1, called XCSP3, to build\nintegrated representations of combinatorial constrained problems. This new\nformat is able to deal with mono/multi optimization, many types of variables,\ncost functions, reification, views, annotations, variable quantification,\ndistributed, probabilistic and qualitative reasoning. The new format is made\ncompact, highly readable, and rather easy to parse. Interestingly, it captures\nthe structure of the problem models, through the possibilities of declaring\narrays of variables, and identifying syntactic and semantic groups of\nconstraints. The number of constraints is kept under control by introducing a\nlimited set of basic constraint forms, and producing almost automatically some\nof their variations through lifting, restriction, sliding, logical combination\nand relaxation mechanisms. As a result, XCSP3 encompasses practically all\nconstraints that can be found in major constraint solvers developed by the CP\ncommunity. A website, which is developed conjointly with the format, contains\nmany models and series of instances. The user can make sophisticated queries\nfor selecting instances from very precise criteria. The objective of XCSP3 is\nto ease the effort required to test and compare different algorithms by\nproviding a common test-bed of combinatorial constrained instances.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 17:00:56 GMT"}, {"version": "v2", "created": "Fri, 6 Apr 2018 09:06:18 GMT"}, {"version": "v3", "created": "Sat, 16 Jan 2021 12:18:55 GMT"}], "update_date": "2021-01-19", "authors_parsed": [["Boussemart", "Frederic", ""], ["Lecoutre", "Christophe", ""], ["Audemard", "Gilles", ""], ["Piette", "C\u00e9dric", ""]]}, {"id": "1611.03451", "submitter": "Philip Thomas", "authors": "Philip S. Thomas and Emma Brunskill", "title": "Importance Sampling with Unequal Support", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Importance sampling is often used in machine learning when training and\ntesting data come from different distributions. In this paper we propose a new\nvariant of importance sampling that can reduce the variance of importance\nsampling-based estimates by orders of magnitude when the supports of the\ntraining and testing distributions differ. After motivating and presenting our\nnew importance sampling estimator, we provide a detailed theoretical analysis\nthat characterizes both its bias and variance relative to the ordinary\nimportance sampling estimator (in various settings, which include cases where\nordinary importance sampling is biased, while our new estimator is not, and\nvice versa). We conclude with an example of how our new importance sampling\nestimator can be used to improve estimates of how well a new treatment policy\nfor diabetes will work for an individual, using only data from when the\nindividual used a previous treatment policy.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 19:11:09 GMT"}], "update_date": "2016-11-11", "authors_parsed": [["Thomas", "Philip S.", ""], ["Brunskill", "Emma", ""]]}, {"id": "1611.03477", "submitter": "Hang Chu", "authors": "Hang Chu, Raquel Urtasun, Sanja Fidler", "title": "Song From PI: A Musically Plausible Network for Pop Music Generation", "comments": "under review at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a novel framework for generating pop music. Our model is a\nhierarchical Recurrent Neural Network, where the layers and the structure of\nthe hierarchy encode our prior knowledge about how pop music is composed. In\nparticular, the bottom layers generate the melody, while the higher levels\nproduce the drums and chords. We conduct several human studies that show strong\npreference of our generated music over that produced by the recent method by\nGoogle. We additionally show two applications of our framework: neural dancing\nand karaoke, as well as neural story singing.\n", "versions": [{"version": "v1", "created": "Thu, 10 Nov 2016 20:35:47 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Chu", "Hang", ""], ["Urtasun", "Raquel", ""], ["Fidler", "Sanja", ""]]}, {"id": "1611.03553", "submitter": "Abram Friesen", "authors": "Abram L. Friesen and Pedro Domingos", "title": "The Sum-Product Theorem: A Foundation for Learning Tractable Models", "comments": "15 pages (10 body, 5 pages of appendices)", "journal-ref": "Proceedings of the 33rd International Conference on Machine\n  Learning, pp. 1909-1918, 2016", "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inference in expressive probabilistic models is generally intractable, which\nmakes them difficult to learn and limits their applicability. Sum-product\nnetworks are a class of deep models where, surprisingly, inference remains\ntractable even when an arbitrary number of hidden layers are present. In this\npaper, we generalize this result to a much broader set of learning problems:\nall those where inference consists of summing a function over a semiring. This\nincludes satisfiability, constraint satisfaction, optimization, integration,\nand others. In any semiring, for summation to be tractable it suffices that the\nfactors of every product have disjoint scopes. This unifies and extends many\nprevious results in the literature. Enforcing this condition at learning time\nthus ensures that the learned models are tractable. We illustrate the power and\ngenerality of this approach by applying it to a new type of structured\nprediction problem: learning a nonconvex function that can be globally\noptimized in polynomial time. We show empirically that this greatly outperforms\nthe standard approach of learning without regard to the cost of optimization.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 00:46:33 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Friesen", "Abram L.", ""], ["Domingos", "Pedro", ""]]}, {"id": "1611.03558", "submitter": "ShiLiang Zhang", "authors": "Dan Liu and Wei Lin and Shiliang Zhang and Si Wei and Hui Jiang", "title": "Neural Networks Models for Entity Discovery and Linking", "comments": "9 pages, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper describes the USTC_NELSLIP systems submitted to the Trilingual\nEntity Detection and Linking (EDL) track in 2016 TAC Knowledge Base Population\n(KBP) contests. We have built two systems for entity discovery and mention\ndetection (MD): one uses the conditional RNNLM and the other one uses the\nattention-based encoder-decoder framework. The entity linking (EL) system\nconsists of two modules: a rule based candidate generation and a neural\nnetworks probability ranking model. Moreover, some simple string matching rules\nare used for NIL clustering. At the end, our best system has achieved an F1\nscore of 0.624 in the end-to-end typed mention ceaf plus metric.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 01:21:20 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Liu", "Dan", ""], ["Lin", "Wei", ""], ["Zhang", "Shiliang", ""], ["Wei", "Si", ""], ["Jiang", "Hui", ""]]}, {"id": "1611.03599", "submitter": "Wei-Fan Chen", "authors": "Wei-Fan Chen and Lun-Wei Ku", "title": "UTCNN: a Deep Learning Model of Stance Classificationon on Social Media\n  Text", "comments": "11 pages, to appear in COLING 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Most neural network models for document classification on social media focus\non text infor-mation to the neglect of other information on these platforms. In\nthis paper, we classify post stance on social media channels and develop UTCNN,\na neural network model that incorporates user tastes, topic tastes, and user\ncomments on posts. UTCNN not only works on social media texts, but also\nanalyzes texts in forums and message boards. Experiments performed on Chinese\nFacebook data and English online debate forum data show that UTCNN achieves a\n0.755 macro-average f-score for supportive, neutral, and unsupportive stance\nclasses on Facebook data, which is significantly better than models in which\neither user, topic, or comment information is withheld. This model design\ngreatly mitigates the lack of data for the minor class without the use of\noversampling. In addition, UTCNN yields a 0.842 accuracy on English online\ndebate forum data, which also significantly outperforms results from previous\nwork as well as other deep learning models, showing that UTCNN performs well\nregardless of language or platform.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 07:05:49 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Chen", "Wei-Fan", ""], ["Ku", "Lun-Wei", ""]]}, {"id": "1611.03652", "submitter": "Bernardo Gon\\c{c}alves", "authors": "Bernardo Gon\\c{c}alves", "title": "Show me the material evidence: Initial experiments on evaluating\n  hypotheses from user-generated multimedia data", "comments": "6 pages, 6 figures, 3 tables in Proc. of the 1st Workshop on\n  Multimedia Support for Decision-Making Processes, at IEEE Intl. Symposium on\n  Multimedia (ISM'16), San Jose, CA, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.DB cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Subjective questions such as `does neymar dive', or `is clinton lying', or\n`is trump a fascist', are popular queries to web search engines, as can be seen\nby autocompletion suggestions on Google, Yahoo and Bing. In the era of\ncognitive computing, beyond search, they could be handled as hypotheses issued\nfor evaluation. Our vision is to leverage on unstructured data and metadata of\nthe rich user-generated multimedia that is often shared as material evidence in\nfavor or against hypotheses in social media platforms. In this paper we present\ntwo preliminary experiments along those lines and discuss challenges for a\ncognitive computing system that collects material evidence from user-generated\nmultimedia towards aggregating it into some form of collective decision on the\nhypothesis.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 10:46:58 GMT"}], "update_date": "2016-11-14", "authors_parsed": [["Gon\u00e7alves", "Bernardo", ""]]}, {"id": "1611.03673", "submitter": "Piotr Mirowski", "authors": "Piotr Mirowski, Razvan Pascanu, Fabio Viola, Hubert Soyer, Andrew J.\n  Ballard, Andrea Banino, Misha Denil, Ross Goroshin, Laurent Sifre, Koray\n  Kavukcuoglu, Dharshan Kumaran and Raia Hadsell", "title": "Learning to Navigate in Complex Environments", "comments": "11 pages, 5 appendix pages, 11 figures, 3 tables, under review as a\n  conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV cs.LG cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Learning to navigate in complex environments with dynamic elements is an\nimportant milestone in developing AI agents. In this work we formulate the\nnavigation question as a reinforcement learning problem and show that data\nefficiency and task performance can be dramatically improved by relying on\nadditional auxiliary tasks leveraging multimodal sensory inputs. In particular\nwe consider jointly learning the goal-driven reinforcement learning problem\nwith auxiliary depth prediction and loop closure classification tasks. This\napproach can learn to navigate from raw sensory input in complicated 3D mazes,\napproaching human-level performance even under conditions where the goal\nlocation changes frequently. We provide detailed analysis of the agent\nbehaviour, its ability to localise, and its network activity dynamics, showing\nthat the agent implicitly learns key navigation abilities.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 12:14:45 GMT"}, {"version": "v2", "created": "Wed, 30 Nov 2016 18:02:53 GMT"}, {"version": "v3", "created": "Fri, 13 Jan 2017 11:15:22 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Mirowski", "Piotr", ""], ["Pascanu", "Razvan", ""], ["Viola", "Fabio", ""], ["Soyer", "Hubert", ""], ["Ballard", "Andrew J.", ""], ["Banino", "Andrea", ""], ["Denil", "Misha", ""], ["Goroshin", "Ross", ""], ["Sifre", "Laurent", ""], ["Kavukcuoglu", "Koray", ""], ["Kumaran", "Dharshan", ""], ["Hadsell", "Raia", ""]]}, {"id": "1611.03799", "submitter": "Rishin Haldar", "authors": "Rohan Kar, Rishin Haldar", "title": "Applying Chatbots to the Internet of Things: Opportunities and\n  Architectural Elements", "comments": "9 pages, 3 figures, 5 Use Cases", "journal-ref": null, "doi": "10.14569/IJACSA.2016.071119", "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Internet of Things (IoT) is emerging as a significant technology in shaping\nthe future by connecting physical devices or things with internet. It also\npresents various opportunities for intersection of other technological trends\nwhich can allow it to become even more intelligent and efficient. In this paper\nwe focus our attention on the integration of Intelligent Conversational\nSoftware Agents or Chatbots with IoT. Literature surveys have looked into\nvarious applications, features, underlying technologies and known challenges of\nIoT. On the other hand, Chatbots are being adopted in greater numbers due to\nmajor strides in development of platforms and frameworks. The novelty of this\npaper lies in the specific integration of Chatbots in the IoT scenario. We\nanalyzed the shortcomings of existing IoT systems and put forward ways to\ntackle them by incorporating chatbots. A general architecture is proposed for\nimplementing such a system, as well as platforms and frameworks, both\ncommercial and open source, which allow for implementation of such systems.\nIdentification of the newer challenges and possible future directions with this\nnew integration, have also been addressed.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 17:56:04 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Kar", "Rohan", ""], ["Haldar", "Rishin", ""]]}, {"id": "1611.03852", "submitter": "Chelsea Finn", "authors": "Chelsea Finn, Paul Christiano, Pieter Abbeel, Sergey Levine", "title": "A Connection between Generative Adversarial Networks, Inverse\n  Reinforcement Learning, and Energy-Based Models", "comments": "NIPS 2016 Workshop on Adversarial Training. First two authors\n  contributed equally", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Generative adversarial networks (GANs) are a recently proposed class of\ngenerative models in which a generator is trained to optimize a cost function\nthat is being simultaneously learned by a discriminator. While the idea of\nlearning cost functions is relatively new to the field of generative modeling,\nlearning costs has long been studied in control and reinforcement learning (RL)\ndomains, typically for imitation learning from demonstrations. In these fields,\nlearning cost function underlying observed behavior is known as inverse\nreinforcement learning (IRL) or inverse optimal control. While at first the\nconnection between cost learning in RL and cost learning in generative modeling\nmay appear to be a superficial one, we show in this paper that certain IRL\nmethods are in fact mathematically equivalent to GANs. In particular, we\ndemonstrate an equivalence between a sample-based algorithm for maximum entropy\nIRL and a GAN in which the generator's density can be evaluated and is provided\nas an additional input to the discriminator. Interestingly, maximum entropy IRL\nis a special case of an energy-based model. We discuss the interpretation of\nGANs as an algorithm for training energy-based models, and relate this\ninterpretation to other recent work that seeks to connect GANs and EBMs. By\nformally highlighting the connection between GANs, IRL, and EBMs, we hope that\nresearchers in all three communities can better identify and apply transferable\nideas from one domain to another, particularly for developing more stable and\nscalable algorithms: a major challenge in all three domains.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 20:53:45 GMT"}, {"version": "v2", "created": "Wed, 16 Nov 2016 18:11:26 GMT"}, {"version": "v3", "created": "Fri, 25 Nov 2016 08:09:55 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Finn", "Chelsea", ""], ["Christiano", "Paul", ""], ["Abbeel", "Pieter", ""], ["Levine", "Sergey", ""]]}, {"id": "1611.03907", "submitter": "Kamyar Azizzadenesheli Ph.D.", "authors": "Kamyar Azizzadenesheli, Alessandro Lazaric, Animashree Anandkumar", "title": "Reinforcement Learning in Rich-Observation MDPs using Spectral Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning (RL) in Markov decision processes (MDPs) with large\nstate spaces is a challenging problem. The performance of standard RL\nalgorithms degrades drastically with the dimensionality of state space.\nHowever, in practice, these large MDPs typically incorporate a latent or hidden\nlow-dimensional structure. In this paper, we study the setting of\nrich-observation Markov decision processes (ROMDP), where there are a small\nnumber of hidden states which possess an injective mapping to the observation\nstates. In other words, every observation state is generated through a single\nhidden state, and this mapping is unknown a priori. We introduce a spectral\ndecomposition method that consistently learns this mapping, and more\nimportantly, achieves it with low regret. The estimated mapping is integrated\ninto an optimistic RL algorithm (UCRL), which operates on the estimated hidden\nspace. We derive finite-time regret bounds for our algorithm with a weak\ndependence on the dimensionality of the observed space. In fact, our algorithm\nasymptotically achieves the same average regret as the oracle UCRL algorithm,\nwhich has the knowledge of the mapping from hidden to observed spaces. Thus, we\nderive an efficient spectral RL algorithm for ROMDPs.\n", "versions": [{"version": "v1", "created": "Fri, 11 Nov 2016 22:39:01 GMT"}, {"version": "v2", "created": "Mon, 19 Jun 2017 01:52:32 GMT"}, {"version": "v3", "created": "Mon, 18 Jun 2018 01:03:33 GMT"}, {"version": "v4", "created": "Tue, 19 Jun 2018 20:14:54 GMT"}], "update_date": "2018-06-21", "authors_parsed": [["Azizzadenesheli", "Kamyar", ""], ["Lazaric", "Alessandro", ""], ["Anandkumar", "Animashree", ""]]}, {"id": "1611.03954", "submitter": "Muhao Chen", "authors": "Muhao Chen, Yingtao Tian, Mohan Yang, Carlo Zaniolo", "title": "Multilingual Knowledge Graph Embeddings for Cross-lingual Knowledge\n  Alignment", "comments": "Extended version of the IJCAI-17 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many recent works have demonstrated the benefits of knowledge graph\nembeddings in completing monolingual knowledge graphs. Inasmuch as related\nknowledge bases are built in several different languages, achieving\ncross-lingual knowledge alignment will help people in constructing a coherent\nknowledge base, and assist machines in dealing with different expressions of\nentity relationships across diverse human languages. Unfortunately, achieving\nthis highly desirable crosslingual alignment by human labor is very costly and\nerrorprone. Thus, we propose MTransE, a translation-based model for\nmultilingual knowledge graph embeddings, to provide a simple and automated\nsolution. By encoding entities and relations of each language in a separated\nembedding space, MTransE provides transitions for each embedding vector to its\ncross-lingual counterparts in other spaces, while preserving the\nfunctionalities of monolingual embeddings. We deploy three different techniques\nto represent cross-lingual transitions, namely axis calibration, translation\nvectors, and linear transformations, and derive five variants for MTransE using\ndifferent loss functions. Our models can be trained on partially aligned\ngraphs, where just a small portion of triples are aligned with their\ncross-lingual counterparts. The experiments on cross-lingual entity matching\nand triple-wise alignment verification show promising results, with some\nvariants consistently outperforming others on different tasks. We also explore\nhow MTransE preserves the key properties of its monolingual counterpart TransE.\n", "versions": [{"version": "v1", "created": "Sat, 12 Nov 2016 04:28:04 GMT"}, {"version": "v2", "created": "Tue, 21 Feb 2017 01:20:30 GMT"}, {"version": "v3", "created": "Wed, 17 May 2017 19:33:53 GMT"}], "update_date": "2017-05-19", "authors_parsed": [["Chen", "Muhao", ""], ["Tian", "Yingtao", ""], ["Yang", "Mohan", ""], ["Zaniolo", "Carlo", ""]]}, {"id": "1611.03977", "submitter": "Kui Yu", "authors": "Kui Yu, Jiuyong Li, Lin Liu", "title": "A Review on Algorithms for Constraint-based Causal Discovery", "comments": "This paper has been withdrawn by the author due to further\n  improvement", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Causal discovery studies the problem of mining causal relationships between\nvariables from data, which is of primary interest in science. During the past\ndecades, significant amount of progresses have been made toward this\nfundamental data mining paradigm. Recent years, as the availability of abundant\nlarge-sized and complex observational data, the constrain-based approaches have\ngradually attracted a lot of interest and have been widely applied to many\ndiverse real-world problems due to the fast running speed and easy generalizing\nto the problem of causal insufficiency. In this paper, we aim to review the\nconstraint-based causal discovery algorithms. Firstly, we discuss the learning\nparadigm of the constraint-based approaches. Secondly and primarily, the\nstate-of-the-art constraint-based casual inference algorithms are surveyed with\nthe detailed analysis. Thirdly, several related open-source software packages\nand benchmark data repositories are briefly summarized. As a conclusion, some\nopen problems in constraint-based causal discovery are outlined for future\nresearch.\n", "versions": [{"version": "v1", "created": "Sat, 12 Nov 2016 09:25:38 GMT"}, {"version": "v2", "created": "Thu, 24 Nov 2016 22:33:25 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Yu", "Kui", ""], ["Li", "Jiuyong", ""], ["Liu", "Lin", ""]]}, {"id": "1611.04021", "submitter": "Tseng-Hung Chen", "authors": "Kuo-Hao Zeng, Tseng-Hung Chen, Ching-Yao Chuang, Yuan-Hong Liao, Juan\n  Carlos Niebles, Min Sun", "title": "Leveraging Video Descriptions to Learn Video Question Answering", "comments": "7 pages, 5 figures. Accepted to AAAI 2017. Camera-ready version", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a scalable approach to learn video-based question answering (QA):\nanswer a \"free-form natural language question\" about a video content. Our\napproach automatically harvests a large number of videos and descriptions\nfreely available online. Then, a large number of candidate QA pairs are\nautomatically generated from descriptions rather than manually annotated. Next,\nwe use these candidate QA pairs to train a number of video-based QA methods\nextended fromMN (Sukhbaatar et al. 2015), VQA (Antol et al. 2015), SA (Yao et\nal. 2015), SS (Venugopalan et al. 2015). In order to handle non-perfect\ncandidate QA pairs, we propose a self-paced learning procedure to iteratively\nidentify them and mitigate their effects in training. Finally, we evaluate\nperformance on manually generated video-based QA pairs. The results show that\nour self-paced learning procedure is effective, and the extended SS model\noutperforms various baselines.\n", "versions": [{"version": "v1", "created": "Sat, 12 Nov 2016 17:15:57 GMT"}, {"version": "v2", "created": "Mon, 19 Dec 2016 16:07:33 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Zeng", "Kuo-Hao", ""], ["Chen", "Tseng-Hung", ""], ["Chuang", "Ching-Yao", ""], ["Liao", "Yuan-Hong", ""], ["Niebles", "Juan Carlos", ""], ["Sun", "Min", ""]]}, {"id": "1611.04035", "submitter": "Murat Kocaoglu", "authors": "Murat Kocaoglu, Alexandros G. Dimakis, Sriram Vishwanath, Babak\n  Hassibi", "title": "Entropic Causal Inference", "comments": "To appear in AAAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IT math.IT stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of identifying the causal direction between two\ndiscrete random variables using observational data. Unlike previous work, we\nkeep the most general functional model but make an assumption on the unobserved\nexogenous variable: Inspired by Occam's razor, we assume that the exogenous\nvariable is simple in the true causal direction. We quantify simplicity using\nR\\'enyi entropy. Our main result is that, under natural assumptions, if the\nexogenous variable has low $H_0$ entropy (cardinality) in the true direction,\nit must have high $H_0$ entropy in the wrong direction. We establish several\nalgorithmic hardness results about estimating the minimum entropy exogenous\nvariable. We show that the problem of finding the exogenous variable with\nminimum entropy is equivalent to the problem of finding minimum joint entropy\ngiven $n$ marginal distributions, also known as minimum entropy coupling\nproblem. We propose an efficient greedy algorithm for the minimum entropy\ncoupling problem, that for $n=2$ provably finds a local optimum. This gives a\ngreedy algorithm for finding the exogenous variable with minimum $H_1$ (Shannon\nEntropy). Our greedy entropy-based causal inference algorithm has similar\nperformance to the state of the art additive noise models in real datasets. One\nadvantage of our approach is that we make no use of the values of random\nvariables but only their distributions. Our method can therefore be used for\ncausal inference for both ordinal and also categorical data, unlike additive\nnoise models.\n", "versions": [{"version": "v1", "created": "Sat, 12 Nov 2016 18:56:34 GMT"}, {"version": "v2", "created": "Tue, 15 Nov 2016 03:09:53 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Kocaoglu", "Murat", ""], ["Dimakis", "Alexandros G.", ""], ["Vishwanath", "Sriram", ""], ["Hassibi", "Babak", ""]]}, {"id": "1611.04146", "submitter": "Quan Liu", "authors": "Quan Liu, Hui Jiang, Zhen-Hua Ling, Xiaodan Zhu, Si Wei, Yu Hu", "title": "Commonsense Knowledge Enhanced Embeddings for Solving Pronoun\n  Disambiguation Problems in Winograd Schema Challenge", "comments": "Winograd Schema Challenge, Pronoun Disambiguation Problems, Neural\n  Embedding Methods, Commonsense Knowledge", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we propose commonsense knowledge enhanced embeddings (KEE) for\nsolving the Pronoun Disambiguation Problems (PDP). The PDP task we investigate\nin this paper is a complex coreference resolution task which requires the\nutilization of commonsense knowledge. This task is a standard first round test\nset in the 2016 Winograd Schema Challenge. In this task, traditional linguistic\nfeatures that are useful for coreference resolution, e.g. context and gender\ninformation, are no longer effective anymore. Therefore, the KEE models are\nproposed to provide a general framework to make use of commonsense knowledge\nfor solving the PDP problems. Since the PDP task doesn't have training data,\nthe KEE models would be used during the unsupervised feature extraction\nprocess. To evaluate the effectiveness of the KEE models, we propose to\nincorporate various commonsense knowledge bases, including ConceptNet, WordNet,\nand CauseCom, into the KEE training process. We achieved the best performance\nby applying the proposed methods to the 2016 Winograd Schema Challenge. In\naddition, experiments conducted on the standard PDP task indicate that, the\nproposed KEE models could solve the PDP problems by achieving 66.7% accuracy,\nwhich is a new state-of-the-art performance.\n", "versions": [{"version": "v1", "created": "Sun, 13 Nov 2016 15:38:32 GMT"}, {"version": "v2", "created": "Thu, 22 Dec 2016 02:27:16 GMT"}], "update_date": "2016-12-23", "authors_parsed": [["Liu", "Quan", ""], ["Jiang", "Hui", ""], ["Ling", "Zhen-Hua", ""], ["Zhu", "Xiaodan", ""], ["Wei", "Si", ""], ["Hu", "Yu", ""]]}, {"id": "1611.04175", "submitter": "Palash Dey", "authors": "Palash Dey", "title": "Recognizing and Eliciting Weakly Single Crossing Profiles on Trees", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.MA cs.AI cs.DS", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The domain of single crossing preference profiles is a widely studied domain\nin social choice theory. It has been generalized to the domain of single\ncrossing preference profiles with respect to trees which inherits many\ndesirable properties from the single crossing domain, for example, transitivity\nof majority relation, existence of polynomial time algorithms for finding\nwinners of Kemeny voting rule, etc. In this paper, we consider a further\ngeneralization of the domain of single crossing profiles on trees to the domain\nconsisting of all preference profiles which can be extended to single crossing\npreference profiles with respect to some tree by adding more preferences to it.\nWe call this domain the weakly single crossing domain on trees. We present a\npolynomial time algorithm for recognizing weakly single crossing profiles on\ntrees. We then move on to develop a polynomial time algorithm with low query\ncomplexity for eliciting weakly single crossing profiles on trees even when we\ndo not know any tree with respect to which the closure of the input profile is\nsingle crossing and the preferences can be queried only sequentially; moreover,\nthe sequential order is also unknown. We complement the performance of our\npreference elicitation algorithm by proving that our algorithm makes an optimal\nnumber of queries up to constant factors when the number of preferences is\nlarge compared to the number of candidates, even if the input profile is known\nto be single crossing with respect to some given tree and the preferences can\nbe accessed randomly.\n", "versions": [{"version": "v1", "created": "Sun, 13 Nov 2016 19:35:52 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Dey", "Palash", ""]]}, {"id": "1611.04180", "submitter": "Sanjiban Choudhury", "authors": "Sanjiban Choudhury, Ashish Kapoor, Gireeja Ranade, Debadeepta Dey", "title": "Learning to Gather Information via Imitation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.RO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The budgeted information gathering problem - where a robot with a fixed fuel\nbudget is required to maximize the amount of information gathered from the\nworld - appears in practice across a wide range of applications in autonomous\nexploration and inspection with mobile robots. Although there is an extensive\namount of prior work investigating effective approximations of the problem,\nthese methods do not address the fact that their performance is heavily\ndependent on distribution of objects in the world. In this paper, we attempt to\naddress this issue by proposing a novel data-driven imitation learning\nframework.\n  We present an efficient algorithm, EXPLORE, that trains a policy on the\ntarget distribution to imitate a clairvoyant oracle - an oracle that has full\ninformation about the world and computes non-myopic solutions to maximize\ninformation gathered. We validate the approach on a spectrum of results on a\nnumber of 2D and 3D exploration problems that demonstrates the ability of\nEXPLORE to adapt to different object distributions. Additionally, our analysis\nprovides theoretical insight into the behavior of EXPLORE. Our approach paves\nthe way forward for efficiently applying data-driven methods to the domain of\ninformation gathering.\n", "versions": [{"version": "v1", "created": "Sun, 13 Nov 2016 20:20:49 GMT"}], "update_date": "2016-11-15", "authors_parsed": [["Choudhury", "Sanjiban", ""], ["Kapoor", "Ashish", ""], ["Ranade", "Gireeja", ""], ["Dey", "Debadeepta", ""]]}, {"id": "1611.04363", "submitter": "Yujie Qian", "authors": "Yujie Qian, Jie Tang, Kan Wu", "title": "Weakly Learning to Match Experts in Online Community", "comments": "IJCAI 2018", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In online question-and-answer (QA) websites like Quora, one central issue is\nto find (invite) users who are able to provide answers to a given question and\nat the same time would be unlikely to say \"no\" to the invitation. The challenge\nis how to trade off the matching degree between users' expertise and the\nquestion topic, and the likelihood of positive response from the invited users.\nIn this paper, we formally formulate the problem and develop a weakly\nsupervised factor graph (WeakFG) model to address the problem. The model\nexplicitly captures expertise matching degree between questions and users. To\nmodel the likelihood that an invited user is willing to answer a specific\nquestion, we incorporate a set of correlations based on social identity theory\ninto the WeakFG model. We use two different genres of datasets: QA-Expert and\nPaper-Reviewer, to validate the proposed model. Our experimental results show\nthat the proposed model can significantly outperform (+1.5-10.7% by MAP) the\nstate-of-the-art algorithms for matching users (experts) with community\nquestions. We have also developed an online system to further demonstrate the\nadvantages of the proposed method.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 12:46:24 GMT"}, {"version": "v2", "created": "Mon, 7 May 2018 21:35:10 GMT"}], "update_date": "2018-05-09", "authors_parsed": [["Qian", "Yujie", ""], ["Tang", "Jie", ""], ["Wu", "Kan", ""]]}, {"id": "1611.04369", "submitter": "Yujie Qian", "authors": "Yujie Qian, Yinpeng Dong, Ye Ma, Hailong Jin, and Juanzi Li", "title": "Feature Engineering and Ensemble Modeling for Paper Acceptance Rank\n  Prediction", "comments": "2nd place winner report of KDD Cup 2016. More details can be found at\n  https://kddcup2016.azurewebsites.net", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Measuring research impact and ranking academic achievement are important and\nchallenging problems. Having an objective picture of research institution is\nparticularly valuable for students, parents and funding agencies, and also\nattracts attention from government and industry. KDD Cup 2016 proposes the\npaper acceptance rank prediction task, in which the participants are asked to\nrank the importance of institutions based on predicting how many of their\npapers will be accepted at the 8 top conferences in computer science. In our\nwork, we adopt a three-step feature engineering method, including basic\nfeatures definition, finding similar conferences to enhance the feature set,\nand dimension reduction using PCA. We propose three ranking models and the\nensemble methods for combining such models. Our experiment verifies the\neffectiveness of our approach. In KDD Cup 2016, we achieved the overall rank of\nthe 2nd place.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 12:59:07 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Qian", "Yujie", ""], ["Dong", "Yinpeng", ""], ["Ma", "Ye", ""], ["Jin", "Hailong", ""], ["Li", "Juanzi", ""]]}, {"id": "1611.04488", "submitter": "Danica J. Sutherland", "authors": "Danica J. Sutherland, Hsiao-Yu Tung, Heiko Strathmann, Soumyajit De,\n  Aaditya Ramdas, Alex Smola, Arthur Gretton", "title": "Generative Models and Model Criticism via Optimized Maximum Mean\n  Discrepancy", "comments": "Published at ICLR 2017 (public comments:\n  http://openreview.net/forum?id=HJWHIKqgl )", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.NE stat.ME", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a method to optimize the representation and distinguishability of\nsamples from two probability distributions, by maximizing the estimated power\nof a statistical test based on the maximum mean discrepancy (MMD). This\noptimized MMD is applied to the setting of unsupervised learning by generative\nadversarial networks (GAN), in which a model attempts to generate realistic\nsamples, and a discriminator attempts to tell these apart from data samples. In\nthis context, the MMD may be used in two roles: first, as a discriminator,\neither directly on the samples, or on features of the samples. Second, the MMD\ncan be used to evaluate the performance of a generative model, by testing the\nmodel's samples against a reference data set. In the latter role, the optimized\nMMD is particularly helpful, as it gives an interpretable indication of how the\nmodel and data distributions differ, even in cases where individual model\nsamples are not easily distinguished either by eye or by classifier.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 17:28:27 GMT"}, {"version": "v2", "created": "Tue, 15 Nov 2016 13:07:50 GMT"}, {"version": "v3", "created": "Thu, 17 Nov 2016 20:30:37 GMT"}, {"version": "v4", "created": "Fri, 10 Feb 2017 18:28:49 GMT"}, {"version": "v5", "created": "Thu, 6 Jun 2019 19:54:37 GMT"}, {"version": "v6", "created": "Thu, 14 Jan 2021 06:14:42 GMT"}], "update_date": "2021-01-15", "authors_parsed": [["Sutherland", "Danica J.", ""], ["Tung", "Hsiao-Yu", ""], ["Strathmann", "Heiko", ""], ["De", "Soumyajit", ""], ["Ramdas", "Aaditya", ""], ["Smola", "Alex", ""], ["Gretton", "Arthur", ""]]}, {"id": "1611.04503", "submitter": "Hideki Nakayama", "authors": "Hideki Nakayama and Noriki Nishida", "title": "Zero-resource Machine Translation by Multimodal Encoder-decoder Network\n  with Multimedia Pivot", "comments": "Some error corrections in Sect.2.2 and Table 5, Machine Translation,\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.CV cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose an approach to build a neural machine translation system with no\nsupervised resources (i.e., no parallel corpora) using multimodal embedded\nrepresentation over texts and images. Based on the assumption that text\ndocuments are often likely to be described with other multimedia information\n(e.g., images) somewhat related to the content, we try to indirectly estimate\nthe relevance between two languages. Using multimedia as the \"pivot\", we\nproject all modalities into one common hidden space where samples belonging to\nsimilar semantic concepts should come close to each other, whatever the\nobserved space of each sample is. This modality-agnostic representation is the\nkey to bridging the gap between different modalities. Putting a decoder on top\nof it, our network can flexibly draw the outputs from any input modality.\nNotably, in the testing phase, we need only source language texts as the input\nfor translation. In experiments, we tested our method on two benchmarks to show\nthat it can achieve reasonable translation performance. We compared and\ninvestigated several possible implementations and found that an end-to-end\nmodel that simultaneously optimized both rank loss in multimodal encoders and\ncross-entropy loss in decoders performed the best.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 18:07:54 GMT"}, {"version": "v2", "created": "Sun, 21 May 2017 17:36:30 GMT"}, {"version": "v3", "created": "Sun, 23 Jul 2017 15:52:08 GMT"}], "update_date": "2017-07-25", "authors_parsed": [["Nakayama", "Hideki", ""], ["Nishida", "Noriki", ""]]}, {"id": "1611.04535", "submitter": "Ellen Vitercik", "authors": "Maria-Florina Balcan, Vaishnavh Nagarajan, Ellen Vitercik, and Colin\n  White", "title": "Learning-Theoretic Foundations of Algorithm Configuration for\n  Combinatorial Partitioning Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Max-cut, clustering, and many other partitioning problems that are of\nsignificant importance to machine learning and other scientific fields are\nNP-hard, a reality that has motivated researchers to develop a wealth of\napproximation algorithms and heuristics. Although the best algorithm to use\ntypically depends on the specific application domain, a worst-case analysis is\noften used to compare algorithms. This may be misleading if worst-case\ninstances occur infrequently, and thus there is a demand for optimization\nmethods which return the algorithm configuration best suited for the given\napplication's typical inputs. We address this problem for clustering, max-cut,\nand other partitioning problems, such as integer quadratic programming, by\ndesigning computationally efficient and sample efficient learning algorithms\nwhich receive samples from an application-specific distribution over problem\ninstances and learn a partitioning algorithm with high expected performance.\nOur algorithms learn over common integer quadratic programming and clustering\nalgorithm families: SDP rounding algorithms and agglomerative clustering\nalgorithms with dynamic programming. For our sample complexity analysis, we\nprovide tight bounds on the pseudodimension of these algorithm classes, and\nshow that surprisingly, even for classes of algorithms parameterized by a\nsingle parameter, the pseudo-dimension is superconstant. In this way, our work\nboth contributes to the foundations of algorithm configuration and pushes the\nboundaries of learning theory, since the algorithm classes we analyze consist\nof multi-stage optimization procedures and are significantly more complex than\nclasses typically studied in learning theory.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 19:22:21 GMT"}, {"version": "v2", "created": "Wed, 10 May 2017 23:57:09 GMT"}, {"version": "v3", "created": "Wed, 17 May 2017 10:08:24 GMT"}, {"version": "v4", "created": "Tue, 16 Oct 2018 16:07:08 GMT"}], "update_date": "2018-10-17", "authors_parsed": [["Balcan", "Maria-Florina", ""], ["Nagarajan", "Vaishnavh", ""], ["Vitercik", "Ellen", ""], ["White", "Colin", ""]]}, {"id": "1611.04558", "submitter": "Mike Schuster", "authors": "Melvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu,\n  Zhifeng Chen, Nikhil Thorat, Fernanda Vi\\'egas, Martin Wattenberg, Greg\n  Corrado, Macduff Hughes, Jeffrey Dean", "title": "Google's Multilingual Neural Machine Translation System: Enabling\n  Zero-Shot Translation", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a simple solution to use a single Neural Machine Translation (NMT)\nmodel to translate between multiple languages. Our solution requires no change\nin the model architecture from our base system but instead introduces an\nartificial token at the beginning of the input sentence to specify the required\ntarget language. The rest of the model, which includes encoder, decoder and\nattention, remains unchanged and is shared across all languages. Using a shared\nwordpiece vocabulary, our approach enables Multilingual NMT using a single\nmodel without any increase in parameters, which is significantly simpler than\nprevious proposals for Multilingual NMT. Our method often improves the\ntranslation quality of all involved language pairs, even while keeping the\ntotal number of model parameters constant. On the WMT'14 benchmarks, a single\nmultilingual model achieves comparable performance for\nEnglish$\\rightarrow$French and surpasses state-of-the-art results for\nEnglish$\\rightarrow$German. Similarly, a single multilingual model surpasses\nstate-of-the-art results for French$\\rightarrow$English and\nGerman$\\rightarrow$English on WMT'14 and WMT'15 benchmarks respectively. On\nproduction corpora, multilingual models of up to twelve language pairs allow\nfor better translation of many individual pairs. In addition to improving the\ntranslation quality of language pairs that the model was trained with, our\nmodels can also learn to perform implicit bridging between language pairs never\nseen explicitly during training, showing that transfer learning and zero-shot\ntranslation is possible for neural translation. Finally, we show analyses that\nhints at a universal interlingua representation in our models and show some\ninteresting examples when mixing languages.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 20:24:39 GMT"}, {"version": "v2", "created": "Mon, 21 Aug 2017 20:33:43 GMT"}], "update_date": "2017-08-23", "authors_parsed": [["Johnson", "Melvin", ""], ["Schuster", "Mike", ""], ["Le", "Quoc V.", ""], ["Krikun", "Maxim", ""], ["Wu", "Yonghui", ""], ["Chen", "Zhifeng", ""], ["Thorat", "Nikhil", ""], ["Vi\u00e9gas", "Fernanda", ""], ["Wattenberg", "Martin", ""], ["Corrado", "Greg", ""], ["Hughes", "Macduff", ""], ["Dean", "Jeffrey", ""]]}, {"id": "1611.04636", "submitter": "Honglin Zheng", "authors": "Honglin Zheng, Tianlang Chen, Jiebo Luo", "title": "When Saliency Meets Sentiment: Understanding How Image Content Invokes\n  Emotion and Sentiment", "comments": "7 pages, 5 figures, submitted to AAAI-17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Sentiment analysis is crucial for extracting social signals from social media\ncontent. Due to the prevalence of images in social media, image sentiment\nanalysis is receiving increasing attention in recent years. However, most\nexisting systems are black-boxes that do not provide insight on how image\ncontent invokes sentiment and emotion in the viewers. Psychological studies\nhave confirmed that salient objects in an image often invoke emotions. In this\nwork, we investigate more fine-grained and more comprehensive interaction\nbetween visual saliency and visual sentiment. In particular, we partition\nimages in several primary scene-type dimensions, including: open-closed,\nnatural-manmade, indoor-outdoor, and face-noface. Using state of the art\nsaliency detection algorithm and sentiment classification algorithm, we examine\nhow the sentiment of the salient region(s) in an image relates to the overall\nsentiment of the image. The experiments on a representative image emotion\ndataset have shown interesting correlation between saliency and sentiment in\ndifferent scene types and in turn shed light on the mechanism of visual\nsentiment evocation.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 22:02:09 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Zheng", "Honglin", ""], ["Chen", "Tianlang", ""], ["Luo", "Jiebo", ""]]}, {"id": "1611.04642", "submitter": "Po-Sen Huang", "authors": "Yelong Shen, Po-Sen Huang, Ming-Wei Chang, Jianfeng Gao", "title": "Link Prediction using Embedded Knowledge Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Since large knowledge bases are typically incomplete, missing facts need to\nbe inferred from observed facts in a task called knowledge base completion. The\nmost successful approaches to this task have typically explored explicit paths\nthrough sequences of triples. These approaches have usually resorted to\nhuman-designed sampling procedures, since large knowledge graphs produce\nprohibitively large numbers of possible paths, most of which are uninformative.\nAs an alternative approach, we propose performing a single, short sequence of\ninteractive lookup operations on an embedded knowledge graph which has been\ntrained through end-to-end backpropagation to be an optimized and compressed\nversion of the initial knowledge base. Our proposed model, called Embedded\nKnowledge Graph Network (EKGN), achieves new state-of-the-art results on\npopular knowledge base completion benchmarks.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 22:54:45 GMT"}, {"version": "v2", "created": "Sat, 22 Apr 2017 19:46:44 GMT"}, {"version": "v3", "created": "Sat, 28 Oct 2017 03:02:10 GMT"}, {"version": "v4", "created": "Wed, 8 Nov 2017 18:59:19 GMT"}, {"version": "v5", "created": "Sun, 22 Apr 2018 05:22:58 GMT"}], "update_date": "2018-04-24", "authors_parsed": [["Shen", "Yelong", ""], ["Huang", "Po-Sen", ""], ["Chang", "Ming-Wei", ""], ["Gao", "Jianfeng", ""]]}, {"id": "1611.04660", "submitter": "Pranjul Yadav", "authors": "Pranjul Yadav, Lisiane Prunelli, Alexander Hoff, Michael Steinbach,\n  Bonnie Westra, Vipin Kumar, Gyorgy Simon", "title": "Causal Inference in Observational Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Our aging population increasingly suffers from multiple chronic diseases\nsimultaneously, necessitating the comprehensive treatment of these conditions.\nFinding the optimal set of drugs for a combinatorial set of diseases is a\ncombinatorial pattern exploration problem. Association rule mining is a popular\ntool for such problems, but the requirement of health care for finding causal,\nrather than associative, patterns renders association rule mining unsuitable.\nTo address this issue, we propose a novel framework based on the Rubin-Neyman\ncausal model for extracting causal rules from observational data, correcting\nfor a number of common biases. Specifically, given a set of interventions and a\nset of items that define subpopulations (e.g., diseases), we wish to find all\nsubpopulations in which effective intervention combinations exist and in each\nsuch subpopulation, we wish to find all intervention combinations such that\ndropping any intervention from this combination will reduce the efficacy of the\ntreatment. A key aspect of our framework is the concept of closed intervention\nsets which extend the concept of quantifying the effect of a single\nintervention to a set of concurrent interventions. We also evaluated our causal\nrule mining framework on the Electronic Health Records (EHR) data of a large\ncohort of patients from Mayo Clinic and showed that the patterns we extracted\nare sufficiently rich to explain the controversial findings in the medical\nliterature regarding the effect of a class of cholesterol drugs on Type-II\nDiabetes Mellitus (T2DM).\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 00:59:21 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Yadav", "Pranjul", ""], ["Prunelli", "Lisiane", ""], ["Hoff", "Alexander", ""], ["Steinbach", "Michael", ""], ["Westra", "Bonnie", ""], ["Kumar", "Vipin", ""], ["Simon", "Gyorgy", ""]]}, {"id": "1611.04709", "submitter": "Jin Tian", "authors": "Jin Tian", "title": "Recoverability of Joint Distribution from Missing Data", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A probabilistic query may not be estimable from observed data corrupted by\nmissing values if the data are not missing at random (MAR). It is therefore of\ntheoretical interest and practical importance to determine in principle whether\na probabilistic query is estimable from missing data or not when the data are\nnot MAR. We present an algorithm that systematically determines whether the\njoint probability is estimable from observed data with missing values, assuming\nthat the data-generation model is represented as a Bayesian network containing\nunobserved latent variables that not only encodes the dependencies among the\nvariables but also explicitly portrays the mechanisms responsible for the\nmissingness process. The result significantly advances the existing work.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 06:06:42 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Tian", "Jin", ""]]}, {"id": "1611.04717", "submitter": "Haoran Tang", "authors": "Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, Xi Chen, Yan\n  Duan, John Schulman, Filip De Turck, Pieter Abbeel", "title": "#Exploration: A Study of Count-Based Exploration for Deep Reinforcement\n  Learning", "comments": "10 pages main text + 10 pages supplementary. Published at NIPS 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Count-based exploration algorithms are known to perform near-optimally when\nused in conjunction with tabular reinforcement learning (RL) methods for\nsolving small discrete Markov decision processes (MDPs). It is generally\nthought that count-based methods cannot be applied in high-dimensional state\nspaces, since most states will only occur once. Recent deep RL exploration\nstrategies are able to deal with high-dimensional continuous state spaces\nthrough complex heuristics, often relying on optimism in the face of\nuncertainty or intrinsic motivation. In this work, we describe a surprising\nfinding: a simple generalization of the classic count-based approach can reach\nnear state-of-the-art performance on various high-dimensional and/or continuous\ndeep RL benchmarks. States are mapped to hash codes, which allows to count\ntheir occurrences with a hash table. These counts are then used to compute a\nreward bonus according to the classic count-based exploration theory. We find\nthat simple hash functions can achieve surprisingly good results on many\nchallenging tasks. Furthermore, we show that a domain-dependent learned hash\ncode may further improve these results. Detailed analysis reveals important\naspects of a good hash function: 1) having appropriate granularity and 2)\nencoding information relevant to solving the MDP. This exploration strategy\nachieves near state-of-the-art performance on both continuous control tasks and\nAtari 2600 games, hence providing a simple yet powerful baseline for solving\nMDPs that require considerable exploration.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 06:42:24 GMT"}, {"version": "v2", "created": "Wed, 11 Jan 2017 18:29:16 GMT"}, {"version": "v3", "created": "Tue, 5 Dec 2017 16:44:47 GMT"}], "update_date": "2017-12-06", "authors_parsed": [["Tang", "Haoran", ""], ["Houthooft", "Rein", ""], ["Foote", "Davis", ""], ["Stooke", "Adam", ""], ["Chen", "Xi", ""], ["Duan", "Yan", ""], ["Schulman", "John", ""], ["De Turck", "Filip", ""], ["Abbeel", "Pieter", ""]]}, {"id": "1611.04810", "submitter": "V\\'ictor Mart\\'inez", "authors": "V\\'ictor Mart\\'inez, Fernando Berzal, Juan-Carlos Cubero", "title": "The NOESIS Network-Oriented Exploration, Simulation, and Induction\n  System", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SI cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Network data mining has become an important area of study due to the large\nnumber of problems it can be applied to. This paper presents NOESIS, an open\nsource framework for network data mining that provides a large collection of\nnetwork analysis techniques, including the analysis of network structural\nproperties, community detection methods, link scoring, and link prediction, as\nwell as network visualization algorithms. It also features a complete\nstand-alone graphical user interface that facilitates the use of all these\ntechniques. The NOESIS framework has been designed using solid object-oriented\ndesign principles and structured parallel programming. As a lightweight library\nwith minimal external dependencies and a permissive software license, NOESIS\ncan be incorporated into other software projects. Released under a BSD license,\nit is available from http://noesis.ikor.org.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 12:55:49 GMT"}, {"version": "v2", "created": "Fri, 23 Jun 2017 14:08:42 GMT"}], "update_date": "2017-06-26", "authors_parsed": [["Mart\u00ednez", "V\u00edctor", ""], ["Berzal", "Fernando", ""], ["Cubero", "Juan-Carlos", ""]]}, {"id": "1611.04845", "submitter": "Xinyi Wu", "authors": "Xinyi Wu, Kartik Balkumar, Qi Luo, Robert Hampshire, Romesh Saigal", "title": "An Evaluation of Information Sharing Parking Guidance Policies Using a\n  Bayesian Approach", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Real-time parking occupancy information is critical for a parking management\nsystem to facilitate drivers to park more efficiently. Recent advances in\nconnected and automated vehicle technologies enable sensor-equipped cars (probe\ncars) to detect and broadcast available parking spaces when driving through\nparking lots. In this paper, we evaluate the impact of market penetration of\nprobe cars on the system performance, and investigate different parking\nguidance policies to improve the data acquisition process. We adopt a\nsimulation-based approach to impose four policies on an off- street parking lot\ninfluencing the behavior of probe cars to park in assigned parking spaces. This\nin turn effects the scanning route and the parking space occupancy estimations.\nThe last policy we propose is a near-optimal guidance strategy that maximizes\nthe information gain of posteriors. The results suggest that an efficient\ninformation gathering policy can compensate for low penetration of connected\nand automated vehicles. We also highlight the policy trade-off that occur while\nattempting to maximize information gain through explorations and improve\nassignment accuracy through exploitations. Our results can assist urban policy\nmakers in designing and managing smart parking systems.\n", "versions": [{"version": "v1", "created": "Mon, 14 Nov 2016 15:30:42 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Wu", "Xinyi", ""], ["Balkumar", "Kartik", ""], ["Luo", "Qi", ""], ["Hampshire", "Robert", ""], ["Saigal", "Romesh", ""]]}, {"id": "1611.04969", "submitter": "Philip Gasteiger", "authors": "Philip Gasteiger, Carmine Dodaro, Benjamin Musitsch, Kristian Reale,\n  Francesco Ricca, Konstantin Schekotihin", "title": "An integrated Graphical User Interface for Debugging Answer Set Programs", "comments": "Paper presented at the 1st Workshop on Trends and Applications of\n  Answer Set Programming (TAASP 2016), Klagenfurt, Austria, 26 September 2016,\n  15 pages, LaTeX, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Answer Set Programming (ASP) is an expressive knowledge representation and\nreasoning framework. Due to its rather simple syntax paired with\nhigh-performance solvers, ASP is interesting for industrial applications.\nHowever, to err is human and thus debugging is an important activity during the\ndevelopment process. Therefore, tools for debugging non-ground answer set\nprograms are needed. In this paper, we present a new graphical debugging\ninterface for non-ground answer set programs. The tool is based on the\nrecently-introduced DWASP approach for debugging and it simplifies the\ninteraction with the debugger. Furthermore, the debugging interface is\nintegrated in ASPIDE, a rich IDE for answer set programs. With our extension\nASPIDE turns into a full-fledged IDE by offering debugging support.\n", "versions": [{"version": "v1", "created": "Tue, 15 Nov 2016 18:11:43 GMT"}], "update_date": "2016-11-16", "authors_parsed": [["Gasteiger", "Philip", ""], ["Dodaro", "Carmine", ""], ["Musitsch", "Benjamin", ""], ["Reale", "Kristian", ""], ["Ricca", "Francesco", ""], ["Schekotihin", "Konstantin", ""]]}, {"id": "1611.05104", "submitter": "Sabeek Pradhan", "authors": "Shayne Longpre, Sabeek Pradhan, Caiming Xiong, Richard Socher", "title": "A Way out of the Odyssey: Analyzing and Combining Recent Insights for\n  LSTMs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  LSTMs have become a basic building block for many deep NLP models. In recent\nyears, many improvements and variations have been proposed for deep sequence\nmodels in general, and LSTMs in particular. We propose and analyze a series of\naugmentations and modifications to LSTM networks resulting in improved\nperformance for text classification datasets. We observe compounding\nimprovements on traditional LSTMs using Monte Carlo test-time model averaging,\naverage pooling, and residual connections, along with four other suggested\nmodifications. Our analysis provides a simple, reliable, and high quality\nbaseline model.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 00:53:01 GMT"}, {"version": "v2", "created": "Sat, 17 Dec 2016 06:47:05 GMT"}], "update_date": "2016-12-20", "authors_parsed": [["Longpre", "Shayne", ""], ["Pradhan", "Sabeek", ""], ["Xiong", "Caiming", ""], ["Socher", "Richard", ""]]}, {"id": "1611.05170", "submitter": "Charith Perera", "authors": "Luiz H. Nunes, Julio C. Estrella, Alexandre C. B. Delbem, Charith\n  Perera, Stephan Reiff-Marganiec", "title": "The Effects of Relative Importance of User Constraints in Cloud of\n  Things Resource Discovery: A Case Study", "comments": "Proceedings of the 9th IEEE/ACM International Conference on Utility\n  and Cloud Computing (UCC 2016) Shaghai, China, December, 2016", "journal-ref": "Proceedings of the 9th IEEE/ACM International Conference on\n  Utility and Cloud Computing (UCC 2016) Shaghai, China, December, 2016", "doi": null, "report-no": null, "categories": "cs.AI cs.DC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Over the last few years, the number of smart objects connected to the\nInternet has grown exponentially in comparison to the number of services and\napplications. The integration between Cloud Computing and Internet of Things,\nnamed as Cloud of Things, plays a key role in managing the connected things,\ntheir data and services. One of the main challenges in Cloud of Things is the\nresource discovery of the smart objects and their reuse in different contexts.\nMost of the existent work uses some kind of multi-criteria decision analysis\nalgorithm to perform the resource discovery, but do not evaluate the impact\nthat the user constraints has in the final solution. In this paper, we analyse\nthe behaviour of the SAW, TOPSIS and VIKOR multi-objective decision analyses\nalgorithms and the impact of user constraints on them. We evaluated the quality\nof the proposed solutions using the Pareto-optimality concept.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 07:15:03 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Nunes", "Luiz H.", ""], ["Estrella", "Julio C.", ""], ["Delbem", "Alexandre C. B.", ""], ["Perera", "Charith", ""], ["Reiff-Marganiec", "Stephan", ""]]}, {"id": "1611.05187", "submitter": "Paolo Detti", "authors": "Paolo Detti and Garazi Zabalo Manrique de Lara", "title": "Variable Neighborhood Search Algorithms for the multi-depot dial-a-ride\n  problem with heterogeneous vehicles and users", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, a study on Variable Neighborhood Search algorithms for\nmulti-depot dial-a-ride problems is presented. In dial-a-ride problems patients\nneed to be transported from pre-specified pickup locations to pre-specified\ndelivery locations, under different considerations. The addressed problem\npresents several constraints and features, such as heterogeneous vehicles,\ndistributed in different depots, and heterogeneous patients. The aim is of\nminimizing the total routing cost, while respecting time-window, ride-time,\ncapacity and route duration constraints. The objective of the study is of\ndetermining the best algorithm configuration in terms of initial solution,\nneighborhood and local search procedures. At this aim, two different procedures\nfor the computation of an initial solution, six different type of neighborhoods\nand five local search procedures, where only intra-route changes are made, have\nbeen considered and compared.\n  We have also evaluated an \"adjusting procedure\" that aims to produce feasible\nsolutions from infeasible solutions with small constraints violations. The\ndifferent VNS algorithms have been tested on instances from literature as well\nas on random instances arising from a real-world healthcare application.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 09:05:54 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Detti", "Paolo", ""], ["de Lara", "Garazi Zabalo Manrique", ""]]}, {"id": "1611.05190", "submitter": "Carmine Dodaro", "authors": "Carmine Dodaro, Philip Gasteiger, Nicola Leone, Benjamin Musitsch,\n  Francesco Ricca, and Konstantin Schekotihin", "title": "Driving CDCL Search", "comments": "Paper presented at the 1st Workshop on Trends and Applications of\n  Answer Set Programming (TAASP 2016), Klagenfurt, Austria, 26 September 2016,\n  15 pages, LaTeX, 5 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The CDCL algorithm is the leading solution adopted by state-of-the-art\nsolvers for SAT, SMT, ASP, and others. Experiments show that the performance of\nCDCL solvers can be significantly boosted by embedding domain-specific\nheuristics, especially on large real-world problems. However, a proper\nintegration of such criteria in off-the-shelf CDCL implementations is not\nobvious. In this paper, we distill the key ingredients that drive the search of\nCDCL solvers, and propose a general framework for designing and implementing\nnew heuristics. We implemented our strategy in an ASP solver, and we\nexperimented on two industrial domains. On hard problem instances,\nstate-of-the-art implementations fail to find any solution in acceptable time,\nwhereas our implementation is very successful and finds all solutions.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 09:13:26 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Dodaro", "Carmine", ""], ["Gasteiger", "Philip", ""], ["Leone", "Nicola", ""], ["Musitsch", "Benjamin", ""], ["Ricca", "Francesco", ""], ["Schekotihin", "Konstantin", ""]]}, {"id": "1611.05368", "submitter": "Jeremiah Johnson", "authors": "Jeremiah Johnson", "title": "Neural Style Representations and the Large-Scale Classification of\n  Artistic Style", "comments": "10 pages, 4 figures, 2 tables", "journal-ref": "Proceedings of the Future Technologies Conference, 2017", "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.IR stat.AP stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The artistic style of a painting is a subtle aesthetic judgment used by art\nhistorians for grouping and classifying artwork. The recently introduced\n`neural-style' algorithm substantially succeeds in merging the perceived\nartistic style of one image or set of images with the perceived content of\nanother. In light of this and other recent developments in image analysis via\nconvolutional neural networks, we investigate the effectiveness of a\n`neural-style' representation for classifying the artistic style of paintings.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 17:04:04 GMT"}], "update_date": "2019-06-04", "authors_parsed": [["Johnson", "Jeremiah", ""]]}, {"id": "1611.05379", "submitter": "Roger Moore", "authors": "Prof. Roger K. Moore", "title": "PCT and Beyond: Towards a Computational Framework for `Intelligent'\n  Communicative Systems", "comments": "To appear in A. McElhone & W. Mansell (Eds.), Living Control Systems\n  IV: Perceptual Control Theory and the Future of the Life and Social Sciences,\n  Benchmark Publications Inc", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.HC cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent years have witnessed increasing interest in the potential benefits of\n`intelligent' autonomous machines such as robots. Honda's Asimo humanoid robot,\niRobot's Roomba robot vacuum cleaner and Google's driverless cars have fired\nthe imagination of the general public, and social media buzz with speculation\nabout a utopian world of helpful robot assistants or the coming robot\napocalypse! However, there is a long way to go before autonomous systems reach\nthe level of capabilities required for even the simplest of tasks involving\nhuman-robot interaction - especially if it involves communicative behaviour\nsuch as speech and language. Of course the field of Artificial Intelligence\n(AI) has made great strides in these areas, and has moved on from abstract\nhigh-level rule-based paradigms to embodied architectures whose operations are\ngrounded in real physical environments. What is still missing, however, is an\noverarching theory of intelligent communicative behaviour that informs\nsystem-level design decisions in order to provide a more coherent approach to\nsystem integration. This chapter introduces the beginnings of such a framework\ninspired by the principles of Perceptual Control Theory (PCT). In particular,\nit is observed that PCT has hitherto tended to view perceptual processes as a\nrelatively straightforward series of transformations from sensation to\nperception, and has overlooked the potential of powerful generative model-based\nsolutions that have emerged in practical fields such as visual or auditory\nscene analysis. Starting from first principles, a sequence of arguments is\npresented which not only shows how these ideas might be integrated into PCT,\nbut which also extend PCT towards a remarkably symmetric architecture for a\nneeds-driven communicative agent. It is concluded that, if behaviour is the\ncontrol of perception, then perception is the simulation of behaviour.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 17:32:10 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Moore", "Prof. Roger K.", ""]]}, {"id": "1611.05416", "submitter": "Xiao Zhang", "authors": "Zheng Sun, Jiaqi Liu, Zewang Zhang, Jingwen Chen, Zhao Huo, Ching Hua\n  Lee, and Xiao Zhang", "title": "Composing Music with Grammar Argumented Neural Networks and Note-Level\n  Encoding", "comments": "6 pages, 4 figures", "journal-ref": "2018 Asia-Pacific Signal and Information Processing Association\n  Annual Summit and Conference (APSIPA ASC), p1864-1867", "doi": "10.23919/APSIPA.2018.8659792", "report-no": null, "categories": "cs.LG cs.AI cs.SD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Creating aesthetically pleasing pieces of art, including music, has been a\nlong-term goal for artificial intelligence research. Despite recent successes\nof long-short term memory (LSTM) recurrent neural networks (RNNs) in sequential\nlearning, LSTM neural networks have not, by themselves, been able to generate\nnatural-sounding music conforming to music theory. To transcend this\ninadequacy, we put forward a novel method for music composition that combines\nthe LSTM with Grammars motivated by music theory. The main tenets of music\ntheory are encoded as grammar argumented (GA) filters on the training data,\nsuch that the machine can be trained to generate music inheriting the\nnaturalness of human-composed pieces from the original dataset while adhering\nto the rules of music theory. Unlike previous approaches, pitches and durations\nare encoded as one semantic entity, which we refer to as note-level encoding.\nThis allows easy implementation of music theory grammars, as well as closer\nemulation of the thinking pattern of a musician. Although the GA rules are\napplied to the training data and never directly to the LSTM music generation,\nour machine still composes music that possess high incidences of diatonic scale\nnotes, small pitch intervals and chords, in deference to music theory.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 19:42:40 GMT"}, {"version": "v2", "created": "Wed, 7 Dec 2016 20:51:36 GMT"}], "update_date": "2019-03-25", "authors_parsed": [["Sun", "Zheng", ""], ["Liu", "Jiaqi", ""], ["Zhang", "Zewang", ""], ["Chen", "Jingwen", ""], ["Huo", "Zhao", ""], ["Lee", "Ching Hua", ""], ["Zhang", "Xiao", ""]]}, {"id": "1611.05425", "submitter": "Tim Weninger PhD", "authors": "Baoxu Shi and Tim Weninger", "title": "ProjE: Embedding Projection for Knowledge Graph Completion", "comments": "14 pages, Accepted to AAAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  With the large volume of new information created every day, determining the\nvalidity of information in a knowledge graph and filling in its missing parts\nare crucial tasks for many researchers and practitioners. To address this\nchallenge, a number of knowledge graph completion methods have been developed\nusing low-dimensional graph embeddings. Although researchers continue to\nimprove these models using an increasingly complex feature space, we show that\nsimple changes in the architecture of the underlying model can outperform\nstate-of-the-art models without the need for complex feature engineering. In\nthis work, we present a shared variable neural network model called ProjE that\nfills-in missing information in a knowledge graph by learning joint embeddings\nof the knowledge graph's entities and edges, and through subtle, but important,\nchanges to the standard loss function. In doing so, ProjE has a parameter size\nthat is smaller than 11 out of 15 existing methods while performing $37\\%$\nbetter than the current-best method on standard datasets. We also show, via a\nnew fact checking task, that ProjE is capable of accurately determining the\nveracity of many declarative statements.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 20:09:08 GMT"}], "update_date": "2016-11-17", "authors_parsed": [["Shi", "Baoxu", ""], ["Weninger", "Tim", ""]]}, {"id": "1611.05497", "submitter": "Anagha Kulkarni", "authors": "Anagha Kulkarni, Yantian Zha, Tathagata Chakraborti, Satya Gautam\n  Vadlamudi, Yu Zhang, Subbarao Kambhampati", "title": "Explicablility as Minimizing Distance from Expected Behavior", "comments": "9 pages, 8 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to have effective human-AI collaboration, it is necessary to address\nhow the AI agent's behavior is being perceived by the humans-in-the-loop. When\nthe agent's task plans are generated without such considerations, they may\noften demonstrate inexplicable behavior from the human's point of view. This\nproblem may arise due to the human's partial or inaccurate understanding of the\nagent's planning model. This may have serious implications from increased\ncognitive load to more serious concerns of safety around a physical agent. In\nthis paper, we address this issue by modeling plan explicability as a function\nof the distance between a plan that agent makes and the plan that human expects\nit to make. We learn a regression model for mapping the plan distances to\nexplicability scores of plans and develop an anytime search algorithm that can\nuse this model as a heuristic to come up with progressively explicable plans.\nWe evaluate the effectiveness of our approach in a simulated autonomous car\ndomain and a physical robot domain.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 23:24:38 GMT"}, {"version": "v2", "created": "Tue, 5 Dec 2017 17:25:02 GMT"}, {"version": "v3", "created": "Thu, 26 Jul 2018 04:49:09 GMT"}, {"version": "v4", "created": "Wed, 13 Mar 2019 18:40:29 GMT"}], "update_date": "2019-03-15", "authors_parsed": [["Kulkarni", "Anagha", ""], ["Zha", "Yantian", ""], ["Chakraborti", "Tathagata", ""], ["Vadlamudi", "Satya Gautam", ""], ["Zhang", "Yu", ""], ["Kambhampati", "Subbarao", ""]]}, {"id": "1611.05546", "submitter": "Damien Teney", "authors": "Damien Teney, Anton van den Hengel", "title": "Zero-Shot Visual Question Answering", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Part of the appeal of Visual Question Answering (VQA) is its promise to\nanswer new questions about previously unseen images. Most current methods\ndemand training questions that illustrate every possible concept, and will\ntherefore never achieve this capability, since the volume of required training\ndata would be prohibitive. Answering general questions about images requires\nmethods capable of Zero-Shot VQA, that is, methods able to answer questions\nbeyond the scope of the training questions. We propose a new evaluation\nprotocol for VQA methods which measures their ability to perform Zero-Shot VQA,\nand in doing so highlights significant practical deficiencies of current\napproaches, some of which are masked by the biases in current datasets. We\npropose and evaluate several strategies for achieving Zero-Shot VQA, including\nmethods based on pretrained word embeddings, object classifiers with semantic\nembeddings, and test-time retrieval of example images. Our extensive\nexperiments are intended to serve as baselines for Zero-Shot VQA, and they also\nachieve state-of-the-art performance in the standard VQA evaluation setting.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 03:21:00 GMT"}, {"version": "v2", "created": "Sun, 20 Nov 2016 21:51:24 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Teney", "Damien", ""], ["Hengel", "Anton van den", ""]]}, {"id": "1611.05640", "submitter": "J\\\"org P\\\"uhrer", "authors": "Stefan Ellmauthaler, J\\\"org P\\\"uhrer", "title": "Stream Packing for Asynchronous Multi-Context Systems using ASP", "comments": "Workshop on Trends and Applications of Answer Set Programming (TAASP\n  2016)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  When a processing unit relies on data from external streams, we may face the\nproblem that the stream data needs to be rearranged in a way that allows the\nunit to perform its task(s). On arrival of new data, we must decide whether\nthere is sufficient information available to start processing or whether to\nwait for more data. Furthermore, we need to ensure that the data meets the\ninput specification of the processing step. In the case of multiple input\nstreams it is also necessary to coordinate which data from which incoming\nstream should form the input of the next process instantiation. In this work,\nwe propose a declarative approach as an interface between multiple streams and\na processing unit. The idea is to specify via answer-set programming how to\narrange incoming data in packages that are suitable as input for subsequent\nprocessing. Our approach is intended for use in asynchronous multi-context\nsystems (aMCSs), a recently proposed framework for loose coupling of knowledge\nrepresentation formalisms that allows for online reasoning in a dynamic\nenvironment. Contexts in aMCSs process data streams from external sources and\nother contexts.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 11:39:58 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Ellmauthaler", "Stefan", ""], ["P\u00fchrer", "J\u00f6rg", ""]]}, {"id": "1611.05664", "submitter": "Bastien Moysset", "authors": "Bastien Moysset, Christoper Kermorvant and Christian Wolf", "title": "Learning to detect and localize many objects from few examples", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The current trend in object detection and localization is to learn\npredictions with high capacity deep neural networks trained on a very large\namount of annotated data and using a high amount of processing power. In this\nwork, we propose a new neural model which directly predicts bounding box\ncoordinates. The particularity of our contribution lies in the local\ncomputations of predictions with a new form of local parameter sharing which\nkeeps the overall amount of trainable parameters low. Key components of the\nmodel are spatial 2D-LSTM recurrent layers which convey contextual information\nbetween the regions of the image. We show that this model is more powerful than\nthe state of the art in applications where training data is not as abundant as\nin the classical configuration of natural images and Imagenet/Pascal VOC tasks.\nWe particularly target the detection of text in document images, but our method\nis not limited to this setting. The proposed model also facilitates the\ndetection of many objects in a single image and can deal with inputs of\nvariable sizes without resizing.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 12:51:18 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Moysset", "Bastien", ""], ["Kermorvant", "Christoper", ""], ["Wolf", "Christian", ""]]}, {"id": "1611.05675", "submitter": "Xi Ma", "authors": "Xi Ma, Zhiyong Wu, Jia Jia, Mingxing Xu, Helen Meng, Lianhong Cai", "title": "Study on Feature Subspace of Archetypal Emotions for Speech Emotion\n  Recognition", "comments": "5 pages, 4 figures, ICASSP-2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Feature subspace selection is an important part in speech emotion\nrecognition. Most of the studies are devoted to finding a feature subspace for\nrepresenting all emotions. However, some studies have indicated that the\nfeatures associated with different emotions are not exactly the same. Hence,\ntraditional methods may fail to distinguish some of the emotions with just one\nglobal feature subspace. In this work, we propose a new divide and conquer idea\nto solve the problem. First, the feature subspaces are constructed for all the\ncombinations of every two different emotions (emotion-pair). Bi-classifiers are\nthen trained on these feature subspaces respectively. The final emotion\nrecognition result is derived by the voting and competition method.\nExperimental results demonstrate that the proposed method can get better\nresults than the traditional multi-classification method.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 13:32:59 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Ma", "Xi", ""], ["Wu", "Zhiyong", ""], ["Jia", "Jia", ""], ["Xu", "Mingxing", ""], ["Meng", "Helen", ""], ["Cai", "Lianhong", ""]]}, {"id": "1611.05735", "submitter": "Yaniv Altshuler", "authors": "Yaniv Altshuler, Alex Pentland, Shlomo Bekhor, Yoram Shiftan, Alfred\n  Bruckstein", "title": "Optimal Dynamic Coverage Infrastructure for Large-Scale Fleets of\n  Reconnaissance UAVs", "comments": "35 pages, 19 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Current state of the art in the field of UAV activation relies solely on\nhuman operators for the design and adaptation of the drones' flying routes.\nFurthermore, this is being done today on an individual level (one vehicle per\noperators), with some exceptions of a handful of new systems, that are\ncomprised of a small number of self-organizing swarms, manually guided by a\nhuman operator.\n  Drones-based monitoring is of great importance in variety of civilian\ndomains, such as road safety, homeland security, and even environmental\ncontrol. In its military aspect, efficiently detecting evading targets by a\nfleet of unmanned drones has an ever increasing impact on the ability of modern\narmies to engage in warfare. The latter is true both traditional symmetric\nconflicts among armies as well as asymmetric ones. Be it a speeding driver, a\npolluting trailer or a covert convoy, the basic challenge remains the same --\nhow can its detection probability be maximized using as little number of drones\nas possible.\n  In this work we propose a novel approach for the optimization of large scale\nswarms of reconnaissance drones -- capable of producing on-demand optimal\ncoverage strategies for any given search scenario. Given an estimation cost of\nthe threat's potential damages, as well as types of monitoring drones available\nand their comparative performance, our proposed method generates an\nanalytically provable strategy, stating the optimal number and types of drones\nto be deployed, in order to cost-efficiently monitor a pre-defined region for\ntargets maneuvering using a given roads networks.\n  We demonstrate our model using a unique dataset of the Israeli transportation\nnetwork, on which different deployment schemes for drones deployment are\nevaluated.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 15:28:14 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Altshuler", "Yaniv", ""], ["Pentland", "Alex", ""], ["Bekhor", "Shlomo", ""], ["Shiftan", "Yoram", ""], ["Bruckstein", "Alfred", ""]]}, {"id": "1611.05740", "submitter": "Wacha Bounliphone", "authors": "Wacha Bounliphone, Eugene Belilovsky, Arthur Tenenhaus, Ioannis\n  Antonoglou, Arthur Gretton, Matthew B. Blashcko", "title": "Fast Non-Parametric Tests of Relative Dependency and Similarity", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce two novel non-parametric statistical hypothesis tests. The first\ntest, called the relative test of dependency, enables us to determine whether\none source variable is significantly more dependent on a first target variable\nor a second. Dependence is measured via the Hilbert-Schmidt Independence\nCriterion (HSIC). The second test, called the relative test of similarity, is\nuse to determine which of the two samples from arbitrary distributions is\nsignificantly closer to a reference sample of interest and the relative measure\nof similarity is based on the Maximum Mean Discrepancy (MMD). To construct\nthese tests, we have used as our test statistics the difference of HSIC\nstatistics and of MMD statistics, respectively. The resulting tests are\nconsistent and unbiased, and have favorable convergence properties. The\neffectiveness of the relative dependency test is demonstrated on several\nreal-world problems: we identify languages groups from a multilingual parallel\ncorpus, and we show that tumor location is more dependent on gene expression\nthan chromosome imbalance. We also demonstrate the performance of the relative\ntest of similarity over a broad selection of model comparisons problems in deep\ngenerative models.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 15:36:31 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Bounliphone", "Wacha", ""], ["Belilovsky", "Eugene", ""], ["Tenenhaus", "Arthur", ""], ["Antonoglou", "Ioannis", ""], ["Gretton", "Arthur", ""], ["Blashcko", "Matthew B.", ""]]}, {"id": "1611.05763", "submitter": "Jane Wang", "authors": "Jane X Wang, Zeb Kurth-Nelson, Dhruva Tirumala, Hubert Soyer, Joel Z\n  Leibo, Remi Munos, Charles Blundell, Dharshan Kumaran, Matt Botvinick", "title": "Learning to reinforcement learn", "comments": "17 pages, 7 figures, 1 table", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In recent years deep reinforcement learning (RL) systems have attained\nsuperhuman performance in a number of challenging task domains. However, a\nmajor limitation of such applications is their demand for massive amounts of\ntraining data. A critical present objective is thus to develop deep RL methods\nthat can adapt rapidly to new tasks. In the present work we introduce a novel\napproach to this challenge, which we refer to as deep meta-reinforcement\nlearning. Previous work has shown that recurrent networks can support\nmeta-learning in a fully supervised context. We extend this approach to the RL\nsetting. What emerges is a system that is trained using one RL algorithm, but\nwhose recurrent dynamics implement a second, quite separate RL procedure. This\nsecond, learned RL algorithm can differ from the original one in arbitrary\nways. Importantly, because it is learned, it is configured to exploit structure\nin the training domain. We unpack these points in a series of seven\nproof-of-concept experiments, each of which examines a key aspect of deep\nmeta-RL. We consider prospects for extending and scaling up the approach, and\nalso point out some potentially important implications for neuroscience.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 16:29:11 GMT"}, {"version": "v2", "created": "Thu, 24 Nov 2016 15:35:02 GMT"}, {"version": "v3", "created": "Mon, 23 Jan 2017 12:38:24 GMT"}], "update_date": "2017-01-24", "authors_parsed": [["Wang", "Jane X", ""], ["Kurth-Nelson", "Zeb", ""], ["Tirumala", "Dhruva", ""], ["Soyer", "Hubert", ""], ["Leibo", "Joel Z", ""], ["Munos", "Remi", ""], ["Blundell", "Charles", ""], ["Kumaran", "Dharshan", ""], ["Botvinick", "Matt", ""]]}, {"id": "1611.05817", "submitter": "Marco Tulio Ribeiro", "authors": "Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin", "title": "Nothing Else Matters: Model-Agnostic Explanations By Identifying\n  Prediction Invariance", "comments": "Presented at NIPS 2016 Workshop on Interpretable Machine Learning in\n  Complex Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  At the core of interpretable machine learning is the question of whether\nhumans are able to make accurate predictions about a model's behavior. Assumed\nin this question are three properties of the interpretable output: coverage,\nprecision, and effort. Coverage refers to how often humans think they can\npredict the model's behavior, precision to how accurate humans are in those\npredictions, and effort is either the up-front effort required in interpreting\nthe model, or the effort required to make predictions about a model's behavior.\n  In this work, we propose anchor-LIME (aLIME), a model-agnostic technique that\nproduces high-precision rule-based explanations for which the coverage\nboundaries are very clear. We compare aLIME to linear LIME with simulated\nexperiments, and demonstrate the flexibility of aLIME with qualitative examples\nfrom a variety of domains and tasks.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 19:07:00 GMT"}], "update_date": "2016-11-18", "authors_parsed": [["Ribeiro", "Marco Tulio", ""], ["Singh", "Sameer", ""], ["Guestrin", "Carlos", ""]]}, {"id": "1611.05827", "submitter": "Hao Shen", "authors": "Hao Shen", "title": "Towards a Mathematical Understanding of the Difficulty in Learning with\n  Feedforward Neural Networks", "comments": "22 pages, 1 figure, submitted for publication", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training deep neural networks for solving machine learning problems is one\ngreat challenge in the field, mainly due to its associated optimisation problem\nbeing highly non-convex. Recent developments have suggested that many training\nalgorithms do not suffer from undesired local minima under certain scenario,\nand consequently led to great efforts in pursuing mathematical explanations for\nsuch observations. This work provides an alternative mathematical understanding\nof the challenge from a smooth optimisation perspective. By assuming exact\nlearning of finite samples, sufficient conditions are identified via a critical\npoint analysis to ensure any local minimum to be globally minimal as well.\nFurthermore, a state of the art algorithm, known as the Generalised\nGauss-Newton (GGN) algorithm, is rigorously revisited as an approximate\nNewton's algorithm, which shares the property of being locally quadratically\nconvergent to a global minimum under the condition of exact learning.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 19:29:27 GMT"}, {"version": "v2", "created": "Sun, 2 Apr 2017 21:49:39 GMT"}, {"version": "v3", "created": "Tue, 21 Nov 2017 22:11:13 GMT"}], "update_date": "2017-11-23", "authors_parsed": [["Shen", "Hao", ""]]}, {"id": "1611.05896", "submitter": "Somak Aditya", "authors": "Somak Aditya, Yezhou Yang, Chitta Baral, Yiannis Aloimonos", "title": "Answering Image Riddles using Vision and Reasoning through Probabilistic\n  Soft Logic", "comments": "14 pages, 10 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we explore a genre of puzzles (\"image riddles\") which involves\na set of images and a question. Answering these puzzles require both\ncapabilities involving visual detection (including object, activity\nrecognition) and, knowledge-based or commonsense reasoning. We compile a\ndataset of over 3k riddles where each riddle consists of 4 images and a\ngroundtruth answer. The annotations are validated using crowd-sourced\nevaluation. We also define an automatic evaluation metric to track future\nprogress. Our task bears similarity with the commonly known IQ tasks such as\nanalogy solving, sequence filling that are often used to test intelligence.\n  We develop a Probabilistic Reasoning-based approach that utilizes\nprobabilistic commonsense knowledge to answer these riddles with a reasonable\naccuracy. We demonstrate the results of our approach using both automatic and\nhuman evaluations. Our approach achieves some promising results for these\nriddles and provides a strong baseline for future attempts. We make the entire\ndataset and related materials publicly available to the community in\nImageRiddle Website (http://bit.ly/22f9Ala).\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 21:10:33 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Aditya", "Somak", ""], ["Yang", "Yezhou", ""], ["Baral", "Chitta", ""], ["Aloimonos", "Yiannis", ""]]}, {"id": "1611.05950", "submitter": "Christopher Meek", "authors": "Christopher Meek, Patrice Simard, Xiaojin Zhu", "title": "Analysis of a Design Pattern for Teaching with Features and Labels", "comments": "Also available at\n  https://www.microsoft.com/en-us/research/publication/a-design-pattern-for-teaching-with-features-and-labels/", "journal-ref": null, "doi": null, "report-no": "MSR-TR-2016-1104", "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study the task of teaching a machine to classify objects using features\nand labels. We introduce the Error-Driven-Featuring design pattern for teaching\nusing features and labels in which a teacher prefers to introduce features only\nif they are needed. We analyze the potential risks and benefits of this\nteaching pattern through the use of teaching protocols, illustrative examples,\nand by providing bounds on the effort required for an optimal machine teacher\nusing a linear learning algorithm, the most commonly used type of learners in\ninteractive machine learning systems. Our analysis provides a deeper\nunderstanding of potential trade-offs of using different learning algorithms\nand between the effort required for featuring (creating new features) and\nlabeling (providing labels for objects).\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 02:04:57 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Meek", "Christopher", ""], ["Simard", "Patrice", ""], ["Zhu", "Xiaojin", ""]]}, {"id": "1611.05973", "submitter": "Marcos Martinez-Romero PhD", "authors": "Marcos Martinez-Romero, Clement Jonquet, Martin J. O'Connor, John\n  Graybeal, Alejandro Pazos, Mark A. Musen", "title": "NCBO Ontology Recommender 2.0: An Enhanced Approach for Biomedical\n  Ontology Recommendation", "comments": "29 pages, 8 figures, 11 tables", "journal-ref": "Journal of Biomedical Semantics 8 (2017) 1-22", "doi": "10.1186/s13326-017-0128-y", "report-no": null, "categories": "cs.AI cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Biomedical researchers use ontologies to annotate their data with ontology\nterms, enabling better data integration and interoperability. However, the\nnumber, variety and complexity of current biomedical ontologies make it\ncumbersome for researchers to determine which ones to reuse for their specific\nneeds. To overcome this problem, in 2010 the National Center for Biomedical\nOntology (NCBO) released the Ontology Recommender, which is a service that\nreceives a biomedical text corpus or a list of keywords and suggests ontologies\nappropriate for referencing the indicated terms. We developed a new version of\nthe NCBO Ontology Recommender. Called Ontology Recommender 2.0, it uses a new\nrecommendation approach that evaluates the relevance of an ontology to\nbiomedical text data according to four criteria: (1) the extent to which the\nontology covers the input data; (2) the acceptance of the ontology in the\nbiomedical community; (3) the level of detail of the ontology classes that\ncover the input data; and (4) the specialization of the ontology to the domain\nof the input data. Our evaluation shows that the enhanced recommender provides\nhigher quality suggestions than the original approach, providing better\ncoverage of the input data, more detailed information about their concepts,\nincreased specialization for the domain of the input data, and greater\nacceptance and use in the community. In addition, it provides users with more\nexplanatory information, along with suggestions of not only individual\nontologies but also groups of ontologies. It also can be customized to fit the\nneeds of different scenarios. Ontology Recommender 2.0 combines the strengths\nof its predecessor with a range of adjustments and new features that improve\nits reliability and usefulness. Ontology Recommender 2.0 recommends over 500\nbiomedical ontologies from the NCBO BioPortal platform, where it is openly\navailable.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 04:58:54 GMT"}, {"version": "v2", "created": "Thu, 25 May 2017 21:32:40 GMT"}], "update_date": "2017-06-09", "authors_parsed": [["Martinez-Romero", "Marcos", ""], ["Jonquet", "Clement", ""], ["O'Connor", "Martin J.", ""], ["Graybeal", "John", ""], ["Pazos", "Alejandro", ""], ["Musen", "Mark A.", ""]]}, {"id": "1611.05990", "submitter": "Cezary Kaliszyk", "authors": "Michael F\\\"arber, Cezary Kaliszyk, Josef Urban", "title": "Monte Carlo Tableau Proof Search", "comments": null, "journal-ref": "Proceedings of the 26th International Conference on Automated\n  Deduction, CADE 2017", "doi": "10.1007/978-3-319-63046-5_34", "report-no": null, "categories": "cs.LO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study Monte Carlo Tree Search to guide proof search in tableau calculi.\nThis includes proposing a number of proof-state evaluation heuristics, some of\nwhich are learnt from previous proofs. We present an implementation based on\nthe leanCoP prover. The system is trained and evaluated on a large suite of\nrelated problems coming from the Mizar proof assistant, showing that it is\ncapable to find new and different proofs.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 06:30:09 GMT"}, {"version": "v2", "created": "Sat, 15 Jun 2019 00:34:50 GMT"}], "update_date": "2019-06-18", "authors_parsed": [["F\u00e4rber", "Michael", ""], ["Kaliszyk", "Cezary", ""], ["Urban", "Josef", ""]]}, {"id": "1611.06086", "submitter": "Timothy Ganesan PhD", "authors": "T. Ganesan, I. Elamvazuthi and P.Vasant", "title": "Swarm Intelligence for Multiobjective Optimization of Extraction Process", "comments": null, "journal-ref": "Handbook of Research on Modern Optimization Algorithms and\n  Applications in Engineering and Economics, (2016), IGI Global, pp 516 - 544", "doi": null, "report-no": null, "categories": "cs.NE cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multi objective (MO) optimization is an emerging field which is increasingly\nbeing implemented in many industries globally. In this work, the MO\noptimization of the extraction process of bioactive compounds from the Gardenia\nJasminoides Ellis fruit was solved. Three swarm-based algorithms have been\napplied in conjunction with normal-boundary intersection (NBI) method to solve\nthis MO problem. The gravitational search algorithm (GSA) and the particle\nswarm optimization (PSO) technique were implemented in this work. In addition,\na novel Hopfield-enhanced particle swarm optimization was developed and applied\nto the extraction problem. By measuring the levels of dominance, the optimality\nof the approximate Pareto frontiers produced by all the algorithms were gauged\nand compared. Besides, by measuring the levels of convergence of the frontier,\nsome understanding regarding the structure of the objective space in terms of\nits relation to the level of frontier dominance is uncovered. Detail\ncomparative studies were conducted on all the algorithms employed and developed\nin this work.\n", "versions": [{"version": "v1", "created": "Fri, 30 Sep 2016 04:18:19 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Ganesan", "T.", ""], ["Elamvazuthi", "I.", ""], ["Vasant", "P.", ""]]}, {"id": "1611.06108", "submitter": "Daniil Galaktionov", "authors": "Daniil Galaktionov, Miguel R. Luaces, \\'Angeles S. Places", "title": "Navigational Rule Derivation: An algorithm to determine the effect of\n  traffic signs on road networks", "comments": "This research has received funding from the European Union's Horizon\n  2020 research and innovation programme under the Marie Sk{\\l}odowska-Curie\n  Actions H2020-MSCA-RISE-2015 BIRDS GA No. 690941. in PACIS 2016 Online\n  Proceedings", "journal-ref": "Proceeding of the 20th Pacific Asia Conference on Information\n  Systems (PACIS 2016). Association for Information Systems. AIS Electronic\n  Library (AISeL). Paper 94. ISBN: 9789860491029", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we present an algorithm to build a road network map enriched\nwith traffic rules such as one-way streets and forbidden turns, based on the\ninterpretation of already detected and classified traffic signs. Such algorithm\nhelps to automatize the elaboration of maps for commercial navigation systems.\nOur solution is based on simulating navigation along the road network,\ndetermining at each point of interest the visibility of the signs and their\neffect on the roads. We test our approach in a small urban network and discuss\nvarious ways to generalize it to support more complex environments.\n", "versions": [{"version": "v1", "created": "Thu, 17 Nov 2016 18:39:44 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Galaktionov", "Daniil", ""], ["Luaces", "Miguel R.", ""], ["Places", "\u00c1ngeles S.", ""]]}, {"id": "1611.06132", "submitter": "Pavel Izmailov", "authors": "Pavel Izmailov and Dmitry Kropotov", "title": "Faster variational inducing input Gaussian process classification", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Gaussian processes (GP) provide a prior over functions and allow finding\ncomplex regularities in data. Gaussian processes are successfully used for\nclassification/regression problems and dimensionality reduction. In this work\nwe consider the classification problem only. The complexity of standard methods\nfor GP-classification scales cubically with the size of the training dataset.\nThis complexity makes them inapplicable to big data problems. Therefore, a\nvariety of methods were introduced to overcome this limitation. In the paper we\nfocus on methods based on so called inducing inputs. This approach is based on\nvariational inference and proposes a particular lower bound for marginal\nlikelihood (evidence). This bound is then maximized w.r.t. parameters of kernel\nfunction of the Gaussian process, thus fitting the model to data. The\ncomputational complexity of this method is $O(nm^2)$, where $m$ is the number\nof inducing inputs used by the model and is assumed to be substantially smaller\nthan the size of the dataset $n$. Recently, a new evidence lower bound for\nGP-classification problem was introduced. It allows using stochastic\noptimization, which makes it suitable for big data problems. However, the new\nlower bound depends on $O(m^2)$ variational parameter, which makes optimization\nchallenging in case of big m. In this work we develop a new approach for\ntraining inducing input GP models for classification problems. Here we use\nquadratic approximation of several terms in the aforementioned evidence lower\nbound, obtaining analytical expressions for optimal values of most of the\nparameters in the optimization, thus sufficiently reducing the dimension of\noptimization space. In our experiments we achieve as well or better results,\ncompared to the existing method. Moreover, our method doesn't require the user\nto manually set the learning rate, making it more practical, than the existing\nmethod.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 15:53:50 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Izmailov", "Pavel", ""], ["Kropotov", "Dmitry", ""]]}, {"id": "1611.06134", "submitter": "Andrea Celli", "authors": "Nicola Basilico, Andrea Celli, Giuseppe De Nittis and Nicola Gatti", "title": "Team-maxmin equilibrium: efficiency bounds and algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Team-maxmin equilibrium prescribes the optimal strategies for a team of\nrational players sharing the same goal and without the capability of\ncorrelating their strategies in strategic games against an adversary. This\nsolution concept can capture situations in which an agent controls multiple\nresources-corresponding to the team members-that cannot communicate. It is\nknown that such equilibrium always exists and it is unique (unless degeneracy)\nand these properties make it a credible solution concept to be used in\nreal-world applications, especially in security scenarios. Nevertheless, to the\nbest of our knowledge, the Team-maxmin equilibrium is almost completely\nunexplored in the literature. In this paper, we investigate bounds of\n(in)efficiency of the Team-maxmin equilibrium w.r.t. the Nash equilibria and\nw.r.t. the Maxmin equilibrium when the team members can play correlated\nstrategies. Furthermore, we study a number of algorithms to find and/or\napproximate an equilibrium, discussing their theoretical guarantees and\nevaluating their performance by using a standard testbed of game instances.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 15:56:48 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Basilico", "Nicola", ""], ["Celli", "Andrea", ""], ["De Nittis", "Giuseppe", ""], ["Gatti", "Nicola", ""]]}, {"id": "1611.06174", "submitter": "Ondrej Kuzelka", "authors": "Ondrej Kuzelka, Jesse Davis, Steven Schockaert", "title": "Stratified Knowledge Bases as Interpretable Probabilistic Models\n  (Extended Abstract)", "comments": "Presented at NIPS 2016 Workshop on Interpretable Machine Learning in\n  Complex Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper, we advocate the use of stratified logical theories for\nrepresenting probabilistic models. We argue that such encodings can be more\ninterpretable than those obtained in existing frameworks such as Markov logic\nnetworks. Among others, this allows for the use of domain experts to improve\nlearned models by directly removing, adding, or modifying logical formulas.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 17:51:56 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Kuzelka", "Ondrej", ""], ["Davis", "Jesse", ""], ["Schockaert", "Steven", ""]]}, {"id": "1611.06175", "submitter": "Adrien Bibal", "authors": "Adrien Bibal and Benoit Fr\\'enay", "title": "Learning Interpretability for Visualizations using Adapted Cox Models\n  through a User Experiment", "comments": "Presented at NIPS 2016 Workshop on Interpretable Machine Learning in\n  Complex Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.HC cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In order to be useful, visualizations need to be interpretable. This paper\nuses a user-based approach to combine and assess quality measures in order to\nbetter model user preferences. Results show that cluster separability measures\nare outperformed by a neighborhood conservation measure, even though the former\nare usually considered as intuitively representative of user motives. Moreover,\ncombining measures, as opposed to using a single measure, further improves\nprediction performances.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 17:52:23 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Bibal", "Adrien", ""], ["Fr\u00e9nay", "Benoit", ""]]}, {"id": "1611.06188", "submitter": "Yacine Jernite", "authors": "Yacine Jernite, Edouard Grave, Armand Joulin, Tomas Mikolov", "title": "Variable Computation in Recurrent Neural Networks", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recurrent neural networks (RNNs) have been used extensively and with\nincreasing success to model various types of sequential data. Much of this\nprogress has been achieved through devising recurrent units and architectures\nwith the flexibility to capture complex statistics in the data, such as long\nrange dependency or localized attention phenomena. However, while many\nsequential data (such as video, speech or language) can have highly variable\ninformation flow, most recurrent models still consume input features at a\nconstant rate and perform a constant number of computations per time step,\nwhich can be detrimental to both speed and model capacity. In this paper, we\nexplore a modification to existing recurrent units which allows them to learn\nto vary the amount of computation they perform at each step, without prior\nknowledge of the sequence's time structure. We show experimentally that not\nonly do our models require fewer operations, they also lead to better\nperformance overall on evaluation tasks.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 18:13:46 GMT"}, {"version": "v2", "created": "Thu, 2 Mar 2017 19:47:59 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Jernite", "Yacine", ""], ["Grave", "Edouard", ""], ["Joulin", "Armand", ""], ["Mikolov", "Tomas", ""]]}, {"id": "1611.06189", "submitter": "Palash Dey", "authors": "Palash Dey", "title": "Query Complexity of Tournament Solutions", "comments": "To appear in AAAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.DM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A directed graph where there is exactly one edge between every pair of\nvertices is called a {\\em tournament}. Finding the \"best\" set of vertices of a\ntournament is a well studied problem in social choice theory. A {\\em tournament\nsolution} takes a tournament as input and outputs a subset of vertices of the\ninput tournament. However, in many applications, for example, choosing the best\nset of drugs from a given set of drugs, the edges of the tournament are given\nonly implicitly and knowing the orientation of an edge is costly. In such\nscenarios, we would like to know the best set of vertices (according to some\ntournament solution) by \"querying\" as few edges as possible. We, in this paper,\nprecisely study this problem for commonly used tournament solutions: given an\noracle access to the edges of a tournament T, find $f(T)$ by querying as few\nedges as possible, for a tournament solution f. We first show that the set of\nCondorcet non-losers in a tournament can be found by querying $2n-\\lfloor \\log\nn \\rfloor -2$ edges only and this is tight in the sense that every algorithm\nfor finding the set of Condorcet non-losers needs to query at least $2n-\\lfloor\n\\log n \\rfloor -2$ edges in the worst case, where $n$ is the number of vertices\nin the input tournament. We then move on to study other popular tournament\nsolutions and show that any algorithm for finding the Copeland set, the Slater\nset, the Markov set, the bipartisan set, the uncovered set, the Banks set, and\nthe top cycle must query $\\Omega(n^2)$ edges in the worst case. On the positive\nside, we are able to circumvent our strong query complexity lower bound results\nby proving that, if the size of the top cycle of the input tournament is at\nmost $k$, then we can find all the tournament solutions mentioned above by\nquerying $O(nk + \\frac{n\\log n}{\\log(1-\\frac{1}{k})})$ edges only.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 18:19:32 GMT"}, {"version": "v2", "created": "Mon, 21 Nov 2016 13:26:00 GMT"}, {"version": "v3", "created": "Wed, 25 Jan 2017 05:45:42 GMT"}], "update_date": "2017-01-26", "authors_parsed": [["Dey", "Palash", ""]]}, {"id": "1611.06194", "submitter": "Rahaf Aljundi", "authors": "Rahaf Aljundi, Punarjay Chakravarty and Tinne Tuytelaars", "title": "Expert Gate: Lifelong Learning with a Network of Experts", "comments": "CVPR 2017 paper", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a model of lifelong learning, based on a Network\nof Experts. New tasks / experts are learned and added to the model\nsequentially, building on what was learned before. To ensure scalability of\nthis process,data from previous tasks cannot be stored and hence is not\navailable when learning a new task. A critical issue in such context, not\naddressed in the literature so far, relates to the decision which expert to\ndeploy at test time. We introduce a set of gating autoencoders that learn a\nrepresentation for the task at hand, and, at test time, automatically forward\nthe test sample to the relevant expert. This also brings memory efficiency as\nonly one expert network has to be loaded into memory at any given time.\nFurther, the autoencoders inherently capture the relatedness of one task to\nanother, based on which the most relevant prior model to be used for training a\nnew expert, with finetuning or learning without-forgetting, can be selected. We\nevaluate our method on image classification and video prediction problems.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 18:50:15 GMT"}, {"version": "v2", "created": "Wed, 19 Apr 2017 09:25:55 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Aljundi", "Rahaf", ""], ["Chakravarty", "Punarjay", ""], ["Tuytelaars", "Tinne", ""]]}, {"id": "1611.06216", "submitter": "Iulian Vlad Serban", "authors": "Iulian Vlad Serban, Ryan Lowe, Laurent Charlin, Joelle Pineau", "title": "Generative Deep Neural Networks for Dialogue: A Short Review", "comments": "6 pages, 1 figure, 3 tables; NIPS 2016 workshop on Learning Methods\n  for Dialogue", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Researchers have recently started investigating deep neural networks for\ndialogue applications. In particular, generative sequence-to-sequence (Seq2Seq)\nmodels have shown promising results for unstructured tasks, such as word-level\ndialogue response generation. The hope is that such models will be able to\nleverage massive amounts of data to learn meaningful natural language\nrepresentations and response generation strategies, while requiring a minimum\namount of domain knowledge and hand-crafting. An important challenge is to\ndevelop models that can effectively incorporate dialogue context and generate\nmeaningful and diverse responses. In support of this goal, we review recently\nproposed models based on generative encoder-decoder neural network\narchitectures, and show that these models have better ability to incorporate\nlong-term dialogue history, to model uncertainty and ambiguity in dialogue, and\nto generate responses with high-level compositional structure.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 20:11:51 GMT"}], "update_date": "2016-11-21", "authors_parsed": [["Serban", "Iulian Vlad", ""], ["Lowe", "Ryan", ""], ["Charlin", "Laurent", ""], ["Pineau", "Joelle", ""]]}, {"id": "1611.06221", "submitter": "Stephan Bongers", "authors": "Stephan Bongers, Patrick Forr\\'e, Jonas Peters, Joris M. Mooij", "title": "Foundations of Structural Causal Models with Cycles and Latent Variables", "comments": "75 pages (including supplementary material)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Structural causal models (SCMs), also known as (nonparametric) structural\nequation models (SEMs), are widely used for causal modeling purposes. In\nparticular, acyclic SCMs, also known as recursive SEMs, form a well-studied\nsubclass of SCMs that generalize causal Bayesian networks to allow for latent\nconfounders. In this paper, we investigate SCMs in a more general setting,\nallowing for the presence of both latent confounders and cycles. We show that\nin the presence of cycles, many of the convenient properties of acyclic SCMs do\nnot hold in general: they do not always have a solution; they do not always\ninduce unique observational, interventional and counterfactual distributions; a\nmarginalization does not always exist, and if it exists the marginal model does\nnot always respect the latent projection; they do not always satisfy a Markov\nproperty; and their graphs are not always consistent with their causal\nsemantics. We prove that for SCMs in general each of these properties does hold\nunder certain solvability conditions. Our work generalizes results for SCMs\nwith cycles that were only known for certain special cases so far. We introduce\nthe class of simple SCMs that extends the class of acyclic SCMs to the cyclic\nsetting, while preserving many of the convenient properties of acyclic SCMs.\nWith this paper we aim to provide the foundations for a general theory of\nstatistical causal modeling with SCMs.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 20:54:03 GMT"}, {"version": "v2", "created": "Sun, 5 Aug 2018 02:15:32 GMT"}, {"version": "v3", "created": "Tue, 5 May 2020 16:19:44 GMT"}, {"version": "v4", "created": "Thu, 8 Oct 2020 13:27:13 GMT"}, {"version": "v5", "created": "Mon, 10 May 2021 13:58:29 GMT"}], "update_date": "2021-05-11", "authors_parsed": [["Bongers", "Stephan", ""], ["Forr\u00e9", "Patrick", ""], ["Peters", "Jonas", ""], ["Mooij", "Joris M.", ""]]}, {"id": "1611.06355", "submitter": "Guim Perarnau", "authors": "Guim Perarnau, Joost van de Weijer, Bogdan Raducanu, Jose M. \\'Alvarez", "title": "Invertible Conditional GANs for image editing", "comments": "Accepted paper at NIPS 2016 Workshop on Adversarial Training", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Generative Adversarial Networks (GANs) have recently demonstrated to\nsuccessfully approximate complex data distributions. A relevant extension of\nthis model is conditional GANs (cGANs), where the introduction of external\ninformation allows to determine specific representations of the generated\nimages. In this work, we evaluate encoders to inverse the mapping of a cGAN,\ni.e., mapping a real image into a latent space and a conditional\nrepresentation. This allows, for example, to reconstruct and modify real images\nof faces conditioning on arbitrary attributes. Additionally, we evaluate the\ndesign of cGANs. The combination of an encoder with a cGAN, which we call\nInvertible cGAN (IcGAN), enables to re-generate real images with deterministic\ncomplex modifications.\n", "versions": [{"version": "v1", "created": "Sat, 19 Nov 2016 12:35:01 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Perarnau", "Guim", ""], ["van de Weijer", "Joost", ""], ["Raducanu", "Bogdan", ""], ["\u00c1lvarez", "Jose M.", ""]]}, {"id": "1611.06439", "submitter": "Reza Ebrahimi Atani", "authors": "SamanehSorournejad, Zahra Zojaji, Reza Ebrahimi Atani, Amir Hassan\n  Monadjemi", "title": "A Survey of Credit Card Fraud Detection Techniques: Data and Technique\n  Oriented Perspective", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CR cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Credit card plays a very important rule in today's economy. It becomes an\nunavoidable part of household, business and global activities. Although using\ncredit cards provides enormous benefits when used carefully and\nresponsibly,significant credit and financial damages may be caused by\nfraudulent activities. Many techniques have been proposed to confront the\ngrowth in credit card fraud. However, all of these techniques have the same\ngoal of avoiding the credit card fraud; each one has its own drawbacks,\nadvantages and characteristics. In this paper, after investigating difficulties\nof credit card fraud detection, we seek to review the state of the art in\ncredit card fraud detection techniques, data sets and evaluation criteria.The\nadvantages and disadvantages of fraud detection methods are enumerated and\ncompared.Furthermore, a classification of mentioned techniques into two main\nfraud detection approaches, namely, misuses (supervised) and anomaly detection\n(unsupervised) is presented. Again, a classification of techniques is proposed\nbased on capability to process the numerical and categorical data sets.\nDifferent data sets used in literature are then described and grouped into real\nand synthesized data and the effective and common attributes are extracted for\nfurther usage.Moreover, evaluation employed criterions in literature are\ncollected and discussed.Consequently, open issues for credit card fraud\ndetection are explained as guidelines for new researchers.\n", "versions": [{"version": "v1", "created": "Sat, 19 Nov 2016 22:46:13 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["SamanehSorournejad", "", ""], ["Zojaji", "Zahra", ""], ["Atani", "Reza Ebrahimi", ""], ["Monadjemi", "Amir Hassan", ""]]}, {"id": "1611.06468", "submitter": "Rui Liu", "authors": "Rui Liu, Xiaoli Zhang", "title": "Generating machine-executable plans from end-user's natural-language\n  instructions", "comments": "16 pages, 10 figures, article submitted to Robotics and\n  Computer-Integrated Manufacturing, 2016 Aug", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is critical for advanced manufacturing machines to autonomously execute a\ntask by following an end-user's natural language (NL) instructions. However, NL\ninstructions are usually ambiguous and abstract so that the machines may\nmisunderstand and incorrectly execute the task. To address this NL-based\nhuman-machine communication problem and enable the machines to appropriately\nexecute tasks by following the end-user's NL instructions, we developed a\nMachine-Executable-Plan-Generation (exePlan) method. The exePlan method\nconducts task-centered semantic analysis to extract task-related information\nfrom ambiguous NL instructions. In addition, the method specifies machine\nexecution parameters to generate a machine-executable plan by interpreting\nabstract NL instructions. To evaluate the exePlan method, an industrial robot\nBaxter was instructed by NL to perform three types of industrial tasks {'drill\na hole', 'clean a spot', 'install a screw'}. The experiment results proved that\nthe exePlan method was effective in generating machine-executable plans from\nthe end-user's NL instructions. Such a method has the promise to endow a\nmachine with the ability of NL-instructed task execution.\n", "versions": [{"version": "v1", "created": "Sun, 20 Nov 2016 04:06:47 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Liu", "Rui", ""], ["Zhang", "Xiaoli", ""]]}, {"id": "1611.06589", "submitter": "Rediet Abebe", "authors": "Rediet Abebe, Jon Kleinberg, David Parkes", "title": "Fair Division via Social Comparison", "comments": "18 pages, 3 figures, Proceedings of the 16th Conference on Autonomous\n  Agents and Multi-Agent Systems (AAMAS, 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI cs.GT math.CO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In the classical cake cutting problem, a resource must be divided among\nagents with different utilities so that each agent believes they have received\na fair share of the resource relative to the other agents. We introduce a\nvariant of the problem in which we model an underlying social network on the\nagents with a graph, and agents only evaluate their shares relative to their\nneighbors' in the network. This formulation captures many situations in which\nit is unrealistic to assume a global view, and also exposes interesting\nphenomena in the original problem.\n  Specifically, we say an allocation is locally envy-free if no agent envies a\nneighbor's allocation and locally proportional if each agent values her own\nallocation as much as the average value of her neighbor's allocations, with the\nformer implying the latter. While global envy-freeness implies local\nenvy-freeness, global proportionality does not imply local proportionality, or\nvice versa. A general result is that for any two distinct graphs on the same\nset of nodes and an allocation, there exists a set of valuation functions such\nthat the allocation is locally proportional on one but not the other.\n  We fully characterize the set of graphs for which an oblivious single-cutter\nprotocol-- a protocol that uses a single agent to cut the cake into pieces\n--admits a bounded protocol with $O(n^2)$ query complexity for locally\nenvy-free allocations in the Robertson-Webb model. We also consider the price\nof envy-freeness, which compares the total utility of an optimal allocation to\nthe best utility of an allocation that is envy-free. We show that a lower bound\nof $\\Omega(\\sqrt{n})$ on the price of envy-freeness for global allocations in\nfact holds for local envy-freeness in any connected undirected graph. Thus,\nsparse graphs surprisingly do not provide more flexibility with respect to the\nquality of envy-free allocations.\n", "versions": [{"version": "v1", "created": "Sun, 20 Nov 2016 20:42:07 GMT"}, {"version": "v2", "created": "Sun, 25 Feb 2018 19:16:36 GMT"}], "update_date": "2018-02-27", "authors_parsed": [["Abebe", "Rediet", ""], ["Kleinberg", "Jon", ""], ["Parkes", "David", ""]]}, {"id": "1611.06757", "submitter": "Stamatios Lefkimmiatis", "authors": "Stamatios Lefkimmiatis", "title": "Non-Local Color Image Denoising with Convolutional Neural Networks", "comments": "15 pages, accepted to CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a novel deep network architecture for grayscale and color image\ndenoising that is based on a non-local image model. Our motivation for the\noverall design of the proposed network stems from variational methods that\nexploit the inherent non-local self-similarity property of natural images. We\nbuild on this concept and introduce deep networks that perform non-local\nprocessing and at the same time they significantly benefit from discriminative\nlearning. Experiments on the Berkeley segmentation dataset, comparing several\nstate-of-the-art methods, show that the proposed non-local models achieve the\nbest reported denoising performance both for grayscale and color images for all\nthe tested noise levels. It is also worth noting that this increase in\nperformance comes at no extra cost on the capacity of the network compared to\nexisting alternative deep network architectures. In addition, we highlight a\ndirect link of the proposed non-local models to convolutional neural networks.\nThis connection is of significant importance since it allows our models to take\nfull advantage of the latest advances on GPU computing in deep learning and\nmakes them amenable to efficient implementations through their inherent\nparallelism.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 12:36:10 GMT"}, {"version": "v2", "created": "Mon, 10 Jul 2017 20:06:26 GMT"}], "update_date": "2017-07-12", "authors_parsed": [["Lefkimmiatis", "Stamatios", ""]]}, {"id": "1611.06791", "submitter": "Suraj Srinivas", "authors": "Suraj Srinivas, R. Venkatesh Babu", "title": "Generalized Dropout", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.CV cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Deep Neural Networks often require good regularizers to generalize well.\nDropout is one such regularizer that is widely used among Deep Learning\npractitioners. Recent work has shown that Dropout can also be viewed as\nperforming Approximate Bayesian Inference over the network parameters. In this\nwork, we generalize this notion and introduce a rich family of regularizers\nwhich we call Generalized Dropout. One set of methods in this family, called\nDropout++, is a version of Dropout with trainable parameters. Classical Dropout\nemerges as a special case of this method. Another member of this family selects\nthe width of neural network layers. Experiments show that these methods help in\nimproving generalization performance over Dropout.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 14:06:48 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Srinivas", "Suraj", ""], ["Babu", "R. Venkatesh", ""]]}, {"id": "1611.06824", "submitter": "Aur\\'elia L\\'eon", "authors": "Aur\\'elia L\\'eon, Ludovic Denoyer", "title": "Options Discovery with Budgeted Reinforcement Learning", "comments": "Under review as a conference paper at IJCAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of learning hierarchical policies for Reinforcement\nLearning able to discover options, an option corresponding to a sub-policy over\na set of primitive actions. Different models have been proposed during the last\ndecade that usually rely on a predefined set of options. We specifically\naddress the problem of automatically discovering options in decision processes.\nWe describe a new learning model called Budgeted Option Neural Network (BONN)\nable to discover options based on a budgeted learning objective. The BONN model\nis evaluated on different classical RL problems, demonstrating both\nquantitative and qualitative interesting results.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 15:05:55 GMT"}, {"version": "v2", "created": "Tue, 24 Jan 2017 17:06:24 GMT"}, {"version": "v3", "created": "Wed, 22 Feb 2017 13:12:33 GMT"}], "update_date": "2017-02-23", "authors_parsed": [["L\u00e9on", "Aur\u00e9lia", ""], ["Denoyer", "Ludovic", ""]]}, {"id": "1611.06882", "submitter": "Luca de Alfaro", "authors": "Rakshit Agrawal, Luca de Alfaro, Vassilis Polychronopoulos", "title": "Learning From Graph Neighborhoods Using LSTMs", "comments": null, "journal-ref": null, "doi": null, "report-no": "Technical Report UCSC-SOE-16-17, School of Engineering, University\n  of California, Santa Cruz", "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many prediction problems can be phrased as inferences over local\nneighborhoods of graphs. The graph represents the interaction between entities,\nand the neighborhood of each entity contains information that allows the\ninferences or predictions. We present an approach for applying machine learning\ndirectly to such graph neighborhoods, yielding predicitons for graph nodes on\nthe basis of the structure of their local neighborhood and the features of the\nnodes in it. Our approach allows predictions to be learned directly from\nexamples, bypassing the step of creating and tuning an inference model or\nsummarizing the neighborhoods via a fixed set of hand-crafted features. The\napproach is based on a multi-level architecture built from Long Short-Term\nMemory neural nets (LSTMs); the LSTMs learn how to summarize the neighborhood\nfrom data. We demonstrate the effectiveness of the proposed technique on a\nsynthetic example and on real-world data related to crowdsourced grading,\nBitcoin transactions, and Wikipedia edit reversions.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 16:25:34 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Agrawal", "Rakshit", ""], ["de Alfaro", "Luca", ""], ["Polychronopoulos", "Vassilis", ""]]}, {"id": "1611.06928", "submitter": "Christoph Dann", "authors": "Christoph Dann, Katja Hofmann, Sebastian Nowozin", "title": "Memory Lens: How Much Memory Does an Agent Use?", "comments": "Presented at NIPS 2016 Workshop on Interpretable Machine Learning in\n  Complex Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a new method to study the internal memory used by reinforcement\nlearning policies. We estimate the amount of relevant past information by\nestimating mutual information between behavior histories and the current action\nof an agent. We perform this estimation in the passive setting, that is, we do\nnot intervene but merely observe the natural behavior of the agent. Moreover,\nwe provide a theoretical justification for our approach by showing that it\nyields an implementation-independent lower bound on the minimal memory capacity\nof any agent that implement the observed policy. We demonstrate our approach by\nestimating the use of memory of DQN policies on concatenated Atari frames,\ndemonstrating sharply different use of memory across 49 games. The study of\nmemory as information that flows from the past to the current action opens\navenues to understand and improve successful reinforcement learning algorithms.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 18:22:27 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Dann", "Christoph", ""], ["Hofmann", "Katja", ""], ["Nowozin", "Sebastian", ""]]}, {"id": "1611.06951", "submitter": "Leopoldo Bertossi", "authors": "Zeinab Bahmani and Leopoldo Bertossi", "title": "Enforcing Relational Matching Dependencies with Datalog for Entity\n  Resolution", "comments": "New revisions applied. To appear in Proc. FLAIRS'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DB cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Entity resolution (ER) is about identifying and merging records in a database\nthat represent the same real-world entity. Matching dependencies (MDs) have\nbeen introduced and investigated as declarative rules that specify ER policies.\nAn ER process induced by MDs over a dirty instance leads to multiple clean\ninstances, in general. General \"answer sets programs\" have been proposed to\nspecify the MD-based cleaning task and its results. In this work, we extend MDs\nto \"relational MDs\", which capture more application semantics, and identify\nclasses of relational MDs for which the general ASP can be automatically\nrewritten into a stratified Datalog program, with the single clean instance as\nits standard model.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 19:02:19 GMT"}, {"version": "v2", "created": "Sat, 25 Feb 2017 23:23:27 GMT"}], "update_date": "2017-02-28", "authors_parsed": [["Bahmani", "Zeinab", ""], ["Bertossi", "Leopoldo", ""]]}, {"id": "1611.06953", "submitter": "Asli Celikyilmaz", "authors": "Tarik Arici and Asli Celikyilmaz", "title": "Associative Adversarial Networks", "comments": "NIPS 2016 Workshop on Adversarial Training", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We propose a higher-level associative memory for learning adversarial\nnetworks. Generative adversarial network (GAN) framework has a discriminator\nand a generator network. The generator (G) maps white noise (z) to data samples\nwhile the discriminator (D) maps data samples to a single scalar. To do so, G\nlearns how to map from high-level representation space to data space, and D\nlearns to do the opposite. We argue that higher-level representation spaces\nneed not necessarily follow a uniform probability distribution. In this work,\nwe use Restricted Boltzmann Machines (RBMs) as a higher-level associative\nmemory and learn the probability distribution for the high-level features\ngenerated by D. The associative memory samples its underlying probability\ndistribution and G learns how to map these samples to data space. The proposed\nassociative adversarial networks (AANs) are generative models in the\nhigher-levels of the learning, and use adversarial non-stochastic models D and\nG for learning the mapping between data and higher-level representation spaces.\nExperiments show the potential of the proposed networks.\n", "versions": [{"version": "v1", "created": "Fri, 18 Nov 2016 02:11:40 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Arici", "Tarik", ""], ["Celikyilmaz", "Asli", ""]]}, {"id": "1611.06997", "submitter": "Hongyuan Mei", "authors": "Hongyuan Mei and Mohit Bansal and Matthew R. Walter", "title": "Coherent Dialogue with Attention-based Language Models", "comments": "To appear at AAAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We model coherent conversation continuation via RNN-based dialogue models\nequipped with a dynamic attention mechanism. Our attention-RNN language model\ndynamically increases the scope of attention on the history as the conversation\ncontinues, as opposed to standard attention (or alignment) models with a fixed\ninput scope in a sequence-to-sequence model. This allows each generated word to\nbe associated with the most relevant words in its corresponding conversation\nhistory. We evaluate the model on two popular dialogue datasets, the\nopen-domain MovieTriples dataset and the closed-domain Ubuntu Troubleshoot\ndataset, and achieve significant improvements over the state-of-the-art and\nbaselines on several metrics, including complementary diversity-based metrics,\nhuman evaluation, and qualitative visualizations. We also show that a vanilla\nRNN with dynamic attention outperforms more complex memory models (e.g., LSTM\nand GRU) by allowing for flexible, long-distance memory. We promote further\ncoherence via topic modeling-based reranking.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 20:25:19 GMT"}], "update_date": "2016-11-22", "authors_parsed": [["Mei", "Hongyuan", ""], ["Bansal", "Mohit", ""], ["Walter", "Matthew R.", ""]]}, {"id": "1611.07054", "submitter": "Sebastian P\\\"olsterl", "authors": "Sebastian P\\\"olsterl, Nassir Navab, Amin Katouzian", "title": "An Efficient Training Algorithm for Kernel Survival Support Vector\n  Machines", "comments": "ECML PKDD MLLS 2016: 3rd Workshop on Machine Learning in Life\n  Sciences", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Survival analysis is a fundamental tool in medical research to identify\npredictors of adverse events and develop systems for clinical decision support.\nIn order to leverage large amounts of patient data, efficient optimisation\nroutines are paramount. We propose an efficient training algorithm for the\nkernel survival support vector machine (SSVM). We directly optimise the primal\nobjective function and employ truncated Newton optimisation and order statistic\ntrees to significantly lower computational costs compared to previous training\nalgorithms, which require $O(n^4)$ space and $O(p n^6)$ time for datasets with\n$n$ samples and $p$ features. Our results demonstrate that our proposed\noptimisation scheme allows analysing data of a much larger scale with no loss\nin prediction performance. Experiments on synthetic and 5 real-world datasets\nshow that our technique outperforms existing kernel SSVM formulations if the\namount of right censoring is high ($\\geq85\\%$), and performs comparably\notherwise.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 21:09:33 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["P\u00f6lsterl", "Sebastian", ""], ["Navab", "Nassir", ""], ["Katouzian", "Amin", ""]]}, {"id": "1611.07078", "submitter": "Felix Leibfried", "authors": "Felix Leibfried, Nate Kushman, Katja Hofmann", "title": "A Deep Learning Approach for Joint Video Frame and Reward Prediction in\n  Atari Games", "comments": "Presented at the ICML 2017 Workshop on Principled Approaches to Deep\n  Learning, Sydney, Australia, 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Reinforcement learning is concerned with identifying reward-maximizing\nbehaviour policies in environments that are initially unknown. State-of-the-art\nreinforcement learning approaches, such as deep Q-networks, are model-free and\nlearn to act effectively across a wide range of environments such as Atari\ngames, but require huge amounts of data. Model-based techniques are more\ndata-efficient, but need to acquire explicit knowledge about the environment.\n  In this paper, we take a step towards using model-based techniques in\nenvironments with a high-dimensional visual state space by demonstrating that\nit is possible to learn system dynamics and the reward structure jointly. Our\ncontribution is to extend a recently developed deep neural network for video\nframe prediction in Atari games to enable reward prediction as well. To this\nend, we phrase a joint optimization problem for minimizing both video frame and\nreward reconstruction loss, and adapt network parameters accordingly. Empirical\nevaluations on five Atari games demonstrate accurate cumulative reward\nprediction of up to 200 frames. We consider these results as opening up\nimportant directions for model-based reinforcement learning in complex,\ninitially unknown environments.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 22:06:23 GMT"}, {"version": "v2", "created": "Thu, 17 Aug 2017 09:00:01 GMT"}], "update_date": "2017-08-18", "authors_parsed": [["Leibfried", "Felix", ""], ["Kushman", "Nate", ""], ["Hofmann", "Katja", ""]]}, {"id": "1611.07100", "submitter": "Qin Lin", "authors": "Christian Albert Hammerschmidt, Sicco Verwer, Qin Lin, Radu State", "title": "Interpreting Finite Automata for Sequential Data", "comments": "Presented at NIPS 2016 Workshop on Interpretable Machine Learning in\n  Complex Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Automaton models are often seen as interpretable models. Interpretability\nitself is not well defined: it remains unclear what interpretability means\nwithout first explicitly specifying objectives or desired attributes. In this\npaper, we identify the key properties used to interpret automata and propose a\nmodification of a state-merging approach to learn variants of finite state\nautomata. We apply the approach to problems beyond typical grammar inference\ntasks. Additionally, we cover several use-cases for prediction, classification,\nand clustering on sequential data in both supervised and unsupervised scenarios\nto show how the identified key properties are applicable in a wide range of\ncontexts.\n", "versions": [{"version": "v1", "created": "Mon, 21 Nov 2016 23:21:13 GMT"}, {"version": "v2", "created": "Thu, 24 Nov 2016 20:52:50 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Hammerschmidt", "Christian Albert", ""], ["Verwer", "Sicco", ""], ["Lin", "Qin", ""], ["State", "Radu", ""]]}, {"id": "1611.07233", "submitter": "Lukas Cavigelli", "authors": "Lukas Cavigelli, Pascal Hager, Luca Benini", "title": "CAS-CNN: A Deep Convolutional Neural Network for Image Compression\n  Artifact Suppression", "comments": "8 pages", "journal-ref": null, "doi": "10.1109/IJCNN.2017.7965927", "report-no": null, "categories": "cs.CV cs.AI cs.GR cs.IR cs.MM", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Lossy image compression algorithms are pervasively used to reduce the size of\nimages transmitted over the web and recorded on data storage media. However, we\npay for their high compression rate with visual artifacts degrading the user\nexperience. Deep convolutional neural networks have become a widespread tool to\naddress high-level computer vision tasks very successfully. Recently, they have\nfound their way into the areas of low-level computer vision and image\nprocessing to solve regression problems mostly with relatively shallow\nnetworks.\n  We present a novel 12-layer deep convolutional network for image compression\nartifact suppression with hierarchical skip connections and a multi-scale loss\nfunction. We achieve a boost of up to 1.79 dB in PSNR over ordinary JPEG and an\nimprovement of up to 0.36 dB over the best previous ConvNet result. We show\nthat a network trained for a specific quality factor (QF) is resilient to the\nQF used to compress the input image - a single network trained for QF 60\nprovides a PSNR gain of more than 1.5 dB over the wide QF range from 40 to 76.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 10:11:58 GMT"}], "update_date": "2018-01-30", "authors_parsed": [["Cavigelli", "Lukas", ""], ["Hager", "Pascal", ""], ["Benini", "Luca", ""]]}, {"id": "1611.07343", "submitter": "Jean-Baptiste Mouret", "authors": "Antoine Cully, Konstantinos Chatzilygeroudis, Federico Allocati,\n  Jean-Baptiste Mouret", "title": "Limbo: A Fast and Flexible Library for Bayesian Optimization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Limbo is an open-source C++11 library for Bayesian optimization which is\ndesigned to be both highly flexible and very fast. It can be used to optimize\nfunctions for which the gradient is unknown, evaluations are expensive, and\nruntime cost matters (e.g., on embedded systems or robots). Benchmarks on\nstandard functions show that Limbo is about 2 times faster than BayesOpt\n(another C++ library) for a similar accuracy.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 15:00:08 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Cully", "Antoine", ""], ["Chatzilygeroudis", "Konstantinos", ""], ["Allocati", "Federico", ""], ["Mouret", "Jean-Baptiste", ""]]}, {"id": "1611.07379", "submitter": "Jia Zhang", "authors": "Jia Zhang, Weidong Ma, Tao Qin, Xiaoming Sun and Tie-Yan Liu", "title": "Randomized Mechanisms for Selling Reserved Instances in Cloud", "comments": "Already accepted by AAAI'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Selling reserved instances (or virtual machines) is a basic service in cloud\ncomputing. In this paper, we consider a more flexible pricing model for\ninstance reservation, in which a customer can propose the time length and\nnumber of resources of her request, while in today's industry, customers can\nonly choose from several predefined reservation packages. Under this model, we\ndesign randomized mechanisms for customers coming online to optimize social\nwelfare and providers' revenue.\n  We first consider a simple case, where the requests from the customers do not\nvary too much in terms of both length and value density. We design a randomized\nmechanism that achieves a competitive ratio $\\frac{1}{42}$ for both\n\\emph{social welfare} and \\emph{revenue}, which is a improvement as there is\nusually no revenue guarantee in previous works such as\n\\cite{azar2015ec,wang2015selling}. This ratio can be improved up to\n$\\frac{1}{11}$ when we impose a realistic constraint on the maximum number of\nresources used by each request. On the hardness side, we show an upper bound\n$\\frac{1}{3}$ on competitive ratio for any randomized mechanism. We then extend\nour mechanism to the general case and achieve a competitive ratio\n$\\frac{1}{42\\log k\\log T}$ for both social welfare and revenue, where $T$ is\nthe ratio of the maximum request length to the minimum request length and $k$\nis the ratio of the maximum request value density to the minimum request value\ndensity. This result outperforms the previous upper bound $\\frac{1}{CkT}$ for\ndeterministic mechanisms \\cite{wang2015selling}. We also prove an upper bound\n$\\frac{2}{\\log 8kT}$ for any randomized mechanism. All the mechanisms we\nprovide are in a greedy style. They are truthful and easy to be integrated into\npractical cloud systems.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 16:03:58 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Zhang", "Jia", ""], ["Ma", "Weidong", ""], ["Qin", "Tao", ""], ["Sun", "Xiaoming", ""], ["Liu", "Tie-Yan", ""]]}, {"id": "1611.07422", "submitter": "Jiequn Han", "authors": "Jiequn Han, Weinan E", "title": "Deep Learning Approximation for Stochastic Control Problems", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.NE math.OC stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Many real world stochastic control problems suffer from the \"curse of\ndimensionality\". To overcome this difficulty, we develop a deep learning\napproach that directly solves high-dimensional stochastic control problems\nbased on Monte-Carlo sampling. We approximate the time-dependent controls as\nfeedforward neural networks and stack these networks together through model\ndynamics. The objective function for the control problem plays the role of the\nloss function for the deep neural network. We test this approach using examples\nfrom the areas of optimal trading and energy storage. Our results suggest that\nthe algorithm presented here achieves satisfactory accuracy and at the same\ntime, can handle rather high dimensional problems.\n", "versions": [{"version": "v1", "created": "Wed, 2 Nov 2016 02:47:26 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Han", "Jiequn", ""], ["E", "Weinan", ""]]}, {"id": "1611.07478", "submitter": "Scott Lundberg", "authors": "Scott Lundberg and Su-In Lee", "title": "An unexpected unity among methods for interpreting model predictions", "comments": "Presented at NIPS 2016 Workshop on Interpretable Machine Learning in\n  Complex Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding why a model made a certain prediction is crucial in many data\nscience fields. Interpretable predictions engender appropriate trust and\nprovide insight into how the model may be improved. However, with large modern\ndatasets the best accuracy is often achieved by complex models even experts\nstruggle to interpret, which creates a tension between accuracy and\ninterpretability. Recently, several methods have been proposed for interpreting\npredictions from complex models by estimating the importance of input features.\nHere, we present how a model-agnostic additive representation of the importance\nof input features unifies current methods. This representation is optimal, in\nthe sense that it is the only set of additive values that satisfies important\nproperties. We show how we can leverage these properties to create novel visual\nexplanations of model predictions. The thread of unity that this representation\nweaves through the literature indicates that there are common principles to be\nlearned about the interpretation of model predictions that apply in many\nscenarios.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 19:30:28 GMT"}, {"version": "v2", "created": "Wed, 23 Nov 2016 06:44:36 GMT"}, {"version": "v3", "created": "Thu, 8 Dec 2016 08:24:15 GMT"}], "update_date": "2016-12-09", "authors_parsed": [["Lundberg", "Scott", ""], ["Lee", "Su-In", ""]]}, {"id": "1611.07507", "submitter": "Karol Gregor", "authors": "Karol Gregor, Danilo Jimenez Rezende, Daan Wierstra", "title": "Variational Intrinsic Control", "comments": "15 pages, 6 figures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this paper we introduce a new unsupervised reinforcement learning method\nfor discovering the set of intrinsic options available to an agent. This set is\nlearned by maximizing the number of different states an agent can reliably\nreach, as measured by the mutual information between the set of options and\noption termination states. To this end, we instantiate two policy gradient\nbased algorithms, one that creates an explicit embedding space of options and\none that represents options implicitly. The algorithms also provide an explicit\nmeasure of empowerment in a given state that can be used by an empowerment\nmaximizing agent. The algorithm scales well with function approximation and we\ndemonstrate the applicability of the algorithm on a range of tasks.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 20:44:39 GMT"}], "update_date": "2016-11-23", "authors_parsed": [["Gregor", "Karol", ""], ["Rezende", "Danilo Jimenez", ""], ["Wierstra", "Daan", ""]]}, {"id": "1611.07567", "submitter": "Marina Vidovic", "authors": "Marina M.-C. Vidovic, Nico G\\\"ornitz, Klaus-Robert M\\\"uller, Marius\n  Kloft", "title": "Feature Importance Measure for Non-linear Learning Algorithms", "comments": "Presented at NIPS 2016 Workshop on Interpretable Machine Learning in\n  Complex Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Complex problems may require sophisticated, non-linear learning methods such\nas kernel machines or deep neural networks to achieve state of the art\nprediction accuracies. However, high prediction accuracies are not the only\nobjective to consider when solving problems using machine learning. Instead,\nparticular scientific applications require some explanation of the learned\nprediction function. Unfortunately, most methods do not come with out of the\nbox straight forward interpretation. Even linear prediction functions are not\nstraight forward to explain if features exhibit complex correlation structure.\n  In this paper, we propose the Measure of Feature Importance (MFI). MFI is\ngeneral and can be applied to any arbitrary learning machine (including kernel\nmachines and deep learning). MFI is intrinsically non-linear and can detect\nfeatures that by itself are inconspicuous and only impact the prediction\nfunction through their interaction with other features. Lastly, MFI can be used\nfor both --- model-based feature importance and instance-based feature\nimportance (i.e, measuring the importance of a feature for a particular data\npoint).\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 22:36:31 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Vidovic", "Marina M. -C.", ""], ["G\u00f6rnitz", "Nico", ""], ["M\u00fcller", "Klaus-Robert", ""], ["Kloft", "Marius", ""]]}, {"id": "1611.07579", "submitter": "Sameer Singh", "authors": "Sameer Singh and Marco Tulio Ribeiro and Carlos Guestrin", "title": "Programs as Black-Box Explanations", "comments": "Presented at NIPS 2016 Workshop on Interpretable Machine Learning in\n  Complex Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Recent work in model-agnostic explanations of black-box machine learning has\ndemonstrated that interpretability of complex models does not have to come at\nthe cost of accuracy or model flexibility. However, it is not clear what kind\nof explanations, such as linear models, decision trees, and rule lists, are the\nappropriate family to consider, and different tasks and models may benefit from\ndifferent kinds of explanations. Instead of picking a single family of\nrepresentations, in this work we propose to use \"programs\" as model-agnostic\nexplanations. We show that small programs can be expressive yet intuitive as\nexplanations, and generalize over a number of existing interpretable families.\nWe propose a prototype program induction method based on simulated annealing\nthat approximates the local behavior of black-box classifiers around a specific\nprediction using random perturbations. Finally, we present preliminary\napplication on small datasets and show that the generated explanations are\nintuitive and accurate for a number of classifiers.\n", "versions": [{"version": "v1", "created": "Tue, 22 Nov 2016 23:35:03 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Singh", "Sameer", ""], ["Ribeiro", "Marco Tulio", ""], ["Guestrin", "Carlos", ""]]}, {"id": "1611.07599", "submitter": "Jia Zhang", "authors": "Jia Zhang, Zheng Wang, Qian Li, Jialin Zhang, Yanyan Lan, Qiang Li and\n  Xiaoming Sun", "title": "Efficient Delivery Policy to Minimize User Traffic Consumption in\n  Guaranteed Advertising", "comments": "Already accepted by AAAI'17", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In this work, we study the guaranteed delivery model which is widely used in\nonline display advertising. In the guaranteed delivery scenario, ad exposures\n(which are also called impressions in some works) to users are guaranteed by\ncontracts signed in advance between advertisers and publishers. A crucial\nproblem for the advertising platform is how to fully utilize the valuable user\ntraffic to generate as much as possible revenue.\n  Different from previous works which usually minimize the penalty of\nunsatisfied contracts and some other cost (e.g. representativeness), we propose\nthe novel consumption minimization model, in which the primary objective is to\nminimize the user traffic consumed to satisfy all contracts. Under this model,\nwe develop a near optimal method to deliver ads for users. The main advantage\nof our method lies in that it consumes nearly as least as possible user traffic\nto satisfy all contracts, therefore more contracts can be accepted to produce\nmore revenue. It also enables the publishers to estimate how much user traffic\nis redundant or short so that they can sell or buy this part of traffic in bulk\nin the exchange market. Furthermore, it is robust with regard to priori\nknowledge of user type distribution. Finally, the simulation shows that our\nmethod outperforms the traditional state-of-the-art methods.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 01:55:36 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Zhang", "Jia", ""], ["Wang", "Zheng", ""], ["Li", "Qian", ""], ["Zhang", "Jialin", ""], ["Lan", "Yanyan", ""], ["Li", "Qiang", ""], ["Sun", "Xiaoming", ""]]}, {"id": "1611.07941", "submitter": "Pierre Baque", "authors": "Pierre Baqu\\'e, Fran\\c{c}ois Fleuret and Pascal Fua", "title": "Multi-Modal Mean-Fields via Cardinality-Based Clamping", "comments": "Submitted for review to CVPR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Mean Field inference is central to statistical physics. It has attracted much\ninterest in the Computer Vision community to efficiently solve problems\nexpressible in terms of large Conditional Random Fields. However, since it\nmodels the posterior probability distribution as a product of marginal\nprobabilities, it may fail to properly account for important dependencies\nbetween variables. We therefore replace the fully factorized distribution of\nMean Field by a weighted mixture of such distributions, that similarly\nminimizes the KL-Divergence to the true posterior. By introducing two new\nideas, namely, conditioning on groups of variables instead of single ones and\nusing a parameter of the conditional random field potentials, that we identify\nto the temperature in the sense of statistical physics to select such groups,\nwe can perform this minimization efficiently. Our extension of the clamping\nmethod proposed in previous works allows us to both produce a more descriptive\napproximation of the true posterior and, inspired by the diverse MAP paradigms,\nfit a mixture of Mean Field approximations. We demonstrate that this positively\nimpacts real-world algorithms that initially relied on mean fields.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 19:14:25 GMT"}], "update_date": "2016-11-24", "authors_parsed": [["Baqu\u00e9", "Pierre", ""], ["Fleuret", "Fran\u00e7ois", ""], ["Fua", "Pascal", ""]]}, {"id": "1611.08037", "submitter": "Lantao Liu", "authors": "Zhibei Ma, Kai Yin, Lantao Liu, Gaurav S. Sukhatme", "title": "A Spatio-Temporal Representation for the Orienteering Problem with\n  Time-Varying Profits", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider an orienteering problem (OP) where an agent needs to visit a\nseries (possibly a subset) of depots, from which the maximal accumulated\nprofits are desired within given limited time budget. Different from most\nexisting works where the profits are assumed to be static, in this work we\ninvestigate a variant that has arbitrary time-dependent profits. Specifically,\nthe profits to be collected change over time and they follow different (e.g.,\nindependent) time-varying functions. The problem is of inherent nonlinearity\nand difficult to solve by existing methods. To tackle the challenge, we present\na simple and effective framework that incorporates time-variations into the\nfundamental planning process. Specifically, we propose a deterministic\nspatio-temporal representation where both spatial description and temporal\nlogic are unified into one routing topology. By employing existing basic\nsorting and searching algorithms, the routing solutions can be computed in an\nextremely efficient way. The proposed method is easy to implement and extensive\nnumerical results show that our approach is time efficient and generates\nnear-optimal solutions.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 00:07:56 GMT"}, {"version": "v2", "created": "Sun, 2 Jul 2017 04:56:41 GMT"}], "update_date": "2017-07-04", "authors_parsed": [["Ma", "Zhibei", ""], ["Yin", "Kai", ""], ["Liu", "Lantao", ""], ["Sukhatme", "Gaurav S.", ""]]}, {"id": "1611.08070", "submitter": "Jung-Su Ha", "authors": "Jung-Su Ha and Han-Lim Choi", "title": "Multiscale Inverse Reinforcement Learning using Diffusion Wavelets", "comments": "Presented at NIPS 2016 Workshop on Interpretable Machine Learning in\n  Complex Systems", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This work presents a multiscale framework to solve an inverse reinforcement\nlearning (IRL) problem for continuous-time/state stochastic systems. We take\nadvantage of a diffusion wavelet representation of the associated Markov chain\nto abstract the state space. This not only allows for effectively handling the\nlarge (and geometrically complex) decision space but also provides more\ninterpretable representations of the demonstrated state trajectories and also\nof the resulting policy of IRL. In the proposed framework, the problem is\ndivided into the global and local IRL, where the global approximation of the\noptimal value functions are obtained using coarse features and the local\ndetails are quantified using fine local features. An illustrative numerical\nexample on robot path control in a complex environment is presented to verify\nthe proposed method.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 05:20:52 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Ha", "Jung-Su", ""], ["Choi", "Han-Lim", ""]]}, {"id": "1611.08103", "submitter": "Guangming Lang", "authors": "Guangming Lang", "title": "Double-quantitative $\\gamma^{\\ast}-$fuzzy coverings approximation\n  operators", "comments": "It enriches the fuzzy covering rough set theory", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  In digital-based information boom, the fuzzy covering rough set model is an\nimportant mathematical tool for artificial intelligence, and how to build the\nbridge between the fuzzy covering rough set theory and Pawlak's model is\nbecoming a hot research topic. In this paper, we first present the\n$\\gamma-$fuzzy covering based probabilistic and grade approximation operators\nand double-quantitative approximation operators. We also study the\nrelationships among the three types of $\\gamma-$fuzzy covering based\napproximation operators. Second, we propose the $\\gamma^{\\ast}-$fuzzy coverings\nbased multi-granulation probabilistic and grade lower and upper approximation\noperators and multi-granulation double-quantitative lower and upper\napproximation operators. We also investigate the relationships among these\ntypes of $\\gamma-$fuzzy coverings based approximation operators. Finally, we\nemploy several examples to illustrate how to construct the lower and upper\napproximations of fuzzy sets with the absolute and relative quantitative\ninformation.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 09:06:57 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Lang", "Guangming", ""]]}, {"id": "1611.08104", "submitter": "Peter Wittek", "authors": "Peter Wittek, Christian Gogolin", "title": "Quantum Enhanced Inference in Markov Logic Networks", "comments": "8 pages, 1 figure", "journal-ref": "Scientific Reports 7, 45672 (2017)", "doi": "10.1038/srep45672", "report-no": null, "categories": "stat.ML cs.AI quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Markov logic networks (MLNs) reconcile two opposing schools in machine\nlearning and artificial intelligence: causal networks, which account for\nuncertainty extremely well, and first-order logic, which allows for formal\ndeduction. An MLN is essentially a first-order logic template to generate\nMarkov networks. Inference in MLNs is probabilistic and it is often performed\nby approximate methods such as Markov chain Monte Carlo (MCMC) Gibbs sampling.\nAn MLN has many regular, symmetric structures that can be exploited at both\nfirst-order level and in the generated Markov network. We analyze the graph\nstructures that are produced by various lifting methods and investigate the\nextent to which quantum protocols can be used to speed up Gibbs sampling with\nstate preparation and measurement schemes. We review different such approaches,\ndiscuss their advantages, theoretical limitations, and their appeal to\nimplementations. We find that a straightforward application of a recent result\nyields exponential speedup compared to classical heuristics in approximate\nprobabilistic inference, thereby demonstrating another example where advanced\nquantum resources can potentially prove useful in machine learning.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 09:07:22 GMT"}], "update_date": "2017-04-20", "authors_parsed": [["Wittek", "Peter", ""], ["Gogolin", "Christian", ""]]}, {"id": "1611.08108", "submitter": "Jiani Zhang", "authors": "Jiani Zhang, Xingjian Shi, Irwin King and Dit-Yan Yeung", "title": "Dynamic Key-Value Memory Networks for Knowledge Tracing", "comments": "To appear in 26th International Conference on World Wide Web (WWW),\n  2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Knowledge Tracing (KT) is a task of tracing evolving knowledge state of\nstudents with respect to one or more concepts as they engage in a sequence of\nlearning activities. One important purpose of KT is to personalize the practice\nsequence to help students learn knowledge concepts efficiently. However,\nexisting methods such as Bayesian Knowledge Tracing and Deep Knowledge Tracing\neither model knowledge state for each predefined concept separately or fail to\npinpoint exactly which concepts a student is good at or unfamiliar with. To\nsolve these problems, this work introduces a new model called Dynamic Key-Value\nMemory Networks (DKVMN) that can exploit the relationships between underlying\nconcepts and directly output a student's mastery level of each concept. Unlike\nstandard memory-augmented neural networks that facilitate a single memory\nmatrix or two static memory matrices, our model has one static matrix called\nkey, which stores the knowledge concepts and the other dynamic matrix called\nvalue, which stores and updates the mastery levels of corresponding concepts.\nExperiments show that our model consistently outperforms the state-of-the-art\nmodel in a range of KT datasets. Moreover, the DKVMN model can automatically\ndiscover underlying concepts of exercises typically performed by human\nannotations and depict the changing knowledge state of a student.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 09:12:47 GMT"}, {"version": "v2", "created": "Fri, 17 Feb 2017 06:09:27 GMT"}], "update_date": "2017-02-20", "authors_parsed": [["Zhang", "Jiani", ""], ["Shi", "Xingjian", ""], ["King", "Irwin", ""], ["Yeung", "Dit-Yan", ""]]}, {"id": "1611.08219", "submitter": "Dylan Hadfield-Menell", "authors": "Dylan Hadfield-Menell, Anca Dragan, Pieter Abbeel, Stuart Russell", "title": "The Off-Switch Game", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  It is clear that one of the primary tools we can use to mitigate the\npotential risk from a misbehaving AI system is the ability to turn the system\noff. As the capabilities of AI systems improve, it is important to ensure that\nsuch systems do not adopt subgoals that prevent a human from switching them\noff. This is a challenge because many formulations of rational agents create\nstrong incentives for self-preservation. This is not caused by a built-in\ninstinct, but because a rational agent will maximize expected utility and\ncannot achieve whatever objective it has been given if it is dead. Our goal is\nto study the incentives an agent has to allow itself to be switched off. We\nanalyze a simple game between a human H and a robot R, where H can press R's\noff switch but R can disable the off switch. A traditional agent takes its\nreward function for granted: we show that such agents have an incentive to\ndisable the off switch, except in the special case where H is perfectly\nrational. Our key insight is that for R to want to preserve its off switch, it\nneeds to be uncertain about the utility associated with the outcome, and to\ntreat H's actions as important observations about that utility. (R also has no\nincentive to switch itself off in this setting.) We conclude that giving\nmachines an appropriate level of uncertainty about their objectives leads to\nsafer designs, and we argue that this setting is a useful generalization of the\nclassical AI paradigm of rational agents.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 15:23:48 GMT"}, {"version": "v2", "created": "Thu, 25 May 2017 17:05:16 GMT"}, {"version": "v3", "created": "Fri, 16 Jun 2017 01:41:59 GMT"}], "update_date": "2017-06-19", "authors_parsed": [["Hadfield-Menell", "Dylan", ""], ["Dragan", "Anca", ""], ["Abbeel", "Pieter", ""], ["Russell", "Stuart", ""]]}, {"id": "1611.08307", "submitter": "Tim Rockt\\\"aschel", "authors": "Avishkar Bhoopchand, Tim Rockt\\\"aschel, Earl Barr, Sebastian Riedel", "title": "Learning Python Code Suggestion with a Sparse Pointer Network", "comments": "Under review as a conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.CL cs.SE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  To enhance developer productivity, all modern integrated development\nenvironments (IDEs) include code suggestion functionality that proposes likely\nnext tokens at the cursor. While current IDEs work well for statically-typed\nlanguages, their reliance on type annotations means that they do not provide\nthe same level of support for dynamic programming languages as for\nstatically-typed languages. Moreover, suggestion engines in modern IDEs do not\npropose expressions or multi-statement idiomatic code. Recent work has shown\nthat language models can improve code suggestion systems by learning from\nsoftware repositories. This paper introduces a neural language model with a\nsparse pointer network aimed at capturing very long-range dependencies. We\nrelease a large-scale code suggestion corpus of 41M lines of Python code\ncrawled from GitHub. On this corpus, we found standard neural language models\nto perform well at suggesting local phenomena, but struggle to refer to\nidentifiers that are introduced many tokens in the past. By augmenting a neural\nlanguage model with a pointer network specialized in referring to predefined\nclasses of identifiers, we obtain a much lower perplexity and a 5 percentage\npoints increase in accuracy for code suggestion compared to an LSTM baseline.\nIn fact, this increase in code suggestion accuracy is due to a 13 times more\naccurate prediction of identifiers. Furthermore, a qualitative analysis shows\nthis model indeed captures interesting long-range dependencies, like referring\nto a class member defined over 60 tokens in the past.\n", "versions": [{"version": "v1", "created": "Thu, 24 Nov 2016 21:01:46 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Bhoopchand", "Avishkar", ""], ["Rockt\u00e4schel", "Tim", ""], ["Barr", "Earl", ""], ["Riedel", "Sebastian", ""]]}, {"id": "1611.08366", "submitter": "Muhammad Yousefnezhad", "authors": "Muhammad Yousefnezhad, Daoqiang Zhang", "title": "Local Discriminant Hyperalignment for multi-subject fMRI data alignment", "comments": "Published in the Thirty-First AAAI Conference on Artificial\n  Intelligence (AAAI-17)", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Multivariate Pattern (MVP) classification can map different cognitive states\nto the brain tasks. One of the main challenges in MVP analysis is validating\nthe generated results across subjects. However, analyzing multi-subject fMRI\ndata requires accurate functional alignments between neuronal activities of\ndifferent subjects, which can rapidly increase the performance and robustness\nof the final results. Hyperalignment (HA) is one of the most effective\nfunctional alignment methods, which can be mathematically formulated by the\nCanonical Correlation Analysis (CCA) methods. Since HA mostly uses the\nunsupervised CCA techniques, its solution may not be optimized for MVP\nanalysis. By incorporating the idea of Local Discriminant Analysis (LDA) into\nCCA, this paper proposes Local Discriminant Hyperalignment (LDHA) as a novel\nsupervised HA method, which can provide better functional alignment for MVP\nanalysis. Indeed, the locality is defined based on the stimuli categories in\nthe train-set, where the correlation between all stimuli in the same category\nwill be maximized and the correlation between distinct categories of stimuli\napproaches to near zero. Experimental studies on multi-subject MVP analysis\nconfirm that the LDHA method achieves superior performance to other\nstate-of-the-art HA algorithms.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2016 07:27:34 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Yousefnezhad", "Muhammad", ""], ["Zhang", "Daoqiang", ""]]}, {"id": "1611.08374", "submitter": "Bj{\\o}rn Magnus Mathisen", "authors": "Bj{\\o}rn Magnus Mathisen, Peter Haro, B{\\aa}rd Hanssen, Sara Bj\\\"ork,\n  St{\\aa}le Walderhaug", "title": "Decision Support Systems in Fisheries and Aquaculture: A systematic\n  review", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by-nc-sa/4.0/", "abstract": "  Decision support systems help decision makers make better decisions in the\nface of complex decision problems (e.g. investment or policy decisions).\nFisheries and Aquaculture is a domain where decision makers face such decisions\nsince they involve factors from many different scientific fields. No systematic\noverview of literature describing decision support systems and their\napplication in fisheries and aquaculture has been conducted. This paper\nsummarizes scientific literature that describes decision support systems\napplied to the domain of Fisheries and Aquaculture. We use an established\nsystematic mapping survey method to conduct our literature mapping. Our\nresearch questions are: What decision support systems for fisheries and\naquaculture exists? What are the most investigated fishery and aquaculture\ndecision support systems topics and how have these changed over time? Do any\ncurrent DSS for fisheries provide real- time analytics? Do DSSes in Fisheries\nand Aquaculture build their models using machine learning done on captured and\ngrounded data? The paper then detail how we employ the systematic mapping\nmethod in answering these questions. This results in 27 papers being identified\nas relevant and gives an exposition on the primary methods concluded in the\nstudy for designing a decision support system. We provide an analysis of the\nresearch done in the studies collected. We discovered that most literature does\nnot consider multiple aspects for multiple stakeholders in their work. In\naddition we observed that little or no work has been done with real-time\nanalysis in these decision support systems.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2016 08:13:51 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Mathisen", "Bj\u00f8rn Magnus", ""], ["Haro", "Peter", ""], ["Hanssen", "B\u00e5rd", ""], ["Bj\u00f6rk", "Sara", ""], ["Walderhaug", "St\u00e5le", ""]]}, {"id": "1611.08481", "submitter": "Harm de Vries", "authors": "Harm de Vries, Florian Strub, Sarath Chandar, Olivier Pietquin, Hugo\n  Larochelle, Aaron Courville", "title": "GuessWhat?! Visual object discovery through multi-modal dialogue", "comments": "23 pages; CVPR 2017 submission; see https://guesswhat.ai", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CV", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce GuessWhat?!, a two-player guessing game as a testbed for\nresearch on the interplay of computer vision and dialogue systems. The goal of\nthe game is to locate an unknown object in a rich image scene by asking a\nsequence of questions. Higher-level image understanding, like spatial reasoning\nand language grounding, is required to solve the proposed task. Our key\ncontribution is the collection of a large-scale dataset consisting of 150K\nhuman-played games with a total of 800K visual question-answer pairs on 66K\nimages. We explain our design decisions in collecting the dataset and introduce\nthe oracle and questioner tasks that are associated with the two players of the\ngame. We prototyped deep learning models to establish initial baselines of the\nintroduced tasks.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 20:56:13 GMT"}, {"version": "v2", "created": "Mon, 6 Feb 2017 12:52:53 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["de Vries", "Harm", ""], ["Strub", "Florian", ""], ["Chandar", "Sarath", ""], ["Pietquin", "Olivier", ""], ["Larochelle", "Hugo", ""], ["Courville", "Aaron", ""]]}, {"id": "1611.08499", "submitter": "Nhien Pham Hoang Bao", "authors": "Nhien Pham Hoang Bao, Hiroyuki Iida", "title": "An Analysis of Tournament Structure", "comments": "10 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  This paper explores a novel way for analyzing the tournament structures to\nfind a best suitable one for the tournament under consideration. It concerns\nabout three aspects such as tournament conducting cost, competitiveness\ndevelopment and ranking precision. It then proposes a new method using progress\ntree to detect potential throwaway matches. The analysis performed using the\nproposed method reveals the strengths and weaknesses of tournament structures.\nAs a conclusion, single elimination is best if we want to qualify one winner\nonly, all matches conducted are exciting in term of competitiveness. Double\nelimination with proper seeding system is a better choice if we want to qualify\nmore winners. A reasonable number of extra matches need to be conducted in\nexchange of being able to qualify top four winners. Round-robin gives reliable\nranking precision for all participants. However, its conduction cost is very\nhigh, and it fails to maintain competitiveness development.\n", "versions": [{"version": "v1", "created": "Wed, 16 Nov 2016 07:09:16 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Bao", "Nhien Pham Hoang", ""], ["Iida", "Hiroyuki", ""]]}, {"id": "1611.08555", "submitter": "Florentin Smarandache", "authors": "Florentin Smarandache, Surapati Pramanik (Editors)", "title": "New Trends in Neutrosophic Theory and Applications", "comments": "424 pages", "journal-ref": "Pons asbl, Brussels, 2016", "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Neutrosophic theory and applications have been expanding in all directions at\nan astonishing rate especially after the introduction the journal entitled\nNeutrosophic Sets and Systems. New theories, techniques, algorithms have been\nrapidly developed. One of the most striking trends in the neutrosophic theory\nis the hybridization of neutrosophic set with other potential sets such as\nrough set, bipolar set, soft set, hesitant fuzzy set, etc. The different hybrid\nstructure such as rough neutrosophic set, single valued neutrosophic rough set,\nbipolar neutrosophic set, single valued neutrosophic hesitant fuzzy set, etc.\nare proposed in the literature in a short period of time. Neutrosophic set has\nbeen a very important tool in all various areas of data mining, decision\nmaking, e-learning, engineering, medicine, social science, and some more. The\nbook New Trends in Neutrosophic Theories and Applications focuses on theories,\nmethods, algorithms for decision making and also applications involving\nneutrosophic information. Some topics deal with data mining, decision making,\ne-learning, graph theory, medical diagnosis, probability theory, topology, and\nsome more.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 19:16:49 GMT"}], "update_date": "2016-11-28", "authors_parsed": [["Smarandache", "Florentin", "", "Editors"], ["Pramanik", "Surapati", "", "Editors"]]}, {"id": "1611.08572", "submitter": "Till Mossakowski", "authors": "Till Mossakowski and Fabian Neuhaus", "title": "Bipolar Weighted Argumentation Graphs", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper discusses the semantics of weighted argumentation graphs that are\nbiplor, i.e. contain both attacks and support graphs. The work builds on\nprevious work by Amgoud, Ben-Naim et. al., which presents and compares several\nsemantics for argumentation graphs that contain only supports or only attacks\nrelationships, respectively.\n", "versions": [{"version": "v1", "created": "Fri, 25 Nov 2016 20:04:17 GMT"}, {"version": "v2", "created": "Fri, 23 Dec 2016 08:33:34 GMT"}], "update_date": "2016-12-26", "authors_parsed": [["Mossakowski", "Till", ""], ["Neuhaus", "Fabian", ""]]}, {"id": "1611.08657", "submitter": "Amir Zadeh", "authors": "Amir Zadeh, Tadas Baltru\\v{s}aitis, Louis-Philippe Morency", "title": "Convolutional Experts Constrained Local Model for Facial Landmark\n  Detection", "comments": "Accepted at CVPR-W 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constrained Local Models (CLMs) are a well-established family of methods for\nfacial landmark detection. However, they have recently fallen out of favor to\ncascaded regression-based approaches. This is in part due to the inability of\nexisting CLM local detectors to model the very complex individual landmark\nappearance that is affected by expression, illumination, facial hair, makeup,\nand accessories. In our work, we present a novel local detector --\nConvolutional Experts Network (CEN) -- that brings together the advantages of\nneural architectures and mixtures of experts in an end-to-end framework. We\nfurther propose a Convolutional Experts Constrained Local Model (CE-CLM)\nalgorithm that uses CEN as local detectors. We demonstrate that our proposed\nCE-CLM algorithm outperforms competitive state-of-the-art baselines for facial\nlandmark detection by a large margin on four publicly-available datasets. Our\napproach is especially accurate and robust on challenging profile images.\n", "versions": [{"version": "v1", "created": "Sat, 26 Nov 2016 04:47:34 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 16:00:45 GMT"}, {"version": "v3", "created": "Wed, 30 Nov 2016 18:03:56 GMT"}, {"version": "v4", "created": "Sun, 23 Jul 2017 10:15:06 GMT"}, {"version": "v5", "created": "Wed, 26 Jul 2017 19:46:15 GMT"}], "update_date": "2017-07-28", "authors_parsed": [["Zadeh", "Amir", ""], ["Baltru\u0161aitis", "Tadas", ""], ["Morency", "Louis-Philippe", ""]]}, {"id": "1611.08666", "submitter": "Heriberto Cuay\\'ahuitl", "authors": "Heriberto Cuay\\'ahuitl, Guillaume Couly, Cl\\'ement Olalainty", "title": "Training an Interactive Humanoid Robot Using Multimodal Deep\n  Reinforcement Learning", "comments": "NIPS Workshop on Future of Interactive Learning Machines, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI cs.RO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Training robots to perceive, act and communicate using multiple modalities\nstill represents a challenging problem, particularly if robots are expected to\nlearn efficiently from small sets of example interactions. We describe a\nlearning approach as a step in this direction, where we teach a humanoid robot\nhow to play the game of noughts and crosses. Given that multiple multimodal\nskills can be trained to play this game, we focus our attention to training the\nrobot to perceive the game, and to interact in this game. Our multimodal deep\nreinforcement learning agent perceives multimodal features and exhibits verbal\nand non-verbal actions while playing. Experimental results using simulations\nshow that the robot can learn to win or draw up to 98% of the games. A pilot\ntest of the proposed multimodal system for the targeted game---integrating\nspeech, vision and gestures---reports that reasonable and fluent interactions\ncan be achieved using the proposed approach.\n", "versions": [{"version": "v1", "created": "Sat, 26 Nov 2016 06:25:08 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Cuay\u00e1huitl", "Heriberto", ""], ["Couly", "Guillaume", ""], ["Olalainty", "Cl\u00e9ment", ""]]}, {"id": "1611.08669", "submitter": "Satwik Kottur", "authors": "Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav,\n  Jos\\'e M. F. Moura, Devi Parikh, Dhruv Batra", "title": "Visual Dialog", "comments": "23 pages, 18 figures, CVPR 2017 camera-ready, results on VisDial v0.9\n  dataset, Webpage: http://visualdialog.org", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the task of Visual Dialog, which requires an AI agent to hold a\nmeaningful dialog with humans in natural, conversational language about visual\ncontent. Specifically, given an image, a dialog history, and a question about\nthe image, the agent has to ground the question in image, infer context from\nhistory, and answer the question accurately. Visual Dialog is disentangled\nenough from a specific downstream task so as to serve as a general test of\nmachine intelligence, while being grounded in vision enough to allow objective\nevaluation of individual responses and benchmark progress. We develop a novel\ntwo-person chat data-collection protocol to curate a large-scale Visual Dialog\ndataset (VisDial). VisDial v0.9 has been released and contains 1 dialog with 10\nquestion-answer pairs on ~120k images from COCO, with a total of ~1.2M dialog\nquestion-answer pairs.\n  We introduce a family of neural encoder-decoder models for Visual Dialog with\n3 encoders -- Late Fusion, Hierarchical Recurrent Encoder and Memory Network --\nand 2 decoders (generative and discriminative), which outperform a number of\nsophisticated baselines. We propose a retrieval-based evaluation protocol for\nVisual Dialog where the AI agent is asked to sort a set of candidate answers\nand evaluated on metrics such as mean-reciprocal-rank of human response. We\nquantify gap between machine and human performance on the Visual Dialog task\nvia human studies. Putting it all together, we demonstrate the first 'visual\nchatbot'! Our dataset, code, trained models and visual chatbot are available on\nhttps://visualdialog.org\n", "versions": [{"version": "v1", "created": "Sat, 26 Nov 2016 06:39:28 GMT"}, {"version": "v2", "created": "Mon, 5 Dec 2016 02:00:49 GMT"}, {"version": "v3", "created": "Fri, 21 Apr 2017 16:29:55 GMT"}, {"version": "v4", "created": "Mon, 24 Apr 2017 02:10:49 GMT"}, {"version": "v5", "created": "Tue, 1 Aug 2017 22:04:37 GMT"}], "update_date": "2017-08-03", "authors_parsed": [["Das", "Abhishek", ""], ["Kottur", "Satwik", ""], ["Gupta", "Khushi", ""], ["Singh", "Avi", ""], ["Yadav", "Deshraj", ""], ["Moura", "Jos\u00e9 M. F.", ""], ["Parikh", "Devi", ""], ["Batra", "Dhruv", ""]]}, {"id": "1611.08675", "submitter": "Heriberto Cuay\\'ahuitl", "authors": "Heriberto Cuay\\'ahuitl, Seunghak Yu, Ashley Williamson, Jacob Carse", "title": "Deep Reinforcement Learning for Multi-Domain Dialogue Systems", "comments": "NIPS Workshop on Deep Reinforcement Learning, 2016", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Standard deep reinforcement learning methods such as Deep Q-Networks (DQN)\nfor multiple tasks (domains) face scalability problems. We propose a method for\nmulti-domain dialogue policy learning---termed NDQN, and apply it to an\ninformation-seeking spoken dialogue system in the domains of restaurants and\nhotels. Experimental results comparing DQN (baseline) versus NDQN (proposed)\nusing simulations report that our proposed method exhibits better scalability\nand is promising for optimising the behaviour of multi-domain dialogue systems.\n", "versions": [{"version": "v1", "created": "Sat, 26 Nov 2016 07:53:22 GMT"}], "update_date": "2021-05-10", "authors_parsed": [["Cuay\u00e1huitl", "Heriberto", ""], ["Yu", "Seunghak", ""], ["Williamson", "Ashley", ""], ["Carse", "Jacob", ""]]}, {"id": "1611.08691", "submitter": "Piotr Skowron", "authors": "Markus Brill and Jean-Fran\\c{c}ois Laslier and Piotr Skowron", "title": "Multiwinner Approval Rules as Apportionment Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.GT cs.AI cs.MA", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We establish a link between multiwinner elections and apportionment problems\nby showing how approval-based multiwinner election rules can be interpreted as\nmethods of apportionment. We consider several multiwinner rules and observe\nthat they induce apportionment methods that are well-established in the\nliterature on proportional representation. For instance, we show that\nProportional Approval Voting induces the D'Hondt method and that Monroe's rule\ninduces the largest reminder method. We also consider properties of\napportionment methods and exhibit multiwinner rules that induce apportionment\nmethods satisfying these properties.\n", "versions": [{"version": "v1", "created": "Sat, 26 Nov 2016 09:47:07 GMT"}, {"version": "v2", "created": "Tue, 20 Apr 2021 07:23:46 GMT"}], "update_date": "2021-04-21", "authors_parsed": [["Brill", "Markus", ""], ["Laslier", "Jean-Fran\u00e7ois", ""], ["Skowron", "Piotr", ""]]}, {"id": "1611.08696", "submitter": "Guillermo P\\'erez", "authors": "Krishnendu Chatterjee, Petr Novotn\\'y, Guillermo A. P\\'erez,\n  Jean-Fran\\c{c}ois Raskin, {\\DJ}or{\\dj}e \\v{Z}ikeli\\'c", "title": "Optimizing Expectation with Guarantees in POMDPs (Technical Report)", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.GT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  A standard objective in partially-observable Markov decision processes\n(POMDPs) is to find a policy that maximizes the expected discounted-sum payoff.\nHowever, such policies may still permit unlikely but highly undesirable\noutcomes, which is problematic especially in safety-critical applications.\nRecently, there has been a surge of interest in POMDPs where the goal is to\nmaximize the probability to ensure that the payoff is at least a given\nthreshold, but these approaches do not consider any optimization beyond\nsatisfying this threshold constraint. In this work we go beyond both the\n\"expectation\" and \"threshold\" approaches and consider a \"guaranteed payoff\noptimization (GPO)\" problem for POMDPs, where we are given a threshold $t$ and\nthe objective is to find a policy $\\sigma$ such that a) each possible outcome\nof $\\sigma$ yields a discounted-sum payoff of at least $t$, and b) the expected\ndiscounted-sum payoff of $\\sigma$ is optimal (or near-optimal) among all\npolicies satisfying a). We present a practical approach to tackle the GPO\nproblem and evaluate it on standard POMDP benchmarks.\n", "versions": [{"version": "v1", "created": "Sat, 26 Nov 2016 10:55:40 GMT"}, {"version": "v2", "created": "Sun, 29 Jan 2017 13:31:54 GMT"}], "update_date": "2017-01-31", "authors_parsed": [["Chatterjee", "Krishnendu", ""], ["Novotn\u00fd", "Petr", ""], ["P\u00e9rez", "Guillermo A.", ""], ["Raskin", "Jean-Fran\u00e7ois", ""], ["\u017dikeli\u0107", "\u0110or\u0111e", ""]]}, {"id": "1611.08733", "submitter": "Jan Jakubuv", "authors": "Jan Jakubuv, Josef Urban", "title": "BliStrTune: Hierarchical Invention of Theorem Proving Strategies", "comments": "Submitted to Certified Programs and Proofs (CPP 2017)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LO cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Inventing targeted proof search strategies for specific problem sets is a\ndifficult task. State-of-the-art automated theorem provers (ATPs) such as E\nallow a large number of user-specified proof search strategies described in a\nrich domain specific language. Several machine learning methods that invent\nstrategies automatically for ATPs were proposed previously. One of them is the\nBlind Strategymaker (BliStr), a system for automated invention of ATP\nstrategies.\n  In this paper we introduce BliStrTune -- a hierarchical extension of BliStr.\nBliStrTune allows exploring much larger space of E strategies by interleaving\nsearch for high-level parameters with their fine-tuning. We use BliStrTune to\ninvent new strategies based also on new clause weight functions targeted at\nproblems from large ITP libraries. We show that the new strategies\nsignificantly improve E's performance in solving problems from the Mizar\nMathematical Library.\n", "versions": [{"version": "v1", "created": "Sat, 26 Nov 2016 18:48:43 GMT"}], "update_date": "2017-01-25", "authors_parsed": [["Jakubuv", "Jan", ""], ["Urban", "Josef", ""]]}, {"id": "1611.08773", "submitter": "Abdullah Al-Dujaili", "authors": "Abdullah Al-Dujaili, S. Suresh", "title": "Embedded Bandits for Large-Scale Black-Box Optimization", "comments": "To appear at AAAI 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Random embedding has been applied with empirical success to large-scale\nblack-box optimization problems with low effective dimensions. This paper\nproposes the EmbeddedHunter algorithm, which incorporates the technique in a\nhierarchical stochastic bandit setting, following the optimism in the face of\nuncertainty principle and breaking away from the multiple-run framework in\nwhich random embedding has been conventionally applied similar to stochastic\nblack-box optimization solvers. Our proposition is motivated by the bounded\nmean variation in the objective value for a low-dimensional point projected\nrandomly into the decision space of Lipschitz-continuous problems. In essence,\nthe EmbeddedHunter algorithm expands optimistically a partitioning tree over a\nlow-dimensional---equal to the effective dimension of the problem---search\nspace based on a bounded number of random embeddings of sampled points from the\nlow-dimensional space. In contrast to the probabilistic theoretical guarantees\nof multiple-run random-embedding algorithms, the finite-time analysis of the\nproposed algorithm presents a theoretical upper bound on the regret as a\nfunction of the algorithm's number of iterations. Furthermore, numerical\nexperiments were conducted to validate its performance. The results show a\nclear performance gain over recently proposed random embedding methods for\nlarge-scale problems, provided the intrinsic dimensionality is low.\n", "versions": [{"version": "v1", "created": "Sun, 27 Nov 2016 02:18:09 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Al-Dujaili", "Abdullah", ""], ["Suresh", "S.", ""]]}, {"id": "1611.08788", "submitter": "Biswarup Bhattacharya", "authors": "Arna Ghosh, Biswarup Bhattacharya, Somnath Basu Roy Chowdhury", "title": "SAD-GAN: Synthetic Autonomous Driving using Generative Adversarial\n  Networks", "comments": "5 pages; 4 figures; Accepted at the Deep Learning for Action and\n  Interaction Workshop, 30th Conference on Neural Information Processing\n  Systems (NIPS 2016), Barcelona, Spain; All authors have equal contribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Autonomous driving is one of the most recent topics of interest which is\naimed at replicating human driving behavior keeping in mind the safety issues.\nWe approach the problem of learning synthetic driving using generative neural\nnetworks. The main idea is to make a controller trainer network using images\nplus key press data to mimic human learning. We used the architecture of a\nstable GAN to make predictions between driving scenes using key presses. We\ntrain our model on one video game (Road Rash) and tested the accuracy and\ncompared it by running the model on other maps in Road Rash to determine the\nextent of learning.\n", "versions": [{"version": "v1", "created": "Sun, 27 Nov 2016 05:01:39 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Ghosh", "Arna", ""], ["Bhattacharya", "Biswarup", ""], ["Chowdhury", "Somnath Basu Roy", ""]]}, {"id": "1611.08789", "submitter": "Biswarup Bhattacharya", "authors": "Arna Ghosh, Biswarup Bhattacharya, Somnath Basu Roy Chowdhury", "title": "Handwriting Profiling using Generative Adversarial Networks", "comments": "2 pages; 2 figures; Accepted at The Thirty-First AAAI Conference on\n  Artificial Intelligence (AAAI-17 Student Abstract and Poster Program), San\n  Francisco, USA; All authors have equal contribution", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Handwriting is a skill learned by humans from a very early age. The ability\nto develop one's own unique handwriting as well as mimic another person's\nhandwriting is a task learned by the brain with practice. This paper deals with\nthis very problem where an intelligent system tries to learn the handwriting of\nan entity using Generative Adversarial Networks (GANs). We propose a modified\narchitecture of DCGAN (Radford, Metz, and Chintala 2015) to achieve this. We\nalso discuss about applying reinforcement learning techniques to achieve faster\nlearning. Our algorithm hopes to give new insights in this area and its uses\ninclude identification of forged documents, signature verification, computer\ngenerated art, digitization of documents among others. Our early implementation\nof the algorithm illustrates a good performance with MNIST datasets.\n", "versions": [{"version": "v1", "created": "Sun, 27 Nov 2016 05:02:47 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Ghosh", "Arna", ""], ["Bhattacharya", "Biswarup", ""], ["Chowdhury", "Somnath Basu Roy", ""]]}, {"id": "1611.08908", "submitter": "Thierry Petit", "authors": "Thierry Petit", "title": "\"Model and Run\" Constraint Networks with a MILP Engine", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Constraint Programming (CP) users need significant expertise in order to\nmodel their problems appropriately, notably to select propagators and search\nstrategies. This puts the brakes on a broader uptake of CP. In this paper, we\nintroduce MICE, a complete Java CP modeler that can use any Mixed Integer\nLinear Programming (MILP) solver as a solution technique. Our aim is to provide\nan alternative tool for democratizing the \"CP-style\" modeling thanks to its\nsimplicity of use, with reasonable solving capabilities. Our contributions\ninclude new decompositions of (reified) constraints and constraints on\nnumerical variables.\n", "versions": [{"version": "v1", "created": "Sun, 27 Nov 2016 20:43:27 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Petit", "Thierry", ""]]}, {"id": "1611.08942", "submitter": "Roberto Rossi", "authors": "Roberto Rossi, \\\"Ozg\\\"ur Akg\\\"un, Steven Prestwich, Armagan Tarim", "title": "The BIN_COUNTS Constraint: Filtering and Applications", "comments": "20 pages, working draft", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI math.PR stat.OT", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We introduce the BIN_COUNTS constraint, which deals with the problem of\ncounting the number of decision variables in a set which are assigned values\nthat lie in given bins. We illustrate a decomposition and a filtering algorithm\nthat achieves generalised arc consistency. We contrast the filtering power of\nthese two approaches and we discuss a number of applications. We show that\nBIN_COUNTS can be employed to develop a decomposition for the $\\chi^2$ test\nconstraint, a new statistical constraint that we introduce in this work. We\nalso show how this new constraint can be employed in the context of the\nBalanced Academic Curriculum Problem and of the Balanced Nursing Workload\nProblem. For both these problems we carry out numerical studies involving our\nreformulations. Finally, we present a further application of the $\\chi^2$ test\nconstraint in the context of confidence interval analysis.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 00:23:46 GMT"}, {"version": "v2", "created": "Wed, 7 Dec 2016 17:08:35 GMT"}, {"version": "v3", "created": "Thu, 8 Dec 2016 02:02:10 GMT"}, {"version": "v4", "created": "Sat, 10 Dec 2016 15:49:03 GMT"}, {"version": "v5", "created": "Wed, 14 Dec 2016 21:26:10 GMT"}], "update_date": "2016-12-16", "authors_parsed": [["Rossi", "Roberto", ""], ["Akg\u00fcn", "\u00d6zg\u00fcr", ""], ["Prestwich", "Steven", ""], ["Tarim", "Armagan", ""]]}, {"id": "1611.08944", "submitter": "Jan Leike", "authors": "Jan Leike", "title": "Nonparametric General Reinforcement Learning", "comments": "PhD thesis", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://creativecommons.org/licenses/by/4.0/", "abstract": "  Reinforcement learning (RL) problems are often phrased in terms of Markov\ndecision processes (MDPs). In this thesis we go beyond MDPs and consider RL in\nenvironments that are non-Markovian, non-ergodic and only partially observable.\nOur focus is not on practical algorithms, but rather on the fundamental\nunderlying problems: How do we balance exploration and exploitation? How do we\nexplore optimally? When is an agent optimal? We follow the nonparametric\nrealizable paradigm.\n  We establish negative results on Bayesian RL agents, in particular AIXI. We\nshow that unlucky or adversarial choices of the prior cause the agent to\nmisbehave drastically. Therefore Legg-Hutter intelligence and balanced Pareto\noptimality, which depend crucially on the choice of the prior, are entirely\nsubjective. Moreover, in the class of all computable environments every policy\nis Pareto optimal. This undermines all existing optimality properties for AIXI.\nHowever, there are Bayesian approaches to general RL that satisfy objective\noptimality guarantees: We prove that Thompson sampling is asymptotically\noptimal in stochastic environments in the sense that its value converges to the\nvalue of the optimal policy. We connect asymptotic optimality to regret given a\nrecoverability assumption on the environment that allows the agent to recover\nfrom mistakes. Hence Thompson sampling achieves sublinear regret in these\nenvironments.\n  Our results culminate in a formal solution to the grain of truth problem: A\nBayesian agent acting in a multi-agent environment learns to predict the other\nagents' policies if its prior assigns positive probability to them (the prior\ncontains a grain of truth). We construct a large but limit computable class\ncontaining a grain of truth and show that agents based on Thompson sampling\nover this class converge to play Nash equilibria in arbitrary unknown\ncomputable multi-agent environments.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 00:36:40 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Leike", "Jan", ""]]}, {"id": "1611.08998", "submitter": "Seyed Hamid Rezatofighi", "authors": "S. Hamid Rezatofighi, Vijay Kumar B G, Anton Milan, Ehsan Abbasnejad,\n  Anthony Dick, Ian Reid", "title": "DeepSetNet: Predicting Sets with Deep Neural Networks", "comments": "Accepted in IEEE International Conference on Computer Vision (ICCV),\n  Venice, 2017, (Spotlight)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CV cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper addresses the task of set prediction using deep learning. This is\nimportant because the output of many computer vision tasks, including image\ntagging and object detection, are naturally expressed as sets of entities\nrather than vectors. As opposed to a vector, the size of a set is not fixed in\nadvance, and it is invariant to the ordering of entities within it. We define a\nlikelihood for a set distribution and learn its parameters using a deep neural\nnetwork. We also derive a loss for predicting a discrete distribution\ncorresponding to set cardinality. Set prediction is demonstrated on the problem\nof multi-class image classification. Moreover, we show that the proposed\ncardinality loss can also trivially be applied to the tasks of object counting\nand pedestrian detection. Our approach outperforms existing methods in all\nthree cases on standard datasets.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 06:42:56 GMT"}, {"version": "v2", "created": "Fri, 2 Dec 2016 06:18:14 GMT"}, {"version": "v3", "created": "Mon, 12 Dec 2016 01:10:13 GMT"}, {"version": "v4", "created": "Fri, 31 Mar 2017 06:45:52 GMT"}, {"version": "v5", "created": "Fri, 11 Aug 2017 02:52:36 GMT"}], "update_date": "2017-08-14", "authors_parsed": [["Rezatofighi", "S. Hamid", ""], ["G", "Vijay Kumar B", ""], ["Milan", "Anton", ""], ["Abbasnejad", "Ehsan", ""], ["Dick", "Anthony", ""], ["Reid", "Ian", ""]]}, {"id": "1611.09014", "submitter": "Renate Schmidt", "authors": "Peter Baumgartner and Renate A. Schmidt", "title": "Blocking and Other Enhancements for Bottom-Up Model Generation Methods", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LO", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Model generation is a problem complementary to theorem proving and is\nimportant for fault analysis and debugging of formal specifications of security\nprotocols, programs and terminological definitions. This paper discusses\nseveral ways of enhancing the paradigm of bottom-up model generation. The two\nmain contributions are new, generalized blocking techniques and a new\nrange-restriction transformation. The blocking techniques are based on simple\ntransformations of the input set together with standard equality reasoning and\nredundancy elimination techniques. These provide general methods for finding\nsmall, finite models. The range-restriction transformation refines existing\ntransformations to range-restricted clauses by carefully limiting the creation\nof domain terms. All possible combinations of the introduced techniques and\nclassical range-restriction were tested on the clausal problems of the TPTP\nVersion 6.0.0 with an implementation based on the SPASS theorem prover using a\nhyperresolution-like refinement. Unrestricted domain blocking gave best results\nfor satisfiable problems showing it is a powerful technique indispensable for\nbottom-up model generation methods. Both in combination with the new\nrange-restricting transformation, and the classical range-restricting\ntransformation, good results have been obtained. Limiting the creation of terms\nduring the inference process by using the new range restricting transformation\nhas paid off, especially when using it together with a shifting transformation.\nThe experimental results also show that classical range restriction with\nunrestricted blocking provides a useful complementary method. Overall, the\nresults showed bottom-up model generation methods were good for disproving\ntheorems and generating models for satisfiable problems, but less efficient\nthan SPASS in auto mode for unsatisfiable problems.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 07:54:50 GMT"}, {"version": "v2", "created": "Tue, 29 Nov 2016 22:30:09 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Baumgartner", "Peter", ""], ["Schmidt", "Renate A.", ""]]}, {"id": "1611.09028", "submitter": "Albin Zehe", "authors": "Fotis Jannidis, Isabella Reger, Albin Zehe, Martin Becker, Lena\n  Hettinger, Andreas Hotho", "title": "Analyzing Features for the Detection of Happy Endings in German Novels", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.IR cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With regard to a computational representation of literary plot, this paper\nlooks at the use of sentiment analysis for happy ending detection in German\nnovels. Its focus lies on the investigation of previously proposed sentiment\nfeatures in order to gain insight about the relevance of specific features on\nthe one hand and the implications of their performance on the other hand.\nTherefore, we study various partitionings of novels, considering the highly\nvariable concept of \"ending\". We also show that our approach, even though still\nrather simple, can potentially lead to substantial findings relevant to\nliterary studies.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 08:56:04 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Jannidis", "Fotis", ""], ["Reger", "Isabella", ""], ["Zehe", "Albin", ""], ["Becker", "Martin", ""], ["Hettinger", "Lena", ""], ["Hotho", "Andreas", ""]]}, {"id": "1611.09212", "submitter": "Riccardo Franco", "authors": "Riccardo Franco", "title": "Towards a new quantum cognition model", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI quant-ph", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This article presents a new quantum-like model for cognition explicitly based\non knowledge. It is shown that this model, called QKT (quantum knowledge-based\ntheory), is able to coherently describe some experimental results that are\nproblematic for the prior quantum-like decision models. In particular, I\nconsider the experimental results relevant to the post-decision cognitive\ndissonance, the problems relevant to the question order effect and response\nreplicability, and those relevant to the grand-reciprocity equations. A new set\nof postulates is proposed, which evidence the different meaning given to the\nprojectors and to the quantum states. In the final part, I show that the use of\nquantum gates can help to better describe and understand the evolution of\nquantum-like models.\n", "versions": [{"version": "v1", "created": "Wed, 23 Nov 2016 23:17:10 GMT"}], "update_date": "2016-11-29", "authors_parsed": [["Franco", "Riccardo", ""]]}, {"id": "1611.09321", "submitter": "Ofir Nachum", "authors": "Ofir Nachum, Mohammad Norouzi, Dale Schuurmans", "title": "Improving Policy Gradient by Exploring Under-appreciated Rewards", "comments": "Published as a conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a novel form of policy gradient for model-free\nreinforcement learning (RL) with improved exploration properties. Current\npolicy-based methods use entropy regularization to encourage undirected\nexploration of the reward landscape, which is ineffective in high dimensional\nspaces with sparse rewards. We propose a more directed exploration strategy\nthat promotes exploration of under-appreciated reward regions. An action\nsequence is considered under-appreciated if its log-probability under the\ncurrent policy under-estimates its resulting reward. The proposed exploration\nstrategy is easy to implement, requiring small modifications to an\nimplementation of the REINFORCE algorithm. We evaluate the approach on a set of\nalgorithmic tasks that have long challenged RL methods. Our approach reduces\nhyper-parameter sensitivity and demonstrates significant improvements over\nbaseline methods. Our algorithm successfully solves a benchmark multi-digit\naddition task and generalizes to long sequences. This is, to our knowledge, the\nfirst time that a pure RL method has solved addition using only reward\nfeedback.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 20:15:55 GMT"}, {"version": "v2", "created": "Wed, 25 Jan 2017 22:35:03 GMT"}, {"version": "v3", "created": "Wed, 15 Mar 2017 22:55:17 GMT"}], "update_date": "2017-03-17", "authors_parsed": [["Nachum", "Ofir", ""], ["Norouzi", "Mohammad", ""], ["Schuurmans", "Dale", ""]]}, {"id": "1611.09328", "submitter": "Yangchen Pan", "authors": "Yangchen Pan, Adam White, Martha White", "title": "Accelerated Gradient Temporal Difference Learning", "comments": "AAAI Conference on Artificial Intelligence (AAAI), 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The family of temporal difference (TD) methods span a spectrum from\ncomputationally frugal linear methods like TD({\\lambda}) to data efficient\nleast squares methods. Least square methods make the best use of available data\ndirectly computing the TD solution and thus do not require tuning a typically\nhighly sensitive learning rate parameter, but require quadratic computation and\nstorage. Recent algorithmic developments have yielded several sub-quadratic\nmethods that use an approximation to the least squares TD solution, but incur\nbias. In this paper, we propose a new family of accelerated gradient TD (ATD)\nmethods that (1) provide similar data efficiency benefits to least-squares\nmethods, at a fraction of the computation and storage (2) significantly reduce\nparameter sensitivity compared to linear TD methods, and (3) are asymptotically\nunbiased. We illustrate these claims with a proof of convergence in expectation\nand experiments on several benchmark domains and a large-scale industrial\nenergy allocation domain.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 20:33:15 GMT"}, {"version": "v2", "created": "Thu, 9 Mar 2017 22:36:45 GMT"}], "update_date": "2017-03-13", "authors_parsed": [["Pan", "Yangchen", ""], ["White", "Adam", ""], ["White", "Martha", ""]]}, {"id": "1611.09351", "submitter": "Jan Bergstra", "authors": "Jan A. Bergstra", "title": "Adams Conditioning and Likelihood Ratio Transfer Mediated Inference", "comments": "Based on reviewer's comments many minor improvements have been made", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Bayesian inference as applied in a legal setting is about belief transfer and\ninvolves a plurality of agents and communication protocols.\n  A forensic expert (FE) may communicate to a trier of fact (TOF) first its\nvalue of a certain likelihood ratio with respect to FE's belief state as\nrepresented by a probability function on FE's proposition space. Subsequently\nFE communicates its recently acquired confirmation that a certain evidence\nproposition is true. Then TOF performs likelihood ratio transfer mediated\nreasoning thereby revising their own belief state.\n  The logical principles involved in likelihood transfer mediated reasoning are\ndiscussed in a setting where probabilistic arithmetic is done within a meadow,\nand with Adams conditioning placed in a central role.\n", "versions": [{"version": "v1", "created": "Sat, 26 Nov 2016 22:31:02 GMT"}, {"version": "v2", "created": "Sun, 4 Dec 2016 10:07:29 GMT"}, {"version": "v3", "created": "Sun, 11 Dec 2016 11:17:38 GMT"}, {"version": "v4", "created": "Tue, 18 Dec 2018 23:09:23 GMT"}, {"version": "v5", "created": "Fri, 16 Aug 2019 09:55:15 GMT"}], "update_date": "2019-08-19", "authors_parsed": [["Bergstra", "Jan A.", ""]]}, {"id": "1611.09414", "submitter": "Amit Sharma", "authors": "Amit Sharma, Jake M. Hofman, Duncan J. Watts", "title": "Split-door criterion: Identification of causal effects through auxiliary\n  outcomes", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ME cs.AI stat.AP", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present a method for estimating causal effects in time series data when\nfine-grained information about the outcome of interest is available.\nSpecifically, we examine what we call the split-door setting, where the outcome\nvariable can be split into two parts: one that is potentially affected by the\ncause being studied and another that is independent of it, with both parts\nsharing the same (unobserved) confounders. We show that under these conditions,\nthe problem of identification reduces to that of testing for independence among\nobserved variables, and present a method that uses this approach to\nautomatically find subsets of the data that are causally identified. We\ndemonstrate the method by estimating the causal impact of Amazon's recommender\nsystem on traffic to product pages, finding thousands of examples within the\ndataset that satisfy the split-door criterion. Unlike past studies based on\nnatural experiments that were limited to a single product category, our method\napplies to a large and representative sample of products viewed on the site. In\nline with previous work, we find that the widely-used click-through rate (CTR)\nmetric overestimates the causal impact of recommender systems; depending on the\nproduct category, we estimate that 50-80\\% of the traffic attributed to\nrecommender systems would have happened even without any recommendations. We\nconclude with guidelines for using the split-door criterion as well as a\ndiscussion of other contexts where the method can be applied.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 22:32:16 GMT"}, {"version": "v2", "created": "Thu, 14 Jun 2018 13:09:13 GMT"}], "update_date": "2018-06-15", "authors_parsed": [["Sharma", "Amit", ""], ["Hofman", "Jake M.", ""], ["Watts", "Duncan J.", ""]]}, {"id": "1611.09430", "submitter": "Brian Cheung", "authors": "Brian Cheung, Eric Weiss, Bruno Olshausen", "title": "Emergence of foveal image sampling from learning to attend in visual\n  scenes", "comments": "Published as a conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.NE cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a neural attention model with a learnable retinal sampling\nlattice. The model is trained on a visual search task requiring the\nclassification of an object embedded in a visual scene amidst background\ndistractors using the smallest number of fixations. We explore the tiling\nproperties that emerge in the model's retinal sampling lattice after training.\nSpecifically, we show that this lattice resembles the eccentricity dependent\nsampling lattice of the primate retina, with a high resolution region in the\nfovea surrounded by a low resolution periphery. Furthermore, we find conditions\nwhere these emergent properties are amplified or eliminated providing clues to\ntheir function.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 23:39:20 GMT"}, {"version": "v2", "created": "Sat, 21 Oct 2017 08:26:16 GMT"}], "update_date": "2017-10-24", "authors_parsed": [["Cheung", "Brian", ""], ["Weiss", "Eric", ""], ["Olshausen", "Bruno", ""]]}, {"id": "1611.09434", "submitter": "Jakob Foerster", "authors": "Jakob N. Foerster, Justin Gilmer, Jan Chorowski, Jascha\n  Sohl-Dickstein, David Sussillo", "title": "Input Switched Affine Networks: An RNN Architecture Designed for\n  Interpretability", "comments": "ICLR 2107 submission: https://openreview.net/forum?id=H1MjAnqxg", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  There exist many problem domains where the interpretability of neural network\nmodels is essential for deployment. Here we introduce a recurrent architecture\ncomposed of input-switched affine transformations - in other words an RNN\nwithout any explicit nonlinearities, but with input-dependent recurrent\nweights. This simple form allows the RNN to be analyzed via straightforward\nlinear methods: we can exactly characterize the linear contribution of each\ninput to the model predictions; we can use a change-of-basis to disentangle\ninput, output, and computational hidden unit subspaces; we can fully\nreverse-engineer the architecture's solution to a simple task. Despite this\nease of interpretation, the input switched affine network achieves reasonable\nperformance on a text modeling tasks, and allows greater computational\nefficiency than networks with standard nonlinearities.\n", "versions": [{"version": "v1", "created": "Mon, 28 Nov 2016 23:48:41 GMT"}, {"version": "v2", "created": "Mon, 12 Jun 2017 20:29:48 GMT"}], "update_date": "2017-06-14", "authors_parsed": [["Foerster", "Jakob N.", ""], ["Gilmer", "Justin", ""], ["Chorowski", "Jan", ""], ["Sohl-Dickstein", "Jascha", ""], ["Sussillo", "David", ""]]}, {"id": "1611.09474", "submitter": "Ali Khodabakhsh", "authors": "Ali Khodabakhsh, Evdokia Nikolova", "title": "Maximizing Non-Monotone DR-Submodular Functions with Cardinality\n  Constraints", "comments": "Error description: The proposed algorithms have running time issues,\n  in particular they are pseudo-polynomial and not fully polynomial-time", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DS cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We consider the problem of maximizing a non-monotone DR-submodular function\nsubject to a cardinality constraint. Diminishing returns (DR) submodularity is\na generalization of the diminishing returns property for functions defined over\nthe integer lattice. This generalization can be used to solve many machine\nlearning or combinatorial optimization problems such as optimal budget\nallocation, revenue maximization, etc. In this work we propose the first\npolynomial-time approximation algorithms for non-monotone constrained\nmaximization. We implement our algorithms for a revenue maximization problem\nwith a real-world dataset to check their efficiency and performance.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 03:40:36 GMT"}, {"version": "v2", "created": "Sun, 3 Sep 2017 15:35:48 GMT"}], "update_date": "2017-09-05", "authors_parsed": [["Khodabakhsh", "Ali", ""], ["Nikolova", "Evdokia", ""]]}, {"id": "1611.09526", "submitter": "Shuhui Qu", "authors": "Shuhui Qu, Juncheng Li, Wei Dai, Samarjit Das", "title": "Learning Filter Banks Using Deep Learning For Acoustic Signals", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.SD cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Designing appropriate features for acoustic event recognition tasks is an\nactive field of research. Expressive features should both improve the\nperformance of the tasks and also be interpret-able. Currently, heuristically\ndesigned features based on the domain knowledge requires tremendous effort in\nhand-crafting, while features extracted through deep network are difficult for\nhuman to interpret. In this work, we explore the experience guided learning\nmethod for designing acoustic features. This is a novel hybrid approach\ncombining both domain knowledge and purely data driven feature designing. Based\non the procedure of log Mel-filter banks, we design a filter bank learning\nlayer. We concatenate this layer with a convolutional neural network (CNN)\nmodel. After training the network, the weight of the filter bank learning layer\nis extracted to facilitate the design of acoustic features. We smooth the\ntrained weight of the learning layer and re-initialize it in filter bank\nlearning layer as audio feature extractor. For the environmental sound\nrecognition task based on the Urban- sound8K dataset, the experience guided\nlearning leads to a 2% accuracy improvement compared with the fixed feature\nextractors (the log Mel-filter bank). The shape of the new filter banks are\nvisualized and explained to prove the effectiveness of the feature design\nprocess.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 08:46:26 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Qu", "Shuhui", ""], ["Li", "Juncheng", ""], ["Dai", "Wei", ""], ["Das", "Samarjit", ""]]}, {"id": "1611.09573", "submitter": "Anoop V S", "authors": "V. S. Anoop, S. Asharaf and P. Deepak", "title": "Learning Concept Hierarchies through Probabilistic Topic Modeling", "comments": null, "journal-ref": "International Journal of Information Processing (IJIP), Volume 10,\n  Issue 3, 2016", "doi": null, "report-no": null, "categories": "cs.AI cs.CL cs.IR", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  With the advent of semantic web, various tools and techniques have been\nintroduced for presenting and organizing knowledge. Concept hierarchies are one\nsuch technique which gained significant attention due to its usefulness in\ncreating domain ontologies that are considered as an integral part of semantic\nweb. Automated concept hierarchy learning algorithms focus on extracting\nrelevant concepts from unstructured text corpus and connect them together by\nidentifying some potential relations exist between them. In this paper, we\npropose a novel approach for identifying relevant concepts from plain text and\nthen learns hierarchy of concepts by exploiting subsumption relation between\nthem. To start with, we model topics using a probabilistic topic model and then\nmake use of some lightweight linguistic process to extract semantically rich\nconcepts. Then we connect concepts by identifying an \"is-a\" relationship\nbetween pair of concepts. The proposed method is completely unsupervised and\nthere is no need for a domain specific training corpus for concept extraction\nand learning. Experiments on large and real-world text corpora such as BBC News\ndataset and Reuters News corpus shows that the proposed method outperforms some\nof the existing methods for concept extraction and efficient concept hierarchy\nlearning is possible if the overall task is guided by a probabilistic topic\nmodeling algorithm.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 11:28:59 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Anoop", "V. S.", ""], ["Asharaf", "S.", ""], ["Deepak", "P.", ""]]}, {"id": "1611.09666", "submitter": "Yong Tan", "authors": "Yong Tan", "title": "Generic and Efficient Solution Solves the Shortest Paths Problem in\n  Square Runtime", "comments": "26 pages, 11,100 words, 2 pictures", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.DM cs.AI math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study a group of new methods to solve an open problem that is the shortest\npaths problem on a given fix-weighted instance. It is the real significance at\na considerable altitude to reach our aim to meet these qualities of generic,\nefficiency, precision which we generally require to a methodology. Besides our\nproof to guarantee our measures might work normally, we pay more interest to\nroot out the vital theory about calculation and logic in favor of our extension\nto range over a wide field about decision, operator, economy, management,\nrobot, AI and etc.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 14:56:31 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Tan", "Yong", ""]]}, {"id": "1611.09703", "submitter": "Ji\\v{r}\\'i Vysko\\v{c}il", "authors": "Cezary Kaliszyk, Josef Urban, Ji\\v{r}\\'i Vysko\\v{c}il", "title": "Semantic Parsing of Mathematics by Context-based Learning from Aligned\n  Corpora and Theorem Proving", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We study methods for automated parsing of informal mathematical expressions\ninto formal ones, a main prerequisite for deep computer understanding of\ninformal mathematical texts. We propose a context-based parsing approach that\ncombines efficient statistical learning of deep parse trees with their semantic\npruning by type checking and large-theory automated theorem proving. We show\nthat the methods very significantly improve on previous results in parsing\ntheorems from the Flyspeck corpus.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 16:20:24 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Kaliszyk", "Cezary", ""], ["Urban", "Josef", ""], ["Vysko\u010dil", "Ji\u0159\u00ed", ""]]}, {"id": "1611.09755", "submitter": "Saptarshi Das", "authors": "Indranil Pan and Saptarshi Das", "title": "Fractional Order AGC for Distributed Energy Resources Using Robust\n  Optimization", "comments": "12 pages, 16 figures, 5 tables", "journal-ref": "IEEE Transactions on Smart Grid, Volume 7, Issue 5, Pages 2175 -\n  2186, Sept 2016", "doi": "10.1109/TSG.2015.2459766", "report-no": null, "categories": "cs.SY cs.AI cs.NE math.OC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The applicability of fractional order (FO) automatic generation control (AGC)\nfor power system frequency oscillation damping is investigated in this paper,\nemploying distributed energy generation. The hybrid power system employs\nvarious autonomous generation systems like wind turbine, solar photovoltaic,\ndiesel engine, fuel-cell and aqua electrolyzer along with other energy storage\ndevices like the battery and flywheel. The controller is placed in a remote\nlocation while receiving and sending signals over an unreliable communication\nnetwork with stochastic delay. The controller parameters are tuned using robust\noptimization techniques employing different variants of Particle Swarm\nOptimization (PSO) and are compared with the corresponding optimal solutions.\nAn archival based strategy is used for reducing the number of function\nevaluations for the robust optimization methods. The solutions obtained through\nthe robust optimization are able to handle higher variation in the controller\ngains and orders without significant decrease in the system performance. This\nis desirable from the FO controller implementation point of view, as the design\nis able to accommodate variations in the system parameter which may result due\nto the approximation of FO operators, using different realization methods and\norder of accuracy. Also a comparison is made between the FO and the integer\norder (IO) controllers to highlight the merits and demerits of each scheme.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 17:56:02 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Pan", "Indranil", ""], ["Das", "Saptarshi", ""]]}, {"id": "1611.09809", "submitter": "Saptarshi Das", "authors": "Indranil Pan and Saptarshi Das", "title": "Fractional Order Fuzzy Control of Hybrid Power System with Renewable\n  Generation Using Chaotic PSO", "comments": "21 pages, 12 figures, 4 tables", "journal-ref": "ISA Transactions, Volume 62, May 2016, Pages 19-29", "doi": "10.1016/j.isatra.2015.03.003", "report-no": null, "categories": "cs.SY cs.AI math.OC nlin.CD", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper investigates the operation of a hybrid power system through a\nnovel fuzzy control scheme. The hybrid power system employs various autonomous\ngeneration systems like wind turbine, solar photovoltaic, diesel engine,\nfuel-cell, aqua electrolyzer etc. Other energy storage devices like the\nbattery, flywheel and ultra-capacitor are also present in the network. A novel\nfractional order (FO) fuzzy control scheme is employed and its parameters are\ntuned with a particle swarm optimization (PSO) algorithm augmented with two\nchaotic maps for achieving an improved performance. This FO fuzzy controller\nshows better performance over the classical PID, and the integer order fuzzy\nPID controller in both linear and nonlinear operating regimes. The FO fuzzy\ncontroller also shows stronger robustness properties against system parameter\nvariation and rate constraint nonlinearity, than that with the other controller\nstructures. The robustness is a highly desirable property in such a scenario\nsince many components of the hybrid power system may be switched on/off or may\nrun at lower/higher power output, at different time instants.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 19:54:44 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Pan", "Indranil", ""], ["Das", "Saptarshi", ""]]}, {"id": "1611.09819", "submitter": "Daniel Harari", "authors": "Daniel Harari, Tao Gao, Nancy Kanwisher, Joshua Tenenbaum, Shimon\n  Ullman", "title": "Measuring and modeling the perception of natural and unconstrained gaze\n  in humans and machines", "comments": "Daniel Harari and Tao Gao contributed equally to this work", "journal-ref": null, "doi": null, "report-no": "Center for Brains, Minds and Machines Memo No. 059", "categories": "q-bio.NC cs.AI cs.CV cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Humans are remarkably adept at interpreting the gaze direction of other\nindividuals in their surroundings. This skill is at the core of the ability to\nengage in joint visual attention, which is essential for establishing social\ninteractions. How accurate are humans in determining the gaze direction of\nothers in lifelike scenes, when they can move their heads and eyes freely, and\nwhat are the sources of information for the underlying perceptual processes?\nThese questions pose a challenge from both empirical and computational\nperspectives, due to the complexity of the visual input in real-life\nsituations. Here we measure empirically human accuracy in perceiving the gaze\ndirection of others in lifelike scenes, and study computationally the sources\nof information and representations underlying this cognitive capacity. We show\nthat humans perform better in face-to-face conditions compared with recorded\nconditions, and that this advantage is not due to the availability of input\ndynamics. We further show that humans are still performing well when only the\neyes-region is visible, rather than the whole face. We develop a computational\nmodel, which replicates the pattern of human performance, including the finding\nthat the eyes-region contains on its own, the required information for\nestimating both head orientation and direction of gaze. Consistent with\nneurophysiological findings on task-specific face regions in the brain, the\nlearned computational representations reproduce perceptual effects such as the\nWollaston illusion, when trained to estimate direction of gaze, but not when\ntrained to recognize objects or faces.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 20:11:09 GMT"}], "update_date": "2016-11-30", "authors_parsed": [["Harari", "Daniel", ""], ["Gao", "Tao", ""], ["Kanwisher", "Nancy", ""], ["Tenenbaum", "Joshua", ""], ["Ullman", "Shimon", ""]]}, {"id": "1611.09823", "submitter": "Jason  Weston", "authors": "Jiwei Li, Alexander H. Miller, Sumit Chopra, Marc'Aurelio Ranzato,\n  Jason Weston", "title": "Dialogue Learning With Human-In-The-Loop", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CL", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  An important aspect of developing conversational agents is to give a bot the\nability to improve through communicating with humans and to learn from the\nmistakes that it makes. Most research has focused on learning from fixed\ntraining sets of labeled data rather than interacting with a dialogue partner\nin an online fashion. In this paper we explore this direction in a\nreinforcement learning setting where the bot improves its question-answering\nability from feedback a teacher gives following its generated responses. We\nbuild a simulator that tests various aspects of such learning in a synthetic\nenvironment, and introduce models that work in this regime. Finally, real\nexperiments with Mechanical Turk validate the approach.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 20:16:44 GMT"}, {"version": "v2", "created": "Fri, 16 Dec 2016 00:22:53 GMT"}, {"version": "v3", "created": "Fri, 13 Jan 2017 21:12:38 GMT"}], "update_date": "2017-01-17", "authors_parsed": [["Li", "Jiwei", ""], ["Miller", "Alexander H.", ""], ["Chopra", "Sumit", ""], ["Ranzato", "Marc'Aurelio", ""], ["Weston", "Jason", ""]]}, {"id": "1611.09830", "submitter": "Tong Wang", "authors": "Adam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro\n  Sordoni, Philip Bachman, Kaheer Suleman", "title": "NewsQA: A Machine Comprehension Dataset", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.CL cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present NewsQA, a challenging machine comprehension dataset of over\n100,000 human-generated question-answer pairs. Crowdworkers supply questions\nand answers based on a set of over 10,000 news articles from CNN, with answers\nconsisting of spans of text from the corresponding articles. We collect this\ndataset through a four-stage process designed to solicit exploratory questions\nthat require reasoning. A thorough analysis confirms that NewsQA demands\nabilities beyond simple word matching and recognizing textual entailment. We\nmeasure human performance on the dataset and compare it to several strong\nneural models. The performance gap between humans and machines (0.198 in F1)\nindicates that significant progress can be made on NewsQA through future\nresearch. The dataset is freely available at\nhttps://datasets.maluuba.com/NewsQA.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 20:38:07 GMT"}, {"version": "v2", "created": "Thu, 22 Dec 2016 18:12:57 GMT"}, {"version": "v3", "created": "Tue, 7 Feb 2017 16:27:59 GMT"}], "update_date": "2017-02-08", "authors_parsed": [["Trischler", "Adam", ""], ["Wang", "Tong", ""], ["Yuan", "Xingdi", ""], ["Harris", "Justin", ""], ["Sordoni", "Alessandro", ""], ["Bachman", "Philip", ""], ["Suleman", "Kaheer", ""]]}, {"id": "1611.09894", "submitter": "Sai Praveen Bangaru", "authors": "Sai Praveen Bangaru, JS Suhas and Balaraman Ravindran", "title": "Exploration for Multi-task Reinforcement Learning with Deep Generative\n  Models", "comments": "9 pages, 5 figures; NIPS Deep Reinforcement Learning Workshop 2016,\n  Barcelona", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Exploration in multi-task reinforcement learning is critical in training\nagents to deduce the underlying MDP. Many of the existing exploration\nframeworks such as $E^3$, $R_{max}$, Thompson sampling assume a single\nstationary MDP and are not suitable for system identification in the multi-task\nsetting. We present a novel method to facilitate exploration in multi-task\nreinforcement learning using deep generative models. We supplement our method\nwith a low dimensional energy model to learn the underlying MDP distribution\nand provide a resilient and adaptive exploration signal to the agent. We\nevaluate our method on a new set of environments and provide intuitive\ninterpretation of our results.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 21:32:25 GMT"}], "update_date": "2016-12-04", "authors_parsed": [["Bangaru", "Sai Praveen", ""], ["Suhas", "JS", ""], ["Ravindran", "Balaraman", ""]]}, {"id": "1611.09904", "submitter": "Olof Mogren", "authors": "Olof Mogren", "title": "C-RNN-GAN: Continuous recurrent neural networks with adversarial\n  training", "comments": "Accepted to Constructive Machine Learning Workshop (CML) at NIPS 2016\n  in Barcelona, Spain, December 10", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG", "license": "http://creativecommons.org/licenses/by-sa/4.0/", "abstract": "  Generative adversarial networks have been proposed as a way of efficiently\ntraining deep generative neural networks. We propose a generative adversarial\nmodel that works on continuous sequential data, and apply it by training it on\na collection of classical music. We conclude that it generates music that\nsounds better and better as the model is trained, report statistics on\ngenerated music, and let the reader judge the quality by downloading the\ngenerated songs.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 21:53:09 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Mogren", "Olof", ""]]}, {"id": "1611.09913", "submitter": "Jasmine Collins", "authors": "Jasmine Collins, Jascha Sohl-Dickstein and David Sussillo", "title": "Capacity and Trainability in Recurrent Neural Networks", "comments": "Published as a conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.ML cs.AI cs.LG cs.NE", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Two potential bottlenecks on the expressiveness of recurrent neural networks\n(RNNs) are their ability to store information about the task in their\nparameters, and to store information about the input history in their units. We\nshow experimentally that all common RNN architectures achieve nearly the same\nper-task and per-unit capacity bounds with careful training, for a variety of\ntasks and stacking depths. They can store an amount of task information which\nis linear in the number of parameters, and is approximately 5 bits per\nparameter. They can additionally store approximately one real number from their\ninput history per hidden unit. We further find that for several tasks it is the\nper-task parameter capacity bound that determines performance. These results\nsuggest that many previous results comparing RNN architectures are driven\nprimarily by differences in training effectiveness, rather than differences in\ncapacity. Supporting this observation, we compare training difficulty for\nseveral architectures, and show that vanilla RNNs are far more difficult to\ntrain, yet have slightly higher capacity. Finally, we propose two novel RNN\narchitectures, one of which is easier to train than the LSTM or GRU for deeply\nstacked architectures.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 22:13:20 GMT"}, {"version": "v2", "created": "Tue, 28 Feb 2017 22:21:44 GMT"}, {"version": "v3", "created": "Fri, 3 Mar 2017 17:39:34 GMT"}], "update_date": "2017-03-06", "authors_parsed": [["Collins", "Jasmine", ""], ["Sohl-Dickstein", "Jascha", ""], ["Sussillo", "David", ""]]}, {"id": "1611.09926", "submitter": "Mikhail Timonin", "authors": "Mikhail Timonin", "title": "Choquet integral in decision analysis - lessons from the axiomatization", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "q-fin.EC cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The Choquet integral is a powerful aggregation operator which lists many\nwell-known models as its special cases. We look at these special cases and\nprovide their axiomatic analysis. In cases where an axiomatization has been\npreviously given in the literature, we connect the existing results with the\nframework that we have developed. Next we turn to the question of learning,\nwhich is especially important for the practical applications of the model. So\nfar, learning of the Choquet integral has been mostly confined to the learning\nof the capacity. Such an approach requires making a powerful assumption that\nall dimensions (e.g. criteria) are evaluated on the same scale, which is rarely\njustified in practice. Too often categorical data is given arbitrary numerical\nlabels (e.g. AHP), and numerical data is considered cardinally and ordinally\ncommensurate, sometimes after a simple normalization. Such approaches clearly\nlack scientific rigour, and yet they are commonly seen in all kinds of\napplications. We discuss the pros and cons of making such an assumption and\nlook at the consequences which axiomatization uniqueness results have for the\nlearning problems. Finally, we review some of the applications of the Choquet\nintegral in decision analysis. Apart from MCDA, which is the main area of\ninterest for our results, we also discuss how the model can be interpreted in\nthe social choice context. We look in detail at the state-dependent utility,\nand show how comonotonicity, central to the previous axiomatizations, actually\nimplies state-independency in the Choquet integral model. We also discuss the\nconditions required to have a meaningful state-dependent utility representation\nand show the novelty of our results compared to the previous methods of\nbuilding state-dependent models.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 22:36:01 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Timonin", "Mikhail", ""]]}, {"id": "1611.09940", "submitter": "Irwan Bello", "authors": "Irwan Bello, Hieu Pham, Quoc V. Le, Mohammad Norouzi, Samy Bengio", "title": "Neural Combinatorial Optimization with Reinforcement Learning", "comments": "Under review as a conference paper at ICLR 2017", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a framework to tackle combinatorial optimization problems\nusing neural networks and reinforcement learning. We focus on the traveling\nsalesman problem (TSP) and train a recurrent network that, given a set of city\ncoordinates, predicts a distribution over different city permutations. Using\nnegative tour length as the reward signal, we optimize the parameters of the\nrecurrent network using a policy gradient method. We compare learning the\nnetwork parameters on a set of training graphs against learning them on\nindividual test graphs. Despite the computational expense, without much\nengineering and heuristic designing, Neural Combinatorial Optimization achieves\nclose to optimal results on 2D Euclidean graphs with up to 100 nodes. Applied\nto the KnapSack, another NP-hard problem, the same method obtains optimal\nsolutions for instances with up to 200 items.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 23:22:39 GMT"}, {"version": "v2", "created": "Sun, 11 Dec 2016 00:31:39 GMT"}, {"version": "v3", "created": "Thu, 12 Jan 2017 23:55:36 GMT"}], "update_date": "2017-01-16", "authors_parsed": [["Bello", "Irwan", ""], ["Pham", "Hieu", ""], ["Le", "Quoc V.", ""], ["Norouzi", "Mohammad", ""], ["Bengio", "Samy", ""]]}, {"id": "1611.09948", "submitter": "Fionn Murtagh", "authors": "Fionn Murtagh and Mohsen Farid", "title": "Contextualizing Geometric Data Analysis and Related Data Analytics: A\n  Virtual Microscope for Big Data Analytics", "comments": "19 pages, 8 figures, 2 tables, Journal of Interdisciplinary\n  Methodologies and Issues in Science, vol. 3, 2017. This version contains DOI,\n  ISSN", "journal-ref": "Journal of Interdisciplinary Methodologies and Issues in Sciences,\n  Digital Contextualization (September 19, 2017) jimis:3936", "doi": "10.18713/JIMIS-010917-3-1", "report-no": null, "categories": "cs.AI cs.CY", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The relevance and importance of contextualizing data analytics is described.\nQualitative characteristics might form the context of quantitative analysis.\nTopics that are at issue include: contrast, baselining, secondary data sources,\nsupplementary data sources, dynamic and heterogeneous data. In geometric data\nanalysis, especially with the Correspondence Analysis platform, various case\nstudies are both experimented with, and are reviewed. In such aspects as\nparadigms followed, and technical implementation, implicitly and explicitly, an\nimportant point made is the major relevance of such work for both burgeoning\nanalytical needs and for new analytical areas including Big Data analytics, and\nso on. For the general reader, it is aimed to display and describe, first of\nall, the analytical outcomes that are subject to analysis here, and then\nproceed to detail the more quantitative outcomes that fully support the\nanalytics carried out.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 00:04:06 GMT"}, {"version": "v2", "created": "Tue, 23 May 2017 22:57:08 GMT"}, {"version": "v3", "created": "Sat, 3 Jun 2017 11:23:21 GMT"}, {"version": "v4", "created": "Fri, 15 Sep 2017 15:44:23 GMT"}], "update_date": "2017-10-11", "authors_parsed": [["Murtagh", "Fionn", ""], ["Farid", "Mohsen", ""]]}, {"id": "1611.09957", "submitter": "Ehsan Amid", "authors": "Ehsan Amid, Nikos Vlassis, Manfred K. Warmuth", "title": "Low-dimensional Data Embedding via Robust Ranking", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.LG stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We describe a new method called t-ETE for finding a low-dimensional embedding\nof a set of objects in Euclidean space. We formulate the embedding problem as a\njoint ranking problem over a set of triplets, where each triplet captures the\nrelative similarities between three objects in the set. By exploiting recent\nadvances in robust ranking, t-ETE produces high-quality embeddings even in the\npresence of a significant amount of noise and better preserves local scale than\nknown methods, such as t-STE and t-SNE. In particular, our method produces\nsignificantly better results than t-SNE on signature datasets while also being\nfaster to compute.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 01:03:11 GMT"}, {"version": "v2", "created": "Tue, 16 May 2017 21:21:03 GMT"}], "update_date": "2017-05-18", "authors_parsed": [["Amid", "Ehsan", ""], ["Vlassis", "Nikos", ""], ["Warmuth", "Manfred K.", ""]]}, {"id": "1611.10095", "submitter": "Pietro Speroni Di Fenizio Ph.D.", "authors": "Pietro Speroni di Fenizio, Cyril Velikanov", "title": "System-Generated Requests for Rewriting Proposals", "comments": "9 pages, 1 figure, presented at e-Part 2011 conference", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.CY cs.HC cs.SI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We present an online deliberation system using mutual evaluation in order to\ncollaboratively develop solutions. Participants submit their proposals and\nevaluate each other's proposals; some of them may then be invited by the system\nto rewrite 'problematic' proposals. Two cases are discussed: a proposal\nsupported by many, but not by a given person, who is then invited to rewrite it\nfor making yet more acceptable; and a poorly presented but presumably\ninteresting proposal. The first of these cases has been successfully\nimplemented. Proposals are evaluated along two axes-understandability (or\nclarity, or, more generally, quality), and agreement. The latter is used by the\nsystem to cluster proposals according to their ideas, while the former is used\nboth to present the best proposals on top of their clusters, and to find poorly\nwritten proposals candidates for rewriting. These functionalities may be\nconsidered as important components of a large scale online deliberation system.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 11:29:25 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["di Fenizio", "Pietro Speroni", ""], ["Velikanov", "Cyril", ""]]}, {"id": "1611.10120", "submitter": "Nattapong Thammasan", "authors": "Nattapong Thammasan, Ken-ichi Fukui, Masayuki Numao", "title": "Fusion of EEG and Musical Features in Continuous Music-emotion\n  Recognition", "comments": "The short version of this paper is accepted to appear as an abstract\n  in the proceedings of AAAI-17 (student abstract and poster program)", "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.AI cs.HC", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Emotion estimation in music listening is confronting challenges to capture\nthe emotion variation of listeners. Recent years have witnessed attempts to\nexploit multimodality fusing information from musical contents and\nphysiological signals captured from listeners to improve the performance of\nemotion recognition. In this paper, we present a study of fusion of signals of\nelectroencephalogram (EEG), a tool to capture brainwaves at a high-temporal\nresolution, and musical features at decision level in recognizing the\ntime-varying binary classes of arousal and valence. Our empirical results\nshowed that the fusion could outperform the performance of emotion recognition\nusing only EEG modality that was suffered from inter-subject variability, and\nthis suggested the promise of multimodal fusion in improving the accuracy of\nmusic-emotion recognition.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 12:24:57 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Thammasan", "Nattapong", ""], ["Fukui", "Ken-ichi", ""], ["Numao", "Masayuki", ""]]}, {"id": "1611.10215", "submitter": "Gal Dalal", "authors": "Gal Dalal, Elad Gilboa, Shie Mannor, Louis Wehenkel", "title": "Unit Commitment using Nearest Neighbor as a Short-Term Proxy", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We devise the Unit Commitment Nearest Neighbor (UCNN) algorithm to be used as\na proxy for quickly approximating outcomes of short-term decisions, to make\ntractable hierarchical long-term assessment and planning for large power\nsystems. Experimental results on updated versions of IEEE-RTS79 and IEEE-RTS96\nshow high accuracy measured on operational cost, achieved in runtimes that are\nlower in several orders of magnitude than the traditional approach.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 15:24:55 GMT"}, {"version": "v2", "created": "Wed, 18 Oct 2017 19:29:49 GMT"}, {"version": "v3", "created": "Wed, 28 Feb 2018 11:28:55 GMT"}], "update_date": "2018-03-01", "authors_parsed": [["Dalal", "Gal", ""], ["Gilboa", "Elad", ""], ["Mannor", "Shie", ""], ["Wehenkel", "Louis", ""]]}, {"id": "1611.10252", "submitter": "Jingkang Yang", "authors": "Jingkang Yang, Haohan Wang, Jun Zhu, Eric P. Xing", "title": "SeDMiD for Confusion Detection: Uncovering Mind State from Time Series\n  Brain Wave Data", "comments": "11 pages, 2 figures, NIPS 2016 Time Series Workshop", "journal-ref": null, "doi": null, "report-no": null, "categories": "q-bio.NC cs.AI cs.LG", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  Understanding how brain functions has been an intriguing topic for years.\nWith the recent progress on collecting massive data and developing advanced\ntechnology, people have become interested in addressing the challenge of\ndecoding brain wave data into meaningful mind states, with many machine\nlearning models and algorithms being revisited and developed, especially the\nones that handle time series data because of the nature of brain waves.\nHowever, many of these time series models, like HMM with hidden state in\ndiscrete space or State Space Model with hidden state in continuous space, only\nwork with one source of data and cannot handle different sources of information\nsimultaneously. In this paper, we propose an extension of State Space Model to\nwork with different sources of information together with its learning and\ninference algorithms. We apply this model to decode the mind state of students\nduring lectures based on their brain waves and reach a significant better\nresults compared to traditional methods.\n", "versions": [{"version": "v1", "created": "Tue, 29 Nov 2016 18:11:00 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Yang", "Jingkang", ""], ["Wang", "Haohan", ""], ["Zhu", "Jun", ""], ["Xing", "Eric P.", ""]]}, {"id": "1611.10328", "submitter": "Maciej Wielgosz", "authors": "Maciej Wielgosz", "title": "The observer-assisted method for adjusting hyper-parameters in deep\n  learning algorithms", "comments": null, "journal-ref": null, "doi": null, "report-no": null, "categories": "cs.LG cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  This paper presents a concept of a novel method for adjusting\nhyper-parameters in Deep Learning (DL) algorithms. An external agent-observer\nmonitors a performance of a selected Deep Learning algorithm. The observer\nlearns to model the DL algorithm using a series of random experiments.\nConsequently, it may be used for predicting a response of the DL algorithm in\nterms of a selected quality measurement to a set of hyper-parameters. This\nallows to construct an ensemble composed of a series of evaluators which\nconstitute an observer-assisted architecture. The architecture may be used to\ngradually iterate towards to the best achievable quality score in tiny steps\ngoverned by a unit of progress. The algorithm is stopped when the maximum\nnumber of steps is reached or no further progress is made.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 19:37:48 GMT"}], "update_date": "2016-12-01", "authors_parsed": [["Wielgosz", "Maciej", ""]]}, {"id": "1611.10331", "submitter": "John Mount", "authors": "John Mount and Nina Zumel", "title": "Comparing Apples and Oranges: Two Examples of the Limits of Statistical\n  Inference, With an Application to Google Advertising Markets", "comments": "21 pages", "journal-ref": null, "doi": null, "report-no": null, "categories": "stat.AP cs.AI", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  We show how the classic Cramer-Rao bound limits how accurately one can\nsimultaneously estimate values of a large number of Google Ad campaigns (or\nsimilarly limit the measurement rate of many confounding A/B tests).\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 19:49:55 GMT"}], "update_date": "2016-12-04", "authors_parsed": [["Mount", "John", ""], ["Zumel", "Nina", ""]]}, {"id": "1611.10351", "submitter": "Joris Mooij", "authors": "Joris M. Mooij, Sara Magliacane, Tom Claassen", "title": "Joint Causal Inference from Multiple Contexts", "comments": "Final version, as published by JMLR", "journal-ref": "Journal of Machine Learning Research 21(99):1-108, 2020", "doi": null, "report-no": null, "categories": "cs.LG cs.AI stat.ML", "license": "http://arxiv.org/licenses/nonexclusive-distrib/1.0/", "abstract": "  The gold standard for discovering causal relations is by means of\nexperimentation. Over the last decades, alternative methods have been proposed\nthat can infer causal relations between variables from certain statistical\npatterns in purely observational data. We introduce Joint Causal Inference\n(JCI), a novel approach to causal discovery from multiple data sets from\ndifferent contexts that elegantly unifies both approaches. JCI is a causal\nmodeling framework rather than a specific algorithm, and it can be implemented\nusing any causal discovery algorithm that can take into account certain\nbackground knowledge. JCI can deal with different types of interventions (e.g.,\nperfect, imperfect, stochastic, etc.) in a unified fashion, and does not\nrequire knowledge of intervention targets or types in case of interventional\ndata. We explain how several well-known causal discovery algorithms can be seen\nas addressing special cases of the JCI framework, and we also propose novel\nimplementations that extend existing causal discovery methods for purely\nobservational data to the JCI setting. We evaluate different JCI\nimplementations on synthetic data and on flow cytometry protein expression data\nand conclude that JCI implementations can considerably outperform\nstate-of-the-art causal discovery algorithms.\n", "versions": [{"version": "v1", "created": "Wed, 30 Nov 2016 20:50:33 GMT"}, {"version": "v2", "created": "Thu, 9 Mar 2017 12:43:04 GMT"}, {"version": "v3", "created": "Thu, 29 Mar 2018 21:18:18 GMT"}, {"version": "v4", "created": "Mon, 8 Apr 2019 12:42:55 GMT"}, {"version": "v5", "created": "Fri, 14 Feb 2020 12:14:39 GMT"}, {"version": "v6", "created": "Thu, 20 Aug 2020 08:23:07 GMT"}], "update_date": "2020-08-21", "authors_parsed": [["Mooij", "Joris M.", ""], ["Magliacane", "Sara", ""], ["Claassen", "Tom", ""]]}]